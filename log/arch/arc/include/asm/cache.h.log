commit f091d5a426447cc427680bdd3adc7773aa2867df
Author: Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com>
Date:   Fri Nov 8 19:20:22 2019 +0300

    ARC: ARCv2: jump label: implement jump label patching
    
    Implement jump label patching for ARC. Jump labels provide
    an interface to generate dynamic branches using
    self-modifying code.
    
    This allows us to implement conditional branches where
    changing branch direction is expensive but branch selection
    is basically 'free'
    
    This implementation uses 32-bit NOP and BRANCH instructions
    which forced to be aligned by 4 to guarantee that they don't
    cross L1 cache line boundary and can be update atomically.
    
    Signed-off-by: Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 918804c7c1a4..d8ece4292388 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -25,6 +25,8 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/build_bug.h>
+
 /* Uncached access macros */
 #define arc_read_uncached_32(ptr)	\
 ({					\

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 2ad77fb43639..918804c7c1a4 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -1,9 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2004, 2007-2010, 2011-2012 Synopsys, Inc. (www.synopsys.com)
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #ifndef __ARC_ASM_CACHE_H

commit b6835ea77729e7faf4656ca637ba53f42b8ee3fd
Author: Alexey Brodkin <abrodkin@synopsys.com>
Date:   Fri Feb 8 13:55:19 2019 +0300

    ARC: define ARCH_SLAB_MINALIGN = 8
    
    The default value of ARCH_SLAB_MINALIGN in "include/linux/slab.h" is
    "__alignof__(unsigned long long)" which for ARC unexpectedly turns out
    to be 4. This is not a compiler bug, but as defined by ARC ABI [1]
    
    Thus slab allocator would allocate a struct which is 32-bit aligned,
    which is generally OK even if struct has long long members.
    There was however potetial problem when it had any atomic64_t which
    use LLOCKD/SCONDD instructions which are required by ISA to take
    64-bit addresses. This is the problem we ran into
    
    [    4.015732] EXT4-fs (mmcblk0p2): re-mounted. Opts: (null)
    [    4.167881] Misaligned Access
    [    4.172356] Path: /bin/busybox.nosuid
    [    4.176004] CPU: 2 PID: 171 Comm: rm Not tainted 4.19.14-yocto-standard #1
    [    4.182851]
    [    4.182851] [ECR   ]: 0x000d0000 => Check Programmer's Manual
    [    4.190061] [EFA   ]: 0xbeaec3fc
    [    4.190061] [BLINK ]: ext4_delete_entry+0x210/0x234
    [    4.190061] [ERET  ]: ext4_delete_entry+0x13e/0x234
    [    4.202985] [STAT32]: 0x80080002 : IE K
    [    4.207236] BTA: 0x9009329c   SP: 0xbe5b1ec4  FP: 0x00000000
    [    4.212790] LPS: 0x9074b118  LPE: 0x9074b120 LPC: 0x00000000
    [    4.218348] r00: 0x00000040  r01: 0x00000021 r02: 0x00000001
    ...
    ...
    [    4.270510] Stack Trace:
    [    4.274510]   ext4_delete_entry+0x13e/0x234
    [    4.278695]   ext4_rmdir+0xe0/0x238
    [    4.282187]   vfs_rmdir+0x50/0xf0
    [    4.285492]   do_rmdir+0x9e/0x154
    [    4.288802]   EV_Trap+0x110/0x114
    
    The fix is to make sure slab allocations are 64-bit aligned.
    
    Do note that atomic64_t is __attribute__((aligned(8)) which means gcc
    does generate 64-bit aligned references, relative to beginning of
    container struct. However the issue is if the container itself is not
    64-bit aligned, atomic64_t ends up unaligned which is what this patch
    ensures.
    
    [1] https://github.com/foss-for-synopsys-dwc-arc-processors/toolchain/wiki/files/ARCv2_ABI.pdf
    
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    Cc: <stable@vger.kernel.org> # 4.8+
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    [vgupta: reworked changelog, added dependency on LL64+LLSC]

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index f393b663413e..2ad77fb43639 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -52,6 +52,17 @@
 #define cache_line_size()	SMP_CACHE_BYTES
 #define ARCH_DMA_MINALIGN	SMP_CACHE_BYTES
 
+/*
+ * Make sure slab-allocated buffers are 64-bit aligned when atomic64_t uses
+ * ARCv2 64-bit atomics (LLOCKD/SCONDD). This guarantess runtime 64-bit
+ * alignment for any atomic64_t embedded in buffer.
+ * Default ARCH_SLAB_MINALIGN is __alignof__(long long) which has a relaxed
+ * value of 4 (and not 8) in ARC ABI.
+ */
+#if defined(CONFIG_ARC_HAS_LL64) && defined(CONFIG_ARC_HAS_LLSC)
+#define ARCH_SLAB_MINALIGN	8
+#endif
+
 extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
 extern void read_decode_cache_bcr(void);

commit 3624379d90ad2b65f9dbb30d7f7ce5498d2fe322
Author: Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com>
Date:   Thu Oct 4 16:12:12 2018 +0300

    ARC: IOC: panic if kernel was started with previously enabled IOC
    
    If IOC was already enabled (due to bootloader) it technically needs to
    be reconfigured with aperture base,size corresponding to Linux memory map
    which will certainly be different than uboot's. But disabling and
    reenabling IOC when DMA might be potentially active is tricky business.
    To avoid random memory issues later, just panic here and ask user to
    upgrade bootloader to one which doesn't enable IOC
    
    This was actually seen as issue on some of the HSDK board with a version
    of uboot which enabled IOC. There were random issues later with starting
    of X or peripherals etc.
    
    Also while I'm at it, replace hardcoded bits in ARC_REG_IO_COH_PARTIAL
    and ARC_REG_IO_COH_ENABLE registers by definitions.
    
    Inspired by: https://lkml.org/lkml/2018/1/19/557
    Signed-off-by: Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index ff7d3232764a..f393b663413e 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -113,7 +113,9 @@ extern unsigned long perip_base, perip_end;
 
 /* IO coherency related Auxiliary registers */
 #define ARC_REG_IO_COH_ENABLE	0x500
+#define ARC_IO_COH_ENABLE_BIT	BIT(0)
 #define ARC_REG_IO_COH_PARTIAL	0x501
+#define ARC_IO_COH_PARTIAL_BIT	BIT(0)
 #define ARC_REG_IO_COH_AP0_BASE	0x508
 #define ARC_REG_IO_COH_AP0_SIZE	0x509
 

commit eb2777397fd83a4a7eaa26984d09d3babb845d2a
Author: Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com>
Date:   Thu Jul 26 16:15:43 2018 +0300

    ARC: dma [non-IOC] setup SMP_CACHE_BYTES and cache_line_size
    
    As for today we don't setup SMP_CACHE_BYTES and cache_line_size for
    ARC, so they are set to L1_CACHE_BYTES by default. L1 line length
    (L1_CACHE_BYTES) might be easily smaller than L2 line (which is
    usually the case BTW). This breaks code.
    
    For example this breaks ethernet infrastructure on HSDK/AXS103 boards
    with IOC disabled, involving manual cache flushes
    Functions which alloc and manage sk_buff packet data area rely on
    SMP_CACHE_BYTES define. In the result we can share last L2 cache
    line in sk_buff linear packet data area between DMA buffer and
    some useful data in other structure. So we can lose this data when
    we invalidate DMA buffer.
    
       sk_buff linear packet data area
                    |
                    |
                    |         skb->end        skb->tail
                    V            |                |
                                 V                V
    ----------------------------------------------.
          packet data            | <tail padding> |  <useful data in other struct>
    ----------------------------------------------.
    
    ---------------------.--------------------------------------------------.
         SLC line        |             SLC (L2 cache) line (128B)           |
    ---------------------.--------------------------------------------------.
            ^                                     ^
            |                                     |
         These cache lines will be invalidated when we invalidate skb
         linear packet data area before DMA transaction starting.
    
    This leads to issues painful to debug as it reproduces only if
    (sk_buff->end - sk_buff->tail) < SLC_LINE_SIZE and
    if we have some useful data right after sk_buff->end.
    
    Fix that by hardcode SMP_CACHE_BYTES to max line length we may have.
    
    Signed-off-by: Eugeniy Paltsev <Eugeniy.Paltsev@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 8486f328cc5d..ff7d3232764a 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -48,7 +48,9 @@
 })
 
 /* Largest line length for either L1 or L2 is 128 bytes */
-#define ARCH_DMA_MINALIGN      128
+#define SMP_CACHE_BYTES		128
+#define cache_line_size()	SMP_CACHE_BYTES
+#define ARCH_DMA_MINALIGN	SMP_CACHE_BYTES
 
 extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);

commit ae0b63d97d8efc377cc5b161abccc6e3586b206f
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Tue Aug 1 10:23:27 2017 +0530

    ARCv2: SLC: provide a line based flush routine for debugging
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 6349af790c65..8486f328cc5d 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -96,6 +96,8 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_SLC_CTRL	0x903
 #define ARC_REG_SLC_FLUSH	0x904
 #define ARC_REG_SLC_INVALIDATE	0x905
+#define ARC_AUX_SLC_IVDL	0x910
+#define ARC_AUX_SLC_FLDL	0x912
 #define ARC_REG_SLC_RGN_START	0x914
 #define ARC_REG_SLC_RGN_START1	0x915
 #define ARC_REG_SLC_RGN_END	0x916

commit 9f82e90a6668e522c7fd0e0322c52d86f29b624d
Author: Alexey Brodkin <alexey.brodkin@gmail.com>
Date:   Tue Jul 18 17:31:24 2017 +0300

    ARC: Hardcode ARCH_DMA_MINALIGN to max line length we may have
    
    Current implementation relies on L1 line length which might easily
    be smaller than L2 line (which is usually the case BTW).
    
    Imagine this typical case: L2 line is 128 bytes while L1 line is
    64-bytes. Now we want to allocate small buffer and later use it for DMA
    (consider IOC is not available).
    
    kmalloc() allocates small KMALLOC_MIN_SIZE-sized, KMALLOC_MIN_SIZE-aligned
    That way if buffer happens to be aligned to L1 line and not L2 line we'll be
    flushing and invalidating extra portions of data from L2 which will cause
    cache coherency issues.
    
    And since KMALLOC_MIN_SIZE is bound to ARCH_DMA_MINALIGN the fix could
    be simple - set ARCH_DMA_MINALIGN to the largest cache line we may ever
    get. As of today neither L1 of ARC700 and ARC HS38 nor SLC might not be
    longer than 128 bytes.
    
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 02fd1cece6ef..6349af790c65 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -47,7 +47,8 @@
 	: "r"(data), "r"(ptr));		\
 })
 
-#define ARCH_DMA_MINALIGN      L1_CACHE_BYTES
+/* Largest line length for either L1 or L2 is 128 bytes */
+#define ARCH_DMA_MINALIGN      128
 
 extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);

commit 7d79cee2c6540ea64dd917a14e2fd63d4ac3d3c0
Author: Alexey Brodkin <Alexey.Brodkin@synopsys.com>
Date:   Tue Aug 1 12:58:47 2017 +0300

    ARCv2: PAE40: Explicitly set MSB counterpart of SLC region ops addresses
    
    It is necessary to explicitly set both SLC_AUX_RGN_START1 and SLC_AUX_RGN_END1
    which hold MSB bits of the physical address correspondingly of region start
    and end otherwise SLC region operation is executed in unpredictable manner
    
    Without this patch, SLC flushes on HSDK (IOC disabled) were taking
    seconds.
    
    Cc: stable@vger.kernel.org   #4.4+
    Reported-by: Vladimir Kondratiev <vladimir.kondratiev@intel.com>
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    [vgupta: PAR40 regs only written if PAE40 exist]

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 19ebddffb279..02fd1cece6ef 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -96,7 +96,9 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_SLC_FLUSH	0x904
 #define ARC_REG_SLC_INVALIDATE	0x905
 #define ARC_REG_SLC_RGN_START	0x914
+#define ARC_REG_SLC_RGN_START1	0x915
 #define ARC_REG_SLC_RGN_END	0x916
+#define ARC_REG_SLC_RGN_END1	0x917
 
 /* Bit val in SLC_CONTROL */
 #define SLC_CTRL_DIS		0x001

commit f734a31083324b8f4f24b2c5cba178c7459db309
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Tue May 2 16:23:57 2017 -0700

    ARCv2: mm: micro-optimize region flush generated code
    
    DC_CTRL.RGN_OP is 3 bits wide, however only 1 bit is used in current
    programming model (0: flush, 1: invalidate)
    
    The current code targetting 3 bits leads to additional 8 byte AND
    operation which can be elided given that only 1 bit is ever set by
    software and/or looked at by hardware
    
    before
    ------
    
    | 80b63324 <__dma_cache_wback_inv_l1>:
    | 80b63324:     clri    r3
    | 80b63328:     lr      r2,[dc_ctrl]
    | 80b6332c:     and     r2,r2,0xfffff1ff        <--- 8 bytes insn
    | 80b63334:     or      r2,r2,576
    | 80b63338:     sr      r2,[dc_ctrl]
    | ...
    | ...
    | 80b63360 <__dma_cache_inv_l1>:
    | 80b63360:     clri    r3
    | 80b63364:     lr      r2,[dc_ctrl]
    | 80b63368:     and     r2,r2,0xfffff1ff        <--- 8 bytes insn
    | 80b63370:     bset_s  r2,r2,0x9
    | 80b63372:     sr      r2,[dc_ctrl]
    | ...
    | ...
    | 80b6338c <__dma_cache_wback_l1>:
    | 80b6338c:     clri    r3
    | 80b63390:     lr      r2,[dc_ctrl]
    | 80b63394:     and     r2,r2,0xfffff1ff        <--- 8 bytes insn
    | 80b6339c:     sr      r2,[dc_ctrl]
    
    after (AND elided totally in 2 cases, replaced with 2 byte BCLR in 3rd)
    -----
    
    | 80b63324 <__dma_cache_wback_inv_l1>:
    | 80b63324:     clri    r3
    | 80b63328:     lr      r2,[dc_ctrl]
    | 80b6332c:     or      r2,r2,576
    | 80b63330:     sr      r2,[dc_ctrl]
    | ...
    | ...
    | 80b63358 <__dma_cache_inv_l1>:
    | 80b63358:     clri    r3
    | 80b6335c:     lr      r2,[dc_ctrl]
    | 80b63360:     bset_s  r2,r2,0x9
    | 80b63362:     sr      r2,[dc_ctrl]
    | ...
    | ...
    | 80b6337c <__dma_cache_wback_l1>:
    | 80b6337c:     clri    r3
    | 80b63380:     lr      r2,[dc_ctrl]
    | 80b63384:     bclr_s  r2,r2,0x9
    | 80b63386:     sr      r2,[dc_ctrl]
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 16e457706129..19ebddffb279 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -88,7 +88,7 @@ extern unsigned long perip_base, perip_end;
 #define DC_CTRL_INV_MODE_FLUSH	0x040
 #define DC_CTRL_FLUSH_STATUS	0x100
 #define DC_CTRL_RGN_OP_INV	0x200
-#define DC_CTRL_RGN_OP_MSK	0xE00
+#define DC_CTRL_RGN_OP_MSK	0x200
 
 /*System-level cache (L2 cache) related Auxiliary registers */
 #define ARC_REG_SLC_CFG		0x901

commit 0d77117fc5c0333d024a183d6790167bb90c3b62
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Aug 29 10:55:15 2014 +0530

    ARCv2: mm: Implement cache region flush operations
    
    These are more efficient than the per-line ops
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 5008021fba98..16e457706129 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -62,6 +62,8 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_IC_BCR		0x77	/* Build Config reg */
 #define ARC_REG_IC_IVIC		0x10
 #define ARC_REG_IC_CTRL		0x11
+#define ARC_REG_IC_IVIR		0x16
+#define ARC_REG_IC_ENDR		0x17
 #define ARC_REG_IC_IVIL		0x19
 #define ARC_REG_IC_PTAG		0x1E
 #define ARC_REG_IC_PTAG_HI	0x1F
@@ -76,6 +78,8 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_DC_IVDL		0x4A
 #define ARC_REG_DC_FLSH		0x4B
 #define ARC_REG_DC_FLDL		0x4C
+#define ARC_REG_DC_STARTR	0x4D
+#define ARC_REG_DC_ENDR		0x4E
 #define ARC_REG_DC_PTAG		0x5C
 #define ARC_REG_DC_PTAG_HI	0x5F
 
@@ -83,6 +87,8 @@ extern unsigned long perip_base, perip_end;
 #define DC_CTRL_DIS		0x001
 #define DC_CTRL_INV_MODE_FLUSH	0x040
 #define DC_CTRL_FLUSH_STATUS	0x100
+#define DC_CTRL_RGN_OP_INV	0x200
+#define DC_CTRL_RGN_OP_MSK	0xE00
 
 /*System-level cache (L2 cache) related Auxiliary registers */
 #define ARC_REG_SLC_CFG		0x901

commit 8c47f83ba45928ce9495fcf1b29e828c28e3c839
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Wed Jun 22 16:01:19 2016 +0530

    ARCv2: IOC: Adhere to progamming model guidelines to avoid DMA corruption
    
    On AXS103 release bitfiles, DMA data corruptions were seen because IOC
    setup was not following the recommended way in documentation.
    
    Flipping IOC on when caches are enabled or coherency transactions are in
    flight, might cause some of the memory operations to not observe
    coherency as expected.
    
    So strictly follow the programming model recommendations as documented
    in comment header above arc_ioc_setup()
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 5f924a1024fa..5008021fba98 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -67,7 +67,7 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_IC_PTAG_HI	0x1F
 
 /* Bit val in IC_CTRL */
-#define IC_CTRL_CACHE_DISABLE   0x1
+#define IC_CTRL_DIS		0x1
 
 /* Data cache related Auxiliary registers */
 #define ARC_REG_DC_BCR		0x72	/* Build Config reg */
@@ -80,8 +80,9 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_DC_PTAG_HI	0x5F
 
 /* Bit val in DC_CTRL */
-#define DC_CTRL_INV_MODE_FLUSH  0x40
-#define DC_CTRL_FLUSH_STATUS    0x100
+#define DC_CTRL_DIS		0x001
+#define DC_CTRL_INV_MODE_FLUSH	0x040
+#define DC_CTRL_FLUSH_STATUS	0x100
 
 /*System-level cache (L2 cache) related Auxiliary registers */
 #define ARC_REG_SLC_CFG		0x901

commit d4911cdd3270da45d3a1c55bf28e88a932bbba7b
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Wed Jun 22 15:43:22 2016 +0530

    ARCv2: IOC: refactor the IOC and SLC operations into own functions
    
     - Move IOC setup into arc_ioc_setup()
     - Move SLC disabling into arc_slc_disable()
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index b3410ff6a62d..5f924a1024fa 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -92,8 +92,8 @@ extern unsigned long perip_base, perip_end;
 #define ARC_REG_SLC_RGN_END	0x916
 
 /* Bit val in SLC_CONTROL */
+#define SLC_CTRL_DIS		0x001
 #define SLC_CTRL_IM		0x040
-#define SLC_CTRL_DISABLE	0x001
 #define SLC_CTRL_BUSY		0x100
 #define SLC_CTRL_RGN_OP_INV	0x200
 

commit cf986d470208fbdd68b6934a86ccd81c04408484
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu Oct 13 15:58:59 2016 -0700

    ARCv2: IOC: use @ioc_enable not @ioc_exist where intended
    
    if user disables IOC from debugger at startup (by clearing @ioc_enable),
    @ioc_exists is cleared too. This means boot prints don't capture the
    fact that IOC was present but disabled which could be misleading.
    
    So invert how we use @ioc_enable and @ioc_exists and make it more
    canonical. @ioc_exists represent whether hardware is present or not and
    stays same whether enabled or not. @ioc_enable is still user driven,
    but will be auto-disabled if IOC hardware is not present, i.e. if
    @ioc_exist=0. This is opposite to what we were doing before, but much
    clearer.
    
    This means @ioc_enable is now the "exported" toggle in rest of code such
    as dma mapping API.
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index fb781e34f322..b3410ff6a62d 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -53,7 +53,7 @@ extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
 extern void read_decode_cache_bcr(void);
 
-extern int ioc_exists;
+extern int ioc_enable;
 extern unsigned long perip_base, perip_end;
 
 #endif	/* !__ASSEMBLY__ */

commit 26c01c49d559268527d78f45a6818fae0c204a45
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Aug 26 15:41:29 2016 -0700

    ARCv2: Support dynamic peripheral address space in HS38 rel 3.0 cores
    
    HS release 3.0 provides for even more flexibility in specifying the
    volatile address space for mapping peripherals.
    
    With HS 2.1 @start was made flexible / programmable - with HS 3.0 even
    @end can be setup (vs. fixed to 0xFFFF_FFFF before).
    
    So add code to reflect that and while at it remove an unused struct
    defintion
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 23706c635c30..fb781e34f322 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -54,7 +54,7 @@ extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
 extern void read_decode_cache_bcr(void);
 
 extern int ioc_exists;
-extern unsigned long perip_base;
+extern unsigned long perip_base, perip_end;
 
 #endif	/* !__ASSEMBLY__ */
 

commit deaf7565eb618a80534844300aeacffa14125182
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Sat Oct 24 19:31:16 2015 +0530

    ARCv2: ioremap: Support dynamic peripheral address space
    
    The peripheral address space is architectural address window which is
    uncached and typically used to wire up peripherals.
    
    For ARC700 cores (ARCompact ISA based) this was fixed to 1GB region
    0xC000_0000 - 0xFFFF_FFFF.
    
    For ARCv2 based HS38 cores the start address is flexible and can be
    0xC, 0xD, 0xE, 0xF 000_000 by programming AUX_NON_VOLATILE_LIMIT reg
    (typically done in bootloader)
    
    Further in cas of PAE, the physical address can extend beyond 4GB so
    need to confine this check, otherwise all pages beyond 4GB will be
    treated as uncached
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 210ef3e72332..23706c635c30 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -54,6 +54,7 @@ extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
 extern void read_decode_cache_bcr(void);
 
 extern int ioc_exists;
+extern unsigned long perip_base;
 
 #endif	/* !__ASSEMBLY__ */
 

commit 4b32e89af7a054ae3f84f388cb622aeeb8beec9d
Author: Alexey Brodkin <Alexey.Brodkin@synopsys.com>
Date:   Fri Dec 18 22:28:51 2015 +0300

    ARC: mm: fix building for MMU v2
    
    ARC700 cores with MMU v2 don't have IC_PTAG AUX register and so we only
    define ARC_REG_IC_PTAG for MMU versions >= 3.
    
    But current implementation of cache_line_loop_vX() routines assumes
    availability of all of them (v2, v3 and v4) simultaneously.
    
    And given undefined ARC_REG_IC_PTAG if CONFIG_MMU_VER=2 we're seeing
    compilation problem:
    ---------------------------------->8-------------------------------
      CC      arch/arc/mm/cache.o
    arch/arc/mm/cache.c: In function '__cache_line_loop_v3':
    arch/arc/mm/cache.c:270:13: error: 'ARC_REG_IC_PTAG' undeclared (first use in this function)
       aux_tag = ARC_REG_IC_PTAG;
                 ^
    arch/arc/mm/cache.c:270:13: note: each undeclared identifier is reported only once for each function it appears in
    scripts/Makefile.build:258: recipe for target 'arch/arc/mm/cache.o' failed
    ---------------------------------->8-------------------------------
    
    The simples fix is to have ARC_REG_IC_PTAG defined regardless MMU
    version being used.
    
    We don't use it in cache_line_loop_v2() anyways so who cares.
    
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index abf06e81c929..210ef3e72332 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -62,9 +62,7 @@ extern int ioc_exists;
 #define ARC_REG_IC_IVIC		0x10
 #define ARC_REG_IC_CTRL		0x11
 #define ARC_REG_IC_IVIL		0x19
-#if defined(CONFIG_ARC_MMU_V3) || defined(CONFIG_ARC_MMU_V4)
 #define ARC_REG_IC_PTAG		0x1E
-#endif
 #define ARC_REG_IC_PTAG_HI	0x1F
 
 /* Bit val in IC_CTRL */

commit 5a364c2a1762e8a78721fafc93144509c0b6cb84
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Feb 6 18:44:57 2015 +0300

    ARC: mm: PAE40 support
    
    This is the first working implementation of 40-bit physical address
    extension on ARCv2.
    
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index e23ea6e7633a..abf06e81c929 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -65,6 +65,7 @@ extern int ioc_exists;
 #if defined(CONFIG_ARC_MMU_V3) || defined(CONFIG_ARC_MMU_V4)
 #define ARC_REG_IC_PTAG		0x1E
 #endif
+#define ARC_REG_IC_PTAG_HI	0x1F
 
 /* Bit val in IC_CTRL */
 #define IC_CTRL_CACHE_DISABLE   0x1
@@ -77,6 +78,7 @@ extern int ioc_exists;
 #define ARC_REG_DC_FLSH		0x4B
 #define ARC_REG_DC_FLDL		0x4C
 #define ARC_REG_DC_PTAG		0x5C
+#define ARC_REG_DC_PTAG_HI	0x5F
 
 /* Bit val in DC_CTRL */
 #define DC_CTRL_INV_MODE_FLUSH  0x40

commit f2b0b25a37a6db12580dcdfdf00f020e5e0e3a43
Author: Alexey Brodkin <abrodkin@synopsys.com>
Date:   Mon May 25 19:54:28 2015 +0300

    ARCv2: Support IO Coherency and permutations involving L1 and L2 caches
    
    In case of ARCv2 CPU there're could be following configurations
    that affect cache handling for data exchanged with peripherals
    via DMA:
     [1] Only L1 cache exists
     [2] Both L1 and L2 exist, but no IO coherency unit
     [3] L1, L2 caches and IO coherency unit exist
    
    Current implementation takes care of [1] and [2].
    Moreover support of [2] is implemented with run-time check
    for SLC existence which is not super optimal.
    
    This patch introduces support of [3] and rework of DMA ops
    usage. Instead of doing run-time check every time a particular
    DMA op is executed we'll have 3 different implementations of
    DMA ops and select appropriate one during init.
    
    As for IOC support for it we need:
     [a] Implement empty DMA ops because IOC takes care of cache
         coherency with DMAed data
     [b] Route dma_alloc_coherent() via dma_alloc_noncoherent()
         This is required to make IOC work in first place and also
         serves as optimization as LD/ST to coherent buffers can be
         srviced from caches w/o going all the way to memory
    
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    [vgupta:
      -Added some comments about IOC gains
      -Marked dma ops as static,
      -Massaged changelog a bit]
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index d67345d3e2d4..e23ea6e7633a 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -53,6 +53,8 @@ extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
 extern void read_decode_cache_bcr(void);
 
+extern int ioc_exists;
+
 #endif	/* !__ASSEMBLY__ */
 
 /* Instruction cache related Auxiliary registers */
@@ -94,4 +96,10 @@ extern void read_decode_cache_bcr(void);
 #define SLC_CTRL_BUSY		0x100
 #define SLC_CTRL_RGN_OP_INV	0x200
 
+/* IO coherency related Auxiliary registers */
+#define ARC_REG_IO_COH_ENABLE	0x500
+#define ARC_REG_IO_COH_PARTIAL	0x501
+#define ARC_REG_IO_COH_AP0_BASE	0x508
+#define ARC_REG_IO_COH_AP0_SIZE	0x509
+
 #endif /* _ASM_CACHE_H */

commit 795f4558562fd5318260d5d8144a2f8612aeda7b
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Apr 3 12:37:07 2015 +0300

    ARCv2: SLC: Handle explcit flush for DMA ops (w/o IO-coherency)
    
    L2 cache on ARCHS processors is called SLC (System Level Cache)
    For working DMA (in absence of hardware assisted IO Coherency) we need
    to manage SLC explicitly when buffers transition between cpu and
    controllers.
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index d21c76d6b054..d67345d3e2d4 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -82,5 +82,16 @@ extern void read_decode_cache_bcr(void);
 
 /*System-level cache (L2 cache) related Auxiliary registers */
 #define ARC_REG_SLC_CFG		0x901
+#define ARC_REG_SLC_CTRL	0x903
+#define ARC_REG_SLC_FLUSH	0x904
+#define ARC_REG_SLC_INVALIDATE	0x905
+#define ARC_REG_SLC_RGN_START	0x914
+#define ARC_REG_SLC_RGN_END	0x916
+
+/* Bit val in SLC_CONTROL */
+#define SLC_CTRL_IM		0x040
+#define SLC_CTRL_DISABLE	0x001
+#define SLC_CTRL_BUSY		0x100
+#define SLC_CTRL_RGN_OP_INV	0x200
 
 #endif /* _ASM_CACHE_H */

commit bcc4d65abec2adb74157b34519e80331eb4427eb
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu Jun 4 14:39:15 2015 +0530

    ARCv2: MMUv4: support aliasing icache config
    
    This is also default for AXS103 release
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index e54977a7d006..d21c76d6b054 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -60,7 +60,7 @@ extern void read_decode_cache_bcr(void);
 #define ARC_REG_IC_IVIC		0x10
 #define ARC_REG_IC_CTRL		0x11
 #define ARC_REG_IC_IVIL		0x19
-#if defined(CONFIG_ARC_MMU_V3)
+#if defined(CONFIG_ARC_MMU_V3) || defined(CONFIG_ARC_MMU_V4)
 #define ARC_REG_IC_PTAG		0x1E
 #endif
 
@@ -74,9 +74,7 @@ extern void read_decode_cache_bcr(void);
 #define ARC_REG_DC_IVDL		0x4A
 #define ARC_REG_DC_FLSH		0x4B
 #define ARC_REG_DC_FLDL		0x4C
-#if defined(CONFIG_ARC_MMU_V3)
 #define ARC_REG_DC_PTAG		0x5C
-#endif
 
 /* Bit val in DC_CTRL */
 #define DC_CTRL_INV_MODE_FLUSH  0x40

commit d1f317d8254413447bcd6b6adbde24a985d256c2
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Mon Apr 6 17:23:57 2015 +0530

    ARCv2: MMUv4: cache programming model changes
    
    Caveats about cache flush on ARCv2 based cores
    
    - dcache is PIPT so paddr is sufficient for cache maintenance ops (no
      need to setup PTAG reg
    
    - icache is still VIPT but only aliasing configs need PTAG setup
    
    So basically this is departure from MMU-v3 which always need vaddr in
    line ops registers (DC_IVDL, DC_FLDL, IC_IVIL) but paddr in DC_PTAG,
    IC_PTAG respectively.
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 7861255da32d..e54977a7d006 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -82,4 +82,7 @@ extern void read_decode_cache_bcr(void);
 #define DC_CTRL_INV_MODE_FLUSH  0x40
 #define DC_CTRL_FLUSH_STATUS    0x100
 
+/*System-level cache (L2 cache) related Auxiliary registers */
+#define ARC_REG_SLC_CFG		0x901
+
 #endif /* _ASM_CACHE_H */

commit c4aa49df4dca6d41d3a7488cf582a0ab778ad06d
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Sep 19 01:28:24 2014 +0530

    ARC: Update comments about uncached address space
    
    Suggested-by: Noam Camus <noamc@ezchip.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index b3c750979aa1..7861255da32d 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -20,7 +20,7 @@
 #define CACHE_LINE_MASK		(~(L1_CACHE_BYTES - 1))
 
 /*
- * ARC700 doesn't cache any access in top 256M.
+ * ARC700 doesn't cache any access in top 1G (0xc000_0000 to 0xFFFF_FFFF)
  * Ideal for wiring memory mapped peripherals as we don't need to do
  * explicit uncached accesses (LD.di/ST.di) hence more portable drivers
  */

commit 230a15f171ba24226486664fc44542b175107ab7
Author: Paul Bolle <pebolle@tiscali.nl>
Date:   Wed Jun 11 10:57:50 2014 +0200

    ARC: remove checks for CONFIG_ARC_MMU_V4
    
    There's no Kconfig symbol ARC_MMU_V4 so the checks for CONFIG_ARC_MMU_V4
    will always evaluate to false. Remove them.
    
    Signed-off-by: Paul Bolle <pebolle@tiscali.nl>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index c1d3d2da1191..b3c750979aa1 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -60,7 +60,7 @@ extern void read_decode_cache_bcr(void);
 #define ARC_REG_IC_IVIC		0x10
 #define ARC_REG_IC_CTRL		0x11
 #define ARC_REG_IC_IVIL		0x19
-#if defined(CONFIG_ARC_MMU_V3) || defined (CONFIG_ARC_MMU_V4)
+#if defined(CONFIG_ARC_MMU_V3)
 #define ARC_REG_IC_PTAG		0x1E
 #endif
 
@@ -74,7 +74,7 @@ extern void read_decode_cache_bcr(void);
 #define ARC_REG_DC_IVDL		0x4A
 #define ARC_REG_DC_FLSH		0x4B
 #define ARC_REG_DC_FLDL		0x4C
-#if defined(CONFIG_ARC_MMU_V3) || defined (CONFIG_ARC_MMU_V4)
+#if defined(CONFIG_ARC_MMU_V3)
 #define ARC_REG_DC_PTAG		0x5C
 #endif
 

commit ef680cdc24376f394841a3f19b3a7ef6d57a009d
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Mar 7 18:08:11 2014 +0530

    ARC: Disable caches in early boot if so configured
    
    Requested-by: Noam Camus <noamc@ezchip.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 2fd3162ec4df..c1d3d2da1191 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -55,4 +55,31 @@ extern void read_decode_cache_bcr(void);
 
 #endif	/* !__ASSEMBLY__ */
 
+/* Instruction cache related Auxiliary registers */
+#define ARC_REG_IC_BCR		0x77	/* Build Config reg */
+#define ARC_REG_IC_IVIC		0x10
+#define ARC_REG_IC_CTRL		0x11
+#define ARC_REG_IC_IVIL		0x19
+#if defined(CONFIG_ARC_MMU_V3) || defined (CONFIG_ARC_MMU_V4)
+#define ARC_REG_IC_PTAG		0x1E
+#endif
+
+/* Bit val in IC_CTRL */
+#define IC_CTRL_CACHE_DISABLE   0x1
+
+/* Data cache related Auxiliary registers */
+#define ARC_REG_DC_BCR		0x72	/* Build Config reg */
+#define ARC_REG_DC_IVDC		0x47
+#define ARC_REG_DC_CTRL		0x48
+#define ARC_REG_DC_IVDL		0x4A
+#define ARC_REG_DC_FLSH		0x4B
+#define ARC_REG_DC_FLDL		0x4C
+#if defined(CONFIG_ARC_MMU_V3) || defined (CONFIG_ARC_MMU_V4)
+#define ARC_REG_DC_PTAG		0x5C
+#endif
+
+/* Bit val in DC_CTRL */
+#define DC_CTRL_INV_MODE_FLUSH  0x40
+#define DC_CTRL_FLUSH_STATUS    0x100
+
 #endif /* _ASM_CACHE_H */

commit 63d2dfdbf4b12a6993adf5005fd308d611d453d6
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu Sep 5 13:17:49 2013 +0530

    ARC: cacheflush refactor #2: I and D caches lines to have same size
    
    Having them be different seems an obscure configuration.
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index e4abdaac6f9f..2fd3162ec4df 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -17,13 +17,7 @@
 #endif
 
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
-
-/* For a rare case where customers have differently config I/D */
-#define ARC_ICACHE_LINE_LEN	L1_CACHE_BYTES
-#define ARC_DCACHE_LINE_LEN	L1_CACHE_BYTES
-
-#define ICACHE_LINE_MASK	(~(ARC_ICACHE_LINE_LEN - 1))
-#define DCACHE_LINE_MASK	(~(ARC_DCACHE_LINE_LEN - 1))
+#define CACHE_LINE_MASK		(~(L1_CACHE_BYTES - 1))
 
 /*
  * ARC700 doesn't cache any access in top 256M.

commit 07b9b65147d1d7cc03b9ff1e1f3b1c163ba4d067
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu Sep 5 19:19:06 2013 +0530

    ARC: fix new Section mismatches in build (post __cpuinit cleanup)
    
    --------------->8--------------------
    WARNING: vmlinux.o(.text+0x708): Section mismatch in reference from the
    function read_arc_build_cfg_regs() to the function
    .init.text:read_decode_cache_bcr()
    
    WARNING: vmlinux.o(.text+0x702): Section mismatch in reference from the
    function read_arc_build_cfg_regs() to the function
    .init.text:read_decode_mmu_bcr()
    --------------->8--------------------
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 5802849a6cae..e4abdaac6f9f 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -57,7 +57,7 @@
 
 extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
-extern void __init read_decode_cache_bcr(void);
+extern void read_decode_cache_bcr(void);
 
 #endif	/* !__ASSEMBLY__ */
 

commit 30499186602afa1d62c2e5d354d02214a0ee00b7
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Sat Jun 15 10:21:51 2013 +0530

    ARC: cache detection code bitrot
    
    * Number of (i|d)cache ways can be retrieved from BCRs and hence no need
      to cross check with with built-in constants
    * Use of IS_ENABLED() to check for a Kconfig option
    * is_not_cache_aligned() not used anymore
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 44eb07eb92e5..5802849a6cae 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -18,22 +18,13 @@
 
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
-#define ARC_ICACHE_WAYS	2
-#define ARC_DCACHE_WAYS	4
-
-/* Helpers */
+/* For a rare case where customers have differently config I/D */
 #define ARC_ICACHE_LINE_LEN	L1_CACHE_BYTES
 #define ARC_DCACHE_LINE_LEN	L1_CACHE_BYTES
 
 #define ICACHE_LINE_MASK	(~(ARC_ICACHE_LINE_LEN - 1))
 #define DCACHE_LINE_MASK	(~(ARC_DCACHE_LINE_LEN - 1))
 
-#if ARC_ICACHE_LINE_LEN != ARC_DCACHE_LINE_LEN
-#error "Need to fix some code as I/D cache lines not same"
-#else
-#define is_not_cache_aligned(p)	((unsigned long)p & (~DCACHE_LINE_MASK))
-#endif
-
 /*
  * ARC700 doesn't cache any access in top 256M.
  * Ideal for wiring memory mapped peripherals as we don't need to do

commit da1677b02d3ef674dfd8a4ba1ed32153dc717fa2
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Tue May 14 13:28:17 2013 +0530

    ARC: Disintegrate arcregs.h
    
    * Move the various sub-system defines/types into relevant files/functions
      (reduces compilation time)
    
    * move CPU specific stuff out of asm/tlb.h into asm/mmu.h
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 2fe8e41a551c..44eb07eb92e5 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -9,8 +9,6 @@
 #ifndef __ARC_ASM_CACHE_H
 #define __ARC_ASM_CACHE_H
 
-#include <asm/mmu.h>	/* some of cache registers depend on MMU ver */
-
 /* In case $$ not config, setup a dummy number for rest of kernel */
 #ifndef CONFIG_ARC_CACHE_LINE_SHIFT
 #define L1_CACHE_SHIFT		6
@@ -36,6 +34,13 @@
 #define is_not_cache_aligned(p)	((unsigned long)p & (~DCACHE_LINE_MASK))
 #endif
 
+/*
+ * ARC700 doesn't cache any access in top 256M.
+ * Ideal for wiring memory mapped peripherals as we don't need to do
+ * explicit uncached accesses (LD.di/ST.di) hence more portable drivers
+ */
+#define ARC_UNCACHED_ADDR_SPACE	0xc0000000
+
 #ifndef __ASSEMBLY__
 
 /* Uncached access macros */
@@ -59,16 +64,10 @@
 
 #define ARCH_DMA_MINALIGN      L1_CACHE_BYTES
 
-/*
- * ARC700 doesn't cache any access in top 256M.
- * Ideal for wiring memory mapped peripherals as we don't need to do
- * explicit uncached accesses (LD.di/ST.di) hence more portable drivers
- */
-#define ARC_UNCACHED_ADDR_SPACE	0xc0000000
-
 extern void arc_cache_init(void);
 extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
 extern void __init read_decode_cache_bcr(void);
-#endif
+
+#endif	/* !__ASSEMBLY__ */
 
 #endif /* _ASM_CACHE_H */

commit 8235703e103579bdcedadcaf63bc1896f82b191b
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Sat Jun 1 12:55:42 2013 +0530

    ARC: Use kconfig helper IS_ENABLED() to get rid of defines.h
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index d5555fe4742a..2fe8e41a551c 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -9,6 +9,8 @@
 #ifndef __ARC_ASM_CACHE_H
 #define __ARC_ASM_CACHE_H
 
+#include <asm/mmu.h>	/* some of cache registers depend on MMU ver */
+
 /* In case $$ not config, setup a dummy number for rest of kernel */
 #ifndef CONFIG_ARC_CACHE_LINE_SHIFT
 #define L1_CACHE_SHIFT		6

commit 5bba49f5397c012d873c73860ad7b50c526e613b
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu May 9 19:20:43 2013 +0530

    ARC: [mm] Aliasing VIPT dcache support 4/4
    
    Enforce congruency of userspace shared mappings
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 6632273861fd..d5555fe4742a 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -55,9 +55,6 @@
 	: "r"(data), "r"(ptr));		\
 })
 
-/* used to give SHMLBA a value to avoid Cache Aliasing */
-extern unsigned int ARC_shmlba;
-
 #define ARCH_DMA_MINALIGN      L1_CACHE_BYTES
 
 /*

commit 95d6976d20a25fa1684f849f26cd3387b5ba7150
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Jan 18 15:12:19 2013 +0530

    ARC: Cache Flush Management
    
    * ARC700 has VIPT L1 Caches
    * Caches don't snoop and are not coherent
    * Given the PAGE_SIZE and Cache associativity, we don't support aliasing
      D$ configurations (yet), but do allow aliasing I$ configs
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
index 30c72a4d2d9f..6632273861fd 100644
--- a/arch/arc/include/asm/cache.h
+++ b/arch/arc/include/asm/cache.h
@@ -18,4 +18,58 @@
 
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
+#define ARC_ICACHE_WAYS	2
+#define ARC_DCACHE_WAYS	4
+
+/* Helpers */
+#define ARC_ICACHE_LINE_LEN	L1_CACHE_BYTES
+#define ARC_DCACHE_LINE_LEN	L1_CACHE_BYTES
+
+#define ICACHE_LINE_MASK	(~(ARC_ICACHE_LINE_LEN - 1))
+#define DCACHE_LINE_MASK	(~(ARC_DCACHE_LINE_LEN - 1))
+
+#if ARC_ICACHE_LINE_LEN != ARC_DCACHE_LINE_LEN
+#error "Need to fix some code as I/D cache lines not same"
+#else
+#define is_not_cache_aligned(p)	((unsigned long)p & (~DCACHE_LINE_MASK))
+#endif
+
+#ifndef __ASSEMBLY__
+
+/* Uncached access macros */
+#define arc_read_uncached_32(ptr)	\
+({					\
+	unsigned int __ret;		\
+	__asm__ __volatile__(		\
+	"	ld.di %0, [%1]	\n"	\
+	: "=r"(__ret)			\
+	: "r"(ptr));			\
+	__ret;				\
+})
+
+#define arc_write_uncached_32(ptr, data)\
+({					\
+	__asm__ __volatile__(		\
+	"	st.di %0, [%1]	\n"	\
+	:				\
+	: "r"(data), "r"(ptr));		\
+})
+
+/* used to give SHMLBA a value to avoid Cache Aliasing */
+extern unsigned int ARC_shmlba;
+
+#define ARCH_DMA_MINALIGN      L1_CACHE_BYTES
+
+/*
+ * ARC700 doesn't cache any access in top 256M.
+ * Ideal for wiring memory mapped peripherals as we don't need to do
+ * explicit uncached accesses (LD.di/ST.di) hence more portable drivers
+ */
+#define ARC_UNCACHED_ADDR_SPACE	0xc0000000
+
+extern void arc_cache_init(void);
+extern char *arc_cache_mumbojumbo(int cpu_id, char *buf, int len);
+extern void __init read_decode_cache_bcr(void);
+#endif
+
 #endif /* _ASM_CACHE_H */

commit 3be80aaef861a60b85a9323462ebb5f623774f7a
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Jan 18 15:12:17 2013 +0530

    ARC: Fundamental ARCH data-types/defines
    
    * L1_CACHE_SHIFT
    * PAGE_SIZE, PAGE_OFFSET
    * struct pt_regs, struct user_regs_struct
    * struct thread_struct, cpu_relax(), task_pt_regs(), start_thread(), ...
    * struct thread_info, THREAD_SIZE, INIT_THREAD_INFO(), TIF_*, ...
    * BUG()
    * ELF_*
    * Elf_*
    
    To disallow user-space visibility into some of the core kernel data-types
    such as struct pt_regs, #ifdef __KERNEL__ which also makes the UAPI header
    spit (further patch in the series) to NOT export it to asm/uapi/ptrace.h
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
    Cc: Jonas Bonn <jonas.bonn@gmail.com>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arc/include/asm/cache.h b/arch/arc/include/asm/cache.h
new file mode 100644
index 000000000000..30c72a4d2d9f
--- /dev/null
+++ b/arch/arc/include/asm/cache.h
@@ -0,0 +1,21 @@
+/*
+ * Copyright (C) 2004, 2007-2010, 2011-2012 Synopsys, Inc. (www.synopsys.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __ARC_ASM_CACHE_H
+#define __ARC_ASM_CACHE_H
+
+/* In case $$ not config, setup a dummy number for rest of kernel */
+#ifndef CONFIG_ARC_CACHE_LINE_SHIFT
+#define L1_CACHE_SHIFT		6
+#else
+#define L1_CACHE_SHIFT		CONFIG_ARC_CACHE_LINE_SHIFT
+#endif
+
+#define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
+
+#endif /* _ASM_CACHE_H */
