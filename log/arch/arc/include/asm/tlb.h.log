commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index 90cac97643a4..975b35d3738d 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -1,9 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2004, 2007-2010, 2011-2012 Synopsys, Inc. (www.synopsys.com)
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #ifndef _ASM_ARC_TLB_H

commit 6137fed0823247e32306bde2b48cac627c24f894
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 4 17:04:07 2018 +0200

    arch/tlb: Clean up simple architectures
    
    For the architectures that do not implement their own tlb_flush() but
    do already use the generic mmu_gather, there are two options:
    
     1) the platform has an efficient flush_tlb_range() and
        asm-generic/tlb.h doesn't need any overrides at all.
    
     2) the platform lacks an efficient flush_tlb_range() and
        we select MMU_GATHER_NO_RANGE to minimize full invalidates.
    
    Convert all 'simple' architectures to one of these two forms.
    
    alpha:      has no range invalidate -> 2
    arc:        already used flush_tlb_range() -> 1
    c6x:        has no range invalidate -> 2
    hexagon:    has an efficient flush_tlb_range() -> 1
                (flush_tlb_mm() is in fact a full range invalidate,
                 so no need to shoot down everything)
    m68k:       has inefficient flush_tlb_range() -> 2
    microblaze: has no flush_tlb_range() -> 2
    mips:       has efficient flush_tlb_range() -> 1
                (even though it currently seems to use flush_tlb_mm())
    nds32:      already uses flush_tlb_range() -> 1
    nios2:      has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    openrisc:   has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    parisc:     already uses flush_tlb_range() -> 1
    sparc32:    already uses flush_tlb_range() -> 1
    unicore32:  has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    xtensa:     has efficient flush_tlb_range() -> 1
    
    Note this also fixes a bug in the existing code for a number
    platforms. Those platforms that did:
    
      tlb_end_vma() -> if (!full_mm) flush_tlb_*()
      tlb_flush -> if (full_mm) flush_tlb_mm()
    
    missed the case of shift_arg_pages(), which doesn't have @fullmm set,
    nor calls into tlb_*vma(), but still frees page-tables and thus needs
    an invalidate. The new code handles this by detecting a non-empty
    range, and either issuing the matching range invalidate or a full
    invalidate, depending on the capabilities.
    
    No change in behavior intended.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Piggin <npiggin@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index 7af2b373ebe7..90cac97643a4 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -9,29 +9,6 @@
 #ifndef _ASM_ARC_TLB_H
 #define _ASM_ARC_TLB_H
 
-#define tlb_flush(tlb)				\
-do {						\
-	if (tlb->fullmm)			\
-		flush_tlb_mm((tlb)->mm);	\
-} while (0)
-
-/*
- * This pair is called at time of munmap/exit to flush cache and TLB entries
- * for mappings being torn down.
- * 1) cache-flush part -implemented via tlb_start_vma( ) for VIPT aliasing D$
- * 2) tlb-flush part - implemted via tlb_end_vma( ) flushes the TLB range
- *
- * Note, read http://lkml.org/lkml/2004/1/15/6
- */
-
-#define tlb_end_vma(tlb, vma)						\
-do {									\
-	if (!tlb->fullmm)						\
-		flush_tlb_range(vma, vma->vm_start, vma->vm_end);	\
-} while (0)
-
-#define __tlb_remove_tlb_entry(tlb, ptep, address)
-
 #include <linux/pagemap.h>
 #include <asm-generic/tlb.h>
 

commit e7fd28a706bfaf9cd65dccf18140187f7ad04839
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 27 13:00:17 2018 +0200

    asm-generic/tlb, arch: Provide generic VIPT cache flush
    
    The one obvious thing SH and ARM want is a sensible default for
    tlb_start_vma(). (also: https://lkml.org/lkml/2004/1/15/6 )
    
    Avoid all VIPT architectures providing their own tlb_start_vma()
    implementation and rely on architectures to provide a no-op
    flush_cache_range() when it is not relevant.
    
    This patch makes tlb_start_vma() default to flush_cache_range(), which
    should be right and sufficient. The only exceptions that I found where
    (oddly):
    
      - m68k-mmu
      - sparc64
      - unicore
    
    Those architectures appear to have flush_cache_range(), but their
    current tlb_start_vma() does not call it.
    
    No change in behavior intended.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nick Piggin <npiggin@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index a9db5f62aaf3..7af2b373ebe7 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -23,15 +23,6 @@ do {						\
  *
  * Note, read http://lkml.org/lkml/2004/1/15/6
  */
-#ifndef CONFIG_ARC_CACHE_VIPT_ALIASING
-#define tlb_start_vma(tlb, vma)
-#else
-#define tlb_start_vma(tlb, vma)						\
-do {									\
-	if (!tlb->fullmm)						\
-		flush_cache_range(vma, vma->vm_start, vma->vm_end);	\
-} while(0)
-#endif
 
 #define tlb_end_vma(tlb, vma)						\
 do {									\

commit da1677b02d3ef674dfd8a4ba1ed32153dc717fa2
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Tue May 14 13:28:17 2013 +0530

    ARC: Disintegrate arcregs.h
    
    * Move the various sub-system defines/types into relevant files/functions
      (reduces compilation time)
    
    * move CPU specific stuff out of asm/tlb.h into asm/mmu.h
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index cb0c708ca665..a9db5f62aaf3 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -9,18 +9,6 @@
 #ifndef _ASM_ARC_TLB_H
 #define _ASM_ARC_TLB_H
 
-#ifdef __KERNEL__
-
-#include <asm/pgtable.h>
-
-/* Masks for actual TLB "PD"s */
-#define PTE_BITS_IN_PD0	(_PAGE_GLOBAL | _PAGE_PRESENT)
-#define PTE_BITS_IN_PD1	(PAGE_MASK | _PAGE_CACHEABLE | \
-			 _PAGE_U_EXECUTE | _PAGE_U_WRITE | _PAGE_U_READ | \
-			 _PAGE_K_EXECUTE | _PAGE_K_WRITE | _PAGE_K_READ)
-
-#ifndef __ASSEMBLY__
-
 #define tlb_flush(tlb)				\
 do {						\
 	if (tlb->fullmm)			\
@@ -56,18 +44,4 @@ do {									\
 #include <linux/pagemap.h>
 #include <asm-generic/tlb.h>
 
-#ifdef CONFIG_ARC_DBG_TLB_PARANOIA
-void tlb_paranoid_check(unsigned int pid_sw, unsigned long address);
-#else
-#define tlb_paranoid_check(a, b)
-#endif
-
-void arc_mmu_init(void);
-extern char *arc_mmu_mumbojumbo(int cpu_id, char *buf, int len);
-void __init read_decode_mmu_bcr(void);
-
-#endif	/* __ASSEMBLY__ */
-
-#endif	/* __KERNEL__ */
-
 #endif /* _ASM_ARC_TLB_H */

commit a950549c675f2c8c504469dec7d780da8a6433dc
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Tue May 21 15:25:11 2013 +0530

    ARC: copy_(to|from)_user() to honor usermode-access permissions
    
    This manifested as grep failing psuedo-randomly:
    
    -------------->8---------------------
    [ARCLinux]$ ip address show lo | grep inet
    [ARCLinux]$ ip address show lo | grep inet
    [ARCLinux]$ ip address show lo | grep inet
    [ARCLinux]$
    [ARCLinux]$ ip address show lo | grep inet
        inet 127.0.0.1/8 scope host lo
    -------------->8---------------------
    
    ARC700 MMU provides fully orthogonal permission bits per page:
    Ur, Uw, Ux, Kr, Kw, Kx
    
    The user mode page permission templates used to have all Kernel mode
    access bits enabled.
    This caused a tricky race condition observed with uClibc buffered file
    read and UNIX pipes.
    
    1. Read access to an anon mapped page in libc .bss: write-protected
       zero_page mapped: TLB Entry installed with Ur + K[rwx]
    
    2. grep calls libc:getc() -> buffered read layer calls read(2) with the
       internal read buffer in same .bss page.
       The read() call is on STDIN which has been redirected to a pipe.
       read(2) => sys_read() => pipe_read() => copy_to_user()
    
    3. Since page has Kernel-write permission (despite being user-mode
       write-protected), copy_to_user() suceeds w/o taking a MMU TLB-Miss
       Exception (page-fault for ARC). core-MM is unaware that kernel
       erroneously wrote to the reserved read-only zero-page (BUG #1)
    
    4. Control returns to userspace which now does a write to same .bss page
       Since Linux MM is not aware that page has been modified by kernel, it
       simply reassigns a new writable zero-init page to mapping, loosing the
       prior write by kernel - effectively zero'ing out the libc read buffer
       under the hood - hence grep doesn't see right data (BUG #2)
    
    The fix is to make all kernel-mode access permissions mirror the
    user-mode ones. Note that the kernel still has full access to pages,
    when accessed directly (w/o MMU) - this fix ensures that kernel-mode
    access in copy_to_from() path uses the same faulting access model as for
    pure user accesses to keep MM fully aware of page state.
    
    The issue is peudo-random because it only shows up if the TLB entry
    installed in #1 is present at the time of #3. If it is evicted out, due
    to TLB pressure or some-such, then copy_to_user() does take a TLB Miss
    Exception, with a routine write-to-anon COW processing installing a
    fresh page for kernel writes and also usable as it is in userspace.
    
    Further the issue was dormant for so long as it depends on where the
    libc internal read buffer (in .bss) is mapped at runtime.
    If it happens to reside in file-backed data mapping of libc (in the
    page-aligned slack space trailing the file backed data), loader zero
    padding the slack space, does the early cow page replacement, setting
    things up at the very beginning itself.
    
    With gcc 4.8 based builds, the libc buffer got pushed out to a real
    anon mapping which triggers the issue.
    
    Reported-by: Anton Kolesov <akolesov@synopsys.com>
    Cc: <stable@vger.kernel.org> # 3.9
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index 85b6df839bd7..cb0c708ca665 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -16,7 +16,7 @@
 /* Masks for actual TLB "PD"s */
 #define PTE_BITS_IN_PD0	(_PAGE_GLOBAL | _PAGE_PRESENT)
 #define PTE_BITS_IN_PD1	(PAGE_MASK | _PAGE_CACHEABLE | \
-			 _PAGE_EXECUTE | _PAGE_WRITE | _PAGE_READ | \
+			 _PAGE_U_EXECUTE | _PAGE_U_WRITE | _PAGE_U_READ | \
 			 _PAGE_K_EXECUTE | _PAGE_K_WRITE | _PAGE_K_READ)
 
 #ifndef __ASSEMBLY__

commit 4102b53392d6397d80b6e09b516517efacf7ea77
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Thu May 9 21:54:51 2013 +0530

    ARC: [mm] Aliasing VIPT dcache support 2/4
    
    This is the meat of the series which prevents any dcache alias creation
    by always keeping the U and K mapping of a page congruent.
    If a mapping already exists, and other tries to access the page, prev
    one is flushed to physical page (wback+inv)
    
    Essentially flush_dcache_page()/copy_user_highpage() create K-mapping
    of a page, but try to defer flushing, unless U-mapping exist.
    When page is actually mapped to userspace, update_mmu_cache() flushes
    the K-mapping (in certain cases this can be optimised out)
    
    Additonally flush_cache_mm(), flush_cache_range(), flush_cache_page()
    handle the puring of stale userspace mappings on exit/munmap...
    
    flush_anon_page() handles the existing U-mapping for anon page before
    kernel reads it via the GUP path.
    
    Note that while not complete, this is enough to boot a simple
    dynamically linked Busybox based rootfs
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index fe91719866a5..85b6df839bd7 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -30,13 +30,20 @@ do {						\
 /*
  * This pair is called at time of munmap/exit to flush cache and TLB entries
  * for mappings being torn down.
- * 1) cache-flush part -implemented via tlb_start_vma( ) can be NOP (for now)
- *    as we don't support aliasing configs in our VIPT D$.
+ * 1) cache-flush part -implemented via tlb_start_vma( ) for VIPT aliasing D$
  * 2) tlb-flush part - implemted via tlb_end_vma( ) flushes the TLB range
  *
  * Note, read http://lkml.org/lkml/2004/1/15/6
  */
+#ifndef CONFIG_ARC_CACHE_VIPT_ALIASING
 #define tlb_start_vma(tlb, vma)
+#else
+#define tlb_start_vma(tlb, vma)						\
+do {									\
+	if (!tlb->fullmm)						\
+		flush_cache_range(vma, vma->vm_start, vma->vm_end);	\
+} while(0)
+#endif
 
 #define tlb_end_vma(tlb, vma)						\
 do {									\

commit 8d56bec2f2945b7e500b413d1bdc24e7dca12877
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Apr 5 18:38:31 2013 +0530

    ARC: [mm] optimize needless full mm TLB flush on munmap
    
    munmap ends up calling tlb_flush() which for ARC was flushing the entire
    TLB unconditionally (by moving the MMU to a new ASID)
    
    do_munmap
      unmap_region
        unmap_vmas
          unmap_single_vma
             unmap_page_range
                tlb_start_vma
                zap_pud_range
                tlb_end_vma()
      tlb_finish_mmu
        tlb_flush()  ---> unconditional flush_tlb_mm()
    
    So even a single page munmap, a frequent operation when uClibc dynamic
    linker (ldso) is loading the dependent shared libraries, would move the
    the ASID multiple times - needlessly invalidating the pre-faulted TLB
    entries (and increasing the rate of ASID wraparound + full TLB flush).
    
    This is now optimised to only be called if tlb->full_mm (which means
    for exit/execve) cases only. And for those cases, flush_tlb_mm() is
    already optimised to be a no-op for mm->mm_users == 0.
    
    So essentially there are no mmore full mm flushes - except for fork which
    anyhow needs it for properly COW'ing parent address space.
    
    munmap now needs to do TLB range flush, which is implemented with
    tlb_end_vma()
    
    Results
    -------
    1. ASID now consistenly moves by 4 during a simple ls (as opposed to 5 or
       7 before).
    
    2. LMBench microbenchmark also shows improvements
    
    Basic system parameters
    ------------------------------------------------------------------------------
    Host                 OS Description              Mhz  tlb  cache  mem scal
                                                         pages line   par load
                                                               bytes
    --------- ------------- ----------------------- ---- ----- ----- ------ ----
    3.9-rc5-0 Linux 3.9.0-r 3.9-rc5-0404-gcc-4.4-ba   80     8    64 1.1000 1
    3.9-rc5-0 Linux 3.9.0-r 3.9-rc5-0405-avoid-full   80     8    64 1.1200 1
    
    Processor, Processes - times in microseconds - smaller is better
    ------------------------------------------------------------------------------
    Host                 OS  Mhz null null      open slct sig  sig  fork exec sh
                                 call  I/O stat clos TCP  inst hndl proc proc proc
    --------- ------------- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----
    3.9-rc5-0 Linux 3.9.0-r   80 4.81 8.69 68.6 118. 239. 8.53 31.6 4839 13.K 34.K
    3.9-rc5-0 Linux 3.9.0-r   80 4.46 8.36 53.8 91.3 223. 8.12 24.2 4725 13.K 33.K
    
    File & VM system latencies in microseconds - smaller is better
    -------------------------------------------------------------------------------
    Host                 OS   0K File      10K File     Mmap    Prot   Page 100fd
                            Create Delete Create Delete Latency Fault  Fault selct
    --------- ------------- ------ ------ ------ ------ ------- ----- ------- -----
    3.9-rc5-0 Linux 3.9.0-r  314.7  223.2 1054.9  390.2  3615.0 1.590 20.1 126.6
    3.9-rc5-0 Linux 3.9.0-r  265.8  183.8 1014.2  314.1  3193.0 6.910 18.8 110.4
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index 3eb2ce0bdfa3..fe91719866a5 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -21,20 +21,28 @@
 
 #ifndef __ASSEMBLY__
 
-#define tlb_flush(tlb) local_flush_tlb_mm((tlb)->mm)
+#define tlb_flush(tlb)				\
+do {						\
+	if (tlb->fullmm)			\
+		flush_tlb_mm((tlb)->mm);	\
+} while (0)
 
 /*
  * This pair is called at time of munmap/exit to flush cache and TLB entries
  * for mappings being torn down.
  * 1) cache-flush part -implemented via tlb_start_vma( ) can be NOP (for now)
  *    as we don't support aliasing configs in our VIPT D$.
- * 2) tlb-flush part - implemted via tlb_end_vma( ) can be NOP as well-
- *    albiet for difft reasons - its better handled by moving to new ASID
+ * 2) tlb-flush part - implemted via tlb_end_vma( ) flushes the TLB range
  *
  * Note, read http://lkml.org/lkml/2004/1/15/6
  */
 #define tlb_start_vma(tlb, vma)
-#define tlb_end_vma(tlb, vma)
+
+#define tlb_end_vma(tlb, vma)						\
+do {									\
+	if (!tlb->fullmm)						\
+		flush_tlb_range(vma, vma->vm_start, vma->vm_end);	\
+} while (0)
 
 #define __tlb_remove_tlb_entry(tlb, ptep, address)
 

commit d79e678d746d3d4234477f08ce7d27d55ebe283a
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Jan 18 15:12:20 2013 +0530

    ARC: TLB flush Handling
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
index b571e121929b..3eb2ce0bdfa3 100644
--- a/arch/arc/include/asm/tlb.h
+++ b/arch/arc/include/asm/tlb.h
@@ -21,6 +21,23 @@
 
 #ifndef __ASSEMBLY__
 
+#define tlb_flush(tlb) local_flush_tlb_mm((tlb)->mm)
+
+/*
+ * This pair is called at time of munmap/exit to flush cache and TLB entries
+ * for mappings being torn down.
+ * 1) cache-flush part -implemented via tlb_start_vma( ) can be NOP (for now)
+ *    as we don't support aliasing configs in our VIPT D$.
+ * 2) tlb-flush part - implemted via tlb_end_vma( ) can be NOP as well-
+ *    albiet for difft reasons - its better handled by moving to new ASID
+ *
+ * Note, read http://lkml.org/lkml/2004/1/15/6
+ */
+#define tlb_start_vma(tlb, vma)
+#define tlb_end_vma(tlb, vma)
+
+#define __tlb_remove_tlb_entry(tlb, ptep, address)
+
 #include <linux/pagemap.h>
 #include <asm-generic/tlb.h>
 

commit cc562d2eae93bc2768a6575d31c089719e8939e8
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Jan 18 15:12:19 2013 +0530

    ARC: MMU Exception Handling
    
    * MMU I-TLB / D-TLB Miss Exceptions
      - Fast Path TLB Refill Handler
      - slowpath TLB creation via do_page_fault() -> update_mmu_cache()
    * Duplicate PD Exception Handler
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/include/asm/tlb.h b/arch/arc/include/asm/tlb.h
new file mode 100644
index 000000000000..b571e121929b
--- /dev/null
+++ b/arch/arc/include/asm/tlb.h
@@ -0,0 +1,41 @@
+/*
+ * Copyright (C) 2004, 2007-2010, 2011-2012 Synopsys, Inc. (www.synopsys.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _ASM_ARC_TLB_H
+#define _ASM_ARC_TLB_H
+
+#ifdef __KERNEL__
+
+#include <asm/pgtable.h>
+
+/* Masks for actual TLB "PD"s */
+#define PTE_BITS_IN_PD0	(_PAGE_GLOBAL | _PAGE_PRESENT)
+#define PTE_BITS_IN_PD1	(PAGE_MASK | _PAGE_CACHEABLE | \
+			 _PAGE_EXECUTE | _PAGE_WRITE | _PAGE_READ | \
+			 _PAGE_K_EXECUTE | _PAGE_K_WRITE | _PAGE_K_READ)
+
+#ifndef __ASSEMBLY__
+
+#include <linux/pagemap.h>
+#include <asm-generic/tlb.h>
+
+#ifdef CONFIG_ARC_DBG_TLB_PARANOIA
+void tlb_paranoid_check(unsigned int pid_sw, unsigned long address);
+#else
+#define tlb_paranoid_check(a, b)
+#endif
+
+void arc_mmu_init(void);
+extern char *arc_mmu_mumbojumbo(int cpu_id, char *buf, int len);
+void __init read_decode_mmu_bcr(void);
+
+#endif	/* __ASSEMBLY__ */
+
+#endif	/* __KERNEL__ */
+
+#endif /* _ASM_ARC_TLB_H */
