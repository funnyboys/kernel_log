commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index 9881bd740ccc..fac4adc90204 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -1,9 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2004, 2007-2010, 2011-2012 Synopsys, Inc. (www.synopsys.com)
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #include <linux/vmalloc.h>

commit 26c01c49d559268527d78f45a6818fae0c204a45
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Aug 26 15:41:29 2016 -0700

    ARCv2: Support dynamic peripheral address space in HS38 rel 3.0 cores
    
    HS release 3.0 provides for even more flexibility in specifying the
    volatile address space for mapping peripherals.
    
    With HS 2.1 @start was made flexible / programmable - with HS 3.0 even
    @end can be setup (vs. fixed to 0xFFFF_FFFF before).
    
    So add code to reflect that and while at it remove an unused struct
    defintion
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index f52b7db67fd3..9881bd740ccc 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -19,7 +19,7 @@ static inline bool arc_uncached_addr_space(phys_addr_t paddr)
 	if (is_isa_arcompact()) {
 		if (paddr >= ARC_UNCACHED_ADDR_SPACE)
 			return true;
-	} else if (paddr >= perip_base && paddr <= 0xFFFFFFFF) {
+	} else if (paddr >= perip_base && paddr <= perip_end) {
 		return true;
 	}
 

commit 627c88b68f0b7a1c4c1f12c8c4628e684f2185e2
Author: Alexey Brodkin <Alexey.Brodkin@synopsys.com>
Date:   Wed Jun 29 20:43:58 2016 +0300

    ARC: typo fix in mm/ioremap.c
    
    Signed-off-by: Alexey Brodkin <abrodkin@synopsys.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index 49b8abd1115c..f52b7db67fd3 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -49,7 +49,7 @@ EXPORT_SYMBOL(ioremap);
 /*
  * ioremap with access flags
  * Cache semantics wise it is same as ioremap - "forced" uncached.
- * However unline vanilla ioremap which bypasses ARC MMU for addresses in
+ * However unlike vanilla ioremap which bypasses ARC MMU for addresses in
  * ARC hardware uncached region, this one still goes thru the MMU as caller
  * might need finer access control (R/W/X)
  */

commit deaf7565eb618a80534844300aeacffa14125182
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Sat Oct 24 19:31:16 2015 +0530

    ARCv2: ioremap: Support dynamic peripheral address space
    
    The peripheral address space is architectural address window which is
    uncached and typically used to wire up peripherals.
    
    For ARC700 cores (ARCompact ISA based) this was fixed to 1GB region
    0xC000_0000 - 0xFFFF_FFFF.
    
    For ARCv2 based HS38 cores the start address is flexible and can be
    0xC, 0xD, 0xE, 0xF 000_000 by programming AUX_NON_VOLATILE_LIMIT reg
    (typically done in bootloader)
    
    Further in cas of PAE, the physical address can extend beyond 4GB so
    need to confine this check, otherwise all pages beyond 4GB will be
    treated as uncached
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index 75b0ca6e387e..49b8abd1115c 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -14,6 +14,18 @@
 #include <linux/slab.h>
 #include <linux/cache.h>
 
+static inline bool arc_uncached_addr_space(phys_addr_t paddr)
+{
+	if (is_isa_arcompact()) {
+		if (paddr >= ARC_UNCACHED_ADDR_SPACE)
+			return true;
+	} else if (paddr >= perip_base && paddr <= 0xFFFFFFFF) {
+		return true;
+	}
+
+	return false;
+}
+
 void __iomem *ioremap(phys_addr_t paddr, unsigned long size)
 {
 	phys_addr_t end;
@@ -27,7 +39,7 @@ void __iomem *ioremap(phys_addr_t paddr, unsigned long size)
 	 * If the region is h/w uncached, MMU mapping can be elided as optim
 	 * The cast to u32 is fine as this region can only be inside 4GB
 	 */
-	if (paddr >= ARC_UNCACHED_ADDR_SPACE)
+	if (arc_uncached_addr_space(paddr))
 		return (void __iomem *)(u32)paddr;
 
 	return ioremap_prot(paddr, size, PAGE_KERNEL_NO_CACHE);
@@ -85,7 +97,8 @@ EXPORT_SYMBOL(ioremap_prot);
 
 void iounmap(const void __iomem *addr)
 {
-	if (addr >= (void __force __iomem *)ARC_UNCACHED_ADDR_SPACE)
+	/* weird double cast to handle phys_addr_t > 32 bits */
+	if (arc_uncached_addr_space((phys_addr_t)(u32)addr))
 		return;
 
 	vfree((void *)(PAGE_MASK & (unsigned long __force)addr));

commit f5db19e93f680160a0fb3e2b05ceb4832b24d486
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Wed Mar 16 15:04:39 2016 +0530

    ARC: dma: ioremap: use phys_addr_t consistenctly in code paths
    
    To support dma in physical memory beyond 4GB with PAE40
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index 739e65f355de..75b0ca6e387e 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -14,18 +14,21 @@
 #include <linux/slab.h>
 #include <linux/cache.h>
 
-void __iomem *ioremap(unsigned long paddr, unsigned long size)
+void __iomem *ioremap(phys_addr_t paddr, unsigned long size)
 {
-	unsigned long end;
+	phys_addr_t end;
 
 	/* Don't allow wraparound or zero size */
 	end = paddr + size - 1;
 	if (!size || (end < paddr))
 		return NULL;
 
-	/* If the region is h/w uncached, avoid MMU mappings */
+	/*
+	 * If the region is h/w uncached, MMU mapping can be elided as optim
+	 * The cast to u32 is fine as this region can only be inside 4GB
+	 */
 	if (paddr >= ARC_UNCACHED_ADDR_SPACE)
-		return (void __iomem *)paddr;
+		return (void __iomem *)(u32)paddr;
 
 	return ioremap_prot(paddr, size, PAGE_KERNEL_NO_CACHE);
 }
@@ -41,9 +44,9 @@ EXPORT_SYMBOL(ioremap);
 void __iomem *ioremap_prot(phys_addr_t paddr, unsigned long size,
 			   unsigned long flags)
 {
-	void __iomem *vaddr;
+	unsigned long vaddr;
 	struct vm_struct *area;
-	unsigned long off, end;
+	phys_addr_t off, end;
 	pgprot_t prot = __pgprot(flags);
 
 	/* Don't allow wraparound, zero size */
@@ -70,9 +73,8 @@ void __iomem *ioremap_prot(phys_addr_t paddr, unsigned long size,
 	if (!area)
 		return NULL;
 	area->phys_addr = paddr;
-	vaddr = (void __iomem *)area->addr;
-	if (ioremap_page_range((unsigned long)vaddr,
-			       (unsigned long)vaddr + size, paddr, prot)) {
+	vaddr = (unsigned long)area->addr;
+	if (ioremap_page_range(vaddr, vaddr + size, paddr, prot)) {
 		vunmap((void __force *)vaddr);
 		return NULL;
 	}

commit 1ec9db1056b0c4b8b9dfca4736634c7c8e0833d5
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Wed Mar 6 16:53:44 2013 +0530

    ARC: Use <linux/*> headers instead of <asm/*>
    
    Silences the following checkpatch warnings:
    WARNING: Use #include <linux/ptrace.h> instead of <asm/ptrace.h>
    WARNING: Use #include <linux/kprobes.h> instead of <asm/kprobes.h>
    WARNING: Use #include <linux/kgdb.h> instead of <asm/kgdb.h>
    WARNING: Use #include <linux/uaccess.h> instead of <asm/uaccess.h>
    WARNING: Use #include <linux/cache.h> instead of <asm/cache.h>
    
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index 3e5c92c79936..739e65f355de 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -12,7 +12,7 @@
 #include <linux/io.h>
 #include <linux/mm.h>
 #include <linux/slab.h>
-#include <asm/cache.h>
+#include <linux/cache.h>
 
 void __iomem *ioremap(unsigned long paddr, unsigned long size)
 {

commit 4368902bb90f0e208387f336c3fce0e6b2a110fc
Author: Gilad Ben-Yossef <gilad@benyossef.com>
Date:   Tue Jan 22 16:48:45 2013 +0530

    ARC: Add support for ioremap_prot API
    
    Implement ioremap_prot() to allow mapping IO memory with variable
    protection
    via TLB.
    
    Implementing this allows the /dev/mem driver to use its generic access()
    VMA callback, which in turn allows ptrace to examine data in memory
    mapped regions mapped via /dev/mem, such as Arc DCCM.
    
    The end result is that it is possible to examine values of variables
    placed into DCCM in user space programs via GDB.
    
    CC: Alexey Brodkin <Alexey.Brodkin@synopsys.com>
    CC: Noam Camus <noamc@ezchip.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Signed-off-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
index 52518b6d0f28..3e5c92c79936 100644
--- a/arch/arc/mm/ioremap.c
+++ b/arch/arc/mm/ioremap.c
@@ -16,25 +16,49 @@
 
 void __iomem *ioremap(unsigned long paddr, unsigned long size)
 {
-	unsigned long vaddr;
-	struct vm_struct *area;
-	unsigned long off, end;
-	const pgprot_t prot = PAGE_KERNEL_NO_CACHE;
+	unsigned long end;
 
 	/* Don't allow wraparound or zero size */
 	end = paddr + size - 1;
 	if (!size || (end < paddr))
 		return NULL;
 
-	/* If the region is h/w uncached, nothing special needed */
+	/* If the region is h/w uncached, avoid MMU mappings */
 	if (paddr >= ARC_UNCACHED_ADDR_SPACE)
 		return (void __iomem *)paddr;
 
+	return ioremap_prot(paddr, size, PAGE_KERNEL_NO_CACHE);
+}
+EXPORT_SYMBOL(ioremap);
+
+/*
+ * ioremap with access flags
+ * Cache semantics wise it is same as ioremap - "forced" uncached.
+ * However unline vanilla ioremap which bypasses ARC MMU for addresses in
+ * ARC hardware uncached region, this one still goes thru the MMU as caller
+ * might need finer access control (R/W/X)
+ */
+void __iomem *ioremap_prot(phys_addr_t paddr, unsigned long size,
+			   unsigned long flags)
+{
+	void __iomem *vaddr;
+	struct vm_struct *area;
+	unsigned long off, end;
+	pgprot_t prot = __pgprot(flags);
+
+	/* Don't allow wraparound, zero size */
+	end = paddr + size - 1;
+	if ((!size) || (end < paddr))
+		return NULL;
+
 	/* An early platform driver might end up here */
 	if (!slab_is_available())
 		return NULL;
 
-	/* Mappings have to be page-aligned, page-sized */
+	/* force uncached */
+	prot = pgprot_noncached(prot);
+
+	/* Mappings have to be page-aligned */
 	off = paddr & ~PAGE_MASK;
 	paddr &= PAGE_MASK;
 	size = PAGE_ALIGN(end + 1) - paddr;
@@ -45,17 +69,17 @@ void __iomem *ioremap(unsigned long paddr, unsigned long size)
 	area = get_vm_area(size, VM_IOREMAP);
 	if (!area)
 		return NULL;
-
 	area->phys_addr = paddr;
-	vaddr = (unsigned long)area->addr;
-	if (ioremap_page_range(vaddr, vaddr + size, paddr, prot)) {
-		vfree(area->addr);
+	vaddr = (void __iomem *)area->addr;
+	if (ioremap_page_range((unsigned long)vaddr,
+			       (unsigned long)vaddr + size, paddr, prot)) {
+		vunmap((void __force *)vaddr);
 		return NULL;
 	}
-
 	return (void __iomem *)(off + (char __iomem *)vaddr);
 }
-EXPORT_SYMBOL(ioremap);
+EXPORT_SYMBOL(ioremap_prot);
+
 
 void iounmap(const void __iomem *addr)
 {

commit 1162b0701b14ba112d4e3fe5c27c694caf983539
Author: Vineet Gupta <vgupta@synopsys.com>
Date:   Fri Jan 18 15:12:20 2013 +0530

    ARC: I/O and DMA Mappings
    
    Signed-off-by: Vineet Gupta <vgupta@synopsys.com>

diff --git a/arch/arc/mm/ioremap.c b/arch/arc/mm/ioremap.c
new file mode 100644
index 000000000000..52518b6d0f28
--- /dev/null
+++ b/arch/arc/mm/ioremap.c
@@ -0,0 +1,67 @@
+/*
+ * Copyright (C) 2004, 2007-2010, 2011-2012 Synopsys, Inc. (www.synopsys.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <asm/cache.h>
+
+void __iomem *ioremap(unsigned long paddr, unsigned long size)
+{
+	unsigned long vaddr;
+	struct vm_struct *area;
+	unsigned long off, end;
+	const pgprot_t prot = PAGE_KERNEL_NO_CACHE;
+
+	/* Don't allow wraparound or zero size */
+	end = paddr + size - 1;
+	if (!size || (end < paddr))
+		return NULL;
+
+	/* If the region is h/w uncached, nothing special needed */
+	if (paddr >= ARC_UNCACHED_ADDR_SPACE)
+		return (void __iomem *)paddr;
+
+	/* An early platform driver might end up here */
+	if (!slab_is_available())
+		return NULL;
+
+	/* Mappings have to be page-aligned, page-sized */
+	off = paddr & ~PAGE_MASK;
+	paddr &= PAGE_MASK;
+	size = PAGE_ALIGN(end + 1) - paddr;
+
+	/*
+	 * Ok, go for it..
+	 */
+	area = get_vm_area(size, VM_IOREMAP);
+	if (!area)
+		return NULL;
+
+	area->phys_addr = paddr;
+	vaddr = (unsigned long)area->addr;
+	if (ioremap_page_range(vaddr, vaddr + size, paddr, prot)) {
+		vfree(area->addr);
+		return NULL;
+	}
+
+	return (void __iomem *)(off + (char __iomem *)vaddr);
+}
+EXPORT_SYMBOL(ioremap);
+
+void iounmap(const void __iomem *addr)
+{
+	if (addr >= (void __force __iomem *)ARC_UNCACHED_ADDR_SPACE)
+		return;
+
+	vfree((void *)(PAGE_MASK & (unsigned long __force)addr));
+}
+EXPORT_SYMBOL(iounmap);
