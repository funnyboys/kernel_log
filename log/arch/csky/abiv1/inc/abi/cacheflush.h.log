commit 997153b9a75c08d545ad45e6f8ceb432435d2425
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Fri Jan 31 20:33:10 2020 +0800

    csky: Add flush_icache_mm to defer flush icache all
    
    Some CPUs don't support icache.va instruction to maintain the whole
    smp cores' icache. Using icache.all + IPI casue a lot on performace
    and using defer mechanism could reduce the number of calling icache
    _flush_all functions.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/abiv1/inc/abi/cacheflush.h b/arch/csky/abiv1/inc/abi/cacheflush.h
index a73702704f38..d3e04208d53c 100644
--- a/arch/csky/abiv1/inc/abi/cacheflush.h
+++ b/arch/csky/abiv1/inc/abi/cacheflush.h
@@ -48,6 +48,8 @@ extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, u
 
 #define flush_icache_page(vma, page)		do {} while (0);
 #define flush_icache_range(start, end)		cache_wbinv_range(start, end)
+#define flush_icache_mm_range(mm, start, end)	cache_wbinv_range(start, end)
+#define flush_icache_deferred(mm)		do {} while (0);
 
 #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
 do { \

commit a1176734132c630b50908c36563e05fb3599682c
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Sat Jan 25 00:37:09 2020 +0800

    csky: Remove unnecessary flush_icache_* implementation
    
    The abiv2 CPUs are all PIPT cache, so there is no need to implement
    flush_icache_page function.
    
    The function flush_icache_user_range hasn't been used, so just
    remove it.
    
    The function flush_cache_range is not necessary for PIPT cache when
    tlb mapping changed.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/abiv1/inc/abi/cacheflush.h b/arch/csky/abiv1/inc/abi/cacheflush.h
index 79ef9e8c1afd..a73702704f38 100644
--- a/arch/csky/abiv1/inc/abi/cacheflush.h
+++ b/arch/csky/abiv1/inc/abi/cacheflush.h
@@ -49,9 +49,6 @@ extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, u
 #define flush_icache_page(vma, page)		do {} while (0);
 #define flush_icache_range(start, end)		cache_wbinv_range(start, end)
 
-#define flush_icache_user_range(vma,page,addr,len) \
-	flush_dcache_page(page)
-
 #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
 do { \
 	memcpy(dst, src, len); \

commit 4ad35c1f56386c8e7019c921bba1af109fde9693
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Wed Aug 21 19:15:52 2019 +0800

    csky: Fixup 610 vipt cache flush mechanism
    
    610 has vipt aliasing issue, so we need to finish the cache flush
    apis mentioned in cachetlb.rst to avoid data corruption.
    
    Here is the list of modified apis in the patch:
    
     - flush_kernel_dcache_page      (new add)
     - flush_dcache_mmap_lock        (new add)
     - flush_dcache_mmap_unlock      (new add)
     - flush_kernel_vmap_range       (new add)
     - invalidate_kernel_vmap_range  (new add)
     - flush_anon_page               (new add)
     - flush_cache_range             (new add)
     - flush_cache_vmap              (flush all)
     - flush_cache_vunmap            (flush all)
     - flush_cache_mm                (only dcache flush)
     - flush_icache_page             (just nop)
     - copy_from_user_page           (remove no need flush)
     - copy_to_user_page             (remove no need flush)
    
    Change to V2:
     - Fixup compile error with xa_lock*(&mapping->i_pages)
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@infradead.org>

diff --git a/arch/csky/abiv1/inc/abi/cacheflush.h b/arch/csky/abiv1/inc/abi/cacheflush.h
index fce5604cef40..79ef9e8c1afd 100644
--- a/arch/csky/abiv1/inc/abi/cacheflush.h
+++ b/arch/csky/abiv1/inc/abi/cacheflush.h
@@ -4,26 +4,49 @@
 #ifndef __ABI_CSKY_CACHEFLUSH_H
 #define __ABI_CSKY_CACHEFLUSH_H
 
-#include <linux/compiler.h>
+#include <linux/mm.h>
 #include <asm/string.h>
 #include <asm/cache.h>
 
 #define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
 extern void flush_dcache_page(struct page *);
 
-#define flush_cache_mm(mm)			cache_wbinv_all()
+#define flush_cache_mm(mm)			dcache_wbinv_all()
 #define flush_cache_page(vma, page, pfn)	cache_wbinv_all()
 #define flush_cache_dup_mm(mm)			cache_wbinv_all()
 
+#define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
+extern void flush_kernel_dcache_page(struct page *);
+
+#define flush_dcache_mmap_lock(mapping)		xa_lock_irq(&mapping->i_pages)
+#define flush_dcache_mmap_unlock(mapping)	xa_unlock_irq(&mapping->i_pages)
+
+static inline void flush_kernel_vmap_range(void *addr, int size)
+{
+	dcache_wbinv_all();
+}
+static inline void invalidate_kernel_vmap_range(void *addr, int size)
+{
+	dcache_wbinv_all();
+}
+
+#define ARCH_HAS_FLUSH_ANON_PAGE
+static inline void flush_anon_page(struct vm_area_struct *vma,
+			 struct page *page, unsigned long vmaddr)
+{
+	if (PageAnon(page))
+		cache_wbinv_all();
+}
+
 /*
  * if (current_mm != vma->mm) cache_wbinv_range(start, end) will be broken.
  * Use cache_wbinv_all() here and need to be improved in future.
  */
-#define flush_cache_range(vma, start, end)	cache_wbinv_all()
-#define flush_cache_vmap(start, end)		cache_wbinv_range(start, end)
-#define flush_cache_vunmap(start, end)		cache_wbinv_range(start, end)
+extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
+#define flush_cache_vmap(start, end)		cache_wbinv_all()
+#define flush_cache_vunmap(start, end)		cache_wbinv_all()
 
-#define flush_icache_page(vma, page)		cache_wbinv_all()
+#define flush_icache_page(vma, page)		do {} while (0);
 #define flush_icache_range(start, end)		cache_wbinv_range(start, end)
 
 #define flush_icache_user_range(vma,page,addr,len) \
@@ -31,19 +54,13 @@ extern void flush_dcache_page(struct page *);
 
 #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
 do { \
-	cache_wbinv_all(); \
 	memcpy(dst, src, len); \
-	cache_wbinv_all(); \
 } while (0)
 
 #define copy_to_user_page(vma, page, vaddr, dst, src, len) \
 do { \
-	cache_wbinv_all(); \
 	memcpy(dst, src, len); \
 	cache_wbinv_all(); \
 } while (0)
 
-#define flush_dcache_mmap_lock(mapping)		do {} while (0)
-#define flush_dcache_mmap_unlock(mapping)	do {} while (0)
-
 #endif /* __ABI_CSKY_CACHEFLUSH_H */

commit dc140045c0cace809af872e3799e8fbe1b7d7f86
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Tue Aug 20 12:47:24 2019 +0800

    csky: Fixup defer cache flush for 610
    
    We use defer cache flush mechanism to improve the performance of
    610, but the implementation is wrong. We fix it up now and update
    the mechanism:
    
     - Zero page needn't be flushed.
     - If page is file mapping & non-touched in user space, defer flush.
     - If page is anon mapping or dirty file mapping, flush immediately.
     - In update_mmu_cache finish the defer flush by flush_dcache_page().
    
    For 610 we need take care the dcache aliasing issue:
     - VIPT cache with 8K-bytes size per way in 4K page granularity.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/abiv1/inc/abi/cacheflush.h b/arch/csky/abiv1/inc/abi/cacheflush.h
index 5f663aef9b1b..fce5604cef40 100644
--- a/arch/csky/abiv1/inc/abi/cacheflush.h
+++ b/arch/csky/abiv1/inc/abi/cacheflush.h
@@ -26,8 +26,8 @@ extern void flush_dcache_page(struct page *);
 #define flush_icache_page(vma, page)		cache_wbinv_all()
 #define flush_icache_range(start, end)		cache_wbinv_range(start, end)
 
-#define flush_icache_user_range(vma, pg, adr, len) \
-				cache_wbinv_range(adr, adr + len)
+#define flush_icache_user_range(vma,page,addr,len) \
+	flush_dcache_page(page)
 
 #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
 do { \

commit 00a9730e1007c6cc87a7c78af2f24a4105d616ee
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Wed Sep 5 14:25:10 2018 +0800

    csky: Cache and TLB routines
    
    This patch adds cache and tlb sync codes for abiv1 & abiv2.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/abiv1/inc/abi/cacheflush.h b/arch/csky/abiv1/inc/abi/cacheflush.h
new file mode 100644
index 000000000000..5f663aef9b1b
--- /dev/null
+++ b/arch/csky/abiv1/inc/abi/cacheflush.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+// Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.
+
+#ifndef __ABI_CSKY_CACHEFLUSH_H
+#define __ABI_CSKY_CACHEFLUSH_H
+
+#include <linux/compiler.h>
+#include <asm/string.h>
+#include <asm/cache.h>
+
+#define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
+extern void flush_dcache_page(struct page *);
+
+#define flush_cache_mm(mm)			cache_wbinv_all()
+#define flush_cache_page(vma, page, pfn)	cache_wbinv_all()
+#define flush_cache_dup_mm(mm)			cache_wbinv_all()
+
+/*
+ * if (current_mm != vma->mm) cache_wbinv_range(start, end) will be broken.
+ * Use cache_wbinv_all() here and need to be improved in future.
+ */
+#define flush_cache_range(vma, start, end)	cache_wbinv_all()
+#define flush_cache_vmap(start, end)		cache_wbinv_range(start, end)
+#define flush_cache_vunmap(start, end)		cache_wbinv_range(start, end)
+
+#define flush_icache_page(vma, page)		cache_wbinv_all()
+#define flush_icache_range(start, end)		cache_wbinv_range(start, end)
+
+#define flush_icache_user_range(vma, pg, adr, len) \
+				cache_wbinv_range(adr, adr + len)
+
+#define copy_from_user_page(vma, page, vaddr, dst, src, len) \
+do { \
+	cache_wbinv_all(); \
+	memcpy(dst, src, len); \
+	cache_wbinv_all(); \
+} while (0)
+
+#define copy_to_user_page(vma, page, vaddr, dst, src, len) \
+do { \
+	cache_wbinv_all(); \
+	memcpy(dst, src, len); \
+	cache_wbinv_all(); \
+} while (0)
+
+#define flush_dcache_mmap_lock(mapping)		do {} while (0)
+#define flush_dcache_mmap_unlock(mapping)	do {} while (0)
+
+#endif /* __ABI_CSKY_CACHEFLUSH_H */
