commit c910951a85d8fef5a8dabe08df45201f0637690a
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:47 2020 -0700

    csky: replace definitions of __pXd_offset() with pXd_index()
    
    All architectures use pXd_index() to get an entry in the page table page
    corresponding to a virtual address.
    
    Align csky with other architectures.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-5-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index eda2b4291485..af627128314f 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -157,9 +157,9 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 	unsigned long vaddr;
 
 	vaddr = start;
-	i = __pgd_offset(vaddr);
-	j = __pud_offset(vaddr);
-	k = __pmd_offset(vaddr);
+	i = pgd_index(vaddr);
+	j = pud_index(vaddr);
+	k = pmd_index(vaddr);
 	pgd = pgd_base + i;
 
 	for ( ; (i < PTRS_PER_PGD) && (vaddr != end); pgd++, i++) {

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index cb64d8647a78..eda2b4291485 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -24,7 +24,6 @@
 #include <asm/setup.h>
 #include <asm/cachectl.h>
 #include <asm/dma.h>
-#include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/mmu_context.h>
 #include <asm/sections.h>

commit d46869aaab7981f9153d3271228005a52ef18591
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Tue Oct 8 14:25:13 2019 +0800

    csky: Add setup_initrd check code
    
    We should give some necessary check for initrd just like other
    architectures and it seems that setup_initrd() could be a common
    code for all architectures.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index dbb4b2dfe4b7..cb64d8647a78 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -19,6 +19,7 @@
 #include <linux/swap.h>
 #include <linux/proc_fs.h>
 #include <linux/pfn.h>
+#include <linux/initrd.h>
 
 #include <asm/setup.h>
 #include <asm/cachectl.h>
@@ -36,6 +37,45 @@ unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]
 						__page_aligned_bss;
 EXPORT_SYMBOL(empty_zero_page);
 
+#ifdef CONFIG_BLK_DEV_INITRD
+static void __init setup_initrd(void)
+{
+	unsigned long size;
+
+	if (initrd_start >= initrd_end) {
+		pr_err("initrd not found or empty");
+		goto disable;
+	}
+
+	if (__pa(initrd_end) > PFN_PHYS(max_low_pfn)) {
+		pr_err("initrd extends beyond end of memory");
+		goto disable;
+	}
+
+	size = initrd_end - initrd_start;
+
+	if (memblock_is_region_reserved(__pa(initrd_start), size)) {
+		pr_err("INITRD: 0x%08lx+0x%08lx overlaps in-use memory region",
+		       __pa(initrd_start), size);
+		goto disable;
+	}
+
+	memblock_reserve(__pa(initrd_start), size);
+
+	pr_info("Initial ramdisk at: 0x%p (%lu bytes)\n",
+		(void *)(initrd_start), size);
+
+	initrd_below_start_ok = 1;
+
+	return;
+
+disable:
+	initrd_start = initrd_end = 0;
+
+	pr_err(" - disabling initrd\n");
+}
+#endif
+
 void __init mem_init(void)
 {
 #ifdef CONFIG_HIGHMEM
@@ -47,6 +87,10 @@ void __init mem_init(void)
 #endif
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
+#ifdef CONFIG_BLK_DEV_INITRD
+	setup_initrd();
+#endif
+
 	memblock_free_all();
 
 #ifdef CONFIG_HIGHMEM

commit 7f4a567332f035ab16b29010fbd04a0f10183c77
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Mon Dec 30 15:53:37 2019 +0800

    csky/mm: Fixup export invalid_pte_table symbol
    
    There is no present bit in csky pmd hardware, so we need to prepare invalid_pte_table
    for empty pmd entry and the functions (pmd_none & pmd_present) in pgtable.h need
    invalid_pte_talbe to get result. If a module use these functions, we need export the
    symbol for it.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>
    Cc: Mo Qihui <qihui.mo@verisilicon.com>
    Cc: Zhange Jian <zhang_jian5@dahuatech.com>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index 322eb7bd7962..dbb4b2dfe4b7 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -31,6 +31,7 @@
 
 pgd_t swapper_pg_dir[PTRS_PER_PGD] __page_aligned_bss;
 pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned_bss;
+EXPORT_SYMBOL(invalid_pte_table);
 unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]
 						__page_aligned_bss;
 EXPORT_SYMBOL(empty_zero_page);

commit f136008f31e91060d6b6e6e031cb2c827448e280
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Sun Dec 1 22:34:19 2019 +0800

    csky: Separate fixaddr_init from highmem
    
    After fixaddr_init is separated from highmem, we could use tcm
    without highmem selected. (610 (abiv1) don't support highmem,
    but it could use tcm now.)
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index d4c2292ea46b..322eb7bd7962 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -101,3 +101,50 @@ void __init pre_mmu_init(void)
 	/* Setup page mask to 4k */
 	write_mmu_pagemask(0);
 }
+
+void __init fixrange_init(unsigned long start, unsigned long end,
+			pgd_t *pgd_base)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+	int i, j, k;
+	unsigned long vaddr;
+
+	vaddr = start;
+	i = __pgd_offset(vaddr);
+	j = __pud_offset(vaddr);
+	k = __pmd_offset(vaddr);
+	pgd = pgd_base + i;
+
+	for ( ; (i < PTRS_PER_PGD) && (vaddr != end); pgd++, i++) {
+		pud = (pud_t *)pgd;
+		for ( ; (j < PTRS_PER_PUD) && (vaddr != end); pud++, j++) {
+			pmd = (pmd_t *)pud;
+			for (; (k < PTRS_PER_PMD) && (vaddr != end); pmd++, k++) {
+				if (pmd_none(*pmd)) {
+					pte = (pte_t *) memblock_alloc_low(PAGE_SIZE, PAGE_SIZE);
+					if (!pte)
+						panic("%s: Failed to allocate %lu bytes align=%lx\n",
+						      __func__, PAGE_SIZE,
+						      PAGE_SIZE);
+
+					set_pmd(pmd, __pmd(__pa(pte)));
+					BUG_ON(pte != pte_offset_kernel(pmd, 0));
+				}
+				vaddr += PMD_SIZE;
+			}
+			k = 0;
+		}
+		j = 0;
+	}
+}
+
+void __init fixaddr_init(void)
+{
+	unsigned long vaddr;
+
+	vaddr = __fix_to_virt(__end_of_fixed_addresses - 1) & PMD_MASK;
+	fixrange_init(vaddr, vaddr + PMD_SIZE, swapper_pg_dir);
+}

commit fdbdcddc2c93096e9b956de930d2d710a1342502
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Aug 28 16:35:19 2019 +0300

    csky: Use generic free_initrd_mem()
    
    The csky implementation of free_initrd_mem() is an open-coded version of
    free_reserved_area() without poisoning.
    
    Remove it and make csky use the generic version of free_initrd_mem().
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Guo Ren <guoren@kernel.org>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index eb0dc9e5065f..d4c2292ea46b 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -60,22 +60,6 @@ void __init mem_init(void)
 	mem_init_print_info(NULL);
 }
 
-#ifdef CONFIG_BLK_DEV_INITRD
-void free_initrd_mem(unsigned long start, unsigned long end)
-{
-	if (start < end)
-		pr_info("Freeing initrd memory: %ldk freed\n",
-			(end - start) >> 10);
-
-	for (; start < end; start += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page(start));
-		init_page_count(virt_to_page(start));
-		free_page(start);
-		totalram_pages_inc();
-	}
-}
-#endif
-
 extern char __init_begin[], __init_end[];
 
 void free_initmem(void)

commit 9d35dc3006a9865eb5b55cc79df49933601131f8
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Tue Jun 18 17:20:10 2019 +0800

    csky: Revert mmu ASID mechanism
    
    Current C-SKY ASID mechanism is from mips and it doesn't work well
    with multi-cores. ASID per core mechanism is not suitable for C-SKY
    SMP tlb maintain operations, eg: tlbi.vas need share the same asid
    in all processors and it'll invalid the tlb entry in all cores with
    the same asid.
    
    This patch is prepare for new ASID mechanism.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index 66e597053488..eb0dc9e5065f 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -114,8 +114,6 @@ void __init pre_mmu_init(void)
 	TLBMISS_HANDLER_SETUP_PGD(swapper_pg_dir);
 	TLBMISS_HANDLER_SETUP_PGD_KERNEL(swapper_pg_dir);
 
-	asid_cache(smp_processor_id()) = ASID_FIRST_VERSION;
-
 	/* Setup page mask to 4k */
 	write_mmu_pagemask(0);
 }

commit ca79b0c211af63fa3276f0e3fd7dd9ada2439839
Author: Arun KS <arunks@codeaurora.org>
Date:   Fri Dec 28 00:34:29 2018 -0800

    mm: convert totalram_pages and totalhigh_pages variables to atomic
    
    totalram_pages and totalhigh_pages are made static inline function.
    
    Main motivation was that managed_page_count_lock handling was complicating
    things.  It was discussed in length here,
    https://lore.kernel.org/patchwork/patch/995739/#1181785 So it seemes
    better to remove the lock and convert variables to atomic, with preventing
    poteintial store-to-read tearing as a bonus.
    
    [akpm@linux-foundation.org: coding style fixes]
    Link: http://lkml.kernel.org/r/1542090790-21750-4-git-send-email-arunks@codeaurora.org
    Signed-off-by: Arun KS <arunks@codeaurora.org>
    Suggested-by: Michal Hocko <mhocko@suse.com>
    Suggested-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index dc07c078f9b8..66e597053488 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -71,7 +71,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 		ClearPageReserved(virt_to_page(start));
 		init_page_count(virt_to_page(start));
 		free_page(start);
-		totalram_pages++;
+		totalram_pages_inc();
 	}
 }
 #endif
@@ -88,7 +88,7 @@ void free_initmem(void)
 		ClearPageReserved(virt_to_page(addr));
 		init_page_count(virt_to_page(addr));
 		free_page(addr);
-		totalram_pages++;
+		totalram_pages_inc();
 		addr += PAGE_SIZE;
 	}
 

commit aca52c39838910605b1063a2243f553aa2a02d5c
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:07:44 2018 -0700

    mm: remove CONFIG_HAVE_MEMBLOCK
    
    All architecures use memblock for early memory management. There is no need
    for the CONFIG_HAVE_MEMBLOCK configuration option.
    
    [rppt@linux.vnet.ibm.com: of/fdt: fixup #ifdefs]
      Link: http://lkml.kernel.org/r/20180919103457.GA20545@rapoport-lnx
    [rppt@linux.vnet.ibm.com: csky: fixups after bootmem removal]
      Link: http://lkml.kernel.org/r/20180926112744.GC4628@rapoport-lnx
    [rppt@linux.vnet.ibm.com: remove stale #else and the code it protects]
      Link: http://lkml.kernel.org/r/1538067825-24835-1-git-send-email-rppt@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1536927045-23536-4-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Jonathan Cameron <jonathan.cameron@huawei.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
index ce2711e050ad..dc07c078f9b8 100644
--- a/arch/csky/mm/init.c
+++ b/arch/csky/mm/init.c
@@ -14,7 +14,6 @@
 #include <linux/ptrace.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
-#include <linux/bootmem.h>
 #include <linux/highmem.h>
 #include <linux/memblock.h>
 #include <linux/swap.h>
@@ -47,7 +46,7 @@ void __init mem_init(void)
 #endif
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
-	free_all_bootmem();
+	memblock_free_all();
 
 #ifdef CONFIG_HIGHMEM
 	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {

commit 013de2d6671d89de3397904749c86a69ac0686f7
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Wed Sep 5 14:25:12 2018 +0800

    csky: MMU and page table management
    
    This patch adds files related to memory management and here is our
    memory-layout:
    
       Fixmap       : 0xffc02000 – 0xfffff000       (4 MB - 12KB)
       Pkmap        : 0xff800000 – 0xffc00000       (4 MB)
       Vmalloc      : 0xf0200000 – 0xff000000       (238 MB)
       Lowmem       : 0x80000000 – 0xc0000000       (1GB)
    
    abiv1 CPU (CK610) is VIPT cache and it doesn't support highmem.
    abiv2 CPUs are all PIPT cache and they could support highmem.
    
    Lowmem is directly mapped by msa0 & msa1 reg, and we needn't setup
    memory page table for it.
    
    Link:https://lore.kernel.org/lkml/20180518215548.GH17671@n2100.armlinux.org.uk/
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/init.c b/arch/csky/mm/init.c
new file mode 100644
index 000000000000..ce2711e050ad
--- /dev/null
+++ b/arch/csky/mm/init.c
@@ -0,0 +1,122 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.
+
+#include <linux/bug.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/pagemap.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/bootmem.h>
+#include <linux/highmem.h>
+#include <linux/memblock.h>
+#include <linux/swap.h>
+#include <linux/proc_fs.h>
+#include <linux/pfn.h>
+
+#include <asm/setup.h>
+#include <asm/cachectl.h>
+#include <asm/dma.h>
+#include <asm/pgtable.h>
+#include <asm/pgalloc.h>
+#include <asm/mmu_context.h>
+#include <asm/sections.h>
+#include <asm/tlb.h>
+
+pgd_t swapper_pg_dir[PTRS_PER_PGD] __page_aligned_bss;
+pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned_bss;
+unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]
+						__page_aligned_bss;
+EXPORT_SYMBOL(empty_zero_page);
+
+void __init mem_init(void)
+{
+#ifdef CONFIG_HIGHMEM
+	unsigned long tmp;
+
+	max_mapnr = highend_pfn;
+#else
+	max_mapnr = max_low_pfn;
+#endif
+	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
+
+	free_all_bootmem();
+
+#ifdef CONFIG_HIGHMEM
+	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
+		struct page *page = pfn_to_page(tmp);
+
+		/* FIXME not sure about */
+		if (!memblock_is_reserved(tmp << PAGE_SHIFT))
+			free_highmem_page(page);
+	}
+#endif
+	mem_init_print_info(NULL);
+}
+
+#ifdef CONFIG_BLK_DEV_INITRD
+void free_initrd_mem(unsigned long start, unsigned long end)
+{
+	if (start < end)
+		pr_info("Freeing initrd memory: %ldk freed\n",
+			(end - start) >> 10);
+
+	for (; start < end; start += PAGE_SIZE) {
+		ClearPageReserved(virt_to_page(start));
+		init_page_count(virt_to_page(start));
+		free_page(start);
+		totalram_pages++;
+	}
+}
+#endif
+
+extern char __init_begin[], __init_end[];
+
+void free_initmem(void)
+{
+	unsigned long addr;
+
+	addr = (unsigned long) &__init_begin;
+
+	while (addr < (unsigned long) &__init_end) {
+		ClearPageReserved(virt_to_page(addr));
+		init_page_count(virt_to_page(addr));
+		free_page(addr);
+		totalram_pages++;
+		addr += PAGE_SIZE;
+	}
+
+	pr_info("Freeing unused kernel memory: %dk freed\n",
+	((unsigned int)&__init_end - (unsigned int)&__init_begin) >> 10);
+}
+
+void pgd_init(unsigned long *p)
+{
+	int i;
+
+	for (i = 0; i < PTRS_PER_PGD; i++)
+		p[i] = __pa(invalid_pte_table);
+}
+
+void __init pre_mmu_init(void)
+{
+	/*
+	 * Setup page-table and enable TLB-hardrefill
+	 */
+	flush_tlb_all();
+	pgd_init((unsigned long *)swapper_pg_dir);
+	TLBMISS_HANDLER_SETUP_PGD(swapper_pg_dir);
+	TLBMISS_HANDLER_SETUP_PGD_KERNEL(swapper_pg_dir);
+
+	asid_cache(smp_processor_id()) = ASID_FIRST_VERSION;
+
+	/* Setup page mask to 4k */
+	write_mmu_pagemask(0);
+}
