commit dd7c983e78a28ff0b22f8bcf32a303b4f79cb318
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Tue Mar 31 22:15:42 2020 +0800

    csky/ftrace: Fixup ftrace_modify_code deadlock without CPU_HAS_ICACHE_INS
    
    If ICACHE_INS is not supported, we use IPI to sync icache on each
    core. But ftrace_modify_code is called from stop_machine from default
    implementation of arch_ftrace_update_code and stop_machine callback
    is irq_disabled. When you call ipi with irq_disabled, a deadlock will
    happen.
    
    We couldn't use icache_flush with irq_disabled, but startup make_nop
    is specific case and it needn't ipi other cores.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/mm/cachev2.c b/arch/csky/mm/cachev2.c
index bc419f8039d3..7a9664adce43 100644
--- a/arch/csky/mm/cachev2.c
+++ b/arch/csky/mm/cachev2.c
@@ -7,8 +7,12 @@
 #include <asm/cache.h>
 #include <asm/barrier.h>
 
+/* for L1-cache */
 #define INS_CACHE		(1 << 0)
+#define DATA_CACHE		(1 << 1)
 #define CACHE_INV		(1 << 4)
+#define CACHE_CLR		(1 << 5)
+#define CACHE_OMS		(1 << 6)
 
 void local_icache_inv_all(void *priv)
 {
@@ -16,11 +20,6 @@ void local_icache_inv_all(void *priv)
 	sync_is();
 }
 
-void icache_inv_all(void)
-{
-	on_each_cpu(local_icache_inv_all, NULL, 1);
-}
-
 #ifdef CONFIG_CPU_HAS_ICACHE_INS
 void icache_inv_range(unsigned long start, unsigned long end)
 {
@@ -31,9 +30,43 @@ void icache_inv_range(unsigned long start, unsigned long end)
 	sync_is();
 }
 #else
+struct cache_range {
+	unsigned long start;
+	unsigned long end;
+};
+
+static DEFINE_SPINLOCK(cache_lock);
+
+static inline void cache_op_line(unsigned long i, unsigned int val)
+{
+	mtcr("cr22", i);
+	mtcr("cr17", val);
+}
+
+void local_icache_inv_range(void *priv)
+{
+	struct cache_range *param = priv;
+	unsigned long i = param->start & ~(L1_CACHE_BYTES - 1);
+	unsigned long flags;
+
+	spin_lock_irqsave(&cache_lock, flags);
+
+	for (; i < param->end; i += L1_CACHE_BYTES)
+		cache_op_line(i, INS_CACHE | CACHE_INV | CACHE_OMS);
+
+	spin_unlock_irqrestore(&cache_lock, flags);
+
+	sync_is();
+}
+
 void icache_inv_range(unsigned long start, unsigned long end)
 {
-	icache_inv_all();
+	struct cache_range param = { start, end };
+
+	if (irqs_disabled())
+		local_icache_inv_range(&param);
+	else
+		on_each_cpu(local_icache_inv_range, &param, 1);
 }
 #endif
 

commit 9025fd48a8aeddade845afa353d4bbab7f19dbf2
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Sun Feb 2 10:58:38 2020 +0800

    csky: Remove unused cache implementation
    
    Only for coding convention, these codes are unnecessary for abiv2.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/mm/cachev2.c b/arch/csky/mm/cachev2.c
index 909fdd0f5995..bc419f8039d3 100644
--- a/arch/csky/mm/cachev2.c
+++ b/arch/csky/mm/cachev2.c
@@ -52,23 +52,9 @@ void dcache_wb_range(unsigned long start, unsigned long end)
 	sync_is();
 }
 
-void dcache_inv_range(unsigned long start, unsigned long end)
-{
-	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
-
-	for (; i < end; i += L1_CACHE_BYTES)
-		asm volatile("dcache.civa %0\n"::"r"(i):"memory");
-	sync_is();
-}
-
 void cache_wbinv_range(unsigned long start, unsigned long end)
 {
-	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
-
-	for (; i < end; i += L1_CACHE_BYTES)
-		asm volatile("dcache.cval1 %0\n"::"r"(i):"memory");
-	sync_is();
-
+	dcache_wb_range(start, end);
 	icache_inv_range(start, end);
 }
 EXPORT_SYMBOL(cache_wbinv_range);

commit 761b4f694cb90b63ca2739ac8a8a176342636e5e
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Wed Jan 22 11:15:14 2020 +0800

    csky: Support icache flush without specific instructions
    
    Some CPUs don't support icache specific instructions to flush icache
    lines in broadcast way. We use cpu control registers to flush local
    icache and use IPI to notify other cores.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/mm/cachev2.c b/arch/csky/mm/cachev2.c
index b61be6518e21..909fdd0f5995 100644
--- a/arch/csky/mm/cachev2.c
+++ b/arch/csky/mm/cachev2.c
@@ -3,15 +3,25 @@
 
 #include <linux/spinlock.h>
 #include <linux/smp.h>
+#include <linux/mm.h>
 #include <asm/cache.h>
 #include <asm/barrier.h>
 
-inline void dcache_wb_line(unsigned long start)
+#define INS_CACHE		(1 << 0)
+#define CACHE_INV		(1 << 4)
+
+void local_icache_inv_all(void *priv)
 {
-	asm volatile("dcache.cval1 %0\n"::"r"(start):"memory");
+	mtcr("cr17", INS_CACHE|CACHE_INV);
 	sync_is();
 }
 
+void icache_inv_all(void)
+{
+	on_each_cpu(local_icache_inv_all, NULL, 1);
+}
+
+#ifdef CONFIG_CPU_HAS_ICACHE_INS
 void icache_inv_range(unsigned long start, unsigned long end)
 {
 	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
@@ -20,10 +30,16 @@ void icache_inv_range(unsigned long start, unsigned long end)
 		asm volatile("icache.iva %0\n"::"r"(i):"memory");
 	sync_is();
 }
+#else
+void icache_inv_range(unsigned long start, unsigned long end)
+{
+	icache_inv_all();
+}
+#endif
 
-void icache_inv_all(void)
+inline void dcache_wb_line(unsigned long start)
 {
-	asm volatile("icache.ialls\n":::"memory");
+	asm volatile("dcache.cval1 %0\n"::"r"(start):"memory");
 	sync_is();
 }
 
@@ -53,10 +69,7 @@ void cache_wbinv_range(unsigned long start, unsigned long end)
 		asm volatile("dcache.cval1 %0\n"::"r"(i):"memory");
 	sync_is();
 
-	i = start & ~(L1_CACHE_BYTES - 1);
-	for (; i < end; i += L1_CACHE_BYTES)
-		asm volatile("icache.iva %0\n"::"r"(i):"memory");
-	sync_is();
+	icache_inv_range(start, end);
 }
 EXPORT_SYMBOL(cache_wbinv_range);
 

commit ae76f635d4e1cffa6870cc5472567ca9d6940a22
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Tue Jul 30 17:16:28 2019 +0800

    csky: Optimize arch_sync_dma_for_cpu/device with dma_inv_range
    
    DMA_FROM_DEVICE only need to read dma data of memory into CPU cache,
    so there is no need to clear cache before. Also clear + inv for
    DMA_FROM_DEVICE won't cause problem, because the memory range for dma
    won't be touched by software during dma working.
    
    Changes for V2:
     - Remove clr cache and ignore the DMA_TO_DEVICE in _for_cpu.
     - Change inv to wbinv cache with DMA_FROM_DEVICE in _for_device.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/cachev2.c b/arch/csky/mm/cachev2.c
index baaf05d69f44..b61be6518e21 100644
--- a/arch/csky/mm/cachev2.c
+++ b/arch/csky/mm/cachev2.c
@@ -69,11 +69,20 @@ void dma_wbinv_range(unsigned long start, unsigned long end)
 	sync_is();
 }
 
+void dma_inv_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("dcache.iva %0\n"::"r"(i):"memory");
+	sync_is();
+}
+
 void dma_wb_range(unsigned long start, unsigned long end)
 {
 	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
 
 	for (; i < end; i += L1_CACHE_BYTES)
-		asm volatile("dcache.civa %0\n"::"r"(i):"memory");
+		asm volatile("dcache.cva %0\n"::"r"(i):"memory");
 	sync_is();
 }

commit 00a9730e1007c6cc87a7c78af2f24a4105d616ee
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Wed Sep 5 14:25:10 2018 +0800

    csky: Cache and TLB routines
    
    This patch adds cache and tlb sync codes for abiv1 & abiv2.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/cachev2.c b/arch/csky/mm/cachev2.c
new file mode 100644
index 000000000000..baaf05d69f44
--- /dev/null
+++ b/arch/csky/mm/cachev2.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.
+
+#include <linux/spinlock.h>
+#include <linux/smp.h>
+#include <asm/cache.h>
+#include <asm/barrier.h>
+
+inline void dcache_wb_line(unsigned long start)
+{
+	asm volatile("dcache.cval1 %0\n"::"r"(start):"memory");
+	sync_is();
+}
+
+void icache_inv_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("icache.iva %0\n"::"r"(i):"memory");
+	sync_is();
+}
+
+void icache_inv_all(void)
+{
+	asm volatile("icache.ialls\n":::"memory");
+	sync_is();
+}
+
+void dcache_wb_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("dcache.cval1 %0\n"::"r"(i):"memory");
+	sync_is();
+}
+
+void dcache_inv_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("dcache.civa %0\n"::"r"(i):"memory");
+	sync_is();
+}
+
+void cache_wbinv_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("dcache.cval1 %0\n"::"r"(i):"memory");
+	sync_is();
+
+	i = start & ~(L1_CACHE_BYTES - 1);
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("icache.iva %0\n"::"r"(i):"memory");
+	sync_is();
+}
+EXPORT_SYMBOL(cache_wbinv_range);
+
+void dma_wbinv_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("dcache.civa %0\n"::"r"(i):"memory");
+	sync_is();
+}
+
+void dma_wb_range(unsigned long start, unsigned long end)
+{
+	unsigned long i = start & ~(L1_CACHE_BYTES - 1);
+
+	for (; i < end; i += L1_CACHE_BYTES)
+		asm volatile("dcache.civa %0\n"::"r"(i):"memory");
+	sync_is();
+}
