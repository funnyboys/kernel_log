commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 4055d430c0c8..0b9cbf2cf6a9 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -120,7 +120,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	if (in_atomic() || !mm)
 		goto bad_area_nosemaphore;
 
-	down_read(&mm->mmap_sem);
+	mmap_read_lock(mm);
 	vma = find_vma(mm, address);
 	if (!vma)
 		goto bad_area;
@@ -170,7 +170,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 			      address);
 	}
 
-	up_read(&mm->mmap_sem);
+	mmap_read_unlock(mm);
 	return;
 
 	/*
@@ -178,7 +178,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	 * Fix it, but check if it's kernel or user first..
 	 */
 bad_area:
-	up_read(&mm->mmap_sem);
+	mmap_read_unlock(mm);
 
 bad_area_nosemaphore:
 	/* User mode accesses just cause a SIGSEGV */
@@ -217,7 +217,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 do_sigbus:
 	tsk->thread.trap_no = (regs->sr >> 16) & 0xff;
 
-	up_read(&mm->mmap_sem);
+	mmap_read_unlock(mm);
 
 	/* Kernel mode? Handle exceptions or die */
 	if (!user_mode(regs))

commit c910951a85d8fef5a8dabe08df45201f0637690a
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:47 2020 -0700

    csky: replace definitions of __pXd_offset() with pXd_index()
    
    All architectures use pXd_index() to get an entry in the page table page
    corresponding to a virtual address.
    
    Align csky with other architectures.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-5-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 4e6dc68f3258..4055d430c0c8 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -78,7 +78,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 		 * Do _not_ use "tsk" here. We might be inside
 		 * an interrupt in the middle of a task switch..
 		 */
-		int offset = __pgd_offset(address);
+		int offset = pgd_index(address);
 		pgd_t *pgd, *pgd_k;
 		pud_t *pud, *pud_k;
 		pmd_t *pmd, *pmd_k;

commit 5093c5872be3a041917828b2b53e9f981d754768
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon Apr 6 20:03:59 2020 -0700

    mm/vma: append unlikely() while testing VMA access permissions
    
    It is unlikely that an inaccessible VMA without required permission flags
    will get a page fault.  Hence lets just append unlikely() directive to
    such checks in order to improve performance while also standardizing it
    across various platforms.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Link: http://lkml.kernel.org/r/1582525304-32113-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index a6e8230b6fbf..4e6dc68f3258 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -141,7 +141,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 		if (!(vma->vm_flags & VM_WRITE))
 			goto bad_area;
 	} else {
-		if (!vma_is_accessible(vma))
+		if (unlikely(!vma_is_accessible(vma)))
 			goto bad_area;
 	}
 

commit 3122e80efc0faf4a2accba7a46c7ed795edbfded
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon Apr 6 20:03:47 2020 -0700

    mm/vma: make vma_is_accessible() available for general use
    
    Lets move vma_is_accessible() helper to include/linux/mm.h which makes it
    available for general use.  While here, this replaces all remaining open
    encodings for VMA access check with vma_is_accessible().
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Guo Ren <guoren@kernel.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Nick Piggin <npiggin@gmail.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/1582520593-30704-3-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index d3c61b83e195..a6e8230b6fbf 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -141,7 +141,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 		if (!(vma->vm_flags & VM_WRITE))
 			goto bad_area;
 	} else {
-		if (!(vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))
+		if (!vma_is_accessible(vma))
 			goto bad_area;
 	}
 

commit 33e53ae1ce413a081254e686d9b27cc1b3585e2f
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Wed Apr 1 09:17:02 2020 +0800

    csky: Add kprobes supported
    
    This patch enable kprobes, kretprobes, ftrace interface. It utilized
    software breakpoint and single step debug exceptions, instructions
    simulation on csky.
    
    We use USR_BKPT replace origin instruction, and the kprobe handler
    prepares an excutable memory slot for out-of-line execution with a
    copy of the original instruction being probed. Most of instructions
    could be executed by single-step, but some instructions need origin
    pc value to execute and we need software simulate these instructions.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 562c7f708749..d3c61b83e195 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -18,6 +18,7 @@
 #include <linux/extable.h>
 #include <linux/uaccess.h>
 #include <linux/perf_event.h>
+#include <linux/kprobes.h>
 
 #include <asm/hardirq.h>
 #include <asm/mmu_context.h>
@@ -53,6 +54,9 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	int fault;
 	unsigned long address = mmu_meh & PAGE_MASK;
 
+	if (kprobe_page_fault(regs, tsk->thread.trap_no))
+		return;
+
 	si_code = SEGV_MAPERR;
 
 #ifndef CONFIG_CPU_HAS_TLBI

commit 9c0e343d7654a329d1f9b53d253cbf7fb6eff85d
Author: Guo Ren <guoren@linux.alibaba.com>
Date:   Tue Mar 31 23:45:52 2020 +0800

    csky: Fixup get wrong psr value from phyical reg
    
    We should get psr value from regs->psr in stack, not directly get
    it from phyiscal register then save the vector number in
    tsk->trap_no.
    
    Signed-off-by: Guo Ren <guoren@linux.alibaba.com>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index f76618b630f9..562c7f708749 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -179,11 +179,14 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 bad_area_nosemaphore:
 	/* User mode accesses just cause a SIGSEGV */
 	if (user_mode(regs)) {
+		tsk->thread.trap_no = (regs->sr >> 16) & 0xff;
 		force_sig_fault(SIGSEGV, si_code, (void __user *)address);
 		return;
 	}
 
 no_context:
+	tsk->thread.trap_no = (regs->sr >> 16) & 0xff;
+
 	/* Are we prepared to handle this kernel fault? */
 	if (fixup_exception(regs))
 		return;
@@ -198,6 +201,8 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	die_if_kernel("Oops", regs, write);
 
 out_of_memory:
+	tsk->thread.trap_no = (regs->sr >> 16) & 0xff;
+
 	/*
 	 * We ran out of memory, call the OOM killer, and return the userspace
 	 * (which will retry the fault, or kill us if we got oom-killed).
@@ -206,6 +211,8 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	return;
 
 do_sigbus:
+	tsk->thread.trap_no = (regs->sr >> 16) & 0xff;
+
 	up_read(&mm->mmap_sem);
 
 	/* Kernel mode? Handle exceptions or die */

commit 2e1661d2673667d886cd40ad9f414cb6db48d8da
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 11:04:24 2019 -0500

    signal: Remove the task parameter from force_sig_fault
    
    As synchronous exceptions really only make sense against the current
    task (otherwise how are you synchronous) remove the task parameter
    from from force_sig_fault to make it explicit that is what is going
    on.
    
    The two known exceptions that deliver a synchronous exception to a
    stopped ptraced task have already been changed to
    force_sig_fault_to_task.
    
    The callers have been changed with the following emacs regular expression
    (with obvious variations on the architectures that take more arguments)
    to avoid typos:
    
    force_sig_fault[(]\([^,]+\)[,]\([^,]+\)[,]\([^,]+\)[,]\W+current[)]
    ->
    force_sig_fault(\1,\2,\3)
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 18041f46ded1..f76618b630f9 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -179,7 +179,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 bad_area_nosemaphore:
 	/* User mode accesses just cause a SIGSEGV */
 	if (user_mode(regs)) {
-		force_sig_fault(SIGSEGV, si_code, (void __user *)address, current);
+		force_sig_fault(SIGSEGV, si_code, (void __user *)address);
 		return;
 	}
 
@@ -212,5 +212,5 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	if (!user_mode(regs))
 		goto no_context;
 
-	force_sig_fault(SIGBUS, BUS_ADRERR, (void __user *)address, current);
+	force_sig_fault(SIGBUS, BUS_ADRERR, (void __user *)address);
 }

commit 1a23710c71bbfe2df10584afb9971b99c45e2576
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Mon Apr 22 14:21:09 2019 +0800

    csky: Fixup compile warning
    
    The function of __va() will return "void *", but the pgd_base is
    unsigned long.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index aeb9a5f11e00..18041f46ded1 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -82,7 +82,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 
 		unsigned long pgd_base;
 
-		pgd_base = __va(get_pgd());
+		pgd_base = (unsigned long)__va(get_pgd());
 		pgd = (pgd_t *)pgd_base + offset;
 		pgd_k = init_mm.pgd + offset;
 

commit 0eaf50deec8d550164b3cf6a5d68ec1072916f0e
Author: Mao Han <han_mao@c-sky.com>
Date:   Thu Apr 18 14:20:40 2019 +0800

    csky: add page fault perf event support
    
    This patch add support for page fault count, major fault count
    and minorfault count. Without this patch page faults are not
    sampled for perf event.
    
    Performance counter stats for '/usr/lib/perf-test/callchain_test':
            0      page-faults               #    0.000 K/sec
    
    Signed-off-by: Mao Han <han_mao@c-sky.com>
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 5beb25ca1c79..aeb9a5f11e00 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -17,6 +17,7 @@
 #include <linux/vt_kern.h>
 #include <linux/extable.h>
 #include <linux/uaccess.h>
+#include <linux/perf_event.h>
 
 #include <asm/hardirq.h>
 #include <asm/mmu_context.h>
@@ -106,6 +107,8 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 		return;
 	}
 #endif
+
+	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 	/*
 	 * If we're in an interrupt or have no user
 	 * context, we must not take the fault..
@@ -153,10 +156,15 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 			goto bad_area;
 		BUG();
 	}
-	if (fault & VM_FAULT_MAJOR)
+	if (fault & VM_FAULT_MAJOR) {
 		tsk->maj_flt++;
-	else
+		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs,
+			      address);
+	} else {
 		tsk->min_flt++;
+		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs,
+			      address);
+	}
 
 	up_read(&mm->mmap_sem);
 	return;

commit 683fafebf93bcde9948246849348b888e185cb22
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Fri Apr 19 17:10:52 2019 +0800

    csky: Use va_pa_offset instead of phys_offset
    
    The name of phys_offset is so common for global export and it may
    conflict with some local name. So change phys_offset to va_pa_offset
    which also used by riscv.
    
    Also use __pa() and __va() instead of using phys_offset directly.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Cc: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index e1725f8a06f9..5beb25ca1c79 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -81,7 +81,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 
 		unsigned long pgd_base;
 
-		pgd_base = tlb_get_pgd();
+		pgd_base = __va(get_pgd());
 		pgd = (pgd_t *)pgd_base + offset;
 		pgd_k = init_mm.pgd + offset;
 

commit ce63cd5bd4482ded3c5907a48928627de623b185
Author: Jagadeesh Pagadala <jagdsh.linux@gmail.com>
Date:   Sat Mar 23 17:25:17 2019 +0530

    csky: mm/fault.c: Remove duplicate header
    
    Remove duplicate header which is included twice.
    
    Signed-off-by: Jagadeesh Pagadala <jagdsh.linux@gmail.com>
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index d6f4b66b93e2..e1725f8a06f9 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -15,7 +15,6 @@
 #include <linux/smp.h>
 #include <linux/version.h>
 #include <linux/vt_kern.h>
-#include <linux/kernel.h>
 #include <linux/extable.h>
 #include <linux/uaccess.h>
 

commit 17a68777bc883c8044c8b2d40aa112ff4e8a4fb1
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Sun Dec 9 17:31:53 2018 +0800

    csky: remove unused members in processor.h
    
    Cleanup struct cpuinfo_csky and struct thread_struct, remove all esp0
    related code. We could get pt_regs from sp and backtrace could use fp
    in switch_stack.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 6a946614cc47..d6f4b66b93e2 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -172,8 +172,6 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 bad_area_nosemaphore:
 	/* User mode accesses just cause a SIGSEGV */
 	if (user_mode(regs)) {
-		tsk->thread.address = address;
-		tsk->thread.error_code = write;
 		force_sig_fault(SIGSEGV, si_code, (void __user *)address, current);
 		return;
 	}
@@ -207,6 +205,5 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	if (!user_mode(regs))
 		goto no_context;
 
-	tsk->thread.address = address;
 	force_sig_fault(SIGBUS, BUS_ADRERR, (void __user *)address, current);
 }

commit 8f4f1639a19db231e4579612dbc6d147e8d07ffd
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Sun Dec 9 17:07:20 2018 +0800

    csky: optimize kernel panic print.
    
    Use STACKTRACE to optimize panic print more pretty and align registers
    printing.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index 7df57f90b52c..6a946614cc47 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -188,8 +188,8 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
 	 * terminate things with extreme prejudice.
 	 */
 	bust_spinlocks(1);
-	pr_alert("Unable to %s at vaddr: %08lx, epc: %08lx\n",
-		 __func__, address, regs->pc);
+	pr_alert("Unable to handle kernel paging request at virtual "
+		 "address 0x%08lx, pc: 0x%08lx\n", address, regs->pc);
 	die_if_kernel("Oops", regs, write);
 
 out_of_memory:

commit 081860b970adb69888734df0c8aa849b00ea7254
Author: Guo Ren <ren_guo@c-sky.com>
Date:   Wed Sep 5 14:25:08 2018 +0800

    csky: Exception handling and mm-fault
    
    This patch adds exception handling code, cpuinfo and mm-fault code.
    
    Signed-off-by: Guo Ren <ren_guo@c-sky.com>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
new file mode 100644
index 000000000000..7df57f90b52c
--- /dev/null
+++ b/arch/csky/mm/fault.c
@@ -0,0 +1,212 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2018 Hangzhou C-SKY Microsystems co.,ltd.
+
+#include <linux/signal.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/version.h>
+#include <linux/vt_kern.h>
+#include <linux/kernel.h>
+#include <linux/extable.h>
+#include <linux/uaccess.h>
+
+#include <asm/hardirq.h>
+#include <asm/mmu_context.h>
+#include <asm/traps.h>
+#include <asm/page.h>
+
+int fixup_exception(struct pt_regs *regs)
+{
+	const struct exception_table_entry *fixup;
+
+	fixup = search_exception_tables(instruction_pointer(regs));
+	if (fixup) {
+		regs->pc = fixup->nextinsn;
+
+		return 1;
+	}
+
+	return 0;
+}
+
+/*
+ * This routine handles page faults. It determines the address,
+ * and the problem, and then passes it off to one of the appropriate
+ * routines.
+ */
+asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,
+			      unsigned long mmu_meh)
+{
+	struct vm_area_struct *vma = NULL;
+	struct task_struct *tsk = current;
+	struct mm_struct *mm = tsk->mm;
+	int si_code;
+	int fault;
+	unsigned long address = mmu_meh & PAGE_MASK;
+
+	si_code = SEGV_MAPERR;
+
+#ifndef CONFIG_CPU_HAS_TLBI
+	/*
+	 * We fault-in kernel-space virtual memory on-demand. The
+	 * 'reference' page table is init_mm.pgd.
+	 *
+	 * NOTE! We MUST NOT take any locks for this case. We may
+	 * be in an interrupt or a critical region, and should
+	 * only copy the information from the master page table,
+	 * nothing more.
+	 */
+	if (unlikely(address >= VMALLOC_START) &&
+	    unlikely(address <= VMALLOC_END)) {
+		/*
+		 * Synchronize this task's top level page-table
+		 * with the 'reference' page table.
+		 *
+		 * Do _not_ use "tsk" here. We might be inside
+		 * an interrupt in the middle of a task switch..
+		 */
+		int offset = __pgd_offset(address);
+		pgd_t *pgd, *pgd_k;
+		pud_t *pud, *pud_k;
+		pmd_t *pmd, *pmd_k;
+		pte_t *pte_k;
+
+		unsigned long pgd_base;
+
+		pgd_base = tlb_get_pgd();
+		pgd = (pgd_t *)pgd_base + offset;
+		pgd_k = init_mm.pgd + offset;
+
+		if (!pgd_present(*pgd_k))
+			goto no_context;
+		set_pgd(pgd, *pgd_k);
+
+		pud = (pud_t *)pgd;
+		pud_k = (pud_t *)pgd_k;
+		if (!pud_present(*pud_k))
+			goto no_context;
+
+		pmd = pmd_offset(pud, address);
+		pmd_k = pmd_offset(pud_k, address);
+		if (!pmd_present(*pmd_k))
+			goto no_context;
+		set_pmd(pmd, *pmd_k);
+
+		pte_k = pte_offset_kernel(pmd_k, address);
+		if (!pte_present(*pte_k))
+			goto no_context;
+		return;
+	}
+#endif
+	/*
+	 * If we're in an interrupt or have no user
+	 * context, we must not take the fault..
+	 */
+	if (in_atomic() || !mm)
+		goto bad_area_nosemaphore;
+
+	down_read(&mm->mmap_sem);
+	vma = find_vma(mm, address);
+	if (!vma)
+		goto bad_area;
+	if (vma->vm_start <= address)
+		goto good_area;
+	if (!(vma->vm_flags & VM_GROWSDOWN))
+		goto bad_area;
+	if (expand_stack(vma, address))
+		goto bad_area;
+	/*
+	 * Ok, we have a good vm_area for this memory access, so
+	 * we can handle it..
+	 */
+good_area:
+	si_code = SEGV_ACCERR;
+
+	if (write) {
+		if (!(vma->vm_flags & VM_WRITE))
+			goto bad_area;
+	} else {
+		if (!(vma->vm_flags & (VM_READ | VM_WRITE | VM_EXEC)))
+			goto bad_area;
+	}
+
+	/*
+	 * If for any reason at all we couldn't handle the fault,
+	 * make sure we exit gracefully rather than endlessly redo
+	 * the fault.
+	 */
+	fault = handle_mm_fault(vma, address, write ? FAULT_FLAG_WRITE : 0);
+	if (unlikely(fault & VM_FAULT_ERROR)) {
+		if (fault & VM_FAULT_OOM)
+			goto out_of_memory;
+		else if (fault & VM_FAULT_SIGBUS)
+			goto do_sigbus;
+		else if (fault & VM_FAULT_SIGSEGV)
+			goto bad_area;
+		BUG();
+	}
+	if (fault & VM_FAULT_MAJOR)
+		tsk->maj_flt++;
+	else
+		tsk->min_flt++;
+
+	up_read(&mm->mmap_sem);
+	return;
+
+	/*
+	 * Something tried to access memory that isn't in our memory map..
+	 * Fix it, but check if it's kernel or user first..
+	 */
+bad_area:
+	up_read(&mm->mmap_sem);
+
+bad_area_nosemaphore:
+	/* User mode accesses just cause a SIGSEGV */
+	if (user_mode(regs)) {
+		tsk->thread.address = address;
+		tsk->thread.error_code = write;
+		force_sig_fault(SIGSEGV, si_code, (void __user *)address, current);
+		return;
+	}
+
+no_context:
+	/* Are we prepared to handle this kernel fault? */
+	if (fixup_exception(regs))
+		return;
+
+	/*
+	 * Oops. The kernel tried to access some bad page. We'll have to
+	 * terminate things with extreme prejudice.
+	 */
+	bust_spinlocks(1);
+	pr_alert("Unable to %s at vaddr: %08lx, epc: %08lx\n",
+		 __func__, address, regs->pc);
+	die_if_kernel("Oops", regs, write);
+
+out_of_memory:
+	/*
+	 * We ran out of memory, call the OOM killer, and return the userspace
+	 * (which will retry the fault, or kill us if we got oom-killed).
+	 */
+	pagefault_out_of_memory();
+	return;
+
+do_sigbus:
+	up_read(&mm->mmap_sem);
+
+	/* Kernel mode? Handle exceptions or die */
+	if (!user_mode(regs))
+		goto no_context;
+
+	tsk->thread.address = address;
+	force_sig_fault(SIGBUS, BUS_ADRERR, (void __user *)address, current);
+}
