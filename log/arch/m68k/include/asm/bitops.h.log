commit 3fc2579e6f162fcff964f5aa01c8a29438ca5c05
Author: Matthew Wilcox <willy@infradead.org>
Date:   Thu Jan 3 15:26:41 2019 -0800

    fls: change parameter to unsigned int
    
    When testing in userspace, UBSAN pointed out that shifting into the sign
    bit is undefined behaviour.  It doesn't really make sense to ask for the
    highest set bit of a negative value, so just turn the argument type into
    an unsigned int.
    
    Some architectures (eg ppc) already had it declared as an unsigned int,
    so I don't expect too many problems.
    
    Link: http://lkml.kernel.org/r/20181105221117.31828-1-willy@infradead.org
    Signed-off-by: Matthew Wilcox <willy@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index d979f38af751..10133a968c8e 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -502,7 +502,7 @@ static inline unsigned long __ffs(unsigned long x)
 /*
  *	fls: find last bit set.
  */
-static inline int fls(int x)
+static inline int fls(unsigned int x)
 {
 	int cnt;
 

commit de5d1b39ea0b38a9f4dfb08966042b7b91e2df30
Merge: 1c594774283a fd2efaa4eb53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 12:23:39 2018 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking/atomics update from Thomas Gleixner:
     "The locking, atomics and memory model brains delivered:
    
       - A larger update to the atomics code which reworks the ordering
         barriers, consolidates the atomic primitives, provides the new
         atomic64_fetch_add_unless() primitive and cleans up the include
         hell.
    
       - Simplify cmpxchg() instrumentation and add instrumentation for
         xchg() and cmpxchg_double().
    
       - Updates to the memory model and documentation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (48 commits)
      locking/atomics: Rework ordering barriers
      locking/atomics: Instrument cmpxchg_double*()
      locking/atomics: Instrument xchg()
      locking/atomics: Simplify cmpxchg() instrumentation
      locking/atomics/x86: Reduce arch_cmpxchg64*() instrumentation
      tools/memory-model: Rename litmus tests to comply to norm7
      tools/memory-model/Documentation: Fix typo, smb->smp
      sched/Documentation: Update wake_up() & co. memory-barrier guarantees
      locking/spinlock, sched/core: Clarify requirements for smp_mb__after_spinlock()
      sched/core: Use smp_mb() in wake_woken_function()
      tools/memory-model: Add informal LKMM documentation to MAINTAINERS
      locking/atomics/Documentation: Describe atomic_set() as a write operation
      tools/memory-model: Make scripts executable
      tools/memory-model: Remove ACCESS_ONCE() from model
      tools/memory-model: Remove ACCESS_ONCE() from recipes
      locking/memory-barriers.txt/kokr: Update Korean translation to fix broken DMA vs. MMIO ordering example
      MAINTAINERS: Add Daniel Lustig as an LKMM reviewer
      tools/memory-model: Fix ISA2+pooncelock+pooncelock+pombonce name
      tools/memory-model: Add litmus test for full multicopy atomicity
      locking/refcount: Always allow checked forms
      ...

commit 384052e4ed274747fb96deb2df46a14d023a2986
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Wed Jul 4 09:28:14 2018 +0300

    m68k/bitops: convert __ffs to match generic declaration
    
    The generic bitops declare __ffs as
    
            static inline unsigned long __ffs(unsigned long word);
    
    Convert the m68k version to match the generic declaration.
    
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Greg Ungerer <gerg@linux-m68k.org>
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index 93b47b1f6fb4..54009ea710b3 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -454,7 +454,7 @@ static inline unsigned long ffz(unsigned long word)
  */
 #if (defined(__mcfisaaplus__) || defined(__mcfisac__)) && \
 	!defined(CONFIG_M68000) && !defined(CONFIG_MCPU32)
-static inline int __ffs(int x)
+static inline unsigned long __ffs(unsigned long x)
 {
 	__asm__ __volatile__ ("bitrev %0; ff1 %0"
 		: "=d" (x)
@@ -493,7 +493,11 @@ static inline int ffs(int x)
 		: "dm" (x & -x));
 	return 32 - cnt;
 }
-#define __ffs(x) (ffs(x) - 1)
+
+static inline unsigned long __ffs(unsigned long x)
+{
+	return ffs(x) - 1;
+}
 
 /*
  *	fls: find last bit set.

commit 84038fd98e8e03ec418e3244b98eb248c5349d7e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jun 19 13:53:07 2018 +0100

    locking/atomics/m68k: Don't use <asm-generic/bitops/lock.h>
    
    <asm-generic/bitops/lock.h> is shortly going to be built on top of the
    atomic_long_*() API, which introduces a nasty circular dependency for
    m68k where <linux/atomic.h> pulls in <linux/bitops.h> via:
    
            linux/atomic.h
            asm/atomic.h
            linux/irqflags.h
            asm/irqflags.h
            linux/preempt.h
            asm/preempt.h
            asm-generic/preempt.h
            linux/thread_info.h
            asm/thread_info.h
            asm/page.h
            asm-generic/getorder.h
            linux/log2.h
            linux/bitops.h
    
    Since m68k isn't SMP and doesn't support ACQUIRE/RELEASE barriers, we
    can just define the lock bitops in terms of the atomic bitops in the
    <asm/bitops.h> header.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: yamada.masahiro@socionext.com
    Link: https://lore.kernel.org/lkml/1529412794-17720-3-git-send-email-will.deacon@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index 93b47b1f6fb4..18193419f97d 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -515,12 +515,16 @@ static inline int __fls(int x)
 
 #endif
 
+/* Simple test-and-set bit locks */
+#define test_and_set_bit_lock	test_and_set_bit
+#define clear_bit_unlock	clear_bit
+#define __clear_bit_unlock	clear_bit_unlock
+
 #include <asm-generic/bitops/ext2-atomic.h>
 #include <asm-generic/bitops/le.h>
 #include <asm-generic/bitops/fls64.h>
 #include <asm-generic/bitops/sched.h>
 #include <asm-generic/bitops/hweight.h>
-#include <asm-generic/bitops/lock.h>
 #endif /* __KERNEL__ */
 
 #endif /* _M68K_BITOPS_H */

commit 0ade34c37012ea5c516d9aa4d19a56e9f40a55ed
Author: Clement Courbet <courbet@google.com>
Date:   Tue Feb 6 15:38:34 2018 -0800

    lib: optimize cpumask_next_and()
    
    We've measured that we spend ~0.6% of sys cpu time in cpumask_next_and().
    It's essentially a joined iteration in search for a non-zero bit, which is
    currently implemented as a lookup join (find a nonzero bit on the lhs,
    lookup the rhs to see if it's set there).
    
    Implement a direct join (find a nonzero bit on the incrementally built
    join).  Also add generic bitmap benchmarks in the new `test_find_bit`
    module for new function (see `find_next_and_bit` in [2] and [3] below).
    
    For cpumask_next_and, direct benchmarking shows that it's 1.17x to 14x
    faster with a geometric mean of 2.1 on 32 CPUs [1].  No impact on memory
    usage.  Note that on Arm, the new pure-C implementation still outperforms
    the old one that uses a mix of C and asm (`find_next_bit`) [3].
    
    [1] Approximate benchmark code:
    
    ```
      unsigned long src1p[nr_cpumask_longs] = {pattern1};
      unsigned long src2p[nr_cpumask_longs] = {pattern2};
      for (/*a bunch of repetitions*/) {
        for (int n = -1; n <= nr_cpu_ids; ++n) {
          asm volatile("" : "+rm"(src1p)); // prevent any optimization
          asm volatile("" : "+rm"(src2p));
          unsigned long result = cpumask_next_and(n, src1p, src2p);
          asm volatile("" : "+rm"(result));
        }
      }
    ```
    
    Results:
    pattern1    pattern2     time_before/time_after
    0x0000ffff  0x0000ffff   1.65
    0x0000ffff  0x00005555   2.24
    0x0000ffff  0x00001111   2.94
    0x0000ffff  0x00000000   14.0
    0x00005555  0x0000ffff   1.67
    0x00005555  0x00005555   1.71
    0x00005555  0x00001111   1.90
    0x00005555  0x00000000   6.58
    0x00001111  0x0000ffff   1.46
    0x00001111  0x00005555   1.49
    0x00001111  0x00001111   1.45
    0x00001111  0x00000000   3.10
    0x00000000  0x0000ffff   1.18
    0x00000000  0x00005555   1.18
    0x00000000  0x00001111   1.17
    0x00000000  0x00000000   1.25
    -----------------------------
                   geo.mean  2.06
    
    [2] test_find_next_bit, X86 (skylake)
    
     [ 3913.477422] Start testing find_bit() with random-filled bitmap
     [ 3913.477847] find_next_bit: 160868 cycles, 16484 iterations
     [ 3913.477933] find_next_zero_bit: 169542 cycles, 16285 iterations
     [ 3913.478036] find_last_bit: 201638 cycles, 16483 iterations
     [ 3913.480214] find_first_bit: 4353244 cycles, 16484 iterations
     [ 3913.480216] Start testing find_next_and_bit() with random-filled
     bitmap
     [ 3913.481074] find_next_and_bit: 89604 cycles, 8216 iterations
     [ 3913.481075] Start testing find_bit() with sparse bitmap
     [ 3913.481078] find_next_bit: 2536 cycles, 66 iterations
     [ 3913.481252] find_next_zero_bit: 344404 cycles, 32703 iterations
     [ 3913.481255] find_last_bit: 2006 cycles, 66 iterations
     [ 3913.481265] find_first_bit: 17488 cycles, 66 iterations
     [ 3913.481266] Start testing find_next_and_bit() with sparse bitmap
     [ 3913.481272] find_next_and_bit: 764 cycles, 1 iterations
    
    [3] test_find_next_bit, arm (v7 odroid XU3).
    
    [  267.206928] Start testing find_bit() with random-filled bitmap
    [  267.214752] find_next_bit: 4474 cycles, 16419 iterations
    [  267.221850] find_next_zero_bit: 5976 cycles, 16350 iterations
    [  267.229294] find_last_bit: 4209 cycles, 16419 iterations
    [  267.279131] find_first_bit: 1032991 cycles, 16420 iterations
    [  267.286265] Start testing find_next_and_bit() with random-filled
    bitmap
    [  267.302386] find_next_and_bit: 2290 cycles, 8140 iterations
    [  267.309422] Start testing find_bit() with sparse bitmap
    [  267.316054] find_next_bit: 191 cycles, 66 iterations
    [  267.322726] find_next_zero_bit: 8758 cycles, 32703 iterations
    [  267.329803] find_last_bit: 84 cycles, 66 iterations
    [  267.336169] find_first_bit: 4118 cycles, 66 iterations
    [  267.342627] Start testing find_next_and_bit() with sparse bitmap
    [  267.356919] find_next_and_bit: 91 cycles, 1 iterations
    
    [courbet@google.com: v6]
      Link: http://lkml.kernel.org/r/20171129095715.23430-1-courbet@google.com
    [geert@linux-m68k.org: m68k/bitops: always include <asm-generic/bitops/find.h>]
      Link: http://lkml.kernel.org/r/1512556816-28627-1-git-send-email-geert@linux-m68k.org
    Link: http://lkml.kernel.org/r/20171128131334.23491-1-courbet@google.com
    Signed-off-by: Clement Courbet <courbet@google.com>
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Yury Norov <ynorov@caviumnetworks.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index dda58cfe8c22..93b47b1f6fb4 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -311,7 +311,6 @@ static inline int bfchg_mem_test_and_change_bit(int nr,
  *	functions.
  */
 #if defined(CONFIG_CPU_HAS_NO_BITFIELDS)
-#include <asm-generic/bitops/find.h>
 #include <asm-generic/bitops/ffz.h>
 #else
 
@@ -441,6 +440,8 @@ static inline unsigned long ffz(unsigned long word)
 
 #endif
 
+#include <asm-generic/bitops/find.h>
+
 #ifdef __KERNEL__
 
 #if defined(CONFIG_CPU_HAS_NO_BITFIELDS)

commit 066def56dc712329561abadcea15be9cad7393b6
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Mon Jan 2 13:51:43 2017 +0100

    m68k/bitops: Correct signature of test_bit()
    
    mm/filemap.c: In function ‘clear_bit_unlock_is_negative_byte’:
    mm/filemap.c:933: warning: passing argument 2 of ‘test_bit’ discards qualifiers from pointer target type
    
    Make the bitmask pointed to by the "vaddr" parameter volatile to fix
    this, like is done on other architectures.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index b4a9b0d5928d..dda58cfe8c22 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -148,7 +148,7 @@ static inline void bfchg_mem_change_bit(int nr, volatile unsigned long *vaddr)
 #define __change_bit(nr, vaddr)	change_bit(nr, vaddr)
 
 
-static inline int test_bit(int nr, const unsigned long *vaddr)
+static inline int test_bit(int nr, const volatile unsigned long *vaddr)
 {
 	return (vaddr[nr >> 5] & (1UL << (nr & 31))) != 0;
 }

commit 2db56e8606016e33903c64feaed989ffecd66a1b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Mar 13 19:00:36 2014 +0100

    arch,m68k: Convert smp_mb__*()
    
    m68k uses asm-generic/barrier.h and its smp_mb() is barrier(),
    therefore we can use the generic versions that use smp_mb().
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-s5dvosrb7qhvpmtaffwfn0zg@git.kernel.org
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-m68k@lists.linux-m68k.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index c6baa913592a..b4a9b0d5928d 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -13,6 +13,7 @@
 #endif
 
 #include <linux/compiler.h>
+#include <asm/barrier.h>
 
 /*
  *	Bit access functions vary across the ColdFire and 68k families.
@@ -67,12 +68,6 @@ static inline void bfset_mem_set_bit(int nr, volatile unsigned long *vaddr)
 #define __set_bit(nr, vaddr)	set_bit(nr, vaddr)
 
 
-/*
- * clear_bit() doesn't provide any barrier for the compiler.
- */
-#define smp_mb__before_clear_bit()	barrier()
-#define smp_mb__after_clear_bit()	barrier()
-
 static inline void bclr_reg_clear_bit(int nr, volatile unsigned long *vaddr)
 {
 	char *p = (char *)vaddr + (nr ^ 31) / 8;

commit 171d809df1896c1022f9778cd2788be6c255a7dc
Author: Greg Ungerer <gerg@uclinux.org>
Date:   Tue May 17 16:45:00 2011 +1000

    m68k: merge mmu and non-mmu bitops.h
    
    The following patch merges the mmu and non-mmu versions of the m68k
    bitops.h files. Now there is a good deal of difference between the two
    files, but none of it is actually an mmu specific difference. It is
    all about the specific m68k/coldfire varient we are targeting. So it
    makes an awful lot of sense to merge these into a single bitops.h.
    
    There is a number of ways I can see to factor this code. The approach
    I have taken here is to keep the various versions of each macro/function
    type together. This means that there is some ifdefery with each to handle
    each CPU type.
    
    I have added some comments in a couple of appropriate places to try
    and make it clear what the differences we are dealing with are.
    Specifically the instruction and addressing mode differences we have
    to deal with.
    
    The merged form keeps the same underlying optimizations for each CPU
    type for all the general bit clear/set/change and find bit operations.
    It does switch to using the generic le operations though, instead of
    any local varients.
    
    Build tested on ColdFire, 68328, 68360 (which is cpu32) and 68020+.
    Run tested on ColdFire and ARAnyM.
    
    Signed-off-by: Greg Ungerer <gerg@uclinux.org>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
index ce163abddaba..c6baa913592a 100644
--- a/arch/m68k/include/asm/bitops.h
+++ b/arch/m68k/include/asm/bitops.h
@@ -1,5 +1,530 @@
-#ifdef __uClinux__
-#include "bitops_no.h"
+#ifndef _M68K_BITOPS_H
+#define _M68K_BITOPS_H
+/*
+ * Copyright 1992, Linus Torvalds.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file COPYING in the main directory of this archive
+ * for more details.
+ */
+
+#ifndef _LINUX_BITOPS_H
+#error only <linux/bitops.h> can be included directly
+#endif
+
+#include <linux/compiler.h>
+
+/*
+ *	Bit access functions vary across the ColdFire and 68k families.
+ *	So we will break them out here, and then macro in the ones we want.
+ *
+ *	ColdFire - supports standard bset/bclr/bchg with register operand only
+ *	68000    - supports standard bset/bclr/bchg with memory operand
+ *	>= 68020 - also supports the bfset/bfclr/bfchg instructions
+ *
+ *	Although it is possible to use only the bset/bclr/bchg with register
+ *	operands on all platforms you end up with larger generated code.
+ *	So we use the best form possible on a given platform.
+ */
+
+static inline void bset_reg_set_bit(int nr, volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+
+	__asm__ __volatile__ ("bset %1,(%0)"
+		:
+		: "a" (p), "di" (nr & 7)
+		: "memory");
+}
+
+static inline void bset_mem_set_bit(int nr, volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+
+	__asm__ __volatile__ ("bset %1,%0"
+		: "+m" (*p)
+		: "di" (nr & 7));
+}
+
+static inline void bfset_mem_set_bit(int nr, volatile unsigned long *vaddr)
+{
+	__asm__ __volatile__ ("bfset %1{%0:#1}"
+		:
+		: "d" (nr ^ 31), "o" (*vaddr)
+		: "memory");
+}
+
+#if defined(CONFIG_COLDFIRE)
+#define	set_bit(nr, vaddr)	bset_reg_set_bit(nr, vaddr)
+#elif defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#define	set_bit(nr, vaddr)	bset_mem_set_bit(nr, vaddr)
+#else
+#define set_bit(nr, vaddr)	(__builtin_constant_p(nr) ? \
+				bset_mem_set_bit(nr, vaddr) : \
+				bfset_mem_set_bit(nr, vaddr))
+#endif
+
+#define __set_bit(nr, vaddr)	set_bit(nr, vaddr)
+
+
+/*
+ * clear_bit() doesn't provide any barrier for the compiler.
+ */
+#define smp_mb__before_clear_bit()	barrier()
+#define smp_mb__after_clear_bit()	barrier()
+
+static inline void bclr_reg_clear_bit(int nr, volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+
+	__asm__ __volatile__ ("bclr %1,(%0)"
+		:
+		: "a" (p), "di" (nr & 7)
+		: "memory");
+}
+
+static inline void bclr_mem_clear_bit(int nr, volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+
+	__asm__ __volatile__ ("bclr %1,%0"
+		: "+m" (*p)
+		: "di" (nr & 7));
+}
+
+static inline void bfclr_mem_clear_bit(int nr, volatile unsigned long *vaddr)
+{
+	__asm__ __volatile__ ("bfclr %1{%0:#1}"
+		:
+		: "d" (nr ^ 31), "o" (*vaddr)
+		: "memory");
+}
+
+#if defined(CONFIG_COLDFIRE)
+#define	clear_bit(nr, vaddr)	bclr_reg_clear_bit(nr, vaddr)
+#elif defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#define	clear_bit(nr, vaddr)	bclr_mem_clear_bit(nr, vaddr)
+#else
+#define clear_bit(nr, vaddr)	(__builtin_constant_p(nr) ? \
+				bclr_mem_clear_bit(nr, vaddr) : \
+				bfclr_mem_clear_bit(nr, vaddr))
+#endif
+
+#define __clear_bit(nr, vaddr)	clear_bit(nr, vaddr)
+
+
+static inline void bchg_reg_change_bit(int nr, volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+
+	__asm__ __volatile__ ("bchg %1,(%0)"
+		:
+		: "a" (p), "di" (nr & 7)
+		: "memory");
+}
+
+static inline void bchg_mem_change_bit(int nr, volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+
+	__asm__ __volatile__ ("bchg %1,%0"
+		: "+m" (*p)
+		: "di" (nr & 7));
+}
+
+static inline void bfchg_mem_change_bit(int nr, volatile unsigned long *vaddr)
+{
+	__asm__ __volatile__ ("bfchg %1{%0:#1}"
+		:
+		: "d" (nr ^ 31), "o" (*vaddr)
+		: "memory");
+}
+
+#if defined(CONFIG_COLDFIRE)
+#define	change_bit(nr, vaddr)	bchg_reg_change_bit(nr, vaddr)
+#elif defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#define	change_bit(nr, vaddr)	bchg_mem_change_bit(nr, vaddr)
+#else
+#define change_bit(nr, vaddr)	(__builtin_constant_p(nr) ? \
+				bchg_mem_change_bit(nr, vaddr) : \
+				bfchg_mem_change_bit(nr, vaddr))
+#endif
+
+#define __change_bit(nr, vaddr)	change_bit(nr, vaddr)
+
+
+static inline int test_bit(int nr, const unsigned long *vaddr)
+{
+	return (vaddr[nr >> 5] & (1UL << (nr & 31))) != 0;
+}
+
+
+static inline int bset_reg_test_and_set_bit(int nr,
+					    volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+	char retval;
+
+	__asm__ __volatile__ ("bset %2,(%1); sne %0"
+		: "=d" (retval)
+		: "a" (p), "di" (nr & 7)
+		: "memory");
+	return retval;
+}
+
+static inline int bset_mem_test_and_set_bit(int nr,
+					    volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+	char retval;
+
+	__asm__ __volatile__ ("bset %2,%1; sne %0"
+		: "=d" (retval), "+m" (*p)
+		: "di" (nr & 7));
+	return retval;
+}
+
+static inline int bfset_mem_test_and_set_bit(int nr,
+					     volatile unsigned long *vaddr)
+{
+	char retval;
+
+	__asm__ __volatile__ ("bfset %2{%1:#1}; sne %0"
+		: "=d" (retval)
+		: "d" (nr ^ 31), "o" (*vaddr)
+		: "memory");
+	return retval;
+}
+
+#if defined(CONFIG_COLDFIRE)
+#define	test_and_set_bit(nr, vaddr)	bset_reg_test_and_set_bit(nr, vaddr)
+#elif defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#define	test_and_set_bit(nr, vaddr)	bset_mem_test_and_set_bit(nr, vaddr)
+#else
+#define test_and_set_bit(nr, vaddr)	(__builtin_constant_p(nr) ? \
+					bset_mem_test_and_set_bit(nr, vaddr) : \
+					bfset_mem_test_and_set_bit(nr, vaddr))
+#endif
+
+#define __test_and_set_bit(nr, vaddr)	test_and_set_bit(nr, vaddr)
+
+
+static inline int bclr_reg_test_and_clear_bit(int nr,
+					      volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+	char retval;
+
+	__asm__ __volatile__ ("bclr %2,(%1); sne %0"
+		: "=d" (retval)
+		: "a" (p), "di" (nr & 7)
+		: "memory");
+	return retval;
+}
+
+static inline int bclr_mem_test_and_clear_bit(int nr,
+					      volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+	char retval;
+
+	__asm__ __volatile__ ("bclr %2,%1; sne %0"
+		: "=d" (retval), "+m" (*p)
+		: "di" (nr & 7));
+	return retval;
+}
+
+static inline int bfclr_mem_test_and_clear_bit(int nr,
+					       volatile unsigned long *vaddr)
+{
+	char retval;
+
+	__asm__ __volatile__ ("bfclr %2{%1:#1}; sne %0"
+		: "=d" (retval)
+		: "d" (nr ^ 31), "o" (*vaddr)
+		: "memory");
+	return retval;
+}
+
+#if defined(CONFIG_COLDFIRE)
+#define	test_and_clear_bit(nr, vaddr)	bclr_reg_test_and_clear_bit(nr, vaddr)
+#elif defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#define	test_and_clear_bit(nr, vaddr)	bclr_mem_test_and_clear_bit(nr, vaddr)
+#else
+#define test_and_clear_bit(nr, vaddr)	(__builtin_constant_p(nr) ? \
+					bclr_mem_test_and_clear_bit(nr, vaddr) : \
+					bfclr_mem_test_and_clear_bit(nr, vaddr))
+#endif
+
+#define __test_and_clear_bit(nr, vaddr)	test_and_clear_bit(nr, vaddr)
+
+
+static inline int bchg_reg_test_and_change_bit(int nr,
+					       volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+	char retval;
+
+	__asm__ __volatile__ ("bchg %2,(%1); sne %0"
+		: "=d" (retval)
+		: "a" (p), "di" (nr & 7)
+		: "memory");
+	return retval;
+}
+
+static inline int bchg_mem_test_and_change_bit(int nr,
+					       volatile unsigned long *vaddr)
+{
+	char *p = (char *)vaddr + (nr ^ 31) / 8;
+	char retval;
+
+	__asm__ __volatile__ ("bchg %2,%1; sne %0"
+		: "=d" (retval), "+m" (*p)
+		: "di" (nr & 7));
+	return retval;
+}
+
+static inline int bfchg_mem_test_and_change_bit(int nr,
+						volatile unsigned long *vaddr)
+{
+	char retval;
+
+	__asm__ __volatile__ ("bfchg %2{%1:#1}; sne %0"
+		: "=d" (retval)
+		: "d" (nr ^ 31), "o" (*vaddr)
+		: "memory");
+	return retval;
+}
+
+#if defined(CONFIG_COLDFIRE)
+#define	test_and_change_bit(nr, vaddr)	bchg_reg_test_and_change_bit(nr, vaddr)
+#elif defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#define	test_and_change_bit(nr, vaddr)	bchg_mem_test_and_change_bit(nr, vaddr)
+#else
+#define test_and_change_bit(nr, vaddr)	(__builtin_constant_p(nr) ? \
+					bchg_mem_test_and_change_bit(nr, vaddr) : \
+					bfchg_mem_test_and_change_bit(nr, vaddr))
+#endif
+
+#define __test_and_change_bit(nr, vaddr) test_and_change_bit(nr, vaddr)
+
+
+/*
+ *	The true 68020 and more advanced processors support the "bfffo"
+ *	instruction for finding bits. ColdFire and simple 68000 parts
+ *	(including CPU32) do not support this. They simply use the generic
+ *	functions.
+ */
+#if defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+#include <asm-generic/bitops/find.h>
+#include <asm-generic/bitops/ffz.h>
+#else
+
+static inline int find_first_zero_bit(const unsigned long *vaddr,
+				      unsigned size)
+{
+	const unsigned long *p = vaddr;
+	int res = 32;
+	unsigned int words;
+	unsigned long num;
+
+	if (!size)
+		return 0;
+
+	words = (size + 31) >> 5;
+	while (!(num = ~*p++)) {
+		if (!--words)
+			goto out;
+	}
+
+	__asm__ __volatile__ ("bfffo %1{#0,#0},%0"
+			      : "=d" (res) : "d" (num & -num));
+	res ^= 31;
+out:
+	res += ((long)p - (long)vaddr - 4) * 8;
+	return res < size ? res : size;
+}
+#define find_first_zero_bit find_first_zero_bit
+
+static inline int find_next_zero_bit(const unsigned long *vaddr, int size,
+				     int offset)
+{
+	const unsigned long *p = vaddr + (offset >> 5);
+	int bit = offset & 31UL, res;
+
+	if (offset >= size)
+		return size;
+
+	if (bit) {
+		unsigned long num = ~*p++ & (~0UL << bit);
+		offset -= bit;
+
+		/* Look for zero in first longword */
+		__asm__ __volatile__ ("bfffo %1{#0,#0},%0"
+				      : "=d" (res) : "d" (num & -num));
+		if (res < 32) {
+			offset += res ^ 31;
+			return offset < size ? offset : size;
+		}
+		offset += 32;
+
+		if (offset >= size)
+			return size;
+	}
+	/* No zero yet, search remaining full bytes for a zero */
+	return offset + find_first_zero_bit(p, size - offset);
+}
+#define find_next_zero_bit find_next_zero_bit
+
+static inline int find_first_bit(const unsigned long *vaddr, unsigned size)
+{
+	const unsigned long *p = vaddr;
+	int res = 32;
+	unsigned int words;
+	unsigned long num;
+
+	if (!size)
+		return 0;
+
+	words = (size + 31) >> 5;
+	while (!(num = *p++)) {
+		if (!--words)
+			goto out;
+	}
+
+	__asm__ __volatile__ ("bfffo %1{#0,#0},%0"
+			      : "=d" (res) : "d" (num & -num));
+	res ^= 31;
+out:
+	res += ((long)p - (long)vaddr - 4) * 8;
+	return res < size ? res : size;
+}
+#define find_first_bit find_first_bit
+
+static inline int find_next_bit(const unsigned long *vaddr, int size,
+				int offset)
+{
+	const unsigned long *p = vaddr + (offset >> 5);
+	int bit = offset & 31UL, res;
+
+	if (offset >= size)
+		return size;
+
+	if (bit) {
+		unsigned long num = *p++ & (~0UL << bit);
+		offset -= bit;
+
+		/* Look for one in first longword */
+		__asm__ __volatile__ ("bfffo %1{#0,#0},%0"
+				      : "=d" (res) : "d" (num & -num));
+		if (res < 32) {
+			offset += res ^ 31;
+			return offset < size ? offset : size;
+		}
+		offset += 32;
+
+		if (offset >= size)
+			return size;
+	}
+	/* No one yet, search remaining full bytes for a one */
+	return offset + find_first_bit(p, size - offset);
+}
+#define find_next_bit find_next_bit
+
+/*
+ * ffz = Find First Zero in word. Undefined if no zero exists,
+ * so code should check against ~0UL first..
+ */
+static inline unsigned long ffz(unsigned long word)
+{
+	int res;
+
+	__asm__ __volatile__ ("bfffo %1{#0,#0},%0"
+			      : "=d" (res) : "d" (~word & -~word));
+	return res ^ 31;
+}
+
+#endif
+
+#ifdef __KERNEL__
+
+#if defined(CONFIG_CPU_HAS_NO_BITFIELDS)
+
+/*
+ *	The newer ColdFire family members support a "bitrev" instruction
+ *	and we can use that to implement a fast ffs. Older Coldfire parts,
+ *	and normal 68000 parts don't have anything special, so we use the
+ *	generic functions for those.
+ */
+#if (defined(__mcfisaaplus__) || defined(__mcfisac__)) && \
+	!defined(CONFIG_M68000) && !defined(CONFIG_MCPU32)
+static inline int __ffs(int x)
+{
+	__asm__ __volatile__ ("bitrev %0; ff1 %0"
+		: "=d" (x)
+		: "0" (x));
+	return x;
+}
+
+static inline int ffs(int x)
+{
+	if (!x)
+		return 0;
+	return __ffs(x) + 1;
+}
+
+#else
+#include <asm-generic/bitops/ffs.h>
+#include <asm-generic/bitops/__ffs.h>
+#endif
+
+#include <asm-generic/bitops/fls.h>
+#include <asm-generic/bitops/__fls.h>
+
 #else
-#include "bitops_mm.h"
+
+/*
+ *	ffs: find first bit set. This is defined the same way as
+ *	the libc and compiler builtin ffs routines, therefore
+ *	differs in spirit from the above ffz (man ffs).
+ */
+static inline int ffs(int x)
+{
+	int cnt;
+
+	__asm__ ("bfffo %1{#0:#0},%0"
+		: "=d" (cnt)
+		: "dm" (x & -x));
+	return 32 - cnt;
+}
+#define __ffs(x) (ffs(x) - 1)
+
+/*
+ *	fls: find last bit set.
+ */
+static inline int fls(int x)
+{
+	int cnt;
+
+	__asm__ ("bfffo %1{#0,#0},%0"
+		: "=d" (cnt)
+		: "dm" (x));
+	return 32 - cnt;
+}
+
+static inline int __fls(int x)
+{
+	return fls(x) - 1;
+}
+
 #endif
+
+#include <asm-generic/bitops/ext2-atomic.h>
+#include <asm-generic/bitops/le.h>
+#include <asm-generic/bitops/fls64.h>
+#include <asm-generic/bitops/sched.h>
+#include <asm-generic/bitops/hweight.h>
+#include <asm-generic/bitops/lock.h>
+#endif /* __KERNEL__ */
+
+#endif /* _M68K_BITOPS_H */

commit 49148020bcb6910ce71417bd990a5ce7017f9bd3
Author: Sam Ravnborg <sam@ravnborg.org>
Date:   Fri Jan 16 21:58:10 2009 +1000

    m68k,m68knommu: merge header files
    
    Merge header files for m68k and m68knommu to the single location:
    
        arch/m68k/include/asm
    
    The majority of this patch was the result of the
    script that is included in the changelog below.
    
    The script was originally written by Arnd Bergman and
    exten by me to cover a few more files.
    
    When the header files differed the script uses the following:
    
    The original m68k file is named <file>_mm.h  [mm for memory manager]
    The m68knommu file is named <file>_no.h [no for no memory manager]
    
    The files uses the following include guard:
    
    This include gaurd works as the m68knommu toolchain set
    the __uClinux__ symbol - so this should work in userspace too.
    
    Merging the header files for m68k and m68knommu exposes the
    (unexpected?) ABI differences thus it is easier to actually
    identify these and thus to fix them.
    
    The commit has been build tested with both a m68k and
    a m68knommu toolchain - with success.
    
    The commit has also been tested with "make headers_check"
    and this patch fixes make headers_check for m68knommu.
    
    The script used:
    TARGET=arch/m68k/include/asm
    SOURCE=arch/m68knommu/include/asm
    
    INCLUDE="cachectl.h errno.h fcntl.h hwtest.h ioctls.h ipcbuf.h \
    linkage.h math-emu.h md.h mman.h movs.h msgbuf.h openprom.h \
    oplib.h poll.h posix_types.h resource.h rtc.h sembuf.h shmbuf.h \
    shm.h shmparam.h socket.h sockios.h spinlock.h statfs.h stat.h \
    termbits.h termios.h tlb.h types.h user.h"
    
    EQUAL="auxvec.h cputime.h device.h emergency-restart.h futex.h \
    ioctl.h irq_regs.h kdebug.h local.h mutex.h percpu.h \
    sections.h topology.h"
    
    NOMUUFILES="anchor.h bootstd.h coldfire.h commproc.h dbg.h \
    elia.h flat.h m5206sim.h m520xsim.h m523xsim.h m5249sim.h \
    m5272sim.h m527xsim.h m528xsim.h m5307sim.h m532xsim.h \
    m5407sim.h m68360_enet.h m68360.h m68360_pram.h m68360_quicc.h \
    m68360_regs.h MC68328.h MC68332.h MC68EZ328.h MC68VZ328.h \
    mcfcache.h mcfdma.h mcfmbus.h mcfne.h mcfpci.h mcfpit.h \
    mcfsim.h mcfsmc.h mcftimer.h mcfuart.h mcfwdebug.h \
    nettel.h quicc_simple.h smp.h"
    
    FILES="atomic.h bitops.h bootinfo.h bug.h bugs.h byteorder.h cache.h \
    cacheflush.h checksum.h current.h delay.h div64.h \
    dma-mapping.h dma.h elf.h entry.h fb.h fpu.h hardirq.h hw_irq.h io.h \
    irq.h kmap_types.h machdep.h mc146818rtc.h mmu.h mmu_context.h \
    module.h page.h page_offset.h param.h pci.h pgalloc.h \
    pgtable.h processor.h ptrace.h scatterlist.h segment.h \
    setup.h sigcontext.h siginfo.h signal.h string.h system.h swab.h \
    thread_info.h timex.h tlbflush.h traps.h uaccess.h ucontext.h \
    unaligned.h unistd.h"
    
    mergefile() {
            BASE=${1%.h}
            git mv ${SOURCE}/$1 ${TARGET}/${BASE}_no.h
            git mv ${TARGET}/$1 ${TARGET}/${BASE}_mm.h
    
    cat << EOF > ${TARGET}/$1
    EOF
    
            git add ${TARGET}/$1
    }
    
    set -e
    
    mkdir -p ${TARGET}
    
    git mv include/asm-m68k/* ${TARGET}
    rmdir include/asm-m68k
    
    git rm ${SOURCE}/Kbuild
    for F in $INCLUDE $EQUAL; do
            git rm ${SOURCE}/$F
    done
    
    for F in $NOMUUFILES; do
            git mv ${SOURCE}/$F ${TARGET}/$F
    done
    
    for F in $FILES ; do
            mergefile $F
    done
    
    rmdir arch/m68knommu/include/asm
    rmdir arch/m68knommu/include
    
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Greg Ungerer <gerg@uclinux.org>

diff --git a/arch/m68k/include/asm/bitops.h b/arch/m68k/include/asm/bitops.h
new file mode 100644
index 000000000000..ce163abddaba
--- /dev/null
+++ b/arch/m68k/include/asm/bitops.h
@@ -0,0 +1,5 @@
+#ifdef __uClinux__
+#include "bitops_no.h"
+#else
+#include "bitops_mm.h"
+#endif
