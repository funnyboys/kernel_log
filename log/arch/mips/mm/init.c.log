commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 7c9f0c0a6cd3..336b58173dc7 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -40,7 +40,6 @@
 #include <asm/maar.h>
 #include <asm/mmu_context.h>
 #include <asm/sections.h>
-#include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/tlb.h>
 #include <asm/fixmap.h>

commit ee01c4d72adffb7d424535adf630f2955748fa8b
Merge: c444eb564fb1 09587a09ada2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 20:24:15 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge more updates from Andrew Morton:
     "More mm/ work, plenty more to come
    
      Subsystems affected by this patch series: slub, memcg, gup, kasan,
      pagealloc, hugetlb, vmscan, tools, mempolicy, memblock, hugetlbfs,
      thp, mmap, kconfig"
    
    * akpm: (131 commits)
      arm64: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      x86: mm: use ARCH_HAS_DEBUG_WX instead of arch defined
      riscv: support DEBUG_WX
      mm: add DEBUG_WX support
      drivers/base/memory.c: cache memory blocks in xarray to accelerate lookup
      mm/thp: rename pmd_mknotpresent() as pmd_mkinvalid()
      powerpc/mm: drop platform defined pmd_mknotpresent()
      mm: thp: don't need to drain lru cache when splitting and mlocking THP
      hugetlbfs: get unmapped area below TASK_UNMAPPED_BASE for hugetlbfs
      sparc32: register memory occupied by kernel as memblock.memory
      include/linux/memblock.h: fix minor typo and unclear comment
      mm, mempolicy: fix up gup usage in lookup_node
      tools/vm/page_owner_sort.c: filter out unneeded line
      mm: swap: memcg: fix memcg stats for huge pages
      mm: swap: fix vmstats for huge pages
      mm: vmscan: limit the range of LRU type balancing
      mm: vmscan: reclaim writepage is IO cost
      mm: vmscan: determine anon/file pressure balance at the reclaim root
      mm: balance LRU lists based on relative thrashing
      mm: only count actual rotations as LRU reclaim cost
      ...

commit 9691a071aa26a21fc8dac804a2b98d3c24f76f9a
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:57:10 2020 -0700

    mm: use free_area_init() instead of free_area_init_nodes()
    
    free_area_init() has effectively became a wrapper for
    free_area_init_nodes() and there is no point of keeping it.  Still
    free_area_init() name is shorter and more general as it does not imply
    necessity to initialize multiple nodes.
    
    Rename free_area_init_nodes() to free_area_init(), update the callers and
    drop old version of free_area_init().
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-6-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 79684000de0e..19719e8b41a5 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -418,7 +418,7 @@ void __init paging_init(void)
 	}
 #endif
 
-	free_area_init_nodes(max_zone_pfns);
+	free_area_init(max_zone_pfns);
 }
 
 #ifdef CONFIG_64BIT

commit 9ee195fd1be87719e5fcda4cbd7ba4454249f04f
Author: Serge Semin <Sergey.Semin@baikalelectronics.ru>
Date:   Wed May 6 20:42:30 2020 +0300

    mips: MAAR: Add XPA mode support
    
    When XPA mode is enabled the normally 32-bits MAAR pair registers
    are extended to be of 64-bits width as in pure 64-bits MIPS
    architecture. In this case the MAAR registers can enable the
    speculative loads/stores for addresses of up to 39-bits width.
    But in this case the process of the MAAR initialization changes a bit.
    The upper 32-bits of the registers are supposed to be accessed by mean
    of the dedicated instructions mfhc0/mthc0 and there is a CP0.MAAR.VH
    bit which should be set together with CP0.MAAR.VL as indication
    of the boundary validity. All of these peculiarities were taken into
    account in this commit so the speculative loads/stores would work
    when XPA mode is enabled.
    
    Co-developed-by: Alexey Malahov <Alexey.Malahov@baikalelectronics.ru>
    Signed-off-by: Alexey Malahov <Alexey.Malahov@baikalelectronics.ru>
    Signed-off-by: Serge Semin <Sergey.Semin@baikalelectronics.ru>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: linux-pm@vger.kernel.org
    Cc: devicetree@vger.kernel.org
    Signed-off-by: Thomas Bogendoerfer <tsbogend@alpha.franken.de>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 79684000de0e..620ebfa45ec1 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -358,17 +358,23 @@ void maar_init(void)
 		write_c0_maari(i);
 		back_to_back_c0_hazard();
 		upper = read_c0_maar();
+#ifdef CONFIG_XPA
+		upper |= (phys_addr_t)readx_c0_maar() << MIPS_MAARX_ADDR_SHIFT;
+#endif
 
 		write_c0_maari(i + 1);
 		back_to_back_c0_hazard();
 		lower = read_c0_maar();
+#ifdef CONFIG_XPA
+		lower |= (phys_addr_t)readx_c0_maar() << MIPS_MAARX_ADDR_SHIFT;
+#endif
 
 		attr = lower & upper;
 		lower = (lower & MIPS_MAAR_ADDR) << 4;
 		upper = ((upper & MIPS_MAAR_ADDR) << 4) | 0xffff;
 
 		pr_info("  [%d]: ", i / 2);
-		if (!(attr & MIPS_MAAR_VL)) {
+		if ((attr & MIPS_MAAR_V) != MIPS_MAAR_V) {
 			pr_cont("disabled\n");
 			continue;
 		}

commit f3c560a61b4e32961738b5917674e5d9102aeb7f
Author: Thomas Bogendoerfer <tbogendoerfer@suse.de>
Date:   Thu Jan 9 13:23:31 2020 +0100

    MIPS: mm: Place per_cpu on different nodes, if NUMA is enabled
    
    Implement placing of per_cpu into memory, which is local to the CPU.
    
    Signed-off-by: Thomas Bogendoerfer <tbogendoerfer@suse.de>
    Signed-off-by: Paul Burton <paulburton@kernel.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 50f9ed8c6c1b..79684000de0e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -508,6 +508,51 @@ void __ref free_initmem(void)
 		free_initmem_default(POISON_FREE_INITMEM);
 }
 
+#ifdef CONFIG_HAVE_SETUP_PER_CPU_AREA
+unsigned long __per_cpu_offset[NR_CPUS] __read_mostly;
+EXPORT_SYMBOL(__per_cpu_offset);
+
+static int __init pcpu_cpu_distance(unsigned int from, unsigned int to)
+{
+	return node_distance(cpu_to_node(from), cpu_to_node(to));
+}
+
+static void * __init pcpu_fc_alloc(unsigned int cpu, size_t size,
+				       size_t align)
+{
+	return memblock_alloc_try_nid(size, align, __pa(MAX_DMA_ADDRESS),
+				      MEMBLOCK_ALLOC_ACCESSIBLE,
+				      cpu_to_node(cpu));
+}
+
+static void __init pcpu_fc_free(void *ptr, size_t size)
+{
+	memblock_free_early(__pa(ptr), size);
+}
+
+void __init setup_per_cpu_areas(void)
+{
+	unsigned long delta;
+	unsigned int cpu;
+	int rc;
+
+	/*
+	 * Always reserve area for module percpu variables.  That's
+	 * what the legacy allocator did.
+	 */
+	rc = pcpu_embed_first_chunk(PERCPU_MODULE_RESERVE,
+				    PERCPU_DYNAMIC_RESERVE, PAGE_SIZE,
+				    pcpu_cpu_distance,
+				    pcpu_fc_alloc, pcpu_fc_free);
+	if (rc < 0)
+		panic("Failed to initialize percpu areas.");
+
+	delta = (unsigned long)pcpu_base_addr - (unsigned long)__per_cpu_start;
+	for_each_possible_cpu(cpu)
+		__per_cpu_offset[cpu] = delta + pcpu_unit_offsets[cpu];
+}
+#endif
+
 #ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 unsigned long pgd_current[NR_CPUS];
 #endif

commit 31168f033e3751d1c9245d9ee847d775af16dcd6
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Nov 21 18:21:32 2019 +0200

    mips: drop __pXd_offset() macros that duplicate pXd_index() ones
    
    The __pXd_offset() macros are identical to the pXd_index() macros and there
    is no point to keep both of them. All architectures define and use
    pXd_index() so let's keep only those to make mips consistent with the rest
    of the kernel.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Paul Burton <paulburton@kernel.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: Mike Rapoport <rppt@kernel.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 090fa653dfa9..50f9ed8c6c1b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -239,9 +239,9 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 	unsigned long vaddr;
 
 	vaddr = start;
-	i = __pgd_offset(vaddr);
-	j = __pud_offset(vaddr);
-	k = __pmd_offset(vaddr);
+	i = pgd_index(vaddr);
+	j = pud_index(vaddr);
+	k = pmd_index(vaddr);
 	pgd = pgd_base + i;
 
 	for ( ; (i < PTRS_PER_PGD) && (vaddr < end); pgd++, i++) {

commit 05d013a0366d50f4f0dbebf8c1b22b42020bf49a
Author: Paul Burton <paul.burton@mips.com>
Date:   Wed Sep 18 22:03:30 2019 +0000

    MIPS: Detect bad _PFN_SHIFT values
    
    2 recent commits have fixed issues where _PFN_SHIFT grew too large due
    to the introduction of too many pgprot bits in our PTEs for some MIPS32
    kernel configurations. Tracking down such issues can be tricky, so add a
    BUILD_BUG_ON() to help.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 6fea3b54c961..090fa653dfa9 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -446,6 +446,12 @@ static inline void __init mem_init_free_highmem(void)
 
 void __init mem_init(void)
 {
+	/*
+	 * When _PFN_SHIFT is greater than PAGE_SHIFT we won't have enough PTE
+	 * bits to hold a full 32b physical address on MIPS32 systems.
+	 */
+	BUILD_BUG_ON(IS_ENABLED(CONFIG_32BIT) && (_PFN_SHIFT > PAGE_SHIFT));
+
 #ifdef CONFIG_HIGHMEM
 #ifdef CONFIG_DISCONTIGMEM
 #error "CONFIG_HIGHMEM and CONFIG_DISCONTIGMEM dont work together yet"

commit 625cfb6f20360e82a9c65d8184df49f1d67bc18e
Author: Paul Burton <paul.burton@mips.com>
Date:   Fri Aug 23 17:48:58 2019 +0100

    MIPS: mm: Fix highmem compile
    
    Commit a5718fe8f70f ("MIPS: mm: Drop boot_mem_map") removed the
    definition of a page variable for some reason, but that variable is
    still used. Restore it to fix compilation with CONFIG_HIGHMEM enabled.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index e9e1104e0567..6fea3b54c961 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -434,6 +434,8 @@ static inline void __init mem_init_free_highmem(void)
 		return;
 
 	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
+		struct page *page = pfn_to_page(tmp);
+
 		if (!memblock_is_memory(PFN_PHYS(tmp)))
 			SetPageReserved(page);
 		else

commit a5718fe8f70f33b1b5b47a153057cfdd19684598
Author: Jiaxun Yang <jiaxun.yang@flygoat.com>
Date:   Mon Aug 19 22:23:12 2019 +0800

    MIPS: mm: Drop boot_mem_map
    
    Initialize maar by resource map and replace page_is_ram
    by memblock_is_memory.
    
    Signed-off-by: Jiaxun Yang <jiaxun.yang@flygoat.com>
    [paul.burton@mips.com:
      - Fix bad MAAR address calculations.
      - Use ALIGN() & define maar_align to make it clearer what's going on
        with address manipulations.
      - Drop the new used field from struct maar_config.
      - Rework the RAM walk to avoid iterating over the cfg array needlessly
        to find the first unused entry, then count used entries at the end.
        Instead just keep the count as we go.]
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org
    Cc: yasha.che3@gmail.com
    Cc: aurelien@aurel32.net
    Cc: sfr@canb.auug.org.au
    Cc: fancer.lancer@gmail.com
    Cc: matt.redfearn@mips.com
    Cc: chenhc@lemote.com

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 8a038b30d3c4..e9e1104e0567 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -269,37 +269,46 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 #endif
 }
 
-unsigned __weak platform_maar_init(unsigned num_pairs)
+struct maar_walk_info {
+	struct maar_config cfg[16];
+	unsigned int num_cfg;
+};
+
+static int maar_res_walk(unsigned long start_pfn, unsigned long nr_pages,
+			 void *data)
 {
-	struct maar_config cfg[BOOT_MEM_MAP_MAX];
-	unsigned i, num_configured, num_cfg = 0;
-
-	for (i = 0; i < boot_mem_map.nr_map; i++) {
-		switch (boot_mem_map.map[i].type) {
-		case BOOT_MEM_RAM:
-		case BOOT_MEM_INIT_RAM:
-			break;
-		default:
-			continue;
-		}
+	struct maar_walk_info *wi = data;
+	struct maar_config *cfg = &wi->cfg[wi->num_cfg];
+	unsigned int maar_align;
 
-		/* Round lower up */
-		cfg[num_cfg].lower = boot_mem_map.map[i].addr;
-		cfg[num_cfg].lower = (cfg[num_cfg].lower + 0xffff) & ~0xffff;
+	/* MAAR registers hold physical addresses right shifted by 4 bits */
+	maar_align = BIT(MIPS_MAAR_ADDR_SHIFT + 4);
 
-		/* Round upper down */
-		cfg[num_cfg].upper = boot_mem_map.map[i].addr +
-					boot_mem_map.map[i].size;
-		cfg[num_cfg].upper = (cfg[num_cfg].upper & ~0xffff) - 1;
+	/* Fill in the MAAR config entry */
+	cfg->lower = ALIGN(PFN_PHYS(start_pfn), maar_align);
+	cfg->upper = ALIGN_DOWN(PFN_PHYS(start_pfn + nr_pages), maar_align) - 1;
+	cfg->attrs = MIPS_MAAR_S;
 
-		cfg[num_cfg].attrs = MIPS_MAAR_S;
-		num_cfg++;
-	}
+	/* Ensure we don't overflow the cfg array */
+	if (!WARN_ON(wi->num_cfg >= ARRAY_SIZE(wi->cfg)))
+		wi->num_cfg++;
+
+	return 0;
+}
+
+
+unsigned __weak platform_maar_init(unsigned num_pairs)
+{
+	unsigned int num_configured;
+	struct maar_walk_info wi;
+
+	wi.num_cfg = 0;
+	walk_system_ram_range(0, max_pfn, &wi, maar_res_walk);
 
-	num_configured = maar_config(cfg, num_cfg, num_pairs);
-	if (num_configured < num_cfg)
-		pr_warn("Not enough MAAR pairs (%u) for all bootmem regions (%u)\n",
-			num_pairs, num_cfg);
+	num_configured = maar_config(wi.cfg, wi.num_cfg, num_pairs);
+	if (num_configured < wi.num_cfg)
+		pr_warn("Not enough MAAR pairs (%u) for all memory regions (%u)\n",
+			num_pairs, wi.num_cfg);
 
 	return num_configured;
 }
@@ -382,33 +391,6 @@ void maar_init(void)
 }
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-int page_is_ram(unsigned long pagenr)
-{
-	int i;
-
-	for (i = 0; i < boot_mem_map.nr_map; i++) {
-		unsigned long addr, end;
-
-		switch (boot_mem_map.map[i].type) {
-		case BOOT_MEM_RAM:
-		case BOOT_MEM_INIT_RAM:
-			break;
-		default:
-			/* not usable memory */
-			continue;
-		}
-
-		addr = PFN_UP(boot_mem_map.map[i].addr);
-		end = PFN_DOWN(boot_mem_map.map[i].addr +
-			       boot_mem_map.map[i].size);
-
-		if (pagenr >= addr && pagenr < end)
-			return 1;
-	}
-
-	return 0;
-}
-
 void __init paging_init(void)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
@@ -443,7 +425,7 @@ void __init paging_init(void)
 static struct kcore_list kcore_kseg0;
 #endif
 
-static inline void mem_init_free_highmem(void)
+static inline void __init mem_init_free_highmem(void)
 {
 #ifdef CONFIG_HIGHMEM
 	unsigned long tmp;
@@ -452,9 +434,7 @@ static inline void mem_init_free_highmem(void)
 		return;
 
 	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
-		struct page *page = pfn_to_page(tmp);
-
-		if (!page_is_ram(tmp))
+		if (!memblock_is_memory(PFN_PHYS(tmp)))
 			SetPageReserved(page);
 		else
 			free_highmem_page(page);

commit f94f7434cbbb02f7eb55ed5ad66284023c47968f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 13 17:18:37 2019 -0700

    initramfs: poison freed initrd memory
    
    Various architectures including x86 poison the freed initrd memory.  Do
    the same in the generic free_initrd_mem implementation and switch a few
    more architectures that are identical to the generic code over to it now.
    
    Link: http://lkml.kernel.org/r/20190213174621.29297-9-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>   [arm64]
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>   [m68k]
    Cc: Steven Price <steven.price@arm.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index bbb196ad5f26..8a038b30d3c4 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -504,14 +504,6 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 	printk(KERN_INFO "Freeing %s: %ldk freed\n", what, (end - begin) >> 10);
 }
 
-#ifdef CONFIG_BLK_DEV_INITRD
-void free_initrd_mem(unsigned long start, unsigned long end)
-{
-	free_reserved_area((void *)start, (void *)end, POISON_FREE_INITMEM,
-			   "initrd");
-}
-#endif
-
 void (*free_init_pages_eva)(void *begin, void *end) = NULL;
 
 void __ref free_initmem(void)

commit 8a7f97b902f4fb0d94b355b6b3f1fbd7154cafb9
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Mar 11 23:30:31 2019 -0700

    treewide: add checks for the return value of memblock_alloc*()
    
    Add check for the return value of memblock_alloc*() functions and call
    panic() in case of error.  The panic message repeats the one used by
    panicing memblock allocators with adjustment of parameters to include
    only relevant ones.
    
    The replacement was mostly automated with semantic patches like the one
    below with manual massaging of format strings.
    
      @@
      expression ptr, size, align;
      @@
      ptr = memblock_alloc(size, align);
      + if (!ptr)
      +     panic("%s: Failed to allocate %lu bytes align=0x%lx\n", __func__, size, align);
    
    [anders.roxell@linaro.org: use '%pa' with 'phys_addr_t' type]
      Link: http://lkml.kernel.org/r/20190131161046.21886-1-anders.roxell@linaro.org
    [rppt@linux.ibm.com: fix format strings for panics after memblock_alloc]
      Link: http://lkml.kernel.org/r/1548950940-15145-1-git-send-email-rppt@linux.ibm.com
    [rppt@linux.ibm.com: don't panic if the allocation in sparse_buffer_init fails]
      Link: http://lkml.kernel.org/r/20190131074018.GD28876@rapoport-lnx
    [akpm@linux-foundation.org: fix xtensa printk warning]
    Link: http://lkml.kernel.org/r/1548057848-15136-20-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Reviewed-by: Guo Ren <ren_guo@c-sky.com>                [c-sky]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>    [s390]
    Reviewed-by: Juergen Gross <jgross@suse.com>            [Xen]
    Reviewed-by: Geert Uytterhoeven <geert@linux-m68k.org>  [m68k]
    Acked-by: Max Filippov <jcmvbkbc@gmail.com>             [xtensa]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index c3b45e248806..bbb196ad5f26 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -252,6 +252,11 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 				if (pmd_none(*pmd)) {
 					pte = (pte_t *) memblock_alloc_low(PAGE_SIZE,
 									   PAGE_SIZE);
+					if (!pte)
+						panic("%s: Failed to allocate %lu bytes align=%lx\n",
+						      __func__, PAGE_SIZE,
+						      PAGE_SIZE);
+
 					set_pmd(pmd, __pmd((unsigned long)pte));
 					BUG_ON(pte != pte_offset_kernel(pmd, 0));
 				}

commit c8790d657b0a8d42801fb4536f6f106b4b6306e8
Author: Paul Burton <paul.burton@mips.com>
Date:   Sat Feb 2 01:43:28 2019 +0000

    MIPS: MemoryMapID (MMID) Support
    
    Introduce support for using MemoryMapIDs (MMIDs) as an alternative to
    Address Space IDs (ASIDs). The major difference between the two is that
    MMIDs are global - ie. an MMID uniquely identifies an address space
    across all coherent CPUs. In contrast ASIDs are non-global per-CPU IDs,
    wherein each address space is allocated a separate ASID for each CPU
    upon which it is used. This global namespace allows a new GINVT
    instruction be used to globally invalidate TLB entries associated with a
    particular MMID across all coherent CPUs in the system, removing the
    need for IPIs to invalidate entries with separate ASIDs on each CPU.
    
    The allocation scheme used here is largely borrowed from arm64 (see
    arch/arm64/mm/context.c). In essence we maintain a bitmap to track
    available MMIDs, and MMIDs in active use at the time of a rollover to a
    new MMID version are preserved in the new version. The allocation scheme
    requires efficient 64 bit atomics in order to perform reasonably, so
    this support depends upon CONFIG_GENERIC_ATOMIC64=n (ie. currently it
    will only be included in MIPS64 kernels).
    
    The first, and currently only, available CPU with support for MMIDs is
    the MIPS I6500. This CPU supports 16 bit MMIDs, and so for now we cap
    our MMIDs to 16 bits wide in order to prevent the bitmap growing to
    absurd sizes if any future CPU does implement 32 bit MMIDs as the
    architecture manuals suggest is recommended.
    
    When MMIDs are in use we also make use of GINVT instruction which is
    available due to the global nature of MMIDs. By executing a sequence of
    GINVT & SYNC 0x14 instructions we can avoid the overhead of an IPI to
    each remote CPU in many cases. One complication is that GINVT will
    invalidate wired entries (in all cases apart from type 0, which targets
    the entire TLB). In order to avoid GINVT invalidating any wired TLB
    entries we set up, we make sure to create those entries using a reserved
    MMID (0) that we never associate with any address space.
    
    Also of note is that KVM will require further work in order to support
    MMIDs & GINVT, since KVM is involved in allocating IDs for guests & in
    configuring the MMU. That work is not part of this patch, so for now
    when MMIDs are in use KVM is disabled.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index b521d8e2d359..c3b45e248806 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -84,6 +84,7 @@ void setup_zero_pages(void)
 static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 {
 	enum fixed_addresses idx;
+	unsigned int uninitialized_var(old_mmid);
 	unsigned long vaddr, flags, entrylo;
 	unsigned long old_ctx;
 	pte_t pte;
@@ -110,6 +111,10 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	write_c0_entryhi(vaddr & (PAGE_MASK << 1));
 	write_c0_entrylo0(entrylo);
 	write_c0_entrylo1(entrylo);
+	if (cpu_has_mmid) {
+		old_mmid = read_c0_memorymapid();
+		write_c0_memorymapid(MMID_KERNEL_WIRED);
+	}
 #ifdef CONFIG_XPA
 	if (cpu_has_xpa) {
 		entrylo = (pte.pte_low & _PFNX_MASK);
@@ -124,6 +129,8 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	tlb_write_indexed();
 	tlbw_use_hazard();
 	write_c0_entryhi(old_ctx);
+	if (cpu_has_mmid)
+		write_c0_memorymapid(old_mmid);
 	local_irq_restore(flags);
 
 	return (void*) vaddr;

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 0893b6136498..b521d8e2d359 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -22,7 +22,7 @@
 #include <linux/ptrace.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/highmem.h>
 #include <linux/swap.h>
 #include <linux/proc_fs.h>

commit c6ffc5ca8fb311a89cb6de5c31b6511308ddac8d
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:30 2018 -0700

    memblock: rename free_all_bootmem to memblock_free_all
    
    The conversion is done using
    
    sed -i 's@free_all_bootmem@memblock_free_all@' \
        $(git grep -l free_all_bootmem)
    
    Link: http://lkml.kernel.org/r/1536927045-23536-26-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 842a49ef9909..0893b6136498 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -463,7 +463,7 @@ void __init mem_init(void)
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
 	maar_init();
-	free_all_bootmem();
+	memblock_free_all();
 	setup_zero_pages();	/* Setup zeroed pages.  */
 	mem_init_free_highmem();
 	mem_init_print_info(NULL);

commit e8625dce71b4c23eb81bc9b023c7628807df89e8
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:08:54 2018 -0700

    memblock: replace alloc_bootmem_low_pages with memblock_alloc_low
    
    The alloc_bootmem_low_pages() function allocates PAGE_SIZE aligned regions
    from low memory. memblock_alloc_low() with alignment set to PAGE_SIZE does
    exactly the same thing.
    
    The conversion is done using the following semantic patch:
    
    @@
    expression e;
    @@
    - alloc_bootmem_low_pages(e)
    + memblock_alloc_low(e, PAGE_SIZE)
    
    Link: http://lkml.kernel.org/r/1536927045-23536-19-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 15cae0f11880..842a49ef9909 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -243,7 +243,8 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 			pmd = (pmd_t *)pud;
 			for (; (k < PTRS_PER_PMD) && (vaddr < end); pmd++, k++) {
 				if (pmd_none(*pmd)) {
-					pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
+					pte = (pte_t *) memblock_alloc_low(PAGE_SIZE,
+									   PAGE_SIZE);
 					set_pmd(pmd, __pmd((unsigned long)pte));
 					BUG_ON(pte != pte_offset_kernel(pmd, 0));
 				}

commit 2f0b649b3b4a4fcda3a898d07254d164f082752e
Author: Alexandre Belloni <alexandre.belloni@bootlin.com>
Date:   Fri Sep 28 11:32:02 2018 +0200

    MIPS: stop using _PTRS_PER_PGD
    
    gcc 3.3 has been retired for a while, use PTRS_PER_PGD and remove the
    asm-offsets.h inclusion.
    
    Signed-off-by: Alexandre Belloni <alexandre.belloni@bootlin.com>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Patchwork: https://patchwork.linux-mips.org/patch/20814/
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 400676ce03f4..15cae0f11880 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -32,7 +32,6 @@
 #include <linux/kcore.h>
 #include <linux/initrd.h>
 
-#include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>
 #include <asm/cachectl.h>
 #include <asm/cpu.h>
@@ -521,17 +520,13 @@ unsigned long pgd_current[NR_CPUS];
 #endif
 
 /*
- * gcc 3.3 and older have trouble determining that PTRS_PER_PGD and PGD_ORDER
- * are constants.  So we use the variants from asm-offset.h until that gcc
- * will officially be retired.
- *
  * Align swapper_pg_dir in to 64K, allows its address to be loaded
  * with a single LUI instruction in the TLB handlers.  If we used
  * __aligned(64K), its size would get rounded up to the alignment
  * size, and waste space.  So we place it in its own section and align
  * it in the linker script.
  */
-pgd_t swapper_pg_dir[_PTRS_PER_PGD] __section(.bss..swapper_pg_dir);
+pgd_t swapper_pg_dir[PTRS_PER_PGD] __section(.bss..swapper_pg_dir);
 #ifndef __PAGETABLE_PUD_FOLDED
 pud_t invalid_pud_table[PTRS_PER_PUD] __page_aligned_bss;
 #endif

commit 2fe69ede3eaf3d10fce2ae91fc0552d42e1d5d47
Author: Pravin Shedge <pravin.shedge4linux@gmail.com>
Date:   Sun Dec 10 23:50:17 2017 +0530

    MIPS: Remove duplicate includes
    
    These duplicate includes have been found with scripts/checkincludes.pl
    but they have been removed manually to avoid removing false positives.
    
    Signed-off-by: Pravin Shedge <pravin.shedge4linux@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17920/
    Signed-off-by: James Hogan <jhogan@kernel.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 84b7b592b834..400676ce03f4 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -30,7 +30,6 @@
 #include <linux/hardirq.h>
 #include <linux/gfp.h>
 #include <linux/kcore.h>
-#include <linux/export.h>
 #include <linux/initrd.h>
 
 #include <asm/asm-offsets.h>
@@ -46,7 +45,6 @@
 #include <asm/pgalloc.h>
 #include <asm/tlb.h>
 #include <asm/fixmap.h>
-#include <asm/maar.h>
 
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour

commit ac4f59f88ae84874ac444c742e87b7bf98387e8f
Author: Steven J. Hill <Steven.Hill@cavium.com>
Date:   Fri Sep 15 12:02:55 2017 -0500

    MIPS: Remove unused variable 'lastpfn'
    
    'lastpfn' is never used for anything. Remove it.
    
    Signed-off-by: Steven J. Hill <steven.hill@cavium.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17276/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 5f6ea7d746de..84b7b592b834 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -402,7 +402,6 @@ int page_is_ram(unsigned long pagenr)
 void __init paging_init(void)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
-	unsigned long lastpfn __maybe_unused;
 
 	pagetable_init();
 
@@ -416,17 +415,14 @@ void __init paging_init(void)
 	max_zone_pfns[ZONE_DMA32] = MAX_DMA32_PFN;
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max_low_pfn;
-	lastpfn = max_low_pfn;
 #ifdef CONFIG_HIGHMEM
 	max_zone_pfns[ZONE_HIGHMEM] = highend_pfn;
-	lastpfn = highend_pfn;
 
 	if (cpu_has_dc_aliases && max_low_pfn != highend_pfn) {
 		printk(KERN_WARNING "This processor doesn't support highmem."
 		       " %ldk highmem ignored\n",
 		       (highend_pfn - max_low_pfn) << (PAGE_SHIFT - 10));
 		max_zone_pfns[ZONE_HIGHMEM] = max_low_pfn;
-		lastpfn = max_low_pfn;
 	}
 #endif
 

commit 2aa7687c3cf2764a5dffcd214c18cc9b1cf2be1d
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Wed Aug 23 11:17:50 2017 -0700

    MIPS: Include linux/initrd.h for free_initrd_mem()
    
    arch/mips/mm/init.c provides our implementation of free_initrd_mem(),
    but doesn't include the linux/initrd.h header which declares them. This
    leads to a warning from sparse:
    
      arch/mips/mm/init.c:501:6: warning: symbol 'free_initrd_mem' was not
        declared. Should it be static?
    
    Fix this by including linux/initrd.h to get the declaration of
    free_initrd_mem().
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: trivial@kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/17172/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 8ce2983a7015..5f6ea7d746de 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -31,6 +31,7 @@
 #include <linux/gfp.h>
 #include <linux/kcore.h>
 #include <linux/export.h>
+#include <linux/initrd.h>
 
 #include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>

commit ac3c4aa248c5b5390c40fad1ceb0a15a53f57a36
Merge: 0ba1c1950c32 3e441845caf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 12 09:56:30 2017 -0700

    Merge branch 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    Pull MIPS updates from James Hogan:
     "math-emu:
       - Add missing clearing of BLTZALL and BGEZALL emulation counters
       - Fix BC1EQZ and BC1NEZ condition handling
       - Fix BLEZL and BGTZL identification
    
      BPF:
       - Add JIT support for SKF_AD_HATYPE
       - Use unsigned access for unsigned SKB fields
       - Quit clobbering callee saved registers in JIT code
       - Fix multiple problems in JIT skb access helpers
    
      Loongson 3:
       - Select MIPS_L1_CACHE_SHIFT_6
    
      Octeon:
       - Remove vestiges of CONFIG_CAVIUM_OCTEON_2ND_KERNEL
       - Remove unused L2C types and macros.
       - Remove unused SLI types and macros.
       - Fix compile error when USB is not enabled.
       - Octeon: Remove unused PCIERCX types and macros.
       - Octeon: Clean up platform code.
    
      SNI:
       - Remove recursive include of cpu-feature-overrides.h
    
      Sibyte:
       - Export symbol periph_rev to sb1250-mac network driver.
       - Fix Kconfig warning.
    
      Generic platform:
       - Enable Root FS on NFS in generic_defconfig
    
      SMP-MT:
       - Use CPU interrupt controller IPI IRQ domain support
    
      UASM:
       - Add support for LHU for uasm.
       - Remove needless ISA abstraction
    
      mm:
       - Add 48-bit VA space and 4-level page tables for 4K pages.
    
      PCI:
       - Add controllers before the specified head
    
      irqchip driver for MIPS CPU:
       - Replace magic 0x100 with IE_SW0
       - Prepare for non-legacy IRQ domains
       - Introduce IPI IRQ domain support
    
      MAINTAINERS:
       - Update email-id of Rahul Bedarkar
    
      NET:
       - sb1250-mac: Add missing MODULE_LICENSE()
    
      CPUFREQ:
       - Loongson2: drop set_cpus_allowed_ptr()
    
      Misc:
       - Disable Werror when W= is set
       - Opt into HAVE_COPY_THREAD_TLS
       - Enable GENERIC_CPU_AUTOPROBE
       - Use common outgoing-CPU-notification code
       - Remove dead define of ST_OFF
       - Remove CONFIG_ARCH_HAS_ILOG2_U{32,64}
       - Stengthen IPI IRQ domain sanity check
       - Remove confusing else statement in __do_page_fault()
       - Don't unnecessarily include kmalloc.h into <asm/cache.h>.
       - Delete unused definition of SMP_CACHE_SHIFT.
       - Delete redundant definition of SMP_CACHE_BYTES"
    
    * 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus: (39 commits)
      MIPS: Sibyte: Fix Kconfig warning.
      MIPS: Sibyte: Export symbol periph_rev to sb1250-mac network driver.
      NET: sb1250-mac: Add missing MODULE_LICENSE()
      MAINTAINERS: Update email-id of Rahul Bedarkar
      MIPS: Remove confusing else statement in __do_page_fault()
      MIPS: Stengthen IPI IRQ domain sanity check
      MIPS: smp-mt: Use CPU interrupt controller IPI IRQ domain support
      irqchip: mips-cpu: Introduce IPI IRQ domain support
      irqchip: mips-cpu: Prepare for non-legacy IRQ domains
      irqchip: mips-cpu: Replace magic 0x100 with IE_SW0
      MIPS: Remove CONFIG_ARCH_HAS_ILOG2_U{32,64}
      MIPS: generic: Enable Root FS on NFS in generic_defconfig
      MIPS: mach-rm: Remove recursive include of cpu-feature-overrides.h
      MIPS: Opt into HAVE_COPY_THREAD_TLS
      CPUFREQ: Loongson2: drop set_cpus_allowed_ptr()
      MIPS: uasm: Remove needless ISA abstraction
      MIPS: Remove dead define of ST_OFF
      MIPS: Use common outgoing-CPU-notification code
      MIPS: math-emu: Fix BC1EQZ and BC1NEZ condition handling
      MIPS: r2-on-r6-emu: Clear BLTZALL and BGEZALL debugfs counters
      ...

commit 3377e227af441aff710726437adc20efc359fd9c
Author: Alex Belits <alex.belits@cavium.com>
Date:   Thu Feb 16 17:27:34 2017 -0800

    MIPS: Add 48-bit VA space (and 4-level page tables) for 4K pages.
    
    Some users must have 4K pages while needing a 48-bit VA space size.
    The cleanest way do do this is to go to a 4-level page table for this
    case.  Each page table level using order-0 pages adds 9 bits to the
    VA size (at 4K pages, so for four levels we get 9 * 4 + 12 == 48-bits.
    
    For the 4K page size case only we add support functions for the PUD
    level of the page table tree, also the TLB exception handlers get an
    extra level of tree walk.
    
    [david.daney@cavium.com: Forward port to v4.10.]
    [david.daney@cavium.com: Forward port to v4.11.]
    
    Signed-off-by: Alex Belits <alex.belits@cavium.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Alex Belits <alex.belits@cavium.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/15312/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index aa75849c36bc..37aa931501bf 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -537,6 +537,9 @@ unsigned long pgd_current[NR_CPUS];
  * it in the linker script.
  */
 pgd_t swapper_pg_dir[_PTRS_PER_PGD] __section(.bss..swapper_pg_dir);
+#ifndef __PAGETABLE_PUD_FOLDED
+pud_t invalid_pud_table[PTRS_PER_PUD] __page_aligned_bss;
+#endif
 #ifndef __PAGETABLE_PMD_FOLDED
 pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned_bss;
 EXPORT_SYMBOL_GPL(invalid_pmd_table);

commit f359a1115519122c7c3bc7fc4a01797dcf576709
Author: James Hogan <james.hogan@imgtec.com>
Date:   Tue Mar 14 10:15:09 2017 +0000

    MIPS: Separate MAAR V bit into VL and VH for XPA
    
    The MAAR V bit has been renamed VL since another bit called VH is added
    at the top of the register when it is extended to 64-bits on a 32-bit
    processor with XPA. Rename the V definition, fix the various users, and
    add definitions for the VH bit. Also add a definition for the MAARI
    Index field.
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: linux-mips@linux-mips.org
    Cc: kvm@vger.kernel.org

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index aa75849c36bc..3ca20283b31e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -348,7 +348,7 @@ void maar_init(void)
 		upper = ((upper & MIPS_MAAR_ADDR) << 4) | 0xffff;
 
 		pr_info("  [%d]: ", i / 2);
-		if (!(attr & MIPS_MAAR_V)) {
+		if (!(attr & MIPS_MAAR_VL)) {
 			pr_cont("disabled\n");
 			continue;
 		}

commit f700a420088624671ed955bf818c17012bb57bca
Merge: 98e58b01e167 7170bdc77755
Author: James Hogan <james.hogan@imgtec.com>
Date:   Thu Feb 9 15:20:06 2017 +0000

    Merge tag 'mips_kvm_4.11_1' into mips-for-linux-next
    
    MIPS dependencies for KVM
    
    Miscellaneous MIPS architecture changes depended on by the MIPS KVM
    changes in the KVM tree.
    
    - Move pgd_alloc() out of header.
    - Exports so KVM can access page table management and TLBEX functions.
    - Add return errors to protected cache ops.

commit ccf015166ddbbd4c43a16d6871ea15f1fcf51ccc
Author: James Hogan <james.hogan@imgtec.com>
Date:   Fri Oct 16 16:33:13 2015 +0100

    MIPS: Export pgd/pmd symbols for KVM
    
    Export pmd_init(), invalid_pmd_table and tlbmiss_handler_setup_pgd to
    GPL kernel modules so that MIPS KVM can use the inline page table
    management functions and switch between page tables:
    
    - pmd_init() will be used directly by KVM to initialise newly allocated
      pmd tables with invalid lower level table pointers.
    
    - invalid_pmd_table is used by pud_present(), pud_none(), and
      pud_clear(), which KVM will use to test and clear pud entries.
    
    - tlbmiss_handler_setup_pgd() will be called by KVM entry code to switch
      to the appropriate GVA page tables.
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: "Radim Krčmář" <rkrcmar@redhat.com>
    Cc: linux-mips@linux-mips.org
    Cc: kvm@vger.kernel.org

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index e86ebcf5c071..653569bc0da7 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -538,5 +538,6 @@ unsigned long pgd_current[NR_CPUS];
 pgd_t swapper_pg_dir[_PTRS_PER_PGD] __section(.bss..swapper_pg_dir);
 #ifndef __PAGETABLE_PMD_FOLDED
 pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned_bss;
+EXPORT_SYMBOL_GPL(invalid_pmd_table);
 #endif
 pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned_bss;

commit aa4089e6ce54b5a7aaf5a0e8afb442395cca8503
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Mon Nov 7 11:14:12 2016 +0000

    MIPS: Export invalid_pte_table alongside its definition
    
    It's unclear to me why this wasn't always the case, but move the
    EXPORT_SYMBOL invocation for invalid_pte_table to be alongside its
    definition.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/14511/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index e86ebcf5c071..9d1d54b8e1e8 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -30,6 +30,7 @@
 #include <linux/hardirq.h>
 #include <linux/gfp.h>
 #include <linux/kcore.h>
+#include <linux/export.h>
 
 #include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>
@@ -540,3 +541,4 @@ pgd_t swapper_pg_dir[_PTRS_PER_PGD] __section(.bss..swapper_pg_dir);
 pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned_bss;
 #endif
 pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned_bss;
+EXPORT_SYMBOL(invalid_pte_table);

commit 1031398035a25e5c90c66befb6ff41fa4746df98
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Sat Nov 12 01:26:07 2016 +0000

    MIPS: Mask out limit field when calculating wired entry count
    
    Since MIPSr6 the Wired register is split into 2 fields, with the upper
    16 bits of the register indicating a limit on the value that the wired
    entry count in the bottom 16 bits of the register can take. This means
    that simply reading the wired register doesn't get us a valid TLB entry
    index any longer, and we instead need to retrieve only the lower 16 bits
    of the register. Introduce a new num_wired_entries() function which does
    this on MIPSr6 or higher and simply returns the value of the wired
    register on older architecture revisions, and make use of it when
    reading the number of wired entries.
    
    Since commit e710d6668309 ("MIPS: tlb-r4k: If there are wired entries,
    don't use TLBINVF") we have been using a non-zero number of wired
    entries to determine whether we should avoid use of the tlbinvf
    instruction (which would invalidate wired entries) and instead loop over
    TLB entries in local_flush_tlb_all(). This loop begins with the number
    of wired entries, or before this patch some large bogus TLB index on
    MIPSr6 systems. Thus since the aforementioned commit some MIPSr6 systems
    with FTLBs have been prone to leaving stale address translations in the
    FTLB & crashing in various weird & wonderful ways when we later observe
    the wrong memory.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/14557/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 3a6edecc3f38..e86ebcf5c071 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -118,7 +118,7 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 		writex_c0_entrylo1(entrylo);
 	}
 #endif
-	tlbidx = read_c0_wired();
+	tlbidx = num_wired_entries();
 	write_c0_wired(tlbidx + 1);
 	write_c0_index(tlbidx);
 	mtc0_tlbw_hazard();
@@ -147,7 +147,7 @@ void kunmap_coherent(void)
 
 	local_irq_save(flags);
 	old_ctx = read_c0_entryhi();
-	wired = read_c0_wired() - 1;
+	wired = num_wired_entries() - 1;
 	write_c0_wired(wired);
 	write_c0_index(wired);
 	write_c0_entryhi(UNIQUE_ENTRYHI(wired));

commit d9ba57780637bfde6ac1efb65a8685231ffbc715
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Sun Aug 21 15:58:14 2016 -0400

    MIPS: mm: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/14033/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 72f7478ee068..3a6edecc3f38 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -10,7 +10,7 @@
  */
 #include <linux/bug.h>
 #include <linux/init.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/smp.h>

commit 058effe7fdc5776b017356f690976a857eea473f
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Sep 2 15:17:31 2016 +0100

    MIPS: Fix detection of unsupported highmem with cache aliases
    
    The paging_init() function contains code which detects that highmem is
    in use but unsupported due to dcache aliasing. However this code was
    ineffective because it was being run before the caches are probed,
    meaning that cpu_has_dc_aliases would always evaluate to false (unless a
    platform overrides it to a compile-time constant) and the detection of
    the unsupported case is never triggered. The kernel would then go on to
    attempt to use highmem & either hit coherency issues or trigger the
    BUG_ON in flush_kernel_dcache_page().
    
    Fix this by running paging_init() later than cpu_cache_init(), such that
    the cpu_has_dc_aliases macro will evaluate correctly & the unsupported
    highmem case will be detected successfully.
    
    This then leads to a formerly hidden issue in that
    mem_init_free_highmem() will attempt to free all highmem pages, even
    though we're avoiding use of them & don't have valid page structs for
    them. This leads to an invalid pointer dereference & a TLB exception.
    Avoid this by skipping the loop in mem_init_free_highmem() if
    cpu_has_dc_aliases evaluates true.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Rabin Vincent <rabinv@axis.com>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Alexander Sverdlin <alexander.sverdlin@gmail.com>
    Cc: Aurelien Jarno <aurelien@aurel32.net>
    Cc: Jaedon Shin <jaedon.shin@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Sergey Ryazanov <ryazanov.s.a@gmail.com>
    Cc: Jonas Gorski <jogo@openwrt.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/14184/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 2c3749d98f04..72f7478ee068 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -440,6 +440,9 @@ static inline void mem_init_free_highmem(void)
 #ifdef CONFIG_HIGHMEM
 	unsigned long tmp;
 
+	if (cpu_has_dc_aliases)
+		return;
+
 	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
 		struct page *page = pfn_to_page(tmp);
 

commit ac7e385f2bf1c39615cf58f7e58246fdd9da5bb9
Author: James Hogan <james.hogan@imgtec.com>
Date:   Tue Aug 9 13:21:49 2016 +0100

    MIPS: MAAR: Fix address alignment
    
    The alignment of MIPS MAAR region addresses isn't quite right.
    
    - It rounds an already 64 KiB aligned start address up to the next
      64 KiB boundary, e.g. 0x80000000 is rounded up to 0x80010000.
    
    - It assumes the end address is already on a 64 KiB boundary and doesn't
      round it down. Should that not be the case it will hit the second
      BUG_ON() in write_maar_pair().
    
    Both cases are addressed by rounding up and down to 64 KiB boundaries in
    the more traditional way of adding 0xffff (for rounding up) and masking
    off the low 16 bits.
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13858/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index a5509e7dcad2..2c3749d98f04 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -261,7 +261,6 @@ unsigned __weak platform_maar_init(unsigned num_pairs)
 {
 	struct maar_config cfg[BOOT_MEM_MAP_MAX];
 	unsigned i, num_configured, num_cfg = 0;
-	phys_addr_t skip;
 
 	for (i = 0; i < boot_mem_map.nr_map; i++) {
 		switch (boot_mem_map.map[i].type) {
@@ -272,14 +271,14 @@ unsigned __weak platform_maar_init(unsigned num_pairs)
 			continue;
 		}
 
-		skip = 0x10000 - (boot_mem_map.map[i].addr & 0xffff);
-
+		/* Round lower up */
 		cfg[num_cfg].lower = boot_mem_map.map[i].addr;
-		cfg[num_cfg].lower += skip;
+		cfg[num_cfg].lower = (cfg[num_cfg].lower + 0xffff) & ~0xffff;
 
-		cfg[num_cfg].upper = cfg[num_cfg].lower;
-		cfg[num_cfg].upper += boot_mem_map.map[i].size - 1;
-		cfg[num_cfg].upper -= skip;
+		/* Round upper down */
+		cfg[num_cfg].upper = boot_mem_map.map[i].addr +
+					boot_mem_map.map[i].size;
+		cfg[num_cfg].upper = (cfg[num_cfg].upper & ~0xffff) - 1;
 
 		cfg[num_cfg].attrs = MIPS_MAAR_S;
 		num_cfg++;

commit bd721ea73e1f965569b40620538c942001f76294
Author: Fabian Frederick <fabf@skynet.be>
Date:   Tue Aug 2 14:03:33 2016 -0700

    treewide: replace obsolete _refok by __ref
    
    There was only one use of __initdata_refok and __exit_refok
    
    __init_refok was used 46 times against 82 for __ref.
    
    Those definitions are obsolete since commit 312b1485fb50 ("Introduce new
    section reference annotations tags: __ref, __refdata, __refconst")
    
    This patch removes the following compatibility definitions and replaces
    them treewide.
    
    /* compatibility defines */
    #define __init_refok     __ref
    #define __initdata_refok __refdata
    #define __exit_refok     __ref
    
    I can also provide separate patches if necessary.
    (One patch per tree and check in 1 month or 2 to remove old definitions)
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/1466796271-3043-1-git-send-email-fabf@skynet.be
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 9b58eb5fd0d5..a5509e7dcad2 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -504,7 +504,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 
 void (*free_init_pages_eva)(void *begin, void *end) = NULL;
 
-void __init_refok free_initmem(void)
+void __ref free_initmem(void)
 {
 	prom_free_prom_memory();
 	/*

commit 4b6f99d307ed6c7a28b952bfb7b66fb26a6a4cf0
Author: James Hogan <james.hogan@imgtec.com>
Date:   Tue Apr 19 09:25:10 2016 +0100

    MIPS: mm: Don't do MTHC0 if XPA not present
    
    Performing an MTHC0 instruction without XPA being present will trigger a
    reserved instruction exception, therefore conditionalise the use of this
    instruction when building TLB handlers (build_update_entries()), and in
    __update_tlb().
    
    This allows an XPA kernel to run on non XPA hardware without that
    instruction implemented, just like it can run on XPA capable hardware
    without XPA in use (with the noxpa kernel argument) or with XPA not
    configured in hardware.
    
    [paul.burton@imgtec.com:
      - Rebase atop other TLB work.
      - Add "mm" to subject.
      - Handle the __kmap_pgprot case.]
    
    Fixes: c5b367835cfc ("MIPS: Add support for XPA.")
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/13124/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 134c988bc61f..9b58eb5fd0d5 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -112,9 +112,11 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	write_c0_entrylo0(entrylo);
 	write_c0_entrylo1(entrylo);
 #ifdef CONFIG_XPA
-	entrylo = (pte.pte_low & _PFNX_MASK);
-	writex_c0_entrylo0(entrylo);
-	writex_c0_entrylo1(entrylo);
+	if (cpu_has_xpa) {
+		entrylo = (pte.pte_low & _PFNX_MASK);
+		writex_c0_entrylo0(entrylo);
+		writex_c0_entrylo1(entrylo);
+	}
 #endif
 	tlbidx = read_c0_wired();
 	write_c0_wired(tlbidx + 1);

commit 7b2cb64f91f25a7293b10054e20d1c0734ffab6f
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Tue Apr 19 09:25:05 2016 +0100

    MIPS: mm: Fix MIPS32 36b physical addressing (alchemy, netlogic)
    
    There are 2 distinct cases in which a kernel for a MIPS32 CPU
    (CONFIG_CPU_MIPS32=y) may use 64 bit physical addresses
    (CONFIG_PHYS_ADDR_T_64BIT=y):
    
      - 36 bit physical addressing as used by RMI Alchemy & Netlogic XLP/XLR
        CPUs.
    
      - MIPS32r5 eXtended Physical Addressing (XPA).
    
    These 2 cases are distinct in that they require different behaviour from
    the kernel - the EntryLo registers have different formats. Until Linux
    v4.1 we only supported the first case, with code conditional upon the 2
    aforementioned Kconfig variables being set. Commit c5b367835cfc ("MIPS:
    Add support for XPA.") added support for the second case, but did so by
    modifying the code that existed for the first case rather than treating
    the 2 cases as distinct. Since the EntryLo registers have different
    formats this breaks the 36 bit Alchemy/XLP/XLR case. Fix this by
    splitting the 2 cases, with XPA cases now being conditional upon
    CONFIG_XPA and the non-XPA case matching the code as it existed prior to
    commit c5b367835cfc ("MIPS: Add support for XPA.").
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Reported-by: Manuel Lauss <manuel.lauss@gmail.com>
    Tested-by: Manuel Lauss <manuel.lauss@gmail.com>
    Fixes: c5b367835cfc ("MIPS: Add support for XPA.")
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: Maciej W. Rozycki <macro@linux-mips.org>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Alex Smith <alex.smith@imgtec.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: stable@vger.kernel.org # v4.1+
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/13119/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 6c6a843b0d17..134c988bc61f 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -98,8 +98,10 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, prot);
-#if defined(CONFIG_PHYS_ADDR_T_64BIT) && defined(CONFIG_CPU_MIPS32)
+#if defined(CONFIG_XPA)
 	entrylo = pte_to_entrylo(pte.pte_high);
+#elif defined(CONFIG_PHYS_ADDR_T_64BIT) && defined(CONFIG_CPU_MIPS32)
+	entrylo = pte.pte_high;
 #else
 	entrylo = pte_to_entrylo(pte_val(pte));
 #endif

commit b2a3c5be4d6df3b04781f430c2201cbbc9900f66
Author: James Hogan <james.hogan@imgtec.com>
Date:   Fri Jan 22 10:58:25 2016 +0000

    MIPS: c-r4k: Sync icache when it fills from dcache
    
    It is still necessary to handle icache coherency in flush_cache_range()
    and copy_to_user_page() when the icache fills from the dcache, even
    though the dcache does not need to be written back. However when this
    handling was added in commit 2eaa7ec286db ("[MIPS] Handle I-cache
    coherency in flush_cache_range()"), it did not do any icache flushing
    when it fills from dcache.
    
    Therefore fix r4k_flush_cache_range() to run
    local_r4k_flush_cache_range() without taking into account whether icache
    fills from dcache, so that the icache coherency gets handled. Checks are
    also added in local_r4k_flush_cache_range() so that the dcache blast
    doesn't take place when icache fills from dcache.
    
    A test to mmap a page PROT_READ|PROT_WRITE, modify code in it, and
    mprotect it to VM_READ|VM_EXEC (similar to case described in above
    commit) can hit this case quite easily to verify the fix.
    
    A similar check was added in commit f8829caee311 ("[MIPS] Fix aliasing
    bug in copy_to_user_page / copy_from_user_page"), so also fix
    copy_to_user_page() similarly, to call flush_cache_page() without taking
    into account whether icache fills from dcache, since flush_cache_page()
    already takes that into account to avoid performing a dcache flush.
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: Manuel Lauss <manuel.lauss@gmail.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/12179/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 7e5fa0938c21..6c6a843b0d17 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -196,7 +196,7 @@ void copy_to_user_page(struct vm_area_struct *vma,
 		if (cpu_has_dc_aliases)
 			SetPageDcacheDirty(page);
 	}
-	if ((vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc)
+	if (vma->vm_flags & VM_EXEC)
 		flush_cache_page(vma, vaddr, page_to_pfn(page));
 }
 

commit e1534ae95004d6a307839a44eed40389d608c935
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:53:46 2016 -0800

    mm: differentiate page_mapped() from page_mapcount() for compound pages
    
    Let's define page_mapped() to be true for compound pages if any
    sub-pages of the compound page is mapped (with PMD or PTE).
    
    On other hand page_mapcount() return mapcount for this particular small
    page.
    
    This will make cases like page_get_anon_vma() behave correctly once we
    allow huge pages to be mapped with PTE.
    
    Most users outside core-mm should use page_mapcount() instead of
    page_mapped().
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Tested-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Jerome Marchand <jmarchan@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 8770e619185e..7e5fa0938c21 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -165,7 +165,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 
 	vto = kmap_atomic(to);
 	if (cpu_has_dc_aliases &&
-	    page_mapped(from) && !Page_dcache_dirty(from)) {
+	    page_mapcount(from) && !Page_dcache_dirty(from)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
 		kunmap_coherent();
@@ -187,7 +187,7 @@ void copy_to_user_page(struct vm_area_struct *vma,
 	unsigned long len)
 {
 	if (cpu_has_dc_aliases &&
-	    page_mapped(page) && !Page_dcache_dirty(page)) {
+	    page_mapcount(page) && !Page_dcache_dirty(page)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
 		kunmap_coherent();
@@ -205,7 +205,7 @@ void copy_from_user_page(struct vm_area_struct *vma,
 	unsigned long len)
 {
 	if (cpu_has_dc_aliases &&
-	    page_mapped(page) && !Page_dcache_dirty(page)) {
+	    page_mapcount(page) && !Page_dcache_dirty(page)) {
 		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
 		kunmap_coherent();

commit e060f6ed281669b6d2f22d8dafd664b532386918
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Sep 25 08:59:38 2015 -0700

    MIPS: Initialise MAARs on secondary CPUs
    
    MAARs should be initialised on each CPU (or rather, core) in the system
    in order to achieve consistent behaviour & performance. Previously they
    have only been initialised on the boot CPU which leads to performance
    problems if tasks are later scheduled on a secondary CPU, particularly
    if those tasks make use of unaligned vector accesses where some CPUs
    don't handle any cases in hardware for non-speculative memory regions.
    Fix this by recording the MAAR configuration from the boot CPU and
    applying it to secondary CPUs as part of their bringup.
    
    Reported-by: Doug Gilmore <doug.gilmore@imgtec.com>
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: Andrew Bresticker <abrestic@chromium.org>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Aaro Koskinen <aaro.koskinen@iki.fi>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Markos Chandras <markos.chandras@imgtec.com>
    Cc: Hemmo Nieminen <hemmo.nieminen@iki.fi>
    Cc: Alex Smith <alex.smith@imgtec.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Patchwork: https://patchwork.linux-mips.org/patch/11239/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 023c164b9eb6..8770e619185e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -44,6 +44,7 @@
 #include <asm/pgalloc.h>
 #include <asm/tlb.h>
 #include <asm/fixmap.h>
+#include <asm/maar.h>
 
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
@@ -288,10 +289,14 @@ unsigned __weak platform_maar_init(unsigned num_pairs)
 	return num_configured;
 }
 
-static void maar_init(void)
+void maar_init(void)
 {
 	unsigned num_maars, used, i;
 	phys_addr_t lower, upper, attr;
+	static struct {
+		struct maar_config cfgs[3];
+		unsigned used;
+	} recorded = { { { 0 } }, 0 };
 
 	if (!cpu_has_maar)
 		return;
@@ -304,8 +309,14 @@ static void maar_init(void)
 	/* MAARs should be in pairs */
 	WARN_ON(num_maars % 2);
 
-	/* Configure the required MAARs */
-	used = platform_maar_init(num_maars / 2);
+	/* Set MAARs using values we recorded already */
+	if (recorded.used) {
+		used = maar_config(recorded.cfgs, recorded.used, num_maars / 2);
+		BUG_ON(used != recorded.used);
+	} else {
+		/* Configure the required MAARs */
+		used = platform_maar_init(num_maars / 2);
+	}
 
 	/* Disable any further MAARs */
 	for (i = (used * 2); i < num_maars; i++) {
@@ -315,6 +326,9 @@ static void maar_init(void)
 		back_to_back_c0_hazard();
 	}
 
+	if (recorded.used)
+		return;
+
 	pr_info("MAAR configuration:\n");
 	for (i = 0; i < num_maars; i += 2) {
 		write_c0_maari(i);
@@ -341,6 +355,14 @@ static void maar_init(void)
 			pr_cont(" speculate");
 
 		pr_cont("\n");
+
+		/* Record the setup for use on secondary CPUs */
+		if (used <= ARRAY_SIZE(recorded.cfgs)) {
+			recorded.cfgs[recorded.used].lower = lower;
+			recorded.cfgs[recorded.used].upper = upper;
+			recorded.cfgs[recorded.used].attrs = attr;
+			recorded.used++;
+		}
 	}
 }
 

commit 651ca7f4dab77f07fdac9cfb68bcab6bd2b7f827
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Sep 25 08:59:37 2015 -0700

    MIPS: print MAAR configuration during boot
    
    Verifying that the MAAR configuration is as expected is useful when
    debugging the performance of a system. Print out the memory regions
    configured via MAAR along with their attributes.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Patchwork: https://patchwork.linux-mips.org/patch/11238/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 074ac5459026..023c164b9eb6 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -291,6 +291,7 @@ unsigned __weak platform_maar_init(unsigned num_pairs)
 static void maar_init(void)
 {
 	unsigned num_maars, used, i;
+	phys_addr_t lower, upper, attr;
 
 	if (!cpu_has_maar)
 		return;
@@ -313,6 +314,34 @@ static void maar_init(void)
 		write_c0_maar(0);
 		back_to_back_c0_hazard();
 	}
+
+	pr_info("MAAR configuration:\n");
+	for (i = 0; i < num_maars; i += 2) {
+		write_c0_maari(i);
+		back_to_back_c0_hazard();
+		upper = read_c0_maar();
+
+		write_c0_maari(i + 1);
+		back_to_back_c0_hazard();
+		lower = read_c0_maar();
+
+		attr = lower & upper;
+		lower = (lower & MIPS_MAAR_ADDR) << 4;
+		upper = ((upper & MIPS_MAAR_ADDR) << 4) | 0xffff;
+
+		pr_info("  [%d]: ", i / 2);
+		if (!(attr & MIPS_MAAR_V)) {
+			pr_cont("disabled\n");
+			continue;
+		}
+
+		pr_cont("%pa-%pa", &lower, &upper);
+
+		if (attr & MIPS_MAAR_S)
+			pr_cont(" speculate");
+
+		pr_cont("\n");
+	}
 }
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES

commit def3ab5d0a0fe53026c2495b054dcc46cf923dac
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Sep 25 08:59:36 2015 -0700

    MIPS: mm: compile maar_init unconditionally
    
    maar_init was previously only compiled when CONFIG_NEED_MULTIPLE_NODES
    was not set, which has been fine since it is only called from the
    standard implementation of mem_init which has the same condition. In
    preparation for calling it from the SMP startup code on secondary CPUs,
    move maar_init outside of the #ifndef such that it is always compiled.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Markos Chandras <markos.chandras@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Ingo Molnar <mingo@kernel.org>
    Patchwork: https://patchwork.linux-mips.org/patch/11237/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 66d0f49c5bec..074ac5459026 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -252,6 +252,69 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 #endif
 }
 
+unsigned __weak platform_maar_init(unsigned num_pairs)
+{
+	struct maar_config cfg[BOOT_MEM_MAP_MAX];
+	unsigned i, num_configured, num_cfg = 0;
+	phys_addr_t skip;
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		switch (boot_mem_map.map[i].type) {
+		case BOOT_MEM_RAM:
+		case BOOT_MEM_INIT_RAM:
+			break;
+		default:
+			continue;
+		}
+
+		skip = 0x10000 - (boot_mem_map.map[i].addr & 0xffff);
+
+		cfg[num_cfg].lower = boot_mem_map.map[i].addr;
+		cfg[num_cfg].lower += skip;
+
+		cfg[num_cfg].upper = cfg[num_cfg].lower;
+		cfg[num_cfg].upper += boot_mem_map.map[i].size - 1;
+		cfg[num_cfg].upper -= skip;
+
+		cfg[num_cfg].attrs = MIPS_MAAR_S;
+		num_cfg++;
+	}
+
+	num_configured = maar_config(cfg, num_cfg, num_pairs);
+	if (num_configured < num_cfg)
+		pr_warn("Not enough MAAR pairs (%u) for all bootmem regions (%u)\n",
+			num_pairs, num_cfg);
+
+	return num_configured;
+}
+
+static void maar_init(void)
+{
+	unsigned num_maars, used, i;
+
+	if (!cpu_has_maar)
+		return;
+
+	/* Detect the number of MAARs */
+	write_c0_maari(~0);
+	back_to_back_c0_hazard();
+	num_maars = read_c0_maari() + 1;
+
+	/* MAARs should be in pairs */
+	WARN_ON(num_maars % 2);
+
+	/* Configure the required MAARs */
+	used = platform_maar_init(num_maars / 2);
+
+	/* Disable any further MAARs */
+	for (i = (used * 2); i < num_maars; i++) {
+		write_c0_maari(i);
+		back_to_back_c0_hazard();
+		write_c0_maar(0);
+		back_to_back_c0_hazard();
+	}
+}
+
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 int page_is_ram(unsigned long pagenr)
 {
@@ -334,69 +397,6 @@ static inline void mem_init_free_highmem(void)
 #endif
 }
 
-unsigned __weak platform_maar_init(unsigned num_pairs)
-{
-	struct maar_config cfg[BOOT_MEM_MAP_MAX];
-	unsigned i, num_configured, num_cfg = 0;
-	phys_addr_t skip;
-
-	for (i = 0; i < boot_mem_map.nr_map; i++) {
-		switch (boot_mem_map.map[i].type) {
-		case BOOT_MEM_RAM:
-		case BOOT_MEM_INIT_RAM:
-			break;
-		default:
-			continue;
-		}
-
-		skip = 0x10000 - (boot_mem_map.map[i].addr & 0xffff);
-
-		cfg[num_cfg].lower = boot_mem_map.map[i].addr;
-		cfg[num_cfg].lower += skip;
-
-		cfg[num_cfg].upper = cfg[num_cfg].lower;
-		cfg[num_cfg].upper += boot_mem_map.map[i].size - 1;
-		cfg[num_cfg].upper -= skip;
-
-		cfg[num_cfg].attrs = MIPS_MAAR_S;
-		num_cfg++;
-	}
-
-	num_configured = maar_config(cfg, num_cfg, num_pairs);
-	if (num_configured < num_cfg)
-		pr_warn("Not enough MAAR pairs (%u) for all bootmem regions (%u)\n",
-			num_pairs, num_cfg);
-
-	return num_configured;
-}
-
-static void maar_init(void)
-{
-	unsigned num_maars, used, i;
-
-	if (!cpu_has_maar)
-		return;
-
-	/* Detect the number of MAARs */
-	write_c0_maari(~0);
-	back_to_back_c0_hazard();
-	num_maars = read_c0_maari() + 1;
-
-	/* MAARs should be in pairs */
-	WARN_ON(num_maars % 2);
-
-	/* Configure the required MAARs */
-	used = platform_maar_init(num_maars / 2);
-
-	/* Disable any further MAARs */
-	for (i = (used * 2); i < num_maars; i++) {
-		write_c0_maari(i);
-		back_to_back_c0_hazard();
-		write_c0_maar(0);
-		back_to_back_c0_hazard();
-	}
-}
-
 void __init mem_init(void)
 {
 #ifdef CONFIG_HIGHMEM

commit cbd95a89996d1b5fadf411e9218c64e713a83099
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jul 10 16:52:38 2015 +0100

    MIPS: mm: default platform_maar_init using bootmem data
    
    Introduce a default weak implementation of platform_maar_init which
    makes use of the data that platforms already provide to the bootmem
    allocator. This should hopefully cover the most common configurations,
    reduce the duplication of information provided by platforms & leaves
    platforms with the option of providing a custom implementation if
    required.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Patchwork: https://patchwork.linux-mips.org/patch/10676/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 198a3147dd7d..66d0f49c5bec 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -37,6 +37,7 @@
 #include <asm/cpu.h>
 #include <asm/dma.h>
 #include <asm/kmap_types.h>
+#include <asm/maar.h>
 #include <asm/mmu_context.h>
 #include <asm/sections.h>
 #include <asm/pgtable.h>
@@ -333,9 +334,40 @@ static inline void mem_init_free_highmem(void)
 #endif
 }
 
-unsigned __weak platform_maar_init(unsigned num_maars)
+unsigned __weak platform_maar_init(unsigned num_pairs)
 {
-	return 0;
+	struct maar_config cfg[BOOT_MEM_MAP_MAX];
+	unsigned i, num_configured, num_cfg = 0;
+	phys_addr_t skip;
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		switch (boot_mem_map.map[i].type) {
+		case BOOT_MEM_RAM:
+		case BOOT_MEM_INIT_RAM:
+			break;
+		default:
+			continue;
+		}
+
+		skip = 0x10000 - (boot_mem_map.map[i].addr & 0xffff);
+
+		cfg[num_cfg].lower = boot_mem_map.map[i].addr;
+		cfg[num_cfg].lower += skip;
+
+		cfg[num_cfg].upper = cfg[num_cfg].lower;
+		cfg[num_cfg].upper += boot_mem_map.map[i].size - 1;
+		cfg[num_cfg].upper -= skip;
+
+		cfg[num_cfg].attrs = MIPS_MAAR_S;
+		num_cfg++;
+	}
+
+	num_configured = maar_config(cfg, num_cfg, num_pairs);
+	if (num_configured < num_cfg)
+		pr_warn("Not enough MAAR pairs (%u) for all bootmem regions (%u)\n",
+			num_pairs, num_cfg);
+
+	return num_configured;
 }
 
 static void maar_init(void)

commit ce01948eb85da733558fa77c2a554144a57ab0fb
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 11 17:52:10 2015 +0200

    sched/preempt, mm/kmap, MIPS: Disable preemption in kmap_coherent() explicitly
    
    k(un)map_coherent relies on pagefault_disable() to also disable
    preemption.
    
    Let's make this explicit, to prepare for pagefault_disable() not
    touching preemption anymore.
    
    This patch is based on a patch by Yang Shi on the -rt tree:
    "k{un}map_coherent are just called when cpu_has_dc_aliases == 1 with VIPT
    cache. However, actually, the most modern MIPS processors have PIPT dcache
    without dcache alias issue. In such case, k{un}map_atomic will be called
    with preempt enabled."
    
    Reviewed-and-tested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: benh@kernel.crashing.org
    Cc: bigeasy@linutronix.de
    Cc: borntraeger@de.ibm.com
    Cc: daniel.vetter@intel.com
    Cc: heiko.carstens@de.ibm.com
    Cc: herbert@gondor.apana.org.au
    Cc: hocko@suse.cz
    Cc: hughd@google.com
    Cc: mst@redhat.com
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: schwidefsky@de.ibm.com
    Cc: yang.shi@windriver.com
    Link: http://lkml.kernel.org/r/1431359540-32227-6-git-send-email-dahi@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index faa5c9822ecc..198a3147dd7d 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -90,6 +90,7 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 
 	BUG_ON(Page_dcache_dirty(page));
 
+	preempt_disable();
 	pagefault_disable();
 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
@@ -152,6 +153,7 @@ void kunmap_coherent(void)
 	write_c0_entryhi(old_ctx);
 	local_irq_restore(flags);
 	pagefault_enable();
+	preempt_enable();
 }
 
 void copy_user_highpage(struct page *to, struct page *from,

commit c5b367835cfc7a8ef53b9670a409ffcc95194344
Author: Steven J. Hill <Steven.Hill@imgtec.com>
Date:   Thu Feb 26 18:16:38 2015 -0600

    MIPS: Add support for XPA.
    
    Add support for extended physical addressing (XPA) so that
    32-bit platforms can access equal to or greater than 40 bits
    of physical addresses.
    
    NOTE:
          1) XPA and EVA are not the same and cannot be used
             simultaneously.
          2) If you configure your kernel for XPA, the PTEs
             and all address sizes become 64-bit.
          3) Your platform MUST have working HIGHMEM support.
    
    Signed-off-by: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/9355/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 448cde372af0..faa5c9822ecc 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -96,7 +96,7 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, prot);
 #if defined(CONFIG_PHYS_ADDR_T_64BIT) && defined(CONFIG_CPU_MIPS32)
-	entrylo = pte.pte_high;
+	entrylo = pte_to_entrylo(pte.pte_high);
 #else
 	entrylo = pte_to_entrylo(pte_val(pte));
 #endif
@@ -106,6 +106,11 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	write_c0_entryhi(vaddr & (PAGE_MASK << 1));
 	write_c0_entrylo0(entrylo);
 	write_c0_entrylo1(entrylo);
+#ifdef CONFIG_XPA
+	entrylo = (pte.pte_low & _PFNX_MASK);
+	writex_c0_entrylo0(entrylo);
+	writex_c0_entrylo1(entrylo);
+#endif
 	tlbidx = read_c0_wired();
 	write_c0_wired(tlbidx + 1);
 	write_c0_index(tlbidx);

commit 34adb28d500e644cc260da4ceb66ba6dc0beaf93
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Nov 22 00:16:48 2014 +0100

    MIPS: Replace MIPS-specific 64BIT_PHYS_ADDR with generic PHYS_ADDR_T_64BIT
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index f42e35e42790..448cde372af0 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -95,7 +95,7 @@ static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, prot);
-#if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
+#if defined(CONFIG_PHYS_ADDR_T_64BIT) && defined(CONFIG_CPU_MIPS32)
 	entrylo = pte.pte_high;
 #else
 	entrylo = pte_to_entrylo(pte_val(pte));

commit 0b70068e47e8f0c813a902dc3d6def601fd15acb
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Sep 12 22:17:23 2014 +0200

    mm: export symbol dependencies of is_zero_pfn()
    
    In order to make the static inline function is_zero_pfn() callable by
    modules, export its symbol dependencies 'zero_pfn' and (for s390 and
    mips) 'zero_page_mask'.
    
    We need this for KVM, as CONFIG_KVM is a tristate for all supported
    architectures except ARM and arm64, and testing a pfn whether it refers
    to the zero page is required to correctly distinguish the zero page
    from other special RAM ranges that may also have the PG_reserved bit
    set, but need to be treated as MMIO memory.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 571aab064936..f42e35e42790 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -53,6 +53,7 @@
  */
 unsigned long empty_zero_page, zero_page_mask;
 EXPORT_SYMBOL_GPL(empty_zero_page);
+EXPORT_SYMBOL(zero_page_mask);
 
 /*
  * Not static inline because used by IP27 special magic initialization code

commit ab9988a3dafb8c94c2a19637fb00f7d1db799347
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Mon Jul 14 10:32:15 2014 +0100

    MIPS: Initialise MAARs
    
    Add initialisation for Memory Accessibility Attribute Registers. Generic
    code cannot know the platform-specific requirements with regards to
    speculative accesses, so it simply calls a platform_maar_init function
    which platforms with MAARs are expected to implement by calling the
    provided write_maar_pair function & returning the number of MAAR pairs
    used. A weak default implementation will simply use no MAAR pairs. Any
    present but unused MAAR pairs are then marked invalid, effectively
    disabling them.
    
    The end result of this patch is that MAARs are all marked invalid, until
    platforms implement the platform_maar_init function.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/7331/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 6e4413330e36..571aab064936 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -325,6 +325,38 @@ static inline void mem_init_free_highmem(void)
 #endif
 }
 
+unsigned __weak platform_maar_init(unsigned num_maars)
+{
+	return 0;
+}
+
+static void maar_init(void)
+{
+	unsigned num_maars, used, i;
+
+	if (!cpu_has_maar)
+		return;
+
+	/* Detect the number of MAARs */
+	write_c0_maari(~0);
+	back_to_back_c0_hazard();
+	num_maars = read_c0_maari() + 1;
+
+	/* MAARs should be in pairs */
+	WARN_ON(num_maars % 2);
+
+	/* Configure the required MAARs */
+	used = platform_maar_init(num_maars / 2);
+
+	/* Disable any further MAARs */
+	for (i = (used * 2); i < num_maars; i++) {
+		write_c0_maari(i);
+		back_to_back_c0_hazard();
+		write_c0_maar(0);
+		back_to_back_c0_hazard();
+	}
+}
+
 void __init mem_init(void)
 {
 #ifdef CONFIG_HIGHMEM
@@ -337,6 +369,7 @@ void __init mem_init(void)
 #endif
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
+	maar_init();
 	free_all_bootmem();
 	setup_zero_pages();	/* Setup zeroed pages.  */
 	mem_init_free_highmem();

commit 2e2d663d2dd64ffe9855be0b35aa221c9b8139f2
Merge: 5ec79bf919dd 322014531e1f
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed May 28 19:00:14 2014 +0200

    Merge branch 'wip-mips-pm' of https://github.com/paulburton/linux into mips-for-linux-next

commit e2a9e5ad719fb424ab3c30520733aa0e8fbcf1ce
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Mon Mar 3 12:08:40 2014 +0000

    MIPS: add kmap_noncoherent to wire a cached non-coherent TLB entry
    
    This is identical to kmap_coherent apart from the cache coherency
    attribute used for the TLB entry, so kmap_coherent is abstracted to
    kmap_prot which is then called for both kmap_coherent &
    kmap_noncoherent. This will be used by a subsequent patch.
    
    Suggested-by: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4fc74c78265a..80ff52e27b7f 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -114,7 +114,7 @@ static void __init kmap_coherent_init(void)
 static inline void kmap_coherent_init(void) {}
 #endif
 
-void *kmap_coherent(struct page *page, unsigned long addr)
+static void *__kmap_pgprot(struct page *page, unsigned long addr, pgprot_t prot)
 {
 	enum fixed_addresses idx;
 	unsigned long vaddr, flags, entrylo;
@@ -133,7 +133,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
 #endif
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
-	pte = mk_pte(page, PAGE_KERNEL);
+	pte = mk_pte(page, prot);
 #if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
 	entrylo = pte.pte_high;
 #else
@@ -171,6 +171,16 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 	return (void*) vaddr;
 }
 
+void *kmap_coherent(struct page *page, unsigned long addr)
+{
+	return __kmap_pgprot(page, addr, PAGE_KERNEL);
+}
+
+void *kmap_noncoherent(struct page *page, unsigned long addr)
+{
+	return __kmap_pgprot(page, addr, PAGE_KERNEL_NC);
+}
+
 void kunmap_coherent(void)
 {
 #ifndef CONFIG_MIPS_MT_SMTC

commit b633648c5ad3cfbda0b3daea50d2135d44899259
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri May 23 16:29:44 2014 +0200

    MIPS: MT: Remove SMTC support
    
    Nobody is maintaining SMTC anymore and there also seems to be no userbase.
    Which is a pity - the SMTC technology primarily developed by Kevin D.
    Kissell <kevink@paralogos.com> is an ingenious demonstration for the MT
    ASE's power and elegance.
    
    Based on Markos Chandras <Markos.Chandras@imgtec.com> patch
    https://patchwork.linux-mips.org/patch/6719/ which while very similar did
    no longer apply cleanly when I tried to merge it plus some additional
    post-SMTC cleanup - SMTC was a feature as tricky to remove as it was to
    merge once upon a time.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4fc74c78265a..609a0cd749ff 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -44,27 +44,6 @@
 #include <asm/tlb.h>
 #include <asm/fixmap.h>
 
-/* Atomicity and interruptability */
-#ifdef CONFIG_MIPS_MT_SMTC
-
-#include <asm/mipsmtregs.h>
-
-#define ENTER_CRITICAL(flags) \
-	{ \
-	unsigned int mvpflags; \
-	local_irq_save(flags);\
-	mvpflags = dvpe()
-#define EXIT_CRITICAL(flags) \
-	evpe(mvpflags); \
-	local_irq_restore(flags); \
-	}
-#else
-
-#define ENTER_CRITICAL(flags) local_irq_save(flags)
-#define EXIT_CRITICAL(flags) local_irq_restore(flags)
-
-#endif /* CONFIG_MIPS_MT_SMTC */
-
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
  * when needed.	 This is necessary only on R4000 / R4400 SC and MC versions
@@ -100,20 +79,6 @@ void setup_zero_pages(void)
 	zero_page_mask = ((PAGE_SIZE << order) - 1) & PAGE_MASK;
 }
 
-#ifdef CONFIG_MIPS_MT_SMTC
-static pte_t *kmap_coherent_pte;
-static void __init kmap_coherent_init(void)
-{
-	unsigned long vaddr;
-
-	/* cache the first coherent kmap pte */
-	vaddr = __fix_to_virt(FIX_CMAP_BEGIN);
-	kmap_coherent_pte = kmap_get_fixmap_pte(vaddr);
-}
-#else
-static inline void kmap_coherent_init(void) {}
-#endif
-
 void *kmap_coherent(struct page *page, unsigned long addr)
 {
 	enum fixed_addresses idx;
@@ -126,12 +91,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 
 	pagefault_disable();
 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
-#ifdef CONFIG_MIPS_MT_SMTC
-	idx += FIX_N_COLOURS * smp_processor_id() +
-		(in_interrupt() ? (FIX_N_COLOURS * NR_CPUS) : 0);
-#else
 	idx += in_interrupt() ? FIX_N_COLOURS : 0;
-#endif
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, PAGE_KERNEL);
 #if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
@@ -140,44 +100,29 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 	entrylo = pte_to_entrylo(pte_val(pte));
 #endif
 
-	ENTER_CRITICAL(flags);
+	local_irq_save(flags);
 	old_ctx = read_c0_entryhi();
 	write_c0_entryhi(vaddr & (PAGE_MASK << 1));
 	write_c0_entrylo0(entrylo);
 	write_c0_entrylo1(entrylo);
-#ifdef CONFIG_MIPS_MT_SMTC
-	set_pte(kmap_coherent_pte - (FIX_CMAP_END - idx), pte);
-	/* preload TLB instead of local_flush_tlb_one() */
-	mtc0_tlbw_hazard();
-	tlb_probe();
-	tlb_probe_hazard();
-	tlbidx = read_c0_index();
-	mtc0_tlbw_hazard();
-	if (tlbidx < 0)
-		tlb_write_random();
-	else
-		tlb_write_indexed();
-#else
 	tlbidx = read_c0_wired();
 	write_c0_wired(tlbidx + 1);
 	write_c0_index(tlbidx);
 	mtc0_tlbw_hazard();
 	tlb_write_indexed();
-#endif
 	tlbw_use_hazard();
 	write_c0_entryhi(old_ctx);
-	EXIT_CRITICAL(flags);
+	local_irq_restore(flags);
 
 	return (void*) vaddr;
 }
 
 void kunmap_coherent(void)
 {
-#ifndef CONFIG_MIPS_MT_SMTC
 	unsigned int wired;
 	unsigned long flags, old_ctx;
 
-	ENTER_CRITICAL(flags);
+	local_irq_save(flags);
 	old_ctx = read_c0_entryhi();
 	wired = read_c0_wired() - 1;
 	write_c0_wired(wired);
@@ -189,8 +134,7 @@ void kunmap_coherent(void)
 	tlb_write_indexed();
 	tlbw_use_hazard();
 	write_c0_entryhi(old_ctx);
-	EXIT_CRITICAL(flags);
-#endif
+	local_irq_restore(flags);
 	pagefault_enable();
 }
 
@@ -256,7 +200,7 @@ EXPORT_SYMBOL_GPL(copy_from_user_page);
 void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)
 {
-#if defined(CONFIG_HIGHMEM) || defined(CONFIG_MIPS_MT_SMTC)
+#ifdef CONFIG_HIGHMEM
 	pgd_t *pgd;
 	pud_t *pud;
 	pmd_t *pmd;
@@ -327,8 +271,6 @@ void __init paging_init(void)
 #ifdef CONFIG_HIGHMEM
 	kmap_init();
 #endif
-	kmap_coherent_init();
-
 #ifdef CONFIG_ZONE_DMA
 	max_zone_pfns[ZONE_DMA] = MAX_DMA_PFN;
 #endif

commit 0893d3fb8d27320d4301f14a0b331df291a08044
Author: Markos Chandras <markos.chandras@imgtec.com>
Date:   Wed Jan 15 14:06:03 2014 +0000

    MIPS: mm: init: Add free_init_pages() callback for EVA
    
    A core in EVA mode can have any possible segment mapping, so the
    default free_initmem_default() function may not always work as expected.
    Therefore, add a callback that platforms can use to free up the init section.
    
    Signed-off-by: Markos Chandras <markos.chandras@imgtec.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 6b59617760c1..4fc74c78265a 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -422,10 +422,20 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 }
 #endif
 
+void (*free_init_pages_eva)(void *begin, void *end) = NULL;
+
 void __init_refok free_initmem(void)
 {
 	prom_free_prom_memory();
-	free_initmem_default(POISON_FREE_INITMEM);
+	/*
+	 * Let the platform define a specific function to free the
+	 * init section since EVA may have used any possible mapping
+	 * between virtual and physical addresses.
+	 */
+	if (free_init_pages_eva)
+		free_init_pages_eva((void *)&__init_begin, (void *)&__init_end);
+	else
+		free_initmem_default(POISON_FREE_INITMEM);
 }
 
 #ifndef CONFIG_MIPS_PGD_C0_CONTEXT

commit c01905eeee579db98dd6b39d3f41497065ecc273
Author: Markos Chandras <markos.chandras@imgtec.com>
Date:   Thu Nov 14 16:12:22 2013 +0000

    MIPS: mm: Move UNIQUE_ENTRYHI macro to a header file
    
    The UNIQUE_ENTRYHI definition was duplicated whenever there
    was the need to flush the TLB entries. We move this common
    definition to a header file.
    
    Signed-off-by: Markos Chandras <markos.chandras@imgtec.com>
    Signed-off-by: John Crispin <blogic@openwrt.org>
    Patchwork: http://patchwork.linux-mips.org/patch/6129/

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 12156176c7ca..6b59617760c1 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -171,8 +171,6 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 	return (void*) vaddr;
 }
 
-#define UNIQUE_ENTRYHI(idx) (CKSEG0 + ((idx) << (PAGE_SHIFT + 1)))
-
 void kunmap_coherent(void)
 {
 #ifndef CONFIG_MIPS_MT_SMTC

commit bdb43806589096ac4272fe1307e789846ac08d7c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 10 12:15:23 2013 +0200

    sched: Extract the basic add/sub preempt_count modifiers
    
    Rewrite the preempt_count macros in order to extract the 3 basic
    preempt_count value modifiers:
    
      __preempt_count_add()
      __preempt_count_sub()
    
    and the new:
    
      __preempt_count_dec_and_test()
    
    And since we're at it anyway, replace the unconventional
    $op_preempt_count names with the more conventional preempt_count_$op.
    
    Since these basic operators are equivalent to the previous _notrace()
    variants, do away with the _notrace() versions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ewbpdbupy9xpsjhg960zwbv8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index e205ef598e97..12156176c7ca 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -124,7 +124,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 
 	BUG_ON(Page_dcache_dirty(page));
 
-	inc_preempt_count();
+	pagefault_disable();
 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
 #ifdef CONFIG_MIPS_MT_SMTC
 	idx += FIX_N_COLOURS * smp_processor_id() +
@@ -193,8 +193,7 @@ void kunmap_coherent(void)
 	write_c0_entryhi(old_ctx);
 	EXIT_CRITICAL(flags);
 #endif
-	dec_preempt_count();
-	preempt_check_resched();
+	pagefault_enable();
 }
 
 void copy_user_highpage(struct page *to, struct page *from,

commit bf9621aaa0bb94dcd3b198ca14d3220310655eea
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Thu Sep 5 11:22:45 2013 +0200

    MIPS: Export copy_from_user_page() (needed by lustre)
    
    ERROR: "copy_from_user_page" [drivers/staging/lustre/lustre/libcfs/libcfs.ko] undefined!
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/5808/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4e73f10a7519..e205ef598e97 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -254,6 +254,7 @@ void copy_from_user_page(struct vm_area_struct *vma,
 			SetPageDcacheDirty(page);
 	}
 }
+EXPORT_SYMBOL_GPL(copy_from_user_page);
 
 void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)

commit 1132137e87898d0b6786d85a99de35ce196ecbfb
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:04:04 2013 -0700

    mm/MIPS: prepare for removing num_physpages and simplify mem_init()
    
    Prepare for removing num_physpages and simplify mem_init().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: John Crispin <blogic@openwrt.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index e7333f15b1b7..4e73f10a7519 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -359,11 +359,24 @@ void __init paging_init(void)
 static struct kcore_list kcore_kseg0;
 #endif
 
-void __init mem_init(void)
+static inline void mem_init_free_highmem(void)
 {
-	unsigned long codesize, reservedpages, datasize, initsize;
-	unsigned long tmp, ram;
+#ifdef CONFIG_HIGHMEM
+	unsigned long tmp;
 
+	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
+		struct page *page = pfn_to_page(tmp);
+
+		if (!page_is_ram(tmp))
+			SetPageReserved(page);
+		else
+			free_highmem_page(page);
+	}
+#endif
+}
+
+void __init mem_init(void)
+{
 #ifdef CONFIG_HIGHMEM
 #ifdef CONFIG_DISCONTIGMEM
 #error "CONFIG_HIGHMEM and CONFIG_DISCONTIGMEM dont work together yet"
@@ -376,32 +389,8 @@ void __init mem_init(void)
 
 	free_all_bootmem();
 	setup_zero_pages();	/* Setup zeroed pages.  */
-
-	reservedpages = ram = 0;
-	for (tmp = 0; tmp < max_low_pfn; tmp++)
-		if (page_is_ram(tmp) && pfn_valid(tmp)) {
-			ram++;
-			if (PageReserved(pfn_to_page(tmp)))
-				reservedpages++;
-		}
-	num_physpages = ram;
-
-#ifdef CONFIG_HIGHMEM
-	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
-		struct page *page = pfn_to_page(tmp);
-
-		if (!page_is_ram(tmp)) {
-			SetPageReserved(page);
-			continue;
-		}
-		free_highmem_page(page);
-	}
-	num_physpages += totalhigh_pages;
-#endif
-
-	codesize =  (unsigned long) &_etext - (unsigned long) &_text;
-	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
-	initsize =  (unsigned long) &__init_end - (unsigned long) &__init_begin;
+	mem_init_free_highmem();
+	mem_init_print_info(NULL);
 
 #ifdef CONFIG_64BIT
 	if ((unsigned long) &_text > (unsigned long) CKSEG0)
@@ -410,16 +399,6 @@ void __init mem_init(void)
 		kclist_add(&kcore_kseg0, (void *) CKSEG0,
 				0x80000000 - 4, KCORE_TEXT);
 #endif
-
-	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
-	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",
-	       nr_free_pages() << (PAGE_SHIFT-10),
-	       ram << (PAGE_SHIFT-10),
-	       codesize >> 10,
-	       reservedpages << (PAGE_SHIFT-10),
-	       datasize >> 10,
-	       initsize >> 10,
-	       totalhigh_pages << (PAGE_SHIFT-10));
 }
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 

commit 0c988534737a358fdff42fcce78f0ff1a12dbfc5
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:03:24 2013 -0700

    mm: concentrate modification of totalram_pages into the mm core
    
    Concentrate code to modify totalram_pages into the mm core, so the arch
    memory initialized code doesn't need to take care of it.  With these
    changes applied, only following functions from mm core modify global
    variable totalram_pages: free_bootmem_late(), free_all_bootmem(),
    free_all_bootmem_node(), adjust_managed_page_count().
    
    With this patch applied, it will be much more easier for us to keep
    totalram_pages and zone->managed_pages in consistence.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 268f2a94031b..e7333f15b1b7 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -374,7 +374,7 @@ void __init mem_init(void)
 #endif
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
-	totalram_pages += free_all_bootmem();
+	free_all_bootmem();
 	setup_zero_pages();	/* Setup zeroed pages.  */
 
 	reservedpages = ram = 0;

commit 11199692d83dd3fe1511203024fb9853d176ec4c
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:02:48 2013 -0700

    mm: change signature of free_reserved_area() to fix building warnings
    
    Change signature of free_reserved_area() according to Russell King's
    suggestion to fix following build warnings:
    
      arch/arm/mm/init.c: In function 'mem_init':
      arch/arm/mm/init.c:603:2: warning: passing argument 1 of 'free_reserved_area' makes integer from pointer without a cast [enabled by default]
        free_reserved_area(__va(PHYS_PFN_OFFSET), swapper_pg_dir, 0, NULL);
        ^
      In file included from include/linux/mman.h:4:0,
                       from arch/arm/mm/init.c:15:
      include/linux/mm.h:1301:22: note: expected 'long unsigned int' but argument is of type 'void *'
       extern unsigned long free_reserved_area(unsigned long start, unsigned long end,
    
       mm/page_alloc.c: In function 'free_reserved_area':
    >> mm/page_alloc.c:5134:3: warning: passing argument 1 of 'virt_to_phys' makes pointer from integer without a cast [enabled by default]
       In file included from arch/mips/include/asm/page.h:49:0,
                        from include/linux/mmzone.h:20,
                        from include/linux/gfp.h:4,
                        from include/linux/mm.h:8,
                        from mm/page_alloc.c:18:
       arch/mips/include/asm/io.h:119:29: note: expected 'const volatile void *' but argument is of type 'long unsigned int'
       mm/page_alloc.c: In function 'free_area_init_nodes':
       mm/page_alloc.c:5030:34: warning: array subscript is below array bounds [-Warray-bounds]
    
    Also address some minor code review comments.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 9b973e0af9cb..268f2a94031b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -440,7 +440,8 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	free_reserved_area(start, end, POISON_FREE_INITMEM, "initrd");
+	free_reserved_area((void *)start, (void *)end, POISON_FREE_INITMEM,
+			   "initrd");
 }
 #endif
 

commit 20b4fb485227404329e41ad15588afad3df23050
Merge: b9394d8a657c ac3e3c5b1164
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 17:51:54 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull VFS updates from Al Viro,
    
    Misc cleanups all over the place, mainly wrt /proc interfaces (switch
    create_proc_entry to proc_create(), get rid of the deprecated
    create_proc_read_entry() in favor of using proc_create_data() and
    seq_file etc).
    
    7kloc removed.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (204 commits)
      don't bother with deferred freeing of fdtables
      proc: Move non-public stuff from linux/proc_fs.h to fs/proc/internal.h
      proc: Make the PROC_I() and PDE() macros internal to procfs
      proc: Supply a function to remove a proc entry by PDE
      take cgroup_open() and cpuset_open() to fs/proc/base.c
      ppc: Clean up scanlog
      ppc: Clean up rtas_flash driver somewhat
      hostap: proc: Use remove_proc_subtree()
      drm: proc: Use remove_proc_subtree()
      drm: proc: Use minor->index to label things, not PDE->name
      drm: Constify drm_proc_list[]
      zoran: Don't print proc_dir_entry data in debug
      reiserfs: Don't access the proc_dir_entry in r_open(), r_start() r_show()
      proc: Supply an accessor for getting the data from a PDE's parent
      airo: Use remove_proc_subtree()
      rtl8192u: Don't need to save device proc dir PDE
      rtl8187se: Use a dir under /proc/net/r8180/
      proc: Add proc_mkdir_data()
      proc: Move some bits from linux/proc_fs.h to linux/{of.h,signal.h,tty.h}
      proc: Move PDE_NET() to fs/proc/proc_net.c
      ...

commit 0f354869e3eca76d2e9bbe5798d17a85dc975f73
Author: Jiang Liu <liuj97@gmail.com>
Date:   Mon Apr 29 15:07:07 2013 -0700

    mm/MIPS: use free_highmem_page() to free highmem pages into buddy system
    
    Use helper function free_highmem_page() to free highmem pages into
    the buddy system.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Cong Wang <amwang@redhat.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 60f7c6122d6e..3d0346dbccf4 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -393,12 +393,8 @@ void __init mem_init(void)
 			SetPageReserved(page);
 			continue;
 		}
-		ClearPageReserved(page);
-		init_page_count(page);
-		__free_page(page);
-		totalhigh_pages++;
+		free_highmem_page(page);
 	}
-	totalram_pages += totalhigh_pages;
 	num_physpages += totalhigh_pages;
 #endif
 

commit 31605922613dfc59402cd6f3a1b019bc2138ff47
Author: Jiang Liu <liuj97@gmail.com>
Date:   Mon Apr 29 15:06:43 2013 -0700

    mm/MIPS: use common help functions to free reserved pages
    
    Use common help functions to free reserved pages.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 67929251286c..60f7c6122d6e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -77,10 +77,9 @@ EXPORT_SYMBOL_GPL(empty_zero_page);
 /*
  * Not static inline because used by IP27 special magic initialization code
  */
-unsigned long setup_zero_pages(void)
+void setup_zero_pages(void)
 {
-	unsigned int order;
-	unsigned long size;
+	unsigned int order, i;
 	struct page *page;
 
 	if (cpu_has_vce)
@@ -94,15 +93,10 @@ unsigned long setup_zero_pages(void)
 
 	page = virt_to_page((void *)empty_zero_page);
 	split_page(page, order);
-	while (page < virt_to_page((void *)(empty_zero_page + (PAGE_SIZE << order)))) {
-		SetPageReserved(page);
-		page++;
-	}
-
-	size = PAGE_SIZE << order;
-	zero_page_mask = (size - 1) & PAGE_MASK;
+	for (i = 0; i < (1 << order); i++, page++)
+		mark_page_reserved(page);
 
-	return 1UL << order;
+	zero_page_mask = ((PAGE_SIZE << order) - 1) & PAGE_MASK;
 }
 
 #ifdef CONFIG_MIPS_MT_SMTC
@@ -380,7 +374,7 @@ void __init mem_init(void)
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
 	totalram_pages += free_all_bootmem();
-	totalram_pages -= setup_zero_pages();	/* Setup zeroed pages.	*/
+	setup_zero_pages();	/* Setup zeroed pages.  */
 
 	reservedpages = ram = 0;
 	for (tmp = 0; tmp < max_low_pfn; tmp++)
@@ -440,11 +434,8 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 		struct page *page = pfn_to_page(pfn);
 		void *addr = phys_to_virt(PFN_PHYS(pfn));
 
-		ClearPageReserved(page);
-		init_page_count(page);
 		memset(addr, POISON_FREE_INITMEM, PAGE_SIZE);
-		__free_page(page);
-		totalram_pages++;
+		free_reserved_page(page);
 	}
 	printk(KERN_INFO "Freeing %s: %ldk freed\n", what, (end - begin) >> 10);
 }
@@ -452,18 +443,14 @@ void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	free_init_pages("initrd memory",
-			virt_to_phys((void *)start),
-			virt_to_phys((void *)end));
+	free_reserved_area(start, end, POISON_FREE_INITMEM, "initrd");
 }
 #endif
 
 void __init_refok free_initmem(void)
 {
 	prom_free_prom_memory();
-	free_init_pages("unused kernel memory",
-			__pa_symbol(&__init_begin),
-			__pa_symbol(&__init_end));
+	free_initmem_default(POISON_FREE_INITMEM);
 }
 
 #ifndef CONFIG_MIPS_PGD_C0_CONTEXT

commit 2f96b8c1d5d492c1d0457b253015330f844136f6
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 12 00:10:25 2013 +0100

    proc: Split kcore bits from linux/procfs.h into linux/kcore.h
    
    Split kcore bits from linux/procfs.h into linux/kcore.h.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    cc: linux-mips@linux-mips.org
    cc: sparclinux@vger.kernel.org
    cc: x86@kernel.org
    cc: linux-mm@kvack.org
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 67929251286c..60547b7fe2ff 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -29,6 +29,7 @@
 #include <linux/pfn.h>
 #include <linux/hardirq.h>
 #include <linux/gfp.h>
+#include <linux/kcore.h>
 
 #include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>

commit 7034228792cc561e79ff8600f02884bd4c80e287
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Jan 22 12:59:30 2013 +0100

    MIPS: Whitespace cleanup.
    
    Having received another series of whitespace patches I decided to do this
    once and for all rather than dealing with this kind of patches trickling
    in forever.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index be9acb2b959d..67929251286c 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -66,7 +66,7 @@
 
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
- * when needed.  This is necessary only on R4000 / R4400 SC and MC versions
+ * when needed.	 This is necessary only on R4000 / R4400 SC and MC versions
  * where we have to avoid VCED / VECI exceptions for good performance at
  * any price.  Since page is never written to after the initialization we
  * don't have to care about aliases on other CPUs.
@@ -380,7 +380,7 @@ void __init mem_init(void)
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
 	totalram_pages += free_all_bootmem();
-	totalram_pages -= setup_zero_pages();	/* Setup zeroed pages.  */
+	totalram_pages -= setup_zero_pages();	/* Setup zeroed pages.	*/
 
 	reservedpages = ram = 0;
 	for (tmp = 0; tmp < max_low_pfn; tmp++)

commit 485172b3dff29f888a88d3dba8caec5d589a4049
Author: David Daney <david.daney@cavium.com>
Date:   Tue Aug 14 11:08:01 2012 -0700

    MIPS: Align swapper_pg_dir to 64K for better TLB Refill code.
    
    We can save an instruction in the TLB Refill path for kernel mappings
    by aligning swapper_pg_dir on a 64K boundary.  The address of
    swapper_pg_dir can be generated with a single LUI instead of
    LUI/{D}ADDUI.
    
    The alignment of __init_end is bumped up to 64K so there are no holes
    between it and swapper_pg_dir, which is placed at the very beginning
    of .bss.
    
    The alignment of invalid_pmd_table and invalid_pte_table can be
    relaxed to PAGE_SIZE.  We do this by using __page_aligned_bss, which
    has the added benefit of eliminating alignment holes in .bss.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-arch@vger.kernel.org,
    Cc: linux-kernel@vger.kernel.org
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Patchwork: https://patchwork.linux-mips.org/patch/4220/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1a85ba92eb5c..be9acb2b959d 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -469,19 +469,20 @@ void __init_refok free_initmem(void)
 #ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 unsigned long pgd_current[NR_CPUS];
 #endif
-/*
- * On 64-bit we've got three-level pagetables with a slightly
- * different layout ...
- */
-#define __page_aligned(order) __attribute__((__aligned__(PAGE_SIZE<<order)))
 
 /*
  * gcc 3.3 and older have trouble determining that PTRS_PER_PGD and PGD_ORDER
  * are constants.  So we use the variants from asm-offset.h until that gcc
  * will officially be retired.
+ *
+ * Align swapper_pg_dir in to 64K, allows its address to be loaded
+ * with a single LUI instruction in the TLB handlers.  If we used
+ * __aligned(64K), its size would get rounded up to the alignment
+ * size, and waste space.  So we place it in its own section and align
+ * it in the linker script.
  */
-pgd_t swapper_pg_dir[_PTRS_PER_PGD] __page_aligned(_PGD_ORDER);
+pgd_t swapper_pg_dir[_PTRS_PER_PGD] __section(.bss..swapper_pg_dir);
 #ifndef __PAGETABLE_PMD_FOLDED
-pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned(PMD_ORDER);
+pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned_bss;
 #endif
-pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned(PTE_ORDER);
+pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned_bss;

commit 9c02048fcdf3cd0ab2d04a0a01de100582db28e1
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:15 2011 +0800

    mips: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 3b3ffd439cd7..1a85ba92eb5c 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -207,21 +207,21 @@ void copy_user_highpage(struct page *to, struct page *from,
 {
 	void *vfrom, *vto;
 
-	vto = kmap_atomic(to, KM_USER1);
+	vto = kmap_atomic(to);
 	if (cpu_has_dc_aliases &&
 	    page_mapped(from) && !Page_dcache_dirty(from)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
 		kunmap_coherent();
 	} else {
-		vfrom = kmap_atomic(from, KM_USER0);
+		vfrom = kmap_atomic(from);
 		copy_page(vto, vfrom);
-		kunmap_atomic(vfrom, KM_USER0);
+		kunmap_atomic(vfrom);
 	}
 	if ((!cpu_has_ic_fills_f_dc) ||
 	    pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
 		flush_data_cache_page((unsigned long)vto);
-	kunmap_atomic(vto, KM_USER1);
+	kunmap_atomic(vto);
 	/* Make sure this page is cleared on other CPU's too before using it */
 	smp_wmb();
 }

commit 43064c0c8ee2ada8edd421520c633584d648e100
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Tue Nov 22 14:38:03 2011 +0000

    MIPS: Handle initmem in systems with kernel not in add_memory_region() mem
    
    This patch addresses a couple of related problems:
    
    1) The kernel may reside in physical memory outside of the ranges set
       by plat_mem_setup().  If this is the case, init mem cannot be
       reused as it resides outside of the range of pages that the kernel
       memory allocators control.
    
    2) initrd images might be loaded in physical memory outside of the
       ranges set by plat_mem_setup().  The memory likewise cannot be
       reused.  The patch doesn't handle this specific case, but the
       infrastructure is useful for future patches that do.
    
    The crux of the problem is that there are memory regions that need be
    memory_present(), but that cannot be free_bootmem() at the time of
    arch_mem_init().  We create a new type of memory (BOOT_MEM_INIT_RAM)
    for use with add_memory_region().  Then arch_mem_init() adds the init
    mem with this type if the init mem is not already covered by existing
    ranges.
    
    When memory is being freed into the bootmem allocator, we skip the
    BOOT_MEM_INIT_RAM ranges so they are not clobbered, but we do signal
    them as memory_present().  This way when they are later freed, the
    necessary memory manager structures have initialized and the Sparse
    allocater is prevented from crashing.
    
    The Octeon specific code that handled this case is removed, because
    the new general purpose code handles the case.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/1988/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index b7ebc4fa89bc..3b3ffd439cd7 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -304,9 +304,14 @@ int page_is_ram(unsigned long pagenr)
 	for (i = 0; i < boot_mem_map.nr_map; i++) {
 		unsigned long addr, end;
 
-		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+		switch (boot_mem_map.map[i].type) {
+		case BOOT_MEM_RAM:
+		case BOOT_MEM_INIT_RAM:
+			break;
+		default:
 			/* not usable memory */
 			continue;
+		}
 
 		addr = PFN_UP(boot_mem_map.map[i].addr);
 		end = PFN_DOWN(boot_mem_map.map[i].addr +
@@ -379,7 +384,7 @@ void __init mem_init(void)
 
 	reservedpages = ram = 0;
 	for (tmp = 0; tmp < max_low_pfn; tmp++)
-		if (page_is_ram(tmp)) {
+		if (page_is_ram(tmp) && pfn_valid(tmp)) {
 			ram++;
 			if (PageReserved(pfn_to_page(tmp)))
 				reservedpages++;

commit 464fd83e841a16f4ea1325b33eb08170ef5cd1f4
Author: Kevin Cernekee <cernekee@gmail.com>
Date:   Wed Jan 5 23:31:30 2011 -0800

    MIPS: Limit fixrange_init() to the FIXMAP region
    
    fixrange_init() allocates page tables for all addresses higher than
    FIXADDR_TOP.  On processors that override the default FIXADDR_TOP
    address of 0xfffe_0000, this can consume up to 4 pages (1 page per 4MB)
    for pgd's that are never used.
    
    Signed-off-by: Kevin Cernekee <cernekee@gmail.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/1980/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 11689e1a2924..b7ebc4fa89bc 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -277,11 +277,11 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 	k = __pmd_offset(vaddr);
 	pgd = pgd_base + i;
 
-	for ( ; (i < PTRS_PER_PGD) && (vaddr != end); pgd++, i++) {
+	for ( ; (i < PTRS_PER_PGD) && (vaddr < end); pgd++, i++) {
 		pud = (pud_t *)pgd;
-		for ( ; (j < PTRS_PER_PUD) && (vaddr != end); pud++, j++) {
+		for ( ; (j < PTRS_PER_PUD) && (vaddr < end); pud++, j++) {
 			pmd = (pmd_t *)pud;
-			for (; (k < PTRS_PER_PMD) && (vaddr != end); pmd++, k++) {
+			for (; (k < PTRS_PER_PMD) && (vaddr < end); pmd++, k++) {
 				if (pmd_none(*pmd)) {
 					pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
 					set_pmd(pmd, __pmd((unsigned long)pte));

commit b6da0ffb09ad4468e6749488909f04f1efac5de3
Author: Kevin Cernekee <cernekee@gmail.com>
Date:   Sun May 30 00:32:51 2010 -0700

    MIPS: pfn_valid() is broken on low memory HIGHMEM systems
    
    pfn_valid() compares the PFN to max_mapnr:
    
            __pfn >= min_low_pfn && __pfn < max_mapnr;
    
    On HIGHMEM kernels, highend_pfn is used to set the value of max_mapnr.
    Unfortunately, highend_pfn is left at zero if the system does not
    actually have enough RAM to reach into the HIGHMEM range.  This causes
    pfn_valid() to always return false, and when debug checks are enabled
    the kernel will fail catastrophically:
    
    Memory: 22432k/32768k available (2249k kernel code, 10336k reserved, 653k data, 1352k init, 0k highmem)
    NR_IRQS:128
    kfree_debugcheck: out of range ptr 81c02900h.
    Kernel bug detected[#1]:
    Cpu 0
    $ 0   : 00000000 10008400 00000034 00000000
    $ 4   : 8003e160 802a0000 8003e160 00000000
    $ 8   : 00000000 0000003e 00000747 00000747
    ...
    
    On such a configuration, max_low_pfn should be used to set max_mapnr.
    
    This was seen on 2.6.34.
    
    Signed-off-by: Kevin Cernekee <cernekee@gmail.com>
    To: Ralf Baechle <ralf@linux-mips.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/1992/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1aadeb42c5a5..11689e1a2924 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -368,7 +368,7 @@ void __init mem_init(void)
 #ifdef CONFIG_DISCONTIGMEM
 #error "CONFIG_HIGHMEM and CONFIG_DISCONTIGMEM dont work together yet"
 #endif
-	max_mapnr = highend_pfn;
+	max_mapnr = highend_pfn ? highend_pfn : max_low_pfn;
 #else
 	max_mapnr = max_low_pfn;
 #endif

commit 1c395176962176660bb108f90e97e1686cfe0d85
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 24 17:11:58 2011 -0700

    mm: now that all old mmu_gather code is gone, remove the storage
    
    Fold all the mmu_gather rework patches into one for submission
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reported-by: Hugh Dickins <hughd@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 279599e9a779..1aadeb42c5a5 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -64,8 +64,6 @@
 
 #endif /* CONFIG_MIPS_MT_SMTC */
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
-
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
  * when needed.  This is necessary only on R4000 / R4400 SC and MC versions

commit d3ce0e98b7fe17bb1dec9f6d7c50213db01e7189
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Mon Jan 24 14:51:37 2011 -0800

    MIPS: Fix GCC-4.6 'set but not used' warning in arch/mips/mm/init.c
    
    Under some combinations of CONFIG_*, lastpfn in page_is_ram is 'set
    but not used'.  Mark it as __maybe_unused to quiet the warning/error.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/2033/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 2efcbd24c82f..279599e9a779 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -324,7 +324,7 @@ int page_is_ram(unsigned long pagenr)
 void __init paging_init(void)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
-	unsigned long lastpfn;
+	unsigned long lastpfn __maybe_unused;
 
 	pagetable_init();
 

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 12539af38a99..2efcbd24c82f 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -28,6 +28,7 @@
 #include <linux/proc_fs.h>
 #include <linux/pfn.h>
 #include <linux/hardirq.h>
+#include <linux/gfp.h>
 
 #include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>

commit 46bbffad54bd48bb809f2691c1970a79a588976b
Merge: 85fe20bfd415 c1fd1b43831f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 28 10:38:45 2010 -0800

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      x86, mm: Unify kernel_physical_mapping_init() API
      x86, mm: Allow highmem user page tables to be disabled at boot time
      x86: Do not reserve brk for DMI if it's not going to be used
      x86: Convert tlbstate_lock to raw_spinlock
      x86: Use the generic page_is_ram()
      x86: Remove BIOS data range from e820
      Move page_is_ram() declaration to mm.h
      Generic page_is_ram: use __weak
      resources: introduce generic page_is_ram()

commit 6dd9344cfc41bcc60a01cdc828cb278be7a10e01
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Wed Feb 10 15:12:47 2010 -0800

    MIPS: Implement Read Inhibit/eXecute Inhibit
    
    The SmartMIPS ASE specifies how Read Inhibit (RI) and eXecute Inhibit
    (XI) bits in the page tables work.  The upper two bits of EntryLo{0,1}
    are RI and XI when the feature is enabled in the PageGrain register.
    SmartMIPS only covers 32-bit systems.  Cavium Octeon+ extends this to
    64-bit systems by continuing to place the RI and XI bits in the top of
    EntryLo even when EntryLo is 64-bits wide.
    
    Because we need to carry the RI and XI bits in the PTE, the layout of
    the PTE is changed.  There is a two instruction overhead in the TLB
    refill hot path to get the EntryLo bits into the proper position.
    Also the TLB load exception has to probe the TLB to check if RI or XI
    caused the exception.
    
    Also of note is that the layout of the PTE bits is done at compile and
    runtime rather than statically.  In the 32-bit case this allows for
    the same number of PFN bits as before the patch as the _PAGE_HUGE is
    not supported in 32-bit kernels (we have _PAGE_NO_EXEC and
    _PAGE_NO_READ instead of _PAGE_READ and _PAGE_HUGE).
    
    The patch is tested on Cavium Octeon+, but should also work on 32-bit
    systems with the Smart-MIPS ASE.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    Patchwork: http://patchwork.linux-mips.org/patch/952/
    Patchwork: http://patchwork.linux-mips.org/patch/956/
    Patchwork: http://patchwork.linux-mips.org/patch/962/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 3c5b7de10af5..f34c26439a32 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -143,7 +143,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 #if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
 	entrylo = pte.pte_high;
 #else
-	entrylo = pte_val(pte) >> 6;
+	entrylo = pte_to_entrylo(pte_val(pte));
 #endif
 
 	ENTER_CRITICAL(flags);

commit 325f8a0a31df567dbafafc48f8e60f3c1f101a46
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Fri Dec 4 13:52:36 2009 -0800

    MIPS: Two-level pagetables for 64-bit kernels with 64KB pages.
    
    For 64-bit kernels with 64KB pages and two level page tables, there are
    42 bits worth of virtual address space This is larger than the 40 bits of
    virtual address space obtained with the default 4KB Page size and three
    levels, so there are no draw backs for using two level tables with this
    configuration.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: http://patchwork.linux-mips.org/patch/761/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1651942f7feb..3c5b7de10af5 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -477,7 +477,7 @@ unsigned long pgd_current[NR_CPUS];
  * will officially be retired.
  */
 pgd_t swapper_pg_dir[_PTRS_PER_PGD] __page_aligned(_PGD_ORDER);
-#ifdef CONFIG_64BIT
+#ifndef __PAGETABLE_PMD_FOLDED
 pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned(PMD_ORDER);
 #endif
 pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned(PTE_ORDER);

commit b7e56edba4b02f2079042c326a8cd72a44635817
Merge: 13ca0fcaa33f b0483e78e5c4
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Feb 17 18:27:37 2010 +0100

    Merge branch 'linus' into x86/mm
    
    x86/mm is on 32-rc4 and missing the spinlock namespace changes which
    are needed for further commits into this topic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit 61ef2489dbf587258526cfd4ebf4bba3b079f401
Author: Wu Fengguang <fengguang.wu@intel.com>
Date:   Fri Jan 22 16:16:19 2010 +0800

    resources: introduce generic page_is_ram()
    
    It's based on walk_system_ram_range(), for archs that don't have
    their own page_is_ram().
    
    The static verions in MIPS and SCORE are also made global.
    
    v4: prefer plain 1 instead of PAGE_IS_RAM (H. Peter Anvin)
    v3: add comment (KAMEZAWA Hiroyuki)
        "AFAIK, this "System RAM" information has been used for kdump to
        grab valid memory area and seems good for the kernel itself."
    v2: add PAGE_IS_RAM macro (Américo Wang)
    
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Américo Wang <xiyou.wangcong@gmail.com>
    Cc: linux-mips@linux-mips.org
    Cc: Yinghai Lu <yinghai@kernel.org>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Wu Fengguang <fengguang.wu@intel.com>
    LKML-Reference: <20100122081619.GA6431@localhost>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 15aa1902a788..4d72aabe8352 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -294,7 +294,7 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 }
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-static int __init page_is_ram(unsigned long pagenr)
+int page_is_ram(unsigned long pagenr)
 {
 	int i;
 

commit 4b529401c5089cf33f7165607cbc2fde43357bfb
Author: Andreas Fenkart <andreas.fenkart@streamunlimited.com>
Date:   Fri Jan 8 14:42:31 2010 -0800

    mm: make totalhigh_pages unsigned long
    
    Makes it consistent with the extern declaration, used when CONFIG_HIGHMEM
    is set Removes redundant casts in printout messages
    
    Signed-off-by: Andreas Fenkart <andreas.fenkart@streamunlimited.com>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 9e8d00389eef..1651942f7feb 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -424,7 +424,7 @@ void __init mem_init(void)
 	       reservedpages << (PAGE_SHIFT-10),
 	       datasize >> 10,
 	       initsize >> 10,
-	       (unsigned long) (totalhigh_pages << (PAGE_SHIFT-10)));
+	       totalhigh_pages << (PAGE_SHIFT-10));
 }
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 

commit 82622284dd2f8791f9759f3cef601520a8bc63b2
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Wed Oct 14 12:16:56 2009 -0700

    MIPS: Put PGD in C0_CONTEXT for 64-bit R2 processors.
    
    Processors that support the mips64r2 ISA can in four instructions
    convert a shifted PGD pointer stored in the upper bits of c0_context
    into a usable pointer.  By doing this we save a memory load and
    associated potential cache miss in the TLB exception handlers.
    
    Since the upper bits of c0_context were holding the CPU number, we
    move this to the upper bits of c0_xcontext which doesn't have enough
    bits to hold the PGD pointer, but has plenty for the CPU number.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 8d1f4f363049..9e8d00389eef 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -462,7 +462,9 @@ void __init_refok free_initmem(void)
 			__pa_symbol(&__init_end));
 }
 
+#ifndef CONFIG_MIPS_PGD_C0_CONTEXT
 unsigned long pgd_current[NR_CPUS];
+#endif
 /*
  * On 64-bit we've got three-level pagetables with a slightly
  * different layout ...

commit 0f334a3e8c3586f81286345eb077ed32b375e8d6
Author: Kevin Cernekee <cernekee@gmail.com>
Date:   Mon Sep 7 11:11:31 2009 -0700

    MIPS: Fix machine check exception in kmap_coherent()
    
    On an SMP system with cache aliases, the following sequence of events may
    happen:
    
    1) copy_user_highpage() runs on CPU0, invoking kmap_coherent() to create a
       temporary mapping in the fixmap region
    2) copy_page() starts on CPU0
    3) CPU1 sends CPU0 an IPI asking CPU0 to run local_r4k_flush_cache_page()
    4) CPU0 takes the interrupt, interrupting copy_page()
    5) local_r4k_flush_cache_page() on CPU0 calls kmap_coherent() again
    6) The second invocation of kmap_coherent() on CPU0 tries to use the
       same fixmap virtual address that was being used by copy_user_highpage()
    7) CPU0 throws a machine check exception for the TLB address conflict
    
    Fixed by creating an extra set of fixmap entries for use in interrupt
    handlers.  This prevents fixmap VA conflicts between copy_user_highpage()
    running in user context, and local_r4k_flush_cache_page() invoked from an
    SMP IPI.
    
    Signed-off-by: Kevin Cernekee <cernekee@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 15aa1902a788..8d1f4f363049 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -27,6 +27,7 @@
 #include <linux/swap.h>
 #include <linux/proc_fs.h>
 #include <linux/pfn.h>
+#include <linux/hardirq.h>
 
 #include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>
@@ -132,7 +133,10 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 	inc_preempt_count();
 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
 #ifdef CONFIG_MIPS_MT_SMTC
-	idx += FIX_N_COLOURS * smp_processor_id();
+	idx += FIX_N_COLOURS * smp_processor_id() +
+		(in_interrupt() ? (FIX_N_COLOURS * NR_CPUS) : 0);
+#else
+	idx += in_interrupt() ? FIX_N_COLOURS : 0;
 #endif
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, PAGE_KERNEL);

commit 3089aa1b0c07fb7c48f9829c619f50198307789d
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Sep 22 16:45:48 2009 -0700

    kcore: use registerd physmem information
    
    For /proc/kcore, each arch registers its memory range by kclist_add().
    In usual,
    
            - range of physical memory
            - range of vmalloc area
            - text, etc...
    
    are registered but "range of physical memory" has some troubles.  It
    doesn't updated at memory hotplug and it tend to include unnecessary
    memory holes.  Now, /proc/iomem (kernel/resource.c) includes required
    physical memory range information and it's properly updated at memory
    hotplug.  Then, it's good to avoid using its own code(duplicating
    information) and to rebuild kclist for physical memory based on
    /proc/iomem.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Jiri Slaby <jirislaby@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4a83da499e2d..15aa1902a788 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -352,7 +352,6 @@ void __init paging_init(void)
 	free_area_init_nodes(max_zone_pfns);
 }
 
-static struct kcore_list kcore_mem;
 #ifdef CONFIG_64BIT
 static struct kcore_list kcore_kseg0;
 #endif
@@ -412,7 +411,6 @@ void __init mem_init(void)
 		kclist_add(&kcore_kseg0, (void *) CKSEG0,
 				0x80000000 - 4, KCORE_TEXT);
 #endif
-	kclist_add(&kcore_mem, __va(0), max_low_pfn << PAGE_SHIFT, KCORE_RAM);
 
 	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
 	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",

commit a0614da88b67ffa3dbcc0d40b817e682c7c4a0ee
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Sep 22 16:45:44 2009 -0700

    kcore: register vmalloc area in generic way
    
    For /proc/kcore, vmalloc areas are registered per arch.  But, all of them
    registers same range of [VMALLOC_START...VMALLOC_END) This patch unifies
    them.  By this.  archs which have no kclist_add() hooks can see vmalloc
    area correctly.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: WANG Cong <xiyou.wangcong@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index f8661985bff6..4a83da499e2d 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -352,7 +352,7 @@ void __init paging_init(void)
 	free_area_init_nodes(max_zone_pfns);
 }
 
-static struct kcore_list kcore_mem, kcore_vmalloc;
+static struct kcore_list kcore_mem;
 #ifdef CONFIG_64BIT
 static struct kcore_list kcore_kseg0;
 #endif
@@ -413,8 +413,6 @@ void __init mem_init(void)
 				0x80000000 - 4, KCORE_TEXT);
 #endif
 	kclist_add(&kcore_mem, __va(0), max_low_pfn << PAGE_SHIFT, KCORE_RAM);
-	kclist_add(&kcore_vmalloc, (void *)VMALLOC_START,
-		   VMALLOC_END-VMALLOC_START, KCORE_VMALLOC);
 
 	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
 	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",

commit c30bb2a25fcfde6157e6154a32c14686fb0bedbe
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Sep 22 16:45:43 2009 -0700

    kcore: add kclist types
    
    Presently, kclist_add() only eats start address and size as its arguments.
    Considering to make kclist dynamically reconfigulable, it's necessary to
    know which kclists are for System RAM and which are not.
    
    This patch add kclist types as
      KCORE_RAM
      KCORE_VMALLOC
      KCORE_TEXT
      KCORE_OTHER
    
    This "type" is used in a patch following this for detecting KCORE_RAM.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: WANG Cong <xiyou.wangcong@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 1f4ee4797a6e..f8661985bff6 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -409,11 +409,12 @@ void __init mem_init(void)
 	if ((unsigned long) &_text > (unsigned long) CKSEG0)
 		/* The -4 is a hack so that user tools don't have to handle
 		   the overflow.  */
-		kclist_add(&kcore_kseg0, (void *) CKSEG0, 0x80000000 - 4);
+		kclist_add(&kcore_kseg0, (void *) CKSEG0,
+				0x80000000 - 4, KCORE_TEXT);
 #endif
-	kclist_add(&kcore_mem, __va(0), max_low_pfn << PAGE_SHIFT);
+	kclist_add(&kcore_mem, __va(0), max_low_pfn << PAGE_SHIFT, KCORE_RAM);
 	kclist_add(&kcore_vmalloc, (void *)VMALLOC_START,
-		   VMALLOC_END-VMALLOC_START);
+		   VMALLOC_END-VMALLOC_START, KCORE_VMALLOC);
 
 	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
 	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",

commit cc013a88906bad9d2832d6316de1c7dbc1c2a794
Author: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
Date:   Mon Sep 21 17:02:36 2009 -0700

    arches: drop superfluous casts in nr_free_pages() callers
    
    Commit 96177299416dbccb73b54e6b344260154a445375 ("Drop free_pages()")
    modified nr_free_pages() to return 'unsigned long' instead of 'unsigned
    int'.  This made the casts to 'unsigned long' in most callers superfluous,
    so remove them.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Kyle McMartin <kyle@mcmartin.ca>
    Acked-by: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <zankel@tensilica.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 38c79c55b060..1f4ee4797a6e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -417,7 +417,7 @@ void __init mem_init(void)
 
 	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
 	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",
-	       (unsigned long) nr_free_pages() << (PAGE_SHIFT-10),
+	       nr_free_pages() << (PAGE_SHIFT-10),
 	       ram << (PAGE_SHIFT-10),
 	       codesize >> 10,
 	       reservedpages << (PAGE_SHIFT-10),

commit e0cc87f59490d7d62a8ab2a76498dc8a2b64927a
Author: Wu Fei <at.wufei@gmail.com>
Date:   Thu Sep 3 22:29:53 2009 +0800

    MIPS: Shrink the size of tlb handler
    
    By combining swapper_pg_dir and module_pg_dir, several if conditions
    can be eliminated from the tlb exception handler. The reason they
    can be combined is that, the effective virtual address of vmalloc
    returned is at the bottom, and of module_alloc returned is at the
    top. It also fixes the bug in vmalloc(), which happens when its
    return address is not covered by the first pgd.
    
    Signed-off-by: Wu Fei <at.wufei@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 0e820508ff23..38c79c55b060 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -475,9 +475,6 @@ unsigned long pgd_current[NR_CPUS];
  */
 pgd_t swapper_pg_dir[_PTRS_PER_PGD] __page_aligned(_PGD_ORDER);
 #ifdef CONFIG_64BIT
-#ifdef MODULE_START
-pgd_t module_pg_dir[PTRS_PER_PGD] __page_aligned(PGD_ORDER);
-#endif
 pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned(PMD_ORDER);
 #endif
 pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned(PTE_ORDER);

commit 631330f5847b3f8a7ea67d689e9f7c56833ccaa6
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Jun 19 14:05:26 2009 +0100

    MIPS: Build fix - include <linux/smp.h> into all smp_processor_id() users.
    
    Some of the were relying into smp.h being dragged in by another header
    which of course is fragile.  <asm/cpu-info.h> uses smp_processor_id()
    only in macros and including smp.h there leads to an include loop, so
    don't change cpu-info.h.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index c5511294a9ee..0e820508ff23 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -13,6 +13,7 @@
 #include <linux/module.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
+#include <linux/smp.h>
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/string.h>

commit bb86bf28aec6d0a207ae09f38a43e94133d4d6db
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Apr 25 11:25:34 2009 +0200

    MIPS: Fix highmem.
    
    Commit 351336929ccf222ae38ff0cb7a8dd5fd5c6236a0 (kernel.org) rsp.
    b3594a089f1c17ff919f8f78505c3f20e1f6f8ce (linux-mips.org):
    
    > From: Chris Dearman <chris@mips.com>
    > Date: Wed, 19 Sep 2007 00:58:24 +0100
    > Subject: [PATCH] [MIPS] Allow setting of the cache attribute at run time.
    >
    > Slightly tacky, but there is a precedent in the sparc archirecture code.
    
    introduces the variable _page_cachable_default, which defaults to zero and.
    is used to create the prototype PTE for __kmap_atomic in
    arch/mips/mm/init.c:kmap_init before initialization in
    arch/mips/mm/c-r4k.c:coherency_setup, so the default value of 0 will be
    used as the CCA of kmap atomic pages which on many processors is not a
    defined CCA value and may result in writes to kmap_atomic pages getting
    corrupted.  Debugged by Jon Fraser (jfraser@broadcom.com).
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index d9348946a19e..c5511294a9ee 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -104,14 +104,6 @@ unsigned long setup_zero_pages(void)
 	return 1UL << order;
 }
 
-/*
- * These are almost like kmap_atomic / kunmap_atmic except they take an
- * additional address argument as the hint.
- */
-
-#define kmap_get_fixmap_pte(vaddr)					\
-	pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr), (vaddr)), (vaddr)), (vaddr))
-
 #ifdef CONFIG_MIPS_MT_SMTC
 static pte_t *kmap_coherent_pte;
 static void __init kmap_coherent_init(void)
@@ -264,24 +256,6 @@ void copy_from_user_page(struct vm_area_struct *vma,
 	}
 }
 
-#ifdef CONFIG_HIGHMEM
-unsigned long highstart_pfn, highend_pfn;
-
-pte_t *kmap_pte;
-pgprot_t kmap_prot;
-
-static void __init kmap_init(void)
-{
-	unsigned long kmap_vstart;
-
-	/* cache the first kmap pte */
-	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
-	kmap_pte = kmap_get_fixmap_pte(kmap_vstart);
-
-	kmap_prot = PAGE_KERNEL;
-}
-#endif /* CONFIG_HIGHMEM */
-
 void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)
 {

commit b72b7092f8f5f0729cc9f0868997351f21dbc5cd
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Mar 30 14:49:44 2009 +0200

    MIPS: Use BUG_ON() where possible.
    
    Based on original patch by Stoyan Gaydarov <stoyboyker@gmail.com> which
    missed a few places.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 137c14bafd6b..d9348946a19e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -307,8 +307,7 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 				if (pmd_none(*pmd)) {
 					pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
 					set_pmd(pmd, __pmd((unsigned long)pte));
-					if (pte != pte_offset_kernel(pmd, 0))
-						BUG();
+					BUG_ON(pte != pte_offset_kernel(pmd, 0));
 				}
 				vaddr += PMD_SIZE;
 			}

commit 497d2adcbf50bccec6c56620d61d77429d23993a
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Jun 6 14:23:06 2008 +0100

    [MIPS] Export empty_zero_page for sake of the ext4 module.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index ecd562d2c348..137c14bafd6b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -71,6 +71,7 @@ DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
  * don't have to care about aliases on other CPUs.
  */
 unsigned long empty_zero_page, zero_page_mask;
+EXPORT_SYMBOL_GPL(empty_zero_page);
 
 /*
  * Not static inline because used by IP27 special magic initialization code

commit e698d66b813bbe38940d2d80a2488214bdaf96b6
Author: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
Date:   Wed Apr 2 03:58:36 2008 +0400

    [MIPS] unexport copy_from_user_page()
    
    No users for the copy_from_user_page() routine exist outside of the
    core kernel code. Therefore, EXPORT_SYMBOL(copy_from_user_page) is
    useless, and this patch removes it.
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index c1b197815283..ecd562d2c348 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -263,9 +263,6 @@ void copy_from_user_page(struct vm_area_struct *vma,
 	}
 }
 
-EXPORT_SYMBOL(copy_from_user_page);
-
-
 #ifdef CONFIG_HIGHMEM
 unsigned long highstart_pfn, highend_pfn;
 

commit 715243e6c3e8e74202eb663a8e45880ff6a1dff7
Author: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
Date:   Wed Apr 2 03:58:35 2008 +0400

    [MIPS] unexport copy_to_user_page()
    
    The copy_to_user_page() function is called only in the core kernel
    code. Therefore, there is no need to export it. This patch removes
    EXPORT_SYMBOL(copy_to_user_page).
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 527d326bfa15..c1b197815283 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -247,8 +247,6 @@ void copy_to_user_page(struct vm_area_struct *vma,
 		flush_cache_page(vma, vaddr, page_to_pfn(page));
 }
 
-EXPORT_SYMBOL(copy_to_user_page);
-
 void copy_from_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)

commit 11327ef77f8b6104bd4a855bbe783cce84416b03
Author: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
Date:   Wed Apr 2 03:58:34 2008 +0400

    [MIPS] unexport copy_user_highpage()
    
    The copy_user_highpage() routine has no users outside of the
    core kernel code, so exporting this symbol is pointless.
    This patch removes EXPORT_SYMBOL(copy_user_highpage).
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 05ac6c6123ca..527d326bfa15 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -229,8 +229,6 @@ void copy_user_highpage(struct page *to, struct page *from,
 	smp_wmb();
 }
 
-EXPORT_SYMBOL(copy_user_highpage);
-
 void copy_to_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)

commit 39b8d5254246ac56342b72f812255c8f7a74dca9
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Apr 28 17:14:26 2008 +0100

    [MIPS] Add support for MIPS CMP platform.
    
    Signed-off-by: Chris Dearman <chris@mips.com>
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 235833af3a8b..05ac6c6123ca 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -221,7 +221,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 		copy_page(vto, vfrom);
 		kunmap_atomic(vfrom, KM_USER0);
 	}
-	if (((vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc) ||
+	if ((!cpu_has_ic_fills_f_dc) ||
 	    pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
 		flush_data_cache_page((unsigned long)vto);
 	kunmap_atomic(vto, KM_USER1);

commit 962f480e0f9024ecdcfe2ba1d216c038ee328ced
Author: Chris Dearman <chris@mips.com>
Date:   Wed Sep 19 00:46:32 2007 +0100

    [MIPS] All MIPS32 processors support64-bit physical addresses.
    
    Still, only the 4K may actually implement it.
    
    Signed-off-by: Chris Dearman <chris@mips.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index c7aed133d11d..235833af3a8b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -142,7 +142,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 #endif
 	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
 	pte = mk_pte(page, PAGE_KERNEL);
-#if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32_R1)
+#if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32)
 	entrylo = pte.pte_high;
 #else
 	entrylo = pte_val(pte) >> 6;

commit 9a74b3eb22f2d67a5681301f52aca5b7703382c8
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Feb 16 22:34:25 2008 +0000

    [MIPS] Fix buggy invocations of kmap_coherent()
    
    kmap_coherent will only work correctly if the page it is called on is
    not marked dirty.  If it's dirty the kernel address of the page should
    be used instead of a temporary mapping.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 480dec04f552..c7aed133d11d 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -211,7 +211,8 @@ void copy_user_highpage(struct page *to, struct page *from,
 	void *vfrom, *vto;
 
 	vto = kmap_atomic(to, KM_USER1);
-	if (cpu_has_dc_aliases && page_mapped(from)) {
+	if (cpu_has_dc_aliases &&
+	    page_mapped(from) && !Page_dcache_dirty(from)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
 		kunmap_coherent();
@@ -234,7 +235,8 @@ void copy_to_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)
 {
-	if (cpu_has_dc_aliases && page_mapped(page)) {
+	if (cpu_has_dc_aliases &&
+	    page_mapped(page) && !Page_dcache_dirty(page)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
 		kunmap_coherent();
@@ -253,7 +255,8 @@ void copy_from_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)
 {
-	if (cpu_has_dc_aliases && page_mapped(page)) {
+	if (cpu_has_dc_aliases &&
+	    page_mapped(page) && !Page_dcache_dirty(page)) {
 		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
 		kunmap_coherent();

commit cce335ae47e231398269fb05fa48e0e9cbf289e0
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Nov 3 02:05:43 2007 +0000

    [MIPS] 64-bit Sibyte kernels need DMA32.
    
    Sibyte SOCs only have 32-bit PCI.  Due to the sparse use of the address
    space only the first 1GB of memory is mapped at physical addresses
    below 1GB.  If a system has more than 1GB of memory 32-bit DMA will
    not be able to reach all of it.
    
    For now this patch is good enough to keep Sibyte users happy but it seems
    eventually something like swiotlb will be needed for Sibyte.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index ec3b9e9f30f4..480dec04f552 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -347,11 +347,8 @@ static int __init page_is_ram(unsigned long pagenr)
 
 void __init paging_init(void)
 {
-	unsigned long zones_size[MAX_NR_ZONES] = { 0, };
-#ifndef CONFIG_FLATMEM
-	unsigned long zholes_size[MAX_NR_ZONES] = { 0, };
-	unsigned long i, j, pfn;
-#endif
+	unsigned long max_zone_pfns[MAX_NR_ZONES];
+	unsigned long lastpfn;
 
 	pagetable_init();
 
@@ -361,35 +358,27 @@ void __init paging_init(void)
 	kmap_coherent_init();
 
 #ifdef CONFIG_ZONE_DMA
-	if (min_low_pfn < MAX_DMA_PFN && MAX_DMA_PFN <= max_low_pfn) {
-		zones_size[ZONE_DMA] = MAX_DMA_PFN - min_low_pfn;
-		zones_size[ZONE_NORMAL] = max_low_pfn - MAX_DMA_PFN;
-	} else if (max_low_pfn < MAX_DMA_PFN)
-		zones_size[ZONE_DMA] = max_low_pfn - min_low_pfn;
-	else
+	max_zone_pfns[ZONE_DMA] = MAX_DMA_PFN;
 #endif
-	zones_size[ZONE_NORMAL] = max_low_pfn - min_low_pfn;
-
+#ifdef CONFIG_ZONE_DMA32
+	max_zone_pfns[ZONE_DMA32] = MAX_DMA32_PFN;
+#endif
+	max_zone_pfns[ZONE_NORMAL] = max_low_pfn;
+	lastpfn = max_low_pfn;
 #ifdef CONFIG_HIGHMEM
-	zones_size[ZONE_HIGHMEM] = highend_pfn - highstart_pfn;
+	max_zone_pfns[ZONE_HIGHMEM] = highend_pfn;
+	lastpfn = highend_pfn;
 
-	if (cpu_has_dc_aliases && zones_size[ZONE_HIGHMEM]) {
+	if (cpu_has_dc_aliases && max_low_pfn != highend_pfn) {
 		printk(KERN_WARNING "This processor doesn't support highmem."
-		       " %ldk highmem ignored\n", zones_size[ZONE_HIGHMEM]);
-		zones_size[ZONE_HIGHMEM] = 0;
+		       " %ldk highmem ignored\n",
+		       (highend_pfn - max_low_pfn) << (PAGE_SHIFT - 10));
+		max_zone_pfns[ZONE_HIGHMEM] = max_low_pfn;
+		lastpfn = max_low_pfn;
 	}
 #endif
 
-#ifdef CONFIG_FLATMEM
-	free_area_init(zones_size);
-#else
-	pfn = min_low_pfn;
-	for (i = 0; i < MAX_NR_ZONES; i++)
-		for (j = 0; j < zones_size[i]; j++, pfn++)
-			if (!page_is_ram(pfn))
-				zholes_size[i]++;
-	free_area_init_node(0, NODE_DATA(0), zones_size, 0, zholes_size);
-#endif
+	free_area_init_nodes(max_zone_pfns);
 }
 
 static struct kcore_list kcore_mem, kcore_vmalloc;

commit a8049c53cdad347b5b1234969dba65a179fdf8f1
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sun Nov 4 04:42:03 2007 +0000

    [MIPS] Convert reference to mem_map to pfn_to_page().
    
    This was crashing the combination of highmem and sparsemem.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 110ee7656b41..ec3b9e9f30f4 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -426,7 +426,7 @@ void __init mem_init(void)
 
 #ifdef CONFIG_HIGHMEM
 	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
-		struct page *page = mem_map + tmp;
+		struct page *page = pfn_to_page(tmp);
 
 		if (!page_is_ram(tmp)) {
 			SetPageReserved(page);

commit 985c30ef4d7c2a4f0e979a507a7e2f7f09b096c3
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Oct 15 16:30:24 2007 +0100

    [MIPS] Fix aliasing bug in copy_user_highpage, take 2.
    
    Turns out b868868ae0f7272228c95cc760338ffe35bb739d  wasn't quite right.
    When called for a page that isn't marked dirty it would artificially
    create an alias instead of doing the obvious thing and access the page
    via KSEG0.
    
    The same issue also exists in copy_to_user_page and copy_from_user_page
    which was causing the machine to die under rare circumstances for example
    when running ps if the BUG_ON() assertion added by the earlier fix was
    getting triggered.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 5240432e6d1d..110ee7656b41 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -211,7 +211,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 	void *vfrom, *vto;
 
 	vto = kmap_atomic(to, KM_USER1);
-	if (cpu_has_dc_aliases && !Page_dcache_dirty(from)) {
+	if (cpu_has_dc_aliases && page_mapped(from)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
 		kunmap_coherent();
@@ -234,12 +234,15 @@ void copy_to_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)
 {
-	if (cpu_has_dc_aliases) {
+	if (cpu_has_dc_aliases && page_mapped(page)) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
 		kunmap_coherent();
-	} else
+	} else {
 		memcpy(dst, src, len);
+		if (cpu_has_dc_aliases)
+			SetPageDcacheDirty(page);
+	}
 	if ((vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc)
 		flush_cache_page(vma, vaddr, page_to_pfn(page));
 }
@@ -250,13 +253,15 @@ void copy_from_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)
 {
-	if (cpu_has_dc_aliases) {
-		void *vfrom =
-			kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
+	if (cpu_has_dc_aliases && page_mapped(page)) {
+		void *vfrom = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
 		kunmap_coherent();
-	} else
+	} else {
 		memcpy(dst, src, len);
+		if (cpu_has_dc_aliases)
+			SetPageDcacheDirty(page);
+	}
 }
 
 EXPORT_SYMBOL(copy_from_user_page);

commit b868868ae0f7272228c95cc760338ffe35bb739d
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Sep 11 18:05:33 2007 +0100

    [MIPS] Fix aliasing bug in copy_user_highpage.
    
    Copy_user_highpage was written assuming it was only being called for
    breaking COW pages in which case the source page isn't cached as in
    marked cachable under it kernel virtual address.  If it is called anyway
    the aliasing avoidance strategy implemented by kmap_coherent will fail.
    Avoid the use of kmap_coherent for pages marked dirty and to avoid
    another instance of this sort of bug, place a BUG_ON in kmap_coherent.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 09d91505b90c..5240432e6d1d 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -8,6 +8,7 @@
  * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com
  * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.
  */
+#include <linux/bug.h>
 #include <linux/init.h>
 #include <linux/module.h>
 #include <linux/signal.h>
@@ -132,6 +133,8 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 	pte_t pte;
 	int tlbidx;
 
+	BUG_ON(Page_dcache_dirty(page));
+
 	inc_preempt_count();
 	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
 #ifdef CONFIG_MIPS_MT_SMTC
@@ -208,7 +211,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 	void *vfrom, *vto;
 
 	vto = kmap_atomic(to, KM_USER1);
-	if (cpu_has_dc_aliases) {
+	if (cpu_has_dc_aliases && !Page_dcache_dirty(from)) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
 		kunmap_coherent();

commit 9975e77df5428a1afff57fd8f76a1bc0bfc247fc
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Aug 13 12:44:41 2007 +0100

    [MIPS] Gcc 3.3 build fixes.
    
    Work around gcc 3.3's unability to evaluate that certain expressions indeed
    are constant.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index b8cb0dde3af0..09d91505b90c 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -26,6 +26,7 @@
 #include <linux/proc_fs.h>
 #include <linux/pfn.h>
 
+#include <asm/asm-offsets.h>
 #include <asm/bootinfo.h>
 #include <asm/cachectl.h>
 #include <asm/cpu.h>
@@ -498,7 +499,13 @@ unsigned long pgd_current[NR_CPUS];
  * different layout ...
  */
 #define __page_aligned(order) __attribute__((__aligned__(PAGE_SIZE<<order)))
-pgd_t swapper_pg_dir[PTRS_PER_PGD] __page_aligned(PGD_ORDER);
+
+/*
+ * gcc 3.3 and older have trouble determining that PTRS_PER_PGD and PGD_ORDER
+ * are constants.  So we use the variants from asm-offset.h until that gcc
+ * will officially be retired.
+ */
+pgd_t swapper_pg_dir[_PTRS_PER_PGD] __page_aligned(_PGD_ORDER);
 #ifdef CONFIG_64BIT
 #ifdef MODULE_START
 pgd_t module_pg_dir[PTRS_PER_PGD] __page_aligned(PGD_ORDER);

commit fb4bb133ada7423b2504bb64bb6cc95c079a7d9d
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Sun Jul 22 23:44:20 2007 +0900

    [MIPS] Mark prom_free_prom_memory as __init_refok
    
    > WARNING: vmlinux.o(.text+0xbf20): Section mismatch: reference to
    > .init.text:prom_free_prom_memory (between 'free_initmem' and 'copy_from_user_page')
    
    prom_free_prom_memory() is called _before_ freeing init sections, so
    it is false positive.  __init_refok can be used for such cases.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4c80528deadd..b8cb0dde3af0 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -484,7 +484,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 }
 #endif
 
-void free_initmem(void)
+void __init_refok free_initmem(void)
 {
 	prom_free_prom_memory();
 	free_init_pages("unused kernel memory",

commit 599ca0fb639ba843da46c3ad1a53590763539750
Author: Franck Bui-Huu <fbuihuu@gmail.com>
Date:   Mon May 7 18:01:53 2007 +0200

    [MIPS] Remove LIMITED_DMA support
    
    This code was needed only by Jaguar ATX.
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 2d1c2c024822..4c80528deadd 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -424,9 +424,6 @@ void __init mem_init(void)
 			continue;
 		}
 		ClearPageReserved(page);
-#ifdef CONFIG_LIMITED_DMA
-		set_page_address(page, lowmem_page_address(page));
-#endif
 		init_page_count(page);
 		__free_page(page);
 		totalhigh_pages++;

commit eacb9d61919db56482dcea7ec943c9508175dc16
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Apr 26 15:46:25 2007 +0100

    [MIPS] Remove unused argument from kunmap_coherent().
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 8800c88cbbf6..2d1c2c024822 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -177,7 +177,7 @@ void *kmap_coherent(struct page *page, unsigned long addr)
 
 #define UNIQUE_ENTRYHI(idx) (CKSEG0 + ((idx) << (PAGE_SHIFT + 1)))
 
-void kunmap_coherent(struct page *page)
+void kunmap_coherent(void)
 {
 #ifndef CONFIG_MIPS_MT_SMTC
 	unsigned int wired;
@@ -210,7 +210,7 @@ void copy_user_highpage(struct page *to, struct page *from,
 	if (cpu_has_dc_aliases) {
 		vfrom = kmap_coherent(from, vaddr);
 		copy_page(vto, vfrom);
-		kunmap_coherent(from);
+		kunmap_coherent();
 	} else {
 		vfrom = kmap_atomic(from, KM_USER0);
 		copy_page(vto, vfrom);
@@ -233,7 +233,7 @@ void copy_to_user_page(struct vm_area_struct *vma,
 	if (cpu_has_dc_aliases) {
 		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(vto, src, len);
-		kunmap_coherent(page);
+		kunmap_coherent();
 	} else
 		memcpy(dst, src, len);
 	if ((vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc)
@@ -250,7 +250,7 @@ void copy_from_user_page(struct vm_area_struct *vma,
 		void *vfrom =
 			kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
 		memcpy(dst, vfrom, len);
-		kunmap_coherent(page);
+		kunmap_coherent();
 	} else
 		memcpy(dst, src, len);
 }

commit 05502339332564ffd545be9ca37b208296a2eaad
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Wed Mar 21 00:36:02 2007 +0900

    [MIPS] Select ZONE_DMA only if GENERIC_ISA_DMA selected
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index e9951c0e689f..8800c88cbbf6 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -351,18 +351,15 @@ void __init paging_init(void)
 #endif
 	kmap_coherent_init();
 
-#ifdef CONFIG_ISA
-	if (max_low_pfn >= MAX_DMA_PFN)
-		if (min_low_pfn >= MAX_DMA_PFN) {
-			zones_size[ZONE_DMA] = 0;
-			zones_size[ZONE_NORMAL] = max_low_pfn - min_low_pfn;
-		} else {
-			zones_size[ZONE_DMA] = MAX_DMA_PFN - min_low_pfn;
-			zones_size[ZONE_NORMAL] = max_low_pfn - MAX_DMA_PFN;
-		}
+#ifdef CONFIG_ZONE_DMA
+	if (min_low_pfn < MAX_DMA_PFN && MAX_DMA_PFN <= max_low_pfn) {
+		zones_size[ZONE_DMA] = MAX_DMA_PFN - min_low_pfn;
+		zones_size[ZONE_NORMAL] = max_low_pfn - MAX_DMA_PFN;
+	} else if (max_low_pfn < MAX_DMA_PFN)
+		zones_size[ZONE_DMA] = max_low_pfn - min_low_pfn;
 	else
 #endif
-	zones_size[ZONE_DMA] = max_low_pfn - min_low_pfn;
+	zones_size[ZONE_NORMAL] = max_low_pfn - min_low_pfn;
 
 #ifdef CONFIG_HIGHMEM
 	zones_size[ZONE_HIGHMEM] = highend_pfn - highstart_pfn;

commit 7575a49f209190ca640e0da792565a1bcb641f3e
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Mar 23 21:36:37 2007 +0000

    [MIPS] Implement flush_anon_page().
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 25abe91da4ac..e9951c0e689f 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -123,7 +123,7 @@ static void __init kmap_coherent_init(void)
 static inline void kmap_coherent_init(void) {}
 #endif
 
-static inline void *kmap_coherent(struct page *page, unsigned long addr)
+void *kmap_coherent(struct page *page, unsigned long addr)
 {
 	enum fixed_addresses idx;
 	unsigned long vaddr, flags, entrylo;
@@ -177,7 +177,7 @@ static inline void *kmap_coherent(struct page *page, unsigned long addr)
 
 #define UNIQUE_ENTRYHI(idx) (CKSEG0 + ((idx) << (PAGE_SHIFT + 1)))
 
-static inline void kunmap_coherent(struct page *page)
+void kunmap_coherent(struct page *page)
 {
 #ifndef CONFIG_MIPS_MT_SMTC
 	unsigned int wired;

commit 80e89593e5a574a572f4e45e157733b5ea5ddfdb
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Wed Mar 21 00:56:28 2007 +0900

    [MIPS] SPARSEMEM: The first pfn of zone should be min_low_pfn, not 0.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index f08ae71c46ff..25abe91da4ac 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -377,7 +377,7 @@ void __init paging_init(void)
 #ifdef CONFIG_FLATMEM
 	free_area_init(zones_size);
 #else
-	pfn = 0;
+	pfn = min_low_pfn;
 	for (i = 0; i < MAX_NR_ZONES; i++)
 		for (j = 0; j < zones_size[i]; j++, pfn++)
 			if (!page_is_ram(pfn))

commit bf15f7679b5cd943758dfd538b852c2ab406bee5
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Feb 19 15:00:49 2007 +0000

    [MIPS] Declare highstart_pfn, highend_pfn only if CONFIG_HIGHMEM=y
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 13a420844436..f08ae71c46ff 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -61,8 +61,6 @@
 
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
 
-unsigned long highstart_pfn, highend_pfn;
-
 /*
  * We have up to 8 empty zeroed pages so we can map one of the right colour
  * when needed.  This is necessary only on R4000 / R4400 SC and MC versions
@@ -261,6 +259,8 @@ EXPORT_SYMBOL(copy_from_user_page);
 
 
 #ifdef CONFIG_HIGHMEM
+unsigned long highstart_pfn, highend_pfn;
+
 pte_t *kmap_pte;
 pgprot_t kmap_prot;
 

commit 6a1e552947b0d73310e534bf4eb09e14db1e5056
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Mon Feb 19 01:27:34 2007 +0900

    [MIPS] Add external declaration of pagetable_init() to pgalloc.h
    
    This fixes some sparse warnings.
    
    pgtable-32.c:15:6: warning: symbol 'pgd_init' was not declared. Should it be static?
    pgtable-32.c:32:13: warning: symbol 'pagetable_init' was not declared. Should it be static?
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 125a4a85ec05..13a420844436 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -314,8 +314,6 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 }
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-extern void pagetable_init(void);
-
 static int __init page_is_ram(unsigned long pagenr)
 {
 	int i;

commit 69a6c312e5ebb2e929ceb67e6246e2d9314f1d29
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Wed Jan 24 01:21:05 2007 +0900

    [MIPS] Move some kernel globals from asm file to C file.
    
    This get rid of some undesirable hole in BSS section due to random
    order of placement.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 5257f7b42fd2..125a4a85ec05 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -499,3 +499,18 @@ void free_initmem(void)
 			__pa_symbol(&__init_begin),
 			__pa_symbol(&__init_end));
 }
+
+unsigned long pgd_current[NR_CPUS];
+/*
+ * On 64-bit we've got three-level pagetables with a slightly
+ * different layout ...
+ */
+#define __page_aligned(order) __attribute__((__aligned__(PAGE_SIZE<<order)))
+pgd_t swapper_pg_dir[PTRS_PER_PGD] __page_aligned(PGD_ORDER);
+#ifdef CONFIG_64BIT
+#ifdef MODULE_START
+pgd_t module_pg_dir[PTRS_PER_PGD] __page_aligned(PGD_ORDER);
+#endif
+pmd_t invalid_pmd_table[PTRS_PER_PMD] __page_aligned(PMD_ORDER);
+#endif
+pte_t invalid_pte_table[PTRS_PER_PTE] __page_aligned(PTE_ORDER);

commit db84dc61552ae0d198a8133d28b80c3838930ba8
Author: Franck Bui-Huu <fbuihuu@gmail.com>
Date:   Wed Jan 10 09:44:04 2007 +0100

    [MIPS] Setup min_low_pfn/max_low_pfn correctly
    
    This patch makes a better usage of these two globals.
    'min_low_pfn' is now correctly setup for all configs, which
    allow us to rely on it in boot memory code init.
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index fb427dbfe71e..5257f7b42fd2 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -341,7 +341,6 @@ static int __init page_is_ram(unsigned long pagenr)
 void __init paging_init(void)
 {
 	unsigned long zones_size[MAX_NR_ZONES] = { 0, };
-	unsigned long max_dma, low;
 #ifndef CONFIG_FLATMEM
 	unsigned long zholes_size[MAX_NR_ZONES] = { 0, };
 	unsigned long i, j, pfn;
@@ -354,19 +353,19 @@ void __init paging_init(void)
 #endif
 	kmap_coherent_init();
 
-	max_dma = virt_to_phys((char *)MAX_DMA_ADDRESS) >> PAGE_SHIFT;
-	low = max_low_pfn;
-
 #ifdef CONFIG_ISA
-	if (low < max_dma)
-		zones_size[ZONE_DMA] = low;
-	else {
-		zones_size[ZONE_DMA] = max_dma;
-		zones_size[ZONE_NORMAL] = low - max_dma;
-	}
-#else
-	zones_size[ZONE_DMA] = low;
+	if (max_low_pfn >= MAX_DMA_PFN)
+		if (min_low_pfn >= MAX_DMA_PFN) {
+			zones_size[ZONE_DMA] = 0;
+			zones_size[ZONE_NORMAL] = max_low_pfn - min_low_pfn;
+		} else {
+			zones_size[ZONE_DMA] = MAX_DMA_PFN - min_low_pfn;
+			zones_size[ZONE_NORMAL] = max_low_pfn - MAX_DMA_PFN;
+		}
+	else
 #endif
+	zones_size[ZONE_DMA] = max_low_pfn - min_low_pfn;
+
 #ifdef CONFIG_HIGHMEM
 	zones_size[ZONE_HIGHMEM] = highend_pfn - highstart_pfn;
 

commit c44e8d5e47b8ba672440b92eab0735628469116c
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Sat Dec 30 00:43:59 2006 +0900

    [MIPS] prom_free_prom_memory cleanup
    
    Current prom_free_prom_memory() implementations are almost same as
    free_init_pages(), or no-op.  Make free_init_pages() extern (again)
    and make prom_free_prom_memory() use it.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 49065c133ebf..fb427dbfe71e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -467,7 +467,7 @@ void __init mem_init(void)
 }
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
-static void free_init_pages(char *what, unsigned long begin, unsigned long end)
+void free_init_pages(const char *what, unsigned long begin, unsigned long end)
 {
 	unsigned long pfn;
 
@@ -493,17 +493,9 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 }
 #endif
 
-extern unsigned long prom_free_prom_memory(void);
-
 void free_initmem(void)
 {
-	unsigned long freed;
-
-	freed = prom_free_prom_memory();
-	if (freed)
-		printk(KERN_INFO "Freeing firmware memory: %ldkb freed\n",
-		       freed >> 10);
-
+	prom_free_prom_memory();
 	free_init_pages("unused kernel memory",
 			__pa_symbol(&__init_begin),
 			__pa_symbol(&__init_end));

commit 80aac59ed5b3e5ae2132bc620ff5607929c6a304
Author: Thiemo Seufer <ths@networkno.de>
Date:   Wed Jan 24 01:29:15 2007 +0000

    [MIPS] Fix reported amount of freed memory - it's in kB not bytes
    
    While at it, change message on DEC for consistency.
    
    Signed-off-by: Thiemo Seufer <ths@networkno.de>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 30245c09d025..49065c133ebf 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -501,7 +501,8 @@ void free_initmem(void)
 
 	freed = prom_free_prom_memory();
 	if (freed)
-		printk(KERN_INFO "Freeing firmware memory: %ldk freed\n",freed);
+		printk(KERN_INFO "Freeing firmware memory: %ldkb freed\n",
+		       freed >> 10);
 
 	free_init_pages("unused kernel memory",
 			__pa_symbol(&__init_begin),

commit bcd022801ee514e28c32837f0b3ce18c775f1a7b
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Tue Dec 12 17:14:56 2006 +0000

    [PATCH] MIPS: Fix COW D-cache aliasing on fork
    
    Provide a custom copy_user_highpage() to deal with aliasing issues on
    MIPS.  It uses kmap_coherent() to map an user page for kernel with same
    color.  Rewrite copy_to_user_page() and copy_from_user_page() with the
    new interfaces to avoid extra cache flushing.
    
    The main part of this patch was originally written by Ralf Baechle;
    Atushi Nemoto did the the debugging.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index ea2d15370bb7..30245c09d025 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -203,6 +203,31 @@ static inline void kunmap_coherent(struct page *page)
 	preempt_check_resched();
 }
 
+void copy_user_highpage(struct page *to, struct page *from,
+	unsigned long vaddr, struct vm_area_struct *vma)
+{
+	void *vfrom, *vto;
+
+	vto = kmap_atomic(to, KM_USER1);
+	if (cpu_has_dc_aliases) {
+		vfrom = kmap_coherent(from, vaddr);
+		copy_page(vto, vfrom);
+		kunmap_coherent(from);
+	} else {
+		vfrom = kmap_atomic(from, KM_USER0);
+		copy_page(vto, vfrom);
+		kunmap_atomic(vfrom, KM_USER0);
+	}
+	if (((vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc) ||
+	    pages_do_alias((unsigned long)vto, vaddr & PAGE_MASK))
+		flush_data_cache_page((unsigned long)vto);
+	kunmap_atomic(vto, KM_USER1);
+	/* Make sure this page is cleared on other CPU's too before using it */
+	smp_wmb();
+}
+
+EXPORT_SYMBOL(copy_user_highpage);
+
 void copy_to_user_page(struct vm_area_struct *vma,
 	struct page *page, unsigned long vaddr, void *dst, const void *src,
 	unsigned long len)

commit cbb8fc07974073543fdc61da23713ab49ddd3ced
Author: Franck Bui-Huu <fbuihuu@gmail.com>
Date:   Wed Dec 6 16:48:28 2006 +0100

    [MIPS] paging_init(): use highend_pfn/highstart_pfn
    
    This patch makes paging_init() use highend_pfn/highstart_pfn globals.
    
    It removes the need of 'high' local which was needed only by HIGHMEM config.
    
    More important perhaps, it fixes a bug when HIGHMEM is set but there's
    actually no physical highmem (highend_pfn = 0)
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 9e29ba9205f0..ea2d15370bb7 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -316,7 +316,7 @@ static int __init page_is_ram(unsigned long pagenr)
 void __init paging_init(void)
 {
 	unsigned long zones_size[MAX_NR_ZONES] = { 0, };
-	unsigned long max_dma, high, low;
+	unsigned long max_dma, low;
 #ifndef CONFIG_FLATMEM
 	unsigned long zholes_size[MAX_NR_ZONES] = { 0, };
 	unsigned long i, j, pfn;
@@ -331,7 +331,6 @@ void __init paging_init(void)
 
 	max_dma = virt_to_phys((char *)MAX_DMA_ADDRESS) >> PAGE_SHIFT;
 	low = max_low_pfn;
-	high = highend_pfn;
 
 #ifdef CONFIG_ISA
 	if (low < max_dma)
@@ -344,13 +343,13 @@ void __init paging_init(void)
 	zones_size[ZONE_DMA] = low;
 #endif
 #ifdef CONFIG_HIGHMEM
-	if (cpu_has_dc_aliases) {
-		printk(KERN_WARNING "This processor doesn't support highmem.");
-		if (high - low)
-			printk(" %ldk highmem ignored", high - low);
-		printk("\n");
-	} else
-		zones_size[ZONE_HIGHMEM] = high - low;
+	zones_size[ZONE_HIGHMEM] = highend_pfn - highstart_pfn;
+
+	if (cpu_has_dc_aliases && zones_size[ZONE_HIGHMEM]) {
+		printk(KERN_WARNING "This processor doesn't support highmem."
+		       " %ldk highmem ignored\n", zones_size[ZONE_HIGHMEM]);
+		zones_size[ZONE_HIGHMEM] = 0;
+	}
 #endif
 
 #ifdef CONFIG_FLATMEM

commit acd86b8622099c3206e0a1665545ae2318089b9c
Author: Franck Bui-Huu <vagabon.xyz@gmail.com>
Date:   Thu Oct 19 13:20:05 2006 +0200

    [MIPS] Make free_init_pages() arguments to be physical addresses
    
    It allows caller of this function to not care about CKSEG0/XKPHYS
    address mixes. It's now automatically done by free_init_pages().
    
    We can now safely remove hack needed by 64 bit kernels with
    CONFIG_BUILD_ELF64=n in free_initmem().
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4076963c601e..9e29ba9205f0 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -443,15 +443,18 @@ void __init mem_init(void)
 }
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
-void free_init_pages(char *what, unsigned long begin, unsigned long end)
+static void free_init_pages(char *what, unsigned long begin, unsigned long end)
 {
-	unsigned long addr;
+	unsigned long pfn;
 
-	for (addr = begin; addr < end; addr += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page((void *)addr));
-		init_page_count(virt_to_page((void *)addr));
-		memset((void *)addr, 0xcc, PAGE_SIZE);
-		free_page(addr);
+	for (pfn = PFN_UP(begin); pfn < PFN_DOWN(end); pfn++) {
+		struct page *page = pfn_to_page(pfn);
+		void *addr = phys_to_virt(PFN_PHYS(pfn));
+
+		ClearPageReserved(page);
+		init_page_count(page);
+		memset(addr, POISON_FREE_INITMEM, PAGE_SIZE);
+		__free_page(page);
 		totalram_pages++;
 	}
 	printk(KERN_INFO "Freeing %s: %ldk freed\n", what, (end - begin) >> 10);
@@ -460,7 +463,9 @@ void free_init_pages(char *what, unsigned long begin, unsigned long end)
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	free_init_pages("initrd memory", start, end);
+	free_init_pages("initrd memory",
+			virt_to_phys((void *)start),
+			virt_to_phys((void *)end));
 }
 #endif
 
@@ -468,17 +473,13 @@ extern unsigned long prom_free_prom_memory(void);
 
 void free_initmem(void)
 {
-	unsigned long start, end, freed;
+	unsigned long freed;
 
 	freed = prom_free_prom_memory();
 	if (freed)
 		printk(KERN_INFO "Freeing firmware memory: %ldk freed\n",freed);
 
-	start = (unsigned long)(&__init_begin);
-	end = (unsigned long)(&__init_end);
-#ifdef CONFIG_64BIT
-	start = PAGE_OFFSET | CPHYSADDR(start);
-	end = PAGE_OFFSET | CPHYSADDR(end);
-#endif
-	free_init_pages("unused kernel memory", start, end);
+	free_init_pages("unused kernel memory",
+			__pa_symbol(&__init_begin),
+			__pa_symbol(&__init_end));
 }

commit a7837b76b6de932c31d0b7c71176ca8d1213a3ce
Author: Franck Bui-Huu <vagabon.xyz@gmail.com>
Date:   Thu Oct 19 13:20:04 2006 +0200

    [MIPS] setup.c: clean up initrd related code
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 304991263f6b..4076963c601e 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -460,11 +460,6 @@ void free_init_pages(char *what, unsigned long begin, unsigned long end)
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-#ifdef CONFIG_64BIT
-	/* Switch from KSEG0 to XKPHYS addresses */
-	start = (unsigned long)phys_to_virt(CPHYSADDR(start));
-	end = (unsigned long)phys_to_virt(CPHYSADDR(end));
-#endif
 	free_init_pages("initrd memory", start, end);
 }
 #endif

commit 99e3b942c62f42c8d5added63305e12372b06daf
Author: Franck Bui-Huu <vagabon.xyz@gmail.com>
Date:   Thu Oct 19 13:19:59 2006 +0200

    [MIPS] page.h: remove __pa() usages.
    
    __pa() was used by virt_to_page() and virt_addr_valid(). These
    latter are used when kernel is initialised so __pa() is not
    appropriate, we use virt_to_phys() instead.
    
    Futhermore __pa() is going to take care of CKSEG0/XKPHYS
    address mix for 64 bit kernels. This makes __pa() more complex
    than virt_to_phys() and this extra work is not needed by
    virt_to_page() and virt_addr_valid().
    
    Eventually it consolidates virt_to_phys() prototype by making
    its argument 'const'. this avoids some warnings that was due
    to some virt_to_page() usages which pass const pointer.
    
    Signed-off-by: Franck Bui-Huu <fbuihuu@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 2de4d3c367a2..304991263f6b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -90,9 +90,9 @@ unsigned long setup_zero_pages(void)
 	if (!empty_zero_page)
 		panic("Oh boy, that early out of memory?");
 
-	page = virt_to_page(empty_zero_page);
+	page = virt_to_page((void *)empty_zero_page);
 	split_page(page, order);
-	while (page < virt_to_page(empty_zero_page + (PAGE_SIZE << order))) {
+	while (page < virt_to_page((void *)(empty_zero_page + (PAGE_SIZE << order)))) {
 		SetPageReserved(page);
 		page++;
 	}
@@ -448,8 +448,8 @@ void free_init_pages(char *what, unsigned long begin, unsigned long end)
 	unsigned long addr;
 
 	for (addr = begin; addr < end; addr += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page(addr));
-		init_page_count(virt_to_page(addr));
+		ClearPageReserved(virt_to_page((void *)addr));
+		init_page_count(virt_to_page((void *)addr));
 		memset((void *)addr, 0xcc, PAGE_SIZE);
 		free_page(addr);
 		totalram_pages++;

commit f8829caee311207afbc882794bdc5aa0db5caf33
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Oct 21 23:17:35 2006 +0100

    [MIPS] Fix aliasing bug in copy_to_user_page / copy_from_user_page
    
    The current implementation uses a sequence of a cacheflush and a copy.
    This is racy in case of a multithreaded debuggee and renders GDB
    virtually unusable.
    
    Aside this fixes a performance hog rendering access to /proc/cmdline very
    slow and resulting in a enough cache stalls for the 34K AP/SP programming
    model to make the bare metal code on the non-Linux VPE miss RT deadlines.
    
    The main part of this patch was originally written by Ralf Baechle;
    Atushi Nemoto did the the debugging.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 88b72c9a8495..2de4d3c367a2 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -30,11 +30,34 @@
 #include <asm/cachectl.h>
 #include <asm/cpu.h>
 #include <asm/dma.h>
+#include <asm/kmap_types.h>
 #include <asm/mmu_context.h>
 #include <asm/sections.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/tlb.h>
+#include <asm/fixmap.h>
+
+/* Atomicity and interruptability */
+#ifdef CONFIG_MIPS_MT_SMTC
+
+#include <asm/mipsmtregs.h>
+
+#define ENTER_CRITICAL(flags) \
+	{ \
+	unsigned int mvpflags; \
+	local_irq_save(flags);\
+	mvpflags = dvpe()
+#define EXIT_CRITICAL(flags) \
+	evpe(mvpflags); \
+	local_irq_restore(flags); \
+	}
+#else
+
+#define ENTER_CRITICAL(flags) local_irq_save(flags)
+#define EXIT_CRITICAL(flags) local_irq_restore(flags)
+
+#endif /* CONFIG_MIPS_MT_SMTC */
 
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
 
@@ -80,13 +103,142 @@ unsigned long setup_zero_pages(void)
 	return 1UL << order;
 }
 
-#ifdef CONFIG_HIGHMEM
-pte_t *kmap_pte;
-pgprot_t kmap_prot;
+/*
+ * These are almost like kmap_atomic / kunmap_atmic except they take an
+ * additional address argument as the hint.
+ */
 
 #define kmap_get_fixmap_pte(vaddr)					\
 	pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr), (vaddr)), (vaddr)), (vaddr))
 
+#ifdef CONFIG_MIPS_MT_SMTC
+static pte_t *kmap_coherent_pte;
+static void __init kmap_coherent_init(void)
+{
+	unsigned long vaddr;
+
+	/* cache the first coherent kmap pte */
+	vaddr = __fix_to_virt(FIX_CMAP_BEGIN);
+	kmap_coherent_pte = kmap_get_fixmap_pte(vaddr);
+}
+#else
+static inline void kmap_coherent_init(void) {}
+#endif
+
+static inline void *kmap_coherent(struct page *page, unsigned long addr)
+{
+	enum fixed_addresses idx;
+	unsigned long vaddr, flags, entrylo;
+	unsigned long old_ctx;
+	pte_t pte;
+	int tlbidx;
+
+	inc_preempt_count();
+	idx = (addr >> PAGE_SHIFT) & (FIX_N_COLOURS - 1);
+#ifdef CONFIG_MIPS_MT_SMTC
+	idx += FIX_N_COLOURS * smp_processor_id();
+#endif
+	vaddr = __fix_to_virt(FIX_CMAP_END - idx);
+	pte = mk_pte(page, PAGE_KERNEL);
+#if defined(CONFIG_64BIT_PHYS_ADDR) && defined(CONFIG_CPU_MIPS32_R1)
+	entrylo = pte.pte_high;
+#else
+	entrylo = pte_val(pte) >> 6;
+#endif
+
+	ENTER_CRITICAL(flags);
+	old_ctx = read_c0_entryhi();
+	write_c0_entryhi(vaddr & (PAGE_MASK << 1));
+	write_c0_entrylo0(entrylo);
+	write_c0_entrylo1(entrylo);
+#ifdef CONFIG_MIPS_MT_SMTC
+	set_pte(kmap_coherent_pte - (FIX_CMAP_END - idx), pte);
+	/* preload TLB instead of local_flush_tlb_one() */
+	mtc0_tlbw_hazard();
+	tlb_probe();
+	tlb_probe_hazard();
+	tlbidx = read_c0_index();
+	mtc0_tlbw_hazard();
+	if (tlbidx < 0)
+		tlb_write_random();
+	else
+		tlb_write_indexed();
+#else
+	tlbidx = read_c0_wired();
+	write_c0_wired(tlbidx + 1);
+	write_c0_index(tlbidx);
+	mtc0_tlbw_hazard();
+	tlb_write_indexed();
+#endif
+	tlbw_use_hazard();
+	write_c0_entryhi(old_ctx);
+	EXIT_CRITICAL(flags);
+
+	return (void*) vaddr;
+}
+
+#define UNIQUE_ENTRYHI(idx) (CKSEG0 + ((idx) << (PAGE_SHIFT + 1)))
+
+static inline void kunmap_coherent(struct page *page)
+{
+#ifndef CONFIG_MIPS_MT_SMTC
+	unsigned int wired;
+	unsigned long flags, old_ctx;
+
+	ENTER_CRITICAL(flags);
+	old_ctx = read_c0_entryhi();
+	wired = read_c0_wired() - 1;
+	write_c0_wired(wired);
+	write_c0_index(wired);
+	write_c0_entryhi(UNIQUE_ENTRYHI(wired));
+	write_c0_entrylo0(0);
+	write_c0_entrylo1(0);
+	mtc0_tlbw_hazard();
+	tlb_write_indexed();
+	tlbw_use_hazard();
+	write_c0_entryhi(old_ctx);
+	EXIT_CRITICAL(flags);
+#endif
+	dec_preempt_count();
+	preempt_check_resched();
+}
+
+void copy_to_user_page(struct vm_area_struct *vma,
+	struct page *page, unsigned long vaddr, void *dst, const void *src,
+	unsigned long len)
+{
+	if (cpu_has_dc_aliases) {
+		void *vto = kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
+		memcpy(vto, src, len);
+		kunmap_coherent(page);
+	} else
+		memcpy(dst, src, len);
+	if ((vma->vm_flags & VM_EXEC) && !cpu_has_ic_fills_f_dc)
+		flush_cache_page(vma, vaddr, page_to_pfn(page));
+}
+
+EXPORT_SYMBOL(copy_to_user_page);
+
+void copy_from_user_page(struct vm_area_struct *vma,
+	struct page *page, unsigned long vaddr, void *dst, const void *src,
+	unsigned long len)
+{
+	if (cpu_has_dc_aliases) {
+		void *vfrom =
+			kmap_coherent(page, vaddr) + (vaddr & ~PAGE_MASK);
+		memcpy(dst, vfrom, len);
+		kunmap_coherent(page);
+	} else
+		memcpy(dst, src, len);
+}
+
+EXPORT_SYMBOL(copy_from_user_page);
+
+
+#ifdef CONFIG_HIGHMEM
+pte_t *kmap_pte;
+pgprot_t kmap_prot;
+
 static void __init kmap_init(void)
 {
 	unsigned long kmap_vstart;
@@ -97,11 +249,12 @@ static void __init kmap_init(void)
 
 	kmap_prot = PAGE_KERNEL;
 }
+#endif /* CONFIG_HIGHMEM */
 
-#ifdef CONFIG_32BIT
 void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)
 {
+#if defined(CONFIG_HIGHMEM) || defined(CONFIG_MIPS_MT_SMTC)
 	pgd_t *pgd;
 	pud_t *pud;
 	pmd_t *pmd;
@@ -122,7 +275,7 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 			for (; (k < PTRS_PER_PMD) && (vaddr != end); pmd++, k++) {
 				if (pmd_none(*pmd)) {
 					pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
-					set_pmd(pmd, __pmd(pte));
+					set_pmd(pmd, __pmd((unsigned long)pte));
 					if (pte != pte_offset_kernel(pmd, 0))
 						BUG();
 				}
@@ -132,9 +285,8 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 		}
 		j = 0;
 	}
+#endif
 }
-#endif /* CONFIG_32BIT */
-#endif /* CONFIG_HIGHMEM */
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 extern void pagetable_init(void);
@@ -175,6 +327,7 @@ void __init paging_init(void)
 #ifdef CONFIG_HIGHMEM
 	kmap_init();
 #endif
+	kmap_coherent_init();
 
 	max_dma = virt_to_phys((char *)MAX_DMA_ADDRESS) >> PAGE_SHIFT;
 	low = max_low_pfn;

commit b58e5d050ff60335b106a895a87a59a8f5475db4
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Sat Sep 30 03:34:06 2006 +0900

    [MIPS] Fix size of zones_size and zholes_size array
    
    Commit f06a96844a577c43249fce25809a4fae07407f46 broke MIPS.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 5b06349af2d5..88b72c9a8495 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -163,10 +163,10 @@ static int __init page_is_ram(unsigned long pagenr)
 
 void __init paging_init(void)
 {
-	unsigned long zones_size[] = { 0, };
+	unsigned long zones_size[MAX_NR_ZONES] = { 0, };
 	unsigned long max_dma, high, low;
 #ifndef CONFIG_FLATMEM
-	unsigned long zholes_size[] = { 0, };
+	unsigned long zholes_size[MAX_NR_ZONES] = { 0, };
 	unsigned long i, j, pfn;
 #endif
 

commit f06a96844a577c43249fce25809a4fae07407f46
Author: Christoph Lameter <clameter@sgi.com>
Date:   Mon Sep 25 23:31:10 2006 -0700

    [PATCH] reduce MAX_NR_ZONES: fix MAX_NR_ZONES array initializations
    
    Fix array initialization in lots of arches
    
    The number of zones may now be reduced from 4 to 2 for many arches.  Fix the
    array initialization for the zones array for all architectures so that it is
    not initializing a fixed number of elements.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index c52497bb102a..5b06349af2d5 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -163,10 +163,10 @@ static int __init page_is_ram(unsigned long pagenr)
 
 void __init paging_init(void)
 {
-	unsigned long zones_size[] = { [0 ... MAX_NR_ZONES - 1] = 0 };
+	unsigned long zones_size[] = { 0, };
 	unsigned long max_dma, high, low;
 #ifndef CONFIG_FLATMEM
-	unsigned long zholes_size[] = { [0 ... MAX_NR_ZONES - 1] = 0 };
+	unsigned long zholes_size[] = { 0, };
 	unsigned long i, j, pfn;
 #endif
 

commit 565200a14641eb7ab7b96a726441f2e4d663d15c
Author: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
Date:   Fri Jul 7 00:26:02 2006 +0900

    [MIPS] Do not count pages in holes with sparsemem
    
    With some memory model other than FLATMEM, the single node can
    contains some holes so there might be many invalid pages.  For
    example, with two 256M memory and one 256M hole, some variables
    (num_physpage, totalpages, nr_kernel_pages, nr_all_pages, etc.) will
    indicate that there are 768MB on this system.  This is not desired
    because, for example, alloc_large_system_hash() allocates too many
    entries.
    
    Use free_area_init_node() with counted zholes_size[] instead of
    free_area_init().
    
    For num_physpages, use number of ram pages instead of max_low_pfn.
    
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 802bdd32aa2b..c52497bb102a 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -139,10 +139,36 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 extern void pagetable_init(void);
 
+static int __init page_is_ram(unsigned long pagenr)
+{
+	int i;
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long addr, end;
+
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			/* not usable memory */
+			continue;
+
+		addr = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr +
+			       boot_mem_map.map[i].size);
+
+		if (pagenr >= addr && pagenr < end)
+			return 1;
+	}
+
+	return 0;
+}
+
 void __init paging_init(void)
 {
-	unsigned long zones_size[MAX_NR_ZONES] = {0, 0, 0};
+	unsigned long zones_size[] = { [0 ... MAX_NR_ZONES - 1] = 0 };
 	unsigned long max_dma, high, low;
+#ifndef CONFIG_FLATMEM
+	unsigned long zholes_size[] = { [0 ... MAX_NR_ZONES - 1] = 0 };
+	unsigned long i, j, pfn;
+#endif
 
 	pagetable_init();
 
@@ -174,29 +200,16 @@ void __init paging_init(void)
 		zones_size[ZONE_HIGHMEM] = high - low;
 #endif
 
+#ifdef CONFIG_FLATMEM
 	free_area_init(zones_size);
-}
-
-static inline int page_is_ram(unsigned long pagenr)
-{
-	int i;
-
-	for (i = 0; i < boot_mem_map.nr_map; i++) {
-		unsigned long addr, end;
-
-		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
-			/* not usable memory */
-			continue;
-
-		addr = PFN_UP(boot_mem_map.map[i].addr);
-		end = PFN_DOWN(boot_mem_map.map[i].addr +
-			       boot_mem_map.map[i].size);
-
-		if (pagenr >= addr && pagenr < end)
-			return 1;
-	}
-
-	return 0;
+#else
+	pfn = 0;
+	for (i = 0; i < MAX_NR_ZONES; i++)
+		for (j = 0; j < zones_size[i]; j++, pfn++)
+			if (!page_is_ram(pfn))
+				zholes_size[i]++;
+	free_area_init_node(0, NODE_DATA(0), zones_size, 0, zholes_size);
+#endif
 }
 
 static struct kcore_list kcore_mem, kcore_vmalloc;
@@ -213,9 +226,9 @@ void __init mem_init(void)
 #ifdef CONFIG_DISCONTIGMEM
 #error "CONFIG_HIGHMEM and CONFIG_DISCONTIGMEM dont work together yet"
 #endif
-	max_mapnr = num_physpages = highend_pfn;
+	max_mapnr = highend_pfn;
 #else
-	max_mapnr = num_physpages = max_low_pfn;
+	max_mapnr = max_low_pfn;
 #endif
 	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
 
@@ -229,6 +242,7 @@ void __init mem_init(void)
 			if (PageReserved(pfn_to_page(tmp)))
 				reservedpages++;
 		}
+	num_physpages = ram;
 
 #ifdef CONFIG_HIGHMEM
 	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
@@ -247,6 +261,7 @@ void __init mem_init(void)
 		totalhigh_pages++;
 	}
 	totalram_pages += totalhigh_pages;
+	num_physpages += totalhigh_pages;
 #endif
 
 	codesize =  (unsigned long) &_etext - (unsigned long) &_text;

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 33f6e1cdfd5b..802bdd32aa2b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -8,7 +8,6 @@
  * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com
  * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.
  */
-#include <linux/config.h>
 #include <linux/init.h>
 #include <linux/module.h>
 #include <linux/signal.h>

commit b1c231f5a57cb4a417c38a8a946f1e66db3bb9c0
Author: Chad Reese <creese@caviumnetworks.com>
Date:   Tue May 30 17:16:49 2006 -0700

    [MIPS] Fix sparsemem support.
    
    Move memory_present() in arch/mips/kernel/setup.c. When using sparsemem
    extreme, this function does an allocate for bootmem. This would always
    fail since init_bootmem hasn't been called yet.
    
    Move memory_present after free_bootmem. This only marks actual memory
    ranges as present instead of the entire address space.
    
    Signed-off-by: Chad Reese  <creese@caviumnetworks.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index c22308b93ff0..33f6e1cdfd5b 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -227,7 +227,7 @@ void __init mem_init(void)
 	for (tmp = 0; tmp < max_low_pfn; tmp++)
 		if (page_is_ram(tmp)) {
 			ram++;
-			if (PageReserved(mem_map+tmp))
+			if (PageReserved(pfn_to_page(tmp)))
 				reservedpages++;
 		}
 

commit 6fd11a2173709cd598345e15382af480979cf4fc
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Mar 24 13:21:50 2006 +0000

    [MIPS] Cleanup free_initmem the same way as i386 did.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index ad89c442f299..c22308b93ff0 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -276,6 +276,20 @@ void __init mem_init(void)
 }
 #endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
+void free_init_pages(char *what, unsigned long begin, unsigned long end)
+{
+	unsigned long addr;
+
+	for (addr = begin; addr < end; addr += PAGE_SIZE) {
+		ClearPageReserved(virt_to_page(addr));
+		init_page_count(virt_to_page(addr));
+		memset((void *)addr, 0xcc, PAGE_SIZE);
+		free_page(addr);
+		totalram_pages++;
+	}
+	printk(KERN_INFO "Freeing %s: %ldk freed\n", what, (end - begin) >> 10);
+}
+
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
@@ -284,16 +298,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 	start = (unsigned long)phys_to_virt(CPHYSADDR(start));
 	end = (unsigned long)phys_to_virt(CPHYSADDR(end));
 #endif
-	if (start < end)
-		printk(KERN_INFO "Freeing initrd memory: %ldk freed\n",
-		       (end - start) >> 10);
-
-	for (; start < end; start += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page(start));
-		init_page_count(virt_to_page(start));
-		free_page(start);
-		totalram_pages++;
-	}
+	free_init_pages("initrd memory", start, end);
 }
 #endif
 
@@ -301,24 +306,17 @@ extern unsigned long prom_free_prom_memory(void);
 
 void free_initmem(void)
 {
-	unsigned long addr, page, freed;
+	unsigned long start, end, freed;
 
 	freed = prom_free_prom_memory();
+	if (freed)
+		printk(KERN_INFO "Freeing firmware memory: %ldk freed\n",freed);
 
-	addr = (unsigned long) &__init_begin;
-	while (addr < (unsigned long) &__init_end) {
+	start = (unsigned long)(&__init_begin);
+	end = (unsigned long)(&__init_end);
 #ifdef CONFIG_64BIT
-		page = PAGE_OFFSET | CPHYSADDR(addr);
-#else
-		page = addr;
+	start = PAGE_OFFSET | CPHYSADDR(start);
+	end = PAGE_OFFSET | CPHYSADDR(end);
 #endif
-		ClearPageReserved(virt_to_page(page));
-		init_page_count(virt_to_page(page));
-		free_page(page);
-		totalram_pages++;
-		freed += PAGE_SIZE;
-		addr += PAGE_SIZE;
-	}
-	printk(KERN_INFO "Freeing unused kernel memory: %ldk freed\n",
-	       freed >> 10);
+	free_init_pages("unused kernel memory", start, end);
 }

commit 22a9835c350782a5c3257343713932af3ac92ee0
Author: Dave Hansen <haveblue@us.ibm.com>
Date:   Mon Mar 27 01:16:04 2006 -0800

    [PATCH] unify PFN_* macros
    
    Just about every architecture defines some macros to do operations on pfns.
     They're all virtually identical.  This patch consolidates all of them.
    
    One minor glitch is that at least i386 uses them in a very skeletal header
    file.  To keep away from #include dependency hell, I stuck the new
    definitions in a new, isolated header.
    
    Of all of the implementations, sh64 is the only one that varied by a bit.
    It used some masks to ensure that any sign-extension got ripped away before
    the arithmetic is done.  This has been posted to that sh64 maintainers and
    the development list.
    
    Compiles on x86, x86_64, ia64 and ppc64.
    
    Signed-off-by: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 52f7d59fe612..ad89c442f299 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -25,6 +25,7 @@
 #include <linux/highmem.h>
 #include <linux/swap.h>
 #include <linux/proc_fs.h>
+#include <linux/pfn.h>
 
 #include <asm/bootinfo.h>
 #include <asm/cachectl.h>
@@ -177,9 +178,6 @@ void __init paging_init(void)
 	free_area_init(zones_size);
 }
 
-#define PFN_UP(x)	(((x) + PAGE_SIZE - 1) >> PAGE_SHIFT)
-#define PFN_DOWN(x)	((x) >> PAGE_SHIFT)
-
 static inline int page_is_ram(unsigned long pagenr)
 {
 	int i;

commit 7835e98b2e3c66dba79cb0ff8ebb90a2fe030c29
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Mar 22 00:08:40 2006 -0800

    [PATCH] remove set_page_count() outside mm/
    
    set_page_count usage outside mm/ is limited to setting the refcount to 1.
    Remove set_page_count from outside mm/, and replace those users with
    init_page_count() and set_page_refcounted().
    
    This allows more debug checking, and tighter control on how code is allowed
    to play around with page->_count.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index a140da9732db..52f7d59fe612 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -245,7 +245,7 @@ void __init mem_init(void)
 #ifdef CONFIG_LIMITED_DMA
 		set_page_address(page, lowmem_page_address(page));
 #endif
-		set_page_count(page, 1);
+		init_page_count(page);
 		__free_page(page);
 		totalhigh_pages++;
 	}
@@ -292,7 +292,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 
 	for (; start < end; start += PAGE_SIZE) {
 		ClearPageReserved(virt_to_page(start));
-		set_page_count(virt_to_page(start), 1);
+		init_page_count(virt_to_page(start));
 		free_page(start);
 		totalram_pages++;
 	}
@@ -315,7 +315,7 @@ void free_initmem(void)
 		page = addr;
 #endif
 		ClearPageReserved(virt_to_page(page));
-		set_page_count(virt_to_page(page), 1);
+		init_page_count(virt_to_page(page));
 		free_page(page);
 		totalram_pages++;
 		freed += PAGE_SIZE;

commit 8dfcc9ba27e2ed257e5de9539f7f03e57c2c0e33
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Mar 22 00:08:05 2006 -0800

    [PATCH] mm: split highorder pages
    
    Have an explicit mm call to split higher order pages into individual pages.
     Should help to avoid bugs and be more explicit about the code's intention.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Zankel <chris@zankel.net>
    Signed-off-by: Yoichi Yuasa <yoichi_yuasa@tripeaks.co.jp>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 0ff9a348b843..a140da9732db 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -54,7 +54,8 @@ unsigned long empty_zero_page, zero_page_mask;
  */
 unsigned long setup_zero_pages(void)
 {
-	unsigned long order, size;
+	unsigned int order;
+	unsigned long size;
 	struct page *page;
 
 	if (cpu_has_vce)
@@ -67,9 +68,9 @@ unsigned long setup_zero_pages(void)
 		panic("Oh boy, that early out of memory?");
 
 	page = virt_to_page(empty_zero_page);
+	split_page(page, order);
 	while (page < virt_to_page(empty_zero_page + (PAGE_SIZE << order))) {
 		SetPageReserved(page);
-		set_page_count(page, 1);
 		page++;
 	}
 

commit 3d503753b40469b6a19dcc3511f6eb8c55f6d122
Author: Daniel Jacobowitz <dan@debian.org>
Date:   Thu Jan 20 19:59:54 2005 -0500

    [MIPS] Support /proc/kcore for MIPS
    
    I'm pretty sure that the CKSEG0 bits are wrong, but I did need to
    cover that region - because the SB-1 kernel links at 0xffffffff80100000
    or so, disassembly and printing static variables don't work unless the
    debugger can read that region.
    
    Signed-off-by: Daniel Jacobowitz <dan@codesourcery.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 4ee91c9a556f..0ff9a348b843 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -24,6 +24,7 @@
 #include <linux/bootmem.h>
 #include <linux/highmem.h>
 #include <linux/swap.h>
+#include <linux/proc_fs.h>
 
 #include <asm/bootinfo.h>
 #include <asm/cachectl.h>
@@ -200,6 +201,11 @@ static inline int page_is_ram(unsigned long pagenr)
 	return 0;
 }
 
+static struct kcore_list kcore_mem, kcore_vmalloc;
+#ifdef CONFIG_64BIT
+static struct kcore_list kcore_kseg0;
+#endif
+
 void __init mem_init(void)
 {
 	unsigned long codesize, reservedpages, datasize, initsize;
@@ -249,6 +255,16 @@ void __init mem_init(void)
 	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
 	initsize =  (unsigned long) &__init_end - (unsigned long) &__init_begin;
 
+#ifdef CONFIG_64BIT
+	if ((unsigned long) &_text > (unsigned long) CKSEG0)
+		/* The -4 is a hack so that user tools don't have to handle
+		   the overflow.  */
+		kclist_add(&kcore_kseg0, (void *) CKSEG0, 0x80000000 - 4);
+#endif
+	kclist_add(&kcore_mem, __va(0), max_low_pfn << PAGE_SHIFT);
+	kclist_add(&kcore_vmalloc, (void *)VMALLOC_START,
+		   VMALLOC_END-VMALLOC_START);
+
 	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
 	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",
 	       (unsigned long) nr_free_pages() << (PAGE_SHIFT-10),

commit 68352e6ee3675e23b492c51908951058de4f6fe0
Author: Hugh Dickins <hugh@veritas.com>
Date:   Mon Dec 12 00:37:23 2005 -0800

    [PATCH] mips: setup_zero_pages count 1
    
    Page count should be initialized to 1 on each of the MIPS empty zero pages,
    to avoid a bad_page warning whenever one of them is freed from all mappings.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 3a49036e0ae8..4ee91c9a556f 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -67,8 +67,8 @@ unsigned long setup_zero_pages(void)
 
 	page = virt_to_page(empty_zero_page);
 	while (page < virt_to_page(empty_zero_page + (PAGE_SIZE << order))) {
-		set_bit(PG_reserved, &page->flags);
-		reset_page_mapcount(page);
+		SetPageReserved(page);
+		set_page_count(page, 1);
 		page++;
 	}
 

commit d981733aaf970e4537bb8dee9422ee775d92e5ae
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Nov 23 13:49:09 2005 +0000

    [MIPS] Use reset_page_mapcount to initialize empty_zero_page usage counter.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index f75ab748e8cd..3a49036e0ae8 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -68,7 +68,7 @@ unsigned long setup_zero_pages(void)
 	page = virt_to_page(empty_zero_page);
 	while (page < virt_to_page(empty_zero_page + (PAGE_SIZE << order))) {
 		set_bit(PG_reserved, &page->flags);
-		set_page_count(page, 0);
+		reset_page_mapcount(page);
 		page++;
 	}
 

commit dbc571690ec4123e4e9fd6e731d8bdfa77b0d90e
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Oct 29 19:32:38 2005 +0100

    Fix wrong comment.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 5e1967f14255..f75ab748e8cd 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -131,7 +131,7 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 		j = 0;
 	}
 }
-#endif /* CONFIG_64BIT */
+#endif /* CONFIG_32BIT */
 #endif /* CONFIG_HIGHMEM */
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES

commit c6e8b587718c486b55c2ebecc6de231a30beba35
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Feb 10 12:19:59 2005 +0000

    Update MIPS to use the 4-level pagetable code thereby getting rid of
    the compacrapability headers.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 77cbccaff955..5e1967f14255 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -83,7 +83,7 @@ pte_t *kmap_pte;
 pgprot_t kmap_prot;
 
 #define kmap_get_fixmap_pte(vaddr)					\
-	pte_offset_kernel(pmd_offset(pgd_offset_k(vaddr), (vaddr)), (vaddr))
+	pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr), (vaddr)), (vaddr)), (vaddr))
 
 static void __init kmap_init(void)
 {
@@ -101,26 +101,32 @@ void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)
 {
 	pgd_t *pgd;
+	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
-	int i, j;
+	int i, j, k;
 	unsigned long vaddr;
 
 	vaddr = start;
 	i = __pgd_offset(vaddr);
-	j = __pmd_offset(vaddr);
+	j = __pud_offset(vaddr);
+	k = __pmd_offset(vaddr);
 	pgd = pgd_base + i;
 
 	for ( ; (i < PTRS_PER_PGD) && (vaddr != end); pgd++, i++) {
-		pmd = (pmd_t *)pgd;
-		for (; (j < PTRS_PER_PMD) && (vaddr != end); pmd++, j++) {
-			if (pmd_none(*pmd)) {
-				pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
-				set_pmd(pmd, __pmd(pte));
-				if (pte != pte_offset_kernel(pmd, 0))
-					BUG();
+		pud = (pud_t *)pgd;
+		for ( ; (j < PTRS_PER_PUD) && (vaddr != end); pud++, j++) {
+			pmd = (pmd_t *)pud;
+			for (; (k < PTRS_PER_PMD) && (vaddr != end); pmd++, k++) {
+				if (pmd_none(*pmd)) {
+					pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
+					set_pmd(pmd, __pmd(pte));
+					if (pte != pte_offset_kernel(pmd, 0))
+						BUG();
+				}
+				vaddr += PMD_SIZE;
 			}
-			vaddr += PMD_SIZE;
+			k = 0;
 		}
 		j = 0;
 	}

commit 84fd089a425f055ecf4a6a72f2509ccb98314b8f
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Feb 7 16:13:07 2005 +0000

    Delete duplicate copy of fixrange_init.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index dc6830b10fab..77cbccaff955 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -96,8 +96,8 @@ static void __init kmap_init(void)
 	kmap_prot = PAGE_KERNEL;
 }
 
-#ifdef CONFIG_64BIT
-static void __init fixrange_init(unsigned long start, unsigned long end,
+#ifdef CONFIG_32BIT
+void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)
 {
 	pgd_t *pgd;

commit 875d43e72b5bf22161a81de7554f88eccf8a51ae
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Sep 3 15:56:16 2005 -0700

    [PATCH] mips: clean up 32/64-bit configuration
    
    Start cleaning 32-bit vs. 64-bit configuration.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 9c9a271c8a3a..dc6830b10fab 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -96,7 +96,7 @@ static void __init kmap_init(void)
 	kmap_prot = PAGE_KERNEL;
 }
 
-#ifdef CONFIG_MIPS64
+#ifdef CONFIG_64BIT
 static void __init fixrange_init(unsigned long start, unsigned long end,
 	pgd_t *pgd_base)
 {
@@ -125,7 +125,7 @@ static void __init fixrange_init(unsigned long start, unsigned long end,
 		j = 0;
 	}
 }
-#endif /* CONFIG_MIPS64 */
+#endif /* CONFIG_64BIT */
 #endif /* CONFIG_HIGHMEM */
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
@@ -258,7 +258,7 @@ void __init mem_init(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-#ifdef CONFIG_MIPS64
+#ifdef CONFIG_64BIT
 	/* Switch from KSEG0 to XKPHYS addresses */
 	start = (unsigned long)phys_to_virt(CPHYSADDR(start));
 	end = (unsigned long)phys_to_virt(CPHYSADDR(end));
@@ -286,7 +286,7 @@ void free_initmem(void)
 
 	addr = (unsigned long) &__init_begin;
 	while (addr < (unsigned long) &__init_end) {
-#ifdef CONFIG_MIPS64
+#ifdef CONFIG_64BIT
 		page = PAGE_OFFSET | CPHYSADDR(addr);
 #else
 		page = addr;

commit b4819b593740a6d11db07b52e0fe35975b29a185
Author: Yoichi Yuasa <yuasa@hh.iij4u.or.jp>
Date:   Sat Jun 25 14:54:31 2005 -0700

    [PATCH] mips: add MIPS-specific support for flatmem/discontigmem
    
    2.6.12-git6 doesn't boot on some MIPS machines.  They need the support of flat
    memory and discontig memory.
    
    Signed-off-by: Yoichi Yuasa <yuasa@hh.iij4u.or.jp>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index 73843c528778..9c9a271c8a3a 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -128,7 +128,7 @@ static void __init fixrange_init(unsigned long start, unsigned long end,
 #endif /* CONFIG_MIPS64 */
 #endif /* CONFIG_HIGHMEM */
 
-#ifndef CONFIG_DISCONTIGMEM
+#ifndef CONFIG_NEED_MULTIPLE_NODES
 extern void pagetable_init(void);
 
 void __init paging_init(void)
@@ -253,7 +253,7 @@ void __init mem_init(void)
 	       initsize >> 10,
 	       (unsigned long) (totalhigh_pages << (PAGE_SHIFT-10)));
 }
-#endif /* !CONFIG_DISCONTIGMEM */
+#endif /* !CONFIG_NEED_MULTIPLE_NODES */
 
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)

commit cbe37d093707762fc0abb280781e6a82a9d8d568
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Tue Jun 21 17:14:52 2005 -0700

    [PATCH] mm: remove PG_highmem
    
    Remove PG_highmem, to save a page flag.  Use is_highmem() instead.  It'll
    generate a little more code, but we don't use PageHigheMem() in many places.
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
index b027ce7efbc6..73843c528778 100644
--- a/arch/mips/mm/init.c
+++ b/arch/mips/mm/init.c
@@ -232,7 +232,6 @@ void __init mem_init(void)
 #ifdef CONFIG_LIMITED_DMA
 		set_page_address(page, lowmem_page_address(page));
 #endif
-		set_bit(PG_highmem, &page->flags);
 		set_page_count(page, 1);
 		__free_page(page);
 		totalhigh_pages++;

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/mips/mm/init.c b/arch/mips/mm/init.c
new file mode 100644
index 000000000000..b027ce7efbc6
--- /dev/null
+++ b/arch/mips/mm/init.c
@@ -0,0 +1,304 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 1994 - 2000 Ralf Baechle
+ * Copyright (C) 1999, 2000 Silicon Graphics, Inc.
+ * Kevin D. Kissell, kevink@mips.com and Carsten Langgaard, carstenl@mips.com
+ * Copyright (C) 2000 MIPS Technologies, Inc.  All rights reserved.
+ */
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/pagemap.h>
+#include <linux/ptrace.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/bootmem.h>
+#include <linux/highmem.h>
+#include <linux/swap.h>
+
+#include <asm/bootinfo.h>
+#include <asm/cachectl.h>
+#include <asm/cpu.h>
+#include <asm/dma.h>
+#include <asm/mmu_context.h>
+#include <asm/sections.h>
+#include <asm/pgtable.h>
+#include <asm/pgalloc.h>
+#include <asm/tlb.h>
+
+DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+
+unsigned long highstart_pfn, highend_pfn;
+
+/*
+ * We have up to 8 empty zeroed pages so we can map one of the right colour
+ * when needed.  This is necessary only on R4000 / R4400 SC and MC versions
+ * where we have to avoid VCED / VECI exceptions for good performance at
+ * any price.  Since page is never written to after the initialization we
+ * don't have to care about aliases on other CPUs.
+ */
+unsigned long empty_zero_page, zero_page_mask;
+
+/*
+ * Not static inline because used by IP27 special magic initialization code
+ */
+unsigned long setup_zero_pages(void)
+{
+	unsigned long order, size;
+	struct page *page;
+
+	if (cpu_has_vce)
+		order = 3;
+	else
+		order = 0;
+
+	empty_zero_page = __get_free_pages(GFP_KERNEL | __GFP_ZERO, order);
+	if (!empty_zero_page)
+		panic("Oh boy, that early out of memory?");
+
+	page = virt_to_page(empty_zero_page);
+	while (page < virt_to_page(empty_zero_page + (PAGE_SIZE << order))) {
+		set_bit(PG_reserved, &page->flags);
+		set_page_count(page, 0);
+		page++;
+	}
+
+	size = PAGE_SIZE << order;
+	zero_page_mask = (size - 1) & PAGE_MASK;
+
+	return 1UL << order;
+}
+
+#ifdef CONFIG_HIGHMEM
+pte_t *kmap_pte;
+pgprot_t kmap_prot;
+
+#define kmap_get_fixmap_pte(vaddr)					\
+	pte_offset_kernel(pmd_offset(pgd_offset_k(vaddr), (vaddr)), (vaddr))
+
+static void __init kmap_init(void)
+{
+	unsigned long kmap_vstart;
+
+	/* cache the first kmap pte */
+	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
+	kmap_pte = kmap_get_fixmap_pte(kmap_vstart);
+
+	kmap_prot = PAGE_KERNEL;
+}
+
+#ifdef CONFIG_MIPS64
+static void __init fixrange_init(unsigned long start, unsigned long end,
+	pgd_t *pgd_base)
+{
+	pgd_t *pgd;
+	pmd_t *pmd;
+	pte_t *pte;
+	int i, j;
+	unsigned long vaddr;
+
+	vaddr = start;
+	i = __pgd_offset(vaddr);
+	j = __pmd_offset(vaddr);
+	pgd = pgd_base + i;
+
+	for ( ; (i < PTRS_PER_PGD) && (vaddr != end); pgd++, i++) {
+		pmd = (pmd_t *)pgd;
+		for (; (j < PTRS_PER_PMD) && (vaddr != end); pmd++, j++) {
+			if (pmd_none(*pmd)) {
+				pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
+				set_pmd(pmd, __pmd(pte));
+				if (pte != pte_offset_kernel(pmd, 0))
+					BUG();
+			}
+			vaddr += PMD_SIZE;
+		}
+		j = 0;
+	}
+}
+#endif /* CONFIG_MIPS64 */
+#endif /* CONFIG_HIGHMEM */
+
+#ifndef CONFIG_DISCONTIGMEM
+extern void pagetable_init(void);
+
+void __init paging_init(void)
+{
+	unsigned long zones_size[MAX_NR_ZONES] = {0, 0, 0};
+	unsigned long max_dma, high, low;
+
+	pagetable_init();
+
+#ifdef CONFIG_HIGHMEM
+	kmap_init();
+#endif
+
+	max_dma = virt_to_phys((char *)MAX_DMA_ADDRESS) >> PAGE_SHIFT;
+	low = max_low_pfn;
+	high = highend_pfn;
+
+#ifdef CONFIG_ISA
+	if (low < max_dma)
+		zones_size[ZONE_DMA] = low;
+	else {
+		zones_size[ZONE_DMA] = max_dma;
+		zones_size[ZONE_NORMAL] = low - max_dma;
+	}
+#else
+	zones_size[ZONE_DMA] = low;
+#endif
+#ifdef CONFIG_HIGHMEM
+	if (cpu_has_dc_aliases) {
+		printk(KERN_WARNING "This processor doesn't support highmem.");
+		if (high - low)
+			printk(" %ldk highmem ignored", high - low);
+		printk("\n");
+	} else
+		zones_size[ZONE_HIGHMEM] = high - low;
+#endif
+
+	free_area_init(zones_size);
+}
+
+#define PFN_UP(x)	(((x) + PAGE_SIZE - 1) >> PAGE_SHIFT)
+#define PFN_DOWN(x)	((x) >> PAGE_SHIFT)
+
+static inline int page_is_ram(unsigned long pagenr)
+{
+	int i;
+
+	for (i = 0; i < boot_mem_map.nr_map; i++) {
+		unsigned long addr, end;
+
+		if (boot_mem_map.map[i].type != BOOT_MEM_RAM)
+			/* not usable memory */
+			continue;
+
+		addr = PFN_UP(boot_mem_map.map[i].addr);
+		end = PFN_DOWN(boot_mem_map.map[i].addr +
+			       boot_mem_map.map[i].size);
+
+		if (pagenr >= addr && pagenr < end)
+			return 1;
+	}
+
+	return 0;
+}
+
+void __init mem_init(void)
+{
+	unsigned long codesize, reservedpages, datasize, initsize;
+	unsigned long tmp, ram;
+
+#ifdef CONFIG_HIGHMEM
+#ifdef CONFIG_DISCONTIGMEM
+#error "CONFIG_HIGHMEM and CONFIG_DISCONTIGMEM dont work together yet"
+#endif
+	max_mapnr = num_physpages = highend_pfn;
+#else
+	max_mapnr = num_physpages = max_low_pfn;
+#endif
+	high_memory = (void *) __va(max_low_pfn << PAGE_SHIFT);
+
+	totalram_pages += free_all_bootmem();
+	totalram_pages -= setup_zero_pages();	/* Setup zeroed pages.  */
+
+	reservedpages = ram = 0;
+	for (tmp = 0; tmp < max_low_pfn; tmp++)
+		if (page_is_ram(tmp)) {
+			ram++;
+			if (PageReserved(mem_map+tmp))
+				reservedpages++;
+		}
+
+#ifdef CONFIG_HIGHMEM
+	for (tmp = highstart_pfn; tmp < highend_pfn; tmp++) {
+		struct page *page = mem_map + tmp;
+
+		if (!page_is_ram(tmp)) {
+			SetPageReserved(page);
+			continue;
+		}
+		ClearPageReserved(page);
+#ifdef CONFIG_LIMITED_DMA
+		set_page_address(page, lowmem_page_address(page));
+#endif
+		set_bit(PG_highmem, &page->flags);
+		set_page_count(page, 1);
+		__free_page(page);
+		totalhigh_pages++;
+	}
+	totalram_pages += totalhigh_pages;
+#endif
+
+	codesize =  (unsigned long) &_etext - (unsigned long) &_text;
+	datasize =  (unsigned long) &_edata - (unsigned long) &_etext;
+	initsize =  (unsigned long) &__init_end - (unsigned long) &__init_begin;
+
+	printk(KERN_INFO "Memory: %luk/%luk available (%ldk kernel code, "
+	       "%ldk reserved, %ldk data, %ldk init, %ldk highmem)\n",
+	       (unsigned long) nr_free_pages() << (PAGE_SHIFT-10),
+	       ram << (PAGE_SHIFT-10),
+	       codesize >> 10,
+	       reservedpages << (PAGE_SHIFT-10),
+	       datasize >> 10,
+	       initsize >> 10,
+	       (unsigned long) (totalhigh_pages << (PAGE_SHIFT-10)));
+}
+#endif /* !CONFIG_DISCONTIGMEM */
+
+#ifdef CONFIG_BLK_DEV_INITRD
+void free_initrd_mem(unsigned long start, unsigned long end)
+{
+#ifdef CONFIG_MIPS64
+	/* Switch from KSEG0 to XKPHYS addresses */
+	start = (unsigned long)phys_to_virt(CPHYSADDR(start));
+	end = (unsigned long)phys_to_virt(CPHYSADDR(end));
+#endif
+	if (start < end)
+		printk(KERN_INFO "Freeing initrd memory: %ldk freed\n",
+		       (end - start) >> 10);
+
+	for (; start < end; start += PAGE_SIZE) {
+		ClearPageReserved(virt_to_page(start));
+		set_page_count(virt_to_page(start), 1);
+		free_page(start);
+		totalram_pages++;
+	}
+}
+#endif
+
+extern unsigned long prom_free_prom_memory(void);
+
+void free_initmem(void)
+{
+	unsigned long addr, page, freed;
+
+	freed = prom_free_prom_memory();
+
+	addr = (unsigned long) &__init_begin;
+	while (addr < (unsigned long) &__init_end) {
+#ifdef CONFIG_MIPS64
+		page = PAGE_OFFSET | CPHYSADDR(addr);
+#else
+		page = addr;
+#endif
+		ClearPageReserved(virt_to_page(page));
+		set_page_count(virt_to_page(page), 1);
+		free_page(page);
+		totalram_pages++;
+		freed += PAGE_SIZE;
+		addr += PAGE_SIZE;
+	}
+	printk(KERN_INFO "Freeing unused kernel memory: %ldk freed\n",
+	       freed >> 10);
+}
