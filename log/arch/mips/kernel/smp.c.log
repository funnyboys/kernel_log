commit d71e064449a704a026fa032ec852d532f08aefa1
Merge: 58233ccf9460 ba15533275dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 08:51:45 2020 -0700

    Merge tag 'mips_5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mips/linux
    
    Pull MIPS updates from Thomas Bogendoerfer:
    
     - loongson64 irq rework
    
     - dmi support loongson
    
     - replace setup_irq() by request_irq()
    
     - jazz cleanups
    
     - minor cleanups and fixes
    
    * tag 'mips_5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/mips/linux: (44 commits)
      MIPS: ralink: mt7621: Fix soc_device introduction
      MIPS: Exclude more dsemul code when CONFIG_MIPS_FP_SUPPORT=n
      MIPS/tlbex: Fix LDDIR usage in setup_pw() for Loongson-3
      MIPS: do not compile generic functions for CONFIG_CAVIUM_OCTEON_SOC
      MAINTAINERS: Update Loongson64 entry
      MIPS: Loongson64: Load built-in dtbs
      MIPS: Loongson64: Add generic dts
      dt-bindings: mips: Add loongson boards
      MIPS: Loongson64: Drop legacy IRQ code
      dt-bindings: interrupt-controller: Add Loongson-3 HTPIC
      irqchip: Add driver for Loongson-3 HyperTransport PIC controller
      dt-bindings: interrupt-controller: Add Loongson LIOINTC
      irqchip: loongson-liointc: Workaround LPC IRQ Errata
      irqchip: Add driver for Loongson I/O Local Interrupt Controller
      docs: mips: remove no longer needed au1xxx_ide.rst documentation
      MIPS: Alchemy: remove no longer used au1xxx_ide.h header
      ide: remove no longer used au1xxx-ide driver
      MIPS: Add support for Desktop Management Interface (DMI)
      firmware: dmi: Add macro SMBIOS_ENTRY_POINT_SCAN_START
      MIPS: ralink: mt7621: introduce 'soc_device' initialization
      ...

commit e188f0a50f637391f440b9bf0a1066db71a20889
Author: Peter Xu <peterx@redhat.com>
Date:   Mon Dec 16 16:31:24 2019 -0500

    MIPS: smp: Remove tick_broadcast_count
    
    Now smp_call_function_single_async() provides the protection that
    we'll return with -EBUSY if the csd object is still pending, then we
    don't need the tick_broadcast_count counter any more.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20191216213125.9536-3-peterx@redhat.com

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index f510c00bda88..0def6242b3ea 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -696,29 +696,22 @@ EXPORT_SYMBOL(flush_tlb_one);
 
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 
-static DEFINE_PER_CPU(atomic_t, tick_broadcast_count);
 static DEFINE_PER_CPU(call_single_data_t, tick_broadcast_csd);
 
 void tick_broadcast(const struct cpumask *mask)
 {
-	atomic_t *count;
 	call_single_data_t *csd;
 	int cpu;
 
 	for_each_cpu(cpu, mask) {
-		count = &per_cpu(tick_broadcast_count, cpu);
 		csd = &per_cpu(tick_broadcast_csd, cpu);
-
-		if (atomic_inc_return(count) == 1)
-			smp_call_function_single_async(cpu, csd);
+		smp_call_function_single_async(cpu, csd);
 	}
 }
 
 static void tick_broadcast_callee(void *info)
 {
-	int cpu = smp_processor_id();
 	tick_receive_broadcast();
-	atomic_set(&per_cpu(tick_broadcast_count, cpu), 0);
 }
 
 static int __init tick_broadcast_init(void)

commit ac8fd122e070ce0e60c608d4f085f7af77290844
Author: afzal mohammed <afzal.mohd.ma@gmail.com>
Date:   Thu Mar 5 17:27:53 2020 +0530

    MIPS: Replace setup_irq() by request_irq()
    
    request_irq() is preferred over setup_irq(). Invocations of setup_irq()
    occur after memory allocators are ready.
    
    Per tglx[1], setup_irq() existed in olden days when allocators were not
    ready by the time early interrupts were initialized.
    
    Hence replace setup_irq() by request_irq().
    
    remove_irq() has been replaced by free_irq() as well.
    
    There were build error's during previous version, couple of which was
    reported by kbuild test robot <lkp@intel.com> of which one was reported
    by Thomas Bogendoerfer <tsbogend@alpha.franken.de> as well. There were a
    few more issues including build errors, those also have been fixed.
    
    [1] https://lkml.kernel.org/r/alpine.DEB.2.20.1710191609480.1971@nanos
    
    Signed-off-by: afzal mohammed <afzal.mohd.ma@gmail.com>
    Signed-off-by: Thomas Bogendoerfer <tsbogend@alpha.franken.de>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index f510c00bda88..d0e911f2421b 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -207,25 +207,13 @@ static irqreturn_t ipi_call_interrupt(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
-static struct irqaction irq_resched = {
-	.handler	= ipi_resched_interrupt,
-	.flags		= IRQF_PERCPU,
-	.name		= "IPI resched"
-};
-
-static struct irqaction irq_call = {
-	.handler	= ipi_call_interrupt,
-	.flags		= IRQF_PERCPU,
-	.name		= "IPI call"
-};
-
-static void smp_ipi_init_one(unsigned int virq,
-				    struct irqaction *action)
+static void smp_ipi_init_one(unsigned int virq, const char *name,
+			     irq_handler_t handler)
 {
 	int ret;
 
 	irq_set_handler(virq, handle_percpu_irq);
-	ret = setup_irq(virq, action);
+	ret = request_irq(virq, handler, IRQF_PERCPU, name, NULL);
 	BUG_ON(ret);
 }
 
@@ -278,12 +266,15 @@ int mips_smp_ipi_allocate(const struct cpumask *mask)
 		int cpu;
 
 		for_each_cpu(cpu, mask) {
-			smp_ipi_init_one(call_virq + cpu, &irq_call);
-			smp_ipi_init_one(sched_virq + cpu, &irq_resched);
+			smp_ipi_init_one(call_virq + cpu, "IPI call",
+					 ipi_call_interrupt);
+			smp_ipi_init_one(sched_virq + cpu, "IPI resched",
+					 ipi_resched_interrupt);
 		}
 	} else {
-		smp_ipi_init_one(call_virq, &irq_call);
-		smp_ipi_init_one(sched_virq, &irq_resched);
+		smp_ipi_init_one(call_virq, "IPI call", ipi_call_interrupt);
+		smp_ipi_init_one(sched_virq, "IPI resched",
+				 ipi_resched_interrupt);
 	}
 
 	return 0;
@@ -311,8 +302,8 @@ int mips_smp_ipi_free(const struct cpumask *mask)
 		int cpu;
 
 		for_each_cpu(cpu, mask) {
-			remove_irq(call_virq + cpu, &irq_call);
-			remove_irq(sched_virq + cpu, &irq_resched);
+			free_irq(call_virq + cpu, NULL);
+			free_irq(sched_virq + cpu, NULL);
 		}
 	}
 	irq_destroy_ipi(call_virq, mask);

commit 1a59d1b8e05ea6ab45f7e18897de1ef0e6bc3da6
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:05 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 156
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version this program is distributed in the
      hope that it will be useful but without any warranty without even
      the implied warranty of merchantability or fitness for a particular
      purpose see the gnu general public license for more details you
      should have received a copy of the gnu general public license along
      with this program if not write to the free software foundation inc
      59 temple place suite 330 boston ma 02111 1307 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 1334 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070033.113240726@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bc4bb3c6bd00..f510c00bda88 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -1,17 +1,5 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version 2
- * of the License, or (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
  *
  * Copyright (C) 2000, 2001 Kanoj Sarcar
  * Copyright (C) 2000, 2001 Ralf Baechle

commit 2c8656204742a5e2d373972b139d0cc26ae93ff0
Author: Thomas Bogendoerfer <tbogendoerfer@suse.de>
Date:   Tue Feb 19 16:57:19 2019 +0100

    MIPS: SGI-IP27: do boot CPU init later
    
    To make use of per_cpu variables in interrupt code per_cpu_init() must
    be done after setup_per_cpu_areas(). This is achieved by calling it
    in smp_prepare_boot_cpu() via a new smp_ops method.
    
    Signed-off-by: Thomas Bogendoerfer <tbogendoerfer@suse.de>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6fd9e94fc87e..bc4bb3c6bd00 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -444,6 +444,8 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 /* preload SMP state for boot cpu */
 void smp_prepare_boot_cpu(void)
 {
+	if (mp_ops->prepare_boot_cpu)
+		mp_ops->prepare_boot_cpu();
 	set_cpu_possible(0, true);
 	set_cpu_online(0, true);
 }

commit c8790d657b0a8d42801fb4536f6f106b4b6306e8
Author: Paul Burton <paul.burton@mips.com>
Date:   Sat Feb 2 01:43:28 2019 +0000

    MIPS: MemoryMapID (MMID) Support
    
    Introduce support for using MemoryMapIDs (MMIDs) as an alternative to
    Address Space IDs (ASIDs). The major difference between the two is that
    MMIDs are global - ie. an MMID uniquely identifies an address space
    across all coherent CPUs. In contrast ASIDs are non-global per-CPU IDs,
    wherein each address space is allocated a separate ASID for each CPU
    upon which it is used. This global namespace allows a new GINVT
    instruction be used to globally invalidate TLB entries associated with a
    particular MMID across all coherent CPUs in the system, removing the
    need for IPIs to invalidate entries with separate ASIDs on each CPU.
    
    The allocation scheme used here is largely borrowed from arm64 (see
    arch/arm64/mm/context.c). In essence we maintain a bitmap to track
    available MMIDs, and MMIDs in active use at the time of a rollover to a
    new MMID version are preserved in the new version. The allocation scheme
    requires efficient 64 bit atomics in order to perform reasonably, so
    this support depends upon CONFIG_GENERIC_ATOMIC64=n (ie. currently it
    will only be included in MIPS64 kernels).
    
    The first, and currently only, available CPU with support for MMIDs is
    the MIPS I6500. This CPU supports 16 bit MMIDs, and so for now we cap
    our MMIDs to 16 bits wide in order to prevent the bitmap growing to
    absurd sizes if any future CPU does implement 32 bit MMIDs as the
    architecture manuals suggest is recommended.
    
    When MMIDs are in use we also make use of GINVT instruction which is
    available due to the global nature of MMIDs. By executing a sequence of
    GINVT & SYNC 0x14 instructions we can avoid the overhead of an IPI to
    each remote CPU in many cases. One complication is that GINVT will
    invalidate wired entries (in all cases apart from type 0, which targets
    the entire TLB). In order to avoid GINVT invalidating any wired TLB
    entries we set up, we make sure to create those entries using a reserved
    MMID (0) that we never associate with any address space.
    
    Also of note is that KVM will require further work in order to support
    MMIDs & GINVT, since KVM is involved in allocating IDs for guests & in
    configuring the MMU. That work is not part of this patch, so for now
    when MMIDs are in use KVM is disabled.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index f9dbd95e1d68..6fd9e94fc87e 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -39,6 +39,7 @@
 
 #include <linux/atomic.h>
 #include <asm/cpu.h>
+#include <asm/ginvt.h>
 #include <asm/processor.h>
 #include <asm/idle.h>
 #include <asm/r4k-timer.h>
@@ -482,6 +483,15 @@ static void flush_tlb_all_ipi(void *info)
 
 void flush_tlb_all(void)
 {
+	if (cpu_has_mmid) {
+		htw_stop();
+		ginvt_full();
+		sync_ginv();
+		instruction_hazard();
+		htw_start();
+		return;
+	}
+
 	on_each_cpu(flush_tlb_all_ipi, NULL, 1);
 }
 
@@ -530,7 +540,12 @@ void flush_tlb_mm(struct mm_struct *mm)
 {
 	preempt_disable();
 
-	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
+	if (cpu_has_mmid) {
+		/*
+		 * No need to worry about other CPUs - the ginvt in
+		 * drop_mmu_context() will be globalized.
+		 */
+	} else if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		smp_on_other_tlbs(flush_tlb_mm_ipi, mm);
 	} else {
 		unsigned int cpu;
@@ -561,9 +576,26 @@ static void flush_tlb_range_ipi(void *info)
 void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
 {
 	struct mm_struct *mm = vma->vm_mm;
+	unsigned long addr;
+	u32 old_mmid;
 
 	preempt_disable();
-	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
+	if (cpu_has_mmid) {
+		htw_stop();
+		old_mmid = read_c0_memorymapid();
+		write_c0_memorymapid(cpu_asid(0, mm));
+		mtc0_tlbw_hazard();
+		addr = round_down(start, PAGE_SIZE * 2);
+		end = round_up(end, PAGE_SIZE * 2);
+		do {
+			ginvt_va_mmid(addr);
+			sync_ginv();
+			addr += PAGE_SIZE * 2;
+		} while (addr < end);
+		write_c0_memorymapid(old_mmid);
+		instruction_hazard();
+		htw_start();
+	} else if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		struct flush_tlb_data fd = {
 			.vma = vma,
 			.addr1 = start,
@@ -571,6 +603,7 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 		};
 
 		smp_on_other_tlbs(flush_tlb_range_ipi, &fd);
+		local_flush_tlb_range(vma, start, end);
 	} else {
 		unsigned int cpu;
 		int exec = vma->vm_flags & VM_EXEC;
@@ -585,8 +618,8 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 			if (cpu != smp_processor_id() && cpu_context(cpu, mm))
 				set_cpu_context(cpu, mm, !exec);
 		}
+		local_flush_tlb_range(vma, start, end);
 	}
-	local_flush_tlb_range(vma, start, end);
 	preempt_enable();
 }
 
@@ -616,14 +649,28 @@ static void flush_tlb_page_ipi(void *info)
 
 void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 {
+	u32 old_mmid;
+
 	preempt_disable();
-	if ((atomic_read(&vma->vm_mm->mm_users) != 1) || (current->mm != vma->vm_mm)) {
+	if (cpu_has_mmid) {
+		htw_stop();
+		old_mmid = read_c0_memorymapid();
+		write_c0_memorymapid(cpu_asid(0, vma->vm_mm));
+		mtc0_tlbw_hazard();
+		ginvt_va_mmid(page);
+		sync_ginv();
+		write_c0_memorymapid(old_mmid);
+		instruction_hazard();
+		htw_start();
+	} else if ((atomic_read(&vma->vm_mm->mm_users) != 1) ||
+		   (current->mm != vma->vm_mm)) {
 		struct flush_tlb_data fd = {
 			.vma = vma,
 			.addr1 = page,
 		};
 
 		smp_on_other_tlbs(flush_tlb_page_ipi, &fd);
+		local_flush_tlb_page(vma, page);
 	} else {
 		unsigned int cpu;
 
@@ -637,8 +684,8 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 			if (cpu != smp_processor_id() && cpu_context(cpu, vma->vm_mm))
 				set_cpu_context(cpu, vma->vm_mm, 1);
 		}
+		local_flush_tlb_page(vma, page);
 	}
-	local_flush_tlb_page(vma, page);
 	preempt_enable();
 }
 

commit 0b317c389c6771cbe1c5a12fe9322285a808a9bd
Author: Paul Burton <paul.burton@mips.com>
Date:   Sat Feb 2 01:43:25 2019 +0000

    MIPS: mm: Add set_cpu_context() for ASID assignments
    
    When we gain MMID support we'll be storing MMIDs as atomic64_t values
    and accessing them via atomic64_* functions. This necessitates that we
    don't use cpu_context() as the left hand side of an assignment, ie. as a
    modifiable lvalue. In preparation for this introduce a new
    set_cpu_context() function & replace all assignments with cpu_context()
    on their left hand side with an equivalent call to set_cpu_context().
    
    To enforce that cpu_context() should not be used for assignments, we
    rewrite it as a static inline function.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index d7088ca31e43..f9dbd95e1d68 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -537,7 +537,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 
 		for_each_online_cpu(cpu) {
 			if (cpu != smp_processor_id() && cpu_context(cpu, mm))
-				cpu_context(cpu, mm) = 0;
+				set_cpu_context(cpu, mm, 0);
 		}
 	}
 	drop_mmu_context(mm);
@@ -583,7 +583,7 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 			 * mm has been completely unused by that CPU.
 			 */
 			if (cpu != smp_processor_id() && cpu_context(cpu, mm))
-				cpu_context(cpu, mm) = !exec;
+				set_cpu_context(cpu, mm, !exec);
 		}
 	}
 	local_flush_tlb_range(vma, start, end);
@@ -635,7 +635,7 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 			 * by that CPU.
 			 */
 			if (cpu != smp_processor_id() && cpu_context(cpu, vma->vm_mm))
-				cpu_context(cpu, vma->vm_mm) = 1;
+				set_cpu_context(cpu, vma->vm_mm, 1);
 		}
 	}
 	local_flush_tlb_page(vma, page);

commit 558ec8ad71c9ec8fd67f388dd74149f9bd095878
Author: Paul Burton <paul.burton@mips.com>
Date:   Sat Feb 2 01:43:22 2019 +0000

    MIPS: mm: Remove local_flush_tlb_mm()
    
    All 3 variants of local_flush_tlb_mm() are now effectively simple calls
    to drop_mmu_context(). Remove them and use drop_mmu_context() directly.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: linux-mips@vger.kernel.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index d84b9066b465..d7088ca31e43 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -487,7 +487,7 @@ void flush_tlb_all(void)
 
 static void flush_tlb_mm_ipi(void *mm)
 {
-	local_flush_tlb_mm((struct mm_struct *)mm);
+	drop_mmu_context((struct mm_struct *)mm);
 }
 
 /*
@@ -540,7 +540,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 				cpu_context(cpu, mm) = 0;
 		}
 	}
-	local_flush_tlb_mm(mm);
+	drop_mmu_context(mm);
 
 	preempt_enable();
 }

commit 892204e06cb9e89fbc4b299a678f9ca358e97cac
Merge: c9b012e5f4a1 e0c5f36b2a63
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 11:36:08 2017 -0800

    Merge tag 'mips_4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/jhogan/mips
    
    Pull MIPS updates from James Hogan:
     "These are the main MIPS changes for 4.15.
    
      Fixes:
       - ralink: Fix MT7620 PCI build issues (4.5)
       - Disable cmpxchg64() and HAVE_VIRT_CPU_ACCOUNTING_GEN for 32-bit SMP
         (4.1)
       - Fix MIPS64 FP save/restore on 32-bit kernels (4.0)
       - ptrace: Pick up ptrace/seccomp changed syscall numbers (3.19)
       - ralink: Fix MT7628 pinmux (3.19)
       - BCM47XX: Fix LED inversion on WRT54GSv1 (3.17)
       - Fix n32 core dumping as o32 since regset support (3.13)
       - ralink: Drop obsolete USB_ARCH_HAS_HCD select
    
      Build system:
       - Default to "generic" (multiplatform) system type instead of IP22
       - Use generic little endian MIPS32 r2 configuration as default
         defconfig instead of ip22_defconfig
    
      FPU emulation:
       - Fix exception generation for certain R6 FPU instructions
    
      SMP:
       - Allow __cpu_number_map to be larger than NR_CPUS for sparse CPU id
         spaces
    
      Miscellaneous:
       - Add iomem resource for kernel bss section for kexec/kdump
       - Atomics: Nudge writes on bit unlock
       - DT files: Standardise "ok" -> "okay"
    
      Minor cleanups:
       - Define virt_to_pfn()
       - Make thread_saved_pc static
       - Simplify 32-bit sign extension in __read_64bit_c0_split()
       - DMA: Use vma_pages() helper
       - FPU emulation: Replace unsigned with unsigned int
       - MM: Removed unused lastpfn
       - Alchemy: Make clk_ops const
       - Lasat: Use setup_timer() helper
       - ralink: Use BIT() in MT7620 PCI driver
    
      Platform support:
    
      BMIPS:
      - Enable HARDIRQS_SW_RESEND
    
      Broadcom BCM63XX:
      - Add clkdev lookup support
      - Update clk driver, UART driver, DTs to handle named refclk from DTs
      - Split apart various clocks to more closely match hardware
      - Add ethernet clocks
    
      Cavium Octeon:
      - Remove usage of cvmx_wait() in favour of __delay()
    
      ImgTec Pistachio:
      - DT: Drop deprecated dwmmc num-slots property
    
      Ingenic JZ4780:
      - Add NFS root to Ci20 defconfig
      - Add watchdog to Ci20 DT & defconfig, and allow building of watchdog
        driver with this SoC
    
      Generic (multiplatform):
      - Migrate xilfpga (MIPSfpga) platform to the generic platform
    
      Lantiq xway:
      - Fix ASC0/ASC1 clocks"
    
    * tag 'mips_4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/jhogan/mips: (46 commits)
      MIPS: Add iomem resource for kernel bss section.
      MIPS: cmpxchg64() and HAVE_VIRT_CPU_ACCOUNTING_GEN don't work for 32-bit SMP
      MIPS: BMIPS: Enable HARDIRQS_SW_RESEND
      MIPS: pci: Make use of the BIT() macro inside the mt7620 driver
      MIPS: pci: Remove KERN_WARN instance inside the mt7620 driver
      MIPS: pci: Remove duplicate define in mt7620 driver
      MIPS: ralink: Fix typo in mt7628 pinmux function
      MIPS: ralink: Fix MT7628 pinmux
      MIPS: Fix odd fp register warnings with MIPS64r2
      watchdog: jz4780: Allow selection of jz4740-wdt driver
      MIPS/ptrace: Update syscall nr on register changes
      MIPS/ptrace: Pick up ptrace/seccomp changed syscalls
      MIPS: Fix an n32 core file generation regset support regression
      MIPS: Fix MIPS64 FP save/restore on 32-bit kernels
      MIPS: page.h: Define virt_to_pfn()
      MIPS: Xilfpga: Switch to using generic defconfigs
      MIPS: generic: Add support for MIPSfpga
      MIPS: Set defconfig target to a generic system for 32r2el
      MIPS: Kconfig: Set default MIPS system type as generic
      MIPS: DTS: Remove num-slots from Pistachio SoC
      ...

commit 7820b84be844d8d863122b1323f2a4bc2441b783
Author: David Daney <david.daney@cavium.com>
Date:   Thu Sep 28 12:34:04 2017 -0500

    MIPS: Allow __cpu_number_map to be larger than NR_CPUS
    
    In systems where the CPU id space is sparse, this allows a smaller
    NR_CPUS to be chosen, thus keeping internal data structures smaller.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Carlos Munoz <cmunoz@caviumnetworks.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17388/
    [jhogan@kernel.org: Add depends on SMP to fix
     "warning: symbol value '' invalid for MIPS_NR_CPU_NR_MAP"]
    Signed-off-by: James Hogan <jhogan@kernel.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bbe19b64def5..5576888e4a2a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -48,7 +48,7 @@
 #include <asm/setup.h>
 #include <asm/maar.h>
 
-int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
+int __cpu_number_map[CONFIG_MIPS_NR_CPU_NR_MAP];   /* Map physical to logical */
 EXPORT_SYMBOL(__cpu_number_map);
 
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */

commit 9e8c399a88f0b87e41a894911475ed2a8f8dff9e
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 27 10:13:25 2017 +0100

    MIPS: SMP: Fix deadlock & online race
    
    Commit 6f542ebeaee0 ("MIPS: Fix race on setting and getting
    cpu_online_mask") effectively reverted commit 8f46cca1e6c06 ("MIPS: SMP:
    Fix possibility of deadlock when bringing CPUs online") and thus has
    reinstated the possibility of deadlock.
    
    The commit was based on testing of kernel v4.4, where the CPU hotplug
    core code issued a BUG() if the starting CPU is not marked online when
    the boot CPU returns from __cpu_up. The commit fixes this race (in
    v4.4), but re-introduces the deadlock situation.
    
    As noted in the commit message, upstream differs in this area. Commit
    8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu bring itself fully up")
    adds a completion event in the CPU hotplug core code, making this race
    impossible. However, people were unhappy with relying on the core code
    to do the right thing.
    
    To address the issues both commits were trying to fix, add a second
    completion event in the MIPS smp hotplug path. It removes the
    possibility of a race, since the MIPS smp hotplug code now synchronises
    both the boot and secondary CPUs before they return to the hotplug core
    code. It also addresses the deadlock by ensuring that the secondary CPU
    is not marked online before it's counters are synchronised.
    
    This fix should also be backported to fix the race condition introduced
    by the backport of commit 8f46cca1e6c06 ("MIPS: SMP: Fix possibility of
    deadlock when bringing CPUs online"), through really that race only
    existed before commit 8df3e07e7f21f ("cpu/hotplug: Let upcoming cpu
    bring itself fully up").
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Fixes: 6f542ebeaee0 ("MIPS: Fix race on setting and getting cpu_online_mask")
    CC: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Cc: <stable@vger.kernel.org> # v4.1+: 8f46cca1e6c0: "MIPS: SMP: Fix possibility of deadlock when bringing CPUs online"
    Cc: <stable@vger.kernel.org> # v4.1+: a00eeede507c: "MIPS: SMP: Use a completion event to signal CPU up"
    Cc: <stable@vger.kernel.org> # v4.1+: 6f542ebeaee0: "MIPS: Fix race on setting and getting cpu_online_mask"
    Cc: <stable@vger.kernel.org> # v4.1+
    Patchwork: https://patchwork.linux-mips.org/patch/17376/
    Signed-off-by: James Hogan <jhogan@kernel.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 3cd2f70d1c18..88be966d3e61 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -66,6 +66,7 @@ EXPORT_SYMBOL(cpu_sibling_map);
 cpumask_t cpu_core_map[NR_CPUS] __read_mostly;
 EXPORT_SYMBOL(cpu_core_map);
 
+static DECLARE_COMPLETION(cpu_starting);
 static DECLARE_COMPLETION(cpu_running);
 
 /*
@@ -374,6 +375,12 @@ asmlinkage void start_secondary(void)
 	cpumask_set_cpu(cpu, &cpu_coherent_mask);
 	notify_cpu_starting(cpu);
 
+	/* Notify boot CPU that we're starting & ready to sync counters */
+	complete(&cpu_starting);
+
+	synchronise_count_slave(cpu);
+
+	/* The CPU is running and counters synchronised, now mark it online */
 	set_cpu_online(cpu, true);
 
 	set_cpu_sibling_map(cpu);
@@ -381,8 +388,11 @@ asmlinkage void start_secondary(void)
 
 	calculate_cpu_foreign_map();
 
+	/*
+	 * Notify boot CPU that we're up & online and it can safely return
+	 * from __cpu_up
+	 */
 	complete(&cpu_running);
-	synchronise_count_slave(cpu);
 
 	/*
 	 * irq will be enabled in ->smp_finish(), enabling it too early
@@ -445,17 +455,17 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	if (err)
 		return err;
 
-	/*
-	 * We must check for timeout here, as the CPU will not be marked
-	 * online until the counters are synchronised.
-	 */
-	if (!wait_for_completion_timeout(&cpu_running,
+	/* Wait for CPU to start and be ready to sync counters */
+	if (!wait_for_completion_timeout(&cpu_starting,
 					 msecs_to_jiffies(1000))) {
 		pr_crit("CPU%u: failed to start\n", cpu);
 		return -EIO;
 	}
 
 	synchronise_count_master(cpu);
+
+	/* Wait for CPU to finish startup & mark itself online before return */
+	wait_for_completion(&cpu_running);
 	return 0;
 }
 

commit 7f005f112f527f0762386c6fd182966d0e066eaf
Author: Matt Redfearn <matt.redfearn@mips.com>
Date:   Mon Oct 16 11:06:49 2017 +0100

    MIPS: generic: Fix compilation error from include asm/mips-cpc.h
    
    Commit e83f7e02af50c ("MIPS: CPS: Have asm/mips-cps.h include CM & CPC
    headers") adds a #error to arch/mips/include/asm/mips-cpc.h if it is
    included directly. While this commit replaced almost all direct includes
    of mips-cm.h and mips-cpc.h, 2 remain.
    
    With some defconfigs, mips-cps.h is indirectly included before
    mips-cpc.h, but in others this results in compilation errors:
    
    In file included from arch/mips/generic/init.c:23:0:
    ./arch/mips/include/asm/mips-cpc.h:12:3: error: #error Please include
    asm/mips-cps.h rather than asm/mips-cpc.h
     # error Please include asm/mips-cps.h rather than asm/mips-cpc.h
    
    In file included from arch/mips/kernel/smp.c:23:0:
    ./arch/mips/include/asm/mips-cpc.h:12:3: error: #error Please include
    asm/mips-cps.h rather than asm/mips-cpc.h
     # error Please include asm/mips-cps.h rather than asm/mips-cpc.h
    
    In both cases, fix this by including mips-cps.h instead.
    
    Fixes: e83f7e02af50c ("MIPS: CPS: Have asm/mips-cps.h include CM & CPC headers")
    Signed-off-by: Matt Redfearn <matt.redfearn@mips.com>
    Patchwork: https://patchwork.linux-mips.org/patch/17492/
    Signed-off-by: James Hogan <jhogan@kernel.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bbe19b64def5..3cd2f70d1c18 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -42,7 +42,7 @@
 #include <asm/processor.h>
 #include <asm/idle.h>
 #include <asm/r4k-timer.h>
-#include <asm/mips-cpc.h>
+#include <asm/mips-cps.h>
 #include <asm/mmu_context.h>
 #include <asm/time.h>
 #include <asm/setup.h>

commit 7318413077a5141a50a753b1fab687b7907eef16
Merge: 8d93c7a43157 35eed7cb2cf1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 15 20:43:33 2017 -0700

    Merge branch '4.14-features' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    Pull MIPS updates from Ralf Baechle:
     "This is the main pull request for 4.14 for MIPS; below a summary of
      the non-merge commits:
    
      CM:
       - Rename mips_cm_base to mips_gcr_base
       - Specify register size when generating accessors
       - Use BIT/GENMASK for register fields, order & drop shifts
       - Add cluster & block args to mips_cm_lock_other()
    
      CPC:
       - Use common CPS accessor generation macros
       - Use BIT/GENMASK for register fields, order & drop shifts
       - Introduce register modify (set/clear/change) accessors
       - Use change_*, set_* & clear_* where appropriate
       - Add CM/CPC 3.5 register definitions
       - Use GlobalNumber macros rather than magic numbers
       - Have asm/mips-cps.h include CM & CPC headers
       - Cluster support for topology functions
       - Detect CPUs in secondary clusters
    
      CPS:
       - Read GIC_VL_IDENT directly, not via irqchip driver
    
      DMA:
       - Consolidate coherent and non-coherent dma_alloc code
       - Don't use dma_cache_sync to implement fd_cacheflush
    
      FPU emulation / FP assist code:
       - Another series of 14 commits fixing corner cases such as NaN
         propgagation and other special input values.
       - Zero bits 32-63 of the result for a CLASS.D instruction.
       - Enhanced statics via debugfs
       - Do not use bools for arithmetic. GCC 7.1 moans about this.
       - Correct user fault_addr type
    
      Generic MIPS:
       - Enhancement of stack backtraces
       - Cleanup from non-existing options
       - Handle non word sized instructions when examining frame
       - Fix detection and decoding of ADDIUSP instruction
       - Fix decoding of SWSP16 instruction
       - Refactor handling of stack pointer in get_frame_info
       - Remove unreachable code from force_fcr31_sig()
       - Convert to using %pOF instead of full_name
       - Remove the R6000 support.
       - Move FP code from *_switch.S to *_fpu.S
       - Remove unused ST_OFF from r2300_switch.S
       - Allow platform to specify multiple its.S files
       - Add #includes to various files to ensure code builds reliable and
         without warning..
       - Remove __invalidate_kernel_vmap_range
       - Remove plat_timer_setup
       - Declare various variables & functions static
       - Abstract CPU core & VP(E) ID access through accessor functions
       - Store core & VP IDs in GlobalNumber-style variable
       - Unify checks for sibling CPUs
       - Add CPU cluster number accessors
       - Prevent direct use of generic_defconfig
       - Make CONFIG_MIPS_MT_SMP default y
       - Add __ioread64_copy
       - Remove unnecessary inclusions of linux/irqchip/mips-gic.h
    
      GIC:
       - Introduce asm/mips-gic.h with accessor functions
       - Use new GIC accessor functions in mips-gic-timer
       - Remove counter access functions from irq-mips-gic.c
       - Remove gic_read_local_vp_id() from irq-mips-gic.c
       - Simplify shared interrupt pending/mask reads in irq-mips-gic.c
       - Simplify gic_local_irq_domain_map() in irq-mips-gic.c
       - Drop gic_(re)set_mask() functions in irq-mips-gic.c
       - Remove gic_set_polarity(), gic_set_trigger(), gic_set_dual_edge(),
         gic_map_to_pin() and gic_map_to_vpe() from irq-mips-gic.c.
       - Convert remaining shared reg access, local int mask access and
         remaining local reg access to new accessors
       - Move GIC_LOCAL_INT_* to asm/mips-gic.h
       - Remove GIC_CPU_INT* macros from irq-mips-gic.c
       - Move various definitions to the driver
       - Remove gic_get_usm_range()
       - Remove __gic_irq_dispatch() forward declaration
       - Remove gic_init()
       - Use mips_gic_present() in place of gic_present and remove
         gic_present
       - Move gic_get_c0_*_int() to asm/mips-gic.h
       - Remove linux/irqchip/mips-gic.h
       - Inline __gic_init()
       - Inline gic_basic_init()
       - Make pcpu_masks a per-cpu variable
       - Use pcpu_masks to avoid reading GIC_SH_MASK*
       - Clean up mti, reserved-cpu-vectors handling
       - Use cpumask_first_and() in gic_set_affinity()
       - Let the core set struct irq_common_data affinity
    
      microMIPS:
       - Fix microMIPS stack unwinding on big endian systems
    
      MIPS-GIC:
       - SYNC after enabling GIC region
    
      NUMA:
       - Remove the unused parent_node() macro
    
      R6:
       - Constify r2_decoder_tables
       - Add accessor & bit definitions for GlobalNumber
    
      SMP:
       - Constify smp ops
       - Allow boot_secondary SMP op to return errors
    
      VDSO:
       - Drop gic_get_usm_range() usage
       - Avoid use of linux/irqchip/mips-gic.h
    
      Platform changes:
    
      Alchemy:
       - Add devboard machine type to cpuinfo
       - update cpu feature overrides
       - Threaded carddetect irqs for devboards
    
      AR7:
       - allow NULL clock for clk_get_rate
    
      BCM63xx:
       - Fix ENETDMA_6345_MAXBURST_REG offset
       - Allow NULL clock for clk_get_rate
    
      CI20:
       - Enable GPIO and RTC drivers in defconfig
       - Add ethernet and fixed-regulator nodes to DTS
    
      Generic platform:
       - Move Boston and NI 169445 FIT image source to their own files
       - Include asm/bootinfo.h for plat_fdt_relocated()
       - Include asm/time.h for get_c0_*_int()
       - Include asm/bootinfo.h for plat_fdt_relocated()
       - Include asm/time.h for get_c0_*_int()
       - Allow filtering enabled boards by requirements
       - Don't explicitly disable CONFIG_USB_SUPPORT
       - Bump default NR_CPUS to 16
    
      JZ4700:
       - Probe the jz4740-rtc driver from devicetree
    
      Lantiq:
       - Drop check of boot select from the spi-falcon driver.
       - Drop check of boot select from the lantiq-flash MTD driver.
       - Access boot cause register in the watchdog driver through regmap
       - Add device tree binding documentation for the watchdog driver
       - Add docs for the RCU DT bindings.
       - Convert the fpi bus driver to a platform_driver
       - Remove ltq_reset_cause() and ltq_boot_select(
       - Switch to a proper reset driver
       - Switch to a new drivers/soc GPHY driver
       - Add an USB PHY driver for the Lantiq SoCs using the RCU module
       - Use of_platform_default_populate instead of __dt_register_buses
       - Enable MFD_SYSCON to be able to use it for the RCU MFD
       - Replace ltq_boot_select() with dummy implementation.
    
      Loongson 2F:
       - Allow NULL clock for clk_get_rate
    
      Malta:
       - Use new GIC accessor functions
    
      NI 169445:
       - Add support for NI 169445 board.
       - Only include in 32r2el kernels
    
      Octeon:
       - Add support for watchdog of 78XX SOCs.
       - Add support for watchdog of CN68XX SOCs.
       - Expose support for mips32r1, mips32r2 and mips64r1
       - Enable more drivers in config file
       - Add support for accessing the boot vector.
       - Remove old boot vector code from watchdog driver
       - Define watchdog registers for 70xx, 73xx, 78xx, F75xx.
       - Make CSR functions node aware.
       - Allow access to CIU3 IRQ domains.
       - Misc cleanups in the watchdog driver
    
      Omega2+:
       - New board, add support and defconfig
    
      Pistachio:
       - Enable Root FS on NFS in defconfig
    
      Ralink:
       - Add Mediatek MT7628A SoC
       - Allow NULL clock for clk_get_rate
       - Explicitly request exclusive reset control in the pci-mt7620 PCI driver.
    
      SEAD3:
       - Only include in 32 bit kernels by default
    
      VoCore:
       - Add VoCore as a vendor t0 dt-bindings
       - Add defconfig file"
    
    * '4.14-features' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus: (167 commits)
      MIPS: Refactor handling of stack pointer in get_frame_info
      MIPS: Stacktrace: Fix microMIPS stack unwinding on big endian systems
      MIPS: microMIPS: Fix decoding of swsp16 instruction
      MIPS: microMIPS: Fix decoding of addiusp instruction
      MIPS: microMIPS: Fix detection of addiusp instruction
      MIPS: Handle non word sized instructions when examining frame
      MIPS: ralink: allow NULL clock for clk_get_rate
      MIPS: Loongson 2F: allow NULL clock for clk_get_rate
      MIPS: BCM63XX: allow NULL clock for clk_get_rate
      MIPS: AR7: allow NULL clock for clk_get_rate
      MIPS: BCM63XX: fix ENETDMA_6345_MAXBURST_REG offset
      mips: Save all registers when saving the frame
      MIPS: Add DWARF unwinding to assembly
      MIPS: Make SAVE_SOME more standard
      MIPS: Fix issues in backtraces
      MIPS: jz4780: DTS: Probe the jz4740-rtc driver from devicetree
      MIPS: Ci20: Enable RTC driver
      watchdog: octeon-wdt: Add support for 78XX SOCs.
      watchdog: octeon-wdt: Add support for cn68XX SOCs.
      watchdog: octeon-wdt: File cleaning.
      ...

commit d595d423d06071bd7a4892c3c2f16bfe1d5b3a85
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Sat Aug 12 19:49:40 2017 -0700

    MIPS: SMP: Allow boot_secondary SMP op to return errors
    
    Allow the boot_secondary SMP op to return an error to __cpu_up(), which
    will in turn return it to its caller.
    
    This will allow SMP implementations to return errors quickly in cases
    they they know have failed, rather than relying upon __cpu_up()
    eventually timing out waiting for the cpu_running completion.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17014/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6248a5a3ec9e..a4a59ed0164c 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -439,7 +439,11 @@ void smp_prepare_boot_cpu(void)
 
 int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
-	mp_ops->boot_secondary(cpu, tidle);
+	int err;
+
+	err = mp_ops->boot_secondary(cpu, tidle);
+	if (err)
+		return err;
 
 	/*
 	 * We must check for timeout here, as the CPU will not be marked

commit 68923cdc2eb34124d77bc27f7945d7ff16b236dd
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Sat Aug 12 19:49:39 2017 -0700

    MIPS: CM: Add cluster & block args to mips_cm_lock_other()
    
    With CM >= 3.5 we have the notion of multiple clusters & can access
    their CM, CPC & GIC registers via the apporpriate redirect/other
    register blocks. In order to allow for this introduce cluster & block
    arguments to mips_cm_lock_other() which configures the redirect/other
    region to point at the appropriate cluster, core, VP & register block.
    
    Since we now have 4 arguments to mips_cm_lock_other() & a common use is
    likely to be to target the cluster, core & VP corresponding to a
    particular Linux CPU number we also add a new mips_cm_lock_other_cpu()
    helper function which handles that without the caller needing to
    manually pull out the cluster, core & VP numbers.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17013/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 4cc43892b959..6248a5a3ec9e 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -190,7 +190,7 @@ void mips_smp_send_ipi_mask(const struct cpumask *mask, unsigned int action)
 			core = cpu_core(&cpu_data[cpu]);
 
 			while (!cpumask_test_cpu(cpu, &cpu_coherent_mask)) {
-				mips_cm_lock_other(core, 0);
+				mips_cm_lock_other_cpu(cpu, CM_GCR_Cx_OTHER_BLOCK_LOCAL);
 				mips_cpc_lock_other(core);
 				write_cpc_co_cmd(CPC_Cx_CMD_PWRUP);
 				mips_cpc_unlock_other();

commit fe7a38c625a2ee375870567c9fc8302e51e550f7
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Sat Aug 12 19:49:37 2017 -0700

    MIPS: Unify checks for sibling CPUs
    
    Up until now we have open-coded checks for whether CPUs are siblings,
    with slight variations on whether we consider the package ID or not.
    
    This will only get more complex when we introduce cluster support, so in
    preparation for that this patch introduces a cpus_are_siblings()
    function which can be used to check whether or not 2 CPUs are siblings
    in a consistent manner.
    
    By checking globalnumber with the VP ID masked out this also has the
    neat side effect of being ready for multi-cluster systems already.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Acked-by: Rafael J. Wysocki <rjw@rjwysocki.net>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17011/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index a54e5857c227..4cc43892b959 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -96,8 +96,7 @@ static inline void set_cpu_sibling_map(int cpu)
 
 	if (smp_num_siblings > 1) {
 		for_each_cpu(i, &cpu_sibling_setup_map) {
-			if (cpu_data[cpu].package == cpu_data[i].package &&
-			    cpu_core(&cpu_data[cpu]) == cpu_core(&cpu_data[i])) {
+			if (cpus_are_siblings(cpu, i)) {
 				cpumask_set_cpu(i, &cpu_sibling_map[cpu]);
 				cpumask_set_cpu(cpu, &cpu_sibling_map[i]);
 			}
@@ -134,8 +133,7 @@ void calculate_cpu_foreign_map(void)
 	for_each_online_cpu(i) {
 		core_present = 0;
 		for_each_cpu(k, &temp_foreign_map)
-			if (cpu_data[i].package == cpu_data[k].package &&
-			    cpu_core(&cpu_data[i]) == cpu_core(&cpu_data[k]))
+			if (cpus_are_siblings(i, k))
 				core_present = 1;
 		if (!core_present)
 			cpumask_set_cpu(i, &temp_foreign_map);
@@ -186,11 +184,11 @@ void mips_smp_send_ipi_mask(const struct cpumask *mask, unsigned int action)
 
 	if (mips_cpc_present()) {
 		for_each_cpu(cpu, mask) {
-			core = cpu_core(&cpu_data[cpu]);
-
-			if (core == cpu_core(&current_cpu_data))
+			if (cpus_are_siblings(cpu, smp_processor_id()))
 				continue;
 
+			core = cpu_core(&cpu_data[cpu]);
+
 			while (!cpumask_test_cpu(cpu, &cpu_coherent_mask)) {
 				mips_cm_lock_other(core, 0);
 				mips_cpc_lock_other(core);

commit f875a832d2028523f9b53c261b67e05a359bab8b
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Sat Aug 12 19:49:35 2017 -0700

    MIPS: Abstract CPU core & VP(E) ID access through accessor functions
    
    We currently have fields in struct cpuinfo_mips for the core & VP(E) ID
    of a particular CPU, and various pieces of code directly access those
    fields. This patch abstracts such access by introducing accessor
    functions cpu_core(), cpu_set_core(), cpu_vpe_id() & cpu_set_vpe_id()
    and having code that needs to access these values call those functions
    rather than directly accessing the struct cpuinfo_mips fields. This
    prepares us for changes to the way in which those values are stored in
    later patches.
    
    The cpu_vpe_id() function is introduced even though we already had a
    cpu_vpe_id() macro for a couple of reasons:
    
      1) It's more consistent with the core, and future cluster, accessors.
    
      2) It ensures a sensible return type without explicit casts.
    
      3) It's generally preferable to use functions rather than macros.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17009/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 20c1f9ac946a..a54e5857c227 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -97,7 +97,7 @@ static inline void set_cpu_sibling_map(int cpu)
 	if (smp_num_siblings > 1) {
 		for_each_cpu(i, &cpu_sibling_setup_map) {
 			if (cpu_data[cpu].package == cpu_data[i].package &&
-				    cpu_data[cpu].core == cpu_data[i].core) {
+			    cpu_core(&cpu_data[cpu]) == cpu_core(&cpu_data[i])) {
 				cpumask_set_cpu(i, &cpu_sibling_map[cpu]);
 				cpumask_set_cpu(cpu, &cpu_sibling_map[i]);
 			}
@@ -135,7 +135,7 @@ void calculate_cpu_foreign_map(void)
 		core_present = 0;
 		for_each_cpu(k, &temp_foreign_map)
 			if (cpu_data[i].package == cpu_data[k].package &&
-			    cpu_data[i].core == cpu_data[k].core)
+			    cpu_core(&cpu_data[i]) == cpu_core(&cpu_data[k]))
 				core_present = 1;
 		if (!core_present)
 			cpumask_set_cpu(i, &temp_foreign_map);
@@ -186,9 +186,9 @@ void mips_smp_send_ipi_mask(const struct cpumask *mask, unsigned int action)
 
 	if (mips_cpc_present()) {
 		for_each_cpu(cpu, mask) {
-			core = cpu_data[cpu].core;
+			core = cpu_core(&cpu_data[cpu]);
 
-			if (core == current_cpu_data.core)
+			if (core == cpu_core(&current_cpu_data))
 				continue;
 
 			while (!cpumask_test_cpu(cpu, &cpu_coherent_mask)) {

commit ff2c8252bfbf069dda1e53353a63b560f1369f59
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Jul 19 09:21:03 2017 +0100

    MIPS: SMP: Constify smp ops
    
    smp_ops providers do not modify their ops structures, so they should be
    made const for robustness. Since currently the MIPS kernel is not mapped
    with memory protection, this does not in itself provide any security
    benefit, but it still makes sense to make this change.
    
    There are also slight code size efficincies from the structure being
    made read-only, saving 128 bytes of kernel text on a
    pistachio_defconfig.
    Before:
       text    data     bss     dec     hex filename
    7187239 1772752  470224 9430215  8fe4c7 vmlinux
    After:
       text    data     bss     dec     hex filename
    7187111 1772752  470224 9430087  8fe447 vmlinux
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Marcin Nowakowski <marcin.nowakowski@imgtec.com>
    Cc: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Kevin Cernekee <cernekee@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Doug Ledford <dledford@redhat.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Steven J. Hill <steven.hill@cavium.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/16784/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6bace7695788..20c1f9ac946a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -146,10 +146,10 @@ void calculate_cpu_foreign_map(void)
 			       &temp_foreign_map, &cpu_sibling_map[i]);
 }
 
-struct plat_smp_ops *mp_ops;
+const struct plat_smp_ops *mp_ops;
 EXPORT_SYMBOL(mp_ops);
 
-void register_smp_ops(struct plat_smp_ops *ops)
+void register_smp_ops(const struct plat_smp_ops *ops)
 {
 	if (mp_ops)
 		printk(KERN_WARNING "Overriding previously set SMP ops\n");

commit 966a967116e699762dbf4af7f9e0d1955c25aa37
Author: Ying Huang <ying.huang@intel.com>
Date:   Tue Aug 8 12:30:00 2017 +0800

    smp: Avoid using two cache lines for struct call_single_data
    
    struct call_single_data is used in IPIs to transfer information between
    CPUs.  Its size is bigger than sizeof(unsigned long) and less than
    cache line size.  Currently it is not allocated with any explicit alignment
    requirements.  This makes it possible for allocated call_single_data to
    cross two cache lines, which results in double the number of the cache lines
    that need to be transferred among CPUs.
    
    This can be fixed by requiring call_single_data to be aligned with the
    size of call_single_data. Currently the size of call_single_data is the
    power of 2.  If we add new fields to call_single_data, we may need to
    add padding to make sure the size of new definition is the power of 2
    as well.
    
    Fortunately, this is enforced by GCC, which will report bad sizes.
    
    To set alignment requirements of call_single_data to the size of
    call_single_data, a struct definition and a typedef is used.
    
    To test the effect of the patch, I used the vm-scalability multiple
    thread swap test case (swap-w-seq-mt).  The test will create multiple
    threads and each thread will eat memory until all RAM and part of swap
    is used, so that huge number of IPIs are triggered when unmapping
    memory.  In the test, the throughput of memory writing improves ~5%
    compared with misaligned call_single_data, because of faster IPIs.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Huang, Ying <ying.huang@intel.com>
    [ Add call_single_data_t and align with size of call_single_data. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/87bmnqd6lz.fsf@yhuang-mobile.sh.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6bace7695788..c7cbddfcdc3b 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -648,12 +648,12 @@ EXPORT_SYMBOL(flush_tlb_one);
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 
 static DEFINE_PER_CPU(atomic_t, tick_broadcast_count);
-static DEFINE_PER_CPU(struct call_single_data, tick_broadcast_csd);
+static DEFINE_PER_CPU(call_single_data_t, tick_broadcast_csd);
 
 void tick_broadcast(const struct cpumask *mask)
 {
 	atomic_t *count;
-	struct call_single_data *csd;
+	call_single_data_t *csd;
 	int cpu;
 
 	for_each_cpu(cpu, mask) {
@@ -674,7 +674,7 @@ static void tick_broadcast_callee(void *info)
 
 static int __init tick_broadcast_init(void)
 {
-	struct call_single_data *csd;
+	call_single_data_t *csd;
 	int cpu;
 
 	for (cpu = 0; cpu < NR_CPUS; cpu++) {

commit 6f542ebeaee0ee552a902ce3892220fc22c7ec8e
Author: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
Date:   Thu Aug 3 08:20:22 2017 +0200

    MIPS: Fix race on setting and getting cpu_online_mask
    
    While testing cpu hoptlug (cpu down and up in loops) on kernel 4.4, it was
    observed that occasionally check for cpu online will fail in kernel/cpu.c,
    _cpu_up:
    
    https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/tree/kernel/cpu.c?h=v4.4.79#n485
     518        /* Arch-specific enabling code. */
     519        ret = __cpu_up(cpu, idle);
     520
     521        if (ret != 0)
     522                goto out_notify;
     523        BUG_ON(!cpu_online(cpu));
    
    Reason is race between start_secondary and _cpu_up. cpu_callin_map is set
    before cpu_online_mask. In __cpu_up, cpu_callin_map is waited for, but cpu
    online mask is not, resulting in race in which secondary processor started
    and set cpu_callin_map, but not yet set the online mask,resulting in above
    BUG being hit.
    
    Upstream differs in the area. cpu_online check is in bringup_wait_for_ap,
    which is after cpu reached AP_ONLINE_IDLE,where secondary passed its start
    function. Nonetheless, fix makes start_secondary safe and not depending on
    other locks throughout the code. It protects as well against cpu_online
    checks put in between sometimes in the future.
    
    Fix this by moving completion after all flags are set.
    
    Signed-off-by: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Cc: Alexander Sverdlin <alexander.sverdlin@nokia.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/16925/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 770d4d1516cb..6bace7695788 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -376,9 +376,6 @@ asmlinkage void start_secondary(void)
 	cpumask_set_cpu(cpu, &cpu_coherent_mask);
 	notify_cpu_starting(cpu);
 
-	complete(&cpu_running);
-	synchronise_count_slave(cpu);
-
 	set_cpu_online(cpu, true);
 
 	set_cpu_sibling_map(cpu);
@@ -386,6 +383,9 @@ asmlinkage void start_secondary(void)
 
 	calculate_cpu_foreign_map();
 
+	complete(&cpu_running);
+	synchronise_count_slave(cpu);
+
 	/*
 	 * irq will be enabled in ->smp_finish(), enabling it too early
 	 * is dangerous.

commit 9b03d8abe06d92195712fd489b2e8983de27fa68
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jun 2 14:48:49 2017 -0700

    MIPS: Skip IPI setup if we only have 1 CPU
    
    If we're running on a system with only 1 possible CPU then it makes no
    sense to reserve or initialise IPIs since we'll never use them. Avoid
    doing so.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/16192/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index aba1afb64b62..770d4d1516cb 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -335,6 +335,9 @@ int mips_smp_ipi_free(const struct cpumask *mask)
 
 static int __init mips_smp_ipi_init(void)
 {
+	if (num_possible_cpus() == 1)
+		return 0;
+
 	mips_smp_ipi_allocate(cpu_possible_mask);
 
 	call_desc = irq_to_desc(call_virq);

commit e64889823d58305a5ddb2828ae0a988c59e87d7e
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Thu Mar 30 12:06:13 2017 -0700

    MIPS: Stengthen IPI IRQ domain sanity check
    
    Commit fbde2d7d8290 ("MIPS: Add generic SMP IPI support") introduced a
    sanity check that an IPI IRQ domain can be found during boot, in order
    to ensure that IPIs are able to be set up in systems using such domains.
    However it was added at a point where systems may have used an IPI IRQ
    domain in some situations but not others, and we could not know which
    were the case until runtime, so commit 578bffc82ec5 ("MIPS: Don't BUG_ON
    when no IPI domain is found") made that check simply skip IPI init if no
    domain were found in order to fix the boot for systems such as QEMU
    Malta.
    
    We now use IPI IRQ domains for the MIPS CPU interrupt controller, which
    means systems which make use of IPI IRQ domains will always do so when
    running on multiple CPUs. As a result we now strengthen the sanity check
    to ensure that an IPI IRQ domain is found when multiple CPUs are present
    in the system.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/15838/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6e71130549ea..aba1afb64b62 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -261,16 +261,20 @@ int mips_smp_ipi_allocate(const struct cpumask *mask)
 		ipidomain = irq_find_matching_host(NULL, DOMAIN_BUS_IPI);
 
 	/*
-	 * There are systems which only use IPI domains some of the time,
-	 * depending upon configuration we don't know until runtime. An
-	 * example is Malta where we may compile in support for GIC & the
-	 * MT ASE, but run on a system which has multiple VPEs in a single
-	 * core and doesn't include a GIC. Until all IPI implementations
-	 * have been converted to use IPI domains the best we can do here
-	 * is to return & hope some other code sets up the IPIs.
+	 * There are systems which use IPI IRQ domains, but only have one
+	 * registered when some runtime condition is met. For example a Malta
+	 * kernel may include support for GIC & CPU interrupt controller IPI
+	 * IRQ domains, but if run on a system with no GIC & no MT ASE then
+	 * neither will be supported or registered.
+	 *
+	 * We only have a problem if we're actually using multiple CPUs so fail
+	 * loudly if that is the case. Otherwise simply return, skipping IPI
+	 * setup, if we're running with only a single CPU.
 	 */
-	if (!ipidomain)
+	if (!ipidomain) {
+		BUG_ON(num_present_cpus() > 1);
 		return 0;
+	}
 
 	virq = irq_reserve_ipi(ipidomain, mask);
 	BUG_ON(!virq);

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 8c60a296294c..6e71130549ea 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -28,7 +28,7 @@
 #include <linux/export.h>
 #include <linux/time.h>
 #include <linux/timex.h>
-#include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/cpumask.h>
 #include <linux/cpu.h>
 #include <linux/err.h>

commit c83c2eed67e578576acf08611bfb630bd199714b
Author: Marcin Nowakowski <marcin.nowakowski@imgtec.com>
Date:   Fri Dec 2 09:58:28 2016 +0100

    MIPS: kexec: remove SMP_DUMP
    
    SMP_DUMP has been added as a new IPI signal when kexec support was added
    for Cavium Octeon CPUs ('commit 7aa1c8f47e7e ("MIPS: kdump: Add support")'.
    However, the new signal doesn't appear to ever have a proper handler
    added (octeon_message_functions[] array has an empty handler for it),
    and generic IPI handlers now trigger a BUG() on unhandled signal.
    
    As the method is unused remove it completely and replace its only
    invocation with a smp_call_function().
    
    [ralf@linux-mips.org: Renumber SMP_ASK_C0COUNT to avoid numbering gaps.]
    
    Signed-off-by: Marcin Nowakowski <marcin.nowakowski@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/14630/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 0a831f63b0ec..8c60a296294c 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -638,23 +638,6 @@ void flush_tlb_one(unsigned long vaddr)
 EXPORT_SYMBOL(flush_tlb_page);
 EXPORT_SYMBOL(flush_tlb_one);
 
-#if defined(CONFIG_KEXEC)
-void (*dump_ipi_function_ptr)(void *) = NULL;
-void dump_send_ipi(void (*dump_ipi_callback)(void *))
-{
-	int i;
-	int cpu = smp_processor_id();
-
-	dump_ipi_function_ptr = dump_ipi_callback;
-	smp_mb();
-	for_each_online_cpu(i)
-		if (i != cpu)
-			mp_ops->send_ipi_single(i, SMP_DUMP);
-
-}
-EXPORT_SYMBOL(dump_send_ipi);
-#endif
-
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 
 static DEFINE_PER_CPU(atomic_t, tick_broadcast_count);

commit 5892d6a60341d50e1765a86fba0976c747f4fb19
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Fri Nov 4 09:28:57 2016 +0000

    MIPS: SMP: Remove cpu_callin_map
    
    The previous commit made cpu_callin_map redundant, since it is no longer
    used to signal secondary CPUs starting, or going offline. Remove it now.
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: Kevin Cernekee <cernekee@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Cc: Adam Buchbinder <adam.buchbinder@gmail.com>
    Cc: Yang Shi <yang.shi@windriver.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/14503/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 03daf9008124..0a831f63b0ec 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -48,8 +48,6 @@
 #include <asm/setup.h>
 #include <asm/maar.h>
 
-cpumask_t cpu_callin_map;		/* Bitmask of started secondaries */
-
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 EXPORT_SYMBOL(__cpu_number_map);
 

commit a00eeede507c975087b7b8df8cf2c9f88ba285de
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Fri Nov 4 09:28:56 2016 +0000

    MIPS: SMP: Use a completion event to signal CPU up
    
    If a secondary CPU failed to start, for any reason, the CPU requesting
    the secondary to start would get stuck in the loop waiting for the
    secondary to be present in the cpu_callin_map.
    
    Rather than that, use a completion event to signal that the secondary
    CPU has started and is waiting to synchronise counters.
    
    Since the CPU presence will no longer be marked in cpu_callin_map,
    remove the redundant test from arch_cpu_idle_dead().
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Maciej W. Rozycki <macro@imgtec.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Marcin Nowakowski <marcin.nowakowski@imgtec.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/14502/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 7ebb1918e2ac..03daf9008124 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -68,6 +68,8 @@ EXPORT_SYMBOL(cpu_sibling_map);
 cpumask_t cpu_core_map[NR_CPUS] __read_mostly;
 EXPORT_SYMBOL(cpu_core_map);
 
+static DECLARE_COMPLETION(cpu_running);
+
 /*
  * A logcal cpu mask containing only one VPE per core to
  * reduce the number of IPIs on large MT systems.
@@ -369,7 +371,7 @@ asmlinkage void start_secondary(void)
 	cpumask_set_cpu(cpu, &cpu_coherent_mask);
 	notify_cpu_starting(cpu);
 
-	cpumask_set_cpu(cpu, &cpu_callin_map);
+	complete(&cpu_running);
 	synchronise_count_slave(cpu);
 
 	set_cpu_online(cpu, true);
@@ -430,7 +432,6 @@ void smp_prepare_boot_cpu(void)
 {
 	set_cpu_possible(0, true);
 	set_cpu_online(0, true);
-	cpumask_set_cpu(0, &cpu_callin_map);
 }
 
 int __cpu_up(unsigned int cpu, struct task_struct *tidle)
@@ -438,11 +439,13 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	mp_ops->boot_secondary(cpu, tidle);
 
 	/*
-	 * Trust is futile.  We should really have timeouts ...
+	 * We must check for timeout here, as the CPU will not be marked
+	 * online until the counters are synchronised.
 	 */
-	while (!cpumask_test_cpu(cpu, &cpu_callin_map)) {
-		udelay(100);
-		schedule();
+	if (!wait_for_completion_timeout(&cpu_running,
+					 msecs_to_jiffies(1000))) {
+		pr_crit("CPU%u: failed to start\n", cpu);
+		return -EIO;
 	}
 
 	synchronise_count_master(cpu);

commit d9d5417755eda87db8e370e4dd2175fbd8814acc
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Sun Aug 21 15:58:13 2016 -0400

    MIPS: kernel: Audit and remove any unnecessary uses of module.h
    
    Historically a lot of these existed because we did not have
    a distinction between what was modular code and what was providing
    support to modules via EXPORT_SYMBOL and friends.  That changed
    when we forked out support for the latter into the export.h file.
    
    This means we should be able to reduce the usage of module.h
    in code that is obj-y Makefile or bool Kconfig.  The advantage
    in doing so is that module.h itself sources about 15 other headers;
    adding significantly to what we feed cpp, and it can obscure what
    headers we are effectively using.
    
    Since module.h was the source for init.h (for __init) and for
    export.h (for EXPORT_SYMBOL) we consider each obj-y/bool instance
    for the presence of either and replace as needed.
    
    In the case of the n32/o32 files, we have to get rid of a couple
    no-op MODULE_ tags to facilitate the module.h removal.  They piggy
    back off the fs/ elf binary support, which is also a bool Kconfig.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/14032/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 0e131c9c39f6..7ebb1918e2ac 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -25,7 +25,7 @@
 #include <linux/smp.h>
 #include <linux/spinlock.h>
 #include <linux/threads.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/time.h>
 #include <linux/timex.h>
 #include <linux/sched.h>

commit 7688c5391038e60377275f078e6d7043dc115efc
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Tue Sep 20 09:47:26 2016 +0100

    MIPS: smp.c: Introduce mechanism for freeing and allocating IPIs
    
    For the MIPS remote processor implementation, we need additional IPIs to
    talk to the remote processor. Since MIPS GIC reserves exactly the right
    number of IPI IRQs required by Linux for the number of VPs in the
    system, this is not possible without releasing some recources.
    
    This commit introduces mips_smp_ipi_allocate() which allocates IPIs to a
    given cpumask. It is called as normal with the cpu_possible_mask at
    bootup to initialise IPIs to all CPUs. mips_smp_ipi_free() may then be
    used to free IPIs to a subset of those CPUs so that their hardware
    resources can be reused.
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Bjorn Andersson <bjorn.andersson@linaro.org>
    Cc: Ohad Ben-Cohen <ohad@wizery.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Lisa Parratt <Lisa.Parratt@imgtec.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-remoteproc@vger.kernel.org
    Cc: lisa.parratt@imgtec.com
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/14285/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index cb02df215365..0e131c9c39f6 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -231,7 +231,7 @@ static struct irqaction irq_call = {
 	.name		= "IPI call"
 };
 
-static __init void smp_ipi_init_one(unsigned int virq,
+static void smp_ipi_init_one(unsigned int virq,
 				    struct irqaction *action)
 {
 	int ret;
@@ -241,9 +241,11 @@ static __init void smp_ipi_init_one(unsigned int virq,
 	BUG_ON(ret);
 }
 
-static int __init mips_smp_ipi_init(void)
+static unsigned int call_virq, sched_virq;
+
+int mips_smp_ipi_allocate(const struct cpumask *mask)
 {
-	unsigned int call_virq, sched_virq;
+	int virq;
 	struct irq_domain *ipidomain;
 	struct device_node *node;
 
@@ -270,16 +272,20 @@ static int __init mips_smp_ipi_init(void)
 	if (!ipidomain)
 		return 0;
 
-	call_virq = irq_reserve_ipi(ipidomain, cpu_possible_mask);
-	BUG_ON(!call_virq);
+	virq = irq_reserve_ipi(ipidomain, mask);
+	BUG_ON(!virq);
+	if (!call_virq)
+		call_virq = virq;
 
-	sched_virq = irq_reserve_ipi(ipidomain, cpu_possible_mask);
-	BUG_ON(!sched_virq);
+	virq = irq_reserve_ipi(ipidomain, mask);
+	BUG_ON(!virq);
+	if (!sched_virq)
+		sched_virq = virq;
 
 	if (irq_domain_is_ipi_per_cpu(ipidomain)) {
 		int cpu;
 
-		for_each_cpu(cpu, cpu_possible_mask) {
+		for_each_cpu(cpu, mask) {
 			smp_ipi_init_one(call_virq + cpu, &irq_call);
 			smp_ipi_init_one(sched_virq + cpu, &irq_resched);
 		}
@@ -288,6 +294,45 @@ static int __init mips_smp_ipi_init(void)
 		smp_ipi_init_one(sched_virq, &irq_resched);
 	}
 
+	return 0;
+}
+
+int mips_smp_ipi_free(const struct cpumask *mask)
+{
+	struct irq_domain *ipidomain;
+	struct device_node *node;
+
+	node = of_irq_find_parent(of_root);
+	ipidomain = irq_find_matching_host(node, DOMAIN_BUS_IPI);
+
+	/*
+	 * Some platforms have half DT setup. So if we found irq node but
+	 * didn't find an ipidomain, try to search for one that is not in the
+	 * DT.
+	 */
+	if (node && !ipidomain)
+		ipidomain = irq_find_matching_host(NULL, DOMAIN_BUS_IPI);
+
+	BUG_ON(!ipidomain);
+
+	if (irq_domain_is_ipi_per_cpu(ipidomain)) {
+		int cpu;
+
+		for_each_cpu(cpu, mask) {
+			remove_irq(call_virq + cpu, &irq_call);
+			remove_irq(sched_virq + cpu, &irq_resched);
+		}
+	}
+	irq_destroy_ipi(call_virq, mask);
+	irq_destroy_ipi(sched_virq, mask);
+	return 0;
+}
+
+
+static int __init mips_smp_ipi_init(void)
+{
+	mips_smp_ipi_allocate(cpu_possible_mask);
+
 	call_desc = irq_to_desc(call_virq);
 	sched_desc = irq_to_desc(sched_virq);
 

commit 4b6401365a6ccc53de820209cb328507a2103d30
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Wed Sep 7 10:45:19 2016 +0100

    MIPS: SMP: Wrap call to mips_cpc_lock_other in mips_cm_lock_other
    
    All calls to mips_cpc_lock_other should be wrapped in
    mips_cm_lock_other. This only matters if the system has CM3 and is using
    cpu idle, since otherwise a) the CPC lock is sufficent for CM < 3 and b)
    any systems with CM > 3 have not been able to use cpu idle until now.
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Reviewed-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Patchwork: https://patchwork.linux-mips.org/patch/14227/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index b0baf48951fa..cb02df215365 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -192,9 +192,11 @@ void mips_smp_send_ipi_mask(const struct cpumask *mask, unsigned int action)
 				continue;
 
 			while (!cpumask_test_cpu(cpu, &cpu_coherent_mask)) {
+				mips_cm_lock_other(core, 0);
 				mips_cpc_lock_other(core);
 				write_cpc_co_cmd(CPC_Cx_CMD_PWRUP);
 				mips_cpc_unlock_other();
+				mips_cm_unlock_other();
 			}
 		}
 	}

commit 8f46cca1e6c06a058374816887059bcc017b382f
Author: Matt Redfearn <matt.redfearn@imgtec.com>
Date:   Thu Sep 22 17:15:47 2016 +0100

    MIPS: SMP: Fix possibility of deadlock when bringing CPUs online
    
    This patch fixes the possibility of a deadlock when bringing up
    secondary CPUs.
    The deadlock occurs because the set_cpu_online() is called before
    synchronise_count_slave(). This can cause a deadlock if the boot CPU,
    having scheduled another thread, attempts to send an IPI to the
    secondary CPU, which it sees has been marked online. The secondary is
    blocked in synchronise_count_slave() waiting for the boot CPU to enter
    synchronise_count_master(), but the boot cpu is blocked in
    smp_call_function_many() waiting for the secondary to respond to it's
    IPI request.
    
    Fix this by marking the CPU online in cpu_callin_map and synchronising
    counters before declaring the CPU online and calculating the maps for
    IPIs.
    
    Signed-off-by: Matt Redfearn <matt.redfearn@imgtec.com>
    Reported-by: Justin Chen <justinpopo6@gmail.com>
    Tested-by: Justin Chen <justinpopo6@gmail.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: stable@vger.kernel.org # v4.1+
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/14302/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index f95f094f36e4..b0baf48951fa 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -322,6 +322,9 @@ asmlinkage void start_secondary(void)
 	cpumask_set_cpu(cpu, &cpu_coherent_mask);
 	notify_cpu_starting(cpu);
 
+	cpumask_set_cpu(cpu, &cpu_callin_map);
+	synchronise_count_slave(cpu);
+
 	set_cpu_online(cpu, true);
 
 	set_cpu_sibling_map(cpu);
@@ -329,10 +332,6 @@ asmlinkage void start_secondary(void)
 
 	calculate_cpu_foreign_map();
 
-	cpumask_set_cpu(cpu, &cpu_callin_map);
-
-	synchronise_count_slave(cpu);
-
 	/*
 	 * irq will be enabled in ->smp_finish(), enabling it too early
 	 * is dangerous.

commit 640511ae92466800c75da77a3c7f72b8488c93a1
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:52 2016 +0100

    MIPS: c-r4k: Exclude sibling CPUs in SMP calls
    
    When performing SMP calls to foreign cores, exclude sibling CPUs from
    the provided map, as we already handle the local core on the current
    CPU. This prevents an SMP call from for example core 0, VPE 1 to VPE 0
    on the same core.
    
    In the process the cpu_foreign_map cpumask is turned into an array of
    cpumasks, so that each CPU has its own version of it which excludes
    sibling CPUs. r4k_op_needs_ipi() is also updated to reflect that cache
    management SMP calls are not needed when all CPUs are siblings (i.e.
    there are no foreign CPUs according to the new cpu_foreign_map[]
    semantics which exclude siblings).
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: Felix Fietkau <nbd@nbd.name>
    Cc: Jayachandran C. <jchandra@broadcom.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13801/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 783d5f50ab9d..f95f094f36e4 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -72,7 +72,7 @@ EXPORT_SYMBOL(cpu_core_map);
  * A logcal cpu mask containing only one VPE per core to
  * reduce the number of IPIs on large MT systems.
  */
-cpumask_t cpu_foreign_map __read_mostly;
+cpumask_t cpu_foreign_map[NR_CPUS] __read_mostly;
 EXPORT_SYMBOL(cpu_foreign_map);
 
 /* representing cpus for which sibling maps can be computed */
@@ -141,7 +141,9 @@ void calculate_cpu_foreign_map(void)
 			cpumask_set_cpu(i, &temp_foreign_map);
 	}
 
-	cpumask_copy(&cpu_foreign_map, &temp_foreign_map);
+	for_each_online_cpu(i)
+		cpumask_andnot(&cpu_foreign_map[i],
+			       &temp_foreign_map, &cpu_sibling_map[i]);
 }
 
 struct plat_smp_ops *mp_ops;

commit 926963160ca4d6267957541a85591b7c426066d6
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:46 2016 +0100

    MIPS: SMP: Drop stop_this_cpu() cpu_foreign_map hack
    
    Commit cccf34e9411c ("MIPS: c-r4k: Fix cache flushing for MT cores")
    added the cpu_foreign_map cpumask containing a single VPE from each
    online core, and recalculated it when secondary CPUs are brought up.
    
    stop_this_cpu() was also updated to recalculate cpu_foreign_map, but
    with an additional hack before marking the CPU as offline to copy
    cpu_online_mask into cpu_foreign_map and perform an SMP memory barrier.
    
    This appears to have been intended to prevent cache management IPIs
    being missed when the VPE representing the core in cpu_foreign_map is
    taken offline while other VPEs remain online. Unfortunately there is
    nothing in this hack to prevent r4k_on_each_cpu() from reading the old
    cpu_foreign_map, and smp_call_function_many() from reading that new
    cpu_online_mask with the core's representative VPE marked offline. It
    then wouldn't send an IPI to any online VPEs of that core.
    
    stop_this_cpu() is only actually called in panic and system shutdown /
    halt / reboot situations, in which case all CPUs are going down and we
    don't really need to care about cache management, so drop this hack.
    
    Note that the __cpu_disable() case for CPU hotplug is handled in the
    previous commit, and no synchronisation is needed there due to the use
    of stop_machine() which prevents hotplug from taking place while any CPU
    has disabled preemption (as r4k_on_each_cpu() does).
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13796/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index a4d4309ecff2..783d5f50ab9d 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -344,16 +344,9 @@ asmlinkage void start_secondary(void)
 static void stop_this_cpu(void *dummy)
 {
 	/*
-	 * Remove this CPU. Be a bit slow here and
-	 * set the bits for every online CPU so we don't miss
-	 * any IPI whilst taking this VPE down.
+	 * Remove this CPU:
 	 */
 
-	cpumask_copy(&cpu_foreign_map, cpu_online_mask);
-
-	/* Make it visible to every other CPU */
-	smp_mb();
-
 	set_cpu_online(smp_processor_id(), false);
 	calculate_cpu_foreign_map();
 	local_irq_disable();

commit 826e99be6ab5189dbfb096389016ffb8d20a683e
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:45 2016 +0100

    MIPS: SMP: Update cpu_foreign_map on CPU disable
    
    When a CPU is disabled via CPU hotplug, cpu_foreign_map is not updated.
    This could result in cache management SMP calls being sent to offline
    CPUs instead of online siblings in the same core.
    
    Add a call to calculate_cpu_foreign_map() in the various MIPS cpu
    disable callbacks after set_cpu_online(). All cases are updated for
    consistency and to keep cpu_foreign_map strictly up to date, not just
    those which may support hardware multithreading.
    
    Fixes: cccf34e9411c ("MIPS: c-r4k: Fix cache flushing for MT cores")
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Kevin Cernekee <cernekee@gmail.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: Hongliang Tao <taohl@lemote.com>
    Cc: Hua Yan <yanh@lemote.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13799/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 0c98b4a313be..a4d4309ecff2 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -124,7 +124,7 @@ static inline void set_cpu_core_map(int cpu)
  * Calculate a new cpu_foreign_map mask whenever a
  * new cpu appears or disappears.
  */
-static inline void calculate_cpu_foreign_map(void)
+void calculate_cpu_foreign_map(void)
 {
 	int i, k, core_present;
 	cpumask_t temp_foreign_map;

commit a05c392032e2bb0f6d8f8cf2dd39c36b0407db72
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Jul 13 14:12:44 2016 +0100

    MIPS: SMP: Clear ASID without confusing has_valid_asid()
    
    The SMP flush_tlb_*() functions may clear the memory map's ASIDs for
    other CPUs if the mm has only a single user (the current CPU) in order
    to avoid SMP calls. However this makes it appear to has_valid_asid(),
    which is used by various cache flush functions, as if the CPUs have
    never run in the mm, and therefore can't have cached any of its memory.
    
    For flush_tlb_mm() this doesn't sound unreasonable.
    
    flush_tlb_range() corresponds to flush_cache_range() which does do full
    indexed cache flushes, but only on the icache if the specified mapping
    is executable, otherwise it doesn't guarantee that there are no cache
    contents left for the mm.
    
    flush_tlb_page() corresponds to flush_cache_page(), which will perform
    address based cache ops on the specified page only, and also only
    touches the icache if the page is executable. It does not guarantee that
    there are no cache contents left for the mm.
    
    For example, this affects flush_cache_range() which uses the
    has_valid_asid() optimisation. It is required to flush the icache when
    mappings are made executable (e.g. using mprotect) so they are
    immediately usable. If some code is changed to non executable in order
    to be modified then it will not be flushed from the icache during that
    time, but the ASID on other CPUs may still be cleared for TLB flushing.
    When the code is changed back to executable, flush_cache_range() will
    assume the code hasn't run on those other CPUs due to the zero ASID, and
    won't invalidate the icache on them.
    
    This is fixed by clearing the other CPUs ASIDs to 1 instead of 0 for the
    above two flush_tlb_*() functions when the corresponding cache flushes
    are likely to be incomplete (non executable range flush, or any page
    flush). This ASID appears valid to has_valid_asid(), but still triggers
    ASID regeneration due to the upper ASID version bits being 0, which is
    less than the minimum ASID version of 1 and so always treated as stale.
    
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13795/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index f9d01e953acb..0c98b4a313be 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -512,10 +512,17 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 		smp_on_other_tlbs(flush_tlb_range_ipi, &fd);
 	} else {
 		unsigned int cpu;
+		int exec = vma->vm_flags & VM_EXEC;
 
 		for_each_online_cpu(cpu) {
+			/*
+			 * flush_cache_range() will only fully flush icache if
+			 * the VMA is executable, otherwise we must invalidate
+			 * ASID without it appearing to has_valid_asid() as if
+			 * mm has been completely unused by that CPU.
+			 */
 			if (cpu != smp_processor_id() && cpu_context(cpu, mm))
-				cpu_context(cpu, mm) = 0;
+				cpu_context(cpu, mm) = !exec;
 		}
 	}
 	local_flush_tlb_range(vma, start, end);
@@ -560,8 +567,14 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 		unsigned int cpu;
 
 		for_each_online_cpu(cpu) {
+			/*
+			 * flush_cache_page() only does partial flushes, so
+			 * invalidate ASID without it appearing to
+			 * has_valid_asid() as if mm has been completely unused
+			 * by that CPU.
+			 */
 			if (cpu != smp_processor_id() && cpu_context(cpu, vma->vm_mm))
-				cpu_context(cpu, vma->vm_mm) = 0;
+				cpu_context(cpu, vma->vm_mm) = 1;
 		}
 	}
 	local_flush_tlb_page(vma, page);

commit 578bffc82ec5332d97860ac529f90b0bcfaf8b5f
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Mon Apr 4 10:04:52 2016 +0100

    MIPS: Don't BUG_ON when no IPI domain is found
    
    Commit fbde2d7d8290 ("MIPS: Add generic SMP IPI support") introduced
    code that BUG_ON's in the case of a kernel that supports IPI domains but
    does not have one at runtime. This case is possible on Malta where for
    IPIs we may use either the GIC (which has an IPI IRQ domain
    implementation) or core-local software interrupts between VPEs (which do
    not currently have an IPI IRQ domain implementation). We can not know
    which will be used until runtime when we know whether a GIC is actually
    present, and if we run on a system with multiple VPEs and no GIC then
    the BUG_ON is hit.
    
    Commit 19fb5818ed60 ("IPS: Fix broken malta qemu") worked around this
    for the single-core single-VPE case typically seen using QEMU, but does
    not catch the multi-VPE case. This patch removes the insufficient CPU
    presence check that was added and works around the bug differently,
    effectively reverting that commit.
    
    A simple way to reproduce this bug is by using QEMU, which partially
    implements the MT ASE but does not implement the GIC as of version 2.5.
    Using "-cpu 34Kf -smp 2" will present a system with 2 VPEs in one core &
    no GIC, hitting the BUG_ON.
    
    Given that we're post-merge-window on the way to v4.6, avoid this by
    just returning from mips_smp_ipi_init when no IPI IRQ domain is found.
    Ideally at some point all IPI implementations would be converted to the
    same IPI IRQ domain interface & we'd be able to restore the check.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Fixes: fbde2d7d8290 ("MIPS: Add generic SMP IPI support")
    Fixes: 19fb5818ed60 ("IPS: Fix broken malta qemu")
    Reverts: 19fb5818ed60 ("IPS: Fix broken malta qemu")
    Cc: Qais Yousef <qsyousef@gmail.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Alex Smith <alex.smith@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/13007/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 27cb638f0824..f9d01e953acb 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -243,18 +243,6 @@ static int __init mips_smp_ipi_init(void)
 	struct irq_domain *ipidomain;
 	struct device_node *node;
 
-	/*
-	 * In some cases like qemu-malta, it is desired to try SMP with
-	 * a single core. Qemu-malta has no GIC, so an attempt to set any IPIs
-	 * would cause a BUG_ON() to be triggered since there's no ipidomain.
-	 *
-	 * Since for a single core system IPIs aren't required really, skip the
-	 * initialisation which should generally keep any such configurations
-	 * happy and only fail hard when trying to truely run SMP.
-	 */
-	if (cpumask_weight(cpu_possible_mask) == 1)
-		return 0;
-
 	node = of_irq_find_parent(of_root);
 	ipidomain = irq_find_matching_host(node, DOMAIN_BUS_IPI);
 
@@ -266,7 +254,17 @@ static int __init mips_smp_ipi_init(void)
 	if (node && !ipidomain)
 		ipidomain = irq_find_matching_host(NULL, DOMAIN_BUS_IPI);
 
-	BUG_ON(!ipidomain);
+	/*
+	 * There are systems which only use IPI domains some of the time,
+	 * depending upon configuration we don't know until runtime. An
+	 * example is Malta where we may compile in support for GIC & the
+	 * MT ASE, but run on a system which has multiple VPEs in a single
+	 * core and doesn't include a GIC. Until all IPI implementations
+	 * have been converted to use IPI domains the best we can do here
+	 * is to return & hope some other code sets up the IPIs.
+	 */
+	if (!ipidomain)
+		return 0;
 
 	call_virq = irq_reserve_ipi(ipidomain, cpu_possible_mask);
 	BUG_ON(!call_virq);

commit 19fb5818ed60ac2e9609ad16bc48116f4ce269a8
Author: Qais Yousef <qsyousef@gmail.com>
Date:   Thu Mar 17 21:08:09 2016 +0000

    MIPS: Fix broken malta qemu
    
    Malta defconfig compiles with GIC on. Hence when compiling for SMP it causes
    the new IPI code to be activated. But on qemu malta there's no GIC causing a
    BUG_ON(!ipidomain) to be hit in mips_smp_ipi_init().
    
    Since in that configuration one can only run a single core SMP (!), skip IPI
    initialisation if we detect that this is the case. It is a sensible
    behaviour to introduce and should keep such possible configuration to run
    rather than die hard unnecessarily.
    
    Signed-off-by: Qais Yousef <qsyousef@gmail.com>
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/12892/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 37708d9af638..27cb638f0824 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -243,6 +243,18 @@ static int __init mips_smp_ipi_init(void)
 	struct irq_domain *ipidomain;
 	struct device_node *node;
 
+	/*
+	 * In some cases like qemu-malta, it is desired to try SMP with
+	 * a single core. Qemu-malta has no GIC, so an attempt to set any IPIs
+	 * would cause a BUG_ON() to be triggered since there's no ipidomain.
+	 *
+	 * Since for a single core system IPIs aren't required really, skip the
+	 * initialisation which should generally keep any such configurations
+	 * happy and only fail hard when trying to truely run SMP.
+	 */
+	if (cpumask_weight(cpu_possible_mask) == 1)
+		return 0;
+
 	node = of_irq_find_parent(of_root);
 	ipidomain = irq_find_matching_host(node, DOMAIN_BUS_IPI);
 

commit 710d60cbf1b312a8075a2158cbfbbd9c66132dcc
Merge: df2e37c814d5 d10ef6f9380b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 13:50:29 2016 -0700

    Merge branch 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull cpu hotplug updates from Thomas Gleixner:
     "This is the first part of the ongoing cpu hotplug rework:
    
       - Initial implementation of the state machine
    
       - Runs all online and prepare down callbacks on the plugged cpu and
         not on some random processor
    
       - Replaces busy loop waiting with completions
    
       - Adds tracepoints so the states can be followed"
    
    More detailed commentary on this work from an earlier email:
     "What's wrong with the current cpu hotplug infrastructure?
    
       - Asymmetry
    
         The hotplug notifier mechanism is asymmetric versus the bringup and
         teardown.  This is mostly caused by the notifier mechanism.
    
       - Largely undocumented dependencies
    
         While some notifiers use explicitely defined notifier priorities,
         we have quite some notifiers which use numerical priorities to
         express dependencies without any documentation why.
    
       - Control processor driven
    
         Most of the bringup/teardown of a cpu is driven by a control
         processor.  While it is understandable, that preperatory steps,
         like idle thread creation, memory allocation for and initialization
         of essential facilities needs to be done before a cpu can boot,
         there is no reason why everything else must run on a control
         processor.  Before this patch series, bringup looks like this:
    
           Control CPU                     Booting CPU
    
           do preparatory steps
           kick cpu into life
    
                                           do low level init
    
           sync with booting cpu           sync with control cpu
    
           bring the rest up
    
       - All or nothing approach
    
         There is no way to do partial bringups.  That's something which is
         really desired because we waste e.g.  at boot substantial amount of
         time just busy waiting that the cpu comes to life.  That's stupid
         as we could very well do preparatory steps and the initial IPI for
         other cpus and then go back and do the necessary low level
         synchronization with the freshly booted cpu.
    
       - Minimal debuggability
    
         Due to the notifier based design, it's impossible to switch between
         two stages of the bringup/teardown back and forth in order to test
         the correctness.  So in many hotplug notifiers the cancel
         mechanisms are either not existant or completely untested.
    
       - Notifier [un]registering is tedious
    
         To [un]register notifiers we need to protect against hotplug at
         every callsite.  There is no mechanism that bringup/teardown
         callbacks are issued on the online cpus, so every caller needs to
         do it itself.  That also includes error rollback.
    
      What's the new design?
    
         The base of the new design is a symmetric state machine, where both
         the control processor and the booting/dying cpu execute a well
         defined set of states.  Each state is symmetric in the end, except
         for some well defined exceptions, and the bringup/teardown can be
         stopped and reversed at almost all states.
    
         So the bringup of a cpu will look like this in the future:
    
           Control CPU                     Booting CPU
    
           do preparatory steps
           kick cpu into life
    
                                           do low level init
    
           sync with booting cpu           sync with control cpu
    
                                           bring itself up
    
         The synchronization step does not require the control cpu to wait.
         That mechanism can be done asynchronously via a worker or some
         other mechanism.
    
         The teardown can be made very similar, so that the dying cpu cleans
         up and brings itself down.  Cleanups which need to be done after
         the cpu is gone, can be scheduled asynchronously as well.
    
      There is a long way to this, as we need to refactor the notion when a
      cpu is available.  Today we set the cpu online right after it comes
      out of the low level bringup, which is not really correct.
    
      The proper mechanism is to set it to available, i.e. cpu local
      threads, like softirqd, hotplug thread etc. can be scheduled on that
      cpu, and once it finished all booting steps, it's set to online, so
      general workloads can be scheduled on it.  The reverse happens on
      teardown.  First thing to do is to forbid scheduling of general
      workloads, then teardown all the per cpu resources and finally shut it
      off completely.
    
      This patch series implements the basic infrastructure for this at the
      core level.  This includes the following:
    
       - Basic state machine implementation with well defined states, so
         ordering and prioritization can be expressed.
    
       - Interfaces to [un]register state callbacks
    
         This invokes the bringup/teardown callback on all online cpus with
         the proper protection in place and [un]installs the callbacks in
         the state machine array.
    
         For callbacks which have no particular ordering requirement we have
         a dynamic state space, so that drivers don't have to register an
         explicit hotplug state.
    
         If a callback fails, the code automatically does a rollback to the
         previous state.
    
       - Sysfs interface to drive the state machine to a particular step.
    
         This is only partially functional today.  Full functionality and
         therefor testability will be achieved once we converted all
         existing hotplug notifiers over to the new scheme.
    
       - Run all CPU_ONLINE/DOWN_PREPARE notifiers on the booting/dying
         processor:
    
           Control CPU                     Booting CPU
    
           do preparatory steps
           kick cpu into life
    
                                           do low level init
    
           sync with booting cpu           sync with control cpu
           wait for boot
                                           bring itself up
    
                                           Signal completion to control cpu
    
         In a previous step of this work we've done a full tree mechanical
         conversion of all hotplug notifiers to the new scheme.  The balance
         is a net removal of about 4000 lines of code.
    
         This is not included in this series, as we decided to take a
         different approach.  Instead of mechanically converting everything
         over, we will do a proper overhaul of the usage sites one by one so
         they nicely fit into the symmetric callback scheme.
    
         I decided to do that after I looked at the ugliness of some of the
         converted sites and figured out that their hotplug mechanism is
         completely buggered anyway.  So there is no point to do a
         mechanical conversion first as we need to go through the usage
         sites one by one again in order to achieve a full symmetric and
         testable behaviour"
    
    * 'smp-hotplug-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      cpu/hotplug: Document states better
      cpu/hotplug: Fix smpboot thread ordering
      cpu/hotplug: Remove redundant state check
      cpu/hotplug: Plug death reporting race
      rcu: Make CPU_DYING_IDLE an explicit call
      cpu/hotplug: Make wait for dead cpu completion based
      cpu/hotplug: Let upcoming cpu bring itself fully up
      arch/hotplug: Call into idle with a proper state
      cpu/hotplug: Move online calls to hotplugged cpu
      cpu/hotplug: Create hotplug threads
      cpu/hotplug: Split out the state walk into functions
      cpu/hotplug: Unpark smpboot threads from the state machine
      cpu/hotplug: Move scheduler cpu_online notifier to hotplug core
      cpu/hotplug: Implement setup/removal interface
      cpu/hotplug: Make target state writeable
      cpu/hotplug: Add sysfs state interface
      cpu/hotplug: Hand in target state to _cpu_up/down
      cpu/hotplug: Convert the hotplugged cpu work to a state machine
      cpu/hotplug: Convert to a state machine for the control processor
      cpu/hotplug: Add tracepoints
      ...

commit df2e37c814d51692803245fcbecca360d4882e96
Merge: 8a284c062ec9 8e7fe2660d4a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 15 12:48:48 2016 -0700

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The 4.6 pile of irq updates contains:
    
       - Support for IPI irqdomains to support proper integration of IPIs to
         and from coprocessors.  The first user of this new facility is
         MIPS.  The relevant MIPS patches come with the core to avoid merge
         ordering issues and have been acked by Ralf.
    
       - A new command line option to set the default interrupt affinity
         mask at boot time.
    
       - Support for some more new ARM and MIPS interrupt controllers:
         tango, alpine-msix and bcm6345-l1
    
       - Two small cleanups for x86/apic which we merged into irq/core to
         avoid yet another branch in x86 with two tiny commits.
    
       - The usual set of updates, cleanups in drivers/irqchip.  Mostly in
         the area of ARM-GIC, arada-37-xp and atmel chips.  Nothing
         outstanding here"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      irqchip/irq-alpine-msi: Release the correct domain on error
      irqchip/mxs: Fix error check of of_io_request_and_map()
      irqchip/sunxi-nmi: Fix error check of of_io_request_and_map()
      genirq: Export IRQ functions for module use
      irqchip/gic/realview: Support more RealView DCC variants
      Documentation/bindings: Document the Alpine MSIX driver
      irqchip: Add the Alpine MSIX interrupt controller
      irqchip/gic-v3: Always return IRQ_SET_MASK_OK_DONE in gic_set_affinity
      irqchip/gic-v3-its: Mark its_init() and its children as __init
      irqchip/gic-v3: Remove gic_root_node variable from the ITS code
      irqchip/gic-v3: ACPI: Add redistributor support via GICC structures
      irqchip/gic-v3: Add ACPI support for GICv3/4 initialization
      irqchip/gic-v3: Refactor gic_of_init() for GICv3 driver
      x86/apic: Deinline _flat_send_IPI_mask, save ~150 bytes
      x86/apic: Deinline __default_send_IPI_*, save ~200 bytes
      dt-bindings: interrupt-controller: Add SoC-specific compatible string to Marvell ODMI
      irqchip/mips-gic: Add new DT property to reserve IPIs
      MIPS: Delete smp-gic.c
      MIPS: Make smp CMP, CPS and MT use the new generic IPI functions
      MIPS: Add generic SMP IPI support
      ...

commit d825c06bfe8b885b797f917ad47365d0e9c21fbb
Author: James Hogan <james.hogan@imgtec.com>
Date:   Fri Mar 4 10:10:51 2016 +0000

    MIPS: smp.c: Fix uninitialised temp_foreign_map
    
    When calculate_cpu_foreign_map() recalculates the cpu_foreign_map
    cpumask it uses the local variable temp_foreign_map without initialising
    it to zero. Since the calculation only ever sets bits in this cpumask
    any existing bits at that memory location will remain set and find their
    way into cpu_foreign_map too. This could potentially lead to cache
    operations suboptimally doing smp calls to multiple VPEs in the same
    core, even though the VPEs share primary caches.
    
    Therefore initialise temp_foreign_map using cpumask_clear() before use.
    
    Fixes: cccf34e9411c ("MIPS: c-r4k: Fix cache flushing for MT cores")
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/12759/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bd4385a8e6e8..2b521e07b860 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -121,6 +121,7 @@ static inline void calculate_cpu_foreign_map(void)
 	cpumask_t temp_foreign_map;
 
 	/* Re-calculate the mask */
+	cpumask_clear(&temp_foreign_map);
 	for_each_online_cpu(i) {
 		core_present = 0;
 		for_each_cpu(k, &temp_foreign_map)

commit fc6d73d67436e7784758a831227bd019547a3f73
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:40 2016 +0000

    arch/hotplug: Call into idle with a proper state
    
    Let the non boot cpus call into idle with the corresponding hotplug state, so
    the hotplug core can handle the further bringup. That's a first step to
    convert the boot side of the hotplugged cpus to do all the synchronization
    with the other side through the state machine. For now it'll only start the
    hotplug thread and kick the full bringup of the cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.614102639@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bd4385a8e6e8..f2112a8ddf15 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -191,7 +191,7 @@ asmlinkage void start_secondary(void)
 	WARN_ON_ONCE(!irqs_disabled());
 	mp_ops->smp_finish();
 
-	cpu_startup_entry(CPUHP_ONLINE);
+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 }
 
 static void stop_this_cpu(void *dummy)

commit fbde2d7d8290d8c642d591a471356abda2446874
Author: Qais Yousef <qais.yousef@imgtec.com>
Date:   Tue Dec 8 13:20:27 2015 +0000

    MIPS: Add generic SMP IPI support
    
    Use the new generic IPI layer to provide generic SMP IPI support if the irqchip
    supports it.
    
    Signed-off-by: Qais Yousef <qais.yousef@imgtec.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: <jason@lakedaemon.net>
    Cc: <marc.zyngier@arm.com>
    Cc: <jiang.liu@linux.intel.com>
    Cc: <linux-mips@linux-mips.org>
    Cc: <lisa.parratt@imgtec.com>
    Cc: Qais Yousef <qsyousef@gmail.com>
    Link: http://lkml.kernel.org/r/1449580830-23652-17-git-send-email-qais.yousef@imgtec.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bd4385a8e6e8..c012e1903f6b 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -33,12 +33,16 @@
 #include <linux/cpu.h>
 #include <linux/err.h>
 #include <linux/ftrace.h>
+#include <linux/irqdomain.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
 
 #include <linux/atomic.h>
 #include <asm/cpu.h>
 #include <asm/processor.h>
 #include <asm/idle.h>
 #include <asm/r4k-timer.h>
+#include <asm/mips-cpc.h>
 #include <asm/mmu_context.h>
 #include <asm/time.h>
 #include <asm/setup.h>
@@ -79,6 +83,11 @@ static cpumask_t cpu_core_setup_map;
 
 cpumask_t cpu_coherent_mask;
 
+#ifdef CONFIG_GENERIC_IRQ_IPI
+static struct irq_desc *call_desc;
+static struct irq_desc *sched_desc;
+#endif
+
 static inline void set_cpu_sibling_map(int cpu)
 {
 	int i;
@@ -145,6 +154,133 @@ void register_smp_ops(struct plat_smp_ops *ops)
 	mp_ops = ops;
 }
 
+#ifdef CONFIG_GENERIC_IRQ_IPI
+void mips_smp_send_ipi_single(int cpu, unsigned int action)
+{
+	mips_smp_send_ipi_mask(cpumask_of(cpu), action);
+}
+
+void mips_smp_send_ipi_mask(const struct cpumask *mask, unsigned int action)
+{
+	unsigned long flags;
+	unsigned int core;
+	int cpu;
+
+	local_irq_save(flags);
+
+	switch (action) {
+	case SMP_CALL_FUNCTION:
+		__ipi_send_mask(call_desc, mask);
+		break;
+
+	case SMP_RESCHEDULE_YOURSELF:
+		__ipi_send_mask(sched_desc, mask);
+		break;
+
+	default:
+		BUG();
+	}
+
+	if (mips_cpc_present()) {
+		for_each_cpu(cpu, mask) {
+			core = cpu_data[cpu].core;
+
+			if (core == current_cpu_data.core)
+				continue;
+
+			while (!cpumask_test_cpu(cpu, &cpu_coherent_mask)) {
+				mips_cpc_lock_other(core);
+				write_cpc_co_cmd(CPC_Cx_CMD_PWRUP);
+				mips_cpc_unlock_other();
+			}
+		}
+	}
+
+	local_irq_restore(flags);
+}
+
+
+static irqreturn_t ipi_resched_interrupt(int irq, void *dev_id)
+{
+	scheduler_ipi();
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t ipi_call_interrupt(int irq, void *dev_id)
+{
+	generic_smp_call_function_interrupt();
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction irq_resched = {
+	.handler	= ipi_resched_interrupt,
+	.flags		= IRQF_PERCPU,
+	.name		= "IPI resched"
+};
+
+static struct irqaction irq_call = {
+	.handler	= ipi_call_interrupt,
+	.flags		= IRQF_PERCPU,
+	.name		= "IPI call"
+};
+
+static __init void smp_ipi_init_one(unsigned int virq,
+				    struct irqaction *action)
+{
+	int ret;
+
+	irq_set_handler(virq, handle_percpu_irq);
+	ret = setup_irq(virq, action);
+	BUG_ON(ret);
+}
+
+static int __init mips_smp_ipi_init(void)
+{
+	unsigned int call_virq, sched_virq;
+	struct irq_domain *ipidomain;
+	struct device_node *node;
+
+	node = of_irq_find_parent(of_root);
+	ipidomain = irq_find_matching_host(node, DOMAIN_BUS_IPI);
+
+	/*
+	 * Some platforms have half DT setup. So if we found irq node but
+	 * didn't find an ipidomain, try to search for one that is not in the
+	 * DT.
+	 */
+	if (node && !ipidomain)
+		ipidomain = irq_find_matching_host(NULL, DOMAIN_BUS_IPI);
+
+	BUG_ON(!ipidomain);
+
+	call_virq = irq_reserve_ipi(ipidomain, cpu_possible_mask);
+	BUG_ON(!call_virq);
+
+	sched_virq = irq_reserve_ipi(ipidomain, cpu_possible_mask);
+	BUG_ON(!sched_virq);
+
+	if (irq_domain_is_ipi_per_cpu(ipidomain)) {
+		int cpu;
+
+		for_each_cpu(cpu, cpu_possible_mask) {
+			smp_ipi_init_one(call_virq + cpu, &irq_call);
+			smp_ipi_init_one(sched_virq + cpu, &irq_resched);
+		}
+	} else {
+		smp_ipi_init_one(call_virq, &irq_call);
+		smp_ipi_init_one(sched_virq, &irq_resched);
+	}
+
+	call_desc = irq_to_desc(call_virq);
+	sched_desc = irq_to_desc(sched_virq);
+
+	return 0;
+}
+early_initcall(mips_smp_ipi_init);
+#endif
+
 /*
  * First C code run on the secondary CPUs after being started up by
  * the master.

commit e060f6ed281669b6d2f22d8dafd664b532386918
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Sep 25 08:59:38 2015 -0700

    MIPS: Initialise MAARs on secondary CPUs
    
    MAARs should be initialised on each CPU (or rather, core) in the system
    in order to achieve consistent behaviour & performance. Previously they
    have only been initialised on the boot CPU which leads to performance
    problems if tasks are later scheduled on a secondary CPU, particularly
    if those tasks make use of unaligned vector accesses where some CPUs
    don't handle any cases in hardware for non-speculative memory regions.
    Fix this by recording the MAAR configuration from the boot CPU and
    applying it to secondary CPUs as part of their bringup.
    
    Reported-by: Doug Gilmore <doug.gilmore@imgtec.com>
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: Andrew Bresticker <abrestic@chromium.org>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: Aaro Koskinen <aaro.koskinen@iki.fi>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Markos Chandras <markos.chandras@imgtec.com>
    Cc: Hemmo Nieminen <hemmo.nieminen@iki.fi>
    Cc: Alex Smith <alex.smith@imgtec.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Patchwork: https://patchwork.linux-mips.org/patch/11239/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index a31896c33716..bd4385a8e6e8 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -42,6 +42,7 @@
 #include <asm/mmu_context.h>
 #include <asm/time.h>
 #include <asm/setup.h>
+#include <asm/maar.h>
 
 cpumask_t cpu_callin_map;		/* Bitmask of started secondaries */
 
@@ -157,6 +158,7 @@ asmlinkage void start_secondary(void)
 	mips_clockevent_init();
 	mp_ops->init_secondary();
 	cpu_report();
+	maar_init();
 
 	/*
 	 * XXX parity protection should be folded in here when it's converted

commit 4ace6139bf23ab4f152ba4207fc10b76cc01d2a5
Author: Alex Smith <alex.smith@imgtec.com>
Date:   Fri Jul 24 16:57:49 2015 +0100

    MIPS: SMP: Don't increment irq_count multiple times for call function IPIs
    
    The majority of SMP platforms handle their IPIs through do_IRQ()
    which calls irq_{enter/exit}(). When a call function IPI is received,
    smp_call_function_interrupt() is called which also calls
    irq_{enter,exit}(), meaning irq_count is raised twice.
    
    When tick broadcasting is used (which is implemented via a call
    function IPI), this incorrectly causes all CPU idle time on the core
    receiving broadcast ticks to be accounted as time spent servicing
    IRQs, as account_process_tick() will account as such if irq_count is
    greater than 1. This results in 100% CPU usage being reported on a
    core which receives its ticks via broadcast.
    
    This patch removes the SMP smp_call_function_interrupt() wrapper which
    calls irq_{enter,exit}(). Platforms which handle their IPIs through
    do_IRQ() now call generic_smp_call_function_interrupt() directly to
    avoid incrementing irq_count a second time. Platforms which don't
    (loongson, sgi-ip27, sibyte) call generic_smp_call_function_interrupt()
    wrapped in irq_{enter,exit}().
    
    Signed-off-by: Alex Smith <alex.smith@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/10770/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index d0744cc77ea7..a31896c33716 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -192,16 +192,6 @@ asmlinkage void start_secondary(void)
 	cpu_startup_entry(CPUHP_ONLINE);
 }
 
-/*
- * Call into both interrupt handlers, as we share the IPI for them
- */
-void __irq_entry smp_call_function_interrupt(void)
-{
-	irq_enter();
-	generic_smp_call_function_interrupt();
-	irq_exit();
-}
-
 static void stop_this_cpu(void *dummy)
 {
 	/*

commit cccf34e9411c41b0cbfb41980fe55fc8e7c98fd2
Author: Markos Chandras <markos.chandras@imgtec.com>
Date:   Fri Jul 10 09:29:10 2015 +0100

    MIPS: c-r4k: Fix cache flushing for MT cores
    
    MT_SMP is not the only SMP option for MT cores. The MT_SMP option
    allows more than one VPE per core to appear as a secondary CPU in the
    system. Because of how CM works, it propagates the address-based
    cache ops to the secondary cores but not the index-based ones.
    Because of that, the code does not use IPIs to flush the L1 caches on
    secondary cores because the CM would have done that already. However,
    the CM functionality is independent of the type of SMP kernel so even in
    non-MT kernels, IPIs are not necessary. As a result of which, we change
    the conditional to depend on the CM presence. Moreover, since VPEs on
    the same core share the same L1 caches, there is no need to send an
    IPI on all of them so we calculate a suitable cpumask with only one
    VPE per core.
    
    Signed-off-by: Markos Chandras <markos.chandras@imgtec.com>
    Cc: <stable@vger.kernel.org> # 3.15+
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/10654/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index faa46ebd9dda..d0744cc77ea7 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -63,6 +63,13 @@ EXPORT_SYMBOL(cpu_sibling_map);
 cpumask_t cpu_core_map[NR_CPUS] __read_mostly;
 EXPORT_SYMBOL(cpu_core_map);
 
+/*
+ * A logcal cpu mask containing only one VPE per core to
+ * reduce the number of IPIs on large MT systems.
+ */
+cpumask_t cpu_foreign_map __read_mostly;
+EXPORT_SYMBOL(cpu_foreign_map);
+
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
@@ -103,6 +110,29 @@ static inline void set_cpu_core_map(int cpu)
 	}
 }
 
+/*
+ * Calculate a new cpu_foreign_map mask whenever a
+ * new cpu appears or disappears.
+ */
+static inline void calculate_cpu_foreign_map(void)
+{
+	int i, k, core_present;
+	cpumask_t temp_foreign_map;
+
+	/* Re-calculate the mask */
+	for_each_online_cpu(i) {
+		core_present = 0;
+		for_each_cpu(k, &temp_foreign_map)
+			if (cpu_data[i].package == cpu_data[k].package &&
+			    cpu_data[i].core == cpu_data[k].core)
+				core_present = 1;
+		if (!core_present)
+			cpumask_set_cpu(i, &temp_foreign_map);
+	}
+
+	cpumask_copy(&cpu_foreign_map, &temp_foreign_map);
+}
+
 struct plat_smp_ops *mp_ops;
 EXPORT_SYMBOL(mp_ops);
 
@@ -146,6 +176,8 @@ asmlinkage void start_secondary(void)
 	set_cpu_sibling_map(cpu);
 	set_cpu_core_map(cpu);
 
+	calculate_cpu_foreign_map();
+
 	cpumask_set_cpu(cpu, &cpu_callin_map);
 
 	synchronise_count_slave(cpu);
@@ -173,9 +205,18 @@ void __irq_entry smp_call_function_interrupt(void)
 static void stop_this_cpu(void *dummy)
 {
 	/*
-	 * Remove this CPU:
+	 * Remove this CPU. Be a bit slow here and
+	 * set the bits for every online CPU so we don't miss
+	 * any IPI whilst taking this VPE down.
 	 */
+
+	cpumask_copy(&cpu_foreign_map, cpu_online_mask);
+
+	/* Make it visible to every other CPU */
+	smp_mb();
+
 	set_cpu_online(smp_processor_id(), false);
+	calculate_cpu_foreign_map();
 	local_irq_disable();
 	while (1);
 }
@@ -197,6 +238,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	mp_ops->prepare_cpus(max_cpus);
 	set_cpu_sibling_map(0);
 	set_cpu_core_map(0);
+	calculate_cpu_foreign_map();
 #ifndef CONFIG_HOTPLUG_CPU
 	init_cpu_present(cpu_possible_mask);
 #endif

commit cafb45b2562baa57cb58bef0636c073705954cc4
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue May 12 06:43:04 2015 +0200

    MIPS: SMP: Fix build error.
    
      CC      arch/mips/kernel/smp.o
    arch/mips/kernel/smp.c: In function start_secondary:
    arch/mips/kernel/smp.c:149:2: error: passing argument 2 of cpumask_set_cpu discards volatile qualifier from pointer target type [-Werror]
      cpumask_set_cpu(cpu, &cpu_callin_map);
      ^
    In file included from ./arch/mips/include/asm/processor.h:14:0,
                     from ./arch/mips/include/asm/thread_info.h:15,
                     from include/linux/thread_info.h:54,
                     from include/asm-generic/preempt.h:4,
                     from arch/mips/include/generated/asm/preempt.h:1,
                     from include/linux/preempt.h:18,
                     from include/linux/interrupt.h:8,
                     from arch/mips/kernel/smp.c:24:
    include/linux/cpumask.h:272:91: note: expected struct cpumask * but argument is of type volatile struct cpumask_t *
     static inline void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
                                                                                               ^
    arch/mips/kernel/smp.c: In function smp_prepare_boot_cpu:
    arch/mips/kernel/smp.c:211:2: error: passing argument 2 of cpumask_set_cpu discards volatile qualifier from pointer target type [-Werror]
      cpumask_set_cpu(0, &cpu_callin_map);
      ^
    In file included from ./arch/mips/include/asm/processor.h:14:0,
                     from ./arch/mips/include/asm/thread_info.h:15,
                     from include/linux/thread_info.h:54,
                     from include/asm-generic/preempt.h:4,
                     from arch/mips/include/generated/asm/preempt.h:1,
                     from include/linux/preempt.h:18,
                     from include/linux/interrupt.h:8,
                     from arch/mips/kernel/smp.c:24:
    include/linux/cpumask.h:272:91: note: expected struct cpumask * but argument is of type volatile struct cpumask_t *
     static inline void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
                                                                                               ^
    arch/mips/kernel/smp.c: In function __cpu_up:
    arch/mips/kernel/smp.c:221:10: error: passing argument 2 of cpumask_test_cpu discards volatile qualifier from pointer target type [-Werror]
      while (!cpumask_test_cpu(cpu, &cpu_callin_map))
              ^
    In file included from ./arch/mips/include/asm/processor.h:14:0,
                     from ./arch/mips/include/asm/thread_info.h:15,
                     from include/linux/thread_info.h:54,
                     from include/asm-generic/preempt.h:4,
                     from arch/mips/include/generated/asm/preempt.h:1,
                     from include/linux/preempt.h:18,
                     from include/linux/interrupt.h:8,
                     from arch/mips/kernel/smp.c:24:
    include/linux/cpumask.h:294:90: note: expected const struct cpumask * but argument is of type volatile struct cpumask_t *
     static inline int cpumask_test_cpu(int cpu, const struct cpumask *cpumask)
                                                                                              ^
    cc1: all warnings being treated as errors
    make[2]: *** [arch/mips/kernel/smp.o] Error 1
    make[1]: *** [arch/mips/kernel] Error 2
    make: *** [arch/mips] Error 2
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 193ace7955fb..faa46ebd9dda 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -43,7 +43,7 @@
 #include <asm/time.h>
 #include <asm/setup.h>
 
-volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
+cpumask_t cpu_callin_map;		/* Bitmask of started secondaries */
 
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 EXPORT_SYMBOL(__cpu_number_map);
@@ -218,8 +218,10 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	/*
 	 * Trust is futile.  We should really have timeouts ...
 	 */
-	while (!cpumask_test_cpu(cpu, &cpu_callin_map))
+	while (!cpumask_test_cpu(cpu, &cpu_callin_map)) {
 		udelay(100);
+		schedule();
+	}
 
 	synchronise_count_master(cpu);
 	return 0;

commit 6496edfce95f943e1da43631c2f437509e56af7f
Merge: b19a42e3cb9e e4afa120c982
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 20 10:19:03 2015 -0700

    Merge tag 'cpumask-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux
    
    Pull final removal of deprecated cpus_* cpumask functions from Rusty Russell:
     "This is the final removal (after several years!) of the obsolete
      cpus_* functions, prompted by their mis-use in staging.
    
      With these function removed, all cpu functions should only iterate to
      nr_cpu_ids, so we finally only allocate that many bits when cpumasks
      are allocated offstack"
    
    * tag 'cpumask-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux: (25 commits)
      cpumask: remove __first_cpu / __next_cpu
      cpumask: resurrect CPU_MASK_CPU0
      linux/cpumask.h: add typechecking to cpumask_test_cpu
      cpumask: only allocate nr_cpumask_bits.
      Fix weird uses of num_online_cpus().
      cpumask: remove deprecated functions.
      mips: fix obsolete cpumask_of_cpu usage.
      x86: fix more deprecated cpu function usage.
      ia64: remove deprecated cpus_ usage.
      powerpc: fix deprecated CPU_MASK_CPU0 usage.
      CPU_MASK_ALL/CPU_MASK_NONE: remove from deprecated region.
      staging/lustre/o2iblnd: Don't use cpus_weight
      staging/lustre/libcfs: replace deprecated cpus_ calls with cpumask_
      staging/lustre/ptlrpc: Do not use deprecated cpus_* functions
      blackfin: fix up obsolete cpu function usage.
      parisc: fix up obsolete cpu function usage.
      tile: fix up obsolete cpu function usage.
      arm64: fix up obsolete cpu function usage.
      mips: fix up obsolete cpu function usage.
      x86: fix up obsolete cpu function usage.
      ...

commit ea925a72a271f6868dddef98426b396f110da211
Author: Andrew Bresticker <abrestic@chromium.org>
Date:   Wed Mar 25 10:25:43 2015 -0700

    MIPS: smp: Make stop_this_cpu() actually stop the CPU
    
    Since cpu_wait() enables interrupts upon return, CPUs which have
    entered stop_this_cpu() may still end up handling interrupts.
    This can lead to the softlockup detector firing on a panic or
    restart/poweroff/halt.  Just disable interrupts and spin to ensure
    nothing else runs on the CPU once it has entered stop_this_cpu().
    
    Signed-off-by: Andrew Bresticker <abrestic@chromium.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Maciej W. Rozycki <macro@linux-mips.org>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/9601/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 1c0d8c50b7e1..5b020bda3e05 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -176,10 +176,8 @@ static void stop_this_cpu(void *dummy)
 	 * Remove this CPU:
 	 */
 	set_cpu_online(smp_processor_id(), false);
-	for (;;) {
-		if (cpu_wait)
-			(*cpu_wait)();		/* Wait if available. */
-	}
+	local_irq_disable();
+	while (1);
 }
 
 void smp_send_stop(void)

commit 8dd928915a73bf95a727a46037964243eb1e042c
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Mar 5 10:49:17 2015 +1030

    mips: fix up obsolete cpu function usage.
    
    Thanks to spatch, plus manual removal of "&*".  Then a sweep for
    for_each_cpu_mask => for_each_cpu.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kevin Cernekee <cernekee@gmail.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: linux-mips@linux-mips.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 1c0d8c50b7e1..357acd3ac0e6 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -75,30 +75,30 @@ static inline void set_cpu_sibling_map(int cpu)
 {
 	int i;
 
-	cpu_set(cpu, cpu_sibling_setup_map);
+	cpumask_set_cpu(cpu, &cpu_sibling_setup_map);
 
 	if (smp_num_siblings > 1) {
-		for_each_cpu_mask(i, cpu_sibling_setup_map) {
+		for_each_cpu(i, &cpu_sibling_setup_map) {
 			if (cpu_data[cpu].package == cpu_data[i].package &&
 				    cpu_data[cpu].core == cpu_data[i].core) {
-				cpu_set(i, cpu_sibling_map[cpu]);
-				cpu_set(cpu, cpu_sibling_map[i]);
+				cpumask_set_cpu(i, &cpu_sibling_map[cpu]);
+				cpumask_set_cpu(cpu, &cpu_sibling_map[i]);
 			}
 		}
 	} else
-		cpu_set(cpu, cpu_sibling_map[cpu]);
+		cpumask_set_cpu(cpu, &cpu_sibling_map[cpu]);
 }
 
 static inline void set_cpu_core_map(int cpu)
 {
 	int i;
 
-	cpu_set(cpu, cpu_core_setup_map);
+	cpumask_set_cpu(cpu, &cpu_core_setup_map);
 
-	for_each_cpu_mask(i, cpu_core_setup_map) {
+	for_each_cpu(i, &cpu_core_setup_map) {
 		if (cpu_data[cpu].package == cpu_data[i].package) {
-			cpu_set(i, cpu_core_map[cpu]);
-			cpu_set(cpu, cpu_core_map[i]);
+			cpumask_set_cpu(i, &cpu_core_map[cpu]);
+			cpumask_set_cpu(cpu, &cpu_core_map[i]);
 		}
 	}
 }
@@ -138,7 +138,7 @@ asmlinkage void start_secondary(void)
 	cpu = smp_processor_id();
 	cpu_data[cpu].udelay_val = loops_per_jiffy;
 
-	cpu_set(cpu, cpu_coherent_mask);
+	cpumask_set_cpu(cpu, &cpu_coherent_mask);
 	notify_cpu_starting(cpu);
 
 	set_cpu_online(cpu, true);
@@ -146,7 +146,7 @@ asmlinkage void start_secondary(void)
 	set_cpu_sibling_map(cpu);
 	set_cpu_core_map(cpu);
 
-	cpu_set(cpu, cpu_callin_map);
+	cpumask_set_cpu(cpu, &cpu_callin_map);
 
 	synchronise_count_slave(cpu);
 
@@ -210,7 +210,7 @@ void smp_prepare_boot_cpu(void)
 {
 	set_cpu_possible(0, true);
 	set_cpu_online(0, true);
-	cpu_set(0, cpu_callin_map);
+	cpumask_set_cpu(0, &cpu_callin_map);
 }
 
 int __cpu_up(unsigned int cpu, struct task_struct *tidle)
@@ -220,7 +220,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	/*
 	 * Trust is futile.  We should really have timeouts ...
 	 */
-	while (!cpu_isset(cpu, cpu_callin_map))
+	while (!cpumask_test_cpu(cpu, &cpu_callin_map))
 		udelay(100);
 
 	synchronise_count_master(cpu);

commit c7754e75100ed5e3068ac5085747f2bfc386c8d6
Author: Hemmo Nieminen <hemmo.nieminen@iki.fi>
Date:   Thu Jan 15 23:01:59 2015 +0200

    MIPS: Fix kernel lockup or crash after CPU offline/online
    
    As printk() invocation can cause e.g. a TLB miss, printk() cannot be
    called before the exception handlers have been properly initialized.
    This can happen e.g. when netconsole has been loaded as a kernel module
    and the TLB table has been cleared when a CPU was offline.
    
    Call cpu_report() in start_secondary() only after the exception handlers
    have been initialized to fix this.
    
    Without the patch the kernel will randomly either lockup or crash
    after a CPU is onlined and the console driver is a module.
    
    Signed-off-by: Hemmo Nieminen <hemmo.nieminen@iki.fi>
    Signed-off-by: Aaro Koskinen <aaro.koskinen@iki.fi>
    Cc: stable@vger.kernel.org
    Cc: David Daney <david.daney@cavium.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/8953/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index c94c4e92e17d..1c0d8c50b7e1 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -123,10 +123,10 @@ asmlinkage void start_secondary(void)
 	unsigned int cpu;
 
 	cpu_probe();
-	cpu_report();
 	per_cpu_trap_init(false);
 	mips_clockevent_init();
 	mp_ops->init_secondary();
+	cpu_report();
 
 	/*
 	 * XXX parity protection should be folded in here when it's converted

commit bda4584cd943d7bb6cf677a8d694700c1984cf3e
Author: Huacai Chen <chenhc@lemote.com>
Date:   Thu Jun 26 11:41:26 2014 +0800

    MIPS: Support CPU topology files in sysfs
    
    This patch is prepared for Loongson's NUMA support, it offer meaningful
    sysfs files such as physical_package_id, core_id, core_siblings and
    thread_siblings in /sys/devices/system/cpu/cpu?/topology.
    
    Signed-off-by: Huacai Chen <chenhc@lemote.com>
    Reviewed-by: Andreas Herrmann <andreas.herrmann@caviumnetworks.com>
    Cc: John Crispin <john@phrozen.org>
    Cc: Steven J. Hill <Steven.Hill@imgtec.com>
    Cc: Aurelien Jarno <aurelien@aurel32.net>
    Cc: linux-mips@linux-mips.org
    Cc: Fuxin Zhang <zhangfx@lemote.com>
    Cc: Zhangjin Wu <wuzhangjin@gmail.com>
    Patchwork: https://patchwork.linux-mips.org/patch/7184/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 9bad52ede903..c94c4e92e17d 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -59,9 +59,16 @@ EXPORT_SYMBOL(smp_num_siblings);
 cpumask_t cpu_sibling_map[NR_CPUS] __read_mostly;
 EXPORT_SYMBOL(cpu_sibling_map);
 
+/* representing the core map of multi-core chips of each logical CPU */
+cpumask_t cpu_core_map[NR_CPUS] __read_mostly;
+EXPORT_SYMBOL(cpu_core_map);
+
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
+/* representing cpus for which core maps can be computed */
+static cpumask_t cpu_core_setup_map;
+
 cpumask_t cpu_coherent_mask;
 
 static inline void set_cpu_sibling_map(int cpu)
@@ -72,7 +79,8 @@ static inline void set_cpu_sibling_map(int cpu)
 
 	if (smp_num_siblings > 1) {
 		for_each_cpu_mask(i, cpu_sibling_setup_map) {
-			if (cpu_data[cpu].core == cpu_data[i].core) {
+			if (cpu_data[cpu].package == cpu_data[i].package &&
+				    cpu_data[cpu].core == cpu_data[i].core) {
 				cpu_set(i, cpu_sibling_map[cpu]);
 				cpu_set(cpu, cpu_sibling_map[i]);
 			}
@@ -81,6 +89,20 @@ static inline void set_cpu_sibling_map(int cpu)
 		cpu_set(cpu, cpu_sibling_map[cpu]);
 }
 
+static inline void set_cpu_core_map(int cpu)
+{
+	int i;
+
+	cpu_set(cpu, cpu_core_setup_map);
+
+	for_each_cpu_mask(i, cpu_core_setup_map) {
+		if (cpu_data[cpu].package == cpu_data[i].package) {
+			cpu_set(i, cpu_core_map[cpu]);
+			cpu_set(cpu, cpu_core_map[i]);
+		}
+	}
+}
+
 struct plat_smp_ops *mp_ops;
 EXPORT_SYMBOL(mp_ops);
 
@@ -122,6 +144,7 @@ asmlinkage void start_secondary(void)
 	set_cpu_online(cpu, true);
 
 	set_cpu_sibling_map(cpu);
+	set_cpu_core_map(cpu);
 
 	cpu_set(cpu, cpu_callin_map);
 
@@ -175,6 +198,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	current_thread_info()->cpu = 0;
 	mp_ops->prepare_cpus(max_cpus);
 	set_cpu_sibling_map(0);
+	set_cpu_core_map(0);
 #ifndef CONFIG_HOTPLUG_CPU
 	init_cpu_present(cpu_possible_mask);
 #endif

commit 2e2d663d2dd64ffe9855be0b35aa221c9b8139f2
Merge: 5ec79bf919dd 322014531e1f
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed May 28 19:00:14 2014 +0200

    Merge branch 'wip-mips-pm' of https://github.com/paulburton/linux into mips-for-linux-next

commit 1461df59f0de0ecdebf9db090164d793e5b94442
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue May 27 10:56:23 2014 +0200

    MIPS: SMP: Remove plat_smp_ops cpus_done method.
    
    Nothing was using the method and there isn't any need for this hook.  This
    leaves smp_cpus_done() empty for the moment.
    
    As suggested by Paul Bolle <pebolle@tiscali.nl>.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 35bb05a13f05..ce7677523b68 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -163,7 +163,6 @@ void smp_send_stop(void)
 
 void __init smp_cpus_done(unsigned int max_cpus)
 {
-	mp_ops->cpus_done();
 }
 
 /* called from main before smp_init() */

commit b633648c5ad3cfbda0b3daea50d2135d44899259
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri May 23 16:29:44 2014 +0200

    MIPS: MT: Remove SMTC support
    
    Nobody is maintaining SMTC anymore and there also seems to be no userbase.
    Which is a pity - the SMTC technology primarily developed by Kevin D.
    Kissell <kevink@paralogos.com> is an ingenious demonstration for the MT
    ASE's power and elegance.
    
    Based on Markos Chandras <Markos.Chandras@imgtec.com> patch
    https://patchwork.linux-mips.org/patch/6719/ which while very similar did
    no longer apply cleanly when I tried to merge it plus some additional
    post-SMTC cleanup - SMTC was a feature as tricky to remove as it was to
    merge once upon a time.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 0a022ee33b2a..35bb05a13f05 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -43,10 +43,6 @@
 #include <asm/time.h>
 #include <asm/setup.h>
 
-#ifdef CONFIG_MIPS_MT_SMTC
-#include <asm/mipsmtregs.h>
-#endif /* CONFIG_MIPS_MT_SMTC */
-
 volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
@@ -102,12 +98,6 @@ asmlinkage void start_secondary(void)
 {
 	unsigned int cpu;
 
-#ifdef CONFIG_MIPS_MT_SMTC
-	/* Only do cpu_probe for first TC of CPU */
-	if ((read_c0_tcbind() & TCBIND_CURTC) != 0)
-		__cpu_name[smp_processor_id()] = __cpu_name[0];
-	else
-#endif /* CONFIG_MIPS_MT_SMTC */
 	cpu_probe();
 	cpu_report();
 	per_cpu_trap_init(false);
@@ -238,13 +228,10 @@ static void flush_tlb_mm_ipi(void *mm)
  *  o collapses to normal function call on UP kernels
  *  o collapses to normal function call on systems with a single shared
  *    primary cache.
- *  o CONFIG_MIPS_MT_SMTC currently implies there is only one physical core.
  */
 static inline void smp_on_other_tlbs(void (*func) (void *info), void *info)
 {
-#ifndef CONFIG_MIPS_MT_SMTC
 	smp_call_function(func, info, 1);
-#endif
 }
 
 static inline void smp_on_each_tlb(void (*func) (void *info), void *info)

commit 76306f4272e036e600254a98bc291df50cedd949
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Feb 14 16:30:52 2014 +0000

    MIPS: introduce cpu_coherent_mask
    
    Add a mask of CPUs which are currently known to be operating coherently.
    This is setup initially to be all present CPUs, but in a subsequent
    patch CPUs in a MIPS Coherent Processing System will be cleared in this
    mask as they enter non-coherent idle states. This will be used in order
    to determine when a CPU within a CPS system may need to be powered back
    up, but may also be used in future to optimise away wakeups for cache
    operations or TLB invalidations.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 9a5226443bc1..991ae968ef26 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -66,6 +66,8 @@ EXPORT_SYMBOL(cpu_sibling_map);
 /* representing cpus for which sibling maps can be computed */
 static cpumask_t cpu_sibling_setup_map;
 
+cpumask_t cpu_coherent_mask;
+
 static inline void set_cpu_sibling_map(int cpu)
 {
 	int i;
@@ -124,6 +126,7 @@ asmlinkage void start_secondary(void)
 	cpu = smp_processor_id();
 	cpu_data[cpu].udelay_val = loops_per_jiffy;
 
+	cpu_set(cpu, cpu_coherent_mask);
 	notify_cpu_starting(cpu);
 
 	set_cpu_online(cpu, true);
@@ -186,6 +189,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 #ifndef CONFIG_HOTPLUG_CPU
 	init_cpu_present(cpu_possible_mask);
 #endif
+	cpumask_copy(&cpu_coherent_mask, cpu_possible_mask);
 }
 
 /* preload SMP state for boot cpu */

commit cc7964af8f997a20240d3ec5bf90c4fd20d3c48a
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Feb 14 09:24:58 2014 +0000

    MIPS: support for generic clockevents broadcast
    
    This patch adds support for generic clockevents broadcast using the a
    dummy clockevent device and the tick_broadcast function introduced by
    commit 12ad10004645 "clockevents: Add generic timer broadcast function".
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 0a022ee33b2a..9a5226443bc1 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -404,3 +404,46 @@ void dump_send_ipi(void (*dump_ipi_callback)(void *))
 }
 EXPORT_SYMBOL(dump_send_ipi);
 #endif
+
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+
+static DEFINE_PER_CPU(atomic_t, tick_broadcast_count);
+static DEFINE_PER_CPU(struct call_single_data, tick_broadcast_csd);
+
+void tick_broadcast(const struct cpumask *mask)
+{
+	atomic_t *count;
+	struct call_single_data *csd;
+	int cpu;
+
+	for_each_cpu(cpu, mask) {
+		count = &per_cpu(tick_broadcast_count, cpu);
+		csd = &per_cpu(tick_broadcast_csd, cpu);
+
+		if (atomic_inc_return(count) == 1)
+			smp_call_function_single_async(cpu, csd);
+	}
+}
+
+static void tick_broadcast_callee(void *info)
+{
+	int cpu = smp_processor_id();
+	tick_receive_broadcast();
+	atomic_set(&per_cpu(tick_broadcast_count, cpu), 0);
+}
+
+static int __init tick_broadcast_init(void)
+{
+	struct call_single_data *csd;
+	int cpu;
+
+	for (cpu = 0; cpu < NR_CPUS; cpu++) {
+		csd = &per_cpu(tick_broadcast_csd, cpu);
+		csd->func = tick_broadcast_callee;
+	}
+
+	return 0;
+}
+early_initcall(tick_broadcast_init);
+
+#endif /* CONFIG_GENERIC_CLOCKEVENTS_BROADCAST */

commit dbee7169742e81e42bcef0bbc6c42b20d4f665f0
Author: Jiang Liu <jiang.liu@huawei.com>
Date:   Thu Sep 12 00:07:15 2013 +0800

    MIPS: SMP: kill redundant call of generic_smp_call_function_single_interrupt()
    
    Since commit 9a46ad6d6df3b54 "smp: make smp_call_function_many() use
    logic similar to smp_call_function_single()",
    generic_smp_call_function_single_interrupt() is an alias of
    generic_smp_call_function_interrupt(), so kill the redundant call.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jiri Kosina <trivial@kernel.org>
    Cc: Wang YanQing <udknight@gmail.com>
    Cc: linux-mips@linux-mips.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/5820/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 5c208ed8f856..0a022ee33b2a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -150,7 +150,6 @@ asmlinkage void start_secondary(void)
 void __irq_entry smp_call_function_interrupt(void)
 {
 	irq_enter();
-	generic_smp_call_function_single_interrupt();
 	generic_smp_call_function_interrupt();
 	irq_exit();
 }

commit 078a55fc824c1633b3a507e4ad48b4637c1dfc18
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 13:38:59 2013 +0000

    MIPS: Delete __cpuinit/__CPUINIT usage from MIPS code
    
    commit 3747069b25e419f6b51395f48127e9812abc3596 upstream.
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    and are flagged as __cpuinit  -- so if we remove the __cpuinit from
    the arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    related content into no-ops as early as possible, since that will get
    rid of these warnings.  In any case, they are temporary and harmless.
    
    Here, we remove all the MIPS __cpuinit from C code and __CPUINIT
    from asm files.  MIPS is interesting in this respect, because there
    are also uasm users hiding behind their own renamed versions of the
    __cpuinit macros.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    [ralf@linux-mips.org: Folded in Paul's followup fix.]
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/5494/
    Patchwork: https://patchwork.linux-mips.org/patch/5495/
    Patchwork: https://patchwork.linux-mips.org/patch/5509/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6e7862ab46cc..5c208ed8f856 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -86,7 +86,7 @@ static inline void set_cpu_sibling_map(int cpu)
 struct plat_smp_ops *mp_ops;
 EXPORT_SYMBOL(mp_ops);
 
-__cpuinit void register_smp_ops(struct plat_smp_ops *ops)
+void register_smp_ops(struct plat_smp_ops *ops)
 {
 	if (mp_ops)
 		printk(KERN_WARNING "Overriding previously set SMP ops\n");
@@ -98,7 +98,7 @@ __cpuinit void register_smp_ops(struct plat_smp_ops *ops)
  * First C code run on the secondary CPUs after being started up by
  * the master.
  */
-asmlinkage __cpuinit void start_secondary(void)
+asmlinkage void start_secondary(void)
 {
 	unsigned int cpu;
 
@@ -197,7 +197,7 @@ void smp_prepare_boot_cpu(void)
 	cpu_set(0, cpu_callin_map);
 }
 
-int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
+int __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	mp_ops->boot_secondary(cpu, tidle);
 

commit bdc92d74e0ec95a8101447467c25f015105f2e5a
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue May 21 16:59:19 2013 +0200

    MIPS: Idle: Consolidate all declarations in <asm/idle.h>.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index c17619fe18e3..6e7862ab46cc 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -37,6 +37,7 @@
 #include <linux/atomic.h>
 #include <asm/cpu.h>
 #include <asm/processor.h>
+#include <asm/idle.h>
 #include <asm/r4k-timer.h>
 #include <asm/mmu_context.h>
 #include <asm/time.h>

commit daf799cca8abbf7f3e253ecf1d41d244070773d7
Merge: 6019958d146a b22d1b6a91ca
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 10 07:48:05 2013 -0700

    Merge branch 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    Pull MIPS updates from Ralf Baechle:
    
     - More work on DT support for various platforms
    
     - Various fixes that were to late to make it straight into 3.9
    
     - Improved platform support, in particular the Netlogic XLR and
       BCM63xx, and the SEAD3 and Malta eval boards.
    
     - Support for several Ralink SOC families.
    
     - Complete support for the microMIPS ASE which basically reencodes the
       existing MIPS32/MIPS64 ISA to use non-constant size instructions.
    
     - Some fallout from LTO work which remove old cruft and will generally
       make the MIPS kernel easier to maintain and resistant to compiler
       optimization, even in absence of LTO.
    
     - KVM support.  While MIPS has announced hardware virtualization
       extensions this KVM extension uses trap and emulate mode for
       virtualization of MIPS32.  More KVM work to add support for VZ
       hardware virtualizaiton extensions and MIPS64 will probably already
       be merged for 3.11.
    
    Most of this has been sitting in -next for a long time.  All defconfigs
    have been build or run time tested except three for which fixes are being
    sent by other maintainers.
    
    Semantic conflict with kvm updates done as per Ralf
    
    * 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus: (118 commits)
      MIPS: Add new GIC clockevent driver.
      MIPS: Formatting clean-ups for clocksources.
      MIPS: Refactor GIC clocksource code.
      MIPS: Move 'gic_frequency' to common location.
      MIPS: Move 'gic_present' to common location.
      MIPS: MIPS16e: Add unaligned access support.
      MIPS: MIPS16e: Support handling of delay slots.
      MIPS: MIPS16e: Add instruction formats.
      MIPS: microMIPS: Optimise 'strnlen' core library function.
      MIPS: microMIPS: Optimise 'strlen' core library function.
      MIPS: microMIPS: Optimise 'strncpy' core library function.
      MIPS: microMIPS: Optimise 'memset' core library function.
      MIPS: microMIPS: Add configuration option for microMIPS kernel.
      MIPS: microMIPS: Disable LL/SC and fix linker bug.
      MIPS: microMIPS: Add vdso support.
      MIPS: microMIPS: Add unaligned access support.
      MIPS: microMIPS: Support handling of delay slots.
      MIPS: microMIPS: Add support for exception handling.
      MIPS: microMIPS: Floating point support.
      MIPS: microMIPS: Fix macro naming in micro-assembler.
      ...

commit 82d45de6554951b6e863697cca95c927fb90724c
Author: Sanjay Lal <sanjayl@kymasys.com>
Date:   Wed Nov 21 18:34:14 2012 -0800

    MIPS: Export symbols used by KVM/MIPS module
    
    Signed-off-by: Sanjay Lal <sanjayl@kymasys.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 66bf4e22d9b9..596620dd7ee2 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -83,6 +83,7 @@ static inline void set_cpu_sibling_map(int cpu)
 }
 
 struct plat_smp_ops *mp_ops;
+EXPORT_SYMBOL(mp_ops);
 
 __cpuinit void register_smp_ops(struct plat_smp_ops *ops)
 {

commit cdbedc61c8d0122ad682815936f0d11df1fe5f57
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:52 2013 +0100

    mips: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Link: http://lkml.kernel.org/r/20130321215234.754954871@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 66bf4e22d9b9..aee04af213c5 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -139,7 +139,7 @@ asmlinkage __cpuinit void start_secondary(void)
 	WARN_ON_ONCE(!irqs_disabled());
 	mp_ops->smp_finish();
 
-	cpu_idle();
+	cpu_startup_entry(CPUHP_ONLINE);
 }
 
 /*

commit 28eb0e46612a08a235c8b103eb2bd6a1aea83210
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Fri Dec 21 14:04:39 2012 -0800

    MIPS: drivers: remove __dev* attributes.
    
    CONFIG_HOTPLUG is going away as an option.  As a result, the __dev*
    markings need to be removed.
    
    This change removes the use of __devinit, __devexit_p, __devinitdata,
    and __devexit from these drivers.
    
    Based on patches originally written by Bill Pemberton, but redone by me
    in order to handle some of the coding style issues better, by hand.
    
    Cc: Bill Pemberton <wfp5p@virginia.edu>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 2e6374a589ec..66bf4e22d9b9 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -188,7 +188,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 }
 
 /* preload SMP state for boot cpu */
-void __devinit smp_prepare_boot_cpu(void)
+void smp_prepare_boot_cpu(void)
 {
 	set_cpu_possible(0, true);
 	set_cpu_online(0, true);

commit 7aa1c8f47e7e792d11f898cbdddaf6fa21ff08cc
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Oct 11 18:14:58 2012 +0200

    MIPS: kdump: Add support
    
    [ralf@linux-mips.org: Original patch by Maxim Uvarov <muvarov@gmail.com>
    with plenty of further shining, polishing, debugging and testing by me.]
    
    Signed-off-by: Maxim Uvarov <muvarov@gmail.com>
    Cc: linux-mips@linux-mips.org
    Cc: kexec@lists.infradead.org
    Cc: horms@verge.net.au
    Patchwork: https://patchwork.linux-mips.org/patch/1025/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 9005bf9fb859..2e6374a589ec 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -386,3 +386,20 @@ void flush_tlb_one(unsigned long vaddr)
 
 EXPORT_SYMBOL(flush_tlb_page);
 EXPORT_SYMBOL(flush_tlb_one);
+
+#if defined(CONFIG_KEXEC)
+void (*dump_ipi_function_ptr)(void *) = NULL;
+void dump_send_ipi(void (*dump_ipi_callback)(void *))
+{
+	int i;
+	int cpu = smp_processor_id();
+
+	dump_ipi_function_ptr = dump_ipi_callback;
+	smp_mb();
+	for_each_online_cpu(i)
+		if (i != cpu)
+			mp_ops->send_ipi_single(i, SMP_DUMP);
+
+}
+EXPORT_SYMBOL(dump_send_ipi);
+#endif

commit cf9bfe55f24973a8f40e2c922a7e82cf09e486fd
Author: Jayachandran C <jchandra@broadcom.com>
Date:   Tue Aug 14 18:56:13 2012 +0530

    MIPS: Synchronize MIPS count one CPU at a time
    
    The current implementation of synchronise_count_{master,slave} blocks
    slave CPUs in early boot until all of them come up. This no longer
    works because blocking a CPU with interrupts off after notifying the
    CPU to be online causes problems with the current kernel.
    
    Specifically, after the workqueue changes
    (commit a08489c569dc1 "Pull workqueue changes from Tejun Heo")
    the CPU_ONLINE notification callback workqueue_cpu_up_callback()
    will hang on wait_for_completion(&idle_rebind.done), if the slave
    CPUs are blocked for synchronize_count_slave().
    
    The changes are to update synchronize_count_{master,slave}() to handle
    one CPU at a time and to call synchronise_count_master() in __cpu_up()
    so that the CPU_ONLINE notification goes out only after the COP0 COUNT
    register is synchronized.
    
    [ralf@linux-mips.org: This matter only to those few platforms which are
    using the cp0 counter as their clocksource which are XLP, XLR and MIPS'
    CMP solution.]
    
    Signed-off-by: Jayachandran C <jchandra@broadcom.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/4216/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 31637d8c8738..9005bf9fb859 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -130,7 +130,7 @@ asmlinkage __cpuinit void start_secondary(void)
 
 	cpu_set(cpu, cpu_callin_map);
 
-	synchronise_count_slave();
+	synchronise_count_slave(cpu);
 
 	/*
 	 * irq will be enabled in ->smp_finish(), enabling it too early
@@ -173,7 +173,6 @@ void smp_send_stop(void)
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	mp_ops->cpus_done();
-	synchronise_count_master();
 }
 
 /* called from main before smp_init() */
@@ -206,6 +205,7 @@ int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	while (!cpu_isset(cpu, cpu_callin_map))
 		udelay(100);
 
+	synchronise_count_master(cpu);
 	return 0;
 }
 

commit 889a4c7b33607bf03c04b8f2d5607fd4274f47f3
Author: Steven J. Hill <sjhill@mips.com>
Date:   Mon Apr 9 10:58:39 2012 -0500

    MIPS: SMTC: Support for Multi-threaded FPUs
    
    Signed-off-by: Steven J. Hill <sjhill@mips.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/3603/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 1268392f1d27..31637d8c8738 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -102,7 +102,9 @@ asmlinkage __cpuinit void start_secondary(void)
 
 #ifdef CONFIG_MIPS_MT_SMTC
 	/* Only do cpu_probe for first TC of CPU */
-	if ((read_c0_tcbind() & TCBIND_CURTC) == 0)
+	if ((read_c0_tcbind() & TCBIND_CURTC) != 0)
+		__cpu_name[smp_processor_id()] = __cpu_name[0];
+	else
 #endif /* CONFIG_MIPS_MT_SMTC */
 	cpu_probe();
 	cpu_report();

commit b789ad63ac46b00f0862bdc23fc95556adc3cf2f
Author: Yong Zhang <yong.zhang@windriver.com>
Date:   Thu Jul 19 09:13:53 2012 +0200

    MIPS: smp: Warn on too early irq enable
    
    Just to catch a potential issue.
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Sergei Shtylyov <sshtylyov@mvista.com>
    Cc: David Daney <david.daney@cavium.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Patchwork: https://patchwork.linux-mips.org/patch/3852/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index eb3e2b112a0d..1268392f1d27 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -130,6 +130,11 @@ asmlinkage __cpuinit void start_secondary(void)
 
 	synchronise_count_slave();
 
+	/*
+	 * irq will be enabled in ->smp_finish(), enabling it too early
+	 * is dangerous.
+	 */
+	WARN_ON_ONCE(!irqs_disabled());
 	mp_ops->smp_finish();
 
 	cpu_idle();

commit b9a09a0660aa9174e489ac531244680971950ef8
Author: Yong Zhang <yong.zhang@windriver.com>
Date:   Thu Jul 19 09:13:53 2012 +0200

    MIPS: call set_cpu_online() on cpu being brought up with irq disabled
    
    To prevent a problem as commit 5fbd036b [sched: Cleanup cpu_active madness]
    and commit 2baab4e9 [sched: Fix select_fallback_rq() vs cpu_active/cpu_online]
    try to resolve, move set_cpu_online() to the brought up CPU and with irq
    disabled.
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Sergei Shtylyov <sshtylyov@mvista.com>
    Cc: David Daney <david.daney@cavium.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Patchwork: https://patchwork.linux-mips.org/patch/3851/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index b181bbf410de..eb3e2b112a0d 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -122,6 +122,8 @@ asmlinkage __cpuinit void start_secondary(void)
 
 	notify_cpu_starting(cpu);
 
+	set_cpu_online(cpu, true);
+
 	set_cpu_sibling_map(cpu);
 
 	cpu_set(cpu, cpu_callin_map);
@@ -197,8 +199,6 @@ int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 	while (!cpu_isset(cpu, cpu_callin_map))
 		udelay(100);
 
-	set_cpu_online(cpu, true);
-
 	return 0;
 }
 

commit 5309bdac7029c7d8659d6653761f3402e17ed1ab
Author: Yong Zhang <yong.zhang@windriver.com>
Date:   Thu Jul 19 09:13:53 2012 +0200

    MIPS: call ->smp_finish() a little late
    
    We have move irq enable to ->smp_finish. Place ->smp_finish() a little
    late to prepare for move set_cpu_online() into start_secondary.
    And it's not necessary to call cpu_set(cpu, cpu_callin_map) and
    synchronise_count_slave() with irq enabled.
    
    Signed-off-by: Yong Zhang <yong.zhang0@gmail.com>
    Cc: Sergei Shtylyov <sshtylyov@mvista.com>
    Cc: David Daney <david.daney@cavium.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Patchwork: https://patchwork.linux-mips.org/patch/3850/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 48650c818040..b181bbf410de 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -122,13 +122,14 @@ asmlinkage __cpuinit void start_secondary(void)
 
 	notify_cpu_starting(cpu);
 
-	mp_ops->smp_finish();
 	set_cpu_sibling_map(cpu);
 
 	cpu_set(cpu, cpu_callin_map);
 
 	synchronise_count_slave();
 
+	mp_ops->smp_finish();
+
 	cpu_idle();
 }
 

commit 7e5b2db77b05746613516599c916a8cc2e321077
Merge: 227d1e4319ff c819baf31f5f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 29 18:27:19 2012 -0700

    Merge branch 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus
    
    Pull MIPS updates from Ralf Baechle:
     "The whole series has been sitting in -next for quite a while with no
      complaints.  The last change to the series was before the weekend the
      removal of an SPI patch which Grant - even though previously acked by
      himself - appeared to raise objections.  So I removed it until the
      situation is clarified.  Other than that all the patches have the acks
      from their respective maintainers, all MIPS and x86 defconfigs are
      building fine and I'm not aware of any problems introduced by this
      series.
    
      Among the key features for this patch series is a sizable patchset for
      Lantiq which among other things introduces support for Lantiq's
      flagship product, the FALCON SOC.  It also means that the opensource
      developers behind this patchset have overtaken Lantiq's competing
      inhouse development team that was working behind closed doors.
    
      Less noteworthy the ath79 patchset which adds support for a few more
      chip variants, cleanups and fixes.  Finally the usual dose of tweaking
      of generic code."
    
    Fix up trivial conflicts in arch/mips/lantiq/xway/gpio_{ebu,stp}.c where
    printk spelling fixes clashed with file move and eventual removal of the
    printk.
    
    * 'upstream' of git://git.linux-mips.org/pub/scm/ralf/upstream-linus: (81 commits)
      MIPS: lantiq: remove orphaned code
      MIPS: Remove all -Wall and almost all -Werror usage from arch/mips.
      MIPS: lantiq: implement support for FALCON soc
      MTD: MIPS: lantiq: verify that the NOR interface is available on falcon soc
      MTD: MIPS: lantiq: implement OF support
      watchdog: MIPS: lantiq: implement OF support and minor fixes
      SERIAL: MIPS: lantiq: implement OF support
      GPIO: MIPS: lantiq: convert gpio-stp-xway to OF
      GPIO: MIPS: lantiq: convert gpio-mm-lantiq to OF and of_mm_gpio
      GPIO: MIPS: lantiq: move gpio-stp and gpio-ebu to the subsystem folder
      MIPS: pci: convert lantiq driver to OF
      MIPS: lantiq: convert dma to platform driver
      MIPS: lantiq: implement support for clkdev api
      MIPS: lantiq: drop ltq_gpio_request() and gpio_to_irq()
      OF: MIPS: lantiq: implement irq_domain support
      OF: MIPS: lantiq: implement OF support
      MIPS: lantiq: drop mips_machine support
      OF: PCI: const usage needed by MIPS
      MIPS: Cavium: Remove smp_reserve_lock.
      MIPS: Move cache setup to setup_arch().
      ...

commit 6650df3c380e0db558dbfec63ed860402c6afb2a
Author: David Daney <david.daney@cavium.com>
Date:   Tue May 15 00:04:50 2012 -0700

    MIPS: Move cache setup to setup_arch().
    
    commit 97ce2c88f9ad42e3c60a9beb9fca87abf3639faa (jump-label: initialize
    jump-label subsystem much earlier) breaks MIPS.  The jump_label_init()
    call was moved before trap_init() which is where we initialize
    flush_icache_range().
    
    In order to be good citizens, we move cache initialization earlier so
    that we don't jump through a null flush_icache_range function pointer
    when doing the jump label initialization.
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/3822/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index ba9376bf52a1..dc019a1f128d 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -106,7 +106,7 @@ asmlinkage __cpuinit void start_secondary(void)
 #endif /* CONFIG_MIPS_MT_SMTC */
 	cpu_probe();
 	cpu_report();
-	per_cpu_trap_init();
+	per_cpu_trap_init(false);
 	mips_clockevent_init();
 	mp_ops->init_secondary();
 

commit 360014a36170464ebd9935514f0e0e3d558b0e56
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:51 2012 +0000

    mips: Use generic idle thread allocation
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Link: http://lkml.kernel.org/r/20120420124557.512158271@linutronix.de

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 41079b256092..71a95f55a649 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -186,61 +186,9 @@ void __devinit smp_prepare_boot_cpu(void)
 	cpu_set(0, cpu_callin_map);
 }
 
-/*
- * Called once for each "cpu_possible(cpu)".  Needs to spin up the cpu
- * and keep control until "cpu_online(cpu)" is set.  Note: cpu is
- * physical, not logical.
- */
-static struct task_struct *cpu_idle_thread[NR_CPUS];
-
-struct create_idle {
-	struct work_struct work;
-	struct task_struct *idle;
-	struct completion done;
-	int cpu;
-};
-
-static void __cpuinit do_fork_idle(struct work_struct *work)
-{
-	struct create_idle *c_idle =
-		container_of(work, struct create_idle, work);
-
-	c_idle->idle = fork_idle(c_idle->cpu);
-	complete(&c_idle->done);
-}
-
 int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
-	struct task_struct *idle;
-
-	/*
-	 * Processor goes to start_secondary(), sets online flag
-	 * The following code is purely to make sure
-	 * Linux can schedule processes on this slave.
-	 */
-	if (!cpu_idle_thread[cpu]) {
-		/*
-		 * Schedule work item to avoid forking user task
-		 * Ported from arch/x86/kernel/smpboot.c
-		 */
-		struct create_idle c_idle = {
-			.cpu    = cpu,
-			.done   = COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
-		};
-
-		INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
-		schedule_work(&c_idle.work);
-		wait_for_completion(&c_idle.done);
-		idle = cpu_idle_thread[cpu] = c_idle.idle;
-
-		if (IS_ERR(idle))
-			panic(KERN_ERR "Fork failed for CPU %d", cpu);
-	} else {
-		idle = cpu_idle_thread[cpu];
-		init_idle(idle, cpu);
-	}
-
-	mp_ops->boot_secondary(cpu, idle);
+	mp_ops->boot_secondary(cpu, tidle);
 
 	/*
 	 * Trust is futile.  We should really have timeouts ...

commit 8239c25f47d2b318156993b15f33900a86ea5e17
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 20 13:05:42 2012 +0000

    smp: Add task_struct argument to __cpu_up()
    
    Preparatory patch to make the idle thread allocation for secondary
    cpus generic.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20120420124556.964170564@linutronix.de

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index ba9376bf52a1..41079b256092 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -209,7 +209,7 @@ static void __cpuinit do_fork_idle(struct work_struct *work)
 	complete(&c_idle->done);
 }
 
-int __cpuinit __cpu_up(unsigned int cpu)
+int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *tidle)
 {
 	struct task_struct *idle;
 

commit 0b5f9c005def154f9c21f9be0223b65b50d54368
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Mar 29 15:38:30 2012 +1030

    remove references to cpu_*_map in arch/
    
    This has been obsolescent for a while; time for the final push.
    
    In adjacent context, replaced old cpus_* with cpumask_*.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Acked-by: David S. Miller <davem@davemloft.net> (arch/sparc)
    Acked-by: Chris Metcalf <cmetcalf@tilera.com> (arch/tile)
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: linux-hexagon@vger.kernel.org
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: linux-mips@linux-mips.org
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Helge Deller <deller@gmx.de>
    Cc: sparclinux@vger.kernel.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 9c1cce9de35f..ba9376bf52a1 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -148,7 +148,7 @@ static void stop_this_cpu(void *dummy)
 	/*
 	 * Remove this CPU:
 	 */
-	cpu_clear(smp_processor_id(), cpu_online_map);
+	set_cpu_online(smp_processor_id(), false);
 	for (;;) {
 		if (cpu_wait)
 			(*cpu_wait)();		/* Wait if available. */
@@ -174,7 +174,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	mp_ops->prepare_cpus(max_cpus);
 	set_cpu_sibling_map(0);
 #ifndef CONFIG_HOTPLUG_CPU
-	init_cpu_present(&cpu_possible_map);
+	init_cpu_present(cpu_possible_mask);
 #endif
 }
 
@@ -248,7 +248,7 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	while (!cpu_isset(cpu, cpu_callin_map))
 		udelay(100);
 
-	cpu_set(cpu, cpu_online_map);
+	set_cpu_online(cpu, true);
 
 	return 0;
 }
@@ -320,13 +320,12 @@ void flush_tlb_mm(struct mm_struct *mm)
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		smp_on_other_tlbs(flush_tlb_mm_ipi, mm);
 	} else {
-		cpumask_t mask = cpu_online_map;
 		unsigned int cpu;
 
-		cpu_clear(smp_processor_id(), mask);
-		for_each_cpu_mask(cpu, mask)
-			if (cpu_context(cpu, mm))
+		for_each_online_cpu(cpu) {
+			if (cpu != smp_processor_id() && cpu_context(cpu, mm))
 				cpu_context(cpu, mm) = 0;
+		}
 	}
 	local_flush_tlb_mm(mm);
 
@@ -360,13 +359,12 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 
 		smp_on_other_tlbs(flush_tlb_range_ipi, &fd);
 	} else {
-		cpumask_t mask = cpu_online_map;
 		unsigned int cpu;
 
-		cpu_clear(smp_processor_id(), mask);
-		for_each_cpu_mask(cpu, mask)
-			if (cpu_context(cpu, mm))
+		for_each_online_cpu(cpu) {
+			if (cpu != smp_processor_id() && cpu_context(cpu, mm))
 				cpu_context(cpu, mm) = 0;
+		}
 	}
 	local_flush_tlb_range(vma, start, end);
 	preempt_enable();
@@ -407,13 +405,12 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 
 		smp_on_other_tlbs(flush_tlb_page_ipi, &fd);
 	} else {
-		cpumask_t mask = cpu_online_map;
 		unsigned int cpu;
 
-		cpu_clear(smp_processor_id(), mask);
-		for_each_cpu_mask(cpu, mask)
-			if (cpu_context(cpu, vma->vm_mm))
+		for_each_online_cpu(cpu) {
+			if (cpu != smp_processor_id() && cpu_context(cpu, vma->vm_mm))
 				cpu_context(cpu, vma->vm_mm) = 0;
+		}
 	}
 	local_flush_tlb_page(vma, page);
 	preempt_enable();

commit b81947c646bfefdf98e2fde5d7d39cbbda8525d4
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for MIPS
    
    Disintegrate asm/system.h for MIPS.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    cc: linux-mips@linux-mips.org

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 32c1e954cd37..9c1cce9de35f 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -38,9 +38,9 @@
 #include <asm/cpu.h>
 #include <asm/processor.h>
 #include <asm/r4k-timer.h>
-#include <asm/system.h>
 #include <asm/mmu_context.h>
 #include <asm/time.h>
+#include <asm/setup.h>
 
 #ifdef CONFIG_MIPS_MT_SMTC
 #include <asm/mipsmtregs.h>

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 32a256101082..32c1e954cd37 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -34,7 +34,7 @@
 #include <linux/err.h>
 #include <linux/ftrace.h>
 
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <asm/cpu.h>
 #include <asm/processor.h>
 #include <asm/r4k-timer.h>

commit 6667deb69ee3b8a31ea88e1303cf3ad7d4f221da
Author: Maksim Rayskiy <mrayskiy@broadcom.com>
Date:   Sat Feb 12 10:21:32 2011 -0800

    MIPS: Move idle task creation to work queue
    
    To avoid forking usermode thread when creating an idle task, move fork_idle
    to a work queue.
    
    If kernel starts with maxcpus= option which does not bring all available
    cpus online at boot time, idle tasks for offline cpus are not created. If
    later offline cpus are hotplugged through sysfs, __cpu_up is called in
    the context of the user task, and fork_idle copies its non-zero mm
    pointer.  This causes BUG() in per_cpu_trap_init.
    
    This also avoids issues with resource limits of the CPU writing to sysfs,
    containers, maybe others.
    
    Signed-off-by: Maksim Rayskiy <mrayskiy@broadcom.com>
    To: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/2070/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 383aeb95cb49..32a256101082 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -193,6 +193,22 @@ void __devinit smp_prepare_boot_cpu(void)
  */
 static struct task_struct *cpu_idle_thread[NR_CPUS];
 
+struct create_idle {
+	struct work_struct work;
+	struct task_struct *idle;
+	struct completion done;
+	int cpu;
+};
+
+static void __cpuinit do_fork_idle(struct work_struct *work)
+{
+	struct create_idle *c_idle =
+		container_of(work, struct create_idle, work);
+
+	c_idle->idle = fork_idle(c_idle->cpu);
+	complete(&c_idle->done);
+}
+
 int __cpuinit __cpu_up(unsigned int cpu)
 {
 	struct task_struct *idle;
@@ -203,8 +219,19 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	 * Linux can schedule processes on this slave.
 	 */
 	if (!cpu_idle_thread[cpu]) {
-		idle = fork_idle(cpu);
-		cpu_idle_thread[cpu] = idle;
+		/*
+		 * Schedule work item to avoid forking user task
+		 * Ported from arch/x86/kernel/smpboot.c
+		 */
+		struct create_idle c_idle = {
+			.cpu    = cpu,
+			.done   = COMPLETION_INITIALIZER_ONSTACK(c_idle.done),
+		};
+
+		INIT_WORK_ONSTACK(&c_idle.work, do_fork_idle);
+		schedule_work(&c_idle.work);
+		wait_for_completion(&c_idle.done);
+		idle = cpu_idle_thread[cpu] = c_idle.idle;
 
 		if (IS_ERR(idle))
 			panic(KERN_ERR "Fork failed for CPU %d", cpu);

commit 2dc2ae344e0e655fb4296266036d2ac5064634ae
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Fri Jul 23 18:41:45 2010 -0700

    MIPS: Export __cpu_number_map and __cpu_logical_map.
    
    The forthcoming Octeon watchdog driver will use them.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    To: wim@iguana.be
    Cc: linux-kernel@vger.kernel.org
    Patchwork: https://patchwork.linux-mips.org/patch/1499/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 6cdca1956b77..383aeb95cb49 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -47,8 +47,12 @@
 #endif /* CONFIG_MIPS_MT_SMTC */
 
 volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
+
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
+EXPORT_SYMBOL(__cpu_number_map);
+
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
+EXPORT_SYMBOL(__cpu_logical_map);
 
 /* Number of TCs (or siblings in Intel speak) per CPU core */
 int smp_num_siblings = 1;

commit 8f99a162653531ef25a3dd0f92bfb6332cd2b295
Author: Wu Zhangjin <wuzhangjin@gmail.com>
Date:   Fri Nov 20 20:34:33 2009 +0800

    MIPS: Tracing: Add IRQENTRY_EXIT section for MIPS
    
    This patch add a new section for MIPS to record the block of the hardirq
    handling for function graph tracer(print_graph_irq) via adding the
    __irq_entry annotation to the the entrypoints of the hardirqs(the block
    with irq_enter()...irq_exit()).
    
    Thanks goes to Steven & Frederic Weisbecker for their feedbacks.
    
    Signed-off-by: Wu Zhangjin <wuzhangjin@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Nicholas Mc Guire <der.herr@hofr.at>
    Cc: zhangfx@lemote.com
    Cc: Wu Zhangjin <wuzhangjin@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Reviewed-by: Frederic Weisbecker <fweisbec@gmail.com>
    Patchwork: http://patchwork.linux-mips.org/patch/676/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index e72e6844d134..6cdca1956b77 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -32,6 +32,7 @@
 #include <linux/cpumask.h>
 #include <linux/cpu.h>
 #include <linux/err.h>
+#include <linux/ftrace.h>
 
 #include <asm/atomic.h>
 #include <asm/cpu.h>
@@ -130,7 +131,7 @@ asmlinkage __cpuinit void start_secondary(void)
 /*
  * Call into both interrupt handlers, as we share the IPI for them
  */
-void smp_call_function_interrupt(void)
+void __irq_entry smp_call_function_interrupt(void)
 {
 	irq_enter();
 	generic_smp_call_function_single_interrupt();

commit eef34ec514054e4685745236dd5c9658f7aca69e
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Sep 25 15:35:28 2009 +0100

    MIPS: SMP: Inline arch_send_call_function_{single_ipi,ipi_mask}
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index d74b4dd2dfb2..e72e6844d134 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -127,19 +127,6 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_idle();
 }
 
-void arch_send_call_function_ipi_mask(const struct cpumask *mask)
-{
-	mp_ops->send_ipi_mask(mask, SMP_CALL_FUNCTION);
-}
-
-/*
- * We reuse the same vector for the single IPI
- */
-void arch_send_call_function_single_ipi(int cpu)
-{
-	mp_ops->send_ipi_mask(&cpumask_of_cpu(cpu), SMP_CALL_FUNCTION);
-}
-
 /*
  * Call into both interrupt handlers, as we share the IPI for them
  */

commit b533e652df3bbb8f0ea63dc22054edf4cd4155df
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Sep 25 15:08:01 2009 +0100

    MIPS: SMP: Fix build.
    
    commit 48a048fed82a8e5fdd8618574f6d3de1a0d67a50
    Author: Rusty Russell <rusty@rustcorp.com.au>
    Date:   Thu Sep 24 09:34:44 2009 -0600
    
    apparently only passed the "looks good" level of QA ;-)
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 09e8dcf72835..d74b4dd2dfb2 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -137,7 +137,7 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
  */
 void arch_send_call_function_single_ipi(int cpu)
 {
-	mp_ops->send_ipi_mask(cpumask_of_cpu(cpu), SMP_CALL_FUNCTION);
+	mp_ops->send_ipi_mask(&cpumask_of_cpu(cpu), SMP_CALL_FUNCTION);
 }
 
 /*

commit 742db5d1085df97354ce65daf9b6b657696d5268
Author: Huang Weiyi <weiyi.huang@gmail.com>
Date:   Fri Sep 18 15:14:35 2009 +0800

    MIPS: Remove duplicated #include
    
    Remove duplicated #include in arch/mips/kernel/smp.c.
    
    Signed-off-by: Huang Weiyi <weiyi.huang@gmail.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 4eb106c6a3ec..09e8dcf72835 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -32,7 +32,6 @@
 #include <linux/cpumask.h>
 #include <linux/cpu.h>
 #include <linux/err.h>
-#include <linux/smp.h>
 
 #include <asm/atomic.h>
 #include <asm/cpu.h>

commit 4037ac6e2cb4e3148c25124b431eead4e704a4ff
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:47 2009 -0600

    cpumask: Use accessors for cpu_*_mask: mips
    
    Use the accessors rather than frobbing bits directly (the new versions
    are const).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Mike Travis <travis@sgi.com>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index df2ace9558b9..4eb106c6a3ec 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -183,15 +183,15 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	mp_ops->prepare_cpus(max_cpus);
 	set_cpu_sibling_map(0);
 #ifndef CONFIG_HOTPLUG_CPU
-	cpu_present_map = cpu_possible_map;
+	init_cpu_present(&cpu_possible_map);
 #endif
 }
 
 /* preload SMP state for boot cpu */
 void __devinit smp_prepare_boot_cpu(void)
 {
-	cpu_set(0, cpu_possible_map);
-	cpu_set(0, cpu_online_map);
+	set_cpu_possible(0, true);
+	set_cpu_online(0, true);
 	cpu_set(0, cpu_callin_map);
 }
 

commit 48a048fed82a8e5fdd8618574f6d3de1a0d67a50
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:44 2009 -0600

    cpumask: arch_send_call_function_ipi_mask: mips
    
    We're weaning the core code off handing cpumask's around on-stack.
    This introduces arch_send_call_function_ipi_mask(), and by defining
    it, the old arch_send_call_function_ipi is defined by the core code.
    
    We also take the chance to wean the implementations off the
    obsolescent for_each_cpu_mask(): making send_ipi_mask take the pointer
    seemed the most natural way to ensure all implementations used
    for_each_cpu.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 64668a93248b..df2ace9558b9 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -128,7 +128,7 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_idle();
 }
 
-void arch_send_call_function_ipi(cpumask_t mask)
+void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 {
 	mp_ops->send_ipi_mask(mask, SMP_CALL_FUNCTION);
 }

commit 7e17615c45980fc34d3f7d04bc7063cfc32180ec
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Sep 15 13:36:13 2009 +0200

    MIPS: Get rid of duplicate cpu_idle() prototype.
    
    Since 2.6.11-rc1 there is a prototype in <linux/smp.h>.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index bc7d9b05e2f4..64668a93248b 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -32,6 +32,7 @@
 #include <linux/cpumask.h>
 #include <linux/cpu.h>
 #include <linux/err.h>
+#include <linux/smp.h>
 
 #include <asm/atomic.h>
 #include <asm/cpu.h>
@@ -49,8 +50,6 @@ volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 
-extern void cpu_idle(void);
-
 /* Number of TCs (or siblings in Intel speak) per CPU core */
 int smp_num_siblings = 1;
 EXPORT_SYMBOL(smp_num_siblings);

commit 1b2bc75c1bde6581d2694cb3ed7fb06b69685008
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Jun 23 10:00:31 2009 +0100

    MIPS: Add arch generic CPU hotplug
    
    Each platform has to add support for CPU hotplugging itself by providing
    suitable definitions for the cpu_disable and cpu_die of the smp_ops
    methods and setting SYS_SUPPORTS_HOTPLUG_CPU.  A platform should only set
    SYS_SUPPORTS_HOTPLUG_CPU once all it's smp_ops definitions have the
    necessary changes.  This patch contains the changes to the dummy smp_ops
    definition for uni-processor systems.
    
    Parts of the code contributed by Cavium Inc.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 58f4679bbd43..bc7d9b05e2f4 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -45,7 +45,7 @@
 #include <asm/mipsmtregs.h>
 #endif /* CONFIG_MIPS_MT_SMTC */
 
-static volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
+volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 
@@ -201,6 +201,8 @@ void __devinit smp_prepare_boot_cpu(void)
  * and keep control until "cpu_online(cpu)" is set.  Note: cpu is
  * physical, not logical.
  */
+static struct task_struct *cpu_idle_thread[NR_CPUS];
+
 int __cpuinit __cpu_up(unsigned int cpu)
 {
 	struct task_struct *idle;
@@ -210,9 +212,16 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	 * The following code is purely to make sure
 	 * Linux can schedule processes on this slave.
 	 */
-	idle = fork_idle(cpu);
-	if (IS_ERR(idle))
-		panic(KERN_ERR "Fork failed for CPU %d", cpu);
+	if (!cpu_idle_thread[cpu]) {
+		idle = fork_idle(cpu);
+		cpu_idle_thread[cpu] = idle;
+
+		if (IS_ERR(idle))
+			panic(KERN_ERR "Fork failed for CPU %d", cpu);
+	} else {
+		idle = cpu_idle_thread[cpu];
+		init_idle(idle, cpu);
+	}
 
 	mp_ops->boot_secondary(cpu, idle);
 

commit 631330f5847b3f8a7ea67d689e9f7c56833ccaa6
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Jun 19 14:05:26 2009 +0100

    MIPS: Build fix - include <linux/smp.h> into all smp_processor_id() users.
    
    Some of the were relying into smp.h being dragged in by another header
    which of course is fragile.  <asm/cpu-info.h> uses smp_processor_id()
    only in macros and including smp.h there leads to an include loop, so
    don't change cpu-info.h.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index c937506a03aa..58f4679bbd43 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -22,6 +22,7 @@
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/interrupt.h>
+#include <linux/smp.h>
 #include <linux/spinlock.h>
 #include <linux/threads.h>
 #include <linux/module.h>

commit 76544504aebc606b8279a5314595af5d568e7fea
Author: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
Date:   Mon Mar 23 00:12:29 2009 +0200

    MIPS: Make a needlessly global symbol static in arch/mips/kernel/smp.c
    
    The variable cpu_callin_map is needlessly defined global, so let's
    make it static now.
    
    Build-tested using malta_defconfig.
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 3da94704f816..c937506a03aa 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -44,7 +44,7 @@
 #include <asm/mipsmtregs.h>
 #endif /* CONFIG_MIPS_MT_SMTC */
 
-volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
+static volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 

commit 98a79d6a50181ca1ecf7400eda01d5dc1bc0dbf0
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Sat Dec 13 21:19:41 2008 +1030

    cpumask: centralize cpu_online_map and cpu_possible_map
    
    Impact: cleanup
    
    Each SMP arch defines these themselves.  Move them to a central
    location.
    
    Twists:
    1) Some archs (m32, parisc, s390) set possible_map to all 1, so we add a
       CONFIG_INIT_ALL_POSSIBLE for this rather than break them.
    
    2) mips and sparc32 '#define cpu_possible_map phys_cpu_present_map'.
       Those archs simply have phys_cpu_present_map replaced everywhere.
    
    3) Alpha defined cpu_possible_map to cpu_present_map; this is tricky
       so I just manipulate them both in sync.
    
    4) IA64, cris and m32r have gratuitous 'extern cpumask_t cpu_possible_map'
       declarations.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Grant Grundler <grundler@parisc-linux.org>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Mike Travis <travis@sgi.com>
    Cc: ink@jurassic.park.msu.ru
    Cc: rmk@arm.linux.org.uk
    Cc: starvik@axis.com
    Cc: tony.luck@intel.com
    Cc: takata@linux-m32r.org
    Cc: ralf@linux-mips.org
    Cc: grundler@parisc-linux.org
    Cc: paulus@samba.org
    Cc: schwidefsky@de.ibm.com
    Cc: lethal@linux-sh.org
    Cc: wli@holomorphy.com
    Cc: davem@davemloft.net
    Cc: jdike@addtoit.com
    Cc: mingo@redhat.com

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 8bf88faf5afd..3da94704f816 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -44,15 +44,10 @@
 #include <asm/mipsmtregs.h>
 #endif /* CONFIG_MIPS_MT_SMTC */
 
-cpumask_t phys_cpu_present_map;		/* Bitmask of available CPUs */
 volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
-cpumask_t cpu_online_map;		/* Bitmask of currently online CPUs */
 int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
 int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 
-EXPORT_SYMBOL(phys_cpu_present_map);
-EXPORT_SYMBOL(cpu_online_map);
-
 extern void cpu_idle(void);
 
 /* Number of TCs (or siblings in Intel speak) per CPU core */
@@ -195,7 +190,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 /* preload SMP state for boot cpu */
 void __devinit smp_prepare_boot_cpu(void)
 {
-	cpu_set(0, phys_cpu_present_map);
+	cpu_set(0, cpu_possible_map);
 	cpu_set(0, cpu_online_map);
 	cpu_set(0, cpu_callin_map);
 }

commit 076c6e4f4d81113615f50e5bc2c569f628bcd54a
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Oct 28 10:03:57 2008 +0000

    MIPS: SMP: Do not initialize __cpu_number_map/__cpu_logical_map for CPU 0.
    
    A system isn't necessarily booted on physical processor 0 as this code
    assumes.  Also the array happens to be allocated in .bss so it's zero
    initialized anyway.  Systems which need to override this can do so in
    their mp_ops->smp_setup() method.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index b79ea7055ec3..8bf88faf5afd 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -195,12 +195,6 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 /* preload SMP state for boot cpu */
 void __devinit smp_prepare_boot_cpu(void)
 {
-	/*
-	 * This assumes that bootup is always handled by the processor
-	 * with the logic and physical number 0.
-	 */
-	__cpu_number_map[0] = 0;
-	__cpu_logical_map[0] = 0;
 	cpu_set(0, phys_cpu_present_map);
 	cpu_set(0, cpu_online_map);
 	cpu_set(0, cpu_callin_map);

commit 7920c4d658ff2b0f7b8acf3bd7c700875c2bc163
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Oct 18 13:23:10 2008 +0100

    MIPS: SMP: Don't reenable interrupts in stop_this_cpu; use WAIT instruction.
    
    Noticed by Anirban Sinha <ASinha@zeugmasystems.com>; patch by me.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 7b59cfb7e602..b79ea7055ec3 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -163,8 +163,10 @@ static void stop_this_cpu(void *dummy)
 	 * Remove this CPU:
 	 */
 	cpu_clear(smp_processor_id(), cpu_online_map);
-	local_irq_enable();	/* May need to service _machine_restart IPI */
-	for (;;);		/* Wait if available. */
+	for (;;) {
+		if (cpu_wait)
+			(*cpu_wait)();		/* Wait if available. */
+	}
 }
 
 void smp_send_stop(void)

commit e545a6140b698b2494daf0b32107bdcc5e901390
Author: Manfred Spraul <manfred@colorfullife.com>
Date:   Sun Sep 7 16:57:22 2008 +0200

    kernel/cpu.c: create a CPU_STARTING cpu_chain notifier
    
    Right now, there is no notifier that is called on a new cpu, before the new
    cpu begins processing interrupts/softirqs.
    Various kernel function would need that notification, e.g. kvm works around
    by calling smp_call_function_single(), rcu polls cpu_online_map.
    
    The patch adds a CPU_STARTING notification. It also adds a helper function
    that sends the message to all cpu_chain handlers.
    
    Tested on x86-64.
    All other archs are untested. Especially on sparc, I'm not sure if I got
    it right.
    
    Signed-off-by: Manfred Spraul <manfred@colorfullife.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 4410f172b8ab..7b59cfb7e602 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -121,6 +121,8 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu = smp_processor_id();
 	cpu_data[cpu].udelay_val = loops_per_jiffy;
 
+	notify_cpu_starting(cpu);
+
 	mp_ops->smp_finish();
 	set_cpu_sibling_map(cpu);
 

commit 15c8b6c1aaaf1c4edd67e2f02e4d8e1bd1a51c0d
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri May 9 09:39:44 2008 +0200

    on_each_cpu(): kill unused 'retry' parameter
    
    It's not even passed on to smp_call_function() anymore, since that
    was removed. So kill it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 7a9ae830be86..4410f172b8ab 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -246,7 +246,7 @@ static void flush_tlb_all_ipi(void *info)
 
 void flush_tlb_all(void)
 {
-	on_each_cpu(flush_tlb_all_ipi, NULL, 1, 1);
+	on_each_cpu(flush_tlb_all_ipi, NULL, 1);
 }
 
 static void flush_tlb_mm_ipi(void *mm)
@@ -366,7 +366,7 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 		.addr2 = end,
 	};
 
-	on_each_cpu(flush_tlb_kernel_range_ipi, &fd, 1, 1);
+	on_each_cpu(flush_tlb_kernel_range_ipi, &fd, 1);
 }
 
 static void flush_tlb_page_ipi(void *info)

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index c75b26cb61df..7a9ae830be86 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -167,7 +167,7 @@ static void stop_this_cpu(void *dummy)
 
 void smp_send_stop(void)
 {
-	smp_call_function(stop_this_cpu, NULL, 1, 0);
+	smp_call_function(stop_this_cpu, NULL, 0);
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)
@@ -266,7 +266,7 @@ static void flush_tlb_mm_ipi(void *mm)
 static inline void smp_on_other_tlbs(void (*func) (void *info), void *info)
 {
 #ifndef CONFIG_MIPS_MT_SMTC
-	smp_call_function(func, info, 1, 1);
+	smp_call_function(func, info, 1);
 #endif
 }
 

commit 2f304c0a0a55072b80957580f1b66256a615d8da
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Tue Jun 17 10:45:23 2008 +0200

    mips: convert to generic helpers for IPI function calls
    
    This converts mips to use the new helpers for smp_call_function() and
    friends, and adds support for smp_call_function_single(). Not tested,
    but it compiles.
    
    mips shares the same IPI for smp_call_function() and
    smp_call_function_single(), since not all mips platforms have enough
    available IPIs to support seperate setups.
    
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index cdf87a9dd4ba..c75b26cb61df 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -131,148 +131,29 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_idle();
 }
 
-DEFINE_SPINLOCK(smp_call_lock);
-
-struct call_data_struct *call_data;
-
-/*
- * Run a function on all other CPUs.
- *
- *  <mask>	cpuset_t of all processors to run the function on.
- *  <func>      The function to run. This must be fast and non-blocking.
- *  <info>      An arbitrary pointer to pass to the function.
- *  <retry>     If true, keep retrying until ready.
- *  <wait>      If true, wait until function has completed on other CPUs.
- *  [RETURNS]   0 on success, else a negative status code.
- *
- * Does not return until remote CPUs are nearly ready to execute <func>
- * or are or have executed.
- *
- * You must not call this function with disabled interrupts or from a
- * hardware interrupt handler or from a bottom half handler:
- *
- * CPU A                               CPU B
- * Disable interrupts
- *                                     smp_call_function()
- *                                     Take call_lock
- *                                     Send IPIs
- *                                     Wait for all cpus to acknowledge IPI
- *                                     CPU A has not responded, spin waiting
- *                                     for cpu A to respond, holding call_lock
- * smp_call_function()
- * Spin waiting for call_lock
- * Deadlock                            Deadlock
- */
-int smp_call_function_mask(cpumask_t mask, void (*func) (void *info),
-	void *info, int retry, int wait)
+void arch_send_call_function_ipi(cpumask_t mask)
 {
-	struct call_data_struct data;
-	int cpu = smp_processor_id();
-	int cpus;
-
-	/*
-	 * Can die spectacularly if this CPU isn't yet marked online
-	 */
-	BUG_ON(!cpu_online(cpu));
-
-	cpu_clear(cpu, mask);
-	cpus = cpus_weight(mask);
-	if (!cpus)
-		return 0;
-
-	/* Can deadlock when called with interrupts disabled */
-	WARN_ON(irqs_disabled());
-
-	data.func = func;
-	data.info = info;
-	atomic_set(&data.started, 0);
-	data.wait = wait;
-	if (wait)
-		atomic_set(&data.finished, 0);
-
-	spin_lock(&smp_call_lock);
-	call_data = &data;
-	smp_mb();
-
-	/* Send a message to all other CPUs and wait for them to respond */
 	mp_ops->send_ipi_mask(mask, SMP_CALL_FUNCTION);
-
-	/* Wait for response */
-	/* FIXME: lock-up detection, backtrace on lock-up */
-	while (atomic_read(&data.started) != cpus)
-		barrier();
-
-	if (wait)
-		while (atomic_read(&data.finished) != cpus)
-			barrier();
-	call_data = NULL;
-	spin_unlock(&smp_call_lock);
-
-	return 0;
 }
 
-int smp_call_function(void (*func) (void *info), void *info, int retry,
-	int wait)
+/*
+ * We reuse the same vector for the single IPI
+ */
+void arch_send_call_function_single_ipi(int cpu)
 {
-	return smp_call_function_mask(cpu_online_map, func, info, retry, wait);
+	mp_ops->send_ipi_mask(cpumask_of_cpu(cpu), SMP_CALL_FUNCTION);
 }
-EXPORT_SYMBOL(smp_call_function);
 
+/*
+ * Call into both interrupt handlers, as we share the IPI for them
+ */
 void smp_call_function_interrupt(void)
 {
-	void (*func) (void *info) = call_data->func;
-	void *info = call_data->info;
-	int wait = call_data->wait;
-
-	/*
-	 * Notify initiating CPU that I've grabbed the data and am
-	 * about to execute the function.
-	 */
-	smp_mb();
-	atomic_inc(&call_data->started);
-
-	/*
-	 * At this point the info structure may be out of scope unless wait==1.
-	 */
 	irq_enter();
-	(*func)(info);
+	generic_smp_call_function_single_interrupt();
+	generic_smp_call_function_interrupt();
 	irq_exit();
-
-	if (wait) {
-		smp_mb();
-		atomic_inc(&call_data->finished);
-	}
-}
-
-int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
-			     int retry, int wait)
-{
-	int ret, me;
-
-	/*
-	 * Can die spectacularly if this CPU isn't yet marked online
-	 */
-	if (!cpu_online(cpu))
-		return 0;
-
-	me = get_cpu();
-	BUG_ON(!cpu_online(me));
-
-	if (cpu == me) {
-		local_irq_disable();
-		func(info);
-		local_irq_enable();
-		put_cpu();
-		return 0;
-	}
-
-	ret = smp_call_function_mask(cpumask_of_cpu(cpu), func, info, retry,
-				     wait);
-
-	put_cpu();
-	return 0;
 }
-EXPORT_SYMBOL(smp_call_function_single);
 
 static void stop_this_cpu(void *dummy)
 {

commit a9ad02bdbb0193203a477bbd0e833adf9fb29ac4
Author: Zenon Fortuna <zenon@mips.com>
Date:   Fri May 16 17:29:48 2008 -0700

    [MIPS] Export smp_call_function and smp_call_function_single.
    
    Signed-off-by: Chris Dearman <chris@mips.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 63370cdd3c90..cdf87a9dd4ba 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -216,6 +216,7 @@ int smp_call_function(void (*func) (void *info), void *info, int retry,
 {
 	return smp_call_function_mask(cpu_online_map, func, info, retry, wait);
 }
+EXPORT_SYMBOL(smp_call_function);
 
 void smp_call_function_interrupt(void)
 {
@@ -271,6 +272,7 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
 	put_cpu();
 	return 0;
 }
+EXPORT_SYMBOL(smp_call_function_single);
 
 static void stop_this_cpu(void *dummy)
 {

commit 83738e307365aa2de4a1be65ed574aaebce52ea0
Author: Thiemo Seufer <ths@networkno.de>
Date:   Tue May 6 11:21:22 2008 +0100

    [MIPS] fix warning message on SMP kernels
    
    This patch fixes a (harmless) warning message.
    
    Signed-off-by: Thiemo Seufer <ths@networkno.de>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 33780cc61ce9..63370cdd3c90 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -87,8 +87,8 @@ struct plat_smp_ops *mp_ops;
 
 __cpuinit void register_smp_ops(struct plat_smp_ops *ops)
 {
-	if (ops)
-		printk(KERN_WARNING "Overriding previous set SMP ops\n");
+	if (mp_ops)
+		printk(KERN_WARNING "Overriding previously set SMP ops\n");
 
 	mp_ops = ops;
 }

commit 39b8d5254246ac56342b72f812255c8f7a74dca9
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Apr 28 17:14:26 2008 +0100

    [MIPS] Add support for MIPS CMP platform.
    
    Signed-off-by: Chris Dearman <chris@mips.com>
    Signed-off-by: Atsushi Nemoto <anemo@mba.ocn.ne.jp>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 9d41dab90a80..33780cc61ce9 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -35,6 +35,7 @@
 #include <asm/atomic.h>
 #include <asm/cpu.h>
 #include <asm/processor.h>
+#include <asm/r4k-timer.h>
 #include <asm/system.h>
 #include <asm/mmu_context.h>
 #include <asm/time.h>
@@ -125,6 +126,8 @@ asmlinkage __cpuinit void start_secondary(void)
 
 	cpu_set(cpu, cpu_callin_map);
 
+	synchronise_count_slave();
+
 	cpu_idle();
 }
 
@@ -287,6 +290,7 @@ void smp_send_stop(void)
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	mp_ops->cpus_done();
+	synchronise_count_master();
 }
 
 /* called from main before smp_init() */

commit 6c81c32f9616fd6f2795dceae2f70943cb4d8609
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Feb 6 01:37:51 2008 -0800

    calibrate_delay() must be __cpuinit
    
    calibrate_delay() must be __cpuinit, not __{dev,}init.
    
    I've verified that this is correct for all users.
    
    While doing the latter, I also did the following cleanups:
    - remove pointless additional prototypes in C files
    - ensure all users #include <linux/delay.h>
    
    This fixes the following section mismatches with CONFIG_HOTPLUG=n,
    CONFIG_HOTPLUG_CPU=y:
    
    WARNING: vmlinux.o(.text+0x1128d): Section mismatch: reference to .init.text.1:calibrate_delay (between 'check_cx686_slop' and 'set_cx86_reorder')
    WARNING: vmlinux.o(.text+0x25102): Section mismatch: reference to .init.text.1:calibrate_delay (between 'smp_callin' and 'cpu_coregroup_map')
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Christian Zankel <chris@zankel.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 1e5dfc28294a..9d41dab90a80 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -52,7 +52,6 @@ int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 EXPORT_SYMBOL(phys_cpu_present_map);
 EXPORT_SYMBOL(cpu_online_map);
 
-extern void __init calibrate_delay(void);
 extern void cpu_idle(void);
 
 /* Number of TCs (or siblings in Intel speak) per CPU core */

commit 87353d8ac39c52784da605ecbe965ecdfad609ad
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon Nov 19 12:23:51 2007 +0000

    [MIPS] SMP: Call platform methods via ops structure.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 335be9bcf0dc..1e5dfc28294a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -37,7 +37,6 @@
 #include <asm/processor.h>
 #include <asm/system.h>
 #include <asm/mmu_context.h>
-#include <asm/smp.h>
 #include <asm/time.h>
 
 #ifdef CONFIG_MIPS_MT_SMTC
@@ -84,6 +83,16 @@ static inline void set_cpu_sibling_map(int cpu)
 		cpu_set(cpu, cpu_sibling_map[cpu]);
 }
 
+struct plat_smp_ops *mp_ops;
+
+__cpuinit void register_smp_ops(struct plat_smp_ops *ops)
+{
+	if (ops)
+		printk(KERN_WARNING "Overriding previous set SMP ops\n");
+
+	mp_ops = ops;
+}
+
 /*
  * First C code run on the secondary CPUs after being started up by
  * the master.
@@ -100,7 +109,7 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_report();
 	per_cpu_trap_init();
 	mips_clockevent_init();
-	prom_init_secondary();
+	mp_ops->init_secondary();
 
 	/*
 	 * XXX parity protection should be folded in here when it's converted
@@ -112,7 +121,7 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu = smp_processor_id();
 	cpu_data[cpu].udelay_val = loops_per_jiffy;
 
-	prom_smp_finish();
+	mp_ops->smp_finish();
 	set_cpu_sibling_map(cpu);
 
 	cpu_set(cpu, cpu_callin_map);
@@ -184,7 +193,7 @@ int smp_call_function_mask(cpumask_t mask, void (*func) (void *info),
 	smp_mb();
 
 	/* Send a message to all other CPUs and wait for them to respond */
-	core_send_ipi_mask(mask, SMP_CALL_FUNCTION);
+	mp_ops->send_ipi_mask(mask, SMP_CALL_FUNCTION);
 
 	/* Wait for response */
 	/* FIXME: lock-up detection, backtrace on lock-up */
@@ -278,7 +287,7 @@ void smp_send_stop(void)
 
 void __init smp_cpus_done(unsigned int max_cpus)
 {
-	prom_cpus_done();
+	mp_ops->cpus_done();
 }
 
 /* called from main before smp_init() */
@@ -286,7 +295,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	init_new_context(current, &init_mm);
 	current_thread_info()->cpu = 0;
-	plat_prepare_cpus(max_cpus);
+	mp_ops->prepare_cpus(max_cpus);
 	set_cpu_sibling_map(0);
 #ifndef CONFIG_HOTPLUG_CPU
 	cpu_present_map = cpu_possible_map;
@@ -325,7 +334,7 @@ int __cpuinit __cpu_up(unsigned int cpu)
 	if (IS_ERR(idle))
 		panic(KERN_ERR "Fork failed for CPU %d", cpu);
 
-	prom_boot_secondary(cpu, idle);
+	mp_ops->boot_secondary(cpu, idle);
 
 	/*
 	 * Trust is futile.  We should really have timeouts ...

commit 0ab7aefc4d43a6dee26c891b41ef9c7a67d2379b
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Mar 2 20:42:04 2007 +0000

    [MIPS] MT: Scheduler support for SMT
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 63989e9df4f9..335be9bcf0dc 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -56,6 +56,34 @@ EXPORT_SYMBOL(cpu_online_map);
 extern void __init calibrate_delay(void);
 extern void cpu_idle(void);
 
+/* Number of TCs (or siblings in Intel speak) per CPU core */
+int smp_num_siblings = 1;
+EXPORT_SYMBOL(smp_num_siblings);
+
+/* representing the TCs (or siblings in Intel speak) of each logical CPU */
+cpumask_t cpu_sibling_map[NR_CPUS] __read_mostly;
+EXPORT_SYMBOL(cpu_sibling_map);
+
+/* representing cpus for which sibling maps can be computed */
+static cpumask_t cpu_sibling_setup_map;
+
+static inline void set_cpu_sibling_map(int cpu)
+{
+	int i;
+
+	cpu_set(cpu, cpu_sibling_setup_map);
+
+	if (smp_num_siblings > 1) {
+		for_each_cpu_mask(i, cpu_sibling_setup_map) {
+			if (cpu_data[cpu].core == cpu_data[i].core) {
+				cpu_set(i, cpu_sibling_map[cpu]);
+				cpu_set(cpu, cpu_sibling_map[i]);
+			}
+		}
+	} else
+		cpu_set(cpu, cpu_sibling_map[cpu]);
+}
+
 /*
  * First C code run on the secondary CPUs after being started up by
  * the master.
@@ -85,6 +113,7 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_data[cpu].udelay_val = loops_per_jiffy;
 
 	prom_smp_finish();
+	set_cpu_sibling_map(cpu);
 
 	cpu_set(cpu, cpu_callin_map);
 
@@ -258,6 +287,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	init_new_context(current, &init_mm);
 	current_thread_info()->cpu = 0;
 	plat_prepare_cpus(max_cpus);
+	set_cpu_sibling_map(0);
 #ifndef CONFIG_HOTPLUG_CPU
 	cpu_present_map = cpu_possible_map;
 #endif

commit ece8a9e4f011b038396c7649a8407ca9171be4a9
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Oct 12 15:03:38 2007 +0100

    [MIPS] SMP: Fix use of cpumasks.
    
    Noticed by Nick Piggin <nickpiggin@yahoo.com.au>.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 432f2e376aea..63989e9df4f9 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -379,7 +379,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 		unsigned int cpu;
 
 		cpu_clear(smp_processor_id(), mask);
-		for_each_online_cpu(cpu)
+		for_each_cpu_mask(cpu, mask)
 			if (cpu_context(cpu, mm))
 				cpu_context(cpu, mm) = 0;
 	}
@@ -419,7 +419,7 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 		unsigned int cpu;
 
 		cpu_clear(smp_processor_id(), mask);
-		for_each_online_cpu(cpu)
+		for_each_cpu_mask(cpu, mask)
 			if (cpu_context(cpu, mm))
 				cpu_context(cpu, mm) = 0;
 	}
@@ -466,7 +466,7 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 		unsigned int cpu;
 
 		cpu_clear(smp_processor_id(), mask);
-		for_each_online_cpu(cpu)
+		for_each_cpu_mask(cpu, mask)
 			if (cpu_context(cpu, vma->vm_mm))
 				cpu_context(cpu, vma->vm_mm) = 0;
 	}

commit 89a8a5a6c965d3dd2571a263ef59e7b23a036d5e
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Oct 4 18:18:52 2007 +0100

    [MIPS] SMP: Use ISO C struct initializer for local structs.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 54464fd36198..432f2e376aea 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -407,11 +407,12 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 
 	preempt_disable();
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
-		struct flush_tlb_data fd;
+		struct flush_tlb_data fd = {
+			.vma = vma,
+			.addr1 = start,
+			.addr2 = end,
+		};
 
-		fd.vma = vma;
-		fd.addr1 = start;
-		fd.addr2 = end;
 		smp_on_other_tlbs(flush_tlb_range_ipi, &fd);
 	} else {
 		cpumask_t mask = cpu_online_map;
@@ -435,10 +436,11 @@ static void flush_tlb_kernel_range_ipi(void *info)
 
 void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 {
-	struct flush_tlb_data fd;
+	struct flush_tlb_data fd = {
+		.addr1 = start,
+		.addr2 = end,
+	};
 
-	fd.addr1 = start;
-	fd.addr2 = end;
 	on_each_cpu(flush_tlb_kernel_range_ipi, &fd, 1, 1);
 }
 
@@ -453,10 +455,11 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 {
 	preempt_disable();
 	if ((atomic_read(&vma->vm_mm->mm_users) != 1) || (current->mm != vma->vm_mm)) {
-		struct flush_tlb_data fd;
+		struct flush_tlb_data fd = {
+			.vma = vma,
+			.addr1 = page,
+		};
 
-		fd.vma = vma;
-		fd.addr1 = page;
 		smp_on_other_tlbs(flush_tlb_page_ipi, &fd);
 	} else {
 		cpumask_t mask = cpu_online_map;

commit c50cade95b075e1116fc4fae6a9c63829720efb9
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Oct 4 16:57:08 2007 +0100

    [MIPS] SMP: Kill useless casts.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 5ca3809a1b45..54464fd36198 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -373,7 +373,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 	preempt_disable();
 
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
-		smp_on_other_tlbs(flush_tlb_mm_ipi, (void *)mm);
+		smp_on_other_tlbs(flush_tlb_mm_ipi, mm);
 	} else {
 		cpumask_t mask = cpu_online_map;
 		unsigned int cpu;
@@ -396,7 +396,7 @@ struct flush_tlb_data {
 
 static void flush_tlb_range_ipi(void *info)
 {
-	struct flush_tlb_data *fd = (struct flush_tlb_data *)info;
+	struct flush_tlb_data *fd = info;
 
 	local_flush_tlb_range(fd->vma, fd->addr1, fd->addr2);
 }
@@ -412,7 +412,7 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 		fd.vma = vma;
 		fd.addr1 = start;
 		fd.addr2 = end;
-		smp_on_other_tlbs(flush_tlb_range_ipi, (void *)&fd);
+		smp_on_other_tlbs(flush_tlb_range_ipi, &fd);
 	} else {
 		cpumask_t mask = cpu_online_map;
 		unsigned int cpu;
@@ -428,7 +428,7 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 
 static void flush_tlb_kernel_range_ipi(void *info)
 {
-	struct flush_tlb_data *fd = (struct flush_tlb_data *)info;
+	struct flush_tlb_data *fd = info;
 
 	local_flush_tlb_kernel_range(fd->addr1, fd->addr2);
 }
@@ -439,12 +439,12 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 
 	fd.addr1 = start;
 	fd.addr2 = end;
-	on_each_cpu(flush_tlb_kernel_range_ipi, (void *)&fd, 1, 1);
+	on_each_cpu(flush_tlb_kernel_range_ipi, &fd, 1, 1);
 }
 
 static void flush_tlb_page_ipi(void *info)
 {
-	struct flush_tlb_data *fd = (struct flush_tlb_data *)info;
+	struct flush_tlb_data *fd = info;
 
 	local_flush_tlb_page(fd->vma, fd->addr1);
 }
@@ -457,7 +457,7 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 
 		fd.vma = vma;
 		fd.addr1 = page;
-		smp_on_other_tlbs(flush_tlb_page_ipi, (void *)&fd);
+		smp_on_other_tlbs(flush_tlb_page_ipi, &fd);
 	} else {
 		cpumask_t mask = cpu_online_map;
 		unsigned int cpu;

commit b5eb551145395382ddf302548991a5fbaabc5341
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Oct 3 19:16:57 2007 +0100

    [MIPS] Kill num_online_cpus() loops.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 481ba5355dcb..5ca3809a1b45 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -375,10 +375,13 @@ void flush_tlb_mm(struct mm_struct *mm)
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
 		smp_on_other_tlbs(flush_tlb_mm_ipi, (void *)mm);
 	} else {
-		int i;
-		for (i = 0; i < num_online_cpus(); i++)
-			if (smp_processor_id() != i)
-				cpu_context(i, mm) = 0;
+		cpumask_t mask = cpu_online_map;
+		unsigned int cpu;
+
+		cpu_clear(smp_processor_id(), mask);
+		for_each_online_cpu(cpu)
+			if (cpu_context(cpu, mm))
+				cpu_context(cpu, mm) = 0;
 	}
 	local_flush_tlb_mm(mm);
 
@@ -411,10 +414,13 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 		fd.addr2 = end;
 		smp_on_other_tlbs(flush_tlb_range_ipi, (void *)&fd);
 	} else {
-		int i;
-		for (i = 0; i < num_online_cpus(); i++)
-			if (smp_processor_id() != i)
-				cpu_context(i, mm) = 0;
+		cpumask_t mask = cpu_online_map;
+		unsigned int cpu;
+
+		cpu_clear(smp_processor_id(), mask);
+		for_each_online_cpu(cpu)
+			if (cpu_context(cpu, mm))
+				cpu_context(cpu, mm) = 0;
 	}
 	local_flush_tlb_range(vma, start, end);
 	preempt_enable();
@@ -453,10 +459,13 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 		fd.addr1 = page;
 		smp_on_other_tlbs(flush_tlb_page_ipi, (void *)&fd);
 	} else {
-		int i;
-		for (i = 0; i < num_online_cpus(); i++)
-			if (smp_processor_id() != i)
-				cpu_context(i, vma->vm_mm) = 0;
+		cpumask_t mask = cpu_online_map;
+		unsigned int cpu;
+
+		cpu_clear(smp_processor_id(), mask);
+		for_each_online_cpu(cpu)
+			if (cpu_context(cpu, vma->vm_mm))
+				cpu_context(cpu, vma->vm_mm) = 0;
 	}
 	local_flush_tlb_page(vma, page);
 	preempt_enable();

commit bd6aeeffcc0be716e4d2d1f27fb132741e345cc0
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Oct 3 01:24:16 2007 +0100

    [MIPS] SMP: Implement smp_call_function_mask().
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 500a7ec2880f..481ba5355dcb 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -97,6 +97,8 @@ struct call_data_struct *call_data;
 
 /*
  * Run a function on all other CPUs.
+ *
+ *  <mask>	cpuset_t of all processors to run the function on.
  *  <func>      The function to run. This must be fast and non-blocking.
  *  <info>      An arbitrary pointer to pass to the function.
  *  <retry>     If true, keep retrying until ready.
@@ -121,18 +123,20 @@ struct call_data_struct *call_data;
  * Spin waiting for call_lock
  * Deadlock                            Deadlock
  */
-int smp_call_function (void (*func) (void *info), void *info, int retry,
-								int wait)
+int smp_call_function_mask(cpumask_t mask, void (*func) (void *info),
+	void *info, int retry, int wait)
 {
 	struct call_data_struct data;
-	int i, cpus = num_online_cpus() - 1;
 	int cpu = smp_processor_id();
+	int cpus;
 
 	/*
 	 * Can die spectacularly if this CPU isn't yet marked online
 	 */
 	BUG_ON(!cpu_online(cpu));
 
+	cpu_clear(cpu, mask);
+	cpus = cpus_weight(mask);
 	if (!cpus)
 		return 0;
 
@@ -151,9 +155,7 @@ int smp_call_function (void (*func) (void *info), void *info, int retry,
 	smp_mb();
 
 	/* Send a message to all other CPUs and wait for them to respond */
-	for_each_online_cpu(i)
-		if (i != cpu)
-			core_send_ipi(i, SMP_CALL_FUNCTION);
+	core_send_ipi_mask(mask, SMP_CALL_FUNCTION);
 
 	/* Wait for response */
 	/* FIXME: lock-up detection, backtrace on lock-up */
@@ -169,6 +171,11 @@ int smp_call_function (void (*func) (void *info), void *info, int retry,
 	return 0;
 }
 
+int smp_call_function(void (*func) (void *info), void *info, int retry,
+	int wait)
+{
+	return smp_call_function_mask(cpu_online_map, func, info, retry, wait);
+}
 
 void smp_call_function_interrupt(void)
 {
@@ -199,8 +206,7 @@ void smp_call_function_interrupt(void)
 int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
 			     int retry, int wait)
 {
-	struct call_data_struct data;
-	int me;
+	int ret, me;
 
 	/*
 	 * Can die spectacularly if this CPU isn't yet marked online
@@ -219,33 +225,8 @@ int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
 		return 0;
 	}
 
-	/* Can deadlock when called with interrupts disabled */
-	WARN_ON(irqs_disabled());
-
-	data.func = func;
-	data.info = info;
-	atomic_set(&data.started, 0);
-	data.wait = wait;
-	if (wait)
-		atomic_set(&data.finished, 0);
-
-	spin_lock(&smp_call_lock);
-	call_data = &data;
-	smp_mb();
-
-	/* Send a message to the other CPU */
-	core_send_ipi(cpu, SMP_CALL_FUNCTION);
-
-	/* Wait for response */
-	/* FIXME: lock-up detection, backtrace on lock-up */
-	while (atomic_read(&data.started) != 1)
-		barrier();
-
-	if (wait)
-		while (atomic_read(&data.finished) != 1)
-			barrier();
-	call_data = NULL;
-	spin_unlock(&smp_call_lock);
+	ret = smp_call_function_mask(cpumask_of_cpu(cpu), func, info, retry,
+				     wait);
 
 	put_cpu();
 	return 0;

commit 7bcf7717b6a047c272410d0cd00213185fe6b99d
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Oct 11 23:46:09 2007 +0100

    [MIPS] Implement clockevents for R4000-style cp0 count/compare interrupt
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 73b0dab02668..500a7ec2880f 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -38,6 +38,7 @@
 #include <asm/system.h>
 #include <asm/mmu_context.h>
 #include <asm/smp.h>
+#include <asm/time.h>
 
 #ifdef CONFIG_MIPS_MT_SMTC
 #include <asm/mipsmtregs.h>
@@ -70,6 +71,7 @@ asmlinkage __cpuinit void start_secondary(void)
 	cpu_probe();
 	cpu_report();
 	per_cpu_trap_init();
+	mips_clockevent_init();
 	prom_init_secondary();
 
 	/*

commit b4b2917cc8babe8eaf4bc133bca31b11ed7dac13
Author: Peter Watkins <pwatkins@sicortex.com>
Date:   Mon Jul 30 18:01:29 2007 -0400

    [MIPS] Add smp_call_function_single()
    
    In the other archs, there is more factoring of smp call code, and more care
    in the use of get_cpu(). That can be a follow-up MIPS patch.
    
    Signed-off-by: Peter Watkins <pwatkins@sicortex.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 04bbbd8d91ab..73b0dab02668 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -194,6 +194,61 @@ void smp_call_function_interrupt(void)
 	}
 }
 
+int smp_call_function_single(int cpu, void (*func) (void *info), void *info,
+			     int retry, int wait)
+{
+	struct call_data_struct data;
+	int me;
+
+	/*
+	 * Can die spectacularly if this CPU isn't yet marked online
+	 */
+	if (!cpu_online(cpu))
+		return 0;
+
+	me = get_cpu();
+	BUG_ON(!cpu_online(me));
+
+	if (cpu == me) {
+		local_irq_disable();
+		func(info);
+		local_irq_enable();
+		put_cpu();
+		return 0;
+	}
+
+	/* Can deadlock when called with interrupts disabled */
+	WARN_ON(irqs_disabled());
+
+	data.func = func;
+	data.info = info;
+	atomic_set(&data.started, 0);
+	data.wait = wait;
+	if (wait)
+		atomic_set(&data.finished, 0);
+
+	spin_lock(&smp_call_lock);
+	call_data = &data;
+	smp_mb();
+
+	/* Send a message to the other CPU */
+	core_send_ipi(cpu, SMP_CALL_FUNCTION);
+
+	/* Wait for response */
+	/* FIXME: lock-up detection, backtrace on lock-up */
+	while (atomic_read(&data.started) != 1)
+		barrier();
+
+	if (wait)
+		while (atomic_read(&data.finished) != 1)
+			barrier();
+	call_data = NULL;
+	spin_unlock(&smp_call_lock);
+
+	put_cpu();
+	return 0;
+}
+
 static void stop_this_cpu(void *dummy)
 {
 	/*

commit 4e950f6f0189f65f8bf069cf2272649ef418f5e4
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Jul 30 02:36:13 2007 +0400

    Remove fs.h from mm.h
    
    Remove fs.h from mm.h. For this,
     1) Uninline vma_wants_writenotify(). It's pretty huge anyway.
     2) Add back fs.h or less bloated headers (err.h) to files that need it.
    
    As result, on x86_64 allyesconfig, fs.h dependencies cut down from 3929 files
    rebuilt down to 3444 (-12.3%).
    
    Cross-compile tested without regressions on my two usual configs and (sigh):
    
    alpha              arm-mx1ads        mips-bigsur          powerpc-ebony
    alpha-allnoconfig  arm-neponset      mips-capcella        powerpc-g5
    alpha-defconfig    arm-netwinder     mips-cobalt          powerpc-holly
    alpha-up           arm-netx          mips-db1000          powerpc-iseries
    arm                arm-ns9xxx        mips-db1100          powerpc-linkstation
    arm-assabet        arm-omap_h2_1610  mips-db1200          powerpc-lite5200
    arm-at91rm9200dk   arm-onearm        mips-db1500          powerpc-maple
    arm-at91rm9200ek   arm-picotux200    mips-db1550          powerpc-mpc7448_hpc2
    arm-at91sam9260ek  arm-pleb          mips-ddb5477         powerpc-mpc8272_ads
    arm-at91sam9261ek  arm-pnx4008       mips-decstation      powerpc-mpc8313_rdb
    arm-at91sam9263ek  arm-pxa255-idp    mips-e55             powerpc-mpc832x_mds
    arm-at91sam9rlek   arm-realview      mips-emma2rh         powerpc-mpc832x_rdb
    arm-ateb9200       arm-realview-smp  mips-excite          powerpc-mpc834x_itx
    arm-badge4         arm-rpc           mips-fulong          powerpc-mpc834x_itxgp
    arm-carmeva        arm-s3c2410       mips-ip22            powerpc-mpc834x_mds
    arm-cerfcube       arm-shannon       mips-ip27            powerpc-mpc836x_mds
    arm-clps7500       arm-shark         mips-ip32            powerpc-mpc8540_ads
    arm-collie         arm-simpad        mips-jazz            powerpc-mpc8544_ds
    arm-corgi          arm-spitz         mips-jmr3927         powerpc-mpc8560_ads
    arm-csb337         arm-trizeps4      mips-malta           powerpc-mpc8568mds
    arm-csb637         arm-versatile     mips-mipssim         powerpc-mpc85xx_cds
    arm-ebsa110        i386              mips-mpc30x          powerpc-mpc8641_hpcn
    arm-edb7211        i386-allnoconfig  mips-msp71xx         powerpc-mpc866_ads
    arm-em_x270        i386-defconfig    mips-ocelot          powerpc-mpc885_ads
    arm-ep93xx         i386-up           mips-pb1100          powerpc-pasemi
    arm-footbridge     ia64              mips-pb1500          powerpc-pmac32
    arm-fortunet       ia64-allnoconfig  mips-pb1550          powerpc-ppc64
    arm-h3600          ia64-bigsur       mips-pnx8550-jbs     powerpc-prpmc2800
    arm-h7201          ia64-defconfig    mips-pnx8550-stb810  powerpc-ps3
    arm-h7202          ia64-gensparse    mips-qemu            powerpc-pseries
    arm-hackkit        ia64-sim          mips-rbhma4200       powerpc-up
    arm-integrator     ia64-sn2          mips-rbhma4500       s390
    arm-iop13xx        ia64-tiger        mips-rm200           s390-allnoconfig
    arm-iop32x         ia64-up           mips-sb1250-swarm    s390-defconfig
    arm-iop33x         ia64-zx1          mips-sead            s390-up
    arm-ixp2000        m68k              mips-tb0219          sparc
    arm-ixp23xx        m68k-amiga        mips-tb0226          sparc-allnoconfig
    arm-ixp4xx         m68k-apollo       mips-tb0287          sparc-defconfig
    arm-jornada720     m68k-atari        mips-workpad         sparc-up
    arm-kafa           m68k-bvme6000     mips-wrppmc          sparc64
    arm-kb9202         m68k-hp300        mips-yosemite        sparc64-allnoconfig
    arm-ks8695         m68k-mac          parisc               sparc64-defconfig
    arm-lart           m68k-mvme147      parisc-allnoconfig   sparc64-up
    arm-lpd270         m68k-mvme16x      parisc-defconfig     um-x86_64
    arm-lpd7a400       m68k-q40          parisc-up            x86_64
    arm-lpd7a404       m68k-sun3         powerpc              x86_64-allnoconfig
    arm-lubbock        m68k-sun3x        powerpc-cell         x86_64-defconfig
    arm-lusl7200       mips              powerpc-celleb       x86_64-up
    arm-mainstone      mips-atlas        powerpc-chrp32
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index be7362bc2c9a..04bbbd8d91ab 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -30,6 +30,7 @@
 #include <linux/sched.h>
 #include <linux/cpumask.h>
 #include <linux/cpu.h>
+#include <linux/err.h>
 
 #include <asm/atomic.h>
 #include <asm/cpu.h>

commit b3f6df9f21c6efc4641613188204aa0742bc9e22
Author: Robert P. J. Day <rpjday@mindspring.com>
Date:   Fri May 25 14:32:28 2007 -0400

    [MIPS] Transform old-style macros to newer "__noreturn"
    
    Convert old/obsolete NORET_TYPE and ATTRIB_NORET macros to use the
    newer standard of "__noreturn" as defined in compiler-gcc.h.
    
    Signed-off-by: Robert P. J. Day <rpjday@mindspring.com>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index a1b017f2dbb3..be7362bc2c9a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -52,7 +52,7 @@ EXPORT_SYMBOL(phys_cpu_present_map);
 EXPORT_SYMBOL(cpu_online_map);
 
 extern void __init calibrate_delay(void);
-extern ATTRIB_NORET void cpu_idle(void);
+extern void cpu_idle(void);
 
 /*
  * First C code run on the secondary CPUs after being started up by

commit 0437e109e1841607f2988891eaa36c531c6aa6ac
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 9 18:51:57 2007 +0200

    sched: zap the migration init / cache-hot balancing code
    
    the SMP load-balancer uses the boot-time migration-cost estimation
    code to attempt to improve the quality of balancing. The reason for
    this code is that the discrete priority queues do not preserve
    the order of scheduling accurately, so the load-balancer skips
    tasks that were running on a CPU 'recently'.
    
    this code is fundamental fragile: the boot-time migration cost detector
    doesnt really work on systems that had large L3 caches, it caused boot
    delays on large systems and the whole cache-hot concept made the
    balancing code pretty undeterministic as well.
    
    (and hey, i wrote most of it, so i can say it out loud that it sucks ;-)
    
    under CFS the same purpose of cache affinity can be achieved without
    any special cache-hot special-case: tasks are sorted in the 'timeline'
    tree and the SMP balancer picks tasks from the left side of the
    tree, thus the most cache-cold task is balanced automatically.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 67edfa7ed93a..a1b017f2dbb3 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -51,16 +51,6 @@ int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 EXPORT_SYMBOL(phys_cpu_present_map);
 EXPORT_SYMBOL(cpu_online_map);
 
-/* This happens early in bootup, can't really do it better */
-static void smp_tune_scheduling (void)
-{
-	struct cache_desc *cd = &current_cpu_data.scache;
-	unsigned long cachesize = cd->linesz * cd->sets * cd->ways;
-
-	if (cachesize > max_cache_size)
-		max_cache_size = cachesize;
-}
-
 extern void __init calibrate_delay(void);
 extern ATTRIB_NORET void cpu_idle(void);
 
@@ -228,7 +218,6 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	init_new_context(current, &init_mm);
 	current_thread_info()->cpu = 0;
-	smp_tune_scheduling();
 	plat_prepare_cpus(max_cpus);
 #ifndef CONFIG_HOTPLUG_CPU
 	cpu_present_map = cpu_possible_map;

commit 4ebd5233f0420f1e383c38f962ec84c4d53fbbad
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu May 31 16:15:01 2007 +0100

    [MIPS] Fix modpost warnings by making start_secondary __cpuinit
    
    WARNING: arch/mips/kernel/built-in.o(.text+0x9a58): Section mismatch: reference to .init.text:cpu_report (between 'start_secondary' and 'smp_prepare_boot_cpu')
    WARNING: arch/mips/kernel/built-in.o(.text+0x9a60): Section mismatch: reference to .init.text:per_cpu_trap_init (between 'start_secondary' and 'smp_prepare_boot_cpu')
    WARNING: arch/mips/kernel/built-in.o(.text+0x9adc): Section mismatch: reference to .init.text:cpu_probe (between 'start_secondary' and 'smp_prepare_boot_cpu')
    mipsel-linux-objcopy -S -O srec --remove-section=.reginfo --remove-section=.mdebug --remove-section=.comment --remove-section=.note --remove-section=.pdr --remove-section=.options --remove-section=.MIPS.options vmlinux arch/mips/boot/vmlinux.srec
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index c46e479c992b..67edfa7ed93a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -68,7 +68,7 @@ extern ATTRIB_NORET void cpu_idle(void);
  * First C code run on the secondary CPUs after being started up by
  * the master.
  */
-asmlinkage void start_secondary(void)
+asmlinkage __cpuinit void start_secondary(void)
 {
 	unsigned int cpu;
 

commit de7fa296b60c9086fa038350404975b7ee4e60c2
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Feb 21 00:10:19 2007 +0000

    [MIPS] SMP: Get smp_tune_scheduling to do something useful.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 0555fc554f65..c46e479c992b 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -51,31 +51,14 @@ int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
 EXPORT_SYMBOL(phys_cpu_present_map);
 EXPORT_SYMBOL(cpu_online_map);
 
+/* This happens early in bootup, can't really do it better */
 static void smp_tune_scheduling (void)
 {
 	struct cache_desc *cd = &current_cpu_data.scache;
-	unsigned long cachesize;       /* kB   */
-	unsigned long cpu_khz;
+	unsigned long cachesize = cd->linesz * cd->sets * cd->ways;
 
-	/*
-	 * Crude estimate until we actually meassure ...
-	 */
-	cpu_khz = loops_per_jiffy * 2 * HZ / 1000;
-
-	/*
-	 * Rough estimation for SMP scheduling, this is the number of
-	 * cycles it takes for a fully memory-limited process to flush
-	 * the SMP-local cache.
-	 *
-	 * (For a P5 this pretty much means we will choose another idle
-	 *  CPU almost always at wakeup time (this is due to the small
-	 *  L1 cache), on PIIs it's around 50-100 usecs, depending on
-	 *  the cache size)
-	 */
-	if (!cpu_khz)
-		return;
-
-	cachesize = cd->linesz * cd->sets * cd->ways;
+	if (cachesize > max_cache_size)
+		max_cache_size = cachesize;
 }
 
 extern void __init calibrate_delay(void);

commit b282b6f8a8d1cf3e132ce3769d7d1cac81d9dd2d
Author: Gautham R Shenoy <ego@in.ibm.com>
Date:   Wed Jan 10 23:15:34 2007 -0800

    [PATCH] Change cpu_up and co from __devinit to __cpuinit
    
    Compiling the kernel with CONFIG_HOTPLUG = y and CONFIG_HOTPLUG_CPU = n
    with CONFIG_RELOCATABLE = y generates the following modpost warnings
    
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141b7d) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141b9c) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.text:__cpu_up
    from .text between '_cpu_up' (at offset 0xc0141bd8) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141c05) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141c26) and 'cpu_up'
    WARNING: vmlinux - Section mismatch: reference to .init.data: from
    .text between '_cpu_up' (at offset 0xc0141c37) and 'cpu_up'
    
    This is because cpu_up, _cpu_up and __cpu_up (in some architectures) are
    defined as __devinit
    AND
    __cpu_up calls some __cpuinit functions.
    
    Since __cpuinit would map to __init with this kind of a configuration,
    we get a .text refering .init.data warning.
    
    This patch solves the problem by converting all of __cpu_up, _cpu_up
    and cpu_up from __devinit to __cpuinit. The approach is justified since
    the callers of cpu_up are either dependent on CONFIG_HOTPLUG_CPU or
    are of __init type.
    
    Thus when CONFIG_HOTPLUG_CPU=y, all these cpu up functions would land up
    in .text section, and when CONFIG_HOTPLUG_CPU=n, all these functions would
    land up in .init section.
    
    Tested on a i386 SMP machine running linux-2.6.20-rc3-mm1.
    
    Signed-off-by: Gautham R Shenoy <ego@in.ibm.com>
    Cc: Vivek Goyal <vgoyal@in.ibm.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index f2a8701e414d..0555fc554f65 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -271,7 +271,7 @@ void __devinit smp_prepare_boot_cpu(void)
  * and keep control until "cpu_online(cpu)" is set.  Note: cpu is
  * physical, not logical.
  */
-int __devinit __cpu_up(unsigned int cpu)
+int __cpuinit __cpu_up(unsigned int cpu)
 {
 	struct task_struct *idle;
 

commit 0004a9dfeaa709a7f853487aba19932c9b1a87c8
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Oct 31 03:45:07 2006 +0000

    [MIPS] Cleanup memory barriers for weakly ordered systems.
    
    Also the R4000 / R4600 LL/SC instructions imply a sync so no explicit sync
    needed.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 49db516789e0..f2a8701e414d 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -172,7 +172,7 @@ int smp_call_function (void (*func) (void *info), void *info, int retry,
 
 	spin_lock(&smp_call_lock);
 	call_data = &data;
-	mb();
+	smp_mb();
 
 	/* Send a message to all other CPUs and wait for them to respond */
 	for_each_online_cpu(i)
@@ -204,7 +204,7 @@ void smp_call_function_interrupt(void)
 	 * Notify initiating CPU that I've grabbed the data and am
 	 * about to execute the function.
 	 */
-	mb();
+	smp_mb();
 	atomic_inc(&call_data->started);
 
 	/*
@@ -215,7 +215,7 @@ void smp_call_function_interrupt(void)
 	irq_exit();
 
 	if (wait) {
-		mb();
+		smp_mb();
 		atomic_inc(&call_data->finished);
 	}
 }

commit f5d6c63a67a8f124ddae88511427249d1dd87880
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Nov 29 15:04:08 2006 +0000

    [MIPS] Do topology_init even on uniprocessor kernels.
    
    Otherwise CPU 0 doesn't show up in sysfs which breaks some software.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index db80957ada89..49db516789e0 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -463,28 +463,5 @@ void flush_tlb_one(unsigned long vaddr)
 	smp_on_each_tlb(flush_tlb_one_ipi, (void *) vaddr);
 }
 
-static DEFINE_PER_CPU(struct cpu, cpu_devices);
-
-static int __init topology_init(void)
-{
-	int i, ret;
-
-#ifdef CONFIG_NUMA
-	for_each_online_node(i)
-		register_one_node(i);
-#endif /* CONFIG_NUMA */
-
-	for_each_present_cpu(i) {
-		ret = register_cpu(&per_cpu(cpu_devices, i), i);
-		if (ret)
-			printk(KERN_WARNING "topology_init: register_cpu %d "
-			       "failed (%d)\n", i, ret);
-	}
-
-	return 0;
-}
-
-subsys_initcall(topology_init);
-
 EXPORT_SYMBOL(flush_tlb_page);
 EXPORT_SYMBOL(flush_tlb_one);

commit 9a244b95ddb62a17b62f4b061b6e13ca4d177942
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Oct 11 19:30:03 2006 +0100

    [MIPS] Pass NULL not 0 for pointer value.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 1af3612a1ce8..db80957ada89 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -310,7 +310,7 @@ static void flush_tlb_all_ipi(void *info)
 
 void flush_tlb_all(void)
 {
-	on_each_cpu(flush_tlb_all_ipi, 0, 1, 1);
+	on_each_cpu(flush_tlb_all_ipi, NULL, 1, 1);
 }
 
 static void flush_tlb_mm_ipi(void *mm)

commit 0e8f8f54c1537d22eb0168622a5f6aef4040da6a
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Jul 8 01:07:40 2006 +0200

    [MIPS] NUMA: Register all nodes before cpus or sysfs will barf.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 221895802dca..1af3612a1ce8 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -467,14 +467,18 @@ static DEFINE_PER_CPU(struct cpu, cpu_devices);
 
 static int __init topology_init(void)
 {
-	int cpu;
-	int ret;
+	int i, ret;
 
-	for_each_present_cpu(cpu) {
-		ret = register_cpu(&per_cpu(cpu_devices, cpu), cpu);
+#ifdef CONFIG_NUMA
+	for_each_online_node(i)
+		register_one_node(i);
+#endif /* CONFIG_NUMA */
+
+	for_each_present_cpu(i) {
+		ret = register_cpu(&per_cpu(cpu_devices, i), i);
 		if (ret)
 			printk(KERN_WARNING "topology_init: register_cpu %d "
-			       "failed (%d)\n", cpu, ret);
+			       "failed (%d)\n", i, ret);
 	}
 
 	return 0;

commit 25969354a385f347b55aafb1040dfc21263fa7c3
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Jun 22 22:42:32 2006 +0100

    [MIPS] Avoid interprocessor function calls.
    
    On the 34K where multiple virtual processors are implemented in a single
    core and share a single TLB, interprocessor function calls are not needed
    to flush a cache, so avoid them.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 9096a5ea4229..221895802dca 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -318,6 +318,32 @@ static void flush_tlb_mm_ipi(void *mm)
 	local_flush_tlb_mm((struct mm_struct *)mm);
 }
 
+/*
+ * Special Variant of smp_call_function for use by TLB functions:
+ *
+ *  o No return value
+ *  o collapses to normal function call on UP kernels
+ *  o collapses to normal function call on systems with a single shared
+ *    primary cache.
+ *  o CONFIG_MIPS_MT_SMTC currently implies there is only one physical core.
+ */
+static inline void smp_on_other_tlbs(void (*func) (void *info), void *info)
+{
+#ifndef CONFIG_MIPS_MT_SMTC
+	smp_call_function(func, info, 1, 1);
+#endif
+}
+
+static inline void smp_on_each_tlb(void (*func) (void *info), void *info)
+{
+	preempt_disable();
+
+	smp_on_other_tlbs(func, info);
+	func(info);
+
+	preempt_enable();
+}
+
 /*
  * The following tlb flush calls are invoked when old translations are
  * being torn down, or pte attributes are changing. For single threaded
@@ -336,7 +362,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 	preempt_disable();
 
 	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
-		smp_call_function(flush_tlb_mm_ipi, (void *)mm, 1, 1);
+		smp_on_other_tlbs(flush_tlb_mm_ipi, (void *)mm);
 	} else {
 		int i;
 		for (i = 0; i < num_online_cpus(); i++)
@@ -372,7 +398,7 @@ void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned l
 		fd.vma = vma;
 		fd.addr1 = start;
 		fd.addr2 = end;
-		smp_call_function(flush_tlb_range_ipi, (void *)&fd, 1, 1);
+		smp_on_other_tlbs(flush_tlb_range_ipi, (void *)&fd);
 	} else {
 		int i;
 		for (i = 0; i < num_online_cpus(); i++)
@@ -414,7 +440,7 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
 
 		fd.vma = vma;
 		fd.addr1 = page;
-		smp_call_function(flush_tlb_page_ipi, (void *)&fd, 1, 1);
+		smp_on_other_tlbs(flush_tlb_page_ipi, (void *)&fd);
 	} else {
 		int i;
 		for (i = 0; i < num_online_cpus(); i++)
@@ -434,8 +460,7 @@ static void flush_tlb_one_ipi(void *info)
 
 void flush_tlb_one(unsigned long vaddr)
 {
-	smp_call_function(flush_tlb_one_ipi, (void *) vaddr, 1, 1);
-	local_flush_tlb_one(vaddr);
+	smp_on_each_tlb(flush_tlb_one_ipi, (void *) vaddr);
 }
 
 static DEFINE_PER_CPU(struct cpu, cpu_devices);

commit 76b67ed9dce69a6a329cdd66f94af1787f417b62
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:41 2006 -0700

    [PATCH] node hotplug: register cpu: remove node struct
    
    With Goto-san's patch, we can add new pgdat/node at runtime.  I'm now
    considering node-hot-add with cpu + memory on ACPI.
    
    I found acpi container, which describes node, could evaluate cpu before
    memory. This means cpu-hot-add occurs before memory hot add.
    
    In most part, cpu-hot-add doesn't depend on node hot add.  But register_cpu(),
    which creates symbolic link from node to cpu, requires that node should be
    onlined before register_cpu().  When a node is onlined, its pgdat should be
    there.
    
    This patch-set holds off creating symbolic link from node to cpu
    until node is onlined.
    
    This removes node arguments from register_cpu().
    
    Now, register_cpu() requires 'struct node' as its argument.  But the array of
    struct node is now unified in driver/base/node.c now (By Goto's node hotplug
    patch).  We can get struct node in generic way.  So, this argument is not
    necessary now.
    
    This patch also guarantees add cpu under node only when node is onlined.  It
    is necessary for node-hot-add vs.  cpu-hot-add patch following this.
    
    Moreover, register_cpu calculates cpu->node_id by cpu_to_node() without regard
    to its 'struct node *root' argument.  This patch removes it.
    
    Also modify callers of register_cpu()/unregister_cpu, whose args are changed
    by register-cpu-remove-node-struct patch.
    
    [Brice.Goglin@ens-lyon.org: fix it]
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Ashok Raj <ashok.raj@intel.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Signed-off-by: Brice Goglin <Brice.Goglin@ens-lyon.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 298f82fe8440..9096a5ea4229 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -446,7 +446,7 @@ static int __init topology_init(void)
 	int ret;
 
 	for_each_present_cpu(cpu) {
-		ret = register_cpu(&per_cpu(cpu_devices, cpu), cpu, NULL);
+		ret = register_cpu(&per_cpu(cpu_devices, cpu), cpu);
 		if (ret)
 			printk(KERN_WARNING "topology_init: register_cpu %d "
 			       "failed (%d)\n", cpu, ret);

commit 320e6aba26892b016293190e079f15e83a5c28b9
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Mon May 22 14:24:04 2006 +0100

    [MIPS] Fix SMP now that fixup_cpu_present_map is gone.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index d42f358754ad..298f82fe8440 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -247,6 +247,9 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	current_thread_info()->cpu = 0;
 	smp_tune_scheduling();
 	plat_prepare_cpus(max_cpus);
+#ifndef CONFIG_HOTPLUG_CPU
+	cpu_present_map = cpu_possible_map;
+#endif
 }
 
 /* preload SMP state for boot cpu */
@@ -442,7 +445,7 @@ static int __init topology_init(void)
 	int cpu;
 	int ret;
 
-	for_each_cpu(cpu) {
+	for_each_present_cpu(cpu) {
 		ret = register_cpu(&per_cpu(cpu_devices, cpu), cpu, NULL);
 		if (ret)
 			printk(KERN_WARNING "topology_init: register_cpu %d "

commit 41c594ab65fc89573af296d192aa5235d09717ab
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Apr 5 09:45:45 2006 +0100

    [MIPS] MT: Improved multithreading support.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 72a287aa937e..d42f358754ad 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -38,6 +38,10 @@
 #include <asm/mmu_context.h>
 #include <asm/smp.h>
 
+#ifdef CONFIG_MIPS_MT_SMTC
+#include <asm/mipsmtregs.h>
+#endif /* CONFIG_MIPS_MT_SMTC */
+
 cpumask_t phys_cpu_present_map;		/* Bitmask of available CPUs */
 volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
 cpumask_t cpu_online_map;		/* Bitmask of currently online CPUs */
@@ -85,6 +89,10 @@ asmlinkage void start_secondary(void)
 {
 	unsigned int cpu;
 
+#ifdef CONFIG_MIPS_MT_SMTC
+	/* Only do cpu_probe for first TC of CPU */
+	if ((read_c0_tcbind() & TCBIND_CURTC) == 0)
+#endif /* CONFIG_MIPS_MT_SMTC */
 	cpu_probe();
 	cpu_report();
 	per_cpu_trap_init();
@@ -179,11 +187,13 @@ int smp_call_function (void (*func) (void *info), void *info, int retry,
 	if (wait)
 		while (atomic_read(&data.finished) != cpus)
 			barrier();
+	call_data = NULL;
 	spin_unlock(&smp_call_lock);
 
 	return 0;
 }
 
+
 void smp_call_function_interrupt(void)
 {
 	void (*func) (void *info) = call_data->func;

commit e49ed7f5917de5baa9e25cf8af332b3b35724e51
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Sat Apr 1 07:49:52 2006 +0100

    [MIPS] Sort out duplicate exports.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 78d171bfa331..72a287aa937e 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -446,5 +446,3 @@ subsys_initcall(topology_init);
 
 EXPORT_SYMBOL(flush_tlb_page);
 EXPORT_SYMBOL(flush_tlb_one);
-EXPORT_SYMBOL(cpu_data);
-EXPORT_SYMBOL(synchronize_irq);

commit 394e3902c55e667945f6f1c2bdbc59842cce70f7
Author: Andrew Morton <akpm@osdl.org>
Date:   Thu Mar 23 03:01:05 2006 -0800

    [PATCH] more for_each_cpu() conversions
    
    When we stop allocating percpu memory for not-possible CPUs we must not touch
    the percpu data for not-possible CPUs at all.  The correct way of doing this
    is to test cpu_possible() or to use for_each_cpu().
    
    This patch is a kernel-wide sweep of all instances of NR_CPUS.  I found very
    few instances of this bug, if any.  But the patch converts lots of open-coded
    test to use the preferred helper macros.
    
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Acked-by: Kyle McMartin <kyle@parisc-linux.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Christian Zankel <chris@zankel.net>
    Cc: Philippe Elie <phil.el@wanadoo.fr>
    Cc: Nathan Scott <nathans@sgi.com>
    Cc: Jens Axboe <axboe@suse.de>
    Cc: Eric Dumazet <dada1@cosmosbay.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 06ed90752424..78d171bfa331 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -167,8 +167,8 @@ int smp_call_function (void (*func) (void *info), void *info, int retry,
 	mb();
 
 	/* Send a message to all other CPUs and wait for them to respond */
-	for (i = 0; i < NR_CPUS; i++)
-		if (cpu_online(i) && i != cpu)
+	for_each_online_cpu(i)
+		if (i != cpu)
 			core_send_ipi(i, SMP_CALL_FUNCTION);
 
 	/* Wait for response */

commit 9b6695a8adfe0916e81ddd810a5b9db3eb8b0e46
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Feb 23 12:23:27 2006 +0000

    [MIPS] SMP: Fix initialization order bug.
    
    A recent change requires cpu_possible_map to be initialized before
    smp_sched_init() but most MIPS platforms were initializing their
    processors in the prom_prepare_cpus callback of smp_prepare_cpus.  The
    simple fix of calling prom_prepare_cpus from one of the earlier SMP
    initialization hooks doesn't work well either since IPIs may require
    init_IRQ() to have completed, so bit the bullet and split
    prom_prepare_cpus into two initialization functions, plat_smp_setup
    which is called early from setup_arch and plat_prepare_cpus called where
    prom_prepare_cpus used to be called.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 5e189862e523..06ed90752424 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -236,7 +236,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	init_new_context(current, &init_mm);
 	current_thread_info()->cpu = 0;
 	smp_tune_scheduling();
-	prom_prepare_cpus(max_cpus);
+	plat_prepare_cpus(max_cpus);
 }
 
 /* preload SMP state for boot cpu */

commit 1e35aabab82f8a6f7ab884b642e68be260f92f2c
Author: Rojhalat Ibrahim <imr@rtschenk.de>
Date:   Mon Feb 20 13:35:27 2006 +0000

    [MIPS] Add topology_init.
    
    A recent patch introduced cpu topology in sysfs.  When you run a kernel
    with SMP and sysfs enabled, you now get an Oops on boot. The following
    patch fixes that by adding topology_init to arch/mips/kernel/smp.c. The
    code is copied from arch/s390/kernel/smp.c.
    
    Signed-off-by: Rojhalat Ibrahim <imr@rtschenk.de>
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 25472fcaf715..5e189862e523 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -29,6 +29,7 @@
 #include <linux/timex.h>
 #include <linux/sched.h>
 #include <linux/cpumask.h>
+#include <linux/cpu.h>
 
 #include <asm/atomic.h>
 #include <asm/cpu.h>
@@ -424,6 +425,25 @@ void flush_tlb_one(unsigned long vaddr)
 	local_flush_tlb_one(vaddr);
 }
 
+static DEFINE_PER_CPU(struct cpu, cpu_devices);
+
+static int __init topology_init(void)
+{
+	int cpu;
+	int ret;
+
+	for_each_cpu(cpu) {
+		ret = register_cpu(&per_cpu(cpu_devices, cpu), cpu, NULL);
+		if (ret)
+			printk(KERN_WARNING "topology_init: register_cpu %d "
+			       "failed (%d)\n", cpu, ret);
+	}
+
+	return 0;
+}
+
+subsys_initcall(topology_init);
+
 EXPORT_SYMBOL(flush_tlb_page);
 EXPORT_SYMBOL(flush_tlb_one);
 EXPORT_SYMBOL(cpu_data);

commit 5bfb5d690f36d316a5f3b4f7775fda996faa6b12
Author: Nick Piggin <nickpiggin@yahoo.com.au>
Date:   Tue Nov 8 21:39:01 2005 -0800

    [PATCH] sched: disable preempt in idle tasks
    
    Run idle threads with preempt disabled.
    
    Also corrected a bugs in arm26's cpu_idle (make it actually call schedule()).
    How did it ever work before?
    
    Might fix the CPU hotplugging hang which Nigel Cunningham noted.
    
    We think the bug hits if the idle thread is preempted after checking
    need_resched() and before going to sleep, then the CPU offlined.
    
    After calling stop_machine_run, the CPU eventually returns from preemption and
    into the idle thread and goes to sleep.  The CPU will continue executing
    previous idle and have no chance to call play_dead.
    
    By disabling preemption until we are ready to explicitly schedule, this bug is
    fixed and the idle threads generally become more robust.
    
    From: alexs <ashepard@u.washington.edu>
    
      PPC build fix
    
    From: Yoichi Yuasa <yuasa@hh.iij4u.or.jp>
    
      MIPS build fix
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Yoichi Yuasa <yuasa@hh.iij4u.or.jp>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index fcacf1aae98a..25472fcaf715 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -82,7 +82,7 @@ extern ATTRIB_NORET void cpu_idle(void);
  */
 asmlinkage void start_secondary(void)
 {
-	unsigned int cpu = smp_processor_id();
+	unsigned int cpu;
 
 	cpu_probe();
 	cpu_report();
@@ -95,6 +95,8 @@ asmlinkage void start_secondary(void)
 	 */
 
 	calibrate_delay();
+	preempt_disable();
+	cpu = smp_processor_id();
 	cpu_data[cpu].udelay_val = loops_per_jiffy;
 
 	prom_smp_finish();

commit 3bffe736d93ce527a27fd5a4845fb2a0e82fcd99
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Aug 16 16:10:18 2005 +0000

    Delete old junk.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 798fce509398..fcacf1aae98a 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -50,7 +50,6 @@ static void smp_tune_scheduling (void)
 {
 	struct cache_desc *cd = &current_cpu_data.scache;
 	unsigned long cachesize;       /* kB   */
-	unsigned long bandwidth = 350; /* MB/s */
 	unsigned long cpu_khz;
 
 	/*

commit ae1b3d51c89a96c641111e2c103557592577cf51
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Fri Jul 15 15:44:02 2005 +0000

    Make sure that the processor is actually online or die spectacularly.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 25762917e822..798fce509398 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -142,6 +142,11 @@ int smp_call_function (void (*func) (void *info), void *info, int retry,
 	int i, cpus = num_online_cpus() - 1;
 	int cpu = smp_processor_id();
 
+	/*
+	 * Can die spectacularly if this CPU isn't yet marked online
+	 */
+	BUG_ON(!cpu_online(cpu));
+
 	if (!cpus)
 		return 0;
 

commit f03da6e28ea2d20f1a8451869fd1c9ea9935022b
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Wed Apr 13 13:37:32 2005 +0000

    Fix BogoMIPS display on UP and some minor cosmetical things.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index d1828ef5ffd6..25762917e822 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -226,7 +226,6 @@ void __init smp_cpus_done(unsigned int max_cpus)
 /* called from main before smp_init() */
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
-	cpu_data[0].udelay_val = loops_per_jiffy;
 	init_new_context(current, &init_mm);
 	current_thread_info()->cpu = 0;
 	smp_tune_scheduling();

commit b727a60258730b331519fedda503a8da78638791
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Tue Feb 22 21:18:01 2005 +0000

    Merge do_boot_cpu() into the new style __cpu_up().
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index 1d3a4b501949..d1828ef5ffd6 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -248,23 +248,28 @@ void __devinit smp_prepare_boot_cpu(void)
 }
 
 /*
- * Startup the CPU with this logical number
+ * Called once for each "cpu_possible(cpu)".  Needs to spin up the cpu
+ * and keep control until "cpu_online(cpu)" is set.  Note: cpu is
+ * physical, not logical.
  */
-static int __init do_boot_cpu(int cpu)
+int __devinit __cpu_up(unsigned int cpu)
 {
 	struct task_struct *idle;
 
 	/*
+	 * Processor goes to start_secondary(), sets online flag
 	 * The following code is purely to make sure
 	 * Linux can schedule processes on this slave.
 	 */
 	idle = fork_idle(cpu);
 	if (IS_ERR(idle))
-		panic("failed fork for CPU %d\n", cpu);
+		panic(KERN_ERR "Fork failed for CPU %d", cpu);
 
 	prom_boot_secondary(cpu, idle);
 
-	/* XXXKW timeout */
+	/*
+	 * Trust is futile.  We should really have timeouts ...
+	 */
 	while (!cpu_isset(cpu, cpu_callin_map))
 		udelay(100);
 
@@ -273,23 +278,6 @@ static int __init do_boot_cpu(int cpu)
 	return 0;
 }
 
-/*
- * Called once for each "cpu_possible(cpu)".  Needs to spin up the cpu
- * and keep control until "cpu_online(cpu)" is set.  Note: cpu is
- * physical, not logical.
- */
-int __devinit __cpu_up(unsigned int cpu)
-{
-	int ret;
-
-	/* Processor goes to start_secondary(), sets online flag */
-	ret = do_boot_cpu(cpu);
-	if (ret < 0)
-		return ret;
-
-	return 0;
-}
-
 /* Not really SMP stuff ... */
 int setup_profiling_timer(unsigned int multiplier)
 {

commit 57f0060b8a2bb2a70a4cce1a37d5e0158cea92a6
Author: Ralf Baechle <ralf@linux-mips.org>
Date:   Thu Feb 10 12:00:06 2005 +0000

    Document why calling smp_call_function will deadlock when called with
    interrupts disabled.
    
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
index af5cd3b8a396..1d3a4b501949 100644
--- a/arch/mips/kernel/smp.c
+++ b/arch/mips/kernel/smp.c
@@ -121,7 +121,19 @@ struct call_data_struct *call_data;
  * or are or have executed.
  *
  * You must not call this function with disabled interrupts or from a
- * hardware interrupt handler or from a bottom half handler.
+ * hardware interrupt handler or from a bottom half handler:
+ *
+ * CPU A                               CPU B
+ * Disable interrupts
+ *                                     smp_call_function()
+ *                                     Take call_lock
+ *                                     Send IPIs
+ *                                     Wait for all cpus to acknowledge IPI
+ *                                     CPU A has not responded, spin waiting
+ *                                     for cpu A to respond, holding call_lock
+ * smp_call_function()
+ * Spin waiting for call_lock
+ * Deadlock                            Deadlock
  */
 int smp_call_function (void (*func) (void *info), void *info, int retry,
 								int wait)

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/mips/kernel/smp.c b/arch/mips/kernel/smp.c
new file mode 100644
index 000000000000..af5cd3b8a396
--- /dev/null
+++ b/arch/mips/kernel/smp.c
@@ -0,0 +1,425 @@
+/*
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
+ *
+ * Copyright (C) 2000, 2001 Kanoj Sarcar
+ * Copyright (C) 2000, 2001 Ralf Baechle
+ * Copyright (C) 2000, 2001 Silicon Graphics, Inc.
+ * Copyright (C) 2000, 2001, 2003 Broadcom Corporation
+ */
+#include <linux/cache.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/threads.h>
+#include <linux/module.h>
+#include <linux/time.h>
+#include <linux/timex.h>
+#include <linux/sched.h>
+#include <linux/cpumask.h>
+
+#include <asm/atomic.h>
+#include <asm/cpu.h>
+#include <asm/processor.h>
+#include <asm/system.h>
+#include <asm/mmu_context.h>
+#include <asm/smp.h>
+
+cpumask_t phys_cpu_present_map;		/* Bitmask of available CPUs */
+volatile cpumask_t cpu_callin_map;	/* Bitmask of started secondaries */
+cpumask_t cpu_online_map;		/* Bitmask of currently online CPUs */
+int __cpu_number_map[NR_CPUS];		/* Map physical to logical */
+int __cpu_logical_map[NR_CPUS];		/* Map logical to physical */
+
+EXPORT_SYMBOL(phys_cpu_present_map);
+EXPORT_SYMBOL(cpu_online_map);
+
+static void smp_tune_scheduling (void)
+{
+	struct cache_desc *cd = &current_cpu_data.scache;
+	unsigned long cachesize;       /* kB   */
+	unsigned long bandwidth = 350; /* MB/s */
+	unsigned long cpu_khz;
+
+	/*
+	 * Crude estimate until we actually meassure ...
+	 */
+	cpu_khz = loops_per_jiffy * 2 * HZ / 1000;
+
+	/*
+	 * Rough estimation for SMP scheduling, this is the number of
+	 * cycles it takes for a fully memory-limited process to flush
+	 * the SMP-local cache.
+	 *
+	 * (For a P5 this pretty much means we will choose another idle
+	 *  CPU almost always at wakeup time (this is due to the small
+	 *  L1 cache), on PIIs it's around 50-100 usecs, depending on
+	 *  the cache size)
+	 */
+	if (!cpu_khz)
+		return;
+
+	cachesize = cd->linesz * cd->sets * cd->ways;
+}
+
+extern void __init calibrate_delay(void);
+extern ATTRIB_NORET void cpu_idle(void);
+
+/*
+ * First C code run on the secondary CPUs after being started up by
+ * the master.
+ */
+asmlinkage void start_secondary(void)
+{
+	unsigned int cpu = smp_processor_id();
+
+	cpu_probe();
+	cpu_report();
+	per_cpu_trap_init();
+	prom_init_secondary();
+
+	/*
+	 * XXX parity protection should be folded in here when it's converted
+	 * to an option instead of something based on .cputype
+	 */
+
+	calibrate_delay();
+	cpu_data[cpu].udelay_val = loops_per_jiffy;
+
+	prom_smp_finish();
+
+	cpu_set(cpu, cpu_callin_map);
+
+	cpu_idle();
+}
+
+DEFINE_SPINLOCK(smp_call_lock);
+
+struct call_data_struct *call_data;
+
+/*
+ * Run a function on all other CPUs.
+ *  <func>      The function to run. This must be fast and non-blocking.
+ *  <info>      An arbitrary pointer to pass to the function.
+ *  <retry>     If true, keep retrying until ready.
+ *  <wait>      If true, wait until function has completed on other CPUs.
+ *  [RETURNS]   0 on success, else a negative status code.
+ *
+ * Does not return until remote CPUs are nearly ready to execute <func>
+ * or are or have executed.
+ *
+ * You must not call this function with disabled interrupts or from a
+ * hardware interrupt handler or from a bottom half handler.
+ */
+int smp_call_function (void (*func) (void *info), void *info, int retry,
+								int wait)
+{
+	struct call_data_struct data;
+	int i, cpus = num_online_cpus() - 1;
+	int cpu = smp_processor_id();
+
+	if (!cpus)
+		return 0;
+
+	/* Can deadlock when called with interrupts disabled */
+	WARN_ON(irqs_disabled());
+
+	data.func = func;
+	data.info = info;
+	atomic_set(&data.started, 0);
+	data.wait = wait;
+	if (wait)
+		atomic_set(&data.finished, 0);
+
+	spin_lock(&smp_call_lock);
+	call_data = &data;
+	mb();
+
+	/* Send a message to all other CPUs and wait for them to respond */
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_online(i) && i != cpu)
+			core_send_ipi(i, SMP_CALL_FUNCTION);
+
+	/* Wait for response */
+	/* FIXME: lock-up detection, backtrace on lock-up */
+	while (atomic_read(&data.started) != cpus)
+		barrier();
+
+	if (wait)
+		while (atomic_read(&data.finished) != cpus)
+			barrier();
+	spin_unlock(&smp_call_lock);
+
+	return 0;
+}
+
+void smp_call_function_interrupt(void)
+{
+	void (*func) (void *info) = call_data->func;
+	void *info = call_data->info;
+	int wait = call_data->wait;
+
+	/*
+	 * Notify initiating CPU that I've grabbed the data and am
+	 * about to execute the function.
+	 */
+	mb();
+	atomic_inc(&call_data->started);
+
+	/*
+	 * At this point the info structure may be out of scope unless wait==1.
+	 */
+	irq_enter();
+	(*func)(info);
+	irq_exit();
+
+	if (wait) {
+		mb();
+		atomic_inc(&call_data->finished);
+	}
+}
+
+static void stop_this_cpu(void *dummy)
+{
+	/*
+	 * Remove this CPU:
+	 */
+	cpu_clear(smp_processor_id(), cpu_online_map);
+	local_irq_enable();	/* May need to service _machine_restart IPI */
+	for (;;);		/* Wait if available. */
+}
+
+void smp_send_stop(void)
+{
+	smp_call_function(stop_this_cpu, NULL, 1, 0);
+}
+
+void __init smp_cpus_done(unsigned int max_cpus)
+{
+	prom_cpus_done();
+}
+
+/* called from main before smp_init() */
+void __init smp_prepare_cpus(unsigned int max_cpus)
+{
+	cpu_data[0].udelay_val = loops_per_jiffy;
+	init_new_context(current, &init_mm);
+	current_thread_info()->cpu = 0;
+	smp_tune_scheduling();
+	prom_prepare_cpus(max_cpus);
+}
+
+/* preload SMP state for boot cpu */
+void __devinit smp_prepare_boot_cpu(void)
+{
+	/*
+	 * This assumes that bootup is always handled by the processor
+	 * with the logic and physical number 0.
+	 */
+	__cpu_number_map[0] = 0;
+	__cpu_logical_map[0] = 0;
+	cpu_set(0, phys_cpu_present_map);
+	cpu_set(0, cpu_online_map);
+	cpu_set(0, cpu_callin_map);
+}
+
+/*
+ * Startup the CPU with this logical number
+ */
+static int __init do_boot_cpu(int cpu)
+{
+	struct task_struct *idle;
+
+	/*
+	 * The following code is purely to make sure
+	 * Linux can schedule processes on this slave.
+	 */
+	idle = fork_idle(cpu);
+	if (IS_ERR(idle))
+		panic("failed fork for CPU %d\n", cpu);
+
+	prom_boot_secondary(cpu, idle);
+
+	/* XXXKW timeout */
+	while (!cpu_isset(cpu, cpu_callin_map))
+		udelay(100);
+
+	cpu_set(cpu, cpu_online_map);
+
+	return 0;
+}
+
+/*
+ * Called once for each "cpu_possible(cpu)".  Needs to spin up the cpu
+ * and keep control until "cpu_online(cpu)" is set.  Note: cpu is
+ * physical, not logical.
+ */
+int __devinit __cpu_up(unsigned int cpu)
+{
+	int ret;
+
+	/* Processor goes to start_secondary(), sets online flag */
+	ret = do_boot_cpu(cpu);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+/* Not really SMP stuff ... */
+int setup_profiling_timer(unsigned int multiplier)
+{
+	return 0;
+}
+
+static void flush_tlb_all_ipi(void *info)
+{
+	local_flush_tlb_all();
+}
+
+void flush_tlb_all(void)
+{
+	on_each_cpu(flush_tlb_all_ipi, 0, 1, 1);
+}
+
+static void flush_tlb_mm_ipi(void *mm)
+{
+	local_flush_tlb_mm((struct mm_struct *)mm);
+}
+
+/*
+ * The following tlb flush calls are invoked when old translations are
+ * being torn down, or pte attributes are changing. For single threaded
+ * address spaces, a new context is obtained on the current cpu, and tlb
+ * context on other cpus are invalidated to force a new context allocation
+ * at switch_mm time, should the mm ever be used on other cpus. For
+ * multithreaded address spaces, intercpu interrupts have to be sent.
+ * Another case where intercpu interrupts are required is when the target
+ * mm might be active on another cpu (eg debuggers doing the flushes on
+ * behalf of debugees, kswapd stealing pages from another process etc).
+ * Kanoj 07/00.
+ */
+
+void flush_tlb_mm(struct mm_struct *mm)
+{
+	preempt_disable();
+
+	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
+		smp_call_function(flush_tlb_mm_ipi, (void *)mm, 1, 1);
+	} else {
+		int i;
+		for (i = 0; i < num_online_cpus(); i++)
+			if (smp_processor_id() != i)
+				cpu_context(i, mm) = 0;
+	}
+	local_flush_tlb_mm(mm);
+
+	preempt_enable();
+}
+
+struct flush_tlb_data {
+	struct vm_area_struct *vma;
+	unsigned long addr1;
+	unsigned long addr2;
+};
+
+static void flush_tlb_range_ipi(void *info)
+{
+	struct flush_tlb_data *fd = (struct flush_tlb_data *)info;
+
+	local_flush_tlb_range(fd->vma, fd->addr1, fd->addr2);
+}
+
+void flush_tlb_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+{
+	struct mm_struct *mm = vma->vm_mm;
+
+	preempt_disable();
+	if ((atomic_read(&mm->mm_users) != 1) || (current->mm != mm)) {
+		struct flush_tlb_data fd;
+
+		fd.vma = vma;
+		fd.addr1 = start;
+		fd.addr2 = end;
+		smp_call_function(flush_tlb_range_ipi, (void *)&fd, 1, 1);
+	} else {
+		int i;
+		for (i = 0; i < num_online_cpus(); i++)
+			if (smp_processor_id() != i)
+				cpu_context(i, mm) = 0;
+	}
+	local_flush_tlb_range(vma, start, end);
+	preempt_enable();
+}
+
+static void flush_tlb_kernel_range_ipi(void *info)
+{
+	struct flush_tlb_data *fd = (struct flush_tlb_data *)info;
+
+	local_flush_tlb_kernel_range(fd->addr1, fd->addr2);
+}
+
+void flush_tlb_kernel_range(unsigned long start, unsigned long end)
+{
+	struct flush_tlb_data fd;
+
+	fd.addr1 = start;
+	fd.addr2 = end;
+	on_each_cpu(flush_tlb_kernel_range_ipi, (void *)&fd, 1, 1);
+}
+
+static void flush_tlb_page_ipi(void *info)
+{
+	struct flush_tlb_data *fd = (struct flush_tlb_data *)info;
+
+	local_flush_tlb_page(fd->vma, fd->addr1);
+}
+
+void flush_tlb_page(struct vm_area_struct *vma, unsigned long page)
+{
+	preempt_disable();
+	if ((atomic_read(&vma->vm_mm->mm_users) != 1) || (current->mm != vma->vm_mm)) {
+		struct flush_tlb_data fd;
+
+		fd.vma = vma;
+		fd.addr1 = page;
+		smp_call_function(flush_tlb_page_ipi, (void *)&fd, 1, 1);
+	} else {
+		int i;
+		for (i = 0; i < num_online_cpus(); i++)
+			if (smp_processor_id() != i)
+				cpu_context(i, vma->vm_mm) = 0;
+	}
+	local_flush_tlb_page(vma, page);
+	preempt_enable();
+}
+
+static void flush_tlb_one_ipi(void *info)
+{
+	unsigned long vaddr = (unsigned long) info;
+
+	local_flush_tlb_one(vaddr);
+}
+
+void flush_tlb_one(unsigned long vaddr)
+{
+	smp_call_function(flush_tlb_one_ipi, (void *) vaddr, 1, 1);
+	local_flush_tlb_one(vaddr);
+}
+
+EXPORT_SYMBOL(flush_tlb_page);
+EXPORT_SYMBOL(flush_tlb_one);
+EXPORT_SYMBOL(cpu_data);
+EXPORT_SYMBOL(synchronize_irq);
