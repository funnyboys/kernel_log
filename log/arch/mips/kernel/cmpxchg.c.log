commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/mips/kernel/cmpxchg.c b/arch/mips/kernel/cmpxchg.c
index 6b2a4a902a98..89107deb03fc 100644
--- a/arch/mips/kernel/cmpxchg.c
+++ b/arch/mips/kernel/cmpxchg.c
@@ -1,11 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Copyright (C) 2017 Imagination Technologies
  * Author: Paul Burton <paul.burton@mips.com>
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License as published by the
- * Free Software Foundation;  either version 2 of the  License, or (at your
- * option) any later version.
  */
 
 #include <linux/bitops.h>

commit 94ee12b507db8b5876e31c9d6c9d84f556a4b49f
Author: Michael Clark <michaeljclark@mac.com>
Date:   Mon Feb 11 17:38:29 2019 +1300

    MIPS: fix truncation in __cmpxchg_small for short values
    
    __cmpxchg_small erroneously uses u8 for load comparison which can
    be either char or short. This patch changes the local variable to
    u32 which is sufficiently sized, as the loaded value is already
    masked and shifted appropriately. Using an integer size avoids
    any unnecessary canonicalization from use of non native widths.
    
    This patch is part of a series that adapts the MIPS small word
    atomics code for xchg and cmpxchg on short and char to RISC-V.
    
    Cc: RISC-V Patches <patches@groups.riscv.org>
    Cc: Linux RISC-V <linux-riscv@lists.infradead.org>
    Cc: Linux MIPS <linux-mips@linux-mips.org>
    Signed-off-by: Michael Clark <michaeljclark@mac.com>
    [paul.burton@mips.com:
      - Fix varialble typo per Jonas Gorski.
      - Consolidate load variable with other declarations.]
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Fixes: 3ba7f44d2b19 ("MIPS: cmpxchg: Implement 1 byte & 2 byte cmpxchg()")
    Cc: stable@vger.kernel.org # v4.13+

diff --git a/arch/mips/kernel/cmpxchg.c b/arch/mips/kernel/cmpxchg.c
index 0b9535bc2c53..6b2a4a902a98 100644
--- a/arch/mips/kernel/cmpxchg.c
+++ b/arch/mips/kernel/cmpxchg.c
@@ -54,10 +54,9 @@ unsigned long __xchg_small(volatile void *ptr, unsigned long val, unsigned int s
 unsigned long __cmpxchg_small(volatile void *ptr, unsigned long old,
 			      unsigned long new, unsigned int size)
 {
-	u32 mask, old32, new32, load32;
+	u32 mask, old32, new32, load32, load;
 	volatile u32 *ptr32;
 	unsigned int shift;
-	u8 load;
 
 	/* Check that ptr is naturally aligned */
 	WARN_ON((unsigned long)ptr & (size - 1));

commit fb615d61b5583db92e3793709b97e35dc9499c2a
Author: Paul Burton <paul.burton@mips.com>
Date:   Wed Oct 25 17:04:33 2017 -0700

    Update MIPS email addresses
    
    MIPS will soon not be a part of Imagination Technologies, and as such
    many @imgtec.com email addresses will no longer be valid. This patch
    updates the addresses for those who:
    
     - Have 10 or more patches in mainline authored using an @imgtec.com
       email address, or any patches dated within the past year.
    
     - Are still with Imagination but leaving as part of the MIPS business
       unit, as determined from an internal email address list.
    
     - Haven't already updated their email address (ie. JamesH) or expressed
       a desire to be excluded (ie. Maciej).
    
     - Acked v2 or earlier of this patch, which leaves Deng-Cheng, Matt &
       myself.
    
    New addresses are of the form firstname.lastname@mips.com, and all
    verified against an internal email address list.  An entry is added to
    .mailmap for each person such that get_maintainer.pl will report the new
    addresses rather than @imgtec.com addresses which will soon be dead.
    
    Instances of the affected addresses throughout the tree are then
    mechanically replaced with the new @mips.com address.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@imgtec.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@mips.com>
    Acked-by: Dengcheng Zhu <dengcheng.zhu@mips.com>
    Cc: Matt Redfearn <matt.redfearn@imgtec.com>
    Cc: Matt Redfearn <matt.redfearn@mips.com>
    Acked-by: Matt Redfearn <matt.redfearn@mips.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: trivial@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/cmpxchg.c b/arch/mips/kernel/cmpxchg.c
index 7730f1d3434f..0b9535bc2c53 100644
--- a/arch/mips/kernel/cmpxchg.c
+++ b/arch/mips/kernel/cmpxchg.c
@@ -1,6 +1,6 @@
 /*
  * Copyright (C) 2017 Imagination Technologies
- * Author: Paul Burton <paul.burton@imgtec.com>
+ * Author: Paul Burton <paul.burton@mips.com>
  *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License as published by the

commit 3ba7f44d2b19166b34031db48ce613d1bddbd384
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jun 9 17:26:40 2017 -0700

    MIPS: cmpxchg: Implement 1 byte & 2 byte cmpxchg()
    
    Implement support for 1 & 2 byte cmpxchg() using read-modify-write atop
    a 4 byte cmpxchg(). This allows us to support these atomic operations
    despite the MIPS ISA only providing 4 & 8 byte atomic operations.
    
    This is required in order to support queued rwlocks (qrwlock) in a later
    patch, since these make use of a 1 byte cmpxchg() in their slow path.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/16355/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/cmpxchg.c b/arch/mips/kernel/cmpxchg.c
index 5acfbf9fb2c5..7730f1d3434f 100644
--- a/arch/mips/kernel/cmpxchg.c
+++ b/arch/mips/kernel/cmpxchg.c
@@ -50,3 +50,60 @@ unsigned long __xchg_small(volatile void *ptr, unsigned long val, unsigned int s
 
 	return (load32 & mask) >> shift;
 }
+
+unsigned long __cmpxchg_small(volatile void *ptr, unsigned long old,
+			      unsigned long new, unsigned int size)
+{
+	u32 mask, old32, new32, load32;
+	volatile u32 *ptr32;
+	unsigned int shift;
+	u8 load;
+
+	/* Check that ptr is naturally aligned */
+	WARN_ON((unsigned long)ptr & (size - 1));
+
+	/* Mask inputs to the correct size. */
+	mask = GENMASK((size * BITS_PER_BYTE) - 1, 0);
+	old &= mask;
+	new &= mask;
+
+	/*
+	 * Calculate a shift & mask that correspond to the value we wish to
+	 * compare & exchange within the naturally aligned 4 byte integer
+	 * that includes it.
+	 */
+	shift = (unsigned long)ptr & 0x3;
+	if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+		shift ^= sizeof(u32) - size;
+	shift *= BITS_PER_BYTE;
+	mask <<= shift;
+
+	/*
+	 * Calculate a pointer to the naturally aligned 4 byte integer that
+	 * includes our byte of interest, and load its value.
+	 */
+	ptr32 = (volatile u32 *)((unsigned long)ptr & ~0x3);
+	load32 = *ptr32;
+
+	while (true) {
+		/*
+		 * Ensure the byte we want to exchange matches the expected
+		 * old value, and if not then bail.
+		 */
+		load = (load32 & mask) >> shift;
+		if (load != old)
+			return load;
+
+		/*
+		 * Calculate the old & new values of the naturally aligned
+		 * 4 byte integer that include the byte we want to exchange.
+		 * Attempt to exchange the old value for the new value, and
+		 * return if we succeed.
+		 */
+		old32 = (load32 & ~mask) | (old << shift);
+		new32 = (load32 & ~mask) | (new << shift);
+		load32 = cmpxchg(ptr32, old32, new32);
+		if (load32 == old32)
+			return old;
+	}
+}

commit b70eb30056dc84568f3d32440d9be6a558025843
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jun 9 17:26:39 2017 -0700

    MIPS: cmpxchg: Implement 1 byte & 2 byte xchg()
    
    Implement 1 & 2 byte xchg() using read-modify-write atop a 4 byte
    cmpxchg(). This allows us to support these atomic operations despite the
    MIPS ISA only providing for 4 & 8 byte atomic operations.
    
    This is required in order to support queued spinlocks (qspinlock) in a
    later patch, since these make use of a 2 byte xchg() in their slow path.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/16354/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/cmpxchg.c b/arch/mips/kernel/cmpxchg.c
new file mode 100644
index 000000000000..5acfbf9fb2c5
--- /dev/null
+++ b/arch/mips/kernel/cmpxchg.c
@@ -0,0 +1,52 @@
+/*
+ * Copyright (C) 2017 Imagination Technologies
+ * Author: Paul Burton <paul.burton@imgtec.com>
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ */
+
+#include <linux/bitops.h>
+#include <asm/cmpxchg.h>
+
+unsigned long __xchg_small(volatile void *ptr, unsigned long val, unsigned int size)
+{
+	u32 old32, new32, load32, mask;
+	volatile u32 *ptr32;
+	unsigned int shift;
+
+	/* Check that ptr is naturally aligned */
+	WARN_ON((unsigned long)ptr & (size - 1));
+
+	/* Mask value to the correct size. */
+	mask = GENMASK((size * BITS_PER_BYTE) - 1, 0);
+	val &= mask;
+
+	/*
+	 * Calculate a shift & mask that correspond to the value we wish to
+	 * exchange within the naturally aligned 4 byte integerthat includes
+	 * it.
+	 */
+	shift = (unsigned long)ptr & 0x3;
+	if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+		shift ^= sizeof(u32) - size;
+	shift *= BITS_PER_BYTE;
+	mask <<= shift;
+
+	/*
+	 * Calculate a pointer to the naturally aligned 4 byte integer that
+	 * includes our byte of interest, and load its value.
+	 */
+	ptr32 = (volatile u32 *)((unsigned long)ptr & ~0x3);
+	load32 = *ptr32;
+
+	do {
+		old32 = load32;
+		new32 = (load32 & ~mask) | (val << shift);
+		load32 = cmpxchg(ptr32, old32, new32);
+	} while (load32 != old32);
+
+	return (load32 & mask) >> shift;
+}
