commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 3adb7354bc01..242dc5e83847 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -94,7 +94,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	struct vm_area_struct *vma;
 	int ret;
 
-	if (down_write_killable(&mm->mmap_sem))
+	if (mmap_write_lock_killable(mm))
 		return -EINTR;
 
 	if (IS_ENABLED(CONFIG_MIPS_FP_SUPPORT)) {
@@ -187,6 +187,6 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	ret = 0;
 
 out:
-	up_write(&mm->mmap_sem);
+	mmap_write_unlock(mm);
 	return ret;
 }

commit aebdc6ff3b2e79327d8eff7c0e2d835bb1d3221f
Author: Yousong Zhou <yszhou4tech@gmail.com>
Date:   Tue Mar 24 23:27:51 2020 +0800

    MIPS: Exclude more dsemul code when CONFIG_MIPS_FP_SUPPORT=n
    
    This furthers what commit 42b10815d559 ("MIPS: Don't compile math-emu
    when CONFIG_MIPS_FP_SUPPORT=n") has done
    
    Signed-off-by: Yousong Zhou <yszhou4tech@gmail.com>
    Signed-off-by: Thomas Bogendoerfer <tsbogend@alpha.franken.de>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index bc35f8499111..3adb7354bc01 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -71,10 +71,12 @@ subsys_initcall(init_vdso);
 
 static unsigned long vdso_base(void)
 {
-	unsigned long base;
+	unsigned long base = STACK_TOP;
 
-	/* Skip the delay slot emulation page */
-	base = STACK_TOP + PAGE_SIZE;
+	if (IS_ENABLED(CONFIG_MIPS_FP_SUPPORT)) {
+		/* Skip the delay slot emulation page */
+		base += PAGE_SIZE;
+	}
 
 	if (current->flags & PF_RANDOMIZE) {
 		base += get_random_int() & (VDSO_RANDOMIZE_SIZE - 1);
@@ -95,14 +97,16 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	if (down_write_killable(&mm->mmap_sem))
 		return -EINTR;
 
-	/* Map delay slot emulation page */
-	base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,
-			   VM_READ | VM_EXEC |
-			   VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC,
-			   0, NULL);
-	if (IS_ERR_VALUE(base)) {
-		ret = base;
-		goto out;
+	if (IS_ENABLED(CONFIG_MIPS_FP_SUPPORT)) {
+		/* Map delay slot emulation page */
+		base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,
+				VM_READ | VM_EXEC |
+				VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC,
+				0, NULL);
+		if (IS_ERR_VALUE(base)) {
+			ret = base;
+			goto out;
+		}
 	}
 
 	/*

commit 24640f233b466051ad3a5d2786d2951e43026c9d
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Jun 21 10:52:46 2019 +0100

    mips: Add support for generic vDSO
    
    The mips vDSO library requires some adaptations to take advantage of the
    newly introduced generic vDSO library.
    
    Introduce the following changes:
     - Modification of vdso.c to be compliant with the common vdso datapage
     - Use of lib/vdso for gettimeofday
    
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paul.burton@mips.com>
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    [paul.burton@mips.com: Prepend $(src) to config-n32-o32-env.c path.]
    Signed-off-by: Paul Burton <paul.burton@mips.com>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 3a372686ffca..bc35f8499111 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -20,9 +20,12 @@
 #include <asm/mips-cps.h>
 #include <asm/page.h>
 #include <asm/vdso.h>
+#include <vdso/helpers.h>
+#include <vdso/vsyscall.h>
 
 /* Kernel-provided data used by the VDSO. */
-static union mips_vdso_data vdso_data __page_aligned_data;
+static union mips_vdso_data mips_vdso_data __page_aligned_data;
+struct vdso_data *vdso_data = mips_vdso_data.data;
 
 /*
  * Mapping for the VDSO data/GIC pages. The real pages are mapped manually, as
@@ -66,34 +69,6 @@ static int __init init_vdso(void)
 }
 subsys_initcall(init_vdso);
 
-void update_vsyscall(struct timekeeper *tk)
-{
-	vdso_data_write_begin(&vdso_data);
-
-	vdso_data.xtime_sec = tk->xtime_sec;
-	vdso_data.xtime_nsec = tk->tkr_mono.xtime_nsec;
-	vdso_data.wall_to_mono_sec = tk->wall_to_monotonic.tv_sec;
-	vdso_data.wall_to_mono_nsec = tk->wall_to_monotonic.tv_nsec;
-	vdso_data.cs_shift = tk->tkr_mono.shift;
-
-	vdso_data.clock_mode = tk->tkr_mono.clock->archdata.vdso_clock_mode;
-	if (vdso_data.clock_mode != VDSO_CLOCK_NONE) {
-		vdso_data.cs_mult = tk->tkr_mono.mult;
-		vdso_data.cs_cycle_last = tk->tkr_mono.cycle_last;
-		vdso_data.cs_mask = tk->tkr_mono.mask;
-	}
-
-	vdso_data_write_end(&vdso_data);
-}
-
-void update_vsyscall_tz(void)
-{
-	if (vdso_data.clock_mode != VDSO_CLOCK_NONE) {
-		vdso_data.tz_minuteswest = sys_tz.tz_minuteswest;
-		vdso_data.tz_dsttime = sys_tz.tz_dsttime;
-	}
-}
-
 static unsigned long vdso_base(void)
 {
 	unsigned long base;
@@ -163,7 +138,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	 */
 	if (cpu_has_dc_aliases) {
 		base = __ALIGN_MASK(base, shm_align_mask);
-		base += ((unsigned long)&vdso_data - gic_size) & shm_align_mask;
+		base += ((unsigned long)vdso_data - gic_size) & shm_align_mask;
 	}
 
 	data_addr = base + gic_size;
@@ -189,7 +164,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 
 	/* Map data page. */
 	ret = remap_pfn_range(vma, data_addr,
-			      virt_to_phys(&vdso_data) >> PAGE_SHIFT,
+			      virt_to_phys(vdso_data) >> PAGE_SHIFT,
 			      PAGE_SIZE, PAGE_READONLY);
 	if (ret)
 		goto out;

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 9df3ebdc7b0f..3a372686ffca 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -1,11 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Copyright (C) 2015 Imagination Technologies
  * Author: Alex Smith <alex.smith@imgtec.com>
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms of the GNU General Public License as published by the
- * Free Software Foundation;  either version 2 of the  License, or (at your
- * option) any later version.
  */
 
 #include <linux/binfmts.h>

commit adcc81f148d733b7e8e641300c5590a2cdc13bf3
Author: Paul Burton <paul.burton@mips.com>
Date:   Thu Dec 20 17:45:43 2018 +0000

    MIPS: math-emu: Write-protect delay slot emulation pages
    
    Mapping the delay slot emulation page as both writeable & executable
    presents a security risk, in that if an exploit can write to & jump into
    the page then it can be used as an easy way to execute arbitrary code.
    
    Prevent this by mapping the page read-only for userland, and using
    access_process_vm() with the FOLL_FORCE flag to write to it from
    mips_dsemul().
    
    This will likely be less efficient due to copy_to_user_page() performing
    cache maintenance on a whole page, rather than a single line as in the
    previous use of flush_cache_sigtramp(). However this delay slot
    emulation code ought not to be running in any performance critical paths
    anyway so this isn't really a problem, and we can probably do better in
    copy_to_user_page() anyway in future.
    
    A major advantage of this approach is that the fix is small & simple to
    backport to stable kernels.
    
    Reported-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Fixes: 432c6bacbd0c ("MIPS: Use per-mm page to execute branch delay slot instructions")
    Cc: stable@vger.kernel.org # v4.8+
    Cc: linux-mips@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Rich Felker <dalias@libc.org>
    Cc: David Daney <david.daney@cavium.com>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 48a9c6b90e07..9df3ebdc7b0f 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -126,8 +126,8 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 
 	/* Map delay slot emulation page */
 	base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,
-			   VM_READ|VM_WRITE|VM_EXEC|
-			   VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
+			   VM_READ | VM_EXEC |
+			   VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC,
 			   0, NULL);
 	if (IS_ERR_VALUE(base)) {
 		ret = base;

commit ea7e0480a4b695d0aa6b3fa99bd658a003122113
Author: Paul Burton <paul.burton@mips.com>
Date:   Tue Sep 25 15:51:26 2018 -0700

    MIPS: VDSO: Always map near top of user memory
    
    When using the legacy mmap layout, for example triggered using ulimit -s
    unlimited, get_unmapped_area() fills memory from bottom to top starting
    from a fairly low address near TASK_UNMAPPED_BASE.
    
    This placement is suboptimal if the user application wishes to allocate
    large amounts of heap memory using the brk syscall. With the VDSO being
    located low in the user's virtual address space, the amount of space
    available for access using brk is limited much more than it was prior to
    the introduction of the VDSO.
    
    For example:
    
      # ulimit -s unlimited; cat /proc/self/maps
      00400000-004ec000 r-xp 00000000 08:00 71436      /usr/bin/coreutils
      004fc000-004fd000 rwxp 000ec000 08:00 71436      /usr/bin/coreutils
      004fd000-0050f000 rwxp 00000000 00:00 0
      00cc3000-00ce4000 rwxp 00000000 00:00 0          [heap]
      2ab96000-2ab98000 r--p 00000000 00:00 0          [vvar]
      2ab98000-2ab99000 r-xp 00000000 00:00 0          [vdso]
      2ab99000-2ab9d000 rwxp 00000000 00:00 0
      ...
    
    Resolve this by adjusting STACK_TOP to reserve space for the VDSO &
    providing an address hint to get_unmapped_area() causing it to use this
    space even when using the legacy mmap layout.
    
    We reserve enough space for the VDSO, plus 1MB or 256MB for 32 bit & 64
    bit systems respectively within which we randomize the VDSO base
    address. Previously this randomization was taken care of by the mmap
    base address randomization performed by arch_mmap_rnd(). The 1MB & 256MB
    sizes are somewhat arbitrary but chosen such that we have some
    randomization without taking up too much of the user's virtual address
    space, which is often in short supply for 32 bit systems.
    
    With this the VDSO is always mapped at a high address, leaving lots of
    space for statically linked programs to make use of brk:
    
      # ulimit -s unlimited; cat /proc/self/maps
      00400000-004ec000 r-xp 00000000 08:00 71436      /usr/bin/coreutils
      004fc000-004fd000 rwxp 000ec000 08:00 71436      /usr/bin/coreutils
      004fd000-0050f000 rwxp 00000000 00:00 0
      00c28000-00c49000 rwxp 00000000 00:00 0          [heap]
      ...
      7f67c000-7f69d000 rwxp 00000000 00:00 0          [stack]
      7f7fc000-7f7fd000 rwxp 00000000 00:00 0
      7fcf1000-7fcf3000 r--p 00000000 00:00 0          [vvar]
      7fcf3000-7fcf4000 r-xp 00000000 00:00 0          [vdso]
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Reported-by: Huacai Chen <chenhc@lemote.com>
    Fixes: ebb5e78cc634 ("MIPS: Initial implementation of a VDSO")
    Cc: Huacai Chen <chenhc@lemote.com>
    Cc: linux-mips@linux-mips.org
    Cc: stable@vger.kernel.org # v4.4+

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 8f845f6e5f42..48a9c6b90e07 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -15,6 +15,7 @@
 #include <linux/ioport.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
+#include <linux/random.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/timekeeper_internal.h>
@@ -97,6 +98,21 @@ void update_vsyscall_tz(void)
 	}
 }
 
+static unsigned long vdso_base(void)
+{
+	unsigned long base;
+
+	/* Skip the delay slot emulation page */
+	base = STACK_TOP + PAGE_SIZE;
+
+	if (current->flags & PF_RANDOMIZE) {
+		base += get_random_int() & (VDSO_RANDOMIZE_SIZE - 1);
+		base = PAGE_ALIGN(base);
+	}
+
+	return base;
+}
+
 int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 {
 	struct mips_vdso_image *image = current->thread.abi->vdso;
@@ -137,7 +153,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	if (cpu_has_dc_aliases)
 		size += shm_align_mask + 1;
 
-	base = get_unmapped_area(NULL, 0, size, 0, 0);
+	base = get_unmapped_area(NULL, vdso_base(), size, 0, 0);
 	if (IS_ERR_VALUE(base)) {
 		ret = base;
 		goto out;

commit 0f02cfbc3d9e413d450d8d0fd660077c23f67eff
Author: Paul Burton <paul.burton@mips.com>
Date:   Thu Aug 30 11:01:21 2018 -0700

    MIPS: VDSO: Match data page cache colouring when D$ aliases
    
    When a system suffers from dcache aliasing a user program may observe
    stale VDSO data from an aliased cache line. Notably this can break the
    expectation that clock_gettime(CLOCK_MONOTONIC, ...) is, as its name
    suggests, monotonic.
    
    In order to ensure that users observe updates to the VDSO data page as
    intended, align the user mappings of the VDSO data page such that their
    cache colouring matches that of the virtual address range which the
    kernel will use to update the data page - typically its unmapped address
    within kseg0.
    
    This ensures that we don't introduce aliasing cache lines for the VDSO
    data page, and therefore that userland will observe updates without
    requiring cache invalidation.
    
    Signed-off-by: Paul Burton <paul.burton@mips.com>
    Reported-by: Hauke Mehrtens <hauke@hauke-m.de>
    Reported-by: Rene Nielsen <rene.nielsen@microsemi.com>
    Reported-by: Alexandre Belloni <alexandre.belloni@bootlin.com>
    Fixes: ebb5e78cc634 ("MIPS: Initial implementation of a VDSO")
    Patchwork: https://patchwork.linux-mips.org/patch/20344/
    Tested-by: Alexandre Belloni <alexandre.belloni@bootlin.com>
    Tested-by: Hauke Mehrtens <hauke@hauke-m.de>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: linux-mips@linux-mips.org
    Cc: stable@vger.kernel.org # v4.4+

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 019035d7225c..8f845f6e5f42 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -13,6 +13,7 @@
 #include <linux/err.h>
 #include <linux/init.h>
 #include <linux/ioport.h>
+#include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
@@ -20,6 +21,7 @@
 
 #include <asm/abi.h>
 #include <asm/mips-cps.h>
+#include <asm/page.h>
 #include <asm/vdso.h>
 
 /* Kernel-provided data used by the VDSO. */
@@ -128,12 +130,30 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	vvar_size = gic_size + PAGE_SIZE;
 	size = vvar_size + image->size;
 
+	/*
+	 * Find a region that's large enough for us to perform the
+	 * colour-matching alignment below.
+	 */
+	if (cpu_has_dc_aliases)
+		size += shm_align_mask + 1;
+
 	base = get_unmapped_area(NULL, 0, size, 0, 0);
 	if (IS_ERR_VALUE(base)) {
 		ret = base;
 		goto out;
 	}
 
+	/*
+	 * If we suffer from dcache aliasing, ensure that the VDSO data page
+	 * mapping is coloured the same as the kernel's mapping of that memory.
+	 * This ensures that when the kernel updates the VDSO data userland
+	 * will observe it without requiring cache invalidations.
+	 */
+	if (cpu_has_dc_aliases) {
+		base = __ALIGN_MASK(base, shm_align_mask);
+		base += ((unsigned long)&vdso_data - gic_size) & shm_align_mask;
+	}
+
 	data_addr = base + gic_size;
 	vdso_addr = data_addr + PAGE_SIZE;
 

commit 00578cd864d45ae4b8fa3f684f8d6f783dd8d15d
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Sat Aug 12 21:36:30 2017 -0700

    MIPS: VDSO: Drop gic_get_usm_range() usage
    
    We don't really need gic_get_usm_range() to abstract discovery of the
    address of the GIC user-visible section now that we have access to its
    base address globally.
    
    Switch to calculating it ourselves, which will allow us to stop
    requiring the irqchip driver to care about a counter exposed to userland
    for use via the VDSO.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/17040/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 093517e85a6c..019035d7225c 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -13,13 +13,13 @@
 #include <linux/err.h>
 #include <linux/init.h>
 #include <linux/ioport.h>
-#include <linux/irqchip/mips-gic.h>
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/timekeeper_internal.h>
 
 #include <asm/abi.h>
+#include <asm/mips-cps.h>
 #include <asm/vdso.h>
 
 /* Kernel-provided data used by the VDSO. */
@@ -99,9 +99,8 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 {
 	struct mips_vdso_image *image = current->thread.abi->vdso;
 	struct mm_struct *mm = current->mm;
-	unsigned long gic_size, vvar_size, size, base, data_addr, vdso_addr;
+	unsigned long gic_size, vvar_size, size, base, data_addr, vdso_addr, gic_pfn;
 	struct vm_area_struct *vma;
-	struct resource gic_res;
 	int ret;
 
 	if (down_write_killable(&mm->mmap_sem))
@@ -125,7 +124,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	 * only map a page even though the total area is 64K, as we only need
 	 * the counter registers at the start.
 	 */
-	gic_size = gic_present ? PAGE_SIZE : 0;
+	gic_size = mips_gic_present() ? PAGE_SIZE : 0;
 	vvar_size = gic_size + PAGE_SIZE;
 	size = vvar_size + image->size;
 
@@ -148,13 +147,9 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 
 	/* Map GIC user page. */
 	if (gic_size) {
-		ret = gic_get_usm_range(&gic_res);
-		if (ret)
-			goto out;
+		gic_pfn = virt_to_phys(mips_gic_base + MIPS_GIC_USER_OFS) >> PAGE_SHIFT;
 
-		ret = io_remap_pfn_range(vma, base,
-					 gic_res.start >> PAGE_SHIFT,
-					 gic_size,
+		ret = io_remap_pfn_range(vma, base, gic_pfn, gic_size,
 					 pgprot_noncached(PAGE_READONLY));
 		if (ret)
 			goto out;

commit 897ab3e0c49e24b62e2d54d165c7afec6bbca65b
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Fri Feb 24 14:58:22 2017 -0800

    userfaultfd: non-cooperative: add event for memory unmaps
    
    When a non-cooperative userfaultfd monitor copies pages in the
    background, it may encounter regions that were already unmapped.
    Addition of UFFD_EVENT_UNMAP allows the uffd monitor to track precisely
    changes in the virtual memory layout.
    
    Since there might be different uffd contexts for the affected VMAs, we
    first should create a temporary representation for the unmap event for
    each uffd context and then notify them one by one to the appropriate
    userfault file descriptors.
    
    The event notification occurs after the mmap_sem has been released.
    
    [arnd@arndb.de: fix nommu build]
      Link: http://lkml.kernel.org/r/20170203165141.3665284-1-arnd@arndb.de
    [mhocko@suse.com: fix nommu build]
      Link: http://lkml.kernel.org/r/20170202091503.GA22823@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/1485542673-24387-3-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Hillf Danton <hillf.zj@alibaba-inc.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: "Dr. David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Pavel Emelyanov <xemul@virtuozzo.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index f9dbfb14af33..093517e85a6c 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -111,7 +111,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,
 			   VM_READ|VM_WRITE|VM_EXEC|
 			   VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
-			   0);
+			   0, NULL);
 	if (IS_ERR_VALUE(base)) {
 		ret = base;
 		goto out;

commit 554af0c396380baf416f54c439b99b495180b2f4
Author: James Hogan <james.hogan@imgtec.com>
Date:   Wed Sep 7 13:37:01 2016 +0100

    MIPS: vDSO: Fix Malta EVA mapping to vDSO page structs
    
    The page structures associated with the vDSO pages in the kernel image
    are calculated using virt_to_page(), which uses __pa() under the hood to
    find the pfn associated with the virtual address. The vDSO data pointers
    however point to kernel symbols, so __pa_symbol() should really be used
    instead.
    
    Since there is no equivalent to virt_to_page() which uses __pa_symbol(),
    fix init_vdso_image() to work directly with pfns, calculated with
    __phys_to_pfn(__pa_symbol(...)).
    
    This issue broke the Malta Enhanced Virtual Addressing (EVA)
    configuration which has a non-default implementation of __pa_symbol().
    This is because it uses a physical alias so that the kernel executes
    from KSeg0 (VA 0x80000000 -> PA 0x00000000), while RAM is provided to
    the kernel in the KUSeg range (VA 0x00000000 -> PA 0x80000000) which
    uses the same underlying RAM.
    
    Since there are no page structures associated with the low physical
    address region, some arbitrary kernel memory would be interpreted as a
    page structure for the vDSO pages and badness ensues.
    
    Fixes: ebb5e78cc634 ("MIPS: Initial implementation of a VDSO")
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Cc: <stable@vger.kernel.org> # 4.4.x-
    Patchwork: https://patchwork.linux-mips.org/patch/14229/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 9abe447a4b48..f9dbfb14af33 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -39,16 +39,16 @@ static struct vm_special_mapping vdso_vvar_mapping = {
 static void __init init_vdso_image(struct mips_vdso_image *image)
 {
 	unsigned long num_pages, i;
+	unsigned long data_pfn;
 
 	BUG_ON(!PAGE_ALIGNED(image->data));
 	BUG_ON(!PAGE_ALIGNED(image->size));
 
 	num_pages = image->size / PAGE_SIZE;
 
-	for (i = 0; i < num_pages; i++) {
-		image->mapping.pages[i] =
-			virt_to_page(image->data + (i * PAGE_SIZE));
-	}
+	data_pfn = __phys_to_pfn(__pa_symbol(image->data));
+	for (i = 0; i < num_pages; i++)
+		image->mapping.pages[i] = pfn_to_page(data_pfn + i);
 }
 
 static int __init init_vdso(void)

commit 432c6bacbd0c16ec210c43da411ccc3855c4c010
Author: Paul Burton <paul.burton@imgtec.com>
Date:   Fri Jul 8 11:06:19 2016 +0100

    MIPS: Use per-mm page to execute branch delay slot instructions
    
    In some cases the kernel needs to execute an instruction from the delay
    slot of an emulated branch instruction. These cases include:
    
      - Emulated floating point branch instructions (bc1[ft]l?) for systems
        which don't include an FPU, or upon which the kernel is run with the
        "nofpu" parameter.
    
      - MIPSr6 systems running binaries targeting older revisions of the
        architecture, which may include branch instructions whose encodings
        are no longer valid in MIPSr6.
    
    Executing instructions from such delay slots is done by writing the
    instruction to memory followed by a trap, as part of an "emuframe", and
    executing it. This avoids the requirement of an emulator for the entire
    MIPS instruction set. Prior to this patch such emuframes are written to
    the user stack and executed from there.
    
    This patch moves FP branch delay emuframes off of the user stack and
    into a per-mm page. Allocating a page per-mm leaves userland with access
    to only what it had access to previously, and compared to other
    solutions is relatively simple.
    
    When a thread requires a delay slot emulation, it is allocated a frame.
    A thread may only have one frame allocated at any one time, since it may
    only ever be executing one instruction at any one time. In order to
    ensure that we can free up allocated frame later, its index is recorded
    in struct thread_struct. In the typical case, after executing the delay
    slot instruction we'll execute a break instruction with the BRK_MEMU
    code. This traps back to the kernel & leads to a call to do_dsemulret
    which frees the allocated frame & moves the user PC back to the
    instruction that would have executed following the emulated branch.
    In some cases the delay slot instruction may be invalid, such as a
    branch, or may trigger an exception. In these cases the BRK_MEMU break
    instruction will not be hit. In order to ensure that frames are freed
    this patch introduces dsemul_thread_cleanup() and calls it to free any
    allocated frame upon thread exit. If the instruction generated an
    exception & leads to a signal being delivered to the thread, or indeed
    if a signal simply happens to be delivered to the thread whilst it is
    executing from the struct emuframe, then we need to take care to exit
    the frame appropriately. This is done by either rolling back the user PC
    to the branch or advancing it to the continuation PC prior to signal
    delivery, using dsemul_thread_rollback(). If this were not done then a
    sigreturn would return to the struct emuframe, and if that frame had
    meanwhile been used in response to an emulated branch instruction within
    the signal handler then we would execute the wrong user code.
    
    Whilst a user could theoretically place something like a compact branch
    to self in a delay slot and cause their thread to become stuck in an
    infinite loop with the frame never being deallocated, this would:
    
      - Only affect the users single process.
    
      - Be architecturally invalid since there would be a branch in the
        delay slot, which is forbidden.
    
      - Be extremely unlikely to happen by mistake, and provide a program
        with no more ability to harm the system than a simple infinite loop
        would.
    
    If a thread requires a delay slot emulation & no frame is available to
    it (ie. the process has enough other threads that all frames are
    currently in use) then the thread joins a waitqueue. It will sleep until
    a frame is freed by another thread in the process.
    
    Since we now know whether a thread has an allocated frame due to our
    tracking of its index, the cookie field of struct emuframe is removed as
    we can be more certain whether we have a valid frame. Since a thread may
    only ever have a single frame at any given time, the epc field of struct
    emuframe is also removed & the PC to continue from is instead stored in
    struct thread_struct. Together these changes simplify & shrink struct
    emuframe somewhat, allowing twice as many frames to fit into the page
    allocated for them.
    
    The primary benefit of this patch is that we are now free to mark the
    user stack non-executable where that is possible.
    
    Signed-off-by: Paul Burton <paul.burton@imgtec.com>
    Cc: Leonid Yegoshin <leonid.yegoshin@imgtec.com>
    Cc: Maciej Rozycki <maciej.rozycki@imgtec.com>
    Cc: Faraz Shahbazker <faraz.shahbazker@imgtec.com>
    Cc: Raghu Gandham <raghu.gandham@imgtec.com>
    Cc: Matthew Fortune <matthew.fortune@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/13764/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 54e1663ce639..9abe447a4b48 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -107,6 +107,16 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	if (down_write_killable(&mm->mmap_sem))
 		return -EINTR;
 
+	/* Map delay slot emulation page */
+	base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,
+			   VM_READ|VM_WRITE|VM_EXEC|
+			   VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
+			   0);
+	if (IS_ERR_VALUE(base)) {
+		ret = base;
+		goto out;
+	}
+
 	/*
 	 * Determine total area size. This includes the VDSO data itself, the
 	 * data page, and the GIC user page if present. Always create a mapping

commit 69048176078adda4087a648c9b1812ddd800fad1
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 23 16:25:54 2016 -0700

    vdso: make arch_setup_additional_pages wait for mmap_sem for write killable
    
    most architectures are relying on mmap_sem for write in their
    arch_setup_additional_pages.  If the waiting task gets killed by the oom
    killer it would block oom_reaper from asynchronous address space reclaim
    and reduce the chances of timely OOM resolving.  Wait for the lock in
    the killable mode and return with EINTR if the task got killed while
    waiting.
    
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Andy Lutomirski <luto@amacapital.net> [x86 vdso]
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 975e99759bab..54e1663ce639 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -104,7 +104,8 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	struct resource gic_res;
 	int ret;
 
-	down_write(&mm->mmap_sem);
+	if (down_write_killable(&mm->mmap_sem))
+		return -EINTR;
 
 	/*
 	 * Determine total area size. This includes the VDSO data itself, the

commit a7f4df4e21dd8a8dab96e88acd2c9c5017b83fc6
Author: Alex Smith <alex.smith@imgtec.com>
Date:   Wed Oct 21 09:57:44 2015 +0100

    MIPS: VDSO: Add implementations of gettimeofday() and clock_gettime()
    
    Add user-mode implementations of gettimeofday() and clock_gettime() to
    the VDSO. This is currently usable with 2 clocksources: the CP0 count
    register, which is accessible to user-mode via RDHWR on R2 and later
    cores, or the MIPS Global Interrupt Controller (GIC) timer, which
    provides a "user-mode visible" section containing a mirror of its
    counter registers. This section must be mapped into user memory, which
    is done below the VDSO data page.
    
    When a supported clocksource is not in use, the VDSO functions will
    return -ENOSYS, which causes libc to fall back on the standard syscall
    path.
    
    When support for neither of these clocksources is compiled into the
    kernel at all, the VDSO still provides clock_gettime(), as the coarse
    realtime/monotonic clocks can still be implemented. However,
    gettimeofday() is not provided in this case as nothing can be done
    without a suitable clocksource. This causes the symbol lookup to fail
    in libc and it will then always use the standard syscall path.
    
    This patch includes a workaround for a bug in QEMU which results in
    RDHWR on the CP0 count register always returning a constant (incorrect)
    value. A fix for this has been submitted, and the workaround can be
    removed after the fix has been in stable releases for a reasonable
    amount of time.
    
    A simple performance test which calls gettimeofday() 1000 times in a
    loop and calculates the average execution time gives the following
    results on a Malta + I6400 (running at 20MHz):
    
     - Syscall:    ~31000 ns
     - VDSO (GIC): ~15000 ns
     - VDSO (CP0): ~9500 ns
    
    [markos.chandras@imgtec.com:
    - Minor code re-arrangements in order for mappings to be made
    in the order they appear to the process' address space.
    - Move do_{monotonic, realtime} outside of the MIPS_CLOCK_VSYSCALL ifdef
    - Use gic_get_usm_range so we can do the GIC mapping in the
    arch/mips/kernel/vdso instead of the GIC irqchip driver]
    
    Signed-off-by: Alex Smith <alex.smith@imgtec.com>
    Signed-off-by: Markos Chandras <markos.chandras@imgtec.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/11338/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 56cc3c4377fb..975e99759bab 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -12,9 +12,12 @@
 #include <linux/elf.h>
 #include <linux/err.h>
 #include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/irqchip/mips-gic.h>
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/slab.h>
+#include <linux/timekeeper_internal.h>
 
 #include <asm/abi.h>
 #include <asm/vdso.h>
@@ -23,7 +26,7 @@
 static union mips_vdso_data vdso_data __page_aligned_data;
 
 /*
- * Mapping for the VDSO data pages. The real pages are mapped manually, as
+ * Mapping for the VDSO data/GIC pages. The real pages are mapped manually, as
  * what we map and where within the area they are mapped is determined at
  * runtime.
  */
@@ -64,25 +67,67 @@ static int __init init_vdso(void)
 }
 subsys_initcall(init_vdso);
 
+void update_vsyscall(struct timekeeper *tk)
+{
+	vdso_data_write_begin(&vdso_data);
+
+	vdso_data.xtime_sec = tk->xtime_sec;
+	vdso_data.xtime_nsec = tk->tkr_mono.xtime_nsec;
+	vdso_data.wall_to_mono_sec = tk->wall_to_monotonic.tv_sec;
+	vdso_data.wall_to_mono_nsec = tk->wall_to_monotonic.tv_nsec;
+	vdso_data.cs_shift = tk->tkr_mono.shift;
+
+	vdso_data.clock_mode = tk->tkr_mono.clock->archdata.vdso_clock_mode;
+	if (vdso_data.clock_mode != VDSO_CLOCK_NONE) {
+		vdso_data.cs_mult = tk->tkr_mono.mult;
+		vdso_data.cs_cycle_last = tk->tkr_mono.cycle_last;
+		vdso_data.cs_mask = tk->tkr_mono.mask;
+	}
+
+	vdso_data_write_end(&vdso_data);
+}
+
+void update_vsyscall_tz(void)
+{
+	if (vdso_data.clock_mode != VDSO_CLOCK_NONE) {
+		vdso_data.tz_minuteswest = sys_tz.tz_minuteswest;
+		vdso_data.tz_dsttime = sys_tz.tz_dsttime;
+	}
+}
+
 int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 {
 	struct mips_vdso_image *image = current->thread.abi->vdso;
 	struct mm_struct *mm = current->mm;
-	unsigned long base, vdso_addr;
+	unsigned long gic_size, vvar_size, size, base, data_addr, vdso_addr;
 	struct vm_area_struct *vma;
+	struct resource gic_res;
 	int ret;
 
 	down_write(&mm->mmap_sem);
 
-	base = get_unmapped_area(NULL, 0, PAGE_SIZE + image->size, 0, 0);
+	/*
+	 * Determine total area size. This includes the VDSO data itself, the
+	 * data page, and the GIC user page if present. Always create a mapping
+	 * for the GIC user area if the GIC is present regardless of whether it
+	 * is the current clocksource, in case it comes into use later on. We
+	 * only map a page even though the total area is 64K, as we only need
+	 * the counter registers at the start.
+	 */
+	gic_size = gic_present ? PAGE_SIZE : 0;
+	vvar_size = gic_size + PAGE_SIZE;
+	size = vvar_size + image->size;
+
+	base = get_unmapped_area(NULL, 0, size, 0, 0);
 	if (IS_ERR_VALUE(base)) {
 		ret = base;
 		goto out;
 	}
 
-	vdso_addr = base + PAGE_SIZE;
+	data_addr = base + gic_size;
+	vdso_addr = data_addr + PAGE_SIZE;
 
-	vma = _install_special_mapping(mm, base, PAGE_SIZE,
+	vma = _install_special_mapping(mm, base, vvar_size,
 				       VM_READ | VM_MAYREAD,
 				       &vdso_vvar_mapping);
 	if (IS_ERR(vma)) {
@@ -90,8 +135,22 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 		goto out;
 	}
 
+	/* Map GIC user page. */
+	if (gic_size) {
+		ret = gic_get_usm_range(&gic_res);
+		if (ret)
+			goto out;
+
+		ret = io_remap_pfn_range(vma, base,
+					 gic_res.start >> PAGE_SHIFT,
+					 gic_size,
+					 pgprot_noncached(PAGE_READONLY));
+		if (ret)
+			goto out;
+	}
+
 	/* Map data page. */
-	ret = remap_pfn_range(vma, base,
+	ret = remap_pfn_range(vma, data_addr,
 			      virt_to_phys(&vdso_data) >> PAGE_SHIFT,
 			      PAGE_SIZE, PAGE_READONLY);
 	if (ret)

commit ebb5e78cc63417a35254a791de66e1cc84f963cc
Author: Alex Smith <alex.smith@imgtec.com>
Date:   Wed Oct 21 09:54:38 2015 +0100

    MIPS: Initial implementation of a VDSO
    
    Add an initial implementation of a proper (i.e. an ELF shared library)
    VDSO. With this commit it does not export any symbols, it only replaces
    the current signal return trampoline page. A later commit will add user
    implementations of gettimeofday()/clock_gettime().
    
    To support both new toolchains and old ones which don't generate ABI
    flags section, we define its content manually and then use a tool
    (genvdso) to patch up the section to have the correct name and type.
    genvdso also extracts symbol offsets ({,rt_}sigreturn) needed by the
    kernel, and generates a C file containing a "struct mips_vdso_image"
    containing both the VDSO data and these offsets. This C file is
    compiled into the kernel.
    
    On 64-bit kernels we require a different VDSO for each supported ABI,
    so we may build up to 3 different VDSOs. The VDSO to use is selected by
    the mips_abi structure.
    
    A kernel/user shared data page is created and mapped below the VDSO
    image. This is currently empty, but will be used by the user time
    function implementations which are added later.
    
    [markos.chandras@imgtec.com:
    - Add more comments
    - Move abi detection in genvdso.h since it's the get_symbol function
    that needs it.
    - Add an R6 specific way to calculate the base address of VDSO in order
    to avoid the branch instruction which affects performance.
    - Do not patch .gnu.attributes since it's not needed for dynamic linking.
    - Simplify Makefile a little bit.
    - checkpatch fixes
    - Restrict VDSO support for binutils < 2.25 for pre-R6
    - Include atomic64.h for O32 variant on MIPS64]
    
    Signed-off-by: Alex Smith <alex.smith@imgtec.com>
    Signed-off-by: Markos Chandras <markos.chandras@imgtec.com>
    Cc: Matthew Fortune <matthew.fortune@imgtec.com>
    Cc: linux-mips@linux-mips.org
    Patchwork: https://patchwork.linux-mips.org/patch/11337/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index ed2a278722a9..56cc3c4377fb 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -1,122 +1,116 @@
 /*
- * This file is subject to the terms and conditions of the GNU General Public
- * License.  See the file "COPYING" in the main directory of this archive
- * for more details.
+ * Copyright (C) 2015 Imagination Technologies
+ * Author: Alex Smith <alex.smith@imgtec.com>
  *
- * Copyright (C) 2009, 2010 Cavium Networks, Inc.
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
  */
 
-
-#include <linux/kernel.h>
-#include <linux/err.h>
-#include <linux/sched.h>
-#include <linux/mm.h>
-#include <linux/init.h>
 #include <linux/binfmts.h>
 #include <linux/elf.h>
-#include <linux/vmalloc.h>
-#include <linux/unistd.h>
-#include <linux/random.h>
+#include <linux/err.h>
+#include <linux/init.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
 
+#include <asm/abi.h>
 #include <asm/vdso.h>
-#include <asm/uasm.h>
-#include <asm/processor.h>
+
+/* Kernel-provided data used by the VDSO. */
+static union mips_vdso_data vdso_data __page_aligned_data;
 
 /*
- * Including <asm/unistd.h> would give use the 64-bit syscall numbers ...
+ * Mapping for the VDSO data pages. The real pages are mapped manually, as
+ * what we map and where within the area they are mapped is determined at
+ * runtime.
  */
-#define __NR_O32_sigreturn		4119
-#define __NR_O32_rt_sigreturn		4193
-#define __NR_N32_rt_sigreturn		6211
+static struct page *no_pages[] = { NULL };
+static struct vm_special_mapping vdso_vvar_mapping = {
+	.name = "[vvar]",
+	.pages = no_pages,
+};
 
-static struct page *vdso_page;
-
-static void __init install_trampoline(u32 *tramp, unsigned int sigreturn)
+static void __init init_vdso_image(struct mips_vdso_image *image)
 {
-	uasm_i_addiu(&tramp, 2, 0, sigreturn);	/* li v0, sigreturn */
-	uasm_i_syscall(&tramp, 0);
+	unsigned long num_pages, i;
+
+	BUG_ON(!PAGE_ALIGNED(image->data));
+	BUG_ON(!PAGE_ALIGNED(image->size));
+
+	num_pages = image->size / PAGE_SIZE;
+
+	for (i = 0; i < num_pages; i++) {
+		image->mapping.pages[i] =
+			virt_to_page(image->data + (i * PAGE_SIZE));
+	}
 }
 
 static int __init init_vdso(void)
 {
-	struct mips_vdso *vdso;
-
-	vdso_page = alloc_page(GFP_KERNEL);
-	if (!vdso_page)
-		panic("Cannot allocate vdso");
-
-	vdso = vmap(&vdso_page, 1, 0, PAGE_KERNEL);
-	if (!vdso)
-		panic("Cannot map vdso");
-	clear_page(vdso);
-
-	install_trampoline(vdso->rt_signal_trampoline, __NR_rt_sigreturn);
-#ifdef CONFIG_32BIT
-	install_trampoline(vdso->signal_trampoline, __NR_sigreturn);
-#else
-	install_trampoline(vdso->n32_rt_signal_trampoline,
-			   __NR_N32_rt_sigreturn);
-	install_trampoline(vdso->o32_signal_trampoline, __NR_O32_sigreturn);
-	install_trampoline(vdso->o32_rt_signal_trampoline,
-			   __NR_O32_rt_sigreturn);
+	init_vdso_image(&vdso_image);
+
+#ifdef CONFIG_MIPS32_O32
+	init_vdso_image(&vdso_image_o32);
 #endif
 
-	vunmap(vdso);
+#ifdef CONFIG_MIPS32_N32
+	init_vdso_image(&vdso_image_n32);
+#endif
 
 	return 0;
 }
 subsys_initcall(init_vdso);
 
-static unsigned long vdso_addr(unsigned long start)
-{
-	unsigned long offset = 0UL;
-
-	if (current->flags & PF_RANDOMIZE) {
-		offset = get_random_int();
-		offset <<= PAGE_SHIFT;
-		if (TASK_IS_32BIT_ADDR)
-			offset &= 0xfffffful;
-		else
-			offset &= 0xffffffful;
-	}
-
-	return STACK_TOP + offset;
-}
-
 int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 {
-	int ret;
-	unsigned long addr;
+	struct mips_vdso_image *image = current->thread.abi->vdso;
 	struct mm_struct *mm = current->mm;
+	unsigned long base, vdso_addr;
+	struct vm_area_struct *vma;
+	int ret;
 
 	down_write(&mm->mmap_sem);
 
-	addr = vdso_addr(mm->start_stack);
-
-	addr = get_unmapped_area(NULL, addr, PAGE_SIZE, 0, 0);
-	if (IS_ERR_VALUE(addr)) {
-		ret = addr;
-		goto up_fail;
+	base = get_unmapped_area(NULL, 0, PAGE_SIZE + image->size, 0, 0);
+	if (IS_ERR_VALUE(base)) {
+		ret = base;
+		goto out;
 	}
 
-	ret = install_special_mapping(mm, addr, PAGE_SIZE,
-				      VM_READ|VM_EXEC|
-				      VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
-				      &vdso_page);
+	vdso_addr = base + PAGE_SIZE;
+
+	vma = _install_special_mapping(mm, base, PAGE_SIZE,
+				       VM_READ | VM_MAYREAD,
+				       &vdso_vvar_mapping);
+	if (IS_ERR(vma)) {
+		ret = PTR_ERR(vma);
+		goto out;
+	}
 
+	/* Map data page. */
+	ret = remap_pfn_range(vma, base,
+			      virt_to_phys(&vdso_data) >> PAGE_SHIFT,
+			      PAGE_SIZE, PAGE_READONLY);
 	if (ret)
-		goto up_fail;
+		goto out;
+
+	/* Map VDSO image. */
+	vma = _install_special_mapping(mm, vdso_addr, image->size,
+				       VM_READ | VM_EXEC |
+				       VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC,
+				       &image->mapping);
+	if (IS_ERR(vma)) {
+		ret = PTR_ERR(vma);
+		goto out;
+	}
 
-	mm->context.vdso = (void *)addr;
+	mm->context.vdso = (void *)vdso_addr;
+	ret = 0;
 
-up_fail:
+out:
 	up_write(&mm->mmap_sem);
 	return ret;
 }
-
-const char *arch_vma_name(struct vm_area_struct *vma)
-{
-	if (vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso)
-		return "[vdso]";
-	return NULL;
-}

commit ccd3988086364837d0c0fb4563d715c691636313
Author: Prem Karat <pkarat@mvista.com>
Date:   Mon Apr 21 09:33:16 2014 +0530

    MIPS: Enable VDSO randomization
    
    Based on commit 1091458d09e1a (mmap randomization)
    
    For 32-bit address spaces randomize within a
    16MB space, for 64-bit within a 256MB space.
    
    Test Results:
    ------------
    Without Patch (VDSO is not randomized)
    ---------------------------------------
    root@Maleo:~# ./aslr vdso
    FAIL: ASLR not functional (vdso always at 0x7fff7000)
    
    root@Maleo:~# ./aslr rekey vdso
    pre_val==cur_val
    value=0x7fff7000
    
    With patch:(VDSO is randmoized and doesn't interfere with stack)
    ----------------------------------------------------------------
    root@cavium-octeon2:~# ./aslr rekey vdso
    pre_val!=cur_val
    previous_value=0x7f830ea2
    current_value=0x776e2000
    root@cavium-octeon2:~# ./aslr rekey vdso
    pre_val!=cur_val
    previous_value=0x7fb0cea2
    current_value=0x77209000
    root@cavium-octeon2:~# ./aslr rekey vdso
    pre_val!=cur_val
    previous_value=0x7f985ea2
    current_value=0x7770c000
    root@cavium-octeon2:~# ./aslr rekey vdso
    pre_val!=cur_val
    previous_value=0x7fbc6ea2
    current_value=0x7fe25000
    
    Maps file output:
    -------------------------
    root@cavium-octeon2:~# ./aslr rekey maps
    78584000-785a5000 rwxp 00000000 00:00 0                                  [heap]
    7f9d0000-7f9f1000 rw-p 00000000 00:00 0                                  [stack]
    7ffa5000-7ffa6000 r-xp 00000000 00:00 0                                  [vdso]
    
    root@cavium-octeon2:~# ./aslr rekey maps
    77de0000-77e01000 rwxp 00000000 00:00 0                                  [heap]
    7f91b000-7f93c000 rw-p 00000000 00:00 0                                  [stack]
    7ff99000-7ff9a000 r-xp 00000000 00:00 0                                  [vdso]
    
    root@cavium-octeon2:~# ./aslr rekey maps
    77d7f000-77da0000 rwxp 00000000 00:00 0                                  [heap]
    7fc2a000-7fc4b000 rw-p 00000000 00:00 0                                  [stack]
    7fe09000-7fe0a000 r-xp 00000000 00:00 0                                  [vdso]
    
    root@cavium-octeon2:~# ./aslr rekey maps
    7794c000-7794d000 r-xp 00000000 00:00 0                                  [vdso]
    77e4b000-77e6c000 rwxp 00000000 00:00 0                                  [heap]
    7f6e7000-7f708000 rw-p 00000000 00:00 0                                  [stack]
    root@cavium-octeon2:~#
    
    Signed-off-by: Prem Karat <pkarat@mvista.com>
    Cc: linux-mips@linux-mips.org
    Cc: sergei.shtylyov@cogentembedded.com
    Cc: ddaney.cavm@gmail.com
    Patchwork: https://patchwork.linux-mips.org/patch/6812
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 0f1af58b036a..ed2a278722a9 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -16,9 +16,11 @@
 #include <linux/elf.h>
 #include <linux/vmalloc.h>
 #include <linux/unistd.h>
+#include <linux/random.h>
 
 #include <asm/vdso.h>
 #include <asm/uasm.h>
+#include <asm/processor.h>
 
 /*
  * Including <asm/unistd.h> would give use the 64-bit syscall numbers ...
@@ -67,7 +69,18 @@ subsys_initcall(init_vdso);
 
 static unsigned long vdso_addr(unsigned long start)
 {
-	return STACK_TOP;
+	unsigned long offset = 0UL;
+
+	if (current->flags & PF_RANDOMIZE) {
+		offset = get_random_int();
+		offset <<= PAGE_SHIFT;
+		if (TASK_IS_32BIT_ADDR)
+			offset &= 0xfffffful;
+		else
+			offset &= 0xffffffful;
+	}
+
+	return STACK_TOP + offset;
 }
 
 int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)

commit 909af768e88867016f427264ae39d27a57b6a8ed
Author: Jason Baron <jbaron@redhat.com>
Date:   Fri Mar 23 15:02:51 2012 -0700

    coredump: remove VM_ALWAYSDUMP flag
    
    The motivation for this patchset was that I was looking at a way for a
    qemu-kvm process, to exclude the guest memory from its core dump, which
    can be quite large.  There are already a number of filter flags in
    /proc/<pid>/coredump_filter, however, these allow one to specify 'types'
    of kernel memory, not specific address ranges (which is needed in this
    case).
    
    Since there are no more vma flags available, the first patch eliminates
    the need for the 'VM_ALWAYSDUMP' flag.  The flag is used internally by
    the kernel to mark vdso and vsyscall pages.  However, it is simple
    enough to check if a vma covers a vdso or vsyscall page without the need
    for this flag.
    
    The second patch then replaces the 'VM_ALWAYSDUMP' flag with a new
    'VM_NODUMP' flag, which can be set by userspace using new madvise flags:
    'MADV_DONTDUMP', and unset via 'MADV_DODUMP'.  The core dump filters
    continue to work the same as before unless 'MADV_DONTDUMP' is set on the
    region.
    
    The qemu code which implements this features is at:
    
      http://people.redhat.com/~jbaron/qemu-dump/qemu-dump.patch
    
    In my testing the qemu core dump shrunk from 383MB -> 13MB with this
    patch.
    
    I also believe that the 'MADV_DONTDUMP' flag might be useful for
    security sensitive apps, which might want to select which areas are
    dumped.
    
    This patch:
    
    The VM_ALWAYSDUMP flag is currently used by the coredump code to
    indicate that a vma is part of a vsyscall or vdso section.  However, we
    can determine if a vma is in one these sections by checking it against
    the gate_vma and checking for a non-NULL return value from
    arch_vma_name().  Thus, freeing a valuable vma bit.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Roland McGrath <roland@hack.frob.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Avi Kivity <avi@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index e5cdfd603f8f..0f1af58b036a 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -88,8 +88,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 
 	ret = install_special_mapping(mm, addr, PAGE_SIZE,
 				      VM_READ|VM_EXEC|
-				      VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC|
-				      VM_ALWAYSDUMP,
+				      VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
 				      &vdso_page);
 
 	if (ret)

commit 1ed845375b1f2938acc4496a186e180892b00c71
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Wed Jun 16 15:00:28 2010 -0700

    MIPS: Make init_vdso a subsys_initcall.
    
    Quoting from Jiri Slaby's patch of a similar nature for x86:
    
        When initrd is in use and a driver does request_module() in its
        module_init (i.e. __initcall or device_initcall), a modprobe
        process is created with VDSO mapping. But VDSO is inited even in
        __initcall, i.e. on the same level (at the same time), so it may
        not be inited yet (link order matters).
    
    Move init_vdso up to subsys_initcall to avoid the issue.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    Cc: David Daney <ddaney@caviumnetworks.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Patchwork: http://patchwork.linux-mips.org/patch/1386/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index 6c6ee94378d5..e5cdfd603f8f 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -63,7 +63,7 @@ static int __init init_vdso(void)
 
 	return 0;
 }
-device_initcall(init_vdso);
+subsys_initcall(init_vdso);
 
 static unsigned long vdso_addr(unsigned long start)
 {

commit 57d15018aa48ecc5fafef3374dcebcf0bbbfa764
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Wed Jun 16 15:00:27 2010 -0700

    MIPS: "Fix" useless 'init_vdso successfully' message.
    
    In addition to being useless, it was mis-spelled.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    Cc: David Daney <ddaney@caviumnetworks.com>
    Patchwork: http://patchwork.linux-mips.org/patch/1385/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
index b773c1112b14..6c6ee94378d5 100644
--- a/arch/mips/kernel/vdso.c
+++ b/arch/mips/kernel/vdso.c
@@ -61,8 +61,6 @@ static int __init init_vdso(void)
 
 	vunmap(vdso);
 
-	pr_notice("init_vdso successfull\n");
-
 	return 0;
 }
 device_initcall(init_vdso);

commit c52d0d30aef84aa8893b34e5254716c8ab5c4472
Author: David Daney <ddaney@caviumnetworks.com>
Date:   Thu Feb 18 16:13:04 2010 -0800

    MIPS: Preliminary VDSO
    
    This is a preliminary patch to add a vdso to all user processes.  Still
    missing are ELF headers and .eh_frame information.  But it is enough to
    allow us to move signal trampolines off of the stack.  Note that emulation
    of branch delay slots in the FPU emulator still requires the stack.
    
    We allocate a single page (the vdso) and write all possible signal
    trampolines into it.  The stack is moved down by one page and the vdso is
    mapped into this space.
    
    Signed-off-by: David Daney <ddaney@caviumnetworks.com>
    To: linux-mips@linux-mips.org
    Patchwork: http://patchwork.linux-mips.org/patch/975/
    Signed-off-by: Ralf Baechle <ralf@linux-mips.org>

diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c
new file mode 100644
index 000000000000..b773c1112b14
--- /dev/null
+++ b/arch/mips/kernel/vdso.c
@@ -0,0 +1,112 @@
+/*
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file "COPYING" in the main directory of this archive
+ * for more details.
+ *
+ * Copyright (C) 2009, 2010 Cavium Networks, Inc.
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/err.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/init.h>
+#include <linux/binfmts.h>
+#include <linux/elf.h>
+#include <linux/vmalloc.h>
+#include <linux/unistd.h>
+
+#include <asm/vdso.h>
+#include <asm/uasm.h>
+
+/*
+ * Including <asm/unistd.h> would give use the 64-bit syscall numbers ...
+ */
+#define __NR_O32_sigreturn		4119
+#define __NR_O32_rt_sigreturn		4193
+#define __NR_N32_rt_sigreturn		6211
+
+static struct page *vdso_page;
+
+static void __init install_trampoline(u32 *tramp, unsigned int sigreturn)
+{
+	uasm_i_addiu(&tramp, 2, 0, sigreturn);	/* li v0, sigreturn */
+	uasm_i_syscall(&tramp, 0);
+}
+
+static int __init init_vdso(void)
+{
+	struct mips_vdso *vdso;
+
+	vdso_page = alloc_page(GFP_KERNEL);
+	if (!vdso_page)
+		panic("Cannot allocate vdso");
+
+	vdso = vmap(&vdso_page, 1, 0, PAGE_KERNEL);
+	if (!vdso)
+		panic("Cannot map vdso");
+	clear_page(vdso);
+
+	install_trampoline(vdso->rt_signal_trampoline, __NR_rt_sigreturn);
+#ifdef CONFIG_32BIT
+	install_trampoline(vdso->signal_trampoline, __NR_sigreturn);
+#else
+	install_trampoline(vdso->n32_rt_signal_trampoline,
+			   __NR_N32_rt_sigreturn);
+	install_trampoline(vdso->o32_signal_trampoline, __NR_O32_sigreturn);
+	install_trampoline(vdso->o32_rt_signal_trampoline,
+			   __NR_O32_rt_sigreturn);
+#endif
+
+	vunmap(vdso);
+
+	pr_notice("init_vdso successfull\n");
+
+	return 0;
+}
+device_initcall(init_vdso);
+
+static unsigned long vdso_addr(unsigned long start)
+{
+	return STACK_TOP;
+}
+
+int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
+{
+	int ret;
+	unsigned long addr;
+	struct mm_struct *mm = current->mm;
+
+	down_write(&mm->mmap_sem);
+
+	addr = vdso_addr(mm->start_stack);
+
+	addr = get_unmapped_area(NULL, addr, PAGE_SIZE, 0, 0);
+	if (IS_ERR_VALUE(addr)) {
+		ret = addr;
+		goto up_fail;
+	}
+
+	ret = install_special_mapping(mm, addr, PAGE_SIZE,
+				      VM_READ|VM_EXEC|
+				      VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC|
+				      VM_ALWAYSDUMP,
+				      &vdso_page);
+
+	if (ret)
+		goto up_fail;
+
+	mm->context.vdso = (void *)addr;
+
+up_fail:
+	up_write(&mm->mmap_sem);
+	return ret;
+}
+
+const char *arch_vma_name(struct vm_area_struct *vma)
+{
+	if (vma->vm_mm && vma->vm_start == (long)vma->vm_mm->context.vdso)
+		return "[vdso]";
+	return NULL;
+}
