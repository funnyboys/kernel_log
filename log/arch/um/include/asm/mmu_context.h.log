commit aaa2cc56c1cd757efec88a4978ffce4cbf884352
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:33 2020 -0700

    mmap locking API: convert nested write lock sites
    
    Add API for nested write locks and convert the few call sites doing that.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-7-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 62262c5c7785..17ddd4edf875 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -8,6 +8,7 @@
 
 #include <linux/sched.h>
 #include <linux/mm_types.h>
+#include <linux/mmap_lock.h>
 
 #include <asm/mmu.h>
 
@@ -47,7 +48,7 @@ static inline void activate_mm(struct mm_struct *old, struct mm_struct *new)
 	 * when the new ->mm is used for the first time.
 	 */
 	__switch_mm(&new->context.id);
-	down_write_nested(&new->mmap_sem, 1);
+	mmap_write_lock_nested(new, SINGLE_DEPTH_NESTING);
 	uml_setup_stubs(new);
 	mmap_write_unlock(new);
 }

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index b4deb1bfbb68..62262c5c7785 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -49,7 +49,7 @@ static inline void activate_mm(struct mm_struct *old, struct mm_struct *new)
 	__switch_mm(&new->context.id);
 	down_write_nested(&new->mmap_sem, 1);
 	uml_setup_stubs(new);
-	up_write(&new->mmap_sem);
+	mmap_write_unlock(new);
 }
 
 static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next, 

commit 42222eae17f7c930833dfda7896ef280879de94a
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jan 23 10:41:16 2020 -0800

    mm: remove arch_bprm_mm_init() hook
    
    From: Dave Hansen <dave.hansen@linux.intel.com>
    
    MPX is being removed from the kernel due to a lack of support
    in the toolchain going forward (gcc).
    
    arch_bprm_mm_init() is used at execve() time.  The only non-stub
    implementation is on x86 for MPX.  Remove the hook entirely from
    all architectures and generic code.
    
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: x86@kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arch@vger.kernel.org
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 5aee0626e390..b4deb1bfbb68 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -25,11 +25,6 @@ static inline void arch_unmap(struct mm_struct *mm,
 			unsigned long start, unsigned long end)
 {
 }
-static inline void arch_bprm_mm_init(struct mm_struct *mm,
-				     struct vm_area_struct *vma)
-{
-}
-
 static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
 		bool write, bool execute, bool foreign)
 {

commit f2f4bf5aabadd6575f5daabcb0a2f506e3f5f68c
Author: Alex Dewar <alex.dewar@gmx.co.uk>
Date:   Sun Aug 25 10:49:19 2019 +0100

    um: Add SPDX headers for files in arch/um/include
    
    Convert files to use SPDX header. All files are licensed under the GPLv2.
    
    Signed-off-by: Alex Dewar <alex.dewar@gmx.co.uk>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 00cefd33afdd..5aee0626e390 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -1,6 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /* 
  * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
- * Licensed under the GPL
  */
 
 #ifndef __UM_MMU_CONTEXT_H

commit 80bf6ceaf9310b3f61934c69b382d4912deee049
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Fri May 24 21:54:14 2019 +0200

    um: Silence lockdep complaint about mmap_sem
    
    When we get into activate_mm(), lockdep complains that we're doing
    something strange:
    
        WARNING: possible circular locking dependency detected
        5.1.0-10252-gb00152307319-dirty #121 Not tainted
        ------------------------------------------------------
        inside.sh/366 is trying to acquire lock:
        (____ptrval____) (&(&p->alloc_lock)->rlock){+.+.}, at: flush_old_exec+0x703/0x8d7
    
        but task is already holding lock:
        (____ptrval____) (&mm->mmap_sem){++++}, at: flush_old_exec+0x6c5/0x8d7
    
        which lock already depends on the new lock.
    
        the existing dependency chain (in reverse order) is:
    
        -> #1 (&mm->mmap_sem){++++}:
               [...]
               __lock_acquire+0x12ab/0x139f
               lock_acquire+0x155/0x18e
               down_write+0x3f/0x98
               flush_old_exec+0x748/0x8d7
               load_elf_binary+0x2ca/0xddb
               [...]
    
        -> #0 (&(&p->alloc_lock)->rlock){+.+.}:
               [...]
               __lock_acquire+0x12ab/0x139f
               lock_acquire+0x155/0x18e
               _raw_spin_lock+0x30/0x83
               flush_old_exec+0x703/0x8d7
               load_elf_binary+0x2ca/0xddb
               [...]
    
        other info that might help us debug this:
    
         Possible unsafe locking scenario:
    
               CPU0                    CPU1
               ----                    ----
          lock(&mm->mmap_sem);
                                       lock(&(&p->alloc_lock)->rlock);
                                       lock(&mm->mmap_sem);
          lock(&(&p->alloc_lock)->rlock);
    
         *** DEADLOCK ***
    
        2 locks held by inside.sh/366:
         #0: (____ptrval____) (&sig->cred_guard_mutex){+.+.}, at: __do_execve_file+0x12d/0x869
         #1: (____ptrval____) (&mm->mmap_sem){++++}, at: flush_old_exec+0x6c5/0x8d7
    
        stack backtrace:
        CPU: 0 PID: 366 Comm: inside.sh Not tainted 5.1.0-10252-gb00152307319-dirty #121
        Stack:
         [...]
        Call Trace:
         [<600420de>] show_stack+0x13b/0x155
         [<6048906b>] dump_stack+0x2a/0x2c
         [<6009ae64>] print_circular_bug+0x332/0x343
         [<6009c5c6>] check_prev_add+0x669/0xdad
         [<600a06b4>] __lock_acquire+0x12ab/0x139f
         [<6009f3d0>] lock_acquire+0x155/0x18e
         [<604a07e0>] _raw_spin_lock+0x30/0x83
         [<60151e6a>] flush_old_exec+0x703/0x8d7
         [<601a8eb8>] load_elf_binary+0x2ca/0xddb
         [...]
    
    I think it's because in exec_mmap() we have
    
            down_read(&old_mm->mmap_sem);
    ...
            task_lock(tsk);
    ...
            activate_mm(active_mm, mm);
            (which does down_write(&mm->mmap_sem))
    
    I'm not really sure why lockdep throws in the whole knowledge
    about the task lock, but it seems that old_mm and mm shouldn't
    ever be the same (and it doesn't deadlock) so tell lockdep that
    they're different.
    
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 9f4b4bb78120..00cefd33afdd 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -52,7 +52,7 @@ static inline void activate_mm(struct mm_struct *old, struct mm_struct *new)
 	 * when the new ->mm is used for the first time.
 	 */
 	__switch_mm(&new->context.id);
-	down_write(&new->mmap_sem);
+	down_write_nested(&new->mmap_sem, 1);
 	uml_setup_stubs(new);
 	up_write(&new->mmap_sem);
 }

commit 5a28fc94c9143db766d1ba5480cae82d856ad080
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Apr 19 12:47:47 2019 -0700

    x86/mpx, mm/core: Fix recursive munmap() corruption
    
    This is a bit of a mess, to put it mildly.  But, it's a bug
    that only seems to have showed up in 4.20 but wasn't noticed
    until now, because nobody uses MPX.
    
    MPX has the arch_unmap() hook inside of munmap() because MPX
    uses bounds tables that protect other areas of memory.  When
    memory is unmapped, there is also a need to unmap the MPX
    bounds tables.  Barring this, unused bounds tables can eat 80%
    of the address space.
    
    But, the recursive do_munmap() that gets called vi arch_unmap()
    wreaks havoc with __do_munmap()'s state.  It can result in
    freeing populated page tables, accessing bogus VMA state,
    double-freed VMAs and more.
    
    See the "long story" further below for the gory details.
    
    To fix this, call arch_unmap() before __do_unmap() has a chance
    to do anything meaningful.  Also, remove the 'vma' argument
    and force the MPX code to do its own, independent VMA lookup.
    
    == UML / unicore32 impact ==
    
    Remove unused 'vma' argument to arch_unmap().  No functional
    change.
    
    I compile tested this on UML but not unicore32.
    
    == powerpc impact ==
    
    powerpc uses arch_unmap() well to watch for munmap() on the
    VDSO and zeroes out 'current->mm->context.vdso_base'.  Moving
    arch_unmap() makes this happen earlier in __do_munmap().  But,
    'vdso_base' seems to only be used in perf and in the signal
    delivery that happens near the return to userspace.  I can not
    find any likely impact to powerpc, other than the zeroing
    happening a little earlier.
    
    powerpc does not use the 'vma' argument and is unaffected by
    its removal.
    
    I compile-tested a 64-bit powerpc defconfig.
    
    == x86 impact ==
    
    For the common success case this is functionally identical to
    what was there before.  For the munmap() failure case, it's
    possible that some MPX tables will be zapped for memory that
    continues to be in use.  But, this is an extraordinarily
    unlikely scenario and the harm would be that MPX provides no
    protection since the bounds table got reset (zeroed).
    
    I can't imagine anyone doing this:
    
            ptr = mmap();
            // use ptr
            ret = munmap(ptr);
            if (ret)
                    // oh, there was an error, I'll
                    // keep using ptr.
    
    Because if you're doing munmap(), you are *done* with the
    memory.  There's probably no good data in there _anyway_.
    
    This passes the original reproducer from Richard Biener as
    well as the existing mpx selftests/.
    
    The long story:
    
    munmap() has a couple of pieces:
    
     1. Find the affected VMA(s)
     2. Split the start/end one(s) if neceesary
     3. Pull the VMAs out of the rbtree
     4. Actually zap the memory via unmap_region(), including
        freeing page tables (or queueing them to be freed).
     5. Fix up some of the accounting (like fput()) and actually
        free the VMA itself.
    
    This specific ordering was actually introduced by:
    
      dd2283f2605e ("mm: mmap: zap pages with read mmap_sem in munmap")
    
    during the 4.20 merge window.  The previous __do_munmap() code
    was actually safe because the only thing after arch_unmap() was
    remove_vma_list().  arch_unmap() could not see 'vma' in the
    rbtree because it was detached, so it is not even capable of
    doing operations unsafe for remove_vma_list()'s use of 'vma'.
    
    Richard Biener reported a test that shows this in dmesg:
    
      [1216548.787498] BUG: Bad rss-counter state mm:0000000017ce560b idx:1 val:551
      [1216548.787500] BUG: non-zero pgtables_bytes on freeing mm: 24576
    
    What triggered this was the recursive do_munmap() called via
    arch_unmap().  It was freeing page tables that has not been
    properly zapped.
    
    But, the problem was bigger than this.  For one, arch_unmap()
    can free VMAs.  But, the calling __do_munmap() has variables
    that *point* to VMAs and obviously can't handle them just
    getting freed while the pointer is still in use.
    
    I tried a couple of things here.  First, I tried to fix the page
    table freeing problem in isolation, but I then found the VMA
    issue.  I also tried having the MPX code return a flag if it
    modified the rbtree which would force __do_munmap() to re-walk
    to restart.  That spiralled out of control in complexity pretty
    fast.
    
    Just moving arch_unmap() and accepting that the bonkers failure
    case might eat some bounds tables seems like the simplest viable
    fix.
    
    This was also reported in the following kernel bugzilla entry:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=203123
    
    There are some reports that this commit triggered this bug:
    
      dd2283f2605 ("mm: mmap: zap pages with read mmap_sem in munmap")
    
    While that commit certainly made the issues easier to hit, I believe
    the fundamental issue has been with us as long as MPX itself, thus
    the Fixes: tag below is for one of the original MPX commits.
    
    [ mingo: Minor edits to the changelog and the patch. ]
    
    Reported-by: Richard Biener <rguenther@suse.de>
    Reported-by: H.J. Lu <hjl.tools@gmail.com>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Yang Shi <yang.shi@linux.alibaba.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-um@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: stable@vger.kernel.org
    Fixes: dd2283f2605e ("mm: mmap: zap pages with read mmap_sem in munmap")
    Link: http://lkml.kernel.org/r/20190419194747.5E1AD6DC@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index fca34b2177e2..9f4b4bb78120 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -22,7 +22,6 @@ static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
 }
 extern void arch_exit_mmap(struct mm_struct *mm);
 static inline void arch_unmap(struct mm_struct *mm,
-			struct vm_area_struct *vma,
 			unsigned long start, unsigned long end)
 {
 }

commit c10e83f598d08046dd1ebc8360d4bb12d802d51b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 14 12:27:29 2017 +0100

    arch, mm: Allow arch_dup_mmap() to fail
    
    In order to sanitize the LDT initialization on x86 arch_dup_mmap() must be
    allowed to fail. Fix up all instances.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirsky <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: dan.j.williams@intel.com
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: linux-mm@kvack.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index b668e351fd6c..fca34b2177e2 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -15,9 +15,10 @@ extern void uml_setup_stubs(struct mm_struct *mm);
 /*
  * Needed since we do not use the asm-generic/mm_hooks.h:
  */
-static inline void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
+static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
 {
 	uml_setup_stubs(mm);
+	return 0;
 }
 extern void arch_exit_mmap(struct mm_struct *mm);
 static inline void arch_unmap(struct mm_struct *mm,

commit 9a804fecee232e71b47ac37d62fd3d5d66b08b91
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 16 18:26:49 2017 +0300

    mm/gup: Drop the arch_pte_access_permitted() MMU callback
    
    The only arch that defines it to something meaningful is x86.
    But x86 doesn't use the generic GUP_fast() implementation -- the
    only place where the callback is called.
    
    Let's drop it.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aneesh Kumar K . V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dann Frazier <dann.frazier@canonical.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170316152655.37789-2-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 94ac2739918c..b668e351fd6c 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -37,12 +37,6 @@ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
 	return true;
 }
 
-static inline bool arch_pte_access_permitted(pte_t pte, bool write)
-{
-	/* by default, allow everything */
-	return true;
-}
-
 /*
  * end asm-generic/mm_hooks.h functions
  */

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 1a60e1328e2f..94ac2739918c 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -7,6 +7,8 @@
 #define __UM_MMU_CONTEXT_H
 
 #include <linux/sched.h>
+#include <linux/mm_types.h>
+
 #include <asm/mmu.h>
 
 extern void uml_setup_stubs(struct mm_struct *mm);

commit 9d95b1759e0504890049deb2de62e31d7c241c30
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Feb 18 10:35:57 2016 -0800

    um, pkeys: Add UML arch_*_access_permitted() methods
    
    UML has a special mmu_context.h and needs updates whenever the generic one
    is updated.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: user-mode-linux-user@lists.sourceforge.net
    Link: http://lkml.kernel.org/r/20160218183557.AE1DB383@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 941527e507f7..1a60e1328e2f 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -27,6 +27,20 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,
 				     struct vm_area_struct *vma)
 {
 }
+
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+		bool write, bool execute, bool foreign)
+{
+	/* by default, allow everything */
+	return true;
+}
+
+static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
+
 /*
  * end asm-generic/mm_hooks.h functions
  */

commit 62e88b1c00de9cb30d937841ed5debed871070b8
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Tue Nov 18 10:23:50 2014 -0800

    mm: Make arch_unmap()/bprm_mm_init() available to all architectures
    
    The x86 MPX patch set calls arch_unmap() and arch_bprm_mm_init()
    from fs/exec.c, so we need at least a stub for them in all
    architectures.  They are only called under an #ifdef for
    CONFIG_MMU=y, so we can at least restict this to architectures
    with MMU support.
    
    blackfin/c6x have no MMU support, so do not call arch_unmap().
    They also do not include mm_hooks.h or mmu_context.h at all and
    do not need to be touched.
    
    s390, um and unicore32 do not use asm-generic/mm_hooks.h, so got
    their own arch_unmap() versions.  (I also moved um's
    arch_dup_mmap() to be closer to the other mm_hooks.h functions).
    
    xtensa only includes mm_hooks when MMU=y, which should be fine
    since arch_unmap() is called only from MMU=y code.
    
    For the rest, we use the stub copies of these functions in
    asm-generic/mm_hook.h.
    
    I cross compiled defconfigs for cris (to check NOMMU) and s390
    to make sure that this works.  I also checked a 64-bit build
    of UML and all my normal x86 builds including PARAVIRT on and
    off.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: linux-arch@vger.kernel.org
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20141118182350.8B4AA2C2@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index aa4a743dc4ab..941527e507f7 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -10,7 +10,26 @@
 #include <asm/mmu.h>
 
 extern void uml_setup_stubs(struct mm_struct *mm);
+/*
+ * Needed since we do not use the asm-generic/mm_hooks.h:
+ */
+static inline void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
+{
+	uml_setup_stubs(mm);
+}
 extern void arch_exit_mmap(struct mm_struct *mm);
+static inline void arch_unmap(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			unsigned long start, unsigned long end)
+{
+}
+static inline void arch_bprm_mm_init(struct mm_struct *mm,
+				     struct vm_area_struct *vma)
+{
+}
+/*
+ * end asm-generic/mm_hooks.h functions
+ */
 
 #define deactivate_mm(tsk,mm)	do { } while (0)
 
@@ -41,11 +60,6 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	}
 }
 
-static inline void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
-{
-	uml_setup_stubs(mm);
-}
-
 static inline void enter_lazy_tlb(struct mm_struct *mm, 
 				  struct task_struct *tsk)
 {

commit ac2aca2817ca10ad30f86d0d3caa41ea8cc4f0f0
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Feb 23 01:37:19 2012 -0500

    um: uml_dup_mmap() relies on ->mmap_sem being held, but activate_mm() doesn't hold it
    
    ... while calling uml_dup_mmap()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 591b3d8d7614..aa4a743dc4ab 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -9,7 +9,7 @@
 #include <linux/sched.h>
 #include <asm/mmu.h>
 
-extern void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm);
+extern void uml_setup_stubs(struct mm_struct *mm);
 extern void arch_exit_mmap(struct mm_struct *mm);
 
 #define deactivate_mm(tsk,mm)	do { } while (0)
@@ -23,7 +23,9 @@ static inline void activate_mm(struct mm_struct *old, struct mm_struct *new)
 	 * when the new ->mm is used for the first time.
 	 */
 	__switch_mm(&new->context.id);
-	arch_dup_mmap(old, new);
+	down_write(&new->mmap_sem);
+	uml_setup_stubs(new);
+	up_write(&new->mmap_sem);
 }
 
 static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next, 
@@ -39,6 +41,11 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	}
 }
 
+static inline void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
+{
+	uml_setup_stubs(mm);
+}
+
 static inline void enter_lazy_tlb(struct mm_struct *mm, 
 				  struct task_struct *tsk)
 {

commit 4dc706c2f292b2c28016a27f400af84a62ec4a63
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Aug 18 20:09:09 2011 +0100

    um: take um_mmu.h to asm/mmu.h, clean asm/mmu_context.h a bit
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 34d813011b7a..591b3d8d7614 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -6,15 +6,12 @@
 #ifndef __UM_MMU_CONTEXT_H
 #define __UM_MMU_CONTEXT_H
 
-#include "linux/sched.h"
-#include "um_mmu.h"
+#include <linux/sched.h>
+#include <asm/mmu.h>
 
 extern void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm);
 extern void arch_exit_mmap(struct mm_struct *mm);
 
-#define get_mmu_context(task) do ; while(0)
-#define activate_context(tsk) do ; while(0)
-
 #define deactivate_mm(tsk,mm)	do { } while (0)
 
 extern void force_flush_all(void);

commit fa40699b975131028a61aa8e095b0b17f350da40
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:51 2009 -0600

    cpumask: use mm_cpumask() wrapper: um
    
    Makes code futureproof against the impending change to mm->cpu_vm_mask.
    
    It's also a chance to use the new cpumask_ ops which take a pointer
    (the older ones are deprecated, but there's no hurry for arch code).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
index 54f42e8b0105..34d813011b7a 100644
--- a/arch/um/include/asm/mmu_context.h
+++ b/arch/um/include/asm/mmu_context.h
@@ -35,8 +35,8 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	unsigned cpu = smp_processor_id();
 
 	if(prev != next){
-		cpu_clear(cpu, prev->cpu_vm_mask);
-		cpu_set(cpu, next->cpu_vm_mask);
+		cpumask_clear_cpu(cpu, mm_cpumask(prev));
+		cpumask_set_cpu(cpu, mm_cpumask(next));
 		if(next != &init_mm)
 			__switch_mm(&next->context.id);
 	}

commit 8ede0bdb63305d3353efd97e9af6210afb05734e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 17 19:13:17 2008 -0400

    x86, um: initial part of asm-um move
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: H. Peter Anvin <hpa@zytor.com>

diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h
new file mode 100644
index 000000000000..54f42e8b0105
--- /dev/null
+++ b/arch/um/include/asm/mmu_context.h
@@ -0,0 +1,54 @@
+/* 
+ * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
+ * Licensed under the GPL
+ */
+
+#ifndef __UM_MMU_CONTEXT_H
+#define __UM_MMU_CONTEXT_H
+
+#include "linux/sched.h"
+#include "um_mmu.h"
+
+extern void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm);
+extern void arch_exit_mmap(struct mm_struct *mm);
+
+#define get_mmu_context(task) do ; while(0)
+#define activate_context(tsk) do ; while(0)
+
+#define deactivate_mm(tsk,mm)	do { } while (0)
+
+extern void force_flush_all(void);
+
+static inline void activate_mm(struct mm_struct *old, struct mm_struct *new)
+{
+	/*
+	 * This is called by fs/exec.c and sys_unshare()
+	 * when the new ->mm is used for the first time.
+	 */
+	__switch_mm(&new->context.id);
+	arch_dup_mmap(old, new);
+}
+
+static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next, 
+			     struct task_struct *tsk)
+{
+	unsigned cpu = smp_processor_id();
+
+	if(prev != next){
+		cpu_clear(cpu, prev->cpu_vm_mask);
+		cpu_set(cpu, next->cpu_vm_mask);
+		if(next != &init_mm)
+			__switch_mm(&next->context.id);
+	}
+}
+
+static inline void enter_lazy_tlb(struct mm_struct *mm, 
+				  struct task_struct *tsk)
+{
+}
+
+extern int init_new_context(struct task_struct *task, struct mm_struct *mm);
+
+extern void destroy_context(struct mm_struct *mm);
+
+#endif
