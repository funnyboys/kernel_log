commit c1e8d7c6a7a682e1405e3e242d32fc377fd196ff
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:54 2020 -0700

    mmap locking API: convert mmap_sem comments
    
    Convert comments that reference mmap_sem to reference mmap_lock instead.
    
    [akpm@linux-foundation.org: fix up linux-next leftovers]
    [akpm@linux-foundation.org: s/lockaphore/lock/, per Vlastimil]
    [akpm@linux-foundation.org: more linux-next fixups, per Michel]
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-13-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 60ea924e551d..d9961163da66 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -114,7 +114,7 @@ void uml_setup_stubs(struct mm_struct *mm)
 	mm->context.stub_pages[0] = virt_to_page(__syscall_stub_start);
 	mm->context.stub_pages[1] = virt_to_page(mm->context.id.stack);
 
-	/* dup_mmap already holds mmap_sem */
+	/* dup_mmap already holds mmap_lock */
 	err = install_special_mapping(mm, STUB_START, STUB_END - STUB_START,
 				      VM_READ | VM_MAYREAD | VM_EXEC |
 				      VM_MAYEXEC | VM_DONTCOPY | VM_PFNMAP,

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 3f0d9a573fd6..60ea924e551d 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -9,7 +9,6 @@
 #include <linux/slab.h>
 
 #include <asm/pgalloc.h>
-#include <asm/pgtable.h>
 #include <asm/sections.h>
 #include <as-layout.h>
 #include <os.h>

commit e19f97ed67d8f9b60e4ce14a7551d3dd45825570
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Dec 4 16:54:28 2019 -0800

    um: add support for folded p4d page tables
    
    The UML port uses 4 and 5 level fixups to support higher level page
    table directories in the generic VM code.
    
    Implement primitives necessary for the 4th level folding, add walks of
    p4d level where appropriate and drop usage of __ARCH_USE_5LEVEL_HACK.
    
    Link: http://lkml.kernel.org/r/1572938135-31886-13-git-send-email-rppt@kernel.org
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Anatoly Pugachev <matorola@gmail.com>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Peter Rosin <peda@axentia.se>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rolf Eike Beer <eike-kernel@sf-tec.de>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Sam Creasey <sammy@sammy.net>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index b5e3d91fc9c2..3f0d9a573fd6 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -19,15 +19,21 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 			 unsigned long kernel)
 {
 	pgd_t *pgd;
+	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 	pte_t *pte;
 
 	pgd = pgd_offset(mm, proc);
-	pud = pud_alloc(mm, pgd, proc);
-	if (!pud)
+
+	p4d = p4d_alloc(mm, pgd, proc);
+	if (!p4d)
 		goto out;
 
+	pud = pud_alloc(mm, p4d, proc);
+	if (!pud)
+		goto out_pud;
+
 	pmd = pmd_alloc(mm, pud, proc);
 	if (!pmd)
 		goto out_pmd;
@@ -44,6 +50,8 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	pmd_free(mm, pmd);
  out_pmd:
 	pud_free(mm, pud);
+ out_pud:
+	p4d_free(mm, p4d);
  out:
 	return -ENOMEM;
 }

commit 0d1fb0a47c09b21d82c680476da26035f402660a
Author: Alex Dewar <alex.dewar@gmx.co.uk>
Date:   Sun Aug 25 10:49:17 2019 +0100

    um: Add SPDX headers to files in arch/um/kernel/
    
    Convert files to use SPDX header. All files are licensed under the
    GPLv2.
    
    Signed-off-by: Alex Dewar <alex.dewar@gmx.co.uk>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index ace71f805700..b5e3d91fc9c2 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -1,7 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)
  * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
- * Licensed under the GPL
  */
 
 #include <linux/mm.h>

commit 0dafcbe128d2af48919619f79332ef219b5e5514
Author: Johannes Berg <johannes.berg@intel.com>
Date:   Fri Aug 23 13:16:23 2019 +0200

    um: Implement TRACE_IRQFLAGS_SUPPORT
    
    UML enables TRACE_IRQFLAGS_SUPPORT but doesn't actually implement
    it. It seems to have been added for lockdep support, but that can't
    actually really work well without IRQ flags tracing, as is also
    very noisily reported when enabling CONFIG_DEBUG_LOCKDEP.
    
    Implement it now.
    
    Fixes: 711553efa5b8 ("[PATCH] uml: declare in Kconfig our partial LOCKDEP support")
    Signed-off-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 29e7f5f9f188..ace71f805700 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -63,12 +63,12 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 	if (current->mm != NULL && current->mm != &init_mm)
 		from_mm = &current->mm->context;
 
-	block_signals();
+	block_signals_trace();
 	if (from_mm)
 		to_mm->id.u.pid = copy_context_skas0(stack,
 						     from_mm->id.u.pid);
 	else to_mm->id.u.pid = start_userspace(stack);
-	unblock_signals();
+	unblock_signals_trace();
 
 	if (to_mm->id.u.pid < 0) {
 		ret = to_mm->id.u.pid;

commit cb44c9a0ab21a9ae4dfcabac1ed8e38aa872d1af
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue May 21 10:03:48 2019 -0500

    signal: Remove task parameter from force_sigsegv
    
    The function force_sigsegv is always called on the current task
    so passing in current is redundant and not passing in current
    makes this fact obvious.
    
    This also makes it clear force_sigsegv always calls force_sig
    on the current task.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 7a1f2a936fd1..29e7f5f9f188 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -119,7 +119,7 @@ void uml_setup_stubs(struct mm_struct *mm)
 	return;
 
 out:
-	force_sigsegv(SIGSEGV, current);
+	force_sigsegv(SIGSEGV);
 }
 
 void arch_exit_mmap(struct mm_struct *mm)

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 3943e9d7d13d..7a1f2a936fd1 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -5,8 +5,9 @@
  */
 
 #include <linux/mm.h>
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/slab.h>
+
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
 #include <asm/sections.h>

commit 3ed3a4f0ddffece942bb2661924d87be4ce63cb7
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 17 14:19:11 2016 -0700

    mm: cleanup *pte_alloc* interfaces
    
    There are few things about *pte_alloc*() helpers worth cleaning up:
    
     - 'vma' argument is unused, let's drop it;
    
     - most __pte_alloc() callers do speculative check for pmd_none(),
       before taking ptl: let's introduce pte_alloc() macro which does
       the check.
    
       The only direct user of __pte_alloc left is userfaultfd, which has
       different expectation about atomicity wrt pmd.
    
     - pte_alloc_map() and pte_alloc_map_lock() are redefined using
       pte_alloc().
    
    [sudeep.holla@arm.com: fix build for arm64 hugetlbpage]
    [sfr@canb.auug.org.au: fix arch/arm/mm/mmu.c some more]
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 9591a66aa5c5..3943e9d7d13d 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -31,7 +31,7 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	if (!pmd)
 		goto out_pmd;
 
-	pte = pte_alloc_map(mm, NULL, pmd, proc);
+	pte = pte_alloc_map(mm, pmd, proc);
 	if (!pte)
 		goto out_pte;
 

commit 2eb5f31bc4ea24bb293e82934cfa1cce9573304b
Author: Anton Ivanov <aivanov@brocade.com>
Date:   Mon Nov 2 16:16:37 2015 +0000

    um: Switch clocksource to hrtimers
    
    UML is using an obsolete itimer call for
    all timers and "polls" for kernel space timer firing
    in its userspace portion resulting in a long list
    of bugs and incorrect behaviour(s). It also uses
    ITIMER_VIRTUAL for its timer which results in the
    timer being dependent on it running and the cpu
    load.
    
    This patch fixes this by moving to posix high resolution
    timers firing off CLOCK_MONOTONIC and relaying the timer
    correctly to the UML userspace.
    
    Fixes:
     - crashes when hosts suspends/resumes
     - broken userspace timers - effecive ~40Hz instead
       of what they should be. Note - this modifies skas behavior
       by no longer setting an itimer per clone(). Timer events
       are relayed instead.
     - kernel network packet scheduling disciplines
     - tcp behaviour especially under load
     - various timer related corner cases
    
    Finally, overall responsiveness of userspace is better.
    
    Signed-off-by: Thomas Meyer <thomas@m3y3r.de>
    Signed-off-by: Anton Ivanov <aivanov@brocade.com>
    [rw: massaged commit message]
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index fda1deba1757..9591a66aa5c5 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -1,4 +1,5 @@
 /*
+ * Copyright (C) 2015 Thomas Meyer (thomas@m3y3r.de)
  * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
  * Licensed under the GPL
  */
@@ -61,10 +62,12 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 	if (current->mm != NULL && current->mm != &init_mm)
 		from_mm = &current->mm->context;
 
+	block_signals();
 	if (from_mm)
 		to_mm->id.u.pid = copy_context_skas0(stack,
 						     from_mm->id.u.pid);
 	else to_mm->id.u.pid = start_userspace(stack);
+	unblock_signals();
 
 	if (to_mm->id.u.pid < 0) {
 		ret = to_mm->id.u.pid;

commit 05eacfd00ccf239a24cfb409b084fa126c2e9468
Author: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date:   Sun Oct 12 13:02:12 2014 +0200

    um: Use char[] for linker script address declarations
    
    The linker script defines some variables which are declared either with
    type char[] in include/asm-generic/sections.h or with a meaningless
    integer type in arch/um/include/asm/sections.h.
    
    Fix this inconsistency by declaring every variable char[].
    
    Signed-off-by: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 2216eddc6893..fda1deba1757 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -92,7 +92,7 @@ void uml_setup_stubs(struct mm_struct *mm)
 	int err, ret;
 
 	ret = init_stub_pte(mm, STUB_CODE,
-			    (unsigned long) &__syscall_stub_start);
+			    (unsigned long) __syscall_stub_start);
 	if (ret)
 		goto out;
 
@@ -100,7 +100,7 @@ void uml_setup_stubs(struct mm_struct *mm)
 	if (ret)
 		goto out;
 
-	mm->context.stub_pages[0] = virt_to_page(&__syscall_stub_start);
+	mm->context.stub_pages[0] = virt_to_page(__syscall_stub_start);
 	mm->context.stub_pages[1] = virt_to_page(mm->context.id.stack);
 
 	/* dup_mmap already holds mmap_sem */

commit d5f20be7cab85e6e701751323eea77c7935ae522
Author: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date:   Sun Oct 12 13:02:11 2014 +0200

    um: Create asm/sections.h
    
    arch/um/kernel/dyn.lds.S and arch/um/kernel/uml.lds.S define some
    UML-specific symbols.  These symbols are used in the kernel part of UML
    with extern declarations.
    
    Move these declarations to a new header, asm/sections.h, like other
    architectures do.
    
    Signed-off-by: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 94abdcc1d6ad..2216eddc6893 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -8,12 +8,11 @@
 #include <linux/slab.h>
 #include <asm/pgalloc.h>
 #include <asm/pgtable.h>
+#include <asm/sections.h>
 #include <as-layout.h>
 #include <os.h>
 #include <skas.h>
 
-extern int __syscall_stub_start;
-
 static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 			 unsigned long kernel)
 {

commit d0b5e15f0c0fdd759dd3dd48dc2dc2e7199e0da0
Author: Richard Weinberger <richard@nod.at>
Date:   Wed Mar 18 21:31:27 2015 +0100

    um: Remove SKAS3/4 support
    
    Before we had SKAS0 UML had two modes of operation
    TT (tracing thread) and SKAS3/4 (separated kernel address space).
    TT was known to be insecure and got removed a long time ago.
    SKAS3/4 required a few (3 or 4) patches on the host side which never went
    mainline. The last host patch is 10 years old.
    
    With SKAS0 mode (separated kernel address space using 0 host patches),
    default since 2005, SKAS3/4 is obsolete and can be removed.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 007d5503f49b..94abdcc1d6ad 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -54,35 +54,22 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 	unsigned long stack = 0;
 	int ret = -ENOMEM;
 
-	if (skas_needs_stub) {
-		stack = get_zeroed_page(GFP_KERNEL);
-		if (stack == 0)
-			goto out;
-	}
+	stack = get_zeroed_page(GFP_KERNEL);
+	if (stack == 0)
+		goto out;
 
 	to_mm->id.stack = stack;
 	if (current->mm != NULL && current->mm != &init_mm)
 		from_mm = &current->mm->context;
 
-	if (proc_mm) {
-		ret = new_mm(stack);
-		if (ret < 0) {
-			printk(KERN_ERR "init_new_context_skas - "
-			       "new_mm failed, errno = %d\n", ret);
-			goto out_free;
-		}
-		to_mm->id.u.mm_fd = ret;
-	}
-	else {
-		if (from_mm)
-			to_mm->id.u.pid = copy_context_skas0(stack,
-							     from_mm->id.u.pid);
-		else to_mm->id.u.pid = start_userspace(stack);
-
-		if (to_mm->id.u.pid < 0) {
-			ret = to_mm->id.u.pid;
-			goto out_free;
-		}
+	if (from_mm)
+		to_mm->id.u.pid = copy_context_skas0(stack,
+						     from_mm->id.u.pid);
+	else to_mm->id.u.pid = start_userspace(stack);
+
+	if (to_mm->id.u.pid < 0) {
+		ret = to_mm->id.u.pid;
+		goto out_free;
 	}
 
 	ret = init_new_ldt(to_mm, from_mm);
@@ -105,9 +92,6 @@ void uml_setup_stubs(struct mm_struct *mm)
 {
 	int err, ret;
 
-	if (!skas_needs_stub)
-		return;
-
 	ret = init_stub_pte(mm, STUB_CODE,
 			    (unsigned long) &__syscall_stub_start);
 	if (ret)
@@ -154,25 +138,19 @@ void destroy_context(struct mm_struct *mm)
 {
 	struct mm_context *mmu = &mm->context;
 
-	if (proc_mm)
-		os_close_file(mmu->id.u.mm_fd);
-	else {
-		/*
-		 * If init_new_context wasn't called, this will be
-		 * zero, resulting in a kill(0), which will result in the
-		 * whole UML suddenly dying.  Also, cover negative and
-		 * 1 cases, since they shouldn't happen either.
-		 */
-		if (mmu->id.u.pid < 2) {
-			printk(KERN_ERR "corrupt mm_context - pid = %d\n",
-			       mmu->id.u.pid);
-			return;
-		}
-		os_kill_ptraced_process(mmu->id.u.pid, 1);
+	/*
+	 * If init_new_context wasn't called, this will be
+	 * zero, resulting in a kill(0), which will result in the
+	 * whole UML suddenly dying.  Also, cover negative and
+	 * 1 cases, since they shouldn't happen either.
+	 */
+	if (mmu->id.u.pid < 2) {
+		printk(KERN_ERR "corrupt mm_context - pid = %d\n",
+		       mmu->id.u.pid);
+		return;
 	}
+	os_kill_ptraced_process(mmu->id.u.pid, 1);
 
-	if (skas_needs_stub)
-		free_page(mmu->id.stack);
-
+	free_page(mmu->id.stack);
 	free_ldt(mmu);
 }

commit dee20035b42d569984d2c16041b51e4d75e233b5
Author: Richard Weinberger <richard@nod.at>
Date:   Sun May 12 23:26:03 2013 +0200

    um: Mark stub pages mapping with VM_PFNMAP
    
    Ensure that a process cannot destroy his stub pages with
    using MADV_DONTNEED and friends.
    
    Reported-by: toralf.foerster@gmx.de
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index ff03067a3b14..007d5503f49b 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -123,7 +123,7 @@ void uml_setup_stubs(struct mm_struct *mm)
 	/* dup_mmap already holds mmap_sem */
 	err = install_special_mapping(mm, STUB_START, STUB_END - STUB_START,
 				      VM_READ | VM_MAYREAD | VM_EXEC |
-				      VM_MAYEXEC | VM_DONTCOPY,
+				      VM_MAYEXEC | VM_DONTCOPY | VM_PFNMAP,
 				      mm->context.stub_pages);
 	if (err) {
 		printk(KERN_ERR "install_special_mapping returned %d\n", err);

commit 37185b33240870719b6b5913a46e6a441f1ae96f
Author: Al Viro <viro@ZenIV.linux.org.uk>
Date:   Mon Oct 8 03:27:32 2012 +0100

    um: get rid of pointless include "..." where include <...> will do
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 0a49ef0c2bf4..ff03067a3b14 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -3,14 +3,14 @@
  * Licensed under the GPL
  */
 
-#include "linux/mm.h"
-#include "linux/sched.h"
-#include "linux/slab.h"
-#include "asm/pgalloc.h"
-#include "asm/pgtable.h"
-#include "as-layout.h"
-#include "os.h"
-#include "skas.h"
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <asm/pgalloc.h>
+#include <asm/pgtable.h>
+#include <as-layout.h>
+#include <os.h>
+#include <skas.h>
 
 extern int __syscall_stub_start;
 

commit 657b12d3a1508a3f06f7afe21d5dda7252279520
Author: Boaz Harrosh <bharrosh@panasas.com>
Date:   Mon Mar 26 19:18:22 2012 -0700

    um: uml_setup_stubs': warning: unused variable 'pages'
    
    Fix the following gcc complain
    arch/um/kernel/skas/mmu.c: In function 'uml_setup_stubs':
    arch/um/kernel/skas/mmu.c:106:16: warning: unused variable 'pages' [-Wunused-variable]
    
    Signed-Signed-off-by: Boaz Harrosh <bharrosh@panasas.com>
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 4947b319f53a..0a49ef0c2bf4 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -103,7 +103,6 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 
 void uml_setup_stubs(struct mm_struct *mm)
 {
-	struct page **pages;
 	int err, ret;
 
 	if (!skas_needs_stub)

commit ac2aca2817ca10ad30f86d0d3caa41ea8cc4f0f0
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Feb 23 01:37:19 2012 -0500

    um: uml_dup_mmap() relies on ->mmap_sem being held, but activate_mm() doesn't hold it
    
    ... while calling uml_dup_mmap()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index db2ef422c1b8..4947b319f53a 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -101,7 +101,7 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 	return ret;
 }
 
-void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
+void uml_setup_stubs(struct mm_struct *mm)
 {
 	struct page **pages;
 	int err, ret;

commit ea6fb41752dda2e450bd8a318c7400f8cfa9111e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Feb 23 01:33:19 2012 -0500

    um: embed ->stub_pages[] into mmu_context
    
    seriously, kmalloc() for two-element array of pointers?
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 1aee587e9c5d..db2ef422c1b8 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -92,8 +92,6 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 		goto out_free;
 	}
 
-	to_mm->stub_pages = NULL;
-
 	return 0;
 
  out_free:
@@ -120,29 +118,20 @@ void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
 	if (ret)
 		goto out;
 
-	pages = kmalloc(2 * sizeof(struct page *), GFP_KERNEL);
-	if (pages == NULL) {
-		printk(KERN_ERR "arch_dup_mmap failed to allocate 2 page "
-		       "pointers\n");
-		goto out;
-	}
-
-	pages[0] = virt_to_page(&__syscall_stub_start);
-	pages[1] = virt_to_page(mm->context.id.stack);
-	mm->context.stub_pages = pages;
+	mm->context.stub_pages[0] = virt_to_page(&__syscall_stub_start);
+	mm->context.stub_pages[1] = virt_to_page(mm->context.id.stack);
 
 	/* dup_mmap already holds mmap_sem */
 	err = install_special_mapping(mm, STUB_START, STUB_END - STUB_START,
 				      VM_READ | VM_MAYREAD | VM_EXEC |
-				      VM_MAYEXEC | VM_DONTCOPY, pages);
+				      VM_MAYEXEC | VM_DONTCOPY,
+				      mm->context.stub_pages);
 	if (err) {
 		printk(KERN_ERR "install_special_mapping returned %d\n", err);
-		goto out_free;
+		goto out;
 	}
 	return;
 
-out_free:
-	kfree(pages);
 out:
 	force_sigsegv(SIGSEGV, current);
 }
@@ -151,8 +140,6 @@ void arch_exit_mmap(struct mm_struct *mm)
 {
 	pte_t *pte;
 
-	if (mm->context.stub_pages != NULL)
-		kfree(mm->context.stub_pages);
 	pte = virt_to_pte(mm, STUB_CODE);
 	if (pte != NULL)
 		pte_clear(mm, STUB_CODE, pte);

commit 8ac1f8320a0073f28cf9e0491af4cd98f504f92a
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Thu Jan 13 15:46:43 2011 -0800

    thp: pte alloc trans splitting
    
    pte alloc routines must wait for split_huge_page if the pmd is not present
    and not null (i.e.  pmd_trans_splitting).  The additional branches are
    optimized away at compile time by pmd_trans_splitting if the config option
    is off.  However we must pass the vma down in order to know the anon_vma
    lock to wait for.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 3d099f974785..1aee587e9c5d 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -31,7 +31,7 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	if (!pmd)
 		goto out_pmd;
 
-	pte = pte_alloc_map(mm, pmd, proc);
+	pte = pte_alloc_map(mm, NULL, pmd, proc);
 	if (!pte)
 		goto out_pte;
 

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 8bfd1e905812..3d099f974785 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -5,6 +5,7 @@
 
 #include "linux/mm.h"
 #include "linux/sched.h"
+#include "linux/slab.h"
 #include "asm/pgalloc.h"
 #include "asm/pgtable.h"
 #include "as-layout.h"

commit 4d04c70754eec6d0fd342a5bc3f684db69cc2226
Author: Roel Kluin <roel.kluin@gmail.com>
Date:   Mon Sep 21 17:04:11 2009 -0700

    uml: fix order of pud and pmd_free()
    
    If pmd_alloc() fails we should only free the prior allocated pud, if
    pte_alloc_map() fails, we should free pmd as well.
    
    Signed-off-by: Roel Kluin <roel.kluin@gmail.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 0cd9a7a05e77..8bfd1e905812 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -38,10 +38,10 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	*pte = pte_mkread(*pte);
 	return 0;
 
- out_pmd:
-	pud_free(mm, pud);
  out_pte:
 	pmd_free(mm, pmd);
+ out_pmd:
+	pud_free(mm, pud);
  out:
 	return -ENOMEM;
 }

commit ac2a659968f5318a180213f0409c2ea21f072820
Author: Jeff Dike <jdike@addtoit.com>
Date:   Fri Feb 8 04:22:11 2008 -0800

    uml: fix mm_context memory leak
    
    [ Spotted by Miklos ]
    
    Fix a memory leak in init_new_context.  The struct page ** buffer allocated
    for install_special_mapping was never recorded, and thus leaked when the
    mm_struct was freed.  Fix it by saving the pointer in mm_context_t and freeing
    it in arch_exit_mmap.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Cc: Miklos Szeredi <miklos@szeredi.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 78b3e9f69d57..0cd9a7a05e77 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -91,6 +91,8 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 		goto out_free;
 	}
 
+	to_mm->stub_pages = NULL;
+
 	return 0;
 
  out_free:
@@ -126,6 +128,7 @@ void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
 
 	pages[0] = virt_to_page(&__syscall_stub_start);
 	pages[1] = virt_to_page(mm->context.id.stack);
+	mm->context.stub_pages = pages;
 
 	/* dup_mmap already holds mmap_sem */
 	err = install_special_mapping(mm, STUB_START, STUB_END - STUB_START,
@@ -147,6 +150,8 @@ void arch_exit_mmap(struct mm_struct *mm)
 {
 	pte_t *pte;
 
+	if (mm->context.stub_pages != NULL)
+		kfree(mm->context.stub_pages);
 	pte = virt_to_pte(mm, STUB_CODE);
 	if (pte != NULL)
 		pte_clear(mm, STUB_CODE, pte);

commit 00a905e6145ba200308a6a13e00248b85c600bd0
Author: Jeff Dike <jdike@addtoit.com>
Date:   Mon Feb 4 22:31:10 2008 -0800

    uml: don't kill pid 0
    
    A bit of defensive programming - during development, it ocassionally
    happens that a call to init_new_context is missed, resulting in
    context holding a host pid of zero.  When that address space is torn
    down, destroy_context does a kill(0), which instantly kills the whole
    UML without any errors whatsoever.
    
    This patch add a check for pids less than 2, to also catch 1 and
    negative pids.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index e8dc8540d444..78b3e9f69d57 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -164,8 +164,20 @@ void destroy_context(struct mm_struct *mm)
 
 	if (proc_mm)
 		os_close_file(mmu->id.u.mm_fd);
-	else
+	else {
+		/*
+		 * If init_new_context wasn't called, this will be
+		 * zero, resulting in a kill(0), which will result in the
+		 * whole UML suddenly dying.  Also, cover negative and
+		 * 1 cases, since they shouldn't happen either.
+		 */
+		if (mmu->id.u.pid < 2) {
+			printk(KERN_ERR "corrupt mm_context - pid = %d\n",
+			       mmu->id.u.pid);
+			return;
+		}
 		os_kill_ptraced_process(mmu->id.u.pid, 1);
+	}
 
 	if (skas_needs_stub)
 		free_page(mmu->id.stack);

commit 3963333fe6767f15141ab2dc3b933721c636c212
Author: Jeff Dike <jdike@addtoit.com>
Date:   Mon Feb 4 22:31:01 2008 -0800

    uml: cover stubs with a VMA
    
    Give the stubs a VMA.  This allows the removal of a truly nasty kludge to make
    sure that mm->nr_ptes was correct in exit_mmap.  The underlying problem was
    always that the stubs, which have ptes, and thus allocated a page table,
    weren't covered by a VMA.
    
    This patch fixes that by using install_special_mapping in arch_dup_mmap and
    activate_context to create the VMA.  The stubs have to be moved, since
    shift_arg_pages seems to assume that the stack is the only VMA present at that
    point during exec, and uses vma_adjust to fiddle its VMA.  However, that
    extends the stub VMA by the amount removed from the stack VMA.
    
    To avoid this problem, the stubs were moved to a different fixed location at
    the start of the address space.
    
    The init_stub_pte calls were moved from init_new_context to arch_dup_mmap
    because I was occasionally seeing arch_dup_mmap not being called, causing
    exit_mmap to die.  Rather than figure out what was really happening, I decided
    it was cleaner to just move the calls so that there's no doubt that both the
    pte and VMA creation happen, no matter what.  arch_exit_mmap is used to clear
    the stub ptes at exit time.
    
    The STUB_* constants in as-layout.h no longer depend on UM_TASK_SIZE, that
    that definition is removed, along with the comments complaining about gcc.
    
    Because the stubs are no longer at the top of the address space, some care is
    needed while flushing TLBs.  update_pte_range checks for addresses in the stub
    range and skips them.  flush_thread now issues two unmaps, one for the range
    before STUB_START and one for the range after STUB_END.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 6da9ab4f5a18..e8dc8540d444 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -34,25 +34,6 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	if (!pte)
 		goto out_pte;
 
-	/*
-	 * There's an interaction between the skas0 stub pages, stack
-	 * randomization, and the BUG at the end of exit_mmap.  exit_mmap
-	 * checks that the number of page tables freed is the same as had
-	 * been allocated.  If the stack is on the last page table page,
-	 * then the stack pte page will be freed, and if not, it won't.  To
-	 * avoid having to know where the stack is, or if the process mapped
-	 * something at the top of its address space for some other reason,
-	 * we set TASK_SIZE to end at the start of the last page table.
-	 * This keeps exit_mmap off the last page, but introduces a leak
-	 * of that page.  So, we hang onto it here and free it in
-	 * destroy_context_skas.
-	 */
-
-	mm->context.last_page_table = pmd_page_vaddr(*pmd);
-#ifdef CONFIG_3_LEVEL_PGTABLES
-	mm->context.last_pmd = (unsigned long) __va(pud_val(*pud));
-#endif
-
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
 	*pte = pte_mkread(*pte);
 	return 0;
@@ -76,24 +57,6 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 		stack = get_zeroed_page(GFP_KERNEL);
 		if (stack == 0)
 			goto out;
-
-		/*
-		 * This zeros the entry that pgd_alloc didn't, needed since
-		 * we are about to reinitialize it, and want mm.nr_ptes to
-		 * be accurate.
-		 */
-		mm->pgd[USER_PTRS_PER_PGD] = __pgd(0);
-
-		ret = init_stub_pte(mm, STUB_CODE,
-				    (unsigned long) &__syscall_stub_start);
-		if (ret)
-			goto out_free;
-
-		ret = init_stub_pte(mm, STUB_DATA, stack);
-		if (ret)
-			goto out_free;
-
-		mm->nr_ptes--;
 	}
 
 	to_mm->id.stack = stack;
@@ -137,6 +100,64 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 	return ret;
 }
 
+void arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)
+{
+	struct page **pages;
+	int err, ret;
+
+	if (!skas_needs_stub)
+		return;
+
+	ret = init_stub_pte(mm, STUB_CODE,
+			    (unsigned long) &__syscall_stub_start);
+	if (ret)
+		goto out;
+
+	ret = init_stub_pte(mm, STUB_DATA, mm->context.id.stack);
+	if (ret)
+		goto out;
+
+	pages = kmalloc(2 * sizeof(struct page *), GFP_KERNEL);
+	if (pages == NULL) {
+		printk(KERN_ERR "arch_dup_mmap failed to allocate 2 page "
+		       "pointers\n");
+		goto out;
+	}
+
+	pages[0] = virt_to_page(&__syscall_stub_start);
+	pages[1] = virt_to_page(mm->context.id.stack);
+
+	/* dup_mmap already holds mmap_sem */
+	err = install_special_mapping(mm, STUB_START, STUB_END - STUB_START,
+				      VM_READ | VM_MAYREAD | VM_EXEC |
+				      VM_MAYEXEC | VM_DONTCOPY, pages);
+	if (err) {
+		printk(KERN_ERR "install_special_mapping returned %d\n", err);
+		goto out_free;
+	}
+	return;
+
+out_free:
+	kfree(pages);
+out:
+	force_sigsegv(SIGSEGV, current);
+}
+
+void arch_exit_mmap(struct mm_struct *mm)
+{
+	pte_t *pte;
+
+	pte = virt_to_pte(mm, STUB_CODE);
+	if (pte != NULL)
+		pte_clear(mm, STUB_CODE, pte);
+
+	pte = virt_to_pte(mm, STUB_DATA);
+	if (pte == NULL)
+		return;
+
+	pte_clear(mm, STUB_DATA, pte);
+}
+
 void destroy_context(struct mm_struct *mm)
 {
 	struct mm_context *mmu = &mm->context;
@@ -146,15 +167,8 @@ void destroy_context(struct mm_struct *mm)
 	else
 		os_kill_ptraced_process(mmu->id.u.pid, 1);
 
-	if (!proc_mm || !ptrace_faultinfo) {
+	if (skas_needs_stub)
 		free_page(mmu->id.stack);
-		pte_lock_deinit(virt_to_page(mmu->last_page_table));
-		pte_free_kernel(mm, (pte_t *) mmu->last_page_table);
-		dec_zone_page_state(virt_to_page(mmu->last_page_table), NR_PAGETABLE);
-#ifdef CONFIG_3_LEVEL_PGTABLES
-		pmd_free(mm, (pmd_t *) mmu->last_pmd);
-#endif
-	}
 
 	free_ldt(mmu);
 }

commit 3e6f2ac480ce398ade2fd6b5e02d00d1265f1e0f
Author: Jeff Dike <jdike@addtoit.com>
Date:   Mon Feb 4 22:30:58 2008 -0800

    uml: kill processes instead of panicing kernel
    
    UML was panicing in the case of failures of libc calls which shouldn't happen.
     This is an overreaction since a failure from libc doesn't normally mean that
    kernel data structures are in an unknown state.  Instead, the current process
    should just be killed if there is no way to recover.
    
    The case that prompted this was a failure of PTRACE_SETREGS restoring the same
    state that was read by PTRACE_GETREGS.  It appears that when a process tries
    to load a bogus value into a segment register, it segfaults (as expected) and
    the value is actually loaded and is seen by PTRACE_GETREGS (not expected).
    
    This case is fixed by forcing a fatal SIGSEGV on the process so that it
    immediately dies.  fatal_sigsegv was added for this purpose.  It was declared
    as noreturn, so in order to pursuade gcc that it actually does not return, I
    added a call to os_dump_core (and declared it noreturn) so that I get a core
    file if somehow the process survives.
    
    All other calls in arch/um/os-Linux/skas/process.c got the same treatment,
    with failures causing the process to die instead of a kernel panic, with some
    exceptions.
    
    userspace_tramp exits with status 1 if anything goes wrong there.  That will
    cause start_userspace to return an error.  copy_context_skas0 and
    map_stub_pages also now return errors instead of panicing.  Callers of thes
    functions were changed to check for errors and do something appropriate.
    Usually that's to return an error to their callers.
    check_skas3_ptrace_faultinfo just exits since that's too early to do anything
    else.
    
    save_registers, restore_registers, and init_registers now return status
    instead of panicing on failure, with their callers doing something
    appropriate.
    
    There were also duplicate declarations of save_registers and restore_registers
    in os.h - these are gone.
    
    I noticed and fixed up some whitespace damage.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index b56fe8b67a81..6da9ab4f5a18 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -114,6 +114,11 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 			to_mm->id.u.pid = copy_context_skas0(stack,
 							     from_mm->id.u.pid);
 		else to_mm->id.u.pid = start_userspace(stack);
+
+		if (to_mm->id.u.pid < 0) {
+			ret = to_mm->id.u.pid;
+			goto out_free;
+		}
 	}
 
 	ret = init_new_ldt(to_mm, from_mm);

commit 5e5419734c8719cbc01af959ad9c0844002c0df5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Feb 4 22:29:14 2008 -0800

    add mm argument to pte/pmd/pud/pgd_free
    
    (with Martin Schwidefsky <schwidefsky@de.ibm.com>)
    
    The pgd/pud/pmd/pte page table allocation functions get a mm_struct pointer as
    first argument.  The free functions do not get the mm_struct argument.  This
    is 1) asymmetrical and 2) to do mm related page table allocations the mm
    argument is needed on the free function as well.
    
    [kamalesh@linux.vnet.ibm.com: i386 fix]
    [akpm@linux-foundation.org: coding-syle fixes]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index f859ec306cd5..b56fe8b67a81 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -58,9 +58,9 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	return 0;
 
  out_pmd:
-	pud_free(pud);
+	pud_free(mm, pud);
  out_pte:
-	pmd_free(pmd);
+	pmd_free(mm, pmd);
  out:
 	return -ENOMEM;
 }
@@ -144,10 +144,10 @@ void destroy_context(struct mm_struct *mm)
 	if (!proc_mm || !ptrace_faultinfo) {
 		free_page(mmu->id.stack);
 		pte_lock_deinit(virt_to_page(mmu->last_page_table));
-		pte_free_kernel((pte_t *) mmu->last_page_table);
+		pte_free_kernel(mm, (pte_t *) mmu->last_page_table);
 		dec_zone_page_state(virt_to_page(mmu->last_page_table), NR_PAGETABLE);
 #ifdef CONFIG_3_LEVEL_PGTABLES
-		pmd_free((pmd_t *) mmu->last_pmd);
+		pmd_free(mm, (pmd_t *) mmu->last_pmd);
 #endif
 	}
 

commit 54ae36f24b103e521dd683f66fe72b0584ccb7e2
Author: Jeff Dike <jdike@addtoit.com>
Date:   Tue Oct 16 01:27:33 2007 -0700

    uml: fix stub address calculations
    
    The calculation of CONFIG_STUB_CODE and CONFIG_STUB_DATA didn't take into
    account anything but 3G/1G and 2G/2G, leaving the other vmsplits out in the
    cold.
    
    I'd rather not duplicate the four known host vmsplit cases for each of these
    symbols.  I'd also like to calculate them based on the highest userspace
    address.
    
    The Kconfig language seems not to allow calculation of hex constants, so I
    moved this to as-layout.h.  CONFIG_STUB_CODE, CONFIG_STUB_DATA, and
    CONFIG_STUB_START are now gone.  In their place are STUB_CODE, STUB_DATA, and
    STUB_START in as-layout.h.
    
    i386 and x86_64 seem to differ as to whether an unadorned constant is an int
    or a long, so I cast them to unsigned long so they can be printed
    consistently.  However, they are also used in stub.S, where C types don't work
    so well.  So, there are ASM_ versions of these constants for use in stub.S.  I
    also ifdef-ed the non-asm-friendly portion of as-layout.h.
    
    With this in place, most of the rest of this patch is changing CONFIG_STUB_*
    to STUB_*, except in stub.S, where they are changed to ASM_STUB_*.
    
    defconfig has the old symbols deleted.
    
    I also print these addresses out in case there is any problem mapping them on
    the host.
    
    The two stub.S files had some trailing whitespace, so that is cleaned up here.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index ae79888cf520..f859ec306cd5 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -7,6 +7,7 @@
 #include "linux/sched.h"
 #include "asm/pgalloc.h"
 #include "asm/pgtable.h"
+#include "as-layout.h"
 #include "os.h"
 #include "skas.h"
 
@@ -83,12 +84,12 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 		 */
 		mm->pgd[USER_PTRS_PER_PGD] = __pgd(0);
 
-		ret = init_stub_pte(mm, CONFIG_STUB_CODE,
+		ret = init_stub_pte(mm, STUB_CODE,
 				    (unsigned long) &__syscall_stub_start);
 		if (ret)
 			goto out_free;
 
-		ret = init_stub_pte(mm, CONFIG_STUB_DATA, stack);
+		ret = init_stub_pte(mm, STUB_DATA, stack);
 		if (ret)
 			goto out_free;
 

commit 28078e8f9790be0854a54f06de822689ab571944
Author: Jeff Dike <jdike@addtoit.com>
Date:   Tue Oct 16 01:27:08 2007 -0700

    uml: free LDT state on process exit
    
    The space allocated for a process LDT wasn't being freed when the process
    exited.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 48c8c136c038..ae79888cf520 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -149,4 +149,6 @@ void destroy_context(struct mm_struct *mm)
 		pmd_free((pmd_t *) mmu->last_pmd);
 #endif
 	}
+
+	free_ldt(mmu);
 }

commit 6c738ffa9fea6869f5d51882dfefbba746e432b1
Author: Jeff Dike <jdike@addtoit.com>
Date:   Tue Oct 16 01:27:06 2007 -0700

    uml: fold mmu_context_skas into mm_context
    
    This patch folds mmu_context_skas into struct mm_context, changing all users
    of these structures as needed.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index c5475ecd9fd4..48c8c136c038 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -47,9 +47,9 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	 * destroy_context_skas.
 	 */
 
-	mm->context.skas.last_page_table = pmd_page_vaddr(*pmd);
+	mm->context.last_page_table = pmd_page_vaddr(*pmd);
 #ifdef CONFIG_3_LEVEL_PGTABLES
-	mm->context.skas.last_pmd = (unsigned long) __va(pud_val(*pud));
+	mm->context.last_pmd = (unsigned long) __va(pud_val(*pud));
 #endif
 
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
@@ -66,8 +66,8 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 
 int init_new_context(struct task_struct *task, struct mm_struct *mm)
 {
-	struct mmu_context_skas *from_mm = NULL;
-	struct mmu_context_skas *to_mm = &mm->context.skas;
+ 	struct mm_context *from_mm = NULL;
+	struct mm_context *to_mm = &mm->context;
 	unsigned long stack = 0;
 	int ret = -ENOMEM;
 
@@ -97,7 +97,7 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 
 	to_mm->id.stack = stack;
 	if (current->mm != NULL && current->mm != &init_mm)
-		from_mm = &current->mm->context.skas;
+		from_mm = &current->mm->context;
 
 	if (proc_mm) {
 		ret = new_mm(stack);
@@ -133,7 +133,7 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 
 void destroy_context(struct mm_struct *mm)
 {
-	struct mmu_context_skas *mmu = &mm->context.skas;
+	struct mm_context *mmu = &mm->context;
 
 	if (proc_mm)
 		os_close_file(mmu->id.u.mm_fd);

commit ba180fd437156f7fd8cfb2fdd021d949eeef08d6
Author: Jeff Dike <jdike@addtoit.com>
Date:   Tue Oct 16 01:27:00 2007 -0700

    uml: style fixes pass 3
    
    Formatting changes in the files which have been changed in the course
    of folding foo_skas functions into their callers.  These include:
            copyright updates
            header file trimming
            style fixes
            adding severity to printks
    
    These changes should be entirely non-functional.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 902d74138952..c5475ecd9fd4 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -1,20 +1,12 @@
-/* 
- * Copyright (C) 2002 Jeff Dike (jdike@karaya.com)
+/*
+ * Copyright (C) 2002 - 2007 Jeff Dike (jdike@{addtoit,linux.intel}.com)
  * Licensed under the GPL
  */
 
-#include "linux/sched.h"
-#include "linux/list.h"
-#include "linux/spinlock.h"
-#include "linux/slab.h"
-#include "linux/errno.h"
 #include "linux/mm.h"
-#include "asm/current.h"
-#include "asm/segment.h"
-#include "asm/mmu.h"
+#include "linux/sched.h"
 #include "asm/pgalloc.h"
 #include "asm/pgtable.h"
-#include "asm/ldt.h"
 #include "os.h"
 #include "skas.h"
 
@@ -41,10 +33,11 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	if (!pte)
 		goto out_pte;
 
-	/* There's an interaction between the skas0 stub pages, stack
+	/*
+	 * There's an interaction between the skas0 stub pages, stack
 	 * randomization, and the BUG at the end of exit_mmap.  exit_mmap
-         * checks that the number of page tables freed is the same as had
-         * been allocated.  If the stack is on the last page table page,
+	 * checks that the number of page tables freed is the same as had
+	 * been allocated.  If the stack is on the last page table page,
 	 * then the stack pte page will be freed, and if not, it won't.  To
 	 * avoid having to know where the stack is, or if the process mapped
 	 * something at the top of its address space for some other reason,
@@ -54,36 +47,37 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	 * destroy_context_skas.
 	 */
 
-        mm->context.skas.last_page_table = pmd_page_vaddr(*pmd);
+	mm->context.skas.last_page_table = pmd_page_vaddr(*pmd);
 #ifdef CONFIG_3_LEVEL_PGTABLES
-        mm->context.skas.last_pmd = (unsigned long) __va(pud_val(*pud));
+	mm->context.skas.last_pmd = (unsigned long) __va(pud_val(*pud));
 #endif
 
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
 	*pte = pte_mkread(*pte);
-	return(0);
+	return 0;
 
  out_pmd:
 	pud_free(pud);
  out_pte:
 	pmd_free(pmd);
  out:
-	return(-ENOMEM);
+	return -ENOMEM;
 }
 
 int init_new_context(struct task_struct *task, struct mm_struct *mm)
 {
- 	struct mmu_context_skas *from_mm = NULL;
+	struct mmu_context_skas *from_mm = NULL;
 	struct mmu_context_skas *to_mm = &mm->context.skas;
 	unsigned long stack = 0;
 	int ret = -ENOMEM;
 
-	if(skas_needs_stub){
+	if (skas_needs_stub) {
 		stack = get_zeroed_page(GFP_KERNEL);
-		if(stack == 0)
+		if (stack == 0)
 			goto out;
 
-		/* This zeros the entry that pgd_alloc didn't, needed since
+		/*
+		 * This zeros the entry that pgd_alloc didn't, needed since
 		 * we are about to reinitialize it, and want mm.nr_ptes to
 		 * be accurate.
 		 */
@@ -91,39 +85,39 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 
 		ret = init_stub_pte(mm, CONFIG_STUB_CODE,
 				    (unsigned long) &__syscall_stub_start);
-		if(ret)
+		if (ret)
 			goto out_free;
 
 		ret = init_stub_pte(mm, CONFIG_STUB_DATA, stack);
-		if(ret)
+		if (ret)
 			goto out_free;
 
 		mm->nr_ptes--;
 	}
 
 	to_mm->id.stack = stack;
-	if(current->mm != NULL && current->mm != &init_mm)
+	if (current->mm != NULL && current->mm != &init_mm)
 		from_mm = &current->mm->context.skas;
 
-	if(proc_mm){
+	if (proc_mm) {
 		ret = new_mm(stack);
-		if(ret < 0){
-			printk("init_new_context_skas - new_mm failed, "
-			       "errno = %d\n", ret);
+		if (ret < 0) {
+			printk(KERN_ERR "init_new_context_skas - "
+			       "new_mm failed, errno = %d\n", ret);
 			goto out_free;
 		}
 		to_mm->id.u.mm_fd = ret;
 	}
 	else {
-		if(from_mm)
+		if (from_mm)
 			to_mm->id.u.pid = copy_context_skas0(stack,
 							     from_mm->id.u.pid);
 		else to_mm->id.u.pid = start_userspace(stack);
 	}
 
 	ret = init_new_ldt(to_mm, from_mm);
-	if(ret < 0){
-		printk("init_new_context_skas - init_ldt"
+	if (ret < 0) {
+		printk(KERN_ERR "init_new_context_skas - init_ldt"
 		       " failed, errno = %d\n", ret);
 		goto out_free;
 	}
@@ -131,7 +125,7 @@ int init_new_context(struct task_struct *task, struct mm_struct *mm)
 	return 0;
 
  out_free:
-	if(to_mm->id.stack != 0)
+	if (to_mm->id.stack != 0)
 		free_page(to_mm->id.stack);
  out:
 	return ret;
@@ -141,12 +135,12 @@ void destroy_context(struct mm_struct *mm)
 {
 	struct mmu_context_skas *mmu = &mm->context.skas;
 
-	if(proc_mm)
+	if (proc_mm)
 		os_close_file(mmu->id.u.mm_fd);
 	else
 		os_kill_ptraced_process(mmu->id.u.pid, 1);
 
-	if(!proc_mm || !ptrace_faultinfo){
+	if (!proc_mm || !ptrace_faultinfo) {
 		free_page(mmu->id.stack);
 		pte_lock_deinit(virt_to_page(mmu->last_page_table));
 		pte_free_kernel((pte_t *) mmu->last_page_table);

commit 77bf4400319db9d2a8af6b00c2be6faa0f3d07cb
Author: Jeff Dike <jdike@addtoit.com>
Date:   Tue Oct 16 01:26:58 2007 -0700

    uml: remove code made redundant by CHOOSE_MODE removal
    
    This patch makes a number of simplifications enabled by the removal of
    CHOOSE_MODE.  There were lots of functions that looked like
    
            int foo(args){
                    foo_skas(args);
            }
    
    The bodies of foo_skas are now folded into foo, and their declarations (and
    sometimes entire header files) are deleted.
    
    In addition, the union uml_pt_regs, which was a union between the tt and skas
    register formats, is now a struct, with the tt-mode arm of the union being
    removed.
    
    It turns out that usr2_handler was unused, so it is gone.
    
    Signed-off-by: Jeff Dike <jdike@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 2c6d090a2e87..902d74138952 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -71,7 +71,7 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	return(-ENOMEM);
 }
 
-int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
+int init_new_context(struct task_struct *task, struct mm_struct *mm)
 {
  	struct mmu_context_skas *from_mm = NULL;
 	struct mmu_context_skas *to_mm = &mm->context.skas;
@@ -137,7 +137,7 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 	return ret;
 }
 
-void destroy_context_skas(struct mm_struct *mm)
+void destroy_context(struct mm_struct *mm)
 {
 	struct mmu_context_skas *mmu = &mm->context.skas;
 

commit 21c935e5298a3bc4f16cdff7ce3b1efab92b995e
Author: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
Date:   Wed Oct 11 01:21:32 2006 -0700

    [PATCH] uml: correct removal of pte_mkexec
    
    Correct commit 5906e4171ad61ce68de95e51b773146707671f80 - this makes more
    sense: we turn pte_mkexec + pte_wrprotect to pte_mkread.  However, due to a
    bug in pte_mkread, it does the exact same thing as pte_mkwrite, so this patch
    improves the code but does not change anything in practice.  The pte_mkread
    bug is fixed separately, as it may have big impact.
    
    Signed-off-by: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index c17eddcf89b3..2c6d090a2e87 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -60,10 +60,7 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 #endif
 
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
-	/* This is wrong for the code page, but it doesn't matter since the
-	 * stub is mapped by hand with the correct permissions.
-	 */
-	*pte = pte_mkwrite(*pte);
+	*pte = pte_mkread(*pte);
 	return(0);
 
  out_pmd:

commit 038b0a6d8d32db934bba6a24e74e76e4e327a94f
Author: Dave Jones <davej@redhat.com>
Date:   Wed Oct 4 03:38:54 2006 -0400

    Remove all inclusions of <linux/config.h>
    kbuild explicitly includes this at build time.
    
    Signed-off-by: Dave Jones <davej@redhat.com>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 4cd2ff546ef6..c17eddcf89b3 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -3,7 +3,6 @@
  * Licensed under the GPL
  */
 
-#include "linux/config.h"
 #include "linux/sched.h"
 #include "linux/list.h"
 #include "linux/spinlock.h"

commit 5906e4171ad61ce68de95e51b773146707671f80
Author: Jeff Dike <jdike@addtoit.com>
Date:   Fri Sep 29 01:58:54 2006 -0700

    [PATCH] uml: remove pte_mkexec
    
    Andi is making pte_mkexec go away, and UML had one of the last uses.
    
    This removes the use and the definition.
    
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Andi Kleen <ak@muc.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 79c22707a637..4cd2ff546ef6 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -61,8 +61,10 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 #endif
 
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
-	*pte = pte_mkexec(*pte);
-	*pte = pte_wrprotect(*pte);
+	/* This is wrong for the code page, but it doesn't matter since the
+	 * stub is mapped by hand with the correct permissions.
+	 */
+	*pte = pte_mkwrite(*pte);
 	return(0);
 
  out_pmd:

commit 46a82b2d5591335277ed2930611f6acb4ce654ed
Author: Dave McCracken <dmccr@us.ibm.com>
Date:   Mon Sep 25 23:31:48 2006 -0700

    [PATCH] Standardize pxx_page macros
    
    One of the changes necessary for shared page tables is to standardize the
    pxx_page macros.  pte_page and pmd_page have always returned the struct
    page associated with their entry, while pte_page_kernel and pmd_page_kernel
    have returned the kernel virtual address.  pud_page and pgd_page, on the
    other hand, return the kernel virtual address.
    
    Shared page tables needs pud_page and pgd_page to return the actual page
    structures.  There are very few actual users of these functions, so it is
    simple to standardize their usage.
    
    Since this is basic cleanup, I am submitting these changes as a standalone
    patch.  Per Hugh Dickins' comments about it, I am also changing the
    pxx_page_kernel macros to pxx_page_vaddr to clarify their meaning.
    
    Signed-off-by: Dave McCracken <dmccr@us.ibm.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 624ca238d1fd..79c22707a637 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -55,7 +55,7 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	 * destroy_context_skas.
 	 */
 
-        mm->context.skas.last_page_table = pmd_page_kernel(*pmd);
+        mm->context.skas.last_page_table = pmd_page_vaddr(*pmd);
 #ifdef CONFIG_3_LEVEL_PGTABLES
         mm->context.skas.last_pmd = (unsigned long) __va(pud_val(*pud));
 #endif

commit df849a1529c106f7460e51479ca78fe07b07dc8c
Author: Christoph Lameter <clameter@sgi.com>
Date:   Fri Jun 30 01:55:38 2006 -0700

    [PATCH] zoned vm counters: conversion of nr_pagetables to per zone counter
    
    Conversion of nr_page_table_pages to a per zone counter
    
    [akpm@osdl.org: bugfix]
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index c5c9885a8297..624ca238d1fd 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -152,7 +152,7 @@ void destroy_context_skas(struct mm_struct *mm)
 		free_page(mmu->id.stack);
 		pte_lock_deinit(virt_to_page(mmu->last_page_table));
 		pte_free_kernel((pte_t *) mmu->last_page_table);
-                dec_page_state(nr_page_table_pages);
+		dec_zone_page_state(virt_to_page(mmu->last_page_table), NR_PAGETABLE);
 #ifdef CONFIG_3_LEVEL_PGTABLES
 		pmd_free((pmd_t *) mmu->last_pmd);
 #endif

commit 12919aa6e015dd85170fc3b1a3e10a5dfd116c72
Author: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
Date:   Wed Jan 18 17:42:39 2006 -0800

    [PATCH] uml: move LDT creation
    
    s390 doesn't have a LDT.  So MM_COPY_SEGMENTS will not be supported on s390.
    
    The only user of MM_COPY_SEGMENTS is new_mm(), but that's no longer useful, as
    arch/sys-i386/ldt.c defines init_new_ldt(), which is called immediately after
    new_mm().  So we should copy host's LDT in init_new_ldt(), if /proc/mm is
    available, to have this subarch specific call in subarch code.
    
    Signed-off-by: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 677871f1b37c..c5c9885a8297 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -78,7 +78,7 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
  	struct mmu_context_skas *from_mm = NULL;
 	struct mmu_context_skas *to_mm = &mm->context.skas;
 	unsigned long stack = 0;
-	int from_fd, ret = -ENOMEM;
+	int ret = -ENOMEM;
 
 	if(skas_needs_stub){
 		stack = get_zeroed_page(GFP_KERNEL);
@@ -108,11 +108,7 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 		from_mm = &current->mm->context.skas;
 
 	if(proc_mm){
-		if(from_mm)
-			from_fd = from_mm->id.u.mm_fd;
-		else from_fd = -1;
-
-		ret = new_mm(from_fd, stack);
+		ret = new_mm(stack);
 		if(ret < 0){
 			printk("init_new_context_skas - new_mm failed, "
 			       "errno = %d\n", ret);

commit 858259cf7d1c443c836a2022b78cb281f0a9b95e
Author: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
Date:   Mon Nov 7 00:58:55 2005 -0800

    [PATCH] uml: maintain own LDT entries
    
    Patch imlements full LDT handling in SKAS:
     * UML holds it's own LDT table, used to deliver data on
       modify_ldt(READ)
     * UML disables the default_ldt, inherited from the host (SKAS3)
       or resets LDT entries, set by host's clib and inherited in
       SKAS0
     * A new global variable skas_needs_stub is inserted, that
       can be used to decide, whether stub-pages must be supported
       or not.
     * Uses the syscall-stub to replace missing PTRACE_LDT (therefore,
       write_ldt_entry needs to be modified)
    
    Signed-off-by: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo Giarrusso <blaisorblade@yahoo.it>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 9e5e39cea821..677871f1b37c 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -15,6 +15,7 @@
 #include "asm/mmu.h"
 #include "asm/pgalloc.h"
 #include "asm/pgtable.h"
+#include "asm/ldt.h"
 #include "os.h"
 #include "skas.h"
 
@@ -74,13 +75,12 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 
 int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 {
-	struct mm_struct *cur_mm = current->mm;
-	struct mm_id *cur_mm_id = &cur_mm->context.skas.id;
-	struct mm_id *mm_id = &mm->context.skas.id;
+ 	struct mmu_context_skas *from_mm = NULL;
+	struct mmu_context_skas *to_mm = &mm->context.skas;
 	unsigned long stack = 0;
-	int from, ret = -ENOMEM;
+	int from_fd, ret = -ENOMEM;
 
-	if(!proc_mm || !ptrace_faultinfo){
+	if(skas_needs_stub){
 		stack = get_zeroed_page(GFP_KERNEL);
 		if(stack == 0)
 			goto out;
@@ -102,33 +102,43 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 
 		mm->nr_ptes--;
 	}
-	mm_id->stack = stack;
+
+	to_mm->id.stack = stack;
+	if(current->mm != NULL && current->mm != &init_mm)
+		from_mm = &current->mm->context.skas;
 
 	if(proc_mm){
-		if((cur_mm != NULL) && (cur_mm != &init_mm))
-			from = cur_mm_id->u.mm_fd;
-		else from = -1;
+		if(from_mm)
+			from_fd = from_mm->id.u.mm_fd;
+		else from_fd = -1;
 
-		ret = new_mm(from, stack);
+		ret = new_mm(from_fd, stack);
 		if(ret < 0){
 			printk("init_new_context_skas - new_mm failed, "
 			       "errno = %d\n", ret);
 			goto out_free;
 		}
-		mm_id->u.mm_fd = ret;
+		to_mm->id.u.mm_fd = ret;
 	}
 	else {
-		if((cur_mm != NULL) && (cur_mm != &init_mm))
-			mm_id->u.pid = copy_context_skas0(stack,
-							  cur_mm_id->u.pid);
-		else mm_id->u.pid = start_userspace(stack);
+		if(from_mm)
+			to_mm->id.u.pid = copy_context_skas0(stack,
+							     from_mm->id.u.pid);
+		else to_mm->id.u.pid = start_userspace(stack);
+	}
+
+	ret = init_new_ldt(to_mm, from_mm);
+	if(ret < 0){
+		printk("init_new_context_skas - init_ldt"
+		       " failed, errno = %d\n", ret);
+		goto out_free;
 	}
 
 	return 0;
 
  out_free:
-	if(mm_id->stack != 0)
-		free_page(mm_id->stack);
+	if(to_mm->id.stack != 0)
+		free_page(to_mm->id.stack);
  out:
 	return ret;
 }

commit 4c21e2f2441dc5fbb957b030333f5a3f2d02dea7
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:40 2005 -0700

    [PATCH] mm: split page table lock
    
    Christoph Lameter demonstrated very poor scalability on the SGI 512-way, with
    a many-threaded application which concurrently initializes different parts of
    a large anonymous area.
    
    This patch corrects that, by using a separate spinlock per page table page, to
    guard the page table entries in that page, instead of using the mm's single
    page_table_lock.  (But even then, page_table_lock is still used to guard page
    table allocation, and anon_vma allocation.)
    
    In this implementation, the spinlock is tucked inside the struct page of the
    page table page: with a BUILD_BUG_ON in case it overflows - which it would in
    the case of 32-bit PA-RISC with spinlock debugging enabled.
    
    Splitting the lock is not quite for free: another cacheline access.  Ideally,
    I suppose we would use split ptlock only for multi-threaded processes on
    multi-cpu machines; but deciding that dynamically would have its own costs.
    So for now enable it by config, at some number of cpus - since the Kconfig
    language doesn't support inequalities, let preprocessor compare that with
    NR_CPUS.  But I don't think it's worth being user-configurable: for good
    testing of both split and unsplit configs, split now at 4 cpus, and perhaps
    change that to 8 later.
    
    There is a benefit even for singly threaded processes: kswapd can be attacking
    one part of the mm while another part is busy faulting.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 02cf36e0331a..9e5e39cea821 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -144,6 +144,7 @@ void destroy_context_skas(struct mm_struct *mm)
 
 	if(!proc_mm || !ptrace_faultinfo){
 		free_page(mmu->id.stack);
+		pte_lock_deinit(virt_to_page(mmu->last_page_table));
 		pte_free_kernel((pte_t *) mmu->last_page_table);
                 dec_page_state(nr_page_table_pages);
 #ifdef CONFIG_3_LEVEL_PGTABLES

commit b462705ac679f6195d1b23a752cda592d9107495
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:24 2005 -0700

    [PATCH] mm: arches skip ptlock
    
    Convert those few architectures which are calling pud_alloc, pmd_alloc,
    pte_alloc_map on a user mm, not to take the page_table_lock first, nor drop it
    after.  Each of these can continue to use pte_alloc_map, no need to change
    over to pte_alloc_map_lock, they're neither racy nor swappable.
    
    In the sparc64 io_remap_pfn_range, flush_tlb_range then falls outside of the
    page_table_lock: that's okay, on sparc64 it's like flush_tlb_mm, and that has
    always been called from outside of page_table_lock in dup_mmap.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 240143b616a2..02cf36e0331a 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -28,7 +28,6 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	pmd_t *pmd;
 	pte_t *pte;
 
-	spin_lock(&mm->page_table_lock);
 	pgd = pgd_offset(mm, proc);
 	pud = pud_alloc(mm, pgd, proc);
 	if (!pud)
@@ -63,7 +62,6 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
 	*pte = pte_mkexec(*pte);
 	*pte = pte_wrprotect(*pte);
-	spin_unlock(&mm->page_table_lock);
 	return(0);
 
  out_pmd:
@@ -71,7 +69,6 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
  out_pte:
 	pmd_free(pmd);
  out:
-	spin_unlock(&mm->page_table_lock);
 	return(-ENOMEM);
 }
 

commit 7ef939054139ef857cebbec07cbd12d7cf7beedd
Author: Jeff Dike <jdike@addtoit.com>
Date:   Sat Sep 3 15:57:52 2005 -0700

    [PATCH] uml: fix x86_64 page leak
    
    We were leaking pmd pages when 3_LEVEL_PGTABLES was enabled.  This fixes that.
    
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index d837223e22af..240143b616a2 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -56,6 +56,9 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 	 */
 
         mm->context.skas.last_page_table = pmd_page_kernel(*pmd);
+#ifdef CONFIG_3_LEVEL_PGTABLES
+        mm->context.skas.last_pmd = (unsigned long) __va(pud_val(*pud));
+#endif
 
 	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
 	*pte = pte_mkexec(*pte);
@@ -144,6 +147,10 @@ void destroy_context_skas(struct mm_struct *mm)
 
 	if(!proc_mm || !ptrace_faultinfo){
 		free_page(mmu->id.stack);
-		free_page(mmu->last_page_table);
+		pte_free_kernel((pte_t *) mmu->last_page_table);
+                dec_page_state(nr_page_table_pages);
+#ifdef CONFIG_3_LEVEL_PGTABLES
+		pmd_free((pmd_t *) mmu->last_pmd);
+#endif
 	}
 }

commit 8b51304ed3184826fb262c1e9d3e58b0b00fd083
Author: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
Date:   Sat Sep 3 15:57:49 2005 -0700

    [PATCH] uml: increase granularity of host capability checking
    
    This change enables SKAS0/SKAS3 to work with all combinations of /proc/mm and
    PTRACE_FAULTINFO being available or not.
    
    Also it changes the initialization of proc_mm and ptrace_faultinfo slightly,
    to ease forcing SKAS0 on a patched host.  Forcing UML to run without /proc/mm
    or PTRACE_FAULTINFO by cmdline parameter can be implemented with a setup
    resetting the related variable.
    
    Signed-off-by: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index d232daa42c31..d837223e22af 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -77,23 +77,14 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 	struct mm_struct *cur_mm = current->mm;
 	struct mm_id *cur_mm_id = &cur_mm->context.skas.id;
 	struct mm_id *mm_id = &mm->context.skas.id;
-	unsigned long stack;
-	int from, ret;
+	unsigned long stack = 0;
+	int from, ret = -ENOMEM;
 
-	if(proc_mm){
-		if((cur_mm != NULL) && (cur_mm != &init_mm))
-			from = cur_mm->context.skas.id.u.mm_fd;
-		else from = -1;
+	if(!proc_mm || !ptrace_faultinfo){
+		stack = get_zeroed_page(GFP_KERNEL);
+		if(stack == 0)
+			goto out;
 
-		ret = new_mm(from);
-		if(ret < 0){
-			printk("init_new_context_skas - new_mm failed, "
-			       "errno = %d\n", ret);
-			return ret;
-		}
-		mm_id->u.mm_fd = ret;
-	}
-	else {
 		/* This zeros the entry that pgd_alloc didn't, needed since
 		 * we are about to reinitialize it, and want mm.nr_ptes to
 		 * be accurate.
@@ -103,20 +94,30 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 		ret = init_stub_pte(mm, CONFIG_STUB_CODE,
 				    (unsigned long) &__syscall_stub_start);
 		if(ret)
-			goto out;
-
-		ret = -ENOMEM;
-		stack = get_zeroed_page(GFP_KERNEL);
-		if(stack == 0)
-			goto out;
-		mm_id->stack = stack;
+			goto out_free;
 
 		ret = init_stub_pte(mm, CONFIG_STUB_DATA, stack);
 		if(ret)
 			goto out_free;
 
 		mm->nr_ptes--;
+	}
+	mm_id->stack = stack;
 
+	if(proc_mm){
+		if((cur_mm != NULL) && (cur_mm != &init_mm))
+			from = cur_mm_id->u.mm_fd;
+		else from = -1;
+
+		ret = new_mm(from, stack);
+		if(ret < 0){
+			printk("init_new_context_skas - new_mm failed, "
+			       "errno = %d\n", ret);
+			goto out_free;
+		}
+		mm_id->u.mm_fd = ret;
+	}
+	else {
 		if((cur_mm != NULL) && (cur_mm != &init_mm))
 			mm_id->u.pid = copy_context_skas0(stack,
 							  cur_mm_id->u.pid);
@@ -126,7 +127,8 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 	return 0;
 
  out_free:
-	free_page(mm_id->stack);
+	if(mm_id->stack != 0)
+		free_page(mm_id->stack);
  out:
 	return ret;
 }
@@ -137,8 +139,10 @@ void destroy_context_skas(struct mm_struct *mm)
 
 	if(proc_mm)
 		os_close_file(mmu->id.u.mm_fd);
-	else {
+	else
 		os_kill_ptraced_process(mmu->id.u.pid, 1);
+
+	if(!proc_mm || !ptrace_faultinfo){
 		free_page(mmu->id.stack);
 		free_page(mmu->last_page_table);
 	}

commit 9786a8f3cbc61f990266e23ffdb338ee3118b03d
Author: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
Date:   Thu Jul 7 17:56:50 2005 -0700

    [PATCH] uml: Proper clone support for skas0
    
    This patch implements the clone-stub mechanism, which allows skas0 to run
    with proc_mm==0, even if the clib in UML uses modify_ldt.
    
    Note: There is a bug in skas3.v7 host patch, that avoids UML-skas from
    running properly on a SMP-box.  In full skas3, I never really saw problems,
    but in skas0 they showed up.
    
    More commentary by jdike - What this patch does is makes sure that the host
    parent of each new host process matches the UML parent of the corresponding
    UML process.  This ensures that any changed LDTs are inherited.  This is
    done by having clone actually called by the UML process from its stub,
    rather than by the kernel.  We have special syscall stubs that are loaded
    onto the stub code page because that code must be completely
    self-contained.  These stubs are given C interfaces, and used like normal C
    functions, but there are subtleties.  Principally, we have to be careful
    about stack variables in stub_clone_handler after the clone.  The code is
    written so that there aren't any - everything boils down to a fixed
    address.  If there were any locals, references to them after the clone
    would be wrong because the stack just changed.
    
    Signed-off-by: Bodo Stroesser <bstroesser@fujitsu-siemens.com>
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 511a855c9ec0..d232daa42c31 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -75,6 +75,7 @@ static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
 int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 {
 	struct mm_struct *cur_mm = current->mm;
+	struct mm_id *cur_mm_id = &cur_mm->context.skas.id;
 	struct mm_id *mm_id = &mm->context.skas.id;
 	unsigned long stack;
 	int from, ret;
@@ -115,7 +116,11 @@ int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 			goto out_free;
 
 		mm->nr_ptes--;
-		mm_id->u.pid = start_userspace(stack);
+
+		if((cur_mm != NULL) && (cur_mm != &init_mm))
+			mm_id->u.pid = copy_context_skas0(stack,
+							  cur_mm_id->u.pid);
+		else mm_id->u.pid = start_userspace(stack);
 	}
 
 	return 0;

commit d67b569f5f620c0fb95d5212642746b7ba9d29e4
Author: Jeff Dike <jdike@addtoit.com>
Date:   Thu Jul 7 17:56:49 2005 -0700

    [PATCH] uml: skas0 - separate kernel address space on stock hosts
    
    UML has had two modes of operation - an insecure, slow mode (tt mode) in
    which the kernel is mapped into every process address space which requires
    no host kernel modifications, and a secure, faster mode (skas mode) in
    which the UML kernel is in a separate host address space, which requires a
    patch to the host kernel.
    
    This patch implements something very close to skas mode for hosts which
    don't support skas - I'm calling this skas0.  It provides the security of
    the skas host patch, and some of the performance gains.
    
    The two main things that are provided by the skas patch, /proc/mm and
    PTRACE_FAULTINFO, are implemented in a way that require no host patch.
    
    For the remote address space changing stuff (mmap, munmap, and mprotect),
    we set aside two pages in the process above its stack, one of which
    contains a little bit of code which can call mmap et al.
    
    To update the address space, the system call information (system call
    number and arguments) are written to the stub page above the code.  The
    %esp is set to the beginning of the data, the %eip is set the the start of
    the stub, and it repeatedly pops the information into its registers and
    makes the system call until it sees a system call number of zero.  This is
    to amortize the cost of the context switch across multiple address space
    updates.
    
    When the updates are done, it SIGSTOPs itself, and the kernel process
    continues what it was doing.
    
    For a PTRACE_FAULTINFO replacement, we set up a SIGSEGV handler in the
    child, and let it handle segfaults rather than nullifying them.  The
    handler is in the same page as the mmap stub.  The second page is used as
    the stack.  The handler reads cr2 and err from the sigcontext, sticks them
    at the base of the stack in a faultinfo struct, and SIGSTOPs itself.  The
    kernel then reads the faultinfo and handles the fault.
    
    A complication on x86_64 is that this involves resetting the registers to
    the segfault values when the process is inside the kill system call.  This
    breaks on x86_64 because %rcx will contain %rip because you tell SYSRET
    where to return to by putting the value in %rcx.  So, this corrupts $rcx on
    return from the segfault.  To work around this, I added an
    arch_finish_segv, which on x86 does nothing, but which on x86_64 ptraces
    the child back through the sigreturn.  This causes %rcx to be restored by
    sigreturn and avoids the corruption.  Ultimately, I think I will replace
    this with the trick of having it send itself a blocked signal which will be
    unblocked by the sigreturn.  This will allow it to be stopped just after
    the sigreturn, and PTRACE_SYSCALLed without all the back-and-forth of
    PTRACE_SYSCALLing it through sigreturn.
    
    This runs on a stock host, so theoretically (and hopefully), tt mode isn't
    needed any more.  We need to make sure that this is better in every way
    than tt mode, though.  I'm concerned about the speed of address space
    updates and page fault handling, since they involve extra round-trips to
    the child.  We can amortize the round-trip cost for large address space
    updates by writing all of the operations to the data page and having the
    child execute them all at the same time.  This will help fork and exec, but
    not page faults, since they involve only one page.
    
    I can't think of any way to help page faults, except to add something like
    PTRACE_FAULTINFO to the host.  There is PTRACE_SIGINFO, but UML doesn't use
    siginfo for SIGSEGV (or anything else) because there isn't enough
    information in the siginfo struct to handle page faults (the faulting
    operation type is missing).  Adding that would make PTRACE_SIGINFO a usable
    equivalent to PTRACE_FAULTINFO.
    
    As for the code itself:
    
    - The system call stub is in arch/um/kernel/sys-$(SUBARCH)/stub.S.  It is
      put in its own section of the binary along with stub_segv_handler in
      arch/um/kernel/skas/process.c.  This is manipulated with run_syscall_stub
      in arch/um/kernel/skas/mem_user.c.  syscall_stub will execute any system
      call at all, but it's only used for mmap, munmap, and mprotect.
    
    - The x86_64 stub calls sigreturn by hand rather than allowing the normal
      sigreturn to happen, because the normal sigreturn is a SA_RESTORER in
      UML's address space provided by libc.  Needless to say, this is not
      available in the child's address space.  Also, it does a couple of odd
      pops before that which restore the stack to the state it was in at the
      time the signal handler was called.
    
    - There is a new field in the arch mmu_context, which is now a union.
      This is the pid to be manipulated rather than the /proc/mm file
      descriptor.  Code which deals with this now checks proc_mm to see whether
      it should use the usual skas code or the new code.
    
    - userspace_tramp is now used to create a new host process for every UML
      process, rather than one per UML processor.  It checks proc_mm and
      ptrace_faultinfo to decide whether to map in the pages above its stack.
    
    - start_userspace now makes CLONE_VM conditional on proc_mm since we need
      separate address spaces now.
    
    - switch_mm_skas now just sets userspace_pid[0] to the new pid rather
      than PTRACE_SWITCH_MM.  There is an addition to userspace which updates
      its idea of the pid being manipulated each time around the loop.  This is
      important on exec, when the pid will change underneath userspace().
    
    - The stub page has a pte, but it can't be mapped in using tlb_flush
      because it is part of tlb_flush.  This is why it's required for it to be
      mapped in by userspace_tramp.
    
    Other random things:
    
    - The stub section in uml.lds.S is page aligned.  This page is written
      out to the backing vm file in setup_physmem because it is mapped from
      there into user processes.
    
    - There's some confusion with TASK_SIZE now that there are a couple of
      extra pages that the process can't use.  TASK_SIZE is considered by the
      elf code to be the usable process memory, which is reasonable, so it is
      decreased by two pages.  This confuses the definition of
      USER_PGDS_IN_LAST_PML4, making it too small because of the rounding down
      of the uneven division.  So we round it to the nearest PGDIR_SIZE rather
      than the lower one.
    
    - I added a missing PT_SYSCALL_ARG6_OFFSET macro.
    
    - um_mmu.h was made into a userspace-usable file.
    
    - proc_mm and ptrace_faultinfo are globals which say whether the host
      supports these features.
    
    - There is a bad interaction between the mm.nr_ptes check at the end of
      exit_mmap, stack randomization, and skas0.  exit_mmap will stop freeing
      pages at the PGDIR_SIZE boundary after the last vma.  If the stack isn't
      on the last page table page, the last pte page won't be freed, as it
      should be since the stub ptes are there, and exit_mmap will BUG because
      there is an unfreed page.  To get around this, TASK_SIZE is set to the
      next lowest PGDIR_SIZE boundary and mm->nr_ptes is decremented after the
      calls to init_stub_pte.  This ensures that we know the process stack (and
      all other process mappings) will be below the top page table page, and
      thus we know that mm->nr_ptes will be one too many, and can be
      decremented.
    
    Things that need fixing:
    
    - We may need better assurrences that the stub code is PIC.
    
    - The stub pte is set up in init_new_context_skas.
    
    - alloc_pgdir is probably the right place.
    
    Signed-off-by: Jeff Dike <jdike@addtoit.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
index 6cb9a6d028a9..511a855c9ec0 100644
--- a/arch/um/kernel/skas/mmu.c
+++ b/arch/um/kernel/skas/mmu.c
@@ -3,46 +3,138 @@
  * Licensed under the GPL
  */
 
+#include "linux/config.h"
 #include "linux/sched.h"
 #include "linux/list.h"
 #include "linux/spinlock.h"
 #include "linux/slab.h"
+#include "linux/errno.h"
+#include "linux/mm.h"
 #include "asm/current.h"
 #include "asm/segment.h"
 #include "asm/mmu.h"
+#include "asm/pgalloc.h"
+#include "asm/pgtable.h"
 #include "os.h"
 #include "skas.h"
 
+extern int __syscall_stub_start;
+
+static int init_stub_pte(struct mm_struct *mm, unsigned long proc,
+			 unsigned long kernel)
+{
+	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
+	pte_t *pte;
+
+	spin_lock(&mm->page_table_lock);
+	pgd = pgd_offset(mm, proc);
+	pud = pud_alloc(mm, pgd, proc);
+	if (!pud)
+		goto out;
+
+	pmd = pmd_alloc(mm, pud, proc);
+	if (!pmd)
+		goto out_pmd;
+
+	pte = pte_alloc_map(mm, pmd, proc);
+	if (!pte)
+		goto out_pte;
+
+	/* There's an interaction between the skas0 stub pages, stack
+	 * randomization, and the BUG at the end of exit_mmap.  exit_mmap
+         * checks that the number of page tables freed is the same as had
+         * been allocated.  If the stack is on the last page table page,
+	 * then the stack pte page will be freed, and if not, it won't.  To
+	 * avoid having to know where the stack is, or if the process mapped
+	 * something at the top of its address space for some other reason,
+	 * we set TASK_SIZE to end at the start of the last page table.
+	 * This keeps exit_mmap off the last page, but introduces a leak
+	 * of that page.  So, we hang onto it here and free it in
+	 * destroy_context_skas.
+	 */
+
+        mm->context.skas.last_page_table = pmd_page_kernel(*pmd);
+
+	*pte = mk_pte(virt_to_page(kernel), __pgprot(_PAGE_PRESENT));
+	*pte = pte_mkexec(*pte);
+	*pte = pte_wrprotect(*pte);
+	spin_unlock(&mm->page_table_lock);
+	return(0);
+
+ out_pmd:
+	pud_free(pud);
+ out_pte:
+	pmd_free(pmd);
+ out:
+	spin_unlock(&mm->page_table_lock);
+	return(-ENOMEM);
+}
+
 int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
 {
-	int from;
+	struct mm_struct *cur_mm = current->mm;
+	struct mm_id *mm_id = &mm->context.skas.id;
+	unsigned long stack;
+	int from, ret;
 
-	if((current->mm != NULL) && (current->mm != &init_mm))
-		from = current->mm->context.skas.mm_fd;
-	else from = -1;
+	if(proc_mm){
+		if((cur_mm != NULL) && (cur_mm != &init_mm))
+			from = cur_mm->context.skas.id.u.mm_fd;
+		else from = -1;
 
-	mm->context.skas.mm_fd = new_mm(from);
-	if(mm->context.skas.mm_fd < 0){
-		printk("init_new_context_skas - new_mm failed, errno = %d\n",
-		       mm->context.skas.mm_fd);
-		return(mm->context.skas.mm_fd);
+		ret = new_mm(from);
+		if(ret < 0){
+			printk("init_new_context_skas - new_mm failed, "
+			       "errno = %d\n", ret);
+			return ret;
+		}
+		mm_id->u.mm_fd = ret;
 	}
+	else {
+		/* This zeros the entry that pgd_alloc didn't, needed since
+		 * we are about to reinitialize it, and want mm.nr_ptes to
+		 * be accurate.
+		 */
+		mm->pgd[USER_PTRS_PER_PGD] = __pgd(0);
 
-	return(0);
+		ret = init_stub_pte(mm, CONFIG_STUB_CODE,
+				    (unsigned long) &__syscall_stub_start);
+		if(ret)
+			goto out;
+
+		ret = -ENOMEM;
+		stack = get_zeroed_page(GFP_KERNEL);
+		if(stack == 0)
+			goto out;
+		mm_id->stack = stack;
+
+		ret = init_stub_pte(mm, CONFIG_STUB_DATA, stack);
+		if(ret)
+			goto out_free;
+
+		mm->nr_ptes--;
+		mm_id->u.pid = start_userspace(stack);
+	}
+
+	return 0;
+
+ out_free:
+	free_page(mm_id->stack);
+ out:
+	return ret;
 }
 
 void destroy_context_skas(struct mm_struct *mm)
 {
-	os_close_file(mm->context.skas.mm_fd);
-}
+	struct mmu_context_skas *mmu = &mm->context.skas;
 
-/*
- * Overrides for Emacs so that we follow Linus's tabbing style.
- * Emacs will notice this stuff at the end of the file and automatically
- * adjust the settings for this buffer only.  This must remain at the end
- * of the file.
- * ---------------------------------------------------------------------------
- * Local variables:
- * c-file-style: "linux"
- * End:
- */
+	if(proc_mm)
+		os_close_file(mmu->id.u.mm_fd);
+	else {
+		os_kill_ptraced_process(mmu->id.u.pid, 1);
+		free_page(mmu->id.stack);
+		free_page(mmu->last_page_table);
+	}
+}

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/um/kernel/skas/mmu.c b/arch/um/kernel/skas/mmu.c
new file mode 100644
index 000000000000..6cb9a6d028a9
--- /dev/null
+++ b/arch/um/kernel/skas/mmu.c
@@ -0,0 +1,48 @@
+/* 
+ * Copyright (C) 2002 Jeff Dike (jdike@karaya.com)
+ * Licensed under the GPL
+ */
+
+#include "linux/sched.h"
+#include "linux/list.h"
+#include "linux/spinlock.h"
+#include "linux/slab.h"
+#include "asm/current.h"
+#include "asm/segment.h"
+#include "asm/mmu.h"
+#include "os.h"
+#include "skas.h"
+
+int init_new_context_skas(struct task_struct *task, struct mm_struct *mm)
+{
+	int from;
+
+	if((current->mm != NULL) && (current->mm != &init_mm))
+		from = current->mm->context.skas.mm_fd;
+	else from = -1;
+
+	mm->context.skas.mm_fd = new_mm(from);
+	if(mm->context.skas.mm_fd < 0){
+		printk("init_new_context_skas - new_mm failed, errno = %d\n",
+		       mm->context.skas.mm_fd);
+		return(mm->context.skas.mm_fd);
+	}
+
+	return(0);
+}
+
+void destroy_context_skas(struct mm_struct *mm)
+{
+	os_close_file(mm->context.skas.mm_fd);
+}
+
+/*
+ * Overrides for Emacs so that we follow Linus's tabbing style.
+ * Emacs will notice this stuff at the end of the file and automatically
+ * adjust the settings for this buffer only.  This must remain at the end
+ * of the file.
+ * ---------------------------------------------------------------------------
+ * Local variables:
+ * c-file-style: "linux"
+ * End:
+ */
