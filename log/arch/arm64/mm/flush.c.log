commit a50b854e073cd3335bbbada8dcff83a857297dd7
Author: Matthew Wilcox (Oracle) <willy@infradead.org>
Date:   Mon Sep 23 15:34:25 2019 -0700

    mm: introduce page_size()
    
    Patch series "Make working with compound pages easier", v2.
    
    These three patches add three helpers and convert the appropriate
    places to use them.
    
    This patch (of 3):
    
    It's unnecessarily hard to find out the size of a potentially huge page.
    Replace 'PAGE_SIZE << compound_order(page)' with page_size(page).
    
    Link: http://lkml.kernel.org/r/20190721104612.19120-2-willy@infradead.org
    Signed-off-by: Matthew Wilcox (Oracle) <willy@infradead.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index dc19300309d2..ac485163a4a7 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -56,8 +56,7 @@ void __sync_icache_dcache(pte_t pte)
 	struct page *page = pte_page(pte);
 
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
-		sync_icache_aliases(page_address(page),
-				    PAGE_SIZE << compound_order(page));
+		sync_icache_aliases(page_address(page), page_size(page));
 }
 EXPORT_SYMBOL_GPL(__sync_icache_dcache);
 

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 5c9073bace83..dc19300309d2 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -1,20 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/mm/flush.c
  *
  * Copyright (C) 1995-2002 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/export.h>

commit 132fdc379eb143932d209a20fd581e1ce7630960
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jan 24 17:28:37 2019 +0000

    arm64: Do not issue IPIs for user executable ptes
    
    Commit 3b8c9f1cdfc5 ("arm64: IPI each CPU after invalidating the I-cache
    for kernel mappings") was aimed at fixing the I-cache invalidation for
    kernel mappings. However, it inadvertently caused all cache maintenance
    for user mappings via set_pte_at() -> __sync_icache_dcache() ->
    sync_icache_aliases() to call kick_all_cpus_sync().
    
    Reported-by: Shijith Thotton <sthotton@marvell.com>
    Tested-by: Shijith Thotton <sthotton@marvell.com>
    Reported-by: Wandun Chen <chenwandun@huawei.com>
    Fixes: 3b8c9f1cdfc5 ("arm64: IPI each CPU after invalidating the I-cache for kernel mappings")
    Cc: <stable@vger.kernel.org> # 4.19.x-
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 30695a868107..5c9073bace83 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -33,7 +33,11 @@ void sync_icache_aliases(void *kaddr, unsigned long len)
 		__clean_dcache_area_pou(kaddr, len);
 		__flush_icache_all();
 	} else {
-		flush_icache_range(addr, addr + len);
+		/*
+		 * Don't issue kick_all_cpus_sync() after I-cache invalidation
+		 * for user mappings.
+		 */
+		__flush_icache_range(addr, addr + len);
 	}
 }
 

commit c5157101e7793b42a56e07368c7f4cb73fb58008
Author: Ben Hutchings <ben@decadent.org.uk>
Date:   Thu Jul 12 00:18:22 2018 +0100

    arm64: mm: Export __sync_icache_dcache() for xen-privcmd
    
    The xen-privcmd driver, which can be modular, calls set_pte_at()
    which in turn may call __sync_icache_dcache().
    
    The call to __sync_icache_dcache() may be optimised out because it is
    conditional on !pte_special(), and xen-privcmd calls pte_mkspecial().
    But it seems unwise to rely on this optimisation.
    
    Fixes: 3ad0876554ca ("xen/privcmd: add IOCTL_PRIVCMD_MMAP_RESOURCE")
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 9786f9d5d3dc..30695a868107 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -66,6 +66,7 @@ void __sync_icache_dcache(pte_t pte)
 		sync_icache_aliases(page_address(page),
 				    PAGE_SIZE << compound_order(page));
 }
+EXPORT_SYMBOL_GPL(__sync_icache_dcache);
 
 /*
  * This function is called when a page has been modified by the kernel. Mark

commit bedbeec65c6cdec25aab609d557b630c97f81866
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jul 6 16:21:17 2018 +0100

    arm64: mm: Export __flush_icache_range() to modules
    
    lkdtm calls flush_icache_range(), which results in an out-of-line call
    to __flush_icache_range(), which is not exported to modules.
    
    Export the symbol to modules to fix this build breakage.
    
    Fixes: 3b8c9f1cdfc5 ("arm64: IPI each CPU after invalidating the I-cache for kernel mappings")
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 1059884f9a6f..9786f9d5d3dc 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -82,7 +82,7 @@ EXPORT_SYMBOL(flush_dcache_page);
 /*
  * Additional functions defined in assembly.
  */
-EXPORT_SYMBOL(flush_icache_range);
+EXPORT_SYMBOL(__flush_icache_range);
 
 #ifdef CONFIG_ARCH_HAS_PMEM_API
 void arch_wb_cache_pmem(void *addr, size_t size)

commit 907e21c15c883c2c15d1e5ee3cdbb7824ab1da59
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Tue Apr 17 20:03:09 2018 +0800

    arm64: mm: drop addr parameter from sync icache and dcache
    
    The addr parameter isn't used for anything. Let's simplify and get rid of
    it, like arm.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index e36ed5087b5c..1059884f9a6f 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -58,7 +58,7 @@ void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 	flush_ptrace_access(vma, page, uaddr, dst, len);
 }
 
-void __sync_icache_dcache(pte_t pte, unsigned long addr)
+void __sync_icache_dcache(pte_t pte)
 {
 	struct page *page = pte_page(pte);
 

commit caf5ef7d15c511bbef691d0931adad56c2967435
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Aug 10 16:52:31 2017 +0200

    arm64: fix pmem interface definition
    
    Defining the two functions as 'static inline' and exporting them
    leads to the interesting case where we can use the interface
    from loadable modules, but not from built-in drivers, as shown
    in this link failure:
    
    vers/nvdimm/claim.o: In function `nsio_rw_bytes':
    claim.c:(.text+0x1b8): undefined reference to `arch_invalidate_pmem'
    drivers/nvdimm/pmem.o: In function `pmem_dax_flush':
    pmem.c:(.text+0x11c): undefined reference to `arch_wb_cache_pmem'
    drivers/nvdimm/pmem.o: In function `pmem_make_request':
    pmem.c:(.text+0x5a4): undefined reference to `arch_invalidate_pmem'
    pmem.c:(.text+0x650): undefined reference to `arch_invalidate_pmem'
    pmem.c:(.text+0x6d4): undefined reference to `arch_invalidate_pmem'
    
    This removes the bogus 'static inline'.
    
    Fixes: d50e071fdaa3 ("arm64: Implement pmem API support")
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 280f90ff33a2..e36ed5087b5c 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -85,7 +85,7 @@ EXPORT_SYMBOL(flush_dcache_page);
 EXPORT_SYMBOL(flush_icache_range);
 
 #ifdef CONFIG_ARCH_HAS_PMEM_API
-static inline void arch_wb_cache_pmem(void *addr, size_t size)
+void arch_wb_cache_pmem(void *addr, size_t size)
 {
 	/* Ensure order against any prior non-cacheable writes */
 	dmb(osh);
@@ -93,7 +93,7 @@ static inline void arch_wb_cache_pmem(void *addr, size_t size)
 }
 EXPORT_SYMBOL_GPL(arch_wb_cache_pmem);
 
-static inline void arch_invalidate_pmem(void *addr, size_t size)
+void arch_invalidate_pmem(void *addr, size_t size)
 {
 	__inval_dcache_area(addr, size);
 }

commit d50e071fdaa33c1b399c764c44fa1ce879881185
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:42 2017 +0100

    arm64: Implement pmem API support
    
    Add a clean-to-point-of-persistence cache maintenance helper, and wire
    up the basic architectural support for the pmem driver based on it.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    [catalin.marinas@arm.com: move arch_*_pmem() functions to arch/arm64/mm/flush.c]
    [catalin.marinas@arm.com: change dmb(sy) to dmb(osh)]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 21a8d828cbf4..280f90ff33a2 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -83,3 +83,19 @@ EXPORT_SYMBOL(flush_dcache_page);
  * Additional functions defined in assembly.
  */
 EXPORT_SYMBOL(flush_icache_range);
+
+#ifdef CONFIG_ARCH_HAS_PMEM_API
+static inline void arch_wb_cache_pmem(void *addr, size_t size)
+{
+	/* Ensure order against any prior non-cacheable writes */
+	dmb(osh);
+	__clean_dcache_area_pop(addr, size);
+}
+EXPORT_SYMBOL_GPL(arch_wb_cache_pmem);
+
+static inline void arch_invalidate_pmem(void *addr, size_t size)
+{
+	__inval_dcache_area(addr, size);
+}
+EXPORT_SYMBOL_GPL(arch_invalidate_pmem);
+#endif

commit 02f7760e6e5c3d726cd9622749cdae17c571b9a3
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Mar 10 20:32:23 2017 +0000

    arm64: cache: Merge cachetype.h into cache.h
    
    cachetype.h and cache.h are small and both obviously related to caches.
    Merge them together to reduce clutter.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 1e968222a544..21a8d828cbf4 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -22,7 +22,7 @@
 #include <linux/pagemap.h>
 
 #include <asm/cacheflush.h>
-#include <asm/cachetype.h>
+#include <asm/cache.h>
 #include <asm/tlbflush.h>
 
 void sync_icache_aliases(void *kaddr, unsigned long len)

commit 155433cb365ee4666bdf7c3c7bc2978b17be36a4
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Mar 10 20:32:22 2017 +0000

    arm64: cache: Remove support for ASID-tagged VIVT I-caches
    
    As a recent change to ARMv8, ASID-tagged VIVT I-caches are removed
    retrospectively from the architecture. Consequently, we don't need to
    support them in Linux either.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 554a2558c12e..1e968222a544 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -65,8 +65,6 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
 		sync_icache_aliases(page_address(page),
 				    PAGE_SIZE << compound_order(page));
-	else if (icache_is_aivivt())
-		__flush_icache_all();
 }
 
 /*

commit ee6a7fce8e5ecd90794ad7c9f62518c753fb3cb6
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Nov 23 18:05:52 2016 +0000

    arm64: Remove I-cache invalidation from flush_cache_range()
    
    The flush_cache_range() function (similarly for flush_cache_page()) is
    called when the kernel is changing an existing VA->PA mapping range to
    either a new PA or to different attributes. Since ARMv8 has PIPT-like
    D-caches, this function does not need to perform any D-cache
    maintenance. The I-cache maintenance is already handled via set_pte_at()
    and flush_cache_range() cannot anyway guarantee that there are no cache
    lines left after invalidation due to the speculative loads.
    
    This patch makes flush_cache_range() a no-op.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 2d78d5a9b89f..554a2558c12e 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -25,13 +25,6 @@
 #include <asm/cachetype.h>
 #include <asm/tlbflush.h>
 
-void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
-		       unsigned long end)
-{
-	if (vma->vm_flags & VM_EXEC)
-		__flush_icache_all();
-}
-
 void sync_icache_aliases(void *kaddr, unsigned long len)
 {
 	unsigned long addr = (unsigned long)kaddr;

commit 9842ceae9fa8deae141533d52a6ead7666962c09
Author: Pratyush Anand <panand@redhat.com>
Date:   Wed Nov 2 14:40:46 2016 +0530

    arm64: Add uprobe support
    
    This patch adds support for uprobe on ARM64 architecture.
    
    Unit tests for following have been done so far and they have been found
    working
        1. Step-able instructions, like sub, ldr, add etc.
        2. Simulation-able like ret, cbnz, cbz etc.
        3. uretprobe
        4. Reject-able instructions like sev, wfe etc.
        5. trapped and abort xol path
        6. probe at unaligned user address.
        7. longjump test cases
    
    Currently it does not support aarch32 instruction probing.
    
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 8377329d8c97..2d78d5a9b89f 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -32,7 +32,7 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		__flush_icache_all();
 }
 
-static void sync_icache_aliases(void *kaddr, unsigned long len)
+void sync_icache_aliases(void *kaddr, unsigned long len)
 {
 	unsigned long addr = (unsigned long)kaddr;
 

commit dae8c235d9a21a564793ea9fe716233e11d30e21
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Mon Sep 5 19:30:22 2016 +0800

    arm64: mm: drop fixup_init() and mm.h
    
    There is only fixup_init() in mm.h , and it is only called
    in free_initmem(), so move the codes from fixup_init() into
    free_initmem(), then drop fixup_init() and mm.h.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 43a76b07eb32..8377329d8c97 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -25,8 +25,6 @@
 #include <asm/cachetype.h>
 #include <asm/tlbflush.h>
 
-#include "mm.h"
-
 void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long end)
 {

commit 20c27a4270c775d7ed661491af8ac03264d60fc6
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Tue Jun 21 15:32:57 2016 +0800

    arm64: mm: remove page_mapping check in __sync_icache_dcache
    
    __sync_icache_dcache unconditionally skips the cache maintenance for
    anonymous pages, under the assumption that flushing is only required in
    the presence of D-side aliases [see 7249b79f6b4cc ("arm64: Do not flush
    the D-cache for anonymous pages")].
    
    Unfortunately, this breaks migration of anonymous pages holding
    self-modifying code, where userspace cannot be reasonably expected to
    reissue maintenance instructions in response to a migration.
    
    This patch fixes the problem by removing the broken page_mapping(page)
    check from the cache syncing code, otherwise we may end up fetching and
    executing stale instructions from the PoU.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index dbd12ea8ce68..43a76b07eb32 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -71,10 +71,6 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
 {
 	struct page *page = pte_page(pte);
 
-	/* no flushing needed for anonymous pages */
-	if (!page_mapping(page))
-		return;
-
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
 		sync_icache_aliases(page_address(page),
 				    PAGE_SIZE << compound_order(page));

commit 691b1e2ebf727167a2e3cdcd1ea0851dee10247b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Mar 22 10:11:28 2016 +0000

    arm64: mm: allow preemption in copy_to_user_page
    
    Currently we disable preemption in copy_to_user_page; a behaviour that
    we inherited from the 32-bit arm code. This was necessary for older
    cores without broadcast data cache maintenance, and ensured that cache
    lines were dirtied and cleaned by the same CPU. On these systems dirty
    cache line migration was not possible, so this was sufficient to
    guarantee coherency.
    
    On contemporary systems, cache coherence protocols permit (dirty) cache
    lines to migrate between CPUs as a result of speculation, prefetching,
    and other behaviours. To account for this, in ARMv8 data cache
    maintenance operations are broadcast and affect all data caches in the
    domain associated with the VA (i.e. ISH for kernel and user mappings).
    
    In __switch_to we ensure that tasks can be safely migrated in the middle
    of a maintenance sequence, using a dsb(ish) to ensure prior explicit
    memory accesses are observed and cache maintenance operations are
    completed before a task can be run on another CPU.
    
    Given the above, it is not necessary to disable preemption in
    copy_to_user_page. This patch removes the preempt_{disable,enable}
    calls, permitting preemption.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 60585bde1264..dbd12ea8ce68 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -58,17 +58,13 @@ static void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
  * Copy user data from/to a page which is mapped into a different processes
  * address space.  Really, we want to allow our "user space" model to handle
  * this.
- *
- * Note that this code needs to run on the current CPU.
  */
 void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long uaddr, void *dst, const void *src,
 		       unsigned long len)
 {
-	preempt_disable();
 	memcpy(dst, src, len);
 	flush_ptrace_access(vma, page, uaddr, dst, len);
-	preempt_enable();
 }
 
 void __sync_icache_dcache(pte_t pte, unsigned long addr)

commit b7ed934a7c3fd4d2651eae81cb658cf1926ff54f
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:53:10 2016 -0800

    arm64, thp: remove infrastructure for handling splitting PMDs
    
    With new refcounting we don't need to mark PMDs splitting.  Let's drop
    code to handle this.
    
    pmdp_splitting_flush() is not needed too: on splitting PMD we will do
    pmdp_clear_flush() + set_pte_at().  pmdp_clear_flush() will do IPI as
    needed for fast_gup.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 46649d6e6c5a..60585bde1264 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -102,19 +102,3 @@ EXPORT_SYMBOL(flush_dcache_page);
  * Additional functions defined in assembly.
  */
 EXPORT_SYMBOL(flush_icache_range);
-
-#ifdef CONFIG_TRANSPARENT_HUGEPAGE
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
-void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
-			  pmd_t *pmdp)
-{
-	pmd_t pmd = pmd_mksplitting(*pmdp);
-
-	VM_BUG_ON(address & ~PMD_MASK);
-	set_pmd_at(vma->vm_mm, address, pmdp, pmd);
-
-	/* dummy IPI to serialise against fast_gup */
-	kick_all_cpus_sync();
-}
-#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
-#endif /* CONFIG_TRANSPARENT_HUGEPAGE */

commit 0a28714c53fd4f7aea709be7577dfbe0095c8c3e
Author: Ashok Kumar <ashoks@broadcom.com>
Date:   Thu Dec 17 01:38:32 2015 -0800

    arm64: Use PoU cache instr for I/D coherency
    
    In systems with three levels of cache(PoU at L1 and PoC at L3),
    PoC cache flush instructions flushes L2 and L3 caches which could affect
    performance.
    For cache flushes for I and D coherency, PoU should suffice.
    So changing all I and D coherency related cache flushes to PoU.
    
    Introduced a new __clean_dcache_area_pou API for dcache flush till PoU
    and provided a common macro for __flush_dcache_area and
    __clean_dcache_area_pou.
    
    Also, now in __sync_icache_dcache, icache invalidation for non-aliasing
    VIPT icache is done only for that particular page instead of the earlier
    __flush_icache_all.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ashok Kumar <ashoks@broadcom.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index c26b804015e8..46649d6e6c5a 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -34,19 +34,24 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		__flush_icache_all();
 }
 
+static void sync_icache_aliases(void *kaddr, unsigned long len)
+{
+	unsigned long addr = (unsigned long)kaddr;
+
+	if (icache_is_aliasing()) {
+		__clean_dcache_area_pou(kaddr, len);
+		__flush_icache_all();
+	} else {
+		flush_icache_range(addr, addr + len);
+	}
+}
+
 static void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 				unsigned long uaddr, void *kaddr,
 				unsigned long len)
 {
-	if (vma->vm_flags & VM_EXEC) {
-		unsigned long addr = (unsigned long)kaddr;
-		if (icache_is_aliasing()) {
-			__flush_dcache_area(kaddr, len);
-			__flush_icache_all();
-		} else {
-			flush_icache_range(addr, addr + len);
-		}
-	}
+	if (vma->vm_flags & VM_EXEC)
+		sync_icache_aliases(kaddr, len);
 }
 
 /*
@@ -74,13 +79,11 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
 	if (!page_mapping(page))
 		return;
 
-	if (!test_and_set_bit(PG_dcache_clean, &page->flags)) {
-		__flush_dcache_area(page_address(page),
-				PAGE_SIZE << compound_order(page));
+	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
+		sync_icache_aliases(page_address(page),
+				    PAGE_SIZE << compound_order(page));
+	else if (icache_is_aivivt())
 		__flush_icache_all();
-	} else if (icache_is_aivivt()) {
-		__flush_icache_all();
-	}
 }
 
 /*

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 4dfa3975ce5b..c26b804015e8 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -60,14 +60,10 @@ void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 		       unsigned long uaddr, void *dst, const void *src,
 		       unsigned long len)
 {
-#ifdef CONFIG_SMP
 	preempt_disable();
-#endif
 	memcpy(dst, src, len);
 	flush_ptrace_access(vma, page, uaddr, dst, len);
-#ifdef CONFIG_SMP
 	preempt_enable();
-#endif
 }
 
 void __sync_icache_dcache(pte_t pte, unsigned long addr)

commit 68234df4ea7939f98431aa81113fbdce10c4a84b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Apr 20 10:24:35 2015 +0100

    arm64: kill flush_cache_all()
    
    The documented semantics of flush_cache_all are not possible to provide
    for arm64 (short of flushing the entire physical address space by VA),
    and there are currently no users; KVM uses VA maintenance exclusively,
    cpu_reset is never called, and the only two users outside of arch code
    cannot be built for arm64.
    
    While cpu_soft_reset and related functions (which call flush_cache_all)
    were thought to be useful for kexec, their current implementations only
    serve to mask bugs. For correctness kexec will need to perform
    maintenance by VA anyway to account for system caches, line migration,
    and other subtleties of the cache architecture. As the extent of this
    cache maintenance will be kexec-specific, it should probably live in the
    kexec code.
    
    This patch removes flush_cache_all, and related unused components,
    preventing further abuse.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index b6f14e8d2121..4dfa3975ce5b 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -102,7 +102,6 @@ EXPORT_SYMBOL(flush_dcache_page);
 /*
  * Additional functions defined in assembly.
  */
-EXPORT_SYMBOL(flush_cache_all);
 EXPORT_SYMBOL(flush_icache_range);
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE

commit 29e5694054149acd25b0d5538c95fb6d64478315
Author: Steve Capper <steve.capper@linaro.org>
Date:   Thu Oct 9 15:29:25 2014 -0700

    arm64: mm: enable RCU fast_gup
    
    Activate the RCU fast_gup for ARM64.  We also need to force THP splits to
    broadcast an IPI s.t.  we block in the fast_gup page walker.  As THP
    splits are comparatively rare, this should not lead to a noticeable
    performance degradation.
    
    Some pre-requisite functions pud_write and pud_page are also added.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Tested-by: Dann Frazier <dann.frazier@canonical.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 0d64089d28b5..b6f14e8d2121 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -104,3 +104,19 @@ EXPORT_SYMBOL(flush_dcache_page);
  */
 EXPORT_SYMBOL(flush_cache_all);
 EXPORT_SYMBOL(flush_icache_range);
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
+			  pmd_t *pmdp)
+{
+	pmd_t pmd = pmd_mksplitting(*pmdp);
+
+	VM_BUG_ON(address & ~PMD_MASK);
+	set_pmd_at(vma->vm_mm, address, pmdp, pmd);
+
+	/* dummy IPI to serialise against fast_gup */
+	kick_all_cpus_sync();
+}
+#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */

commit 923b8f5044da753e4985ab15c1374ced2cdf616c
Author: Steve Capper <steve.capper@linaro.org>
Date:   Wed Jul 2 11:46:23 2014 +0100

    arm64: mm: Make icache synchronisation logic huge page aware
    
    The __sync_icache_dcache routine will only flush the dcache for the
    first page of a compound page, potentially leading to stale icache
    data residing further on in a hugetlb page.
    
    This patch addresses this issue by taking into consideration the
    order of the page when flushing the dcache.
    
    Reported-by: Mark Brown <broonie@linaro.org>
    Tested-by: Mark Brown <broonie@linaro.org>
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: <stable@vger.kernel.org> # v3.11+

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index e4193e3adc7f..0d64089d28b5 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -79,7 +79,8 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
 		return;
 
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags)) {
-		__flush_dcache_area(page_address(page), PAGE_SIZE);
+		__flush_dcache_area(page_address(page),
+				PAGE_SIZE << compound_order(page));
 		__flush_icache_all();
 	} else if (icache_is_aivivt()) {
 		__flush_icache_all();

commit ebd88367de80f9509bd30a09342d0a19c925b23e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed May 1 16:38:23 2013 +0100

    arm64: Remove __flush_dcache_page()
    
    This function is only used in __sync_icache_dcache(), so remove it and
    call __flush_dcache_area() directly. The flush_icache_user_range()
    function is not used in the arm64 kernel.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 7c716634a671..e4193e3adc7f 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -70,11 +70,6 @@ void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
 #endif
 }
 
-void __flush_dcache_page(struct page *page)
-{
-	__flush_dcache_area(page_address(page), PAGE_SIZE);
-}
-
 void __sync_icache_dcache(pte_t pte, unsigned long addr)
 {
 	struct page *page = pte_page(pte);
@@ -84,7 +79,7 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
 		return;
 
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags)) {
-		__flush_dcache_page(page);
+		__flush_dcache_area(page_address(page), PAGE_SIZE);
 		__flush_icache_all();
 	} else if (icache_is_aivivt()) {
 		__flush_icache_all();

commit 7249b79f6b4cc3c2aa9138dca52e535a4c789107
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed May 1 16:34:22 2013 +0100

    arm64: Do not flush the D-cache for anonymous pages
    
    The D-cache on AArch64 is VIPT non-aliasing, so there is no need to
    flush it for anonymous pages.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index b9cd7a4deeca..7c716634a671 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -77,14 +77,12 @@ void __flush_dcache_page(struct page *page)
 
 void __sync_icache_dcache(pte_t pte, unsigned long addr)
 {
-	unsigned long pfn;
-	struct page *page;
+	struct page *page = pte_page(pte);
 
-	pfn = pte_pfn(pte);
-	if (!pfn_valid(pfn))
+	/* no flushing needed for anonymous pages */
+	if (!page_mapping(page))
 		return;
 
-	page = pfn_to_page(pfn);
 	if (!test_and_set_bit(PG_dcache_clean, &page->flags)) {
 		__flush_dcache_page(page);
 		__flush_icache_all();

commit b5b6c9e9149d8a7c3f1d7b9d0c046c6184e1dd17
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed May 1 12:23:05 2013 +0100

    arm64: Avoid cache flushing in flush_dcache_page()
    
    The flush_dcache_page() function is called when the kernel modified a
    page cache page. Since the D-cache on AArch64 does not have aliases
    this function can simply mark the page as dirty for later flushing via
    set_pte_at()/__sync_icache_dcache() if the page is executable (to ensure
    the I-D cache coherency).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 88611c3a421a..b9cd7a4deeca 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -94,28 +94,14 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
 }
 
 /*
- * Ensure cache coherency between kernel mapping and userspace mapping of this
- * page.
+ * This function is called when a page has been modified by the kernel. Mark
+ * it as dirty for later flushing when mapped in user space (if executable,
+ * see __sync_icache_dcache).
  */
 void flush_dcache_page(struct page *page)
 {
-	struct address_space *mapping;
-
-	/*
-	 * The zero page is never written to, so never has any dirty cache
-	 * lines, and therefore never needs to be flushed.
-	 */
-	if (page == ZERO_PAGE(0))
-		return;
-
-	mapping = page_mapping(page);
-	if (mapping && mapping_mapped(mapping)) {
-		__flush_dcache_page(page);
-		__flush_icache_all();
-		set_bit(PG_dcache_clean, &page->flags);
-	} else {
+	if (test_bit(PG_dcache_clean, &page->flags))
 		clear_bit(PG_dcache_clean, &page->flags);
-	}
 }
 EXPORT_SYMBOL(flush_dcache_page);
 

commit 8f3bfa584ed05e9e7d290707c48eee026fb94ece
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 23 18:15:32 2012 +0000

    arm64: Convert empty flush_cache_{mm,page} functions to static inline
    
    These functions are empty, just make them static inline in the header.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index c144adb1682f..88611c3a421a 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -27,10 +27,6 @@
 
 #include "mm.h"
 
-void flush_cache_mm(struct mm_struct *mm)
-{
-}
-
 void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		       unsigned long end)
 {
@@ -38,11 +34,6 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
 		__flush_icache_all();
 }
 
-void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr,
-		      unsigned long pfn)
-{
-}
-
 static void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 				unsigned long uaddr, void *kaddr,
 				unsigned long len)

commit f1a0c4aa0937975b53991842a494f741d7769b02
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: Cache maintenance routines
    
    The patch adds functionality required for cache maintenance. The AArch64
    architecture mandates non-aliasing VIPT or PIPT D-cache and VIPT (may
    have aliases) or ASID-tagged VIVT I-cache. Cache maintenance operations
    are automatically broadcast in hardware between CPUs.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
new file mode 100644
index 000000000000..c144adb1682f
--- /dev/null
+++ b/arch/arm64/mm/flush.c
@@ -0,0 +1,135 @@
+/*
+ * Based on arch/arm/mm/flush.c
+ *
+ * Copyright (C) 1995-2002 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/export.h>
+#include <linux/mm.h>
+#include <linux/pagemap.h>
+
+#include <asm/cacheflush.h>
+#include <asm/cachetype.h>
+#include <asm/tlbflush.h>
+
+#include "mm.h"
+
+void flush_cache_mm(struct mm_struct *mm)
+{
+}
+
+void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+		       unsigned long end)
+{
+	if (vma->vm_flags & VM_EXEC)
+		__flush_icache_all();
+}
+
+void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr,
+		      unsigned long pfn)
+{
+}
+
+static void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
+				unsigned long uaddr, void *kaddr,
+				unsigned long len)
+{
+	if (vma->vm_flags & VM_EXEC) {
+		unsigned long addr = (unsigned long)kaddr;
+		if (icache_is_aliasing()) {
+			__flush_dcache_area(kaddr, len);
+			__flush_icache_all();
+		} else {
+			flush_icache_range(addr, addr + len);
+		}
+	}
+}
+
+/*
+ * Copy user data from/to a page which is mapped into a different processes
+ * address space.  Really, we want to allow our "user space" model to handle
+ * this.
+ *
+ * Note that this code needs to run on the current CPU.
+ */
+void copy_to_user_page(struct vm_area_struct *vma, struct page *page,
+		       unsigned long uaddr, void *dst, const void *src,
+		       unsigned long len)
+{
+#ifdef CONFIG_SMP
+	preempt_disable();
+#endif
+	memcpy(dst, src, len);
+	flush_ptrace_access(vma, page, uaddr, dst, len);
+#ifdef CONFIG_SMP
+	preempt_enable();
+#endif
+}
+
+void __flush_dcache_page(struct page *page)
+{
+	__flush_dcache_area(page_address(page), PAGE_SIZE);
+}
+
+void __sync_icache_dcache(pte_t pte, unsigned long addr)
+{
+	unsigned long pfn;
+	struct page *page;
+
+	pfn = pte_pfn(pte);
+	if (!pfn_valid(pfn))
+		return;
+
+	page = pfn_to_page(pfn);
+	if (!test_and_set_bit(PG_dcache_clean, &page->flags)) {
+		__flush_dcache_page(page);
+		__flush_icache_all();
+	} else if (icache_is_aivivt()) {
+		__flush_icache_all();
+	}
+}
+
+/*
+ * Ensure cache coherency between kernel mapping and userspace mapping of this
+ * page.
+ */
+void flush_dcache_page(struct page *page)
+{
+	struct address_space *mapping;
+
+	/*
+	 * The zero page is never written to, so never has any dirty cache
+	 * lines, and therefore never needs to be flushed.
+	 */
+	if (page == ZERO_PAGE(0))
+		return;
+
+	mapping = page_mapping(page);
+	if (mapping && mapping_mapped(mapping)) {
+		__flush_dcache_page(page);
+		__flush_icache_all();
+		set_bit(PG_dcache_clean, &page->flags);
+	} else {
+		clear_bit(PG_dcache_clean, &page->flags);
+	}
+}
+EXPORT_SYMBOL(flush_dcache_page);
+
+/*
+ * Additional functions defined in assembly.
+ */
+EXPORT_SYMBOL(flush_cache_all);
+EXPORT_SYMBOL(flush_icache_range);
