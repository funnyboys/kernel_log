commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index fe30e95e95b2..8afb238ff335 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -497,11 +497,11 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	 * validly references user space from well defined areas of the code,
 	 * we can bug out early if this is from code which shouldn't.
 	 */
-	if (!down_read_trylock(&mm->mmap_sem)) {
+	if (!mmap_read_trylock(mm)) {
 		if (!user_mode(regs) && !search_exception_tables(regs->pc))
 			goto no_context;
 retry:
-		down_read(&mm->mmap_sem);
+		mmap_read_lock(mm);
 	} else {
 		/*
 		 * The above down_read_trylock() might have succeeded in which
@@ -510,7 +510,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		might_sleep();
 #ifdef CONFIG_DEBUG_VM
 		if (!user_mode(regs) && !search_exception_tables(regs->pc)) {
-			up_read(&mm->mmap_sem);
+			mmap_read_unlock(mm);
 			goto no_context;
 		}
 #endif
@@ -532,7 +532,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			goto retry;
 		}
 	}
-	up_read(&mm->mmap_sem);
+	mmap_read_unlock(mm);
 
 	/*
 	 * Handle the "normal" (no error) case first.

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index df8ae73d950b..fe30e95e95b2 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -36,7 +36,6 @@
 #include <asm/processor.h>
 #include <asm/sysreg.h>
 #include <asm/system_misc.h>
-#include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
 

commit e9f6376858b9799148d07e58b72b681d4b8fa4c7
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Jun 4 16:46:23 2020 -0700

    arm64: add support for folded p4d page tables
    
    Implement primitives necessary for the 4th level folding, add walks of p4d
    level where appropriate, replace 5level-fixup.h with pgtable-nop4d.h and
    remove __ARCH_USE_5LEVEL_HACK.
    
    [arnd@arndb.de: fix gcc-10 shift warning]
      Link: http://lkml.kernel.org/r/20200429185657.4085975-1-arnd@arndb.de
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200414153455.21744-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index dff2d72b0883..df8ae73d950b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -145,6 +145,7 @@ static void show_pte(unsigned long addr)
 	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));
 
 	do {
+		p4d_t *p4dp, p4d;
 		pud_t *pudp, pud;
 		pmd_t *pmdp, pmd;
 		pte_t *ptep, pte;
@@ -152,7 +153,13 @@ static void show_pte(unsigned long addr)
 		if (pgd_none(pgd) || pgd_bad(pgd))
 			break;
 
-		pudp = pud_offset(pgdp, addr);
+		p4dp = p4d_offset(pgdp, addr);
+		p4d = READ_ONCE(*p4dp);
+		pr_cont(", p4d=%016llx", p4d_val(p4d));
+		if (p4d_none(p4d) || p4d_bad(p4d))
+			break;
+
+		pudp = pud_offset(p4dp, addr);
 		pud = READ_ONCE(*pudp);
 		pr_cont(", pud=%016llx", pud_val(pud));
 		if (pud_none(pud) || pud_bad(pud))

commit 8fcc4ae6faf8b455eeef00bc9ae70744e3b0f462
Author: James Morse <james.morse@arm.com>
Date:   Fri May 1 17:45:43 2020 +0100

    arm64: acpi: Make apei_claim_sea() synchronise with APEI's irq work
    
    APEI is unable to do all of its error handling work in nmi-context, so
    it defers non-fatal work onto the irq_work queue. arch_irq_work_raise()
    sends an IPI to the calling cpu, but this is not guaranteed to be taken
    before returning to user-space.
    
    Unless the exception interrupted a context with irqs-masked,
    irq_work_run() can run immediately. Otherwise return -EINPROGRESS to
    indicate ghes_notify_sea() found some work to do, but it hasn't
    finished yet.
    
    With this apei_claim_sea() returning '0' means this external-abort was
    also notification of a firmware-first RAS error, and that APEI has
    processed the CPER records.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Tested-by: Tyler Baicar <baicar@os.amperecomputing.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c9cedc0432d2..dff2d72b0883 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -635,11 +635,13 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 
 	inf = esr_to_fault_info(esr);
 
-	/*
-	 * Return value ignored as we rely on signal merging.
-	 * Future patches will make this more robust.
-	 */
-	apei_claim_sea(regs);
+	if (user_mode(regs) && apei_claim_sea(regs) == 0) {
+		/*
+		 * APEI claimed this as a firmware-first notification.
+		 * Some processing deferred to task_work before ret_to_user().
+		 */
+		return 0;
+	}
 
 	if (esr & ESR_ELx_FnV)
 		siaddr = NULL;

commit 6cb4d9a2870d2062e34c93bfef4d52fca3fe42d1
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:09 2020 -0700

    mm/vma: introduce VM_ACCESS_FLAGS
    
    There are many places where all basic VMA access flags (read, write,
    exec) are initialized or checked against as a group.  One such example
    is during page fault.  Existing vma_is_accessible() wrapper already
    creates the notion of VMA accessibility as a group access permissions.
    
    Hence lets just create VM_ACCESS_FLAGS (VM_READ|VM_WRITE|VM_EXEC) which
    will not only reduce code duplication but also extend the VMA
    accessibility concept in general.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Rob Springer <rspringer@google.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Link: http://lkml.kernel.org/r/1583391014-8170-3-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 1027851d469a..c9cedc0432d2 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -445,7 +445,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	const struct fault_info *inf;
 	struct mm_struct *mm = current->mm;
 	vm_fault_t fault, major = 0;
-	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
+	unsigned long vm_flags = VM_ACCESS_FLAGS;
 	unsigned int mm_flags = FAULT_FLAG_DEFAULT;
 
 	if (kprobe_page_fault(regs, esr))

commit 4064b982706375025628094e51d11cf1a958a5d3
Author: Peter Xu <peterx@redhat.com>
Date:   Wed Apr 1 21:08:45 2020 -0700

    mm: allow VM_FAULT_RETRY for multiple times
    
    The idea comes from a discussion between Linus and Andrea [1].
    
    Before this patch we only allow a page fault to retry once.  We achieved
    this by clearing the FAULT_FLAG_ALLOW_RETRY flag when doing
    handle_mm_fault() the second time.  This was majorly used to avoid
    unexpected starvation of the system by looping over forever to handle the
    page fault on a single page.  However that should hardly happen, and after
    all for each code path to return a VM_FAULT_RETRY we'll first wait for a
    condition (during which time we should possibly yield the cpu) to happen
    before VM_FAULT_RETRY is really returned.
    
    This patch removes the restriction by keeping the FAULT_FLAG_ALLOW_RETRY
    flag when we receive VM_FAULT_RETRY.  It means that the page fault handler
    now can retry the page fault for multiple times if necessary without the
    need to generate another page fault event.  Meanwhile we still keep the
    FAULT_FLAG_TRIED flag so page fault handler can still identify whether a
    page fault is the first attempt or not.
    
    Then we'll have these combinations of fault flags (only considering
    ALLOW_RETRY flag and TRIED flag):
    
      - ALLOW_RETRY and !TRIED:  this means the page fault allows to
                                 retry, and this is the first try
    
      - ALLOW_RETRY and TRIED:   this means the page fault allows to
                                 retry, and this is not the first try
    
      - !ALLOW_RETRY and !TRIED: this means the page fault does not allow
                                 to retry at all
    
      - !ALLOW_RETRY and TRIED:  this is forbidden and should never be used
    
    In existing code we have multiple places that has taken special care of
    the first condition above by checking against (fault_flags &
    FAULT_FLAG_ALLOW_RETRY).  This patch introduces a simple helper to detect
    the first retry of a page fault by checking against both (fault_flags &
    FAULT_FLAG_ALLOW_RETRY) and !(fault_flag & FAULT_FLAG_TRIED) because now
    even the 2nd try will have the ALLOW_RETRY set, then use that helper in
    all existing special paths.  One example is in __lock_page_or_retry(), now
    we'll drop the mmap_sem only in the first attempt of page fault and we'll
    keep it in follow up retries, so old locking behavior will be retained.
    
    This will be a nice enhancement for current code [2] at the same time a
    supporting material for the future userfaultfd-writeprotect work, since in
    that work there will always be an explicit userfault writeprotect retry
    for protected pages, and if that cannot resolve the page fault (e.g., when
    userfaultfd-writeprotect is used in conjunction with swapped pages) then
    we'll possibly need a 3rd retry of the page fault.  It might also benefit
    other potential users who will have similar requirement like userfault
    write-protection.
    
    GUP code is not touched yet and will be covered in follow up patch.
    
    Please read the thread below for more information.
    
    [1] https://lore.kernel.org/lkml/20171102193644.GB22686@redhat.com/
    [2] https://lore.kernel.org/lkml/20181230154648.GB9832@redhat.com/
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Suggested-by: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Brian Geffon <bgeffon@google.com>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Denis Plotnikov <dplotnikov@virtuozzo.com>
    Cc: "Dr . David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Kirill A . Shutemov" <kirill@shutemov.name>
    Cc: Martin Cracauer <cracauer@cons.org>
    Cc: Marty McFadden <mcfadden8@llnl.gov>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Maya Gokhale <gokhale2@llnl.gov>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Link: http://lkml.kernel.org/r/20200220160246.9790-1-peterx@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index cbb29a43aa7f..1027851d469a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -521,12 +521,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	if (fault & VM_FAULT_RETRY) {
-		/*
-		 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of
-		 * starvation.
-		 */
 		if (mm_flags & FAULT_FLAG_ALLOW_RETRY) {
-			mm_flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			mm_flags |= FAULT_FLAG_TRIED;
 			goto retry;
 		}

commit dde1607248328cdb7570e3a252e8fb76b3411d66
Author: Peter Xu <peterx@redhat.com>
Date:   Wed Apr 1 21:08:37 2020 -0700

    mm: introduce FAULT_FLAG_DEFAULT
    
    Although there're tons of arch-specific page fault handlers, most of them
    are still sharing the same initial value of the page fault flags.  Say,
    merely all of the page fault handlers would allow the fault to be retried,
    and they also allow the fault to respond to SIGKILL.
    
    Let's define a default value for the fault flags to replace those initial
    page fault flags that were copied over.  With this, it'll be far easier to
    introduce new fault flag that can be used by all the architectures instead
    of touching all the archs.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Brian Geffon <bgeffon@google.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: Denis Plotnikov <dplotnikov@virtuozzo.com>
    Cc: "Dr . David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Kirill A . Shutemov" <kirill@shutemov.name>
    Cc: Martin Cracauer <cracauer@cons.org>
    Cc: Marty McFadden <mcfadden8@llnl.gov>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Maya Gokhale <gokhale2@llnl.gov>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Link: http://lkml.kernel.org/r/20200220160238.9694-1-peterx@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6f4b69d712b1..cbb29a43aa7f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -446,7 +446,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct mm_struct *mm = current->mm;
 	vm_fault_t fault, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
-	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
+	unsigned int mm_flags = FAULT_FLAG_DEFAULT;
 
 	if (kprobe_page_fault(regs, esr))
 		return 0;

commit b502f038f2ffc97a60fefcc120a868aa46009060
Author: Peter Xu <peterx@redhat.com>
Date:   Wed Apr 1 21:08:18 2020 -0700

    arm64/mm: use helper fault_signal_pending()
    
    Let the arm64 fault handling to use the new fault_signal_pending() helper,
    by moving the signal handling out of the retry logic.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Brian Geffon <bgeffon@google.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Bobby Powers <bobbypowers@gmail.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Denis Plotnikov <dplotnikov@virtuozzo.com>
    Cc: "Dr . David Alan Gilbert" <dgilbert@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Kirill A . Shutemov" <kirill@shutemov.name>
    Cc: Martin Cracauer <cracauer@cons.org>
    Cc: Marty McFadden <mcfadden8@llnl.gov>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Maya Gokhale <gokhale2@llnl.gov>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Pavel Emelyanov <xemul@openvz.org>
    Link: http://lkml.kernel.org/r/20200220155927.9264-1-peterx@redhat.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 85566d32958f..6f4b69d712b1 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -513,19 +513,14 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	fault = __do_page_fault(mm, addr, mm_flags, vm_flags);
 	major |= fault & VM_FAULT_MAJOR;
 
-	if (fault & VM_FAULT_RETRY) {
-		/*
-		 * If we need to retry but a fatal signal is pending,
-		 * handle the signal first. We do not need to release
-		 * the mmap_sem because it would already be released
-		 * in __lock_page_or_retry in mm/filemap.c.
-		 */
-		if (fatal_signal_pending(current)) {
-			if (!user_mode(regs))
-				goto no_context;
-			return 0;
-		}
+	/* Quick path to respond to signals */
+	if (fault_signal_pending(fault, regs)) {
+		if (!user_mode(regs))
+			goto no_context;
+		return 0;
+	}
 
+	if (fault & VM_FAULT_RETRY) {
 		/*
 		 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of
 		 * starvation.

commit 24cecc37746393432d994c0dbc251fb9ac7c5d72
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jan 6 14:35:39 2020 +0000

    arm64: Revert support for execute-only user mappings
    
    The ARMv8 64-bit architecture supports execute-only user permissions by
    clearing the PTE_USER and PTE_UXN bits, practically making it a mostly
    privileged mapping but from which user running at EL0 can still execute.
    
    The downside, however, is that the kernel at EL1 inadvertently reading
    such mapping would not trip over the PAN (privileged access never)
    protection.
    
    Revert the relevant bits from commit cab15ce604e5 ("arm64: Introduce
    execute-only page access permissions") so that PROT_EXEC implies
    PROT_READ (and therefore PTE_USER) until the architecture gains proper
    support for execute-only user mappings.
    
    Fixes: cab15ce604e5 ("arm64: Introduce execute-only page access permissions")
    Cc: <stable@vger.kernel.org> # 4.9.x-
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 077b02a2d4d3..85566d32958f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -445,7 +445,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	const struct fault_info *inf;
 	struct mm_struct *mm = current->mm;
 	vm_fault_t fault, major = 0;
-	unsigned long vm_flags = VM_READ | VM_WRITE;
+	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
 	if (kprobe_page_fault(regs, esr))

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit e44ec4a35dbdf3f3fe772f176fab3b8be7e02b0f
Author: Xiang Zheng <zhengxiang9@huawei.com>
Date:   Tue Oct 29 20:41:31 2019 +0800

    arm64: print additional fault message when executing non-exec memory
    
    When attempting to executing non-executable memory, the fault message
    shows:
    
      Unable to handle kernel read from unreadable memory at virtual address
      ffff802dac469000
    
    This may confuse someone, so add a new fault message for instruction
    abort.
    
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Xiang Zheng <zhengxiang9@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 855f2a7954e6..d46a2bb90f54 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -314,6 +314,8 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	if (is_el1_permission_fault(addr, esr, regs)) {
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
+		else if (is_el1_instruction_abort(esr))
+			msg = "execute from non-executable memory";
 		else
 			msg = "read from unreadable memory";
 	} else if (addr < PAGE_SIZE) {

commit 8301ae822d8d502b0ecc4b1c557221ecc6d97815
Merge: 346f6a4636f6 bfe298745afc
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 17:02:56 2019 +0000

    Merge branch 'for-next/entry-s-to-c' into for-next/core
    
    Move the synchronous exception paths from entry.S into a C file to
    improve the code readability.
    
    * for-next/entry-s-to-c:
      arm64: entry-common: don't touch daif before bp-hardening
      arm64: Remove asmlinkage from updated functions
      arm64: entry: convert el0_sync to C
      arm64: entry: convert el1_sync to C
      arm64: add local_daif_inherit()
      arm64: Add prototypes for functions called by entry.S
      arm64: remove __exception annotations

commit bfe298745afc9548ad9344a9a3f26c81fd1a76c4
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:16 2019 +0100

    arm64: entry-common: don't touch daif before bp-hardening
    
    The previous patches mechanically transformed the assembly version of
    entry.S to entry-common.c for synchronous exceptions.
    
    The C version of local_daif_restore() doesn't quite do the same thing
    as the assembly versions if pseudo-NMI is in use. In particular,
    | local_daif_restore(DAIF_PROCCTX_NOIRQ)
    will still allow pNMI to be delivered. This is not the behaviour
    do_el0_ia_bp_hardening() and do_sp_pc_abort() want as it should not
    be possible for the PMU handler to run as an NMI until the bp-hardening
    sequence has run.
    
    The bp-hardening calls were placed where they are because this was the
    first C code to run after the relevant exceptions. As we've now moved
    that point earlier, move the checks and calls earlier too.
    
    This makes it clearer that this stuff runs before any kind of exception,
    and saves modifying PSTATE twice.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index cb13f4daa878..1bb2e3737e51 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -32,8 +32,8 @@
 #include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
-#include <asm/kasan.h>
 #include <asm/kprobes.h>
+#include <asm/processor.h>
 #include <asm/sysreg.h>
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
@@ -102,18 +102,6 @@ static void mem_abort_decode(unsigned int esr)
 		data_abort_decode(esr);
 }
 
-static inline bool is_ttbr0_addr(unsigned long addr)
-{
-	/* entry assembly clears tags for TTBR0 addrs */
-	return addr < TASK_SIZE;
-}
-
-static inline bool is_ttbr1_addr(unsigned long addr)
-{
-	/* TTBR1 addresses may have a tag if KASAN_SW_TAGS is in use */
-	return arch_kasan_reset_tag(addr) >= PAGE_OFFSET;
-}
-
 static inline unsigned long mm_to_pgd_phys(struct mm_struct *mm)
 {
 	/* Either init_pg_dir or swapper_pg_dir */
@@ -758,30 +746,8 @@ void do_el0_irq_bp_hardening(void)
 }
 NOKPROBE_SYMBOL(do_el0_irq_bp_hardening);
 
-void do_el0_ia_bp_hardening(unsigned long addr,  unsigned int esr,
-			    struct pt_regs *regs)
-{
-	/*
-	 * We've taken an instruction abort from userspace and not yet
-	 * re-enabled IRQs. If the address is a kernel address, apply
-	 * BP hardening prior to enabling IRQs and pre-emption.
-	 */
-	if (!is_ttbr0_addr(addr))
-		arm64_apply_bp_hardening();
-
-	local_daif_restore(DAIF_PROCCTX);
-	do_mem_abort(addr, esr, regs);
-}
-NOKPROBE_SYMBOL(do_el0_ia_bp_hardening);
-
 void do_sp_pc_abort(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
-	if (user_mode(regs)) {
-		if (!is_ttbr0_addr(instruction_pointer(regs)))
-			arm64_apply_bp_hardening();
-		local_daif_restore(DAIF_PROCCTX);
-	}
-
 	arm64_notify_die("SP/PC alignment exception", regs,
 			 SIGBUS, BUS_ADRALN, (void __user *)addr, esr);
 }

commit afa7c0e5b965cdb945ad8a2e2973c6d7e19969f9
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:15 2019 +0100

    arm64: Remove asmlinkage from updated functions
    
    Now that the callers of these functions have moved into C, they no longer
    need the asmlinkage annotation. Remove it.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 844cd2535826..cb13f4daa878 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -733,8 +733,7 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 63"			},
 };
 
-asmlinkage void do_mem_abort(unsigned long addr, unsigned int esr,
-			     struct pt_regs *regs)
+void do_mem_abort(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_fault_info(esr);
 
@@ -752,15 +751,15 @@ asmlinkage void do_mem_abort(unsigned long addr, unsigned int esr,
 }
 NOKPROBE_SYMBOL(do_mem_abort);
 
-asmlinkage void do_el0_irq_bp_hardening(void)
+void do_el0_irq_bp_hardening(void)
 {
 	/* PC has already been checked in entry.S */
 	arm64_apply_bp_hardening();
 }
 NOKPROBE_SYMBOL(do_el0_irq_bp_hardening);
 
-asmlinkage void do_el0_ia_bp_hardening(unsigned long addr,  unsigned int esr,
-				       struct pt_regs *regs)
+void do_el0_ia_bp_hardening(unsigned long addr,  unsigned int esr,
+			    struct pt_regs *regs)
 {
 	/*
 	 * We've taken an instruction abort from userspace and not yet
@@ -775,8 +774,7 @@ asmlinkage void do_el0_ia_bp_hardening(unsigned long addr,  unsigned int esr,
 }
 NOKPROBE_SYMBOL(do_el0_ia_bp_hardening);
 
-asmlinkage void do_sp_pc_abort(unsigned long addr, unsigned int esr,
-			       struct pt_regs *regs)
+void do_sp_pc_abort(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	if (user_mode(regs)) {
 		if (!is_ttbr0_addr(instruction_pointer(regs)))
@@ -896,8 +894,8 @@ static int cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
 #endif /* CONFIG_ARM64_ERRATUM_1463225 */
 NOKPROBE_SYMBOL(cortex_a76_erratum_1463225_debug_handler);
 
-asmlinkage void do_debug_exception(unsigned long addr_if_watchpoint,
-				   unsigned int esr, struct pt_regs *regs)
+void do_debug_exception(unsigned long addr_if_watchpoint, unsigned int esr,
+			struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_debug_fault_info(esr);
 	unsigned long pc = instruction_pointer(regs);

commit b6e43c0e3129ffe87e65c85f20fcbdf0eb86fba0
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:10 2019 +0100

    arm64: remove __exception annotations
    
    Since commit 732674980139 ("arm64: unwind: reference pt_regs via embedded
    stack frame") arm64 has not used the __exception annotation to dump
    the pt_regs during stack tracing. in_exception_text() has no callers.
    
    This annotation is only used to blacklist kprobes, it means the same as
    __kprobes.
    
    Section annotations like this require the functions to be grouped
    together between the start/end markers, and placed according to
    the linker script. For kprobes we also have NOKPROBE_SYMBOL() which
    logs the symbol address in a section that kprobes parses and
    blacklists at boot.
    
    Using NOKPROBE_SYMBOL() instead lets kprobes publish the list of
    blacklisted symbols, and saves us from having an arm64 specific
    spelling of __kprobes.
    
    do_debug_exception() already has a NOKPROBE_SYMBOL() annotation.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 855f2a7954e6..844cd2535826 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -33,6 +33,7 @@
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
 #include <asm/kasan.h>
+#include <asm/kprobes.h>
 #include <asm/sysreg.h>
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
@@ -732,8 +733,8 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 63"			},
 };
 
-asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
-					 struct pt_regs *regs)
+asmlinkage void do_mem_abort(unsigned long addr, unsigned int esr,
+			     struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_fault_info(esr);
 
@@ -749,16 +750,17 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	arm64_notify_die(inf->name, regs,
 			 inf->sig, inf->code, (void __user *)addr, esr);
 }
+NOKPROBE_SYMBOL(do_mem_abort);
 
-asmlinkage void __exception do_el0_irq_bp_hardening(void)
+asmlinkage void do_el0_irq_bp_hardening(void)
 {
 	/* PC has already been checked in entry.S */
 	arm64_apply_bp_hardening();
 }
+NOKPROBE_SYMBOL(do_el0_irq_bp_hardening);
 
-asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
-						   unsigned int esr,
-						   struct pt_regs *regs)
+asmlinkage void do_el0_ia_bp_hardening(unsigned long addr,  unsigned int esr,
+				       struct pt_regs *regs)
 {
 	/*
 	 * We've taken an instruction abort from userspace and not yet
@@ -771,11 +773,10 @@ asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
 	local_daif_restore(DAIF_PROCCTX);
 	do_mem_abort(addr, esr, regs);
 }
+NOKPROBE_SYMBOL(do_el0_ia_bp_hardening);
 
-
-asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
-					   unsigned int esr,
-					   struct pt_regs *regs)
+asmlinkage void do_sp_pc_abort(unsigned long addr, unsigned int esr,
+			       struct pt_regs *regs)
 {
 	if (user_mode(regs)) {
 		if (!is_ttbr0_addr(instruction_pointer(regs)))
@@ -786,6 +787,7 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 	arm64_notify_die("SP/PC alignment exception", regs,
 			 SIGBUS, BUS_ADRALN, (void __user *)addr, esr);
 }
+NOKPROBE_SYMBOL(do_sp_pc_abort);
 
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		       struct pt_regs *regs);
@@ -868,8 +870,7 @@ NOKPROBE_SYMBOL(debug_exception_exit);
 #ifdef CONFIG_ARM64_ERRATUM_1463225
 DECLARE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
 
-static int __exception
-cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+static int cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
 {
 	if (user_mode(regs))
 		return 0;
@@ -888,16 +889,15 @@ cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
 	return 1;
 }
 #else
-static int __exception
-cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+static int cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
 {
 	return 0;
 }
 #endif /* CONFIG_ARM64_ERRATUM_1463225 */
+NOKPROBE_SYMBOL(cortex_a76_erratum_1463225_debug_handler);
 
-asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
-					       unsigned int esr,
-					       struct pt_regs *regs)
+asmlinkage void do_debug_exception(unsigned long addr_if_watchpoint,
+				   unsigned int esr, struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_debug_fault_info(esr);
 	unsigned long pc = instruction_pointer(regs);

commit 3813733595c0c7c0674d106309b04e871d54dc1c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Oct 16 12:03:04 2019 +0100

    arm64: mm: fix inverted PAR_EL1.F check
    
    When detecting a spurious EL1 translation fault, we have the CPU retry
    the translation using an AT S1E1R instruction, and inspect PAR_EL1 to
    determine if the fault was spurious.
    
    When PAR_EL1.F == 0, the AT instruction successfully translated the
    address without a fault, which implies the original fault was spurious.
    However, in this case we return false and treat the original fault as if
    it was not spurious.
    
    Invert the return value so that we treat such a case as spurious.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Fixes: 42f91093b043 ("arm64: mm: Ignore spurious translation faults taken from the kernel")
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 855f2a7954e6..9fc6db0bcbad 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -268,8 +268,12 @@ static bool __kprobes is_spurious_el1_translation_fault(unsigned long addr,
 	par = read_sysreg(par_el1);
 	local_irq_restore(flags);
 
+	/*
+	 * If we now have a valid translation, treat the translation fault as
+	 * spurious.
+	 */
 	if (!(par & SYS_PAR_EL1_F))
-		return false;
+		return true;
 
 	/*
 	 * If we got a different type of fault from the AT instruction,

commit 308c51561720547a90767a2f367f5390052c51da
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Oct 4 14:58:47 2019 +0100

    arm64: mm: fix spurious fault detection
    
    When detecting a spurious EL1 translation fault, we attempt to compare
    ESR_EL1.DFSC with PAR_EL1.FST. We erroneously use FIELD_PREP() to
    extract PAR_EL1.FST, when we should be using FIELD_GET().
    
    In the wise words of Robin Murphy:
    
    | FIELD_GET() is a UBFX, FIELD_PREP() is a BFI
    
    Using FIELD_PREP() means that that dfsc & ESR_ELx_FSC_TYPE is always
    zero, and hence not equal to ESR_ELx_FSC_FAULT. Thus we detect any
    unhandled translation fault as spurious.
    
    ... so let's use FIELD_GET() to ensure we don't decide all translation
    faults are spurious. ESR_EL1.DFSC occupies bits [5:0], and requires no
    shifting.
    
    Fixes: 42f91093b043332a ("arm64: mm: Ignore spurious translation faults taken from the kernel")
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Robin Murphy <robin.murphy@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6acd866f31fd..855f2a7954e6 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -275,7 +275,7 @@ static bool __kprobes is_spurious_el1_translation_fault(unsigned long addr,
 	 * If we got a different type of fault from the AT instruction,
 	 * treat the translation fault as spurious.
 	 */
-	dfsc = FIELD_PREP(SYS_PAR_EL1_FST, par);
+	dfsc = FIELD_GET(SYS_PAR_EL1_FST, par);
 	return (dfsc & ESR_ELx_FSC_TYPE) != ESR_ELx_FSC_FAULT;
 }
 

commit e4365f968fcd5ef4a2f69f11ebc9ee66f47fc879
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 3 10:49:32 2019 +0100

    arm64: mm: avoid virt_to_phys(init_mm.pgd)
    
    If we take an unhandled fault in the kernel, we call show_pte() to dump
    the {PGDP,PGD,PUD,PMD,PTE} values for the corresponding page table walk,
    where the PGDP value is virt_to_phys(mm->pgd).
    
    The boot-time and runtime kernel page tables, init_pg_dir and
    swapper_pg_dir respectively, are kernel symbols. Thus, it is not valid
    to call virt_to_phys() on either of these, though we'll do so if we take
    a fault on a TTBR1 address.
    
    When CONFIG_DEBUG_VIRTUAL is not selected, virt_to_phys() will silently
    fix this up. However, when CONFIG_DEBUG_VIRTUAL is selected, this
    results in splats as below. Depending on when these occur, they can
    happen to suppress information needed to debug the original unhandled
    fault, such as the backtrace:
    
    | Unable to handle kernel paging request at virtual address ffff7fffec73cf0f
    | Mem abort info:
    |   ESR = 0x96000004
    |   EC = 0x25: DABT (current EL), IL = 32 bits
    |   SET = 0, FnV = 0
    |   EA = 0, S1PTW = 0
    | Data abort info:
    |   ISV = 0, ISS = 0x00000004
    |   CM = 0, WnR = 0
    | ------------[ cut here ]------------
    | virt_to_phys used for non-linear address: 00000000102c9dbe (swapper_pg_dir+0x0/0x1000)
    | WARNING: CPU: 1 PID: 7558 at arch/arm64/mm/physaddr.c:15 __virt_to_phys+0xe0/0x170 arch/arm64/mm/physaddr.c:12
    | Kernel panic - not syncing: panic_on_warn set ...
    | SMP: stopping secondary CPUs
    | Dumping ftrace buffer:
    |    (ftrace buffer empty)
    | Kernel Offset: disabled
    | CPU features: 0x0002,23000438
    | Memory Limit: none
    | Rebooting in 1 seconds..
    
    We can avoid this by ensuring that we call __pa_symbol() for
    init_mm.pgd, as this will always be a kernel symbol. As the dumped
    {PGD,PUD,PMD,PTE} values are the raw values from the relevant entries we
    don't need to handle these specially.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 115d7a0e4b08..6acd866f31fd 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -113,6 +113,15 @@ static inline bool is_ttbr1_addr(unsigned long addr)
 	return arch_kasan_reset_tag(addr) >= PAGE_OFFSET;
 }
 
+static inline unsigned long mm_to_pgd_phys(struct mm_struct *mm)
+{
+	/* Either init_pg_dir or swapper_pg_dir */
+	if (mm == &init_mm)
+		return __pa_symbol(mm->pgd);
+
+	return (unsigned long)virt_to_phys(mm->pgd);
+}
+
 /*
  * Dump out the page tables associated with 'addr' in the currently active mm.
  */
@@ -141,7 +150,7 @@ static void show_pte(unsigned long addr)
 
 	pr_alert("%s pgtable: %luk pages, %llu-bit VAs, pgdp=%016lx\n",
 		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
-		 vabits_actual, (unsigned long)virt_to_phys(mm->pgd));
+		 vabits_actual, mm_to_pgd_phys(mm));
 	pgdp = pgd_offset(mm, addr);
 	pgd = READ_ONCE(*pgdp);
 	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit 42f91093b043332ad75cea7aeafecda6fe81814c
Author: Will Deacon <will@kernel.org>
Date:   Thu Aug 22 17:22:14 2019 +0100

    arm64: mm: Ignore spurious translation faults taken from the kernel
    
    Thanks to address translation being performed out of order with respect to
    loads and stores, it is possible for a CPU to take a translation fault when
    accessing a page that was mapped by a different CPU.
    
    For example, in the case that one CPU maps a page and then sets a flag to
    tell another CPU:
    
            CPU 0
            -----
    
            MOV     X0, <valid pte>
            STR     X0, [Xptep]     // Store new PTE to page table
            DSB     ISHST
            ISB
            MOV     X1, #1
            STR     X1, [Xflag]     // Set the flag
    
            CPU 1
            -----
    
    loop:   LDAR    X0, [Xflag]     // Poll flag with Acquire semantics
            CBZ     X0, loop
            LDR     X1, [X2]        // Translates using the new PTE
    
    then the final load on CPU 1 can raise a translation fault because the
    translation can be performed speculatively before the read of the flag and
    marked as "faulting" by the CPU. This isn't quite as bad as it sounds
    since, in reality, code such as:
    
            CPU 0                           CPU 1
            -----                           -----
            spin_lock(&lock);               spin_lock(&lock);
            *ptr = vmalloc(size);           if (*ptr)
            spin_unlock(&lock);                     foo = **ptr;
                                            spin_unlock(&lock);
    
    will not trigger the fault because there is an address dependency on CPU 1
    which prevents the speculative translation. However, more exotic code where
    the virtual address is known ahead of time, such as:
    
            CPU 0                           CPU 1
            -----                           -----
            spin_lock(&lock);               spin_lock(&lock);
            set_fixmap(0, paddr, prot);     if (mapped)
            mapped = true;                          foo = *fix_to_virt(0);
            spin_unlock(&lock);             spin_unlock(&lock);
    
    could fault. This can be avoided by any of:
    
            * Introducing broadcast TLB maintenance on the map path
            * Adding a DSB;ISB sequence after checking a flag which indicates
              that a virtual address is now mapped
            * Handling the spurious fault
    
    Given that we have never observed a problem due to this under Linux and
    future revisions of the architecture are being tightened so that
    translation table walks are effectively ordered in the same way as explicit
    memory accesses, we no longer treat spurious kernel faults as fatal if an
    AT instruction indicates that the access does not trigger a translation
    fault.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index cfd65b63f36f..9808da29a653 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -8,6 +8,7 @@
  */
 
 #include <linux/acpi.h>
+#include <linux/bitfield.h>
 #include <linux/extable.h>
 #include <linux/signal.h>
 #include <linux/mm.h>
@@ -242,6 +243,34 @@ static inline bool is_el1_permission_fault(unsigned long addr, unsigned int esr,
 	return false;
 }
 
+static bool __kprobes is_spurious_el1_translation_fault(unsigned long addr,
+							unsigned int esr,
+							struct pt_regs *regs)
+{
+	unsigned long flags;
+	u64 par, dfsc;
+
+	if (ESR_ELx_EC(esr) != ESR_ELx_EC_DABT_CUR ||
+	    (esr & ESR_ELx_FSC_TYPE) != ESR_ELx_FSC_FAULT)
+		return false;
+
+	local_irq_save(flags);
+	asm volatile("at s1e1r, %0" :: "r" (addr));
+	isb();
+	par = read_sysreg(par_el1);
+	local_irq_restore(flags);
+
+	if (!(par & SYS_PAR_EL1_F))
+		return false;
+
+	/*
+	 * If we got a different type of fault from the AT instruction,
+	 * treat the translation fault as spurious.
+	 */
+	dfsc = FIELD_PREP(SYS_PAR_EL1_FST, par);
+	return (dfsc & ESR_ELx_FSC_TYPE) != ESR_ELx_FSC_FAULT;
+}
+
 static void die_kernel_fault(const char *msg, unsigned long addr,
 			     unsigned int esr, struct pt_regs *regs)
 {
@@ -270,6 +299,10 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	if (!is_el1_instruction_abort(esr) && fixup_exception(regs))
 		return;
 
+	if (WARN_RATELIMIT(is_spurious_el1_translation_fault(addr, esr, regs),
+	    "Ignoring spurious kernel translation fault at virtual address %016lx\n", addr))
+		return;
+
 	if (is_el1_permission_fault(addr, esr, regs)) {
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";

commit 233947ef16a18952d22786770dab1ddafa1ac377
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Aug 14 14:28:47 2019 +0100

    arm64: memory: fix flipped VA space fallout
    
    VA_START used to be the start of the TTBR1 address space, but now it's a
    point midway though. In a couple of places we still use VA_START to get
    the start of the TTBR1 address space, so let's fix these up to use
    PAGE_OFFSET instead.
    
    Fixes: 14c127c957c1c607 ("arm64: mm: Flip kernel VA space")
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 75eff57bd9ef..bb4e4f3fffd8 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -109,7 +109,7 @@ static inline bool is_ttbr0_addr(unsigned long addr)
 static inline bool is_ttbr1_addr(unsigned long addr)
 {
 	/* TTBR1 addresses may have a tag if KASAN_SW_TAGS is in use */
-	return arch_kasan_reset_tag(addr) >= VA_START;
+	return arch_kasan_reset_tag(addr) >= PAGE_OFFSET;
 }
 
 /*

commit 2c624fe68715e76eba1a7089f91e122310dc663c
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:23 2019 +0100

    arm64: mm: Remove vabits_user
    
    Previous patches have enabled 52-bit kernel + user VAs and there is no
    longer any scenario where user VA != kernel VA size.
    
    This patch removes the, now redundant, vabits_user variable and replaces
    usage with vabits_actual where appropriate.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6b195871769a..75eff57bd9ef 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -140,8 +140,7 @@ static void show_pte(unsigned long addr)
 
 	pr_alert("%s pgtable: %luk pages, %llu-bit VAs, pgdp=%016lx\n",
 		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
-		 mm == &init_mm ? vabits_actual : (int)vabits_user,
-		 (unsigned long)virt_to_phys(mm->pgd));
+		 vabits_actual, (unsigned long)virt_to_phys(mm->pgd));
 	pgdp = pgd_offset(mm, addr);
 	pgd = READ_ONCE(*pgdp);
 	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));

commit 5383cc6efed13784ddb3cff2cc183b6b8c50c8db
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:18 2019 +0100

    arm64: mm: Introduce vabits_actual
    
    In order to support 52-bit kernel addresses detectable at boot time, one
    needs to know the actual VA_BITS detected. A new variable vabits_actual
    is introduced in this commit and employed for the KVM hypervisor layout,
    KASAN, fault handling and phys-to/from-virt translation where there
    would normally be compile time constants.
    
    In order to maintain performance in phys_to_virt, another variable
    physvirt_offset is introduced.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index cfd65b63f36f..6b195871769a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -138,9 +138,9 @@ static void show_pte(unsigned long addr)
 		return;
 	}
 
-	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgdp=%016lx\n",
+	pr_alert("%s pgtable: %luk pages, %llu-bit VAs, pgdp=%016lx\n",
 		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
-		 mm == &init_mm ? VA_BITS : (int)vabits_user,
+		 mm == &init_mm ? vabits_actual : (int)vabits_user,
 		 (unsigned long)virt_to_phys(mm->pgd));
 	pgdp = pgd_offset(mm, addr);
 	pgd = READ_ONCE(*pgdp);

commit 2951d5efaf8b67cc4851373de92a91a60899611c
Author: Miles Chen <miles.chen@mediatek.com>
Date:   Wed Aug 7 08:33:36 2019 +0800

    arm64: mm: print hexadecimal EC value in mem_abort_decode()
    
    This change prints the hexadecimal EC value in mem_abort_decode(),
    which makes it easier to lookup the corresponding EC in
    the ARM Architecture Reference Manual.
    
    The commit 1f9b8936f36f ("arm64: Decode information from ESR upon mem
    faults") prints useful information when memory abort occurs. It would
    be easier to lookup "0x25" instead of "DABT" in the document. Then we
    can check the corresponding ISS.
    
    For example:
    Current info            Document
                            EC      Exception class
    "CP15 MCR/MRC"          0x3     "MCR or MRC access to CP15a..."
    "ASIMD"                 0x7     "Access to SIMD or floating-point..."
    "DABT (current EL)"     0x25    "Data Abort taken without..."
    ...
    
    Before:
    Unable to handle kernel paging request at virtual address 000000000000c000
    Mem abort info:
      ESR = 0x96000046
      Exception class = DABT (current EL), IL = 32 bits
      SET = 0, FnV = 0
      EA = 0, S1PTW = 0
    Data abort info:
      ISV = 0, ISS = 0x00000046
      CM = 0, WnR = 1
    
    After:
    Unable to handle kernel paging request at virtual address 000000000000c000
    Mem abort info:
      ESR = 0x96000046
      EC = 0x25: DABT (current EL), IL = 32 bits
      SET = 0, FnV = 0
      EA = 0, S1PTW = 0
    Data abort info:
      ISV = 0, ISS = 0x00000046
      CM = 0, WnR = 1
    
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <Mark.rutland@arm.com>
    Signed-off-by: Miles Chen <miles.chen@mediatek.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index cfd65b63f36f..ad4980a27edb 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -86,8 +86,8 @@ static void mem_abort_decode(unsigned int esr)
 	pr_alert("Mem abort info:\n");
 
 	pr_alert("  ESR = 0x%08x\n", esr);
-	pr_alert("  Exception class = %s, IL = %u bits\n",
-		 esr_get_class_string(esr),
+	pr_alert("  EC = 0x%02lx: %s, IL = %u bits\n",
+		 ESR_ELx_EC(esr), esr_get_class_string(esr),
 		 (esr & ESR_ELx_IL) ? 32 : 16);
 	pr_alert("  SET = %lu, FnV = %lu\n",
 		 (esr & ESR_ELx_SET_MASK) >> ESR_ELx_SET_SHIFT,

commit d8bb6718c4db9bcd075dde7ff55d46091ccfae15
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Thu Aug 1 23:36:14 2019 +0900

    arm64: Make debug exception handlers visible from RCU
    
    Make debug exceptions visible from RCU so that synchronize_rcu()
    correctly track the debug exception handler.
    
    This also introduces sanity checks for user-mode exceptions as same
    as x86's ist_enter()/ist_exit().
    
    The debug exception can interrupt in idle task. For example, it warns
    if we put a kprobe on a function called from idle task as below.
    The warning message showed that the rcu_read_lock() caused this
    problem. But actually, this means the RCU is lost the context which
    is already in NMI/IRQ.
    
      /sys/kernel/debug/tracing # echo p default_idle_call >> kprobe_events
      /sys/kernel/debug/tracing # echo 1 > events/kprobes/enable
      /sys/kernel/debug/tracing # [  135.122237]
      [  135.125035] =============================
      [  135.125310] WARNING: suspicious RCU usage
      [  135.125581] 5.2.0-08445-g9187c508bdc7 #20 Not tainted
      [  135.125904] -----------------------------
      [  135.126205] include/linux/rcupdate.h:594 rcu_read_lock() used illegally while idle!
      [  135.126839]
      [  135.126839] other info that might help us debug this:
      [  135.126839]
      [  135.127410]
      [  135.127410] RCU used illegally from idle CPU!
      [  135.127410] rcu_scheduler_active = 2, debug_locks = 1
      [  135.128114] RCU used illegally from extended quiescent state!
      [  135.128555] 1 lock held by swapper/0/0:
      [  135.128944]  #0: (____ptrval____) (rcu_read_lock){....}, at: call_break_hook+0x0/0x178
      [  135.130499]
      [  135.130499] stack backtrace:
      [  135.131192] CPU: 0 PID: 0 Comm: swapper/0 Not tainted 5.2.0-08445-g9187c508bdc7 #20
      [  135.131841] Hardware name: linux,dummy-virt (DT)
      [  135.132224] Call trace:
      [  135.132491]  dump_backtrace+0x0/0x140
      [  135.132806]  show_stack+0x24/0x30
      [  135.133133]  dump_stack+0xc4/0x10c
      [  135.133726]  lockdep_rcu_suspicious+0xf8/0x108
      [  135.134171]  call_break_hook+0x170/0x178
      [  135.134486]  brk_handler+0x28/0x68
      [  135.134792]  do_debug_exception+0x90/0x150
      [  135.135051]  el1_dbg+0x18/0x8c
      [  135.135260]  default_idle_call+0x0/0x44
      [  135.135516]  cpu_startup_entry+0x2c/0x30
      [  135.135815]  rest_init+0x1b0/0x280
      [  135.136044]  arch_call_rest_init+0x14/0x1c
      [  135.136305]  start_kernel+0x4d4/0x500
      [  135.136597]
    
    So make debug exception visible to RCU can fix this warning.
    
    Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
    Acked-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 9568c116ac7f..cfd65b63f36f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -777,6 +777,53 @@ void __init hook_debug_fault_code(int nr,
 	debug_fault_info[nr].name	= name;
 }
 
+/*
+ * In debug exception context, we explicitly disable preemption despite
+ * having interrupts disabled.
+ * This serves two purposes: it makes it much less likely that we would
+ * accidentally schedule in exception context and it will force a warning
+ * if we somehow manage to schedule by accident.
+ */
+static void debug_exception_enter(struct pt_regs *regs)
+{
+	/*
+	 * Tell lockdep we disabled irqs in entry.S. Do nothing if they were
+	 * already disabled to preserve the last enabled/disabled addresses.
+	 */
+	if (interrupts_enabled(regs))
+		trace_hardirqs_off();
+
+	if (user_mode(regs)) {
+		RCU_LOCKDEP_WARN(!rcu_is_watching(), "entry code didn't wake RCU");
+	} else {
+		/*
+		 * We might have interrupted pretty much anything.  In
+		 * fact, if we're a debug exception, we can even interrupt
+		 * NMI processing. We don't want this code makes in_nmi()
+		 * to return true, but we need to notify RCU.
+		 */
+		rcu_nmi_enter();
+	}
+
+	preempt_disable();
+
+	/* This code is a bit fragile.  Test it. */
+	RCU_LOCKDEP_WARN(!rcu_is_watching(), "exception_enter didn't work");
+}
+NOKPROBE_SYMBOL(debug_exception_enter);
+
+static void debug_exception_exit(struct pt_regs *regs)
+{
+	preempt_enable_no_resched();
+
+	if (!user_mode(regs))
+		rcu_nmi_exit();
+
+	if (interrupts_enabled(regs))
+		trace_hardirqs_on();
+}
+NOKPROBE_SYMBOL(debug_exception_exit);
+
 #ifdef CONFIG_ARM64_ERRATUM_1463225
 DECLARE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
 
@@ -817,12 +864,7 @@ asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
 	if (cortex_a76_erratum_1463225_debug_handler(regs))
 		return;
 
-	/*
-	 * Tell lockdep we disabled irqs in entry.S. Do nothing if they were
-	 * already disabled to preserve the last enabled/disabled addresses.
-	 */
-	if (interrupts_enabled(regs))
-		trace_hardirqs_off();
+	debug_exception_enter(regs);
 
 	if (user_mode(regs) && !is_ttbr0_addr(pc))
 		arm64_apply_bp_hardening();
@@ -832,7 +874,6 @@ asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
 				 inf->sig, inf->code, (void __user *)pc, esr);
 	}
 
-	if (interrupts_enabled(regs))
-		trace_hardirqs_on();
+	debug_exception_exit(regs);
 }
 NOKPROBE_SYMBOL(do_debug_exception);

commit b98cca444d287a63dd96df04af7fb9793567599e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Jul 16 16:28:00 2019 -0700

    mm, kprobes: generalize and rename notify_page_fault() as kprobe_page_fault()
    
    Architectures which support kprobes have very similar boilerplate around
    calling kprobe_fault_handler().  Use a helper function in kprobes.h to
    unify them, based on the x86 code.
    
    This changes the behaviour for other architectures when preemption is
    enabled.  Previously, they would have disabled preemption while calling
    the kprobe handler.  However, preemption would be disabled if this fault
    was due to a kprobe, so we know the fault was not due to a kprobe
    handler and can simply return failure.
    
    This behaviour was introduced in commit a980c0ef9f6d ("x86/kprobes:
    Refactor kprobes_fault() like kprobe_exceptions_notify()")
    
    [anshuman.khandual@arm.com: export kprobe_fault_handler()]
      Link: http://lkml.kernel.org/r/1561133358-8876-1-git-send-email-anshuman.khandual@arm.com
    Link: http://lkml.kernel.org/r/1560420444-25737-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c8c61b1eb479..9568c116ac7f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -59,28 +59,6 @@ static inline const struct fault_info *esr_to_debug_fault_info(unsigned int esr)
 	return debug_fault_info + DBG_ESR_EVT(esr);
 }
 
-#ifdef CONFIG_KPROBES
-static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
-{
-	int ret = 0;
-
-	/* kprobe_running() needs smp_processor_id() */
-	if (!user_mode(regs)) {
-		preempt_disable();
-		if (kprobe_running() && kprobe_fault_handler(regs, esr))
-			ret = 1;
-		preempt_enable();
-	}
-
-	return ret;
-}
-#else
-static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
-{
-	return 0;
-}
-#endif
-
 static void data_abort_decode(unsigned int esr)
 {
 	pr_alert("Data abort info:\n");
@@ -434,7 +412,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
-	if (notify_page_fault(regs, esr))
+	if (kprobe_page_fault(regs, esr))
 		return 0;
 
 	/*

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index a30818ed9c60..2d115016feb4 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -1,21 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/mm/fault.c
  *
  * Copyright (C) 1995  Linus Torvalds
  * Copyright (C) 1995-2004 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/acpi.h>

commit 4745224b45097d333358bce298aea2137246183c
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Jun 7 14:43:06 2019 +0530

    arm64/mm: Refactor __do_page_fault()
    
    __do_page_fault() is over complicated with multiple goto statements. This
    cleans up the code flow and while there drops local variable vm_fault_t.
    
    Reviewed-by: Mark Rutland <Mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 765b5eb601ca..582061dec89f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -397,37 +397,29 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 static vm_fault_t __do_page_fault(struct mm_struct *mm, unsigned long addr,
 			   unsigned int mm_flags, unsigned long vm_flags)
 {
-	struct vm_area_struct *vma;
-	vm_fault_t fault;
+	struct vm_area_struct *vma = find_vma(mm, addr);
 
-	vma = find_vma(mm, addr);
-	fault = VM_FAULT_BADMAP;
 	if (unlikely(!vma))
-		goto out;
-	if (unlikely(vma->vm_start > addr))
-		goto check_stack;
+		return VM_FAULT_BADMAP;
 
 	/*
 	 * Ok, we have a good vm_area for this memory access, so we can handle
 	 * it.
 	 */
-good_area:
+	if (unlikely(vma->vm_start > addr)) {
+		if (!(vma->vm_flags & VM_GROWSDOWN))
+			return VM_FAULT_BADMAP;
+		if (expand_stack(vma, addr))
+			return VM_FAULT_BADMAP;
+	}
+
 	/*
 	 * Check that the permissions on the VMA allow for the fault which
 	 * occurred.
 	 */
-	if (!(vma->vm_flags & vm_flags)) {
-		fault = VM_FAULT_BADACCESS;
-		goto out;
-	}
-
+	if (!(vma->vm_flags & vm_flags))
+		return VM_FAULT_BADACCESS;
 	return handle_mm_fault(vma, addr & PAGE_MASK, mm_flags);
-
-check_stack:
-	if (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))
-		goto good_area;
-out:
-	return fault;
 }
 
 static bool is_el0_instruction_abort(unsigned int esr)

commit c49bd02f4c7412f0182252ae2ef6e916ca4ff359
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Jun 7 14:43:05 2019 +0530

    arm64/mm: Document write abort detection from ESR
    
    This patch adds an is_write_abort() wrapper and documents the detection
    of the abort type on cache maintenance operations.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    [catalin.marinas@arm.com: only keep the is_write_abort() wrapper]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 7c1c8f435f86..765b5eb601ca 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -435,6 +435,15 @@ static bool is_el0_instruction_abort(unsigned int esr)
 	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_LOW;
 }
 
+/*
+ * Note: not valid for EL1 DC IVAC, but we never use that such that it
+ * should fault. EL0 cannot issue DC IVAC (undef).
+ */
+static bool is_write_abort(unsigned int esr)
+{
+	return (esr & ESR_ELx_WNR) && !(esr & ESR_ELx_CM);
+}
+
 static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				   struct pt_regs *regs)
 {
@@ -460,7 +469,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	if (is_el0_instruction_abort(esr)) {
 		vm_flags = VM_EXEC;
 		mm_flags |= FAULT_FLAG_INSTRUCTION;
-	} else if ((esr & ESR_ELx_WNR) && !(esr & ESR_ELx_CM)) {
+	} else if (is_write_abort(esr)) {
 		vm_flags = VM_WRITE;
 		mm_flags |= FAULT_FLAG_WRITE;
 	}

commit 616810360043183a9db73e39f5aadbe48952c383
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon Jun 3 12:11:23 2019 +0530

    arm64/mm: Drop task_struct argument from __do_page_fault()
    
    The task_struct argument is not getting used in __do_page_fault(). Hence
    just drop it and use current or cuurent->mm instead where ever required.
    This does not change any functionality.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 2256a1a09f1b..7c1c8f435f86 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -395,8 +395,7 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 #define VM_FAULT_BADACCESS	0x020000
 
 static vm_fault_t __do_page_fault(struct mm_struct *mm, unsigned long addr,
-			   unsigned int mm_flags, unsigned long vm_flags,
-			   struct task_struct *tsk)
+			   unsigned int mm_flags, unsigned long vm_flags)
 {
 	struct vm_area_struct *vma;
 	vm_fault_t fault;
@@ -440,8 +439,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				   struct pt_regs *regs)
 {
 	const struct fault_info *inf;
-	struct task_struct *tsk;
-	struct mm_struct *mm;
+	struct mm_struct *mm = current->mm;
 	vm_fault_t fault, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
@@ -449,9 +447,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	if (notify_page_fault(regs, esr))
 		return 0;
 
-	tsk = current;
-	mm  = tsk->mm;
-
 	/*
 	 * If we're in an interrupt or have no user context, we must not take
 	 * the fault.
@@ -511,7 +506,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 #endif
 	}
 
-	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk);
+	fault = __do_page_fault(mm, addr, mm_flags, vm_flags);
 	major |= fault & VM_FAULT_MAJOR;
 
 	if (fault & VM_FAULT_RETRY) {
@@ -551,11 +546,11 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 * that point.
 		 */
 		if (major) {
-			tsk->maj_flt++;
+			current->maj_flt++;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs,
 				      addr);
 		} else {
-			tsk->min_flt++;
+			current->min_flt++;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs,
 				      addr);
 		}

commit a0509313d5dea0a27a5968f04bd556d05e6349fd
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon Jun 3 12:11:22 2019 +0530

    arm64/mm: Drop mmap_sem before calling __do_kernel_fault()
    
    There is an inconsistency between down_read_trylock() success and failure
    paths while dealing with kernel access for non exception table areas where
    it calls __do_kernel_fault(). In case of failure it just bails out without
    holding mmap_sem but when it succeeds it does so while holding mmap_sem.
    Fix this inconsistency by just dropping mmap_sem in success path as well.
    
    __do_kernel_fault() calls die_kernel_fault() which then calls show_pte().
    show_pte() in this path might become bit more unreliable without holding
    mmap_sem. But there are already instances [1] in do_page_fault() where
    die_kernel_fault() gets called without holding mmap_sem. show_pte() can
    be made more robust independently but in a later patch.
    
    [1] Conditional block for (is_ttbr0_addr && is_el1_permission_fault)
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 392386a693fe..2256a1a09f1b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -504,8 +504,10 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 */
 		might_sleep();
 #ifdef CONFIG_DEBUG_VM
-		if (!user_mode(regs) && !search_exception_tables(regs->pc))
+		if (!user_mode(regs) && !search_exception_tables(regs->pc)) {
+			up_read(&mm->mmap_sem);
 			goto no_context;
+		}
 #endif
 	}
 

commit 01de1776f62e6437ccd207181ec503e8a56e6128
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Sun May 5 09:45:12 2019 +0530

    arm64/mm: Identify user instruction aborts
    
    We don't currently set the FAULT_FLAG_INSTRUCTION mm flag for EL0
    instruction aborts. This has no functional impact, as we don't override
    arch_vma_access_permitted(), and the default implementation always returns
    true. However, it would be helpful to provide the flag so that it can be
    consumed by tracepoints such as dax_pmd_fault.
    
    This patch sets the FAULT_FLAG_INSTRUCTION flag for EL0 instruction aborts.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index a30818ed9c60..392386a693fe 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -464,6 +464,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 
 	if (is_el0_instruction_abort(esr)) {
 		vm_flags = VM_EXEC;
+		mm_flags |= FAULT_FLAG_INSTRUCTION;
 	} else if ((esr & ESR_ELx_WNR) && !(esr & ESR_ELx_CM)) {
 		vm_flags = VM_WRITE;
 		mm_flags |= FAULT_FLAG_WRITE;

commit 0a72ef89901409847036664c23ba6eee7cf08e0e
Merge: c50bbf615f2f edbcf50eb8ae
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 24 11:03:26 2019 -0700

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull more arm64 fixes from Will Deacon:
    
     - Fix incorrect LDADD instruction encoding in our disassembly macros
    
     - Disable the broken ARM64_PSEUDO_NMI support for now
    
     - Add workaround for Cortex-A76 CPU erratum #1463225
    
     - Handle Cortex-A76/Neoverse-N1 erratum #1418040 w/ existing workaround
    
     - Fix IORT build failure if IOMMU_SUPPORT=n
    
     - Fix place-relative module relocation range checking and its
       interaction with KASLR
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: insn: Add BUILD_BUG_ON() for invalid masks
      arm64: insn: Fix ldadd instruction encoding
      arm64: Kconfig: Make ARM64_PSEUDO_NMI depend on BROKEN for now
      arm64: Handle erratum 1418040 as a superset of erratum 1188873
      arm64/module: deal with ambiguity in PRELxx relocation ranges
      ACPI/IORT: Fix build error when IOMMU_SUPPORT is disabled
      arm64/kernel: kaslr: reduce module randomization range to 2 GB
      arm64: errata: Add workaround for Cortex-A76 erratum #1463225
      arm64: Remove useless message during oops

commit 969f5ea627570e91c9d54403287ee3ed657f58fe
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 29 13:03:57 2019 +0100

    arm64: errata: Add workaround for Cortex-A76 erratum #1463225
    
    Revisions of the Cortex-A76 CPU prior to r4p0 are affected by an erratum
    that can prevent interrupts from being taken when single-stepping.
    
    This patch implements a software workaround to prevent userspace from
    effectively being able to disable interrupts.
    
    Cc: <stable@vger.kernel.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0cb0e09995e1..9a84a4071561 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -810,6 +810,36 @@ void __init hook_debug_fault_code(int nr,
 	debug_fault_info[nr].name	= name;
 }
 
+#ifdef CONFIG_ARM64_ERRATUM_1463225
+DECLARE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+
+static int __exception
+cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+{
+	if (user_mode(regs))
+		return 0;
+
+	if (!__this_cpu_read(__in_cortex_a76_erratum_1463225_wa))
+		return 0;
+
+	/*
+	 * We've taken a dummy step exception from the kernel to ensure
+	 * that interrupts are re-enabled on the syscall path. Return back
+	 * to cortex_a76_erratum_1463225_svc_handler() with debug exceptions
+	 * masked so that we can safely restore the mdscr and get on with
+	 * handling the syscall.
+	 */
+	regs->pstate |= PSR_D_BIT;
+	return 1;
+}
+#else
+static int __exception
+cortex_a76_erratum_1463225_debug_handler(struct pt_regs *regs)
+{
+	return 0;
+}
+#endif /* CONFIG_ARM64_ERRATUM_1463225 */
+
 asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
 					       unsigned int esr,
 					       struct pt_regs *regs)
@@ -817,6 +847,9 @@ asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
 	const struct fault_info *inf = esr_to_debug_fault_info(esr);
 	unsigned long pc = instruction_pointer(regs);
 
+	if (cortex_a76_erratum_1463225_debug_handler(regs))
+		return;
+
 	/*
 	 * Tell lockdep we disabled irqs in entry.S. Do nothing if they were
 	 * already disabled to preserve the last enabled/disabled addresses.

commit 48caebf7e1313eb9f0a06fe59a07ac05b38a5806
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue May 14 12:25:28 2019 +0100

    arm64: Print physical address of page table base in show_pte()
    
    When dumping the page table in response to an unexpected kernel page
    fault, we print the virtual (hashed) address of the page table base, but
    display physical addresses for everything else.
    
    Make the page table dumping code in show_pte() consistent, by printing
    the page table base pointer as a physical address.
    
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0cb0e09995e1..dda234bcc020 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -171,9 +171,10 @@ static void show_pte(unsigned long addr)
 		return;
 	}
 
-	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgdp = %p\n",
+	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgdp=%016lx\n",
 		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
-		 mm == &init_mm ? VA_BITS : (int) vabits_user, mm->pgd);
+		 mm == &init_mm ? VA_BITS : (int)vabits_user,
+		 (unsigned long)virt_to_phys(mm->pgd));
 	pgdp = pgd_offset(mm, addr);
 	pgd = READ_ONCE(*pgdp);
 	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));

commit 52c6d145da15a9a9cea14678be307c127ccc6ef5
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 25 12:06:43 2019 +0000

    arm64: debug: Remove unused return value from do_debug_exception()
    
    do_debug_exception() goes out of its way to return a value that isn't
    ever used, so just make the thing void.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 4f343e603925..0cb0e09995e1 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -810,13 +810,12 @@ void __init hook_debug_fault_code(int nr,
 	debug_fault_info[nr].name	= name;
 }
 
-asmlinkage int __exception do_debug_exception(unsigned long addr_if_watchpoint,
-					      unsigned int esr,
-					      struct pt_regs *regs)
+asmlinkage void __exception do_debug_exception(unsigned long addr_if_watchpoint,
+					       unsigned int esr,
+					       struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_debug_fault_info(esr);
 	unsigned long pc = instruction_pointer(regs);
-	int rv;
 
 	/*
 	 * Tell lockdep we disabled irqs in entry.S. Do nothing if they were
@@ -828,17 +827,12 @@ asmlinkage int __exception do_debug_exception(unsigned long addr_if_watchpoint,
 	if (user_mode(regs) && !is_ttbr0_addr(pc))
 		arm64_apply_bp_hardening();
 
-	if (!inf->fn(addr_if_watchpoint, esr, regs)) {
-		rv = 1;
-	} else {
+	if (inf->fn(addr_if_watchpoint, esr, regs)) {
 		arm64_notify_die(inf->name, regs,
 				 inf->sig, inf->code, (void __user *)pc, esr);
-		rv = 0;
 	}
 
 	if (interrupts_enabled(regs))
 		trace_hardirqs_on();
-
-	return rv;
 }
 NOKPROBE_SYMBOL(do_debug_exception);

commit 7048a5973eb1d6a747b516334c052d92d8ed7ab4
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Apr 3 13:36:54 2019 +0100

    arm64: mm: Make show_pte() a static function
    
    show_pte() doesn't have any external callers, so make it static.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 1a7e92ab69eb..4f343e603925 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -148,7 +148,7 @@ static inline bool is_ttbr1_addr(unsigned long addr)
 /*
  * Dump out the page tables associated with 'addr' in the currently active mm.
  */
-void show_pte(unsigned long addr)
+static void show_pte(unsigned long addr)
 {
 	struct mm_struct *mm;
 	pgd_t *pgdp;

commit 3d8dfe75ef69f4dd4ba35c09b20a5aa58b4a5078
Merge: d60752629693 b855b58ac1b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 10:17:23 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Pseudo NMI support for arm64 using GICv3 interrupt priorities
    
     - uaccess macros clean-up (unsafe user accessors also merged but
       reverted, waiting for objtool support on arm64)
    
     - ptrace regsets for Pointer Authentication (ARMv8.3) key management
    
     - inX() ordering w.r.t. delay() on arm64 and riscv (acks in place by
       the riscv maintainers)
    
     - arm64/perf updates: PMU bindings converted to json-schema, unused
       variable and misleading comment removed
    
     - arm64/debug fixes to ensure checking of the triggering exception
       level and to avoid the propagation of the UNKNOWN FAR value into the
       si_code for debug signals
    
     - Workaround for Fujitsu A64FX erratum 010001
    
     - lib/raid6 ARM NEON optimisations
    
     - NR_CPUS now defaults to 256 on arm64
    
     - Minor clean-ups (documentation/comments, Kconfig warning, unused
       asm-offsets, clang warnings)
    
     - MAINTAINERS update for list information to the ARM64 ACPI entry
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      arm64: mmu: drop paging_init comments
      arm64: debug: Ensure debug handlers check triggering exception level
      arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
      Revert "arm64: uaccess: Implement unsafe accessors"
      arm64: avoid clang warning about self-assignment
      arm64: Kconfig.platforms: fix warning unmet direct dependencies
      lib/raid6: arm: optimize away a mask operation in NEON recovery routine
      lib/raid6: use vdupq_n_u8 to avoid endianness warnings
      arm64: io: Hook up __io_par() for inX() ordering
      riscv: io: Update __io_[p]ar() macros to take an argument
      asm-generic/io: Pass result of I/O accessor to __io_[p]ar()
      arm64: Add workaround for Fujitsu A64FX erratum 010001
      arm64: Rename get_thread_info()
      arm64: Remove documentation about TIF_USEDFPU
      arm64: irqflags: Fix clang build warnings
      arm64: Enable the support of pseudo-NMIs
      arm64: Skip irqflags tracing for NMI in IRQs disabled context
      arm64: Skip preemption when exiting an NMI
      arm64: Handle serror in NMI context
      irqchip/gic-v3: Allow interrupts to be set as pseudo-NMI
      ...

commit b9a4b9d084d978f80eb9210727c81804588b42ff
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Mar 1 13:28:00 2019 +0000

    arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
    
    FAR_EL1 is UNKNOWN for all debug exceptions other than those caused by
    taking a hardware watchpoint. Unfortunately, if a debug handler returns
    a non-zero value, then we will propagate the UNKNOWN FAR value to
    userspace via the si_addr field of the SIGTRAP siginfo_t.
    
    Instead, let's set si_addr to take on the PC of the faulting instruction,
    which we have available in the current pt_regs.
    
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index efb7b2cbead5..ef46925096f0 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -824,11 +824,12 @@ void __init hook_debug_fault_code(int nr,
 	debug_fault_info[nr].name	= name;
 }
 
-asmlinkage int __exception do_debug_exception(unsigned long addr,
+asmlinkage int __exception do_debug_exception(unsigned long addr_if_watchpoint,
 					      unsigned int esr,
 					      struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_debug_fault_info(esr);
+	unsigned long pc = instruction_pointer(regs);
 	int rv;
 
 	/*
@@ -838,14 +839,14 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (interrupts_enabled(regs))
 		trace_hardirqs_off();
 
-	if (user_mode(regs) && !is_ttbr0_addr(instruction_pointer(regs)))
+	if (user_mode(regs) && !is_ttbr0_addr(pc))
 		arm64_apply_bp_hardening();
 
-	if (!inf->fn(addr, esr, regs)) {
+	if (!inf->fn(addr_if_watchpoint, esr, regs)) {
 		rv = 1;
 	} else {
 		arm64_notify_die(inf->name, regs,
-				 inf->sig, inf->code, (void __user *)addr, esr);
+				 inf->sig, inf->code, (void __user *)pc, esr);
 		rv = 0;
 	}
 

commit d44f1b8dd7e66d80cc4205809e5ace866bd851da
Author: James Morse <james.morse@arm.com>
Date:   Tue Jan 29 18:48:50 2019 +0000

    arm64: KVM/mm: Move SEA handling behind a single 'claim' interface
    
    To split up APEIs in_nmi() path, the caller needs to always be
    in_nmi(). Add a helper to do the work and claim the notification.
    
    When KVM or the arch code takes an exception that might be a RAS
    notification, it asks the APEI firmware-first code whether it wants
    to claim the exception. A future kernel-first mechanism may be queried
    afterwards, and claim the notification, otherwise we fall through
    to the existing default behaviour.
    
    The NOTIFY_SEA code was merged before considering multiple, possibly
    interacting, NMI-like notifications and the need to consider kernel
    first in the future. Make the 'claiming' behaviour explicit.
    
    Restructuring the APEI code to allow multiple NMI-like notifications
    means any notification that might interrupt interrupts-masked
    code must always be wrapped in nmi_enter()/nmi_exit(). This will
    allow APEI to use in_nmi() to use the right fixmap entries.
    
    Mask SError over this window to prevent an asynchronous RAS error
    arriving and tripping 'nmi_enter()'s BUG_ON(in_nmi()).
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Tyler Baicar <tbaicar@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c76dc981e3fc..e1c84c2e1cab 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -18,6 +18,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/acpi.h>
 #include <linux/extable.h>
 #include <linux/signal.h>
 #include <linux/mm.h>
@@ -33,6 +34,7 @@
 #include <linux/preempt.h>
 #include <linux/hugetlb.h>
 
+#include <asm/acpi.h>
 #include <asm/bug.h>
 #include <asm/cmpxchg.h>
 #include <asm/cpufeature.h>
@@ -47,8 +49,6 @@
 #include <asm/tlbflush.h>
 #include <asm/traps.h>
 
-#include <acpi/ghes.h>
-
 struct fault_info {
 	int	(*fn)(unsigned long addr, unsigned int esr,
 		      struct pt_regs *regs);
@@ -643,19 +643,10 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 	inf = esr_to_fault_info(esr);
 
 	/*
-	 * Synchronous aborts may interrupt code which had interrupts masked.
-	 * Before calling out into the wider kernel tell the interested
-	 * subsystems.
+	 * Return value ignored as we rely on signal merging.
+	 * Future patches will make this more robust.
 	 */
-	if (IS_ENABLED(CONFIG_ACPI_APEI_SEA)) {
-		if (interrupts_enabled(regs))
-			nmi_enter();
-
-		ghes_notify_sea();
-
-		if (interrupts_enabled(regs))
-			nmi_exit();
-	}
+	apei_claim_sea(regs);
 
 	if (esr & ESR_ELx_FnV)
 		siaddr = NULL;
@@ -733,11 +724,6 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 63"			},
 };
 
-int kvm_handle_guest_sea(phys_addr_t addr, unsigned int esr)
-{
-	return ghes_notify_sea();
-}
-
 asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 					 struct pt_regs *regs)
 {

commit 0db5e0223035b2c84e6186831fc27511270af812
Author: James Morse <james.morse@arm.com>
Date:   Tue Jan 29 18:48:49 2019 +0000

    KVM: arm/arm64: Add kvm_ras.h to collect kvm specific RAS plumbing
    
    To split up APEIs in_nmi() path, the caller needs to always be
    in_nmi(). KVM shouldn't have to know about this, pull the RAS plumbing
    out into a header file.
    
    Currently guest synchronous external aborts are claimed as RAS
    notifications by handle_guest_sea(), which is hidden in the arch codes
    mm/fault.c. 32bit gets a dummy declaration in system_misc.h.
    
    There is going to be more of this in the future if/when the kernel
    supports the SError-based firmware-first notification mechanism and/or
    kernel-first notifications for both synchronous external abort and
    SError. Each of these will come with some Kconfig symbols and a
    handful of header files.
    
    Create a header file for all this.
    
    This patch gives handle_guest_sea() a 'kvm_' prefix, and moves the
    declarations to kvm_ras.h as preparation for a future patch that moves
    the ACPI-specific RAS code out of mm/fault.c.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Punit Agrawal <punit.agrawal@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Tyler Baicar <tbaicar@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index efb7b2cbead5..c76dc981e3fc 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -733,7 +733,7 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 63"			},
 };
 
-int handle_guest_sea(phys_addr_t addr, unsigned int esr)
+int kvm_handle_guest_sea(phys_addr_t addr, unsigned int esr)
 {
 	return ghes_notify_sea();
 }

commit 356607f21e603523d4b0a4f918722845214fc6a8
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:27 2018 -0800

    kasan, arm64: fix up fault handling logic
    
    Right now arm64 fault handling code removes pointer tags from addresses
    covered by TTBR0 in faults taken from both EL0 and EL1, but doesn't do
    that for pointers covered by TTBR1.
    
    This patch adds two helper functions is_ttbr0_addr() and is_ttbr1_addr(),
    where the latter one accounts for the fact that TTBR1 pointers might be
    tagged when tag-based KASAN is in use, and uses these helper functions to
    perform pointer checks in arch/arm64/mm/fault.c.
    
    Link: http://lkml.kernel.org/r/3f349b0e9e48b5df3298a6b4ae0634332274494a.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 5fe6d2e40e9b..efb7b2cbead5 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -40,6 +40,7 @@
 #include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
+#include <asm/kasan.h>
 #include <asm/sysreg.h>
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
@@ -132,6 +133,18 @@ static void mem_abort_decode(unsigned int esr)
 		data_abort_decode(esr);
 }
 
+static inline bool is_ttbr0_addr(unsigned long addr)
+{
+	/* entry assembly clears tags for TTBR0 addrs */
+	return addr < TASK_SIZE;
+}
+
+static inline bool is_ttbr1_addr(unsigned long addr)
+{
+	/* TTBR1 addresses may have a tag if KASAN_SW_TAGS is in use */
+	return arch_kasan_reset_tag(addr) >= VA_START;
+}
+
 /*
  * Dump out the page tables associated with 'addr' in the currently active mm.
  */
@@ -141,7 +154,7 @@ void show_pte(unsigned long addr)
 	pgd_t *pgdp;
 	pgd_t pgd;
 
-	if (addr < TASK_SIZE) {
+	if (is_ttbr0_addr(addr)) {
 		/* TTBR0 */
 		mm = current->active_mm;
 		if (mm == &init_mm) {
@@ -149,7 +162,7 @@ void show_pte(unsigned long addr)
 				 addr);
 			return;
 		}
-	} else if (addr >= VA_START) {
+	} else if (is_ttbr1_addr(addr)) {
 		/* TTBR1 */
 		mm = &init_mm;
 	} else {
@@ -254,7 +267,7 @@ static inline bool is_el1_permission_fault(unsigned long addr, unsigned int esr,
 	if (fsc_type == ESR_ELx_FSC_PERM)
 		return true;
 
-	if (addr < TASK_SIZE && system_uses_ttbr0_pan())
+	if (is_ttbr0_addr(addr) && system_uses_ttbr0_pan())
 		return fsc_type == ESR_ELx_FSC_FAULT &&
 			(regs->pstate & PSR_PAN_BIT);
 
@@ -319,7 +332,7 @@ static void set_thread_esr(unsigned long address, unsigned int esr)
 	 * type", so we ignore this wrinkle and just return the translation
 	 * fault.)
 	 */
-	if (current->thread.fault_address >= TASK_SIZE) {
+	if (!is_ttbr0_addr(current->thread.fault_address)) {
 		switch (ESR_ELx_EC(esr)) {
 		case ESR_ELx_EC_DABT_LOW:
 			/*
@@ -455,7 +468,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (addr < TASK_SIZE && is_el1_permission_fault(addr, esr, regs)) {
+	if (is_ttbr0_addr(addr) && is_el1_permission_fault(addr, esr, regs)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die_kernel_fault("access to user memory with fs=KERNEL_DS",
@@ -603,7 +616,7 @@ static int __kprobes do_translation_fault(unsigned long addr,
 					  unsigned int esr,
 					  struct pt_regs *regs)
 {
-	if (addr < TASK_SIZE)
+	if (is_ttbr0_addr(addr))
 		return do_page_fault(addr, esr, regs);
 
 	do_bad_area(addr, esr, regs);
@@ -758,7 +771,7 @@ asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
 	 * re-enabled IRQs. If the address is a kernel address, apply
 	 * BP hardening prior to enabling IRQs and pre-emption.
 	 */
-	if (addr > TASK_SIZE)
+	if (!is_ttbr0_addr(addr))
 		arm64_apply_bp_hardening();
 
 	local_daif_restore(DAIF_PROCCTX);
@@ -771,7 +784,7 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   struct pt_regs *regs)
 {
 	if (user_mode(regs)) {
-		if (instruction_pointer(regs) > TASK_SIZE)
+		if (!is_ttbr0_addr(instruction_pointer(regs)))
 			arm64_apply_bp_hardening();
 		local_daif_restore(DAIF_PROCCTX);
 	}
@@ -825,7 +838,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (interrupts_enabled(regs))
 		trace_hardirqs_off();
 
-	if (user_mode(regs) && instruction_pointer(regs) > TASK_SIZE)
+	if (user_mode(regs) && !is_ttbr0_addr(instruction_pointer(regs)))
 		arm64_apply_bp_hardening();
 
 	if (!inf->fn(addr, esr, regs)) {

commit 67e7fdfcc6824a4f768d76d89377b33baad58fad
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:41 2018 +0000

    arm64: mm: introduce 52-bit userspace support
    
    On arm64 there is optional support for a 52-bit virtual address space.
    To exploit this one has to be running with a 64KB page size and be
    running on hardware that supports this.
    
    For an arm64 kernel supporting a 48 bit VA with a 64KB page size,
    some changes are needed to support a 52-bit userspace:
     * TCR_EL1.T0SZ needs to be 12 instead of 16,
     * TASK_SIZE needs to reflect the new size.
    
    This patch implements the above when the support for 52-bit VAs is
    detected at early boot time.
    
    On arm64 userspace addresses translation is controlled by TTBR0_EL1. As
    well as userspace, TTBR0_EL1 controls:
     * The identity mapping,
     * EFI runtime code.
    
    It is possible to run a kernel with an identity mapping that has a
    larger VA size than userspace (and for this case __cpu_set_tcr_t0sz()
    would set TCR_EL1.T0SZ as appropriate). However, when the conditions for
    52-bit userspace are met; it is possible to keep TCR_EL1.T0SZ fixed at
    12. Thus in this patch, the TCR_EL1.T0SZ size changing logic is
    disabled.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 7d9571f4ae3d..5fe6d2e40e9b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -160,7 +160,7 @@ void show_pte(unsigned long addr)
 
 	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgdp = %p\n",
 		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
-		 VA_BITS, mm->pgd);
+		 mm == &init_mm ? VA_BITS : (int) vabits_user, mm->pgd);
 	pgdp = pgd_offset(mm, addr);
 	pgd = READ_ONCE(*pgdp);
 	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));

commit ba9f6f8954afa5224e3ed60332f7b92242b7ed0f
Merge: a978a5b8d83f a36700589b85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:22:39 2018 +0100

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "I have been slowly sorting out siginfo and this is the culmination of
      that work.
    
      The primary result is in several ways the signal infrastructure has
      been made less error prone. The code has been updated so that manually
      specifying SEND_SIG_FORCED is never necessary. The conversion to the
      new siginfo sending functions is now complete, which makes it
      difficult to send a signal without filling in the proper siginfo
      fields.
    
      At the tail end of the patchset comes the optimization of decreasing
      the size of struct siginfo in the kernel from 128 bytes to about 48
      bytes on 64bit. The fundamental observation that enables this is by
      definition none of the known ways to use struct siginfo uses the extra
      bytes.
    
      This comes at the cost of a small user space observable difference.
      For the rare case of siginfo being injected into the kernel only what
      can be copied into kernel_siginfo is delivered to the destination, the
      rest of the bytes are set to 0. For cases where the signal and the
      si_code are known this is safe, because we know those bytes are not
      used. For cases where the signal and si_code combination is unknown
      the bits that won't fit into struct kernel_siginfo are tested to
      verify they are zero, and the send fails if they are not.
    
      I made an extensive search through userspace code and I could not find
      anything that would break because of the above change. If it turns out
      I did break something it will take just the revert of a single change
      to restore kernel_siginfo to the same size as userspace siginfo.
    
      Testing did reveal dependencies on preferring the signo passed to
      sigqueueinfo over si->signo, so bit the bullet and added the
      complexity necessary to handle that case.
    
      Testing also revealed bad things can happen if a negative signal
      number is passed into the system calls. Something no sane application
      will do but something a malicious program or a fuzzer might do. So I
      have fixed the code that performs the bounds checks to ensure negative
      signal numbers are handled"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (80 commits)
      signal: Guard against negative signal numbers in copy_siginfo_from_user32
      signal: Guard against negative signal numbers in copy_siginfo_from_user
      signal: In sigqueueinfo prefer sig not si_signo
      signal: Use a smaller struct siginfo in the kernel
      signal: Distinguish between kernel_siginfo and siginfo
      signal: Introduce copy_siginfo_from_user and use it's return value
      signal: Remove the need for __ARCH_SI_PREABLE_SIZE and SI_PAD_SIZE
      signal: Fail sigqueueinfo if si_signo != sig
      signal/sparc: Move EMT_TAGOVF into the generic siginfo.h
      signal/unicore32: Use force_sig_fault where appropriate
      signal/unicore32: Generate siginfo in ucs32_notify_die
      signal/unicore32: Use send_sig_fault where appropriate
      signal/arc: Use force_sig_fault where appropriate
      signal/arc: Push siginfo generation into unhandled_exception
      signal/ia64: Use force_sig_fault where appropriate
      signal/ia64: Use the force_sig(SIGSEGV,...) in ia64_rt_sigreturn
      signal/ia64: Use the generic force_sigsegv in setup_frame
      signal/arm/kvm: Use send_sig_mceerr
      signal/arm: Use send_sig_fault where appropriate
      signal/arm: Use force_sig_fault where appropriate
      ...

commit 9a0c032825e073e002ee80cf2577431d7296a7f7
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Aug 28 16:51:15 2018 +0100

    arm64: Use daifflag_restore after bp_hardening
    
    For EL0 entries requiring bp_hardening, daif status is kept at
    DAIF_PROCCTX_NOIRQ until after hardening has been done. Then interrupts
    are enabled through local_irq_enable().
    
    Before using local_irq_* functions, daifflags should be properly restored
    to a state where IRQs are enabled.
    
    Enable IRQs by restoring DAIF_PROCCTX state after bp hardening.
    
    Acked-by: James Morse <james.morse@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 1aa487a37a0a..d0e638ef3af6 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -37,6 +37,7 @@
 #include <asm/cmpxchg.h>
 #include <asm/cpufeature.h>
 #include <asm/exception.h>
+#include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
 #include <asm/sysreg.h>
@@ -776,7 +777,7 @@ asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
 	if (addr > TASK_SIZE)
 		arm64_apply_bp_hardening();
 
-	local_irq_enable();
+	local_daif_restore(DAIF_PROCCTX);
 	do_mem_abort(addr, esr, regs);
 }
 
@@ -790,7 +791,7 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 	if (user_mode(regs)) {
 		if (instruction_pointer(regs) > TASK_SIZE)
 			arm64_apply_bp_hardening();
-		local_irq_enable();
+		local_daif_restore(DAIF_PROCCTX);
 	}
 
 	clear_siginfo(&info);

commit 359048f91db4be5e8be9c2b95c788c79a6756797
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Sat Sep 22 21:09:54 2018 +0530

    arm64/mm: Define esr_to_debug_fault_info()
    
    fault_info[] and debug_fault_info[] are static arrays defining memory abort
    exception handling functions looking into ESR fault status code encodings.
    As esr_to_fault_info() is already available providing fault_info[] array
    lookup, it really makes sense to have a corresponding debug_fault_info[]
    array lookup function as well. This just adds an equivalent helper function
    esr_to_debug_fault_info().
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0e0ea5fe6ab3..1aa487a37a0a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -56,12 +56,18 @@ struct fault_info {
 };
 
 static const struct fault_info fault_info[];
+static struct fault_info debug_fault_info[];
 
 static inline const struct fault_info *esr_to_fault_info(unsigned int esr)
 {
 	return fault_info + (esr & ESR_ELx_FSC);
 }
 
+static inline const struct fault_info *esr_to_debug_fault_info(unsigned int esr)
+{
+	return debug_fault_info + DBG_ESR_EVT(esr);
+}
+
 #ifdef CONFIG_KPROBES
 static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
 {
@@ -830,7 +836,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 					      unsigned int esr,
 					      struct pt_regs *regs)
 {
-	const struct fault_info *inf = debug_fault_info + DBG_ESR_EVT(esr);
+	const struct fault_info *inf = esr_to_debug_fault_info(esr);
 	int rv;
 
 	/*

commit dbfe3828a6f36be904427ed7f196744cb02d3072
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Sat Sep 22 21:09:53 2018 +0530

    arm64/mm: Reorganize arguments for is_el1_permission_fault()
    
    Most memory abort exception handling related functions have the arguments
    in the order (addr, esr, regs) except is_el1_permission_fault(). This
    changes the argument order in this function as (addr, esr, regs) like
    others.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6d9acd910104..0e0ea5fe6ab3 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -235,9 +235,8 @@ static bool is_el1_instruction_abort(unsigned int esr)
 	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_CUR;
 }
 
-static inline bool is_el1_permission_fault(unsigned int esr,
-					   struct pt_regs *regs,
-					   unsigned long addr)
+static inline bool is_el1_permission_fault(unsigned long addr, unsigned int esr,
+					   struct pt_regs *regs)
 {
 	unsigned int ec       = ESR_ELx_EC(esr);
 	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
@@ -283,7 +282,7 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	if (!is_el1_instruction_abort(esr) && fixup_exception(regs))
 		return;
 
-	if (is_el1_permission_fault(esr, regs, addr)) {
+	if (is_el1_permission_fault(addr, esr, regs)) {
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
 		else
@@ -454,7 +453,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (addr < TASK_SIZE && is_el1_permission_fault(esr, regs, addr)) {
+	if (addr < TASK_SIZE && is_el1_permission_fault(addr, esr, regs)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die_kernel_fault("access to user memory with fs=KERNEL_DS",

commit 00bbd5d9016d49fa8dddef020a06b94fedca9148
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Sat Sep 22 21:09:52 2018 +0530

    arm64/mm: Use ESR_ELx_FSC macro while decoding fault exception
    
    Just replace hard code value of 63 (0x111111) with an existing macro
    ESR_ELx_FSC when parsing for the status code during fault exception.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6342f1793c70..6d9acd910104 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -59,7 +59,7 @@ static const struct fault_info fault_info[];
 
 static inline const struct fault_info *esr_to_fault_info(unsigned int esr)
 {
-	return fault_info + (esr & 63);
+	return fault_info + (esr & ESR_ELx_FSC);
 }
 
 #ifdef CONFIG_KPROBES

commit b4d5557caa07a01796ca8a2d756eeaa5308f6876
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:37:15 2018 +0200

    signal/arm64: Add and use arm64_force_sig_mceerr as appropriate
    
    Add arm64_force_sig_mceerr for consistency with arm64_force_sig_fault,
    and use it in the one location that can take advantage of it.
    
    This removes the fiddly filling out of siginfo before sending a signal
    reporting an memory error to userspace.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 66c295019a9a..f0ccb209d181 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -422,7 +422,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	const struct fault_info *inf;
 	struct task_struct *tsk;
 	struct mm_struct *mm;
-	struct siginfo si;
 	vm_fault_t fault, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
@@ -574,12 +573,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		if (fault & VM_FAULT_HWPOISON_LARGE)
 			lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));
 
-		clear_siginfo(&si);
-		si.si_signo	= SIGBUS;
-		si.si_code	= BUS_MCEERR_AR;
-		si.si_addr = (void __user *)addr;
-		si.si_addr_lsb	= lsb;
-		arm64_force_sig_info(&si, inf->name);
+		arm64_force_sig_mceerr(BUS_MCEERR_AR, (void __user *)addr, lsb,
+				       inf->name);
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory

commit feca355b3d8eba3a2cbca63c97a59a14681983f7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:26:57 2018 +0200

    signal/arm64: Add and use arm64_force_sig_fault where appropriate
    
    Wrap force_sig_fault with a helper that calls arm64_show_signal
    and call arm64_force_sig_fault where appropraite.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 959c4a565c8e..66c295019a9a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -362,15 +362,10 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 	 */
 	if (user_mode(regs)) {
 		const struct fault_info *inf = esr_to_fault_info(esr);
-		struct siginfo si;
-
-		clear_siginfo(&si);
-		si.si_signo	= inf->sig;
-		si.si_code	= inf->code;
-		si.si_addr	= (void __user *)addr;
 
 		set_thread_esr(addr, esr);
-		arm64_force_sig_info(&si, inf->name);
+		arm64_force_sig_fault(inf->sig, inf->code, (void __user *)addr,
+				      inf->name);
 	} else {
 		__do_kernel_fault(addr, esr, regs);
 	}
@@ -570,11 +565,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 * We had some memory, but were unable to successfully fix up
 		 * this page fault.
 		 */
-		clear_siginfo(&si);
-		si.si_signo	= SIGBUS;
-		si.si_code	= BUS_ADRERR;
-		si.si_addr = (void __user *)addr;
-		arm64_force_sig_info(&si, inf->name);
+		arm64_force_sig_fault(SIGBUS, BUS_ADRERR, (void __user *)addr,
+				      inf->name);
 	} else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
 		unsigned int lsb;
 
@@ -593,12 +585,10 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 * Something tried to access memory that isn't in our memory
 		 * map.
 		 */
-		clear_siginfo(&si);
-		si.si_signo	= SIGSEGV;
-		si.si_code	= fault == VM_FAULT_BADACCESS ?
-				  SEGV_ACCERR : SEGV_MAPERR;
-		si.si_addr = (void __user *)addr;
-		arm64_force_sig_info(&si, inf->name);
+		arm64_force_sig_fault(SIGSEGV,
+				      fault == VM_FAULT_BADACCESS ? SEGV_ACCERR : SEGV_MAPERR,
+				      (void __user *)addr,
+				      inf->name);
 	}
 
 	return 0;

commit 559d8d91a89cc2a0190781a5b5ce3faeaef7920f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:18:42 2018 +0200

    signal/arm64: Only call set_thread_esr once in do_page_fault
    
    This code is truly common between the signal sending cases so share it.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index ab85533e2255..959c4a565c8e 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -564,6 +564,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	inf = esr_to_fault_info(esr);
+	set_thread_esr(addr, esr);
 	if (fault & VM_FAULT_SIGBUS) {
 		/*
 		 * We had some memory, but were unable to successfully fix up
@@ -573,7 +574,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_signo	= SIGBUS;
 		si.si_code	= BUS_ADRERR;
 		si.si_addr = (void __user *)addr;
-		set_thread_esr(addr, esr);
 		arm64_force_sig_info(&si, inf->name);
 	} else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
 		unsigned int lsb;
@@ -587,7 +587,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_code	= BUS_MCEERR_AR;
 		si.si_addr = (void __user *)addr;
 		si.si_addr_lsb	= lsb;
-		set_thread_esr(addr, esr);
 		arm64_force_sig_info(&si, inf->name);
 	} else {
 		/*
@@ -599,7 +598,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_code	= fault == VM_FAULT_BADACCESS ?
 				  SEGV_ACCERR : SEGV_MAPERR;
 		si.si_addr = (void __user *)addr;
-		set_thread_esr(addr, esr);
 		arm64_force_sig_info(&si, inf->name);
 	}
 

commit 2d2837fab5fadaf6942e834c2cbe8c9e594917c1
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:16:42 2018 +0200

    signal/arm64: Only perform one esr_to_fault_info call in do_page_fault
    
    As this work is truly common between all of the signal sending cases
    there is no need to repeat it between the different cases.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 7df3d8b561c2..ab85533e2255 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -424,6 +424,7 @@ static bool is_el0_instruction_abort(unsigned int esr)
 static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				   struct pt_regs *regs)
 {
+	const struct fault_info *inf;
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	struct siginfo si;
@@ -562,6 +563,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		return 0;
 	}
 
+	inf = esr_to_fault_info(esr);
 	if (fault & VM_FAULT_SIGBUS) {
 		/*
 		 * We had some memory, but were unable to successfully fix up
@@ -572,7 +574,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_code	= BUS_ADRERR;
 		si.si_addr = (void __user *)addr;
 		set_thread_esr(addr, esr);
-		arm64_force_sig_info(&si, esr_to_fault_info(esr)->name);
+		arm64_force_sig_info(&si, inf->name);
 	} else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
 		unsigned int lsb;
 
@@ -586,7 +588,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_addr = (void __user *)addr;
 		si.si_addr_lsb	= lsb;
 		set_thread_esr(addr, esr);
-		arm64_force_sig_info(&si, esr_to_fault_info(esr)->name);
+		arm64_force_sig_info(&si, inf->name);
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory
@@ -598,7 +600,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				  SEGV_ACCERR : SEGV_MAPERR;
 		si.si_addr = (void __user *)addr;
 		set_thread_esr(addr, esr);
-		arm64_force_sig_info(&si, esr_to_fault_info(esr)->name);
+		arm64_force_sig_info(&si, inf->name);
 	}
 
 	return 0;

commit effb093ad28b69230aad222650ec954a21c06ede
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:05:41 2018 +0200

    signal/arm64: Expand __do_user_fault and remove it
    
    Not all of the signals passed to __do_user_fault can be handled
    the same way so expand the now tiny __do_user_fault in it's callers
    and remove it.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 14d6ff895139..7df3d8b561c2 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -354,12 +354,6 @@ static void set_thread_esr(unsigned long address, unsigned int esr)
 	current->thread.fault_code = esr;
 }
 
-static void __do_user_fault(struct siginfo *info, unsigned int esr)
-{
-	set_thread_esr((unsigned long)info->si_addr, esr);
-	arm64_force_sig_info(info, esr_to_fault_info(esr)->name);
-}
-
 static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	/*
@@ -375,7 +369,8 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 		si.si_code	= inf->code;
 		si.si_addr	= (void __user *)addr;
 
-		__do_user_fault(&si, esr);
+		set_thread_esr(addr, esr);
+		arm64_force_sig_info(&si, inf->name);
 	} else {
 		__do_kernel_fault(addr, esr, regs);
 	}
@@ -576,7 +571,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_signo	= SIGBUS;
 		si.si_code	= BUS_ADRERR;
 		si.si_addr = (void __user *)addr;
-		__do_user_fault(&si, esr);
+		set_thread_esr(addr, esr);
+		arm64_force_sig_info(&si, esr_to_fault_info(esr)->name);
 	} else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
 		unsigned int lsb;
 
@@ -589,7 +585,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_code	= BUS_MCEERR_AR;
 		si.si_addr = (void __user *)addr;
 		si.si_addr_lsb	= lsb;
-		__do_user_fault(&si, esr);
+		set_thread_esr(addr, esr);
+		arm64_force_sig_info(&si, esr_to_fault_info(esr)->name);
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory
@@ -600,7 +597,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		si.si_code	= fault == VM_FAULT_BADACCESS ?
 				  SEGV_ACCERR : SEGV_MAPERR;
 		si.si_addr = (void __user *)addr;
-		__do_user_fault(&si, esr);
+		set_thread_esr(addr, esr);
+		arm64_force_sig_info(&si, esr_to_fault_info(esr)->name);
 	}
 
 	return 0;

commit aefab2b4c01ee67f2b60e400e46bad63d29c2603
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 09:54:33 2018 +0200

    signal/arm64: For clarity separate the 3 signal sending cases in do_page_fault
    
    It gets easy to confuse what is going on when some code is shared and some not
    so stop sharing the trivial bits of signal generation to make future updates
    easier to understand.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0ddc8c6ba53b..14d6ff895139 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -567,16 +567,16 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		return 0;
 	}
 
-	clear_siginfo(&si);
-	si.si_addr = (void __user *)addr;
-
 	if (fault & VM_FAULT_SIGBUS) {
 		/*
 		 * We had some memory, but were unable to successfully fix up
 		 * this page fault.
 		 */
+		clear_siginfo(&si);
 		si.si_signo	= SIGBUS;
 		si.si_code	= BUS_ADRERR;
+		si.si_addr = (void __user *)addr;
+		__do_user_fault(&si, esr);
 	} else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
 		unsigned int lsb;
 
@@ -584,20 +584,25 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		if (fault & VM_FAULT_HWPOISON_LARGE)
 			lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));
 
+		clear_siginfo(&si);
 		si.si_signo	= SIGBUS;
 		si.si_code	= BUS_MCEERR_AR;
+		si.si_addr = (void __user *)addr;
 		si.si_addr_lsb	= lsb;
+		__do_user_fault(&si, esr);
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory
 		 * map.
 		 */
+		clear_siginfo(&si);
 		si.si_signo	= SIGSEGV;
 		si.si_code	= fault == VM_FAULT_BADACCESS ?
 				  SEGV_ACCERR : SEGV_MAPERR;
+		si.si_addr = (void __user *)addr;
+		__do_user_fault(&si, esr);
 	}
 
-	__do_user_fault(&si, esr);
 	return 0;
 
 no_context:

commit 9ea3a9743cac4fd2e296223ae6a95bf11d7365d0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 09:46:39 2018 +0200

    signal/arm64: Consolidate the two hwpoison cases in do_page_fault
    
    These two cases are practically the same and use siginfo differently
    from the other signals sent from do_page_fault.  So consolidate them
    to make future changes easier.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 654a861c4bd0..0ddc8c6ba53b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -577,16 +577,16 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 */
 		si.si_signo	= SIGBUS;
 		si.si_code	= BUS_ADRERR;
-	} else if (fault & VM_FAULT_HWPOISON_LARGE) {
-		unsigned int hindex = VM_FAULT_GET_HINDEX(fault);
+	} else if (fault & (VM_FAULT_HWPOISON_LARGE | VM_FAULT_HWPOISON)) {
+		unsigned int lsb;
+
+		lsb = PAGE_SHIFT;
+		if (fault & VM_FAULT_HWPOISON_LARGE)
+			lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));
 
 		si.si_signo	= SIGBUS;
 		si.si_code	= BUS_MCEERR_AR;
-		si.si_addr_lsb	= hstate_index_to_shift(hindex);
-	} else if (fault & VM_FAULT_HWPOISON) {
-		si.si_signo	= SIGBUS;
-		si.si_code	= BUS_MCEERR_AR;
-		si.si_addr_lsb	= PAGE_SHIFT;
+		si.si_addr_lsb	= lsb;
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory

commit f29ad209e428f5bc596de39dd9b8d73db5739920
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 09:37:55 2018 +0200

    signal/arm64: Factor set_thread_esr out of __do_user_fault
    
    This pepares for sending signals with something other than
    arm64_force_sig_info.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index f42aff0e90ad..654a861c4bd0 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -297,9 +297,9 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	die_kernel_fault(msg, addr, esr, regs);
 }
 
-static void __do_user_fault(struct siginfo *info, unsigned int esr)
+static void set_thread_esr(unsigned long address, unsigned int esr)
 {
-	current->thread.fault_address = (unsigned long)info->si_addr;
+	current->thread.fault_address = address;
 
 	/*
 	 * If the faulting address is in the kernel, we must sanitize the ESR.
@@ -352,6 +352,11 @@ static void __do_user_fault(struct siginfo *info, unsigned int esr)
 	}
 
 	current->thread.fault_code = esr;
+}
+
+static void __do_user_fault(struct siginfo *info, unsigned int esr)
+{
+	set_thread_esr((unsigned long)info->si_addr, esr);
 	arm64_force_sig_info(info, esr_to_fault_info(esr)->name);
 }
 

commit 24b8f79dd8e036da618d158b4c0295208d478c5c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 00:38:41 2018 +0200

    signal/arm64: Remove unneeded tsk parameter from arm64_force_sig_info
    
    Every caller passes in current for tsk so there is no need to pass
    tsk.  Instead make tsk a local variable initialized to current.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 86fe70d8722f..f42aff0e90ad 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -352,7 +352,7 @@ static void __do_user_fault(struct siginfo *info, unsigned int esr)
 	}
 
 	current->thread.fault_code = esr;
-	arm64_force_sig_info(info, esr_to_fault_info(esr)->name, current);
+	arm64_force_sig_info(info, esr_to_fault_info(esr)->name);
 }
 
 static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)

commit 6fa998e83ef9bcc479b0fa088de262a73e139bf8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Sep 21 17:24:40 2018 +0200

    signal/arm64: Push siginfo generation into arm64_notify_die
    
    Instead of generating a struct siginfo before calling arm64_notify_die
    pass the signal number, tne sicode and the fault address into
    arm64_notify_die and have it call force_sig_fault instead of
    force_sig_info to let the generic code generate the struct siginfo.
    
    This keeps code passing just the needed information into
    siginfo generating code, making it easier to see what
    is happening and harder to get wrong.  Further by letting
    the generic code handle the generation of struct siginfo
    it reduces the number of sites generating struct siginfo
    making it possible to review them and verify that all
    of the fiddly details for a structure passed to userspace
    are handled properly.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 50b30ff30de4..86fe70d8722f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -625,8 +625,8 @@ static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 
 static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
-	struct siginfo info;
 	const struct fault_info *inf;
+	void __user *siaddr;
 
 	inf = esr_to_fault_info(esr);
 
@@ -645,15 +645,11 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 			nmi_exit();
 	}
 
-	clear_siginfo(&info);
-	info.si_signo = inf->sig;
-	info.si_errno = 0;
-	info.si_code  = inf->code;
 	if (esr & ESR_ELx_FnV)
-		info.si_addr = NULL;
+		siaddr = NULL;
 	else
-		info.si_addr  = (void __user *)addr;
-	arm64_notify_die(inf->name, regs, &info, esr);
+		siaddr  = (void __user *)addr;
+	arm64_notify_die(inf->name, regs, inf->sig, inf->code, siaddr, esr);
 
 	return 0;
 }
@@ -734,7 +730,6 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 					 struct pt_regs *regs)
 {
 	const struct fault_info *inf = esr_to_fault_info(esr);
-	struct siginfo info;
 
 	if (!inf->fn(addr, esr, regs))
 		return;
@@ -745,12 +740,8 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 		show_pte(addr);
 	}
 
-	clear_siginfo(&info);
-	info.si_signo = inf->sig;
-	info.si_errno = 0;
-	info.si_code  = inf->code;
-	info.si_addr  = (void __user *)addr;
-	arm64_notify_die(inf->name, regs, &info, esr);
+	arm64_notify_die(inf->name, regs,
+			 inf->sig, inf->code, (void __user *)addr, esr);
 }
 
 asmlinkage void __exception do_el0_irq_bp_hardening(void)
@@ -780,20 +771,14 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   unsigned int esr,
 					   struct pt_regs *regs)
 {
-	struct siginfo info;
-
 	if (user_mode(regs)) {
 		if (instruction_pointer(regs) > TASK_SIZE)
 			arm64_apply_bp_hardening();
 		local_irq_enable();
 	}
 
-	clear_siginfo(&info);
-	info.si_signo = SIGBUS;
-	info.si_errno = 0;
-	info.si_code  = BUS_ADRALN;
-	info.si_addr  = (void __user *)addr;
-	arm64_notify_die("SP/PC alignment exception", regs, &info, esr);
+	arm64_notify_die("SP/PC alignment exception", regs,
+			 SIGBUS, BUS_ADRALN, (void __user *)addr, esr);
 }
 
 int __init early_brk64(unsigned long addr, unsigned int esr,
@@ -847,14 +832,8 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (!inf->fn(addr, esr, regs)) {
 		rv = 1;
 	} else {
-		struct siginfo info;
-
-		clear_siginfo(&info);
-		info.si_signo = inf->sig;
-		info.si_errno = 0;
-		info.si_code  = inf->code;
-		info.si_addr  = (void __user *)addr;
-		arm64_notify_die(inf->name, regs, &info, esr);
+		arm64_notify_die(inf->name, regs,
+				 inf->sig, inf->code, (void __user *)addr, esr);
 		rv = 0;
 	}
 

commit b8925ee2e12d1cb9a11d6f28b5814f2bfa59dce1
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:53:41 2018 +0100

    arm64: cpu: Move errata and feature enable callbacks closer to callers
    
    The cpu errata and feature enable callbacks are only called via their
    respective arm64_cpu_capabilities structure and therefore shouldn't
    exist in the global namespace.
    
    Move the PAN, RAS and cache maintenance emulation enable callbacks into
    the same files as their corresponding arm64_cpu_capabilities structures,
    making them static in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 50b30ff30de4..6342f1793c70 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -864,17 +864,3 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	return rv;
 }
 NOKPROBE_SYMBOL(do_debug_exception);
-
-#ifdef CONFIG_ARM64_PAN
-void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused)
-{
-	/*
-	 * We modify PSTATE. This won't work from irq context as the PSTATE
-	 * is discarded once we return from the exception.
-	 */
-	WARN_ON_ONCE(in_interrupt());
-
-	sysreg_clear_set(sctlr_el1, SCTLR_EL1_SPAN, 0);
-	asm(SET_PSTATE_PAN(1));
-}
-#endif /* CONFIG_ARM64_PAN */

commit 50a7ca3c6fc86955f99fc432fc8a186b968b365b
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Fri Aug 17 15:44:47 2018 -0700

    mm: convert return type of handle_mm_fault() caller to vm_fault_t
    
    Use new return type vm_fault_t for fault handler.  For now, this is just
    documenting that the function returns a VM_FAULT value rather than an
    errno.  Once all instances are converted, vm_fault_t will become a
    distinct type.
    
    Ref-> commit 1c8f422059ae ("mm: change return type to vm_fault_t")
    
    In this patch all the caller of handle_mm_fault() are changed to return
    vm_fault_t type.
    
    Link: http://lkml.kernel.org/r/20180617084810.GA6730@jordon-HP-15-Notebook-PC
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Levin, Alexander (Sasha Levin)" <alexander.levin@verizon.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 9943690a3924..50b30ff30de4 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -379,12 +379,12 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 #define VM_FAULT_BADMAP		0x010000
 #define VM_FAULT_BADACCESS	0x020000
 
-static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
+static vm_fault_t __do_page_fault(struct mm_struct *mm, unsigned long addr,
 			   unsigned int mm_flags, unsigned long vm_flags,
 			   struct task_struct *tsk)
 {
 	struct vm_area_struct *vma;
-	int fault;
+	vm_fault_t fault;
 
 	vma = find_vma(mm, addr);
 	fault = VM_FAULT_BADMAP;
@@ -427,7 +427,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	struct siginfo si;
-	int fault, major = 0;
+	vm_fault_t fault, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 

commit 1202f4fdbcb6deeffd3eb39c94b8dc0cc8202b16
Merge: d0055f351e64 3c4d9137eefe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 14 16:39:13 2018 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A bunch of good stuff in here. Worth noting is that we've pulled in
      the x86/mm branch from -tip so that we can make use of the core
      ioremap changes which allow us to put down huge mappings in the
      vmalloc area without screwing up the TLB. Much of the positive
      diffstat is because of the rseq selftest for arm64.
    
      Summary:
    
       - Wire up support for qspinlock, replacing our trusty ticket lock
         code
    
       - Add an IPI to flush_icache_range() to ensure that stale
         instructions fetched into the pipeline are discarded along with the
         I-cache lines
    
       - Support for the GCC "stackleak" plugin
    
       - Support for restartable sequences, plus an arm64 port for the
         selftest
    
       - Kexec/kdump support on systems booting with ACPI
    
       - Rewrite of our syscall entry code in C, which allows us to zero the
         GPRs on entry from userspace
    
       - Support for chained PMU counters, allowing 64-bit event counters to
         be constructed on current CPUs
    
       - Ensure scheduler topology information is kept up-to-date with CPU
         hotplug events
    
       - Re-enable support for huge vmalloc/IO mappings now that the core
         code has the correct hooks to use break-before-make sequences
    
       - Miscellaneous, non-critical fixes and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (90 commits)
      arm64: alternative: Use true and false for boolean values
      arm64: kexec: Add comment to explain use of __flush_icache_range()
      arm64: sdei: Mark sdei stack helper functions as static
      arm64, kaslr: export offset in VMCOREINFO ELF notes
      arm64: perf: Add cap_user_time aarch64
      efi/libstub: Only disable stackleak plugin for arm64
      arm64: drop unused kernel_neon_begin_partial() macro
      arm64: kexec: machine_kexec should call __flush_icache_range
      arm64: svc: Ensure hardirq tracing is updated before return
      arm64: mm: Export __sync_icache_dcache() for xen-privcmd
      drivers/perf: arm-ccn: Use devm_ioremap_resource() to map memory
      arm64: Add support for STACKLEAK gcc plugin
      arm64: Add stack information to on_accessible_stack
      drivers/perf: hisi: update the sccl_id/ccl_id when MT is supported
      arm64: fix ACPI dependencies
      rseq/selftests: Add support for arm64
      arm64: acpi: fix alignment fault in accessing ACPI
      efi/arm: map UEFI memory map even w/o runtime services enabled
      efi/arm: preserve early mapping of UEFI memory map longer for BGRT
      drivers: acpi: add dependency of EFI for arm64
      ...

commit 1035a0783523d8c730c44944a71b254f3a649e25
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Tue Aug 7 12:26:15 2018 -0400

    arm64 / ACPI: clean the additional checks before calling ghes_notify_sea()
    
    In order to remove the additional check before calling the
    ghes_notify_sea(), make stub definition when !CONFIG_ACPI_APEI_SEA.
    
    After this cleanup, we can simply call the ghes_notify_sea() to let
    APEI driver handle the SEA notification.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index b8eecc7b9531..9ffe01d7042a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -727,12 +727,7 @@ static const struct fault_info fault_info[] = {
 
 int handle_guest_sea(phys_addr_t addr, unsigned int esr)
 {
-	int ret = -ENOENT;
-
-	if (IS_ENABLED(CONFIG_ACPI_APEI_SEA))
-		ret = ghes_notify_sea();
-
-	return ret;
+	return ghes_notify_sea();
 }
 
 asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,

commit 25be597ada0b49d2748ab520a78a28c1764d69e4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:38 2018 +0100

    arm64: kill config_sctlr_el1()
    
    Now that we have sysreg_clear_set(), we can consistently use this
    instead of config_sctlr_el1().
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index b8eecc7b9531..ea591c9e5144 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -879,7 +879,7 @@ void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused)
 	 */
 	WARN_ON_ONCE(in_interrupt());
 
-	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
+	sysreg_clear_set(sctlr_el1, SCTLR_EL1_SPAN, 0);
 	asm(SET_PSTATE_PAN(1));
 }
 #endif /* CONFIG_ARM64_PAN */

commit 410feb75de245664d66bc05ab2e2412751d10acf
Merge: 2996148a9d41 0fe42512b2f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 8 11:10:58 2018 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Apart from the core arm64 and perf changes, the Spectre v4 mitigation
      touches the arm KVM code and the ACPI PPTT support touches drivers/
      (acpi and cacheinfo). I should have the maintainers' acks in place.
    
      Summary:
    
       - Spectre v4 mitigation (Speculative Store Bypass Disable) support
         for arm64 using SMC firmware call to set a hardware chicken bit
    
       - ACPI PPTT (Processor Properties Topology Table) parsing support and
         enable the feature for arm64
    
       - Report signal frame size to user via auxv (AT_MINSIGSTKSZ). The
         primary motivation is Scalable Vector Extensions which requires
         more space on the signal frame than the currently defined
         MINSIGSTKSZ
    
       - ARM perf patches: allow building arm-cci as module, demote
         dev_warn() to dev_dbg() in arm-ccn event_init(), miscellaneous
         cleanups
    
       - cmpwait() WFE optimisation to avoid some spurious wakeups
    
       - L1_CACHE_BYTES reverted back to 64 (for performance reasons that
         have to do with some network allocations) while keeping
         ARCH_DMA_MINALIGN to 128. cache_line_size() returns the actual
         hardware Cache Writeback Granule
    
       - Turn LSE atomics on by default in Kconfig
    
       - Kernel fault reporting tidying
    
       - Some #include and miscellaneous cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (53 commits)
      arm64: Fix syscall restarting around signal suppressed by tracer
      arm64: topology: Avoid checking numa mask for scheduler MC selection
      ACPI / PPTT: fix build when CONFIG_ACPI_PPTT is not enabled
      arm64: cpu_errata: include required headers
      arm64: KVM: Move VCPU_WORKAROUND_2_FLAG macros to the top of the file
      arm64: signal: Report signal frame size to userspace via auxv
      arm64/sve: Thin out initialisation sanity-checks for sve_max_vl
      arm64: KVM: Add ARCH_WORKAROUND_2 discovery through ARCH_FEATURES_FUNC_ID
      arm64: KVM: Handle guest's ARCH_WORKAROUND_2 requests
      arm64: KVM: Add ARCH_WORKAROUND_2 support for guests
      arm64: KVM: Add HYP per-cpu accessors
      arm64: ssbd: Add prctl interface for per-thread mitigation
      arm64: ssbd: Introduce thread flag to control userspace mitigation
      arm64: ssbd: Restore mitigation status on CPU resume
      arm64: ssbd: Skip apply_ssbd if not using dynamic mitigation
      arm64: ssbd: Add global mitigation state accessor
      arm64: Add 'ssbd' command-line option
      arm64: Add ARCH_WORKAROUND_2 probing
      arm64: Add per-cpu infrastructure to call ARCH_WORKAROUND_2
      arm64: Call ARCH_WORKAROUND_2 on transitions between EL0 and EL1
      ...

commit 93e95fa57441b6976b39029bd658b6bbe7ccfe28
Merge: d8aed8415b86 26da35010c6d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 15:23:48 2018 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "This set of changes close the known issues with setting si_code to an
      invalid value, and with not fully initializing struct siginfo. There
      remains work to do on nds32, arc, unicore32, powerpc, arm, arm64, ia64
      and x86 to get the code that generates siginfo into a simpler and more
      maintainable state. Most of that work involves refactoring the signal
      handling code and thus careful code review.
    
      Also not included is the work to shrink the in kernel version of
      struct siginfo. That depends on getting the number of places that
      directly manipulate struct siginfo under control, as it requires the
      introduction of struct kernel_siginfo for the in kernel things.
    
      Overall this set of changes looks like it is making good progress, and
      with a little luck I will be wrapping up the siginfo work next
      development cycle"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (46 commits)
      signal/sh: Stop gcc warning about an impossible case in do_divide_error
      signal/mips: Report FPE_FLTUNK for undiagnosed floating point exceptions
      signal/um: More carefully relay signals in relay_signal.
      signal: Extend siginfo_layout with SIL_FAULT_{MCEERR|BNDERR|PKUERR}
      signal: Remove unncessary #ifdef SEGV_PKUERR in 32bit compat code
      signal/signalfd: Add support for SIGSYS
      signal/signalfd: Remove __put_user from signalfd_copyinfo
      signal/xtensa: Use force_sig_fault where appropriate
      signal/xtensa: Consistenly use SIGBUS in do_unaligned_user
      signal/um: Use force_sig_fault where appropriate
      signal/sparc: Use force_sig_fault where appropriate
      signal/sparc: Use send_sig_fault where appropriate
      signal/sh: Use force_sig_fault where appropriate
      signal/s390: Use force_sig_fault where appropriate
      signal/riscv: Replace do_trap_siginfo with force_sig_fault
      signal/riscv: Use force_sig_fault where appropriate
      signal/parisc: Use force_sig_fault where appropriate
      signal/parisc: Use force_sig_mceerr where appropriate
      signal/openrisc: Use force_sig_fault where appropriate
      signal/nios2: Use force_sig_fault where appropriate
      ...

commit c870f14ea115bb1d9e57cad3ec6b876cf5542f9f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon May 21 14:14:51 2018 +0100

    arm64: Unify kernel fault reporting
    
    In do_page_fault(), we handle some kernel faults early, and simply
    die() with a message. For faults handled later, we dump the faulting
    address, decode the ESR, walk the page tables, and perform a number of
    steps to ensure that this data is reported.
    
    Let's unify the handling of fatal kernel faults with a new
    die_kernel_fault() helper, handling all of these details. This is
    largely the same as the existing logic in __do_kernel_fault(), except
    that addresses are consistently padded to 16 hex characters, as would be
    expected for a 64-bit address.
    
    The messages currently logged in do_page_fault are adjusted to fit into
    the die_kernel_fault() message template.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 59990491e72a..27cbe0b38960 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -255,6 +255,22 @@ static inline bool is_el1_permission_fault(unsigned int esr,
 	return false;
 }
 
+static void die_kernel_fault(const char *msg, unsigned long addr,
+			     unsigned int esr, struct pt_regs *regs)
+{
+	bust_spinlocks(1);
+
+	pr_alert("Unable to handle kernel %s at virtual address %016lx\n", msg,
+		 addr);
+
+	mem_abort_decode(esr);
+
+	show_pte(addr);
+	die("Oops", regs, esr);
+	bust_spinlocks(0);
+	do_exit(SIGKILL);
+}
+
 static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 			      struct pt_regs *regs)
 {
@@ -267,8 +283,6 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	if (!is_el1_instruction_abort(esr) && fixup_exception(regs))
 		return;
 
-	bust_spinlocks(1);
-
 	if (is_el1_permission_fault(esr, regs, addr)) {
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
@@ -280,15 +294,7 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 		msg = "paging request";
 	}
 
-	pr_alert("Unable to handle kernel %s at virtual address %08lx\n", msg,
-		 addr);
-
-	mem_abort_decode(esr);
-
-	show_pte(addr);
-	die("Oops", regs, esr);
-	bust_spinlocks(0);
-	do_exit(SIGKILL);
+	die_kernel_fault(msg, addr, esr, regs);
 }
 
 static void __do_user_fault(struct siginfo *info, unsigned int esr)
@@ -399,13 +405,16 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	if (addr < TASK_SIZE && is_el1_permission_fault(esr, regs, addr)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
-			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);
+			die_kernel_fault("access to user memory with fs=KERNEL_DS",
+					 addr, esr, regs);
 
 		if (is_el1_instruction_abort(esr))
-			die("Attempting to execute userspace memory", regs, esr);
+			die_kernel_fault("execution of user memory",
+					 addr, esr, regs);
 
 		if (!search_exception_tables(regs->pc))
-			die("Accessing user space memory outside uaccess.h routines", regs, esr);
+			die_kernel_fault("access to user memory outside uaccess routines",
+					 addr, esr, regs);
 	}
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);

commit 969e61ba87f9195c18130a5e88a966837fdb6a2c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon May 21 14:14:50 2018 +0100

    arm64: make is_permission_fault() name clearer
    
    The naming of is_permission_fault() makes it sound like it should return
    true for permission faults from EL0, but by design, it only does so for
    faults from EL1.
    
    Let's make this clear by dropping el1 in the name, as we do for
    is_el1_instruction_abort().
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 4165485e8b6e..59990491e72a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -235,8 +235,9 @@ static bool is_el1_instruction_abort(unsigned int esr)
 	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_CUR;
 }
 
-static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs,
-				       unsigned long addr)
+static inline bool is_el1_permission_fault(unsigned int esr,
+					   struct pt_regs *regs,
+					   unsigned long addr)
 {
 	unsigned int ec       = ESR_ELx_EC(esr);
 	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
@@ -268,7 +269,7 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 
 	bust_spinlocks(1);
 
-	if (is_permission_fault(esr, regs, addr)) {
+	if (is_el1_permission_fault(esr, regs, addr)) {
 		if (esr & ESR_ELx_WNR)
 			msg = "write to read-only memory";
 		else
@@ -395,7 +396,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (addr < TASK_SIZE && is_permission_fault(esr, regs, addr)) {
+	if (addr < TASK_SIZE && is_el1_permission_fault(esr, regs, addr)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);

commit cc19846079a70abcfd91b5a0791a5f17d69458a5
Author: Peter Maydell <peter.maydell@linaro.org>
Date:   Tue May 22 17:11:20 2018 +0100

    arm64: fault: Don't leak data in ESR context for user fault on kernel VA
    
    If userspace faults on a kernel address, handing them the raw ESR
    value on the sigframe as part of the delivered signal can leak data
    useful to attackers who are using information about the underlying hardware
    fault type (e.g. translation vs permission) as a mechanism to defeat KASLR.
    
    However there are also legitimate uses for the information provided
    in the ESR -- notably the GCC and LLVM sanitizers use this to report
    whether wild pointer accesses by the application are reads or writes
    (since a wild write is a more serious bug than a wild read), so we
    don't want to drop the ESR information entirely.
    
    For faulting addresses in the kernel, sanitize the ESR. We choose
    to present userspace with the illusion that there is nothing mapped
    in the kernel's part of the address space at all, by reporting all
    faults as level 0 translation faults taken to EL1.
    
    These fields are safe to pass through to userspace as they depend
    only on the instruction that userspace used to provoke the fault:
     EC IL (always)
     ISV CM WNR (for all data aborts)
    All the other fields in ESR except DFSC are architecturally RES0
    for an L0 translation fault taken to EL1, so can be zeroed out
    without confusing userspace.
    
    The illusion is not entirely perfect, as there is a tiny wrinkle
    where we will report an alignment fault that was not due to the memory
    type (for instance a LDREX to an unaligned address) as a translation
    fault, whereas if you do this on real unmapped memory the alignment
    fault takes precedence. This is not likely to trip anybody up in
    practice, as the only users we know of for the ESR information who
    care about the behaviour for kernel addresses only really want to
    know about the WnR bit.
    
    Signed-off-by: Peter Maydell <peter.maydell@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 4165485e8b6e..2af3dd89bcdb 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -293,6 +293,57 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 static void __do_user_fault(struct siginfo *info, unsigned int esr)
 {
 	current->thread.fault_address = (unsigned long)info->si_addr;
+
+	/*
+	 * If the faulting address is in the kernel, we must sanitize the ESR.
+	 * From userspace's point of view, kernel-only mappings don't exist
+	 * at all, so we report them as level 0 translation faults.
+	 * (This is not quite the way that "no mapping there at all" behaves:
+	 * an alignment fault not caused by the memory type would take
+	 * precedence over translation fault for a real access to empty
+	 * space. Unfortunately we can't easily distinguish "alignment fault
+	 * not caused by memory type" from "alignment fault caused by memory
+	 * type", so we ignore this wrinkle and just return the translation
+	 * fault.)
+	 */
+	if (current->thread.fault_address >= TASK_SIZE) {
+		switch (ESR_ELx_EC(esr)) {
+		case ESR_ELx_EC_DABT_LOW:
+			/*
+			 * These bits provide only information about the
+			 * faulting instruction, which userspace knows already.
+			 * We explicitly clear bits which are architecturally
+			 * RES0 in case they are given meanings in future.
+			 * We always report the ESR as if the fault was taken
+			 * to EL1 and so ISV and the bits in ISS[23:14] are
+			 * clear. (In fact it always will be a fault to EL1.)
+			 */
+			esr &= ESR_ELx_EC_MASK | ESR_ELx_IL |
+				ESR_ELx_CM | ESR_ELx_WNR;
+			esr |= ESR_ELx_FSC_FAULT;
+			break;
+		case ESR_ELx_EC_IABT_LOW:
+			/*
+			 * Claim a level 0 translation fault.
+			 * All other bits are architecturally RES0 for faults
+			 * reported with that DFSC value, so we clear them.
+			 */
+			esr &= ESR_ELx_EC_MASK | ESR_ELx_IL;
+			esr |= ESR_ELx_FSC_FAULT;
+			break;
+		default:
+			/*
+			 * This should never happen (entry.S only brings us
+			 * into this code for insn and data aborts from a lower
+			 * exception level). Fail safe by not providing an ESR
+			 * context record at all.
+			 */
+			WARN(1, "ESR 0x%x is not DABT or IABT from EL0\n", esr);
+			esr = 0;
+			break;
+		}
+	}
+
 	current->thread.fault_code = esr;
 	arm64_force_sig_info(info, esr_to_fault_info(esr)->name, current);
 }

commit 3eb0f5193b497083391aa05d35210d5645211eef
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 17 15:26:37 2018 -0500

    signal: Ensure every siginfo we send has all bits initialized
    
    Call clear_siginfo to ensure every stack allocated siginfo is properly
    initialized before being passed to the signal sending functions.
    
    Note: It is not safe to depend on C initializers to initialize struct
    siginfo on the stack because C is allowed to skip holes when
    initializing a structure.
    
    The initialization of struct siginfo in tracehook_report_syscall_exit
    was moved from the helper user_single_step_siginfo into
    tracehook_report_syscall_exit itself, to make it clear that the local
    variable siginfo gets fully initialized.
    
    In a few cases the scope of struct siginfo has been reduced to make it
    clear that siginfo siginfo is not used on other paths in the function
    in which it is declared.
    
    Instances of using memset to initialize siginfo have been replaced
    with calls clear_siginfo for clarity.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 4165485e8b6e..91c53a7d2575 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -305,11 +305,12 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 	 */
 	if (user_mode(regs)) {
 		const struct fault_info *inf = esr_to_fault_info(esr);
-		struct siginfo si = {
-			.si_signo	= inf->sig,
-			.si_code	= inf->code,
-			.si_addr	= (void __user *)addr,
-		};
+		struct siginfo si;
+
+		clear_siginfo(&si);
+		si.si_signo	= inf->sig;
+		si.si_code	= inf->code;
+		si.si_addr	= (void __user *)addr;
 
 		__do_user_fault(&si, esr);
 	} else {
@@ -583,6 +584,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 			nmi_exit();
 	}
 
+	clear_siginfo(&info);
 	info.si_signo = inf->sig;
 	info.si_errno = 0;
 	info.si_code  = inf->code;
@@ -687,6 +689,7 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 		show_pte(addr);
 	}
 
+	clear_siginfo(&info);
 	info.si_signo = inf->sig;
 	info.si_errno = 0;
 	info.si_code  = inf->code;
@@ -729,6 +732,7 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 		local_irq_enable();
 	}
 
+	clear_siginfo(&info);
 	info.si_signo = SIGBUS;
 	info.si_errno = 0;
 	info.si_code  = BUS_ADRALN;
@@ -772,7 +776,6 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 					      struct pt_regs *regs)
 {
 	const struct fault_info *inf = debug_fault_info + DBG_ESR_EVT(esr);
-	struct siginfo info;
 	int rv;
 
 	/*
@@ -788,6 +791,9 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (!inf->fn(addr, esr, regs)) {
 		rv = 1;
 	} else {
+		struct siginfo info;
+
+		clear_siginfo(&info);
 		info.si_signo = inf->sig;
 		info.si_errno = 0;
 		info.si_code  = inf->code;

commit c0cda3b8ee6b4b6851b2fd8b6db91fd7b0e2524a
Author: Dave Martin <dave.martin@arm.com>
Date:   Mon Mar 26 15:12:28 2018 +0100

    arm64: capabilities: Update prototype for enable call back
    
    We issue the enable() call back for all CPU hwcaps capabilities
    available on the system, on all the CPUs. So far we have ignored
    the argument passed to the call back, which had a prototype to
    accept a "void *" for use with on_each_cpu() and later with
    stop_machine(). However, with commit 0a0d111d40fd1
    ("arm64: cpufeature: Pass capability structure to ->enable callback"),
    there are some users of the argument who wants the matching capability
    struct pointer where there are multiple matching criteria for a single
    capability. Clean up the declaration of the call back to make it clear.
    
     1) Renamed to cpu_enable(), to imply taking necessary actions on the
        called CPU for the entry.
     2) Pass const pointer to the capability, to allow the call back to
        check the entry. (e.,g to check if any action is needed on the CPU)
     3) We don't care about the result of the call back, turning this to
        a void.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [suzuki: convert more users, rename call back and drop results]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 551d044fb31f..4165485e8b6e 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -804,7 +804,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 NOKPROBE_SYMBOL(do_debug_exception);
 
 #ifdef CONFIG_ARM64_PAN
-int cpu_enable_pan(void *__unused)
+void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused)
 {
 	/*
 	 * We modify PSTATE. This won't work from irq context as the PSTATE
@@ -814,6 +814,5 @@ int cpu_enable_pan(void *__unused)
 
 	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
 	asm(SET_PSTATE_PAN(1));
-	return 0;
 }
 #endif /* CONFIG_ARM64_PAN */

commit af40ff687bc9d351030685fde2f57ba45ab4fc14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Mar 8 17:41:05 2018 +0000

    arm64: signal: Ensure si_code is valid for all fault signals
    
    Currently, as reported by Eric, an invalid si_code value 0 is
    passed in many signals delivered to userspace in response to faults
    and other kernel errors.  Typically 0 is passed when the fault is
    insufficiently diagnosable or when there does not appear to be any
    sensible alternative value to choose.
    
    This appears to violate POSIX, and is intuitively wrong for at
    least two reasons arising from the fact that 0 == SI_USER:
    
     1) si_code is a union selector, and SI_USER (and si_code <= 0 in
        general) implies the existence of a different set of fields
        (siginfo._kill) from that which exists for a fault signal
        (siginfo._sigfault).  However, the code raising the signal
        typically writes only the _sigfault fields, and the _kill
        fields make no sense in this case.
    
        Thus when userspace sees si_code == 0 (SI_USER) it may
        legitimately inspect fields in the inactive union member _kill
        and obtain garbage as a result.
    
        There appears to be software in the wild relying on this,
        albeit generally only for printing diagnostic messages.
    
     2) Software that wants to be robust against spurious signals may
        discard signals where si_code == SI_USER (or <= 0), or may
        filter such signals based on the si_uid and si_pid fields of
        siginfo._sigkill.  In the case of fault signals, this means
        that important (and usually fatal) error conditions may be
        silently ignored.
    
    In practice, many of the faults for which arm64 passes si_code == 0
    are undiagnosable conditions such as exceptions with syndrome
    values in ESR_ELx to which the architecture does not yet assign any
    meaning, or conditions indicative of a bug or error in the kernel
    or system and thus that are unrecoverable and should never occur in
    normal operation.
    
    The approach taken in this patch is to translate all such
    undiagnosable or "impossible" synchronous fault conditions to
    SIGKILL, since these are at least probably localisable to a single
    process.  Some of these conditions should really result in a kernel
    panic, but due to the lack of diagnostic information it is
    difficult to be certain: this patch does not add any calls to
    panic(), but this could change later if justified.
    
    Although si_code will not reach userspace in the case of SIGKILL,
    it is still desirable to pass a nonzero value so that the common
    siginfo handling code can detect incorrect use of si_code == 0
    without false positives.  In this case the si_code dependent
    siginfo fields will not be correctly initialised, but since they
    are not passed to userspace I deem this not to matter.
    
    A few faults can reasonably occur in realistic userspace scenarios,
    and _should_ raise a regular, handleable (but perhaps not
    ignorable/blockable) signal: for these, this patch attempts to
    choose a suitable standard si_code value for the raised signal in
    each case instead of 0.
    
    arm64 was the only arch to define a BUS_FIXME code, so after this
    patch nobody defines it.  This patch therefore also removes the
    relevant code from siginfo_layout().
    
    Cc: James Morse <james.morse@arm.com>
    Reported-by: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 49dfb08a6c4d..551d044fb31f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -583,9 +583,9 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 			nmi_exit();
 	}
 
-	info.si_signo = SIGBUS;
+	info.si_signo = inf->sig;
 	info.si_errno = 0;
-	info.si_code  = BUS_FIXME;
+	info.si_code  = inf->code;
 	if (esr & ESR_ELx_FnV)
 		info.si_addr = NULL;
 	else
@@ -596,70 +596,70 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 }
 
 static const struct fault_info fault_info[] = {
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"ttbr address size fault"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"level 1 address size fault"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"level 2 address size fault"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"level 3 address size fault"	},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"ttbr address size fault"	},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"level 1 address size fault"	},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"level 2 address size fault"	},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"level 3 address size fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 0 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 8"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 8"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 access flag fault"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 12"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 12"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"synchronous external abort"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 17"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 18"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 19"			},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 0 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 1 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 2 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 3 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"synchronous parity or ECC error" },	// Reserved when RAS is implemented
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 25"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 26"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 27"			},
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 0 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 1 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 2 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 3 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 32"			},
+	{ do_sea,		SIGBUS,  BUS_OBJERR,	"synchronous external abort"	},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 17"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 18"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 19"			},
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 0 (translation table walk)"	},
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 1 (translation table walk)"	},
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 2 (translation table walk)"	},
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 3 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  BUS_OBJERR,	"synchronous parity or ECC error" },	// Reserved when RAS is implemented
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 25"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 26"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 27"			},
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 0 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 1 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 2 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGKILL, SI_KERNEL,	"level 3 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 32"			},
 	{ do_alignment_fault,	SIGBUS,  BUS_ADRALN,	"alignment fault"		},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 34"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 35"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 36"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 37"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 38"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 39"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 40"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 41"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 42"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 43"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 44"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 45"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 46"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 47"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"TLB conflict abort"		},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"Unsupported atomic hardware update fault"	},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 50"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 51"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"implementation fault (lockdown abort)" },
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"implementation fault (unsupported exclusive)" },
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 54"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 55"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 56"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 57"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 58" 			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 59"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 60"			},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"section domain fault"		},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"page domain fault"		},
-	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 63"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 34"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 35"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 36"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 37"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 38"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 39"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 40"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 41"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 42"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 43"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 44"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 45"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 46"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 47"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"TLB conflict abort"		},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"Unsupported atomic hardware update fault"	},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 50"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 51"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"implementation fault (lockdown abort)" },
+	{ do_bad,		SIGBUS,  BUS_OBJERR,	"implementation fault (unsupported exclusive)" },
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 54"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 55"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 56"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 57"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 58" 			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 59"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 60"			},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"section domain fault"		},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"page domain fault"		},
+	{ do_bad,		SIGKILL, SI_KERNEL,	"unknown 63"			},
 };
 
 int handle_guest_sea(phys_addr_t addr, unsigned int esr)
@@ -748,11 +748,11 @@ static struct fault_info __refdata debug_fault_info[] = {
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware breakpoint"	},
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware single-step"	},
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware watchpoint"	},
-	{ do_bad,	SIGBUS,		BUS_FIXME,	"unknown 3"		},
+	{ do_bad,	SIGKILL,	SI_KERNEL,	"unknown 3"		},
 	{ do_bad,	SIGTRAP,	TRAP_BRKPT,	"aarch32 BKPT"		},
-	{ do_bad,	SIGTRAP,	TRAP_FIXME,	"aarch32 vector catch"	},
+	{ do_bad,	SIGKILL,	SI_KERNEL,	"aarch32 vector catch"	},
 	{ early_brk64,	SIGTRAP,	TRAP_BRKPT,	"aarch64 BRK"		},
-	{ do_bad,	SIGBUS,		BUS_FIXME,	"unknown 7"		},
+	{ do_bad,	SIGKILL,	SI_KERNEL,	"unknown 7"		},
 };
 
 void __init hook_debug_fault_code(int nr,

commit 92ff0674f5d8013704cbaeaceb8e3576b36754ee
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 14:53:22 2018 +0000

    arm64: mm: Rework unhandled user pagefaults to call arm64_force_sig_info
    
    Reporting unhandled user pagefaults via arm64_force_sig_info means
    that __do_user_fault can be drastically simplified, since it no longer
    has to worry about printing the fault information and can consequently
    just take the siginfo as a parameter.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index fd5928afd9cd..49dfb08a6c4d 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -43,6 +43,7 @@
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
+#include <asm/traps.h>
 
 #include <acpi/ghes.h>
 
@@ -289,58 +290,31 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	do_exit(SIGKILL);
 }
 
-static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
-			    unsigned int esr, unsigned int sig, int code,
-			    struct pt_regs *regs, int fault)
+static void __do_user_fault(struct siginfo *info, unsigned int esr)
 {
-	struct siginfo si;
-	const struct fault_info *inf;
-	unsigned int lsb = 0;
-
-	if (unhandled_signal(tsk, sig) && show_unhandled_signals_ratelimited()) {
-		inf = esr_to_fault_info(esr);
-		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x",
-			tsk->comm, task_pid_nr(tsk), inf->name, sig,
-			addr, esr);
-		print_vma_addr(KERN_CONT ", in ", regs->pc);
-		pr_cont("\n");
-		__show_regs(regs);
-	}
-
-	tsk->thread.fault_address = addr;
-	tsk->thread.fault_code = esr;
-	si.si_signo = sig;
-	si.si_errno = 0;
-	si.si_code = code;
-	si.si_addr = (void __user *)addr;
-	/*
-	 * Either small page or large page may be poisoned.
-	 * In other words, VM_FAULT_HWPOISON_LARGE and
-	 * VM_FAULT_HWPOISON are mutually exclusive.
-	 */
-	if (fault & VM_FAULT_HWPOISON_LARGE)
-		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));
-	else if (fault & VM_FAULT_HWPOISON)
-		lsb = PAGE_SHIFT;
-	si.si_addr_lsb = lsb;
-
-	force_sig_info(sig, &si, tsk);
+	current->thread.fault_address = (unsigned long)info->si_addr;
+	current->thread.fault_code = esr;
+	arm64_force_sig_info(info, esr_to_fault_info(esr)->name, current);
 }
 
 static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
-	struct task_struct *tsk = current;
-	const struct fault_info *inf;
-
 	/*
 	 * If we are in kernel mode at this point, we have no context to
 	 * handle this fault with.
 	 */
 	if (user_mode(regs)) {
-		inf = esr_to_fault_info(esr);
-		__do_user_fault(tsk, addr, esr, inf->sig, inf->code, regs, 0);
-	} else
+		const struct fault_info *inf = esr_to_fault_info(esr);
+		struct siginfo si = {
+			.si_signo	= inf->sig,
+			.si_code	= inf->code,
+			.si_addr	= (void __user *)addr,
+		};
+
+		__do_user_fault(&si, esr);
+	} else {
 		__do_kernel_fault(addr, esr, regs);
+	}
 }
 
 #define VM_FAULT_BADMAP		0x010000
@@ -393,7 +367,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
-	int fault, sig, code, major = 0;
+	struct siginfo si;
+	int fault, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
@@ -525,27 +500,37 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		return 0;
 	}
 
+	clear_siginfo(&si);
+	si.si_addr = (void __user *)addr;
+
 	if (fault & VM_FAULT_SIGBUS) {
 		/*
 		 * We had some memory, but were unable to successfully fix up
 		 * this page fault.
 		 */
-		sig = SIGBUS;
-		code = BUS_ADRERR;
-	} else if (fault & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE)) {
-		sig = SIGBUS;
-		code = BUS_MCEERR_AR;
+		si.si_signo	= SIGBUS;
+		si.si_code	= BUS_ADRERR;
+	} else if (fault & VM_FAULT_HWPOISON_LARGE) {
+		unsigned int hindex = VM_FAULT_GET_HINDEX(fault);
+
+		si.si_signo	= SIGBUS;
+		si.si_code	= BUS_MCEERR_AR;
+		si.si_addr_lsb	= hstate_index_to_shift(hindex);
+	} else if (fault & VM_FAULT_HWPOISON) {
+		si.si_signo	= SIGBUS;
+		si.si_code	= BUS_MCEERR_AR;
+		si.si_addr_lsb	= PAGE_SHIFT;
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory
 		 * map.
 		 */
-		sig = SIGSEGV;
-		code = fault == VM_FAULT_BADACCESS ?
-			SEGV_ACCERR : SEGV_MAPERR;
+		si.si_signo	= SIGSEGV;
+		si.si_code	= fault == VM_FAULT_BADACCESS ?
+				  SEGV_ACCERR : SEGV_MAPERR;
 	}
 
-	__do_user_fault(tsk, addr, esr, sig, code, regs, fault);
+	__do_user_fault(&si, esr);
 	return 0;
 
 no_context:

commit 1049c30871701a6533dd41e555612b31a29acb33
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 14:41:02 2018 +0000

    arm64: Pass user fault info to arm64_notify_die instead of printing it
    
    There's no need for callers of arm64_notify_die to print information
    about user faults. Instead, they can pass a string to arm64_notify_die
    which will be printed subject to show_unhandled_signals.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index bff11553eb05..fd5928afd9cd 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -582,8 +582,6 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 	const struct fault_info *inf;
 
 	inf = esr_to_fault_info(esr);
-	pr_err("Synchronous External Abort: %s (0x%08x) at 0x%016lx\n",
-		inf->name, esr, addr);
 
 	/*
 	 * Synchronous aborts may interrupt code which had interrupts masked.
@@ -607,7 +605,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 		info.si_addr = NULL;
 	else
 		info.si_addr  = (void __user *)addr;
-	arm64_notify_die("", regs, &info, esr);
+	arm64_notify_die(inf->name, regs, &info, esr);
 
 	return 0;
 }
@@ -698,19 +696,17 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	if (!inf->fn(addr, esr, regs))
 		return;
 
-	pr_alert("Unhandled fault: %s at 0x%016lx\n",
-		 inf->name, addr);
-
-	mem_abort_decode(esr);
-
-	if (!user_mode(regs))
+	if (!user_mode(regs)) {
+		pr_alert("Unhandled fault at 0x%016lx\n", addr);
+		mem_abort_decode(esr);
 		show_pte(addr);
+	}
 
 	info.si_signo = inf->sig;
 	info.si_errno = 0;
 	info.si_code  = inf->code;
 	info.si_addr  = (void __user *)addr;
-	arm64_notify_die("", regs, &info, esr);
+	arm64_notify_die(inf->name, regs, &info, esr);
 }
 
 asmlinkage void __exception do_el0_irq_bp_hardening(void)
@@ -741,7 +737,6 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   struct pt_regs *regs)
 {
 	struct siginfo info;
-	struct task_struct *tsk = current;
 
 	if (user_mode(regs)) {
 		if (instruction_pointer(regs) > TASK_SIZE)
@@ -749,17 +744,11 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 		local_irq_enable();
 	}
 
-	if (show_unhandled_signals && unhandled_signal(tsk, SIGBUS))
-		pr_info_ratelimited("%s[%d]: %s exception: pc=%p sp=%p\n",
-				    tsk->comm, task_pid_nr(tsk),
-				    esr_get_class_string(esr), (void *)regs->pc,
-				    (void *)regs->sp);
-
 	info.si_signo = SIGBUS;
 	info.si_errno = 0;
 	info.si_code  = BUS_ADRALN;
 	info.si_addr  = (void __user *)addr;
-	arm64_notify_die("Oops - SP/PC alignment exception", regs, &info, esr);
+	arm64_notify_die("SP/PC alignment exception", regs, &info, esr);
 }
 
 int __init early_brk64(unsigned long addr, unsigned int esr,
@@ -814,14 +803,11 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (!inf->fn(addr, esr, regs)) {
 		rv = 1;
 	} else {
-		pr_alert("Unhandled debug exception: %s (0x%08x) at 0x%016lx\n",
-			 inf->name, esr, addr);
-
 		info.si_signo = inf->sig;
 		info.si_errno = 0;
 		info.si_code  = inf->code;
 		info.si_addr  = (void __user *)addr;
-		arm64_notify_die("", regs, &info, 0);
+		arm64_notify_die(inf->name, regs, &info, esr);
 		rv = 0;
 	}
 

commit 20a004e7b017cce282a46ac5d02c2b9c6b9bb1fa
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Feb 15 11:14:56 2018 +0000

    arm64: mm: Use READ_ONCE/WRITE_ONCE when accessing page tables
    
    In many cases, page tables can be accessed concurrently by either another
    CPU (due to things like fast gup) or by the hardware page table walker
    itself, which may set access/dirty bits. In such cases, it is important
    to use READ_ONCE/WRITE_ONCE when accessing page table entries so that
    entries cannot be torn, merged or subject to apparent loss of coherence
    due to compiler transformations.
    
    Whilst there are some scenarios where this cannot happen (e.g. pinned
    kernel mappings for the linear region), the overhead of using READ_ONCE
    /WRITE_ONCE everywhere is minimal and makes the code an awful lot easier
    to reason about. This patch consistently uses these macros in the arch
    code, as well as explicitly namespacing pointers to page table entries
    from the entries themselves by using adopting a 'p' suffix for the former
    (as is sometimes used elsewhere in the kernel source).
    
    Tested-by: Yury Norov <ynorov@caviumnetworks.com>
    Tested-by: Richard Ruigrok <rruigrok@codeaurora.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index f76bb2c3c943..bff11553eb05 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -130,7 +130,8 @@ static void mem_abort_decode(unsigned int esr)
 void show_pte(unsigned long addr)
 {
 	struct mm_struct *mm;
-	pgd_t *pgd;
+	pgd_t *pgdp;
+	pgd_t pgd;
 
 	if (addr < TASK_SIZE) {
 		/* TTBR0 */
@@ -149,33 +150,37 @@ void show_pte(unsigned long addr)
 		return;
 	}
 
-	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgd = %p\n",
+	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgdp = %p\n",
 		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
 		 VA_BITS, mm->pgd);
-	pgd = pgd_offset(mm, addr);
-	pr_alert("[%016lx] *pgd=%016llx", addr, pgd_val(*pgd));
+	pgdp = pgd_offset(mm, addr);
+	pgd = READ_ONCE(*pgdp);
+	pr_alert("[%016lx] pgd=%016llx", addr, pgd_val(pgd));
 
 	do {
-		pud_t *pud;
-		pmd_t *pmd;
-		pte_t *pte;
+		pud_t *pudp, pud;
+		pmd_t *pmdp, pmd;
+		pte_t *ptep, pte;
 
-		if (pgd_none(*pgd) || pgd_bad(*pgd))
+		if (pgd_none(pgd) || pgd_bad(pgd))
 			break;
 
-		pud = pud_offset(pgd, addr);
-		pr_cont(", *pud=%016llx", pud_val(*pud));
-		if (pud_none(*pud) || pud_bad(*pud))
+		pudp = pud_offset(pgdp, addr);
+		pud = READ_ONCE(*pudp);
+		pr_cont(", pud=%016llx", pud_val(pud));
+		if (pud_none(pud) || pud_bad(pud))
 			break;
 
-		pmd = pmd_offset(pud, addr);
-		pr_cont(", *pmd=%016llx", pmd_val(*pmd));
-		if (pmd_none(*pmd) || pmd_bad(*pmd))
+		pmdp = pmd_offset(pudp, addr);
+		pmd = READ_ONCE(*pmdp);
+		pr_cont(", pmd=%016llx", pmd_val(pmd));
+		if (pmd_none(pmd) || pmd_bad(pmd))
 			break;
 
-		pte = pte_offset_map(pmd, addr);
-		pr_cont(", *pte=%016llx", pte_val(*pte));
-		pte_unmap(pte);
+		ptep = pte_offset_map(pmdp, addr);
+		pte = READ_ONCE(*ptep);
+		pr_cont(", pte=%016llx", pte_val(pte));
+		pte_unmap(ptep);
 	} while(0);
 
 	pr_cont("\n");
@@ -196,8 +201,9 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 			  pte_t entry, int dirty)
 {
 	pteval_t old_pteval, pteval;
+	pte_t pte = READ_ONCE(*ptep);
 
-	if (pte_same(*ptep, entry))
+	if (pte_same(pte, entry))
 		return 0;
 
 	/* only preserve the access flags and write permission */
@@ -210,7 +216,7 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 	 * (calculated as: a & b == ~(~a | ~b)).
 	 */
 	pte_val(entry) ^= PTE_RDONLY;
-	pteval = READ_ONCE(pte_val(*ptep));
+	pteval = pte_val(pte);
 	do {
 		old_pteval = pteval;
 		pteval ^= PTE_RDONLY;

commit c0136321924dd338bb8fc5661c4b0e27441a8d04
Merge: 846ade7dd2e6 3a0a397ff5ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 8 10:44:25 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull more arm64 updates from Catalin Marinas:
     "As I mentioned in the last pull request, there's a second batch of
      security updates for arm64 with mitigations for Spectre/v1 and an
      improved one for Spectre/v2 (via a newly defined firmware interface
      API).
    
      Spectre v1 mitigation:
    
       - back-end version of array_index_mask_nospec()
    
       - masking of the syscall number to restrict speculation through the
         syscall table
    
       - masking of __user pointers prior to deference in uaccess routines
    
      Spectre v2 mitigation update:
    
       - using the new firmware SMC calling convention specification update
    
       - removing the current PSCI GET_VERSION firmware call mitigation as
         vendors are deploying new SMCCC-capable firmware
    
       - additional branch predictor hardening for synchronous exceptions
         and interrupts while in user mode
    
      Meltdown v3 mitigation update:
    
        - Cavium Thunder X is unaffected but a hardware erratum gets in the
          way. The kernel now starts with the page tables mapped as global
          and switches to non-global if kpti needs to be enabled.
    
      Other:
    
       - Theoretical trylock bug fixed"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (38 commits)
      arm64: Kill PSCI_GET_VERSION as a variant-2 workaround
      arm64: Add ARM_SMCCC_ARCH_WORKAROUND_1 BP hardening support
      arm/arm64: smccc: Implement SMCCC v1.1 inline primitive
      arm/arm64: smccc: Make function identifiers an unsigned quantity
      firmware/psci: Expose SMCCC version through psci_ops
      firmware/psci: Expose PSCI conduit
      arm64: KVM: Add SMCCC_ARCH_WORKAROUND_1 fast handling
      arm64: KVM: Report SMCCC_ARCH_WORKAROUND_1 BP hardening support
      arm/arm64: KVM: Turn kvm_psci_version into a static inline
      arm/arm64: KVM: Advertise SMCCC v1.1
      arm/arm64: KVM: Implement PSCI 1.0 support
      arm/arm64: KVM: Add smccc accessors to PSCI code
      arm/arm64: KVM: Add PSCI_VERSION helper
      arm/arm64: KVM: Consolidate the PSCI include files
      arm64: KVM: Increment PC after handling an SMC trap
      arm: KVM: Fix SMCCC handling of unimplemented SMC/HVC calls
      arm64: KVM: Fix SMCCC handling of unimplemented SMC/HVC calls
      arm64: entry: Apply BP hardening for suspicious interrupts from EL0
      arm64: entry: Apply BP hardening for high-priority synchronous exceptions
      arm64: futex: Mask __user pointers prior to dereference
      ...

commit 30d88c0e3ace625a92eead9ca0ad94093a8f59fe
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 2 17:31:40 2018 +0000

    arm64: entry: Apply BP hardening for suspicious interrupts from EL0
    
    It is possible to take an IRQ from EL0 following a branch to a kernel
    address in such a way that the IRQ is prioritised over the instruction
    abort. Whilst an attacker would need to get the stars to align here,
    it might be sufficient with enough calibration so perform BP hardening
    in the rare case that we see a kernel address in the ELR when handling
    an IRQ from EL0.
    
    Reported-by: Dan Hettena <dhettena@nvidia.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 43b28a782ed4..8fd141233dec 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -708,6 +708,12 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	arm64_notify_die("", regs, &info, esr);
 }
 
+asmlinkage void __exception do_el0_irq_bp_hardening(void)
+{
+	/* PC has already been checked in entry.S */
+	arm64_apply_bp_hardening();
+}
+
 asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
 						   unsigned int esr,
 						   struct pt_regs *regs)

commit 5dfc6ed27710c42cbc15db5c0d4475699991da0a
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 2 17:31:39 2018 +0000

    arm64: entry: Apply BP hardening for high-priority synchronous exceptions
    
    Software-step and PC alignment fault exceptions have higher priority than
    instruction abort exceptions, so apply the BP hardening hooks there too
    if the user PC appears to reside in kernel space.
    
    Reported-by: Dan Hettena <dhettena@nvidia.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index af530eb9f2ed..43b28a782ed4 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -732,6 +732,12 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 	struct siginfo info;
 	struct task_struct *tsk = current;
 
+	if (user_mode(regs)) {
+		if (instruction_pointer(regs) > TASK_SIZE)
+			arm64_apply_bp_hardening();
+		local_irq_enable();
+	}
+
 	if (show_unhandled_signals && unhandled_signal(tsk, SIGBUS))
 		pr_info_ratelimited("%s[%d]: %s exception: pc=%p sp=%p\n",
 				    tsk->comm, task_pid_nr(tsk),
@@ -791,6 +797,9 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	if (interrupts_enabled(regs))
 		trace_hardirqs_off();
 
+	if (user_mode(regs) && instruction_pointer(regs) > TASK_SIZE)
+		arm64_apply_bp_hardening();
+
 	if (!inf->fn(addr, esr, regs)) {
 		rv = 1;
 	} else {

commit 51369e398d0d33e8f524314e672b07e8cf870e79
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Feb 5 15:34:18 2018 +0000

    arm64: Make USER_DS an inclusive limit
    
    Currently, USER_DS represents an exclusive limit while KERNEL_DS is
    inclusive. In order to do some clever trickery for speculation-safe
    masking, we need them both to behave equivalently - there aren't enough
    bits to make KERNEL_DS exclusive, so we have precisely one option. This
    also happens to correct a longstanding false negative for a range
    ending on the very top byte of kernel memory.
    
    Mark Rutland points out that we've actually got the semantics of
    addresses vs. segments muddled up in most of the places we need to
    amend, so shuffle the {USER,KERNEL}_DS definitions around such that we
    can correct those properly instead of just pasting "-1"s everywhere.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0e671ddf4855..af530eb9f2ed 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -240,7 +240,7 @@ static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs,
 	if (fsc_type == ESR_ELx_FSC_PERM)
 		return true;
 
-	if (addr < USER_DS && system_uses_ttbr0_pan())
+	if (addr < TASK_SIZE && system_uses_ttbr0_pan())
 		return fsc_type == ESR_ELx_FSC_FAULT &&
 			(regs->pstate & PSR_PAN_BIT);
 
@@ -414,7 +414,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (addr < USER_DS && is_permission_fault(esr, regs, addr)) {
+	if (addr < TASK_SIZE && is_permission_fault(esr, regs, addr)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);

commit d4173023e63cb85ec02eda02d1789bf078719f00
Merge: 0aebc6a440b9 c0f45555b824
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 14:18:52 2018 -0800

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo cleanups from Eric Biederman:
     "Long ago when 2.4 was just a testing release copy_siginfo_to_user was
      made to copy individual fields to userspace, possibly for efficiency
      and to ensure initialized values were not copied to userspace.
    
      Unfortunately the design was complex, it's assumptions unstated, and
      humans are fallible and so while it worked much of the time that
      design failed to ensure unitialized memory is not copied to userspace.
    
      This set of changes is part of a new design to clean up siginfo and
      simplify things, and hopefully make the siginfo handling robust enough
      that a simple inspection of the code can be made to ensure we don't
      copy any unitializied fields to userspace.
    
      The design is to unify struct siginfo and struct compat_siginfo into a
      single definition that is shared between all architectures so that
      anyone adding to the set of information shared with struct siginfo can
      see the whole picture. Hopefully ensuring all future si_code
      assignments are arch independent.
    
      The design is to unify copy_siginfo_to_user32 and
      copy_siginfo_from_user32 so that those function are complete and cope
      with all of the different cases documented in signinfo_layout. I don't
      think there was a single implementation of either of those functions
      that was complete and correct before my changes unified them.
    
      The design is to introduce a series of helpers including
      force_siginfo_fault that take the values that are needed in struct
      siginfo and build the siginfo structure for their callers. Ensuring
      struct siginfo is built correctly.
    
      The remaining work for 4.17 (unless someone thinks it is post -rc1
      material) is to push usage of those helpers down into the
      architectures so that architecture specific code will not need to deal
      with the fiddly work of intializing struct siginfo, and then when
      struct siginfo is guaranteed to be fully initialized change copy
      siginfo_to_user into a simple wrapper around copy_to_user.
    
      Further there is work in progress on the issues that have been
      documented requires arch specific knowledge to sort out.
    
      The changes below fix or at least document all of the issues that have
      been found with siginfo generation. Then proceed to unify struct
      siginfo the 32 bit helpers that copy siginfo to and from userspace,
      and generally clean up anything that is not arch specific with regards
      to siginfo generation.
    
      It is a lot but with the unification you can of siginfo you can
      already see the code reduction in the kernel"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (45 commits)
      signal/memory-failure: Use force_sig_mceerr and send_sig_mceerr
      mm/memory_failure: Remove unused trapno from memory_failure
      signal/ptrace: Add force_sig_ptrace_errno_trap and use it where needed
      signal/powerpc: Remove unnecessary signal_code parameter of do_send_trap
      signal: Helpers for faults with specialized siginfo layouts
      signal: Add send_sig_fault and force_sig_fault
      signal: Replace memset(info,...) with clear_siginfo for clarity
      signal: Don't use structure initializers for struct siginfo
      signal/arm64: Better isolate the COMPAT_TASK portion of ptrace_hbptriggered
      ptrace: Use copy_siginfo in setsiginfo and getsiginfo
      signal: Unify and correct copy_siginfo_to_user32
      signal: Remove the code to clear siginfo before calling copy_siginfo_from_user32
      signal: Unify and correct copy_siginfo_from_user32
      signal/blackfin: Remove pointless UID16_SIGINFO_COMPAT_NEEDED
      signal/blackfin: Move the blackfin specific si_codes to asm-generic/siginfo.h
      signal/tile: Move the tile specific si_codes to asm-generic/siginfo.h
      signal/frv: Move the frv specific si_codes to asm-generic/siginfo.h
      signal/ia64: Move the ia64 specific si_codes to asm-generic/siginfo.h
      signal/powerpc: Remove redefinition of NSIGTRAP on powerpc
      signal: Move addr_lsb into the _sigfault union for clarity
      ...

commit 0aebc6a440b942df6221a7765f077f02217e0114
Merge: 72906f38934a ec89ab50a03a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 13:57:43 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The main theme of this pull request is security covering variants 2
      and 3 for arm64. I expect to send additional patches next week
      covering an improved firmware interface (requires firmware changes)
      for variant 2 and way for KPTI to be disabled on unaffected CPUs
      (Cavium's ThunderX doesn't work properly with KPTI enabled because of
      a hardware erratum).
    
      Summary:
    
       - Security mitigations:
          - variant 2: invalidate the branch predictor with a call to
            secure firmware
          - variant 3: implement KPTI for arm64
    
       - 52-bit physical address support for arm64 (ARMv8.2)
    
       - arm64 support for RAS (firmware first only) and SDEI (software
         delegated exception interface; allows firmware to inject a RAS
         error into the OS)
    
       - perf support for the ARM DynamIQ Shared Unit PMU
    
       - CPUID and HWCAP bits updated for new floating point multiplication
         instructions in ARMv8.4
    
       - remove some virtual memory layout printks during boot
    
       - fix initial page table creation to cope with larger than 32M kernel
         images when 16K pages are enabled"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (104 commits)
      arm64: Fix TTBR + PAN + 52-bit PA logic in cpu_do_switch_mm
      arm64: Turn on KPTI only on CPUs that need it
      arm64: Branch predictor hardening for Cavium ThunderX2
      arm64: Run enable method for errata work arounds on late CPUs
      arm64: Move BP hardening to check_and_switch_context
      arm64: mm: ignore memory above supported physical address size
      arm64: kpti: Fix the interaction between ASID switching and software PAN
      KVM: arm64: Emulate RAS error registers and set HCR_EL2's TERR & TEA
      KVM: arm64: Handle RAS SErrors from EL2 on guest exit
      KVM: arm64: Handle RAS SErrors from EL1 on guest exit
      KVM: arm64: Save ESR_EL2 on guest SError
      KVM: arm64: Save/Restore guest DISR_EL1
      KVM: arm64: Set an impdef ESR for Virtual-SError using VSESR_EL2.
      KVM: arm/arm64: mask/unmask daif around VHE guests
      arm64: kernel: Prepare for a DISR user
      arm64: Unconditionally enable IESB on exception entry/return for firmware-first
      arm64: kernel: Survive corrected RAS errors notified by SError
      arm64: cpufeature: Detect CPU RAS Extentions
      arm64: sysreg: Move to use definitions for all the SCTLR bits
      arm64: cpufeature: __this_cpu_has_cap() shouldn't stop early
      ...

commit 526c3ddb6aa270fe6f71d227eac1e189746cc257
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Jan 3 18:07:12 2018 -0600

    signal/arm64: Document conflicts with SI_USER and SIGFPE,SIGTRAP,SIGBUS
    
    Setting si_code to 0 results in a userspace seeing an si_code of 0.
    This is the same si_code as SI_USER.  Posix and common sense requires
    that SI_USER not be a signal specific si_code.  As such this use of 0
    for the si_code is a pretty horribly broken ABI.
    
    Further use of si_code == 0 guaranteed that copy_siginfo_to_user saw a
    value of __SI_KILL and now sees a value of SIL_KILL with the result
    that uid and pid fields are copied and which might copying the si_addr
    field by accident but certainly not by design.  Making this a very
    flakey implementation.
    
    Utilizing FPE_FIXME, BUS_FIXME, TRAP_FIXME siginfo_layout will now return
    SIL_FAULT and the appropriate fields will be reliably copied.
    
    But folks this is a new and unique kind of bad.  This is massively
    untested code bad.  This is inventing new and unique was to get
    siginfo wrong bad.  This is don't even think about Posix or what
    siginfo means bad.  This is lots of eyeballs all missing the fact
    that the code does the wrong thing bad.  This is getting stuck
    and keep making the same mistake bad.
    
    I really hope we can find a non userspace breaking fix for this on a
    port as new as arm64.
    
    Possible ABI fixes include:
    - Send the signal without siginfo
    - Don't generate a signal
    - Possibly assign and use an appropriate si_code
    - Don't handle cases which can't happen
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tyler Baicar <tbaicar@codeaurora.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Ref: 53631b54c870 ("arm64: Floating point and SIMD")
    Ref: 32015c235603 ("arm64: exception: handle Synchronous External Abort")
    Ref: 1d18c47c735e ("arm64: MMU fault handling and page table management")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 9b7f89df49db..abe200587334 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -596,7 +596,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 
 	info.si_signo = SIGBUS;
 	info.si_errno = 0;
-	info.si_code  = 0;
+	info.si_code  = BUS_FIXME;
 	if (esr & ESR_ELx_FnV)
 		info.si_addr = NULL;
 	else
@@ -607,70 +607,70 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 }
 
 static const struct fault_info fault_info[] = {
-	{ do_bad,		SIGBUS,  0,		"ttbr address size fault"	},
-	{ do_bad,		SIGBUS,  0,		"level 1 address size fault"	},
-	{ do_bad,		SIGBUS,  0,		"level 2 address size fault"	},
-	{ do_bad,		SIGBUS,  0,		"level 3 address size fault"	},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"ttbr address size fault"	},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"level 1 address size fault"	},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"level 2 address size fault"	},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"level 3 address size fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 0 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
-	{ do_bad,		SIGBUS,  0,		"unknown 8"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 8"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 access flag fault"	},
-	{ do_bad,		SIGBUS,  0,		"unknown 12"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 12"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
-	{ do_sea,		SIGBUS,  0,		"synchronous external abort"	},
-	{ do_bad,		SIGBUS,  0,		"unknown 17"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 18"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 19"			},
-	{ do_sea,		SIGBUS,  0,		"level 0 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"level 1 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"level 2 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"level 3 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"synchronous parity or ECC error" },	// Reserved when RAS is implemented
-	{ do_bad,		SIGBUS,  0,		"unknown 25"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 26"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 27"			},
-	{ do_sea,		SIGBUS,  0,		"level 0 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_sea,		SIGBUS,  0,		"level 1 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_sea,		SIGBUS,  0,		"level 2 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_sea,		SIGBUS,  0,		"level 3 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
-	{ do_bad,		SIGBUS,  0,		"unknown 32"			},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"synchronous external abort"	},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 17"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 18"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 19"			},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 0 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 1 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 2 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 3 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"synchronous parity or ECC error" },	// Reserved when RAS is implemented
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 25"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 26"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 27"			},
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 0 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 1 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 2 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGBUS,  BUS_FIXME,	"level 3 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 32"			},
 	{ do_alignment_fault,	SIGBUS,  BUS_ADRALN,	"alignment fault"		},
-	{ do_bad,		SIGBUS,  0,		"unknown 34"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 35"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 36"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 37"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 38"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 39"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 40"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 41"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 42"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 43"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 44"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 45"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 46"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 47"			},
-	{ do_bad,		SIGBUS,  0,		"TLB conflict abort"		},
-	{ do_bad,		SIGBUS,  0,		"Unsupported atomic hardware update fault"	},
-	{ do_bad,		SIGBUS,  0,		"unknown 50"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 51"			},
-	{ do_bad,		SIGBUS,  0,		"implementation fault (lockdown abort)" },
-	{ do_bad,		SIGBUS,  0,		"implementation fault (unsupported exclusive)" },
-	{ do_bad,		SIGBUS,  0,		"unknown 54"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 55"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 56"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 57"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 58" 			},
-	{ do_bad,		SIGBUS,  0,		"unknown 59"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 60"			},
-	{ do_bad,		SIGBUS,  0,		"section domain fault"		},
-	{ do_bad,		SIGBUS,  0,		"page domain fault"		},
-	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 34"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 35"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 36"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 37"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 38"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 39"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 40"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 41"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 42"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 43"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 44"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 45"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 46"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 47"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"TLB conflict abort"		},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"Unsupported atomic hardware update fault"	},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 50"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 51"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"implementation fault (lockdown abort)" },
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"implementation fault (unsupported exclusive)" },
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 54"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 55"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 56"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 57"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 58" 			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 59"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 60"			},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"section domain fault"		},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"page domain fault"		},
+	{ do_bad,		SIGBUS,  BUS_FIXME,	"unknown 63"			},
 };
 
 int handle_guest_sea(phys_addr_t addr, unsigned int esr)
@@ -739,11 +739,11 @@ static struct fault_info __refdata debug_fault_info[] = {
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware breakpoint"	},
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware single-step"	},
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware watchpoint"	},
-	{ do_bad,	SIGBUS,		0,		"unknown 3"		},
+	{ do_bad,	SIGBUS,		BUS_FIXME,	"unknown 3"		},
 	{ do_bad,	SIGTRAP,	TRAP_BRKPT,	"aarch32 BKPT"		},
-	{ do_bad,	SIGTRAP,	0,		"aarch32 vector catch"	},
+	{ do_bad,	SIGTRAP,	TRAP_FIXME,	"aarch32 vector catch"	},
 	{ early_brk64,	SIGTRAP,	TRAP_BRKPT,	"aarch64 BRK"		},
-	{ do_bad,	SIGBUS,		0,		"unknown 7"		},
+	{ do_bad,	SIGBUS,		BUS_FIXME,	"unknown 7"		},
 };
 
 void __init hook_debug_fault_code(int nr,

commit 0f15adbb2861ce6f75ccfc5a92b19eae0ef327d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 3 11:17:58 2018 +0000

    arm64: Add skeleton to harden the branch predictor against aliasing attacks
    
    Aliasing attacks against CPU branch predictors can allow an attacker to
    redirect speculative control flow on some CPUs and potentially divulge
    information from one context to another.
    
    This patch adds initial skeleton code behind a new Kconfig option to
    enable implementation-specific mitigations against these attacks for
    CPUs that are affected.
    
    Co-developed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 22168cd0dde7..0e671ddf4855 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -708,6 +708,23 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	arm64_notify_die("", regs, &info, esr);
 }
 
+asmlinkage void __exception do_el0_ia_bp_hardening(unsigned long addr,
+						   unsigned int esr,
+						   struct pt_regs *regs)
+{
+	/*
+	 * We've taken an instruction abort from userspace and not yet
+	 * re-enabled IRQs. If the address is a kernel address, apply
+	 * BP hardening prior to enabling IRQs and pre-emption.
+	 */
+	if (addr > TASK_SIZE)
+		arm64_apply_bp_hardening();
+
+	local_irq_enable();
+	do_mem_abort(addr, esr, regs);
+}
+
+
 asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   unsigned int esr,
 					   struct pt_regs *regs)

commit faa75e147b583417273902552c61cf3250a44308
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Wed Dec 13 18:36:47 2017 +0800

    arm64: fault: avoid send SIGBUS two times
    
    do_sea() calls arm64_notify_die() which will always signal
    user-space. It also returns whether APEI claimed the external
    abort as a RAS notification. If it returns failure do_mem_abort()
    will signal user-space too.
    
    do_mem_abort() wants to know if we handled the error, we always
    call arm64_notify_die() so can always return success.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 22168cd0dde7..9b7f89df49db 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -574,7 +574,6 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	struct siginfo info;
 	const struct fault_info *inf;
-	int ret = 0;
 
 	inf = esr_to_fault_info(esr);
 	pr_err("Synchronous External Abort: %s (0x%08x) at 0x%016lx\n",
@@ -589,7 +588,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 		if (interrupts_enabled(regs))
 			nmi_enter();
 
-		ret = ghes_notify_sea();
+		ghes_notify_sea();
 
 		if (interrupts_enabled(regs))
 			nmi_exit();
@@ -604,7 +603,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 		info.si_addr  = (void __user *)addr;
 	arm64_notify_die("", regs, &info, esr);
 
-	return ret;
+	return 0;
 }
 
 static const struct fault_info fault_info[] = {

commit c9b012e5f4a1d01dfa8abc6318211a67ba7d5db2
Merge: b293fca43be5 6cfa7cc46b1a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 10:56:56 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "The big highlight is support for the Scalable Vector Extension (SVE)
      which required extensive ABI work to ensure we don't break existing
      applications by blowing away their signal stack with the rather large
      new vector context (<= 2 kbit per vector register). There's further
      work to be done optimising things like exception return, but the ABI
      is solid now.
    
      Much of the line count comes from some new PMU drivers we have, but
      they're pretty self-contained and I suspect we'll have more of them in
      future.
    
      Plenty of acronym soup here:
    
       - initial support for the Scalable Vector Extension (SVE)
    
       - improved handling for SError interrupts (required to handle RAS
         events)
    
       - enable GCC support for 128-bit integer types
    
       - remove kernel text addresses from backtraces and register dumps
    
       - use of WFE to implement long delay()s
    
       - ACPI IORT updates from Lorenzo Pieralisi
    
       - perf PMU driver for the Statistical Profiling Extension (SPE)
    
       - perf PMU driver for Hisilicon's system PMUs
    
       - misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (97 commits)
      arm64: Make ARMV8_DEPRECATED depend on SYSCTL
      arm64: Implement __lshrti3 library function
      arm64: support __int128 on gcc 5+
      arm64/sve: Add documentation
      arm64/sve: Detect SVE and activate runtime support
      arm64/sve: KVM: Hide SVE from CPU features exposed to guests
      arm64/sve: KVM: Treat guest SVE use as undefined instruction execution
      arm64/sve: KVM: Prevent guests from using SVE
      arm64/sve: Add sysctl to set the default vector length for new processes
      arm64/sve: Add prctl controls for userspace vector length management
      arm64/sve: ptrace and ELF coredump support
      arm64/sve: Preserve SVE registers around EFI runtime service calls
      arm64/sve: Preserve SVE registers around kernel-mode NEON use
      arm64/sve: Probe SVE capabilities and usable vector lengths
      arm64: cpufeature: Move sys_caps_initialised declarations
      arm64/sve: Backend logic for setting the vector length
      arm64/sve: Signal handling support
      arm64/sve: Support vector length resetting for new processes
      arm64/sve: Core task context handling
      arm64/sve: Low-level CPU setup
      ...

commit 80b6eb04b5d05a472a37ae33647b213dd04e59b6
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 31 15:56:11 2017 +0000

    arm64: Don't walk page table for user faults in do_mem_abort
    
    Commit 42dbf54e8890 ("arm64: consistently log ESR and page table")
    dumps page table entries for user faults hitting do_bad entries in the
    fault handler table. Whilst this shouldn't really happen in practice,
    it's not beyond the realms of possibility if e.g. running an old kernel
    on a new CPU.
    
    Generally, we want to avoid exposing physical addresses under the control
    of userspace (see commit bf396c09c24 ("arm64: mm: don't print out page
    table entries on EL0 faults")), so walk the page tables only on exceptions
    from EL1.
    
    Reported-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6ee22cd8a249..a2a1a6c83da1 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -698,7 +698,8 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 
 	mem_abort_decode(esr);
 
-	show_pte(addr);
+	if (!user_mode(regs))
+		show_pte(addr);
 
 	info.si_signo = inf->sig;
 	info.si_errno = 0;

commit 42dbf54e88906ffb8ee992c14e45dd36483f5011
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 19 11:19:55 2017 +0100

    arm64: consistently log ESR and page table
    
    When we take a fault we can't handle, we try to dump some relevant
    information, but we're not consistent about doing so.
    
    In do_mem_abort(), we log the full ESR, but don't dump a page table
    walk. In __do_kernel_fault, we dump an attempted decoding of the ESR
    (but not the ESR itself) along with a page table walk.
    
    Let's try to make things more consistent by dumping the full ESR in
    mem_abort_decode(), and having do_mem_abort dump a page table walk. The
    existing dump of the ESR in do_mem_abort() is rendered redundant, and
    removed.
    
    Tested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Julien Thierry <julien.thierry@arm.com>
    Cc: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 84a586dd063a..6ee22cd8a249 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -109,6 +109,7 @@ static void mem_abort_decode(unsigned int esr)
 {
 	pr_alert("Mem abort info:\n");
 
+	pr_alert("  ESR = 0x%08x\n", esr);
 	pr_alert("  Exception class = %s, IL = %u bits\n",
 		 esr_get_class_string(esr),
 		 (esr & ESR_ELx_IL) ? 32 : 16);
@@ -692,11 +693,13 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	if (!inf->fn(addr, esr, regs))
 		return;
 
-	pr_alert("Unhandled fault: %s (0x%08x) at 0x%016lx\n",
-		 inf->name, esr, addr);
+	pr_alert("Unhandled fault: %s at 0x%016lx\n",
+		 inf->name, addr);
 
 	mem_abort_decode(esr);
 
+	show_pte(addr);
+
 	info.si_signo = inf->sig;
 	info.si_errno = 0;
 	info.si_code  = inf->code;

commit 3f7c86b2382ea721c4fe9a7532bf5dc1b414ec1a
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Oct 17 14:11:30 2017 +0100

    arm64: Update fault_info table with new exception types
    
    Based on: ARM Architecture Reference Manual, ARMv8 (DDI 0487B.b).
    
    ARMv8.1 introduces the optional feature ARMv8.1-TTHM which can trigger a
    new type of memory abort. This exception is triggered when hardware update
    of page table flags is not atomic in regards to other memory accesses.
    Replace the corresponding unknown entry with a more accurate one.
    
    Cf: Section D10.2.28 ESR_ELx, Exception Syndrome Register (p D10-2381),
    section D4.4.11 Restriction on memory types for hardware updates on page
    tables (p D4-2116 - D4-2117).
    
    ARMv8.2 does not add new exception types, however it is worth mentioning
    that when obligatory feature RAS (optional for ARMv8.{0,1}) is implemented,
    exceptions related to "Synchronous parity or ECC error on memory access,
    not on translation table walk" become reserved and should not occur.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index d224c9d384a1..84a586dd063a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -631,14 +631,14 @@ static const struct fault_info fault_info[] = {
 	{ do_sea,		SIGBUS,  0,		"level 1 (translation table walk)"	},
 	{ do_sea,		SIGBUS,  0,		"level 2 (translation table walk)"	},
 	{ do_sea,		SIGBUS,  0,		"level 3 (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"synchronous parity or ECC error" },
+	{ do_sea,		SIGBUS,  0,		"synchronous parity or ECC error" },	// Reserved when RAS is implemented
 	{ do_bad,		SIGBUS,  0,		"unknown 25"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 26"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 27"			},
-	{ do_sea,		SIGBUS,  0,		"level 0 synchronous parity error (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"level 1 synchronous parity error (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"level 2 synchronous parity error (translation table walk)"	},
-	{ do_sea,		SIGBUS,  0,		"level 3 synchronous parity error (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 0 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGBUS,  0,		"level 1 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGBUS,  0,		"level 2 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
+	{ do_sea,		SIGBUS,  0,		"level 3 synchronous parity error (translation table walk)"	},	// Reserved when RAS is implemented
 	{ do_bad,		SIGBUS,  0,		"unknown 32"			},
 	{ do_alignment_fault,	SIGBUS,  BUS_ADRALN,	"alignment fault"		},
 	{ do_bad,		SIGBUS,  0,		"unknown 34"			},
@@ -656,7 +656,7 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGBUS,  0,		"unknown 46"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 47"			},
 	{ do_bad,		SIGBUS,  0,		"TLB conflict abort"		},
-	{ do_bad,		SIGBUS,  0,		"unknown 49"			},
+	{ do_bad,		SIGBUS,  0,		"Unsupported atomic hardware update fault"	},
 	{ do_bad,		SIGBUS,  0,		"unknown 50"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 51"			},
 	{ do_bad,		SIGBUS,  0,		"implementation fault (lockdown abort)" },

commit 0a6de8b8668a2cfc0912a1d7df21107e1a075a3a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 2 12:42:00 2017 +0100

    arm64: fix misleading data abort decoding
    
    Currently data_abort_decode() dumps the ISS field as a decimal value
    with a '0x' prefix, which is somewhat misleading.
    
    Fix it to print as hexadecimal, as was intended.
    
    Fixes: 1f9b8936f36f4a8e ("arm64: Decode information from ESR upon mem faults")
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 2069e9bc0fca..b64958b23a7f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -97,7 +97,7 @@ static void data_abort_decode(unsigned int esr)
 			 (esr & ESR_ELx_SF) >> ESR_ELx_SF_SHIFT,
 			 (esr & ESR_ELx_AR) >> ESR_ELx_AR_SHIFT);
 	} else {
-		pr_alert("  ISV = 0, ISS = 0x%08lu\n", esr & ESR_ELx_ISS_MASK);
+		pr_alert("  ISV = 0, ISS = 0x%08lx\n", esr & ESR_ELx_ISS_MASK);
 	}
 
 	pr_alert("  CM = %lu, WnR = %lu\n",

commit f67d5c4fbe8051af33e9109aa3d3024602d29474
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Sep 22 11:01:26 2017 +0100

    arm64: mm: Remove useless and wrong comments from fault.c
    
    Fault.c seems to be a magnet for useless and wrong comments, largely
    due to its ancestry in other architectures where the code has since
    moved on, but the comments have remained intact.
    
    This patch removes both useless and incorrect comments, leaving only
    those that say something correct and relevant.
    
    Reported-by: Wenjia Zhou <zhiyuan_zhu@htc.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 2069e9bc0fca..d224c9d384a1 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -105,9 +105,6 @@ static void data_abort_decode(unsigned int esr)
 		 (esr & ESR_ELx_WNR) >> ESR_ELx_WNR_SHIFT);
 }
 
-/*
- * Decode mem abort information
- */
 static void mem_abort_decode(unsigned int esr)
 {
 	pr_alert("Mem abort info:\n");
@@ -249,9 +246,6 @@ static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs,
 	return false;
 }
 
-/*
- * The kernel tried to access some page that wasn't present.
- */
 static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 			      struct pt_regs *regs)
 {
@@ -264,9 +258,6 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	if (!is_el1_instruction_abort(esr) && fixup_exception(regs))
 		return;
 
-	/*
-	 * No handler, we'll have to terminate things with extreme prejudice.
-	 */
 	bust_spinlocks(1);
 
 	if (is_permission_fault(esr, regs, addr)) {
@@ -291,10 +282,6 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	do_exit(SIGKILL);
 }
 
-/*
- * Something tried to access memory that isn't in our memory map. User mode
- * accesses just cause a SIGSEGV
- */
 static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 			    unsigned int esr, unsigned int sig, int code,
 			    struct pt_regs *regs, int fault)
@@ -559,23 +546,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	return 0;
 }
 
-/*
- * First Level Translation Fault Handler
- *
- * We enter here because the first level page table doesn't contain a valid
- * entry for the address.
- *
- * If the address is in kernel space (>= TASK_SIZE), then we are probably
- * faulting in the vmalloc() area.
- *
- * If the init_task's first level page tables contains the relevant entry, we
- * copy the it to this task.  If not, we send the process a signal, fixup the
- * exception, or oops the kernel.
- *
- * NOTE! We MUST NOT take any locks for this case. We may be in an interrupt
- * or a critical region, and should only copy the information from the master
- * page table, nothing more.
- */
 static int __kprobes do_translation_fault(unsigned long addr,
 					  unsigned int esr,
 					  struct pt_regs *regs)
@@ -594,18 +564,11 @@ static int do_alignment_fault(unsigned long addr, unsigned int esr,
 	return 0;
 }
 
-/*
- * This abort handler always returns "fault".
- */
 static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
-	return 1;
+	return 1; /* "fault" */
 }
 
-/*
- * This abort handler deals with Synchronous External Abort.
- * It calls notifiers, and then returns "fault".
- */
 static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	struct siginfo info;
@@ -710,13 +673,6 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
 };
 
-/*
- * Handle Synchronous External Aborts that occur in a guest kernel.
- *
- * The return value will be zero if the SEA was successfully handled
- * and non-zero if there was an error processing the error or there was
- * no error to process.
- */
 int handle_guest_sea(phys_addr_t addr, unsigned int esr)
 {
 	int ret = -ENOENT;
@@ -727,9 +683,6 @@ int handle_guest_sea(phys_addr_t addr, unsigned int esr)
 	return ret;
 }
 
-/*
- * Dispatch a data abort to the relevant handler.
- */
 asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 					 struct pt_regs *regs)
 {
@@ -751,9 +704,6 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	arm64_notify_die("", regs, &info, esr);
 }
 
-/*
- * Handle stack alignment exceptions.
- */
 asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   unsigned int esr,
 					   struct pt_regs *regs)

commit 760bfb47c36a07741a089bf6a28e854ffbee7dc9
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Sep 29 12:27:41 2017 +0100

    arm64: fault: Route pte translation faults via do_translation_fault
    
    We currently route pte translation faults via do_page_fault, which elides
    the address check against TASK_SIZE before invoking the mm fault handling
    code. However, this can cause issues with the path walking code in
    conjunction with our word-at-a-time implementation because
    load_unaligned_zeropad can end up faulting in kernel space if it reads
    across a page boundary and runs into a page fault (e.g. by attempting to
    read from a guard region).
    
    In the case of such a fault, load_unaligned_zeropad has registered a
    fixup to shift the valid data and pad with zeroes, however the abort is
    reported as a level 3 translation fault and we dispatch it straight to
    do_page_fault, despite it being a kernel address. This results in calling
    a sleeping function from atomic context:
    
      BUG: sleeping function called from invalid context at arch/arm64/mm/fault.c:313
      in_atomic(): 0, irqs_disabled(): 0, pid: 10290
      Internal error: Oops - BUG: 0 [#1] PREEMPT SMP
      [...]
      [<ffffff8e016cd0cc>] ___might_sleep+0x134/0x144
      [<ffffff8e016cd158>] __might_sleep+0x7c/0x8c
      [<ffffff8e016977f0>] do_page_fault+0x140/0x330
      [<ffffff8e01681328>] do_mem_abort+0x54/0xb0
      Exception stack(0xfffffffb20247a70 to 0xfffffffb20247ba0)
      [...]
      [<ffffff8e016844fc>] el1_da+0x18/0x78
      [<ffffff8e017f399c>] path_parentat+0x44/0x88
      [<ffffff8e017f4c9c>] filename_parentat+0x5c/0xd8
      [<ffffff8e017f5044>] filename_create+0x4c/0x128
      [<ffffff8e017f59e4>] SyS_mkdirat+0x50/0xc8
      [<ffffff8e01684e30>] el0_svc_naked+0x24/0x28
      Code: 36380080 d5384100 f9400800 9402566d (d4210000)
      ---[ end trace 2d01889f2bca9b9f ]---
    
    Fix this by dispatching all translation faults to do_translation_faults,
    which avoids invoking the page fault logic for faults on kernel addresses.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Ankit Jain <ankijain@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 89993c4be1be..2069e9bc0fca 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -651,7 +651,7 @@ static const struct fault_info fault_info[] = {
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 0 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
-	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
+	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
 	{ do_bad,		SIGBUS,  0,		"unknown 8"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},

commit 04759194dc447ff0b9ef35bc641ce3bb076c2930
Merge: 9e85ae6af6e9 d1be5c99a034
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 5 09:53:37 2017 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - VMAP_STACK support, allowing the kernel stacks to be allocated in the
       vmalloc space with a guard page for trapping stack overflows. One of
       the patches introduces THREAD_ALIGN and changes the generic
       alloc_thread_stack_node() to use this instead of THREAD_SIZE (no
       functional change for other architectures)
    
     - Contiguous PTE hugetlb support re-enabled (after being reverted a
       couple of times). We now have the semantics agreed in the generic mm
       layer together with API improvements so that the architecture code
       can detect between contiguous and non-contiguous huge PTEs
    
     - Initial support for persistent memory on ARM: DC CVAP instruction
       exposed to user space (HWCAP) and the in-kernel pmem API implemented
    
     - raid6 improvements for arm64: faster algorithm for the delta syndrome
       and implementation of the recovery routines using Neon
    
     - FP/SIMD refactoring and removal of support for Neon in interrupt
       context. This is in preparation for full SVE support
    
     - PTE accessors converted from inline asm to cmpxchg so that we can use
       LSE atomics if available (ARMv8.1)
    
     - Perf support for Cortex-A35 and A73
    
     - Non-urgent fixes and cleanups
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (75 commits)
      arm64: cleanup {COMPAT_,}SET_PERSONALITY() macro
      arm64: introduce separated bits for mm_context_t flags
      arm64: hugetlb: Cleanup setup_hugepagesz
      arm64: Re-enable support for contiguous hugepages
      arm64: hugetlb: Override set_huge_swap_pte_at() to support contiguous hugepages
      arm64: hugetlb: Override huge_pte_clear() to support contiguous hugepages
      arm64: hugetlb: Handle swap entries in huge_pte_offset() for contiguous hugepages
      arm64: hugetlb: Add break-before-make logic for contiguous entries
      arm64: hugetlb: Spring clean huge pte accessors
      arm64: hugetlb: Introduce pte_pgprot helper
      arm64: hugetlb: set_huge_pte_at Add WARN_ON on !pte_present
      arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
      arm64: dma-mapping: Mark atomic_pool as __ro_after_init
      arm64: dma-mapping: Do not pass data to gen_pool_set_algo()
      arm64: Remove the !CONFIG_ARM64_HW_AFDBM alternative code paths
      arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()
      arm64: Move PTE_RDONLY bit handling out of set_pte_at()
      kvm: arm64: Convert kvm_set_s2pte_readonly() from inline asm to cmpxchg()
      arm64: Convert pte handling from inline asm to using (cmp)xchg
      arm64: neon/efi: Make EFI fpsimd save/restore variables static
      ...

commit 289d07a2dc6c6b6f3e4b8a62669320d99dbe6c3d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jul 11 15:19:22 2017 +0100

    arm64: mm: abort uaccess retries upon fatal signal
    
    When there's a fatal signal pending, arm64's do_page_fault()
    implementation returns 0. The intent is that we'll return to the
    faulting userspace instruction, delivering the signal on the way.
    
    However, if we take a fatal signal during fixing up a uaccess, this
    results in a return to the faulting kernel instruction, which will be
    instantly retried, resulting in the same fault being taken forever. As
    the task never reaches userspace, the signal is not delivered, and the
    task is left unkillable. While the task is stuck in this state, it can
    inhibit the forward progress of the system.
    
    To avoid this, we must ensure that when a fatal signal is pending, we
    apply any necessary fixup for a faulting kernel instruction. Thus we
    will return to an error path, and it is up to that code to make forward
    progress towards delivering the fatal signal.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: stable@vger.kernel.org
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 2509e4fe6992..1f22a41565a3 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -435,8 +435,11 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 * the mmap_sem because it would already be released
 		 * in __lock_page_or_retry in mm/filemap.c.
 		 */
-		if (fatal_signal_pending(current))
+		if (fatal_signal_pending(current)) {
+			if (!user_mode(regs))
+				goto no_context;
 			return 0;
+		}
 
 		/*
 		 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of

commit af29678fe785ad79e7386e97b57093482f0dd7c4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jul 6 11:53:08 2017 +0100

    arm64: Remove the !CONFIG_ARM64_HW_AFDBM alternative code paths
    
    Since the pte handling for hardware AF/DBM works even when the hardware
    feature is not present, make the pte accessors implementation permanent
    and remove the corresponding #ifdefs. The Kconfig option is kept as it
    can still be used to disable the feature at the hardware level.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index f75ed5c4b994..778d0bb89551 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -183,7 +183,6 @@ void show_pte(unsigned long addr)
 	pr_cont("\n");
 }
 
-#ifdef CONFIG_ARM64_HW_AFDBM
 /*
  * This function sets the access flags (dirty, accessed), as well as write
  * permission, and only to a more permissive setting.
@@ -225,7 +224,6 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 	flush_tlb_fix_spurious_fault(vma, address);
 	return 1;
 }
-#endif
 
 static bool is_el1_instruction_abort(unsigned int esr)
 {

commit 73e86cb03cf2ec0aa3789dc8621c6d53619cac5e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 4 19:04:18 2017 +0100

    arm64: Move PTE_RDONLY bit handling out of set_pte_at()
    
    Currently PTE_RDONLY is treated as a hardware only bit and not handled
    by the pte_mkwrite(), pte_wrprotect() or the user PAGE_* definitions.
    The set_pte_at() function is responsible for setting this bit based on
    the write permission or dirty state. This patch moves the PTE_RDONLY
    handling out of set_pte_at into the pte_mkwrite()/pte_wrprotect()
    functions. The PAGE_* definitions to need to be updated to explicitly
    include PTE_RDONLY when !PTE_WRITE.
    
    The patch also removes the redundant PAGE_COPY(_EXEC) definitions as
    they are identical to the corresponding PAGE_READONLY(_EXEC).
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 430eaf82da49..f75ed5c4b994 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -204,11 +204,7 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 		return 0;
 
 	/* only preserve the access flags and write permission */
-	pte_val(entry) &= PTE_AF | PTE_WRITE | PTE_DIRTY;
-
-	/* set PTE_RDONLY if actual read-only or clean PTE */
-	if (!pte_write(entry) || !pte_sw_dirty(entry))
-		entry = pte_set_rdonly(entry);
+	pte_val(entry) &= PTE_RDONLY | PTE_AF | PTE_WRITE | PTE_DIRTY;
 
 	/*
 	 * Setting the flags must be done atomically to avoid racing with the

commit 3bbf7157ac66a88d94b291d4d5e2b2a9319a0f90
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jun 26 14:27:36 2017 +0100

    arm64: Convert pte handling from inline asm to using (cmp)xchg
    
    With the support for hardware updates of the access and dirty states,
    the following pte handling functions had to be implemented using
    exclusives: __ptep_test_and_clear_young(), ptep_get_and_clear(),
    ptep_set_wrprotect() and ptep_set_access_flags(). To take advantage of
    the LSE atomic instructions and also make the code cleaner, convert
    these pte functions to use the more generic cmpxchg()/xchg().
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 52ee273afeec..430eaf82da49 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -34,6 +34,7 @@
 #include <linux/hugetlb.h>
 
 #include <asm/bug.h>
+#include <asm/cmpxchg.h>
 #include <asm/cpufeature.h>
 #include <asm/exception.h>
 #include <asm/debug-monitors.h>
@@ -197,8 +198,7 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 			  unsigned long address, pte_t *ptep,
 			  pte_t entry, int dirty)
 {
-	pteval_t old_pteval;
-	unsigned int tmp;
+	pteval_t old_pteval, pteval;
 
 	if (pte_same(*ptep, entry))
 		return 0;
@@ -208,7 +208,7 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 
 	/* set PTE_RDONLY if actual read-only or clean PTE */
 	if (!pte_write(entry) || !pte_sw_dirty(entry))
-		pte_val(entry) |= PTE_RDONLY;
+		entry = pte_set_rdonly(entry);
 
 	/*
 	 * Setting the flags must be done atomically to avoid racing with the
@@ -217,16 +217,14 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 	 * (calculated as: a & b == ~(~a | ~b)).
 	 */
 	pte_val(entry) ^= PTE_RDONLY;
-	asm volatile("//	ptep_set_access_flags\n"
-	"	prfm	pstl1strm, %2\n"
-	"1:	ldxr	%0, %2\n"
-	"	eor	%0, %0, %3		// negate PTE_RDONLY in *ptep\n"
-	"	orr	%0, %0, %4		// set flags\n"
-	"	eor	%0, %0, %3		// negate final PTE_RDONLY\n"
-	"	stxr	%w1, %0, %2\n"
-	"	cbnz	%w1, 1b\n"
-	: "=&r" (old_pteval), "=&r" (tmp), "+Q" (pte_val(*ptep))
-	: "L" (PTE_RDONLY), "r" (pte_val(entry)));
+	pteval = READ_ONCE(pte_val(*ptep));
+	do {
+		old_pteval = pteval;
+		pteval ^= PTE_RDONLY;
+		pteval |= pte_val(entry);
+		pteval ^= PTE_RDONLY;
+		pteval = cmpxchg_relaxed(&pte_val(*ptep), old_pteval, pteval);
+	} while (pteval != old_pteval);
 
 	flush_tlb_fix_spurious_fault(vma, address);
 	return 1;

commit 1f9b8936f36f4a8e1d9923f5d03295d668cdf098
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Fri Aug 4 09:31:42 2017 +0100

    arm64: Decode information from ESR upon mem faults
    
    When receiving unhandled faults from the CPU, description is very sparse.
    Adding information about faults decoded from ESR.
    
    Added defines to esr.h corresponding ESR fields. Values are based on ARM
    Archtecture Reference Manual (DDI 0487B.a), section D7.2.28 ESR_ELx, Exception
    Syndrome Register (ELx) (pages D7-2275 to D7-2280).
    
    New output is of the form:
    [   77.818059] Mem abort info:
    [   77.820826]   Exception class = DABT (current EL), IL = 32 bits
    [   77.826706]   SET = 0, FnV = 0
    [   77.829742]   EA = 0, S1PTW = 0
    [   77.832849] Data abort info:
    [   77.835713]   ISV = 0, ISS = 0x00000070
    [   77.839522]   CM = 0, WnR = 1
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    [catalin.marinas@arm.com: fix "%lu" in a pr_alert() call]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 2509e4fe6992..52ee273afeec 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -82,6 +82,49 @@ static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
 }
 #endif
 
+static void data_abort_decode(unsigned int esr)
+{
+	pr_alert("Data abort info:\n");
+
+	if (esr & ESR_ELx_ISV) {
+		pr_alert("  Access size = %u byte(s)\n",
+			 1U << ((esr & ESR_ELx_SAS) >> ESR_ELx_SAS_SHIFT));
+		pr_alert("  SSE = %lu, SRT = %lu\n",
+			 (esr & ESR_ELx_SSE) >> ESR_ELx_SSE_SHIFT,
+			 (esr & ESR_ELx_SRT_MASK) >> ESR_ELx_SRT_SHIFT);
+		pr_alert("  SF = %lu, AR = %lu\n",
+			 (esr & ESR_ELx_SF) >> ESR_ELx_SF_SHIFT,
+			 (esr & ESR_ELx_AR) >> ESR_ELx_AR_SHIFT);
+	} else {
+		pr_alert("  ISV = 0, ISS = 0x%08lu\n", esr & ESR_ELx_ISS_MASK);
+	}
+
+	pr_alert("  CM = %lu, WnR = %lu\n",
+		 (esr & ESR_ELx_CM) >> ESR_ELx_CM_SHIFT,
+		 (esr & ESR_ELx_WNR) >> ESR_ELx_WNR_SHIFT);
+}
+
+/*
+ * Decode mem abort information
+ */
+static void mem_abort_decode(unsigned int esr)
+{
+	pr_alert("Mem abort info:\n");
+
+	pr_alert("  Exception class = %s, IL = %u bits\n",
+		 esr_get_class_string(esr),
+		 (esr & ESR_ELx_IL) ? 32 : 16);
+	pr_alert("  SET = %lu, FnV = %lu\n",
+		 (esr & ESR_ELx_SET_MASK) >> ESR_ELx_SET_SHIFT,
+		 (esr & ESR_ELx_FnV) >> ESR_ELx_FnV_SHIFT);
+	pr_alert("  EA = %lu, S1PTW = %lu\n",
+		 (esr & ESR_ELx_EA) >> ESR_ELx_EA_SHIFT,
+		 (esr & ESR_ELx_S1PTW) >> ESR_ELx_S1PTW_SHIFT);
+
+	if (esr_is_data_abort(esr))
+		data_abort_decode(esr);
+}
+
 /*
  * Dump out the page tables associated with 'addr' in the currently active mm.
  */
@@ -248,6 +291,8 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
 	pr_alert("Unable to handle kernel %s at virtual address %08lx\n", msg,
 		 addr);
 
+	mem_abort_decode(esr);
+
 	show_pte(addr);
 	die("Oops", regs, esr);
 	bust_spinlocks(0);
@@ -702,6 +747,8 @@ asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 	pr_alert("Unhandled fault: %s (0x%08x) at 0x%016lx\n",
 		 inf->name, esr, addr);
 
+	mem_abort_decode(esr);
+
 	info.si_signo = inf->sig;
 	info.si_errno = 0;
 	info.si_code  = inf->code;

commit 6d332747fa5f0a6843b56b5b129168ba909336d1
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 25 14:53:03 2017 +0100

    arm64: Fix potential race with hardware DBM in ptep_set_access_flags()
    
    In a system with DBM (dirty bit management) capable agents there is a
    possible race between a CPU executing ptep_set_access_flags() (maybe
    non-DBM capable) and a hardware update of the dirty state (clearing of
    PTE_RDONLY). The scenario:
    
    a) the pte is writable (PTE_WRITE set), clean (PTE_RDONLY set) and old
       (PTE_AF clear)
    b) ptep_set_access_flags() is called as a result of a read access and it
       needs to set the pte to writable, clean and young (PTE_AF set)
    c) a DBM-capable agent, as a result of a different write access, is
       marking the entry as young (setting PTE_AF) and dirty (clearing
       PTE_RDONLY)
    
    The current ptep_set_access_flags() implementation would set the
    PTE_RDONLY bit in the resulting value overriding the DBM update and
    losing the dirty state.
    
    This patch fixes such race by setting PTE_RDONLY to the most permissive
    (lowest value) of the current entry and the new one.
    
    Fixes: 66dbd6e61a52 ("arm64: Implement ptep_set_access_flags() for hardware AF/DBM")
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c7861c9864e6..2509e4fe6992 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -163,26 +163,27 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 	/* only preserve the access flags and write permission */
 	pte_val(entry) &= PTE_AF | PTE_WRITE | PTE_DIRTY;
 
-	/*
-	 * PTE_RDONLY is cleared by default in the asm below, so set it in
-	 * back if necessary (read-only or clean PTE).
-	 */
+	/* set PTE_RDONLY if actual read-only or clean PTE */
 	if (!pte_write(entry) || !pte_sw_dirty(entry))
 		pte_val(entry) |= PTE_RDONLY;
 
 	/*
 	 * Setting the flags must be done atomically to avoid racing with the
-	 * hardware update of the access/dirty state.
+	 * hardware update of the access/dirty state. The PTE_RDONLY bit must
+	 * be set to the most permissive (lowest value) of *ptep and entry
+	 * (calculated as: a & b == ~(~a | ~b)).
 	 */
+	pte_val(entry) ^= PTE_RDONLY;
 	asm volatile("//	ptep_set_access_flags\n"
 	"	prfm	pstl1strm, %2\n"
 	"1:	ldxr	%0, %2\n"
-	"	and	%0, %0, %3		// clear PTE_RDONLY\n"
+	"	eor	%0, %0, %3		// negate PTE_RDONLY in *ptep\n"
 	"	orr	%0, %0, %4		// set flags\n"
+	"	eor	%0, %0, %3		// negate final PTE_RDONLY\n"
 	"	stxr	%w1, %0, %2\n"
 	"	cbnz	%w1, 1b\n"
 	: "=&r" (old_pteval), "=&r" (tmp), "+Q" (pte_val(*ptep))
-	: "L" (~PTE_RDONLY), "r" (pte_val(entry)));
+	: "L" (PTE_RDONLY), "r" (pte_val(entry)));
 
 	flush_tlb_fix_spurious_fault(vma, address);
 	return 1;

commit 3edb1dd13ce6f6480c1f2bffc47a49cf959fa9cb
Merge: 9ad95c46c18b 77b246b32b2c
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 26 10:54:27 2017 +0100

    Merge branch 'aarch64/for-next/ras-apei' into aarch64/for-next/core
    
    Merge in arm64 ACPI RAS support (APEI/GHES) from Tyler Baicar.

commit 621f48e40ee9b0100a802531069166d7d94796e0
Author: Tyler Baicar <tbaicar@codeaurora.org>
Date:   Wed Jun 21 12:17:14 2017 -0600

    arm/arm64: KVM: add guest SEA support
    
    Currently external aborts are unsupported by the guest abort
    handling. Add handling for SEAs so that the host kernel reports
    SEAs which occur in the guest kernel.
    
    When an SEA occurs in the guest kernel, the guest exits and is
    routed to kvm_handle_guest_abort(). Prior to this patch, a print
    message of an unsupported FSC would be printed and nothing else
    would happen. With this patch, the code gets routed to the APEI
    handling of SEAs in the host kernel to report the SEA information.
    
    Signed-off-by: Tyler Baicar <tbaicar@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 07481af15fd7..a8c9f2db20ba 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -532,6 +532,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	struct siginfo info;
 	const struct fault_info *inf;
+	int ret = 0;
 
 	inf = esr_to_fault_info(esr);
 	pr_err("Synchronous External Abort: %s (0x%08x) at 0x%016lx\n",
@@ -546,7 +547,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 		if (interrupts_enabled(regs))
 			nmi_enter();
 
-		ghes_notify_sea();
+		ret = ghes_notify_sea();
 
 		if (interrupts_enabled(regs))
 			nmi_exit();
@@ -561,7 +562,7 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 		info.si_addr  = (void __user *)addr;
 	arm64_notify_die("", regs, &info, esr);
 
-	return 0;
+	return ret;
 }
 
 static const struct fault_info fault_info[] = {
@@ -631,6 +632,23 @@ static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
 };
 
+/*
+ * Handle Synchronous External Aborts that occur in a guest kernel.
+ *
+ * The return value will be zero if the SEA was successfully handled
+ * and non-zero if there was an error processing the error or there was
+ * no error to process.
+ */
+int handle_guest_sea(phys_addr_t addr, unsigned int esr)
+{
+	int ret = -ENOENT;
+
+	if (IS_ENABLED(CONFIG_ACPI_APEI_SEA))
+		ret = ghes_notify_sea();
+
+	return ret;
+}
+
 /*
  * Dispatch a data abort to the relevant handler.
  */

commit 7edda0886bc3d1e5418951558a2555af1bc73b0a
Author: Tyler Baicar <tbaicar@codeaurora.org>
Date:   Wed Jun 21 12:17:09 2017 -0600

    acpi: apei: handle SEA notification type for ARMv8
    
    ARM APEI extension proposal added SEA (Synchronous External Abort)
    notification type for ARMv8.
    Add a new GHES error source handling function for SEA. If an error
    source's notification type is SEA, then this function can be registered
    into the SEA exception handler. That way GHES will parse and report
    SEA exceptions when they occur.
    An SEA can interrupt code that had interrupts masked and is treated as
    an NMI. To aid this the page of address space for mapping APEI buffers
    while in_nmi() is always reserved, and ghes_ioremap_pfn_nmi() is
    changed to use the helper methods to find the prot_t to map with in
    the same way as ghes_ioremap_pfn_irq().
    
    Signed-off-by: Tyler Baicar <tbaicar@codeaurora.org>
    CC: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 246e64f60610..07481af15fd7 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -42,6 +42,8 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 
+#include <acpi/ghes.h>
+
 struct fault_info {
 	int	(*fn)(unsigned long addr, unsigned int esr,
 		      struct pt_regs *regs);
@@ -535,6 +537,21 @@ static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 	pr_err("Synchronous External Abort: %s (0x%08x) at 0x%016lx\n",
 		inf->name, esr, addr);
 
+	/*
+	 * Synchronous aborts may interrupt code which had interrupts masked.
+	 * Before calling out into the wider kernel tell the interested
+	 * subsystems.
+	 */
+	if (IS_ENABLED(CONFIG_ACPI_APEI_SEA)) {
+		if (interrupts_enabled(regs))
+			nmi_enter();
+
+		ghes_notify_sea();
+
+		if (interrupts_enabled(regs))
+			nmi_exit();
+	}
+
 	info.si_signo = SIGBUS;
 	info.si_errno = 0;
 	info.si_code  = 0;

commit 32015c235603a209697d1228667b1fb1cc8a8d06
Author: Tyler Baicar <tbaicar@codeaurora.org>
Date:   Wed Jun 21 12:17:08 2017 -0600

    arm64: exception: handle Synchronous External Abort
    
    SEA exceptions are often caused by an uncorrected hardware
    error, and are handled when data abort and instruction abort
    exception classes have specific values for their Fault Status
    Code.
    When SEA occurs, before killing the process, report the error
    in the kernel logs.
    Update fault_info[] with specific SEA faults so that the
    new SEA handler is used.
    
    Signed-off-by: Tyler Baicar <tbaicar@codeaurora.org>
    CC: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    [will: use NULL instead of 0 when assigning si_addr]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 37b95dff0b07..246e64f60610 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -522,6 +522,31 @@ static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 	return 1;
 }
 
+/*
+ * This abort handler deals with Synchronous External Abort.
+ * It calls notifiers, and then returns "fault".
+ */
+static int do_sea(unsigned long addr, unsigned int esr, struct pt_regs *regs)
+{
+	struct siginfo info;
+	const struct fault_info *inf;
+
+	inf = esr_to_fault_info(esr);
+	pr_err("Synchronous External Abort: %s (0x%08x) at 0x%016lx\n",
+		inf->name, esr, addr);
+
+	info.si_signo = SIGBUS;
+	info.si_errno = 0;
+	info.si_code  = 0;
+	if (esr & ESR_ELx_FnV)
+		info.si_addr = NULL;
+	else
+		info.si_addr  = (void __user *)addr;
+	arm64_notify_die("", regs, &info, esr);
+
+	return 0;
+}
+
 static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGBUS,  0,		"ttbr address size fault"	},
 	{ do_bad,		SIGBUS,  0,		"level 1 address size fault"	},
@@ -539,22 +564,22 @@ static const struct fault_info fault_info[] = {
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
-	{ do_bad,		SIGBUS,  0,		"synchronous external abort"	},
+	{ do_sea,		SIGBUS,  0,		"synchronous external abort"	},
 	{ do_bad,		SIGBUS,  0,		"unknown 17"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 18"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 19"			},
-	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error"	},
+	{ do_sea,		SIGBUS,  0,		"level 0 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 1 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 2 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 3 (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"synchronous parity or ECC error" },
 	{ do_bad,		SIGBUS,  0,		"unknown 25"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 26"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 27"			},
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
+	{ do_sea,		SIGBUS,  0,		"level 0 synchronous parity error (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 1 synchronous parity error (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 2 synchronous parity error (translation table walk)"	},
+	{ do_sea,		SIGBUS,  0,		"level 3 synchronous parity error (translation table walk)"	},
 	{ do_bad,		SIGBUS,  0,		"unknown 32"			},
 	{ do_alignment_fault,	SIGBUS,  BUS_ADRALN,	"alignment fault"		},
 	{ do_bad,		SIGBUS,  0,		"unknown 34"			},

commit 0e3a9026396cd7d0eb5777ba923a8f30a3d9db19
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Thu Jun 8 18:25:28 2017 +0100

    arm64: mm: Update perf accounting to handle poison faults
    
    Re-organise the perf accounting for fault handling in preparation for
    enabling handling of hardware poison faults in subsequent commits. The
    change updates perf accounting to be inline with the behaviour on
    x86.
    
    With this update, the perf fault accounting -
    
      * Always report PERF_COUNT_SW_PAGE_FAULTS
    
      * Doesn't report anything else for VM_FAULT_ERROR (which includes
        hwpoison faults)
    
      * Reports PERF_COUNT_SW_PAGE_FAULTS_MAJ if it's a major
        fault (indicated by VM_FAULT_MAJOR)
    
      * Otherwise, reports PERF_COUNT_SW_PAGE_FAULTS_MIN
    
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index d73e7f1fe184..ea2ea68d1bd7 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -359,7 +359,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
-	int fault, sig, code;
+	int fault, sig, code, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
@@ -398,6 +398,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			die("Accessing user space memory outside uaccess.h routines", regs, esr);
 	}
 
+	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
+
 	/*
 	 * As per x86, we may deadlock here. However, since the kernel only
 	 * validly references user space from well defined areas of the code,
@@ -421,24 +423,42 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk);
+	major |= fault & VM_FAULT_MAJOR;
 
-	/*
-	 * If we need to retry but a fatal signal is pending, handle the
-	 * signal first. We do not need to release the mmap_sem because it
-	 * would already be released in __lock_page_or_retry in mm/filemap.c.
-	 */
-	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
-		return 0;
+	if (fault & VM_FAULT_RETRY) {
+		/*
+		 * If we need to retry but a fatal signal is pending,
+		 * handle the signal first. We do not need to release
+		 * the mmap_sem because it would already be released
+		 * in __lock_page_or_retry in mm/filemap.c.
+		 */
+		if (fatal_signal_pending(current))
+			return 0;
+
+		/*
+		 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of
+		 * starvation.
+		 */
+		if (mm_flags & FAULT_FLAG_ALLOW_RETRY) {
+			mm_flags &= ~FAULT_FLAG_ALLOW_RETRY;
+			mm_flags |= FAULT_FLAG_TRIED;
+			goto retry;
+		}
+	}
+	up_read(&mm->mmap_sem);
 
 	/*
-	 * Major/minor page fault accounting is only done on the initial
-	 * attempt. If we go through a retry, it is extremely likely that the
-	 * page will be found in page cache at that point.
+	 * Handle the "normal" (no error) case first.
 	 */
-
-	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
-	if (mm_flags & FAULT_FLAG_ALLOW_RETRY) {
-		if (fault & VM_FAULT_MAJOR) {
+	if (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP |
+			      VM_FAULT_BADACCESS)))) {
+		/*
+		 * Major/minor page fault accounting is only done
+		 * once. If we go through a retry, it is extremely
+		 * likely that the page will be found in page cache at
+		 * that point.
+		 */
+		if (major) {
 			tsk->maj_flt++;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs,
 				      addr);
@@ -447,25 +467,9 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs,
 				      addr);
 		}
-		if (fault & VM_FAULT_RETRY) {
-			/*
-			 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of
-			 * starvation.
-			 */
-			mm_flags &= ~FAULT_FLAG_ALLOW_RETRY;
-			mm_flags |= FAULT_FLAG_TRIED;
-			goto retry;
-		}
-	}
-
-	up_read(&mm->mmap_sem);
 
-	/*
-	 * Handle the "normal" case first - VM_FAULT_MAJOR
-	 */
-	if (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP |
-			      VM_FAULT_BADACCESS))))
 		return 0;
+	}
 
 	/*
 	 * If we are in kernel mode at this point, we have no context to

commit e7c600f149b89e06073ab50f4f12e79828d3d2f0
Author: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
Date:   Thu Jun 8 18:25:27 2017 +0100

    arm64: hwpoison: add VM_FAULT_HWPOISON[_LARGE] handling
    
    Add VM_FAULT_HWPOISON[_LARGE] handling to the arm64 page fault
    handler. Handling of VM_FAULT_HWPOISON[_LARGE] is very similar
    to VM_FAULT_OOM, the only difference is that a different si_code
    (BUS_MCEERR_AR) is passed to user space and si_addr_lsb field is
    initialized.
    
    Signed-off-by: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
    Signed-off-by: Tyler Baicar <tbaicar@codeaurora.org>
    (fix new __do_user_fault call-site)
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Manoj Iyer <manoj.iyer@canonical.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 222427ae23d6..d73e7f1fe184 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -31,6 +31,7 @@
 #include <linux/highmem.h>
 #include <linux/perf_event.h>
 #include <linux/preempt.h>
+#include <linux/hugetlb.h>
 
 #include <asm/bug.h>
 #include <asm/cpufeature.h>
@@ -256,10 +257,11 @@ static void __do_kernel_fault(unsigned long addr, unsigned int esr,
  */
 static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 			    unsigned int esr, unsigned int sig, int code,
-			    struct pt_regs *regs)
+			    struct pt_regs *regs, int fault)
 {
 	struct siginfo si;
 	const struct fault_info *inf;
+	unsigned int lsb = 0;
 
 	if (unhandled_signal(tsk, sig) && show_unhandled_signals_ratelimited()) {
 		inf = esr_to_fault_info(esr);
@@ -277,6 +279,17 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 	si.si_errno = 0;
 	si.si_code = code;
 	si.si_addr = (void __user *)addr;
+	/*
+	 * Either small page or large page may be poisoned.
+	 * In other words, VM_FAULT_HWPOISON_LARGE and
+	 * VM_FAULT_HWPOISON are mutually exclusive.
+	 */
+	if (fault & VM_FAULT_HWPOISON_LARGE)
+		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));
+	else if (fault & VM_FAULT_HWPOISON)
+		lsb = PAGE_SHIFT;
+	si.si_addr_lsb = lsb;
+
 	force_sig_info(sig, &si, tsk);
 }
 
@@ -291,7 +304,7 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 	 */
 	if (user_mode(regs)) {
 		inf = esr_to_fault_info(esr);
-		__do_user_fault(tsk, addr, esr, inf->sig, inf->code, regs);
+		__do_user_fault(tsk, addr, esr, inf->sig, inf->code, regs, 0);
 	} else
 		__do_kernel_fault(addr, esr, regs);
 }
@@ -478,6 +491,9 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		 */
 		sig = SIGBUS;
 		code = BUS_ADRERR;
+	} else if (fault & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE)) {
+		sig = SIGBUS;
+		code = BUS_MCEERR_AR;
 	} else {
 		/*
 		 * Something tried to access memory that isn't in our memory
@@ -488,7 +504,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			SEGV_ACCERR : SEGV_MAPERR;
 	}
 
-	__do_user_fault(tsk, addr, esr, sig, code, regs);
+	__do_user_fault(tsk, addr, esr, sig, code, regs, fault);
 	return 0;
 
 no_context:

commit 1eb34b6e5160f20e1889b2551182bf4d61084d6b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon May 15 15:23:58 2017 +0100

    arm64: fault: Print info about page table structure when dumping pte
    
    Whilst debugging a remote crash, I noticed that show_pte is unhelpful
    when it comes to describing the structure of the page table being walked.
    This is easily fixed by printing out the page table (swapper vs user),
    page size and virtual address size when displaying the PGD address.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index ad7c9c94d379..222427ae23d6 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -104,7 +104,9 @@ void show_pte(unsigned long addr)
 		return;
 	}
 
-	pr_alert("pgd = %p\n", mm->pgd);
+	pr_alert("%s pgtable: %luk pages, %u-bit VAs, pgd = %p\n",
+		 mm == &init_mm ? "swapper" : "user", PAGE_SIZE / SZ_1K,
+		 VA_BITS, mm->pgd);
 	pgd = pgd_offset(mm, addr);
 	pr_alert("[%016lx] *pgd=%016llx", addr, pgd_val(*pgd));
 

commit 83016b204225d69516de3d57489783fafd6a0ecc
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Jun 9 16:35:54 2017 +0100

    arm64: mm: print file name of faulting vma
    
    Print out the name of the file associated with the vma that faulted.
    This is usually the executable or shared library name. We already print
    out the task name, but also printing the library name is useful for
    pinpointing bugs to libraries.
    
    Also print the base address and size of the vma, which together with the
    PC (printed by __show_regs) gives the offset into the library.
    
    Fault prints now look like:
    test[2361]: unhandled level 2 translation fault (11) at 0x00000012, esr 0x92000006, in libfoo.so[ffffa0145000+1000]
    
    This is already done on x86, for more details see commit 03252919b798
    ("x86: print which shared library/executable faulted in segfault etc.
    messages v3").
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index b5a1605398b7..ad7c9c94d379 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -261,9 +261,11 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 
 	if (unhandled_signal(tsk, sig) && show_unhandled_signals_ratelimited()) {
 		inf = esr_to_fault_info(esr);
-		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
+		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x",
 			tsk->comm, task_pid_nr(tsk), inf->name, sig,
 			addr, esr);
+		print_vma_addr(KERN_CONT ", in ", regs->pc);
+		pr_cont("\n");
 		__show_regs(regs);
 	}
 

commit bf396c09c2447a787d02af34cf167e953f85fa42
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Jun 9 16:35:53 2017 +0100

    arm64: mm: don't print out page table entries on EL0 faults
    
    When we take a fault from EL0 that can't be handled, we print out the
    page table entries associated with the faulting address. This allows
    userspace to print out any current page table entries, including kernel
    (TTBR1) entries. Exposing kernel mappings like this could pose a
    security risk, so don't print out page table information on EL0 faults.
    (But still print it out for EL1 faults.) This also follows the same
    behaviour as x86, printing out page table entries on kernel mode faults
    but not user mode faults.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 9d27b1720c52..b5a1605398b7 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -264,7 +264,6 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
 			tsk->comm, task_pid_nr(tsk), inf->name, sig,
 			addr, esr);
-		show_pte(addr);
 		__show_regs(regs);
 	}
 

commit 67ce16ec15ce9d97d3d85e72beabbc5d7017193e
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Jun 9 16:35:52 2017 +0100

    arm64: mm: print out correct page table entries
    
    When we take a fault that can't be handled, we print out the page table
    entries associated with the faulting address. In some cases we currently
    print out the wrong entries. For a faulting TTBR1 address, we sometimes
    print out TTBR0 table entries instead, and for a faulting TTBR0 address
    we sometimes print out TTBR1 table entries. Fix this by choosing the
    tables based on the faulting address.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [will: zero-extend addrs to 64-bit, don't walk swapper w/ TTBR0 addr]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c3f2b1048f83..9d27b1720c52 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -80,18 +80,33 @@ static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
 #endif
 
 /*
- * Dump out the page tables associated with 'addr' in mm 'mm'.
+ * Dump out the page tables associated with 'addr' in the currently active mm.
  */
-void show_pte(struct mm_struct *mm, unsigned long addr)
+void show_pte(unsigned long addr)
 {
+	struct mm_struct *mm;
 	pgd_t *pgd;
 
-	if (!mm)
+	if (addr < TASK_SIZE) {
+		/* TTBR0 */
+		mm = current->active_mm;
+		if (mm == &init_mm) {
+			pr_alert("[%016lx] user address but active_mm is swapper\n",
+				 addr);
+			return;
+		}
+	} else if (addr >= VA_START) {
+		/* TTBR1 */
 		mm = &init_mm;
+	} else {
+		pr_alert("[%016lx] address between user and kernel address ranges\n",
+			 addr);
+		return;
+	}
 
 	pr_alert("pgd = %p\n", mm->pgd);
 	pgd = pgd_offset(mm, addr);
-	pr_alert("[%08lx] *pgd=%016llx", addr, pgd_val(*pgd));
+	pr_alert("[%016lx] *pgd=%016llx", addr, pgd_val(*pgd));
 
 	do {
 		pud_t *pud;
@@ -196,8 +211,8 @@ static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs,
 /*
  * The kernel tried to access some page that wasn't present.
  */
-static void __do_kernel_fault(struct mm_struct *mm, unsigned long addr,
-			      unsigned int esr, struct pt_regs *regs)
+static void __do_kernel_fault(unsigned long addr, unsigned int esr,
+			      struct pt_regs *regs)
 {
 	const char *msg;
 
@@ -227,7 +242,7 @@ static void __do_kernel_fault(struct mm_struct *mm, unsigned long addr,
 	pr_alert("Unable to handle kernel %s at virtual address %08lx\n", msg,
 		 addr);
 
-	show_pte(mm, addr);
+	show_pte(addr);
 	die("Oops", regs, esr);
 	bust_spinlocks(0);
 	do_exit(SIGKILL);
@@ -249,7 +264,7 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
 			tsk->comm, task_pid_nr(tsk), inf->name, sig,
 			addr, esr);
-		show_pte(tsk->mm, addr);
+		show_pte(addr);
 		__show_regs(regs);
 	}
 
@@ -265,7 +280,6 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	struct task_struct *tsk = current;
-	struct mm_struct *mm = tsk->active_mm;
 	const struct fault_info *inf;
 
 	/*
@@ -276,7 +290,7 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 		inf = esr_to_fault_info(esr);
 		__do_user_fault(tsk, addr, esr, inf->sig, inf->code, regs);
 	} else
-		__do_kernel_fault(mm, addr, esr, regs);
+		__do_kernel_fault(addr, esr, regs);
 }
 
 #define VM_FAULT_BADMAP		0x010000
@@ -475,7 +489,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	return 0;
 
 no_context:
-	__do_kernel_fault(mm, addr, esr, regs);
+	__do_kernel_fault(addr, esr, regs);
 	return 0;
 }
 

commit c07ab957d9af8092fc3caf3db5a0fb49098fdc65
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Tue May 9 09:53:36 2017 +0800

    arm64: Call __show_regs directly
    
    Generic code expects show_regs() to also dump the stack, but arm64's
    show_reg() does not do this. Some arm64 callers of show_regs() *only*
    want the registers dumped, without the stack.
    
    To enable generic code to work as expected, we need to make
    show_regs() dump the stack. Where we only want the registers dumped,
    we must use __show_regs().
    
    This patch updates code to use __show_regs() where only registers are
    desired. A subsequent patch will modify show_regs().
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 37b95dff0b07..c3f2b1048f83 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -250,7 +250,7 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 			tsk->comm, task_pid_nr(tsk), inf->name, sig,
 			addr, esr);
 		show_pte(tsk->mm, addr);
-		show_regs(regs);
+		__show_regs(regs);
 	}
 
 	tsk->thread.fault_address = addr;

commit ab182e67ec99ea0c8d7435a32a4a1ed9bb02559a
Merge: 7246f6006884 92f66f84d969
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 12:11:37 2017 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - kdump support, including two necessary memblock additions:
       memblock_clear_nomap() and memblock_cap_memory_range()
    
     - ARMv8.3 HWCAP bits for JavaScript conversion instructions, complex
       numbers and weaker release consistency
    
     - arm64 ACPI platform MSI support
    
     - arm perf updates: ACPI PMU support, L3 cache PMU in some Qualcomm
       SoCs, Cortex-A53 L2 cache events and DTLB refills, MAINTAINERS update
       for DT perf bindings
    
     - architected timer errata framework (the arch/arm64 changes only)
    
     - support for DMA_ATTR_FORCE_CONTIGUOUS in the arm64 iommu DMA API
    
     - arm64 KVM refactoring to use common system register definitions
    
     - remove support for ASID-tagged VIVT I-cache (no ARMv8 implementation
       using it and deprecated in the architecture) together with some
       I-cache handling clean-up
    
     - PE/COFF EFI header clean-up/hardening
    
     - define BUG() instruction without CONFIG_BUG
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      arm64: Fix the DMA mmap and get_sgtable API with DMA_ATTR_FORCE_CONTIGUOUS
      arm64: Print DT machine model in setup_machine_fdt()
      arm64: pmu: Wire-up Cortex A53 L2 cache events and DTLB refills
      arm64: module: split core and init PLT sections
      arm64: pmuv3: handle pmuv3+
      arm64: Add CNTFRQ_EL0 trap handler
      arm64: Silence spurious kbuild warning on menuconfig
      arm64: pmuv3: use arm_pmu ACPI framework
      arm64: pmuv3: handle !PMUv3 when probing
      drivers/perf: arm_pmu: add ACPI framework
      arm64: add function to get a cpu's MADT GICC table
      drivers/perf: arm_pmu: split out platform device probe logic
      drivers/perf: arm_pmu: move irq request/free into probe
      drivers/perf: arm_pmu: split cpu-local irq request/free
      drivers/perf: arm_pmu: rename irq request/free functions
      drivers/perf: arm_pmu: handle no platform_device
      drivers/perf: arm_pmu: simplify cpu_pmu_request_irqs()
      drivers/perf: arm_pmu: factor out pmu registration
      drivers/perf: arm_pmu: fold init into alloc
      drivers/perf: arm_pmu: define armpmu_init_fn
      ...

commit b824b930682308ce453b73b0e3ba9686e1f3a93a
Author: Stephen Boyd <stephen.boyd@linaro.org>
Date:   Wed Apr 5 12:18:31 2017 -0700

    arm64: print a fault message when attempting to write RO memory
    
    If a page is marked read only we should print out that fact,
    instead of printing out that there was a page fault. Right now we
    get a cryptic error message that something went wrong with an
    unhandled fault, but we don't evaluate the esr to figure out that
    it was a read/write permission fault.
    
    Instead of seeing:
    
      Unable to handle kernel paging request at virtual address ffff000008e460d8
      pgd = ffff800003504000
      [ffff000008e460d8] *pgd=0000000083473003, *pud=0000000083503003, *pmd=0000000000000000
      Internal error: Oops: 9600004f [#1] PREEMPT SMP
    
    we'll see:
    
      Unable to handle kernel write to read-only memory at virtual address ffff000008e760d8
      pgd = ffff80003d3de000
      [ffff000008e760d8] *pgd=0000000083472003, *pud=0000000083435003, *pmd=0000000000000000
      Internal error: Oops: 9600004f [#1] PREEMPT SMP
    
    We also add a userspace address check into is_permission_fault()
    so that the function doesn't return true for ttbr0 PAN faults
    when it shouldn't.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Stephen Boyd <stephen.boyd@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 4bf899fb451b..61114b7b6b9e 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -161,12 +161,33 @@ static bool is_el1_instruction_abort(unsigned int esr)
 	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_CUR;
 }
 
+static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs,
+				       unsigned long addr)
+{
+	unsigned int ec       = ESR_ELx_EC(esr);
+	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
+
+	if (ec != ESR_ELx_EC_DABT_CUR && ec != ESR_ELx_EC_IABT_CUR)
+		return false;
+
+	if (fsc_type == ESR_ELx_FSC_PERM)
+		return true;
+
+	if (addr < USER_DS && system_uses_ttbr0_pan())
+		return fsc_type == ESR_ELx_FSC_FAULT &&
+			(regs->pstate & PSR_PAN_BIT);
+
+	return false;
+}
+
 /*
  * The kernel tried to access some page that wasn't present.
  */
 static void __do_kernel_fault(struct mm_struct *mm, unsigned long addr,
 			      unsigned int esr, struct pt_regs *regs)
 {
+	const char *msg;
+
 	/*
 	 * Are we prepared to handle this kernel fault?
 	 * We are almost certainly not prepared to handle instruction faults.
@@ -178,9 +199,20 @@ static void __do_kernel_fault(struct mm_struct *mm, unsigned long addr,
 	 * No handler, we'll have to terminate things with extreme prejudice.
 	 */
 	bust_spinlocks(1);
-	pr_alert("Unable to handle kernel %s at virtual address %08lx\n",
-		 (addr < PAGE_SIZE) ? "NULL pointer dereference" :
-		 "paging request", addr);
+
+	if (is_permission_fault(esr, regs, addr)) {
+		if (esr & ESR_ELx_WNR)
+			msg = "write to read-only memory";
+		else
+			msg = "read from unreadable memory";
+	} else if (addr < PAGE_SIZE) {
+		msg = "NULL pointer dereference";
+	} else {
+		msg = "paging request";
+	}
+
+	pr_alert("Unable to handle kernel %s at virtual address %08lx\n", msg,
+		 addr);
 
 	show_pte(mm, addr);
 	die("Oops", regs, esr);
@@ -270,21 +302,6 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	return fault;
 }
 
-static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs)
-{
-	unsigned int ec       = ESR_ELx_EC(esr);
-	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
-
-	if (ec != ESR_ELx_EC_DABT_CUR && ec != ESR_ELx_EC_IABT_CUR)
-		return false;
-
-	if (system_uses_ttbr0_pan())
-		return fsc_type == ESR_ELx_FSC_FAULT &&
-			(regs->pstate & PSR_PAN_BIT);
-	else
-		return fsc_type == ESR_ELx_FSC_PERM;
-}
-
 static bool is_el0_instruction_abort(unsigned int esr)
 {
 	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_LOW;
@@ -322,7 +339,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (addr < USER_DS && is_permission_fault(esr, regs)) {
+	if (addr < USER_DS && is_permission_fault(esr, regs, addr)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);

commit 09a6adf53d42ca3088fa3fb41f40b768efc711ed
Author: Victor Kamensky <kamensky@cisco.com>
Date:   Mon Apr 3 22:51:01 2017 -0700

    arm64: mm: unaligned access by user-land should be received as SIGBUS
    
    After 52d7523 (arm64: mm: allow the kernel to handle alignment faults on
    user accesses) commit user-land accesses that produce unaligned exceptions
    like in case of aarch32 ldm/stm/ldrd/strd instructions operating on
    unaligned memory received by user-land as SIGSEGV. It is wrong, it should
    be reported as SIGBUS as it was before 52d7523 commit.
    
    Changed do_bad_area function to take signal and code parameters out of esr
    value using fault_info table, so in case of do_alignment_fault fault
    user-land will receive SIGBUS. Wrapped access to fault_info table into
    esr_to_fault_info function.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 52d7523 (arm64: mm: allow the kernel to handle alignment faults on user accesses)
    Signed-off-by: Victor Kamensky <kamensky@cisco.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 4bf899fb451b..1b35b8bddbfb 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -42,7 +42,20 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 
-static const char *fault_name(unsigned int esr);
+struct fault_info {
+	int	(*fn)(unsigned long addr, unsigned int esr,
+		      struct pt_regs *regs);
+	int	sig;
+	int	code;
+	const char *name;
+};
+
+static const struct fault_info fault_info[];
+
+static inline const struct fault_info *esr_to_fault_info(unsigned int esr)
+{
+	return fault_info + (esr & 63);
+}
 
 #ifdef CONFIG_KPROBES
 static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
@@ -197,10 +210,12 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 			    struct pt_regs *regs)
 {
 	struct siginfo si;
+	const struct fault_info *inf;
 
 	if (unhandled_signal(tsk, sig) && show_unhandled_signals_ratelimited()) {
+		inf = esr_to_fault_info(esr);
 		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
-			tsk->comm, task_pid_nr(tsk), fault_name(esr), sig,
+			tsk->comm, task_pid_nr(tsk), inf->name, sig,
 			addr, esr);
 		show_pte(tsk->mm, addr);
 		show_regs(regs);
@@ -219,14 +234,16 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 {
 	struct task_struct *tsk = current;
 	struct mm_struct *mm = tsk->active_mm;
+	const struct fault_info *inf;
 
 	/*
 	 * If we are in kernel mode at this point, we have no context to
 	 * handle this fault with.
 	 */
-	if (user_mode(regs))
-		__do_user_fault(tsk, addr, esr, SIGSEGV, SEGV_MAPERR, regs);
-	else
+	if (user_mode(regs)) {
+		inf = esr_to_fault_info(esr);
+		__do_user_fault(tsk, addr, esr, inf->sig, inf->code, regs);
+	} else
 		__do_kernel_fault(mm, addr, esr, regs);
 }
 
@@ -488,12 +505,7 @@ static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 	return 1;
 }
 
-static const struct fault_info {
-	int	(*fn)(unsigned long addr, unsigned int esr, struct pt_regs *regs);
-	int	sig;
-	int	code;
-	const char *name;
-} fault_info[] = {
+static const struct fault_info fault_info[] = {
 	{ do_bad,		SIGBUS,  0,		"ttbr address size fault"	},
 	{ do_bad,		SIGBUS,  0,		"level 1 address size fault"	},
 	{ do_bad,		SIGBUS,  0,		"level 2 address size fault"	},
@@ -560,19 +572,13 @@ static const struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
 };
 
-static const char *fault_name(unsigned int esr)
-{
-	const struct fault_info *inf = fault_info + (esr & 63);
-	return inf->name;
-}
-
 /*
  * Dispatch a data abort to the relevant handler.
  */
 asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
 					 struct pt_regs *regs)
 {
-	const struct fault_info *inf = fault_info + (esr & 63);
+	const struct fault_info *inf = esr_to_fault_info(esr);
 	struct siginfo info;
 
 	if (!inf->fn(addr, esr, regs))

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 30dd60ab8a65..4bf899fb451b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -27,6 +27,7 @@
 #include <linux/uaccess.h>
 #include <linux/page-flags.h>
 #include <linux/sched/signal.h>
+#include <linux/sched/debug.h>
 #include <linux/highmem.h>
 #include <linux/perf_event.h>
 #include <linux/preempt.h>

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 81283851c9af..30dd60ab8a65 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -26,7 +26,7 @@
 #include <linux/kprobes.h>
 #include <linux/uaccess.h>
 #include <linux/page-flags.h>
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/highmem.h>
 #include <linux/perf_event.h>
 #include <linux/preempt.h>

commit c8b06e3fddddaae1a87ed479edcb8b3d85caecc7
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 9 18:14:02 2017 +0000

    arm64: Remove useless UAO IPI and describe how this gets enabled
    
    Since its introduction, the UAO enable call was broken, and useless.
    commit 2a6dcb2b5f3e ("arm64: cpufeature: Schedule enable() calls instead
    of calling them via IPI"), fixed the framework so that these calls
    are scheduled, so that they can modify PSTATE.
    
    Now it is just useless. Remove it. UAO is enabled by the code patching
    which causes get_user() and friends to use the 'ldtr' family of
    instructions. This relies on the PSTATE.UAO bit being set to match
    addr_limit, which we do in uao_thread_switch() called via __switch_to().
    
    All that is needed to enable UAO is patch the code, and call schedule().
    __apply_alternatives_multi_stop() calls stop_machine() when it modifies
    the kernel text to enable the alternatives, (including the UAO code in
    uao_thread_switch()). Once stop_machine() has finished __switch_to() is
    called to reschedule the original task, this causes PSTATE.UAO to be set
    appropriately. An explicit enable() call is not needed.
    
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 156169c6981b..81283851c9af 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -691,17 +691,3 @@ int cpu_enable_pan(void *__unused)
 	return 0;
 }
 #endif /* CONFIG_ARM64_PAN */
-
-#ifdef CONFIG_ARM64_UAO
-/*
- * Kernel threads have fs=KERNEL_DS by default, and don't need to call
- * set_fs(), devtmpfs in particular relies on this behaviour.
- * We need to enable the feature at runtime (instead of adding it to
- * PSR_MODE_EL1h) as the feature may not be implemented by the cpu.
- */
-int cpu_enable_uao(void *__unused)
-{
-	asm(SET_PSTATE_UAO(1));
-	return 0;
-}
-#endif /* CONFIG_ARM64_UAO */

commit 6ef4fb387d50fa8f3bffdffc868b57e981cdd709
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jan 3 14:27:26 2017 +0000

    arm64: mm: fix show_pte KERN_CONT fallout
    
    Recent changes made KERN_CONT mandatory for continued lines. In the
    absence of KERN_CONT, a newline may be implicit inserted by the core
    printk code.
    
    In show_pte, we (erroneously) use printk without KERN_CONT for continued
    prints, resulting in output being split across a number of lines, and
    not matching the intended output, e.g.
    
    [ff000000000000] *pgd=00000009f511b003
    , *pud=00000009f4a80003
    , *pmd=0000000000000000
    
    Fix this by using pr_cont() for all the continuations.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index a78a5c401806..156169c6981b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -88,21 +88,21 @@ void show_pte(struct mm_struct *mm, unsigned long addr)
 			break;
 
 		pud = pud_offset(pgd, addr);
-		printk(", *pud=%016llx", pud_val(*pud));
+		pr_cont(", *pud=%016llx", pud_val(*pud));
 		if (pud_none(*pud) || pud_bad(*pud))
 			break;
 
 		pmd = pmd_offset(pud, addr);
-		printk(", *pmd=%016llx", pmd_val(*pmd));
+		pr_cont(", *pmd=%016llx", pmd_val(*pmd));
 		if (pmd_none(*pmd) || pmd_bad(*pmd))
 			break;
 
 		pte = pte_offset_map(pmd, addr);
-		printk(", *pte=%016llx", pte_val(*pte));
+		pr_cont(", *pte=%016llx", pte_val(*pte));
 		pte_unmap(pte);
 	} while(0);
 
-	printk("\n");
+	pr_cont("\n");
 }
 
 #ifdef CONFIG_ARM64_HW_AFDBM

commit 786889636ad75296c213547d1ca656af4c59f390
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 1 18:22:39 2016 +0100

    arm64: Handle faults caused by inadvertent user access with PAN enabled
    
    When TTBR0_EL1 is set to the reserved page, an erroneous kernel access
    to user space would generate a translation fault. This patch adds the
    checks for the software-set PSR_PAN_BIT to emulate a permission fault
    and report it accordingly.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index d035cc594445..a78a5c401806 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -269,13 +269,19 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	return fault;
 }
 
-static inline bool is_permission_fault(unsigned int esr)
+static inline bool is_permission_fault(unsigned int esr, struct pt_regs *regs)
 {
 	unsigned int ec       = ESR_ELx_EC(esr);
 	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
 
-	return (ec == ESR_ELx_EC_DABT_CUR && fsc_type == ESR_ELx_FSC_PERM) ||
-	       (ec == ESR_ELx_EC_IABT_CUR && fsc_type == ESR_ELx_FSC_PERM);
+	if (ec != ESR_ELx_EC_DABT_CUR && ec != ESR_ELx_EC_IABT_CUR)
+		return false;
+
+	if (system_uses_ttbr0_pan())
+		return fsc_type == ESR_ELx_FSC_FAULT &&
+			(regs->pstate & PSR_PAN_BIT);
+	else
+		return fsc_type == ESR_ELx_FSC_PERM;
 }
 
 static bool is_el0_instruction_abort(unsigned int esr)
@@ -315,7 +321,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (is_permission_fault(esr) && (addr < USER_DS)) {
+	if (addr < USER_DS && is_permission_fault(esr, regs)) {
 		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);

commit a8ada146f5179793bbaff3c936d73147ccd47ba2
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Oct 28 18:07:37 2016 +0100

    arm64: Update the synchronous external abort fault description
    
    This patch updates the description of the synchronous external aborts on
    translation table walks.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0f8788374815..d035cc594445 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -507,10 +507,10 @@ static const struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"unknown 17"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 18"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 19"			},
-	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
-	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous external abort (translation table walk)" },
 	{ do_bad,		SIGBUS,  0,		"synchronous parity error"	},
 	{ do_bad,		SIGBUS,  0,		"unknown 25"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 26"			},

commit 7209c868600bd8926e37c10b9aae83124ccc1dd8
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:47 2016 +0100

    arm64: mm: Set PSTATE.PAN from the cpu_enable_pan() call
    
    Commit 338d4f49d6f7 ("arm64: kernel: Add support for Privileged Access
    Never") enabled PAN by enabling the 'SPAN' feature-bit in SCTLR_EL1.
    This means the PSTATE.PAN bit won't be set until the next return to the
    kernel from userspace. On a preemptible kernel we may schedule work that
    accesses userspace on a CPU before it has done this.
    
    Now that cpufeature enable() calls are scheduled via stop_machine(), we
    can set PSTATE.PAN from the cpu_enable_pan() call.
    
    Add WARN_ON_ONCE(in_interrupt()) to check the PSTATE value we updated
    is not immediately discarded.
    
    Reported-by: Tony Thompson <anthony.thompson@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [will: fixed typo in comment]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 3e9ff9b0c78d..0f8788374815 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -29,7 +29,9 @@
 #include <linux/sched.h>
 #include <linux/highmem.h>
 #include <linux/perf_event.h>
+#include <linux/preempt.h>
 
+#include <asm/bug.h>
 #include <asm/cpufeature.h>
 #include <asm/exception.h>
 #include <asm/debug-monitors.h>
@@ -672,7 +674,14 @@ NOKPROBE_SYMBOL(do_debug_exception);
 #ifdef CONFIG_ARM64_PAN
 int cpu_enable_pan(void *__unused)
 {
+	/*
+	 * We modify PSTATE. This won't work from irq context as the PSTATE
+	 * is discarded once we return from the exception.
+	 */
+	WARN_ON_ONCE(in_interrupt());
+
 	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
+	asm(SET_PSTATE_PAN(1));
 	return 0;
 }
 #endif /* CONFIG_ARM64_PAN */

commit 2a6dcb2b5f3e21592ca8dfa198dcce7bec09b020
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:46 2016 +0100

    arm64: cpufeature: Schedule enable() calls instead of calling them via IPI
    
    The enable() call for a cpufeature/errata is called using on_each_cpu().
    This issues a cross-call IPI to get the work done. Implicitly, this
    stashes the running PSTATE in SPSR when the CPU receives the IPI, and
    restores it when we return. This means an enable() call can never modify
    PSTATE.
    
    To allow PAN to do this, change the on_each_cpu() call to use
    stop_machine(). This schedules the work on each CPU which allows
    us to modify PSTATE.
    
    This involves changing the protype of all the enable() functions.
    
    enable_cpu_capabilities() is called during boot and enables the feature
    on all online CPUs. This path now uses stop_machine(). CPU features for
    hotplug'd CPUs are enabled by verify_local_cpu_features() which only
    acts on the local CPU, and can already modify the running PSTATE as it
    is called from secondary_start_kernel().
    
    Reported-by: Tony Thompson <anthony.thompson@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 53d9159662fe..3e9ff9b0c78d 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -670,9 +670,10 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 NOKPROBE_SYMBOL(do_debug_exception);
 
 #ifdef CONFIG_ARM64_PAN
-void cpu_enable_pan(void *__unused)
+int cpu_enable_pan(void *__unused)
 {
 	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
+	return 0;
 }
 #endif /* CONFIG_ARM64_PAN */
 
@@ -683,8 +684,9 @@ void cpu_enable_pan(void *__unused)
  * We need to enable the feature at runtime (instead of adding it to
  * PSR_MODE_EL1h) as the feature may not be implemented by the cpu.
  */
-void cpu_enable_uao(void *__unused)
+int cpu_enable_uao(void *__unused)
 {
 	asm(SET_PSTATE_UAO(1));
+	return 0;
 }
 #endif /* CONFIG_ARM64_UAO */

commit 0edfa8391664a4f795c67d0e07480fbe801a0e1d
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon Sep 19 17:38:55 2016 -0400

    arm64: migrate exception table users off module.h and onto extable.h
    
    These files were only including module.h for exception table
    related functions.  We've now separated that content out into its
    own file "extable.h" so now move over to that and avoid all the
    extra header content in module.h that we don't really need to compile
    these files.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index a5f098a5f602..53d9159662fe 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -18,7 +18,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
-#include <linux/module.h>
+#include <linux/extable.h>
 #include <linux/signal.h>
 #include <linux/mm.h>
 #include <linux/hardirq.h>

commit cab15ce604e550020bb7115b779013b91bcdbc21
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Aug 11 18:44:50 2016 +0100

    arm64: Introduce execute-only page access permissions
    
    The ARMv8 architecture allows execute-only user permissions by clearing
    the PTE_UXN and PTE_USER bits. However, the kernel running on a CPU
    implementation without User Access Override (ARMv8.2 onwards) can still
    access such page, so execute-only page permission does not protect
    against read(2)/write(2) etc. accesses. Systems requiring such
    protection must enable features like SECCOMP.
    
    This patch changes the arm64 __P100 and __S100 protection_map[] macros
    to the new __PAGE_EXECONLY attributes. A side effect is that
    pte_user() no longer triggers for __PAGE_EXECONLY since PTE_USER isn't
    set. To work around this, the check is done on the PTE_NG bit via the
    pte_ng() macro. VM_READ is also checked now for page faults.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 05d2bd776c69..a5f098a5f602 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -251,8 +251,7 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 good_area:
 	/*
 	 * Check that the permissions on the VMA allow for the fault which
-	 * occurred. If we encountered a write or exec fault, we must have
-	 * appropriate permissions, otherwise we allow any permission.
+	 * occurred.
 	 */
 	if (!(vma->vm_flags & vm_flags)) {
 		fault = VM_FAULT_BADACCESS;
@@ -288,7 +287,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	int fault, sig, code;
-	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
+	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
 	if (notify_page_fault(regs, esr))

commit 9adeb8e72dbfe976709df01e259ed556ee60e779
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Aug 9 18:25:26 2016 -0700

    arm64: Handle el1 synchronous instruction aborts cleanly
    
    Executing from a non-executable area gives an ugly message:
    
    lkdtm: Performing direct entry EXEC_RODATA
    lkdtm: attempting ok execution at ffff0000084c0e08
    lkdtm: attempting bad execution at ffff000008880700
    Bad mode in Synchronous Abort handler detected on CPU2, code 0x8400000e -- IABT (current EL)
    CPU: 2 PID: 998 Comm: sh Not tainted 4.7.0-rc2+ #13
    Hardware name: linux,dummy-virt (DT)
    task: ffff800077e35780 ti: ffff800077970000 task.ti: ffff800077970000
    PC is at lkdtm_rodata_do_nothing+0x0/0x8
    LR is at execute_location+0x74/0x88
    
    The 'IABT (current EL)' indicates the error but it's a bit cryptic
    without knowledge of the ARM ARM. There is also no indication of the
    specific address which triggered the fault. The increase in kernel
    page permissions makes hitting this case more likely as well.
    Handling the case in the vectors gives a much more familiar looking
    error message:
    
    lkdtm: Performing direct entry EXEC_RODATA
    lkdtm: attempting ok execution at ffff0000084c0840
    lkdtm: attempting bad execution at ffff000008880680
    Unable to handle kernel paging request at virtual address ffff000008880680
    pgd = ffff8000089b2000
    [ffff000008880680] *pgd=00000000489b4003, *pud=0000000048904003, *pmd=0000000000000000
    Internal error: Oops: 8400000e [#1] PREEMPT SMP
    Modules linked in:
    CPU: 1 PID: 997 Comm: sh Not tainted 4.7.0-rc1+ #24
    Hardware name: linux,dummy-virt (DT)
    task: ffff800077f9f080 ti: ffff800008a1c000 task.ti: ffff800008a1c000
    PC is at lkdtm_rodata_do_nothing+0x0/0x8
    LR is at execute_location+0x74/0x88
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c8beaa0da7df..05d2bd776c69 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -153,6 +153,11 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 }
 #endif
 
+static bool is_el1_instruction_abort(unsigned int esr)
+{
+	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_CUR;
+}
+
 /*
  * The kernel tried to access some page that wasn't present.
  */
@@ -161,8 +166,9 @@ static void __do_kernel_fault(struct mm_struct *mm, unsigned long addr,
 {
 	/*
 	 * Are we prepared to handle this kernel fault?
+	 * We are almost certainly not prepared to handle instruction faults.
 	 */
-	if (fixup_exception(regs))
+	if (!is_el1_instruction_abort(esr) && fixup_exception(regs))
 		return;
 
 	/*
@@ -267,7 +273,8 @@ static inline bool is_permission_fault(unsigned int esr)
 	unsigned int ec       = ESR_ELx_EC(esr);
 	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
 
-	return (ec == ESR_ELx_EC_DABT_CUR && fsc_type == ESR_ELx_FSC_PERM);
+	return (ec == ESR_ELx_EC_DABT_CUR && fsc_type == ESR_ELx_FSC_PERM) ||
+	       (ec == ESR_ELx_EC_IABT_CUR && fsc_type == ESR_ELx_FSC_PERM);
 }
 
 static bool is_el0_instruction_abort(unsigned int esr)
@@ -312,6 +319,9 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		if (regs->orig_addr_limit == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);
 
+		if (is_el1_instruction_abort(esr))
+			die("Attempting to execute userspace memory", regs, esr);
+
 		if (!search_exception_tables(regs->pc))
 			die("Accessing user space memory outside uaccess.h routines", regs, esr);
 	}

commit e831101a73fbc8339ef1d1909dad3ef64f089e70
Merge: f9abf53af4c7 fd6380b75065
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 11:16:05 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Kexec support for arm64
    
     - Kprobes support
    
     - Expose MIDR_EL1 and REVIDR_EL1 CPU identification registers to sysfs
    
     - Trapping of user space cache maintenance operations and emulation in
       the kernel (CPU errata workaround)
    
     - Clean-up of the early page tables creation (kernel linear mapping,
       EFI run-time maps) to avoid splitting larger blocks (e.g.  pmds) into
       smaller ones (e.g.  ptes)
    
     - VDSO support for CLOCK_MONOTONIC_RAW in clock_gettime()
    
     - ARCH_HAS_KCOV enabled for arm64
    
     - Optimise IP checksum helpers
    
     - SWIOTLB optimisation to only allocate/initialise the buffer if the
       available RAM is beyond the 32-bit mask
    
     - Properly handle the "nosmp" command line argument
    
     - Fix for the initialisation of the CPU debug state during early boot
    
     - vdso-offsets.h build dependency workaround
    
     - Build fix when RANDOMIZE_BASE is enabled with MODULES off
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (64 commits)
      arm64: arm: Fix-up the removal of the arm64 regs_query_register_name() prototype
      arm64: Only select ARM64_MODULE_PLTS if MODULES=y
      arm64: mm: run pgtable_page_ctor() on non-swapper translation table pages
      arm64: mm: make create_mapping_late() non-allocating
      arm64: Honor nosmp kernel command line option
      arm64: Fix incorrect per-cpu usage for boot CPU
      arm64: kprobes: Add KASAN instrumentation around stack accesses
      arm64: kprobes: Cleanup jprobe_return
      arm64: kprobes: Fix overflow when saving stack
      arm64: kprobes: WARN if attempting to step with PSTATE.D=1
      arm64: debug: remove unused local_dbg_{enable, disable} macros
      arm64: debug: remove redundant spsr manipulation
      arm64: debug: unmask PSTATE.D earlier
      arm64: localise Image objcopy flags
      arm64: ptrace: remove extra define for CPSR's E bit
      kprobes: Add arm64 case in kprobe example module
      arm64: Add kernel return probes support (kretprobes)
      arm64: Add trampoline code for kretprobes
      arm64: kprobes instruction simulation support
      arm64: Treat all entry code as non-kprobe-able
      ...

commit dcddffd41d3f1d3bdcc1dce3f1cd142779b6d4c1
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Jul 26 15:25:18 2016 -0700

    mm: do not pass mm_struct into handle_mm_fault
    
    We always have vma->vm_mm around.
    
    Link: http://lkml.kernel.org/r/1466021202-61880-8-git-send-email-kirill.shutemov@linux.intel.com
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index b1166d1e5955..031820d989a8 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -233,7 +233,7 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 		goto out;
 	}
 
-	return handle_mm_fault(mm, vma, addr & PAGE_MASK, mm_flags);
+	return handle_mm_fault(vma, addr & PAGE_MASK, mm_flags);
 
 check_stack:
 	if (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))

commit a95b0644b38c16c40b753224671b919b9af0b73c
Merge: e75118a7b581 f7e35c5ba432
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jul 21 18:20:41 2016 +0100

    Merge branch 'for-next/kprobes' into for-next/core
    
    * kprobes:
      arm64: kprobes: Add KASAN instrumentation around stack accesses
      arm64: kprobes: Cleanup jprobe_return
      arm64: kprobes: Fix overflow when saving stack
      arm64: kprobes: WARN if attempting to step with PSTATE.D=1
      kprobes: Add arm64 case in kprobe example module
      arm64: Add kernel return probes support (kretprobes)
      arm64: Add trampoline code for kretprobes
      arm64: kprobes instruction simulation support
      arm64: Treat all entry code as non-kprobe-able
      arm64: Blacklist non-kprobe-able symbol
      arm64: Kprobes with single stepping support
      arm64: add conditional instruction simulation support
      arm64: Add more test functions to insn.c
      arm64: Add HAVE_REGS_AND_STACK_ACCESS_API feature

commit 2dd0e8d2d2a157dbc83295a78336c2217110f2f8
Author: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
Date:   Fri Jul 8 12:35:48 2016 -0400

    arm64: Kprobes with single stepping support
    
    Add support for basic kernel probes(kprobes) and jump probes
    (jprobes) for ARM64.
    
    Kprobes utilizes software breakpoint and single step debug
    exceptions supported on ARM v8.
    
    A software breakpoint is placed at the probe address to trap the
    kernel execution into the kprobe handler.
    
    ARM v8 supports enabling single stepping before the break exception
    return (ERET), with next PC in exception return address (ELR_EL1). The
    kprobe handler prepares an executable memory slot for out-of-line
    execution with a copy of the original instruction being probed, and
    enables single stepping. The PC is set to the out-of-line slot address
    before the ERET. With this scheme, the instruction is executed with the
    exact same register context except for the PC (and DAIF) registers.
    
    Debug mask (PSTATE.D) is enabled only when single stepping a recursive
    kprobe, e.g.: during kprobes reenter so that probed instruction can be
    single stepped within the kprobe handler -exception- context.
    The recursion depth of kprobe is always 2, i.e. upon probe re-entry,
    any further re-entry is prevented by not calling handlers and the case
    counted as a missed kprobe).
    
    Single stepping from the x-o-l slot has a drawback for PC-relative accesses
    like branching and symbolic literals access as the offset from the new PC
    (slot address) may not be ensured to fit in the immediate value of
    the opcode. Such instructions need simulation, so reject
    probing them.
    
    Instructions generating exceptions or cpu mode change are rejected
    for probing.
    
    Exclusive load/store instructions are rejected too.  Additionally, the
    code is checked to see if it is inside an exclusive load/store sequence
    (code from Pratyush).
    
    System instructions are mostly enabled for stepping, except MSR/MRS
    accesses to "DAIF" flags in PSTATE, which are not safe for
    probing.
    
    This also changes arch/arm64/include/asm/ptrace.h to use
    include/asm-generic/ptrace.h.
    
    Thanks to Steve Capper and Pratyush Anand for several suggested
    Changes.
    
    Signed-off-by: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
    Signed-off-by: David A. Long <dave.long@linaro.org>
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 013e2cbe7924..2408e51d781f 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -41,6 +41,28 @@
 
 static const char *fault_name(unsigned int esr);
 
+#ifdef CONFIG_KPROBES
+static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
+{
+	int ret = 0;
+
+	/* kprobe_running() needs smp_processor_id() */
+	if (!user_mode(regs)) {
+		preempt_disable();
+		if (kprobe_running() && kprobe_fault_handler(regs, esr))
+			ret = 1;
+		preempt_enable();
+	}
+
+	return ret;
+}
+#else
+static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
+{
+	return 0;
+}
+#endif
+
 /*
  * Dump out the page tables associated with 'addr' in mm 'mm'.
  */
@@ -259,6 +281,9 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
+	if (notify_page_fault(regs, esr))
+		return 0;
+
 	tsk = current;
 	mm  = tsk->mm;
 
@@ -629,6 +654,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 
 	return rv;
 }
+NOKPROBE_SYMBOL(do_debug_exception);
 
 #ifdef CONFIG_ARM64_PAN
 void cpu_enable_pan(void *__unused)

commit e19a6ee2460bdd0d0055a6029383422773f9999a
Author: James Morse <james.morse@arm.com>
Date:   Mon Jun 20 18:28:01 2016 +0100

    arm64: kernel: Save and restore UAO and addr_limit on exception entry
    
    If we take an exception while at EL1, the exception handler inherits
    the original context's addr_limit and PSTATE.UAO values. To be consistent
    always reset addr_limit and PSTATE.UAO on (re-)entry to EL1. This
    prevents accidental re-use of the original context's addr_limit.
    
    Based on a similar patch for arm from Russell King.
    
    Cc: <stable@vger.kernel.org> # 4.6-
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 013e2cbe7924..b1166d1e5955 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -280,7 +280,8 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	if (permission_fault(esr) && (addr < USER_DS)) {
-		if (get_fs() == KERNEL_DS)
+		/* regs->orig_addr_limit may be 0 if we entered from EL0 */
+		if (regs->orig_addr_limit == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);
 
 		if (!search_exception_tables(regs->pc))

commit 541ec870ef31433018d245614254bd9d810a9ac3
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 31 12:33:03 2016 +0100

    arm64: kill ESR_LNX_EXEC
    
    Currently we treat ESR_EL1 bit 24 as software-defined for distinguishing
    instruction aborts from data aborts, but this bit is architecturally
    RES0 for instruction aborts, and could be allocated for an arbitrary
    purpose in future. Additionally, we hard-code the value in entry.S
    without the mnemonic, making the code difficult to understand.
    
    Instead, remove ESR_LNX_EXEC, and distinguish aborts based on the esr,
    which we already pass to the sole use of ESR_LNX_EXEC. A new helper,
    is_el0_instruction_abort() is added to make the logic clear. Any
    instruction aborts taken from EL1 will already have been handled by
    bad_mode, so we need not handle that case in the helper.
    
    For consistency, the existing permission_fault helper is renamed to
    is_permission_fault, and the return type is changed to bool. There
    should be no functional changes as the return value was a boolean
    expression, and the result is only used in another boolean expression.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave P Martin <dave.martin@arm.com>
    Cc: Huang Shijie <shijie.huang@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index d2c124cbd18c..fc5a34a72c6d 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -202,8 +202,6 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 #define VM_FAULT_BADMAP		0x010000
 #define VM_FAULT_BADACCESS	0x020000
 
-#define ESR_LNX_EXEC		(1 << 24)
-
 static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 			   unsigned int mm_flags, unsigned long vm_flags,
 			   struct task_struct *tsk)
@@ -242,7 +240,7 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	return fault;
 }
 
-static inline int permission_fault(unsigned int esr)
+static inline bool is_permission_fault(unsigned int esr)
 {
 	unsigned int ec       = ESR_ELx_EC(esr);
 	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
@@ -250,6 +248,11 @@ static inline int permission_fault(unsigned int esr)
 	return (ec == ESR_ELx_EC_DABT_CUR && fsc_type == ESR_ELx_FSC_PERM);
 }
 
+static bool is_el0_instruction_abort(unsigned int esr)
+{
+	return ESR_ELx_EC(esr) == ESR_ELx_EC_IABT_LOW;
+}
+
 static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				   struct pt_regs *regs)
 {
@@ -272,14 +275,14 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	if (user_mode(regs))
 		mm_flags |= FAULT_FLAG_USER;
 
-	if (esr & ESR_LNX_EXEC) {
+	if (is_el0_instruction_abort(esr)) {
 		vm_flags = VM_EXEC;
 	} else if ((esr & ESR_ELx_WNR) && !(esr & ESR_ELx_CM)) {
 		vm_flags = VM_WRITE;
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	if (permission_fault(esr) && (addr < USER_DS)) {
+	if (is_permission_fault(esr) && (addr < USER_DS)) {
 		if (get_fs() == KERNEL_DS)
 			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);
 

commit 275f344bec51e9100bae81f3cc8c6940bbfb24c0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 31 12:33:01 2016 +0100

    arm64: add macro to extract ESR_ELx.EC
    
    Several places open-code extraction of the EC field from an ESR_ELx
    value, in subtly different ways. This is unfortunate duplication and
    variation, and the precise logic used to extract the field is a
    distraction.
    
    This patch adds a new macro, ESR_ELx_EC(), to extract the EC field from
    an ESR_ELx value in a consistent fashion.
    
    Existing open-coded extractions in core arm64 code are moved over to the
    new helper. KVM code is left as-is for the moment.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Huang Shijie <shijie.huang@arm.com>
    Cc: Dave P Martin <dave.martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 013e2cbe7924..d2c124cbd18c 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -244,7 +244,7 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 
 static inline int permission_fault(unsigned int esr)
 {
-	unsigned int ec       = (esr & ESR_ELx_EC_MASK) >> ESR_ELx_EC_SHIFT;
+	unsigned int ec       = ESR_ELx_EC(esr);
 	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
 
 	return (ec == ESR_ELx_EC_DABT_CUR && fsc_type == ESR_ELx_FSC_PERM);

commit bbb1681ee3653bdcfc6a4ba31902738118311fd4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jun 13 17:57:02 2016 +0100

    arm64: mm: mark fault_info table const
    
    Unlike the debug_fault_info table, we never intentionally alter the
    fault_info table at runtime, and all derived pointers are treated as
    const currently.
    
    Make the table const so that it can be placed in .rodata and protected
    from unintentional writes, as we do for the syscall tables.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index ba3fc12bd272..013e2cbe7924 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -441,7 +441,7 @@ static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 	return 1;
 }
 
-static struct fault_info {
+static const struct fault_info {
 	int	(*fn)(unsigned long addr, unsigned int esr, struct pt_regs *regs);
 	int	sig;
 	int	code;

commit 0106d456c4cb1770253fefc0ab23c9ca760b43f7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jun 7 17:55:15 2016 +0100

    arm64: mm: always take dirty state from new pte in ptep_set_access_flags
    
    Commit 66dbd6e61a52 ("arm64: Implement ptep_set_access_flags() for
    hardware AF/DBM") ensured that pte flags are updated atomically in the
    face of potential concurrent, hardware-assisted updates. However, Alex
    reports that:
    
     | This patch breaks swapping for me.
     | In the broken case, you'll see either systemd cpu time spike (because
     | it's stuck in a page fault loop) or the system hang (because the
     | application owning the screen is stuck in a page fault loop).
    
    It turns out that this is because the 'dirty' argument to
    ptep_set_access_flags is always 0 for read faults, and so we can't use
    it to set PTE_RDONLY. The failing sequence is:
    
      1. We put down a PTE_WRITE | PTE_DIRTY | PTE_AF pte
      2. Memory pressure -> pte_mkold(pte) -> clear PTE_AF
      3. A read faults due to the missing access flag
      4. ptep_set_access_flags is called with dirty = 0, due to the read fault
      5. pte is then made PTE_WRITE | PTE_DIRTY | PTE_AF | PTE_RDONLY (!)
      6. A write faults, but pte_write is true so we get stuck
    
    The solution is to check the new page table entry (as would be done by
    the generic, non-atomic definition of ptep_set_access_flags that just
    calls set_pte_at) to establish the dirty state.
    
    Cc: <stable@vger.kernel.org> # 4.3+
    Fixes: 66dbd6e61a52 ("arm64: Implement ptep_set_access_flags() for hardware AF/DBM")
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Alexander Graf <agraf@suse.de>
    Tested-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 5954881a35ac..ba3fc12bd272 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -109,7 +109,7 @@ int ptep_set_access_flags(struct vm_area_struct *vma,
 	 * PTE_RDONLY is cleared by default in the asm below, so set it in
 	 * back if necessary (read-only or clean PTE).
 	 */
-	if (!pte_write(entry) || !dirty)
+	if (!pte_write(entry) || !pte_sw_dirty(entry))
 		pte_val(entry) |= PTE_RDONLY;
 
 	/*

commit 3a72db703cc6965662ff6063befa7d7cf2b49a2e
Author: Huang Shijie <shijie.huang@arm.com>
Date:   Mon Apr 18 09:49:56 2016 +0800

    arm64: mm: remove the redundant code
    
    We already re-enable interrupts where necessary in the entry code, so
    there is no need to do it again in do_page fault. This patch removes
    the redundant code.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Huang Shijie <shijie.huang@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index d24a5e05a3f2..5954881a35ac 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -262,10 +262,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	tsk = current;
 	mm  = tsk->mm;
 
-	/* Enable interrupts if they were enabled in the parent context. */
-	if (interrupts_enabled(regs))
-		local_irq_enable();
-
 	/*
 	 * If we're in an interrupt or have no user context, we must not take
 	 * the fault.

commit 66dbd6e61a526ae7d11a208238ae2c17e5cacb6b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 13 16:01:22 2016 +0100

    arm64: Implement ptep_set_access_flags() for hardware AF/DBM
    
    When hardware updates of the access and dirty states are enabled, the
    default ptep_set_access_flags() implementation based on calling
    set_pte_at() directly is potentially racy. This triggers the "racy dirty
    state clearing" warning in set_pte_at() because an existing writable PTE
    is overridden with a clean entry.
    
    There are two main scenarios for this situation:
    
    1. The CPU getting an access fault does not support hardware updates of
       the access/dirty flags. However, a different agent in the system
       (e.g. SMMU) can do this, therefore overriding a writable entry with a
       clean one could potentially lose the automatically updated dirty
       status
    
    2. A more complex situation is possible when all CPUs support hardware
       AF/DBM:
    
       a) Initial state: shareable + writable vma and pte_none(pte)
       b) Read fault taken by two threads of the same process on different
          CPUs
       c) CPU0 takes the mmap_sem and proceeds to handling the fault. It
          eventually reaches do_set_pte() which sets a writable + clean pte.
          CPU0 releases the mmap_sem
       d) CPU1 acquires the mmap_sem and proceeds to handle_pte_fault(). The
          pte entry it reads is present, writable and clean and it continues
          to pte_mkyoung()
       e) CPU1 calls ptep_set_access_flags()
    
       If between (d) and (e) the hardware (another CPU) updates the dirty
       state (clears PTE_RDONLY), CPU1 will override the PTR_RDONLY bit
       marking the entry clean again.
    
    This patch implements an arm64-specific ptep_set_access_flags() function
    to perform an atomic update of the PTE flags.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Ming Lei <tom.leiming@gmail.com>
    Tested-by: Julien Grall <julien.grall@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org> # 4.3+
    [will: reworded comment]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c12e96753de9..d24a5e05a3f2 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -81,6 +81,56 @@ void show_pte(struct mm_struct *mm, unsigned long addr)
 	printk("\n");
 }
 
+#ifdef CONFIG_ARM64_HW_AFDBM
+/*
+ * This function sets the access flags (dirty, accessed), as well as write
+ * permission, and only to a more permissive setting.
+ *
+ * It needs to cope with hardware update of the accessed/dirty state by other
+ * agents in the system and can safely skip the __sync_icache_dcache() call as,
+ * like set_pte_at(), the PTE is never changed from no-exec to exec here.
+ *
+ * Returns whether or not the PTE actually changed.
+ */
+int ptep_set_access_flags(struct vm_area_struct *vma,
+			  unsigned long address, pte_t *ptep,
+			  pte_t entry, int dirty)
+{
+	pteval_t old_pteval;
+	unsigned int tmp;
+
+	if (pte_same(*ptep, entry))
+		return 0;
+
+	/* only preserve the access flags and write permission */
+	pte_val(entry) &= PTE_AF | PTE_WRITE | PTE_DIRTY;
+
+	/*
+	 * PTE_RDONLY is cleared by default in the asm below, so set it in
+	 * back if necessary (read-only or clean PTE).
+	 */
+	if (!pte_write(entry) || !dirty)
+		pte_val(entry) |= PTE_RDONLY;
+
+	/*
+	 * Setting the flags must be done atomically to avoid racing with the
+	 * hardware update of the access/dirty state.
+	 */
+	asm volatile("//	ptep_set_access_flags\n"
+	"	prfm	pstl1strm, %2\n"
+	"1:	ldxr	%0, %2\n"
+	"	and	%0, %0, %3		// clear PTE_RDONLY\n"
+	"	orr	%0, %0, %4		// set flags\n"
+	"	stxr	%w1, %0, %2\n"
+	"	cbnz	%w1, 1b\n"
+	: "=&r" (old_pteval), "=&r" (tmp), "+Q" (pte_val(*ptep))
+	: "L" (~PTE_RDONLY), "r" (pte_val(entry)));
+
+	flush_tlb_fix_spurious_fault(vma, address);
+	return 1;
+}
+#endif
+
 /*
  * The kernel tried to access some page that wasn't present.
  */

commit 6afedcd23cfd7ac56c011069e4a8db37b46e4623
Author: James Morse <james.morse@arm.com>
Date:   Wed Apr 13 13:40:00 2016 +0100

    arm64: mm: Add trace_irqflags annotations to do_debug_exception()
    
    With CONFIG_PROVE_LOCKING, CONFIG_DEBUG_LOCKDEP and CONFIG_TRACE_IRQFLAGS
    enabled, lockdep will compare current->hardirqs_enabled with the flags from
    local_irq_save().
    
    When a debug exception occurs, interrupts are disabled in entry.S, but
    lockdep isn't told, resulting in:
    DEBUG_LOCKS_WARN_ON(current->hardirqs_enabled)
    ------------[ cut here ]------------
    WARNING: at ../kernel/locking/lockdep.c:3523
    Modules linked in:
    CPU: 3 PID: 1752 Comm: perf Not tainted 4.5.0-rc4+ #2204
    Hardware name: ARM Juno development board (r1) (DT)
    task: ffffffc974868000 ti: ffffffc975f40000 task.ti: ffffffc975f40000
    PC is at check_flags.part.35+0x17c/0x184
    LR is at check_flags.part.35+0x17c/0x184
    pc : [<ffffff80080fc93c>] lr : [<ffffff80080fc93c>] pstate: 600003c5
    [...]
    ---[ end trace 74631f9305ef5020 ]---
    Call trace:
    [<ffffff80080fc93c>] check_flags.part.35+0x17c/0x184
    [<ffffff80080ffe30>] lock_acquire+0xa8/0xc4
    [<ffffff8008093038>] breakpoint_handler+0x118/0x288
    [<ffffff8008082434>] do_debug_exception+0x3c/0xa8
    [<ffffff80080854b4>] el1_dbg+0x18/0x6c
    [<ffffff80081e82f4>] do_filp_open+0x64/0xdc
    [<ffffff80081d6e60>] do_sys_open+0x140/0x204
    [<ffffff80081d6f58>] SyS_openat+0x10/0x18
    [<ffffff8008085d30>] el0_svc_naked+0x24/0x28
    possible reason: unannotated irqs-off.
    irq event stamp: 65857
    hardirqs last  enabled at (65857): [<ffffff80081fb1c0>] lookup_mnt+0xf4/0x1b4
    hardirqs last disabled at (65856): [<ffffff80081fb188>] lookup_mnt+0xbc/0x1b4
    softirqs last  enabled at (65790): [<ffffff80080bdca4>] __do_softirq+0x1f8/0x290
    softirqs last disabled at (65757): [<ffffff80080be038>] irq_exit+0x9c/0xd0
    
    This patch adds the annotations to do_debug_exception(), while trying not
    to call trace_hardirqs_off() if el1_dbg() interrupted a task that already
    had irqs disabled.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 95df28bc875f..c12e96753de9 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -555,20 +555,33 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 {
 	const struct fault_info *inf = debug_fault_info + DBG_ESR_EVT(esr);
 	struct siginfo info;
+	int rv;
 
-	if (!inf->fn(addr, esr, regs))
-		return 1;
+	/*
+	 * Tell lockdep we disabled irqs in entry.S. Do nothing if they were
+	 * already disabled to preserve the last enabled/disabled addresses.
+	 */
+	if (interrupts_enabled(regs))
+		trace_hardirqs_off();
 
-	pr_alert("Unhandled debug exception: %s (0x%08x) at 0x%016lx\n",
-		 inf->name, esr, addr);
+	if (!inf->fn(addr, esr, regs)) {
+		rv = 1;
+	} else {
+		pr_alert("Unhandled debug exception: %s (0x%08x) at 0x%016lx\n",
+			 inf->name, esr, addr);
+
+		info.si_signo = inf->sig;
+		info.si_errno = 0;
+		info.si_code  = inf->code;
+		info.si_addr  = (void __user *)addr;
+		arm64_notify_die("", regs, &info, 0);
+		rv = 0;
+	}
 
-	info.si_signo = inf->sig;
-	info.si_errno = 0;
-	info.si_code  = inf->code;
-	info.si_addr  = (void __user *)addr;
-	arm64_notify_die("", regs, &info, 0);
+	if (interrupts_enabled(regs))
+		trace_hardirqs_on();
 
-	return 0;
+	return rv;
 }
 
 #ifdef CONFIG_ARM64_PAN

commit 814a2bf957739f367cbebfa1b60237387b72d0ee
Merge: 237045fc3c67 f9310b2f9a19
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 18 19:26:54 2016 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge second patch-bomb from Andrew Morton:
    
     - a couple of hotfixes
    
     - the rest of MM
    
     - a new timer slack control in procfs
    
     - a couple of procfs fixes
    
     - a few misc things
    
     - some printk tweaks
    
     - lib/ updates, notably to radix-tree.
    
     - add my and Nick Piggin's old userspace radix-tree test harness to
       tools/testing/radix-tree/.  Matthew said it was a godsend during the
       radix-tree work he did.
    
     - a few code-size improvements, switching to __always_inline where gcc
       screwed up.
    
     - partially implement character sets in sscanf
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (118 commits)
      sscanf: implement basic character sets
      lib/bug.c: use common WARN helper
      param: convert some "on"/"off" users to strtobool
      lib: add "on"/"off" support to kstrtobool
      lib: update single-char callers of strtobool()
      lib: move strtobool() to kstrtobool()
      include/linux/unaligned: force inlining of byteswap operations
      include/uapi/linux/byteorder, swab: force inlining of some byteswap operations
      include/asm-generic/atomic-long.h: force inlining of some atomic_long operations
      usb: common: convert to use match_string() helper
      ide: hpt366: convert to use match_string() helper
      ata: hpt366: convert to use match_string() helper
      power: ab8500: convert to use match_string() helper
      power: charger_manager: convert to use match_string() helper
      drm/edid: convert to use match_string() helper
      pinctrl: convert to use match_string() helper
      device property: convert to use match_string() helper
      lib/string: introduce match_string() helper
      radix-tree tests: add test for radix_tree_iter_next
      radix-tree tests: add regression3 test
      ...

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit 0e8fb9312fbaf1a687dd731b04d8ab3121c4ff5a
Author: Jan Kara <jack@suse.cz>
Date:   Thu Mar 17 14:19:55 2016 -0700

    mm: remove VM_FAULT_MINOR
    
    The define has a comment from Nick Piggin from 2007:
    
     /* For backwards compat. Remove me quickly. */
    
    I guess 9 years should not be too hurried sense of 'quickly' even for
    kernel measures.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index abe2a9542b3a..97135b61b32a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -295,7 +295,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	up_read(&mm->mmap_sem);
 
 	/*
-	 * Handle the "normal" case first - VM_FAULT_MAJOR / VM_FAULT_MINOR
+	 * Handle the "normal" case first - VM_FAULT_MAJOR
 	 */
 	if (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP |
 			      VM_FAULT_BADACCESS))))

commit 70c8abc28762d04e36c92e07eee2ce6ab41049cb
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Feb 19 14:28:58 2016 +0000

    arm64: User die() instead of panic() in do_page_fault()
    
    The former gives better error reporting on unhandled permission faults
    (introduced by the UAO patches).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index a8eafeceb08a..44e56de23f79 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -235,10 +235,10 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 
 	if (permission_fault(esr) && (addr < USER_DS)) {
 		if (get_fs() == KERNEL_DS)
-			panic("Accessing user space memory with fs=KERNEL_DS");
+			die("Accessing user space memory with fs=KERNEL_DS", regs, esr);
 
 		if (!search_exception_tables(regs->pc))
-			panic("Accessing user space memory outside uaccess.h routines");
+			die("Accessing user space memory outside uaccess.h routines", regs, esr);
 	}
 
 	/*

commit 52d7523d84d534c241ebac5ac89f5c0a6cb51e41
Author: EunTaik Lee <eun.taik.lee@samsung.com>
Date:   Tue Feb 16 04:44:35 2016 +0000

    arm64: mm: allow the kernel to handle alignment faults on user accesses
    
    Although we don't expect to take alignment faults on access to normal
    memory, misbehaving (i.e. buggy) user code can pass MMIO pointers into
    system calls, leading to things like get_user accessing device memory.
    
    Rather than OOPS the kernel, allow any exception fixups to run and
    return something like -EFAULT back to userspace. This makes the
    behaviour more consistent with userspace, even though applications with
    access to device mappings can easily cause other issues if they try
    hard enough.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Eun Taik Lee <eun.taik.lee@samsung.com>
    [will: dropped __kprobes annotation and rewrote commit mesage]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 92ddac1e8ca2..abe2a9542b3a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -371,6 +371,13 @@ static int __kprobes do_translation_fault(unsigned long addr,
 	return 0;
 }
 
+static int do_alignment_fault(unsigned long addr, unsigned int esr,
+			      struct pt_regs *regs)
+{
+	do_bad_area(addr, esr, regs);
+	return 0;
+}
+
 /*
  * This abort handler always returns "fault".
  */
@@ -418,7 +425,7 @@ static struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
 	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
 	{ do_bad,		SIGBUS,  0,		"unknown 32"			},
-	{ do_bad,		SIGBUS,  BUS_ADRALN,	"alignment fault"		},
+	{ do_alignment_fault,	SIGBUS,  BUS_ADRALN,	"alignment fault"		},
 	{ do_bad,		SIGBUS,  0,		"unknown 34"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 35"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 36"			},

commit e950631e84e7e38892ffbeee5e1816b270026b0e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 18 15:50:04 2016 +0000

    arm64: Remove the get_thread_info() function
    
    This function was introduced by previous commits implementing UAO.
    However, it can be replaced with task_thread_info() in
    uao_thread_switch() or get_fs() in do_page_fault() (the latter being
    called only on the current context, so no need for using the saved
    pt_regs).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index d0762a729d01..a8eafeceb08a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -234,7 +234,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	if (permission_fault(esr) && (addr < USER_DS)) {
-		if (get_thread_info(regs->sp)->addr_limit == KERNEL_DS)
+		if (get_fs() == KERNEL_DS)
 			panic("Accessing user space memory with fs=KERNEL_DS");
 
 		if (!search_exception_tables(regs->pc))

commit 705441960033e66b63524521f153fbb28c99ddbd
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:50 2016 +0000

    arm64: kernel: Don't toggle PAN on systems with UAO
    
    If a CPU supports both Privileged Access Never (PAN) and User Access
    Override (UAO), we don't need to disable/re-enable PAN round all
    copy_to_user() like calls.
    
    UAO alternatives cause these calls to use the 'unprivileged' load/store
    instructions, which are overridden to be the privileged kind when
    fs==KERNEL_DS.
    
    This patch changes the copy_to_user() calls to have their PAN toggling
    depend on a new composite 'feature' ARM64_ALT_PAN_NOT_UAO.
    
    If both features are detected, PAN will be enabled, but the copy_to_user()
    alternatives will not be applied. This means PAN will be enabled all the
    time for these functions. If only PAN is detected, the toggling will be
    enabled as normal.
    
    This will save the time taken to disable/re-enable PAN, and allow us to
    catch copy_to_user() accesses that occur with fs==KERNEL_DS.
    
    Futex and swp-emulation code continue to hang their PAN toggling code on
    ARM64_HAS_PAN.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 820d47353cf0..d0762a729d01 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -234,6 +234,9 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	}
 
 	if (permission_fault(esr) && (addr < USER_DS)) {
+		if (get_thread_info(regs->sp)->addr_limit == KERNEL_DS)
+			panic("Accessing user space memory with fs=KERNEL_DS");
+
 		if (!search_exception_tables(regs->pc))
 			panic("Accessing user space memory outside uaccess.h routines");
 	}

commit 57f4959bad0a154aeca125b7d38d1d9471a12422
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:48 2016 +0000

    arm64: kernel: Add support for User Access Override
    
    'User Access Override' is a new ARMv8.2 feature which allows the
    unprivileged load and store instructions to be overridden to behave in
    the normal way.
    
    This patch converts {get,put}_user() and friends to use ldtr*/sttr*
    instructions - so that they can only access EL0 memory, then enables
    UAO when fs==KERNEL_DS so that these functions can access kernel memory.
    
    This allows user space's read/write permissions to be checked against the
    page tables, instead of testing addr<USER_DS, then using the kernel's
    read/write permissions.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: move uao_thread_switch() above dsb()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 92ddac1e8ca2..820d47353cf0 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -192,6 +192,14 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	return fault;
 }
 
+static inline int permission_fault(unsigned int esr)
+{
+	unsigned int ec       = (esr & ESR_ELx_EC_MASK) >> ESR_ELx_EC_SHIFT;
+	unsigned int fsc_type = esr & ESR_ELx_FSC_TYPE;
+
+	return (ec == ESR_ELx_EC_DABT_CUR && fsc_type == ESR_ELx_FSC_PERM);
+}
+
 static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 				   struct pt_regs *regs)
 {
@@ -225,12 +233,10 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
-	/*
-	 * PAN bit set implies the fault happened in kernel space, but not
-	 * in the arch's user access functions.
-	 */
-	if (IS_ENABLED(CONFIG_ARM64_PAN) && (regs->pstate & PSR_PAN_BIT))
-		goto no_context;
+	if (permission_fault(esr) && (addr < USER_DS)) {
+		if (!search_exception_tables(regs->pc))
+			panic("Accessing user space memory outside uaccess.h routines");
+	}
 
 	/*
 	 * As per x86, we may deadlock here. However, since the kernel only
@@ -561,3 +567,16 @@ void cpu_enable_pan(void *__unused)
 	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
 }
 #endif /* CONFIG_ARM64_PAN */
+
+#ifdef CONFIG_ARM64_UAO
+/*
+ * Kernel threads have fs=KERNEL_DS by default, and don't need to call
+ * set_fs(), devtmpfs in particular relies on this behaviour.
+ * We need to enable the feature at runtime (instead of adding it to
+ * PSR_MODE_EL1h) as the feature may not be implemented by the cpu.
+ */
+void cpu_enable_uao(void *__unused)
+{
+	asm(SET_PSTATE_UAO(1));
+}
+#endif /* CONFIG_ARM64_UAO */

commit c03784ee8aa99bf6a412e3febf8801c3df86e0ba
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Nov 23 15:09:36 2015 +0000

    arm64: mm: fix fault_info table xFSC decoding
    
    We are missing descriptions for some valid xFSC values in the fault info
    table (e.g. "TLB conflict abort"), and have erroneous descriptions for
    reserved values (e.g. "asynchronous external abort", "debug event").
    
    This patch adds the missing xFSC values, and removes erroneous decoding
    of values reserved by the architecture, as described in ARM DDI 0487A.h.
    
    At the same time, fixed the unbalanced brackets for the synchronous
    parity error strings in the table.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 19211c4a8911..92ddac1e8ca2 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -393,16 +393,16 @@ static struct fault_info {
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
-	{ do_bad,		SIGBUS,  0,		"reserved access flag fault"	},
+	{ do_bad,		SIGBUS,  0,		"unknown 8"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 access flag fault"	},
-	{ do_bad,		SIGBUS,  0,		"reserved permission fault"	},
+	{ do_bad,		SIGBUS,  0,		"unknown 12"			},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
 	{ do_bad,		SIGBUS,  0,		"synchronous external abort"	},
-	{ do_bad,		SIGBUS,  0,		"asynchronous external abort"	},
+	{ do_bad,		SIGBUS,  0,		"unknown 17"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 18"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 19"			},
 	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
@@ -410,16 +410,16 @@ static struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
 	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
 	{ do_bad,		SIGBUS,  0,		"synchronous parity error"	},
-	{ do_bad,		SIGBUS,  0,		"asynchronous parity error"	},
+	{ do_bad,		SIGBUS,  0,		"unknown 25"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 26"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 27"			},
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
-	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk)" },
 	{ do_bad,		SIGBUS,  0,		"unknown 32"			},
 	{ do_bad,		SIGBUS,  BUS_ADRALN,	"alignment fault"		},
-	{ do_bad,		SIGBUS,  0,		"debug event"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 34"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 35"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 36"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 37"			},
@@ -433,21 +433,21 @@ static struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"unknown 45"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 46"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 47"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 48"			},
+	{ do_bad,		SIGBUS,  0,		"TLB conflict abort"		},
 	{ do_bad,		SIGBUS,  0,		"unknown 49"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 50"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 51"			},
 	{ do_bad,		SIGBUS,  0,		"implementation fault (lockdown abort)" },
-	{ do_bad,		SIGBUS,  0,		"unknown 53"			},
+	{ do_bad,		SIGBUS,  0,		"implementation fault (unsupported exclusive)" },
 	{ do_bad,		SIGBUS,  0,		"unknown 54"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 55"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 56"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 57"			},
-	{ do_bad,		SIGBUS,  0,		"implementation fault (coprocessor abort)" },
+	{ do_bad,		SIGBUS,  0,		"unknown 58" 			},
 	{ do_bad,		SIGBUS,  0,		"unknown 59"			},
 	{ do_bad,		SIGBUS,  0,		"unknown 60"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 61"			},
-	{ do_bad,		SIGBUS,  0,		"unknown 62"			},
+	{ do_bad,		SIGBUS,  0,		"section domain fault"		},
+	{ do_bad,		SIGBUS,  0,		"page domain fault"		},
 	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
 };
 

commit 2dc10ad81fc017837037e60439662e1b16bdffb9
Merge: e627078a0cbd f8f8bdc48851
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 14:47:13 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - "genirq: Introduce generic irq migration for cpu hotunplugged" patch
       merged from tip/irq/for-arm to allow the arm64-specific part to be
       upstreamed via the arm64 tree
    
     - CPU feature detection reworked to cope with heterogeneous systems
       where CPUs may not have exactly the same features.  The features
       reported by the kernel via internal data structures or ELF_HWCAP are
       delayed until all the CPUs are up (and before user space starts)
    
     - Support for 16KB pages, with the additional bonus of a 36-bit VA
       space, though the latter only depending on EXPERT
    
     - Implement native {relaxed, acquire, release} atomics for arm64
    
     - New ASID allocation algorithm which avoids IPI on roll-over, together
       with TLB invalidation optimisations (using local vs global where
       feasible)
    
     - KASan support for arm64
    
     - EFI_STUB clean-up and isolation for the kernel proper (required by
       KASan)
    
     - copy_{to,from,in}_user optimisations (sharing the memcpy template)
    
     - perf: moving arm64 to the arm32/64 shared PMU framework
    
     - L1_CACHE_BYTES increased to 128 to accommodate Cavium hardware
    
     - Support for the contiguous PTE hint on kernel mapping (16 consecutive
       entries may be able to use a single TLB entry)
    
     - Generic CONFIG_HZ now used on arm64
    
     - defconfig updates
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (91 commits)
      arm64/efi: fix libstub build under CONFIG_MODVERSIONS
      ARM64: Enable multi-core scheduler support by default
      arm64/efi: move arm64 specific stub C code to libstub
      arm64: page-align sections for DEBUG_RODATA
      arm64: Fix build with CONFIG_ZONE_DMA=n
      arm64: Fix compat register mappings
      arm64: Increase the max granular size
      arm64: remove bogus TASK_SIZE_64 check
      arm64: make Timer Interrupt Frequency selectable
      arm64/mm: use PAGE_ALIGNED instead of IS_ALIGNED
      arm64: cachetype: fix definitions of ICACHEF_* flags
      arm64: cpufeature: declare enable_cpu_capabilities as static
      genirq: Make the cpuhotplug migration code less noisy
      arm64: Constify hwcap name string arrays
      arm64/kvm: Make use of the system wide safe values
      arm64/debug: Make use of the system wide safe value
      arm64: Move FP/ASIMD hwcap handling to common code
      arm64/HWCAP: Use system wide safe values
      arm64/capabilities: Make use of system wide safe value
      arm64: Delay cpu feature capability checks
      ...

commit dbb4e152b8da1f977d9d8cd7e494ab4ee3622f72
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:50 2015 +0100

    arm64: Delay cpu feature capability checks
    
    At the moment we run through the arm64_features capability list for
    each CPU and set the capability if one of the CPU supports it. This
    could be problematic in a heterogeneous system with differing capabilities.
    Delay the CPU feature checks until all the enabled CPUs are up(i.e,
    smp_cpus_done(), so that we can make better decisions based on the
    overall system capability. Once we decide and advertise the capabilities
    the alternatives can be applied. From this state, we cannot roll back
    a feature to disabled based on the values from a new hotplugged CPU,
    due to the runtime patching and other reasons. So, for all new CPUs,
    we need to make sure that they have the established system capabilities.
    Failing which, we bring the CPU down, preventing it from turning online.
    Once the capabilities are decided, any new CPU booting up goes through
    verification to ensure that it has all the enabled capabilities and also
    invokes the respective enable() method on the CPU.
    
    The CPU errata checks are not delayed and is still executed per-CPU
    to detect the respective capabilities. If we ever come across a non-errata
    capability that needs to be checked on each-CPU, we could introduce them via
    a new capability table(or introduce a flag), which can be processed per CPU.
    
    The next patch will make the feature checks use the system wide
    safe value of a feature register.
    
    NOTE: The enable() methods associated with the capability is scheduled
    on all the CPUs (which is the only use case at the moment). If we need
    a different type of 'enable()' which only needs to be run once on any CPU,
    we should be able to handle that when needed.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    [catalin.marinas@arm.com: static variable and coding style fixes]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index aba9ead1384c..066d5aea16c6 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -555,7 +555,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 }
 
 #ifdef CONFIG_ARM64_PAN
-void cpu_enable_pan(void)
+void cpu_enable_pan(void *__unused)
 {
 	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
 }

commit 569ba74a7ba69f46ce2950bf085b37fea2408385
Author: Mark Salyzyn <salyzyn@android.com>
Date:   Mon Sep 21 21:39:50 2015 +0100

    arm64: readahead: fault retry breaks mmap file read random detection
    
    This is the arm64 portion of commit 45cac65b0fcd ("readahead: fault
    retry breaks mmap file read random detection"), which was absent from
    the initial port and has since gone unnoticed. The original commit says:
    
    > .fault now can retry.  The retry can break state machine of .fault.  In
    > filemap_fault, if page is miss, ra->mmap_miss is increased.  In the second
    > try, since the page is in page cache now, ra->mmap_miss is decreased.  And
    > these are done in one fault, so we can't detect random mmap file access.
    >
    > Add a new flag to indicate .fault is tried once.  In the second try, skip
    > ra->mmap_miss decreasing.  The filemap_fault state machine is ok with it.
    
    With this change, Mark reports that:
    
    > Random read improves by 250%, sequential read improves by 40%, and
    > random write by 400% to an eMMC device with dm crypto wrapped around it.
    
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Mark Salyzyn <salyzyn@android.com>
    Signed-off-by: Riley Andrews <riandrews@android.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index aba9ead1384c..9fadf6d7039b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -287,6 +287,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			 * starvation.
 			 */
 			mm_flags &= ~FAULT_FLAG_ALLOW_RETRY;
+			mm_flags |= FAULT_FLAG_TRIED;
 			goto retry;
 		}
 	}

commit 9fb7410f955f7a62c1f882ca8f9ffd4525907e28
Author: Dave P Martin <Dave.Martin@arm.com>
Date:   Fri Jul 24 16:37:48 2015 +0100

    arm64/BUG: Use BRK instruction for generic BUG traps
    
    Currently, the minimal default BUG() implementation from asm-
    generic is used for arm64.
    
    This patch uses the BRK software breakpoint instruction to generate
    a trap instead, similarly to most other arches, with the generic
    BUG code generating the dmesg boilerplate.
    
    This allows bug metadata to be moved to a separate table and
    reduces the amount of inline code at BUG and WARN sites.  This also
    avoids clobbering any registers before they can be dumped.
    
    To mitigate the size of the bug table further, this patch makes
    use of the existing infrastructure for encoding addresses within
    the bug table as 32-bit offsets instead of absolute pointers.
    (Note that this limits the kernel size to 2GB.)
    
    Traps are registered at arch_initcall time for aarch64, but BUG
    has minimal real dependencies and it is desirable to be able to
    generate bug splats as early as possible.  This patch redirects
    all debug exceptions caused by BRK directly to bug_handler() until
    the full debug exception support has been initialised.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index ce591211434e..aba9ead1384c 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -501,14 +501,22 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 	arm64_notify_die("Oops - SP/PC alignment exception", regs, &info, esr);
 }
 
-static struct fault_info debug_fault_info[] = {
+int __init early_brk64(unsigned long addr, unsigned int esr,
+		       struct pt_regs *regs);
+
+/*
+ * __refdata because early_brk64 is __init, but the reference to it is
+ * clobbered at arch_initcall time.
+ * See traps.c and debug-monitors.c:debug_traps_init().
+ */
+static struct fault_info __refdata debug_fault_info[] = {
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware breakpoint"	},
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware single-step"	},
 	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware watchpoint"	},
 	{ do_bad,	SIGBUS,		0,		"unknown 3"		},
 	{ do_bad,	SIGTRAP,	TRAP_BRKPT,	"aarch32 BKPT"		},
 	{ do_bad,	SIGTRAP,	0,		"aarch32 vector catch"	},
-	{ do_bad,	SIGTRAP,	TRAP_BRKPT,	"aarch64 BRK"		},
+	{ early_brk64,	SIGTRAP,	TRAP_BRKPT,	"aarch64 BRK"		},
 	{ do_bad,	SIGBUS,		0,		"unknown 7"		},
 };
 

commit 338d4f49d6f7114a017d294ccf7374df4f998edc
Author: James Morse <james.morse@arm.com>
Date:   Wed Jul 22 19:05:54 2015 +0100

    arm64: kernel: Add support for Privileged Access Never
    
    'Privileged Access Never' is a new arm8.1 feature which prevents
    privileged code from accessing any virtual address where read or write
    access is also permitted at EL0.
    
    This patch enables the PAN feature on all CPUs, and modifies {get,put}_user
    helpers temporarily to permit access.
    
    This will catch kernel bugs where user memory is accessed directly.
    'Unprivileged loads and stores' using ldtrb et al are unaffected by PAN.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [will: use ALTERNATIVE in asm and tidy up pan_enable check]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 94d98cd1aad8..ce591211434e 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -30,9 +30,11 @@
 #include <linux/highmem.h>
 #include <linux/perf_event.h>
 
+#include <asm/cpufeature.h>
 #include <asm/exception.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
+#include <asm/sysreg.h>
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
@@ -223,6 +225,13 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
 
+	/*
+	 * PAN bit set implies the fault happened in kernel space, but not
+	 * in the arch's user access functions.
+	 */
+	if (IS_ENABLED(CONFIG_ARM64_PAN) && (regs->pstate & PSR_PAN_BIT))
+		goto no_context;
+
 	/*
 	 * As per x86, we may deadlock here. However, since the kernel only
 	 * validly references user space from well defined areas of the code,
@@ -536,3 +545,10 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 
 	return 0;
 }
+
+#ifdef CONFIG_ARM64_PAN
+void cpu_enable_pan(void)
+{
+	config_sctlr_el1(SCTLR_EL1_SPAN, 0);
+}
+#endif /* CONFIG_ARM64_PAN */

commit 6361c845ceab326a306412349fa6b125700b2cec
Merge: 31351f73ea37 f871d2680707
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 3 12:28:30 2015 -0700

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes (and cleanups) from Catalin Marinas:
     "Various arm64 fixes:
    
       - suspicious RCU usage warning
       - BPF (out of bounds array read and endianness conversion)
       - perf (of_node usage after of_node_put, cpu_pmu->plat_device
         assignment)
       - huge pmd/pud check for value 0
       - rate-limiting should only take unhandled signals into account
    
      Clean-up:
    
       - incorrect use of pgprot_t type
       - unused header include
       - __init annotation to arm_cpuidle_init
       - pr_debug instead of pr_error for disabled GICC entries in
         ACPI/MADT"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: Fix show_unhandled_signal_ratelimited usage
      ARM64 / SMP: Switch pr_err() to pr_debug() for disabled GICC entry
      arm64: cpuidle: add __init section marker to arm_cpuidle_init
      arm64: Don't report clear pmds and puds as huge
      arm64: perf: fix unassigned cpu_pmu->plat_device when probing PMU PPIs
      arm64: perf: Don't use of_node after putting it
      arm64: fix incorrect use of pgprot_t variable
      arm64/hw_breakpoint.c: remove unnecessary header
      arm64: bpf: fix endianness conversion bugs
      arm64: bpf: fix out-of-bounds read in bpf2a64_offset()
      ARM64: smp: Fix suspicious RCU usage with ipi tracepoints

commit f871d26807078cf4cc0a64a97ee2c6bb513a4397
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Fri Jul 3 15:08:08 2015 +0100

    arm64: Fix show_unhandled_signal_ratelimited usage
    
    Commit 86dca36e6ba introduced ratelimited usage for
    'unhandled_signal' messages.
    The commit checks the ratelimit irrespective of whether
    the signal is handled or not, which is wrong and leads
    to false reports like the below in dmesg :
    
    __do_user_fault: 127 callbacks suppressed
    
    Do the ratelimit check only if the signal is unhandled.
    
    Fixes: 86dca36e6ba0 ("arm64: use private ratelimit state along with show_unhandled_signals")
    Cc: Vladimir Murzin <Vladimir.Murzin@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 66bd92ab6f7b..ffa36e2d18e6 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -115,7 +115,7 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 {
 	struct siginfo si;
 
-	if (show_unhandled_signals_ratelimited() && unhandled_signal(tsk, sig)) {
+	if (unhandled_signal(tsk, sig) && show_unhandled_signals_ratelimited()) {
 		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
 			tsk->comm, task_pid_nr(tsk), fault_name(esr), sig,
 			addr, esr);

commit e3d8238d7f5c3f539a29f5ac596cd342d847e099
Merge: 4e241557fc1c 86dca36e6ba0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 10:02:15 2015 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Mostly refactoring/clean-up:
    
       - CPU ops and PSCI (Power State Coordination Interface) refactoring
         following the merging of the arm64 ACPI support, together with
         handling of Trusted (secure) OS instances
    
       - Using fixmap for permanent FDT mapping, removing the initial dtb
         placement requirements (within 512MB from the start of the kernel
         image).  This required moving the FDT self reservation out of the
         memreserve processing
    
       - Idmap (1:1 mapping used for MMU on/off) handling clean-up
    
       - Removing flush_cache_all() - not safe on ARM unless the MMU is off.
         Last stages of CPU power down/up are handled by firmware already
    
       - "Alternatives" (run-time code patching) refactoring and support for
         immediate branch patching, GICv3 CPU interface access
    
       - User faults handling clean-up
    
      And some fixes:
    
       - Fix for VDSO building with broken ELF toolchains
    
       - Fix another case of init_mm.pgd usage for user mappings (during
         ASID roll-over broadcasting)
    
       - Fix for FPSIMD reloading after CPU hotplug
    
       - Fix for missing syscall trace exit
    
       - Workaround for .inst asm bug
    
       - Compat fix for switching the user tls tpidr_el0 register"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (42 commits)
      arm64: use private ratelimit state along with show_unhandled_signals
      arm64: show unhandled SP/PC alignment faults
      arm64: vdso: work-around broken ELF toolchains in Makefile
      arm64: kernel: rename __cpu_suspend to keep it aligned with arm
      arm64: compat: print compat_sp instead of sp
      arm64: mm: Fix freeing of the wrong memmap entries with !SPARSEMEM_VMEMMAP
      arm64: entry: fix context tracking for el0_sp_pc
      arm64: defconfig: enable memtest
      arm64: mm: remove reference to tlb.S from comment block
      arm64: Do not attempt to use init_mm in reset_context()
      arm64: KVM: Switch vgic save/restore to alternative_insn
      arm64: alternative: Introduce feature for GICv3 CPU interface
      arm64: psci: fix !CONFIG_HOTPLUG_CPU build warning
      arm64: fix bug for reloading FPSIMD state after CPU hotplug.
      arm64: kernel thread don't need to save fpsimd context.
      arm64: fix missing syscall trace exit
      arm64: alternative: Work around .inst assembler bugs
      arm64: alternative: Merge alternative-asm.h into alternative.h
      arm64: alternative: Allow immediate branch as alternative instruction
      arm64: Rework alternate sequence for ARM erratum 845719
      ...

commit 86dca36e6ba019650a94cadf922ea3d06dec0182
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Fri Jun 19 15:28:03 2015 +0100

    arm64: use private ratelimit state along with show_unhandled_signals
    
    printk_ratelimit() shares the ratelimiting state with other callers what
    may lead to scenarios where at the time we want to print out debug
    information we already limited, so nothing appears in the dmesg - this
    makes exception-trace quite poor helper in debugging.
    
    Additionally, we have imbalance with some messages limited with global
    ratelimit state and other messages limited with their private state
    defined via pr_*_ratelimited().
    
    To address this inconsistency show_unhandled_signals_ratelimited()
    macro is introduced and caller sites are converted to use it.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index e8c608c893cf..66bd92ab6f7b 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -115,8 +115,7 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 {
 	struct siginfo si;
 
-	if (show_unhandled_signals && unhandled_signal(tsk, sig) &&
-	    printk_ratelimit()) {
+	if (show_unhandled_signals_ratelimited() && unhandled_signal(tsk, sig)) {
 		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
 			tsk->comm, task_pid_nr(tsk), fault_name(esr), sig,
 			addr, esr);

commit 9e793ab84ed482047f7226595313f0f3a0aa6854
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Fri Jun 19 15:28:16 2015 +0100

    arm64: show unhandled SP/PC alignment faults
    
    Report unhandled SP/PC alignment faults if the show_unhandled_signals
    variable is set (via /proc/sys/debug/exception-trace).
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 96da13167d4a..e8c608c893cf 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -478,12 +478,19 @@ asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
 					   struct pt_regs *regs)
 {
 	struct siginfo info;
+	struct task_struct *tsk = current;
+
+	if (show_unhandled_signals && unhandled_signal(tsk, SIGBUS))
+		pr_info_ratelimited("%s[%d]: %s exception: pc=%p sp=%p\n",
+				    tsk->comm, task_pid_nr(tsk),
+				    esr_get_class_string(esr), (void *)regs->pc,
+				    (void *)regs->sp);
 
 	info.si_signo = SIGBUS;
 	info.si_errno = 0;
 	info.si_code  = BUS_ADRALN;
 	info.si_addr  = (void __user *)addr;
-	arm64_notify_die("", regs, &info, esr);
+	arm64_notify_die("Oops - SP/PC alignment exception", regs, &info, esr);
 }
 
 static struct fault_info debug_fault_info[] = {

commit 70ffdb9393a7264a069265edded729078dcf0425
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 11 17:52:11 2015 +0200

    mm/fault, arch: Use pagefault_disable() to check for disabled pagefaults in the handler
    
    Introduce faulthandler_disabled() and use it to check for irq context and
    disabled pagefaults (via pagefault_disable()) in the pagefault handlers.
    
    Please note that we keep the in_atomic() checks in place - to detect
    whether in irq context (in which case preemption is always properly
    disabled).
    
    In contrast, preempt_disable() should never be used to disable pagefaults.
    With !CONFIG_PREEMPT_COUNT, preempt_disable() doesn't modify the preempt
    counter, and therefore the result of in_atomic() differs.
    We validate that condition by using might_fault() checks when calling
    might_sleep().
    
    Therefore, add a comment to faulthandler_disabled(), describing why this
    is needed.
    
    faulthandler_disabled() and pagefault_disable() are defined in
    linux/uaccess.h, so let's properly add that include to all relevant files.
    
    This patch is based on a patch from Thomas Gleixner.
    
    Reviewed-and-tested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: benh@kernel.crashing.org
    Cc: bigeasy@linutronix.de
    Cc: borntraeger@de.ibm.com
    Cc: daniel.vetter@intel.com
    Cc: heiko.carstens@de.ibm.com
    Cc: herbert@gondor.apana.org.au
    Cc: hocko@suse.cz
    Cc: hughd@google.com
    Cc: mst@redhat.com
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: schwidefsky@de.ibm.com
    Cc: yang.shi@windriver.com
    Link: http://lkml.kernel.org/r/1431359540-32227-7-git-send-email-dahi@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 96da13167d4a..0948d327d013 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -211,7 +211,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	 * If we're in an interrupt or have no user context, we must not take
 	 * the fault.
 	 */
-	if (in_atomic() || !mm)
+	if (faulthandler_disabled() || !mm)
 		goto no_context;
 
 	if (user_mode(regs))

commit aed40e0144bdbdadb45f2b623e07aa3157eb74c9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Nov 24 12:31:40 2014 +0000

    arm64: move to ESR_ELx macros
    
    Now that we have common ESR_ELx_* macros, move the core arm64 code over
    to them.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c11cd27ca8f5..96da13167d4a 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -219,7 +219,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 
 	if (esr & ESR_LNX_EXEC) {
 		vm_flags = VM_EXEC;
-	} else if ((esr & ESR_EL1_WRITE) && !(esr & ESR_EL1_CM)) {
+	} else if ((esr & ESR_ELx_WNR) && !(esr & ESR_ELx_CM)) {
 		vm_flags = VM_WRITE;
 		mm_flags |= FAULT_FLAG_WRITE;
 	}

commit 7f73f7aef824b8bc27046edaf6b73bca4b0e7669
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Nov 21 14:22:22 2014 +0000

    arm64: mm: report unhandled level-0 translation faults correctly
    
    Translation faults that occur due to the input address being outside
    of the address range mapped by the relevant base register are reported
    as level 0 faults in ESR.DFSC.
    
    If the faulting access cannot be resolved by the kernel (e.g. because
    it is not mapped by a vma), then we report "input address range fault"
    on the console. This was fine until we added support for 48-bit VAs,
    which actually place PGDs at level 0 and can trigger faults for invalid
    addresses that are within the range of the page tables.
    
    This patch changes the string to report "level 0 translation fault",
    which is far less confusing.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 41cb6d3d6075..c11cd27ca8f5 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -380,7 +380,7 @@ static struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"level 1 address size fault"	},
 	{ do_bad,		SIGBUS,  0,		"level 2 address size fault"	},
 	{ do_bad,		SIGBUS,  0,		"level 3 address size fault"	},
-	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"input address range fault"	},
+	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 0 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},

commit c79b954bf6c006f2d3dd9d01f231abeead13a410
Author: Jungseok Lee <jays.lee@samsung.com>
Date:   Mon May 12 18:40:51 2014 +0900

    arm64: mm: Implement 4 levels of translation tables
    
    This patch implements 4 levels of translation tables since 3 levels
    of page tables with 4KB pages cannot support 40-bit physical address
    space described in [1] due to the following issue.
    
    It is a restriction that kernel logical memory map with 4KB + 3 levels
    (0xffffffc000000000-0xffffffffffffffff) cannot cover RAM region from
    544GB to 1024GB in [1]. Specifically, ARM64 kernel fails to create
    mapping for this region in map_mem function since __phys_to_virt for
    this region reaches to address overflow.
    
    If SoC design follows the document, [1], over 32GB RAM would be placed
    from 544GB. Even 64GB system is supposed to use the region from 544GB
    to 576GB for only 32GB RAM. Naturally, it would reach to enable 4 levels
    of page tables to avoid hacking __virt_to_phys and __phys_to_virt.
    
    However, it is recommended 4 levels of page table should be only enabled
    if memory map is too sparse or there is about 512GB RAM.
    
    References
    ----------
    [1]: Principles of ARM Memory Maps, White Paper, Issue C
    
    Signed-off-by: Jungseok Lee <jays.lee@samsung.com>
    Reviewed-by: Sungjinn Chung <sungjinn.chung@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Steve Capper <steve.capper@linaro.org>
    [catalin.marinas@arm.com: MEMBLOCK_INITIAL_LIMIT removed, same as PUD_SIZE]
    [catalin.marinas@arm.com: early_ioremap_init() updated for 4 levels]
    [catalin.marinas@arm.com: 48-bit VA depends on BROKEN until KVM is fixed]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index bcc965e2cce1..41cb6d3d6075 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -62,6 +62,7 @@ void show_pte(struct mm_struct *mm, unsigned long addr)
 			break;
 
 		pud = pud_offset(pgd, addr);
+		printk(", *pud=%016llx", pud_val(*pud));
 		if (pud_none(*pud) || pud_bad(*pud))
 			break;
 

commit 5a0fdfada3a2aa50d7b947a2e958bf00cbe0d830
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 16 16:44:32 2014 +0100

    Revert "arm64: Introduce execute-only page access permissions"
    
    This reverts commit bc07c2c6e9ed125d362af0214b6313dca180cb08.
    
    While the aim is increased security for --x memory maps, it does not
    protect against kernel level reads. Until SECCOMP is implemented for
    arm64, revert this patch to avoid giving a false idea of execute-only
    mappings.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 89c6763d5e7e..bcc965e2cce1 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -173,7 +173,8 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 good_area:
 	/*
 	 * Check that the permissions on the VMA allow for the fault which
-	 * occurred.
+	 * occurred. If we encountered a write or exec fault, we must have
+	 * appropriate permissions, otherwise we allow any permission.
 	 */
 	if (!(vma->vm_flags & vm_flags)) {
 		fault = VM_FAULT_BADACCESS;
@@ -195,7 +196,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	int fault, sig, code;
-	unsigned long vm_flags = VM_READ | VM_WRITE;
+	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
 	tsk = current;

commit bc07c2c6e9ed125d362af0214b6313dca180cb08
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Apr 3 16:17:32 2014 +0100

    arm64: Introduce execute-only page access permissions
    
    The ARMv8 architecture allows execute-only user permissions by clearing
    the PTE_UXN and PTE_USER bits. The kernel, however, can still access
    such page, so execute-only page permission does not protect against
    read(2)/write(2) etc. accesses. Systems requiring such protection must
    implement/enable features like SECCOMP.
    
    This patch changes the arm64 __P100 and __S100 protection_map[] macros
    to the new __PAGE_EXECONLY attributes. A side effect is that
    pte_valid_user() no longer triggers for __PAGE_EXECONLY since PTE_USER
    isn't set. To work around this, the check is done on the PTE_NG bit via
    the pte_valid_ng() macro. VM_READ is also checked now for page faults.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index bcc965e2cce1..89c6763d5e7e 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -173,8 +173,7 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 good_area:
 	/*
 	 * Check that the permissions on the VMA allow for the fault which
-	 * occurred. If we encountered a write or exec fault, we must have
-	 * appropriate permissions, otherwise we allow any permission.
+	 * occurred.
 	 */
 	if (!(vma->vm_flags & vm_flags)) {
 		fault = VM_FAULT_BADACCESS;
@@ -196,7 +195,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	int fault, sig, code;
-	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
+	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
 	tsk = current;

commit 9141300a5884b57cea6d32c4e3fd16a337cfc99a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sun Apr 6 23:04:12 2014 +0100

    arm64: Provide read/write fault information in compat signal handlers
    
    For AArch32, bit 11 (WnR) of the FSR/ESR register is set when the fault
    was caused by a write access and applications like Qemu rely on such
    information being provided in sigcontext. This patch introduces the
    ESR_EL1 tracking for the arm64 kernel faults and sets bit 11 accordingly
    in compat sigcontext.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index c23751b06120..bcc965e2cce1 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -32,6 +32,7 @@
 
 #include <asm/exception.h>
 #include <asm/debug-monitors.h>
+#include <asm/esr.h>
 #include <asm/system_misc.h>
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
@@ -123,6 +124,7 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 	}
 
 	tsk->thread.fault_address = addr;
+	tsk->thread.fault_code = esr;
 	si.si_signo = sig;
 	si.si_errno = 0;
 	si.si_code = code;
@@ -148,8 +150,6 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re
 #define VM_FAULT_BADMAP		0x010000
 #define VM_FAULT_BADACCESS	0x020000
 
-#define ESR_WRITE		(1 << 6)
-#define ESR_CM			(1 << 8)
 #define ESR_LNX_EXEC		(1 << 24)
 
 static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
@@ -218,7 +218,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 
 	if (esr & ESR_LNX_EXEC) {
 		vm_flags = VM_EXEC;
-	} else if ((esr & ESR_WRITE) && !(esr & ESR_CM)) {
+	} else if ((esr & ESR_EL1_WRITE) && !(esr & ESR_EL1_CM)) {
 		vm_flags = VM_WRITE;
 		mm_flags |= FAULT_FLAG_WRITE;
 	}
@@ -525,7 +525,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
 	info.si_errno = 0;
 	info.si_code  = inf->code;
 	info.si_addr  = (void __user *)addr;
-	arm64_notify_die("", regs, &info, esr);
+	arm64_notify_die("", regs, &info, 0);
 
 	return 0;
 }

commit 59f67e16e6b79697241c3fd030e3da300377893e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 16 15:18:28 2013 +0100

    arm64: Make do_bad_area() function static
    
    This function is only called from arch/arm64/mm/fault.c.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6d6acf153bff..c23751b06120 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -130,7 +130,7 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 	force_sig_info(sig, &si, tsk);
 }
 
-void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
+static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 {
 	struct task_struct *tsk = current;
 	struct mm_struct *mm = tsk->active_mm;

commit 759496ba6407c6994d6a5ce3a5e74937d7816208
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Sep 12 15:13:39 2013 -0700

    arch: mm: pass userspace fault flag to generic fault handler
    
    Unlike global OOM handling, memory cgroup code will invoke the OOM killer
    in any OOM situation because it has no way of telling faults occuring in
    kernel context - which could be handled more gracefully - from
    user-triggered faults.
    
    Pass a flag that identifies faults originating in user space from the
    architecture-specific fault handlers to generic code so that memcg OOM
    handling can be improved.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: azurIt <azurit@pobox.sk>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0bb7db41f4fe..6d6acf153bff 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -199,13 +199,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
-	if (esr & ESR_LNX_EXEC) {
-		vm_flags = VM_EXEC;
-	} else if ((esr & ESR_WRITE) && !(esr & ESR_CM)) {
-		vm_flags = VM_WRITE;
-		mm_flags |= FAULT_FLAG_WRITE;
-	}
-
 	tsk = current;
 	mm  = tsk->mm;
 
@@ -220,6 +213,16 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	if (in_atomic() || !mm)
 		goto no_context;
 
+	if (user_mode(regs))
+		mm_flags |= FAULT_FLAG_USER;
+
+	if (esr & ESR_LNX_EXEC) {
+		vm_flags = VM_EXEC;
+	} else if ((esr & ESR_WRITE) && !(esr & ESR_CM)) {
+		vm_flags = VM_WRITE;
+		mm_flags |= FAULT_FLAG_WRITE;
+	}
+
 	/*
 	 * As per x86, we may deadlock here. However, since the kernel only
 	 * validly references user space from well defined areas of the code,

commit 871341023c771ad233620b7a1fb3d9c7031c4e5c
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Thu Sep 12 15:13:38 2013 -0700

    arch: mm: do not invoke OOM killer on kernel fault OOM
    
    Kernel faults are expected to handle OOM conditions gracefully (gup,
    uaccess etc.), so they should never invoke the OOM killer.  Reserve this
    for faults triggered in user context when it is the only option.
    
    Most architectures already do this, fix up the remaining few.
    
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Reviewed-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: azurIt <azurit@pobox.sk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 6c8ba25bf6bb..0bb7db41f4fe 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -288,6 +288,13 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			      VM_FAULT_BADACCESS))))
 		return 0;
 
+	/*
+	 * If we are in kernel mode at this point, we have no context to
+	 * handle this fault with.
+	 */
+	if (!user_mode(regs))
+		goto no_context;
+
 	if (fault & VM_FAULT_OOM) {
 		/*
 		 * We ran out of memory, call the OOM killer, and return to
@@ -298,13 +305,6 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 		return 0;
 	}
 
-	/*
-	 * If we are in kernel mode at this point, we have no context to
-	 * handle this fault with.
-	 */
-	if (!user_mode(regs))
-		goto no_context;
-
 	if (fault & VM_FAULT_SIGBUS) {
 		/*
 		 * We had some memory, but were unable to successfully fix up

commit db6f41063cbdb58b14846e600e6bc3f4e4c2e888
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jul 19 15:37:12 2013 +0100

    arm64: mm: don't treat user cache maintenance faults as writes
    
    On arm64, cache maintenance faults appear as data aborts with the CM
    bit set in the ESR. The WnR bit, usually used to distinguish between
    faulting loads and stores, always reads as 1 and (slightly confusingly)
    the instructions are treated as reads by the architecture.
    
    This patch fixes our fault handling code to treat cache maintenance
    faults in the same way as loads.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 0ecac8980aae..6c8ba25bf6bb 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -152,25 +152,8 @@ void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 #define ESR_CM			(1 << 8)
 #define ESR_LNX_EXEC		(1 << 24)
 
-/*
- * Check that the permissions on the VMA allow for the fault which occurred.
- * If we encountered a write fault, we must have write permission, otherwise
- * we allow any permission.
- */
-static inline bool access_error(unsigned int esr, struct vm_area_struct *vma)
-{
-	unsigned int mask = VM_READ | VM_WRITE | VM_EXEC;
-
-	if (esr & ESR_WRITE)
-		mask = VM_WRITE;
-	if (esr & ESR_LNX_EXEC)
-		mask = VM_EXEC;
-
-	return vma->vm_flags & mask ? false : true;
-}
-
 static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
-			   unsigned int esr, unsigned int flags,
+			   unsigned int mm_flags, unsigned long vm_flags,
 			   struct task_struct *tsk)
 {
 	struct vm_area_struct *vma;
@@ -188,12 +171,17 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	 * it.
 	 */
 good_area:
-	if (access_error(esr, vma)) {
+	/*
+	 * Check that the permissions on the VMA allow for the fault which
+	 * occurred. If we encountered a write or exec fault, we must have
+	 * appropriate permissions, otherwise we allow any permission.
+	 */
+	if (!(vma->vm_flags & vm_flags)) {
 		fault = VM_FAULT_BADACCESS;
 		goto out;
 	}
 
-	return handle_mm_fault(mm, vma, addr & PAGE_MASK, flags);
+	return handle_mm_fault(mm, vma, addr & PAGE_MASK, mm_flags);
 
 check_stack:
 	if (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))
@@ -208,9 +196,15 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	int fault, sig, code;
-	bool write = (esr & ESR_WRITE) && !(esr & ESR_CM);
-	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |
-		(write ? FAULT_FLAG_WRITE : 0);
+	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
+	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
+
+	if (esr & ESR_LNX_EXEC) {
+		vm_flags = VM_EXEC;
+	} else if ((esr & ESR_WRITE) && !(esr & ESR_CM)) {
+		vm_flags = VM_WRITE;
+		mm_flags |= FAULT_FLAG_WRITE;
+	}
 
 	tsk = current;
 	mm  = tsk->mm;
@@ -248,7 +242,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 #endif
 	}
 
-	fault = __do_page_fault(mm, addr, esr, flags, tsk);
+	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
@@ -265,7 +259,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	 */
 
 	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
-	if (flags & FAULT_FLAG_ALLOW_RETRY) {
+	if (mm_flags & FAULT_FLAG_ALLOW_RETRY) {
 		if (fault & VM_FAULT_MAJOR) {
 			tsk->maj_flt++;
 			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs,
@@ -280,7 +274,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 			 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of
 			 * starvation.
 			 */
-			flags &= ~FAULT_FLAG_ALLOW_RETRY;
+			mm_flags &= ~FAULT_FLAG_ALLOW_RETRY;
 			goto retry;
 		}
 	}

commit 084bd29810a5689e423d2f085255a3200a03a06e
Author: Steve Capper <steve.capper@linaro.org>
Date:   Wed Apr 10 13:48:00 2013 +0100

    ARM64: mm: HugeTLB support.
    
    Add huge page support to ARM64, different huge page sizes are
    supported depending on the size of normal pages:
    
    PAGE_SIZE is 4KB:
       2MB - (pmds) these can be allocated at any time.
    1024MB - (puds) usually allocated on bootup with the command line
             with something like: hugepagesz=1G hugepages=6
    
    PAGE_SIZE is 64KB:
     512MB - (pmds) usually allocated on bootup via command line.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 1426468b77f3..0ecac8980aae 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -364,17 +364,6 @@ static int __kprobes do_translation_fault(unsigned long addr,
 	return 0;
 }
 
-/*
- * Some section permission faults need to be handled gracefully.  They can
- * happen due to a __{get,put}_user during an oops.
- */
-static int do_sect_fault(unsigned long addr, unsigned int esr,
-			 struct pt_regs *regs)
-{
-	do_bad_area(addr, esr, regs);
-	return 0;
-}
-
 /*
  * This abort handler always returns "fault".
  */
@@ -398,12 +387,12 @@ static struct fault_info {
 	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
 	{ do_bad,		SIGBUS,  0,		"reserved access flag fault"	},
-	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
-	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 access flag fault"	},
 	{ do_bad,		SIGBUS,  0,		"reserved permission fault"	},
-	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
-	{ do_sect_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
 	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
 	{ do_bad,		SIGBUS,  0,		"synchronous external abort"	},
 	{ do_bad,		SIGBUS,  0,		"asynchronous external abort"	},

commit 953dbbed9ee310100bc841cdea8f992d192531c6
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue May 21 12:16:56 2013 +0100

    arm64: Do not report user faults for handled signals
    
    Currently user faults (page, undefined instruction) are always reported
    even though the user may have a signal handler for them. This patch adds
    unhandled_signal() check together with printk_ratelimit() for these
    cases.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 98af6e760cce..1426468b77f3 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -113,7 +113,8 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 {
 	struct siginfo si;
 
-	if (show_unhandled_signals) {
+	if (show_unhandled_signals && unhandled_signal(tsk, sig) &&
+	    printk_ratelimit()) {
 		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
 			tsk->comm, task_pid_nr(tsk), fault_name(esr), sig,
 			addr, esr);

commit 0e7f7bcc3fc87489cda5aa6aff8ce40eed912279
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue May 7 16:57:06 2013 +0100

    arm64: Ignore the 'write' ESR flag on cache maintenance faults
    
    ESR.WnR bit is always set on data cache maintenance faults even though
    the page is not required to have write permission. If a translation
    fault (page not yet mapped) happens for read-only user address range,
    Linux incorrectly assumes a permission fault. This patch adds the check
    of the ESR.CM bit during the page fault handling to ignore the 'write'
    flag.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Tim Northover <Tim.Northover@arm.com>
    Cc: stable@vger.kernel.org

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 52638171d6fd..98af6e760cce 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -148,6 +148,7 @@ void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
 #define VM_FAULT_BADACCESS	0x020000
 
 #define ESR_WRITE		(1 << 6)
+#define ESR_CM			(1 << 8)
 #define ESR_LNX_EXEC		(1 << 24)
 
 /*
@@ -206,7 +207,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
 	struct task_struct *tsk;
 	struct mm_struct *mm;
 	int fault, sig, code;
-	int write = esr & ESR_WRITE;
+	bool write = (esr & ESR_WRITE) && !(esr & ESR_CM);
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |
 		(write ? FAULT_FLAG_WRITE : 0);
 

commit 4339e3f389081ea90e230a785bdbe10eccd02b71
Author: Steve Capper <steve.capper@linaro.org>
Date:   Fri Apr 19 15:49:31 2013 +0100

    arm64: mm: Correct show_pte behaviour
    
    show_pte makes use of the *_none_or_clear_bad style functions. If a
    pgd, pud or pmd is identified as being bad, it will then be cleared.
    
    As show_pte appears to be called from either the user or kernel
    fault handlers this side effect can lead to unpredictable behaviour;
    especially as TLB entries are not invalidated.
    
    This patch removes the page table sanitisation from show_pte. If a
    bad pgd, pud or pmd is encountered it is left unmodified.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index afadae6682ed..52638171d6fd 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -57,16 +57,16 @@ void show_pte(struct mm_struct *mm, unsigned long addr)
 		pmd_t *pmd;
 		pte_t *pte;
 
-		if (pgd_none_or_clear_bad(pgd))
+		if (pgd_none(*pgd) || pgd_bad(*pgd))
 			break;
 
 		pud = pud_offset(pgd, addr);
-		if (pud_none_or_clear_bad(pud))
+		if (pud_none(*pud) || pud_bad(*pud))
 			break;
 
 		pmd = pmd_offset(pud, addr);
 		printk(", *pmd=%016llx", pmd_val(*pmd));
-		if (pmd_none_or_clear_bad(pmd))
+		if (pmd_none(*pmd) || pmd_bad(*pmd))
 			break;
 
 		pte = pte_offset_map(pmd, addr);

commit 3495386b107510ba7014f42da37034648c1d2cfd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Oct 24 16:34:02 2012 +0100

    arm64: Make the user fault reporting more specific
    
    For user space faults the kernel reports "unhandled page fault" and it
    gives the ESR value. With this patch the error message looked up in the
    fault info array to give a better description.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 1909a69983ca..afadae6682ed 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -36,6 +36,8 @@
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
 
+static const char *fault_name(unsigned int esr);
+
 /*
  * Dump out the page tables associated with 'addr' in mm 'mm'.
  */
@@ -112,8 +114,9 @@ static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
 	struct siginfo si;
 
 	if (show_unhandled_signals) {
-		pr_info("%s[%d]: unhandled page fault (%d) at 0x%08lx, code 0x%03x\n",
-			tsk->comm, task_pid_nr(tsk), sig, addr, esr);
+		pr_info("%s[%d]: unhandled %s (%d) at 0x%08lx, esr 0x%03x\n",
+			tsk->comm, task_pid_nr(tsk), fault_name(esr), sig,
+			addr, esr);
 		show_pte(tsk->mm, addr);
 		show_regs(regs);
 	}
@@ -450,6 +453,12 @@ static struct fault_info {
 	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
 };
 
+static const char *fault_name(unsigned int esr)
+{
+	const struct fault_info *inf = fault_info + (esr & 63);
+	return inf->name;
+}
+
 /*
  * Dispatch a data abort to the relevant handler.
  */

commit 1d18c47c735e8adfe531fc41fae31e98f86b68fe
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: MMU fault handling and page table management
    
    This patch adds support for the handling of the MMU faults (exception
    entry code introduced by a previous patch) and page table management.
    
    The user translation table is pointed to by TTBR0 and the kernel one
    (swapper_pg_dir) by TTBR1. There is no translation information shared or
    address space overlapping between user and kernel page tables.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
new file mode 100644
index 000000000000..1909a69983ca
--- /dev/null
+++ b/arch/arm64/mm/fault.c
@@ -0,0 +1,534 @@
+/*
+ * Based on arch/arm/mm/fault.c
+ *
+ * Copyright (C) 1995  Linus Torvalds
+ * Copyright (C) 1995-2004 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/module.h>
+#include <linux/signal.h>
+#include <linux/mm.h>
+#include <linux/hardirq.h>
+#include <linux/init.h>
+#include <linux/kprobes.h>
+#include <linux/uaccess.h>
+#include <linux/page-flags.h>
+#include <linux/sched.h>
+#include <linux/highmem.h>
+#include <linux/perf_event.h>
+
+#include <asm/exception.h>
+#include <asm/debug-monitors.h>
+#include <asm/system_misc.h>
+#include <asm/pgtable.h>
+#include <asm/tlbflush.h>
+
+/*
+ * Dump out the page tables associated with 'addr' in mm 'mm'.
+ */
+void show_pte(struct mm_struct *mm, unsigned long addr)
+{
+	pgd_t *pgd;
+
+	if (!mm)
+		mm = &init_mm;
+
+	pr_alert("pgd = %p\n", mm->pgd);
+	pgd = pgd_offset(mm, addr);
+	pr_alert("[%08lx] *pgd=%016llx", addr, pgd_val(*pgd));
+
+	do {
+		pud_t *pud;
+		pmd_t *pmd;
+		pte_t *pte;
+
+		if (pgd_none_or_clear_bad(pgd))
+			break;
+
+		pud = pud_offset(pgd, addr);
+		if (pud_none_or_clear_bad(pud))
+			break;
+
+		pmd = pmd_offset(pud, addr);
+		printk(", *pmd=%016llx", pmd_val(*pmd));
+		if (pmd_none_or_clear_bad(pmd))
+			break;
+
+		pte = pte_offset_map(pmd, addr);
+		printk(", *pte=%016llx", pte_val(*pte));
+		pte_unmap(pte);
+	} while(0);
+
+	printk("\n");
+}
+
+/*
+ * The kernel tried to access some page that wasn't present.
+ */
+static void __do_kernel_fault(struct mm_struct *mm, unsigned long addr,
+			      unsigned int esr, struct pt_regs *regs)
+{
+	/*
+	 * Are we prepared to handle this kernel fault?
+	 */
+	if (fixup_exception(regs))
+		return;
+
+	/*
+	 * No handler, we'll have to terminate things with extreme prejudice.
+	 */
+	bust_spinlocks(1);
+	pr_alert("Unable to handle kernel %s at virtual address %08lx\n",
+		 (addr < PAGE_SIZE) ? "NULL pointer dereference" :
+		 "paging request", addr);
+
+	show_pte(mm, addr);
+	die("Oops", regs, esr);
+	bust_spinlocks(0);
+	do_exit(SIGKILL);
+}
+
+/*
+ * Something tried to access memory that isn't in our memory map. User mode
+ * accesses just cause a SIGSEGV
+ */
+static void __do_user_fault(struct task_struct *tsk, unsigned long addr,
+			    unsigned int esr, unsigned int sig, int code,
+			    struct pt_regs *regs)
+{
+	struct siginfo si;
+
+	if (show_unhandled_signals) {
+		pr_info("%s[%d]: unhandled page fault (%d) at 0x%08lx, code 0x%03x\n",
+			tsk->comm, task_pid_nr(tsk), sig, addr, esr);
+		show_pte(tsk->mm, addr);
+		show_regs(regs);
+	}
+
+	tsk->thread.fault_address = addr;
+	si.si_signo = sig;
+	si.si_errno = 0;
+	si.si_code = code;
+	si.si_addr = (void __user *)addr;
+	force_sig_info(sig, &si, tsk);
+}
+
+void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *regs)
+{
+	struct task_struct *tsk = current;
+	struct mm_struct *mm = tsk->active_mm;
+
+	/*
+	 * If we are in kernel mode at this point, we have no context to
+	 * handle this fault with.
+	 */
+	if (user_mode(regs))
+		__do_user_fault(tsk, addr, esr, SIGSEGV, SEGV_MAPERR, regs);
+	else
+		__do_kernel_fault(mm, addr, esr, regs);
+}
+
+#define VM_FAULT_BADMAP		0x010000
+#define VM_FAULT_BADACCESS	0x020000
+
+#define ESR_WRITE		(1 << 6)
+#define ESR_LNX_EXEC		(1 << 24)
+
+/*
+ * Check that the permissions on the VMA allow for the fault which occurred.
+ * If we encountered a write fault, we must have write permission, otherwise
+ * we allow any permission.
+ */
+static inline bool access_error(unsigned int esr, struct vm_area_struct *vma)
+{
+	unsigned int mask = VM_READ | VM_WRITE | VM_EXEC;
+
+	if (esr & ESR_WRITE)
+		mask = VM_WRITE;
+	if (esr & ESR_LNX_EXEC)
+		mask = VM_EXEC;
+
+	return vma->vm_flags & mask ? false : true;
+}
+
+static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
+			   unsigned int esr, unsigned int flags,
+			   struct task_struct *tsk)
+{
+	struct vm_area_struct *vma;
+	int fault;
+
+	vma = find_vma(mm, addr);
+	fault = VM_FAULT_BADMAP;
+	if (unlikely(!vma))
+		goto out;
+	if (unlikely(vma->vm_start > addr))
+		goto check_stack;
+
+	/*
+	 * Ok, we have a good vm_area for this memory access, so we can handle
+	 * it.
+	 */
+good_area:
+	if (access_error(esr, vma)) {
+		fault = VM_FAULT_BADACCESS;
+		goto out;
+	}
+
+	return handle_mm_fault(mm, vma, addr & PAGE_MASK, flags);
+
+check_stack:
+	if (vma->vm_flags & VM_GROWSDOWN && !expand_stack(vma, addr))
+		goto good_area;
+out:
+	return fault;
+}
+
+static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
+				   struct pt_regs *regs)
+{
+	struct task_struct *tsk;
+	struct mm_struct *mm;
+	int fault, sig, code;
+	int write = esr & ESR_WRITE;
+	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE |
+		(write ? FAULT_FLAG_WRITE : 0);
+
+	tsk = current;
+	mm  = tsk->mm;
+
+	/* Enable interrupts if they were enabled in the parent context. */
+	if (interrupts_enabled(regs))
+		local_irq_enable();
+
+	/*
+	 * If we're in an interrupt or have no user context, we must not take
+	 * the fault.
+	 */
+	if (in_atomic() || !mm)
+		goto no_context;
+
+	/*
+	 * As per x86, we may deadlock here. However, since the kernel only
+	 * validly references user space from well defined areas of the code,
+	 * we can bug out early if this is from code which shouldn't.
+	 */
+	if (!down_read_trylock(&mm->mmap_sem)) {
+		if (!user_mode(regs) && !search_exception_tables(regs->pc))
+			goto no_context;
+retry:
+		down_read(&mm->mmap_sem);
+	} else {
+		/*
+		 * The above down_read_trylock() might have succeeded in which
+		 * case, we'll have missed the might_sleep() from down_read().
+		 */
+		might_sleep();
+#ifdef CONFIG_DEBUG_VM
+		if (!user_mode(regs) && !search_exception_tables(regs->pc))
+			goto no_context;
+#endif
+	}
+
+	fault = __do_page_fault(mm, addr, esr, flags, tsk);
+
+	/*
+	 * If we need to retry but a fatal signal is pending, handle the
+	 * signal first. We do not need to release the mmap_sem because it
+	 * would already be released in __lock_page_or_retry in mm/filemap.c.
+	 */
+	if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
+		return 0;
+
+	/*
+	 * Major/minor page fault accounting is only done on the initial
+	 * attempt. If we go through a retry, it is extremely likely that the
+	 * page will be found in page cache at that point.
+	 */
+
+	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
+	if (flags & FAULT_FLAG_ALLOW_RETRY) {
+		if (fault & VM_FAULT_MAJOR) {
+			tsk->maj_flt++;
+			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs,
+				      addr);
+		} else {
+			tsk->min_flt++;
+			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs,
+				      addr);
+		}
+		if (fault & VM_FAULT_RETRY) {
+			/*
+			 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk of
+			 * starvation.
+			 */
+			flags &= ~FAULT_FLAG_ALLOW_RETRY;
+			goto retry;
+		}
+	}
+
+	up_read(&mm->mmap_sem);
+
+	/*
+	 * Handle the "normal" case first - VM_FAULT_MAJOR / VM_FAULT_MINOR
+	 */
+	if (likely(!(fault & (VM_FAULT_ERROR | VM_FAULT_BADMAP |
+			      VM_FAULT_BADACCESS))))
+		return 0;
+
+	if (fault & VM_FAULT_OOM) {
+		/*
+		 * We ran out of memory, call the OOM killer, and return to
+		 * userspace (which will retry the fault, or kill us if we got
+		 * oom-killed).
+		 */
+		pagefault_out_of_memory();
+		return 0;
+	}
+
+	/*
+	 * If we are in kernel mode at this point, we have no context to
+	 * handle this fault with.
+	 */
+	if (!user_mode(regs))
+		goto no_context;
+
+	if (fault & VM_FAULT_SIGBUS) {
+		/*
+		 * We had some memory, but were unable to successfully fix up
+		 * this page fault.
+		 */
+		sig = SIGBUS;
+		code = BUS_ADRERR;
+	} else {
+		/*
+		 * Something tried to access memory that isn't in our memory
+		 * map.
+		 */
+		sig = SIGSEGV;
+		code = fault == VM_FAULT_BADACCESS ?
+			SEGV_ACCERR : SEGV_MAPERR;
+	}
+
+	__do_user_fault(tsk, addr, esr, sig, code, regs);
+	return 0;
+
+no_context:
+	__do_kernel_fault(mm, addr, esr, regs);
+	return 0;
+}
+
+/*
+ * First Level Translation Fault Handler
+ *
+ * We enter here because the first level page table doesn't contain a valid
+ * entry for the address.
+ *
+ * If the address is in kernel space (>= TASK_SIZE), then we are probably
+ * faulting in the vmalloc() area.
+ *
+ * If the init_task's first level page tables contains the relevant entry, we
+ * copy the it to this task.  If not, we send the process a signal, fixup the
+ * exception, or oops the kernel.
+ *
+ * NOTE! We MUST NOT take any locks for this case. We may be in an interrupt
+ * or a critical region, and should only copy the information from the master
+ * page table, nothing more.
+ */
+static int __kprobes do_translation_fault(unsigned long addr,
+					  unsigned int esr,
+					  struct pt_regs *regs)
+{
+	if (addr < TASK_SIZE)
+		return do_page_fault(addr, esr, regs);
+
+	do_bad_area(addr, esr, regs);
+	return 0;
+}
+
+/*
+ * Some section permission faults need to be handled gracefully.  They can
+ * happen due to a __{get,put}_user during an oops.
+ */
+static int do_sect_fault(unsigned long addr, unsigned int esr,
+			 struct pt_regs *regs)
+{
+	do_bad_area(addr, esr, regs);
+	return 0;
+}
+
+/*
+ * This abort handler always returns "fault".
+ */
+static int do_bad(unsigned long addr, unsigned int esr, struct pt_regs *regs)
+{
+	return 1;
+}
+
+static struct fault_info {
+	int	(*fn)(unsigned long addr, unsigned int esr, struct pt_regs *regs);
+	int	sig;
+	int	code;
+	const char *name;
+} fault_info[] = {
+	{ do_bad,		SIGBUS,  0,		"ttbr address size fault"	},
+	{ do_bad,		SIGBUS,  0,		"level 1 address size fault"	},
+	{ do_bad,		SIGBUS,  0,		"level 2 address size fault"	},
+	{ do_bad,		SIGBUS,  0,		"level 3 address size fault"	},
+	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"input address range fault"	},
+	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 1 translation fault"	},
+	{ do_translation_fault,	SIGSEGV, SEGV_MAPERR,	"level 2 translation fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"level 3 translation fault"	},
+	{ do_bad,		SIGBUS,  0,		"reserved access flag fault"	},
+	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"level 1 access flag fault"	},
+	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"level 2 access flag fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 access flag fault"	},
+	{ do_bad,		SIGBUS,  0,		"reserved permission fault"	},
+	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"level 1 permission fault"	},
+	{ do_sect_fault,	SIGSEGV, SEGV_ACCERR,	"level 2 permission fault"	},
+	{ do_page_fault,	SIGSEGV, SEGV_ACCERR,	"level 3 permission fault"	},
+	{ do_bad,		SIGBUS,  0,		"synchronous external abort"	},
+	{ do_bad,		SIGBUS,  0,		"asynchronous external abort"	},
+	{ do_bad,		SIGBUS,  0,		"unknown 18"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 19"			},
+	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous abort (translation table walk)" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error"	},
+	{ do_bad,		SIGBUS,  0,		"asynchronous parity error"	},
+	{ do_bad,		SIGBUS,  0,		"unknown 26"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 27"			},
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
+	{ do_bad,		SIGBUS,  0,		"synchronous parity error (translation table walk" },
+	{ do_bad,		SIGBUS,  0,		"unknown 32"			},
+	{ do_bad,		SIGBUS,  BUS_ADRALN,	"alignment fault"		},
+	{ do_bad,		SIGBUS,  0,		"debug event"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 35"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 36"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 37"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 38"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 39"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 40"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 41"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 42"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 43"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 44"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 45"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 46"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 47"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 48"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 49"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 50"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 51"			},
+	{ do_bad,		SIGBUS,  0,		"implementation fault (lockdown abort)" },
+	{ do_bad,		SIGBUS,  0,		"unknown 53"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 54"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 55"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 56"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 57"			},
+	{ do_bad,		SIGBUS,  0,		"implementation fault (coprocessor abort)" },
+	{ do_bad,		SIGBUS,  0,		"unknown 59"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 60"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 61"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 62"			},
+	{ do_bad,		SIGBUS,  0,		"unknown 63"			},
+};
+
+/*
+ * Dispatch a data abort to the relevant handler.
+ */
+asmlinkage void __exception do_mem_abort(unsigned long addr, unsigned int esr,
+					 struct pt_regs *regs)
+{
+	const struct fault_info *inf = fault_info + (esr & 63);
+	struct siginfo info;
+
+	if (!inf->fn(addr, esr, regs))
+		return;
+
+	pr_alert("Unhandled fault: %s (0x%08x) at 0x%016lx\n",
+		 inf->name, esr, addr);
+
+	info.si_signo = inf->sig;
+	info.si_errno = 0;
+	info.si_code  = inf->code;
+	info.si_addr  = (void __user *)addr;
+	arm64_notify_die("", regs, &info, esr);
+}
+
+/*
+ * Handle stack alignment exceptions.
+ */
+asmlinkage void __exception do_sp_pc_abort(unsigned long addr,
+					   unsigned int esr,
+					   struct pt_regs *regs)
+{
+	struct siginfo info;
+
+	info.si_signo = SIGBUS;
+	info.si_errno = 0;
+	info.si_code  = BUS_ADRALN;
+	info.si_addr  = (void __user *)addr;
+	arm64_notify_die("", regs, &info, esr);
+}
+
+static struct fault_info debug_fault_info[] = {
+	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware breakpoint"	},
+	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware single-step"	},
+	{ do_bad,	SIGTRAP,	TRAP_HWBKPT,	"hardware watchpoint"	},
+	{ do_bad,	SIGBUS,		0,		"unknown 3"		},
+	{ do_bad,	SIGTRAP,	TRAP_BRKPT,	"aarch32 BKPT"		},
+	{ do_bad,	SIGTRAP,	0,		"aarch32 vector catch"	},
+	{ do_bad,	SIGTRAP,	TRAP_BRKPT,	"aarch64 BRK"		},
+	{ do_bad,	SIGBUS,		0,		"unknown 7"		},
+};
+
+void __init hook_debug_fault_code(int nr,
+				  int (*fn)(unsigned long, unsigned int, struct pt_regs *),
+				  int sig, int code, const char *name)
+{
+	BUG_ON(nr < 0 || nr >= ARRAY_SIZE(debug_fault_info));
+
+	debug_fault_info[nr].fn		= fn;
+	debug_fault_info[nr].sig	= sig;
+	debug_fault_info[nr].code	= code;
+	debug_fault_info[nr].name	= name;
+}
+
+asmlinkage int __exception do_debug_exception(unsigned long addr,
+					      unsigned int esr,
+					      struct pt_regs *regs)
+{
+	const struct fault_info *inf = debug_fault_info + DBG_ESR_EVT(esr);
+	struct siginfo info;
+
+	if (!inf->fn(addr, esr, regs))
+		return 1;
+
+	pr_alert("Unhandled debug exception: %s (0x%08x) at 0x%016lx\n",
+		 inf->name, esr, addr);
+
+	info.si_signo = inf->sig;
+	info.si_errno = 0;
+	info.si_code  = inf->code;
+	info.si_addr  = (void __user *)addr;
+	arm64_notify_die("", regs, &info, esr);
+
+	return 0;
+}
