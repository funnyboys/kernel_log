commit 618e07865b7453d02410c1f3407c2d78a670eabb
Author: Barry Song <song.bao.hua@hisilicon.com>
Date:   Thu Jun 18 09:58:28 2020 +1200

    arm64: mm: reserve hugetlb CMA after numa_init
    
    hugetlb_cma_reserve() is called at the wrong place. numa_init has not been
    done yet. so all reserved memory will be located at node0.
    
    Fixes: cf11e85fc08c ("mm: hugetlb: optionally allocate gigantic hugepages using cma")
    Signed-off-by: Barry Song <song.bao.hua@hisilicon.com>
    Reviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Acked-by: Roman Gushchin <guro@fb.com>
    Cc: Matthias Brugger <matthias.bgg@gmail.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200617215828.25296-1-song.bao.hua@hisilicon.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index e631e6425165..1e93cfc7c47a 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -404,11 +404,6 @@ void __init arm64_memblock_init(void)
 	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
 
 	dma_contiguous_reserve(arm64_dma32_phys_limit);
-
-#ifdef CONFIG_ARM64_4K_PAGES
-	hugetlb_cma_reserve(PUD_SHIFT - PAGE_SHIFT);
-#endif
-
 }
 
 void __init bootmem_init(void)
@@ -424,6 +419,16 @@ void __init bootmem_init(void)
 	min_low_pfn = min;
 
 	arm64_numa_init();
+
+	/*
+	 * must be done after arm64_numa_init() which calls numa_init() to
+	 * initialize node_online_map that gets used in hugetlb_cma_reserve()
+	 * while allocating required CMA size across online nodes.
+	 */
+#ifdef CONFIG_ARM64_4K_PAGES
+	hugetlb_cma_reserve(PUD_SHIFT - PAGE_SHIFT);
+#endif
+
 	/*
 	 * Sparsemem tries to allocate bootmem in memory_present(), so must be
 	 * done after the fixed reservations.

commit 584cb13dca27be144a54f9e0b0d71f3db61130a2
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:57:23 2020 -0700

    arm64: simplify detection of memory zone boundaries for UMA configs
    
    The free_area_init() function only requires the definition of maximal PFN
    for each of the supported zone rater than calculation of actual zone sizes
    and the sizes of the holes between the zones.
    
    After removal of CONFIG_HAVE_MEMBLOCK_NODE_MAP the free_area_init() is
    available to all architectures.
    
    Using this function instead of free_area_init_node() simplifies the zone
    detection.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-9-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5dae97f2f628..e631e6425165 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -192,8 +192,6 @@ static phys_addr_t __init max_zone_phys(unsigned int zone_bits)
 	return min(offset + (1ULL << zone_bits), memblock_end_of_DRAM());
 }
 
-#ifdef CONFIG_NUMA
-
 static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
@@ -209,58 +207,6 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	free_area_init(max_zone_pfns);
 }
 
-#else
-
-static void __init zone_sizes_init(unsigned long min, unsigned long max)
-{
-	struct memblock_region *reg;
-	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
-	unsigned long __maybe_unused max_dma, max_dma32;
-
-	memset(zone_size, 0, sizeof(zone_size));
-
-	max_dma = max_dma32 = min;
-#ifdef CONFIG_ZONE_DMA
-	max_dma = max_dma32 = PFN_DOWN(arm64_dma_phys_limit);
-	zone_size[ZONE_DMA] = max_dma - min;
-#endif
-#ifdef CONFIG_ZONE_DMA32
-	max_dma32 = PFN_DOWN(arm64_dma32_phys_limit);
-	zone_size[ZONE_DMA32] = max_dma32 - max_dma;
-#endif
-	zone_size[ZONE_NORMAL] = max - max_dma32;
-
-	memcpy(zhole_size, zone_size, sizeof(zhole_size));
-
-	for_each_memblock(memory, reg) {
-		unsigned long start = memblock_region_memory_base_pfn(reg);
-		unsigned long end = memblock_region_memory_end_pfn(reg);
-
-#ifdef CONFIG_ZONE_DMA
-		if (start >= min && start < max_dma) {
-			unsigned long dma_end = min(end, max_dma);
-			zhole_size[ZONE_DMA] -= dma_end - start;
-			start = dma_end;
-		}
-#endif
-#ifdef CONFIG_ZONE_DMA32
-		if (start >= max_dma && start < max_dma32) {
-			unsigned long dma32_end = min(end, max_dma32);
-			zhole_size[ZONE_DMA32] -= dma32_end - start;
-			start = dma32_end;
-		}
-#endif
-		if (start >= max_dma32 && start < max) {
-			unsigned long normal_end = min(end, max);
-			zhole_size[ZONE_NORMAL] -= normal_end - start;
-		}
-	}
-
-	free_area_init_node(0, zone_size, min, zhole_size);
-}
-
-#endif /* CONFIG_NUMA */
-
 int pfn_valid(unsigned long pfn)
 {
 	phys_addr_t addr = pfn << PAGE_SHIFT;

commit 9691a071aa26a21fc8dac804a2b98d3c24f76f9a
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:57:10 2020 -0700

    mm: use free_area_init() instead of free_area_init_nodes()
    
    free_area_init() has effectively became a wrapper for
    free_area_init_nodes() and there is no point of keeping it.  Still
    free_area_init() name is shorter and more general as it does not imply
    necessity to initialize multiple nodes.
    
    Rename free_area_init_nodes() to free_area_init(), update the callers and
    drop old version of free_area_init().
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-6-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d2df416b840e..5dae97f2f628 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -206,7 +206,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max;
 
-	free_area_init_nodes(max_zone_pfns);
+	free_area_init(max_zone_pfns);
 }
 
 #else

commit 037d9303a7e7bac622e299817c5dd288346db07c
Author: Guixiong Wei <guixiongwei@gmail.com>
Date:   Fri May 1 06:18:58 2020 +1400

    arm: mm: use __pfn_to_section() to get mem_section
    
    Replace the open-coded '__nr_to_section(pfn_to_section_nr(pfn))' in
    pfn_valid() with a more concise call to '__pfn_to_section(pfn)'.
    
    No functional change.
    
    Signed-off-by: Guixiong Wei <guixiongwei@gmail.com>
    Link: https://lore.kernel.org/r/20200430161858.11379-1-guixiongwei@gmail.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index e42727e3568e..d2df416b840e 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -272,7 +272,7 @@ int pfn_valid(unsigned long pfn)
 	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
 		return 0;
 
-	if (!valid_section(__nr_to_section(pfn_to_section_nr(pfn))))
+	if (!valid_section(__pfn_to_section(pfn)))
 		return 0;
 #endif
 	return memblock_is_map_memory(addr);

commit cf11e85fc08cc6a4fe3ac2ba2e610c962bf20bc3
Author: Roman Gushchin <guro@fb.com>
Date:   Fri Apr 10 14:32:45 2020 -0700

    mm: hugetlb: optionally allocate gigantic hugepages using cma
    
    Commit 944d9fec8d7a ("hugetlb: add support for gigantic page allocation
    at runtime") has added the run-time allocation of gigantic pages.
    
    However it actually works only at early stages of the system loading,
    when the majority of memory is free.  After some time the memory gets
    fragmented by non-movable pages, so the chances to find a contiguous 1GB
    block are getting close to zero.  Even dropping caches manually doesn't
    help a lot.
    
    At large scale rebooting servers in order to allocate gigantic hugepages
    is quite expensive and complex.  At the same time keeping some constant
    percentage of memory in reserved hugepages even if the workload isn't
    using it is a big waste: not all workloads can benefit from using 1 GB
    pages.
    
    The following solution can solve the problem:
    1) On boot time a dedicated cma area* is reserved. The size is passed
       as a kernel argument.
    2) Run-time allocations of gigantic hugepages are performed using the
       cma allocator and the dedicated cma area
    
    In this case gigantic hugepages can be allocated successfully with a
    high probability, however the memory isn't completely wasted if nobody
    is using 1GB hugepages: it can be used for pagecache, anon memory, THPs,
    etc.
    
    * On a multi-node machine a per-node cma area is allocated on each node.
      Following gigantic hugetlb allocation are using the first available
      numa node if the mask isn't specified by a user.
    
    Usage:
    1) configure the kernel to allocate a cma area for hugetlb allocations:
       pass hugetlb_cma=10G as a kernel argument
    
    2) allocate hugetlb pages as usual, e.g.
       echo 10 > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
    
    If the option isn't enabled or the allocation of the cma area failed,
    the current behavior of the system is preserved.
    
    x86 and arm-64 are covered by this patch, other architectures can be
    trivially added later.
    
    The patch contains clean-ups and fixes proposed and implemented by Aslan
    Bakirov and Randy Dunlap.  It also contains ideas and suggestions
    proposed by Rik van Riel, Michal Hocko and Mike Kravetz.  Thanks!
    
    Signed-off-by: Roman Gushchin <guro@fb.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Andreas Schaufler <andreas.schaufler@gmx.de>
    Acked-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: Michal Hocko <mhocko@kernel.org>
    Cc: Aslan Bakirov <aslan@fb.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Link: http://lkml.kernel.org/r/20200407163840.92263-3-guro@fb.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index b65dffdfb201..e42727e3568e 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -29,6 +29,7 @@
 #include <linux/mm.h>
 #include <linux/kexec.h>
 #include <linux/crash_dump.h>
+#include <linux/hugetlb.h>
 
 #include <asm/boot.h>
 #include <asm/fixmap.h>
@@ -457,6 +458,11 @@ void __init arm64_memblock_init(void)
 	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
 
 	dma_contiguous_reserve(arm64_dma32_phys_limit);
+
+#ifdef CONFIG_ARM64_4K_PAGES
+	hugetlb_cma_reserve(PUD_SHIFT - PAGE_SHIFT);
+#endif
+
 }
 
 void __init bootmem_init(void)

commit 93b90414c33f59b7960bc8d607da0ce83377e021
Author: Will Deacon <will@kernel.org>
Date:   Tue Dec 3 12:10:13 2019 +0000

    arm64: mm: Fix initialisation of DMA zones on non-NUMA systems
    
    John reports that the recently merged commit 1a8e1cef7603 ("arm64: use
    both ZONE_DMA and ZONE_DMA32") breaks the boot on his DB845C board:
    
      | Booting Linux on physical CPU 0x0000000000 [0x517f803c]
      | Linux version 5.4.0-mainline-10675-g957a03b9e38f
      | Machine model: Thundercomm Dragonboard 845c
      | [...]
      | Built 1 zonelists, mobility grouping on.  Total pages: -188245
      | Kernel command line: earlycon
      | firmware_class.path=/vendor/firmware/ androidboot.hardware=db845c
      | init=/init androidboot.boot_devices=soc/1d84000.ufshc
      | printk.devkmsg=on buildvariant=userdebug root=/dev/sda2
      | androidboot.bootdevice=1d84000.ufshc androidboot.serialno=c4e1189c
      | androidboot.baseband=sda
      | msm_drm.dsi_display0=dsi_lt9611_1080_video_display:
      | androidboot.slot_suffix=_a skip_initramfs rootwait ro init=/init
      |
      | <hangs indefinitely here>
    
    This is because, when CONFIG_NUMA=n, zone_sizes_init() fails to handle
    memblocks that fall entirely within the ZONE_DMA region and erroneously ends up
    trying to add a negatively-sized region into the following ZONE_DMA32, which is
    later interpreted as a large unsigned region by the core MM code.
    
    Rework the non-NUMA implementation of zone_sizes_init() so that the start
    address of the memblock being processed is adjusted according to the end of the
    previous zone, which is then range-checked before updating the hole information
    of subsequent zones.
    
    Cc: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Bjorn Andersson <bjorn.andersson@linaro.org>
    Link: https://lore.kernel.org/lkml/CALAqxLVVcsmFrDKLRGRq7GewcW405yTOxG=KR3csVzQ6bXutkA@mail.gmail.com
    Fixes: 1a8e1cef7603 ("arm64: use both ZONE_DMA and ZONE_DMA32")
    Reported-by: John Stultz <john.stultz@linaro.org>
    Tested-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index be9481cdf3b9..b65dffdfb201 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -214,15 +214,14 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	struct memblock_region *reg;
 	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
-	unsigned long max_dma32 = min;
-	unsigned long __maybe_unused max_dma = min;
+	unsigned long __maybe_unused max_dma, max_dma32;
 
 	memset(zone_size, 0, sizeof(zone_size));
 
+	max_dma = max_dma32 = min;
 #ifdef CONFIG_ZONE_DMA
-	max_dma = PFN_DOWN(arm64_dma_phys_limit);
+	max_dma = max_dma32 = PFN_DOWN(arm64_dma_phys_limit);
 	zone_size[ZONE_DMA] = max_dma - min;
-	max_dma32 = max_dma;
 #endif
 #ifdef CONFIG_ZONE_DMA32
 	max_dma32 = PFN_DOWN(arm64_dma32_phys_limit);
@@ -236,25 +235,23 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 		unsigned long start = memblock_region_memory_base_pfn(reg);
 		unsigned long end = memblock_region_memory_end_pfn(reg);
 
-		if (start >= max)
-			continue;
 #ifdef CONFIG_ZONE_DMA
-		if (start < max_dma) {
-			unsigned long dma_end = min_not_zero(end, max_dma);
+		if (start >= min && start < max_dma) {
+			unsigned long dma_end = min(end, max_dma);
 			zhole_size[ZONE_DMA] -= dma_end - start;
+			start = dma_end;
 		}
 #endif
 #ifdef CONFIG_ZONE_DMA32
-		if (start < max_dma32) {
+		if (start >= max_dma && start < max_dma32) {
 			unsigned long dma32_end = min(end, max_dma32);
-			unsigned long dma32_start = max(start, max_dma);
-			zhole_size[ZONE_DMA32] -= dma32_end - dma32_start;
+			zhole_size[ZONE_DMA32] -= dma32_end - start;
+			start = dma32_end;
 		}
 #endif
-		if (end > max_dma32) {
+		if (start >= max_dma32 && start < max) {
 			unsigned long normal_end = min(end, max);
-			unsigned long normal_start = max(start, max_dma32);
-			zhole_size[ZONE_NORMAL] -= normal_end - normal_start;
+			zhole_size[ZONE_NORMAL] -= normal_end - start;
 		}
 	}
 

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit bff3b04460a80f425442fe8e5c6ee8c3ebef611f
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Thu Nov 7 10:56:11 2019 +0100

    arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
    
    With the introduction of ZONE_DMA in arm64 we moved the default CMA and
    crashkernel reservation into that area. This caused a regression on big
    machines that need big CMA and crashkernel reservations. Note that
    ZONE_DMA is only 1GB big.
    
    Restore the previous behavior as the wide majority of devices are OK
    with reserving these in ZONE_DMA32. The ones that need them in ZONE_DMA
    will configure it explicitly.
    
    Fixes: 1a8e1cef7603 ("arm64: use both ZONE_DMA and ZONE_DMA32")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 35f27b839101..d933589c48e8 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -91,7 +91,7 @@ static void __init reserve_crashkernel(void)
 
 	if (crash_base == 0) {
 		/* Current arm64 boot protocol requires 2MB alignment */
-		crash_base = memblock_find_in_range(0, ARCH_LOW_ADDRESS_LIMIT,
+		crash_base = memblock_find_in_range(0, arm64_dma32_phys_limit,
 				crash_size, SZ_2M);
 		if (crash_base == 0) {
 			pr_warn("cannot allocate crashkernel (size:0x%llx)\n",
@@ -459,7 +459,7 @@ void __init arm64_memblock_init(void)
 
 	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
 
-	dma_contiguous_reserve(arm64_dma_phys_limit ? : arm64_dma32_phys_limit);
+	dma_contiguous_reserve(arm64_dma32_phys_limit);
 }
 
 void __init bootmem_init(void)

commit 8b5369ea580964dbc982781bfb9fb93459fc5e8d
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Mon Oct 14 20:31:03 2019 +0200

    dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
    
    Some architectures, notably ARM, are interested in tweaking this
    depending on their runtime DMA addressing limitations.
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 39fc69873b18..35f27b839101 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -20,6 +20,7 @@
 #include <linux/sort.h>
 #include <linux/of.h>
 #include <linux/of_fdt.h>
+#include <linux/dma-direct.h>
 #include <linux/dma-mapping.h>
 #include <linux/dma-contiguous.h>
 #include <linux/efi.h>
@@ -41,6 +42,8 @@
 #include <asm/tlb.h>
 #include <asm/alternative.h>
 
+#define ARM64_ZONE_DMA_BITS	30
+
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -440,8 +443,10 @@ void __init arm64_memblock_init(void)
 
 	early_init_fdt_scan_reserved_mem();
 
-	if (IS_ENABLED(CONFIG_ZONE_DMA))
-		arm64_dma_phys_limit = max_zone_phys(ARCH_ZONE_DMA_BITS);
+	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
+		zone_dma_bits = ARM64_ZONE_DMA_BITS;
+		arm64_dma_phys_limit = max_zone_phys(ARM64_ZONE_DMA_BITS);
+	}
 
 	if (IS_ENABLED(CONFIG_ZONE_DMA32))
 		arm64_dma32_phys_limit = max_zone_phys(32);

commit 4686da5140c18c84ca01a8ab994571d832c63398
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 16:45:07 2019 +0000

    arm64: Make arm64_dma32_phys_limit static
    
    This variable is only used in the arch/arm64/mm/init.c file for
    ZONE_DMA32 initialisation, no need to expose it.
    
    Reported-by: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 71b45c58218b..39fc69873b18 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -63,7 +63,7 @@ EXPORT_SYMBOL(vmemmap);
  * bit addressable memory area.
  */
 phys_addr_t arm64_dma_phys_limit __ro_after_init;
-phys_addr_t arm64_dma32_phys_limit __ro_after_init;
+static phys_addr_t arm64_dma32_phys_limit __ro_after_init;
 
 #ifdef CONFIG_KEXEC_CORE
 /*

commit 4399d430700d3974ed6c5a1b1380bc6527f17e99
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Wed Oct 16 07:47:14 2019 -0700

    arm64: mm: Fix unused variable warning in zone_sizes_init
    
    When building arm64 allnoconfig, CONFIG_ZONE_DMA and CONFIG_ZONE_DMA32
    get disabled so there is a warning about max_dma being unused.
    
    ../arch/arm64/mm/init.c:215:16: warning: unused variable 'max_dma'
    [-Wunused-variable]
            unsigned long max_dma = min;
                          ^
    1 warning generated.
    
    Add __maybe_unused to make this clear to the compiler.
    
    Fixes: 1a8e1cef7603 ("arm64: use both ZONE_DMA and ZONE_DMA32")
    Reviewed-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 44f07fdf7a59..71b45c58218b 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -212,7 +212,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	struct memblock_region *reg;
 	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
 	unsigned long max_dma32 = min;
-	unsigned long max_dma = min;
+	unsigned long __maybe_unused max_dma = min;
 
 	memset(zone_size, 0, sizeof(zone_size));
 

commit 6ec939f8b809cb06ba7802e17ef7024d1bc0ee84
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Oct 4 09:53:58 2019 +0530

    arm64/mm: Poison initmem while freeing with free_reserved_area()
    
    Platform implementation for free_initmem() should poison the memory while
    freeing it up. Hence pass across POISON_FREE_INITMEM while calling into
    free_reserved_area(). The same is being followed in the generic fallback
    for free_initmem() and some other platforms overriding it.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: linux-kernel@vger.kernel.org
    Reviewed-by: Steven Price <steven.price@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 87a0e3b6c146..7c225d0132b8 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -571,7 +571,7 @@ void free_initmem(void)
 {
 	free_reserved_area(lm_alias(__init_begin),
 			   lm_alias(__init_end),
-			   0, "unused kernel");
+			   POISON_FREE_INITMEM, "unused kernel");
 	/*
 	 * Unmap the __init region but leave the VM area in place. This
 	 * prevents the region from being reused for kernel modules, which

commit 899ee4afe5eb262236717188ccdaa0192c00dc5a
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Sat Sep 28 11:02:26 2019 +0300

    arm64: use generic free_initrd_mem()
    
    arm64 calls memblock_free() for the initrd area in its implementation of
    free_initrd_mem(), but this call has no actual effect that late in the boot
    process. By the time initrd is freed, all the reserved memory is managed by
    the page allocator and the memblock.reserved is unused, so the only purpose
    of the memblock_free() call is to keep track of initrd memory for debugging
    and accounting.
    
    Without the memblock_free() call the only difference between arm64 and the
    generic versions of free_initrd_mem() is the memory poisoning.
    
    Move memblock_free() call to the generic code, enable it there
    for the architectures that define ARCH_KEEP_MEMBLOCK and use the generic
    implementation of free_initrd_mem() on arm64.
    
    Tested-by: Anshuman Khandual <anshuman.khandual@arm.com>        #arm64
    Reviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 45c00a54909c..87a0e3b6c146 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -580,18 +580,6 @@ void free_initmem(void)
 	unmap_kernel_range((u64)__init_begin, (u64)(__init_end - __init_begin));
 }
 
-#ifdef CONFIG_BLK_DEV_INITRD
-void __init free_initrd_mem(unsigned long start, unsigned long end)
-{
-	unsigned long aligned_start, aligned_end;
-
-	aligned_start = __virt_to_phys(start) & PAGE_MASK;
-	aligned_end = PAGE_ALIGN(__virt_to_phys(end));
-	memblock_free(aligned_start, aligned_end - aligned_start);
-	free_reserved_area((void *)start, (void *)end, 0, "initrd");
-}
-#endif
-
 /*
  * Dump out memory limit information on panic.
  */

commit 1a8e1cef7603e218339ac63cb3178b25554524e5
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Wed Sep 11 20:25:45 2019 +0200

    arm64: use both ZONE_DMA and ZONE_DMA32
    
    So far all arm64 devices have supported 32 bit DMA masks for their
    peripherals. This is not true anymore for the Raspberry Pi 4 as most of
    it's peripherals can only address the first GB of memory on a total of
    up to 4 GB.
    
    This goes against ZONE_DMA32's intent, as it's expected for ZONE_DMA32
    to be addressable with a 32 bit mask. So it was decided to re-introduce
    ZONE_DMA in arm64.
    
    ZONE_DMA will contain the lower 1G of memory, which is currently the
    memory area addressable by any peripheral on an arm64 device.
    ZONE_DMA32 will contain the rest of the 32 bit addressable memory.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 8e9bc64c5878..44f07fdf7a59 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -56,6 +56,13 @@ EXPORT_SYMBOL(physvirt_offset);
 struct page *vmemmap __ro_after_init;
 EXPORT_SYMBOL(vmemmap);
 
+/*
+ * We create both ZONE_DMA and ZONE_DMA32. ZONE_DMA covers the first 1G of
+ * memory as some devices, namely the Raspberry Pi 4, have peripherals with
+ * this limited view of the memory. ZONE_DMA32 will cover the rest of the 32
+ * bit addressable memory area.
+ */
+phys_addr_t arm64_dma_phys_limit __ro_after_init;
 phys_addr_t arm64_dma32_phys_limit __ro_after_init;
 
 #ifdef CONFIG_KEXEC_CORE
@@ -169,15 +176,16 @@ static void __init reserve_elfcorehdr(void)
 {
 }
 #endif /* CONFIG_CRASH_DUMP */
+
 /*
- * Return the maximum physical address for ZONE_DMA32 (DMA_BIT_MASK(32)). It
- * currently assumes that for memory starting above 4G, 32-bit devices will
- * use a DMA offset.
+ * Return the maximum physical address for a zone with a given address size
+ * limit. It currently assumes that for memory starting above 4G, 32-bit
+ * devices will use a DMA offset.
  */
-static phys_addr_t __init max_zone_dma32_phys(void)
+static phys_addr_t __init max_zone_phys(unsigned int zone_bits)
 {
-	phys_addr_t offset = memblock_start_of_DRAM() & GENMASK_ULL(63, 32);
-	return min(offset + (1ULL << 32), memblock_end_of_DRAM());
+	phys_addr_t offset = memblock_start_of_DRAM() & GENMASK_ULL(63, zone_bits);
+	return min(offset + (1ULL << zone_bits), memblock_end_of_DRAM());
 }
 
 #ifdef CONFIG_NUMA
@@ -186,6 +194,9 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
 
+#ifdef CONFIG_ZONE_DMA
+	max_zone_pfns[ZONE_DMA] = PFN_DOWN(arm64_dma_phys_limit);
+#endif
 #ifdef CONFIG_ZONE_DMA32
 	max_zone_pfns[ZONE_DMA32] = PFN_DOWN(arm64_dma32_phys_limit);
 #endif
@@ -201,13 +212,18 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	struct memblock_region *reg;
 	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
 	unsigned long max_dma32 = min;
+	unsigned long max_dma = min;
 
 	memset(zone_size, 0, sizeof(zone_size));
 
-	/* 4GB maximum for 32-bit only capable devices */
+#ifdef CONFIG_ZONE_DMA
+	max_dma = PFN_DOWN(arm64_dma_phys_limit);
+	zone_size[ZONE_DMA] = max_dma - min;
+	max_dma32 = max_dma;
+#endif
 #ifdef CONFIG_ZONE_DMA32
 	max_dma32 = PFN_DOWN(arm64_dma32_phys_limit);
-	zone_size[ZONE_DMA32] = max_dma32 - min;
+	zone_size[ZONE_DMA32] = max_dma32 - max_dma;
 #endif
 	zone_size[ZONE_NORMAL] = max - max_dma32;
 
@@ -219,11 +235,17 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 
 		if (start >= max)
 			continue;
-
+#ifdef CONFIG_ZONE_DMA
+		if (start < max_dma) {
+			unsigned long dma_end = min_not_zero(end, max_dma);
+			zhole_size[ZONE_DMA] -= dma_end - start;
+		}
+#endif
 #ifdef CONFIG_ZONE_DMA32
 		if (start < max_dma32) {
-			unsigned long dma_end = min(end, max_dma32);
-			zhole_size[ZONE_DMA32] -= dma_end - start;
+			unsigned long dma32_end = min(end, max_dma32);
+			unsigned long dma32_start = max(start, max_dma);
+			zhole_size[ZONE_DMA32] -= dma32_end - dma32_start;
 		}
 #endif
 		if (end > max_dma32) {
@@ -418,9 +440,11 @@ void __init arm64_memblock_init(void)
 
 	early_init_fdt_scan_reserved_mem();
 
-	/* 4GB maximum for 32-bit only capable devices */
+	if (IS_ENABLED(CONFIG_ZONE_DMA))
+		arm64_dma_phys_limit = max_zone_phys(ARCH_ZONE_DMA_BITS);
+
 	if (IS_ENABLED(CONFIG_ZONE_DMA32))
-		arm64_dma32_phys_limit = max_zone_dma32_phys();
+		arm64_dma32_phys_limit = max_zone_phys(32);
 	else
 		arm64_dma32_phys_limit = PHYS_MASK + 1;
 
@@ -430,7 +454,7 @@ void __init arm64_memblock_init(void)
 
 	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
 
-	dma_contiguous_reserve(arm64_dma32_phys_limit);
+	dma_contiguous_reserve(arm64_dma_phys_limit ? : arm64_dma32_phys_limit);
 }
 
 void __init bootmem_init(void)
@@ -534,7 +558,7 @@ static void __init free_unused_memmap(void)
 void __init mem_init(void)
 {
 	if (swiotlb_force == SWIOTLB_FORCE ||
-	    max_pfn > (arm64_dma32_phys_limit >> PAGE_SHIFT))
+	    max_pfn > PFN_DOWN(arm64_dma_phys_limit ? : arm64_dma32_phys_limit))
 		swiotlb_init(1);
 	else
 		swiotlb_force = SWIOTLB_NO_FORCE;

commit a573cdd7973dedd87e62196c400332896bb236c8
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Wed Sep 11 20:25:44 2019 +0200

    arm64: rename variables used to calculate ZONE_DMA32's size
    
    Let the name indicate that they are used to calculate ZONE_DMA32's size
    as opposed to ZONE_DMA.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 098c0f5bedf6..8e9bc64c5878 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -56,7 +56,7 @@ EXPORT_SYMBOL(physvirt_offset);
 struct page *vmemmap __ro_after_init;
 EXPORT_SYMBOL(vmemmap);
 
-phys_addr_t arm64_dma_phys_limit __ro_after_init;
+phys_addr_t arm64_dma32_phys_limit __ro_after_init;
 
 #ifdef CONFIG_KEXEC_CORE
 /*
@@ -174,7 +174,7 @@ static void __init reserve_elfcorehdr(void)
  * currently assumes that for memory starting above 4G, 32-bit devices will
  * use a DMA offset.
  */
-static phys_addr_t __init max_zone_dma_phys(void)
+static phys_addr_t __init max_zone_dma32_phys(void)
 {
 	phys_addr_t offset = memblock_start_of_DRAM() & GENMASK_ULL(63, 32);
 	return min(offset + (1ULL << 32), memblock_end_of_DRAM());
@@ -187,7 +187,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
 
 #ifdef CONFIG_ZONE_DMA32
-	max_zone_pfns[ZONE_DMA32] = PFN_DOWN(arm64_dma_phys_limit);
+	max_zone_pfns[ZONE_DMA32] = PFN_DOWN(arm64_dma32_phys_limit);
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max;
 
@@ -200,16 +200,16 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	struct memblock_region *reg;
 	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
-	unsigned long max_dma = min;
+	unsigned long max_dma32 = min;
 
 	memset(zone_size, 0, sizeof(zone_size));
 
 	/* 4GB maximum for 32-bit only capable devices */
 #ifdef CONFIG_ZONE_DMA32
-	max_dma = PFN_DOWN(arm64_dma_phys_limit);
-	zone_size[ZONE_DMA32] = max_dma - min;
+	max_dma32 = PFN_DOWN(arm64_dma32_phys_limit);
+	zone_size[ZONE_DMA32] = max_dma32 - min;
 #endif
-	zone_size[ZONE_NORMAL] = max - max_dma;
+	zone_size[ZONE_NORMAL] = max - max_dma32;
 
 	memcpy(zhole_size, zone_size, sizeof(zhole_size));
 
@@ -221,14 +221,14 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 			continue;
 
 #ifdef CONFIG_ZONE_DMA32
-		if (start < max_dma) {
-			unsigned long dma_end = min(end, max_dma);
+		if (start < max_dma32) {
+			unsigned long dma_end = min(end, max_dma32);
 			zhole_size[ZONE_DMA32] -= dma_end - start;
 		}
 #endif
-		if (end > max_dma) {
+		if (end > max_dma32) {
 			unsigned long normal_end = min(end, max);
-			unsigned long normal_start = max(start, max_dma);
+			unsigned long normal_start = max(start, max_dma32);
 			zhole_size[ZONE_NORMAL] -= normal_end - normal_start;
 		}
 	}
@@ -420,9 +420,9 @@ void __init arm64_memblock_init(void)
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA32))
-		arm64_dma_phys_limit = max_zone_dma_phys();
+		arm64_dma32_phys_limit = max_zone_dma32_phys();
 	else
-		arm64_dma_phys_limit = PHYS_MASK + 1;
+		arm64_dma32_phys_limit = PHYS_MASK + 1;
 
 	reserve_crashkernel();
 
@@ -430,7 +430,7 @@ void __init arm64_memblock_init(void)
 
 	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
 
-	dma_contiguous_reserve(arm64_dma_phys_limit);
+	dma_contiguous_reserve(arm64_dma32_phys_limit);
 }
 
 void __init bootmem_init(void)
@@ -534,7 +534,7 @@ static void __init free_unused_memmap(void)
 void __init mem_init(void)
 {
 	if (swiotlb_force == SWIOTLB_FORCE ||
-	    max_pfn > (arm64_dma_phys_limit >> PAGE_SHIFT))
+	    max_pfn > (arm64_dma32_phys_limit >> PAGE_SHIFT))
 		swiotlb_init(1);
 	else
 		swiotlb_force = SWIOTLB_NO_FORCE;

commit ae970dc096b2d39f65f2e18d142e3978dc9ee1c7
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Wed Sep 11 20:25:43 2019 +0200

    arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    By the time we call zones_sizes_init() arm64_dma_phys_limit already
    contains the result of max_zone_dma_phys(). We use the variable instead
    of calling the function directly to save some precious cpu time.
    
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 45c00a54909c..098c0f5bedf6 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -187,7 +187,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
 
 #ifdef CONFIG_ZONE_DMA32
-	max_zone_pfns[ZONE_DMA32] = PFN_DOWN(max_zone_dma_phys());
+	max_zone_pfns[ZONE_DMA32] = PFN_DOWN(arm64_dma_phys_limit);
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max;
 

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit b6d00d47e81a49f6cf462518c10408f37a3e6785
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:22 2019 +0100

    arm64: mm: Introduce 52-bit Kernel VAs
    
    Most of the machinery is now in place to enable 52-bit kernel VAs that
    are detectable at boot time.
    
    This patch adds a Kconfig option for 52-bit user and kernel addresses
    and plumbs in the requisite CONFIG_ macros as well as sets TCR.T1SZ,
    physvirt_offset and vmemmap at early boot.
    
    To simplify things this patch also removes the 52-bit user/48-bit kernel
    kconfig option.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 2940221e5519..531c497c5758 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -325,6 +325,16 @@ void __init arm64_memblock_init(void)
 
 	vmemmap = ((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT));
 
+	/*
+	 * If we are running with a 52-bit kernel VA config on a system that
+	 * does not support it, we have to offset our vmemmap and physvirt_offset
+	 * s.t. we avoid the 52-bit portion of the direct linear map
+	 */
+	if (IS_ENABLED(CONFIG_ARM64_VA_BITS_52) && (vabits_actual != 52)) {
+		vmemmap += (_PAGE_OFFSET(48) - _PAGE_OFFSET(52)) >> PAGE_SHIFT;
+		physvirt_offset = PHYS_OFFSET - _PAGE_OFFSET(48);
+	}
+
 	/*
 	 * Remove the memory that we will not be able to cover with the
 	 * linear mapping. Take care not to clip the kernel which may be

commit c8b6d2ccf9b10ce872cdea037f9685804440bb7e
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:20 2019 +0100

    arm64: mm: Separate out vmemmap
    
    vmemmap is a preprocessor definition that depends on a variable,
    memstart_addr. In a later patch we will need to expand the size of
    the VMEMMAP region and optionally modify vmemmap depending upon
    whether or not hardware support is available for 52-bit virtual
    addresses.
    
    This patch changes vmemmap to be a variable. As the old definition
    depended on a variable load, this should not affect performance
    noticeably.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index e752f46d430e..2940221e5519 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -53,6 +53,9 @@ EXPORT_SYMBOL(memstart_addr);
 s64 physvirt_offset __ro_after_init;
 EXPORT_SYMBOL(physvirt_offset);
 
+struct page *vmemmap __ro_after_init;
+EXPORT_SYMBOL(vmemmap);
+
 phys_addr_t arm64_dma_phys_limit __ro_after_init;
 
 #ifdef CONFIG_KEXEC_CORE
@@ -320,6 +323,8 @@ void __init arm64_memblock_init(void)
 
 	physvirt_offset = PHYS_OFFSET - PAGE_OFFSET;
 
+	vmemmap = ((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT));
+
 	/*
 	 * Remove the memory that we will not be able to cover with the
 	 * linear mapping. Take care not to clip the kernel which may be

commit 5383cc6efed13784ddb3cff2cc183b6b8c50c8db
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:18 2019 +0100

    arm64: mm: Introduce vabits_actual
    
    In order to support 52-bit kernel addresses detectable at boot time, one
    needs to know the actual VA_BITS detected. A new variable vabits_actual
    is introduced in this commit and employed for the KVM hypervisor layout,
    KASAN, fault handling and phys-to/from-virt translation where there
    would normally be compile time constants.
    
    In order to maintain performance in phys_to_virt, another variable
    physvirt_offset is introduced.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 62927ed02229..e752f46d430e 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -50,6 +50,9 @@
 s64 memstart_addr __ro_after_init = -1;
 EXPORT_SYMBOL(memstart_addr);
 
+s64 physvirt_offset __ro_after_init;
+EXPORT_SYMBOL(physvirt_offset);
+
 phys_addr_t arm64_dma_phys_limit __ro_after_init;
 
 #ifdef CONFIG_KEXEC_CORE
@@ -301,7 +304,7 @@ static void __init fdt_enforce_memory_region(void)
 
 void __init arm64_memblock_init(void)
 {
-	const s64 linear_region_size = BIT(VA_BITS - 1);
+	const s64 linear_region_size = BIT(vabits_actual - 1);
 
 	/* Handle linux,usable-memory-range property */
 	fdt_enforce_memory_region();
@@ -315,6 +318,8 @@ void __init arm64_memblock_init(void)
 	memstart_addr = round_down(memblock_start_of_DRAM(),
 				   ARM64_MEMSTART_ALIGN);
 
+	physvirt_offset = PHYS_OFFSET - PAGE_OFFSET;
+
 	/*
 	 * Remove the memory that we will not be able to cover with the
 	 * linear mapping. Take care not to clip the kernel which may be

commit 14c127c957c1c6070647c171e72f06e0db275ebf
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:14 2019 +0100

    arm64: mm: Flip kernel VA space
    
    In order to allow for a KASAN shadow that changes size at boot time, one
    must fix the KASAN_SHADOW_END for both 48 & 52-bit VAs and "grow" the
    start address. Also, it is highly desirable to maintain the same
    function addresses in the kernel .text between VA sizes. Both of these
    requirements necessitate us to flip the kernel address space halves s.t.
    the direct linear map occupies the lower addresses.
    
    This patch puts the direct linear map in the lower addresses of the
    kernel VA range and everything else in the higher ranges.
    
    We need to adjust:
     *) KASAN shadow region placement logic,
     *) KASAN_SHADOW_OFFSET computation logic,
     *) virt_to_phys, phys_to_virt checks,
     *) page table dumper.
    
    These are all small changes, that need to take place atomically, so they
    are bundled into this commit.
    
    As part of the re-arrangement, a guard region of 2MB (to preserve
    alignment for fixed map) is added after the vmemmap. Otherwise the
    vmemmap could intersect with IS_ERR pointers.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f3c795278def..62927ed02229 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -301,7 +301,7 @@ static void __init fdt_enforce_memory_region(void)
 
 void __init arm64_memblock_init(void)
 {
-	const s64 linear_region_size = -(s64)PAGE_OFFSET;
+	const s64 linear_region_size = BIT(VA_BITS - 1);
 
 	/* Handle linux,usable-memory-range property */
 	fdt_enforce_memory_region();
@@ -309,13 +309,6 @@ void __init arm64_memblock_init(void)
 	/* Remove memory above our supported physical address size */
 	memblock_remove(1ULL << PHYS_MASK_SHIFT, ULLONG_MAX);
 
-	/*
-	 * Ensure that the linear region takes up exactly half of the kernel
-	 * virtual address space. This way, we can distinguish a linear address
-	 * from a kernel/module/vmalloc address by testing a single bit.
-	 */
-	BUILD_BUG_ON(linear_region_size != BIT(VA_BITS - 1));
-
 	/*
 	 * Select a suitable value for the base of physical memory.
 	 */

commit 13776f9d40a028a245bb766269e360f5b7a62721
Author: Junhua Huang <huang.junhua@zte.com.cn>
Date:   Sat Jul 6 14:41:15 2019 +0800

    arm64: mm: free the initrd reserved memblock in a aligned manner
    
    We should free the initrd reserved memblock in an aligned manner,
    because the initrd reserves the memblock in an aligned manner
    in arm64_memblock_init().
    Otherwise there are some fragments in memblock_reserved regions
    after free_initrd_mem(). e.g.:
    /sys/kernel/debug/memblock # cat reserved
       0: 0x0000000080080000..0x00000000817fafff
       1: 0x0000000083400000..0x0000000083ffffff
       2: 0x0000000090000000..0x000000009000407f
       3: 0x00000000b0000000..0x00000000b000003f
       4: 0x00000000b26184ea..0x00000000b2618fff
    The fragments like the ranges from b0000000 to b000003f and
    from b26184ea to b2618fff should be freed.
    
    And we can do free_reserved_area() after memblock_free(),
    as free_reserved_area() calls __free_pages(), once we've done
    that it could be allocated somewhere else,
    but memblock and iomem still say this is reserved memory.
    
    Fixes: 05c58752f9dc ("arm64: To remove initrd reserved area entry from memblock")
    Signed-off-by: Junhua Huang <huang.junhua@zte.com.cn>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f3c795278def..b1ee6cb4b17f 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -570,8 +570,12 @@ void free_initmem(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
+	unsigned long aligned_start, aligned_end;
+
+	aligned_start = __virt_to_phys(start) & PAGE_MASK;
+	aligned_end = PAGE_ALIGN(__virt_to_phys(end));
+	memblock_free(aligned_start, aligned_end - aligned_start);
 	free_reserved_area((void *)start, (void *)end, 0, "initrd");
-	memblock_free(__virt_to_phys(start), end - start);
 }
 #endif
 

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d2adffb81b5d..749c9b269f08 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -1,20 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/mm/init.c
  *
  * Copyright (C) 1995-2005 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/kernel.h>

commit 0c1f14ed12262f45a3af1d588e4d7bd12438b8f5
Author: Miles Chen <miles.chen@mediatek.com>
Date:   Wed May 29 00:08:20 2019 +0800

    arm64: mm: make CONFIG_ZONE_DMA32 configurable
    
    This change makes CONFIG_ZONE_DMA32 defuly y and allows users
    to overwrite it only when CONFIG_EXPERT=y.
    
    For the SoCs that do not need CONFIG_ZONE_DMA32, this is the
    first step to manage all available memory by a single
    zone(normal zone) to reduce the overhead of multiple zones.
    
    The change also fixes a build error when CONFIG_NUMA=y and
    CONFIG_ZONE_DMA32=n.
    
    arch/arm64/mm/init.c:195:17: error: use of undeclared identifier 'ZONE_DMA32'
                    max_zone_pfns[ZONE_DMA32] = PFN_DOWN(max_zone_dma_phys());
    
    Change since v1:
    1. only expose CONFIG_ZONE_DMA32 when CONFIG_EXPERT=y
    2. remove redundant IS_ENABLED(CONFIG_ZONE_DMA32)
    
    Cc: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Miles Chen <miles.chen@mediatek.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d2adffb81b5d..f643bd45ff69 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -191,8 +191,9 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
 
-	if (IS_ENABLED(CONFIG_ZONE_DMA32))
-		max_zone_pfns[ZONE_DMA32] = PFN_DOWN(max_zone_dma_phys());
+#ifdef CONFIG_ZONE_DMA32
+	max_zone_pfns[ZONE_DMA32] = PFN_DOWN(max_zone_dma_phys());
+#endif
 	max_zone_pfns[ZONE_NORMAL] = max;
 
 	free_area_init_nodes(max_zone_pfns);

commit 87dfb311b707cd4c4b666c9af0fa15acbe6eee99
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue May 14 15:46:51 2019 -0700

    treewide: replace #include <asm/sizes.h> with #include <linux/sizes.h>
    
    Since commit dccd2304cc90 ("ARM: 7430/1: sizes.h: move from asm-generic
    to <linux/sizes.h>"), <asm/sizes.h> and <asm-generic/sizes.h> are just
    wrappers of <linux/sizes.h>.
    
    This commit replaces all <asm/sizes.h> and <asm-generic/sizes.h> to
    prepare for the removal.
    
    Link: http://lkml.kernel.org/r/1553267665-27228-1-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 007c05a4cce0..d2adffb81b5d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -48,7 +48,7 @@
 #include <asm/numa.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
-#include <asm/sizes.h>
+#include <linux/sizes.h>
 #include <asm/tlb.h>
 #include <asm/alternative.h>
 

commit d8ae8a3765bfa1f9bf977e2496fcc9cf64fbfabd
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 13 17:18:30 2019 -0700

    initramfs: move the legacy keepinitrd parameter to core code
    
    No need to handle the freeing disable in arch code when we already have a
    core hook (and a different name for the option) for it.
    
    Link: http://lkml.kernel.org/r/20190213174621.29297-7-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>   [m68k]
    Cc: Steven Price <steven.price@arm.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 40e2d7e5efcb..007c05a4cce0 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -578,24 +578,11 @@ void free_initmem(void)
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
-
-static int keep_initrd __initdata;
-
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (!keep_initrd) {
-		free_reserved_area((void *)start, (void *)end, 0, "initrd");
-		memblock_free(__virt_to_phys(start), end - start);
-	}
-}
-
-static int __init keepinitrd_setup(char *__unused)
-{
-	keep_initrd = 1;
-	return 1;
+	free_reserved_area((void *)start, (void *)end, 0, "initrd");
+	memblock_free(__virt_to_phys(start), end - start);
 }
-
-__setup("keepinitrd", keepinitrd_setup);
 #endif
 
 /*

commit c620f7bd0ba5c882b3e7fc199a8d5c2f6c2f5263
Merge: dd4e5d6106b2 b33f908811b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 17:54:22 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Mostly just incremental improvements here:
    
       - Introduce AT_HWCAP2 for advertising CPU features to userspace
    
       - Expose SVE2 availability to userspace
    
       - Support for "data cache clean to point of deep persistence" (DC PODP)
    
       - Honour "mitigations=off" on the cmdline and advertise status via
         sysfs
    
       - CPU timer erratum workaround (Neoverse-N1 #1188873)
    
       - Introduce perf PMU driver for the SMMUv3 performance counters
    
       - Add config option to disable the kuser helpers page for AArch32 tasks
    
       - Futex modifications to ensure liveness under contention
    
       - Rework debug exception handling to seperate kernel and user
         handlers
    
       - Non-critical fixes and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      Documentation: Add ARM64 to kernel-parameters.rst
      arm64/speculation: Support 'mitigations=' cmdline option
      arm64: ssbs: Don't treat CPUs with SSBS as unaffected by SSB
      arm64: enable generic CPU vulnerabilites support
      arm64: add sysfs vulnerability show for speculative store bypass
      arm64: Fix size of __early_cpu_boot_status
      clocksource/arm_arch_timer: Use arch_timer_read_counter to access stable counters
      clocksource/arm_arch_timer: Remove use of workaround static key
      clocksource/arm_arch_timer: Drop use of static key in arch_timer_reg_read_stable
      clocksource/arm_arch_timer: Direcly assign set_next_event workaround
      arm64: Use arch_timer_read_counter instead of arch_counter_get_cntvct
      watchdog/sbsa: Use arch_timer_read_counter instead of arch_counter_get_cntvct
      ARM: vdso: Remove dependency with the arch_timer driver internals
      arm64: Apply ARM64_ERRATUM_1188873 to Neoverse-N1
      arm64: Add part number for Neoverse N1
      arm64: Make ARM64_ERRATUM_1188873 depend on COMPAT
      arm64: Restrict ARM64_ERRATUM_1188873 mitigation to AArch32
      arm64: mm: Remove pte_unmap_nested()
      arm64: Fix compiler warning from pte_unmap() with -Wunused-but-set-variable
      arm64: compat: Reduce address limit for 64K pages
      ...

commit d4d18e3ec6091843f607e8929a56723e28f393a6
Author: Bjorn Andersson <bjorn.andersson@linaro.org>
Date:   Wed Apr 17 21:29:29 2019 -0700

    arm64: mm: Ensure tail of unaligned initrd is reserved
    
    In the event that the start address of the initrd is not aligned, but
    has an aligned size, the base + size will not cover the entire initrd
    image and there is a chance that the kernel will corrupt the tail of the
    image.
    
    By aligning the end of the initrd to a page boundary and then
    subtracting the adjusted start address the memblock reservation will
    cover all pages that contains the initrd.
    
    Fixes: c756c592e442 ("arm64: Utilize phys_initrd_start/phys_initrd_size")
    Cc: stable@vger.kernel.org
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 6bc135042f5e..7cae155e81a5 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -363,7 +363,7 @@ void __init arm64_memblock_init(void)
 		 * Otherwise, this is a no-op
 		 */
 		u64 base = phys_initrd_start & PAGE_MASK;
-		u64 size = PAGE_ALIGN(phys_initrd_size);
+		u64 size = PAGE_ALIGN(phys_initrd_start + phys_initrd_size) - base;
 
 		/*
 		 * We can only add back the initrd memory if we don't end up

commit 70b3d237bd7f52d3322c750a358581bf5a267699
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Apr 3 17:58:39 2019 +0100

    arm64: mm: Ensure we ignore the initrd if it is placed out of range
    
    If the initrd payload isn't completely accessible via the linear map,
    then we print a warning during boot and nobble the virtual address of
    the payload so that we ignore it later on.
    
    Unfortunately, since commit c756c592e442 ("arm64: Utilize
    phys_initrd_start/phys_initrd_size"), the virtual address isn't
    initialised until later anyway, so we need to nobble the size of the
    payload to ensure that we don't try to use it later on.
    
    Fixes: c756c592e442 ("arm64: Utilize phys_initrd_start/phys_initrd_size")
    Reported-by: Pierre Kuo <vichy.kuo@gmail.com>
    Acked-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 43fa75601b3d..03a8a6888ec0 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -377,7 +377,7 @@ void __init arm64_memblock_init(void)
 			 base + size > memblock_start_of_DRAM() +
 				       linear_region_size,
 			"initrd not fully accessible via the linear mapping -- please check your bootloader ...\n")) {
-			initrd_start = 0;
+			phys_initrd_size = 0;
 		} else {
 			memblock_remove(base, size); /* clear MEMBLOCK_ flags */
 			memblock_add(base, size);

commit 19d6242ece1f35aa004a7da9adf52e0ec18a854e
Author: Miles Chen <miles.chen@mediatek.com>
Date:   Thu Mar 21 12:21:25 2019 +0800

    arm64: setup min_low_pfn
    
    When debugging with CONFIG_PAGE_OWNER, I noticed that the min_low_pfn
    on arm64 is always zero and the page owner scanning has to start from zero.
    We have to loop a while before we see the first valid pfn.
    (see: read_page_owner())
    
    Setup min_low_pfn to save some loops.
    
    Before setting min_low_pfn:
    
    [   21.265602] min_low_pfn=0, *ppos=0
    Page allocated via order 0, mask 0x100cca(GFP_HIGHUSER_MOVABLE)
    PFN 262144 type Movable Block 512 type Movable Flags 0x8001e
    referenced|uptodate|dirty|lru|swapbacked)
    prep_new_page+0x13c/0x140
    get_page_from_freelist+0x254/0x1068
    __alloc_pages_nodemask+0xd4/0xcb8
    
    After setting min_low_pfn:
    
    [   11.025787] min_low_pfn=262144, *ppos=0
    Page allocated via order 0, mask 0x100cca(GFP_HIGHUSER_MOVABLE)
    PFN 262144 type Movable Block 512 type Movable Flags 0x8001e
    referenced|uptodate|dirty|lru|swapbacked)
    prep_new_page+0x13c/0x140
    get_page_from_freelist+0x254/0x1068
    __alloc_pages_nodemask+0xd4/0xcb8
    shmem_alloc_page+0x7c/0xa0
    shmem_alloc_and_acct_page+0x124/0x1e8
    shmem_getpage_gfp.isra.7+0x118/0x878
    shmem_write_begin+0x38/0x68
    
    Signed-off-by: Miles Chen <miles.chen@mediatek.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index c29b17b520cd..43fa75601b3d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -440,6 +440,7 @@ void __init bootmem_init(void)
 	early_memtest(min << PAGE_SHIFT, max << PAGE_SHIFT);
 
 	max_pfn = max_low_pfn = max;
+	min_low_pfn = min;
 
 	arm64_numa_init();
 	/*

commit 344bf332ceb2364a2fcd3ab4133dce5ea35c2594
Author: Muchun Song <smuchun@gmail.com>
Date:   Sat Mar 30 21:13:46 2019 +0800

    arm64: mm: fix incorrect assignment of 'max_mapnr'
    
    Although we don't actually make use of the 'max_mapnr' global variable,
    we do set it to a junk value for !CONFIG_FLATMEM configurations that
    leave mem_map uninitialised.
    
    To avoid somebody tripping over this in future, set 'max_mapnr' using
    max_pfn, which is calculated directly from the memblock information.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Muchun Song <smuchun@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 6bc135042f5e..c29b17b520cd 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -535,7 +535,7 @@ void __init mem_init(void)
 	else
 		swiotlb_force = SWIOTLB_NO_FORCE;
 
-	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);
+	set_max_mapnr(max_pfn - PHYS_PFN_OFFSET);
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP
 	free_unused_memmap();

commit 3d8dfe75ef69f4dd4ba35c09b20a5aa58b4a5078
Merge: d60752629693 b855b58ac1b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 10:17:23 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Pseudo NMI support for arm64 using GICv3 interrupt priorities
    
     - uaccess macros clean-up (unsafe user accessors also merged but
       reverted, waiting for objtool support on arm64)
    
     - ptrace regsets for Pointer Authentication (ARMv8.3) key management
    
     - inX() ordering w.r.t. delay() on arm64 and riscv (acks in place by
       the riscv maintainers)
    
     - arm64/perf updates: PMU bindings converted to json-schema, unused
       variable and misleading comment removed
    
     - arm64/debug fixes to ensure checking of the triggering exception
       level and to avoid the propagation of the UNKNOWN FAR value into the
       si_code for debug signals
    
     - Workaround for Fujitsu A64FX erratum 010001
    
     - lib/raid6 ARM NEON optimisations
    
     - NR_CPUS now defaults to 256 on arm64
    
     - Minor clean-ups (documentation/comments, Kconfig warning, unused
       asm-offsets, clang warnings)
    
     - MAINTAINERS update for list information to the ARM64 ACPI entry
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      arm64: mmu: drop paging_init comments
      arm64: debug: Ensure debug handlers check triggering exception level
      arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
      Revert "arm64: uaccess: Implement unsafe accessors"
      arm64: avoid clang warning about self-assignment
      arm64: Kconfig.platforms: fix warning unmet direct dependencies
      lib/raid6: arm: optimize away a mask operation in NEON recovery routine
      lib/raid6: use vdupq_n_u8 to avoid endianness warnings
      arm64: io: Hook up __io_par() for inX() ordering
      riscv: io: Update __io_[p]ar() macros to take an argument
      asm-generic/io: Pass result of I/O accessor to __io_[p]ar()
      arm64: Add workaround for Fujitsu A64FX erratum 010001
      arm64: Rename get_thread_info()
      arm64: Remove documentation about TIF_USEDFPU
      arm64: irqflags: Fix clang build warnings
      arm64: Enable the support of pseudo-NMIs
      arm64: Skip irqflags tracing for NMI in IRQs disabled context
      arm64: Skip preemption when exiting an NMI
      arm64: Handle serror in NMI context
      irqchip/gic-v3: Allow interrupts to be set as pseudo-NMI
      ...

commit d9fa9d951779eb8110879f796434876a58321ae9
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Mar 5 15:47:28 2019 -0800

    arm64: kdump: no need to mark crashkernel pages manually PG_reserved
    
    The crashkernel is reserved via memblock_reserve().  memblock_free_all()
    will call free_low_memory_core_early(), which will go over all reserved
    memblocks, marking the pages as PG_reserved.
    
    So manually marking pages as PG_reserved is not necessary, they are
    already in the desired state (otherwise they would have been handed over
    to the buddy as free pages and bad things would happen).
    
    Link: http://lkml.kernel.org/r/20190114125903.24845-8-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Matthias Brugger <mbrugger@suse.com>
    Reviewed-by: Bhupesh Sharma <bhsharma@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Stefan Agner <stefan@agner.ch>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Greg Hackmann <ghackmann@android.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: CHANDAN VN <chandan.vn@samsung.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 7205a9085b4d..c38976b70069 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -118,35 +118,10 @@ static void __init reserve_crashkernel(void)
 	crashk_res.start = crash_base;
 	crashk_res.end = crash_base + crash_size - 1;
 }
-
-static void __init kexec_reserve_crashkres_pages(void)
-{
-#ifdef CONFIG_HIBERNATION
-	phys_addr_t addr;
-	struct page *page;
-
-	if (!crashk_res.end)
-		return;
-
-	/*
-	 * To reduce the size of hibernation image, all the pages are
-	 * marked as Reserved initially.
-	 */
-	for (addr = crashk_res.start; addr < (crashk_res.end + 1);
-			addr += PAGE_SIZE) {
-		page = phys_to_page(addr);
-		SetPageReserved(page);
-	}
-#endif
-}
 #else
 static void __init reserve_crashkernel(void)
 {
 }
-
-static void __init kexec_reserve_crashkres_pages(void)
-{
-}
 #endif /* CONFIG_KEXEC_CORE */
 
 #ifdef CONFIG_CRASH_DUMP
@@ -586,8 +561,6 @@ void __init mem_init(void)
 	/* this will put all unused low memory onto the freelists */
 	memblock_free_all();
 
-	kexec_reserve_crashkres_pages();
-
 	mem_init_print_info(NULL);
 
 	/*

commit a2c801c53d1682871fba1e037c9d3b0c9fffee8a
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Wed Jan 9 13:21:00 2019 -0700

    arm64: mm: make use of new memblocks_present() helper
    
    Cleanup the arm64_memory_present() function seeing it's very
    similar to other arches.
    
    memblocks_present() is a direct replacement of arm64_memory_present()
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 7205a9085b4d..2302b4093a63 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -285,24 +285,6 @@ int pfn_valid(unsigned long pfn)
 }
 EXPORT_SYMBOL(pfn_valid);
 
-#ifndef CONFIG_SPARSEMEM
-static void __init arm64_memory_present(void)
-{
-}
-#else
-static void __init arm64_memory_present(void)
-{
-	struct memblock_region *reg;
-
-	for_each_memblock(memory, reg) {
-		int nid = memblock_get_region_node(reg);
-
-		memory_present(nid, memblock_region_memory_base_pfn(reg),
-				memblock_region_memory_end_pfn(reg));
-	}
-}
-#endif
-
 static phys_addr_t memory_limit = PHYS_ADDR_MAX;
 
 /*
@@ -489,7 +471,7 @@ void __init bootmem_init(void)
 	 * Sparsemem tries to allocate bootmem in memory_present(), so must be
 	 * done after the fixed reservations.
 	 */
-	arm64_memory_present();
+	memblocks_present();
 
 	sparse_init();
 	zone_sizes_init(min, max);

commit 078a5a4faf64fefaf13478a9091782432cad33fa
Merge: 1205b62390ee 7e0b44e870cf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 5 11:28:39 2019 -0800

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "I'm safely chained back up to my desk, so please pull these arm64
      fixes for -rc1 that address some issues that cropped up during the
      merge window:
    
       - Prevent KASLR from mapping the top page of the virtual address
         space
    
       - Fix device-tree probing of SDEI driver
    
       - Fix incorrect register offset definition in Hisilicon DDRC PMU
         driver
    
       - Fix compilation issue with older binutils not liking unsigned
         immediates
    
       - Fix uapi headers so that libc can provide its own sigcontext
         definition
    
       - Fix handling of private compat syscalls
    
       - Hook up compat io_pgetevents() syscall for 32-bit tasks
    
       - Cleanup to arm64 Makefile (including now to avoid silly conflicts)"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: compat: Hook up io_pgetevents() for 32-bit tasks
      arm64: compat: Don't pull syscall number from regs in arm_compat_syscall
      arm64: compat: Avoid sending SIGILL for unallocated syscall numbers
      arm64/sve: Disentangle <uapi/asm/ptrace.h> from <uapi/asm/sigcontext.h>
      arm64/sve: ptrace: Fix SVE_PT_REGS_OFFSET definition
      drivers/perf: hisi: Fixup one DDRC PMU register offset
      arm64: replace arm64-obj-* in Makefile with obj-*
      arm64: kaslr: Reserve size of ARM64_MEMSTART_ALIGN in linear region
      firmware: arm_sdei: Fix DT platform device creation
      firmware: arm_sdei: fix wrong of_node_put() in init function
      arm64: entry: remove unused register aliases
      arm64: smp: Fix compilation error

commit c8a43c18a97845e7f94ed7d181c11f41964976a2
Author: Yueyi Li <liyueyi@live.com>
Date:   Mon Dec 24 07:40:07 2018 +0000

    arm64: kaslr: Reserve size of ARM64_MEMSTART_ALIGN in linear region
    
    When KASLR is enabled (CONFIG_RANDOMIZE_BASE=y), the top 4K of kernel
    virtual address space may be mapped to physical addresses despite being
    reserved for ERR_PTR values.
    
    Fix the randomization of the linear region so that we avoid mapping the
    last page of the virtual address space.
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: liyueyi <liyueyi@live.com>
    [will: rewrote commit message; merged in suggestion from Ard]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 4bfe0fc9edac..124be28e4e16 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -457,7 +457,7 @@ void __init arm64_memblock_init(void)
 		 * memory spans, randomize the linear region as well.
 		 */
 		if (memstart_offset_seed > 0 && range >= ARM64_MEMSTART_ALIGN) {
-			range = range / ARM64_MEMSTART_ALIGN + 1;
+			range /= ARM64_MEMSTART_ALIGN;
 			memstart_addr -= ARM64_MEMSTART_ALIGN *
 					 ((range * memstart_offset_seed) >> 16);
 		}

commit 030672aea826adf3dee9100ee8ac303b62c8fe7f
Merge: 24dc83635ffe 5801169a2ed2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 28 20:08:34 2018 -0800

    Merge tag 'devicetree-for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/robh/linux
    
    Pull Devicetree updates from Rob Herring:
     "The biggest highlight here is the start of using json-schema for DT
      bindings. Being able to validate bindings has been discussed for years
      with little progress.
    
       - Initial support for DT bindings using json-schema language. This is
         the start of converting DT bindings from free-form text to a
         structured format.
    
       - Reworking of initrd address initialization. This moves to using the
         phys address instead of virt addr in the DT parsing code. This
         rework was motivated by CONFIG_DEV_BLK_INITRD causing unnecessary
         rebuilding of lots of files.
    
       - Fix stale phandle entries in phandle cache
    
       - DT overlay validation improvements. This exposed several memory
         leak bugs which have been fixed.
    
       - Use node name and device_type helper functions in DT code
    
       - Last remaining conversions to using %pOFn printk specifier instead
         of device_node.name directly
    
       - Create new common RTC binding doc and move all trivial RTC devices
         out of trivial-devices.txt.
    
       - New bindings for Freescale MAG3110 magnetometer, Cadence Sierra
         PHY, and Xen shared memory
    
       - Update dtc to upstream version v1.4.7-57-gf267e674d145"
    
    * tag 'devicetree-for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/robh/linux: (68 commits)
      of: __of_detach_node() - remove node from phandle cache
      of: of_node_get()/of_node_put() nodes held in phandle cache
      gpio-omap.txt: add reg and interrupts properties
      dt-bindings: mrvl,intc: fix a trivial typo
      dt-bindings: iio: magnetometer: add dt-bindings for freescale mag3110
      dt-bindings: Convert trivial-devices.txt to json-schema
      dt-bindings: arm: mrvl: amend Browstone compatible string
      dt-bindings: arm: Convert Tegra board/soc bindings to json-schema
      dt-bindings: arm: Convert ZTE board/soc bindings to json-schema
      dt-bindings: arm: Add missing Xilinx boards
      dt-bindings: arm: Convert Xilinx board/soc bindings to json-schema
      dt-bindings: arm: Convert VIA board/soc bindings to json-schema
      dt-bindings: arm: Convert ST STi board/soc bindings to json-schema
      dt-bindings: arm: Convert SPEAr board/soc bindings to json-schema
      dt-bindings: arm: Convert CSR SiRF board/soc bindings to json-schema
      dt-bindings: arm: Convert QCom board/soc bindings to json-schema
      dt-bindings: arm: Convert TI nspire board/soc bindings to json-schema
      dt-bindings: arm: Convert TI davinci board/soc bindings to json-schema
      dt-bindings: arm: Convert Calxeda board/soc bindings to json-schema
      dt-bindings: arm: Convert Altera board/soc bindings to json-schema
      ...

commit 5694cecdb092656a822287a6691aa7ce668c8160
Merge: 13e1ad2be3a8 12f799c8c739
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 25 17:41:56 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 festive updates from Will Deacon:
     "In the end, we ended up with quite a lot more than I expected:
    
       - Support for ARMv8.3 Pointer Authentication in userspace (CRIU and
         kernel-side support to come later)
    
       - Support for per-thread stack canaries, pending an update to GCC
         that is currently undergoing review
    
       - Support for kexec_file_load(), which permits secure boot of a kexec
         payload but also happens to improve the performance of kexec
         dramatically because we can avoid the sucky purgatory code from
         userspace. Kdump will come later (requires updates to libfdt).
    
       - Optimisation of our dynamic CPU feature framework, so that all
         detected features are enabled via a single stop_machine()
         invocation
    
       - KPTI whitelisting of Cortex-A CPUs unaffected by Meltdown, so that
         they can benefit from global TLB entries when KASLR is not in use
    
       - 52-bit virtual addressing for userspace (kernel remains 48-bit)
    
       - Patch in LSE atomics for per-cpu atomic operations
    
       - Custom preempt.h implementation to avoid unconditional calls to
         preempt_schedule() from preempt_enable()
    
       - Support for the new 'SB' Speculation Barrier instruction
    
       - Vectorised implementation of XOR checksumming and CRC32
         optimisations
    
       - Workaround for Cortex-A76 erratum #1165522
    
       - Improved compatibility with Clang/LLD
    
       - Support for TX2 system PMUS for profiling the L3 cache and DMC
    
       - Reflect read-only permissions in the linear map by default
    
       - Ensure MMIO reads are ordered with subsequent calls to Xdelay()
    
       - Initial support for memory hotplug
    
       - Tweak the threshold when we invalidate the TLB by-ASID, so that
         mremap() performance is improved for ranges spanning multiple PMDs.
    
       - Minor refactoring and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (125 commits)
      arm64: kaslr: print PHYS_OFFSET in dump_kernel_offset()
      arm64: sysreg: Use _BITUL() when defining register bits
      arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
      arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
      arm64: docs: document pointer authentication
      arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
      arm64: enable pointer authentication
      arm64: add prctl control for resetting ptrauth keys
      arm64: perf: strip PAC when unwinding userspace
      arm64: expose user PAC bit positions via ptrace
      arm64: add basic pointer authentication support
      arm64/cpufeature: detect pointer authentication
      arm64: Don't trap host pointer auth use to EL2
      arm64/kvm: hide ptrauth from guests
      arm64/kvm: consistently handle host HCR_EL2 flags
      arm64: add pointer authentication register bits
      arm64: add comments about EC exception levels
      arm64: perf: Treat EXCLUDE_EL* bit definitions as unsigned
      arm64: kpti: Whitelist Cortex-A CPUs that don't implement the CSV3 field
      arm64: enable per-task stack canaries
      ...

commit d1402fc708e4c355813e49df6d15bc3466ba5114
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Dec 14 14:16:53 2018 -0800

    mm: introduce common STRUCT_PAGE_MAX_SHIFT define
    
    This define is used by arm64 to calculate the size of the vmemmap
    region.  It is defined as the log2 of the upper bound on the size of a
    struct page.
    
    We move it into mm_types.h so it can be defined properly instead of set
    and checked with a build bug.  This also allows us to use the same
    define for riscv.
    
    Link: http://lkml.kernel.org/r/20181107205433.3875-2-logang@deltatee.com
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 9b432d9fcada..0340e45655c6 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -610,14 +610,6 @@ void __init mem_init(void)
 	BUILD_BUG_ON(TASK_SIZE_32			> TASK_SIZE_64);
 #endif
 
-#ifdef CONFIG_SPARSEMEM_VMEMMAP
-	/*
-	 * Make sure we chose the upper bound of sizeof(struct page)
-	 * correctly when sizing the VMEMMAP array.
-	 */
-	BUILD_BUG_ON(sizeof(struct page) > (1 << STRUCT_PAGE_MAX_SHIFT));
-#endif
-
 	if (PAGE_SIZE >= 16384 && get_num_physpages() <= 128) {
 		extern int sysctl_overcommit_memory;
 		/*

commit acc2038738bd6f881e7a277ba14fa7c589fd3058
Merge: cd1cc0bef46f 2c9b0b00af23
Author: Rob Herring <robh@kernel.org>
Date:   Thu Dec 13 11:20:36 2018 -0600

    Merge branch 'yaml-bindings-for-v4.21' into dt/next

commit 4ab215061554ae2a4b78744a5dd3b3c6639f16a7
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Dec 11 18:48:48 2018 +0000

    arm64: Add memory hotplug support
    
    Wire up the basic support for hot-adding memory. Since memory hotplug
    is fairly tightly coupled to sparsemem, we tweak pfn_valid() to also
    cross-check the presence of a section in the manner of the generic
    implementation, before falling back to memblock to check for no-map
    regions within a present section as before. By having arch_add_memory(()
    create the linear mapping first, this then makes everything work in the
    way that __add_section() expects.
    
    We expect hotplug to be ACPI-driven, so the swapper_pg_dir updates
    should be safe from races by virtue of the global device hotplug lock.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 6cde00554e9b..4bfe0fc9edac 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -291,6 +291,14 @@ int pfn_valid(unsigned long pfn)
 
 	if ((addr >> PAGE_SHIFT) != pfn)
 		return 0;
+
+#ifdef CONFIG_SPARSEMEM
+	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
+		return 0;
+
+	if (!valid_section(__nr_to_section(pfn_to_section_nr(pfn))))
+		return 0;
+#endif
 	return memblock_is_map_memory(addr);
 }
 EXPORT_SYMBOL(pfn_valid);

commit 363524d2b12270d86677e1154ecc1c5061f43219
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:37 2018 +0000

    arm64: mm: Introduce DEFAULT_MAP_WINDOW
    
    We wish to introduce a 52-bit virtual address space for userspace but
    maintain compatibility with software that assumes the maximum VA space
    size is 48 bit.
    
    In order to achieve this, on 52-bit VA systems, we make mmap behave as
    if it were running on a 48-bit VA system (unless userspace explicitly
    requests a VA where addr[51:48] != 0).
    
    On a system running a 52-bit userspace we need TASK_SIZE to represent
    the 52-bit limit as it is used in various places to distinguish between
    kernelspace and userspace addresses.
    
    Thus we need a new limit for mmap, stack, ELF loader and EFI (which uses
    TTBR0) to represent the non-extended VA space.
    
    This patch introduces DEFAULT_MAP_WINDOW and DEFAULT_MAP_WINDOW_64 and
    switches the appropriate logic to use that instead of TASK_SIZE.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 2983e0fc1786..6cde00554e9b 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -609,7 +609,7 @@ void __init mem_init(void)
 	 * detected at build time already.
 	 */
 #ifdef CONFIG_COMPAT
-	BUILD_BUG_ON(TASK_SIZE_32			> TASK_SIZE_64);
+	BUILD_BUG_ON(TASK_SIZE_32 > DEFAULT_MAP_WINDOW_64);
 #endif
 
 #ifdef CONFIG_SPARSEMEM_VMEMMAP

commit 03ef055fd364e498c0633c163e7b0b0a52418e96
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:08:15 2018 +0000

    arm64: move memstart_addr export inline
    
    Since we define memstart_addr in a C file, we can have the export
    immediately after the definition of the symbol, as we do elsewhere.
    
    As a step towards removing arm64ksyms.c, move the export of
    memstart_addr to init.c, where the symbol is defined.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 9b432d9fcada..2983e0fc1786 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -59,6 +59,8 @@
  * that cannot be mistaken for a real physical address.
  */
 s64 memstart_addr __ro_after_init = -1;
+EXPORT_SYMBOL(memstart_addr);
+
 phys_addr_t arm64_dma_phys_limit __ro_after_init;
 
 #ifdef CONFIG_BLK_DEV_INITRD

commit 229c55ccb487c0c10721fdb92af874d7b8671cda
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Mon Nov 5 14:54:31 2018 -0800

    arch: Move initrd= parsing into do_mounts_initrd.c
    
    ARC, ARM, ARM64 and Unicore32 are all capable of parsing the "initrd="
    command line parameter to allow specifying the physical address and size
    of an initrd. Move that parsing into init/do_mounts_initrd.c such that
    we no longer duplicate that logic.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index a66ffcde5f13..7474093363bc 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -61,24 +61,6 @@
 s64 memstart_addr __ro_after_init = -1;
 phys_addr_t arm64_dma_phys_limit __ro_after_init;
 
-#ifdef CONFIG_BLK_DEV_INITRD
-static int __init early_initrd(char *p)
-{
-	unsigned long start, size;
-	char *endp;
-
-	start = memparse(p, &endp);
-	if (*endp == ',') {
-		size = memparse(endp + 1, NULL);
-
-		phys_initrd_start = start;
-		phys_initrd_size = size;
-	}
-	return 0;
-}
-early_param("initrd", early_initrd);
-#endif
-
 #ifdef CONFIG_KEXEC_CORE
 /*
  * reserve_crashkernel() - reserves memory for crash kernel

commit c756c592e442ba101c91daed3524ba5b3a784ba6
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Mon Nov 5 14:54:29 2018 -0800

    arm64: Utilize phys_initrd_start/phys_initrd_size
    
    ARM64 is the only architecture that re-defines
    __early_init_dt_declare_initrd() in order for that function to populate
    initrd_start/initrd_end with physical addresses instead of virtual
    addresses. Instead of having an override we can leverage
    drivers/of/fdt.c populating phys_initrd_start/phys_initrd_size to
    populate those variables for us.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 9d9582cac6c4..a66ffcde5f13 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -71,8 +71,8 @@ static int __init early_initrd(char *p)
 	if (*endp == ',') {
 		size = memparse(endp + 1, NULL);
 
-		initrd_start = start;
-		initrd_end = start + size;
+		phys_initrd_start = start;
+		phys_initrd_size = size;
 	}
 	return 0;
 }
@@ -407,14 +407,14 @@ void __init arm64_memblock_init(void)
 		memblock_add(__pa_symbol(_text), (u64)(_end - _text));
 	}
 
-	if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && initrd_start) {
+	if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && phys_initrd_size) {
 		/*
 		 * Add back the memory we just removed if it results in the
 		 * initrd to become inaccessible via the linear mapping.
 		 * Otherwise, this is a no-op
 		 */
-		u64 base = initrd_start & PAGE_MASK;
-		u64 size = PAGE_ALIGN(initrd_end) - base;
+		u64 base = phys_initrd_start & PAGE_MASK;
+		u64 size = PAGE_ALIGN(phys_initrd_size);
 
 		/*
 		 * We can only add back the initrd memory if we don't end up
@@ -458,15 +458,11 @@ void __init arm64_memblock_init(void)
 	 * pagetables with memblock.
 	 */
 	memblock_reserve(__pa_symbol(_text), _end - _text);
-#ifdef CONFIG_BLK_DEV_INITRD
-	if (initrd_start) {
-		memblock_reserve(initrd_start, initrd_end - initrd_start);
-
+	if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && phys_initrd_size) {
 		/* the generic initrd code expects virtual addresses */
-		initrd_start = __phys_to_virt(initrd_start);
-		initrd_end = __phys_to_virt(initrd_end);
+		initrd_start = __phys_to_virt(phys_initrd_start);
+		initrd_end = initrd_start + phys_initrd_size;
 	}
-#endif
 
 	early_init_fdt_scan_reserved_mem();
 

commit 24cc61d8cb5a9232fadf21a830061853c1268fdd
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Nov 7 15:16:06 2018 +0100

    arm64: memblock: don't permit memblock resizing until linear mapping is up
    
    Bhupesh reports that having numerous memblock reservations at early
    boot may result in the following crash:
    
      Unable to handle kernel paging request at virtual address ffff80003ffe0000
      ...
      Call trace:
       __memcpy+0x110/0x180
       memblock_add_range+0x134/0x2e8
       memblock_reserve+0x70/0xb8
       memblock_alloc_base_nid+0x6c/0x88
       __memblock_alloc_base+0x3c/0x4c
       memblock_alloc_base+0x28/0x4c
       memblock_alloc+0x2c/0x38
       early_pgtable_alloc+0x20/0xb0
       paging_init+0x28/0x7f8
    
    This is caused by the fact that we permit memblock resizing before the
    linear mapping is up, and so the memblock_reserved() array is moved
    into memory that is not mapped yet.
    
    So let's ensure that this crash can no longer occur, by deferring to
    call to memblock_allow_resize() to after the linear mapping has been
    created.
    
    Reported-by: Bhupesh Sharma <bhsharma@redhat.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 9d9582cac6c4..9b432d9fcada 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -483,8 +483,6 @@ void __init arm64_memblock_init(void)
 	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
 
 	dma_contiguous_reserve(arm64_dma_phys_limit);
-
-	memblock_allow_resize();
 }
 
 void __init bootmem_init(void)

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d8d73073835f..9d9582cac6c4 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -22,7 +22,6 @@
 #include <linux/errno.h>
 #include <linux/swap.h>
 #include <linux/init.h>
-#include <linux/bootmem.h>
 #include <linux/cache.h>
 #include <linux/mman.h>
 #include <linux/nodemask.h>

commit c6ffc5ca8fb311a89cb6de5c31b6511308ddac8d
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:30 2018 -0700

    memblock: rename free_all_bootmem to memblock_free_all
    
    The conversion is done using
    
    sed -i 's@free_all_bootmem@memblock_free_all@' \
        $(git grep -l free_all_bootmem)
    
    Link: http://lkml.kernel.org/r/1536927045-23536-26-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 2ddb1c5e988d..d8d73073835f 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -599,7 +599,7 @@ void __init mem_init(void)
 	free_unused_memmap();
 #endif
 	/* this will put all unused low memory onto the freelists */
-	free_all_bootmem();
+	memblock_free_all();
 
 	kexec_reserve_crashkres_pages();
 

commit 2013288f723887837d2f1cebef5fcf663b2319de
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:21 2018 -0700

    memblock: replace free_bootmem{_node} with memblock_free
    
    The free_bootmem and free_bootmem_node are merely wrappers for
    memblock_free. Replace their usage with a call to memblock_free using the
    following semantic patch:
    
    @@
    expression e1, e2, e3;
    @@
    (
    - free_bootmem(e1, e2)
    + memblock_free(e1, e2)
    |
    - free_bootmem_node(e1, e2, e3)
    + memblock_free(e2, e3)
    )
    
    Link: http://lkml.kernel.org/r/1536927045-23536-24-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 3cf87341859f..2ddb1c5e988d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -536,7 +536,7 @@ static inline void free_memmap(unsigned long start_pfn, unsigned long end_pfn)
 	 * memmap array.
 	 */
 	if (pg < pgend)
-		free_bootmem(pg, pgend - pg);
+		memblock_free(pg, pgend - pg);
 }
 
 /*

commit 8a695a5873339c2e7c746ee51e3774fedd07d0a9
Author: James Morse <james.morse@arm.com>
Date:   Fri Aug 31 16:19:43 2018 +0100

    arm64: Kconfig: Remove ARCH_HAS_HOLES_MEMORYMODEL
    
    include/linux/mmzone.h describes ARCH_HAS_HOLES_MEMORYMODEL as
    relevant when parts the memmap have been free()d. This would
    happen on systems where memory is smaller than a sparsemem-section,
    and the extra struct pages are expensive. pfn_valid() on these
    systems returns true for the whole sparsemem-section, so an extra
    memmap_valid_within() check is needed.
    
    On arm64 we have nomap memory, so always provide pfn_valid() to test
    for nomap pages. This means ARCH_HAS_HOLES_MEMORYMODEL's extra checks
    are already rolled up into pfn_valid().
    
    Remove it.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 787e27964ab9..3cf87341859f 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -284,7 +284,6 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 
 #endif /* CONFIG_NUMA */
 
-#ifdef CONFIG_HAVE_ARCH_PFN_VALID
 int pfn_valid(unsigned long pfn)
 {
 	phys_addr_t addr = pfn << PAGE_SHIFT;
@@ -294,7 +293,6 @@ int pfn_valid(unsigned long pfn)
 	return memblock_is_map_memory(addr);
 }
 EXPORT_SYMBOL(pfn_valid);
-#endif
 
 #ifndef CONFIG_SPARSEMEM
 static void __init arm64_memory_present(void)

commit edb0a20009363ae787bfe0d6fd52abb504f05113
Merge: 5e2d059b52e3 5ad356eabc47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 17 11:48:04 2018 -0700

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "A couple of arm64 fixes
    
       - Fix boot on Hikey-960 by avoiding an IPI with interrupts disabled
    
       - Fix address truncation in pfn_valid() implementation"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: mm: check for upper PAGE_SHIFT bits in pfn_valid()
      arm64: Avoid calling stop_machine() when patching jump labels

commit 5ad356eabc47d26a92140a0c4b20eba471c10de3
Author: Greg Hackmann <ghackmann@android.com>
Date:   Wed Aug 15 12:51:21 2018 -0700

    arm64: mm: check for upper PAGE_SHIFT bits in pfn_valid()
    
    ARM64's pfn_valid() shifts away the upper PAGE_SHIFT bits of the input
    before seeing if the PFN is valid.  This leads to false positives when
    some of the upper bits are set, but the lower bits match a valid PFN.
    
    For example, the following userspace code looks up a bogus entry in
    /proc/kpageflags:
    
        int pagemap = open("/proc/self/pagemap", O_RDONLY);
        int pageflags = open("/proc/kpageflags", O_RDONLY);
        uint64_t pfn, val;
    
        lseek64(pagemap, [...], SEEK_SET);
        read(pagemap, &pfn, sizeof(pfn));
        if (pfn & (1UL << 63)) {        /* valid PFN */
            pfn &= ((1UL << 55) - 1);   /* clear flag bits */
            pfn |= (1UL << 55);
            lseek64(pageflags, pfn * sizeof(uint64_t), SEEK_SET);
            read(pageflags, &val, sizeof(val));
        }
    
    On ARM64 this causes the userspace process to crash with SIGSEGV rather
    than reading (1 << KPF_NOPAGE).  kpageflags_read() treats the offset as
    valid, and stable_page_flags() will try to access an address between the
    user and kernel address ranges.
    
    Fixes: c1cc1552616d ("arm64: MMU initialisation")
    Cc: stable@vger.kernel.org
    Signed-off-by: Greg Hackmann <ghackmann@google.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 325cfb3b858a..811f9f8b3bb0 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -287,7 +287,11 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 #ifdef CONFIG_HAVE_ARCH_PFN_VALID
 int pfn_valid(unsigned long pfn)
 {
-	return memblock_is_map_memory(pfn << PAGE_SHIFT);
+	phys_addr_t addr = pfn << PAGE_SHIFT;
+
+	if ((addr >> PAGE_SHIFT) != pfn)
+		return 0;
+	return memblock_is_map_memory(addr);
 }
 EXPORT_SYMBOL(pfn_valid);
 #endif

commit 7b0eb6b41a08fa1fa0d04b1c53becd62b5fbfaee
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Mon Jul 23 10:18:23 2018 -0400

    arm64: fix vmemmap BUILD_BUG_ON() triggering on !vmemmap setups
    
    Arnd reports the following arm64 randconfig build error with the PSI
    patches that add another page flag:
    
      /git/arm-soc/arch/arm64/mm/init.c: In function 'mem_init':
      /git/arm-soc/include/linux/compiler.h:357:38: error: call to
      '__compiletime_assert_618' declared with attribute error: BUILD_BUG_ON
      failed: sizeof(struct page) > (1 << STRUCT_PAGE_MAX_SHIFT)
    
    The additional page flag causes other information stored in
    page->flags to get bumped into their own struct page member:
    
      #if SECTIONS_WIDTH+ZONES_WIDTH+NODES_SHIFT+LAST_CPUPID_SHIFT <=
      BITS_PER_LONG - NR_PAGEFLAGS
      #define LAST_CPUPID_WIDTH LAST_CPUPID_SHIFT
      #else
      #define LAST_CPUPID_WIDTH 0
      #endif
    
      #if defined(CONFIG_NUMA_BALANCING) && LAST_CPUPID_WIDTH == 0
      #define LAST_CPUPID_NOT_IN_PAGE_FLAGS
      #endif
    
    which in turn causes the struct page size to exceed the size set in
    STRUCT_PAGE_MAX_SHIFT. This value is an an estimate used to size the
    VMEMMAP page array according to address space and struct page size.
    
    However, the check is performed - and triggers here - on a !VMEMMAP
    config, which consumes an additional 22 page bits for the sparse
    section id. When VMEMMAP is enabled, those bits are returned, cpupid
    doesn't need its own member, and the page passes the VMEMMAP check.
    
    Restrict that check to the situation it was meant to check: that we
    are sizing the VMEMMAP page array correctly.
    
    Says Arnd:
    
        Further experiments show that the build error already existed before,
        but was only triggered with larger values of CONFIG_NR_CPU and/or
        CONFIG_NODES_SHIFT that might be used in actual configurations but
        not in randconfig builds.
    
        With longer CPU and node masks, I could recreate the problem with
        kernels as old as linux-4.7 when arm64 NUMA support got added.
    
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Tested-by: Arnd Bergmann <arnd@arndb.de>
    Cc: stable@vger.kernel.org
    Fixes: 1a2db300348b ("arm64, numa: Add NUMA support for arm64 platforms.")
    Fixes: 3e1907d5bf5a ("arm64: mm: move vmemmap region right below the linear region")
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 325cfb3b858a..9abf8a1e7b25 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -611,11 +611,13 @@ void __init mem_init(void)
 	BUILD_BUG_ON(TASK_SIZE_32			> TASK_SIZE_64);
 #endif
 
+#ifdef CONFIG_SPARSEMEM_VMEMMAP
 	/*
 	 * Make sure we chose the upper bound of sizeof(struct page)
-	 * correctly.
+	 * correctly when sizing the VMEMMAP array.
 	 */
 	BUILD_BUG_ON(sizeof(struct page) > (1 << STRUCT_PAGE_MAX_SHIFT));
+#endif
 
 	if (PAGE_SIZE >= 16384 && get_num_physpages() <= 128) {
 		extern int sysctl_overcommit_memory;

commit d7dc899abefb4412388a5d3ec690070197d07d20
Author: Stefan Agner <stefan@agner.ch>
Date:   Thu Jun 14 15:28:02 2018 -0700

    treewide: use PHYS_ADDR_MAX to avoid type casting ULLONG_MAX
    
    With PHYS_ADDR_MAX there is now a type safe variant for all bits set.
    Make use of it.
    
    Patch created using a semantic patch as follows:
    
    // <smpl>
    @@
    typedef phys_addr_t;
    @@
    -(phys_addr_t)ULLONG_MAX
    +PHYS_ADDR_MAX
    // </smpl>
    
    Link: http://lkml.kernel.org/r/20180419214204.19322-1-stefan@agner.ch
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 1b18b4722420..325cfb3b858a 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -310,7 +310,7 @@ static void __init arm64_memory_present(void)
 }
 #endif
 
-static phys_addr_t memory_limit = (phys_addr_t)ULLONG_MAX;
+static phys_addr_t memory_limit = PHYS_ADDR_MAX;
 
 /*
  * Limit the memory size that was specified via FDT.
@@ -401,7 +401,7 @@ void __init arm64_memblock_init(void)
 	 * high up in memory, add back the kernel region that must be accessible
 	 * via the linear mapping.
 	 */
-	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
+	if (memory_limit != PHYS_ADDR_MAX) {
 		memblock_mem_limit_remove_map(memory_limit);
 		memblock_add(__pa_symbol(_text), (u64)(_end - _text));
 	}
@@ -666,7 +666,7 @@ __setup("keepinitrd", keepinitrd_setup);
  */
 static int dump_mem_limit(struct notifier_block *self, unsigned long v, void *p)
 {
-	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
+	if (memory_limit != PHYS_ADDR_MAX) {
 		pr_emerg("Memory Limit: %llu MB\n", memory_limit >> 20);
 	} else {
 		pr_emerg("Memory Limit: none\n");

commit 05c58752f9dce11e396676eb731a620541590ed0
Author: CHANDAN VN <chandan.vn@samsung.com>
Date:   Mon Apr 30 09:50:18 2018 +0530

    arm64: To remove initrd reserved area entry from memblock
    
    INITRD reserved area entry is not removed from memblock
    even though initrd reserved area is freed. After freeing
    the memory it is released from memblock. The same can be
    checked from /sys/kernel/debug/memblock/reserved.
    
    The patch makes sure that the initrd entry is removed from
    memblock when keepinitrd is not enabled.
    
    The patch only affects accounting and debugging. This does not
    fix any memory leak.
    
    Acked-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: CHANDAN VN <chandan.vn@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 9f3c47acf8ff..1b18b4722420 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -646,8 +646,10 @@ static int keep_initrd __initdata;
 
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (!keep_initrd)
+	if (!keep_initrd) {
 		free_reserved_area((void *)start, (void *)end, 0, "initrd");
+		memblock_free(__virt_to_phys(start), end - start);
+	}
 }
 
 static int __init keepinitrd_setup(char *__unused)

commit 2382dc9a3eca644147be83dd2cd0dd64dc9e3e8c
Merge: 28bc6fb9596f 04f56534786c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 31 11:32:27 2018 -0800

    Merge tag 'dma-mapping-4.16' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma mapping updates from Christoph Hellwig:
     "Except for a runtime warning fix from Christian this is all about
      consolidation of the generic no-IOMMU code, a well as the glue code
      for swiotlb.
    
      All the code is based on the x86 implementation with hooks to allow
      all architectures that aren't cache coherent to use it.
    
      The x86 conversion itself has been deferred because the x86
      maintainers were a little busy in the last months"
    
    * tag 'dma-mapping-4.16' of git://git.infradead.org/users/hch/dma-mapping: (57 commits)
      MAINTAINERS: add the iommu list for swiotlb and xen-swiotlb
      arm64: use swiotlb_alloc and swiotlb_free
      arm64: replace ZONE_DMA with ZONE_DMA32
      mips: use swiotlb_{alloc,free}
      mips/netlogic: remove swiotlb support
      tile: use generic swiotlb_ops
      tile: replace ZONE_DMA with ZONE_DMA32
      unicore32: use generic swiotlb_ops
      ia64: remove an ifdef around the content of pci-dma.c
      ia64: clean up swiotlb support
      ia64: use generic swiotlb_ops
      ia64: replace ZONE_DMA with ZONE_DMA32
      swiotlb: remove various exports
      swiotlb: refactor coherent buffer allocation
      swiotlb: refactor coherent buffer freeing
      swiotlb: wire up ->dma_supported in swiotlb_dma_ops
      swiotlb: add common swiotlb_map_ops
      swiotlb: rename swiotlb_free to swiotlb_exit
      x86: rename swiotlb_dma_ops
      powerpc: rename swiotlb_dma_ops
      ...

commit 0aebc6a440b942df6221a7765f077f02217e0114
Merge: 72906f38934a ec89ab50a03a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 13:57:43 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The main theme of this pull request is security covering variants 2
      and 3 for arm64. I expect to send additional patches next week
      covering an improved firmware interface (requires firmware changes)
      for variant 2 and way for KPTI to be disabled on unaffected CPUs
      (Cavium's ThunderX doesn't work properly with KPTI enabled because of
      a hardware erratum).
    
      Summary:
    
       - Security mitigations:
          - variant 2: invalidate the branch predictor with a call to
            secure firmware
          - variant 3: implement KPTI for arm64
    
       - 52-bit physical address support for arm64 (ARMv8.2)
    
       - arm64 support for RAS (firmware first only) and SDEI (software
         delegated exception interface; allows firmware to inject a RAS
         error into the OS)
    
       - perf support for the ARM DynamIQ Shared Unit PMU
    
       - CPUID and HWCAP bits updated for new floating point multiplication
         instructions in ARMv8.4
    
       - remove some virtual memory layout printks during boot
    
       - fix initial page table creation to cope with larger than 32M kernel
         images when 16K pages are enabled"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (104 commits)
      arm64: Fix TTBR + PAN + 52-bit PA logic in cpu_do_switch_mm
      arm64: Turn on KPTI only on CPUs that need it
      arm64: Branch predictor hardening for Cavium ThunderX2
      arm64: Run enable method for errata work arounds on late CPUs
      arm64: Move BP hardening to check_and_switch_context
      arm64: mm: ignore memory above supported physical address size
      arm64: kpti: Fix the interaction between ASID switching and software PAN
      KVM: arm64: Emulate RAS error registers and set HCR_EL2's TERR & TEA
      KVM: arm64: Handle RAS SErrors from EL2 on guest exit
      KVM: arm64: Handle RAS SErrors from EL1 on guest exit
      KVM: arm64: Save ESR_EL2 on guest SError
      KVM: arm64: Save/Restore guest DISR_EL1
      KVM: arm64: Set an impdef ESR for Virtual-SError using VSESR_EL2.
      KVM: arm/arm64: mask/unmask daif around VHE guests
      arm64: kernel: Prepare for a DISR user
      arm64: Unconditionally enable IESB on exception entry/return for firmware-first
      arm64: kernel: Survive corrected RAS errors notified by SError
      arm64: cpufeature: Detect CPU RAS Extentions
      arm64: sysreg: Move to use definitions for all the SCTLR bits
      arm64: cpufeature: __this_cpu_has_cap() shouldn't stop early
      ...

commit e9eaa8052fe71b95f4fea6072fa3e0b2cf0b620f
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Thu Jan 18 19:13:11 2018 +0000

    arm64: mm: ignore memory above supported physical address size
    
    When booting a kernel without 52-bit PA support (e.g. a kernel with 4k
    pages) on a system with 52-bit memory, the kernel will currently try to
    use the 52-bit memory and crash. Fix this by ignoring any memory higher
    than what the kernel supports.
    
    Fixes: f77d281713d4 ("arm64: enable 52-bit physical address support")
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 672094ed7e07..285745b2ca38 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -366,6 +366,9 @@ void __init arm64_memblock_init(void)
 	/* Handle linux,usable-memory-range property */
 	fdt_enforce_memory_region();
 
+	/* Remove memory above our supported physical address size */
+	memblock_remove(1ULL << PHYS_MASK_SHIFT, ULLONG_MAX);
+
 	/*
 	 * Ensure that the linear region takes up exactly half of the kernel
 	 * virtual address space. This way, we can distinguish a linear address

commit 071929dbdd865f779a89ba3f1e06ba8d17dd3743
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Dec 19 11:28:10 2017 -0800

    arm64: Stop printing the virtual memory layout
    
    Printing kernel addresses should be done in limited circumstances, mostly
    for debugging purposes. Printing out the virtual memory layout at every
    kernel bootup doesn't really fall into this category so delete the prints.
    There are other ways to get the same information.
    
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5960bef0170d..672094ed7e07 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -599,49 +599,6 @@ void __init mem_init(void)
 
 	mem_init_print_info(NULL);
 
-#define MLK(b, t) b, t, ((t) - (b)) >> 10
-#define MLM(b, t) b, t, ((t) - (b)) >> 20
-#define MLG(b, t) b, t, ((t) - (b)) >> 30
-#define MLK_ROUNDUP(b, t) b, t, DIV_ROUND_UP(((t) - (b)), SZ_1K)
-
-	pr_notice("Virtual kernel memory layout:\n");
-#ifdef CONFIG_KASAN
-	pr_notice("    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n",
-		MLG(KASAN_SHADOW_START, KASAN_SHADOW_END));
-#endif
-	pr_notice("    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n",
-		MLM(MODULES_VADDR, MODULES_END));
-	pr_notice("    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n",
-		MLG(VMALLOC_START, VMALLOC_END));
-	pr_notice("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(_text, _etext));
-	pr_notice("    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(__start_rodata, __init_begin));
-	pr_notice("      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(__init_begin, __init_end));
-	pr_notice("      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(_sdata, _edata));
-	pr_notice("       .bss : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(__bss_start, __bss_stop));
-	pr_notice("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
-		MLK(FIXADDR_START, FIXADDR_TOP));
-	pr_notice("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
-		MLM(PCI_IO_START, PCI_IO_END));
-#ifdef CONFIG_SPARSEMEM_VMEMMAP
-	pr_notice("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n",
-		MLG(VMEMMAP_START, VMEMMAP_START + VMEMMAP_SIZE));
-	pr_notice("              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
-		MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
-		    (unsigned long)virt_to_page(high_memory)));
-#endif
-	pr_notice("    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
-		MLM(__phys_to_virt(memblock_start_of_DRAM()),
-		    (unsigned long)high_memory));
-
-#undef MLK
-#undef MLM
-#undef MLK_ROUNDUP
-
 	/*
 	 * Check boundaries twice: Some fundamental inconsistencies can be
 	 * detected at build time already.

commit ad67f5a6545f7fda8ec11d7a81e325a398e1a90f
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 24 13:52:03 2017 +0100

    arm64: replace ZONE_DMA with ZONE_DMA32
    
    arm64 uses ZONE_DMA for allocations below 32-bits.  These days we
    name the zone for that ZONE_DMA32, which will allow to use the
    dma-direct and generic swiotlb code as-is, so rename it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Christian Knig <christian.koenig@amd.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 00e7b900ca41..8f03276443c9 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -217,7 +217,7 @@ static void __init reserve_elfcorehdr(void)
 }
 #endif /* CONFIG_CRASH_DUMP */
 /*
- * Return the maximum physical address for ZONE_DMA (DMA_BIT_MASK(32)). It
+ * Return the maximum physical address for ZONE_DMA32 (DMA_BIT_MASK(32)). It
  * currently assumes that for memory starting above 4G, 32-bit devices will
  * use a DMA offset.
  */
@@ -233,8 +233,8 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
 
-	if (IS_ENABLED(CONFIG_ZONE_DMA))
-		max_zone_pfns[ZONE_DMA] = PFN_DOWN(max_zone_dma_phys());
+	if (IS_ENABLED(CONFIG_ZONE_DMA32))
+		max_zone_pfns[ZONE_DMA32] = PFN_DOWN(max_zone_dma_phys());
 	max_zone_pfns[ZONE_NORMAL] = max;
 
 	free_area_init_nodes(max_zone_pfns);
@@ -251,9 +251,9 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	memset(zone_size, 0, sizeof(zone_size));
 
 	/* 4GB maximum for 32-bit only capable devices */
-#ifdef CONFIG_ZONE_DMA
+#ifdef CONFIG_ZONE_DMA32
 	max_dma = PFN_DOWN(arm64_dma_phys_limit);
-	zone_size[ZONE_DMA] = max_dma - min;
+	zone_size[ZONE_DMA32] = max_dma - min;
 #endif
 	zone_size[ZONE_NORMAL] = max - max_dma;
 
@@ -266,10 +266,10 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 		if (start >= max)
 			continue;
 
-#ifdef CONFIG_ZONE_DMA
+#ifdef CONFIG_ZONE_DMA32
 		if (start < max_dma) {
 			unsigned long dma_end = min(end, max_dma);
-			zhole_size[ZONE_DMA] -= dma_end - start;
+			zhole_size[ZONE_DMA32] -= dma_end - start;
 		}
 #endif
 		if (end > max_dma) {
@@ -467,7 +467,7 @@ void __init arm64_memblock_init(void)
 	early_init_fdt_scan_reserved_mem();
 
 	/* 4GB maximum for 32-bit only capable devices */
-	if (IS_ENABLED(CONFIG_ZONE_DMA))
+	if (IS_ENABLED(CONFIG_ZONE_DMA32))
 		arm64_dma_phys_limit = max_zone_dma_phys();
 	else
 		arm64_dma_phys_limit = PHYS_MASK + 1;

commit f24e5834a2c3f6c5f814a417f858226f0a010ade
Author: Steve Capper <steve.capper@arm.com>
Date:   Mon Dec 4 14:13:05 2017 +0000

    arm64: Initialise high_memory global variable earlier
    
    The high_memory global variable is used by
    cma_declare_contiguous(.) before it is defined.
    
    We don't notice this as we compute __pa(high_memory - 1), and it looks
    like we're processing a VA from the direct linear map.
    
    This problem becomes apparent when we flip the kernel virtual address
    space and the linear map is moved to the bottom of the kernel VA space.
    
    This patch moves the initialisation of high_memory before it used.
    
    Cc: <stable@vger.kernel.org>
    Fixes: f7426b983a6a ("mm: cma: adjust address limit to avoid hitting low/high memory boundary")
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5960bef0170d..00e7b900ca41 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -476,6 +476,8 @@ void __init arm64_memblock_init(void)
 
 	reserve_elfcorehdr();
 
+	high_memory = __va(memblock_end_of_DRAM() - 1) + 1;
+
 	dma_contiguous_reserve(arm64_dma_phys_limit);
 
 	memblock_allow_resize();
@@ -502,7 +504,6 @@ void __init bootmem_init(void)
 	sparse_init();
 	zone_sizes_init(min, max);
 
-	high_memory = __va((max << PAGE_SHIFT) - 1) + 1;
 	memblock_dump_all();
 }
 

commit e62aaeac426ab1ddbdde524797b2a7835f606d91
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:38 2017 +0900

    arm64: kdump: provide /proc/vmcore file
    
    Arch-specific functions are added to allow for implementing a crash dump
    file interface, /proc/vmcore, which can be viewed as a ELF file.
    
    A user space tool, like kexec-tools, is responsible for allocating
    a separate region for the core's ELF header within crash kdump kernel
    memory and filling it in when executing kexec_load().
    
    Then, its location will be advertised to crash dump kernel via a new
    device-tree property, "linux,elfcorehdr", and crash dump kernel preserves
    the region for later use with reserve_elfcorehdr() at boot time.
    
    On crash dump kernel, /proc/vmcore will access the primary kernel's memory
    with copy_oldmem_page(), which feeds the data page-by-page by ioremap'ing
    it since it does not reside in linear mapping on crash dump kernel.
    
    Meanwhile, elfcorehdr_read() is simple as the region is always mapped.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 89ba3cd0fe44..5960bef0170d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -39,6 +39,7 @@
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
 #include <linux/kexec.h>
+#include <linux/crash_dump.h>
 
 #include <asm/boot.h>
 #include <asm/fixmap.h>
@@ -165,6 +166,56 @@ static void __init kexec_reserve_crashkres_pages(void)
 }
 #endif /* CONFIG_KEXEC_CORE */
 
+#ifdef CONFIG_CRASH_DUMP
+static int __init early_init_dt_scan_elfcorehdr(unsigned long node,
+		const char *uname, int depth, void *data)
+{
+	const __be32 *reg;
+	int len;
+
+	if (depth != 1 || strcmp(uname, "chosen") != 0)
+		return 0;
+
+	reg = of_get_flat_dt_prop(node, "linux,elfcorehdr", &len);
+	if (!reg || (len < (dt_root_addr_cells + dt_root_size_cells)))
+		return 1;
+
+	elfcorehdr_addr = dt_mem_next_cell(dt_root_addr_cells, &reg);
+	elfcorehdr_size = dt_mem_next_cell(dt_root_size_cells, &reg);
+
+	return 1;
+}
+
+/*
+ * reserve_elfcorehdr() - reserves memory for elf core header
+ *
+ * This function reserves the memory occupied by an elf core header
+ * described in the device tree. This region contains all the
+ * information about primary kernel's core image and is used by a dump
+ * capture kernel to access the system memory on primary kernel.
+ */
+static void __init reserve_elfcorehdr(void)
+{
+	of_scan_flat_dt(early_init_dt_scan_elfcorehdr, NULL);
+
+	if (!elfcorehdr_size)
+		return;
+
+	if (memblock_is_region_reserved(elfcorehdr_addr, elfcorehdr_size)) {
+		pr_warn("elfcorehdr is overlapped\n");
+		return;
+	}
+
+	memblock_reserve(elfcorehdr_addr, elfcorehdr_size);
+
+	pr_info("Reserving %lldKB of memory at 0x%llx for elfcorehdr\n",
+		elfcorehdr_size >> 10, elfcorehdr_addr);
+}
+#else
+static void __init reserve_elfcorehdr(void)
+{
+}
+#endif /* CONFIG_CRASH_DUMP */
 /*
  * Return the maximum physical address for ZONE_DMA (DMA_BIT_MASK(32)). It
  * currently assumes that for memory starting above 4G, 32-bit devices will
@@ -423,6 +474,8 @@ void __init arm64_memblock_init(void)
 
 	reserve_crashkernel();
 
+	reserve_elfcorehdr();
+
 	dma_contiguous_reserve(arm64_dma_phys_limit);
 
 	memblock_allow_resize();

commit 254a41c0ba0573fa23272945d3fbe39efcc5d07d
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:35 2017 +0900

    arm64: hibernate: preserve kdump image around hibernation
    
    Since arch_kexec_protect_crashkres() removes a mapping for crash dump
    kernel image, the loaded data won't be preserved around hibernation.
    
    In this patch, helper functions, crash_prepare_suspend()/
    crash_post_resume(), are additionally called before/after hibernation so
    that the relevant memory segments will be mapped again and preserved just
    as the others are.
    
    In addition, to minimize the size of hibernation image, crash_is_nosave()
    is added to pfn_is_nosave() in order to recognize only the pages that hold
    loaded crash dump kernel image as saveable. Hibernation excludes any pages
    that are marked as Reserved and yet "nosave."
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 09d19207362d..89ba3cd0fe44 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -134,10 +134,35 @@ static void __init reserve_crashkernel(void)
 	crashk_res.start = crash_base;
 	crashk_res.end = crash_base + crash_size - 1;
 }
+
+static void __init kexec_reserve_crashkres_pages(void)
+{
+#ifdef CONFIG_HIBERNATION
+	phys_addr_t addr;
+	struct page *page;
+
+	if (!crashk_res.end)
+		return;
+
+	/*
+	 * To reduce the size of hibernation image, all the pages are
+	 * marked as Reserved initially.
+	 */
+	for (addr = crashk_res.start; addr < (crashk_res.end + 1);
+			addr += PAGE_SIZE) {
+		page = phys_to_page(addr);
+		SetPageReserved(page);
+	}
+#endif
+}
 #else
 static void __init reserve_crashkernel(void)
 {
 }
+
+static void __init kexec_reserve_crashkres_pages(void)
+{
+}
 #endif /* CONFIG_KEXEC_CORE */
 
 /*
@@ -517,6 +542,8 @@ void __init mem_init(void)
 	/* this will put all unused low memory onto the freelists */
 	free_all_bootmem();
 
+	kexec_reserve_crashkres_pages();
+
 	mem_init_print_info(NULL);
 
 #define MLK(b, t) b, t, ((t) - (b)) >> 10

commit 764b51ead10d5f428cb5f167bf98e336bdc23f8c
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:32 2017 +0900

    arm64: kdump: reserve memory for crash dump kernel
    
    "crashkernel=" kernel parameter specifies the size (and optionally
    the start address) of the system ram to be used by crash dump kernel.
    reserve_crashkernel() will allocate and reserve that memory at boot time
    of primary kernel.
    
    The memory range will be exposed to userspace as a resource named
    "Crash kernel" in /proc/iomem.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 290794b1a0f1..09d19207362d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -30,6 +30,7 @@
 #include <linux/gfp.h>
 #include <linux/memblock.h>
 #include <linux/sort.h>
+#include <linux/of.h>
 #include <linux/of_fdt.h>
 #include <linux/dma-mapping.h>
 #include <linux/dma-contiguous.h>
@@ -37,6 +38,7 @@
 #include <linux/swiotlb.h>
 #include <linux/vmalloc.h>
 #include <linux/mm.h>
+#include <linux/kexec.h>
 
 #include <asm/boot.h>
 #include <asm/fixmap.h>
@@ -77,6 +79,67 @@ static int __init early_initrd(char *p)
 early_param("initrd", early_initrd);
 #endif
 
+#ifdef CONFIG_KEXEC_CORE
+/*
+ * reserve_crashkernel() - reserves memory for crash kernel
+ *
+ * This function reserves memory area given in "crashkernel=" kernel command
+ * line parameter. The memory reserved is used by dump capture kernel when
+ * primary kernel is crashing.
+ */
+static void __init reserve_crashkernel(void)
+{
+	unsigned long long crash_base, crash_size;
+	int ret;
+
+	ret = parse_crashkernel(boot_command_line, memblock_phys_mem_size(),
+				&crash_size, &crash_base);
+	/* no crashkernel= or invalid value specified */
+	if (ret || !crash_size)
+		return;
+
+	crash_size = PAGE_ALIGN(crash_size);
+
+	if (crash_base == 0) {
+		/* Current arm64 boot protocol requires 2MB alignment */
+		crash_base = memblock_find_in_range(0, ARCH_LOW_ADDRESS_LIMIT,
+				crash_size, SZ_2M);
+		if (crash_base == 0) {
+			pr_warn("cannot allocate crashkernel (size:0x%llx)\n",
+				crash_size);
+			return;
+		}
+	} else {
+		/* User specifies base address explicitly. */
+		if (!memblock_is_region_memory(crash_base, crash_size)) {
+			pr_warn("cannot reserve crashkernel: region is not memory\n");
+			return;
+		}
+
+		if (memblock_is_region_reserved(crash_base, crash_size)) {
+			pr_warn("cannot reserve crashkernel: region overlaps reserved memory\n");
+			return;
+		}
+
+		if (!IS_ALIGNED(crash_base, SZ_2M)) {
+			pr_warn("cannot reserve crashkernel: base address is not 2MB aligned\n");
+			return;
+		}
+	}
+	memblock_reserve(crash_base, crash_size);
+
+	pr_info("crashkernel reserved: 0x%016llx - 0x%016llx (%lld MB)\n",
+		crash_base, crash_base + crash_size, crash_size >> 20);
+
+	crashk_res.start = crash_base;
+	crashk_res.end = crash_base + crash_size - 1;
+}
+#else
+static void __init reserve_crashkernel(void)
+{
+}
+#endif /* CONFIG_KEXEC_CORE */
+
 /*
  * Return the maximum physical address for ZONE_DMA (DMA_BIT_MASK(32)). It
  * currently assumes that for memory starting above 4G, 32-bit devices will
@@ -332,6 +395,9 @@ void __init arm64_memblock_init(void)
 		arm64_dma_phys_limit = max_zone_dma_phys();
 	else
 		arm64_dma_phys_limit = PHYS_MASK + 1;
+
+	reserve_crashkernel();
+
 	dma_contiguous_reserve(arm64_dma_phys_limit);
 
 	memblock_allow_resize();

commit 8f579b1c4e347b23bfa747bc2cc0a55dd1b7e5fa
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:31 2017 +0900

    arm64: limit memory regions based on DT property, usable-memory-range
    
    Crash dump kernel uses only a limited range of available memory as System
    RAM. On arm64 kdump, This memory range is advertised to crash dump kernel
    via a device-tree property under /chosen,
       linux,usable-memory-range = <BASE SIZE>
    
    Crash dump kernel reads this property at boot time and calls
    memblock_cap_memory_range() to limit usable memory which are listed either
    in UEFI memory map table or "memory" nodes of a device tree blob.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: Geoff Levand <geoff@infradead.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index e19e06593e37..290794b1a0f1 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -188,10 +188,45 @@ static int __init early_mem(char *p)
 }
 early_param("mem", early_mem);
 
+static int __init early_init_dt_scan_usablemem(unsigned long node,
+		const char *uname, int depth, void *data)
+{
+	struct memblock_region *usablemem = data;
+	const __be32 *reg;
+	int len;
+
+	if (depth != 1 || strcmp(uname, "chosen") != 0)
+		return 0;
+
+	reg = of_get_flat_dt_prop(node, "linux,usable-memory-range", &len);
+	if (!reg || (len < (dt_root_addr_cells + dt_root_size_cells)))
+		return 1;
+
+	usablemem->base = dt_mem_next_cell(dt_root_addr_cells, &reg);
+	usablemem->size = dt_mem_next_cell(dt_root_size_cells, &reg);
+
+	return 1;
+}
+
+static void __init fdt_enforce_memory_region(void)
+{
+	struct memblock_region reg = {
+		.size = 0,
+	};
+
+	of_scan_flat_dt(early_init_dt_scan_usablemem, &reg);
+
+	if (reg.size)
+		memblock_cap_memory_range(reg.base, reg.size);
+}
+
 void __init arm64_memblock_init(void)
 {
 	const s64 linear_region_size = -(s64)PAGE_OFFSET;
 
+	/* Handle linux,usable-memory-range property */
+	fdt_enforce_memory_region();
+
 	/*
 	 * Ensure that the linear region takes up exactly half of the kernel
 	 * virtual address space. This way, we can distinguish a linear address

commit ca78d3173cff3503bcd15723b049757f75762d15
Merge: a4ee7bacd6c0 ffe7afd17135
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:46:44 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     - Errata workarounds for Qualcomm's Falkor CPU
     - Qualcomm L2 Cache PMU driver
     - Qualcomm SMCCC firmware quirk
     - Support for DEBUG_VIRTUAL
     - CPU feature detection for userspace via MRS emulation
     - Preliminary work for the Statistical Profiling Extension
     - Misc cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (74 commits)
      arm64/kprobes: consistently handle MRS/MSR with XZR
      arm64: cpufeature: correctly handle MRS to XZR
      arm64: traps: correctly handle MRS/MSR with XZR
      arm64: ptrace: add XZR-safe regs accessors
      arm64: include asm/assembler.h in entry-ftrace.S
      arm64: fix warning about swapper_pg_dir overflow
      arm64: Work around Falkor erratum 1003
      arm64: head.S: Enable EL1 (host) access to SPE when entered at EL2
      arm64: arch_timer: document Hisilicon erratum 161010101
      arm64: use is_vmalloc_addr
      arm64: use linux/sizes.h for constants
      arm64: uaccess: consistently check object sizes
      perf: add qcom l2 cache perf events driver
      arm64: remove wrong CONFIG_PROC_SYSCTL ifdef
      ARM: smccc: Update HVC comment to describe new quirk parameter
      arm64: do not trace atomic operations
      ACPI/IORT: Fix the error return code in iort_add_smmu_platform_device()
      ACPI/IORT: Fix iort_node_get_id() mapping entries indexing
      arm64: mm: enable CONFIG_HOLES_IN_ZONE for NUMA
      perf: xgene: Include module.h
      ...

commit 524dabe1c68e0bca25ce7b108099e5d89472a101
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jan 16 12:46:33 2017 +0100

    arm64: Fix swiotlb fallback allocation
    
    Commit b67a8b29df introduced logic to skip swiotlb allocation when all memory
    is DMA accessible anyway.
    
    While this is a great idea, __dma_alloc still calls swiotlb code unconditionally
    to allocate memory when there is no CMA memory available. The swiotlb code is
    called to ensure that we at least try get_free_pages().
    
    Without initialization, swiotlb allocation code tries to access io_tlb_list
    which is NULL. That results in a stack trace like this:
    
      Unable to handle kernel NULL pointer dereference at virtual address 00000000
      [...]
      [<ffff00000845b908>] swiotlb_tbl_map_single+0xd0/0x2b0
      [<ffff00000845be94>] swiotlb_alloc_coherent+0x10c/0x198
      [<ffff000008099dc0>] __dma_alloc+0x68/0x1a8
      [<ffff000000a1b410>] drm_gem_cma_create+0x98/0x108 [drm]
      [<ffff000000abcaac>] drm_fbdev_cma_create_with_funcs+0xbc/0x368 [drm_kms_helper]
      [<ffff000000abcd84>] drm_fbdev_cma_create+0x2c/0x40 [drm_kms_helper]
      [<ffff000000abc040>] drm_fb_helper_initial_config+0x238/0x410 [drm_kms_helper]
      [<ffff000000abce88>] drm_fbdev_cma_init_with_funcs+0x98/0x160 [drm_kms_helper]
      [<ffff000000abcf90>] drm_fbdev_cma_init+0x40/0x58 [drm_kms_helper]
      [<ffff000000b47980>] vc4_kms_load+0x90/0xf0 [vc4]
      [<ffff000000b46a94>] vc4_drm_bind+0xec/0x168 [vc4]
      [...]
    
    Thankfully swiotlb code just learned how to not do allocations with the FORCE_NO
    option. This patch configures the swiotlb code to use that if we decide not to
    initialize the swiotlb framework.
    
    Fixes: b67a8b29df ("arm64: mm: only initialize swiotlb when necessary")
    Signed-off-by: Alexander Graf <agraf@suse.de>
    CC: Jisheng Zhang <jszhang@marvell.com>
    CC: Geert Uytterhoeven <geert+renesas@glider.be>
    CC: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 716d1226ba69..380ebe705093 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -404,6 +404,8 @@ void __init mem_init(void)
 	if (swiotlb_force == SWIOTLB_FORCE ||
 	    max_pfn > (arm64_dma_phys_limit >> PAGE_SHIFT))
 		swiotlb_init(1);
+	else
+		swiotlb_force = SWIOTLB_NO_FORCE;
 
 	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);
 

commit 2077be6783b5936c3daa838d8addbb635667927f
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:49 2017 -0800

    arm64: Use __pa_symbol for kernel symbols
    
    __pa_symbol is technically the marcro that should be used for kernel
    symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which
    will do bounds checking.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 716d1226ba69..8a2713018f2f 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -36,6 +36,7 @@
 #include <linux/efi.h>
 #include <linux/swiotlb.h>
 #include <linux/vmalloc.h>
+#include <linux/mm.h>
 
 #include <asm/boot.h>
 #include <asm/fixmap.h>
@@ -209,8 +210,8 @@ void __init arm64_memblock_init(void)
 	 * linear mapping. Take care not to clip the kernel which may be
 	 * high in memory.
 	 */
-	memblock_remove(max_t(u64, memstart_addr + linear_region_size, __pa(_end)),
-			ULLONG_MAX);
+	memblock_remove(max_t(u64, memstart_addr + linear_region_size,
+			__pa_symbol(_end)), ULLONG_MAX);
 	if (memstart_addr + linear_region_size < memblock_end_of_DRAM()) {
 		/* ensure that memstart_addr remains sufficiently aligned */
 		memstart_addr = round_up(memblock_end_of_DRAM() - linear_region_size,
@@ -225,7 +226,7 @@ void __init arm64_memblock_init(void)
 	 */
 	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
 		memblock_mem_limit_remove_map(memory_limit);
-		memblock_add(__pa(_text), (u64)(_end - _text));
+		memblock_add(__pa_symbol(_text), (u64)(_end - _text));
 	}
 
 	if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && initrd_start) {
@@ -278,7 +279,7 @@ void __init arm64_memblock_init(void)
 	 * Register the kernel text, kernel data, initrd, and initial
 	 * pagetables with memblock.
 	 */
-	memblock_reserve(__pa(_text), _end - _text);
+	memblock_reserve(__pa_symbol(_text), _end - _text);
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start) {
 		memblock_reserve(initrd_start, initrd_end - initrd_start);
@@ -484,7 +485,8 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
-	free_reserved_area(__va(__pa(__init_begin)), __va(__pa(__init_end)),
+	free_reserved_area(lm_alias(__init_begin),
+			   lm_alias(__init_end),
 			   0, "unused kernel");
 	/*
 	 * Unmap the __init region but leave the VM area in place. This

commit ae7871be189cb41184f1e05742b4a99e2c59774d
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Fri Dec 16 14:28:41 2016 +0100

    swiotlb: Convert swiotlb_force from int to enum
    
    Convert the flag swiotlb_force from an int to an enum, to prepare for
    the advent of more possible values.
    
    Suggested-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 212c4d1e2f26..716d1226ba69 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -401,7 +401,8 @@ static void __init free_unused_memmap(void)
  */
 void __init mem_init(void)
 {
-	if (swiotlb_force || max_pfn > (arm64_dma_phys_limit >> PAGE_SHIFT))
+	if (swiotlb_force == SWIOTLB_FORCE ||
+	    max_pfn > (arm64_dma_phys_limit >> PAGE_SHIFT))
 		swiotlb_init(1);
 
 	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);

commit f7881bd644474a4a62d7bd1ec801176f635f59ae
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 20 12:24:53 2016 +0100

    arm64: remove pr_cont abuse from mem_init
    
    All the lines printed by mem_init are independent, with each ending with
    a newline. While they logically form a large block, none are actually
    continuations of previous lines.
    
    The kernel-side printk code and the userspace demsg tool differ in their
    handling of KERN_CONT following a newline, and while this isn't always a
    problem kernel-side, it does cause difficulty for userspace. Using
    pr_cont causes the userspace tool to not print line prefix (e.g.
    timestamps) even when following a newline, mis-aligning the output and
    making it harder to read, e.g.
    
    [    0.000000] Virtual kernel memory layout:
    [    0.000000]     modules : 0xffff000000000000 - 0xffff000008000000   (   128 MB)
        vmalloc : 0xffff000008000000 - 0xffff7dffbfff0000   (129022 GB)
          .text : 0xffff000008080000 - 0xffff0000088b0000   (  8384 KB)
        .rodata : 0xffff0000088b0000 - 0xffff000008c50000   (  3712 KB)
          .init : 0xffff000008c50000 - 0xffff000008d50000   (  1024 KB)
          .data : 0xffff000008d50000 - 0xffff000008e25200   (   853 KB)
           .bss : 0xffff000008e25200 - 0xffff000008e6bec0   (   284 KB)
        fixed   : 0xffff7dfffe7fd000 - 0xffff7dfffec00000   (  4108 KB)
        PCI I/O : 0xffff7dfffee00000 - 0xffff7dffffe00000   (    16 MB)
        vmemmap : 0xffff7e0000000000 - 0xffff800000000000   (  2048 GB maximum)
                  0xffff7e0000000000 - 0xffff7e0026000000   (   608 MB actual)
        memory  : 0xffff800000000000 - 0xffff800980000000   ( 38912 MB)
    [    0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=6, Nodes=1
    
    Fix this by using pr_notice consistently for all lines, which both the
    kernel and userspace are happy with.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kefeng Wang <wangkefeng.wang@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 21c489bdeb4e..212c4d1e2f26 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -421,35 +421,35 @@ void __init mem_init(void)
 
 	pr_notice("Virtual kernel memory layout:\n");
 #ifdef CONFIG_KASAN
-	pr_cont("    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n",
+	pr_notice("    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n",
 		MLG(KASAN_SHADOW_START, KASAN_SHADOW_END));
 #endif
-	pr_cont("    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+	pr_notice("    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n",
 		MLM(MODULES_VADDR, MODULES_END));
-	pr_cont("    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n",
+	pr_notice("    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n",
 		MLG(VMALLOC_START, VMALLOC_END));
-	pr_cont("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+	pr_notice("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(_text, _etext));
-	pr_cont("    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+	pr_notice("    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(__start_rodata, __init_begin));
-	pr_cont("      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+	pr_notice("      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(__init_begin, __init_end));
-	pr_cont("      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+	pr_notice("      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(_sdata, _edata));
-	pr_cont("       .bss : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+	pr_notice("       .bss : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(__bss_start, __bss_stop));
-	pr_cont("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
+	pr_notice("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
 		MLK(FIXADDR_START, FIXADDR_TOP));
-	pr_cont("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+	pr_notice("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
 		MLM(PCI_IO_START, PCI_IO_END));
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-	pr_cont("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n",
+	pr_notice("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n",
 		MLG(VMEMMAP_START, VMEMMAP_START + VMEMMAP_SIZE));
-	pr_cont("              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
+	pr_notice("              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
 		MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
 		    (unsigned long)virt_to_page(high_memory)));
 #endif
-	pr_cont("    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+	pr_notice("    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
 		MLM(__phys_to_virt(memblock_start_of_DRAM()),
 		    (unsigned long)high_memory));
 

commit dae8c235d9a21a564793ea9fe716233e11d30e21
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Mon Sep 5 19:30:22 2016 +0800

    arm64: mm: drop fixup_init() and mm.h
    
    There is only fixup_init() in mm.h , and it is only called
    in free_initmem(), so move the codes from fixup_init() into
    free_initmem(), then drop fixup_init() and mm.h.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 251e0824cd82..21c489bdeb4e 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -35,6 +35,7 @@
 #include <linux/dma-contiguous.h>
 #include <linux/efi.h>
 #include <linux/swiotlb.h>
+#include <linux/vmalloc.h>
 
 #include <asm/boot.h>
 #include <asm/fixmap.h>
@@ -48,8 +49,6 @@
 #include <asm/tlb.h>
 #include <asm/alternative.h>
 
-#include "mm.h"
-
 /*
  * We need to be able to catch inadvertent references to memstart_addr
  * that occur (potentially in generic code) before arm64_memblock_init()
@@ -486,7 +485,12 @@ void free_initmem(void)
 {
 	free_reserved_area(__va(__pa(__init_begin)), __va(__pa(__init_end)),
 			   0, "unused kernel");
-	fixup_init();
+	/*
+	 * Unmap the __init region but leave the VM area in place. This
+	 * prevents the region from being reused for kernel modules, which
+	 * is not supported by kallsyms.
+	 */
+	unmap_kernel_range((u64)__init_begin, (u64)(__init_end - __init_begin));
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD

commit 5a9e3e156ec1ab26ba70b4c44157858c92bbeee0
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Mon Aug 15 14:45:46 2016 +0800

    arm64: apply __ro_after_init to some objects
    
    These objects are set during initialization, thereafter are read only.
    
    Previously I only want to mark vdso_pages, vdso_spec, vectors_page and
    cpu_ops as __read_mostly from performance point of view. Then inspired
    by Kees's patch[1] to apply more __ro_after_init for arm, I think it's
    better to mark them as __ro_after_init. What's more, I find some more
    objects are also read only after init. So apply __ro_after_init to all
    of them.
    
    This patch also removes global vdso_pagelist and tries to clean up
    vdso_spec[] assignment code.
    
    [1] http://www.spinics.net/lists/arm-kernel/msg523188.html
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index bbb7ee76e319..251e0824cd82 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -23,6 +23,7 @@
 #include <linux/swap.h>
 #include <linux/init.h>
 #include <linux/bootmem.h>
+#include <linux/cache.h>
 #include <linux/mman.h>
 #include <linux/nodemask.h>
 #include <linux/initrd.h>
@@ -55,8 +56,8 @@
  * executes, which assigns it its actual value. So use a default value
  * that cannot be mistaken for a real physical address.
  */
-s64 memstart_addr __read_mostly = -1;
-phys_addr_t arm64_dma_phys_limit __read_mostly;
+s64 memstart_addr __ro_after_init = -1;
+phys_addr_t arm64_dma_phys_limit __ro_after_init;
 
 #ifdef CONFIG_BLK_DEV_INITRD
 static int __init early_initrd(char *p)

commit cb0a650213b738dbd04951c7f1f6e95b65012758
Author: Dennis Chen <dennis.chen@arm.com>
Date:   Thu Jul 28 15:48:29 2016 -0700

    arm64:acpi: fix the acpi alignment exception when 'mem=' specified
    
    When booting an ACPI enabled kernel with 'mem=x', there is the
    possibility that ACPI data regions from the firmware will lie above the
    memory limit.  Ordinarily these will be removed by
    memblock_enforce_memory_limit(.).
    
    Unfortunately, this means that these regions will then be mapped by
    acpi_os_ioremap(.) as device memory (instead of normal) thus unaligned
    accessess will then provoke alignment faults.
    
    In this patch we adopt memblock_mem_limit_remove_map instead, and this
    preserves these ACPI data regions (marked NOMAP) thus ensuring that
    these regions are not mapped as device memory.
    
    For example, below is an alignment exception observed on ARM platform
    when booting the kernel with 'acpi=on mem=8G':
    
      ...
      Unable to handle kernel paging request at virtual address ffff0000080521e7
      pgd = ffff000008aa0000
      [ffff0000080521e7] *pgd=000000801fffe003, *pud=000000801fffd003, *pmd=000000801fffc003, *pte=00e80083ff1c1707
      Internal error: Oops: 96000021 [#1] PREEMPT SMP
      Modules linked in:
      CPU: 1 PID: 1 Comm: swapper/0 Not tainted 4.7.0-rc3-next-20160616+ #172
      Hardware name: AMD Overdrive/Supercharger/Default string, BIOS ROD1001A 02/09/2016
      task: ffff800001ef0000 ti: ffff800001ef8000 task.ti: ffff800001ef8000
      PC is at acpi_ns_lookup+0x520/0x734
      LR is at acpi_ns_lookup+0x4a4/0x734
      pc : [<ffff0000083b8b10>] lr : [<ffff0000083b8a94>] pstate: 60000045
      sp : ffff800001efb8b0
      x29: ffff800001efb8c0 x28: 000000000000001b
      x27: 0000000000000001 x26: 0000000000000000
      x25: ffff800001efb9e8 x24: ffff000008a10000
      x23: 0000000000000001 x22: 0000000000000001
      x21: ffff000008724000 x20: 000000000000001b
      x19: ffff0000080521e7 x18: 000000000000000d
      x17: 00000000000038ff x16: 0000000000000002
      x15: 0000000000000007 x14: 0000000000007fff
      x13: ffffff0000000000 x12: 0000000000000018
      x11: 000000001fffd200 x10: 00000000ffffff76
      x9 : 000000000000005f x8 : ffff000008725fa8
      x7 : ffff000008a8df70 x6 : ffff000008a8df70
      x5 : ffff000008a8d000 x4 : 0000000000000010
      x3 : 0000000000000010 x2 : 000000000000000c
      x1 : 0000000000000006 x0 : 0000000000000000
      ...
        acpi_ns_lookup+0x520/0x734
        acpi_ds_load1_begin_op+0x174/0x4fc
        acpi_ps_build_named_op+0xf8/0x220
        acpi_ps_create_op+0x208/0x33c
        acpi_ps_parse_loop+0x204/0x838
        acpi_ps_parse_aml+0x1bc/0x42c
        acpi_ns_one_complete_parse+0x1e8/0x22c
        acpi_ns_parse_table+0x8c/0x128
        acpi_ns_load_table+0xc0/0x1e8
        acpi_tb_load_namespace+0xf8/0x2e8
        acpi_load_tables+0x7c/0x110
        acpi_init+0x90/0x2c0
        do_one_initcall+0x38/0x12c
        kernel_init_freeable+0x148/0x1ec
        kernel_init+0x10/0xec
        ret_from_fork+0x10/0x40
      Code: b9009fbc 2a00037b 36380057 3219037b (b9400260)
      ---[ end trace 03381e5eb0a24de4 ]---
      Kernel panic - not syncing: Attempted to kill init! exitcode=0x0000000b
    
    With 'efi=debug', we can see those ACPI regions loaded by firmware on
    that board as:
    
      efi:   0x0083ff185000-0x0083ff1b4fff [Reserved           |   |  |  |  |  |  |  |   |WB|WT|WC|UC]*
      efi:   0x0083ff1b5000-0x0083ff1c2fff [ACPI Reclaim Memory|   |  |  |  |  |  |  |   |WB|WT|WC|UC]*
      efi:   0x0083ff223000-0x0083ff224fff [ACPI Memory NVS    |   |  |  |  |  |  |  |   |WB|WT|WC|UC]*
    
    Link: http://lkml.kernel.org/r/1468475036-5852-3-git-send-email-dennis.chen@arm.com
    Acked-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Dennis Chen <dennis.chen@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Rafael J. Wysocki <rafael@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Kaly Xin <kaly.xin@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 2ade7a6a10a7..bbb7ee76e319 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -224,7 +224,7 @@ void __init arm64_memblock_init(void)
 	 * via the linear mapping.
 	 */
 	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
-		memblock_enforce_memory_limit(memory_limit);
+		memblock_mem_limit_remove_map(memory_limit);
 		memblock_add(__pa(_text), (u64)(_end - _text));
 	}
 

commit 9fdc14c55cd6579d619ccd9d40982e0805e62b6d
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Jun 23 15:53:17 2016 +0200

    arm64: mm: fix location of _etext
    
    As Kees Cook notes in the ARM counterpart of this patch [0]:
    
      The _etext position is defined to be the end of the kernel text code,
      and should not include any part of the data segments. This interferes
      with things that might check memory ranges and expect executable code
      up to _etext.
    
    In particular, Kees is referring to the HARDENED_USERCOPY patch set [1],
    which rejects attempts to call copy_to_user() on kernel ranges containing
    executable code, but does allow access to the .rodata segment. Regardless
    of whether one may or may not agree with the distinction, it makes sense
    for _etext to have the same meaning across architectures.
    
    So let's put _etext where it belongs, between .text and .rodata, and fix
    up existing references to use __init_begin instead, which unlike _end_rodata
    includes the exception and notes sections as well.
    
    The _etext references in kaslr.c are left untouched, since its references
    to [_stext, _etext) are meant to capture potential jump instruction targets,
    and so disregarding .rodata is actually an improvement here.
    
    [0] http://article.gmane.org/gmane.linux.kernel/2245084
    [1] http://thread.gmane.org/gmane.linux.kernel.hardened.devel/2502
    
    Reported-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 64ea28306661..2ade7a6a10a7 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -429,9 +429,9 @@ void __init mem_init(void)
 	pr_cont("    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n",
 		MLG(VMALLOC_START, VMALLOC_END));
 	pr_cont("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(_text, __start_rodata));
+		MLK_ROUNDUP(_text, _etext));
 	pr_cont("    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(__start_rodata, _etext));
+		MLK_ROUNDUP(__start_rodata, __init_begin));
 	pr_cont("      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(__init_begin, __init_end));
 	pr_cont("      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",

commit ea2cbee3bc671390139802dd0d50b08db024b03c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jun 22 12:13:45 2016 +0100

    arm64: mm: simplify memblock numa node extraction
    
    We currently open-code extracting the NUMA node of a memblock region,
    which requires an ifdef to cater for !CONFIG_NUMA builds where the
    memblock_region::nid field does not exist.
    
    The generic memblock_get_region_node helper is intended to cater for
    this. For CONFIG_HAVE_MEMBLOCK_NODE_MAP, builds this returns reg->nid,
    and for for !CONFIG_HAVE_MEMBLOCK_NODE_MAP builds this is a static
    inline that returns 0. Note that for arm64,
    CONFIG_HAVE_MEMBLOCK_NODE_MAP is selected iff CONFIG_NUMA is.
    
    This patch makes use of memblock_get_region_node to simplify the arm64
    code. At the same time, we can move the nid variable definition into the
    loop, as this is the only place it is used.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 7d25b4d00677..64ea28306661 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -160,12 +160,10 @@ static void __init arm64_memory_present(void)
 static void __init arm64_memory_present(void)
 {
 	struct memblock_region *reg;
-	int nid = 0;
 
 	for_each_memblock(memory, reg) {
-#ifdef CONFIG_NUMA
-		nid = reg->nid;
-#endif
+		int nid = memblock_get_region_node(reg);
+
 		memory_present(nid, memblock_region_memory_base_pfn(reg),
 				memblock_region_memory_end_pfn(reg));
 	}

commit b67a8b29df7e6410c605c2759707c96512b15578
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Wed Jun 8 15:53:46 2016 +0800

    arm64: mm: only initialize swiotlb when necessary
    
    we only initialize swiotlb when swiotlb_force is true or not all system
    memory is DMA-able, this trivial optimization saves us 64MB when
    swiotlb is not necessary.
    
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d45f8627012c..7d25b4d00677 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -403,7 +403,8 @@ static void __init free_unused_memmap(void)
  */
 void __init mem_init(void)
 {
-	swiotlb_init(1);
+	if (swiotlb_force || max_pfn > (arm64_dma_phys_limit >> PAGE_SHIFT))
+		swiotlb_init(1);
 
 	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);
 

commit 9974723e31d1fa99e3c0efb2ae6cbbf089c0080c
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Mon Apr 18 11:09:47 2016 +0800

    arm64: mm: Show bss segment in kernel memory layout
    
    Show the bss segment information as with text and data in Virtual
    memory kernel layout.
    
    Acked-by: James Morse <james.morse@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f6ff5b5df9f8..d45f8627012c 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -437,6 +437,8 @@ void __init mem_init(void)
 		MLK_ROUNDUP(__init_begin, __init_end));
 	pr_cont("      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(_sdata, _edata));
+	pr_cont("       .bss : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		MLK_ROUNDUP(__bss_start, __bss_stop));
 	pr_cont("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
 		MLK(FIXADDR_START, FIXADDR_TOP));
 	pr_cont("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",

commit d32351c8242b7067b3f3e3a6caf7a387ff43f978
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Mon Apr 18 11:09:46 2016 +0800

    arm64: mm: make pr_cont() per line in Virtual kernel memory layout
    
    Each line with single pr_cont() in Virtual kernel memory layout,
    or the dump of the kernel memory layout in dmesg is not aligned
    when PRINTK_TIME enabled, due to the missing time stamps.
    
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f64dbd49eca3..f6ff5b5df9f8 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -429,23 +429,22 @@ void __init mem_init(void)
 		MLM(MODULES_VADDR, MODULES_END));
 	pr_cont("    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n",
 		MLG(VMALLOC_START, VMALLOC_END));
-	pr_cont("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		"    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		"      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		"      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
-		MLK_ROUNDUP(_text, __start_rodata),
-		MLK_ROUNDUP(__start_rodata, _etext),
-		MLK_ROUNDUP(__init_begin, __init_end),
+	pr_cont("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		MLK_ROUNDUP(_text, __start_rodata));
+	pr_cont("    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		MLK_ROUNDUP(__start_rodata, _etext));
+	pr_cont("      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		MLK_ROUNDUP(__init_begin, __init_end));
+	pr_cont("      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
 		MLK_ROUNDUP(_sdata, _edata));
 	pr_cont("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
 		MLK(FIXADDR_START, FIXADDR_TOP));
 	pr_cont("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
 		MLM(PCI_IO_START, PCI_IO_END));
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-	pr_cont("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
-		"              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
-		MLG(VMEMMAP_START,
-		    VMEMMAP_START + VMEMMAP_SIZE),
+	pr_cont("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n",
+		MLG(VMEMMAP_START, VMEMMAP_START + VMEMMAP_SIZE));
+	pr_cont("              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
 		MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
 		    (unsigned long)virt_to_page(high_memory)));
 #endif

commit 1a2db300348b799479d2d22b84d51b27ad0458c7
Author: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
Date:   Fri Apr 8 15:50:27 2016 -0700

    arm64, numa: Add NUMA support for arm64 platforms.
    
    Attempt to get the memory and CPU NUMA node via of_numa.  If that
    fails, default the dummy NUMA node and map all memory and CPUs to node
    0.
    
    Tested-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Robert Richter <rrichter@cavium.com>
    Signed-off-by: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d55d720dba79..f64dbd49eca3 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -40,6 +40,7 @@
 #include <asm/kasan.h>
 #include <asm/kernel-pgtable.h>
 #include <asm/memory.h>
+#include <asm/numa.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>
@@ -86,6 +87,21 @@ static phys_addr_t __init max_zone_dma_phys(void)
 	return min(offset + (1ULL << 32), memblock_end_of_DRAM());
 }
 
+#ifdef CONFIG_NUMA
+
+static void __init zone_sizes_init(unsigned long min, unsigned long max)
+{
+	unsigned long max_zone_pfns[MAX_NR_ZONES]  = {0};
+
+	if (IS_ENABLED(CONFIG_ZONE_DMA))
+		max_zone_pfns[ZONE_DMA] = PFN_DOWN(max_zone_dma_phys());
+	max_zone_pfns[ZONE_NORMAL] = max;
+
+	free_area_init_nodes(max_zone_pfns);
+}
+
+#else
+
 static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	struct memblock_region *reg;
@@ -126,6 +142,8 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	free_area_init_node(0, zone_size, min, zhole_size);
 }
 
+#endif /* CONFIG_NUMA */
+
 #ifdef CONFIG_HAVE_ARCH_PFN_VALID
 int pfn_valid(unsigned long pfn)
 {
@@ -142,10 +160,15 @@ static void __init arm64_memory_present(void)
 static void __init arm64_memory_present(void)
 {
 	struct memblock_region *reg;
+	int nid = 0;
 
-	for_each_memblock(memory, reg)
-		memory_present(0, memblock_region_memory_base_pfn(reg),
-			       memblock_region_memory_end_pfn(reg));
+	for_each_memblock(memory, reg) {
+#ifdef CONFIG_NUMA
+		nid = reg->nid;
+#endif
+		memory_present(nid, memblock_region_memory_base_pfn(reg),
+				memblock_region_memory_end_pfn(reg));
+	}
 }
 #endif
 
@@ -278,7 +301,6 @@ void __init arm64_memblock_init(void)
 	dma_contiguous_reserve(arm64_dma_phys_limit);
 
 	memblock_allow_resize();
-	memblock_dump_all();
 }
 
 void __init bootmem_init(void)
@@ -290,6 +312,9 @@ void __init bootmem_init(void)
 
 	early_memtest(min << PAGE_SHIFT, max << PAGE_SHIFT);
 
+	max_pfn = max_low_pfn = max;
+
+	arm64_numa_init();
 	/*
 	 * Sparsemem tries to allocate bootmem in memory_present(), so must be
 	 * done after the fixed reservations.
@@ -300,7 +325,7 @@ void __init bootmem_init(void)
 	zone_sizes_init(min, max);
 
 	high_memory = __va((max << PAGE_SHIFT) - 1) + 1;
-	max_pfn = max_low_pfn = max;
+	memblock_dump_all();
 }
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP

commit 3e1907d5bf5a1e0b182ee599f92586f0165029e2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 16:46:00 2016 +0200

    arm64: mm: move vmemmap region right below the linear region
    
    This moves the vmemmap region right below PAGE_OFFSET, aka the start
    of the linear region, and redefines its size to be a power of two.
    Due to the placement of PAGE_OFFSET in the middle of the address space,
    whose size is a power of two as well, this guarantees that virt to
    page conversions and vice versa can be implemented efficiently, by
    masking and shifting rather than ordinary arithmetic.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 89376f3c65a3..d55d720dba79 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -412,6 +412,10 @@ void __init mem_init(void)
 		MLK_ROUNDUP(__start_rodata, _etext),
 		MLK_ROUNDUP(__init_begin, __init_end),
 		MLK_ROUNDUP(_sdata, _edata));
+	pr_cont("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
+		MLK(FIXADDR_START, FIXADDR_TOP));
+	pr_cont("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+		MLM(PCI_IO_START, PCI_IO_END));
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 	pr_cont("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
 		"              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
@@ -420,10 +424,6 @@ void __init mem_init(void)
 		MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
 		    (unsigned long)virt_to_page(high_memory)));
 #endif
-	pr_cont("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
-		MLK(FIXADDR_START, FIXADDR_TOP));
-	pr_cont("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
-		MLM(PCI_IO_START, PCI_IO_END));
 	pr_cont("    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
 		MLM(__phys_to_virt(memblock_start_of_DRAM()),
 		    (unsigned long)high_memory));
@@ -440,6 +440,12 @@ void __init mem_init(void)
 	BUILD_BUG_ON(TASK_SIZE_32			> TASK_SIZE_64);
 #endif
 
+	/*
+	 * Make sure we chose the upper bound of sizeof(struct page)
+	 * correctly.
+	 */
+	BUILD_BUG_ON(sizeof(struct page) > (1 << STRUCT_PAGE_MAX_SHIFT));
+
 	if (PAGE_SIZE >= 16384 && get_num_physpages() <= 128) {
 		extern int sysctl_overcommit_memory;
 		/*

commit d386825c959efeaae3315c25dd2d2874c68829c7
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 16:45:57 2016 +0200

    arm64: mm: free __init memory via the linear mapping
    
    The implementation of free_initmem_default() expects __init_begin
    and __init_end to be covered by the linear mapping, which is no
    longer the case. So open code it instead, using addresses that are
    explicitly translated from kernel virtual to linear virtual.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 82ced5fa1e66..89376f3c65a3 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -452,7 +452,8 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
-	free_initmem_default(0);
+	free_reserved_area(__va(__pa(__init_begin)), __va(__pa(__init_end)),
+			   0, "unused kernel");
 	fixup_init();
 }
 

commit 177e15f0c1444cd392374ec7175c4787fd911369
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 15:18:42 2016 +0200

    arm64: add the initrd region to the linear mapping explicitly
    
    Instead of going out of our way to relocate the initrd if it turns out
    to occupy memory that is not covered by the linear mapping, just add the
    initrd to the linear mapping. This puts the burden on the bootloader to
    pass initrd= and mem= options that are mutually consistent.
    
    Note that, since the placement of the linear region in the PA space is
    also dependent on the placement of the kernel Image, which may reside
    anywhere in memory, we may still end up with a situation where the initrd
    and the kernel Image are simply too far apart to be covered by the linear
    region.
    
    Since we now leave it up to the bootloader to pass the initrd in memory
    that is guaranteed to be accessible by the kernel, add a mention of this to
    the arm64 boot protocol specification as well.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 83ae8b5e5666..82ced5fa1e66 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -207,6 +207,35 @@ void __init arm64_memblock_init(void)
 		memblock_add(__pa(_text), (u64)(_end - _text));
 	}
 
+	if (IS_ENABLED(CONFIG_BLK_DEV_INITRD) && initrd_start) {
+		/*
+		 * Add back the memory we just removed if it results in the
+		 * initrd to become inaccessible via the linear mapping.
+		 * Otherwise, this is a no-op
+		 */
+		u64 base = initrd_start & PAGE_MASK;
+		u64 size = PAGE_ALIGN(initrd_end) - base;
+
+		/*
+		 * We can only add back the initrd memory if we don't end up
+		 * with more memory than we can address via the linear mapping.
+		 * It is up to the bootloader to position the kernel and the
+		 * initrd reasonably close to each other (i.e., within 32 GB of
+		 * each other) so that all granule/#levels combinations can
+		 * always access both.
+		 */
+		if (WARN(base < memblock_start_of_DRAM() ||
+			 base + size > memblock_start_of_DRAM() +
+				       linear_region_size,
+			"initrd not fully accessible via the linear mapping -- please check your bootloader ...\n")) {
+			initrd_start = 0;
+		} else {
+			memblock_remove(base, size); /* clear MEMBLOCK_ flags */
+			memblock_add(base, size);
+			memblock_reserve(base, size);
+		}
+	}
+
 	if (IS_ENABLED(CONFIG_RANDOMIZE_BASE)) {
 		extern u16 memstart_offset_seed;
 		u64 range = linear_region_size -

commit 2958987f5da2ebcf6a237c5f154d7e3340e60945
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 14:25:46 2016 +0200

    arm64/mm: ensure memstart_addr remains sufficiently aligned
    
    After choosing memstart_addr to be the highest multiple of
    ARM64_MEMSTART_ALIGN less than or equal to the first usable physical memory
    address, we clip the memblocks to the maximum size of the linear region.
    Since the kernel may be high up in memory, we take care not to clip the
    kernel itself, which means we have to clip some memory from the bottom if
    this occurs, to ensure that the distance between the first and the last
    usable physical memory address can be covered by the linear region.
    
    However, we fail to update memstart_addr if this clipping from the bottom
    occurs, which means that we may still end up with virtual addresses that
    wrap into the userland range. So increment memstart_addr as appropriate to
    prevent this from happening.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index ea989d83ea9b..83ae8b5e5666 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -190,8 +190,12 @@ void __init arm64_memblock_init(void)
 	 */
 	memblock_remove(max_t(u64, memstart_addr + linear_region_size, __pa(_end)),
 			ULLONG_MAX);
-	if (memblock_end_of_DRAM() > linear_region_size)
-		memblock_remove(0, memblock_end_of_DRAM() - linear_region_size);
+	if (memstart_addr + linear_region_size < memblock_end_of_DRAM()) {
+		/* ensure that memstart_addr remains sufficiently aligned */
+		memstart_addr = round_up(memblock_end_of_DRAM() - linear_region_size,
+					 ARM64_MEMSTART_ALIGN);
+		memblock_remove(0, memstart_addr);
+	}
 
 	/*
 	 * Apply the memory limit if it was set. Since the kernel may be loaded

commit 9d854607f9005c593dca9672b708f28e6ef96fe4
Merge: 8a20a04bda46 691b1e2ebf72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 19:13:59 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull second set of arm64 updates from Catalin Marinas:
    
     - KASLR bug fixes: use callee-saved register, boot-time I-cache
       maintenance
    
     - inv_entry asm macro fix (EL0 check typo)
    
     - pr_notice("Virtual kernel memory layout...") splitting
    
     - Clean-ups: use p?d_set_huge consistently, allow preemption around
       copy_to_user_page, remove unused __local_flush_icache_all()
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: mm: allow preemption in copy_to_user_page
      arm64: consistently use p?d_set_huge
      arm64: kaslr: use callee saved register to preserve SCTLR across C call
      arm64: Split pr_notice("Virtual kernel memory layout...") into multiple pr_cont()
      arm64: drop unused __local_flush_icache_all()
      arm64: fix KASLR boot-time I-cache maintenance
      arm64/kernel: fix incorrect EL0 check in inv_entry macro

commit f09f1bacfe2b1e64a5d94bc2711f73b654c95514
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Mar 11 17:39:25 2016 +0000

    arm64: Split pr_notice("Virtual kernel memory layout...") into multiple pr_cont()
    
    The printk() implementation has a limit of LOG_LINE_MAX (== 1024 - 32)
    buffer per call which the arm64 mem_init() breaches when printing the
    virtual memory layout with CONFIG_KASAN enabled. The result is that the
    last line is no longer printed. This patch splits the call into a
    pr_notice() + additional pr_cont() calls.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 8c3d7dd91c25..d09603d0e5e9 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -362,42 +362,38 @@ void __init mem_init(void)
 #define MLG(b, t) b, t, ((t) - (b)) >> 30
 #define MLK_ROUNDUP(b, t) b, t, DIV_ROUND_UP(((t) - (b)), SZ_1K)
 
-	pr_notice("Virtual kernel memory layout:\n"
+	pr_notice("Virtual kernel memory layout:\n");
 #ifdef CONFIG_KASAN
-		  "    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n"
+	pr_cont("    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n",
+		MLG(KASAN_SHADOW_START, KASAN_SHADOW_END));
 #endif
-		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
-		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n"
-		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		  "    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+	pr_cont("    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+		MLM(MODULES_VADDR, MODULES_END));
+	pr_cont("    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n",
+		MLG(VMALLOC_START, VMALLOC_END));
+	pr_cont("      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		"    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		"      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		"      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		MLK_ROUNDUP(_text, __start_rodata),
+		MLK_ROUNDUP(__start_rodata, _etext),
+		MLK_ROUNDUP(__init_begin, __init_end),
+		MLK_ROUNDUP(_sdata, _edata));
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
-		  "              0x%16lx - 0x%16lx   (%6ld MB actual)\n"
+	pr_cont("    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
+		"              0x%16lx - 0x%16lx   (%6ld MB actual)\n",
+		MLG((unsigned long)vmemmap,
+		    (unsigned long)vmemmap + VMEMMAP_SIZE),
+		MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
+		    (unsigned long)virt_to_page(high_memory)));
 #endif
-		  "    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n"
-		  "    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n"
-		  "    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
-#ifdef CONFIG_KASAN
-		  MLG(KASAN_SHADOW_START, KASAN_SHADOW_END),
-#endif
-		  MLM(MODULES_VADDR, MODULES_END),
-		  MLG(VMALLOC_START, VMALLOC_END),
-		  MLK_ROUNDUP(_text, __start_rodata),
-		  MLK_ROUNDUP(__start_rodata, _etext),
-		  MLK_ROUNDUP(__init_begin, __init_end),
-		  MLK_ROUNDUP(_sdata, _edata),
-#ifdef CONFIG_SPARSEMEM_VMEMMAP
-		  MLG((unsigned long)vmemmap,
-		      (unsigned long)vmemmap + VMEMMAP_SIZE),
-		  MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
-		      (unsigned long)virt_to_page(high_memory)),
-#endif
-		  MLK(FIXADDR_START, FIXADDR_TOP),
-		  MLM(PCI_IO_START, PCI_IO_END),
-		  MLM(__phys_to_virt(memblock_start_of_DRAM()),
-		      (unsigned long)high_memory));
+	pr_cont("    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n",
+		MLK(FIXADDR_START, FIXADDR_TOP));
+	pr_cont("    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+		MLM(PCI_IO_START, PCI_IO_END));
+	pr_cont("    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
+		MLM(__phys_to_virt(memblock_start_of_DRAM()),
+		    (unsigned long)high_memory));
 
 #undef MLK
 #undef MLM

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit 6d2aa549de1fc998581d216de3853aa131aa4446
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 2 09:47:13 2016 +0100

    arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
    
    Commit 8439e62a1561 ("arm64: mm: use bit ops rather than arithmetic in
    pa/va translations") changed the boundary check against PAGE_OFFSET from
    an arithmetic comparison to a bit test. This means we now silently assume
    that PAGE_OFFSET is a power of 2 that divides the kernel virtual address
    space into two equal halves. So make that assumption explicit.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 22c6758b01aa..8c3d7dd91c25 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -170,6 +170,13 @@ void __init arm64_memblock_init(void)
 {
 	const s64 linear_region_size = -(s64)PAGE_OFFSET;
 
+	/*
+	 * Ensure that the linear region takes up exactly half of the kernel
+	 * virtual address space. This way, we can distinguish a linear address
+	 * from a kernel/module/vmalloc address by testing a single bit.
+	 */
+	BUILD_BUG_ON(linear_region_size != BIT(VA_BITS - 1));
+
 	/*
 	 * Select a suitable value for the base of physical memory.
 	 */

commit 020d044f66874eba058ce8264fc550f3eca67879
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Feb 26 17:57:14 2016 +0100

    arm64: mm: treat memstart_addr as a signed quantity
    
    Commit c031a4213c11 ("arm64: kaslr: randomize the linear region")
    implements randomization of the linear region, by subtracting a random
    multiple of PUD_SIZE from memstart_addr. This causes the virtual mapping
    of system RAM to move upwards in the linear region, and at the same time
    causes memstart_addr to assume a value which may be negative if the offset
    of system RAM in the physical space is smaller than its offset relative to
    PAGE_OFFSET in the virtual space.
    
    Since memstart_addr is effectively an offset now, redefine its type as s64
    so that expressions involving shifting or division preserve its sign.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 3b897b9f8807..22c6758b01aa 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -54,7 +54,7 @@
  * executes, which assigns it its actual value. So use a default value
  * that cannot be mistaken for a real physical address.
  */
-phys_addr_t memstart_addr __read_mostly = ~0ULL;
+s64 memstart_addr __read_mostly = -1;
 phys_addr_t arm64_dma_phys_limit __read_mostly;
 
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -181,7 +181,7 @@ void __init arm64_memblock_init(void)
 	 * linear mapping. Take care not to clip the kernel which may be
 	 * high in memory.
 	 */
-	memblock_remove(max(memstart_addr + linear_region_size, __pa(_end)),
+	memblock_remove(max_t(u64, memstart_addr + linear_region_size, __pa(_end)),
 			ULLONG_MAX);
 	if (memblock_end_of_DRAM() > linear_region_size)
 		memblock_remove(0, memblock_end_of_DRAM() - linear_region_size);

commit a6e1f7273b709328ed19e1b8ca5ae61dc1d9b4c0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Feb 26 23:33:12 2016 +0100

    arm64: mm: list kernel sections in order
    
    In the boot log, instead of listing .init first, list .text, .rodata,
    .init and .data in the same order they appear in memory
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index b938de23dc77..3b897b9f8807 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -361,9 +361,9 @@ void __init mem_init(void)
 #endif
 		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n"
-		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
@@ -377,9 +377,9 @@ void __init mem_init(void)
 #endif
 		  MLM(MODULES_VADDR, MODULES_END),
 		  MLG(VMALLOC_START, VMALLOC_END),
-		  MLK_ROUNDUP(__init_begin, __init_end),
 		  MLK_ROUNDUP(_text, __start_rodata),
 		  MLK_ROUNDUP(__start_rodata, _etext),
+		  MLK_ROUNDUP(__init_begin, __init_end),
 		  MLK_ROUNDUP(_sdata, _edata),
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  MLG((unsigned long)vmemmap,

commit dfd55ad85e4a7fbaa82df12467515ac3c81e8a3e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Feb 26 17:57:13 2016 +0100

    arm64: vmemmap: use virtual projection of linear region
    
    Commit dd006da21646 ("arm64: mm: increase VA range of identity map") made
    some changes to the memory mapping code to allow physical memory to reside
    at an offset that exceeds the size of the virtual mapping.
    
    However, since the size of the vmemmap area is proportional to the size of
    the VA area, but it is populated relative to the physical space, we may
    end up with the struct page array being mapped outside of the vmemmap
    region. For instance, on my Seattle A0 box, I can see the following output
    in the dmesg log.
    
       vmemmap : 0xffffffbdc0000000 - 0xffffffbfc0000000   (     8 GB maximum)
                 0xffffffbfc0000000 - 0xffffffbfd0000000   (   256 MB actual)
    
    We can fix this by deciding that the vmemmap region is not a projection of
    the physical space, but of the virtual space above PAGE_OFFSET, i.e., the
    linear region. This way, we are guaranteed that the vmemmap region is of
    sufficient size, and we can even reduce the size by half.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f3b061e67bfe..7802f216a67a 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -319,8 +319,8 @@ void __init mem_init(void)
 #endif
 		  MLG(VMALLOC_START, VMALLOC_END),
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-		  MLG((unsigned long)vmemmap,
-		      (unsigned long)vmemmap + VMEMMAP_SIZE),
+		  MLG(VMEMMAP_START,
+		      VMEMMAP_START + VMEMMAP_SIZE),
 		  MLM((unsigned long)virt_to_page(PAGE_OFFSET),
 		      (unsigned long)virt_to_page(high_memory)),
 #endif

commit 2f39b5f91eb4bccd786d194e70db1dccad784755
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Fri Feb 19 11:50:32 2016 -0600

    arm64: mm: Mark .rodata as RO
    
    Currently the .rodata section is actually still executable when DEBUG_RODATA
    is enabled. This changes that so the .rodata is actually read only, no execute.
    It also adds the .rodata section to the mem_init banner.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    [catalin.marinas@arm.com: added vm_struct vmlinux_rodata in map_kernel()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 80c8bdeeb41a..b938de23dc77 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -363,6 +363,7 @@ void __init mem_init(void)
 		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n"
 		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		  "    .rodata : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
@@ -377,7 +378,8 @@ void __init mem_init(void)
 		  MLM(MODULES_VADDR, MODULES_END),
 		  MLG(VMALLOC_START, VMALLOC_END),
 		  MLK_ROUNDUP(__init_begin, __init_end),
-		  MLK_ROUNDUP(_text, _etext),
+		  MLK_ROUNDUP(_text, __start_rodata),
+		  MLK_ROUNDUP(__start_rodata, _etext),
 		  MLK_ROUNDUP(_sdata, _edata),
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  MLG((unsigned long)vmemmap,

commit b7dc8d16e76c25409d93ff3aceda42fc386efc4e
Author: Miles Chen <miles.chen@mediatek.com>
Date:   Thu Feb 25 11:44:34 2016 +0800

    arm64/mm: remove unnecessary boundary check
    
    Remove the unnecessary boundary check since there is a huge
    gap between user and kernel address that they would never overlap.
    (arm64 does not have enough levels of page tables to cover 64-bit
    virtual address)
    
    See Documentation/arm64/memory.txt
    
    Signed-off-by: Miles Chen <miles.chen@mediatek.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index e1f425fe5a81..80c8bdeeb41a 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -401,8 +401,6 @@ void __init mem_init(void)
 #ifdef CONFIG_COMPAT
 	BUILD_BUG_ON(TASK_SIZE_32			> TASK_SIZE_64);
 #endif
-	BUILD_BUG_ON(TASK_SIZE_64			> MODULES_VADDR);
-	BUG_ON(TASK_SIZE_64				> MODULES_VADDR);
 
 	if (PAGE_SIZE >= 16384 && get_num_physpages() <= 128) {
 		extern int sysctl_overcommit_memory;

commit c031a4213c11a5db475f528c182f7b3858df11db
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Jan 29 11:59:03 2016 +0100

    arm64: kaslr: randomize the linear region
    
    When KASLR is enabled (CONFIG_RANDOMIZE_BASE=y), and entropy has been
    provided by the bootloader, randomize the placement of RAM inside the
    linear region if sufficient space is available. For instance, on a 4KB
    granule/3 levels kernel, the linear region is 256 GB in size, and we can
    choose any 1 GB aligned offset that is far enough from the top of the
    address space to fit the distance between the start of the lowest memblock
    and the top of the highest memblock.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index c0ea54bd9995..e1f425fe5a81 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -196,6 +196,23 @@ void __init arm64_memblock_init(void)
 		memblock_add(__pa(_text), (u64)(_end - _text));
 	}
 
+	if (IS_ENABLED(CONFIG_RANDOMIZE_BASE)) {
+		extern u16 memstart_offset_seed;
+		u64 range = linear_region_size -
+			    (memblock_end_of_DRAM() - memblock_start_of_DRAM());
+
+		/*
+		 * If the size of the linear region exceeds, by a sufficient
+		 * margin, the size of the region that the available physical
+		 * memory spans, randomize the linear region as well.
+		 */
+		if (memstart_offset_seed > 0 && range >= ARM64_MEMSTART_ALIGN) {
+			range = range / ARM64_MEMSTART_ALIGN + 1;
+			memstart_addr -= ARM64_MEMSTART_ALIGN *
+					 ((range * memstart_offset_seed) >> 16);
+		}
+	}
+
 	/*
 	 * Register the kernel text, kernel data, initrd, and initial
 	 * pagetables with memblock.
@@ -365,12 +382,13 @@ void __init mem_init(void)
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  MLG((unsigned long)vmemmap,
 		      (unsigned long)vmemmap + VMEMMAP_SIZE),
-		  MLM((unsigned long)virt_to_page(PAGE_OFFSET),
+		  MLM((unsigned long)phys_to_page(memblock_start_of_DRAM()),
 		      (unsigned long)virt_to_page(high_memory)),
 #endif
 		  MLK(FIXADDR_START, FIXADDR_TOP),
 		  MLM(PCI_IO_START, PCI_IO_END),
-		  MLM(PAGE_OFFSET, (unsigned long)high_memory));
+		  MLM(__phys_to_virt(memblock_start_of_DRAM()),
+		      (unsigned long)high_memory));
 
 #undef MLK
 #undef MLM

commit a7f8de168ace487fa7b88cb154e413cf40e87fc6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:42 2016 +0100

    arm64: allow kernel Image to be loaded anywhere in physical memory
    
    This relaxes the kernel Image placement requirements, so that it
    may be placed at any 2 MB aligned offset in physical memory.
    
    This is accomplished by ignoring PHYS_OFFSET when installing
    memblocks, and accounting for the apparent virtual offset of
    the kernel Image. As a result, virtual address references
    below PAGE_OFFSET are correctly mapped onto physical references
    into the kernel Image regardless of where it sits in memory.
    
    Special care needs to be taken for dealing with memory limits passed
    via mem=, since the generic implementation clips memory top down, which
    may clip the kernel image itself if it is loaded high up in memory. To
    deal with this case, we simply add back the memory covering the kernel
    image, which may result in more memory to be retained than was passed
    as a mem= parameter.
    
    Since mem= should not be considered a production feature, a panic notifier
    handler is installed that dumps the memory limit at panic time if one was
    set.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 52d1fc465885..c0ea54bd9995 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -35,8 +35,10 @@
 #include <linux/efi.h>
 #include <linux/swiotlb.h>
 
+#include <asm/boot.h>
 #include <asm/fixmap.h>
 #include <asm/kasan.h>
+#include <asm/kernel-pgtable.h>
 #include <asm/memory.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
@@ -46,7 +48,13 @@
 
 #include "mm.h"
 
-phys_addr_t memstart_addr __read_mostly = 0;
+/*
+ * We need to be able to catch inadvertent references to memstart_addr
+ * that occur (potentially in generic code) before arm64_memblock_init()
+ * executes, which assigns it its actual value. So use a default value
+ * that cannot be mistaken for a real physical address.
+ */
+phys_addr_t memstart_addr __read_mostly = ~0ULL;
 phys_addr_t arm64_dma_phys_limit __read_mostly;
 
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -160,7 +168,33 @@ early_param("mem", early_mem);
 
 void __init arm64_memblock_init(void)
 {
-	memblock_enforce_memory_limit(memory_limit);
+	const s64 linear_region_size = -(s64)PAGE_OFFSET;
+
+	/*
+	 * Select a suitable value for the base of physical memory.
+	 */
+	memstart_addr = round_down(memblock_start_of_DRAM(),
+				   ARM64_MEMSTART_ALIGN);
+
+	/*
+	 * Remove the memory that we will not be able to cover with the
+	 * linear mapping. Take care not to clip the kernel which may be
+	 * high in memory.
+	 */
+	memblock_remove(max(memstart_addr + linear_region_size, __pa(_end)),
+			ULLONG_MAX);
+	if (memblock_end_of_DRAM() > linear_region_size)
+		memblock_remove(0, memblock_end_of_DRAM() - linear_region_size);
+
+	/*
+	 * Apply the memory limit if it was set. Since the kernel may be loaded
+	 * high up in memory, add back the kernel region that must be accessible
+	 * via the linear mapping.
+	 */
+	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
+		memblock_enforce_memory_limit(memory_limit);
+		memblock_add(__pa(_text), (u64)(_end - _text));
+	}
 
 	/*
 	 * Register the kernel text, kernel data, initrd, and initial
@@ -386,3 +420,28 @@ static int __init keepinitrd_setup(char *__unused)
 
 __setup("keepinitrd", keepinitrd_setup);
 #endif
+
+/*
+ * Dump out memory limit information on panic.
+ */
+static int dump_mem_limit(struct notifier_block *self, unsigned long v, void *p)
+{
+	if (memory_limit != (phys_addr_t)ULLONG_MAX) {
+		pr_emerg("Memory Limit: %llu MB\n", memory_limit >> 20);
+	} else {
+		pr_emerg("Memory Limit: none\n");
+	}
+	return 0;
+}
+
+static struct notifier_block mem_limit_notifier = {
+	.notifier_call = dump_mem_limit,
+};
+
+static int __init register_mem_limit_dumper(void)
+{
+	atomic_notifier_chain_register(&panic_notifier_list,
+				       &mem_limit_notifier);
+	return 0;
+}
+__initcall(register_mem_limit_dumper);

commit a89dea585371a9d5d85499db47c93f129be8e0c4
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:41 2016 +0100

    arm64: defer __va translation of initrd_start and initrd_end
    
    Before deferring the assignment of memstart_addr in a subsequent patch, to
    the moment where all memory has been discovered and possibly clipped based
    on the size of the linear region and the presence of a mem= command line
    parameter, we need to ensure that memstart_addr is not used to perform __va
    translations before it is assigned.
    
    One such use is in the generic early DT discovery of the initrd location,
    which is recorded as a virtual address in the globals initrd_start and
    initrd_end. So wire up the generic support to declare the initrd addresses,
    and implement it without __va() translations, and perform the translation
    after memstart_addr has been assigned.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 1d627cd8121c..52d1fc465885 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -59,8 +59,8 @@ static int __init early_initrd(char *p)
 	if (*endp == ',') {
 		size = memparse(endp + 1, NULL);
 
-		initrd_start = (unsigned long)__va(start);
-		initrd_end = (unsigned long)__va(start + size);
+		initrd_start = start;
+		initrd_end = start + size;
 	}
 	return 0;
 }
@@ -168,8 +168,13 @@ void __init arm64_memblock_init(void)
 	 */
 	memblock_reserve(__pa(_text), _end - _text);
 #ifdef CONFIG_BLK_DEV_INITRD
-	if (initrd_start)
-		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
+	if (initrd_start) {
+		memblock_reserve(initrd_start, initrd_end - initrd_start);
+
+		/* the generic initrd code expects virtual addresses */
+		initrd_start = __phys_to_virt(initrd_start);
+		initrd_end = __phys_to_virt(initrd_end);
+	}
 #endif
 
 	early_init_fdt_scan_reserved_mem();

commit f9040773b7bbbd9e98eb6184a263512a7cfc133f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:40 2016 +0100

    arm64: move kernel image to base of vmalloc area
    
    This moves the module area to right before the vmalloc area, and moves
    the kernel image to the base of the vmalloc area. This is an intermediate
    step towards implementing KASLR, which allows the kernel image to be
    located anywhere in the vmalloc area.
    
    Since other subsystems such as hibernate may still need to refer to the
    kernel text or data segments via their linears addresses, both are mapped
    in the linear region as well. The linear alias of the text region is
    mapped read-only/non-executable to prevent inadvertent modification or
    execution.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f3b061e67bfe..1d627cd8121c 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -36,6 +36,7 @@
 #include <linux/swiotlb.h>
 
 #include <asm/fixmap.h>
+#include <asm/kasan.h>
 #include <asm/memory.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
@@ -302,22 +303,26 @@ void __init mem_init(void)
 #ifdef CONFIG_KASAN
 		  "    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n"
 #endif
+		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n"
+		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
 		  "              0x%16lx - 0x%16lx   (%6ld MB actual)\n"
 #endif
 		  "    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n"
 		  "    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n"
-		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
-		  "    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n"
-		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
-		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		  "    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n",
 #ifdef CONFIG_KASAN
 		  MLG(KASAN_SHADOW_START, KASAN_SHADOW_END),
 #endif
+		  MLM(MODULES_VADDR, MODULES_END),
 		  MLG(VMALLOC_START, VMALLOC_END),
+		  MLK_ROUNDUP(__init_begin, __init_end),
+		  MLK_ROUNDUP(_text, _etext),
+		  MLK_ROUNDUP(_sdata, _edata),
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  MLG((unsigned long)vmemmap,
 		      (unsigned long)vmemmap + VMEMMAP_SIZE),
@@ -326,11 +331,7 @@ void __init mem_init(void)
 #endif
 		  MLK(FIXADDR_START, FIXADDR_TOP),
 		  MLM(PCI_IO_START, PCI_IO_END),
-		  MLM(MODULES_VADDR, MODULES_END),
-		  MLM(PAGE_OFFSET, (unsigned long)high_memory),
-		  MLK_ROUNDUP(__init_begin, __init_end),
-		  MLK_ROUNDUP(_text, _etext),
-		  MLK_ROUNDUP(_sdata, _edata));
+		  MLM(PAGE_OFFSET, (unsigned long)high_memory));
 
 #undef MLK
 #undef MLM
@@ -358,8 +359,8 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
-	fixup_init();
 	free_initmem_default(0);
+	fixup_init();
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD

commit 129b985cc3f331d0e6908958f50757a3a1b37793
Merge: 32d6397805d0 f7d924894265
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 15 10:59:03 2015 +0000

    Merge branch 'aarch64/efi' into aarch64/for-next/core
    
    Merge in EFI memblock changes from Ard, which form the preparatory work
    for UEFI support on 32-bit ARM.

commit 9aa4ec1571da62366cfddc20f3b923609604fe63
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Dec 9 12:44:38 2015 +0000

    arm64: mm: fold alternatives into .init
    
    Currently we treat the alternatives separately from other data that's
    only used during initialisation, using separate .altinstructions and
    .altinstr_replacement linker sections. These are freed for general
    allocation separately from .init*. This is problematic as:
    
    * We do not remove execute permissions, as we do for .init, leaving the
      memory executable.
    
    * We pad between them, making the kernel Image bianry up to PAGE_SIZE
      bytes larger than necessary.
    
    This patch moves the two sections into the contiguous region used for
    .init*. This saves some memory, ensures that we remove execute
    permissions, and allows us to remove some code made redundant by this
    reorganisation.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 9b8cc673f43c..2ee6c208c318 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -360,7 +360,6 @@ void free_initmem(void)
 {
 	fixup_init();
 	free_initmem_default(0);
-	free_alternatives_memory();
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD

commit 68709f45385aeddb0ca96a060c0c8259944f321b
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Nov 30 13:28:16 2015 +0100

    arm64: only consider memblocks with NOMAP cleared for linear mapping
    
    Take the new memblock attribute MEMBLOCK_NOMAP into account when
    deciding whether a certain region is or should be covered by the
    kernel direct mapping.
    
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 17bf39ac83ba..ac4d7cbbdd2d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -120,7 +120,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 #ifdef CONFIG_HAVE_ARCH_PFN_VALID
 int pfn_valid(unsigned long pfn)
 {
-	return memblock_is_memory(pfn << PAGE_SHIFT);
+	return memblock_is_map_memory(pfn << PAGE_SHIFT);
 }
 EXPORT_SYMBOL(pfn_valid);
 #endif

commit a7c61a3452d39078919f0e1f493ff966fb64f0db
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Fri Nov 20 17:59:10 2015 +0800

    arm64: add __init/__initdata section marker to some functions/variables
    
    These functions/variables are not needed after booting, so mark them
    as __init or __initdata.
    
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 17bf39ac83ba..9b8cc673f43c 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -71,7 +71,7 @@ early_param("initrd", early_initrd);
  * currently assumes that for memory starting above 4G, 32-bit devices will
  * use a DMA offset.
  */
-static phys_addr_t max_zone_dma_phys(void)
+static phys_addr_t __init max_zone_dma_phys(void)
 {
 	phys_addr_t offset = memblock_start_of_DRAM() & GENMASK_ULL(63, 32);
 	return min(offset + (1ULL << 32), memblock_end_of_DRAM());
@@ -126,11 +126,11 @@ EXPORT_SYMBOL(pfn_valid);
 #endif
 
 #ifndef CONFIG_SPARSEMEM
-static void arm64_memory_present(void)
+static void __init arm64_memory_present(void)
 {
 }
 #else
-static void arm64_memory_present(void)
+static void __init arm64_memory_present(void)
 {
 	struct memblock_region *reg;
 

commit 86a5906e4d1df1ec160fa9e18b6f2277a5216c60
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Oct 27 17:40:26 2015 +0000

    arm64: Fix build with CONFIG_ZONE_DMA=n
    
    Trying to build with CONFIG_ZONE_DMA=n leaves visible references
    to the now-undefined ZONE_DMA, resulting in a syntax error.
    
    Hide the references behind an #ifdef instead of using IS_ENABLED.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 7a1f9a05dc82..17bf39ac83ba 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -86,10 +86,10 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	memset(zone_size, 0, sizeof(zone_size));
 
 	/* 4GB maximum for 32-bit only capable devices */
-	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
-		max_dma = PFN_DOWN(arm64_dma_phys_limit);
-		zone_size[ZONE_DMA] = max_dma - min;
-	}
+#ifdef CONFIG_ZONE_DMA
+	max_dma = PFN_DOWN(arm64_dma_phys_limit);
+	zone_size[ZONE_DMA] = max_dma - min;
+#endif
 	zone_size[ZONE_NORMAL] = max - max_dma;
 
 	memcpy(zhole_size, zone_size, sizeof(zhole_size));
@@ -101,11 +101,12 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 		if (start >= max)
 			continue;
 
-		if (IS_ENABLED(CONFIG_ZONE_DMA) && start < max_dma) {
+#ifdef CONFIG_ZONE_DMA
+		if (start < max_dma) {
 			unsigned long dma_end = min(end, max_dma);
 			zhole_size[ZONE_DMA] -= dma_end - start;
 		}
-
+#endif
 		if (end > max_dma) {
 			unsigned long normal_end = min(end, max);
 			unsigned long normal_start = max(start, max_dma);

commit ee7f881b59de4e0e0be250fd0c5d4ade3e30ec34
Author: Linus Walleij <linus.walleij@linaro.org>
Date:   Mon Oct 12 18:52:59 2015 +0300

    ARM64: kasan: print memory assignment
    
    This prints out the virtual memory assigned to KASan in the
    boot crawl along with other memory assignments, if and only
    if KASan is activated.
    
    Example dmesg from the Juno Development board:
    
    Memory: 1691156K/2080768K available (5465K kernel code, 444K rwdata,
    2160K rodata, 340K init, 217K bss, 373228K reserved, 16384K cma-reserved)
    Virtual kernel memory layout:
        kasan   : 0xffffff8000000000 - 0xffffff9000000000   (    64 GB)
        vmalloc : 0xffffff9000000000 - 0xffffffbdbfff0000   (   182 GB)
        vmemmap : 0xffffffbdc0000000 - 0xffffffbfc0000000   (     8 GB maximum)
                  0xffffffbdc2000000 - 0xffffffbdc3fc0000   (    31 MB actual)
        fixed   : 0xffffffbffabfd000 - 0xffffffbffac00000   (    12 KB)
        PCI I/O : 0xffffffbffae00000 - 0xffffffbffbe00000   (    16 MB)
        modules : 0xffffffbffc000000 - 0xffffffc000000000   (    64 MB)
        memory  : 0xffffffc000000000 - 0xffffffc07f000000   (  2032 MB)
          .init : 0xffffffc0007f5000 - 0xffffffc00084a000   (   340 KB)
          .text : 0xffffffc000080000 - 0xffffffc0007f45b4   (  7634 KB)
          .data : 0xffffffc000850000 - 0xffffffc0008bf200   (   445 KB)
    
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f5c0680d17d9..7a1f9a05dc82 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -298,6 +298,9 @@ void __init mem_init(void)
 #define MLK_ROUNDUP(b, t) b, t, DIV_ROUND_UP(((t) - (b)), SZ_1K)
 
 	pr_notice("Virtual kernel memory layout:\n"
+#ifdef CONFIG_KASAN
+		  "    kasan   : 0x%16lx - 0x%16lx   (%6ld GB)\n"
+#endif
 		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n"
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
@@ -310,6 +313,9 @@ void __init mem_init(void)
 		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
 		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+#ifdef CONFIG_KASAN
+		  MLG(KASAN_SHADOW_START, KASAN_SHADOW_END),
+#endif
 		  MLG(VMALLOC_START, VMALLOC_END),
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
 		  MLG((unsigned long)vmemmap,

commit 662ba3dbceca3ca284885a464ecb8c936f417003
Author: Wang Long <long.wanglong@huawei.com>
Date:   Mon Jul 27 03:32:53 2015 +0100

    arm64: mm: add __init section marker to free_initrd_mem
    
    It is not needed after booting, this patch moves the
    free_initrd_mem() function to the __init section.
    
    This patch also make keep_initrd __initdata, to reduce kernel
    size.
    
    Signed-off-by: Wang Long <long.wanglong@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index ad87ce826cce..f5c0680d17d9 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -358,9 +358,9 @@ void free_initmem(void)
 
 #ifdef CONFIG_BLK_DEV_INITRD
 
-static int keep_initrd;
+static int keep_initrd __initdata;
 
-void free_initrd_mem(unsigned long start, unsigned long end)
+void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
 	if (!keep_initrd)
 		free_reserved_area((void *)start, (void *)end, 0, "initrd");

commit b9bcc919931611498e856eae9bf66337330d04cc
Author: Dave P Martin <Dave.Martin@arm.com>
Date:   Tue Jun 16 17:38:47 2015 +0100

    arm64: mm: Fix freeing of the wrong memmap entries with !SPARSEMEM_VMEMMAP
    
    The memmap freeing code in free_unused_memmap() computes the end of
    each memblock by adding the memblock size onto the base.  However,
    if SPARSEMEM is enabled then the value (start) used for the base
    may already have been rounded downwards to work out which memmap
    entries to free after the previous memblock.
    
    This may cause memmap entries that are in use to get freed.
    
    In general, you're not likely to hit this problem unless there
    are at least 2 memblocks and one of them is not aligned to a
    sparsemem section boundary.  Note that carve-outs can increase
    the number of memblocks by splitting the regions listed in the
    device tree.
    
    This problem doesn't occur with SPARSEMEM_VMEMMAP, because the
    vmemmap code deals with freeing the unused regions of the memmap
    instead of requiring the arch code to do it.
    
    This patch gets the memblock base out of the memblock directly when
    computing the block end address to ensure the correct value is used.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 597831bdddf3..ad87ce826cce 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -262,7 +262,7 @@ static void __init free_unused_memmap(void)
 		 * memmap entries are valid from the bank end aligned to
 		 * MAX_ORDER_NR_PAGES.
 		 */
-		prev_end = ALIGN(start + __phys_to_pfn(reg->size),
+		prev_end = ALIGN(__phys_to_pfn(reg->base + reg->size),
 				 MAX_ORDER_NR_PAGES);
 	}
 

commit 61bd93ce801bb6df36eda257a9d2d16c02863cdd
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Jun 1 13:40:32 2015 +0200

    arm64: use fixmap region for permanent FDT mapping
    
    Currently, the FDT blob needs to be in the same 512 MB region as
    the kernel, so that it can be mapped into the kernel virtual memory
    space very early on using a minimal set of statically allocated
    translation tables.
    
    Now that we have early fixmap support, we can relax this restriction,
    by moving the permanent FDT mapping to the fixmap region instead.
    This way, the FDT blob may be anywhere in memory.
    
    This also moves the vetting of the FDT to mmu.c, since the early
    init code in head.S does not handle mapping of the FDT anymore.
    At the same time, fix up some comments in head.S that have gone stale.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 89a05f467ffb..597831bdddf3 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -170,7 +170,6 @@ void __init arm64_memblock_init(void)
 		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
 #endif
 
-	early_init_fdt_reserve_self();
 	early_init_fdt_scan_reserved_mem();
 
 	/* 4GB maximum for 32-bit only capable devices */

commit 24bbd929e6b9e62afd263c42b4318d3b603c956c
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Jun 1 13:40:31 2015 +0200

    of/fdt: split off FDT self reservation from memreserve processing
    
    This splits off the reservation of the memory occupied by the FDT
    binary itself from the processing of the memory reservations it
    contains. This is necessary because the physical address of the FDT,
    which is needed to perform the reservation, may not be known to the
    FDT driver core, i.e., it may be mapped outside the linear direct
    mapping, in which case __pa() returns a bogus value.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Acked-by: Rob Herring <robh@kernel.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 597831bdddf3..89a05f467ffb 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -170,6 +170,7 @@ void __init arm64_memblock_init(void)
 		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
 #endif
 
+	early_init_fdt_reserve_self();
 	early_init_fdt_scan_reserved_mem();
 
 	/* 4GB maximum for 32-bit only capable devices */

commit 36dd9086cb31613ace45e94c18a17241d3d0aac4
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Tue Apr 14 15:48:33 2015 -0700

    arm64: add support for memtest
    
    Add support for memtest command line option.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index ae85da6307bb..597831bdddf3 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -190,6 +190,8 @@ void __init bootmem_init(void)
 	min = PFN_UP(memblock_start_of_DRAM());
 	max = PFN_DOWN(memblock_end_of_DRAM());
 
+	early_memtest(min << PAGE_SHIFT, max << PAGE_SHIFT);
+
 	/*
 	 * Sparsemem tries to allocate bootmem in memory_present(), so must be
 	 * done after the fixed reservations.

commit a1e50a82256ed2f1312e70c52a84323e2e378f49
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 5 18:01:53 2015 +0000

    arm64: Increase the swiotlb buffer size 64MB
    
    With commit 3690951fc6d4 (arm64: Use swiotlb late initialisation), the
    swiotlb buffer size is limited to MAX_ORDER_NR_PAGES. However, there are
    platforms with 32-bit only devices that require bounce buffering via
    swiotlb. This patch changes the swiotlb initialisation to an early 64MB
    memblock allocation. In order to get the swiotlb buffer correctly
    allocated (via memblock_virt_alloc_low_nopanic), this patch also defines
    ARCH_LOW_ADDRESS_LIMIT to the maximum physical address capable of 32-bit
    DMA.
    
    Reported-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Tested-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 71145f952070..ae85da6307bb 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -33,6 +33,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/dma-contiguous.h>
 #include <linux/efi.h>
+#include <linux/swiotlb.h>
 
 #include <asm/fixmap.h>
 #include <asm/memory.h>
@@ -45,6 +46,7 @@
 #include "mm.h"
 
 phys_addr_t memstart_addr __read_mostly = 0;
+phys_addr_t arm64_dma_phys_limit __read_mostly;
 
 #ifdef CONFIG_BLK_DEV_INITRD
 static int __init early_initrd(char *p)
@@ -85,7 +87,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
-		max_dma = PFN_DOWN(max_zone_dma_phys());
+		max_dma = PFN_DOWN(arm64_dma_phys_limit);
 		zone_size[ZONE_DMA] = max_dma - min;
 	}
 	zone_size[ZONE_NORMAL] = max - max_dma;
@@ -156,8 +158,6 @@ early_param("mem", early_mem);
 
 void __init arm64_memblock_init(void)
 {
-	phys_addr_t dma_phys_limit = 0;
-
 	memblock_enforce_memory_limit(memory_limit);
 
 	/*
@@ -174,8 +174,10 @@ void __init arm64_memblock_init(void)
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA))
-		dma_phys_limit = max_zone_dma_phys();
-	dma_contiguous_reserve(dma_phys_limit);
+		arm64_dma_phys_limit = max_zone_dma_phys();
+	else
+		arm64_dma_phys_limit = PHYS_MASK + 1;
+	dma_contiguous_reserve(arm64_dma_phys_limit);
 
 	memblock_allow_resize();
 	memblock_dump_all();
@@ -276,6 +278,8 @@ static void __init free_unused_memmap(void)
  */
 void __init mem_init(void)
 {
+	swiotlb_init(1);
+
 	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP

commit 6b00f7efb5303418c231994c91fb8239f5ada260
Merge: b3d6524ff795 d476d94f180a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 18:03:54 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "arm64 updates for 3.20:
    
       - reimplementation of the virtual remapping of UEFI Runtime Services
         in a way that is stable across kexec
       - emulation of the "setend" instruction for 32-bit tasks (user
         endianness switching trapped in the kernel, SCTLR_EL1.E0E bit set
         accordingly)
       - compat_sys_call_table implemented in C (from asm) and made it a
         constant array together with sys_call_table
       - export CPU cache information via /sys (like other architectures)
       - DMA API implementation clean-up in preparation for IOMMU support
       - macros clean-up for KVM
       - dropped some unnecessary cache+tlb maintenance
       - CONFIG_ARM64_CPU_SUSPEND clean-up
       - defconfig update (CPU_IDLE)
    
      The EFI changes going via the arm64 tree have been acked by Matt
      Fleming.  There is also a patch adding sys_*stat64 prototypes to
      include/linux/syscalls.h, acked by Andrew Morton"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (47 commits)
      arm64: compat: Remove incorrect comment in compat_siginfo
      arm64: Fix section mismatch on alloc_init_p[mu]d()
      arm64: Avoid breakage caused by .altmacro in fpsimd save/restore macros
      arm64: mm: use *_sect to check for section maps
      arm64: drop unnecessary cache+tlb maintenance
      arm64:mm: free the useless initial page table
      arm64: Enable CPU_IDLE in defconfig
      arm64: kernel: remove ARM64_CPU_SUSPEND config option
      arm64: make sys_call_table const
      arm64: Remove asm/syscalls.h
      arm64: Implement the compat_sys_call_table in C
      syscalls: Declare sys_*stat64 prototypes if __ARCH_WANT_(COMPAT_)STAT64
      compat: Declare compat_sys_sigpending and compat_sys_sigprocmask prototypes
      arm64: uapi: expose our struct ucontext to the uapi headers
      smp, ARM64: Kill SMP single function call interrupt
      arm64: Emulate SETEND for AArch32 tasks
      arm64: Consolidate hotplug notifier for instruction emulation
      arm64: Track system support for mixed endian EL0
      arm64: implement generic IOMMU configuration
      arm64: Combine coherent and non-coherent swiotlb dma_ops
      ...

commit aa03c428e67881795f4190f9ffdb62fc14978608
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 22 18:20:35 2015 +0000

    arm64: Fix overlapping VA allocations
    
    PCI IO space was intended to be 16MiB, at 32MiB below MODULES_VADDR, but
    commit d1e6dc91b532d3d3 ("arm64: Add architectural support for PCI")
    extended this to cover the full 32MiB. The final 8KiB of this 32MiB is
    also allocated for the fixmap, allowing for potential clashes between
    the two.
    
    This change was masked by assumptions in mem_init and the page table
    dumping code, which assumed the I/O space to be 16MiB long through
    seaparte hard-coded definitions.
    
    This patch changes the definition of the PCI I/O space allocation to
    live in asm/memory.h, along with the other VA space allocations. As the
    fixmap allocation depends on the number of fixmap entries, this is moved
    below the PCI I/O space allocation. Both the fixmap and PCI I/O space
    are guarded with 2MB of padding. Sites assuming the I/O space was 16MiB
    are moved over use new PCI_IO_{START,END} definitions, which will keep
    in sync with the size of the IO space (now restored to 16MiB).
    
    As a useful side effect, the use of the new PCI_IO_{START,END}
    definitions prevents a build issue in the dumping code due to a (now
    redundant) missing include of io.h for PCI_IOBASE.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: reorder FIXADDR and PCI_IO address_markers_idx enum]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 43cccb5101c0..a8dfb40eefa8 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -35,6 +35,7 @@
 #include <linux/efi.h>
 
 #include <asm/fixmap.h>
+#include <asm/memory.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>
@@ -296,8 +297,8 @@ void __init mem_init(void)
 		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
 		  "              0x%16lx - 0x%16lx   (%6ld MB actual)\n"
 #endif
-		  "    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n"
+		  "    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
@@ -310,8 +311,8 @@ void __init mem_init(void)
 		  MLM((unsigned long)virt_to_page(PAGE_OFFSET),
 		      (unsigned long)virt_to_page(high_memory)),
 #endif
-		  MLM((unsigned long)PCI_IOBASE, (unsigned long)PCI_IOBASE + SZ_16M),
 		  MLK(FIXADDR_START, FIXADDR_TOP),
+		  MLM(PCI_IO_START, PCI_IO_END),
 		  MLM(MODULES_VADDR, MODULES_END),
 		  MLM(PAGE_OFFSET, (unsigned long)high_memory),
 		  MLK_ROUNDUP(__init_begin, __init_end),

commit da141706aea52c1a9fbd28cb8d289b78819f5436
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jan 21 17:36:06 2015 -0800

    arm64: add better page protections to arm64
    
    Add page protections for arm64 similar to those in arm.
    This is for security reasons to prevent certain classes
    of exploits. The current method:
    
    - Map all memory as either RWX or RW. We round to the nearest
      section to avoid creating page tables before everything is mapped
    - Once everything is mapped, if either end of the RWX section should
      not be X, we split the PMD and remap as necessary
    - When initmem is to be freed, we change the permissions back to
      RW (using stop machine if necessary to flush the TLB)
    - If CONFIG_DEBUG_RODATA is set, the read only sections are set
      read only.
    
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Kees Cook <keescook@chromium.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 11c7b701b681..43cccb5101c0 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -344,6 +344,7 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
+	fixup_init();
 	free_initmem_default(0);
 	free_alternatives_memory();
 }

commit 6083fe74b7bfffc2c7be8c711596608bda0cda6e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 15 16:42:14 2015 +0000

    arm64: respect mem= for EFI
    
    When booting with EFI, we acquire the EFI memory map after parsing the
    early params. This unfortuantely renders the option useless as we call
    memblock_enforce_memory_limit (which uses memblock_remove_range behind
    the scenes) before we've added any memblocks. We end up removing
    nothing, then adding all of memory later when efi_init calls
    reserve_regions.
    
    Instead, we can log the limit and apply this later when we do the rest
    of the memblock work in memblock_init, which should work regardless of
    the presence of EFI. At the same time we may as well move the early
    parameter into arm64's mm/init.c, close to arm64_memblock_init.
    
    Any memory which must be mapped (e.g. for use by EFI runtime services)
    must be mapped explicitly reather than relying on the linear mapping,
    which may be truncated as a result of a mem= option passed on the kernel
    command line.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Leif Lindholm <leif.lindholm@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index bac492c12fcc..11c7b701b681 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -136,10 +136,29 @@ static void arm64_memory_present(void)
 }
 #endif
 
+static phys_addr_t memory_limit = (phys_addr_t)ULLONG_MAX;
+
+/*
+ * Limit the memory size that was specified via FDT.
+ */
+static int __init early_mem(char *p)
+{
+	if (!p)
+		return 1;
+
+	memory_limit = memparse(p, &p) & PAGE_MASK;
+	pr_notice("Memory limited to %lldMB\n", memory_limit >> 20);
+
+	return 0;
+}
+early_param("mem", early_mem);
+
 void __init arm64_memblock_init(void)
 {
 	phys_addr_t dma_phys_limit = 0;
 
+	memblock_enforce_memory_limit(memory_limit);
+
 	/*
 	 * Register the kernel text, kernel data, initrd, and initial
 	 * pagetables with memblock.

commit 0145058c3d30b4319d747f64caa16a9cb15f0581
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jan 16 13:56:38 2015 +0000

    arm64: partially revert "ARM: 8167/1: extend the reserved memory for initrd to be page aligned"
    
    This patch partially reverts commit 421520ba98290a73b35b7644e877a48f18e06004
    (only the arm64 part). There is no guarantee that the boot-loader places other
    images like dtb in a different page than initrd start/end, especially when the
    kernel is built with 64KB pages. When this happens, such pages must not be
    freed. The free_reserved_area() already takes care of rounding up "start" and
    rounding down "end" to avoid freeing partially used pages.
    
    Cc: <stable@vger.kernel.org> # 3.17+
    Reported-by: Peter Maydell <Peter.Maydell@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index bac492c12fcc..c95464a33f36 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -335,14 +335,8 @@ static int keep_initrd;
 
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (!keep_initrd) {
-		if (start == initrd_start)
-			start = round_down(start, PAGE_SIZE);
-		if (end == initrd_end)
-			end = round_up(end, PAGE_SIZE);
-
+	if (!keep_initrd)
 		free_reserved_area((void *)start, (void *)end, 0, "initrd");
-	}
 }
 
 static int __init keepinitrd_setup(char *__unused)

commit e039ee4ee3fcf174736f2cb0a2eed6cb908348a6
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:08 2014 +0000

    arm64: add alternative runtime patching
    
    With a blatant copy of some x86 bits we introduce the alternative
    runtime patching "framework" to arm64.
    This is quite basic for now and we only provide the functions we need
    at this time.
    This is connected to the newly introduced feature bits.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 494297c698ca..bac492c12fcc 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -39,6 +39,7 @@
 #include <asm/setup.h>
 #include <asm/sizes.h>
 #include <asm/tlb.h>
+#include <asm/alternative.h>
 
 #include "mm.h"
 
@@ -325,6 +326,7 @@ void __init mem_init(void)
 void free_initmem(void)
 {
 	free_initmem_default(0);
+	free_alternatives_memory();
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD

commit 6325e940e7e0c690c6bdfaf5d54309e71845d3d9
Merge: 536fd93d4328 0a6479b0ffad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 8 05:34:24 2014 -0400

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     - eBPF JIT compiler for arm64
     - CPU suspend backend for PSCI (firmware interface) with standard idle
       states defined in DT (generic idle driver to be merged via a
       different tree)
     - Support for CONFIG_DEBUG_SET_MODULE_RONX
     - Support for unmapped cpu-release-addr (outside kernel linear mapping)
     - set_arch_dma_coherent_ops() implemented and bus notifiers removed
     - EFI_STUB improvements when base of DRAM is occupied
     - Typos in KGDB macros
     - Clean-up to (partially) allow kernel building with LLVM
     - Other clean-ups (extern keyword, phys_addr_t usage)
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (51 commits)
      arm64: Remove unneeded extern keyword
      ARM64: make of_device_ids const
      arm64: Use phys_addr_t type for physical address
      aarch64: filter $x from kallsyms
      arm64: Use DMA_ERROR_CODE to denote failed allocation
      arm64: Fix typos in KGDB macros
      arm64: insn: Add return statements after BUG_ON()
      arm64: debug: don't re-enable debug exceptions on return from el1_dbg
      Revert "arm64: dmi: Add SMBIOS/DMI support"
      arm64: Implement set_arch_dma_coherent_ops() to replace bus notifiers
      of: amba: use of_dma_configure for AMBA devices
      arm64: dmi: Add SMBIOS/DMI support
      arm64: Correct ftrace calls to aarch64_insn_gen_branch_imm()
      arm64:mm: initialize max_mapnr using function set_max_mapnr
      setup: Move unmask of async interrupts after possible earlycon setup
      arm64: LLVMLinux: Fix inline arm64 assembly for use with clang
      arm64: pageattr: Correctly adjust unaligned start addresses
      net: bpf: arm64: fix module memory leak when JIT image build fails
      arm64: add PSCI CPU_SUSPEND based cpu_suspend support
      arm64: kernel: introduce cpu_init_idle CPU operation
      ...

commit d5d16892243e7755da706d03b34da85ea6a74117
Merge: 3467e765a592 ad684dce87fa f3354ab67476 421520ba9829
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Oct 2 21:47:02 2014 +0100

    Merge branches 'fiq' (early part), 'fixes', 'l2c' (early part) and 'misc' into for-next

commit 421520ba98290a73b35b7644e877a48f18e06004
Author: Yalin Wang <Yalin.Wang@sonymobile.com>
Date:   Fri Sep 26 03:07:09 2014 +0100

    ARM: 8167/1: extend the reserved memory for initrd to be page aligned
    
    This patch extends the start and end address of initrd to be page aligned,
    so that we can free all memory including the un-page aligned head or tail
    page of initrd, if the start or end address of initrd are not page
    aligned, the page can't be freed by free_initrd_mem() function.
    
    Signed-off-by: Yalin Wang <yalin.wang@sonymobile.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5472c2401876..c5512f694814 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -334,8 +334,14 @@ static int keep_initrd;
 
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (!keep_initrd)
+	if (!keep_initrd) {
+		if (start == initrd_start)
+			start = round_down(start, PAGE_SIZE);
+		if (end == initrd_end)
+			end = round_up(end, PAGE_SIZE);
+
 		free_reserved_area((void *)start, (void *)end, 0, "initrd");
+	}
 }
 
 static int __init keepinitrd_setup(char *__unused)

commit a6583c7c8114c4850b57365e85da85e37d5fc568
Author: Ganapatrao Kulkarni <ganapatrao.kulkarni@caviumnetworks.com>
Date:   Tue Sep 16 18:53:54 2014 +0100

    arm64:mm: initialize max_mapnr using function set_max_mapnr
    
    Initializing max_mapnr using set_max_mapnr() helper function instead
    of direct reference. Also not adding PHYS_PFN_OFFSET to max_pfn,
    since it already contains it.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@caviumnetworks.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5472c2401876..271e654d852d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -256,7 +256,7 @@ static void __init free_unused_memmap(void)
  */
 void __init mem_init(void)
 {
-	max_mapnr   = pfn_to_page(max_pfn + PHYS_PFN_OFFSET) - mem_map;
+	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP
 	free_unused_memmap();

commit 0ceac9e094b065fe3fec19669740f338d3480498
Author: Mark Salter <msalter@redhat.com>
Date:   Mon Sep 8 13:01:08 2014 -0400

    efi/arm64: Fix fdt-related memory reservation
    
    Commit 86c8b27a01cf:
     "arm64: ignore DT memreserve entries when booting in UEFI mode
    
    prevents early_init_fdt_scan_reserved_mem() from being called for
    arm64 kernels booting via UEFI. This was done because the kernel
    will use the UEFI memory map to determine reserved memory regions.
    That approach has problems in that early_init_fdt_scan_reserved_mem()
    also reserves the FDT itself and any node-specific reserved memory.
    By chance of some kernel configs, the FDT may be overwritten before
    it can be unflattened and the kernel will fail to boot. More subtle
    problems will result if the FDT has node specific reserved memory
    which is not really reserved.
    
    This patch has the UEFI stub remove the memory reserve map entries
    from the FDT as it does with the memory nodes. This allows
    early_init_fdt_scan_reserved_mem() to be called unconditionally
    so that the other needed reservations are made.
    
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5472c2401876..a83061f37e43 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -149,8 +149,7 @@ void __init arm64_memblock_init(void)
 		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
 #endif
 
-	if (!efi_enabled(EFI_MEMMAP))
-		early_init_fdt_scan_reserved_mem();
+	early_init_fdt_scan_reserved_mem();
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA))

commit 86c8b27a01cf6c16fc159ade223cb2ccc70dc4b5
Author: Leif Lindholm <leif.lindholm@linaro.org>
Date:   Mon Jul 28 19:03:03 2014 +0100

    arm64: ignore DT memreserve entries when booting in UEFI mode
    
    UEFI provides its own method for marking regions to reserve, via the
    memory map which is also used to initialise memblock. So when using the
    UEFI memory map, ignore any memreserve entries present in the DT.
    
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Leif Lindholm <leif.lindholm@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5b4526ee3a01..5472c2401876 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -32,6 +32,7 @@
 #include <linux/of_fdt.h>
 #include <linux/dma-mapping.h>
 #include <linux/dma-contiguous.h>
+#include <linux/efi.h>
 
 #include <asm/fixmap.h>
 #include <asm/sections.h>
@@ -148,7 +149,8 @@ void __init arm64_memblock_init(void)
 		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
 #endif
 
-	early_init_fdt_scan_reserved_mem();
+	if (!efi_enabled(EFI_MEMMAP))
+		early_init_fdt_scan_reserved_mem();
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA))

commit 5167d09ffad5b16b574d35ce3047ed34caf1e837
Merge: 8533ce727188 ea1719672f59
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 12:31:53 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Once again, Catalin's off on holiday and I'm looking after the arm64
      tree.  Please can you pull the following arm64 updates for 3.17?
    
      Note that this branch also includes the new GICv3 driver (merged via a
      stable tag from Jason's irqchip tree), since there is a fix for older
      binutils on top.
    
      Changes include:
       - context tracking support (NO_HZ_FULL) which narrowly missed 3.16
       - vDSO layout rework following Andy's work on x86
       - TEXT_OFFSET fuzzing for bootloader testing
       - /proc/cpuinfo tidy-up
       - preliminary work to support 48-bit virtual addresses, but this is
         currently disabled until KVM has been ported to use it (the patches
         do, however, bring some nice clean-up)
       - boot-time CPU sanity checks (especially useful on heterogenous
         systems)
       - support for syscall auditing
       - support for CC_STACKPROTECTOR
       - defconfig updates"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (55 commits)
      arm64: add newline to I-cache policy string
      Revert "arm64: dmi: Add SMBIOS/DMI support"
      arm64: fpsimd: fix a typo in fpsimd_save_partial_state ENDPROC
      arm64: don't call break hooks for BRK exceptions from EL0
      arm64: defconfig: enable devtmpfs mount option
      arm64: vdso: fix build error when switching from LE to BE
      arm64: defconfig: add virtio support for running as a kvm guest
      arm64: gicv3: Allow GICv3 compilation with older binutils
      arm64: fix soft lockup due to large tlb flush range
      arm64/crypto: fix makefile rule for aes-glue-%.o
      arm64: Do not invoke audit_syscall_* functions if !CONFIG_AUDIT_SYSCALL
      arm64: Fix barriers used for page table modifications
      arm64: Add support for 48-bit VA space with 64KB page configuration
      arm64: asm/pgtable.h pmd/pud definitions clean-up
      arm64: Determine the vmalloc/vmemmap space at build time based on VA_BITS
      arm64: Clean up the initial page table creation in head.S
      arm64: Remove asm/pgtable-*level-types.h files
      arm64: Remove asm/pgtable-*level-hwdef.h files
      arm64: Convert bool ARM64_x_LEVELS to int ARM64_PGTABLE_LEVELS
      arm64: mm: Implement 4 levels of translation tables
      ...

commit 08375198b01001c0e43bdd580104b16b019a3754
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jul 16 17:42:43 2014 +0100

    arm64: Determine the vmalloc/vmemmap space at build time based on VA_BITS
    
    Rather than guessing what the maximum vmmemap space should be, this
    patch allows the calculation based on the VA_BITS and sizeof(struct
    page). The vmalloc space extends to the beginning of the vmemmap space.
    
    Since the virtual kernel memory layout now depends on the build
    configuration, this patch removes the detailed description in
    Documentation/arm64/memory.txt in favour of information printed during
    kernel booting.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 7f68804814a1..0b32504e280f 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -33,6 +33,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/dma-contiguous.h>
 
+#include <asm/fixmap.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>
@@ -256,26 +257,33 @@ void __init mem_init(void)
 
 #define MLK(b, t) b, t, ((t) - (b)) >> 10
 #define MLM(b, t) b, t, ((t) - (b)) >> 20
+#define MLG(b, t) b, t, ((t) - (b)) >> 30
 #define MLK_ROUNDUP(b, t) b, t, DIV_ROUND_UP(((t) - (b)), SZ_1K)
 
 	pr_notice("Virtual kernel memory layout:\n"
-		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld GB)\n"
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
-		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld GB maximum)\n"
+		  "              0x%16lx - 0x%16lx   (%6ld MB actual)\n"
 #endif
+		  "    PCI I/O : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+		  "    fixed   : 0x%16lx - 0x%16lx   (%6ld KB)\n"
 		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
 		  "    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n"
-		  "      .init : 0x%p" " - 0x%p" "   (%6ld kB)\n"
-		  "      .text : 0x%p" " - 0x%p" "   (%6ld kB)\n"
-		  "      .data : 0x%p" " - 0x%p" "   (%6ld kB)\n",
-		  MLM(VMALLOC_START, VMALLOC_END),
+		  "      .init : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		  "      .text : 0x%p" " - 0x%p" "   (%6ld KB)\n"
+		  "      .data : 0x%p" " - 0x%p" "   (%6ld KB)\n",
+		  MLG(VMALLOC_START, VMALLOC_END),
 #ifdef CONFIG_SPARSEMEM_VMEMMAP
+		  MLG((unsigned long)vmemmap,
+		      (unsigned long)vmemmap + VMEMMAP_SIZE),
 		  MLM((unsigned long)virt_to_page(PAGE_OFFSET),
 		      (unsigned long)virt_to_page(high_memory)),
 #endif
+		  MLM((unsigned long)PCI_IOBASE, (unsigned long)PCI_IOBASE + SZ_16M),
+		  MLK(FIXADDR_START, FIXADDR_TOP),
 		  MLM(MODULES_VADDR, MODULES_END),
 		  MLM(PAGE_OFFSET, (unsigned long)high_memory),
-
 		  MLK_ROUNDUP(__init_begin, __init_end),
 		  MLK_ROUNDUP(_text, _etext),
 		  MLK_ROUNDUP(_sdata, _edata));

commit d50314a6b0702c630c35b88148c1acb76d2e4ede
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 18 11:54:37 2014 +0100

    arm64: Create non-empty ZONE_DMA when DRAM starts above 4GB
    
    ZONE_DMA is created to allow 32-bit only devices to access memory in the
    absence of an IOMMU. On systems where the memory starts above 4GB, it is
    expected that some devices have a DMA offset hardwired to be able to
    access the bottom of the memory. Linux currently supports DT bindings
    for the DMA offsets but they are not (easily) available early during
    boot.
    
    This patch tries to guess a DMA offset and assumes that ZONE_DMA
    corresponds to the 32-bit mask above the start of DRAM.
    
    Fixes: 2d5a5612bc (arm64: Limit the CMA buffer to 32-bit if ZONE_DMA)
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Mark Salter <msalter@redhat.com>
    Tested-by: Mark Salter <msalter@redhat.com>
    Tested-by: Anup Patel <anup.patel@linaro.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f43db8a69262..e90c5426fe14 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -60,6 +60,17 @@ static int __init early_initrd(char *p)
 early_param("initrd", early_initrd);
 #endif
 
+/*
+ * Return the maximum physical address for ZONE_DMA (DMA_BIT_MASK(32)). It
+ * currently assumes that for memory starting above 4G, 32-bit devices will
+ * use a DMA offset.
+ */
+static phys_addr_t max_zone_dma_phys(void)
+{
+	phys_addr_t offset = memblock_start_of_DRAM() & GENMASK_ULL(63, 32);
+	return min(offset + (1ULL << 32), memblock_end_of_DRAM());
+}
+
 static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	struct memblock_region *reg;
@@ -70,9 +81,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
-		unsigned long max_dma_phys =
-			(unsigned long)(dma_to_phys(NULL, DMA_BIT_MASK(32)) + 1);
-		max_dma = max(min, min(max, max_dma_phys >> PAGE_SHIFT));
+		max_dma = PFN_DOWN(max_zone_dma_phys());
 		zone_size[ZONE_DMA] = max_dma - min;
 	}
 	zone_size[ZONE_NORMAL] = max - max_dma;
@@ -146,7 +155,7 @@ void __init arm64_memblock_init(void)
 
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA))
-		dma_phys_limit = dma_to_phys(NULL, DMA_BIT_MASK(32)) + 1;
+		dma_phys_limit = max_zone_dma_phys();
 	dma_contiguous_reserve(dma_phys_limit);
 
 	memblock_allow_resize();

commit bd00cd5f8c8c3c282bb1e1eac6a6679a4f808091
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jun 24 16:51:35 2014 +0100

    arm64: place initial page tables above the kernel
    
    Currently we place swapper_pg_dir and idmap_pg_dir below the kernel
    image, between PHYS_OFFSET and (PHYS_OFFSET + TEXT_OFFSET). However,
    bootloaders may use portions of this memory below the kernel and we do
    not parse the memory reservation list until after the MMU has been
    enabled. As such we may clobber some memory a bootloader wishes to have
    preserved.
    
    To enable the use of all of this memory by bootloaders (when the
    required memory reservations are communicated to the kernel) it is
    necessary to move our initial page tables elsewhere. As we currently
    have an effectively unbound requirement for memory at the end of the
    kernel image for .bss, we can place the page tables here.
    
    This patch moves the initial page table to the end of the kernel image,
    after the BSS. As they do not consist of any initialised data they will
    be stripped from the kernel Image as with the BSS. The BSS clearing
    routine is updated to stop at __bss_stop rather than _end so as to not
    clobber the page tables, and memory reservations made redundant by the
    new organisation are removed.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f43db8a69262..7f68804814a1 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -128,20 +128,16 @@ void __init arm64_memblock_init(void)
 {
 	phys_addr_t dma_phys_limit = 0;
 
-	/* Register the kernel text, kernel data and initrd with memblock */
+	/*
+	 * Register the kernel text, kernel data, initrd, and initial
+	 * pagetables with memblock.
+	 */
 	memblock_reserve(__pa(_text), _end - _text);
 #ifdef CONFIG_BLK_DEV_INITRD
 	if (initrd_start)
 		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
 #endif
 
-	/*
-	 * Reserve the page tables.  These are already in use,
-	 * and can only be in node 0.
-	 */
-	memblock_reserve(__pa(swapper_pg_dir), SWAPPER_DIR_SIZE);
-	memblock_reserve(__pa(idmap_pg_dir), IDMAP_DIR_SIZE);
-
 	early_init_fdt_scan_reserved_mem();
 
 	/* 4GB maximum for 32-bit only capable devices */

commit 2d5a5612bceda8edd25b29f363c4e2c6cda28bab
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jun 13 13:41:20 2014 +0100

    arm64: Limit the CMA buffer to 32-bit if ZONE_DMA
    
    When the CMA buffer is allocated, it is too early to know whether
    devices will require ZONE_DMA memory. This patch limits the CMA buffer
    to (DMA_BIT_MASK(32) + 1) if CONFIG_ZONE_DMA is enabled.
    
    In addition, it computes the dma_to_phys(DMA_BIT_MASK(32)) before the
    increment (no current functional change).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 091d428d64ac..f43db8a69262 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -71,7 +71,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 	/* 4GB maximum for 32-bit only capable devices */
 	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
 		unsigned long max_dma_phys =
-			(unsigned long)dma_to_phys(NULL, DMA_BIT_MASK(32) + 1);
+			(unsigned long)(dma_to_phys(NULL, DMA_BIT_MASK(32)) + 1);
 		max_dma = max(min, min(max, max_dma_phys >> PAGE_SHIFT));
 		zone_size[ZONE_DMA] = max_dma - min;
 	}
@@ -126,6 +126,8 @@ static void arm64_memory_present(void)
 
 void __init arm64_memblock_init(void)
 {
+	phys_addr_t dma_phys_limit = 0;
+
 	/* Register the kernel text, kernel data and initrd with memblock */
 	memblock_reserve(__pa(_text), _end - _text);
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -141,7 +143,11 @@ void __init arm64_memblock_init(void)
 	memblock_reserve(__pa(idmap_pg_dir), IDMAP_DIR_SIZE);
 
 	early_init_fdt_scan_reserved_mem();
-	dma_contiguous_reserve(0);
+
+	/* 4GB maximum for 32-bit only capable devices */
+	if (IS_ENABLED(CONFIG_ZONE_DMA))
+		dma_phys_limit = dma_to_phys(NULL, DMA_BIT_MASK(32)) + 1;
+	dma_contiguous_reserve(dma_phys_limit);
 
 	memblock_allow_resize();
 	memblock_dump_all();

commit d1552ce449eb0a8d2f0bd6599da3a8a3d7f77a84
Author: Rob Herring <robh@kernel.org>
Date:   Tue Apr 1 22:46:48 2014 -0500

    of/fdt: move memreserve and dtb memory reservations into core
    
    Move the /memreserve/ processing and dtb memory reservations into
    early_init_fdt_scan_reserved_mem. This converts arm, arm64, and powerpc
    as they are the only users of early_init_fdt_scan_reserved_mem.
    
    memblock_reserve is safe to call on the same region twice, so the
    reservation check for the dtb in powerpc 32-bit reservations is safe to
    remove.
    
    Signed-off-by: Rob Herring <robh@kernel.org>
    Tested-by: Michal Simek <michal.simek@xilinx.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Tested-by: Grant Likely <grant.likely@linaro.org>
    Tested-by: Stephen Chivers <schivers@csc.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 51d5352e6ad5..091d428d64ac 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -126,8 +126,6 @@ static void arm64_memory_present(void)
 
 void __init arm64_memblock_init(void)
 {
-	u64 *reserve_map, base, size;
-
 	/* Register the kernel text, kernel data and initrd with memblock */
 	memblock_reserve(__pa(_text), _end - _text);
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -142,25 +140,6 @@ void __init arm64_memblock_init(void)
 	memblock_reserve(__pa(swapper_pg_dir), SWAPPER_DIR_SIZE);
 	memblock_reserve(__pa(idmap_pg_dir), IDMAP_DIR_SIZE);
 
-	/* Reserve the dtb region */
-	memblock_reserve(virt_to_phys(initial_boot_params),
-			 be32_to_cpu(initial_boot_params->totalsize));
-
-	/*
-	 * Process the reserve map.  This will probably overlap the initrd
-	 * and dtb locations which are already reserved, but overlapping
-	 * doesn't hurt anything
-	 */
-	reserve_map = ((void*)initial_boot_params) +
-			be32_to_cpu(initial_boot_params->off_mem_rsvmap);
-	while (1) {
-		base = be64_to_cpup(reserve_map++);
-		size = be64_to_cpup(reserve_map++);
-		if (!size)
-			break;
-		memblock_reserve(base, size);
-	}
-
 	early_init_fdt_scan_reserved_mem();
 	dma_contiguous_reserve(0);
 

commit b9f2b21a32906a47c220b5167b88869f2c90f1c4
Merge: 70f6c087573e a0e7398357f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 2 14:27:15 2014 -0700

    Merge tag 'dt-for-linus' of git://git.secretlab.ca/git/linux
    
    Pull devicetree changes from Grant Likely:
     "Updates to devicetree core code.  This branch contains the following
      notable changes:
    
       - add reserved memory binding
       - make struct device_node a kobject and remove legacy
         /proc/device-tree
       - ePAPR conformance fixes
       - update in-kernel DTC copy to version v1.4.0
       - preparatory changes for dynamic device tree overlays
       - minor bug fixes and documentation changes
    
      The most significant change in this branch is the conversion of struct
      device_node to be a kobject that is exposed via sysfs and removal of
      the old /proc/device-tree code.  This simplifies the device tree
      handling code and tightens up the lifecycle on device tree nodes.
    
      [updated: added fix for dangling select PROC_DEVICETREE]"
    
    * tag 'dt-for-linus' of git://git.secretlab.ca/git/linux: (29 commits)
      dt: Remove dangling "select PROC_DEVICETREE"
      of: Add support for ePAPR "stdout-path" property
      of: device_node kobject lifecycle fixes
      of: only scan for reserved mem when fdt present
      powerpc: add support for reserved memory defined by device tree
      arm64: add support for reserved memory defined by device tree
      of: add missing major vendors
      of: add vendor prefix for SMSC
      of: remove /proc/device-tree
      of/selftest: Add self tests for manipulation of properties
      of: Make device nodes kobjects so they show up in sysfs
      arm: add support for reserved memory defined by device tree
      drivers: of: add support for custom reserved memory drivers
      drivers: of: add initialization code for dynamic reserved memory
      drivers: of: add initialization code for static reserved memory
      of: document bindings for reserved-memory nodes
      Revert "of: fix of_update_property()"
      kbuild: dtbs_install: new make target
      ARM: mvebu: Allows to get the SoC ID even without PCI enabled
      of: Allows to use the PCI translator without the PCI core
      ...

commit 9bf14b7c540ae9ca7747af3a0c0d8470ef77b6ce
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 28 14:42:55 2014 +0100

    arm64: add support for reserved memory defined by device tree
    
    Enable reserved memory initialization from device tree.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Grant Likely <grant.likely@linaro.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d0b4c2efda90..3fb8d50dfdaa 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -160,6 +160,7 @@ void __init arm64_memblock_init(void)
 		memblock_reserve(base, size);
 	}
 
+	early_init_fdt_scan_reserved_mem();
 	dma_contiguous_reserve(0);
 
 	memblock_allow_resize();

commit 3690951fc6d42f3a0903987677d0e592c49dd8db
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 27 12:24:57 2014 +0000

    arm64: Use swiotlb late initialisation
    
    Since arm64 does not support ISA, there is no need for early swiotlb
    initialisation. This patch switches the DMA mapping code to
    swiotlb_tlb_late_init_with_default_size(). A side effect of this is that
    GFP_DMA is used for the swiotlb buffer and devices with a 32-bit
    coherent mask are correctly supported.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index a61a4d560d12..88627c450a6c 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -262,8 +262,6 @@ static void __init free_unused_memmap(void)
  */
 void __init mem_init(void)
 {
-	arm64_swiotlb_init();
-
 	max_mapnr   = pfn_to_page(max_pfn + PHYS_PFN_OFFSET) - mem_map;
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP

commit 19e7640d1f2302c20df2733e3e3df49acb17189e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 27 12:09:22 2014 +0000

    arm64: Replace ZONE_DMA32 with ZONE_DMA
    
    On arm64 we do not have two DMA zones, so it does not make sense to
    implement ZONE_DMA32. This patch changes ZONE_DMA32 with ZONE_DMA, the
    latter covering 32-bit dma address space to honour GFP_DMA allocations.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index d0b4c2efda90..a61a4d560d12 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -30,6 +30,7 @@
 #include <linux/memblock.h>
 #include <linux/sort.h>
 #include <linux/of_fdt.h>
+#include <linux/dma-mapping.h>
 #include <linux/dma-contiguous.h>
 
 #include <asm/sections.h>
@@ -59,22 +60,22 @@ static int __init early_initrd(char *p)
 early_param("initrd", early_initrd);
 #endif
 
-#define MAX_DMA32_PFN ((4UL * 1024 * 1024 * 1024) >> PAGE_SHIFT)
-
 static void __init zone_sizes_init(unsigned long min, unsigned long max)
 {
 	struct memblock_region *reg;
 	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
-	unsigned long max_dma32 = min;
+	unsigned long max_dma = min;
 
 	memset(zone_size, 0, sizeof(zone_size));
 
-#ifdef CONFIG_ZONE_DMA32
 	/* 4GB maximum for 32-bit only capable devices */
-	max_dma32 = max(min, min(max, MAX_DMA32_PFN));
-	zone_size[ZONE_DMA32] = max_dma32 - min;
-#endif
-	zone_size[ZONE_NORMAL] = max - max_dma32;
+	if (IS_ENABLED(CONFIG_ZONE_DMA)) {
+		unsigned long max_dma_phys =
+			(unsigned long)dma_to_phys(NULL, DMA_BIT_MASK(32) + 1);
+		max_dma = max(min, min(max, max_dma_phys >> PAGE_SHIFT));
+		zone_size[ZONE_DMA] = max_dma - min;
+	}
+	zone_size[ZONE_NORMAL] = max - max_dma;
 
 	memcpy(zhole_size, zone_size, sizeof(zhole_size));
 
@@ -84,15 +85,15 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 
 		if (start >= max)
 			continue;
-#ifdef CONFIG_ZONE_DMA32
-		if (start < max_dma32) {
-			unsigned long dma_end = min(end, max_dma32);
-			zhole_size[ZONE_DMA32] -= dma_end - start;
+
+		if (IS_ENABLED(CONFIG_ZONE_DMA) && start < max_dma) {
+			unsigned long dma_end = min(end, max_dma);
+			zhole_size[ZONE_DMA] -= dma_end - start;
 		}
-#endif
-		if (end > max_dma32) {
+
+		if (end > max_dma) {
 			unsigned long normal_end = min(end, max);
-			unsigned long normal_start = max(start, max_dma32);
+			unsigned long normal_start = max(start, max_dma);
 			zhole_size[ZONE_NORMAL] -= normal_end - normal_start;
 		}
 	}

commit 6ac2104debc235b745265b64d610237a6833fd53
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu Dec 12 19:28:33 2013 +0000

    arm64: Enable CMA
    
    arm64 bit targets need the features CMA provides. Add the appropriate
    hooks, header files, and Kconfig to allow this to happen.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 0cb8742de4f2..d0b4c2efda90 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -30,6 +30,7 @@
 #include <linux/memblock.h>
 #include <linux/sort.h>
 #include <linux/of_fdt.h>
+#include <linux/dma-contiguous.h>
 
 #include <asm/sections.h>
 #include <asm/setup.h>
@@ -159,6 +160,8 @@ void __init arm64_memblock_init(void)
 		memblock_reserve(base, size);
 	}
 
+	dma_contiguous_reserve(0);
+
 	memblock_allow_resize();
 	memblock_dump_all();
 }

commit e2d1c994f7194e933236bd874b2d7f31b678bb94
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Sat Sep 7 14:57:42 2013 -0500

    arm64: remove unnecessary prom.h include
    
    Remove unnecessary prom.h include in preparation to make prom.h optional.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Grant Likely <grant.likely@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 8261f4e60795..0cb8742de4f2 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -31,7 +31,6 @@
 #include <linux/sort.h>
 #include <linux/of_fdt.h>
 
-#include <asm/prom.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>

commit ec2eaa73b3d21776f46797a2eef983d7be09a964
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Sun Aug 25 16:47:48 2013 -0500

    arm64: set initrd_start/initrd_end for fdt scan
    
    In order to unify the initrd scanning for DT across architectures, make
    arm64 use initrd_start and initrd_end instead of the physical addresses.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index de2de5db628d..8261f4e60795 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -39,17 +39,9 @@
 
 #include "mm.h"
 
-static unsigned long phys_initrd_start __initdata = 0;
-static unsigned long phys_initrd_size __initdata = 0;
-
 phys_addr_t memstart_addr __read_mostly = 0;
 
-void __init early_init_dt_setup_initrd_arch(u64 start, u64 end)
-{
-	phys_initrd_start = start;
-	phys_initrd_size = end - start;
-}
-
+#ifdef CONFIG_BLK_DEV_INITRD
 static int __init early_initrd(char *p)
 {
 	unsigned long start, size;
@@ -59,12 +51,13 @@ static int __init early_initrd(char *p)
 	if (*endp == ',') {
 		size = memparse(endp + 1, NULL);
 
-		phys_initrd_start = start;
-		phys_initrd_size = size;
+		initrd_start = (unsigned long)__va(start);
+		initrd_end = (unsigned long)__va(start + size);
 	}
 	return 0;
 }
 early_param("initrd", early_initrd);
+#endif
 
 #define MAX_DMA32_PFN ((4UL * 1024 * 1024 * 1024) >> PAGE_SHIFT)
 
@@ -137,13 +130,8 @@ void __init arm64_memblock_init(void)
 	/* Register the kernel text, kernel data and initrd with memblock */
 	memblock_reserve(__pa(_text), _end - _text);
 #ifdef CONFIG_BLK_DEV_INITRD
-	if (phys_initrd_size) {
-		memblock_reserve(phys_initrd_start, phys_initrd_size);
-
-		/* Now convert initrd to virtual addresses */
-		initrd_start = __phys_to_virt(phys_initrd_start);
-		initrd_end = initrd_start + phys_initrd_size;
-	}
+	if (initrd_start)
+		memblock_reserve(__virt_to_phys(initrd_start), initrd_end - initrd_start);
 #endif
 
 	/*

commit 374d5c9964c10373ba39bbe934f4262eb87d7114
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Mon Jul 1 14:20:35 2013 -0400

    of: Specify initrd location using 64-bit
    
    On some PAE architectures, the entire range of physical memory could reside
    outside the 32-bit limit.  These systems need the ability to specify the
    initrd location using 64-bit numbers.
    
    This patch globally modifies the early_init_dt_setup_initrd_arch() function to
    use 64-bit numbers instead of the current unsigned long.
    
    There has been quite a bit of debate about whether to use u64 or phys_addr_t.
    It was concluded to stick to u64 to be consistent with rest of the device
    tree code. As summarized by Geert, "The address to load the initrd is decided
    by the bootloader/user and set at that point later in time. The dtb should not
    be tied to the kernel you are booting"
    
    More details on the discussion can be found here:
    https://lkml.org/lkml/2013/6/20/690
    https://lkml.org/lkml/2012/9/13/544
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Jean-Christophe PLAGNIOL-VILLARD <plagnioj@jcrosoft.com>
    Signed-off-by: Grant Likely <grant.likely@linaro.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 67e8d7ce3fe7..de2de5db628d 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -44,8 +44,7 @@ static unsigned long phys_initrd_size __initdata = 0;
 
 phys_addr_t memstart_addr __read_mostly = 0;
 
-void __init early_init_dt_setup_initrd_arch(unsigned long start,
-					    unsigned long end)
+void __init early_init_dt_setup_initrd_arch(u64 start, u64 end)
 {
 	phys_initrd_start = start;
 	phys_initrd_size = end - start;

commit 6879ea83c6d14e4f21bf48b5780ed549197c668b
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:04:02 2013 -0700

    mm/microblaze: prepare for removing num_physpages and simplify mem_init()
    
    Prepare for removing num_physpages and simplify mem_init().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index b16c778ea0de..67e8d7ce3fe7 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -282,7 +282,7 @@ void __init mem_init(void)
 	/* this will put all unused low memory onto the freelists */
 	free_all_bootmem();
 
-	mem_init_print_info();
+	mem_init_print_info(NULL);
 
 #define MLK(b, t) b, t, ((t) - (b)) >> 10
 #define MLM(b, t) b, t, ((t) - (b)) >> 20

commit bee4ebd117ac943308dd57bc0a5a3cc539f0eaac
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:03:49 2013 -0700

    mm/ARM64: prepare for removing num_physpages and simplify mem_init()
    
    Prepare for removing num_physpages and simplify mem_init().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 93de98afedd7..b16c778ea0de 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -272,59 +272,17 @@ static void __init free_unused_memmap(void)
  */
 void __init mem_init(void)
 {
-	unsigned long reserved_pages, free_pages;
-	struct memblock_region *reg;
-
 	arm64_swiotlb_init();
 
 	max_mapnr   = pfn_to_page(max_pfn + PHYS_PFN_OFFSET) - mem_map;
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP
-	/* this will put all unused low memory onto the freelists */
 	free_unused_memmap();
 #endif
-
+	/* this will put all unused low memory onto the freelists */
 	free_all_bootmem();
 
-	reserved_pages = free_pages = 0;
-
-	for_each_memblock(memory, reg) {
-		unsigned int pfn1, pfn2;
-		struct page *page, *end;
-
-		pfn1 = __phys_to_pfn(reg->base);
-		pfn2 = pfn1 + __phys_to_pfn(reg->size);
-
-		page = pfn_to_page(pfn1);
-		end  = pfn_to_page(pfn2 - 1) + 1;
-
-		do {
-			if (PageReserved(page))
-				reserved_pages++;
-			else if (!page_count(page))
-				free_pages++;
-			page++;
-		} while (page < end);
-	}
-
-	/*
-	 * Since our memory may not be contiguous, calculate the real number
-	 * of pages we have in this system.
-	 */
-	pr_info("Memory:");
-	num_physpages = 0;
-	for_each_memblock(memory, reg) {
-		unsigned long pages = memblock_region_memory_end_pfn(reg) -
-			memblock_region_memory_base_pfn(reg);
-		num_physpages += pages;
-		printk(" %ldMB", pages >> (20 - PAGE_SHIFT));
-	}
-	printk(" = %luMB total\n", num_physpages >> (20 - PAGE_SHIFT));
-
-	pr_notice("Memory: %luk/%luk available, %luk reserved\n",
-		  nr_free_pages() << (PAGE_SHIFT-10),
-		  free_pages << (PAGE_SHIFT-10),
-		  reserved_pages << (PAGE_SHIFT-10));
+	mem_init_print_info();
 
 #define MLK(b, t) b, t, ((t) - (b)) >> 10
 #define MLM(b, t) b, t, ((t) - (b)) >> 20
@@ -366,7 +324,7 @@ void __init mem_init(void)
 	BUILD_BUG_ON(TASK_SIZE_64			> MODULES_VADDR);
 	BUG_ON(TASK_SIZE_64				> MODULES_VADDR);
 
-	if (PAGE_SIZE >= 16384 && num_physpages <= 128) {
+	if (PAGE_SIZE >= 16384 && get_num_physpages() <= 128) {
 		extern int sysctl_overcommit_memory;
 		/*
 		 * On a machine this small we won't get anywhere without

commit 0c988534737a358fdff42fcce78f0ff1a12dbfc5
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:03:24 2013 -0700

    mm: concentrate modification of totalram_pages into the mm core
    
    Concentrate code to modify totalram_pages into the mm core, so the arch
    memory initialized code doesn't need to take care of it.  With these
    changes applied, only following functions from mm core modify global
    variable totalram_pages: free_bootmem_late(), free_all_bootmem(),
    free_all_bootmem_node(), adjust_managed_page_count().
    
    With this patch applied, it will be much more easier for us to keep
    totalram_pages and zone->managed_pages in consistence.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index a398eb9018bb..93de98afedd7 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -284,7 +284,7 @@ void __init mem_init(void)
 	free_unused_memmap();
 #endif
 
-	totalram_pages += free_all_bootmem();
+	free_all_bootmem();
 
 	reserved_pages = free_pages = 0;
 

commit 9af5b80765e4c30b8eba8218724459226fa5e81f
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:02:54 2013 -0700

    mm/ARM64: kill poison_init_mem()
    
    Use free_reserved_area() to poison initmem memory pages and kill
    poison_init_mem() on ARM64.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 997c6345cdd6..a398eb9018bb 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -197,14 +197,6 @@ void __init bootmem_init(void)
 	max_pfn = max_low_pfn = max;
 }
 
-/*
- * Poison init memory with an undefined instruction (0x0).
- */
-static inline void poison_init_mem(void *s, size_t count)
-{
-	memset(s, 0, count);
-}
-
 #ifndef CONFIG_SPARSEMEM_VMEMMAP
 static inline void free_memmap(unsigned long start_pfn, unsigned long end_pfn)
 {
@@ -386,8 +378,7 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
-	poison_init_mem(__init_begin, __init_end - __init_begin);
-	free_initmem_default(-1);
+	free_initmem_default(0);
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -396,10 +387,8 @@ static int keep_initrd;
 
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (!keep_initrd) {
-		poison_init_mem((void *)start, PAGE_ALIGN(end) - start);
-		free_reserved_area((void *)start, (void *)end, -1, "initrd");
-	}
+	if (!keep_initrd)
+		free_reserved_area((void *)start, (void *)end, 0, "initrd");
 }
 
 static int __init keepinitrd_setup(char *__unused)

commit dbe67df4ba78c79db547c7864e1120981c144c97
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:02:51 2013 -0700

    mm: enhance free_reserved_area() to support poisoning memory with zero
    
    Address more review comments from last round of code review.
    1) Enhance free_reserved_area() to support poisoning freed memory with
       pattern '0'. This could be used to get rid of poison_init_mem()
       on ARM64.
    2) A previous patch has disabled memory poison for initmem on s390
       by mistake, so restore to the original behavior.
    3) Remove redundant PAGE_ALIGN() when calling free_reserved_area().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 6041e4008a83..997c6345cdd6 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -387,7 +387,7 @@ void __init mem_init(void)
 void free_initmem(void)
 {
 	poison_init_mem(__init_begin, __init_end - __init_begin);
-	free_initmem_default(0);
+	free_initmem_default(-1);
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -398,7 +398,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 {
 	if (!keep_initrd) {
 		poison_init_mem((void *)start, PAGE_ALIGN(end) - start);
-		free_reserved_area((void *)start, (void *)end, 0, "initrd");
+		free_reserved_area((void *)start, (void *)end, -1, "initrd");
 	}
 }
 

commit 11199692d83dd3fe1511203024fb9853d176ec4c
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:02:48 2013 -0700

    mm: change signature of free_reserved_area() to fix building warnings
    
    Change signature of free_reserved_area() according to Russell King's
    suggestion to fix following build warnings:
    
      arch/arm/mm/init.c: In function 'mem_init':
      arch/arm/mm/init.c:603:2: warning: passing argument 1 of 'free_reserved_area' makes integer from pointer without a cast [enabled by default]
        free_reserved_area(__va(PHYS_PFN_OFFSET), swapper_pg_dir, 0, NULL);
        ^
      In file included from include/linux/mman.h:4:0,
                       from arch/arm/mm/init.c:15:
      include/linux/mm.h:1301:22: note: expected 'long unsigned int' but argument is of type 'void *'
       extern unsigned long free_reserved_area(unsigned long start, unsigned long end,
    
       mm/page_alloc.c: In function 'free_reserved_area':
    >> mm/page_alloc.c:5134:3: warning: passing argument 1 of 'virt_to_phys' makes pointer from integer without a cast [enabled by default]
       In file included from arch/mips/include/asm/page.h:49:0,
                        from include/linux/mmzone.h:20,
                        from include/linux/gfp.h:4,
                        from include/linux/mm.h:8,
                        from mm/page_alloc.c:18:
       arch/mips/include/asm/io.h:119:29: note: expected 'const volatile void *' but argument is of type 'long unsigned int'
       mm/page_alloc.c: In function 'free_area_init_nodes':
       mm/page_alloc.c:5030:34: warning: array subscript is below array bounds [-Warray-bounds]
    
    Also address some minor code review comments.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index f497ca77925a..6041e4008a83 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -398,7 +398,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 {
 	if (!keep_initrd) {
 		poison_init_mem((void *)start, PAGE_ALIGN(end) - start);
-		free_reserved_area(start, end, 0, "initrd");
+		free_reserved_area((void *)start, (void *)end, 0, "initrd");
 	}
 }
 

commit 83db0384a92ac4e52dccd275bc1a58dca86d629d
Author: Jiang Liu <liuj97@gmail.com>
Date:   Mon Apr 29 15:06:26 2013 -0700

    mm/ARM: use common help functions to free reserved pages
    
    Use common help functions to free reserved pages.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 800aac306a08..f497ca77925a 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -197,24 +197,6 @@ void __init bootmem_init(void)
 	max_pfn = max_low_pfn = max;
 }
 
-static inline int free_area(unsigned long pfn, unsigned long end, char *s)
-{
-	unsigned int pages = 0, size = (end - pfn) << (PAGE_SHIFT - 10);
-
-	for (; pfn < end; pfn++) {
-		struct page *page = pfn_to_page(pfn);
-		ClearPageReserved(page);
-		init_page_count(page);
-		__free_page(page);
-		pages++;
-	}
-
-	if (size && s)
-		pr_info("Freeing %s memory: %dK\n", s, size);
-
-	return pages;
-}
-
 /*
  * Poison init memory with an undefined instruction (0x0).
  */
@@ -405,9 +387,7 @@ void __init mem_init(void)
 void free_initmem(void)
 {
 	poison_init_mem(__init_begin, __init_end - __init_begin);
-	totalram_pages += free_area(__phys_to_pfn(__pa(__init_begin)),
-				    __phys_to_pfn(__pa(__init_end)),
-				    "init");
+	free_initmem_default(0);
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -418,9 +398,7 @@ void free_initrd_mem(unsigned long start, unsigned long end)
 {
 	if (!keep_initrd) {
 		poison_init_mem((void *)start, PAGE_ALIGN(end) - start);
-		totalram_pages += free_area(__phys_to_pfn(__pa(start)),
-					    __phys_to_pfn(__pa(end)),
-					    "initrd");
+		free_reserved_area(start, end, 0, "initrd");
 	}
 }
 

commit 938edf5c04202b59b8ff01a4033e9413646b105b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Nov 12 19:19:35 2012 +0000

    arm64: mm: update max_dma32 before calculating size of NORMAL zone
    
    Commit f483a853b0b9 ("arm64: mm: fix booting on systems with no memory
    below 4GB") sets max_dma32 to the minimum of the maximum pfn and
    MAX_DMA32_PFN. This value is later used as the base of the NORMAL zone,
    which is incorrect when MAX_DMA32_PFN is below the minimum pfn (i.e. all
    memory is above 4GB).
    
    This patch fixes the problem by ensuring that max_dma32 is always set to
    the end of the DMA32 zone.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 4cd28931dba9..800aac306a08 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -79,8 +79,8 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 
 #ifdef CONFIG_ZONE_DMA32
 	/* 4GB maximum for 32-bit only capable devices */
-	max_dma32 = min(max, MAX_DMA32_PFN);
-	zone_size[ZONE_DMA32] = max(min, max_dma32) - min;
+	max_dma32 = max(min, min(max, MAX_DMA32_PFN));
+	zone_size[ZONE_DMA32] = max_dma32 - min;
 #endif
 	zone_size[ZONE_NORMAL] = max - max_dma32;
 

commit f483a853b0b932a1d75eb27a1dcbd732862260db
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Nov 8 16:00:16 2012 +0000

    arm64: mm: fix booting on systems with no memory below 4GB
    
    Booting on a system with all of its memory above the 4GB boundary breaks
    for two reasons:
    
            (1) We still try to create a non-empty DMA32 zone
            (2) no-bootmem limits allocations to 0xffffffff
    
    This patch fixes these issues for ARM64.
    
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index efbf7df05d3f..4cd28931dba9 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -80,7 +80,7 @@ static void __init zone_sizes_init(unsigned long min, unsigned long max)
 #ifdef CONFIG_ZONE_DMA32
 	/* 4GB maximum for 32-bit only capable devices */
 	max_dma32 = min(max, MAX_DMA32_PFN);
-	zone_size[ZONE_DMA32] = max_dma32 - min;
+	zone_size[ZONE_DMA32] = max(min, max_dma32) - min;
 #endif
 	zone_size[ZONE_NORMAL] = max - max_dma32;
 

commit 27222a3d2bbb40f80af6abf0cefea1b27125409e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Oct 3 14:35:18 2012 +0100

    arm64: Call swiotlb_init() instead of swiotlb_init_with_default_size()
    
    Following commit 74838b7 (swiotlb: add the late swiotlb initialization
    function with iotlb memory) the swiotlb_init_with_default_size() is a
    static function. This patch changes the arm64 code to call
    swiotlb_init() instead and use the default size of 64MB. It is assumed
    that AArch64 platforms have enough RAM to afford the pre-allocated
    swiotlb memory. It also removes the #ifdef around this call since
    CONFIG_SWIOTLB is always enabled.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
index 5f719ba949bc..efbf7df05d3f 100644
--- a/arch/arm64/mm/init.c
+++ b/arch/arm64/mm/init.c
@@ -301,10 +301,7 @@ void __init mem_init(void)
 	unsigned long reserved_pages, free_pages;
 	struct memblock_region *reg;
 
-#if CONFIG_SWIOTLB
-	extern void __init arm64_swiotlb_init(size_t max_size);
-	arm64_swiotlb_init(max_pfn << (PAGE_SHIFT - 1));
-#endif
+	arm64_swiotlb_init();
 
 	max_mapnr   = pfn_to_page(max_pfn + PHYS_PFN_OFFSET) - mem_map;
 

commit c1cc1552616d0f354d040823151e61634e7ad01f
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: MMU initialisation
    
    This patch contains the initialisation of the memory blocks, MMU
    attributes and the memory map. Only five memory types are defined:
    Device nGnRnE (equivalent to Strongly Ordered), Device nGnRE (classic
    Device memory), Device GRE, Normal Non-cacheable and Normal Cacheable.
    Cache policies are supported via the memory attributes register
    (MAIR_EL1) and only affect the Normal Cacheable mappings.
    
    This patch also adds the SPARSEMEM_VMEMMAP initialisation.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
new file mode 100644
index 000000000000..5f719ba949bc
--- /dev/null
+++ b/arch/arm64/mm/init.c
@@ -0,0 +1,437 @@
+/*
+ * Based on arch/arm/mm/init.c
+ *
+ * Copyright (C) 1995-2005 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/kernel.h>
+#include <linux/export.h>
+#include <linux/errno.h>
+#include <linux/swap.h>
+#include <linux/init.h>
+#include <linux/bootmem.h>
+#include <linux/mman.h>
+#include <linux/nodemask.h>
+#include <linux/initrd.h>
+#include <linux/gfp.h>
+#include <linux/memblock.h>
+#include <linux/sort.h>
+#include <linux/of_fdt.h>
+
+#include <asm/prom.h>
+#include <asm/sections.h>
+#include <asm/setup.h>
+#include <asm/sizes.h>
+#include <asm/tlb.h>
+
+#include "mm.h"
+
+static unsigned long phys_initrd_start __initdata = 0;
+static unsigned long phys_initrd_size __initdata = 0;
+
+phys_addr_t memstart_addr __read_mostly = 0;
+
+void __init early_init_dt_setup_initrd_arch(unsigned long start,
+					    unsigned long end)
+{
+	phys_initrd_start = start;
+	phys_initrd_size = end - start;
+}
+
+static int __init early_initrd(char *p)
+{
+	unsigned long start, size;
+	char *endp;
+
+	start = memparse(p, &endp);
+	if (*endp == ',') {
+		size = memparse(endp + 1, NULL);
+
+		phys_initrd_start = start;
+		phys_initrd_size = size;
+	}
+	return 0;
+}
+early_param("initrd", early_initrd);
+
+#define MAX_DMA32_PFN ((4UL * 1024 * 1024 * 1024) >> PAGE_SHIFT)
+
+static void __init zone_sizes_init(unsigned long min, unsigned long max)
+{
+	struct memblock_region *reg;
+	unsigned long zone_size[MAX_NR_ZONES], zhole_size[MAX_NR_ZONES];
+	unsigned long max_dma32 = min;
+
+	memset(zone_size, 0, sizeof(zone_size));
+
+#ifdef CONFIG_ZONE_DMA32
+	/* 4GB maximum for 32-bit only capable devices */
+	max_dma32 = min(max, MAX_DMA32_PFN);
+	zone_size[ZONE_DMA32] = max_dma32 - min;
+#endif
+	zone_size[ZONE_NORMAL] = max - max_dma32;
+
+	memcpy(zhole_size, zone_size, sizeof(zhole_size));
+
+	for_each_memblock(memory, reg) {
+		unsigned long start = memblock_region_memory_base_pfn(reg);
+		unsigned long end = memblock_region_memory_end_pfn(reg);
+
+		if (start >= max)
+			continue;
+#ifdef CONFIG_ZONE_DMA32
+		if (start < max_dma32) {
+			unsigned long dma_end = min(end, max_dma32);
+			zhole_size[ZONE_DMA32] -= dma_end - start;
+		}
+#endif
+		if (end > max_dma32) {
+			unsigned long normal_end = min(end, max);
+			unsigned long normal_start = max(start, max_dma32);
+			zhole_size[ZONE_NORMAL] -= normal_end - normal_start;
+		}
+	}
+
+	free_area_init_node(0, zone_size, min, zhole_size);
+}
+
+#ifdef CONFIG_HAVE_ARCH_PFN_VALID
+int pfn_valid(unsigned long pfn)
+{
+	return memblock_is_memory(pfn << PAGE_SHIFT);
+}
+EXPORT_SYMBOL(pfn_valid);
+#endif
+
+#ifndef CONFIG_SPARSEMEM
+static void arm64_memory_present(void)
+{
+}
+#else
+static void arm64_memory_present(void)
+{
+	struct memblock_region *reg;
+
+	for_each_memblock(memory, reg)
+		memory_present(0, memblock_region_memory_base_pfn(reg),
+			       memblock_region_memory_end_pfn(reg));
+}
+#endif
+
+void __init arm64_memblock_init(void)
+{
+	u64 *reserve_map, base, size;
+
+	/* Register the kernel text, kernel data and initrd with memblock */
+	memblock_reserve(__pa(_text), _end - _text);
+#ifdef CONFIG_BLK_DEV_INITRD
+	if (phys_initrd_size) {
+		memblock_reserve(phys_initrd_start, phys_initrd_size);
+
+		/* Now convert initrd to virtual addresses */
+		initrd_start = __phys_to_virt(phys_initrd_start);
+		initrd_end = initrd_start + phys_initrd_size;
+	}
+#endif
+
+	/*
+	 * Reserve the page tables.  These are already in use,
+	 * and can only be in node 0.
+	 */
+	memblock_reserve(__pa(swapper_pg_dir), SWAPPER_DIR_SIZE);
+	memblock_reserve(__pa(idmap_pg_dir), IDMAP_DIR_SIZE);
+
+	/* Reserve the dtb region */
+	memblock_reserve(virt_to_phys(initial_boot_params),
+			 be32_to_cpu(initial_boot_params->totalsize));
+
+	/*
+	 * Process the reserve map.  This will probably overlap the initrd
+	 * and dtb locations which are already reserved, but overlapping
+	 * doesn't hurt anything
+	 */
+	reserve_map = ((void*)initial_boot_params) +
+			be32_to_cpu(initial_boot_params->off_mem_rsvmap);
+	while (1) {
+		base = be64_to_cpup(reserve_map++);
+		size = be64_to_cpup(reserve_map++);
+		if (!size)
+			break;
+		memblock_reserve(base, size);
+	}
+
+	memblock_allow_resize();
+	memblock_dump_all();
+}
+
+void __init bootmem_init(void)
+{
+	unsigned long min, max;
+
+	min = PFN_UP(memblock_start_of_DRAM());
+	max = PFN_DOWN(memblock_end_of_DRAM());
+
+	/*
+	 * Sparsemem tries to allocate bootmem in memory_present(), so must be
+	 * done after the fixed reservations.
+	 */
+	arm64_memory_present();
+
+	sparse_init();
+	zone_sizes_init(min, max);
+
+	high_memory = __va((max << PAGE_SHIFT) - 1) + 1;
+	max_pfn = max_low_pfn = max;
+}
+
+static inline int free_area(unsigned long pfn, unsigned long end, char *s)
+{
+	unsigned int pages = 0, size = (end - pfn) << (PAGE_SHIFT - 10);
+
+	for (; pfn < end; pfn++) {
+		struct page *page = pfn_to_page(pfn);
+		ClearPageReserved(page);
+		init_page_count(page);
+		__free_page(page);
+		pages++;
+	}
+
+	if (size && s)
+		pr_info("Freeing %s memory: %dK\n", s, size);
+
+	return pages;
+}
+
+/*
+ * Poison init memory with an undefined instruction (0x0).
+ */
+static inline void poison_init_mem(void *s, size_t count)
+{
+	memset(s, 0, count);
+}
+
+#ifndef CONFIG_SPARSEMEM_VMEMMAP
+static inline void free_memmap(unsigned long start_pfn, unsigned long end_pfn)
+{
+	struct page *start_pg, *end_pg;
+	unsigned long pg, pgend;
+
+	/*
+	 * Convert start_pfn/end_pfn to a struct page pointer.
+	 */
+	start_pg = pfn_to_page(start_pfn - 1) + 1;
+	end_pg = pfn_to_page(end_pfn - 1) + 1;
+
+	/*
+	 * Convert to physical addresses, and round start upwards and end
+	 * downwards.
+	 */
+	pg = (unsigned long)PAGE_ALIGN(__pa(start_pg));
+	pgend = (unsigned long)__pa(end_pg) & PAGE_MASK;
+
+	/*
+	 * If there are free pages between these, free the section of the
+	 * memmap array.
+	 */
+	if (pg < pgend)
+		free_bootmem(pg, pgend - pg);
+}
+
+/*
+ * The mem_map array can get very big. Free the unused area of the memory map.
+ */
+static void __init free_unused_memmap(void)
+{
+	unsigned long start, prev_end = 0;
+	struct memblock_region *reg;
+
+	for_each_memblock(memory, reg) {
+		start = __phys_to_pfn(reg->base);
+
+#ifdef CONFIG_SPARSEMEM
+		/*
+		 * Take care not to free memmap entries that don't exist due
+		 * to SPARSEMEM sections which aren't present.
+		 */
+		start = min(start, ALIGN(prev_end, PAGES_PER_SECTION));
+#endif
+		/*
+		 * If we had a previous bank, and there is a space between the
+		 * current bank and the previous, free it.
+		 */
+		if (prev_end && prev_end < start)
+			free_memmap(prev_end, start);
+
+		/*
+		 * Align up here since the VM subsystem insists that the
+		 * memmap entries are valid from the bank end aligned to
+		 * MAX_ORDER_NR_PAGES.
+		 */
+		prev_end = ALIGN(start + __phys_to_pfn(reg->size),
+				 MAX_ORDER_NR_PAGES);
+	}
+
+#ifdef CONFIG_SPARSEMEM
+	if (!IS_ALIGNED(prev_end, PAGES_PER_SECTION))
+		free_memmap(prev_end, ALIGN(prev_end, PAGES_PER_SECTION));
+#endif
+}
+#endif	/* !CONFIG_SPARSEMEM_VMEMMAP */
+
+/*
+ * mem_init() marks the free areas in the mem_map and tells us how much memory
+ * is free.  This is done after various parts of the system have claimed their
+ * memory after the kernel image.
+ */
+void __init mem_init(void)
+{
+	unsigned long reserved_pages, free_pages;
+	struct memblock_region *reg;
+
+#if CONFIG_SWIOTLB
+	extern void __init arm64_swiotlb_init(size_t max_size);
+	arm64_swiotlb_init(max_pfn << (PAGE_SHIFT - 1));
+#endif
+
+	max_mapnr   = pfn_to_page(max_pfn + PHYS_PFN_OFFSET) - mem_map;
+
+#ifndef CONFIG_SPARSEMEM_VMEMMAP
+	/* this will put all unused low memory onto the freelists */
+	free_unused_memmap();
+#endif
+
+	totalram_pages += free_all_bootmem();
+
+	reserved_pages = free_pages = 0;
+
+	for_each_memblock(memory, reg) {
+		unsigned int pfn1, pfn2;
+		struct page *page, *end;
+
+		pfn1 = __phys_to_pfn(reg->base);
+		pfn2 = pfn1 + __phys_to_pfn(reg->size);
+
+		page = pfn_to_page(pfn1);
+		end  = pfn_to_page(pfn2 - 1) + 1;
+
+		do {
+			if (PageReserved(page))
+				reserved_pages++;
+			else if (!page_count(page))
+				free_pages++;
+			page++;
+		} while (page < end);
+	}
+
+	/*
+	 * Since our memory may not be contiguous, calculate the real number
+	 * of pages we have in this system.
+	 */
+	pr_info("Memory:");
+	num_physpages = 0;
+	for_each_memblock(memory, reg) {
+		unsigned long pages = memblock_region_memory_end_pfn(reg) -
+			memblock_region_memory_base_pfn(reg);
+		num_physpages += pages;
+		printk(" %ldMB", pages >> (20 - PAGE_SHIFT));
+	}
+	printk(" = %luMB total\n", num_physpages >> (20 - PAGE_SHIFT));
+
+	pr_notice("Memory: %luk/%luk available, %luk reserved\n",
+		  nr_free_pages() << (PAGE_SHIFT-10),
+		  free_pages << (PAGE_SHIFT-10),
+		  reserved_pages << (PAGE_SHIFT-10));
+
+#define MLK(b, t) b, t, ((t) - (b)) >> 10
+#define MLM(b, t) b, t, ((t) - (b)) >> 20
+#define MLK_ROUNDUP(b, t) b, t, DIV_ROUND_UP(((t) - (b)), SZ_1K)
+
+	pr_notice("Virtual kernel memory layout:\n"
+		  "    vmalloc : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+#ifdef CONFIG_SPARSEMEM_VMEMMAP
+		  "    vmemmap : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+#endif
+		  "    modules : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+		  "    memory  : 0x%16lx - 0x%16lx   (%6ld MB)\n"
+		  "      .init : 0x%p" " - 0x%p" "   (%6ld kB)\n"
+		  "      .text : 0x%p" " - 0x%p" "   (%6ld kB)\n"
+		  "      .data : 0x%p" " - 0x%p" "   (%6ld kB)\n",
+		  MLM(VMALLOC_START, VMALLOC_END),
+#ifdef CONFIG_SPARSEMEM_VMEMMAP
+		  MLM((unsigned long)virt_to_page(PAGE_OFFSET),
+		      (unsigned long)virt_to_page(high_memory)),
+#endif
+		  MLM(MODULES_VADDR, MODULES_END),
+		  MLM(PAGE_OFFSET, (unsigned long)high_memory),
+
+		  MLK_ROUNDUP(__init_begin, __init_end),
+		  MLK_ROUNDUP(_text, _etext),
+		  MLK_ROUNDUP(_sdata, _edata));
+
+#undef MLK
+#undef MLM
+#undef MLK_ROUNDUP
+
+	/*
+	 * Check boundaries twice: Some fundamental inconsistencies can be
+	 * detected at build time already.
+	 */
+#ifdef CONFIG_COMPAT
+	BUILD_BUG_ON(TASK_SIZE_32			> TASK_SIZE_64);
+#endif
+	BUILD_BUG_ON(TASK_SIZE_64			> MODULES_VADDR);
+	BUG_ON(TASK_SIZE_64				> MODULES_VADDR);
+
+	if (PAGE_SIZE >= 16384 && num_physpages <= 128) {
+		extern int sysctl_overcommit_memory;
+		/*
+		 * On a machine this small we won't get anywhere without
+		 * overcommit, so turn it on by default.
+		 */
+		sysctl_overcommit_memory = OVERCOMMIT_ALWAYS;
+	}
+}
+
+void free_initmem(void)
+{
+	poison_init_mem(__init_begin, __init_end - __init_begin);
+	totalram_pages += free_area(__phys_to_pfn(__pa(__init_begin)),
+				    __phys_to_pfn(__pa(__init_end)),
+				    "init");
+}
+
+#ifdef CONFIG_BLK_DEV_INITRD
+
+static int keep_initrd;
+
+void free_initrd_mem(unsigned long start, unsigned long end)
+{
+	if (!keep_initrd) {
+		poison_init_mem((void *)start, PAGE_ALIGN(end) - start);
+		totalram_pages += free_area(__phys_to_pfn(__pa(start)),
+					    __phys_to_pfn(__pa(end)),
+					    "initrd");
+	}
+}
+
+static int __init keepinitrd_setup(char *__unused)
+{
+	keep_initrd = 1;
+	return 1;
+}
+
+__setup("keepinitrd", keepinitrd_setup);
+#endif
