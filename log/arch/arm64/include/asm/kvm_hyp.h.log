commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit 082af5ec5080b028f7d0846a6c27cbb87f288205
Merge: c350717ec7de 258c3d628fe9
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:03:40 2020 +0100

    Merge branch 'for-next/scs' into for-next/core
    
    Support for Clang's Shadow Call Stack in the kernel
    (Sami Tolvanen and Will Deacon)
    * for-next/scs:
      arm64: entry-ftrace.S: Update comment to indicate that x18 is live
      scs: Move DEFINE_SCS macro into core code
      scs: Remove references to asm/scs.h from core code
      scs: Move scs_overflow_check() out of architecture code
      arm64: scs: Use 'scs_sp' register alias for x18
      scs: Move accounting into alloc/free functions
      arm64: scs: Store absolute SCS stack pointer value in thread_info
      efi/libstub: Disable Shadow Call Stack
      arm64: scs: Add shadow stacks for SDEI
      arm64: Implement Shadow Call Stack
      arm64: Disable SCS for hypervisor code
      arm64: vdso: Disable Shadow Call Stack
      arm64: efi: Restore register x18 if it was corrupted
      arm64: Preserve register x18 when CPU is suspended
      arm64: Reserve register x18 from general allocation with SCS
      scs: Disable when function graph tracing is enabled
      scs: Add support for stack usage debugging
      scs: Add page accounting for shadow call stack allocations
      scs: Add support for Clang's Shadow Call Stack (SCS)

commit fe677be989146b8a8c0f26fe626c6567c4cd3837
Author: Marc Zyngier <maz@kernel.org>
Date:   Thu May 28 14:12:59 2020 +0100

    KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
    
    Having __load_guest_stage2 in kvm_hyp.h is quickly going to trigger
    a circular include problem. In order to avoid this, let's move
    it to kvm_mmu.h, where it will be a better fit anyway.
    
    In the process, drop the __hyp_text annotation, which doesn't help
    as the function is marked as __always_inline.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 238d2e049694..dcb63bf94105 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -10,7 +10,6 @@
 #include <linux/compiler.h>
 #include <linux/kvm_host.h>
 #include <asm/alternative.h>
-#include <asm/kvm_mmu.h>
 #include <asm/sysreg.h>
 
 #define __hyp_text __section(.hyp.text) notrace
@@ -88,22 +87,5 @@ void deactivate_traps_vhe_put(void);
 u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
 void __noreturn __hyp_do_panic(unsigned long, ...);
 
-/*
- * Must be called from hyp code running at EL2 with an updated VTTBR
- * and interrupts disabled.
- */
-static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
-{
-	write_sysreg(kvm->arch.vtcr, vtcr_el2);
-	write_sysreg(kvm_get_vttbr(kvm), vttbr_el2);
-
-	/*
-	 * ARM errata 1165522 and 1530923 require the actual execution of the
-	 * above before we can switch to the EL1/EL0 translation regime used by
-	 * the guest.
-	 */
-	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_SPECULATIVE_AT));
-}
-
 #endif /* __ARM64_KVM_HYP_H__ */
 

commit fc5d1f1a42fba6266ab95dc3b84937933a9b5a66
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Sat Dec 1 08:41:28 2018 -0800

    KVM: arm64: vgic-v3: Take cpu_if pointer directly instead of vcpu
    
    If we move the used_lrs field to the version-specific cpu interface
    structure, the following functions only operate on the struct
    vgic_v3_cpu_if and not the full vcpu:
    
      __vgic_v3_save_state
      __vgic_v3_restore_state
      __vgic_v3_activate_traps
      __vgic_v3_deactivate_traps
      __vgic_v3_save_aprs
      __vgic_v3_restore_aprs
    
    This is going to be very useful for nested virt, so move the used_lrs
    field and change the prototypes and implementations of these functions to
    take the cpu_if parameter directly.
    
    No functional change.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index fe57f60f06a8..4f67b0cdffe8 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -56,12 +56,12 @@
 
 int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
-void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
-void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
-void __vgic_v3_activate_traps(struct kvm_vcpu *vcpu);
-void __vgic_v3_deactivate_traps(struct kvm_vcpu *vcpu);
-void __vgic_v3_save_aprs(struct kvm_vcpu *vcpu);
-void __vgic_v3_restore_aprs(struct kvm_vcpu *vcpu);
+void __vgic_v3_save_state(struct vgic_v3_cpu_if *cpu_if);
+void __vgic_v3_restore_state(struct vgic_v3_cpu_if *cpu_if);
+void __vgic_v3_activate_traps(struct vgic_v3_cpu_if *cpu_if);
+void __vgic_v3_deactivate_traps(struct vgic_v3_cpu_if *cpu_if);
+void __vgic_v3_save_aprs(struct vgic_v3_cpu_if *cpu_if);
+void __vgic_v3_restore_aprs(struct vgic_v3_cpu_if *cpu_if);
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __timer_enable_traps(struct kvm_vcpu *vcpu);

commit 9654736891c3ac6a60b52ce70d33cf57cf95bff7
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Mon Apr 27 09:00:15 2020 -0700

    arm64: Disable SCS for hypervisor code
    
    Disable SCS for code that runs at a different exception level by
    adding __noscs to __hyp_text.
    
    Suggested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index fe57f60f06a8..875b106c5d98 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -13,7 +13,7 @@
 #include <asm/kvm_mmu.h>
 #include <asm/sysreg.h>
 
-#define __hyp_text __section(.hyp.text) notrace
+#define __hyp_text __section(.hyp.text) notrace __noscs
 
 #define read_sysreg_elx(r,nvh,vh)					\
 	({								\

commit 02ab1f5018c3ad0b8677e797b5d3333d2e3b7f20
Author: Andrew Scull <ascull@google.com>
Date:   Mon May 4 10:48:58 2020 +0100

    arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}
    
    Errata 1165522, 1319367 and 1530923 each allow TLB entries to be
    allocated as a result of a speculative AT instruction. In order to
    avoid mandating VHE on certain affected CPUs, apply the workaround to
    both the nVHE and the VHE case for all affected CPUs.
    
    Signed-off-by: Andrew Scull <ascull@google.com>
    Acked-by: Will Deacon <will@kernel.org>
    CC: Marc Zyngier <maz@kernel.org>
    CC: James Morse <james.morse@arm.com>
    CC: Suzuki K Poulose <suzuki.poulose@arm.com>
    CC: Will Deacon <will@kernel.org>
    CC: Steven Price <steven.price@arm.com>
    Link: https://lore.kernel.org/r/20200504094858.108917-1-ascull@google.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index fe57f60f06a8..238d2e049694 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -102,7 +102,7 @@ static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
 	 * above before we can switch to the EL1/EL0 translation regime used by
 	 * the guest.
 	 */
-	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_SPECULATIVE_AT_VHE));
+	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_SPECULATIVE_AT));
 }
 
 #endif /* __ARM64_KVM_HYP_H__ */

commit e951445f4d3b5d0df69c0c5d18ab1e9058c20e52
Merge: ef935c25fd64 e43f1331e2ef
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 28 11:46:59 2020 +0100

    Merge tag 'kvmarm-fixes-5.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm fixes for 5.6, take #1
    
    - Fix compilation on 32bit
    - Move  VHE guest entry/exit into the VHE-specific entry code
    - Make sure all functions called by the non-VHE HYP code is tagged as __always_inline

commit 8c2d146ee7a2e0782eea4dd70fddc1c837140136
Author: James Morse <james.morse@arm.com>
Date:   Thu Feb 20 16:58:38 2020 +0000

    KVM: arm64: Define our own swab32() to avoid a uapi static inline
    
    KVM uses swab32() when mediating GIC MMIO accesses if the GICV is badly
    aligned, and the host and guest differ in endianness.
    
    arm64 doesn't provide a __arch_swab32(), so __fswab32() is always backed
    by the macro implementation that the compiler reduces to a single
    instruction. But the static-inline causes problems for KVM if the compiler
    chooses not to inline this function, it may not be located in the
    __hyp_text where __vgic_v2_perform_cpuif_access() needs it.
    
    Create our own __kvm_swab32() macro that calls ___constant_swab32()
    directly. This way we know it will always be inlined.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200220165839.256881-3-james.morse@arm.com

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 97f21cc66657..5fde137b5150 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -47,6 +47,13 @@
 #define read_sysreg_el2(r)	read_sysreg_elx(r, _EL2, _EL1)
 #define write_sysreg_el2(v,r)	write_sysreg_elx(v, r, _EL2, _EL1)
 
+/*
+ * Without an __arch_swab32(), we fall back to ___constant_swab32(), but the
+ * static inline can allow the compiler to out-of-line this. KVM always wants
+ * the macro version as its always inlined.
+ */
+#define __kvm_swab32(x)	___constant_swab32(x)
+
 int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);

commit 275fa0ea2cf7a84450f9c0ec0d9e7ec168ed2e2d
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:31 2019 +0000

    arm64: Workaround for Cortex-A55 erratum 1530923
    
    Cortex-A55 erratum 1530923 allows TLB entries to be allocated as a
    result of a speculative AT instruction. This may happen in the middle of
    a guest world switch while the relevant VMSA configuration is in an
    inconsistent state, leading to erroneous content being allocated into
    TLBs.
    
    The same workaround as is used for Cortex-A76 erratum 1165522
    (WORKAROUND_SPECULATIVE_AT_VHE) can be used here. Note that this
    mandates the use of VHE on affected parts.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 167a161dd596..a3a6a2ba9a63 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -91,8 +91,8 @@ static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
 	write_sysreg(kvm_get_vttbr(kvm), vttbr_el2);
 
 	/*
-	 * ARM erratum 1165522 requires the actual execution of the above
-	 * before we can switch to the EL1/EL0 translation regime used by
+	 * ARM errata 1165522 and 1530923 require the actual execution of the
+	 * above before we can switch to the EL1/EL0 translation regime used by
 	 * the guest.
 	 */
 	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_SPECULATIVE_AT_VHE));

commit e85d68faed4e79fd0b481c72de8245d4290369db
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:29 2019 +0000

    arm64: Rename WORKAROUND_1165522 to SPECULATIVE_AT_VHE
    
    Cortex-A55 is affected by a similar erratum, so rename the existing
    workaround for errarum 1165522 so it can be used for both errata.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 97f21cc66657..167a161dd596 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -95,7 +95,7 @@ static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
 	 * before we can switch to the EL1/EL0 translation regime used by
 	 * the guest.
 	 */
-	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_1165522));
+	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_SPECULATIVE_AT_VHE));
 }
 
 #endif /* __ARM64_KVM_HYP_H__ */

commit 084b5a80e87258997632dbd975ec3e5fda6bb943
Author: Marc Zyngier <maz@kernel.org>
Date:   Sun Sep 1 22:12:37 2019 +0100

    arm64: KVM: Kill hyp_alternate_select()
    
    hyp_alternate_select() is now completely unused. Goodbye.
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 86825aa20852..97f21cc66657 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -47,30 +47,6 @@
 #define read_sysreg_el2(r)	read_sysreg_elx(r, _EL2, _EL1)
 #define write_sysreg_el2(v,r)	write_sysreg_elx(v, r, _EL2, _EL1)
 
-/**
- * hyp_alternate_select - Generates patchable code sequences that are
- * used to switch between two implementations of a function, depending
- * on the availability of a feature.
- *
- * @fname: a symbol name that will be defined as a function returning a
- * function pointer whose type will match @orig and @alt
- * @orig: A pointer to the default function, as returned by @fname when
- * @cond doesn't hold
- * @alt: A pointer to the alternate function, as returned by @fname
- * when @cond holds
- * @cond: a CPU feature (as described in asm/cpufeature.h)
- */
-#define hyp_alternate_select(fname, orig, alt, cond)			\
-typeof(orig) * __hyp_text fname(void)					\
-{									\
-	typeof(alt) *val = orig;					\
-	asm volatile(ALTERNATIVE("nop		\n",			\
-				 "mov	%0, %1	\n",			\
-				 cond)					\
-		     : "+r" (val) : "r" (alt));				\
-	return val;							\
-}
-
 int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);

commit fdec2a9ef853172529baaa192673b4cdb9a44fac
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Sat Apr 6 11:29:40 2019 +0100

    KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
    
    Currently, the {read,write}_sysreg_el*() accessors for accessing
    particular ELs' sysregs in the presence of VHE rely on some local
    hacks and define their system register encodings in a way that is
    inconsistent with the core definitions in <asm/sysreg.h>.
    
    As a result, it is necessary to add duplicate definitions for any
    system register that already needs a definition in sysreg.h for
    other reasons.
    
    This is a bit of a maintenance headache, and the reasons for the
    _el*() accessors working the way they do is a bit historical.
    
    This patch gets rid of the shadow sysreg definitions in
    <asm/kvm_hyp.h>, converts the _el*() accessors to use the core
    __msr_s/__mrs_s interface, and converts all call sites to use the
    standard sysreg #define names (i.e., upper case, with SYS_ prefix).
    
    This patch will conflict heavily anyway, so the opportunity
    to clean up some bad whitespace in the context of the changes is
    taken.
    
    The change exposes a few system registers that have no sysreg.h
    definition, due to msr_s/mrs_s being used in place of msr/mrs:
    additions are made in order to fill in the gaps.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://www.spinics.net/lists/kvm-arm/msg31717.html
    [Rebased to v4.21-rc1]
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    [Rebased to v5.2-rc5, changelog updates]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 286f7e7e1be4..86825aa20852 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -18,7 +18,7 @@
 #define read_sysreg_elx(r,nvh,vh)					\
 	({								\
 		u64 reg;						\
-		asm volatile(ALTERNATIVE("mrs %0, " __stringify(r##nvh),\
+		asm volatile(ALTERNATIVE(__mrs_s("%0", r##nvh),	\
 					 __mrs_s("%0", r##vh),		\
 					 ARM64_HAS_VIRT_HOST_EXTN)	\
 			     : "=r" (reg));				\
@@ -28,7 +28,7 @@
 #define write_sysreg_elx(v,r,nvh,vh)					\
 	do {								\
 		u64 __val = (u64)(v);					\
-		asm volatile(ALTERNATIVE("msr " __stringify(r##nvh) ", %x0",\
+		asm volatile(ALTERNATIVE(__msr_s(r##nvh, "%x0"),	\
 					 __msr_s(r##vh, "%x0"),		\
 					 ARM64_HAS_VIRT_HOST_EXTN)	\
 					 : : "rZ" (__val));		\
@@ -37,55 +37,15 @@
 /*
  * Unified accessors for registers that have a different encoding
  * between VHE and non-VHE. They must be specified without their "ELx"
- * encoding.
+ * encoding, but with the SYS_ prefix, as defined in asm/sysreg.h.
  */
-#define read_sysreg_el2(r)						\
-	({								\
-		u64 reg;						\
-		asm volatile(ALTERNATIVE("mrs %0, " __stringify(r##_EL2),\
-					 "mrs %0, " __stringify(r##_EL1),\
-					 ARM64_HAS_VIRT_HOST_EXTN)	\
-			     : "=r" (reg));				\
-		reg;							\
-	})
-
-#define write_sysreg_el2(v,r)						\
-	do {								\
-		u64 __val = (u64)(v);					\
-		asm volatile(ALTERNATIVE("msr " __stringify(r##_EL2) ", %x0",\
-					 "msr " __stringify(r##_EL1) ", %x0",\
-					 ARM64_HAS_VIRT_HOST_EXTN)	\
-					 : : "rZ" (__val));		\
-	} while (0)
 
 #define read_sysreg_el0(r)	read_sysreg_elx(r, _EL0, _EL02)
 #define write_sysreg_el0(v,r)	write_sysreg_elx(v, r, _EL0, _EL02)
 #define read_sysreg_el1(r)	read_sysreg_elx(r, _EL1, _EL12)
 #define write_sysreg_el1(v,r)	write_sysreg_elx(v, r, _EL1, _EL12)
-
-/* The VHE specific system registers and their encoding */
-#define sctlr_EL12              sys_reg(3, 5, 1, 0, 0)
-#define cpacr_EL12              sys_reg(3, 5, 1, 0, 2)
-#define ttbr0_EL12              sys_reg(3, 5, 2, 0, 0)
-#define ttbr1_EL12              sys_reg(3, 5, 2, 0, 1)
-#define tcr_EL12                sys_reg(3, 5, 2, 0, 2)
-#define afsr0_EL12              sys_reg(3, 5, 5, 1, 0)
-#define afsr1_EL12              sys_reg(3, 5, 5, 1, 1)
-#define esr_EL12                sys_reg(3, 5, 5, 2, 0)
-#define far_EL12                sys_reg(3, 5, 6, 0, 0)
-#define mair_EL12               sys_reg(3, 5, 10, 2, 0)
-#define amair_EL12              sys_reg(3, 5, 10, 3, 0)
-#define vbar_EL12               sys_reg(3, 5, 12, 0, 0)
-#define contextidr_EL12         sys_reg(3, 5, 13, 0, 1)
-#define cntkctl_EL12            sys_reg(3, 5, 14, 1, 0)
-#define cntp_tval_EL02          sys_reg(3, 5, 14, 2, 0)
-#define cntp_ctl_EL02           sys_reg(3, 5, 14, 2, 1)
-#define cntp_cval_EL02          sys_reg(3, 5, 14, 2, 2)
-#define cntv_tval_EL02          sys_reg(3, 5, 14, 3, 0)
-#define cntv_ctl_EL02           sys_reg(3, 5, 14, 3, 1)
-#define cntv_cval_EL02          sys_reg(3, 5, 14, 3, 2)
-#define spsr_EL12               sys_reg(3, 5, 4, 0, 0)
-#define elr_EL12                sys_reg(3, 5, 4, 0, 1)
+#define read_sysreg_el2(r)	read_sysreg_elx(r, _EL2, _EL1)
+#define write_sysreg_el2(v,r)	write_sysreg_elx(v, r, _EL2, _EL1)
 
 /**
  * hyp_alternate_select - Generates patchable code sequences that are

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 09fe8bd15f6e..286f7e7e1be4 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -1,18 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2015 - ARM Ltd
  * Author: Marc Zyngier <marc.zyngier@arm.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #ifndef __ARM64_KVM_HYP_H__

commit 0ef0fd351550130129bbdb77362488befd7b69d2
Merge: 4489da718309 c011d23ba046
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 17 10:33:30 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for SVE and Pointer Authentication in guests
       - PMU improvements
    
      POWER:
       - support for direct access to the POWER9 XIVE interrupt controller
       - memory and performance optimizations
    
      x86:
       - support for accessing memory not backed by struct page
       - fixes and refactoring
    
      Generic:
       - dirty page tracking improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (155 commits)
      kvm: fix compilation on aarch64
      Revert "KVM: nVMX: Expose RDPMC-exiting only when guest supports PMU"
      kvm: x86: Fix L1TF mitigation for shadow MMU
      KVM: nVMX: Disable intercept for FS/GS base MSRs in vmcs02 when possible
      KVM: PPC: Book3S: Remove useless checks in 'release' method of KVM device
      KVM: PPC: Book3S HV: XIVE: Fix spelling mistake "acessing" -> "accessing"
      KVM: PPC: Book3S HV: Make sure to load LPID for radix VCPUs
      kvm: nVMX: Set nested_run_pending in vmx_set_nested_state after checks complete
      tests: kvm: Add tests for KVM_SET_NESTED_STATE
      KVM: nVMX: KVM_SET_NESTED_STATE - Tear down old EVMCS state before setting new state
      tests: kvm: Add tests for KVM_CAP_MAX_VCPUS and KVM_CAP_MAX_CPU_ID
      tests: kvm: Add tests to .gitignore
      KVM: Introduce KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2
      KVM: Fix kvm_clear_dirty_log_protect off-by-(minus-)one
      KVM: Fix the bitmap range to copy during clear dirty
      KVM: arm64: Fix ptrauth ID register masking logic
      KVM: x86: use direct accessors for RIP and RSP
      KVM: VMX: Use accessors for GPRs outside of dedicated caching logic
      KVM: x86: Omit caching logic for always-available GPRs
      kvm, x86: Properly check whether a pfn is an MMIO or not
      ...

commit be604c616ca71cbf5c860d0cfa4595128ab74189
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 24 09:55:37 2019 -0700

    arm64: sysreg: Make mrs_s and msr_s macros work with Clang and LTO
    
    Clang's integrated assembler does not allow assembly macros defined
    in one inline asm block using the .macro directive to be used across
    separate asm blocks. LLVM developers consider this a feature and not a
    bug, recommending code refactoring:
    
      https://bugs.llvm.org/show_bug.cgi?id=19749
    
    As binutils doesn't allow macros to be redefined, this change uses
    UNDEFINE_MRS_S and UNDEFINE_MSR_S to define corresponding macros
    in-place and workaround gcc and clang limitations on redefining macros
    across different assembler blocks.
    
    Specifically, the current state after preprocessing looks like this:
    
    asm volatile(".macro mXX_s ... .endm");
    void f()
    {
            asm volatile("mXX_s a, b");
    }
    
    With GCC, it gives macro redefinition error because sysreg.h is included
    in multiple source files, and assembler code for all of them is later
    combined for LTO (I've seen an intermediate file with hundreds of
    identical definitions).
    
    With clang, it gives macro undefined error because clang doesn't allow
    sharing macros between inline asm statements.
    
    I also seem to remember catching another sort of undefined error with
    GCC due to reordering of macro definition asm statement and generated
    asm code for function that uses the macro.
    
    The solution with defining and undefining for each use, while certainly
    not elegant, satisfies both GCC and clang, LTO and non-LTO.
    
    Co-developed-by: Alex Matveev <alxmtvv@gmail.com>
    Co-developed-by: Yury Norov <ynorov@caviumnetworks.com>
    Co-developed-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 4da765f2cca5..c3060833b7a5 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -30,7 +30,7 @@
 	({								\
 		u64 reg;						\
 		asm volatile(ALTERNATIVE("mrs %0, " __stringify(r##nvh),\
-					 "mrs_s %0, " __stringify(r##vh),\
+					 __mrs_s("%0", r##vh),		\
 					 ARM64_HAS_VIRT_HOST_EXTN)	\
 			     : "=r" (reg));				\
 		reg;							\
@@ -40,7 +40,7 @@
 	do {								\
 		u64 __val = (u64)(v);					\
 		asm volatile(ALTERNATIVE("msr " __stringify(r##nvh) ", %x0",\
-					 "msr_s " __stringify(r##vh) ", %x0",\
+					 __msr_s(r##vh, "%x0"),		\
 					 ARM64_HAS_VIRT_HOST_EXTN)	\
 					 : : "rZ" (__val));		\
 	} while (0)

commit 38abf22e122e00d20e99408fce4471b5cb65133b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:06 2018 +0100

    KVM: arm64: Delete orphaned declaration for __fpsimd_enabled()
    
    __fpsimd_enabled() no longer exists, but a dangling declaration has
    survived in kvm_hyp.h.
    
    This patch gets rid of it.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benn√©e <alex.bennee@linaro.org>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 4da765f2cca5..ef8b8394d3d1 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -149,7 +149,6 @@ void __debug_switch_to_host(struct kvm_vcpu *vcpu);
 
 void __fpsimd_save_state(struct user_fpsimd_state *fp_regs);
 void __fpsimd_restore_state(struct user_fpsimd_state *fp_regs);
-bool __fpsimd_enabled(void);
 
 void activate_traps_vhe_load(struct kvm_vcpu *vcpu);
 void deactivate_traps_vhe_put(void);

commit e329fb75d519e3dc3eb11b22d5bb846516be3521
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Tue Dec 11 15:26:31 2018 +0100

    KVM: arm/arm64: Factor out VMID into struct kvm_vmid
    
    In preparation for nested virtualization where we are going to have more
    than a single VMID per VM, let's factor out the VMID data into a
    separate VMID data structure and change the VMID allocator to operate on
    this new structure instead of using a struct kvm.
    
    This also means that udate_vttbr now becomes update_vmid, and that the
    vttbr itself is generated on the fly based on the stage 2 page table
    base address and the vmid.
    
    We cache the physical address of the pgd when allocating the pgd to
    avoid doing the calculation on every entry to the guest and to avoid
    calling into potentially non-hyp-mapped code from hyp/EL2.
    
    If we wanted to merge the VMID allocator with the arm64 ASID allocator
    at some point in the future, it should actually become easier to do that
    after this patch.
    
    Note that to avoid mapping the kvm_vmid_bits variable into hyp, we
    simply forego the masking of the vmid value in kvm_get_vttbr and rely on
    update_vmid to always assign a valid vmid value (within the supported
    range).
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    [maz: minor cleanups]
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index a80a7ef57325..4da765f2cca5 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -21,6 +21,7 @@
 #include <linux/compiler.h>
 #include <linux/kvm_host.h>
 #include <asm/alternative.h>
+#include <asm/kvm_mmu.h>
 #include <asm/sysreg.h>
 
 #define __hyp_text __section(.hyp.text) notrace
@@ -163,7 +164,7 @@ void __noreturn __hyp_do_panic(unsigned long, ...);
 static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
 {
 	write_sysreg(kvm->arch.vtcr, vtcr_el2);
-	write_sysreg(kvm->arch.vttbr, vttbr_el2);
+	write_sysreg(kvm_get_vttbr(kvm), vttbr_el2);
 
 	/*
 	 * ARM erratum 1165522 requires the actual execution of the above

commit 1e4448c5ddbe93ab6070160f094f49e7c95477e6
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 6 17:31:24 2018 +0000

    arm64: KVM: Add synchronization on translation regime change for erratum 1165522
    
    In order to ensure that slipping HCR_EL2.TGE is done at the right
    time when switching translation regime, let insert the required ISBs
    that will be patched in when erratum 1165522 is detected.
    
    Take this opportunity to add the missing include of asm/alternative.h
    which was getting there by pure luck.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 23aca66767f9..a80a7ef57325 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -20,6 +20,7 @@
 
 #include <linux/compiler.h>
 #include <linux/kvm_host.h>
+#include <asm/alternative.h>
 #include <asm/sysreg.h>
 
 #define __hyp_text __section(.hyp.text) notrace
@@ -163,6 +164,13 @@ static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
 {
 	write_sysreg(kvm->arch.vtcr, vtcr_el2);
 	write_sysreg(kvm->arch.vttbr, vttbr_el2);
+
+	/*
+	 * ARM erratum 1165522 requires the actual execution of the above
+	 * before we can switch to the EL1/EL0 translation regime used by
+	 * the guest.
+	 */
+	asm(ALTERNATIVE("nop", "isb", ARM64_WORKAROUND_1165522));
 }
 
 #endif /* __ARM64_KVM_HYP_H__ */

commit 7665f3a8491b0ed3c6f65c0bc3a5424ea8f87731
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:43 2018 +0100

    kvm: arm64: Configure VTCR_EL2 per VM
    
    Add support for setting the VTCR_EL2 per VM, rather than hard
    coding a value at boot time per CPU. This would allow us to tune
    the stage2 page table parameters per VM in the later changes.
    
    We compute the VTCR fields based on the system wide sanitised
    feature registers, except for the hardware management of Access
    Flags (VTCR_EL2.HA). It is fine to run a system with a mix of
    CPUs that may or may not update the page table Access Flags.
    Since the bit is RES0 on CPUs that don't support it, the bit
    should be ignored on them.
    
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index d1bd1e0f14d7..23aca66767f9 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -161,6 +161,7 @@ void __noreturn __hyp_do_panic(unsigned long, ...);
  */
 static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
 {
+	write_sysreg(kvm->arch.vtcr, vtcr_el2);
 	write_sysreg(kvm->arch.vttbr, vttbr_el2);
 }
 

commit 9f98ddd6686cc9469fb73b11ddd403271d65cbdf
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:39 2018 +0100

    kvm: arm64: Add helper for loading the stage2 setting for a VM
    
    We load the stage2 context of a guest for different operations,
    including running the guest and tlb maintenance on behalf of the
    guest. As of now only the vttbr is private to the guest, but this
    is about to change with IPA per VM. Add a helper to load the stage2
    configuration for a VM, which could do the right thing with the
    future changes.
    
    Cc: Christoffer Dall <cdall@kernel.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 384c34397619..d1bd1e0f14d7 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -155,5 +155,14 @@ void deactivate_traps_vhe_put(void);
 u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
 void __noreturn __hyp_do_panic(unsigned long, ...);
 
+/*
+ * Must be called from hyp code running at EL2 with an updated VTTBR
+ * and interrupts disabled.
+ */
+static __always_inline void __hyp_text __load_guest_stage2(struct kvm *kvm)
+{
+	write_sysreg(kvm->arch.vttbr, vttbr_el2);
+}
+
 #endif /* __ARM64_KVM_HYP_H__ */
 

commit 2d0e63e030babe19c94b4453ef4b272c0aacd75a
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Oct 5 17:19:19 2017 +0200

    KVM: arm/arm64: Avoid VGICv3 save/restore on VHE with no IRQs
    
    We can finally get completely rid of any calls to the VGICv3
    save/restore functions when the AP lists are empty on VHE systems.  This
    requires carefully factoring out trap configuration from saving and
    restoring state, and carefully choosing what to do on the VHE and
    non-VHE path.
    
    One of the challenges is that we cannot save/restore the VMCR lazily
    because we can only write the VMCR when ICC_SRE_EL1.SRE is cleared when
    emulating a GICv2-on-GICv3, since otherwise all Group-0 interrupts end
    up being delivered as FIQ.
    
    To solve this problem, and still provide fast performance in the fast
    path of exiting a VM when no interrupts are pending (which also
    optimized the latency for actually delivering virtual interrupts coming
    from physical interrupts), we orchestrate a dance of only doing the
    activate/deactivate traps in vgic load/put for VHE systems (which can
    have ICC_SRE_EL1.SRE cleared when running in the host), and doing the
    configuration on every round-trip on non-VHE systems.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 6f3929b2fcf7..384c34397619 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -124,6 +124,8 @@ int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
+void __vgic_v3_activate_traps(struct kvm_vcpu *vcpu);
+void __vgic_v3_deactivate_traps(struct kvm_vcpu *vcpu);
 void __vgic_v3_save_aprs(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_aprs(struct kvm_vcpu *vcpu);
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);

commit 923a2e30e5745a8f94f953f7aacaafd3d551e12d
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Oct 5 00:18:07 2017 +0200

    KVM: arm/arm64: Move VGIC APR save/restore to vgic put/load
    
    The APRs can only have bits set when the guest acknowledges an interrupt
    in the LR and can only have a bit cleared when the guest EOIs an
    interrupt in the LR.  Therefore, if we have no LRs with any
    pending/active interrupts, the APR cannot change value and there is no
    need to clear it on every exit from the VM (hint: it will have already
    been cleared when we exited the guest the last time with the LRs all
    EOIed).
    
    The only case we need to take care of is when we migrate the VCPU away
    from a CPU or migrate a new VCPU onto a CPU, or when we return to
    userspace to capture the state of the VCPU for migration.  To make sure
    this works, factor out the APR save/restore functionality into separate
    functions called from the VCPU (and by extension VGIC) put/load hooks.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index febe417b8b4e..6f3929b2fcf7 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -124,6 +124,8 @@ int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
+void __vgic_v3_save_aprs(struct kvm_vcpu *vcpu);
+void __vgic_v3_restore_aprs(struct kvm_vcpu *vcpu);
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __timer_enable_traps(struct kvm_vcpu *vcpu);

commit 75174ba6ca9dcfddda88aa420da4d7aa2b279fdf
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Dec 22 20:39:10 2016 +0100

    KVM: arm/arm64: Handle VGICv2 save/restore from the main VGIC code
    
    We can program the GICv2 hypervisor control interface logic directly
    from the core vgic code and can instead do the save/restore directly
    from the flush/sync functions, which can lead to a number of future
    optimizations.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 949f2e77ae58..febe417b8b4e 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -120,8 +120,6 @@ typeof(orig) * __hyp_text fname(void)					\
 	return val;							\
 }
 
-void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
-void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
 int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);

commit a2465629b62a82b3145dc7ef40ec6c32432cf002
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Fri Aug 4 13:47:18 2017 +0200

    KVM: arm64: Configure c15, PMU, and debug register traps on cpu load/put for VHE
    
    We do not have to change the c15 trap setting on each switch to/from the
    guest on VHE systems, because this setting only affects guest EL1/EL0
    (and therefore not the VHE host).
    
    The PMU and debug trap configuration can also be done on vcpu load/put
    instead, because they don't affect how the VHE host kernel can access the
    debug registers while executing KVM kernel code.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 2b1fda90dde4..949f2e77ae58 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -147,6 +147,9 @@ void __fpsimd_save_state(struct user_fpsimd_state *fp_regs);
 void __fpsimd_restore_state(struct user_fpsimd_state *fp_regs);
 bool __fpsimd_enabled(void);
 
+void activate_traps_vhe_load(struct kvm_vcpu *vcpu);
+void deactivate_traps_vhe_put(void);
+
 u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
 void __noreturn __hyp_do_panic(unsigned long, ...);
 

commit 4cdecaba0146481f1503a645b8a5a41c1e8566c9
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Oct 10 22:40:13 2017 +0200

    KVM: arm64: Unify non-VHE host/guest sysreg save and restore functions
    
    There is no need to have multiple identical functions with different
    names for saving host and guest state.  When saving and restoring state
    for the host and guest, the state is the same for both contexts, and
    that's why we have the kvm_cpu_context structure.  Delete one
    version and rename the other into simply save/restore.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 23c09d9af343..2b1fda90dde4 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -131,10 +131,8 @@ int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
 void __timer_enable_traps(struct kvm_vcpu *vcpu);
 void __timer_disable_traps(struct kvm_vcpu *vcpu);
 
-void __sysreg_save_host_state_nvhe(struct kvm_cpu_context *ctxt);
-void __sysreg_restore_host_state_nvhe(struct kvm_cpu_context *ctxt);
-void __sysreg_save_guest_state_nvhe(struct kvm_cpu_context *ctxt);
-void __sysreg_restore_guest_state_nvhe(struct kvm_cpu_context *ctxt);
+void __sysreg_save_state_nvhe(struct kvm_cpu_context *ctxt);
+void __sysreg_restore_state_nvhe(struct kvm_cpu_context *ctxt);
 void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt);
 void sysreg_restore_host_state_vhe(struct kvm_cpu_context *ctxt);
 void sysreg_save_guest_state_vhe(struct kvm_cpu_context *ctxt);

commit f837453d0e10e9dd2e4444a940ccef1ed3cb243a
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Oct 10 22:19:31 2017 +0200

    KVM: arm64: Introduce separate VHE/non-VHE sysreg save/restore functions
    
    As we are about to handle system registers quite differently between VHE
    and non-VHE systems.  In preparation for that, we need to split some of
    the handling functions between VHE and non-VHE functionality.
    
    For now, we simply copy the non-VHE functions, but we do change the use
    of static keys for VHE and non-VHE functionality now that we have
    separate functions.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index aeda2a777365..23c09d9af343 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -131,10 +131,14 @@ int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
 void __timer_enable_traps(struct kvm_vcpu *vcpu);
 void __timer_disable_traps(struct kvm_vcpu *vcpu);
 
-void __sysreg_save_host_state(struct kvm_cpu_context *ctxt);
-void __sysreg_restore_host_state(struct kvm_cpu_context *ctxt);
-void __sysreg_save_guest_state(struct kvm_cpu_context *ctxt);
-void __sysreg_restore_guest_state(struct kvm_cpu_context *ctxt);
+void __sysreg_save_host_state_nvhe(struct kvm_cpu_context *ctxt);
+void __sysreg_restore_host_state_nvhe(struct kvm_cpu_context *ctxt);
+void __sysreg_save_guest_state_nvhe(struct kvm_cpu_context *ctxt);
+void __sysreg_restore_guest_state_nvhe(struct kvm_cpu_context *ctxt);
+void sysreg_save_host_state_vhe(struct kvm_cpu_context *ctxt);
+void sysreg_restore_host_state_vhe(struct kvm_cpu_context *ctxt);
+void sysreg_save_guest_state_vhe(struct kvm_cpu_context *ctxt);
+void sysreg_restore_guest_state_vhe(struct kvm_cpu_context *ctxt);
 void __sysreg32_save_state(struct kvm_vcpu *vcpu);
 void __sysreg32_restore_state(struct kvm_vcpu *vcpu);
 

commit 014c4c77aad7660cc7c16cd23b0c3b114cf070d2
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Oct 10 20:10:08 2017 +0200

    KVM: arm64: Improve debug register save/restore flow
    
    Instead of having multiple calls from the world switch path to the debug
    logic, each figuring out if the dirty bit is set and if we should
    save/restore the debug registers, let's just provide two hooks to the
    debug save/restore functionality, one for switching to the guest
    context, and one for switching to the host context, and we get the
    benefit of only having to evaluate the dirty flag once on each path,
    plus we give the compiler some more room to inline some of this
    functionality.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index f26f9cd70c72..aeda2a777365 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -138,14 +138,8 @@ void __sysreg_restore_guest_state(struct kvm_cpu_context *ctxt);
 void __sysreg32_save_state(struct kvm_vcpu *vcpu);
 void __sysreg32_restore_state(struct kvm_vcpu *vcpu);
 
-void __debug_save_state(struct kvm_vcpu *vcpu,
-			struct kvm_guest_debug_arch *dbg,
-			struct kvm_cpu_context *ctxt);
-void __debug_restore_state(struct kvm_vcpu *vcpu,
-			   struct kvm_guest_debug_arch *dbg,
-			   struct kvm_cpu_context *ctxt);
-void __debug_cond_save_host_state(struct kvm_vcpu *vcpu);
-void __debug_cond_restore_host_state(struct kvm_vcpu *vcpu);
+void __debug_switch_to_guest(struct kvm_vcpu *vcpu);
+void __debug_switch_to_host(struct kvm_vcpu *vcpu);
 
 void __fpsimd_save_state(struct user_fpsimd_state *fp_regs);
 void __fpsimd_restore_state(struct user_fpsimd_state *fp_regs);

commit d68119864ef4b253a585a1c897cda6936d4b5de9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 23 17:11:14 2017 +0100

    KVM: arm/arm64: Detangle kvm_mmu.h from kvm_hyp.h
    
    kvm_hyp.h has an odd dependency on kvm_mmu.h, which makes the
    opposite inclusion impossible. Let's start with breaking that
    useless dependency.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 08d3bb66c8b7..f26f9cd70c72 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -20,7 +20,6 @@
 
 #include <linux/compiler.h>
 #include <linux/kvm_host.h>
-#include <asm/kvm_mmu.h>
 #include <asm/sysreg.h>
 
 #define __hyp_text __section(.hyp.text) notrace

commit 688c50aa72f64ca21767486e5eef876ec23e418c
Author: Christoffer Dall <cdall@linaro.org>
Date:   Wed Jan 4 16:10:28 2017 +0100

    KVM: arm/arm64: Move timer save/restore out of the hyp code
    
    As we are about to be lazy with saving and restoring the timer
    registers, we prepare by moving all possible timer configuration logic
    out of the hyp code.  All virtual timer registers can be programmed from
    EL1 and since the arch timer is always a level triggered interrupt we
    can safely do this with interrupts disabled in the host kernel on the
    way to the guest without taking vtimer interrupts in the host kernel
    (yet).
    
    The downside is that the cntvoff register can only be programmed from
    hyp mode, so we jump into hyp mode and back to program it.  This is also
    safe, because the host kernel doesn't use the virtual timer in the KVM
    code.  It may add a little performance performance penalty, but only
    until following commits where we move this operation to vcpu load/put.
    
    Signed-off-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 4572a9b560fa..08d3bb66c8b7 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -129,8 +129,8 @@ void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
 int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
-void __timer_save_state(struct kvm_vcpu *vcpu);
-void __timer_restore_state(struct kvm_vcpu *vcpu);
+void __timer_enable_traps(struct kvm_vcpu *vcpu);
+void __timer_disable_traps(struct kvm_vcpu *vcpu);
 
 void __sysreg_save_host_state(struct kvm_cpu_context *ctxt);
 void __sysreg_restore_host_state(struct kvm_cpu_context *ctxt);

commit 59da1cbfd840d69bd7a310249924da3fc202c417
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:33 2017 +0100

    KVM: arm64: vgic-v3: Add hook to handle guest GICv3 sysreg accesses at EL2
    
    In order to start handling guest access to GICv3 system registers,
    let's add a hook that will get called when we trap a system register
    access. This is gated by a new static key (vgic_v3_cpuif_trap).
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index b18e852d27e8..4572a9b560fa 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -127,6 +127,7 @@ int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
+int __vgic_v3_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __timer_save_state(struct kvm_vcpu *vcpu);
 void __timer_restore_state(struct kvm_vcpu *vcpu);

commit 3272f0d08e4490b792b99cf6034a2bb859bf6c9f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 14:02:17 2016 +0100

    arm64: KVM: Inject a vSerror if detecting a bad GICV access at EL2
    
    If, when proxying a GICV access at EL2, we detect that the guest is
    doing something silly, report an EL1 SError instead ofgnoring the
    access.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 88ec3ac35a32..b18e852d27e8 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -123,7 +123,7 @@ typeof(orig) * __hyp_text fname(void)					\
 
 void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
-bool __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
+int __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);

commit fb5ee369ccd3986b28adc20d43d73a2b2c141977
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 09:28:45 2016 +0100

    arm64: KVM: vgic-v2: Add the GICV emulation infrastructure
    
    In order to efficiently perform the GICV access on behalf of the
    guest, we need to be able to avoid going back all the way to
    the host kernel.
    
    For this, we introduce a new hook in the world switch code,
    conveniently placed just after populating the fault info.
    At that point, we only have saved/restored the GP registers,
    and we can quickly perform all the required checks (data abort,
    translation fault, valid faulting syndrome, not an external
    abort, not a PTW).
    
    Coming back from the emulation code, we need to skip the emulated
    instruction. This involves an additional bit of save/restore in
    order to be able to access the guest's PC (and possibly CPSR if
    this is a 32bit guest).
    
    At this stage, no emulation code is provided.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index cff510574fae..88ec3ac35a32 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -123,6 +123,7 @@ typeof(orig) * __hyp_text fname(void)					\
 
 void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
+bool __vgic_v2_perform_cpuif_access(struct kvm_vcpu *vcpu);
 
 void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
 void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);

commit fd81e6bf3928c14f90a033df164c375d4ce0fd85
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:40 2016 +0100

    arm64: KVM: Refactor kern_hyp_va to deal with multiple offsets
    
    As we move towards a selectable HYP VA range, it is obvious that
    we don't want to test a variable to find out if we need to use
    the bottom VA range, the top VA range, or use the address as is
    (for VHE).
    
    Instead, we can expand our current helper to generate the right
    mask or nop with code patching. We default to using the top VA
    space, with alternatives to switch to the bottom one or to nop
    out the instructions.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 1d81f9abd172..cff510574fae 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -25,17 +25,6 @@
 
 #define __hyp_text __section(.hyp.text) notrace
 
-static inline unsigned long __kern_hyp_va(unsigned long v)
-{
-	asm volatile(ALTERNATIVE("and %0, %0, %1",
-				 "nop",
-				 ARM64_HAS_VIRT_HOST_EXTN)
-		     : "+r" (v) : "i" (HYP_PAGE_OFFSET_MASK));
-	return v;
-}
-
-#define kern_hyp_va(v) (typeof(v))(__kern_hyp_va((unsigned long)(v)))
-
 #define read_sysreg_elx(r,nvh,vh)					\
 	({								\
 		u64 reg;						\

commit 3f0f8830d440e3edf5580424519a7c3434891c64
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:36 2016 +0100

    arm/arm64: KVM: Remove hyp_kern_va helper
    
    hyp_kern_va is now completely unused, so let's remove it entirely.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 44eaff70da6a..1d81f9abd172 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -36,18 +36,6 @@ static inline unsigned long __kern_hyp_va(unsigned long v)
 
 #define kern_hyp_va(v) (typeof(v))(__kern_hyp_va((unsigned long)(v)))
 
-static inline unsigned long __hyp_kern_va(unsigned long v)
-{
-	u64 offset = PAGE_OFFSET - HYP_PAGE_OFFSET;
-	asm volatile(ALTERNATIVE("add %0, %0, %1",
-				 "nop",
-				 ARM64_HAS_VIRT_HOST_EXTN)
-		     : "+r" (v) : "r" (offset));
-	return v;
-}
-
-#define hyp_kern_va(v) (typeof(v))(__hyp_kern_va((unsigned long)(v)))
-
 #define read_sysreg_elx(r,nvh,vh)					\
 	({								\
 		u64 reg;						\

commit b8cfadfcefdc8c306ca2c0b1bdbdd4e01f0155e3
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Thu Mar 24 16:01:16 2016 +0000

    arm64: perf: Move PMU register related defines to asm/perf_event.h
    
    To use the ARMv8 PMU related register defines from the KVM code, we move
    the relevant definitions to asm/perf_event.h header file and rename them
    with prefix ARMV8_PMU_. This allows us to get rid of kvm_perf_event.h.
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index a46b019ebcf5..44eaff70da6a 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -21,7 +21,6 @@
 #include <linux/compiler.h>
 #include <linux/kvm_host.h>
 #include <asm/kvm_mmu.h>
-#include <asm/kvm_perf_event.h>
 #include <asm/sysreg.h>
 
 #define __hyp_text __section(.hyp.text) notrace

commit d692b8ad6ec4814ddd9a37ce5c9c9d971e741088
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Sep 8 15:15:56 2015 +0800

    arm64: KVM: Add access handler for PMUSERENR register
    
    This register resets as unknown in 64bit mode while it resets as zero
    in 32bit mode. Here we choose to reset it as zero for consistency.
    
    PMUSERENR_EL0 holds some bits which decide whether PMU registers can be
    accessed from EL0. Add some check helpers to handle the access from EL0.
    
    When these bits are zero, only reading PMUSERENR will trap to EL2 and
    writing PMUSERENR or reading/writing other PMU registers will trap to
    EL1 other than EL2 when HCR.TGE==0. To current KVM configuration
    (HCR.TGE==0) there is no way to get these traps. Here we write 0xf to
    physical PMUSERENR register on VM entry, so that it will trap PMU access
    from EL0 to EL2. Within the register access handler we check the real
    value of guest PMUSERENR register to decide whether this access is
    allowed. If not allowed, return false to inject UND to guest.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
index 44eaff70da6a..a46b019ebcf5 100644
--- a/arch/arm64/include/asm/kvm_hyp.h
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -21,6 +21,7 @@
 #include <linux/compiler.h>
 #include <linux/kvm_host.h>
 #include <asm/kvm_mmu.h>
+#include <asm/kvm_perf_event.h>
 #include <asm/sysreg.h>
 
 #define __hyp_text __section(.hyp.text) notrace

commit 13720a56edbd8164fbfa251067dea9776e09f54b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jan 28 13:44:07 2016 +0000

    arm64: KVM: Move kvm/hyp/hyp.h to include/asm/kvm_hyp.h
    
    In order to be able to move code outside of kvm/hyp, we need to make
    the global hyp.h file accessible from a standard location.
    
    include/asm/kvm_hyp.h seems good enough.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_hyp.h b/arch/arm64/include/asm/kvm_hyp.h
new file mode 100644
index 000000000000..44eaff70da6a
--- /dev/null
+++ b/arch/arm64/include/asm/kvm_hyp.h
@@ -0,0 +1,180 @@
+/*
+ * Copyright (C) 2015 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __ARM64_KVM_HYP_H__
+#define __ARM64_KVM_HYP_H__
+
+#include <linux/compiler.h>
+#include <linux/kvm_host.h>
+#include <asm/kvm_mmu.h>
+#include <asm/sysreg.h>
+
+#define __hyp_text __section(.hyp.text) notrace
+
+static inline unsigned long __kern_hyp_va(unsigned long v)
+{
+	asm volatile(ALTERNATIVE("and %0, %0, %1",
+				 "nop",
+				 ARM64_HAS_VIRT_HOST_EXTN)
+		     : "+r" (v) : "i" (HYP_PAGE_OFFSET_MASK));
+	return v;
+}
+
+#define kern_hyp_va(v) (typeof(v))(__kern_hyp_va((unsigned long)(v)))
+
+static inline unsigned long __hyp_kern_va(unsigned long v)
+{
+	u64 offset = PAGE_OFFSET - HYP_PAGE_OFFSET;
+	asm volatile(ALTERNATIVE("add %0, %0, %1",
+				 "nop",
+				 ARM64_HAS_VIRT_HOST_EXTN)
+		     : "+r" (v) : "r" (offset));
+	return v;
+}
+
+#define hyp_kern_va(v) (typeof(v))(__hyp_kern_va((unsigned long)(v)))
+
+#define read_sysreg_elx(r,nvh,vh)					\
+	({								\
+		u64 reg;						\
+		asm volatile(ALTERNATIVE("mrs %0, " __stringify(r##nvh),\
+					 "mrs_s %0, " __stringify(r##vh),\
+					 ARM64_HAS_VIRT_HOST_EXTN)	\
+			     : "=r" (reg));				\
+		reg;							\
+	})
+
+#define write_sysreg_elx(v,r,nvh,vh)					\
+	do {								\
+		u64 __val = (u64)(v);					\
+		asm volatile(ALTERNATIVE("msr " __stringify(r##nvh) ", %x0",\
+					 "msr_s " __stringify(r##vh) ", %x0",\
+					 ARM64_HAS_VIRT_HOST_EXTN)	\
+					 : : "rZ" (__val));		\
+	} while (0)
+
+/*
+ * Unified accessors for registers that have a different encoding
+ * between VHE and non-VHE. They must be specified without their "ELx"
+ * encoding.
+ */
+#define read_sysreg_el2(r)						\
+	({								\
+		u64 reg;						\
+		asm volatile(ALTERNATIVE("mrs %0, " __stringify(r##_EL2),\
+					 "mrs %0, " __stringify(r##_EL1),\
+					 ARM64_HAS_VIRT_HOST_EXTN)	\
+			     : "=r" (reg));				\
+		reg;							\
+	})
+
+#define write_sysreg_el2(v,r)						\
+	do {								\
+		u64 __val = (u64)(v);					\
+		asm volatile(ALTERNATIVE("msr " __stringify(r##_EL2) ", %x0",\
+					 "msr " __stringify(r##_EL1) ", %x0",\
+					 ARM64_HAS_VIRT_HOST_EXTN)	\
+					 : : "rZ" (__val));		\
+	} while (0)
+
+#define read_sysreg_el0(r)	read_sysreg_elx(r, _EL0, _EL02)
+#define write_sysreg_el0(v,r)	write_sysreg_elx(v, r, _EL0, _EL02)
+#define read_sysreg_el1(r)	read_sysreg_elx(r, _EL1, _EL12)
+#define write_sysreg_el1(v,r)	write_sysreg_elx(v, r, _EL1, _EL12)
+
+/* The VHE specific system registers and their encoding */
+#define sctlr_EL12              sys_reg(3, 5, 1, 0, 0)
+#define cpacr_EL12              sys_reg(3, 5, 1, 0, 2)
+#define ttbr0_EL12              sys_reg(3, 5, 2, 0, 0)
+#define ttbr1_EL12              sys_reg(3, 5, 2, 0, 1)
+#define tcr_EL12                sys_reg(3, 5, 2, 0, 2)
+#define afsr0_EL12              sys_reg(3, 5, 5, 1, 0)
+#define afsr1_EL12              sys_reg(3, 5, 5, 1, 1)
+#define esr_EL12                sys_reg(3, 5, 5, 2, 0)
+#define far_EL12                sys_reg(3, 5, 6, 0, 0)
+#define mair_EL12               sys_reg(3, 5, 10, 2, 0)
+#define amair_EL12              sys_reg(3, 5, 10, 3, 0)
+#define vbar_EL12               sys_reg(3, 5, 12, 0, 0)
+#define contextidr_EL12         sys_reg(3, 5, 13, 0, 1)
+#define cntkctl_EL12            sys_reg(3, 5, 14, 1, 0)
+#define cntp_tval_EL02          sys_reg(3, 5, 14, 2, 0)
+#define cntp_ctl_EL02           sys_reg(3, 5, 14, 2, 1)
+#define cntp_cval_EL02          sys_reg(3, 5, 14, 2, 2)
+#define cntv_tval_EL02          sys_reg(3, 5, 14, 3, 0)
+#define cntv_ctl_EL02           sys_reg(3, 5, 14, 3, 1)
+#define cntv_cval_EL02          sys_reg(3, 5, 14, 3, 2)
+#define spsr_EL12               sys_reg(3, 5, 4, 0, 0)
+#define elr_EL12                sys_reg(3, 5, 4, 0, 1)
+
+/**
+ * hyp_alternate_select - Generates patchable code sequences that are
+ * used to switch between two implementations of a function, depending
+ * on the availability of a feature.
+ *
+ * @fname: a symbol name that will be defined as a function returning a
+ * function pointer whose type will match @orig and @alt
+ * @orig: A pointer to the default function, as returned by @fname when
+ * @cond doesn't hold
+ * @alt: A pointer to the alternate function, as returned by @fname
+ * when @cond holds
+ * @cond: a CPU feature (as described in asm/cpufeature.h)
+ */
+#define hyp_alternate_select(fname, orig, alt, cond)			\
+typeof(orig) * __hyp_text fname(void)					\
+{									\
+	typeof(alt) *val = orig;					\
+	asm volatile(ALTERNATIVE("nop		\n",			\
+				 "mov	%0, %1	\n",			\
+				 cond)					\
+		     : "+r" (val) : "r" (alt));				\
+	return val;							\
+}
+
+void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
+void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
+
+void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
+void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
+
+void __timer_save_state(struct kvm_vcpu *vcpu);
+void __timer_restore_state(struct kvm_vcpu *vcpu);
+
+void __sysreg_save_host_state(struct kvm_cpu_context *ctxt);
+void __sysreg_restore_host_state(struct kvm_cpu_context *ctxt);
+void __sysreg_save_guest_state(struct kvm_cpu_context *ctxt);
+void __sysreg_restore_guest_state(struct kvm_cpu_context *ctxt);
+void __sysreg32_save_state(struct kvm_vcpu *vcpu);
+void __sysreg32_restore_state(struct kvm_vcpu *vcpu);
+
+void __debug_save_state(struct kvm_vcpu *vcpu,
+			struct kvm_guest_debug_arch *dbg,
+			struct kvm_cpu_context *ctxt);
+void __debug_restore_state(struct kvm_vcpu *vcpu,
+			   struct kvm_guest_debug_arch *dbg,
+			   struct kvm_cpu_context *ctxt);
+void __debug_cond_save_host_state(struct kvm_vcpu *vcpu);
+void __debug_cond_restore_host_state(struct kvm_vcpu *vcpu);
+
+void __fpsimd_save_state(struct user_fpsimd_state *fp_regs);
+void __fpsimd_restore_state(struct user_fpsimd_state *fp_regs);
+bool __fpsimd_enabled(void);
+
+u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
+void __noreturn __hyp_do_panic(unsigned long, ...);
+
+#endif /* __ARM64_KVM_HYP_H__ */
+
