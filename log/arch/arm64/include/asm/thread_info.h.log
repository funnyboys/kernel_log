commit ac2081cdc4d99c57f219c1a6171526e0fa0a6fff
Author: Will Deacon <will@kernel.org>
Date:   Thu Jul 2 21:16:20 2020 +0100

    arm64: ptrace: Consistently use pseudo-singlestep exceptions
    
    Although the arm64 single-step state machine can be fast-forwarded in
    cases where we wish to generate a SIGTRAP without actually executing an
    instruction, this has two major limitations outside of simply skipping
    an instruction due to emulation.
    
    1. Stepping out of a ptrace signal stop into a signal handler where
       SIGTRAP is blocked. Fast-forwarding the stepping state machine in
       this case will result in a forced SIGTRAP, with the handler reset to
       SIG_DFL.
    
    2. The hardware implicitly fast-forwards the state machine when executing
       an SVC instruction for issuing a system call. This can interact badly
       with subsequent ptrace stops signalled during the execution of the
       system call (e.g. SYSCALL_EXIT or seccomp traps), as they may corrupt
       the stepping state by updating the PSTATE for the tracee.
    
    Resolve both of these issues by injecting a pseudo-singlestep exception
    on entry to a signal handler and also on return to userspace following a
    system call.
    
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Luis Machado <luis.machado@linaro.org>
    Reported-by: Keno Fischer <keno@juliacomputing.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 6ea8b6a26ae9..5e784e16ee89 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -93,6 +93,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_FSCHECK		(1 << TIF_FSCHECK)
+#define _TIF_SINGLESTEP		(1 << TIF_SINGLESTEP)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 #define _TIF_SVE		(1 << TIF_SVE)
 

commit 51189c7a7ed1b4ed4493e27275d466ff60406d3a
Author: Will Deacon <will@kernel.org>
Date:   Fri May 15 14:11:05 2020 +0100

    arm64: scs: Store absolute SCS stack pointer value in thread_info
    
    Storing the SCS information in thread_info as a {base,offset} pair
    introduces an additional load instruction on the ret-to-user path,
    since the SCS stack pointer in x18 has to be converted back to an offset
    by subtracting the base.
    
    Replace the offset with the absolute SCS stack pointer value instead
    and avoid the redundant load.
    
    Tested-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 9df79c0a4c43..6ea8b6a26ae9 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -43,7 +43,7 @@ struct thread_info {
 	};
 #ifdef CONFIG_SHADOW_CALL_STACK
 	void			*scs_base;
-	unsigned long		scs_offset;
+	void			*scs_sp;
 #endif
 };
 
@@ -107,7 +107,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #ifdef CONFIG_SHADOW_CALL_STACK
 #define INIT_SCS							\
 	.scs_base	= init_shadow_call_stack,			\
-	.scs_offset	= 0,
+	.scs_sp		= init_shadow_call_stack,
 #else
 #define INIT_SCS
 #endif

commit 5287569a790d2546a06db07e391bf84b8bd6cf51
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Mon Apr 27 09:00:16 2020 -0700

    arm64: Implement Shadow Call Stack
    
    This change implements shadow stack switching, initial SCS set-up,
    and interrupt shadow stacks for arm64.
    
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 512174a8e789..9df79c0a4c43 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -41,6 +41,10 @@ struct thread_info {
 #endif
 		} preempt;
 	};
+#ifdef CONFIG_SHADOW_CALL_STACK
+	void			*scs_base;
+	unsigned long		scs_offset;
+#endif
 };
 
 #define thread_saved_pc(tsk)	\
@@ -100,11 +104,20 @@ void arch_release_task_struct(struct task_struct *tsk);
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
 				 _TIF_SYSCALL_EMU)
 
+#ifdef CONFIG_SHADOW_CALL_STACK
+#define INIT_SCS							\
+	.scs_base	= init_shadow_call_stack,			\
+	.scs_offset	= 0,
+#else
+#define INIT_SCS
+#endif
+
 #define INIT_THREAD_INFO(tsk)						\
 {									\
 	.flags		= _TIF_FOREIGN_FPSTATE,				\
 	.preempt_count	= INIT_PREEMPT_COUNT,				\
 	.addr_limit	= KERNEL_DS,					\
+	INIT_SCS							\
 }
 
 #endif /* __ASM_THREAD_INFO_H */

commit 320a4fc2d1b0c2314342dfdd3348270f126196a4
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Tue Jan 28 13:50:32 2020 +0100

    arm64: Remove TIF_NOHZ
    
    The syscall slow path is spuriously invoked when context tracking is
    activated while the entry code calls context tracking from fast path.
    
    Remove that overhead and the unused flag itself while at it.
    
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index f0cec4160136..512174a8e789 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -63,7 +63,6 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
 #define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
 #define TIF_FSCHECK		5	/* Check FS is USER_DS on return */
-#define TIF_NOHZ		7
 #define TIF_SYSCALL_TRACE	8	/* syscall trace active */
 #define TIF_SYSCALL_AUDIT	9	/* syscall auditing */
 #define TIF_SYSCALL_TRACEPOINT	10	/* syscall tracepoint for ftrace */
@@ -83,7 +82,6 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
 #define _TIF_FOREIGN_FPSTATE	(1 << TIF_FOREIGN_FPSTATE)
-#define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
@@ -100,7 +98,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
-				 _TIF_NOHZ | _TIF_SYSCALL_EMU)
+				 _TIF_SYSCALL_EMU)
 
 #define INIT_THREAD_INFO(tsk)						\
 {									\

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit 63f0c60379650d82250f22e4cf4137ef3dc4f43d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 23 19:58:39 2019 +0200

    arm64: Introduce prctl() options to control the tagged user addresses ABI
    
    It is not desirable to relax the ABI to allow tagged user addresses into
    the kernel indiscriminately. This patch introduces a prctl() interface
    for enabling or disabling the tagged ABI with a global sysctl control
    for preventing applications from enabling the relaxed ABI (meant for
    testing user-space prctl() return error checking without reconfiguring
    the kernel). The ABI properties are inherited by threads of the same
    application and fork()'ed children but cleared on execve(). A Kconfig
    option allows the overall disabling of the relaxed ABI.
    
    The PR_SET_TAGGED_ADDR_CTRL will be expanded in the future to handle
    MTE-specific settings like imprecise vs precise exceptions.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 180b34ec5965..012238d8e58d 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -90,6 +90,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_SVE			23	/* Scalable Vector Extension in use */
 #define TIF_SVE_VL_INHERIT	24	/* Inherit sve_vl_onexec across exec */
 #define TIF_SSBD		25	/* Wants SSB mitigation */
+#define TIF_TAGGED_ADDR		26	/* Allow tagged user addresses */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)

commit 66cbdf5d0c96f5fe570b548e764583ea9d793077
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Wed Jul 31 15:35:20 2019 +0200

    arm64: Move TIF_* documentation to individual definitions
    
    Some TIF_* flags are documented in the comment block at the top, some
    next to their definitions, some in both places.
    
    Move all documentation to the individual definitions for consistency,
    and for easy lookup.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index e35cd84b102c..2a408dd64f5c 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -57,29 +57,18 @@ void arch_release_task_struct(struct task_struct *tsk);
 
 #endif
 
-/*
- * thread information flags:
- *  TIF_SYSCALL_TRACE	- syscall trace active
- *  TIF_SYSCALL_TRACEPOINT - syscall tracepoint for ftrace
- *  TIF_SYSCALL_AUDIT	- syscall auditing
- *  TIF_SECCOMP		- syscall secure computing
- *  TIF_SYSCALL_EMU     - syscall emulation active
- *  TIF_SIGPENDING	- signal pending
- *  TIF_NEED_RESCHED	- rescheduling necessary
- *  TIF_NOTIFY_RESUME	- callback before returning to user
- */
-#define TIF_SIGPENDING		0
-#define TIF_NEED_RESCHED	1
+#define TIF_SIGPENDING		0	/* signal pending */
+#define TIF_NEED_RESCHED	1	/* rescheduling necessary */
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
 #define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
 #define TIF_FSCHECK		5	/* Check FS is USER_DS on return */
 #define TIF_NOHZ		7
-#define TIF_SYSCALL_TRACE	8
-#define TIF_SYSCALL_AUDIT	9
-#define TIF_SYSCALL_TRACEPOINT	10
-#define TIF_SECCOMP		11
-#define TIF_SYSCALL_EMU		12
+#define TIF_SYSCALL_TRACE	8	/* syscall trace active */
+#define TIF_SYSCALL_AUDIT	9	/* syscall auditing */
+#define TIF_SYSCALL_TRACEPOINT	10	/* syscall tracepoint for ftrace */
+#define TIF_SECCOMP		11	/* syscall secure computing */
+#define TIF_SYSCALL_EMU		12	/* syscall emulation active */
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 #define TIF_FREEZE		19
 #define TIF_RESTORE_SIGMASK	20

commit b907b80d7ae7b2b65ef9f534f3e9a32ce6a4b539
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jul 8 17:36:40 2019 +0100

    arm64: remove pointless __KERNEL__ guards
    
    For a number of years, UAPI headers have been split from kernel-internal
    headers. The latter are never exposed to userspace, and always built
    with __KERNEL__ defined.
    
    Most headers under arch/arm64 don't have __KERNEL__ guards, but there
    are a few stragglers lying around. To make things more consistent, and
    to set a good example going forward, let's remove these redundant
    __KERNEL__ guards.
    
    In a couple of cases, a trailing #endif lacked a comment describing its
    corresponding #if or #ifdef, so these are fixes up at the same time.
    
    Guards in auto-generated crypto code are left as-is, as these guards are
    generated by scripting imported from the upstream openssl project
    scripts. Guards in UAPI headers are left as-is, as these can be included
    by userspace or the kernel.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 180b34ec5965..e35cd84b102c 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -8,8 +8,6 @@
 #ifndef __ASM_THREAD_INFO_H
 #define __ASM_THREAD_INFO_H
 
-#ifdef __KERNEL__
-
 #include <linux/compiler.h>
 
 #ifndef __ASSEMBLY__
@@ -121,5 +119,4 @@ void arch_release_task_struct(struct task_struct *tsk);
 	.addr_limit	= KERNEL_DS,					\
 }
 
-#endif /* __KERNEL__ */
 #endif /* __ASM_THREAD_INFO_H */

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index f1d032be628a..2372e97db29c 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -1,20 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Based on arch/arm/include/asm/thread_info.h
  *
  * Copyright (C) 2002 Russell King.
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_THREAD_INFO_H
 #define __ASM_THREAD_INFO_H

commit 2b55d83e9a8ca36ab9e108dab52902a67f573f6f
Author: George G. Davis <george_davis@mentor.com>
Date:   Wed Jun 5 16:30:09 2019 -0400

    ARM64: trivial: s/TIF_SECOMP/TIF_SECCOMP/ comment typo fix
    
    Fix a s/TIF_SECOMP/TIF_SECCOMP/ comment typo
    
    Cc: Jiri Kosina <trivial@kernel.org>
    Reviewed-by: Kees Cook <keescook@chromium.org
    Signed-off-by: George G. Davis <george_davis@mentor.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index eb3ef73e07cf..f1d032be628a 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -75,7 +75,7 @@ void arch_release_task_struct(struct task_struct *tsk);
  *  TIF_SYSCALL_TRACE	- syscall trace active
  *  TIF_SYSCALL_TRACEPOINT - syscall tracepoint for ftrace
  *  TIF_SYSCALL_AUDIT	- syscall auditing
- *  TIF_SECOMP		- syscall secure computing
+ *  TIF_SECCOMP		- syscall secure computing
  *  TIF_SIGPENDING	- signal pending
  *  TIF_NEED_RESCHED	- rescheduling necessary
  *  TIF_NOTIFY_RESUME	- callback before returning to user

commit f086f67485c5c126bcec4b0e96ac7319a2e59ab8
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Thu May 23 10:06:18 2019 +0100

    arm64: ptrace: add support for syscall emulation
    
    Add PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP support on arm64.
    We don't need any special handling for PTRACE_SYSEMU_SINGLESTEP.
    
    It's quite difficult to generalize handling PTRACE_SYSEMU cross
    architectures and avoid calls to tracehook_report_syscall_entry twice.
    Different architecture have different mechanism to indicate NO_SYSCALL
    and trying to generalise adds more code for no gain.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index eb3ef73e07cf..c285d1ce7186 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -75,6 +75,7 @@ void arch_release_task_struct(struct task_struct *tsk);
  *  TIF_SYSCALL_TRACE	- syscall trace active
  *  TIF_SYSCALL_TRACEPOINT - syscall tracepoint for ftrace
  *  TIF_SYSCALL_AUDIT	- syscall auditing
+ *  TIF_SYSCALL_EMU     - syscall emulation active
  *  TIF_SECOMP		- syscall secure computing
  *  TIF_SIGPENDING	- signal pending
  *  TIF_NEED_RESCHED	- rescheduling necessary
@@ -91,6 +92,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_SYSCALL_AUDIT	9
 #define TIF_SYSCALL_TRACEPOINT	10
 #define TIF_SECCOMP		11
+#define TIF_SYSCALL_EMU		12
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 #define TIF_FREEZE		19
 #define TIF_RESTORE_SIGMASK	20
@@ -109,6 +111,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
+#define _TIF_SYSCALL_EMU	(1 << TIF_SYSCALL_EMU)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_FSCHECK		(1 << TIF_FSCHECK)
 #define _TIF_32BIT		(1 << TIF_32BIT)
@@ -120,7 +123,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
-				 _TIF_NOHZ)
+				 _TIF_NOHZ | _TIF_SYSCALL_EMU)
 
 #define INIT_THREAD_INFO(tsk)						\
 {									\

commit 47224e51ab778e918257b96c1a1b3735e4b8c15d
Author: Julien Grall <julien.grall@arm.com>
Date:   Fri Feb 8 17:04:25 2019 +0000

    arm64: Remove documentation about TIF_USEDFPU
    
    TIF_USEDFPU is not defined as thread flags for Arm64. So drop it from
    the documentation.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Julien Grall <julien.grall@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index bbca68b54732..eb3ef73e07cf 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -79,7 +79,6 @@ void arch_release_task_struct(struct task_struct *tsk);
  *  TIF_SIGPENDING	- signal pending
  *  TIF_NEED_RESCHED	- rescheduling necessary
  *  TIF_NOTIFY_RESUME	- callback before returning to user
- *  TIF_USEDFPU		- FPU was used by this task this quantum (SMP)
  */
 #define TIF_SIGPENDING		0
 #define TIF_NEED_RESCHED	1

commit 84931327a807a4dd65d0d6b53a8ae47845c91f79
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Dec 13 13:14:06 2018 +0000

    arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
    
    We don't need to get at the per-thread keys from assembly at all, so
    they can live alongside the rest of the per-thread register state in
    thread_struct instead of thread_info.
    
    This will also allow straighforward whitelisting of the keys for
    hardened usercopy should we expose them via a ptrace request later on.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index f8f66ad9dd8f..bbca68b54732 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -28,7 +28,6 @@
 struct task_struct;
 
 #include <asm/memory.h>
-#include <asm/pointer_auth.h>
 #include <asm/stack_pointer.h>
 #include <asm/types.h>
 
@@ -55,9 +54,6 @@ struct thread_info {
 #endif
 		} preempt;
 	};
-#ifdef CONFIG_ARM64_PTR_AUTH
-	struct ptrauth_keys	keys_user;
-#endif
 };
 
 #define thread_saved_pc(tsk)	\

commit 7503197562567b57ec14feb3a9d5400ebc56812f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:25 2018 +0000

    arm64: add basic pointer authentication support
    
    This patch adds basic support for pointer authentication, allowing
    userspace to make use of APIAKey, APIBKey, APDAKey, APDBKey, and
    APGAKey. The kernel maintains key values for each process (shared by all
    threads within), which are initialised to random values at exec() time.
    
    The ID_AA64ISAR1_EL1.{APA,API,GPA,GPI} fields are exposed to userspace,
    to describe that pointer authentication instructions are available and
    that the kernel is managing the keys. Two new hwcaps are added for the
    same reason: PACA (for address authentication) and PACG (for generic
    authentication).
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Tested-by: Adam Wallis <awallis@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [will: Fix sizeof() usage and unroll address key initialisation]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index bbca68b54732..f8f66ad9dd8f 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -28,6 +28,7 @@
 struct task_struct;
 
 #include <asm/memory.h>
+#include <asm/pointer_auth.h>
 #include <asm/stack_pointer.h>
 #include <asm/types.h>
 
@@ -54,6 +55,9 @@ struct thread_info {
 #endif
 		} preempt;
 	};
+#ifdef CONFIG_ARM64_PTR_AUTH
+	struct ptrauth_keys	keys_user;
+#endif
 };
 
 #define thread_saved_pc(tsk)	\

commit 396244692232fcf0881cb6ba2404be2906f47681
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 20 10:26:40 2018 +0100

    arm64: preempt: Provide our own implementation of asm/preempt.h
    
    The asm-generic/preempt.h implementation doesn't make use of the
    PREEMPT_NEED_RESCHED flag, since this can interact badly with load/store
    architectures which rely on the preempt_count word being unchanged across
    an interrupt.
    
    However, since we're a 64-bit architecture and the preempt count is
    only 32 bits wide, we can simply pack it next to the resched flag and
    load the whole thing in one go, so that a dec-and-test operation doesn't
    need to load twice.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index cb2c10a8f0a8..bbca68b54732 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -42,7 +42,18 @@ struct thread_info {
 #ifdef CONFIG_ARM64_SW_TTBR0_PAN
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
-	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	union {
+		u64		preempt_count;	/* 0 => preemptible, <0 => bug */
+		struct {
+#ifdef CONFIG_CPU_BIG_ENDIAN
+			u32	need_resched;
+			u32	count;
+#else
+			u32	count;
+			u32	need_resched;
+#endif
+		} preempt;
+	};
 };
 
 #define thread_saved_pc(tsk)	\

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit 9dd9614f5476687abbff8d4b12cd08ae70d7c2ad
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:13 2018 +0100

    arm64: ssbd: Introduce thread flag to control userspace mitigation
    
    In order to allow userspace to be mitigated on demand, let's
    introduce a new thread flag that prevents the mitigation from
    being turned off when exiting to userspace, and doesn't turn
    it on on entry into the kernel (with the assumption that the
    mitigation is always enabled in the kernel itself).
    
    This will be used by a prctl interface introduced in a later
    patch.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 740aa03c5f0d..cbcf11b5e637 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -94,6 +94,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_32BIT		22	/* 32bit process */
 #define TIF_SVE			23	/* Scalable Vector Extension in use */
 #define TIF_SVE_VL_INHERIT	24	/* Inherit sve_vl_onexec across exec */
+#define TIF_SSBD		25	/* Wants SSB mitigation */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)

commit 66e48a0d29bdedc574c8fc0af7a5d112b594ced6
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu May 24 15:54:30 2018 +0100

    arm64: fpsimd: Avoid FPSIMD context leakage for the init task
    
    The init task is started with thread_flags equal to 0, which means
    that TIF_FOREIGN_FPSTATE is initially clear.
    
    It is theoretically possible (if unlikely) that the init task could
    reach userspace without ever being scheduled out.  If this occurs,
    data left in the FPSIMD registers by the kernel could be exposed.
    
    This patch fixes this anomaly by ensuring that the init task's
    initial TIF_FOREIGN_FPSTATE is set.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Fixes: 005f78cd8849 ("arm64: defer reloading a task's FPSIMD state to userland resume")
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 740aa03c5f0d..af271f9a6c9f 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -45,12 +45,6 @@ struct thread_info {
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
 };
 
-#define INIT_THREAD_INFO(tsk)						\
-{									\
-	.preempt_count	= INIT_PREEMPT_COUNT,				\
-	.addr_limit	= KERNEL_DS,					\
-}
-
 #define thread_saved_pc(tsk)	\
 	((unsigned long)(tsk->thread.cpu_context.pc))
 #define thread_saved_sp(tsk)	\
@@ -117,5 +111,12 @@ void arch_release_task_struct(struct task_struct *tsk);
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
 				 _TIF_NOHZ)
 
+#define INIT_THREAD_INFO(tsk)						\
+{									\
+	.flags		= _TIF_FOREIGN_FPSTATE,				\
+	.preempt_count	= INIT_PREEMPT_COUNT,				\
+	.addr_limit	= KERNEL_DS,					\
+}
+
 #endif /* __KERNEL__ */
 #endif /* __ASM_THREAD_INFO_H */

commit 0500871f21b237b2bea2d9db405eadf78e5aab05
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Construct init thread stack in the linker script rather than by union
    
    Construct the init thread stack in the linker script rather than doing it
    by means of a union so that ia64's init_task.c can be got rid of.
    
    The following symbols are then made available from INIT_TASK_DATA() linker
    script macro:
    
            init_thread_union
            init_stack
    
    INIT_TASK_DATA() also expands the region to THREAD_SIZE to accommodate the
    size of the init stack.  init_thread_union is given its own section so that
    it can be placed into the stack space in the right order.  I'm assuming
    that the ia64 ordering is correct and that the task_struct is first and the
    thread_info second.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index eb431286bacd..740aa03c5f0d 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -51,8 +51,6 @@ struct thread_info {
 	.addr_limit	= KERNEL_DS,					\
 }
 
-#define init_stack		(init_thread_union.stack)
-
 #define thread_saved_pc(tsk)	\
 	((unsigned long)(tsk->thread.cpu_context.pc))
 #define thread_saved_sp(tsk)	\

commit 79ab047c75d6a9f95d8840d94f405e20cbacac4b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:06 2017 +0000

    arm64/sve: Support vector length resetting for new processes
    
    It's desirable to be able to reset the vector length to some sane
    default for new processes, since the new binary and its libraries
    may or may not be SVE-aware.
    
    This patch tracks the desired post-exec vector length (if any) in a
    new thread member sve_vl_onexec, and adds a new thread flag
    TIF_SVE_VL_INHERIT to control whether to inherit or reset the
    vector length.  Currently these are inactive.  Subsequent patches
    will provide the capability to configure them.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 92b7b48576c8..eb431286bacd 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -95,6 +95,7 @@ void arch_release_task_struct(struct task_struct *tsk);
 #define TIF_SINGLESTEP		21
 #define TIF_32BIT		22	/* 32bit process */
 #define TIF_SVE			23	/* Scalable Vector Extension in use */
+#define TIF_SVE_VL_INHERIT	24	/* Inherit sve_vl_onexec across exec */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)

commit bc0ee476036478a85beeed51f0d94c8729fd0544
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:05 2017 +0000

    arm64/sve: Core task context handling
    
    This patch adds the core support for switching and managing the SVE
    architectural state of user tasks.
    
    Calls to the existing FPSIMD low-level save/restore functions are
    factored out as new functions task_fpsimd_{save,load}(), since SVE
    now dynamically may or may not need to be handled at these points
    depending on the kernel configuration, hardware features discovered
    at boot, and the runtime state of the task.  To make these
    decisions as fast as possible, const cpucaps are used where
    feasible, via the system_supports_sve() helper.
    
    The SVE registers are only tracked for threads that have explicitly
    used SVE, indicated by the new thread flag TIF_SVE.  Otherwise, the
    FPSIMD view of the architectural state is stored in
    thread.fpsimd_state as usual.
    
    When in use, the SVE registers are not stored directly in
    thread_struct due to their potentially large and variable size.
    Because the task_struct slab allocator must be configured very
    early during kernel boot, it is also tricky to configure it
    correctly to match the maximum vector length provided by the
    hardware, since this depends on examining secondary CPUs as well as
    the primary.  Instead, a pointer sve_state in thread_struct points
    to a dynamically allocated buffer containing the SVE register data,
    and code is added to allocate and free this buffer at appropriate
    times.
    
    TIF_SVE is set when taking an SVE access trap from userspace, if
    suitable hardware support has been detected.  This enables SVE for
    the thread: a subsequent return to userspace will disable the trap
    accordingly.  If such a trap is taken without sufficient system-
    wide hardware support, SIGILL is sent to the thread instead as if
    an undefined instruction had been executed: this may happen if
    userspace tries to use SVE in a system where not all CPUs support
    it for example.
    
    The kernel will clear TIF_SVE and disable SVE for the thread
    whenever an explicit syscall is made by userspace.  For backwards
    compatibility reasons and conformance with the spirit of the base
    AArch64 procedure call standard, the subset of the SVE register
    state that aliases the FPSIMD registers is still preserved across a
    syscall even if this happens.  The remainder of the SVE register
    state logically becomes zero at syscall entry, though the actual
    zeroing work is currently deferred until the thread next tries to
    use SVE, causing another trap to the kernel.  This implementation
    is suboptimal: in the future, the fastpath case may be optimised
    to zero the registers in-place and leave SVE enabled for the task,
    where beneficial.
    
    TIF_SVE is also cleared in the following slowpath cases, which are
    taken as reasonable hints that the task may no longer use SVE:
     * exec
     * fork and clone
    
    Code is added to sync data between thread.fpsimd_state and
    thread.sve_state whenever enabling/disabling SVE, in a manner
    consistent with the SVE architectural programmer's model.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    [will: added #include to fix allnoconfig build]
    [will: use enable_daif in do_sve_acc]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index ddded6497a8a..92b7b48576c8 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -63,6 +63,8 @@ struct thread_info {
 void arch_setup_new_exec(void);
 #define arch_setup_new_exec     arch_setup_new_exec
 
+void arch_release_task_struct(struct task_struct *tsk);
+
 #endif
 
 /*
@@ -92,6 +94,7 @@ void arch_setup_new_exec(void);
 #define TIF_RESTORE_SIGMASK	20
 #define TIF_SINGLESTEP		21
 #define TIF_32BIT		22	/* 32bit process */
+#define TIF_SVE			23	/* Scalable Vector Extension in use */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
@@ -105,6 +108,7 @@ void arch_setup_new_exec(void);
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_FSCHECK		(1 << TIF_FSCHECK)
 #define _TIF_32BIT		(1 << TIF_32BIT)
+#define _TIF_SVE		(1 << TIF_SVE)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \

commit 04759194dc447ff0b9ef35bc641ce3bb076c2930
Merge: 9e85ae6af6e9 d1be5c99a034
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 5 09:53:37 2017 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - VMAP_STACK support, allowing the kernel stacks to be allocated in the
       vmalloc space with a guard page for trapping stack overflows. One of
       the patches introduces THREAD_ALIGN and changes the generic
       alloc_thread_stack_node() to use this instead of THREAD_SIZE (no
       functional change for other architectures)
    
     - Contiguous PTE hugetlb support re-enabled (after being reverted a
       couple of times). We now have the semantics agreed in the generic mm
       layer together with API improvements so that the architecture code
       can detect between contiguous and non-contiguous huge PTEs
    
     - Initial support for persistent memory on ARM: DC CVAP instruction
       exposed to user space (HWCAP) and the in-kernel pmem API implemented
    
     - raid6 improvements for arm64: faster algorithm for the delta syndrome
       and implementation of the recovery routines using Neon
    
     - FP/SIMD refactoring and removal of support for Neon in interrupt
       context. This is in preparation for full SVE support
    
     - PTE accessors converted from inline asm to cmpxchg so that we can use
       LSE atomics if available (ARMv8.1)
    
     - Perf support for Cortex-A35 and A73
    
     - Non-urgent fixes and cleanups
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (75 commits)
      arm64: cleanup {COMPAT_,}SET_PERSONALITY() macro
      arm64: introduce separated bits for mm_context_t flags
      arm64: hugetlb: Cleanup setup_hugepagesz
      arm64: Re-enable support for contiguous hugepages
      arm64: hugetlb: Override set_huge_swap_pte_at() to support contiguous hugepages
      arm64: hugetlb: Override huge_pte_clear() to support contiguous hugepages
      arm64: hugetlb: Handle swap entries in huge_pte_offset() for contiguous hugepages
      arm64: hugetlb: Add break-before-make logic for contiguous entries
      arm64: hugetlb: Spring clean huge pte accessors
      arm64: hugetlb: Introduce pte_pgprot helper
      arm64: hugetlb: set_huge_pte_at Add WARN_ON on !pte_present
      arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
      arm64: dma-mapping: Mark atomic_pool as __ro_after_init
      arm64: dma-mapping: Do not pass data to gen_pool_set_algo()
      arm64: Remove the !CONFIG_ARM64_HW_AFDBM alternative code paths
      arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()
      arm64: Move PTE_RDONLY bit handling out of set_pte_at()
      kvm: arm64: Convert kvm_set_s2pte_readonly() from inline asm to cmpxchg()
      arm64: Convert pte handling from inline asm to using (cmp)xchg
      arm64: neon/efi: Make EFI fpsimd save/restore variables static
      ...

commit d1be5c99a0341249bf6f74eb1cbc3d5fc4ef2be7
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Sun Aug 20 13:20:48 2017 +0300

    arm64: cleanup {COMPAT_,}SET_PERSONALITY() macro
    
    There is some work that should be done after setting the personality.
    Currently it's done in the macro, which is not the best idea.
    
    In this patch new arch_setup_new_exec() routine is introduced, and all
    setup code is moved there, as suggested by Catalin:
    https://lkml.org/lkml/2017/8/4/494
    
    Cc: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    [catalin.marinas@arm.com: comments changed or removed]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index aa04b733b349..2eca178bc943 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -60,6 +60,9 @@ struct thread_info {
 #define thread_saved_fp(tsk)	\
 	((unsigned long)(tsk->thread.cpu_context.fp))
 
+void arch_setup_new_exec(void);
+#define arch_setup_new_exec     arch_setup_new_exec
+
 #endif
 
 /*

commit dbc9344a68e506f19f80a9affc8fe7023a9cdc4c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 16:39:21 2017 +0100

    arm64: clean up THREAD_* definitions
    
    Currently we define THREAD_SIZE and THREAD_SIZE_ORDER separately, with
    the latter dependent on particular CONFIG_ARM64_*K_PAGES definitions.
    This is somewhat opaque, and will get in the way of future modifications
    to THREAD_SIZE.
    
    This patch cleans this up, defining both in terms of a common
    THREAD_SHIFT, and using PAGE_SHIFT to calculate THREAD_SIZE_ORDER,
    rather than using a number of definitions dependent on config symbols.
    Subsequent patches will make use of this to alter the stack size used in
    some configurations.
    
    At the same time, these are moved into <asm/memory.h>, which will avoid
    circular include issues in subsequent patches. To ensure that existing
    code isn't adversely affected, <asm/thread_info.h> is updated to
    transitively include these definitions.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index b29ab0e12e60..aa04b733b349 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -23,18 +23,11 @@
 
 #include <linux/compiler.h>
 
-#ifdef CONFIG_ARM64_4K_PAGES
-#define THREAD_SIZE_ORDER	2
-#elif defined(CONFIG_ARM64_16K_PAGES)
-#define THREAD_SIZE_ORDER	0
-#endif
-
-#define THREAD_SIZE		16384
-
 #ifndef __ASSEMBLY__
 
 struct task_struct;
 
+#include <asm/memory.h>
 #include <asm/stack_pointer.h>
 #include <asm/types.h>
 

commit 34be98f4944f99076f049a6806fc5f5207a755d3
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Jul 20 17:15:45 2017 +0100

    arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
    
    For historical reasons, we leave the top 16 bytes of our task and IRQ
    stacks unused, a practice used to ensure that the SP can always be
    masked to find the base of the current stack (historically, where
    thread_info could be found).
    
    However, this is not necessary, as:
    
    * When an exception is taken from a task stack, we decrement the SP by
      S_FRAME_SIZE and stash the exception registers before we compare the
      SP against the task stack. In such cases, the SP must be at least
      S_FRAME_SIZE below the limit, and can be safely masked to determine
      whether the task stack is in use.
    
    * When transitioning to an IRQ stack, we'll place a dummy frame onto the
      IRQ stack before enabling asynchronous exceptions, or executing code
      we expect to trigger faults. Thus, if an exception is taken from the
      IRQ stack, the SP must be at least 16 bytes below the limit.
    
    * We no longer mask the SP to find the thread_info, which is now found
      via sp_el0. Note that historically, the offset was critical to ensure
      that cpu_switch_to() found the correct stack for new threads that
      hadn't yet executed ret_from_fork().
    
    Given that, this initial offset serves no purpose, and can be removed.
    This brings us in-line with other architectures (e.g. x86) which do not
    rely on this masking.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [Mark: rebase, kill THREAD_START_SP, commit msg additions]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 46c3b93cf865..b29ab0e12e60 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -30,7 +30,6 @@
 #endif
 
 #define THREAD_SIZE		16384
-#define THREAD_START_SP		(THREAD_SIZE - 16)
 
 #ifndef __ASSEMBLY__
 

commit cf7de27ab35172a9240f079477cae3146a182998
Author: Thomas Garnier <thgarnie@google.com>
Date:   Wed Jun 14 18:12:03 2017 -0700

    arm64/syscalls: Check address limit on user-mode return
    
    Ensure the address limit is a user-mode segment before returning to
    user-mode. Otherwise a process can corrupt kernel-mode memory and
    elevate privileges [1].
    
    The set_fs function sets the TIF_SETFS flag to force a slow path on
    return. In the slow path, the address limit is checked to be USER_DS if
    needed.
    
    [1] https://bugs.chromium.org/p/project-zero/issues/detail?id=990
    
    Signed-off-by: Thomas Garnier <thgarnie@google.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: kernel-hardening@lists.openwall.com
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Pratyush Anand <panand@redhat.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Will Drewry <wad@chromium.org>
    Cc: linux-api@vger.kernel.org
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Link: http://lkml.kernel.org/r/20170615011203.144108-3-thgarnie@google.com

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 46c3b93cf865..c5ba565544ee 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -86,6 +86,7 @@ struct thread_info {
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
 #define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
+#define TIF_FSCHECK		5	/* Check FS is USER_DS on return */
 #define TIF_NOHZ		7
 #define TIF_SYSCALL_TRACE	8
 #define TIF_SYSCALL_AUDIT	9
@@ -107,11 +108,12 @@ struct thread_info {
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_UPROBE		(1 << TIF_UPROBE)
+#define _TIF_FSCHECK		(1 << TIF_FSCHECK)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \
-				 _TIF_UPROBE)
+				 _TIF_UPROBE | _TIF_FSCHECK)
 
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \

commit 4b65a5db362783ab4b04ca1c1d2ad70ed9b0ba2a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 1 16:53:00 2016 +0100

    arm64: Introduce uaccess_{disable,enable} functionality based on TTBR0_EL1
    
    This patch adds the uaccess macros/functions to disable access to user
    space by setting TTBR0_EL1 to a reserved zeroed page. Since the value
    written to TTBR0_EL1 must be a physical address, for simplicity this
    patch introduces a reserved_ttbr0 page at a constant offset from
    swapper_pg_dir. The uaccess_disable code uses the ttbr1_el1 value
    adjusted by the reserved_ttbr0 offset.
    
    Enabling access to user is done by restoring TTBR0_EL1 with the value
    from the struct thread_info ttbr0 variable. Interrupts must be disabled
    during the uaccess_ttbr0_enable code to ensure the atomicity of the
    thread_info.ttbr0 read and TTBR0_EL1 write. This patch also moves the
    get_thread_info asm macro from entry.S to assembler.h for reuse in the
    uaccess_ttbr0_* macros.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index c17ad4d213d0..46c3b93cf865 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -47,6 +47,9 @@ typedef unsigned long mm_segment_t;
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	mm_segment_t		addr_limit;	/* address limit */
+#ifdef CONFIG_ARM64_SW_TTBR0_PAN
+	u64			ttbr0;		/* saved TTBR0_EL1 */
+#endif
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
 };
 

commit c02433dd6de32f042cf3ffe476746b1115b8c096
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:13 2016 +0000

    arm64: split thread_info from task stack
    
    This patch moves arm64's struct thread_info from the task stack into
    task_struct. This protects thread_info from corruption in the case of
    stack overflows, and makes its address harder to determine if stack
    addresses are leaked, making a number of attacks more difficult. Precise
    detection and handling of overflow is left for subsequent patches.
    
    Largely, this involves changing code to store the task_struct in sp_el0,
    and acquire the thread_info from the task struct. Core code now
    implements current_thread_info(), and as noted in <linux/sched.h> this
    relies on offsetof(task_struct, thread_info) == 0, enforced by core
    code.
    
    This change means that the 'tsk' register used in entry.S now points to
    a task_struct, rather than a thread_info as it used to. To make this
    clear, the TI_* field offsets are renamed to TSK_TI_*, with asm-offsets
    appropriately updated to account for the structural change.
    
    Userspace clobbers sp_el0, and we can no longer restore this from the
    stack. Instead, the current task is cached in a per-cpu variable that we
    can safely access from early assembly as interrupts are disabled (and we
    are thus not preemptible).
    
    Both secondary entry and idle are updated to stash the sp and task
    pointer separately.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index bce0f07483c1..c17ad4d213d0 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -47,41 +47,17 @@ typedef unsigned long mm_segment_t;
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	mm_segment_t		addr_limit;	/* address limit */
-	struct task_struct	*task;		/* main task structure */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
-	int			cpu;		/* cpu */
 };
 
 #define INIT_THREAD_INFO(tsk)						\
 {									\
-	.task		= &tsk,						\
-	.flags		= 0,						\
 	.preempt_count	= INIT_PREEMPT_COUNT,				\
 	.addr_limit	= KERNEL_DS,					\
 }
 
 #define init_stack		(init_thread_union.stack)
 
-/*
- * how to get the thread information struct from C
- */
-static inline struct thread_info *current_thread_info(void) __attribute_const__;
-
-/*
- * struct thread_info can be accessed directly via sp_el0.
- *
- * We don't use read_sysreg() as we want the compiler to cache the value where
- * possible.
- */
-static inline struct thread_info *current_thread_info(void)
-{
-	unsigned long sp_el0;
-
-	asm ("mrs %0, sp_el0" : "=r" (sp_el0));
-
-	return (struct thread_info *)sp_el0;
-}
-
 #define thread_saved_pc(tsk)	\
 	((unsigned long)(tsk->thread.cpu_context.pc))
 #define thread_saved_sp(tsk)	\

commit a9ea0017ebe8889dfa136cac2aa7ae0ee6915e1f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:05 2016 +0000

    arm64: factor out current_stack_pointer
    
    We define current_stack_pointer in <asm/thread_info.h>, though other
    files and header relying upon it do not have this necessary include, and
    are thus fragile to changes in the header soup.
    
    Subsequent patches will affect the header soup such that directly
    including <asm/thread_info.h> may result in a circular header include in
    some of these cases, so we can't simply include <asm/thread_info.h>.
    
    Instead, factor current_thread_info into its own header, and have all
    existing users include this explicitly.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 6ad76efbb3aa..bce0f07483c1 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -36,6 +36,7 @@
 
 struct task_struct;
 
+#include <asm/stack_pointer.h>
 #include <asm/types.h>
 
 typedef unsigned long mm_segment_t;
@@ -61,11 +62,6 @@ struct thread_info {
 
 #define init_stack		(init_thread_union.stack)
 
-/*
- * how to get the current stack pointer from C
- */
-register unsigned long current_stack_pointer asm ("sp");
-
 /*
  * how to get the thread information struct from C
  */

commit dcbe02855f048fdf1e13ebc697e83c8d297f9f5a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:03 2016 +0000

    arm64: thread_info remove stale items
    
    We have a comment claiming __switch_to() cares about where cpu_context
    is located relative to cpu_domain in thread_info. However arm64 has
    never had a thread_info::cpu_domain field, and neither __switch_to nor
    cpu_switch_to care where the cpu_context field is relative to others.
    
    Additionally, the init_thread_info alias is never used anywhere in the
    kernel, and will shortly become problematic when thread_info is moved
    into task_struct.
    
    This patch removes both.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index f6859831462e..6ad76efbb3aa 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -42,7 +42,6 @@ typedef unsigned long mm_segment_t;
 
 /*
  * low level task data that entry.S needs immediate access to.
- * __switch_to() assumes cpu_context follows immediately after cpu_domain.
  */
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
@@ -60,7 +59,6 @@ struct thread_info {
 	.addr_limit	= KERNEL_DS,					\
 }
 
-#define init_thread_info	(init_thread_union.thread_info)
 #define init_stack		(init_thread_union.stack)
 
 /*

commit 9842ceae9fa8deae141533d52a6ead7666962c09
Author: Pratyush Anand <panand@redhat.com>
Date:   Wed Nov 2 14:40:46 2016 +0530

    arm64: Add uprobe support
    
    This patch adds support for uprobe on ARM64 architecture.
    
    Unit tests for following have been done so far and they have been found
    working
        1. Step-able instructions, like sub, ldr, add etc.
        2. Simulation-able like ret, cbnz, cbz etc.
        3. uretprobe
        4. Reject-able instructions like sev, wfe etc.
        5. trapped and abort xol path
        6. probe at unaligned user address.
        7. longjump test cases
    
    Currently it does not support aarch32 instruction probing.
    
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index e9ea5a6bd449..f6859831462e 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -112,6 +112,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
+#define TIF_UPROBE		4	/* uprobe breakpoint or singlestep */
 #define TIF_NOHZ		7
 #define TIF_SYSCALL_TRACE	8
 #define TIF_SYSCALL_AUDIT	9
@@ -132,10 +133,12 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
 #define _TIF_SECCOMP		(1 << TIF_SECCOMP)
+#define _TIF_UPROBE		(1 << TIF_UPROBE)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
-				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE)
+				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE | \
+				 _TIF_UPROBE)
 
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
 				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \

commit adf7589997927b1d84a5d003027b866bbef61ef2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Sep 8 13:55:38 2016 +0100

    arm64: simplify sysreg manipulation
    
    A while back we added {read,write}_sysreg accessors to handle accesses
    to system registers, without the usual boilerplate asm volatile,
    temporary variable, etc.
    
    This patch makes use of these across arm64 to make code shorter and
    clearer. For sequences with a trailing ISB, the existing isb() macro is
    also used so that asm blocks can be removed entirely.
    
    A few uses of inline assembly for msr/mrs are left as-is. Those
    manipulating sp_el0 for the current thread_info value have special
    clobber requiremends.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index abd64bd1f6d9..e9ea5a6bd449 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -75,6 +75,9 @@ static inline struct thread_info *current_thread_info(void) __attribute_const__;
 
 /*
  * struct thread_info can be accessed directly via sp_el0.
+ *
+ * We don't use read_sysreg() as we want the compiler to cache the value where
+ * possible.
  */
 static inline struct thread_info *current_thread_info(void)
 {

commit 6cdf9c7ca687e01840d0215437620a20263012fc
Author: Jungseok Lee <jungseoklee85@gmail.com>
Date:   Fri Dec 4 11:02:25 2015 +0000

    arm64: Store struct thread_info in sp_el0
    
    There is need for figuring out how to manage struct thread_info data when
    IRQ stack is introduced. struct thread_info information should be copied
    to IRQ stack under the current thread_info calculation logic whenever
    context switching is invoked. This is too expensive to keep supporting
    the approach.
    
    Instead, this patch pays attention to sp_el0 which is an unused scratch
    register in EL1 context. sp_el0 utilization not only simplifies the
    management, but also prevents text section size from being increased
    largely due to static allocated IRQ stack as removing masking operation
    using THREAD_SIZE in many places.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 90c7ff233735..abd64bd1f6d9 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -73,10 +73,16 @@ register unsigned long current_stack_pointer asm ("sp");
  */
 static inline struct thread_info *current_thread_info(void) __attribute_const__;
 
+/*
+ * struct thread_info can be accessed directly via sp_el0.
+ */
 static inline struct thread_info *current_thread_info(void)
 {
-	return (struct thread_info *)
-		(current_stack_pointer & ~(THREAD_SIZE - 1));
+	unsigned long sp_el0;
+
+	asm ("mrs %0, sp_el0" : "=r" (sp_el0));
+
+	return (struct thread_info *)sp_el0;
 }
 
 #define thread_saved_pc(tsk)	\

commit 44eaacf1b8999b15cec89bd9d9cd989da4798d53
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:19:37 2015 +0100

    arm64: Add 16K page size support
    
    This patch turns on the 16K page support in the kernel. We
    support 48bit VA (4 level page tables) and 47bit VA (3 level
    page tables).
    
    With 16K we can map 128 entries using contiguous bit hint
    at level 3 to map 2M using single TLB entry.
    
    TODO: 16K supports 32 contiguous entries at level 2 to get us
    1G(which is not yet supported by the infrastructure). That should
    be a separate patch altogether.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 5eac6a2300af..90c7ff233735 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -25,6 +25,8 @@
 
 #ifdef CONFIG_ARM64_4K_PAGES
 #define THREAD_SIZE_ORDER	2
+#elif defined(CONFIG_ARM64_16K_PAGES)
+#define THREAD_SIZE_ORDER	0
 #endif
 
 #define THREAD_SIZE		16384

commit 755e70b7e3f189aa2503c510fb98208e477a5030
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:19:32 2015 +0100

    arm64: Clean config usages for page size
    
    We use !CONFIG_ARM64_64K_PAGES for CONFIG_ARM64_4K_PAGES
    (and vice versa) in code. It all worked well, so far since
    we only had two options. Now, with the introduction of 16K,
    these cases will break. This patch cleans up the code to
    use the required CONFIG symbol expression without the assumption
    that !64K => 4K (and vice versa)
    
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 555c6dec5ef2..5eac6a2300af 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -23,7 +23,7 @@
 
 #include <linux/compiler.h>
 
-#ifndef CONFIG_ARM64_64K_PAGES
+#ifdef CONFIG_ARM64_4K_PAGES
 #define THREAD_SIZE_ORDER	2
 #endif
 

commit 5aec715d7d3122f77cabaa7578d9d25a0c1ed20e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:24 2015 +0100

    arm64: mm: rewrite ASID allocator and MM context-switching code
    
    Our current switch_mm implementation suffers from a number of problems:
    
      (1) The ASID allocator relies on IPIs to synchronise the CPUs on a
          rollover event
    
      (2) Because of (1), we cannot allocate ASIDs with interrupts disabled
          and therefore make use of a TIF_SWITCH_MM flag to postpone the
          actual switch to finish_arch_post_lock_switch
    
      (3) We run context switch with a reserved (invalid) TTBR0 value, even
          though the ASID and pgd are updated atomically
    
      (4) We take a global spinlock (cpu_asid_lock) during context-switch
    
      (5) We use h/w broadcast TLB operations when they are not required
          (e.g. in flush_context)
    
    This patch addresses these problems by rewriting the ASID algorithm to
    match the bitmap-based arch/arm/ implementation more closely. This in
    turn allows us to remove much of the complications surrounding switch_mm,
    including the ugly thread flag.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index dcd06d18a42a..555c6dec5ef2 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -111,7 +111,6 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_RESTORE_SIGMASK	20
 #define TIF_SINGLESTEP		21
 #define TIF_32BIT		22	/* 32bit process */
-#define TIF_SWITCH_MM		23	/* deferred switch_mm */
 
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)

commit 9699a517e0029c4dc34159787a26a746dfab858b
Author: Richard Weinberger <richard@nod.at>
Date:   Sun Jul 13 17:09:40 2014 +0200

    arm64: Remove signal translation and exec_domain
    
    As execution domain support is gone we can remove
    signal translation from the signal code and remove
    exec_domain from thread_info.
    
    Signed-off-by: Richard Weinberger <richard@nod.at>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 702e1e6a0d80..dcd06d18a42a 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -33,7 +33,6 @@
 #ifndef __ASSEMBLY__
 
 struct task_struct;
-struct exec_domain;
 
 #include <asm/types.h>
 
@@ -47,7 +46,6 @@ struct thread_info {
 	unsigned long		flags;		/* low level flags */
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
-	struct exec_domain	*exec_domain;	/* execution domain */
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
 	int			cpu;		/* cpu */
 };
@@ -55,7 +53,6 @@ struct thread_info {
 #define INIT_THREAD_INFO(tsk)						\
 {									\
 	.task		= &tsk,						\
-	.exec_domain	= &default_exec_domain,				\
 	.flags		= 0,						\
 	.preempt_count	= INIT_PREEMPT_COUNT,				\
 	.addr_limit	= KERNEL_DS,					\

commit f56141e3e2d9aabf7e6b89680ab572c2cdbb2a24
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Thu Feb 12 15:01:14 2015 -0800

    all arches, signal: move restart_block to struct task_struct
    
    If an attacker can cause a controlled kernel stack overflow, overwriting
    the restart block is a very juicy exploit target.  This is because the
    restart_block is held in the same memory allocation as the kernel stack.
    
    Moving the restart block to struct task_struct prevents this exploit by
    making the restart_block harder to locate.
    
    Note that there are other fields in thread_info that are also easy
    targets, at least on some architectures.
    
    It's also a decent simplification, since the restart code is more or less
    identical on all architectures.
    
    [james.hogan@imgtec.com: metag: align thread_info::supervisor_stack]
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: David Miller <davem@davemloft.net>
    Acked-by: Richard Weinberger <richard@nod.at>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: James Hogan <james.hogan@imgtec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 459bf8e53208..702e1e6a0d80 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -48,7 +48,6 @@ struct thread_info {
 	mm_segment_t		addr_limit;	/* address limit */
 	struct task_struct	*task;		/* main task structure */
 	struct exec_domain	*exec_domain;	/* execution domain */
-	struct restart_block	restart_block;
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
 	int			cpu;		/* cpu */
 };
@@ -60,9 +59,6 @@ struct thread_info {
 	.flags		= 0,						\
 	.preempt_count	= INIT_PREEMPT_COUNT,				\
 	.addr_limit	= KERNEL_DS,					\
-	.restart_block	= {						\
-		.fn	= do_no_restart_syscall,			\
-	},								\
 }
 
 #define init_thread_info	(init_thread_union.thread_info)

commit 786248705ecf5290f26534e8eef62ba6dd63b806
Author: Behan Webster <behanw@converseincode.com>
Date:   Wed Aug 27 05:29:31 2014 +0100

    arm64: LLVMLinux: Calculate current_thread_info from current_stack_pointer
    
    Use the global current_stack_pointer to get the value of the stack pointer.
    This change supports being able to compile the kernel with both gcc and clang.
    
    Signed-off-by: Behan Webster <behanw@converseincode.com>
    Signed-off-by: Mark Charlebois <charlebm@gmail.com>
    Reviewed-by: Jan-Simon Möller <dl9pf@gmx.de>
    Reviewed-by: Olof Johansson <olof@lixom.net>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 356e03774df4..459bf8e53208 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -80,8 +80,8 @@ static inline struct thread_info *current_thread_info(void) __attribute_const__;
 
 static inline struct thread_info *current_thread_info(void)
 {
-	register unsigned long sp asm ("sp");
-	return (struct thread_info *)(sp & ~(THREAD_SIZE - 1));
+	return (struct thread_info *)
+		(current_stack_pointer & ~(THREAD_SIZE - 1));
 }
 
 #define thread_saved_pc(tsk)	\

commit 3337a10e0d0cbc9225cefc23aa7a604b698367ed
Author: Behan Webster <behanw@converseincode.com>
Date:   Wed Aug 27 05:29:29 2014 +0100

    arm64: LLVMLinux: Add current_stack_pointer() for arm64
    
    Define a global named register for current_stack_pointer. The use of this new
    variable guarantees that both gcc and clang can access this register in C code.
    
    Signed-off-by: Behan Webster <behanw@converseincode.com>
    Reviewed-by: Jan-Simon Möller <dl9pf@gmx.de>
    Reviewed-by: Mark Charlebois <charlebm@gmail.com>
    Reviewed-by: Olof Johansson <olof@lixom.net>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 45108d802f5e..356e03774df4 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -68,6 +68,11 @@ struct thread_info {
 #define init_thread_info	(init_thread_union.thread_info)
 #define init_stack		(init_thread_union.stack)
 
+/*
+ * how to get the current stack pointer from C
+ */
+register unsigned long current_stack_pointer asm ("sp");
+
 /*
  * how to get the thread information struct from C
  */

commit 6c81fe7925cc4c42de49e17be21eb86d1173c3a7
Author: Larry Bassel <larry.bassel@linaro.org>
Date:   Fri May 30 12:34:15 2014 -0700

    arm64: enable context tracking
    
    Make calls to ct_user_enter when the kernel is exited
    and ct_user_exit when the kernel is entered (in el0_da,
    el0_ia, el0_svc, el0_irq and all of the "error" paths).
    
    These macros expand to function calls which will only work
    properly if el0_sync and related code has been rearranged
    (in a previous patch of this series).
    
    The calls to ct_user_exit are made after hw debugging has been
    enabled (enable_dbg_and_irq).
    
    The call to ct_user_enter is made at the beginning of the
    kernel_exit macro.
    
    This patch is based on earlier work by Kevin Hilman.
    Save/restore optimizations were also done by Kevin.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Larry Bassel <larry.bassel@linaro.org>
    Signed-off-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index e40b6d06d515..45108d802f5e 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -103,6 +103,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
+#define TIF_NOHZ		7
 #define TIF_SYSCALL_TRACE	8
 #define TIF_SYSCALL_AUDIT	9
 #define TIF_SYSCALL_TRACEPOINT	10
@@ -118,6 +119,7 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
 #define _TIF_FOREIGN_FPSTATE	(1 << TIF_FOREIGN_FPSTATE)
+#define _TIF_NOHZ		(1 << TIF_NOHZ)
 #define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
 #define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
 #define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
@@ -128,7 +130,8 @@ static inline struct thread_info *current_thread_info(void)
 				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE)
 
 #define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
-				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP)
+				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP | \
+				 _TIF_NOHZ)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_THREAD_INFO_H */

commit cc07aabc53978ae09a1d539237189f7c9841060a
Merge: 9e47aaef0bd3 9358d755bd5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 6 10:43:28 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into next
    
    Pull arm64 updates from Catalin Marinas:
     - Optimised assembly string/memory routines (based on the AArch64
       Cortex Strings library contributed to glibc but re-licensed under
       GPLv2)
     - Optimised crypto algorithms making use of the ARMv8 crypto extensions
       (together with kernel API for using FPSIMD instructions in interrupt
       context)
     - Ftrace support
     - CPU topology parsing from DT
     - ESR_EL1 (Exception Syndrome Register) exposed to user space signal
       handlers for SIGSEGV/SIGBUS (useful to emulation tools like Qemu)
     - 1GB section linear mapping if applicable
     - Barriers usage clean-up
     - Default pgprot clean-up
    
    Conflicts as per Catalin.
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (57 commits)
      arm64: kernel: initialize broadcast hrtimer based clock event device
      arm64: ftrace: Add system call tracepoint
      arm64: ftrace: Add CALLER_ADDRx macros
      arm64: ftrace: Add dynamic ftrace support
      arm64: Add ftrace support
      ftrace: Add arm64 support to recordmcount
      arm64: Add 'notrace' attribute to unwind_frame() for ftrace
      arm64: add __ASSEMBLY__ in asm/insn.h
      arm64: Fix linker script entry point
      arm64: lib: Implement optimized string length routines
      arm64: lib: Implement optimized string compare routines
      arm64: lib: Implement optimized memcmp routine
      arm64: lib: Implement optimized memset routine
      arm64: lib: Implement optimized memmove routine
      arm64: lib: Implement optimized memcpy routine
      arm64: defconfig: enable a few more common/useful options in defconfig
      ftrace: Make CALLER_ADDRx macros more generic
      arm64: Fix deadlock scenario with smp_send_stop()
      arm64: Fix machine_shutdown() definition
      arm64: Support arch_irq_work_raise() via self IPIs
      ...

commit 842514849a616e9b61acad65771c7afe01e651f9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 9 19:04:00 2014 +0200

    arm64: Remove TIF_POLLING_NRFLAG
    
    The only idle method for arm64 is WFI and it therefore
    unconditionally requires the reschedule interrupt when idle.
    
    Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/r/20140509170649.GG13658@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 720e70b66ffd..7b8e3a2a00fb 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -95,13 +95,11 @@ static inline struct thread_info *current_thread_info(void)
  *  TIF_NEED_RESCHED	- rescheduling necessary
  *  TIF_NOTIFY_RESUME	- callback before returning to user
  *  TIF_USEDFPU		- FPU was used by this task this quantum (SMP)
- *  TIF_POLLING_NRFLAG	- true if poll_idle() is polling TIF_NEED_RESCHED
  */
 #define TIF_SIGPENDING		0
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_SYSCALL_TRACE	8
-#define TIF_POLLING_NRFLAG	16
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 #define TIF_FREEZE		19
 #define TIF_RESTORE_SIGMASK	20

commit cf5c95db57ffa02e430c3840c08d1ee0403849d4
Merge: fd92d4a54a06 49788fe2a128
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 16 10:05:11 2014 +0100

    Merge tag 'for-3.16' of git://git.linaro.org/people/ard.biesheuvel/linux-arm into upstream
    
    FPSIMD register bank context switching and crypto algorithms
    optimisations for arm64 from Ard Biesheuvel.
    
    * tag 'for-3.16' of git://git.linaro.org/people/ard.biesheuvel/linux-arm:
      arm64/crypto: AES-ECB/CBC/CTR/XTS using ARMv8 NEON and Crypto Extensions
      arm64: pull in <asm/simd.h> from asm-generic
      arm64/crypto: AES in CCM mode using ARMv8 Crypto Extensions
      arm64/crypto: AES using ARMv8 Crypto Extensions
      arm64/crypto: GHASH secure hash using ARMv8 Crypto Extensions
      arm64/crypto: SHA-224/SHA-256 using ARMv8 Crypto Extensions
      arm64/crypto: SHA-1 using ARMv8 Crypto Extensions
      arm64: add support for kernel mode NEON in interrupt context
      arm64: defer reloading a task's FPSIMD state to userland resume
      arm64: add abstractions for FPSIMD state manipulation
      asm-generic: allow generic unaligned access if the arch supports it
    
    Conflicts:
            arch/arm64/include/asm/thread_info.h

commit 449f81a4da4d99980064943d504bb19d07e86aec
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:51:29 2014 +0100

    arm64: make a single hook to syscall_trace() for all syscall features
    
    Currently syscall_trace() is called only for ptrace.
    With additional TIF_xx flags defined, it is now called in all the cases
    of audit, ftrace and seccomp in addition to ptrace.
    
    Acked-by: Richard Guy Briggs <rgb@redhat.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 720e70b66ffd..0a8b2a97a32e 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -91,6 +91,9 @@ static inline struct thread_info *current_thread_info(void)
 /*
  * thread information flags:
  *  TIF_SYSCALL_TRACE	- syscall trace active
+ *  TIF_SYSCALL_TRACEPOINT - syscall tracepoint for ftrace
+ *  TIF_SYSCALL_AUDIT	- syscall auditing
+ *  TIF_SECOMP		- syscall secure computing
  *  TIF_SIGPENDING	- signal pending
  *  TIF_NEED_RESCHED	- rescheduling necessary
  *  TIF_NOTIFY_RESUME	- callback before returning to user
@@ -101,6 +104,9 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
 #define TIF_SYSCALL_TRACE	8
+#define TIF_SYSCALL_AUDIT	9
+#define TIF_SYSCALL_TRACEPOINT	10
+#define TIF_SECCOMP		11
 #define TIF_POLLING_NRFLAG	16
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
 #define TIF_FREEZE		19
@@ -112,10 +118,17 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_SYSCALL_TRACE	(1 << TIF_SYSCALL_TRACE)
+#define _TIF_SYSCALL_AUDIT	(1 << TIF_SYSCALL_AUDIT)
+#define _TIF_SYSCALL_TRACEPOINT	(1 << TIF_SYSCALL_TRACEPOINT)
+#define _TIF_SECCOMP		(1 << TIF_SECCOMP)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
 				 _TIF_NOTIFY_RESUME)
 
+#define _TIF_SYSCALL_WORK	(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
+				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP)
+
 #endif /* __KERNEL__ */
 #endif /* __ASM_THREAD_INFO_H */

commit 005f78cd88494457ed38ce817f4e3fe5d372f0cb
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu May 8 11:20:23 2014 +0200

    arm64: defer reloading a task's FPSIMD state to userland resume
    
    If a task gets scheduled out and back in again and nothing has touched
    its FPSIMD state in the mean time, there is really no reason to reload
    it from memory. Similarly, repeated calls to kernel_neon_begin() and
    kernel_neon_end() will preserve and restore the FPSIMD state every time.
    
    This patch defers the FPSIMD state restore to the last possible moment,
    i.e., right before the task returns to userland. If a task does not return to
    userland at all (for any reason), the existing FPSIMD state is preserved
    and may be reused by the owning task if it gets scheduled in again on the
    same CPU.
    
    This patch adds two more functions to abstract away from straight FPSIMD
    register file saves and restores:
    - fpsimd_restore_current_state -> ensure current's FPSIMD state is loaded
    - fpsimd_flush_task_state -> invalidate live copies of a task's FPSIMD state
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 720e70b66ffd..4a1ca1cfb2f8 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -100,6 +100,7 @@ static inline struct thread_info *current_thread_info(void)
 #define TIF_SIGPENDING		0
 #define TIF_NEED_RESCHED	1
 #define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
+#define TIF_FOREIGN_FPSTATE	3	/* CPU's FP state is not current's */
 #define TIF_SYSCALL_TRACE	8
 #define TIF_POLLING_NRFLAG	16
 #define TIF_MEMDIE		18	/* is terminating due to OOM killer */
@@ -112,10 +113,11 @@ static inline struct thread_info *current_thread_info(void)
 #define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
 #define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
 #define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_FOREIGN_FPSTATE	(1 << TIF_FOREIGN_FPSTATE)
 #define _TIF_32BIT		(1 << TIF_32BIT)
 
 #define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
-				 _TIF_NOTIFY_RESUME)
+				 _TIF_NOTIFY_RESUME | _TIF_FOREIGN_FPSTATE)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_THREAD_INFO_H */

commit 00d1a39e69d5afa7523dad515a05b21abd17c389
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Sep 17 18:53:09 2013 +0000

    preempt: Make PREEMPT_ACTIVE generic
    
    No point in having this bit defined by architecture.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130917183629.090698799@linutronix.de

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 23a3c4791d86..720e70b66ffd 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -88,12 +88,6 @@ static inline struct thread_info *current_thread_info(void)
 
 #endif
 
-/*
- * We use bit 30 of the preempt_count to indicate that kernel
- * preemption is occurring.  See <asm/hardirq.h>.
- */
-#define PREEMPT_ACTIVE	0x40000000
-
 /*
  * thread information flags:
  *  TIF_SYSCALL_TRACE	- syscall trace active

commit 845ad05ec31e0f3872a321e10dbeaf872022632c
Author: Feng Kan <fkan@apm.com>
Date:   Tue Jul 23 18:52:31 2013 +0100

    arm64: Change kernel stack size to 16K
    
    Written by Catalin Marinas, tested by APM on storm platform. This is needed
    because of the failures encountered when running SpecWeb benchmark test.
    
    Signed-off-by: Feng Kan <fkan@apm.com>
    Acked-by: Kumar Sankaran <ksankaran@apm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
index 3659e460071d..23a3c4791d86 100644
--- a/arch/arm64/include/asm/thread_info.h
+++ b/arch/arm64/include/asm/thread_info.h
@@ -24,10 +24,10 @@
 #include <linux/compiler.h>
 
 #ifndef CONFIG_ARM64_64K_PAGES
-#define THREAD_SIZE_ORDER	1
+#define THREAD_SIZE_ORDER	2
 #endif
 
-#define THREAD_SIZE		8192
+#define THREAD_SIZE		16384
 #define THREAD_START_SP		(THREAD_SIZE - 16)
 
 #ifndef __ASSEMBLY__

commit b3901d54dc4f73acdc6b7c6e5a7a496d3afeae61
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: Process management
    
    The patch adds support for thread creation and context switching. The
    context switching CPU specific code is introduced with the CPU support
    patch (part of the arch/arm64/mm/proc.S file). AArch64 supports
    ASID-tagged TLBs and the ASID can be either 8 or 16-bit wide (detectable
    via the ID_AA64AFR0_EL1 register).
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
new file mode 100644
index 000000000000..3659e460071d
--- /dev/null
+++ b/arch/arm64/include/asm/thread_info.h
@@ -0,0 +1,127 @@
+/*
+ * Based on arch/arm/include/asm/thread_info.h
+ *
+ * Copyright (C) 2002 Russell King.
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_THREAD_INFO_H
+#define __ASM_THREAD_INFO_H
+
+#ifdef __KERNEL__
+
+#include <linux/compiler.h>
+
+#ifndef CONFIG_ARM64_64K_PAGES
+#define THREAD_SIZE_ORDER	1
+#endif
+
+#define THREAD_SIZE		8192
+#define THREAD_START_SP		(THREAD_SIZE - 16)
+
+#ifndef __ASSEMBLY__
+
+struct task_struct;
+struct exec_domain;
+
+#include <asm/types.h>
+
+typedef unsigned long mm_segment_t;
+
+/*
+ * low level task data that entry.S needs immediate access to.
+ * __switch_to() assumes cpu_context follows immediately after cpu_domain.
+ */
+struct thread_info {
+	unsigned long		flags;		/* low level flags */
+	mm_segment_t		addr_limit;	/* address limit */
+	struct task_struct	*task;		/* main task structure */
+	struct exec_domain	*exec_domain;	/* execution domain */
+	struct restart_block	restart_block;
+	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+	int			cpu;		/* cpu */
+};
+
+#define INIT_THREAD_INFO(tsk)						\
+{									\
+	.task		= &tsk,						\
+	.exec_domain	= &default_exec_domain,				\
+	.flags		= 0,						\
+	.preempt_count	= INIT_PREEMPT_COUNT,				\
+	.addr_limit	= KERNEL_DS,					\
+	.restart_block	= {						\
+		.fn	= do_no_restart_syscall,			\
+	},								\
+}
+
+#define init_thread_info	(init_thread_union.thread_info)
+#define init_stack		(init_thread_union.stack)
+
+/*
+ * how to get the thread information struct from C
+ */
+static inline struct thread_info *current_thread_info(void) __attribute_const__;
+
+static inline struct thread_info *current_thread_info(void)
+{
+	register unsigned long sp asm ("sp");
+	return (struct thread_info *)(sp & ~(THREAD_SIZE - 1));
+}
+
+#define thread_saved_pc(tsk)	\
+	((unsigned long)(tsk->thread.cpu_context.pc))
+#define thread_saved_sp(tsk)	\
+	((unsigned long)(tsk->thread.cpu_context.sp))
+#define thread_saved_fp(tsk)	\
+	((unsigned long)(tsk->thread.cpu_context.fp))
+
+#endif
+
+/*
+ * We use bit 30 of the preempt_count to indicate that kernel
+ * preemption is occurring.  See <asm/hardirq.h>.
+ */
+#define PREEMPT_ACTIVE	0x40000000
+
+/*
+ * thread information flags:
+ *  TIF_SYSCALL_TRACE	- syscall trace active
+ *  TIF_SIGPENDING	- signal pending
+ *  TIF_NEED_RESCHED	- rescheduling necessary
+ *  TIF_NOTIFY_RESUME	- callback before returning to user
+ *  TIF_USEDFPU		- FPU was used by this task this quantum (SMP)
+ *  TIF_POLLING_NRFLAG	- true if poll_idle() is polling TIF_NEED_RESCHED
+ */
+#define TIF_SIGPENDING		0
+#define TIF_NEED_RESCHED	1
+#define TIF_NOTIFY_RESUME	2	/* callback before returning to user */
+#define TIF_SYSCALL_TRACE	8
+#define TIF_POLLING_NRFLAG	16
+#define TIF_MEMDIE		18	/* is terminating due to OOM killer */
+#define TIF_FREEZE		19
+#define TIF_RESTORE_SIGMASK	20
+#define TIF_SINGLESTEP		21
+#define TIF_32BIT		22	/* 32bit process */
+#define TIF_SWITCH_MM		23	/* deferred switch_mm */
+
+#define _TIF_SIGPENDING		(1 << TIF_SIGPENDING)
+#define _TIF_NEED_RESCHED	(1 << TIF_NEED_RESCHED)
+#define _TIF_NOTIFY_RESUME	(1 << TIF_NOTIFY_RESUME)
+#define _TIF_32BIT		(1 << TIF_32BIT)
+
+#define _TIF_WORK_MASK		(_TIF_NEED_RESCHED | _TIF_SIGPENDING | \
+				 _TIF_NOTIFY_RESUME)
+
+#endif /* __KERNEL__ */
+#endif /* __ASM_THREAD_INFO_H */
