commit e43f1331e2ef913b8c566920c9af75e0ccdd1d3f
Author: James Morse <james.morse@arm.com>
Date:   Thu Feb 20 16:58:39 2020 +0000

    arm64: Ask the compiler to __always_inline functions used by KVM at HYP
    
    KVM uses some of the static-inline helpers like icache_is_vipt() from
    its HYP code. This assumes the function is inlined so that the code is
    mapped to EL2. The compiler may decide not to inline these, and the
    out-of-line version may not be in the __hyp_text section.
    
    Add the additional __always_ hint to these static-inlines that are used
    by KVM.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200220165839.256881-4-james.morse@arm.com

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 806e9dc2a852..a4d1b5f771f6 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -69,7 +69,7 @@ static inline int icache_is_aliasing(void)
 	return test_bit(ICACHEF_ALIASING, &__icache_flags);
 }
 
-static inline int icache_is_vpipt(void)
+static __always_inline int icache_is_vpipt(void)
 {
 	return test_bit(ICACHEF_VPIPT, &__icache_flags);
 }

commit ee9d90be9ddace01b7fb126567e4b539fbe1f82f
Author: James Morse <james.morse@arm.com>
Date:   Thu Oct 17 18:42:59 2019 +0100

    arm64: Fake the IminLine size on systems affected by Neoverse-N1 #1542419
    
    Systems affected by Neoverse-N1 #1542419 support DIC so do not need to
    perform icache maintenance once new instructions are cleaned to the PoU.
    For the errata workaround, the kernel hides DIC from user-space, so that
    the unnecessary cache maintenance can be trapped by firmware.
    
    To reduce the number of traps, produce a fake IminLine value based on
    PAGE_SIZE.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 43da6dd29592..806e9dc2a852 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -11,6 +11,7 @@
 #define CTR_L1IP_MASK		3
 #define CTR_DMINLINE_SHIFT	16
 #define CTR_IMINLINE_SHIFT	0
+#define CTR_IMINLINE_MASK	0xf
 #define CTR_ERG_SHIFT		20
 #define CTR_CWG_SHIFT		24
 #define CTR_CWG_MASK		15
@@ -18,7 +19,7 @@
 #define CTR_DIC_SHIFT		29
 
 #define CTR_CACHE_MINLINE_MASK	\
-	(0xf << CTR_DMINLINE_SHIFT | 0xf << CTR_IMINLINE_SHIFT)
+	(0xf << CTR_DMINLINE_SHIFT | CTR_IMINLINE_MASK << CTR_IMINLINE_SHIFT)
 
 #define CTR_L1IP(ctr)		(((ctr) >> CTR_L1IP_SHIFT) & CTR_L1IP_MASK)
 

commit 80d838122643a09a9f99824adea4b4261e4451e6
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Mon Aug 12 14:50:45 2019 -0700

    arm64: prefer __section from compiler_attributes.h
    
    GCC unescapes escaped string section names while Clang does not. Because
    __section uses the `#` stringification operator for the section name, it
    doesn't need to be escaped.
    
    This antipattern was found with:
    $ grep -e __section\(\" -e __section__\(\" -r
    
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 64eeaa41e7ca..43da6dd29592 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -78,7 +78,7 @@ static inline u32 cache_type_cwg(void)
 	return (read_cpuid_cachetype() >> CTR_CWG_SHIFT) & CTR_CWG_MASK;
 }
 
-#define __read_mostly __attribute__((__section__(".data..read_mostly")))
+#define __read_mostly __section(.data..read_mostly)
 
 static inline int cache_line_size_of_cpu(void)
 {

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 926434f413fa..a05db636981a 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_CACHE_H
 #define __ASM_CACHE_H

commit 8f5c9037a55b22e847f636f9a39fa98fe67923d1
Author: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
Date:   Fri Jun 14 09:11:41 2019 -0400

    arm64/mm: Correct the cache line size warning with non coherent device
    
    If the cache line size is greater than ARCH_DMA_MINALIGN (128),
    the warning shows and it's tainted as TAINT_CPU_OUT_OF_SPEC.
    
    However, it's not good because as discussed in the thread [1], the cpu
    cache line size will be problem only on non-coherent devices.
    
    Since the coherent flag is already introduced to struct device,
    show the warning only if the device is non-coherent device and
    ARCH_DMA_MINALIGN is smaller than the cpu cache size.
    
    [1] https://lore.kernel.org/linux-arm-kernel/20180514145703.celnlobzn3uh5tc2@localhost/
    
    Signed-off-by: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
    Reviewed-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Tested-by: Zhang Lei <zhang.lei@jp.fujitsu.com>
    [catalin.marinas@arm.com: removed 'if' block for WARN_TAINT]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 758af6340314..d24b7c1ecd9b 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -91,6 +91,13 @@ static inline u32 cache_type_cwg(void)
 
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
 
+static inline int cache_line_size_of_cpu(void)
+{
+	u32 cwg = cache_type_cwg();
+
+	return cwg ? 4 << cwg : ARCH_DMA_MINALIGN;
+}
+
 int cache_line_size(void);
 
 /*

commit 7b8c87b297a7c1b3badabc1d054b6e0b758952df
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Tue May 28 10:16:54 2019 +0800

    arm64: cacheinfo: Update cache_line_size detected from DT or PPTT
    
    cache_line_size is derived from CTR_EL0.CWG field and is called mostly
    for I/O device drivers. For some platforms like the HiSilicon Kunpeng920
    server SoC, cache line sizes are different between L1/2 cache and L3
    cache while L1 cache line size is 64-byte and L3 is 128-byte, but
    CTR_EL0.CWG is misreporting using L1 cache line size.
    
    We shall correct the right value which is important for I/O performance.
    Let's update the cache line size if it is detected from DT or PPTT
    information.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Zhenfa Qiu <qiuzhenfa@hisilicon.com>
    Reported-by: Zhenfa Qiu <qiuzhenfa@hisilicon.com>
    Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 926434f413fa..758af6340314 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -91,11 +91,7 @@ static inline u32 cache_type_cwg(void)
 
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
 
-static inline int cache_line_size(void)
-{
-	u32 cwg = cache_type_cwg();
-	return cwg ? 4 << cwg : ARCH_DMA_MINALIGN;
-}
+int cache_line_size(void);
 
 /*
  * Read the effective value of CTR_EL0.

commit 7fa1e2e6afa7f4c9f46528e61de6a15d9e8dffd9
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Jan 11 14:47:40 2019 +0100

    kasan, arm64: remove redundant ARCH_SLAB_MINALIGN define
    
    Defining ARCH_SLAB_MINALIGN in arch/arm64/include/asm/cache.h when KASAN
    is off is not needed, as it is defined in defined in include/linux/slab.h
    as ifndef.
    
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index eb43e09c1980..926434f413fa 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -60,8 +60,6 @@
 
 #ifdef CONFIG_KASAN_SW_TAGS
 #define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
-#else
-#define ARCH_SLAB_MINALIGN	__alignof__(unsigned long long)
 #endif
 
 #ifndef __ASSEMBLY__

commit eb214f2dda31ffa989033b1e0f848ba0d3cb6188
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Tue Jan 8 15:23:11 2019 -0800

    kasan, arm64: use ARCH_SLAB_MINALIGN instead of manual aligning
    
    Instead of changing cache->align to be aligned to KASAN_SHADOW_SCALE_SIZE
    in kasan_cache_create() we can reuse the ARCH_SLAB_MINALIGN macro.
    
    Link: http://lkml.kernel.org/r/52ddd881916bcc153a9924c154daacde78522227.1546540962.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Suggested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 13dd42c3ad4e..eb43e09c1980 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -58,6 +58,12 @@
  */
 #define ARCH_DMA_MINALIGN	(128)
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define ARCH_SLAB_MINALIGN	(1ULL << KASAN_SHADOW_SCALE_SHIFT)
+#else
+#define ARCH_SLAB_MINALIGN	__alignof__(unsigned long long)
+#endif
+
 #ifndef __ASSEMBLY__
 
 #include <linux/bitops.h>

commit 1602df02f33f61fe0de1bbfeba0d1c97c14bff19
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Oct 9 14:47:06 2018 +0100

    arm64: cpufeature: Fix handling of CTR_EL0.IDC field
    
    CTR_EL0.IDC reports the data cache clean requirements for instruction
    to data coherence. However, if the field is 0, we need to check the
    CLIDR_EL1 fields to detect the status of the feature. Currently we
    don't do this and generate a warning with tainting the kernel, when
    there is a mismatch in the field among the CPUs. Also the userspace
    doesn't have a reliable way to check the CLIDR_EL1 register to check
    the status.
    
    This patch fixes the problem by checking the CLIDR_EL1 fields, when
    (CTR_EL0.IDC == 0) and updates the kernel's copy of the CTR_EL0 for
    the CPU with the actual status of the feature. This would allow the
    sanity check infrastructure to do the proper checking of the fields
    and also allow the CTR_EL0 emulation code to supply the real status
    of the feature.
    
    Now, if a CPU has raw CTR_EL0.IDC == 0 and effective IDC == 1 (with
    overall system wide IDC == 1), we need to expose the real value to
    the user. So, we trap CTR_EL0 access on the CPU which reports incorrect
    CTR_EL0.IDC.
    
    Fixes: commit 6ae4b6e057888 ("arm64: Add support for new control bits CTR_EL0.DIC and CTR_EL0.IDC")
    Cc: Shanker Donthineni <shankerd@codeaurora.org>
    Cc: Philip Elcan <pelcan@codeaurora.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 5ee5bca8c24b..13dd42c3ad4e 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -40,6 +40,15 @@
 #define L1_CACHE_SHIFT		(6)
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
+
+#define CLIDR_LOUU_SHIFT	27
+#define CLIDR_LOC_SHIFT		24
+#define CLIDR_LOUIS_SHIFT	21
+
+#define CLIDR_LOUU(clidr)	(((clidr) >> CLIDR_LOUU_SHIFT) & 0x7)
+#define CLIDR_LOC(clidr)	(((clidr) >> CLIDR_LOC_SHIFT) & 0x7)
+#define CLIDR_LOUIS(clidr)	(((clidr) >> CLIDR_LOUIS_SHIFT) & 0x7)
+
 /*
  * Memory returned by kmalloc() may be used for DMA, so we must make
  * sure that all such allocations are cache aligned. Otherwise,
@@ -84,6 +93,37 @@ static inline int cache_line_size(void)
 	return cwg ? 4 << cwg : ARCH_DMA_MINALIGN;
 }
 
+/*
+ * Read the effective value of CTR_EL0.
+ *
+ * According to ARM ARM for ARMv8-A (ARM DDI 0487C.a),
+ * section D10.2.33 "CTR_EL0, Cache Type Register" :
+ *
+ * CTR_EL0.IDC reports the data cache clean requirements for
+ * instruction to data coherence.
+ *
+ *  0 - dcache clean to PoU is required unless :
+ *     (CLIDR_EL1.LoC == 0) || (CLIDR_EL1.LoUIS == 0 && CLIDR_EL1.LoUU == 0)
+ *  1 - dcache clean to PoU is not required for i-to-d coherence.
+ *
+ * This routine provides the CTR_EL0 with the IDC field updated to the
+ * effective state.
+ */
+static inline u32 __attribute_const__ read_cpuid_effective_cachetype(void)
+{
+	u32 ctr = read_cpuid_cachetype();
+
+	if (!(ctr & BIT(CTR_IDC_SHIFT))) {
+		u64 clidr = read_sysreg(clidr_el1);
+
+		if (CLIDR_LOC(clidr) == 0 ||
+		    (CLIDR_LOUIS(clidr) == 0 && CLIDR_LOUU(clidr) == 0))
+			ctr |= BIT(CTR_IDC_SHIFT);
+	}
+
+	return ctr;
+}
+
 #endif	/* __ASSEMBLY__ */
 
 #endif

commit 4c4a39dd5fe2d13e2d2fa5fceb8ef95d19fc389a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Jul 4 23:07:45 2018 +0100

    arm64: Fix mismatched cache line size detection
    
    If there is a mismatch in the I/D min line size, we must
    always use the system wide safe value both in applications
    and in the kernel, while performing cache operations. However,
    we have been checking more bits than just the min line sizes,
    which triggers false negatives. We may need to trap the user
    accesses in such cases, but not necessarily patch the kernel.
    
    This patch fixes the check to do the right thing as advertised.
    A new capability will be added to check mismatches in other
    fields and ensure we trap the CTR accesses.
    
    Fixes: be68a8aaf925 ("arm64: cpufeature: Fix CTR_EL0 field definitions")
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 5df5cfe1c143..5ee5bca8c24b 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -21,12 +21,16 @@
 #define CTR_L1IP_SHIFT		14
 #define CTR_L1IP_MASK		3
 #define CTR_DMINLINE_SHIFT	16
+#define CTR_IMINLINE_SHIFT	0
 #define CTR_ERG_SHIFT		20
 #define CTR_CWG_SHIFT		24
 #define CTR_CWG_MASK		15
 #define CTR_IDC_SHIFT		28
 #define CTR_DIC_SHIFT		29
 
+#define CTR_CACHE_MINLINE_MASK	\
+	(0xf << CTR_DMINLINE_SHIFT | 0xf << CTR_IMINLINE_SHIFT)
+
 #define CTR_L1IP(ctr)		(((ctr) >> CTR_L1IP_SHIFT) & CTR_L1IP_MASK)
 
 #define ICACHE_POLICY_VPIPT	0

commit ebc7e21e0fa28c46b938baed292c77e2d3ef8165
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 11 13:33:12 2018 +0100

    arm64: Increase ARCH_DMA_MINALIGN to 128
    
    This patch increases the ARCH_DMA_MINALIGN to 128 so that it covers the
    currently known Cache Writeback Granule (CTR_EL0.CWG) on arm64 and moves
    the fallback in cache_line_size() from L1_CACHE_BYTES to this constant.
    In addition, it warns (and taints) if the CWG is larger than
    ARCH_DMA_MINALIGN as this is not safe with non-coherent DMA.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 1dd2c2db0010..5df5cfe1c143 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -43,7 +43,7 @@
  * cache before the transfer is done, causing old data to be seen by
  * the CPU.
  */
-#define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
+#define ARCH_DMA_MINALIGN	(128)
 
 #ifndef __ASSEMBLY__
 
@@ -77,7 +77,7 @@ static inline u32 cache_type_cwg(void)
 static inline int cache_line_size(void)
 {
 	u32 cwg = cache_type_cwg();
-	return cwg ? 4 << cwg : L1_CACHE_BYTES;
+	return cwg ? 4 << cwg : ARCH_DMA_MINALIGN;
 }
 
 #endif	/* __ASSEMBLY__ */

commit d93277b9839b0bde06238a7a7f644114edb2ad4a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 11 13:25:49 2018 +0100

    Revert "arm64: Increase the max granular size"
    
    This reverts commit 97303480753e48fb313dc0e15daaf11b0451cdb8.
    
    Commit 97303480753e ("arm64: Increase the max granular size") increased
    the cache line size to 128 to match Cavium ThunderX, apparently for some
    performance benefit which could not be confirmed. This change, however,
    has an impact on the network packet allocation in certain circumstances,
    requiring slightly over a 4K page with a significant performance
    degradation. The patch reverts L1_CACHE_SHIFT back to 6 (64-byte cache
    line).
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 9bbffc7a301f..1dd2c2db0010 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -33,7 +33,7 @@
 #define ICACHE_POLICY_VIPT	2
 #define ICACHE_POLICY_PIPT	3
 
-#define L1_CACHE_SHIFT		7
+#define L1_CACHE_SHIFT		(6)
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 /*

commit 3f251cf0abec2afb6eca67f71380670dd55bdebe
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Mar 27 12:04:51 2018 +0100

    Revert "arm64: Revert L1_CACHE_SHIFT back to 6 (64-byte cache line size)"
    
    This reverts commit 1f85b42a691cd8329ba82dbcaeec80ac1231b32a.
    
    The internal dma-direct.h API has changed in -next, which collides with
    us trying to use it to manage non-coherent DMA devices on systems with
    unreasonably large cache writeback granules.
    
    This isn't at all trivial to resolve, so revert our changes for now and
    we can revisit this after the merge window. Effectively, this just
    restores our behaviour back to that of 4.16.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 5df5cfe1c143..9bbffc7a301f 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -33,7 +33,7 @@
 #define ICACHE_POLICY_VIPT	2
 #define ICACHE_POLICY_PIPT	3
 
-#define L1_CACHE_SHIFT		(6)
+#define L1_CACHE_SHIFT		7
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 /*
@@ -43,7 +43,7 @@
  * cache before the transfer is done, causing old data to be seen by
  * the CPU.
  */
-#define ARCH_DMA_MINALIGN	(128)
+#define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
 
 #ifndef __ASSEMBLY__
 
@@ -77,7 +77,7 @@ static inline u32 cache_type_cwg(void)
 static inline int cache_line_size(void)
 {
 	u32 cwg = cache_type_cwg();
-	return cwg ? 4 << cwg : ARCH_DMA_MINALIGN;
+	return cwg ? 4 << cwg : L1_CACHE_BYTES;
 }
 
 #endif	/* __ASSEMBLY__ */

commit 6ae4b6e0578886eb36cedbf99f04031d93f9e315
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Wed Mar 7 09:00:08 2018 -0600

    arm64: Add support for new control bits CTR_EL0.DIC and CTR_EL0.IDC
    
    The DCache clean & ICache invalidation requirements for instructions
    to be data coherence are discoverable through new fields in CTR_EL0.
    The following two control bits DIC and IDC were defined for this
    purpose. No need to perform point of unification cache maintenance
    operations from software on systems where CPU caches are transparent.
    
    This patch optimize the three functions __flush_cache_user_range(),
    clean_dcache_area_pou() and invalidate_icache_range() if the hardware
    reports CTR_EL0.IDC and/or CTR_EL0.IDC. Basically it skips the two
    instructions 'DC CVAU' and 'IC IVAU', and the associated loop logic
    in order to avoid the unnecessary overhead.
    
    CTR_EL0.DIC: Instruction cache invalidation requirements for
     instruction to data coherence. The meaning of this bit[29].
      0: Instruction cache invalidation to the point of unification
         is required for instruction to data coherence.
      1: Instruction cache cleaning to the point of unification is
          not required for instruction to data coherence.
    
    CTR_EL0.IDC: Data cache clean requirements for instruction to data
     coherence. The meaning of this bit[28].
      0: Data cache clean to the point of unification is required for
         instruction to data coherence, unless CLIDR_EL1.LoC == 0b000
         or (CLIDR_EL1.LoUIS == 0b000 && CLIDR_EL1.LoUU == 0b000).
      1: Data cache clean to the point of unification is not required
         for instruction to data coherence.
    
    Co-authored-by: Philip Elcan <pelcan@codeaurora.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index b2e6ece23713..5df5cfe1c143 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -20,8 +20,12 @@
 
 #define CTR_L1IP_SHIFT		14
 #define CTR_L1IP_MASK		3
+#define CTR_DMINLINE_SHIFT	16
+#define CTR_ERG_SHIFT		20
 #define CTR_CWG_SHIFT		24
 #define CTR_CWG_MASK		15
+#define CTR_IDC_SHIFT		28
+#define CTR_DIC_SHIFT		29
 
 #define CTR_L1IP(ctr)		(((ctr) >> CTR_L1IP_SHIFT) & CTR_L1IP_MASK)
 

commit 1f85b42a691cd8329ba82dbcaeec80ac1231b32a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Feb 28 18:47:20 2018 +0000

    arm64: Revert L1_CACHE_SHIFT back to 6 (64-byte cache line size)
    
    Commit 97303480753e ("arm64: Increase the max granular size") increased
    the cache line size to 128 to match Cavium ThunderX, apparently for some
    performance benefit which could not be confirmed. This change, however,
    has an impact on the network packets allocation in certain
    circumstances, requiring slightly over a 4K page with a significant
    performance degradation.
    
    This patch reverts L1_CACHE_SHIFT back to 6 (64-byte cache line) while
    keeping ARCH_DMA_MINALIGN at 128. The cache_line_size() function was
    changed to default to ARCH_DMA_MINALIGN in the absence of a meaningful
    CTR_EL0.CWG bit field.
    
    In addition, if a system with ARCH_DMA_MINALIGN < CTR_EL0.CWG is
    detected, the kernel will force swiotlb bounce buffering for all
    non-coherent devices since DMA cache maintenance on sub-CWG ranges is
    not safe, leading to data corruption.
    
    Cc: Tirumalesh Chalamarla <tchalamarla@cavium.com>
    Cc: Timur Tabi <timur@codeaurora.org>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index ea9bb4e0e9bb..b2e6ece23713 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -29,7 +29,7 @@
 #define ICACHE_POLICY_VIPT	2
 #define ICACHE_POLICY_PIPT	3
 
-#define L1_CACHE_SHIFT		7
+#define L1_CACHE_SHIFT		(6)
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 /*
@@ -39,7 +39,7 @@
  * cache before the transfer is done, causing old data to be seen by
  * the CPU.
  */
-#define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
+#define ARCH_DMA_MINALIGN	(128)
 
 #ifndef __ASSEMBLY__
 
@@ -73,7 +73,7 @@ static inline u32 cache_type_cwg(void)
 static inline int cache_line_size(void)
 {
 	u32 cwg = cache_type_cwg();
-	return cwg ? 4 << cwg : L1_CACHE_BYTES;
+	return cwg ? 4 << cwg : ARCH_DMA_MINALIGN;
 }
 
 #endif	/* __ASSEMBLY__ */

commit dda288d7e4f605632dc6e19c69063f1725056208
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Mar 10 20:32:24 2017 +0000

    arm64: cache: Identify VPIPT I-caches
    
    Add support for detecting VPIPT I-caches, as introduced by ARMv8.2.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 7acb52634299..ea9bb4e0e9bb 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -25,6 +25,7 @@
 
 #define CTR_L1IP(ctr)		(((ctr) >> CTR_L1IP_SHIFT) & CTR_L1IP_MASK)
 
+#define ICACHE_POLICY_VPIPT	0
 #define ICACHE_POLICY_VIPT	2
 #define ICACHE_POLICY_PIPT	3
 
@@ -45,6 +46,7 @@
 #include <linux/bitops.h>
 
 #define ICACHEF_ALIASING	0
+#define ICACHEF_VPIPT		1
 extern unsigned long __icache_flags;
 
 /*
@@ -56,6 +58,11 @@ static inline int icache_is_aliasing(void)
 	return test_bit(ICACHEF_ALIASING, &__icache_flags);
 }
 
+static inline int icache_is_vpipt(void)
+{
+	return test_bit(ICACHEF_VPIPT, &__icache_flags);
+}
+
 static inline u32 cache_type_cwg(void)
 {
 	return (read_cpuid_cachetype() >> CTR_CWG_SHIFT) & CTR_CWG_MASK;

commit 02f7760e6e5c3d726cd9622749cdae17c571b9a3
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Mar 10 20:32:23 2017 +0000

    arm64: cache: Merge cachetype.h into cache.h
    
    cachetype.h and cache.h are small and both obviously related to caches.
    Merge them together to reduce clutter.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 5082b30bc2c0..7acb52634299 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -16,7 +16,17 @@
 #ifndef __ASM_CACHE_H
 #define __ASM_CACHE_H
 
-#include <asm/cachetype.h>
+#include <asm/cputype.h>
+
+#define CTR_L1IP_SHIFT		14
+#define CTR_L1IP_MASK		3
+#define CTR_CWG_SHIFT		24
+#define CTR_CWG_MASK		15
+
+#define CTR_L1IP(ctr)		(((ctr) >> CTR_L1IP_SHIFT) & CTR_L1IP_MASK)
+
+#define ICACHE_POLICY_VIPT	2
+#define ICACHE_POLICY_PIPT	3
 
 #define L1_CACHE_SHIFT		7
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
@@ -32,6 +42,25 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/bitops.h>
+
+#define ICACHEF_ALIASING	0
+extern unsigned long __icache_flags;
+
+/*
+ * Whilst the D-side always behaves as PIPT on AArch64, aliasing is
+ * permitted in the I-cache.
+ */
+static inline int icache_is_aliasing(void)
+{
+	return test_bit(ICACHEF_ALIASING, &__icache_flags);
+}
+
+static inline u32 cache_type_cwg(void)
+{
+	return (read_cpuid_cachetype() >> CTR_CWG_SHIFT) & CTR_CWG_MASK;
+}
+
 #define __read_mostly __attribute__((__section__(".data..read_mostly")))
 
 static inline int cache_line_size(void)

commit 97303480753e48fb313dc0e15daaf11b0451cdb8
Author: Tirumalesh Chalamarla <tchalamarla@cavium.com>
Date:   Tue Sep 22 19:59:48 2015 +0200

    arm64: Increase the max granular size
    
    Increase the standard cacheline size to avoid having locks in the same
    cacheline.
    
    Cavium's ThunderX core implements cache lines of 128 byte size. With
    current granulare size of 64 bytes (L1_CACHE_SHIFT=6) two locks could
    share the same cache line leading a performance degradation.
    Increasing the size fixes that.
    
    Increasing the size has no negative impact to cache invalidation on
    systems with a smaller cache line. There is an impact on memory usage,
    but that's not too important for arm64 use cases.
    
    Signed-off-by: Tirumalesh Chalamarla <tchalamarla@cavium.com>
    Signed-off-by: Robert Richter <rrichter@cavium.com>
    Acked-by: Timur Tabi <timur@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index bde449936e2f..5082b30bc2c0 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -18,7 +18,7 @@
 
 #include <asm/cachetype.h>
 
-#define L1_CACHE_SHIFT		6
+#define L1_CACHE_SHIFT		7
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
 /*

commit e4f88d833bec29b8e6fadc1b2488f0c6370935e1
Author: Jungseok Lee <jungseoklee85@gmail.com>
Date:   Tue Dec 2 17:49:24 2014 +0000

    arm64: Implement support for read-mostly sections
    
    As putting data which is read mostly together, we can avoid
    unnecessary cache line bouncing.
    
    Other architectures, such as ARM and x86, adopted the same idea.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 88cc05b5f3ac..bde449936e2f 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -32,6 +32,8 @@
 
 #ifndef __ASSEMBLY__
 
+#define __read_mostly __attribute__((__section__(".data..read_mostly")))
+
 static inline int cache_line_size(void)
 {
 	u32 cwg = cache_type_cwg();

commit a41dc0e841523efe1df7fa5ad48b5e9027a921df
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Apr 3 17:48:54 2014 +0100

    arm64: Implement cache_line_size() based on CTR_EL0.CWG
    
    The hardware provides the maximum cache line size in the system via the
    CTR_EL0.CWG bits. This patch implements the cache_line_size() function
    to read such information, together with a sanity check if the statically
    defined L1_CACHE_BYTES is smaller than the hardware value.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
index 390308a67f0d..88cc05b5f3ac 100644
--- a/arch/arm64/include/asm/cache.h
+++ b/arch/arm64/include/asm/cache.h
@@ -16,6 +16,8 @@
 #ifndef __ASM_CACHE_H
 #define __ASM_CACHE_H
 
+#include <asm/cachetype.h>
+
 #define L1_CACHE_SHIFT		6
 #define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
 
@@ -27,6 +29,15 @@
  * the CPU.
  */
 #define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
-#define ARCH_SLAB_MINALIGN	8
+
+#ifndef __ASSEMBLY__
+
+static inline int cache_line_size(void)
+{
+	u32 cwg = cache_type_cwg();
+	return cwg ? 4 << cwg : L1_CACHE_BYTES;
+}
+
+#endif	/* __ASSEMBLY__ */
 
 #endif

commit f1a0c4aa0937975b53991842a494f741d7769b02
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: Cache maintenance routines
    
    The patch adds functionality required for cache maintenance. The AArch64
    architecture mandates non-aliasing VIPT or PIPT D-cache and VIPT (may
    have aliases) or ASID-tagged VIVT I-cache. Cache maintenance operations
    are automatically broadcast in hardware between CPUs.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/cache.h b/arch/arm64/include/asm/cache.h
new file mode 100644
index 000000000000..390308a67f0d
--- /dev/null
+++ b/arch/arm64/include/asm/cache.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_CACHE_H
+#define __ASM_CACHE_H
+
+#define L1_CACHE_SHIFT		6
+#define L1_CACHE_BYTES		(1 << L1_CACHE_SHIFT)
+
+/*
+ * Memory returned by kmalloc() may be used for DMA, so we must make
+ * sure that all such allocations are cache aligned. Otherwise,
+ * unrelated code may cause parts of the buffer to be read into the
+ * cache before the transfer is done, causing old data to be seen by
+ * the CPU.
+ */
+#define ARCH_DMA_MINALIGN	L1_CACHE_BYTES
+#define ARCH_SLAB_MINALIGN	8
+
+#endif
