commit 49b3deaad3452217d62dbd78da8df24eb0c7e169
Merge: e0135a104c52 15c99816ed93
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 11 14:02:32 2020 -0400

    Merge tag 'kvmarm-fixes-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm64 fixes for Linux 5.8, take #1
    
    * 32bit VM fixes:
      - Fix embarassing mapping issue between AArch32 CSSELR and AArch64
        ACTLR
      - Add ACTLR2 support for AArch32
      - Get rid of the useless ACTLR_EL1 save/restore
      - Fix CP14/15 accesses for AArch32 guests on BE hosts
      - Ensure that we don't loose any state when injecting a 32bit
        exception when running on a VHE host
    
    * 64bit VM fixes:
      - Fix PtrAuth host saving happening in preemptible contexts
      - Optimize PtrAuth lazy enable
      - Drop vcpu to cpu context pointer
      - Fix sparse warnings for HYP per-CPU accesses

commit 15c99816ed9396c548eed2e84f30c14caccad1f4
Merge: 0370964dd3ff 304e2989c93e
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Jun 10 19:09:26 2020 +0100

    Merge branch 'kvm-arm64/ptrauth-fixes' into kvmarm-master/next
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>

commit 3204be4109ad681523e3461ce64454c79278450a
Author: Marc Zyngier <maz@kernel.org>
Date:   Tue Jun 9 08:40:35 2020 +0100

    KVM: arm64: Make vcpu_cp1x() work on Big Endian hosts
    
    AArch32 CP1x registers are overlayed on their AArch64 counterparts
    in the vcpu struct. This leads to an interesting problem as they
    are stored in their CPU-local format, and thus a CP1x register
    doesn't "hit" the lower 32bit portion of the AArch64 register on
    a BE host.
    
    To workaround this unfortunate situation, introduce a bias trick
    in the vcpu_cp1x() accessors which picks the correct half of the
    64bit register.
    
    Cc: stable@vger.kernel.org
    Reported-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Acked-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 59029e90b557..521eaedfdcc3 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -404,8 +404,10 @@ void vcpu_write_sys_reg(struct kvm_vcpu *vcpu, u64 val, int reg);
  * CP14 and CP15 live in the same array, as they are backed by the
  * same system registers.
  */
-#define vcpu_cp14(v,r)		((v)->arch.ctxt.copro[(r)])
-#define vcpu_cp15(v,r)		((v)->arch.ctxt.copro[(r)])
+#define CPx_BIAS		IS_ENABLED(CONFIG_CPU_BIG_ENDIAN)
+
+#define vcpu_cp14(v,r)		((v)->arch.ctxt.copro[(r) ^ CPx_BIAS])
+#define vcpu_cp15(v,r)		((v)->arch.ctxt.copro[(r) ^ CPx_BIAS])
 
 struct kvm_vm_stat {
 	ulong remote_tlb_flush;

commit 07da1ffaa1373f99331712faa67a00b5b807dfe8
Author: Marc Zyngier <maz@kernel.org>
Date:   Fri Jun 5 14:08:13 2020 +0100

    KVM: arm64: Remove host_cpu_context member from vcpu structure
    
    For very long, we have kept this pointer back to the per-cpu
    host state, despite having working per-cpu accessors at EL2
    for some time now.
    
    Recent investigations have shown that this pointer is easy
    to abuse in preemptible context, which is a sure sign that
    it would better be gone. Not to mention that a per-cpu
    pointer is faster to access at all times.
    
    Reported-by: Andrew Scull <ascull@google.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com
    Reviewed-by: Andrew Scull <ascull@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 59029e90b557..ada1faa92211 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -284,9 +284,6 @@ struct kvm_vcpu_arch {
 	struct kvm_guest_debug_arch vcpu_debug_state;
 	struct kvm_guest_debug_arch external_debug_state;
 
-	/* Pointer to host CPU context */
-	struct kvm_cpu_context *host_cpu_context;
-
 	struct thread_info *host_thread_info;	/* hyp VA */
 	struct user_fpsimd_state *host_fpsimd_state;	/* hyp VA */
 

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit 380609445cff6ee7b6c7e43bbdc953985b45a6ed
Merge: 09d952c971a5 8f7f4fe756bd
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jun 1 04:26:27 2020 -0400

    Merge tag 'kvmarm-5.8' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm64 updates for Linux 5.8:
    
    - Move the arch-specific code into arch/arm64/kvm
    - Start the post-32bit cleanup
    - Cherry-pick a few non-invasive pre-NV patches

commit c350717ec7de67daad26d996efe7f3d97d95aa9c
Merge: d27865279f12 fe677be98914
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:02:51 2020 +0100

    Merge branch 'for-next/kvm/errata' into for-next/core
    
    KVM CPU errata rework
    (Andrew Scull and Marc Zyngier)
    * for-next/kvm/errata:
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}

commit b130a8f70cbbf907c399799bd1073a78763ca0e7
Author: Marc Zyngier <maz@kernel.org>
Date:   Thu May 28 14:12:58 2020 +0100

    KVM: arm64: Check advertised Stage-2 page size capability
    
    With ARMv8.5-GTG, the hardware (or more likely a hypervisor) can
    advertise the supported Stage-2 page sizes.
    
    Let's check this at boot time.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Alexandru Elisei <alexandru.elisei@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 32c8a675e5a4..7dd8fefa6aec 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -670,7 +670,7 @@ static inline int kvm_arm_have_ssbd(void)
 void kvm_vcpu_load_sysregs(struct kvm_vcpu *vcpu);
 void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu);
 
-void kvm_set_ipa_limit(void);
+int kvm_set_ipa_limit(void);
 
 #define __KVM_HAVE_ARCH_VM_ALLOC
 struct kvm *kvm_arch_alloc_vm(void);

commit 8f7f4fe756bd5cfef73cf8234445081385bdbf7d
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed May 27 11:38:57 2020 +0100

    KVM: arm64: Drop obsolete comment about sys_reg ordering
    
    The general comment about keeping the enum order in sync
    with the save/restore code has been obsolete for many years now.
    
    Just drop it.
    
    Note that there are other ordering requirements in the enum,
    such as the PtrAuth and PMU registers, which are still valid.
    
    Reported-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 69a338a390a6..59029e90b557 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -115,12 +115,8 @@ struct kvm_vcpu_fault_info {
 	u64 disr_el1;		/* Deferred [SError] Status Register */
 };
 
-/*
- * 0 is reserved as an invalid value.
- * Order should be kept in sync with the save/restore code.
- */
 enum vcpu_sysreg {
-	__INVALID_SYSREG__,
+	__INVALID_SYSREG__,   /* 0 is reserved as an invalid value */
 	MPIDR_EL1,	/* MultiProcessor Affinity Register */
 	CSSELR_EL1,	/* Cache Size Selection Register */
 	SCTLR_EL1,	/* System Control Register */

commit 71b3ec5f221b8b3ff545639be83ddfcd5d7c9800
Author: David Brazdil <dbrazdil@google.com>
Date:   Fri May 15 16:20:56 2020 +0100

    KVM: arm64: Clean up cpu_init_hyp_mode()
    
    Pull bits of code to the only place where it is used. Remove empty function
    __cpu_init_stage2(). Remove redundant has_vhe() check since this function is
    nVHE-only. No functional changes intended.
    
    Signed-off-by: David Brazdil <dbrazdil@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200515152056.83158-1-dbrazdil@google.com

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a723f84fab83..69a338a390a6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -533,39 +533,6 @@ static inline void kvm_init_host_cpu_context(struct kvm_cpu_context *cpu_ctxt)
 	cpu_ctxt->sys_regs[MPIDR_EL1] = read_cpuid_mpidr();
 }
 
-void __kvm_enable_ssbs(void);
-
-static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
-				       unsigned long hyp_stack_ptr,
-				       unsigned long vector_ptr)
-{
-	/*
-	 * Calculate the raw per-cpu offset without a translation from the
-	 * kernel's mapping to the linear mapping, and store it in tpidr_el2
-	 * so that we can use adr_l to access per-cpu variables in EL2.
-	 */
-	u64 tpidr_el2 = ((u64)this_cpu_ptr(&kvm_host_data) -
-			 (u64)kvm_ksym_ref(kvm_host_data));
-
-	/*
-	 * Call initialization code, and switch to the full blown HYP code.
-	 * If the cpucaps haven't been finalized yet, something has gone very
-	 * wrong, and hyp will crash and burn when it uses any
-	 * cpus_have_const_cap() wrapper.
-	 */
-	BUG_ON(!system_capabilities_finalized());
-	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr, tpidr_el2);
-
-	/*
-	 * Disabling SSBD on a non-VHE system requires us to enable SSBS
-	 * at EL2.
-	 */
-	if (!has_vhe() && this_cpu_has_cap(ARM64_SSBS) &&
-	    arm64_get_ssbd_state() == ARM64_SSBD_FORCE_DISABLE) {
-		kvm_call_hyp(__kvm_enable_ssbs);
-	}
-}
-
 static inline bool kvm_arch_requires_vhe(void)
 {
 	/*
@@ -601,8 +568,6 @@ int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,
 int kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr);
 
-static inline void __cpu_init_stage2(void) {}
-
 /* Guest/host FPSIMD coordination helpers */
 int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu);

commit c862626e19efdc26b26481515470b160e8fe52f3
Author: Keqian Zhu <zhukeqian1@huawei.com>
Date:   Mon Apr 13 20:20:23 2020 +0800

    KVM: arm64: Support enabling dirty log gradually in small chunks
    
    There is already support of enabling dirty log gradually in small chunks
    for x86 in commit 3c9bd4006bfc ("KVM: x86: enable dirty log gradually in
    small chunks"). This adds support for arm64.
    
    x86 still writes protect all huge pages when DIRTY_LOG_INITIALLY_ALL_SET
    is enabled. However, for arm64, both huge pages and normal pages can be
    write protected gradually by userspace.
    
    Under the Huawei Kunpeng 920 2.6GHz platform, I did some tests on 128G
    Linux VMs with different page size. The memory pressure is 127G in each
    case. The time taken of memory_global_dirty_log_start in QEMU is listed
    below:
    
    Page Size      Before    After Optimization
      4K            650ms         1.8ms
      2M             4ms          1.8ms
      1G             2ms          1.8ms
    
    Besides the time reduction, the biggest improvement is that we will minimize
    the performance side effect (because of dissolving huge pages and marking
    memslots dirty) on guest after enabling dirty log.
    
    Signed-off-by: Keqian Zhu <zhukeqian1@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200413122023.52583-1-zhukeqian1@huawei.com

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 32c8a675e5a4..a723f84fab83 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -46,6 +46,9 @@
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
 #define KVM_REQ_RELOAD_GICv4	KVM_ARCH_REQ(4)
 
+#define KVM_DIRTY_LOG_MANUAL_CAPS   (KVM_DIRTY_LOG_MANUAL_PROTECT_ENABLE | \
+				     KVM_DIRTY_LOG_INITIALLY_SET)
+
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
 extern unsigned int kvm_sve_max_vl;

commit cb953129bfe5c0f2da835a0469930873fb7e71df
Author: David Matlack <dmatlack@google.com>
Date:   Fri May 8 11:22:40 2020 -0700

    kvm: add halt-polling cpu usage stats
    
    Two new stats for exposing halt-polling cpu usage:
    halt_poll_success_ns
    halt_poll_fail_ns
    
    Thus sum of these 2 stats is the total cpu time spent polling. "success"
    means the VCPU polled until a virtual interrupt was delivered. "fail"
    means the VCPU had to schedule out (either because the maximum poll time
    was reached or it needed to yield the CPU).
    
    To avoid touching every arch's kvm_vcpu_stat struct, only update and
    export halt-polling cpu usage stats if we're on x86.
    
    Exporting cpu usage as a u64 and in nanoseconds means we will overflow at
    ~500 years, which seems reasonably large.
    
    Signed-off-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Jon Cargille <jcargill@google.com>
    Reviewed-by: Jim Mattson <jmattson@google.com>
    
    Message-Id: <20200508182240.68440-1-jcargill@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 32c8a675e5a4..3833736dd064 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -415,6 +415,8 @@ struct kvm_vm_stat {
 struct kvm_vcpu_stat {
 	u64 halt_successful_poll;
 	u64 halt_attempted_poll;
+	u64 halt_poll_success_ns;
+	u64 halt_poll_fail_ns;
 	u64 halt_poll_invalid;
 	u64 halt_wakeup;
 	u64 hvc_exit_stat;

commit 02ab1f5018c3ad0b8677e797b5d3333d2e3b7f20
Author: Andrew Scull <ascull@google.com>
Date:   Mon May 4 10:48:58 2020 +0100

    arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}
    
    Errata 1165522, 1319367 and 1530923 each allow TLB entries to be
    allocated as a result of a speculative AT instruction. In order to
    avoid mandating VHE on certain affected CPUs, apply the workaround to
    both the nVHE and the VHE case for all affected CPUs.
    
    Signed-off-by: Andrew Scull <ascull@google.com>
    Acked-by: Will Deacon <will@kernel.org>
    CC: Marc Zyngier <maz@kernel.org>
    CC: James Morse <james.morse@arm.com>
    CC: Suzuki K Poulose <suzuki.poulose@arm.com>
    CC: Will Deacon <will@kernel.org>
    CC: Steven Price <steven.price@arm.com>
    Link: https://lore.kernel.org/r/20200504094858.108917-1-ascull@google.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 32c8a675e5a4..d0e7d7934a1f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -573,10 +573,6 @@ static inline bool kvm_arch_requires_vhe(void)
 	if (system_supports_sve())
 		return true;
 
-	/* Some implementations have defects that confine them to VHE */
-	if (cpus_have_cap(ARM64_WORKAROUND_SPECULATIVE_AT_VHE))
-		return true;
-
 	return false;
 }
 

commit d9c3872cd2f86b7295446e35b4801270669d2960
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Mar 4 20:33:28 2020 +0000

    KVM: arm64: GICv4.1: Reload VLPI configuration on distributor enable/disable
    
    Each time a Group-enable bit gets flipped, the state of these bits
    needs to be forwarded to the hardware. This is a pretty heavy
    handed operation, requiring all vcpus to reload their GICv4
    configuration. It is thus implemented as a new request type.
    
    These enable bits are programmed into the HW by setting the VGrp{0,1}En
    fields of GICR_VPENDBASER when the vPEs are made resident again.
    
    Of course, we only support Group-1 for now...
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Zenghui Yu <yuzenghui@huawei.com>
    Link: https://lore.kernel.org/r/20200304203330.4967-22-maz@kernel.org

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 57fd46acd058..32c8a675e5a4 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -44,6 +44,7 @@
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 #define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
+#define KVM_REQ_RELOAD_GICv4	KVM_ARCH_REQ(4)
 
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 

commit e951445f4d3b5d0df69c0c5d18ab1e9058c20e52
Merge: ef935c25fd64 e43f1331e2ef
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 28 11:46:59 2020 +0100

    Merge tag 'kvmarm-fixes-5.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm fixes for 5.6, take #1
    
    - Fix compilation on 32bit
    - Move  VHE guest entry/exit into the VHE-specific entry code
    - Make sure all functions called by the non-VHE HYP code is tagged as __always_inline

commit b3f15ec3d809ccf2e171ca4e272a220d3c1a3e05
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Feb 10 11:47:57 2020 +0000

    kvm: arm/arm64: Fold VHE entry/exit work into kvm_vcpu_run_vhe()
    
    With VHE, running a vCPU always requires the sequence:
    
    1. kvm_arm_vhe_guest_enter();
    2. kvm_vcpu_run_vhe();
    3. kvm_arm_vhe_guest_exit()
    
    ... and as we invoke this from the shared arm/arm64 KVM code, 32-bit arm
    has to provide stubs for all three functions.
    
    To simplify the common code, and make it easier to make further
    modifications to the arm64-specific portions in the near future, let's
    fold kvm_arm_vhe_guest_enter() and kvm_arm_vhe_guest_exit() into
    kvm_vcpu_run_vhe().
    
    The 32-bit stubs for kvm_arm_vhe_guest_enter() and
    kvm_arm_vhe_guest_exit() are removed, as they are no longer used. The
    32-bit stub for kvm_vcpu_run_vhe() is left as-is.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200210114757.2889-1-mark.rutland@arm.com

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f6a77ddab956..d740ec00ecd3 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -628,38 +628,6 @@ static inline void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr) {}
 static inline void kvm_clr_pmu_events(u32 clr) {}
 #endif
 
-static inline void kvm_arm_vhe_guest_enter(void)
-{
-	local_daif_mask();
-
-	/*
-	 * Having IRQs masked via PMR when entering the guest means the GIC
-	 * will not signal the CPU of interrupts of lower priority, and the
-	 * only way to get out will be via guest exceptions.
-	 * Naturally, we want to avoid this.
-	 *
-	 * local_daif_mask() already sets GIC_PRIO_PSR_I_SET, we just need a
-	 * dsb to ensure the redistributor is forwards EL2 IRQs to the CPU.
-	 */
-	pmr_sync();
-}
-
-static inline void kvm_arm_vhe_guest_exit(void)
-{
-	/*
-	 * local_daif_restore() takes care to properly restore PSTATE.DAIF
-	 * and the GIC PMR if the host is using IRQ priorities.
-	 */
-	local_daif_restore(DAIF_PROCCTX_NOIRQ);
-
-	/*
-	 * When we exit from the guest we change a number of CPU configuration
-	 * parameters, such as traps.  Make sure these changes take effect
-	 * before running the host or additional guests.
-	 */
-	isb();
-}
-
 #define KVM_BP_HARDEN_UNKNOWN		-1
 #define KVM_BP_HARDEN_WA_NEEDED		0
 #define KVM_BP_HARDEN_NOT_REQUIRED	1

commit e813e65038389b66d2f8dd87588694caf8dc2923
Merge: ccaaaf6fe5a5 4cbc418a44d5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 31 09:30:41 2020 -0800

    Merge tag 'kvm-5.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "This is the first batch of KVM changes.
    
      ARM:
       - cleanups and corner case fixes.
    
      PPC:
       - Bugfixes
    
      x86:
       - Support for mapping DAX areas with large nested page table entries.
    
       - Cleanups and bugfixes here too. A particularly important one is a
         fix for FPU load when the thread has TIF_NEED_FPU_LOAD. There is
         also a race condition which could be used in guest userspace to
         exploit the guest kernel, for which the embargo expired today.
    
       - Fast path for IPI delivery vmexits, shaving about 200 clock cycles
         from IPI latency.
    
       - Protect against "Spectre-v1/L1TF" (bring data in the cache via
         speculative out of bound accesses, use L1TF on the sibling
         hyperthread to read it), which unfortunately is an even bigger
         whack-a-mole game than SpectreV1.
    
      Sean continues his mission to rewrite KVM. In addition to a sizable
      number of x86 patches, this time he contributed a pretty large
      refactoring of vCPU creation that affects all architectures but should
      not have any visible effect.
    
      s390 will come next week together with some more x86 patches"
    
    * tag 'kvm-5.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      x86/KVM: Clean up host's steal time structure
      x86/KVM: Make sure KVM_VCPU_FLUSH_TLB flag is not missed
      x86/kvm: Cache gfn to pfn translation
      x86/kvm: Introduce kvm_(un)map_gfn()
      x86/kvm: Be careful not to clear KVM_VCPU_FLUSH_TLB bit
      KVM: PPC: Book3S PR: Fix -Werror=return-type build failure
      KVM: PPC: Book3S HV: Release lock on page-out failure path
      KVM: arm64: Treat emulated TVAL TimerValue as a signed 32-bit integer
      KVM: arm64: pmu: Only handle supported event counters
      KVM: arm64: pmu: Fix chained SW_INCR counters
      KVM: arm64: pmu: Don't mark a counter as chained if the odd one is disabled
      KVM: arm64: pmu: Don't increment SW_INCR if PMCR.E is unset
      KVM: x86: Use a typedef for fastop functions
      KVM: X86: Add 'else' to unify fastop and execute call path
      KVM: x86: inline memslot_valid_for_gpte
      KVM: x86/mmu: Use huge pages for DAX-backed files
      KVM: x86/mmu: Remove lpage_is_disallowed() check from set_spte()
      KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust()
      KVM: x86/mmu: Zap any compound page when collapsing sptes
      KVM: x86/mmu: Remove obsolete gfn restoration in FNAME(fetch)
      ...

commit 621ab20c06e0c0b45eb2382c048a0426bbff9b0e
Merge: 3009afc6e39e 4a267aa70795
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 30 18:13:14 2020 +0100

    Merge tag 'kvmarm-5.6' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for Linux 5.6
    
    - Fix MMIO sign extension
    - Fix HYP VA tagging on tag space exhaustion
    - Fix PSTATE/CPSR handling when generating exception
    - Fix MMU notifier's advertizing of young pages
    - Fix poisoned page handling
    - Fix PMU SW event handling
    - Fix TVAL register access
    - Fix AArch32 external abort injection
    - Fix ITS unmapped collection handling
    - Various cleanups

commit 7495e22bb165e7030bae4d9c6e84addb5ea17b29
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jan 9 09:57:19 2020 -0500

    KVM: Move running VCPU from ARM to common code
    
    For ring-based dirty log tracking, it will be more efficient to account
    writes during schedule-out or schedule-in to the currently running VCPU.
    We would like to do it even if the write doesn't use the current VCPU's
    address space, as is the case for cached writes (see commit 4e335d9e7ddb,
    "Revert "KVM: Support vCPU-based gfn->hva cache"", 2017-05-02).
    
    Therefore, add a mechanism to track the currently-loaded kvm_vcpu struct.
    There is already something similar in KVM/ARM; one important difference
    is that kvm_arch_vcpu_{load,put} have two callers in virt/kvm/kvm_main.c:
    we have to update both the architecture-independent vcpu_{load,put} and
    the preempt notifiers.
    
    Another change made in the process is to allow using kvm_get_running_vcpu()
    in preemptible code.  This is allowed because preempt notifiers ensure
    that the value does not change even after the VCPU thread is migrated.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 8ab62944e514..eb992eaa4165 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -446,8 +446,6 @@ int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 
-struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
-struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 void kvm_arm_halt_guest(struct kvm *kvm);
 void kvm_arm_resume_guest(struct kvm *kvm);
 

commit ddd259c9aaba08244dba8877687ee856f79c4f45
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:28 2019 -0800

    KVM: Drop kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit()
    
    Remove kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit() now that all
    arch specific implementations are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 6402b2de1844..8ab62944e514 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -54,7 +54,6 @@ int kvm_arm_init_sve(void);
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 void kvm_arm_vcpu_destroy(struct kvm_vcpu *vcpu);
-void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
 int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 

commit 19bcc89eb8a9fa1d4be4bff5b5e7917cb8bbc1f7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:27 2019 -0800

    KVM: arm64: Free sve_state via arm specific hook
    
    Add an arm specific hook to free the arm64-only sve_state.  Doing so
    eliminates the last functional code from kvm_arch_vcpu_uninit() across
    all architectures and paves the way for removing kvm_arch_vcpu_init()
    and kvm_arch_vcpu_uninit() entirely.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c61260cf63c5..6402b2de1844 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -53,6 +53,7 @@ int kvm_arm_init_sve(void);
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
+void kvm_arm_vcpu_destroy(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
 int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);

commit 0e20f5e25556c00ee813469d373b00abcf298708
Author: Marc Zyngier <maz@kernel.org>
Date:   Fri Dec 13 13:25:25 2019 +0000

    KVM: arm/arm64: Cleanup MMIO handling
    
    Our MMIO handling is a bit odd, in the sense that it uses an
    intermediate per-vcpu structure to store the various decoded
    information that describe the access.
    
    But the same information is readily available in the HSR/ESR_EL2
    field, and we actually use this field to populate the structure.
    
    Let's simplify the whole thing by getting rid of the superfluous
    structure and save a (tiny) bit of space in the vcpu structure.
    
    [32bit fix courtesy of Olof Johansson <olof@lixom.net>]
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c61260cf63c5..f6a77ddab956 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -24,7 +24,6 @@
 #include <asm/fpsimd.h>
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
-#include <asm/kvm_mmio.h>
 #include <asm/thread_info.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
@@ -325,9 +324,6 @@ struct kvm_vcpu_arch {
 	/* Don't run the guest (internal implementation need) */
 	bool pause;
 
-	/* IO related fields */
-	struct kvm_decode mmio_decode;
-
 	/* Cache some mmu pages needed inside spinlock regions */
 	struct kvm_mmu_memory_cache mmu_page_cache;
 
@@ -491,6 +487,14 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 void handle_exit_early(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		       int exception_index);
 
+/* MMIO helpers */
+void kvm_mmio_write_buf(void *buf, unsigned int len, unsigned long data);
+unsigned long kvm_mmio_read_buf(const void *buf, unsigned int len);
+
+int kvm_handle_mmio_return(struct kvm_vcpu *vcpu, struct kvm_run *run);
+int io_mem_abort(struct kvm_vcpu *vcpu, struct kvm_run *run,
+		 phys_addr_t fault_ipa);
+
 int kvm_perf_init(void);
 int kvm_perf_teardown(void);
 

commit ab3906c53144837f1a192b5c3ba71ec2f938c187
Merge: aa246c056c43 275fa0ea2cf7
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:35:05 2020 +0000

    Merge branch 'for-next/errata' into for-next/core
    
    * for-next/errata: (3 commits)
      arm64: Workaround for Cortex-A55 erratum 1530923
      ...

commit e85d68faed4e79fd0b481c72de8245d4290369db
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:29 2019 +0000

    arm64: Rename WORKAROUND_1165522 to SPECULATIVE_AT_VHE
    
    Cortex-A55 is affected by a similar erratum, so rename the existing
    workaround for errarum 1165522 so it can be used for both errata.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c61260cf63c5..2a5f58857d12 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -571,7 +571,7 @@ static inline bool kvm_arch_requires_vhe(void)
 		return true;
 
 	/* Some implementations have defects that confine them to VHE */
-	if (cpus_have_cap(ARM64_WORKAROUND_1165522))
+	if (cpus_have_cap(ARM64_WORKAROUND_SPECULATIVE_AT_VHE))
 		return true;
 
 	return false;

commit b51c6ac220f77eb246e940442d970b4065c197b0
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 13 23:30:17 2020 +0000

    arm64: Introduce system_capabilities_finalized() marker
    
    We finalize the system wide capabilities after the SMP CPUs
    are booted by the kernel. This is used as a marker for deciding
    various checks in the kernel. e.g, sanity check the hotplugged
    CPUs for missing mandatory features.
    
    However there is no explicit helper available for this in the
    kernel. There is sys_caps_initialised, which is not exposed.
    The other closest one we have is the jump_label arm64_const_caps_ready
    which denotes that the capabilities are set and the capability checks
    could use the individual jump_labels for fast path. This is
    performed before setting the ELF Hwcaps, which must be checked
    against the new CPUs. We also perform some of the other initialization
    e.g, SVE setup, which is important for the use of FP/SIMD
    where SVE is supported. Normally userspace doesn't get to run
    before we finish this. However the in-kernel users may
    potentially start using the neon mode. So, we need to
    reject uses of neon mode before we are set. Instead of defining
    a new marker for the completion of SVE setup, we could simply
    reuse the arm64_const_caps_ready and enable it once we have
    finished all the setup. Also we could expose this to the
    various users as "system_capabilities_finalized()" to make
    it more meaningful than "const_caps_ready".
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c61260cf63c5..48ce54639eb5 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -547,7 +547,7 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	 * wrong, and hyp will crash and burn when it uses any
 	 * cpus_have_const_cap() wrapper.
 	 */
-	BUG_ON(!static_branch_likely(&arm64_const_caps_ready));
+	BUG_ON(!system_capabilities_finalized());
 	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr, tpidr_el2);
 
 	/*

commit 752272f16dd18f2cac58a583a8673c8e2fb93abb
Merge: 3f3c8be973af 96710247298d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 18:02:36 2019 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - data abort report and injection
       - steal time support
       - GICv4 performance improvements
       - vgic ITS emulation fixes
       - simplify FWB handling
       - enable halt polling counters
       - make the emulated timer PREEMPT_RT compliant
    
      s390:
       - small fixes and cleanups
       - selftest improvements
       - yield improvements
    
      PPC:
       - add capability to tell userspace whether we can single-step the
         guest
       - improve the allocation of XIVE virtual processor IDs
       - rewrite interrupt synthesis code to deliver interrupts in virtual
         mode when appropriate.
       - minor cleanups and improvements.
    
      x86:
       - XSAVES support for AMD
       - more accurate report of nested guest TSC to the nested hypervisor
       - retpoline optimizations
       - support for nested 5-level page tables
       - PMU virtualization optimizations, and improved support for nested
         PMU virtualization
       - correct latching of INITs for nested virtualization
       - IOAPIC optimization
       - TSX_CTRL virtualization for more TAA happiness
       - improved allocation and flushing of SEV ASIDs
       - many bugfixes and cleanups"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (127 commits)
      kvm: nVMX: Relax guest IA32_FEATURE_CONTROL constraints
      KVM: x86: Grab KVM's srcu lock when setting nested state
      KVM: x86: Open code shared_msr_update() in its only caller
      KVM: Fix jump label out_free_* in kvm_init()
      KVM: x86: Remove a spurious export of a static function
      KVM: x86: create mmu/ subdirectory
      KVM: nVMX: Remove unnecessary TLB flushes on L1<->L2 switches when L1 use apic-access-page
      KVM: x86: remove set but not used variable 'called'
      KVM: nVMX: Do not mark vmcs02->apic_access_page as dirty when unpinning
      KVM: vmx: use MSR_IA32_TSX_CTRL to hard-disable TSX on guest that lack it
      KVM: vmx: implement MSR_IA32_TSX_CTRL disable RTM functionality
      KVM: x86: implement MSR_IA32_TSX_CTRL effect on CPUID
      KVM: x86: do not modify masked bits of shared MSRs
      KVM: x86: fix presentation of TSX feature in ARCH_CAPABILITIES
      KVM: PPC: Book3S HV: XIVE: Fix potential page leak on error path
      KVM: PPC: Book3S HV: XIVE: Free previous EQ page when setting up a new one
      KVM: nVMX: Assume TLB entries of L1 and L2 are tagged differently if L0 use EPT
      KVM: x86: Unexport kvm_vcpu_reload_apic_access_page()
      KVM: nVMX: add CR4_LA57 bit to nested CR4_FIXED1
      KVM: nVMX: Use semi-colon instead of comma for exit-handlers initialization
      ...

commit a4b28f5c67983d92c911ca1404728bc4ea958c0e
Merge: da345174ceca c7892db5dd6a
Author: Marc Zyngier <maz@kernel.org>
Date:   Thu Oct 24 15:04:09 2019 +0100

    Merge remote-tracking branch 'kvmarm/kvm-arm64/stolen-time' into kvmarm-master/next

commit 58772e9a3db72d032eeb12bc011bc5184a3925f4
Author: Steven Price <steven.price@arm.com>
Date:   Mon Oct 21 16:28:20 2019 +0100

    KVM: arm64: Provide VCPU attributes for stolen time
    
    Allow user space to inform the KVM host where in the physical memory
    map the paravirtualized time structures should be located.
    
    User space can set an attribute on the VCPU providing the IPA base
    address of the stolen time structure for that VCPU. This must be
    repeated for every VCPU in the VM.
    
    The address is given in terms of the physical address visible to
    the guest and must be 64 byte aligned. The guest will discover the
    address via a hypercall.
    
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 75ef37f79633..eb1f33af45aa 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -490,6 +490,13 @@ long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu);
 gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu);
 void kvm_update_stolen_time(struct kvm_vcpu *vcpu);
 
+int kvm_arm_pvtime_set_attr(struct kvm_vcpu *vcpu,
+			    struct kvm_device_attr *attr);
+int kvm_arm_pvtime_get_attr(struct kvm_vcpu *vcpu,
+			    struct kvm_device_attr *attr);
+int kvm_arm_pvtime_has_attr(struct kvm_vcpu *vcpu,
+			    struct kvm_device_attr *attr);
+
 static inline void kvm_arm_pvtime_vcpu_init(struct kvm_vcpu_arch *vcpu_arch)
 {
 	vcpu_arch->steal.base = GPA_INVALID;

commit 8564d6372a7d8a6d440441b8ed8020f97f744450
Author: Steven Price <steven.price@arm.com>
Date:   Mon Oct 21 16:28:18 2019 +0100

    KVM: arm64: Support stolen time reporting via shared structure
    
    Implement the service call for configuring a shared structure between a
    VCPU and the hypervisor in which the hypervisor can write the time
    stolen from the VCPU's execution time by other tasks on the host.
    
    User space allocates memory which is placed at an IPA also chosen by user
    space. The hypervisor then updates the shared structure using
    kvm_put_guest() to ensure single copy atomicity of the 64-bit value
    reporting the stolen time in nanoseconds.
    
    Whenever stolen time is enabled by the guest, the stolen time counter is
    reset.
    
    The stolen time itself is retrieved from the sched_info structure
    maintained by the Linux scheduler code. We enable SCHEDSTATS when
    selecting KVM Kconfig to ensure this value is meaningful.
    
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 93b46d9526d0..75ef37f79633 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -44,6 +44,7 @@
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 #define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
+#define KVM_REQ_RECORD_STEAL	KVM_ARCH_REQ(3)
 
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
@@ -338,6 +339,13 @@ struct kvm_vcpu_arch {
 	/* True when deferrable sysregs are loaded on the physical CPU,
 	 * see kvm_vcpu_load_sysregs and kvm_vcpu_put_sysregs. */
 	bool sysregs_loaded_on_cpu;
+
+	/* Guest PV state */
+	struct {
+		u64 steal;
+		u64 last_steal;
+		gpa_t base;
+	} steal;
 };
 
 /* Pointer to the vcpu's SVE FFR for sve_{save,load}_state() */
@@ -479,6 +487,18 @@ int kvm_perf_init(void);
 int kvm_perf_teardown(void);
 
 long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu);
+gpa_t kvm_init_stolen_time(struct kvm_vcpu *vcpu);
+void kvm_update_stolen_time(struct kvm_vcpu *vcpu);
+
+static inline void kvm_arm_pvtime_vcpu_init(struct kvm_vcpu_arch *vcpu_arch)
+{
+	vcpu_arch->steal.base = GPA_INVALID;
+}
+
+static inline bool kvm_arm_is_pvtime_enabled(struct kvm_vcpu_arch *vcpu_arch)
+{
+	return (vcpu_arch->steal.base != GPA_INVALID);
+}
 
 void kvm_set_sei_esr(struct kvm_vcpu *vcpu, u64 syndrome);
 

commit b48c1a45a190898103cec28771efc399fd65a05a
Author: Steven Price <steven.price@arm.com>
Date:   Mon Oct 21 16:28:16 2019 +0100

    KVM: arm64: Implement PV_TIME_FEATURES call
    
    This provides a mechanism for querying which paravirtualized time
    features are available in this hypervisor.
    
    Also add the header file which defines the ABI for the paravirtualized
    time features we're about to add.
    
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f656169db8c3..93b46d9526d0 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -478,6 +478,8 @@ void handle_exit_early(struct kvm_vcpu *vcpu, struct kvm_run *run,
 int kvm_perf_init(void);
 int kvm_perf_teardown(void);
 
+long kvm_hypercall_pv_features(struct kvm_vcpu *vcpu);
+
 void kvm_set_sei_esr(struct kvm_vcpu *vcpu, u64 syndrome);
 
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);

commit c726200dd106d4c58a281eea7159b8ba28a4ab34
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Fri Oct 11 13:07:05 2019 +0200

    KVM: arm/arm64: Allow reporting non-ISV data aborts to userspace
    
    For a long time, if a guest accessed memory outside of a memslot using
    any of the load/store instructions in the architecture which doesn't
    supply decoding information in the ESR_EL2 (the ISV bit is not set), the
    kernel would print the following message and terminate the VM as a
    result of returning -ENOSYS to userspace:
    
      load/store instruction decoding not implemented
    
    The reason behind this message is that KVM assumes that all accesses
    outside a memslot is an MMIO access which should be handled by
    userspace, and we originally expected to eventually implement some sort
    of decoding of load/store instructions where the ISV bit was not set.
    
    However, it turns out that many of the instructions which don't provide
    decoding information on abort are not safe to use for MMIO accesses, and
    the remaining few that would potentially make sense to use on MMIO
    accesses, such as those with register writeback, are not used in
    practice.  It also turns out that fetching an instruction from guest
    memory can be a pretty horrible affair, involving stopping all CPUs on
    SMP systems, handling multiple corner cases of address translation in
    software, and more.  It doesn't appear likely that we'll ever implement
    this in the kernel.
    
    What is much more common is that a user has misconfigured his/her guest
    and is actually not accessing an MMIO region, but just hitting some
    random hole in the IPA space.  In this scenario, the error message above
    is almost misleading and has led to a great deal of confusion over the
    years.
    
    It is, nevertheless, ABI to userspace, and we therefore need to
    introduce a new capability that userspace explicitly enables to change
    behavior.
    
    This patch introduces KVM_CAP_ARM_NISV_TO_USER (NISV meaning Non-ISV)
    which does exactly that, and introduces a new exit reason to report the
    event to userspace.  User space can then emulate an exception to the
    guest, restart the guest, suspend the guest, or take any other
    appropriate action as per the policy of the running system.
    
    Reported-by: Heinrich Schuchardt <xypron.glpk@gmx.de>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Alexander Graf <graf@amazon.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f656169db8c3..019bc560edc1 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -83,6 +83,14 @@ struct kvm_arch {
 
 	/* Mandated version of PSCI */
 	u32 psci_version;
+
+	/*
+	 * If we encounter a data abort without valid instruction syndrome
+	 * information, report this to user space.  User space can (and
+	 * should) opt in to this feature if KVM_CAP_ARM_NISV_TO_USER is
+	 * supported.
+	 */
+	bool return_nisv_io_abort_to_user;
 };
 
 #define KVM_NR_MEM_OBJS     40

commit f226650494c6aa87526d12135b7de8b8c074f3de
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Oct 2 10:06:12 2019 +0100

    arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    The GICv3 architecture specification is incredibly misleading when it
    comes to PMR and the requirement for a DSB. It turns out that this DSB
    is only required if the CPU interface sends an Upstream Control
    message to the redistributor in order to update the RD's view of PMR.
    
    This message is only sent when ICC_CTLR_EL1.PMHE is set, which isn't
    the case in Linux. It can still be set from EL3, so some special care
    is required. But the upshot is that in the (hopefuly large) majority
    of the cases, we can drop the DSB altogether.
    
    This relies on a new static key being set if the boot CPU has PMHE
    set. The drawback is that this static key has to be exported to
    modules.
    
    Cc: Will Deacon <will@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f656169db8c3..5ecb091c8576 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -600,8 +600,7 @@ static inline void kvm_arm_vhe_guest_enter(void)
 	 * local_daif_mask() already sets GIC_PRIO_PSR_I_SET, we just need a
 	 * dsb to ensure the redistributor is forwards EL2 IRQs to the CPU.
 	 */
-	if (system_uses_irq_prio_masking())
-		dsb(sy);
+	pmr_sync();
 }
 
 static inline void kvm_arm_vhe_guest_exit(void)

commit 39d7530d7494b4e47ba1856e741f513dafd17e3d
Merge: 16c97650a56a a45ff5994c9c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 15:35:14 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for chained PMU counters in guests
       - improved SError handling
       - handle Neoverse N1 erratum #1349291
       - allow side-channel mitigation status to be migrated
       - standardise most AArch64 system register accesses to msr_s/mrs_s
       - fix host MPIDR corruption on 32bit
       - selftests ckleanups
    
      x86:
       - PMU event {white,black}listing
       - ability for the guest to disable host-side interrupt polling
       - fixes for enlightened VMCS (Hyper-V pv nested virtualization),
       - new hypercall to yield to IPI target
       - support for passing cstate MSRs through to the guest
       - lots of cleanups and optimizations
    
      Generic:
       - Some txt->rST conversions for the documentation"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (128 commits)
      Documentation: virtual: Add toctree hooks
      Documentation: kvm: Convert cpuid.txt to .rst
      Documentation: virtual: Convert paravirt_ops.txt to .rst
      KVM: x86: Unconditionally enable irqs in guest context
      KVM: x86: PMU Event Filter
      kvm: x86: Fix -Wmissing-prototypes warnings
      KVM: Properly check if "page" is valid in kvm_vcpu_unmap
      KVM: arm/arm64: Initialise host's MPIDRs by reading the actual register
      KVM: LAPIC: Retry tune per-vCPU timer_advance_ns if adaptive tuning goes insane
      kvm: LAPIC: write down valid APIC registers
      KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
      KVM: doc: Add API documentation on the KVM_REG_ARM_WORKAROUNDS register
      KVM: arm/arm64: Add save/restore support for firmware workaround state
      arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
      KVM: arm/arm64: Support chained PMU counters
      KVM: arm/arm64: Remove pmc->bitmask
      KVM: arm/arm64: Re-create event when setting counter value
      KVM: arm/arm64: Extract duplicated code to own function
      KVM: arm/arm64: Rename kvm_pmu_{enable/disable}_counter functions
      KVM: LAPIC: ARBPRI is a reserved register for x2APIC
      ...

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit 1e0cf16cdad1ba53e9eeee8746fe57de42f20c97
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jul 5 23:35:56 2019 +0100

    KVM: arm/arm64: Initialise host's MPIDRs by reading the actual register
    
    As part of setting up the host context, we populate its
    MPIDR by using cpu_logical_map(). It turns out that contrary
    to arm64, cpu_logical_map() on 32bit ARM doesn't return the
    *full* MPIDR, but a truncated version.
    
    This leaves the host MPIDR slightly corrupted after the first
    run of a VM, since we won't correctly restore the MPIDR on
    exit. Oops.
    
    Since we cannot trust cpu_logical_map(), let's adopt a different
    strategy. We move the initialization of the host CPU context as
    part of the per-CPU initialization (which, in retrospect, makes
    a lot of sense), and directly read the MPIDR from the HW. This
    is guaranteed to work on both arm and arm64.
    
    Reported-by: Andre Przywara <Andre.Przywara@arm.com>
    Tested-by: Andre Przywara <Andre.Przywara@arm.com>
    Fixes: 32f139551954 ("arm/arm64: KVM: Statically configure the host's view of MPIDR")
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d9770daf3d7d..63a196c19fed 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -19,12 +19,12 @@
 #include <asm/arch_gicv3.h>
 #include <asm/barrier.h>
 #include <asm/cpufeature.h>
+#include <asm/cputype.h>
 #include <asm/daifflags.h>
 #include <asm/fpsimd.h>
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
-#include <asm/smp_plat.h>
 #include <asm/thread_info.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
@@ -484,11 +484,10 @@ struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
 DECLARE_PER_CPU(kvm_host_data_t, kvm_host_data);
 
-static inline void kvm_init_host_cpu_context(struct kvm_cpu_context *cpu_ctxt,
-					     int cpu)
+static inline void kvm_init_host_cpu_context(struct kvm_cpu_context *cpu_ctxt)
 {
 	/* The host's MPIDR is immutable, so let's set it up at boot time */
-	cpu_ctxt->sys_regs[MPIDR_EL1] = cpu_logical_map(cpu);
+	cpu_ctxt->sys_regs[MPIDR_EL1] = read_cpuid_mpidr();
 }
 
 void __kvm_enable_ssbs(void);

commit c118bbb52743df70e6297671606c1c08edc659fe
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri May 3 15:27:48 2019 +0100

    arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
    
    Recent commits added the explicit notion of "workaround not required" to
    the state of the Spectre v2 (aka. BP_HARDENING) workaround, where we
    just had "needed" and "unknown" before.
    
    Export this knowledge to the rest of the kernel and enhance the existing
    kvm_arm_harden_branch_predictor() to report this new state as well.
    Export this new state to guests when they use KVM's firmware interface
    emulation.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c328191aa202..d9770daf3d7d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -620,9 +620,21 @@ static inline void kvm_arm_vhe_guest_exit(void)
 	isb();
 }
 
-static inline bool kvm_arm_harden_branch_predictor(void)
+#define KVM_BP_HARDEN_UNKNOWN		-1
+#define KVM_BP_HARDEN_WA_NEEDED		0
+#define KVM_BP_HARDEN_NOT_REQUIRED	1
+
+static inline int kvm_arm_harden_branch_predictor(void)
 {
-	return cpus_have_const_cap(ARM64_HARDEN_BRANCH_PREDICTOR);
+	switch (get_spectre_v2_workaround_state()) {
+	case ARM64_BP_HARDEN_WA_NEEDED:
+		return KVM_BP_HARDEN_WA_NEEDED;
+	case ARM64_BP_HARDEN_NOT_REQUIRED:
+		return KVM_BP_HARDEN_NOT_REQUIRED;
+	case ARM64_BP_HARDEN_UNKNOWN:
+	default:
+		return KVM_BP_HARDEN_UNKNOWN;
+	}
 }
 
 #define KVM_SSBD_UNKNOWN		-1

commit bd82d4bd21880b7c4d5f5756be435095d6ae07b5
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:10 2019 +0100

    arm64: Fix incorrect irqflag restore for priority masking
    
    When using IRQ priority masking to disable interrupts, in order to deal
    with the PSR.I state, local_irq_save() would convert the I bit into a
    PMR value (GIC_PRIO_IRQOFF). This resulted in local_irq_restore()
    potentially modifying the value of PMR in undesired location due to the
    state of PSR.I upon flag saving [1].
    
    In an attempt to solve this issue in a less hackish manner, introduce
    a bit (GIC_PRIO_IGNORE_PMR) for the PMR values that can represent
    whether PSR.I is being used to disable interrupts, in which case it
    takes precedence of the status of interrupt masking via PMR.
    
    GIC_PRIO_PSR_I_SET is chosen such that (<pmr_value> |
    GIC_PRIO_PSR_I_SET) does not mask more interrupts than <pmr_value> as
    some sections (e.g. arch_cpu_idle(), interrupt acknowledge path)
    requires PMR not to mask interrupts that could be signaled to the
    CPU when using only PSR.I.
    
    [1] https://www.spinics.net/lists/arm-kernel/msg716956.html
    
    Fixes: 4a503217ce37 ("arm64: irqflags: Use ICC_PMR_EL1 for interrupt masking")
    Cc: <stable@vger.kernel.org> # 5.1.x-
    Reported-by: Zenghui Yu <yuzenghui@huawei.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Wei Li <liwei391@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Pouloze <suzuki.poulose@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4bcd9c1291d5..33410635b015 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -608,11 +608,12 @@ static inline void kvm_arm_vhe_guest_enter(void)
 	 * will not signal the CPU of interrupts of lower priority, and the
 	 * only way to get out will be via guest exceptions.
 	 * Naturally, we want to avoid this.
+	 *
+	 * local_daif_mask() already sets GIC_PRIO_PSR_I_SET, we just need a
+	 * dsb to ensure the redistributor is forwards EL2 IRQs to the CPU.
 	 */
-	if (system_uses_irq_prio_masking()) {
-		gic_write_pmr(GIC_PRIO_IRQON);
+	if (system_uses_irq_prio_masking())
 		dsb(sy);
-	}
 }
 
 static inline void kvm_arm_vhe_guest_exit(void)

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4bcd9c1291d5..c328191aa202 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012,2013 - ARM Ltd
  * Author: Marc Zyngier <marc.zyngier@arm.com>
@@ -5,18 +6,6 @@
  * Derived from arch/arm/include/asm/kvm_host.h:
  * Copyright (C) 2012 - Virtual Open Systems and Columbia University
  * Author: Christoffer Dall <c.dall@virtualopensystems.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #ifndef __ARM64_KVM_HOST_H__

commit b7c50fab66ab66e2d3e00f809a09578d78232836
Author: James Morse <james.morse@arm.com>
Date:   Wed May 22 18:47:04 2019 +0100

    KVM: arm64: Move pmu hyp code under hyp's Makefile to avoid instrumentation
    
    KVM's pmu.c contains the __hyp_text needed to switch the pmu registers
    between host and guest. Because this isn't covered by the 'hyp' Makefile,
    it can be built with kasan and friends when these are enabled in Kconfig.
    
    When starting a guest, this results in:
    | Kernel panic - not syncing: HYP panic:
    | PS:a00003c9 PC:000083000028ada0 ESR:86000007
    | FAR:000083000028ada0 HPFAR:0000000029df5300 PAR:0000000000000000
    | VCPU:000000004e10b7d6
    | CPU: 0 PID: 3088 Comm: qemu-system-aar Not tainted 5.2.0-rc1 #11026
    | Hardware name: ARM LTD ARM Juno Development Platform/ARM Juno Development Plat
    | Call trace:
    |  dump_backtrace+0x0/0x200
    |  show_stack+0x20/0x30
    |  dump_stack+0xec/0x158
    |  panic+0x1ec/0x420
    |  panic+0x0/0x420
    | SMP: stopping secondary CPUs
    | Kernel Offset: disabled
    | CPU features: 0x002,25006082
    | Memory Limit: none
    | ---[ end Kernel panic - not syncing: HYP panic:
    
    This is caused by functions in pmu.c calling the instrumented
    code, which isn't mapped to hyp. From objdump -r:
    | RELOCATION RECORDS FOR [.hyp.text]:
    | OFFSET           TYPE              VALUE
    | 0000000000000010 R_AARCH64_CALL26  __sanitizer_cov_trace_pc
    | 0000000000000018 R_AARCH64_CALL26  __asan_load4_noabort
    | 0000000000000024 R_AARCH64_CALL26  __asan_load4_noabort
    
    Move the affected code to a new file under 'hyp's Makefile.
    
    Fixes: 3d91befbb3a0 ("arm64: KVM: Enable !VHE support for :G/:H perf event modifiers")
    Cc: Andrew Murray <Andrew.Murray@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 2a8d3f8ca22c..4bcd9c1291d5 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -592,9 +592,6 @@ static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr);
 void kvm_clr_pmu_events(u32 clr);
 
-void __pmu_switch_to_host(struct kvm_cpu_context *host_ctxt);
-bool __pmu_switch_to_guest(struct kvm_cpu_context *host_ctxt);
-
 void kvm_vcpu_pmu_restore_guest(struct kvm_vcpu *vcpu);
 void kvm_vcpu_pmu_restore_host(struct kvm_vcpu *vcpu);
 #else

commit 435e53fb5e21ad1820c5c69f208304c0e5623d01
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 20:22:15 2019 +0100

    arm64: KVM: Enable VHE support for :G/:H perf event modifiers
    
    With VHE different exception levels are used between the host (EL2) and
    guest (EL1) with a shared exception level for userpace (EL0). We can take
    advantage of this and use the PMU's exception level filtering to avoid
    enabling/disabling counters in the world-switch code. Instead we just
    modify the counter type to include or exclude EL0 at vcpu_{load,put} time.
    
    We also ensure that trapped PMU system register writes do not re-enable
    EL0 when reconfiguring the backing perf events.
    
    This approach completely avoids blackout windows seen with !VHE.
    
    Suggested-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 645d74c705d6..2a8d3f8ca22c 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -580,7 +580,7 @@ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu);
 
 static inline bool kvm_pmu_counter_deferred(struct perf_event_attr *attr)
 {
-	return attr->exclude_host;
+	return (!has_vhe() && attr->exclude_host);
 }
 
 #ifdef CONFIG_KVM /* Avoid conflicts with core headers if CONFIG_KVM=n */
@@ -594,6 +594,9 @@ void kvm_clr_pmu_events(u32 clr);
 
 void __pmu_switch_to_host(struct kvm_cpu_context *host_ctxt);
 bool __pmu_switch_to_guest(struct kvm_cpu_context *host_ctxt);
+
+void kvm_vcpu_pmu_restore_guest(struct kvm_vcpu *vcpu);
+void kvm_vcpu_pmu_restore_host(struct kvm_vcpu *vcpu);
 #else
 static inline void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr) {}
 static inline void kvm_clr_pmu_events(u32 clr) {}

commit 3d91befbb3a0fcec6e1eebde45c8074b88cc9441
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 20:22:14 2019 +0100

    arm64: KVM: Enable !VHE support for :G/:H perf event modifiers
    
    Enable/disable event counters as appropriate when entering and exiting
    the guest to enable support for guest or host only event counting.
    
    For both VHE and non-VHE we switch the counters between host/guest at
    EL2.
    
    The PMU may be on when we change which counters are enabled however
    we avoid adding an isb as we instead rely on existing context
    synchronisation events: the eret to enter the guest (__guest_enter)
    and eret in kvm_call_hyp for __kvm_vcpu_run_nvhe on returning.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 655ad08edc3a..645d74c705d6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -591,6 +591,9 @@ static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 
 void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr);
 void kvm_clr_pmu_events(u32 clr);
+
+void __pmu_switch_to_host(struct kvm_cpu_context *host_ctxt);
+bool __pmu_switch_to_guest(struct kvm_cpu_context *host_ctxt);
 #else
 static inline void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr) {}
 static inline void kvm_clr_pmu_events(u32 clr) {}

commit eb41238cf19fda694e3a99c1f4f58bd88479a5ee
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 20:22:12 2019 +0100

    arm64: KVM: Add accessors to track guest/host only counters
    
    In order to effeciently switch events_{guest,host} perf counters at
    guest entry/exit we add bitfields to kvm_cpu_context for guest and host
    events as well as accessors for updating them.
    
    A function is also provided which allows the PMU driver to determine
    if a counter should start counting when it is enabled. With exclude_host,
    we may only start counting when entering the guest.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9ba59832b71a..655ad08edc3a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -233,8 +233,14 @@ struct kvm_cpu_context {
 	struct kvm_vcpu *__hyp_running_vcpu;
 };
 
+struct kvm_pmu_events {
+	u32 events_host;
+	u32 events_guest;
+};
+
 struct kvm_host_data {
 	struct kvm_cpu_context host_ctxt;
+	struct kvm_pmu_events pmu_events;
 };
 
 typedef struct kvm_host_data kvm_host_data_t;
@@ -572,11 +578,22 @@ void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu);
 
+static inline bool kvm_pmu_counter_deferred(struct perf_event_attr *attr)
+{
+	return attr->exclude_host;
+}
+
 #ifdef CONFIG_KVM /* Avoid conflicts with core headers if CONFIG_KVM=n */
 static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 {
 	return kvm_arch_vcpu_run_map_fp(vcpu);
 }
+
+void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr);
+void kvm_clr_pmu_events(u32 clr);
+#else
+static inline void kvm_set_pmu_events(u32 set, struct perf_event_attr *attr) {}
+static inline void kvm_clr_pmu_events(u32 clr) {}
 #endif
 
 static inline void kvm_arm_vhe_guest_enter(void)

commit 630a16854d2d28d13e96ff27ab43cc5caa4609fc
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 20:22:11 2019 +0100

    arm64: KVM: Encapsulate kvm_cpu_context in kvm_host_data
    
    The virt/arm core allocates a kvm_cpu_context_t percpu, at present this is
    a typedef to kvm_cpu_context and is used to store host cpu context. The
    kvm_cpu_context structure is also used elsewhere to hold vcpu context.
    In order to use the percpu to hold additional future host information we
    encapsulate kvm_cpu_context in a new structure and rename the typedef and
    percpu to match.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f772ac2fb3e9..9ba59832b71a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -233,7 +233,11 @@ struct kvm_cpu_context {
 	struct kvm_vcpu *__hyp_running_vcpu;
 };
 
-typedef struct kvm_cpu_context kvm_cpu_context_t;
+struct kvm_host_data {
+	struct kvm_cpu_context host_ctxt;
+};
+
+typedef struct kvm_host_data kvm_host_data_t;
 
 struct vcpu_reset_state {
 	unsigned long	pc;
@@ -278,7 +282,7 @@ struct kvm_vcpu_arch {
 	struct kvm_guest_debug_arch external_debug_state;
 
 	/* Pointer to host CPU context */
-	kvm_cpu_context_t *host_cpu_context;
+	struct kvm_cpu_context *host_cpu_context;
 
 	struct thread_info *host_thread_info;	/* hyp VA */
 	struct user_fpsimd_state *host_fpsimd_state;	/* hyp VA */
@@ -483,9 +487,9 @@ void kvm_set_sei_esr(struct kvm_vcpu *vcpu, u64 syndrome);
 
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
-DECLARE_PER_CPU(kvm_cpu_context_t, kvm_host_cpu_state);
+DECLARE_PER_CPU(kvm_host_data_t, kvm_host_data);
 
-static inline void kvm_init_host_cpu_context(kvm_cpu_context_t *cpu_ctxt,
+static inline void kvm_init_host_cpu_context(struct kvm_cpu_context *cpu_ctxt,
 					     int cpu)
 {
 	/* The host's MPIDR is immutable, so let's set it up at boot time */
@@ -503,8 +507,8 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	 * kernel's mapping to the linear mapping, and store it in tpidr_el2
 	 * so that we can use adr_l to access per-cpu variables in EL2.
 	 */
-	u64 tpidr_el2 = ((u64)this_cpu_ptr(&kvm_host_cpu_state) -
-			 (u64)kvm_ksym_ref(kvm_host_cpu_state));
+	u64 tpidr_el2 = ((u64)this_cpu_ptr(&kvm_host_data) -
+			 (u64)kvm_ksym_ref(kvm_host_data));
 
 	/*
 	 * Call initialization code, and switch to the full blown HYP code.

commit a22fa321d13b0264976cbbc1d22f4c27c41d3642
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Tue Apr 23 10:12:36 2019 +0530

    KVM: arm64: Add userspace flag to enable pointer authentication
    
    Now that the building blocks of pointer authentication are present, lets
    add userspace flags KVM_ARM_VCPU_PTRAUTH_ADDRESS and
    KVM_ARM_VCPU_PTRAUTH_GENERIC. These flags will enable pointer
    authentication for the KVM guest on a per-vcpu basis through the ioctl
    KVM_ARM_VCPU_INIT.
    
    This features will allow the KVM guest to allow the handling of
    pointer authentication instructions or to treat them as undefined
    if not set.
    
    Necessary documentations are added to reflect the changes done.
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7eebea7059c6..f772ac2fb3e9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -49,7 +49,7 @@
 
 #define KVM_MAX_VCPUS VGIC_V3_MAX_CPUS
 
-#define KVM_VCPU_MAX_FEATURES 5
+#define KVM_VCPU_MAX_FEATURES 7
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)

commit 384b40caa8afae44a54e8f69bd37097c0279fdce
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 23 10:12:35 2019 +0530

    KVM: arm/arm64: Context-switch ptrauth registers
    
    When pointer authentication is supported, a guest may wish to use it.
    This patch adds the necessary KVM infrastructure for this to work, with
    a semi-lazy context switch of the pointer auth state.
    
    Pointer authentication feature is only enabled when VHE is built
    in the kernel and present in the CPU implementation so only VHE code
    paths are modified.
    
    When we schedule a vcpu, we disable guest usage of pointer
    authentication instructions and accesses to the keys. While these are
    disabled, we avoid context-switching the keys. When we trap the guest
    trying to use pointer authentication functionality, we change to eagerly
    context-switching the keys, and enable the feature. The next time the
    vcpu is scheduled out/in, we start again. However the host key save is
    optimized and implemented inside ptrauth instruction/register access
    trap.
    
    Pointer authentication consists of address authentication and generic
    authentication, and CPUs in a system might have varied support for
    either. Where support for either feature is not uniform, it is hidden
    from guests via ID register emulation, as a result of the cpufeature
    framework in the host.
    
    Unfortunately, address authentication and generic authentication cannot
    be trapped separately, as the architecture provides a single EL2 trap
    covering both. If we wish to expose one without the other, we cannot
    prevent a (badly-written) guest from intermittently using a feature
    which is not uniformly supported (when scheduled on a physical CPU which
    supports the relevant feature). Hence, this patch expects both type of
    authentication to be present in a cpu.
    
    This switch of key is done from guest enter/exit assembly as preparation
    for the upcoming in-kernel pointer authentication support. Hence, these
    key switching routines are not implemented in C code as they may cause
    pointer authentication key signing error in some situations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    [Only VHE, key switch in full assembly, vcpu_has_ptrauth checks
    , save host key in ptrauth exception trap]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    [maz: various fixups]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7ccac42a91a6..7eebea7059c6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -161,6 +161,18 @@ enum vcpu_sysreg {
 	PMSWINC_EL0,	/* Software Increment Register */
 	PMUSERENR_EL0,	/* User Enable Register */
 
+	/* Pointer Authentication Registers in a strict increasing order. */
+	APIAKEYLO_EL1,
+	APIAKEYHI_EL1,
+	APIBKEYLO_EL1,
+	APIBKEYHI_EL1,
+	APDAKEYLO_EL1,
+	APDAKEYHI_EL1,
+	APDBKEYLO_EL1,
+	APDBKEYHI_EL1,
+	APGAKEYLO_EL1,
+	APGAKEYHI_EL1,
+
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */
 	IFSR32_EL2,	/* Instruction Fault Status Register */
@@ -530,6 +542,8 @@ static inline bool kvm_arch_requires_vhe(void)
 	return false;
 }
 
+void kvm_arm_vcpu_ptrauth_trap(struct kvm_vcpu *vcpu);
+
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}

commit b890d75c4cdc963c96e7774b088120966c23ab8e
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Tue Apr 23 10:12:34 2019 +0530

    KVM: arm64: Add a vcpu flag to control ptrauth for guest
    
    A per vcpu flag is added to check if pointer authentication is
    enabled for the vcpu or not. This flag may be enabled according to
    the necessary user policies and host capabilities.
    
    This patch also adds a helper to check the flag.
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7a096fdb333d..7ccac42a91a6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -355,10 +355,15 @@ struct kvm_vcpu_arch {
 #define KVM_ARM64_HOST_SVE_ENABLED	(1 << 4) /* SVE enabled for EL0 */
 #define KVM_ARM64_GUEST_HAS_SVE		(1 << 5) /* SVE exposed to guest */
 #define KVM_ARM64_VCPU_SVE_FINALIZED	(1 << 6) /* SVE config completed */
+#define KVM_ARM64_GUEST_HAS_PTRAUTH	(1 << 7) /* PTRAUTH exposed to guest */
 
 #define vcpu_has_sve(vcpu) (system_supports_sve() && \
 			    ((vcpu)->arch.flags & KVM_ARM64_GUEST_HAS_SVE))
 
+#define vcpu_has_ptrauth(vcpu)	((system_supports_address_auth() || \
+				  system_supports_generic_auth()) && \
+				 ((vcpu)->arch.flags & KVM_ARM64_GUEST_HAS_PTRAUTH))
+
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
 
 /*

commit 92e68b2b1ba004be55b2094fb8b5c11d0b24d11d
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Apr 10 17:17:37 2019 +0100

    KVM: arm/arm64: Clean up vcpu finalization function parameter naming
    
    Currently, the internal vcpu finalization functions use a different
    name ("what") for the feature parameter than the name ("feature")
    used in the documentation.
    
    To avoid future confusion, this patch converts everything to use
    the name "feature" consistently.
    
    No functional change.
    
    Suggested-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 6adf08ba9277..7a096fdb333d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -627,7 +627,7 @@ void kvm_arch_free_vm(struct kvm *kvm);
 
 int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type);
 
-int kvm_arm_vcpu_finalize(struct kvm_vcpu *vcpu, int what);
+int kvm_arm_vcpu_finalize(struct kvm_vcpu *vcpu, int feature);
 bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu);
 
 #define kvm_arm_vcpu_sve_finalized(vcpu) \

commit a3be836df7cb777fa8ecbfd662224bfe0394f771
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 12 15:30:58 2019 +0100

    KVM: arm/arm64: Demote kvm_arm_init_arch_resources() to just set up SVE
    
    The introduction of kvm_arm_init_arch_resources() looks like
    premature factoring, since nothing else uses this hook yet and it
    is not clear what will use it in the future.
    
    For now, let's not pretend that this is a general thing:
    
    This patch simply renames the function to kvm_arm_init_sve(),
    retaining the arm stub version under the new name.
    
    Suggested-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9d57cf8be879..6adf08ba9277 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -59,7 +59,7 @@
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
 extern unsigned int kvm_sve_max_vl;
-int kvm_arm_init_arch_resources(void);
+int kvm_arm_init_sve(void);
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit 9a3cdf26e3363ec5460ebe20c508114fa63bcf26
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Feb 28 18:56:50 2019 +0000

    KVM: arm64/sve: Allow userspace to enable SVE for vcpus
    
    Now that all the pieces are in place, this patch offers a new flag
    KVM_ARM_VCPU_SVE that userspace can pass to KVM_ARM_VCPU_INIT to
    turn on SVE for the guest, on a per-vcpu basis.
    
    As part of this, support for initialisation and reset of the SVE
    vector length set and registers is added in the appropriate places,
    as well as finally setting the KVM_ARM64_GUEST_HAS_SVE vcpu flag,
    to turn on the SVE support code.
    
    Allocation of the SVE register storage in vcpu->arch.sve_state is
    deferred until the SVE configuration is finalized, by which time
    the size of the registers is known.
    
    Setting the vector lengths supported by the vcpu is considered
    configuration of the emulated hardware rather than runtime
    configuration, so no support is offered for changing the vector
    lengths available to an existing vcpu across reset.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 5475cc4df35c..9d57cf8be879 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -49,8 +49,7 @@
 
 #define KVM_MAX_VCPUS VGIC_V3_MAX_CPUS
 
-/* Will be incremented when KVM_ARM_VCPU_SVE is fully implemented: */
-#define KVM_VCPU_MAX_FEATURES 4
+#define KVM_VCPU_MAX_FEATURES 5
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)

commit 9033bba4b53527b57bec217509a967a25cb19357
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Feb 28 18:46:44 2019 +0000

    KVM: arm64/sve: Add pseudo-register for the guest's vector lengths
    
    This patch adds a new pseudo-register KVM_REG_ARM64_SVE_VLS to
    allow userspace to set and query the set of vector lengths visible
    to the guest.
    
    In the future, multiple register slices per SVE register may be
    visible through the ioctl interface.  Once the set of slices has
    been determined we would not be able to allow the vector length set
    to be changed any more, in order to avoid userspace seeing
    inconsistent sets of registers.  For this reason, this patch adds
    support for explicit finalization of the SVE configuration via the
    KVM_ARM_VCPU_FINALIZE ioctl.
    
    Finalization is the proper place to allocate the SVE register state
    storage in vcpu->arch.sve_state, so this patch adds that as
    appropriate.  The data is freed via kvm_arch_vcpu_uninit(), which
    was previously a no-op on arm64.
    
    To simplify the logic for determining what vector lengths can be
    supported, some code is added to KVM init to work this out, in the
    kvm_arm_init_arch_resources() hook.
    
    The KVM_REG_ARM64_SVE_VLS pseudo-register is not exposed yet.
    Subsequent patches will allow SVE to be turned on for guest vcpus,
    making it visible.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 98658f7dad68..5475cc4df35c 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -23,7 +23,6 @@
 #define __ARM64_KVM_HOST_H__
 
 #include <linux/bitmap.h>
-#include <linux/errno.h>
 #include <linux/types.h>
 #include <linux/jump_label.h>
 #include <linux/kvm_types.h>
@@ -50,6 +49,7 @@
 
 #define KVM_MAX_VCPUS VGIC_V3_MAX_CPUS
 
+/* Will be incremented when KVM_ARM_VCPU_SVE is fully implemented: */
 #define KVM_VCPU_MAX_FEATURES 4
 
 #define KVM_REQ_SLEEP \
@@ -59,10 +59,12 @@
 
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
-static inline int kvm_arm_init_arch_resources(void) { return 0; }
+extern unsigned int kvm_sve_max_vl;
+int kvm_arm_init_arch_resources(void);
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu);
 int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
@@ -353,6 +355,7 @@ struct kvm_vcpu_arch {
 #define KVM_ARM64_HOST_SVE_IN_USE	(1 << 3) /* backup for host TIF_SVE */
 #define KVM_ARM64_HOST_SVE_ENABLED	(1 << 4) /* SVE enabled for EL0 */
 #define KVM_ARM64_GUEST_HAS_SVE		(1 << 5) /* SVE exposed to guest */
+#define KVM_ARM64_VCPU_SVE_FINALIZED	(1 << 6) /* SVE config completed */
 
 #define vcpu_has_sve(vcpu) (system_supports_sve() && \
 			    ((vcpu)->arch.flags & KVM_ARM64_GUEST_HAS_SVE))
@@ -525,7 +528,6 @@ static inline bool kvm_arch_requires_vhe(void)
 
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
-static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 
@@ -626,7 +628,10 @@ void kvm_arch_free_vm(struct kvm *kvm);
 
 int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type);
 
-#define kvm_arm_vcpu_finalize(vcpu, what) (-EINVAL)
-#define kvm_arm_vcpu_is_finalized(vcpu) true
+int kvm_arm_vcpu_finalize(struct kvm_vcpu *vcpu, int what);
+bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu);
+
+#define kvm_arm_vcpu_sve_finalized(vcpu) \
+	((vcpu)->arch.flags & KVM_ARM64_VCPU_SVE_FINALIZED)
 
 #endif /* __ARM64_KVM_HOST_H__ */

commit 7dd32a0d0103a5941efbb971f85a3e930cc5665e
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Dec 19 14:27:01 2018 +0000

    KVM: arm/arm64: Add KVM_ARM_VCPU_FINALIZE ioctl
    
    Some aspects of vcpu configuration may be too complex to be
    completed inside KVM_ARM_VCPU_INIT.  Thus, there may be a
    requirement for userspace to do some additional configuration
    before various other ioctls will work in a consistent way.
    
    In particular this will be the case for SVE, where userspace will
    need to negotiate the set of vector lengths to be made available to
    the guest before the vcpu becomes fully usable.
    
    In order to provide an explicit way for userspace to confirm that
    it has finished setting up a particular vcpu feature, this patch
    adds a new ioctl KVM_ARM_VCPU_FINALIZE.
    
    When userspace has opted into a feature that requires finalization,
    typically by means of a feature flag passed to KVM_ARM_VCPU_INIT, a
    matching call to KVM_ARM_VCPU_FINALIZE is now required before
    KVM_RUN or KVM_GET_REG_LIST is allowed.  Individual features may
    impose additional restrictions where appropriate.
    
    No existing vcpu features are affected by this, so current
    userspace implementations will continue to work exactly as before,
    with no need to issue KVM_ARM_VCPU_FINALIZE.
    
    As implemented in this patch, KVM_ARM_VCPU_FINALIZE is currently a
    placeholder: no finalizable features exist yet, so ioctl is not
    required and will always yield EINVAL.  Subsequent patches will add
    the finalization logic to make use of this ioctl for SVE.
    
    No functional change for existing userspace.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 3e8950942591..98658f7dad68 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -23,6 +23,7 @@
 #define __ARM64_KVM_HOST_H__
 
 #include <linux/bitmap.h>
+#include <linux/errno.h>
 #include <linux/types.h>
 #include <linux/jump_label.h>
 #include <linux/kvm_types.h>
@@ -625,4 +626,7 @@ void kvm_arch_free_vm(struct kvm *kvm);
 
 int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type);
 
+#define kvm_arm_vcpu_finalize(vcpu, what) (-EINVAL)
+#define kvm_arm_vcpu_is_finalized(vcpu) true
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 0f062bfe36b63cd24f16afe2822d0df7c27904d9
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Feb 28 18:33:00 2019 +0000

    KVM: arm/arm64: Add hook for arch-specific KVM initialisation
    
    This patch adds a kvm_arm_init_arch_resources() hook to perform
    subarch-specific initialisation when starting up KVM.
    
    This will be used in a subsequent patch for global SVE-related
    setup on arm64.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 205438a108f6..3e8950942591 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -58,6 +58,8 @@
 
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
+static inline int kvm_arm_init_arch_resources(void) { return 0; }
+
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext);

commit e1c9c98345b356ad2890ac7e9223593cae8b4dba
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:19 2018 +0100

    KVM: arm64/sve: Add SVE support to register access ioctl interface
    
    This patch adds the following registers for access via the
    KVM_{GET,SET}_ONE_REG interface:
    
     * KVM_REG_ARM64_SVE_ZREG(n, i) (n = 0..31) (in 2048-bit slices)
     * KVM_REG_ARM64_SVE_PREG(n, i) (n = 0..15) (in 256-bit slices)
     * KVM_REG_ARM64_SVE_FFR(i) (in 256-bit slices)
    
    In order to adapt gracefully to future architectural extensions,
    the registers are logically divided up into slices as noted above:
    the i parameter denotes the slice index.
    
    This allows us to reserve space in the ABI for future expansion of
    these registers.  However, as of today the architecture does not
    permit registers to be larger than a single slice, so no code is
    needed in the kernel to expose additional slices, for now.  The
    code can be extended later as needed to expose them up to a maximum
    of 32 slices (as carved out in the architecture itself) if they
    really exist someday.
    
    The registers are only visible for vcpus that have SVE enabled.
    They are not enumerated by KVM_GET_REG_LIST on vcpus that do not
    have SVE.
    
    Accesses to the FPSIMD registers via KVM_REG_ARM_CORE is not
    allowed for SVE-enabled vcpus: SVE-aware userspace can use the
    KVM_REG_ARM64_SVE_ZREG() interface instead to access the same
    register state.  This avoids some complex and pointless emulation
    in the kernel to convert between the two views of these aliased
    registers.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4fabfd250de8..205438a108f6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -329,6 +329,20 @@ struct kvm_vcpu_arch {
 #define vcpu_sve_pffr(vcpu) ((void *)((char *)((vcpu)->arch.sve_state) + \
 				      sve_ffr_offset((vcpu)->arch.sve_max_vl)))
 
+#define vcpu_sve_state_size(vcpu) ({					\
+	size_t __size_ret;						\
+	unsigned int __vcpu_vq;						\
+									\
+	if (WARN_ON(!sve_vl_valid((vcpu)->arch.sve_max_vl))) {		\
+		__size_ret = 0;						\
+	} else {							\
+		__vcpu_vq = sve_vq_from_vl((vcpu)->arch.sve_max_vl);	\
+		__size_ret = SVE_SIG_REGS_SIZE(__vcpu_vq);		\
+	}								\
+									\
+	__size_ret;							\
+})
+
 /* vcpu_arch flags field values: */
 #define KVM_ARM64_DEBUG_DIRTY		(1 << 0)
 #define KVM_ARM64_FP_ENABLED		(1 << 1) /* guest FP regs loaded */

commit b43b5dd990eb28047dafe639ce44db347496cb56
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:17 2018 +0100

    KVM: arm64/sve: Context switch the SVE registers
    
    In order to give each vcpu its own view of the SVE registers, this
    patch adds context storage via a new sve_state pointer in struct
    vcpu_arch.  An additional member sve_max_vl is also added for each
    vcpu, to determine the maximum vector length visible to the guest
    and thus the value to be configured in ZCR_EL2.LEN while the vcpu
    is active.  This also determines the layout and size of the storage
    in sve_state, which is read and written by the same backend
    functions that are used for context-switching the SVE state for
    host tasks.
    
    On SVE-enabled vcpus, SVE access traps are now handled by switching
    in the vcpu's SVE context and disabling the trap before returning
    to the guest.  On other vcpus, the trap is not handled and an exit
    back to the host occurs, where the handle_sve() fallback path
    reflects an undefined instruction exception back to the guest,
    consistently with the behaviour of non-SVE-capable hardware (as was
    done unconditionally prior to this patch).
    
    No SVE handling is added on non-VHE-only paths, since VHE is an
    architectural and Kconfig prerequisite of SVE.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 22cf484b561f..4fabfd250de8 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -228,6 +228,8 @@ struct vcpu_reset_state {
 
 struct kvm_vcpu_arch {
 	struct kvm_cpu_context ctxt;
+	void *sve_state;
+	unsigned int sve_max_vl;
 
 	/* HYP configuration */
 	u64 hcr_el2;
@@ -323,6 +325,10 @@ struct kvm_vcpu_arch {
 	bool sysregs_loaded_on_cpu;
 };
 
+/* Pointer to the vcpu's SVE FFR for sve_{save,load}_state() */
+#define vcpu_sve_pffr(vcpu) ((void *)((char *)((vcpu)->arch.sve_state) + \
+				      sve_ffr_offset((vcpu)->arch.sve_max_vl)))
+
 /* vcpu_arch flags field values: */
 #define KVM_ARM64_DEBUG_DIRTY		(1 << 0)
 #define KVM_ARM64_FP_ENABLED		(1 << 1) /* guest FP regs loaded */

commit 73433762fcaeb9d59e84d299021c6b15466c96dd
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:16 2018 +0100

    KVM: arm64/sve: System register context switch and access support
    
    This patch adds the necessary support for context switching ZCR_EL1
    for each vcpu.
    
    ZCR_EL1 is trapped alongside the FPSIMD/SVE registers, so it makes
    sense for it to be handled as part of the guest FPSIMD/SVE context
    for context switch purposes instead of handling it as a general
    system register.  This means that it can be switched in lazily at
    the appropriate time.  No effort is made to track host context for
    this register, since SVE requires VHE: thus the hosts's value for
    this register lives permanently in ZCR_EL2 and does not alias the
    guest's value at any time.
    
    The Hyp switch and fpsimd context handling code is extended
    appropriately.
    
    Accessors are added in sys_regs.c to expose the SVE system
    registers and ID register fields.  Because these need to be
    conditionally visible based on the guest configuration, they are
    implemented separately for now rather than by use of the generic
    system register helpers.  This may be abstracted better later on
    when/if there are more features requiring this model.
    
    ID_AA64ZFR0_EL1 is RO-RAZ for MRS/MSR when SVE is disabled for the
    guest, but for compatibility with non-SVE aware KVM implementations
    the register should not be enumerated at all for KVM_GET_REG_LIST
    in this case.  For consistency we also reject ioctl access to the
    register.  This ensures that a non-SVE-enabled guest looks the same
    to userspace, irrespective of whether the kernel KVM implementation
    supports SVE.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ad4f7f004498..22cf484b561f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -121,6 +121,7 @@ enum vcpu_sysreg {
 	SCTLR_EL1,	/* System Control Register */
 	ACTLR_EL1,	/* Auxiliary Control Register */
 	CPACR_EL1,	/* Coprocessor Access Control */
+	ZCR_EL1,	/* SVE Control */
 	TTBR0_EL1,	/* Translation Table Base Register 0 */
 	TTBR1_EL1,	/* Translation Table Base Register 1 */
 	TCR_EL1,	/* Translation Control Register */

commit 1765edbab16e3dc73367bda04e45337cea3e51a0
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:12 2018 +0100

    KVM: arm64: Add a vcpu flag to control SVE visibility for the guest
    
    Since SVE will be enabled or disabled on a per-vcpu basis, a flag
    is needed in order to track which vcpus have it enabled.
    
    This patch adds a suitable flag and a helper for checking it.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 6d10100ff870..ad4f7f004498 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -328,6 +328,10 @@ struct kvm_vcpu_arch {
 #define KVM_ARM64_FP_HOST		(1 << 2) /* host FP regs loaded */
 #define KVM_ARM64_HOST_SVE_IN_USE	(1 << 3) /* backup for host TIF_SVE */
 #define KVM_ARM64_HOST_SVE_ENABLED	(1 << 4) /* SVE enabled for EL0 */
+#define KVM_ARM64_GUEST_HAS_SVE		(1 << 5) /* SVE exposed to guest */
+
+#define vcpu_has_sve(vcpu) (system_supports_sve() && \
+			    ((vcpu)->arch.flags & KVM_ARM64_GUEST_HAS_SVE))
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
 

commit 3f61f40947e88b069ac1103727c81582d6e91dea
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:08 2018 +0100

    KVM: arm64: Add missing #includes to kvm_host.h
    
    kvm_host.h uses some declarations from other headers that are
    currently included by accident, without an explicit #include.
    
    This patch adds a few #includes that are clearly missing.  Although
    the header builds without them today, this should help to avoid
    future surprises.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a01fe087e022..6d10100ff870 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -22,9 +22,13 @@
 #ifndef __ARM64_KVM_HOST_H__
 #define __ARM64_KVM_HOST_H__
 
+#include <linux/bitmap.h>
 #include <linux/types.h>
+#include <linux/jump_label.h>
 #include <linux/kvm_types.h>
+#include <linux/percpu.h>
 #include <asm/arch_gicv3.h>
+#include <asm/barrier.h>
 #include <asm/cpufeature.h>
 #include <asm/daifflags.h>
 #include <asm/fpsimd.h>

commit 636deed6c0bc137a7c4f4a97ae1fcf0ad75323da
Merge: aa2e3ac64ace 4a605bc08e98
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 15 15:00:28 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - some cleanups
       - direct physical timer assignment
       - cache sanitization for 32-bit guests
    
      s390:
       - interrupt cleanup
       - introduction of the Guest Information Block
       - preparation for processor subfunctions in cpu models
    
      PPC:
       - bug fixes and improvements, especially related to machine checks
         and protection keys
    
      x86:
       - many, many cleanups, including removing a bunch of MMU code for
         unnecessary optimizations
       - AVIC fixes
    
      Generic:
       - memcg accounting"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (147 commits)
      kvm: vmx: fix formatting of a comment
      KVM: doc: Document the life cycle of a VM and its resources
      MAINTAINERS: Add KVM selftests to existing KVM entry
      Revert "KVM/MMU: Flush tlb directly in the kvm_zap_gfn_range()"
      KVM: PPC: Book3S: Add count cache flush parameters to kvmppc_get_cpu_char()
      KVM: PPC: Fix compilation when KVM is not enabled
      KVM: Minor cleanups for kvm_main.c
      KVM: s390: add debug logging for cpu model subfunctions
      KVM: s390: implement subfunction processor calls
      arm64: KVM: Fix architecturally invalid reset value for FPEXC32_EL2
      KVM: arm/arm64: Remove unused timer variable
      KVM: PPC: Book3S: Improve KVM reference counting
      KVM: PPC: Book3S HV: Fix build failure without IOMMU support
      Revert "KVM: Eliminate extra function calls in kvm_get_dirty_log_protect()"
      x86: kvmguest: use TSC clocksource if invariant TSC is exposed
      KVM: Never start grow vCPU halt_poll_ns from value below halt_poll_ns_grow_start
      KVM: Expose the initial start value in grow_halt_poll_ns() as a module parameter
      KVM: grow_halt_poll_ns() should never shrink vCPU halt_poll_ns
      KVM: x86/mmu: Consolidate kvm_mmu_zap_all() and kvm_mmu_zap_mmio_sptes()
      KVM: x86/mmu: WARN if zapping a MMIO spte results in zapping children
      ...

commit 3d8dfe75ef69f4dd4ba35c09b20a5aa58b4a5078
Merge: d60752629693 b855b58ac1b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 10:17:23 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Pseudo NMI support for arm64 using GICv3 interrupt priorities
    
     - uaccess macros clean-up (unsafe user accessors also merged but
       reverted, waiting for objtool support on arm64)
    
     - ptrace regsets for Pointer Authentication (ARMv8.3) key management
    
     - inX() ordering w.r.t. delay() on arm64 and riscv (acks in place by
       the riscv maintainers)
    
     - arm64/perf updates: PMU bindings converted to json-schema, unused
       variable and misleading comment removed
    
     - arm64/debug fixes to ensure checking of the triggering exception
       level and to avoid the propagation of the UNKNOWN FAR value into the
       si_code for debug signals
    
     - Workaround for Fujitsu A64FX erratum 010001
    
     - lib/raid6 ARM NEON optimisations
    
     - NR_CPUS now defaults to 256 on arm64
    
     - Minor clean-ups (documentation/comments, Kconfig warning, unused
       asm-offsets, clang warnings)
    
     - MAINTAINERS update for list information to the ARM64 ACPI entry
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      arm64: mmu: drop paging_init comments
      arm64: debug: Ensure debug handlers check triggering exception level
      arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
      Revert "arm64: uaccess: Implement unsafe accessors"
      arm64: avoid clang warning about self-assignment
      arm64: Kconfig.platforms: fix warning unmet direct dependencies
      lib/raid6: arm: optimize away a mask operation in NEON recovery routine
      lib/raid6: use vdupq_n_u8 to avoid endianness warnings
      arm64: io: Hook up __io_par() for inX() ordering
      riscv: io: Update __io_[p]ar() macros to take an argument
      asm-generic/io: Pass result of I/O accessor to __io_[p]ar()
      arm64: Add workaround for Fujitsu A64FX erratum 010001
      arm64: Rename get_thread_info()
      arm64: Remove documentation about TIF_USEDFPU
      arm64: irqflags: Fix clang build warnings
      arm64: Enable the support of pseudo-NMIs
      arm64: Skip irqflags tracing for NMI in IRQs disabled context
      arm64: Skip preemption when exiting an NMI
      arm64: Handle serror in NMI context
      irqchip/gic-v3: Allow interrupts to be set as pseudo-NMI
      ...

commit e329fb75d519e3dc3eb11b22d5bb846516be3521
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Tue Dec 11 15:26:31 2018 +0100

    KVM: arm/arm64: Factor out VMID into struct kvm_vmid
    
    In preparation for nested virtualization where we are going to have more
    than a single VMID per VM, let's factor out the VMID data into a
    separate VMID data structure and change the VMID allocator to operate on
    this new structure instead of using a struct kvm.
    
    This also means that udate_vttbr now becomes update_vmid, and that the
    vttbr itself is generated on the fly based on the stage 2 page table
    base address and the vmid.
    
    We cache the physical address of the pgd when allocating the pgd to
    avoid doing the calculation on every entry to the guest and to avoid
    calling into potentially non-hyp-mapped code from hyp/EL2.
    
    If we wanted to merge the VMID allocator with the arm64 ASID allocator
    at some point in the future, it should actually become easier to do that
    after this patch.
    
    Note that to avoid mapping the kvm_vmid_bits variable into hyp, we
    simply forego the masking of the vmid value in kvm_get_vttbr and rely on
    update_vmid to always assign a valid vmid value (within the supported
    range).
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    [maz: minor cleanups]
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f497bb31031f..444dd1cb1958 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -57,16 +57,19 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
-struct kvm_arch {
+struct kvm_vmid {
 	/* The VMID generation used for the virt. memory system */
 	u64    vmid_gen;
 	u32    vmid;
+};
+
+struct kvm_arch {
+	struct kvm_vmid vmid;
 
 	/* stage2 entry level table */
 	pgd_t *pgd;
+	phys_addr_t pgd_phys;
 
-	/* VTTBR value associated with above pgd and vmid */
-	u64    vttbr;
 	/* VTCR_EL2 value for this VM */
 	u64    vtcr;
 

commit 32f139551954512bfdf9d558341af453bb8b12b4
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sat Jan 19 15:29:54 2019 +0000

    arm/arm64: KVM: Statically configure the host's view of MPIDR
    
    We currently eagerly save/restore MPIDR. It turns out to be
    slightly pointless:
    - On the host, this value is known as soon as we're scheduled on a
      physical CPU
    - In the guest, this value cannot change, as it is set by KVM
      (and this is a read-only register)
    
    The result of the above is that we can perfectly avoid the eager
    saving of MPIDR_EL1, and only keep the restore. We just have
    to setup the host contexts appropriately at boot time.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 8b7702bdb219..f497bb31031f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -30,6 +30,7 @@
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
+#include <asm/smp_plat.h>
 #include <asm/thread_info.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
@@ -418,6 +419,13 @@ struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
 DECLARE_PER_CPU(kvm_cpu_context_t, kvm_host_cpu_state);
 
+static inline void kvm_init_host_cpu_context(kvm_cpu_context_t *cpu_ctxt,
+					     int cpu)
+{
+	/* The host's MPIDR is immutable, so let's set it up at boot time */
+	cpu_ctxt->sys_regs[MPIDR_EL1] = cpu_logical_map(cpu);
+}
+
 void __kvm_enable_ssbs(void);
 
 static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,

commit 18fc7bf8e041272f74a3237b99261d59c3ff0388
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sat Jan 5 15:57:56 2019 +0000

    arm64: KVM: Allow for direct call of HYP functions when using VHE
    
    When running VHE, there is no need to jump via some stub to perform
    a "HYP" function call, as there is a single address space.
    
    Let's thus change kvm_call_hyp() and co to perform a direct call
    in this case. Although this results in a bit of code expansion,
    it allows the compiler to check for type compatibility, something
    that we are missing so far.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e54cb7c88a4e..8b7702bdb219 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -370,8 +370,36 @@ void kvm_arm_halt_guest(struct kvm *kvm);
 void kvm_arm_resume_guest(struct kvm *kvm);
 
 u64 __kvm_call_hyp(void *hypfn, ...);
-#define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)
-#define kvm_call_hyp_ret(f, ...) kvm_call_hyp(f, ##__VA_ARGS__)
+
+/*
+ * The couple of isb() below are there to guarantee the same behaviour
+ * on VHE as on !VHE, where the eret to EL1 acts as a context
+ * synchronization event.
+ */
+#define kvm_call_hyp(f, ...)						\
+	do {								\
+		if (has_vhe()) {					\
+			f(__VA_ARGS__);					\
+			isb();						\
+		} else {						\
+			__kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__); \
+		}							\
+	} while(0)
+
+#define kvm_call_hyp_ret(f, ...)					\
+	({								\
+		typeof(f(__VA_ARGS__)) ret;				\
+									\
+		if (has_vhe()) {					\
+			ret = f(__VA_ARGS__);				\
+			isb();						\
+		} else {						\
+			ret = __kvm_call_hyp(kvm_ksym_ref(f),		\
+					     ##__VA_ARGS__);		\
+		}							\
+									\
+		ret;							\
+	})
 
 void force_vm_exit(const cpumask_t *mask);
 void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);

commit 7aa8d14641651a61a0b8892314a0bcfaceebe158
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sat Jan 5 15:49:50 2019 +0000

    arm/arm64: KVM: Introduce kvm_call_hyp_ret()
    
    Until now, we haven't differentiated between HYP calls that
    have a return value and those who don't. As we're about to
    change this, introduce kvm_call_hyp_ret(), and change all
    call sites that actually make use of a return value.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7732d0ba4e60..e54cb7c88a4e 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -371,6 +371,7 @@ void kvm_arm_resume_guest(struct kvm *kvm);
 
 u64 __kvm_call_hyp(void *hypfn, ...);
 #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)
+#define kvm_call_hyp_ret(f, ...) kvm_call_hyp(f, ##__VA_ARGS__)
 
 void force_vm_exit(const cpumask_t *mask);
 void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);

commit 358b28f09f0ab074d781df72b8a671edb1547789
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 20 11:36:07 2018 +0000

    arm/arm64: KVM: Allow a VCPU to fully reset itself
    
    The current kvm_psci_vcpu_on implementation will directly try to
    manipulate the state of the VCPU to reset it.  However, since this is
    not done on the thread that runs the VCPU, we can end up in a strangely
    corrupted state when the source and target VCPUs are running at the same
    time.
    
    Fix this by factoring out all reset logic from the PSCI implementation
    and forwarding the required information along with a request to the
    target VCPU.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7732d0ba4e60..da3fc7324d68 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -48,6 +48,7 @@
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
+#define KVM_REQ_VCPU_RESET	KVM_ARCH_REQ(2)
 
 DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
@@ -208,6 +209,13 @@ struct kvm_cpu_context {
 
 typedef struct kvm_cpu_context kvm_cpu_context_t;
 
+struct vcpu_reset_state {
+	unsigned long	pc;
+	unsigned long	r0;
+	bool		be;
+	bool		reset;
+};
+
 struct kvm_vcpu_arch {
 	struct kvm_cpu_context ctxt;
 
@@ -297,6 +305,9 @@ struct kvm_vcpu_arch {
 	/* Virtual SError ESR to restore when HCR_EL2.VSE is set */
 	u64 vsesr_el2;
 
+	/* Additional reset state */
+	struct vcpu_reset_state	reset_state;
+
 	/* True when deferrable sysregs are loaded on the physical CPU,
 	 * see kvm_vcpu_load_sysregs and kvm_vcpu_put_sysregs. */
 	bool sysregs_loaded_on_cpu;

commit 85738e05dc38a80921e1e1944e5b835f6668fc30
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:48 2019 +0000

    arm64: kvm: Unmask PMR before entering guest
    
    Interrupts masked by ICC_PMR_EL1 will not be signaled to the CPU. This
    means that hypervisor will not receive masked interrupts while running a
    guest.
    
    We need to make sure that all maskable interrupts are masked from the
    time we call local_irq_disable() in the main run loop, and remain so
    until we call local_irq_enable() after returning from the guest, and we
    need to ensure that we see no interrupts at all (including pseudo-NMIs)
    in the middle of the VM world-switch, while at the same time we need to
    ensure we exit the guest when there are interrupts for the host.
    
    We can accomplish this with pseudo-NMIs enabled by:
      (1) local_irq_disable: set the priority mask
      (2) enter guest: set PSTATE.I
      (3)              clear the priority mask
      (4) eret to guest
      (5) exit guest:  set the priotiy mask
                       clear PSTATE.I (and restore other host PSTATE bits)
      (6) local_irq_enable: clear the priority mask.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7732d0ba4e60..292c88263fd1 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -24,6 +24,7 @@
 
 #include <linux/types.h>
 #include <linux/kvm_types.h>
+#include <asm/arch_gicv3.h>
 #include <asm/cpufeature.h>
 #include <asm/daifflags.h>
 #include <asm/fpsimd.h>
@@ -474,10 +475,25 @@ static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 static inline void kvm_arm_vhe_guest_enter(void)
 {
 	local_daif_mask();
+
+	/*
+	 * Having IRQs masked via PMR when entering the guest means the GIC
+	 * will not signal the CPU of interrupts of lower priority, and the
+	 * only way to get out will be via guest exceptions.
+	 * Naturally, we want to avoid this.
+	 */
+	if (system_uses_irq_prio_masking()) {
+		gic_write_pmr(GIC_PRIO_IRQON);
+		dsb(sy);
+	}
 }
 
 static inline void kvm_arm_vhe_guest_exit(void)
 {
+	/*
+	 * local_daif_restore() takes care to properly restore PSTATE.DAIF
+	 * and the GIC PMR if the host is using IRQ priorities.
+	 */
 	local_daif_restore(DAIF_PROCCTX_NOIRQ);
 
 	/*

commit 42b00f122cfbfed79fc29b0b3610f3abbb1e3864
Merge: 460023a5d1d2 a0aea130afeb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 11:46:28 2018 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - selftests improvements
       - large PUD support for HugeTLB
       - single-stepping fixes
       - improved tracing
       - various timer and vGIC fixes
    
      x86:
       - Processor Tracing virtualization
       - STIBP support
       - some correctness fixes
       - refactorings and splitting of vmx.c
       - use the Hyper-V range TLB flush hypercall
       - reduce order of vcpu struct
       - WBNOINVD support
       - do not use -ftrace for __noclone functions
       - nested guest support for PAUSE filtering on AMD
       - more Hyper-V enlightenments (direct mode for synthetic timers)
    
      PPC:
       -  nested VFIO
    
      s390:
       - bugfixes only this time"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (171 commits)
      KVM: x86: Add CPUID support for new instruction WBNOINVD
      kvm: selftests: ucall: fix exit mmio address guessing
      Revert "compiler-gcc: disable -ftracer for __noclone functions"
      KVM: VMX: Move VM-Enter + VM-Exit handling to non-inline sub-routines
      KVM: VMX: Explicitly reference RCX as the vmx_vcpu pointer in asm blobs
      KVM: x86: Use jmp to invoke kvm_spurious_fault() from .fixup
      MAINTAINERS: Add arch/x86/kvm sub-directories to existing KVM/x86 entry
      KVM/x86: Use SVM assembly instruction mnemonics instead of .byte streams
      KVM/MMU: Flush tlb directly in the kvm_zap_gfn_range()
      KVM/MMU: Flush tlb directly in kvm_set_pte_rmapp()
      KVM/MMU: Move tlb flush in kvm_set_pte_rmapp() to kvm_mmu_notifier_change_pte()
      KVM: Make kvm_set_spte_hva() return int
      KVM: Replace old tlb flush function with new one to flush a specified range.
      KVM/MMU: Add tlb flush with range helper function
      KVM/VMX: Add hv tlb range flush support
      x86/hyper-v: Add HvFlushGuestAddressList hypercall support
      KVM: Add tlb_remote_flush_with_range callback in kvm_x86_ops
      KVM: x86: Disable Intel PT when VMXON in L1 guest
      KVM: x86: Set intercept for Intel PT MSRs read/write
      KVM: x86: Implement Intel PT MSRs read/write emulation
      ...

commit 748c0e312fce983bd7854b369b192e24dce90878
Author: Lan Tianyu <Tianyu.Lan@microsoft.com>
Date:   Thu Dec 6 21:21:10 2018 +0800

    KVM: Make kvm_set_spte_hva() return int
    
    The patch is to make kvm_set_spte_hva() return int and caller can
    check return value to determine flush tlb or not.
    
    Signed-off-by: Lan Tianyu <Tianyu.Lan@microsoft.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c22e50ba3473..063886be25ad 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -360,7 +360,7 @@ int __kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva_range(struct kvm *kvm,
 			unsigned long start, unsigned long end);
-void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
+int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 

commit da6f16662a6eba3de0d1a39f3b8913d38754bb2b
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Thu Nov 29 12:20:01 2018 +0100

    KVM: arm64: Make vcpu const in vcpu_read_sys_reg
    
    vcpu_read_sys_reg should not be modifying the VCPU structure.
    Eventually, to handle EL2 sysregs for nested virtualization, we will
    call vcpu_read_sys_reg from places that have a const vcpu pointer, which
    will complain about the lack of the const modifier on the read path.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7a5035f9c5c3..c22e50ba3473 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -319,7 +319,7 @@ struct kvm_vcpu_arch {
  */
 #define __vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
 
-u64 vcpu_read_sys_reg(struct kvm_vcpu *vcpu, int reg);
+u64 vcpu_read_sys_reg(const struct kvm_vcpu *vcpu, int reg);
 void vcpu_write_sys_reg(struct kvm_vcpu *vcpu, u64 val, int reg);
 
 /*

commit bd7d95cafb499e24903b7d21f9eeb2c5208160c2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Nov 9 15:07:11 2018 +0000

    arm64: KVM: Consistently advance singlestep when emulating instructions
    
    When we emulate a guest instruction, we don't advance the hardware
    singlestep state machine, and thus the guest will receive a software
    step exception after a next instruction which is not emulated by the
    host.
    
    We bodge around this in an ad-hoc fashion. Sometimes we explicitly check
    whether userspace requested a single step, and fake a debug exception
    from within the kernel. Other times, we advance the HW singlestep state
    rely on the HW to generate the exception for us. Thus, the observed step
    behaviour differs for host and guest.
    
    Let's make this simpler and consistent by always advancing the HW
    singlestep state machine when we skip an instruction. Thus we can rely
    on the hardware to generate the singlestep exception for us, and never
    need to explicitly check for an active-pending step, nor do we need to
    fake a debug exception from the guest.
    
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 52fbc823ff8c..7a5035f9c5c3 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -445,7 +445,6 @@ void kvm_arm_init_debug(void);
 void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
-bool kvm_arm_handle_step_debug(struct kvm_vcpu *vcpu, struct kvm_run *run);
 int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr);
 int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,

commit 8b2cca9ade2c0f1d2ba94e39781e7306c918e544
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 6 17:31:23 2018 +0000

    arm64: KVM: Force VHE for systems affected by erratum 1165522
    
    In order to easily mitigate ARM erratum 1165522, we need to force
    affected CPUs to run in VHE mode if using KVM.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d6d9aa76a943..9217759afa6b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -432,6 +432,10 @@ static inline bool kvm_arch_requires_vhe(void)
 	if (system_supports_sve())
 		return true;
 
+	/* Some implementations have defects that confine them to VHE */
+	if (cpus_have_cap(ARM64_WORKAROUND_1165522))
+		return true;
+
 	return false;
 }
 

commit 33e5f4e50917c2508c05898f391a971b15eec93e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 6 17:31:20 2018 +0000

    KVM: arm64: Rework detection of SVE, !VHE systems
    
    An SVE system is so far the only case where we mandate VHE. As we're
    starting to grow this requirements, let's slightly rework the way we
    deal with that situation, allowing for easy extension of this check.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 52fbc823ff8c..d6d9aa76a943 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -422,7 +422,7 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	}
 }
 
-static inline bool kvm_arch_check_sve_has_vhe(void)
+static inline bool kvm_arch_requires_vhe(void)
 {
 	/*
 	 * The Arm architecture specifies that implementation of SVE
@@ -430,9 +430,9 @@ static inline bool kvm_arch_check_sve_has_vhe(void)
 	 * relies on this when SVE is present:
 	 */
 	if (system_supports_sve())
-		return has_vhe();
-	else
 		return true;
+
+	return false;
 }
 
 static inline void kvm_arch_hardware_unsetup(void) {}

commit 0d1e8b8d2bcd3150d51754d8d0fdbf44dc88b0d3
Merge: 83c4087ce468 22a7cdcae6a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 17:57:35 2018 -0700

    Merge tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krm:
     "ARM:
       - Improved guest IPA space support (32 to 52 bits)
    
       - RAS event delivery for 32bit
    
       - PMU fixes
    
       - Guest entry hardening
    
       - Various cleanups
    
       - Port of dirty_log_test selftest
    
      PPC:
       - Nested HV KVM support for radix guests on POWER9. The performance
         is much better than with PR KVM. Migration and arbitrary level of
         nesting is supported.
    
       - Disable nested HV-KVM on early POWER9 chips that need a particular
         hardware bug workaround
    
       - One VM per core mode to prevent potential data leaks
    
       - PCI pass-through optimization
    
       - merge ppc-kvm topic branch and kvm-ppc-fixes to get a better base
    
      s390:
       - Initial version of AP crypto virtualization via vfio-mdev
    
       - Improvement for vfio-ap
    
       - Set the host program identifier
    
       - Optimize page table locking
    
      x86:
       - Enable nested virtualization by default
    
       - Implement Hyper-V IPI hypercalls
    
       - Improve #PF and #DB handling
    
       - Allow guests to use Enlightened VMCS
    
       - Add migration selftests for VMCS and Enlightened VMCS
    
       - Allow coalesced PIO accesses
    
       - Add an option to perform nested VMCS host state consistency check
         through hardware
    
       - Automatic tuning of lapic_timer_advance_ns
    
       - Many fixes, minor improvements, and cleanups"
    
    * tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      KVM/nVMX: Do not validate that posted_intr_desc_addr is page aligned
      Revert "kvm: x86: optimize dr6 restore"
      KVM: PPC: Optimize clearing TCEs for sparse tables
      x86/kvm/nVMX: tweak shadow fields
      selftests/kvm: add missing executables to .gitignore
      KVM: arm64: Safety check PSTATE when entering guest and handle IL
      KVM: PPC: Book3S HV: Don't use streamlined entry path on early POWER9 chips
      arm/arm64: KVM: Enable 32 bits kvm vcpu events support
      arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
      KVM: arm64: Fix caching of host MDCR_EL2 value
      KVM: VMX: enable nested virtualization by default
      KVM/x86: Use 32bit xor to clear registers in svm.c
      kvm: x86: Introduce KVM_CAP_EXCEPTION_PAYLOAD
      kvm: vmx: Defer setting of DR6 until #DB delivery
      kvm: x86: Defer setting of CR2 until #PF delivery
      kvm: x86: Add payload operands to kvm_multiple_exception
      kvm: x86: Add exception payload fields to kvm_vcpu_events
      kvm: x86: Add has_payload and payload to kvm_queued_exception
      KVM: Documentation: Fix omission in struct kvm_vcpu_events
      KVM: selftests: add Enlightened VMCS test
      ...

commit 528985117126f11beea339cf39120ee99da04cd2
Merge: 84df9525b0c2 4debef551007
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 22 17:30:06 2018 +0100

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Apart from some new arm64 features and clean-ups, this also contains
      the core mmu_gather changes for tracking the levels of the page table
      being cleared and a minor update to the generic
      compat_sys_sigaltstack() introducing COMPAT_SIGMINSKSZ.
    
      Summary:
    
       - Core mmu_gather changes which allow tracking the levels of
         page-table being cleared together with the arm64 low-level flushing
         routines
    
       - Support for the new ARMv8.5 PSTATE.SSBS bit which can be used to
         mitigate Spectre-v4 dynamically without trapping to EL3 firmware
    
       - Introduce COMPAT_SIGMINSTKSZ for use in compat_sys_sigaltstack
    
       - Optimise emulation of MRS instructions to ID_* registers on ARMv8.4
    
       - Support for Common Not Private (CnP) translations allowing threads
         of the same CPU to share the TLB entries
    
       - Accelerated crc32 routines
    
       - Move swapper_pg_dir to the rodata section
    
       - Trap WFI instruction executed in user space
    
       - ARM erratum 1188874 workaround (arch_timer)
    
       - Miscellaneous fixes and clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (78 commits)
      arm64: KVM: Guests can skip __install_bp_hardening_cb()s HYP work
      arm64: cpufeature: Trap CTR_EL0 access only where it is necessary
      arm64: cpufeature: Fix handling of CTR_EL0.IDC field
      arm64: cpufeature: ctr: Fix cpu capability check for late CPUs
      Documentation/arm64: HugeTLB page implementation
      arm64: mm: Use __pa_symbol() for set_swapper_pgd()
      arm64: Add silicon-errata.txt entry for ARM erratum 1188873
      Revert "arm64: uaccess: implement unsafe accessors"
      arm64: mm: Drop the unused cpu parameter
      MAINTAINERS: fix bad sdei paths
      arm64: mm: Use #ifdef for the __PAGETABLE_P?D_FOLDED defines
      arm64: Fix typo in a comment in arch/arm64/mm/kasan_init.c
      arm64: xen: Use existing helper to check interrupt status
      arm64: Use daifflag_restore after bp_hardening
      arm64: daifflags: Use irqflags functions for daifflags
      arm64: arch_timer: avoid unused function warning
      arm64: Trap WFI executed in userspace
      arm64: docs: Document SSBS HWCAP
      arm64: docs: Fix typos in ELF hwcaps
      arm64/kprobes: remove an extra semicolon in arch_prepare_kprobe
      ...

commit 375bdd3b5d4f7cf146f0df1488b4671b141dd799
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Sat Oct 13 00:12:48 2018 +0800

    arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
    
    Rename kvm_arch_dev_ioctl_check_extension() to
    kvm_arch_vm_ioctl_check_extension(), because it does
    not have any relationship with device.
    
    Renaming this function can make code readable.
    
    Cc: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 376a5b695467..f84052f306af 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -53,7 +53,7 @@ DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
-int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext);
+int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
 struct kvm_arch {

commit bca607ebc76af9540e4aad5b2241a7323354be43
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 1 13:40:36 2018 +0100

    KVM: arm/arm64: Rename kvm_arm_config_vm to kvm_arm_setup_stage2
    
    VM tends to be a very overloaded term in KVM, so let's keep it
    to describe the virtual machine. For the virtual memory setup,
    let's use the "stage2" suffix.
    
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f008f8866b2a..376a5b695467 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -511,6 +511,6 @@ void kvm_set_ipa_limit(void);
 struct kvm *kvm_arch_alloc_vm(void);
 void kvm_arch_free_vm(struct kvm *kvm);
 
-int kvm_arm_config_vm(struct kvm *kvm, unsigned long type);
+int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type);
 
 #endif /* __ARM64_KVM_HOST_H__ */

commit 0f62f0e95be29200ab2ab98ca870e22c9b148dfa
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:52 2018 +0100

    kvm: arm64: Set a limit on the IPA size
    
    So far we have restricted the IPA size of the VM to the default
    value (40bits). Now that we can manage the IPA size per VM and
    support dynamic stage2 page tables, we can allow VMs to have
    larger IPA. This patch introduces a the maximum IPA size
    supported on the host. This is decided by the following factors :
    
     1) Maximum PARange supported by the CPUs - This can be inferred
        from the system wide safe value.
     2) Maximum PA size supported by the host kernel (48 vs 52)
     3) Number of levels in the host page table (as we base our
        stage2 tables on the host table helpers).
    
    Since the stage2 page table code is dependent on the stage1
    page table, we always ensure that :
    
      Number of Levels at Stage1 >= Number of Levels at Stage2
    
    So we limit the IPA to make sure that the above condition
    is satisfied. This will affect the following combinations
    of VA_BITS and IPA for different page sizes.
    
      Host configuration | Unsupported IPA ranges
      39bit VA, 4K       | [44, 48]
      36bit VA, 16K      | [41, 48]
      42bit VA, 64K      | [47, 52]
    
    Supporting the above combinations need independent stage2
    page table manipulation code, which would need substantial
    changes. We could purse the solution independently and
    switch the page table code once we have it ready.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 5ecd457bce7d..f008f8866b2a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -442,15 +442,7 @@ int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,
 int kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr);
 
-static inline void __cpu_init_stage2(void)
-{
-	u32 ps;
-
-	/* Sanity check for minimum IPA size support */
-	ps = id_aa64mmfr0_parange_to_phys_shift(read_sysreg(id_aa64mmfr0_el1) & 0x7);
-	WARN_ONCE(ps < 40,
-		  "PARange is %d bits, unsupported configuration!", ps);
-}
+static inline void __cpu_init_stage2(void) {}
 
 /* Guest/host FPSIMD coordination helpers */
 int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu);
@@ -513,6 +505,8 @@ static inline int kvm_arm_have_ssbd(void)
 void kvm_vcpu_load_sysregs(struct kvm_vcpu *vcpu);
 void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu);
 
+void kvm_set_ipa_limit(void);
+
 #define __KVM_HAVE_ARCH_VM_ALLOC
 struct kvm *kvm_arch_alloc_vm(void);
 void kvm_arch_free_vm(struct kvm *kvm);

commit 7665f3a8491b0ed3c6f65c0bc3a5424ea8f87731
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:43 2018 +0100

    kvm: arm64: Configure VTCR_EL2 per VM
    
    Add support for setting the VTCR_EL2 per VM, rather than hard
    coding a value at boot time per CPU. This would allow us to tune
    the stage2 page table parameters per VM in the later changes.
    
    We compute the VTCR fields based on the system wide sanitised
    feature registers, except for the hardware management of Access
    Flags (VTCR_EL2.HA). It is fine to run a system with a mix of
    CPUs that may or may not update the page table Access Flags.
    Since the bit is RES0 on CPUs that don't support it, the bit
    should be ignored on them.
    
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b04280ae1be0..5ecd457bce7d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -61,11 +61,13 @@ struct kvm_arch {
 	u64    vmid_gen;
 	u32    vmid;
 
-	/* 1-level 2nd stage table, protected by kvm->mmu_lock */
+	/* stage2 entry level table */
 	pgd_t *pgd;
 
 	/* VTTBR value associated with above pgd and vmid */
 	u64    vttbr;
+	/* VTCR_EL2 value for this VM */
+	u64    vtcr;
 
 	/* The last vcpu id that ran on each physical CPU */
 	int __percpu *last_vcpu_ran;
@@ -442,10 +444,12 @@ int kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,
 
 static inline void __cpu_init_stage2(void)
 {
-	u32 parange = kvm_call_hyp(__init_stage2_translation);
+	u32 ps;
 
-	WARN_ONCE(parange < 40,
-		  "PARange is %d bits, unsupported configuration!", parange);
+	/* Sanity check for minimum IPA size support */
+	ps = id_aa64mmfr0_parange_to_phys_shift(read_sysreg(id_aa64mmfr0_el1) & 0x7);
+	WARN_ONCE(ps < 40,
+		  "PARange is %d bits, unsupported configuration!", ps);
 }
 
 /* Guest/host FPSIMD coordination helpers */

commit 5b6c6742b5350a6fb5c631fb99a6bc046a62739c
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:42 2018 +0100

    kvm: arm/arm64: Allow arch specific configurations for VM
    
    Allow the arch backends to perform VM specific initialisation.
    This will be later used to handle IPA size configuration and per-VM
    VTCR configuration on arm64.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 3d6d7336f871..b04280ae1be0 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -513,4 +513,6 @@ void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu);
 struct kvm *kvm_arch_alloc_vm(void);
 void kvm_arch_free_vm(struct kvm *kvm);
 
+int kvm_arm_config_vm(struct kvm *kvm, unsigned long type);
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 7c36447ae5a090729e7b129f24705bb231a07e0b
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 8 16:10:54 2018 +0100

    KVM: arm64: Set SCTLR_EL2.DSSBS if SSBD is forcefully disabled and !vhe
    
    When running without VHE, it is necessary to set SCTLR_EL2.DSSBS if SSBD
    has been forcefully disabled on the kernel command-line.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f26055f2306e..15501921fc75 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -389,6 +389,8 @@ struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
 DECLARE_PER_CPU(kvm_cpu_context_t, kvm_host_cpu_state);
 
+void __kvm_enable_ssbs(void);
+
 static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 				       unsigned long hyp_stack_ptr,
 				       unsigned long vector_ptr)
@@ -409,6 +411,15 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	 */
 	BUG_ON(!static_branch_likely(&arm64_const_caps_ready));
 	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr, tpidr_el2);
+
+	/*
+	 * Disabling SSBD on a non-VHE system requires us to enable SSBS
+	 * at EL2.
+	 */
+	if (!has_vhe() && this_cpu_has_cap(ARM64_SSBS) &&
+	    arm64_get_ssbd_state() == ARM64_SSBD_FORCE_DISABLE) {
+		kvm_call_hyp(__kvm_enable_ssbs);
+	}
 }
 
 static inline bool kvm_arch_check_sve_has_vhe(void)

commit df3190e22016abf74ef67c9691e9fa1012a66bd5
Author: Steven Price <steven.price@arm.com>
Date:   Mon Aug 13 17:04:53 2018 +0100

    arm64: KVM: Remove pgd_lock
    
    The lock has never been used and the page tables are protected by
    mmu_lock in struct kvm.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 8e6d46df38aa..3d6d7336f871 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -61,8 +61,7 @@ struct kvm_arch {
 	u64    vmid_gen;
 	u32    vmid;
 
-	/* 1-level 2nd stage table and lock */
-	spinlock_t pgd_lock;
+	/* 1-level 2nd stage table, protected by kvm->mmu_lock */
 	pgd_t *pgd;
 
 	/* VTTBR value associated with above pgd and vmid */

commit a35381e10dc46dd75e65e4b3832d9a0005d48d44
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Aug 23 10:18:14 2018 +0100

    KVM: Remove obsolete kvm_unmap_hva notifier backend
    
    kvm_unmap_hva is long gone, and we only have kvm_unmap_hva_range to
    deal with. Drop the now obsolete code.
    
    Fixes: fb1522e099f0 ("KVM: update to new mmu_notifier semantic v2")
    Cc: James Hogan <jhogan@kernel.org>
    Reviewed-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f26055f2306e..8e6d46df38aa 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -357,7 +357,6 @@ int __kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
 			      struct kvm_vcpu_events *events);
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
-int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_unmap_hva_range(struct kvm *kvm,
 			unsigned long start, unsigned long end);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);

commit 539aee0edb9fdc8f465e3843c261acc88c47d8ee
Author: James Morse <james.morse@arm.com>
Date:   Thu Jul 19 16:24:24 2018 +0100

    KVM: arm64: Share the parts of get/set events useful to 32bit
    
    The get/set events helpers to do some work to check reserved
    and padding fields are zero. This is useful on 32bit too.
    
    Move this code into virt/kvm/arm/arm.c, and give the arch
    code some underscores.
    
    This is temporarily hidden behind __KVM_HAVE_VCPU_EVENTS until
    32bit is wired up.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index bc244cc6e451..f26055f2306e 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -350,11 +350,11 @@ unsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu);
 int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices);
 int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
 int kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
-int kvm_arm_vcpu_get_events(struct kvm_vcpu *vcpu,
-			    struct kvm_vcpu_events *events);
+int __kvm_arm_vcpu_get_events(struct kvm_vcpu *vcpu,
+			      struct kvm_vcpu_events *events);
 
-int kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
-			    struct kvm_vcpu_events *events);
+int __kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
+			      struct kvm_vcpu_events *events);
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);

commit b7b27facc7b50a5fce0afaa3df56157136ce181a
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Thu Jul 19 16:24:22 2018 +0100

    arm/arm64: KVM: Add KVM_GET/SET_VCPU_EVENTS
    
    For the migrating VMs, user space may need to know the exception
    state. For example, in the machine A, KVM make an SError pending,
    when migrate to B, KVM also needs to pend an SError.
    
    This new IOCTL exports user-invisible states related to SError.
    Together with appropriate user space changes, user space can get/set
    the SError exception state to do migrate/snapshot/suspend.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    [expanded documentation wording]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 268619ce0154..bc244cc6e451 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -350,6 +350,11 @@ unsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu);
 int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices);
 int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
 int kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
+int kvm_arm_vcpu_get_events(struct kvm_vcpu *vcpu,
+			    struct kvm_vcpu_events *events);
+
+int kvm_arm_vcpu_set_events(struct kvm_vcpu *vcpu,
+			    struct kvm_vcpu_events *events);
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
@@ -378,6 +383,8 @@ void handle_exit_early(struct kvm_vcpu *vcpu, struct kvm_run *run,
 int kvm_perf_init(void);
 int kvm_perf_teardown(void);
 
+void kvm_set_sei_esr(struct kvm_vcpu *vcpu, u64 syndrome);
+
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
 DECLARE_PER_CPU(kvm_cpu_context_t, kvm_host_cpu_state);

commit 9bc03f1df31a3228289d5046780071ab8e91aa1a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jul 10 13:20:47 2018 +0100

    arm64: KVM: Cleanup tpidr_el2 init on non-VHE
    
    When running on a non-VHE system, we initialize tpidr_el2 to
    contain the per-CPU offset required to reach per-cpu variables.
    
    Actually, we initialize it twice: the first time as part of the
    EL2 initialization, by copying tpidr_el1 into its el2 counterpart,
    and another time by calling into __kvm_set_tpidr_el2.
    
    It turns out that the first part is wrong, as it includes the
    distance between the kernel mapping and the linear mapping, while
    EL2 only cares about the linear mapping. This was the last vestige
    of the first per-cpu use of tpidr_el2 that came in with SDEI.
    The only caller then was hyp_panic(), and its now using the
    pc-relative get_host_ctxt() stuff, instead of kimage addresses
    from the literal pool.
    
    It is not a big deal, as we override it straight away, but it is
    slightly confusing. In order to clear said confusion, let's
    set this directly as part of the hyp-init code, and drop the
    ad-hoc HYP helper.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index fe8777b12f86..268619ce0154 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -380,14 +380,19 @@ int kvm_perf_teardown(void);
 
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
-void __kvm_set_tpidr_el2(u64 tpidr_el2);
 DECLARE_PER_CPU(kvm_cpu_context_t, kvm_host_cpu_state);
 
 static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 				       unsigned long hyp_stack_ptr,
 				       unsigned long vector_ptr)
 {
-	u64 tpidr_el2;
+	/*
+	 * Calculate the raw per-cpu offset without a translation from the
+	 * kernel's mapping to the linear mapping, and store it in tpidr_el2
+	 * so that we can use adr_l to access per-cpu variables in EL2.
+	 */
+	u64 tpidr_el2 = ((u64)this_cpu_ptr(&kvm_host_cpu_state) -
+			 (u64)kvm_ksym_ref(kvm_host_cpu_state));
 
 	/*
 	 * Call initialization code, and switch to the full blown HYP code.
@@ -396,17 +401,7 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	 * cpus_have_const_cap() wrapper.
 	 */
 	BUG_ON(!static_branch_likely(&arm64_const_caps_ready));
-	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr);
-
-	/*
-	 * Calculate the raw per-cpu offset without a translation from the
-	 * kernel's mapping to the linear mapping, and store it in tpidr_el2
-	 * so that we can use adr_l to access per-cpu variables in EL2.
-	 */
-	tpidr_el2 = (u64)this_cpu_ptr(&kvm_host_cpu_state)
-		- (u64)kvm_ksym_ref(kvm_host_cpu_state);
-
-	kvm_call_hyp(__kvm_set_tpidr_el2, tpidr_el2);
+	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr, tpidr_el2);
 }
 
 static inline bool kvm_arch_check_sve_has_vhe(void)

commit b3eb56b629d1095dde56fa37f4d7bcd5f783c8b2
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Jun 15 16:47:25 2018 +0100

    KVM: arm64/sve: Fix SVE trap restoration for non-current tasks
    
    Commit e6b673b ("KVM: arm64: Optimise FPSIMD handling to reduce
    guest/host thrashing") attempts to restore the configuration of
    userspace SVE trapping via a call to fpsimd_bind_task_to_cpu(), but
    the logic for determining when to do this is not correct.
    
    The patch makes the errnoenous assumption that the only task that
    may try to enter userspace with the currently loaded FPSIMD/SVE
    register content is current.  This may not be the case however:  if
    some other user task T is scheduled on the CPU during the execution
    of the KVM run loop, and the vcpu does not try to use the registers
    in the meantime, then T's state may be left there intact.  If T
    happens to be the next task to enter userspace on this CPU then the
    hooks for reloading the register state and configuring traps will
    be skipped.
    
    (Also, current never has SVE state at this point anyway and should
    always have the trap enabled, as a side-effect of the ioctl()
    syscall needed to reach the KVM run loop in the first place.)
    
    This patch instead restores the state of the EL0 trap from the
    state observed at the most recent vcpu_load(), ensuring that the
    trap is set correctly for the loaded context (if any).
    
    Fixes: e6b673b741ea ("KVM: arm64: Optimise FPSIMD handling to reduce guest/host thrashing")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index fda9a8ca48be..fe8777b12f86 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -306,6 +306,7 @@ struct kvm_vcpu_arch {
 #define KVM_ARM64_FP_ENABLED		(1 << 1) /* guest FP regs loaded */
 #define KVM_ARM64_FP_HOST		(1 << 2) /* host FP regs loaded */
 #define KVM_ARM64_HOST_SVE_IN_USE	(1 << 3) /* backup for host TIF_SVE */
+#define KVM_ARM64_HOST_SVE_ENABLED	(1 << 4) /* SVE enabled for EL0 */
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
 

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit d1e5b0e98ea27b4f17871dc4e8ea4b0447e35221
Author: Marc Orr <marcorr@google.com>
Date:   Tue May 15 04:37:37 2018 -0700

    kvm: Make VM ioctl do valloc for some archs
    
    The kvm struct has been bloating. For example, it's tens of kilo-bytes
    for x86, which turns out to be a large amount of memory to allocate
    contiguously via kzalloc. Thus, this patch does the following:
    1. Uses architecture-specific routines to allocate the kvm struct via
       vzalloc for x86.
    2. Switches arm to __KVM_HAVE_ARCH_VM_ALLOC so that it can use vzalloc
       when has_vhe() is true.
    
    Other architectures continue to default to kalloc, as they have a
    dependency on kalloc or have a small-enough struct kvm.
    
    Signed-off-by: Marc Orr <marcorr@google.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a4ca202ff3f2..c923d3e17ba3 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -482,4 +482,8 @@ static inline bool kvm_arm_harden_branch_predictor(void)
 void kvm_vcpu_load_sysregs(struct kvm_vcpu *vcpu);
 void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu);
 
+#define __KVM_HAVE_ARCH_VM_ALLOC
+struct kvm *kvm_arch_alloc_vm(void);
+void kvm_arch_free_vm(struct kvm *kvm);
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 5d81f7dc9bca4f4963092433e27b508cbe524a32
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:18 2018 +0100

    arm64: KVM: Add ARCH_WORKAROUND_2 discovery through ARCH_FEATURES_FUNC_ID
    
    Now that all our infrastructure is in place, let's expose the
    availability of ARCH_WORKAROUND_2 to guests. We take this opportunity
    to tidy up a couple of SMCCC constants.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9bef3f69bdcd..95d8a0e15b5f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -455,6 +455,29 @@ static inline bool kvm_arm_harden_branch_predictor(void)
 	return cpus_have_const_cap(ARM64_HARDEN_BRANCH_PREDICTOR);
 }
 
+#define KVM_SSBD_UNKNOWN		-1
+#define KVM_SSBD_FORCE_DISABLE		0
+#define KVM_SSBD_KERNEL		1
+#define KVM_SSBD_FORCE_ENABLE		2
+#define KVM_SSBD_MITIGATED		3
+
+static inline int kvm_arm_have_ssbd(void)
+{
+	switch (arm64_get_ssbd_state()) {
+	case ARM64_SSBD_FORCE_DISABLE:
+		return KVM_SSBD_FORCE_DISABLE;
+	case ARM64_SSBD_KERNEL:
+		return KVM_SSBD_KERNEL;
+	case ARM64_SSBD_FORCE_ENABLE:
+		return KVM_SSBD_FORCE_ENABLE;
+	case ARM64_SSBD_MITIGATED:
+		return KVM_SSBD_MITIGATED;
+	case ARM64_SSBD_UNKNOWN:
+	default:
+		return KVM_SSBD_UNKNOWN;
+	}
+}
+
 void kvm_vcpu_load_sysregs(struct kvm_vcpu *vcpu);
 void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu);
 

commit 55e3748e8902ff641e334226bdcb432f9a5d78d3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:16 2018 +0100

    arm64: KVM: Add ARCH_WORKAROUND_2 support for guests
    
    In order to offer ARCH_WORKAROUND_2 support to guests, we need
    a bit of infrastructure.
    
    Let's add a flag indicating whether or not the guest uses
    SSBD mitigation. Depending on the state of this flag, allow
    KVM to disable ARCH_WORKAROUND_2 before entering the guest,
    and enable it when exiting it.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 469de8acd06f..9bef3f69bdcd 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -216,6 +216,9 @@ struct kvm_vcpu_arch {
 	/* Exception Information */
 	struct kvm_vcpu_fault_info fault;
 
+	/* State of various workarounds, see kvm_asm.h for bit assignment */
+	u64 workaround_flags;
+
 	/* Guest debug state */
 	u64 debug_flags;
 

commit 21cdd7fd76e3259b06d78c909e9caeb084c04b65
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 20 17:39:16 2018 +0100

    KVM: arm64: Remove eager host SVE state saving
    
    Now that the host SVE context can be saved on demand from Hyp,
    there is no longer any need to save this state in advance before
    entering the guest.
    
    This patch removes the relevant call to
    kvm_fpsimd_flush_cpu_state().
    
    Since the problem that function was intended to solve now no longer
    exists, the function and its dependencies are also deleted.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index fda9289f3b9c..a4ca202ff3f2 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -457,16 +457,6 @@ static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
 }
 #endif
 
-/*
- * All host FP/SIMD state is restored on guest exit, so nothing needs
- * doing here except in the SVE case:
-*/
-static inline void kvm_fpsimd_flush_cpu_state(void)
-{
-	if (system_supports_sve())
-		sve_flush_cpu_state();
-}
-
 static inline void kvm_arm_vhe_guest_enter(void)
 {
 	local_daif_mask();

commit 85acda3b4a27ee3e20c54783a44f307b51912c2b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 20 16:20:43 2018 +0100

    KVM: arm64: Save host SVE context as appropriate
    
    This patch adds SVE context saving to the hyp FPSIMD context switch
    path.  This means that it is no longer necessary to save the host
    SVE state in advance of entering the guest, when in use.
    
    In order to avoid adding pointless complexity to the code, VHE is
    assumed if SVE is in use.  VHE is an architectural prerequisite for
    SVE, so there is no good reason to turn CONFIG_ARM64_VHE off in
    kernels that support both SVE and KVM.
    
    Historically, software models exist that can expose the
    architecturally invalid configuration of SVE without VHE, so if
    this situation is detected at kvm_init() time then KVM will be
    disabled.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b3fe7301bdbe..fda9289f3b9c 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -405,6 +405,19 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	kvm_call_hyp(__kvm_set_tpidr_el2, tpidr_el2);
 }
 
+static inline bool kvm_arch_check_sve_has_vhe(void)
+{
+	/*
+	 * The Arm architecture specifies that implementation of SVE
+	 * requires VHE also to be implemented.  The KVM code for arm64
+	 * relies on this when SVE is present:
+	 */
+	if (system_supports_sve())
+		return has_vhe();
+	else
+		return true;
+}
+
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}

commit e6b673b741ea0d7cd275ad40748bfc225accc423
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 6 14:55:59 2018 +0100

    KVM: arm64: Optimise FPSIMD handling to reduce guest/host thrashing
    
    This patch refactors KVM to align the host and guest FPSIMD
    save/restore logic with each other for arm64.  This reduces the
    number of redundant save/restore operations that must occur, and
    reduces the common-case IRQ blackout time during guest exit storms
    by saving the host state lazily and optimising away the need to
    restore the host state before returning to the run loop.
    
    Four hooks are defined in order to enable this:
    
     * kvm_arch_vcpu_run_map_fp():
       Called on PID change to map necessary bits of current to Hyp.
    
     * kvm_arch_vcpu_load_fp():
       Set up FP/SIMD for entering the KVM run loop (parse as
       "vcpu_load fp").
    
     * kvm_arch_vcpu_ctxsync_fp():
       Get FP/SIMD into a safe state for re-enabling interrupts after a
       guest exit back to the run loop.
    
       For arm64 specifically, this involves updating the host kernel's
       FPSIMD context tracking metadata so that kernel-mode NEON use
       will cause the vcpu's FPSIMD state to be saved back correctly
       into the vcpu struct.  This must be done before re-enabling
       interrupts because kernel-mode NEON may be used by softirqs.
    
     * kvm_arch_vcpu_put_fp():
       Save guest FP/SIMD state back to memory and dissociate from the
       CPU ("vcpu_put fp").
    
    Also, the arm64 FPSIMD context switch code is updated to enable it
    to save back FPSIMD state for a vcpu, not just current.  A few
    helpers drive this:
    
     * fpsimd_bind_state_to_cpu(struct user_fpsimd_state *fp):
       mark this CPU as having context fp (which may belong to a vcpu)
       currently loaded in its registers.  This is the non-task
       equivalent of the static function fpsimd_bind_to_cpu() in
       fpsimd.c.
    
     * task_fpsimd_save():
       exported to allow KVM to save the guest's FPSIMD state back to
       memory on exit from the run loop.
    
     * fpsimd_flush_state():
       invalidate any context's FPSIMD state that is currently loaded.
       Used to disassociate the vcpu from the CPU regs on run loop exit.
    
    These changes allow the run loop to enable interrupts (and thus
    softirqs that may use kernel-mode NEON) without having to save the
    guest's FPSIMD state eagerly.
    
    Some new vcpu_arch fields are added to make all this work.  Because
    host FPSIMD state can now be saved back directly into current's
    thread_struct as appropriate, host_cpu_context is no longer used
    for preserving the FPSIMD state.  However, it is still needed for
    preserving other things such as the host's system registers.  To
    avoid ABI churn, the redundant storage space in host_cpu_context is
    not removed for now.
    
    arch/arm is not addressed by this patch and continues to use its
    current save/restore logic.  It could provide implementations of
    the helpers later if desired.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 146c16794d32..b3fe7301bdbe 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -30,6 +30,7 @@
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
+#include <asm/thread_info.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 
@@ -238,6 +239,10 @@ struct kvm_vcpu_arch {
 
 	/* Pointer to host CPU context */
 	kvm_cpu_context_t *host_cpu_context;
+
+	struct thread_info *host_thread_info;	/* hyp VA */
+	struct user_fpsimd_state *host_fpsimd_state;	/* hyp VA */
+
 	struct {
 		/* {Break,watch}point registers */
 		struct kvm_guest_debug_arch regs;
@@ -295,6 +300,9 @@ struct kvm_vcpu_arch {
 
 /* vcpu_arch flags field values: */
 #define KVM_ARM64_DEBUG_DIRTY		(1 << 0)
+#define KVM_ARM64_FP_ENABLED		(1 << 1) /* guest FP regs loaded */
+#define KVM_ARM64_FP_HOST		(1 << 2) /* host FP regs loaded */
+#define KVM_ARM64_HOST_SVE_IN_USE	(1 << 3) /* backup for host TIF_SVE */
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
 
@@ -423,6 +431,19 @@ static inline void __cpu_init_stage2(void)
 		  "PARange is %d bits, unsupported configuration!", parange);
 }
 
+/* Guest/host FPSIMD coordination helpers */
+int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu);
+void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu);
+
+#ifdef CONFIG_KVM /* Avoid conflicts with core headers if CONFIG_KVM=n */
+static inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)
+{
+	return kvm_arch_vcpu_run_map_fp(vcpu);
+}
+#endif
+
 /*
  * All host FP/SIMD state is restored on guest exit, so nothing needs
  * doing here except in the SVE case:

commit fa89d31c53061139bd66f9de6e55340ac7bd5480
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue May 8 14:47:23 2018 +0100

    KVM: arm64: Repurpose vcpu_arch.debug_flags for general-purpose flags
    
    In struct vcpu_arch, the debug_flags field is used to store
    debug-related flags about the vcpu state.
    
    Since we are about to add some more flags related to FPSIMD and
    SVE, it makes sense to add them to the existing flags field rather
    than adding new fields.  Since there is only one debug_flags flag
    defined so far, there is plenty of free space for expansion.
    
    In preparation for adding more flags, this patch renames the
    debug_flags field to simply "flags", and updates comments
    appropriately.
    
    The flag definitions are also moved to <asm/kvm_host.h>, since
    their presence in <asm/kvm_asm.h> was for purely historical
    reasons:  these definitions are not used from asm any more, and not
    very likely to be as more Hyp asm is migrated to C.
    
    KVM_ARM64_DEBUG_DIRTY_SHIFT has not been used since commit
    1ea66d27e7b0 ("arm64: KVM: Move away from the assembly version of
    the world switch"), so this patch gets rid of that too.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    [maz: fixed minor conflict]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 469de8acd06f..146c16794d32 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -216,8 +216,8 @@ struct kvm_vcpu_arch {
 	/* Exception Information */
 	struct kvm_vcpu_fault_info fault;
 
-	/* Guest debug state */
-	u64 debug_flags;
+	/* Miscellaneous vcpu state flags */
+	u64 flags;
 
 	/*
 	 * We maintain more than a single set of debug registers to support
@@ -293,6 +293,9 @@ struct kvm_vcpu_arch {
 	bool sysregs_loaded_on_cpu;
 };
 
+/* vcpu_arch flags field values: */
+#define KVM_ARM64_DEBUG_DIRTY		(1 << 0)
+
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
 
 /*

commit 85bd0ba1ff9875798fad94218b627ea9f768f3c3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sun Jan 21 16:42:56 2018 +0000

    arm/arm64: KVM: Add PSCI version selection API
    
    Although we've implemented PSCI 0.1, 0.2 and 1.0, we expose either 0.1
    or 1.0 to a guest, defaulting to the latest version of the PSCI
    implementation that is compatible with the requested version. This is
    no different from doing a firmware upgrade on KVM.
    
    But in order to give a chance to hypothetical badly implemented guests
    that would have a fit by discovering something other than PSCI 0.2,
    let's provide a new API that allows userspace to pick one particular
    version of the API.
    
    This is implemented as a new class of "firmware" registers, where
    we expose the PSCI version. This allows the PSCI version to be
    save/restored as part of a guest migration, and also set to
    any supported version if the guest requires it.
    
    Cc: stable@vger.kernel.org #4.16
    Reviewed-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ab46bc70add6..469de8acd06f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -75,6 +75,9 @@ struct kvm_arch {
 
 	/* Interrupt controller */
 	struct vgic_dist	vgic;
+
+	/* Mandated version of PSCI */
+	u32 psci_version;
 };
 
 #define KVM_NR_MEM_OBJS     40

commit d47533dab9f50eeb7289a38035c39c9eadb1e048
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Sat Dec 23 21:53:48 2017 +0100

    KVM: arm64: Introduce framework for accessing deferred sysregs
    
    We are about to defer saving and restoring some groups of system
    registers to vcpu_put and vcpu_load on supported systems.  This means
    that we need some infrastructure to access system registes which
    supports either accessing the memory backing of the register or directly
    accessing the system registers, depending on the state of the system
    when we access the register.
    
    We do this by defining read/write accessor functions, which can handle
    both "immediate" and "deferrable" system registers.  Immediate registers
    are always saved/restored in the world-switch path, but deferrable
    registers are only saved/restored in vcpu_put/vcpu_load when supported
    and sysregs_loaded_on_cpu will be set in that case.
    
    Note that we don't use the deferred mechanism yet in this patch, but only
    introduce infrastructure.  This is to improve convenience of review in
    the subsequent patches where it is clear which registers become
    deferred.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 179bb9d5760b..ab46bc70add6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -284,6 +284,10 @@ struct kvm_vcpu_arch {
 
 	/* Virtual SError ESR to restore when HCR_EL2.VSE is set */
 	u64 vsesr_el2;
+
+	/* True when deferrable sysregs are loaded on the physical CPU,
+	 * see kvm_vcpu_load_sysregs and kvm_vcpu_put_sysregs. */
+	bool sysregs_loaded_on_cpu;
 };
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
@@ -296,8 +300,8 @@ struct kvm_vcpu_arch {
  */
 #define __vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
 
-#define vcpu_read_sys_reg(v,r)	__vcpu_sys_reg(v,r)
-#define vcpu_write_sys_reg(v,n,r)	do { __vcpu_sys_reg(v,r) = n; } while (0)
+u64 vcpu_read_sys_reg(struct kvm_vcpu *vcpu, int reg);
+void vcpu_write_sys_reg(struct kvm_vcpu *vcpu, u64 val, int reg);
 
 /*
  * CP14 and CP15 live in the same array, as they are backed by the

commit 8d404c4c246137531f94dfee352f350d59d0e5a7
Author: Christoffer Dall <cdall@cs.columbia.edu>
Date:   Wed Mar 16 15:38:53 2016 +0100

    KVM: arm64: Rewrite system register accessors to read/write functions
    
    Currently we access the system registers array via the vcpu_sys_reg()
    macro.  However, we are about to change the behavior to some times
    modify the register file directly, so let's change this to two
    primitives:
    
     * Accessor macros vcpu_write_sys_reg() and vcpu_read_sys_reg()
     * Direct array access macro __vcpu_sys_reg()
    
    The accessor macros should be used in places where the code needs to
    access the currently loaded VCPU's state as observed by the guest.  For
    example, when trapping on cache related registers, a write to a system
    register should go directly to the VCPU version of the register.
    
    The direct array access macro can be used in places where the VCPU is
    known to never be running (for example userspace access) or for
    registers which are never context switched (for example all the PMU
    system registers).
    
    This rewrites all users of vcpu_sys_regs to one of the macros described
    above.
    
    No functional change.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <cdall@cs.columbia.edu>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9001fd0890c9..179bb9d5760b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -287,7 +287,18 @@ struct kvm_vcpu_arch {
 };
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
-#define vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
+
+/*
+ * Only use __vcpu_sys_reg if you know you want the memory backed version of a
+ * register, and not the one most recently accessed by a running VCPU.  For
+ * example, for userspace access or for system registers that are never context
+ * switched, but only emulated.
+ */
+#define __vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
+
+#define vcpu_read_sys_reg(v,r)	__vcpu_sys_reg(v,r)
+#define vcpu_write_sys_reg(v,n,r)	do { __vcpu_sys_reg(v,r) = n; } while (0)
+
 /*
  * CP14 and CP15 live in the same array, as they are backed by the
  * same system registers.

commit 52f6c4f0216427e9d922498048e4bd24f1b00232
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Wed Oct 11 15:20:41 2017 +0200

    KVM: arm64: Change 32-bit handling of VM system registers
    
    We currently handle 32-bit accesses to trapped VM system registers using
    the 32-bit index into the coproc array on the vcpu structure, which is a
    union of the coproc array and the sysreg array.
    
    Since all the 32-bit coproc indices are created to correspond to the
    architectural mapping between 64-bit system registers and 32-bit
    coprocessor registers, and because the AArch64 system registers are the
    double in size of the AArch32 coprocessor registers, we can always find
    the system register entry that we must update by dividing the 32-bit
    coproc index by 2.
    
    This is going to make our lives much easier when we have to start
    accessing system registers that use deferred save/restore and might
    have to be read directly from the physical CPU.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 87abc94fb591..9001fd0890c9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -295,14 +295,6 @@ struct kvm_vcpu_arch {
 #define vcpu_cp14(v,r)		((v)->arch.ctxt.copro[(r)])
 #define vcpu_cp15(v,r)		((v)->arch.ctxt.copro[(r)])
 
-#ifdef CONFIG_CPU_BIG_ENDIAN
-#define vcpu_cp15_64_high(v,r)	vcpu_cp15((v),(r))
-#define vcpu_cp15_64_low(v,r)	vcpu_cp15((v),(r) + 1)
-#else
-#define vcpu_cp15_64_high(v,r)	vcpu_cp15((v),(r) + 1)
-#define vcpu_cp15_64_low(v,r)	vcpu_cp15((v),(r))
-#endif
-
 struct kvm_vm_stat {
 	ulong remote_tlb_flush;
 };

commit 3f5c90b890acfa7ad0b817a67cfc5eaaf0e21f7d
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Oct 3 14:02:12 2017 +0200

    KVM: arm64: Introduce VHE-specific kvm_vcpu_run
    
    So far this is mostly (see below) a copy of the legacy non-VHE switch
    function, but we will start reworking these functions in separate
    directions to work on VHE and non-VHE in the most optimal way in later
    patches.
    
    The only difference after this patch between the VHE and non-VHE run
    functions is that we omit the branch-predictor variant-2 hardening for
    QC Falkor CPUs, because this workaround is specific to a series of
    non-VHE ARMv8.0 CPUs.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c30fc96992df..87abc94fb591 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -428,6 +428,13 @@ static inline void kvm_arm_vhe_guest_enter(void)
 static inline void kvm_arm_vhe_guest_exit(void)
 {
 	local_daif_restore(DAIF_PROCCTX_NOIRQ);
+
+	/*
+	 * When we exit from the guest we change a number of CPU configuration
+	 * parameters, such as traps.  Make sure these changes take effect
+	 * before running the host or additional guests.
+	 */
+	isb();
 }
 
 static inline bool kvm_arm_harden_branch_predictor(void)

commit bc192ceec37108bf6c04a5c5795fcea5f940b0de
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Oct 10 10:21:18 2017 +0200

    KVM: arm/arm64: Add kvm_vcpu_load_sysregs and kvm_vcpu_put_sysregs
    
    As we are about to move a bunch of save/restore logic for VHE kernels to
    the load and put functions, we need some infrastructure to do this.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b027a7f025d4..c30fc96992df 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -435,4 +435,7 @@ static inline bool kvm_arm_harden_branch_predictor(void)
 	return cpus_have_const_cap(ARM64_HARDEN_BRANCH_PREDICTOR);
 }
 
+void kvm_vcpu_load_sysregs(struct kvm_vcpu *vcpu);
+void kvm_vcpu_put_sysregs(struct kvm_vcpu *vcpu);
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 3df59d8dd3c2526b33d51af9e6f66e61262de71b
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Aug 3 12:09:05 2017 +0200

    KVM: arm/arm64: Get rid of vcpu->arch.irq_lines
    
    We currently have a separate read-modify-write of the HCR_EL2 on entry
    to the guest for the sole purpose of setting the VF and VI bits, if set.
    Since this is most rarely the case (only when using userspace IRQ chip
    and interrupts are in flight), let's get rid of this operation and
    instead modify the bits in the vcpu->arch.hcr[_el2] directly when
    needed.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 618cfee7206a..b027a7f025d4 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -272,9 +272,6 @@ struct kvm_vcpu_arch {
 	/* IO related fields */
 	struct kvm_decode mmio_decode;
 
-	/* Interrupt related fields */
-	u64 irq_lines;		/* IRQ and FIQ levels */
-
 	/* Cache some mmu pages needed inside spinlock regions */
 	struct kvm_mmu_memory_cache mmu_page_cache;
 

commit 4464e210de9e80e38de59df052fe09ea2ff80b1b
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Sun Oct 8 17:01:56 2017 +0200

    KVM: arm64: Avoid storing the vcpu pointer on the stack
    
    We already have the percpu area for the host cpu state, which points to
    the VCPU, so there's no need to store the VCPU pointer on the stack on
    every context switch.  We can be a little more clever and just use
    tpidr_el2 for the percpu offset and load the VCPU pointer from the host
    context.
    
    This has the benefit of being able to retrieve the host context even
    when our stack is corrupted, and it has a potential performance benefit
    because we trade a store plus a load for an mrs and a load on a round
    trip to the guest.
    
    This does require us to calculate the percpu offset without including
    the offset from the kernel mapping of the percpu array to the linear
    mapping of the array (which is what we store in tpidr_el1), because a
    PC-relative generated address in EL2 is already giving us the hyp alias
    of the linear mapping of a kernel address.  We do this in
    __cpu_init_hyp_mode() by using kvm_ksym_ref().
    
    The code that accesses ESR_EL2 was previously using an alternative to
    use the _EL1 accessor on VHE systems, but this was actually unnecessary
    as the _EL1 accessor aliases the ESR_EL2 register on VHE, and the _EL2
    accessor does the same thing on both systems.
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 596f8e414a4c..618cfee7206a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -358,10 +358,15 @@ int kvm_perf_teardown(void);
 
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
+void __kvm_set_tpidr_el2(u64 tpidr_el2);
+DECLARE_PER_CPU(kvm_cpu_context_t, kvm_host_cpu_state);
+
 static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 				       unsigned long hyp_stack_ptr,
 				       unsigned long vector_ptr)
 {
+	u64 tpidr_el2;
+
 	/*
 	 * Call initialization code, and switch to the full blown HYP code.
 	 * If the cpucaps haven't been finalized yet, something has gone very
@@ -370,6 +375,16 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	 */
 	BUG_ON(!static_branch_likely(&arm64_const_caps_ready));
 	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr);
+
+	/*
+	 * Calculate the raw per-cpu offset without a translation from the
+	 * kernel's mapping to the linear mapping, and store it in tpidr_el2
+	 * so that we can use adr_l to access per-cpu variables in EL2.
+	 */
+	tpidr_el2 = (u64)this_cpu_ptr(&kvm_host_cpu_state)
+		- (u64)kvm_ksym_ref(kvm_host_cpu_state);
+
+	kvm_call_hyp(__kvm_set_tpidr_el2, tpidr_el2);
 }
 
 static inline void kvm_arch_hardware_unsetup(void) {}

commit 15303ba5d1cd9b28d03a980456c0978c0ea3b208
Merge: 9a61df9e5f74 1ab03c072feb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 10 13:16:35 2018 -0800

    Merge tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krm:
     "ARM:
    
       - icache invalidation optimizations, improving VM startup time
    
       - support for forwarded level-triggered interrupts, improving
         performance for timers and passthrough platform devices
    
       - a small fix for power-management notifiers, and some cosmetic
         changes
    
      PPC:
    
       - add MMIO emulation for vector loads and stores
    
       - allow HPT guests to run on a radix host on POWER9 v2.2 CPUs without
         requiring the complex thread synchronization of older CPU versions
    
       - improve the handling of escalation interrupts with the XIVE
         interrupt controller
    
       - support decrement register migration
    
       - various cleanups and bugfixes.
    
      s390:
    
       - Cornelia Huck passed maintainership to Janosch Frank
    
       - exitless interrupts for emulated devices
    
       - cleanup of cpuflag handling
    
       - kvm_stat counter improvements
    
       - VSIE improvements
    
       - mm cleanup
    
      x86:
    
       - hypervisor part of SEV
    
       - UMIP, RDPID, and MSR_SMI_COUNT emulation
    
       - paravirtualized TLB shootdown using the new KVM_VCPU_PREEMPTED bit
    
       - allow guests to see TOPOEXT, GFNI, VAES, VPCLMULQDQ, and more
         AVX512 features
    
       - show vcpu id in its anonymous inode name
    
       - many fixes and cleanups
    
       - per-VCPU MSR bitmaps (already merged through x86/pti branch)
    
       - stable KVM clock when nesting on Hyper-V (merged through
         x86/hyperv)"
    
    * tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (197 commits)
      KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
      KVM: PPC: Book3S HV: Branch inside feature section
      KVM: PPC: Book3S HV: Make HPT resizing work on POWER9
      KVM: PPC: Book3S HV: Fix handling of secondary HPTEG in HPT resizing code
      KVM: PPC: Book3S PR: Fix broken select due to misspelling
      KVM: x86: don't forget vcpu_put() in kvm_arch_vcpu_ioctl_set_sregs()
      KVM: PPC: Book3S PR: Fix svcpu copying with preemption enabled
      KVM: PPC: Book3S HV: Drop locks before reading guest memory
      kvm: x86: remove efer_reload entry in kvm_vcpu_stat
      KVM: x86: AMD Processor Topology Information
      x86/kvm/vmx: do not use vm-exit instruction length for fast MMIO when running nested
      kvm: embed vcpu id to dentry of vcpu anon inode
      kvm: Map PFN-type memory regions as writable (if possible)
      x86/kvm: Make it compile on 32bit and with HYPYERVISOR_GUEST=n
      KVM: arm/arm64: Fixup userspace irqchip static key optimization
      KVM: arm/arm64: Fix userspace_irqchip_in_use counting
      KVM: arm/arm64: Fix incorrect timer_is_pending logic
      MAINTAINERS: update KVM/s390 maintainers
      MAINTAINERS: add Halil as additional vfio-ccw maintainer
      MAINTAINERS: add David as a reviewer for KVM/s390
      ...

commit 6167ec5c9145cdf493722dfd80a5d48bafc4a18a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:14 2018 +0000

    arm64: KVM: Report SMCCC_ARCH_WORKAROUND_1 BP hardening support
    
    A new feature of SMCCC 1.1 is that it offers firmware-based CPU
    workarounds. In particular, SMCCC_ARCH_WORKAROUND_1 provides
    BP hardening for CVE-2017-5715.
    
    If the host has some mitigation for this issue, report that
    we deal with it using SMCCC_ARCH_WORKAROUND_1, as we apply the
    host workaround on every guest exit.
    
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4485ae8e98de..a73f63aca68e 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -415,4 +415,10 @@ static inline void kvm_arm_vhe_guest_exit(void)
 {
 	local_daif_restore(DAIF_PROCCTX_NOIRQ);
 }
+
+static inline bool kvm_arm_harden_branch_predictor(void)
+{
+	return cpus_have_const_cap(ARM64_HARDEN_BRANCH_PREDICTOR);
+}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 0067df413bd9d7e9ee3a78ece1e1a93535378862
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:05 2018 +0000

    KVM: arm64: Handle RAS SErrors from EL2 on guest exit
    
    We expect to have firmware-first handling of RAS SErrors, with errors
    notified via an APEI method. For systems without firmware-first, add
    some minimal handling to KVM.
    
    There are two ways KVM can take an SError due to a guest, either may be a
    RAS error: we exit the guest due to an SError routed to EL2 by HCR_EL2.AMO,
    or we take an SError from EL2 when we unmask PSTATE.A from __guest_exit.
    
    The current SError from EL2 code unmasks SError and tries to fence any
    pending SError into a single instruction window. It then leaves SError
    unmasked.
    
    With the v8.2 RAS Extensions we may take an SError for a 'corrected'
    error, but KVM is only able to handle SError from EL2 if they occur
    during this single instruction window...
    
    The RAS Extensions give us a new instruction to synchronise and
    consume SErrors. The RAS Extensions document (ARM DDI0587),
    '2.4.1 ESB and Unrecoverable errors' describes ESB as synchronising
    SError interrupts generated by 'instructions, translation table walks,
    hardware updates to the translation tables, and instruction fetches on
    the same PE'. This makes ESB equivalent to KVMs existing
    'dsb, mrs-daifclr, isb' sequence.
    
    Use the alternatives to synchronise and consume any SError using ESB
    instead of unmasking and taking the SError. Set ARM_EXIT_WITH_SERROR_BIT
    in the exit_code so that we can restart the vcpu if it turns out this
    SError has no impact on the vcpu.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index abcfd164e690..4485ae8e98de 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -90,6 +90,7 @@ struct kvm_vcpu_fault_info {
 	u32 esr_el2;		/* Hyp Syndrom Register */
 	u64 far_el2;		/* Hyp Fault Address Register */
 	u64 hpfar_el2;		/* Hyp IPA Fault Address Register */
+	u64 disr_el1;		/* Deferred [SError] Status Register */
 };
 
 /*

commit 3368bd809764d3ef0810e16c1e1531fec32e8d8e
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:04 2018 +0000

    KVM: arm64: Handle RAS SErrors from EL1 on guest exit
    
    We expect to have firmware-first handling of RAS SErrors, with errors
    notified via an APEI method. For systems without firmware-first, add
    some minimal handling to KVM.
    
    There are two ways KVM can take an SError due to a guest, either may be a
    RAS error: we exit the guest due to an SError routed to EL2 by HCR_EL2.AMO,
    or we take an SError from EL2 when we unmask PSTATE.A from __guest_exit.
    
    For SError that interrupt a guest and are routed to EL2 the existing
    behaviour is to inject an impdef SError into the guest.
    
    Add code to handle RAS SError based on the ESR. For uncontained and
    uncategorized errors arm64_is_fatal_ras_serror() will panic(), these
    errors compromise the host too. All other error types are contained:
    For the fatal errors the vCPU can't make progress, so we inject a virtual
    SError. We ignore contained errors where we can make progress as if
    we're lucky, we may not hit them again.
    
    If only some of the CPUs support RAS the guest will see the cpufeature
    sanitised version of the id registers, but we may still take RAS SError
    on this CPU. Move the SError handling out of handle_exit() into a new
    handler that runs before we can be preempted. This allows us to use
    this_cpu_has_cap(), via arm64_is_ras_serror().
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 84fcb2a896a1..abcfd164e690 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -347,6 +347,8 @@ void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);
 
 int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		int exception_index);
+void handle_exit_early(struct kvm_vcpu *vcpu, struct kvm_run *run,
+		       int exception_index);
 
 int kvm_perf_init(void);
 int kvm_perf_teardown(void);

commit c773ae2b34760a1ae409614aa31cdded81a645a5
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:02 2018 +0000

    KVM: arm64: Save/Restore guest DISR_EL1
    
    If we deliver a virtual SError to the guest, the guest may defer it
    with an ESB instruction. The guest reads the deferred value via DISR_EL1,
    but the guests view of DISR_EL1 is re-mapped to VDISR_EL2 when HCR_EL2.AMO
    is set.
    
    Add the KVM code to save/restore VDISR_EL2, and make it accessible to
    userspace as DISR_EL1.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 3014b39b8fe2..84fcb2a896a1 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -121,6 +121,7 @@ enum vcpu_sysreg {
 	PAR_EL1,	/* Physical Address Register */
 	MDSCR_EL1,	/* Monitor Debug System Control Register */
 	MDCCINT_EL1,	/* Monitor Debug Comms Channel Interrupt Enable Reg */
+	DISR_EL1,	/* Deferred Interrupt Status Register */
 
 	/* Performance Monitors Registers */
 	PMCR_EL0,	/* Control Register */

commit 4715c14bc136687bb79d12e24aafdc0f38786eb7
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:01 2018 +0000

    KVM: arm64: Set an impdef ESR for Virtual-SError using VSESR_EL2.
    
    Prior to v8.2's RAS Extensions, the HCR_EL2.VSE 'virtual SError' feature
    generated an SError with an implementation defined ESR_EL1.ISS, because we
    had no mechanism to specify the ESR value.
    
    On Juno this generates an all-zero ESR, the most significant bit 'ISV'
    is clear indicating the remainder of the ISS field is invalid.
    
    With the RAS Extensions we have a mechanism to specify this value, and the
    most significant bit has a new meaning: 'IDS - Implementation Defined
    Syndrome'. An all-zero SError ESR now means: 'RAS error: Uncategorized'
    instead of 'no valid ISS'.
    
    Add KVM support for the VSESR_EL2 register to specify an ESR value when
    HCR_EL2.VSE generates a virtual SError. Change kvm_inject_vabt() to
    specify an implementation-defined value.
    
    We only need to restore the VSESR_EL2 value when HCR_EL2.VSE is set, KVM
    save/restores this bit during __{,de}activate_traps() and hardware clears the
    bit once the guest has consumed the virtual-SError.
    
    Future patches may add an API (or KVM CAP) to pend a virtual SError with
    a specified ESR.
    
    Cc: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index dcdd08edf5a5..3014b39b8fe2 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -280,6 +280,9 @@ struct kvm_vcpu_arch {
 
 	/* Detect first run of a vcpu */
 	bool has_run_once;
+
+	/* Virtual SError ESR to restore when HCR_EL2.VSE is set */
+	u64 vsesr_el2;
 };
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)

commit 4f5abad9e826bd579b0661efa32682d9c9bc3fa8
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:00 2018 +0000

    KVM: arm/arm64: mask/unmask daif around VHE guests
    
    Non-VHE systems take an exception to EL2 in order to world-switch into the
    guest. When returning from the guest KVM implicitly restores the DAIF
    flags when it returns to the kernel at EL1.
    
    With VHE none of this exception-level jumping happens, so KVMs
    world-switch code is exposed to the host kernel's DAIF values, and KVM
    spills the guest-exit DAIF values back into the host kernel.
    On entry to a guest we have Debug and SError exceptions unmasked, KVM
    has switched VBAR but isn't prepared to handle these. On guest exit
    Debug exceptions are left disabled once we return to the host and will
    stay this way until we enter user space.
    
    Add a helper to mask/unmask DAIF around VHE guests. The unmask can only
    happen after the hosts VBAR value has been synchronised by the isb in
    __vhe_hyp_call (via kvm_call_hyp()). Masking could be as late as
    setting KVMs VBAR value, but is kept here for symmetry.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7ee72b402907..dcdd08edf5a5 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -25,6 +25,7 @@
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <asm/cpufeature.h>
+#include <asm/daifflags.h>
 #include <asm/fpsimd.h>
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
@@ -398,4 +399,13 @@ static inline void kvm_fpsimd_flush_cpu_state(void)
 		sve_flush_cpu_state();
 }
 
+static inline void kvm_arm_vhe_guest_enter(void)
+{
+	local_daif_mask();
+}
+
+static inline void kvm_arm_vhe_guest_exit(void)
+{
+	local_daif_restore(DAIF_PROCCTX_NOIRQ);
+}
 #endif /* __ARM64_KVM_HOST_H__ */

commit c97e166e54b662717d20ec2e36761758d2b6a7c2
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:05 2018 +0000

    KVM: arm64: Change hyp_panic()s dependency on tpidr_el2
    
    Make tpidr_el2 a cpu-offset for per-cpu variables in the same way the
    host uses tpidr_el1. This lets tpidr_el{1,2} have the same value, and
    on VHE they can be the same register.
    
    KVM calls hyp_panic() when anything unexpected happens. This may occur
    while a guest owns the EL1 registers. KVM stashes the vcpu pointer in
    tpidr_el2, which it uses to find the host context in order to restore
    the host EL1 registers before parachuting into the host's panic().
    
    The host context is a struct kvm_cpu_context allocated in the per-cpu
    area, and mapped to hyp. Given the per-cpu offset for this CPU, this is
    easy to find. Change hyp_panic() to take a pointer to the
    struct kvm_cpu_context. Wrap these calls with an asm function that
    retrieves the struct kvm_cpu_context from the host's per-cpu area.
    
    Copy the per-cpu offset from the hosts tpidr_el1 into tpidr_el2 during
    kvm init. (Later patches will make this unnecessary for VHE hosts)
    
    We print out the vcpu pointer as part of the panic message. Add a back
    reference to the 'running vcpu' in the host cpu context to preserve this.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ea6cb5b24258..7ee72b402907 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -192,6 +192,8 @@ struct kvm_cpu_context {
 		u64 sys_regs[NR_SYS_REGS];
 		u32 copro[NR_COPRO_REGS];
 	};
+
+	struct kvm_vcpu *__hyp_running_vcpu;
 };
 
 typedef struct kvm_cpu_context kvm_cpu_context_t;

commit 61bbe38027334780964e4b2658f722ed0b3cb2f9
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Fri Oct 27 19:57:51 2017 +0200

    KVM: arm/arm64: Avoid work when userspace iqchips are not used
    
    We currently check if the VM has a userspace irqchip in several places
    along the critical path, and if so, we do some work which is only
    required for having an irqchip in userspace.  This is unfortunate, as we
    could avoid doing any work entirely, if we didn't have to support
    irqchip in userspace.
    
    Realizing the userspace irqchip on ARM is mostly a developer or hobby
    feature, and is unlikely to be used in servers or other scenarios where
    performance is a priority, we can use a refcounted static key to only
    check the irqchip configuration when we have at least one VM that uses
    an irqchip in userspace.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ea6cb5b24258..e7218cf7df2a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -47,6 +47,8 @@
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 #define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 
+DECLARE_STATIC_KEY_FALSE(userspace_irqchip_in_use);
+
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext);

commit 696673d192f52c2c5a702224ee21f005318a844b
Author: Alex Benne <alex.bennee@linaro.org>
Date:   Thu Nov 16 15:39:19 2017 +0000

    KVM: arm/arm64: debug: Introduce helper for single-step
    
    After emulating instructions we may want return to user-space to handle
    single-step debugging. Introduce a helper function, which, if
    single-step is enabled, sets the run structure for return and returns
    true.
    
    Signed-off-by: Alex Benne <alex.bennee@linaro.org>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 674912d7a571..ea6cb5b24258 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -370,6 +370,7 @@ void kvm_arm_init_debug(void);
 void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
+bool kvm_arm_handle_step_debug(struct kvm_vcpu *vcpu, struct kvm_run *run);
 int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr);
 int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,

commit 17eed27b02da88560b4592390952b9a71042ab8b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:16 2017 +0000

    arm64/sve: KVM: Prevent guests from using SVE
    
    Until KVM has full SVE support, guests must not be allowed to
    execute SVE instructions.
    
    This patch enables the necessary traps, and also ensures that the
    traps are disabled again on exit from the guest so that the host
    can still use SVE if it wants to.
    
    On guest exit, high bits of the SVE Zn registers may have been
    clobbered as a side-effect the execution of FPSIMD instructions in
    the guest.  The existing KVM host FPSIMD restore code is not
    sufficient to restore these bits, so this patch explicitly marks
    the CPU as not containing cached vector state for any task, thus
    forcing a reload on the next return to userspace.  This is an
    interim measure, in advance of adding full SVE awareness to KVM.
    
    This marking of cached vector state in the CPU as invalid is done
    using __this_cpu_write(fpsimd_last_state, NULL) in fpsimd.c.  Due
    to the repeated use of this rather obscure operation, it makes
    sense to factor it out as a separate helper with a clearer name.
    This patch factors it out as fpsimd_flush_cpu_state(), and ports
    all callers to use it.
    
    As a side effect of this refactoring, a this_cpu_write() in
    fpsimd_cpu_pm_notifier() is changed to __this_cpu_write().  This
    should be fine, since cpu_pm_enter() is supposed to be called only
    with interrupts disabled.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benne <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e923b58606e2..674912d7a571 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -25,6 +25,7 @@
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <asm/cpufeature.h>
+#include <asm/fpsimd.h>
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
@@ -384,4 +385,14 @@ static inline void __cpu_init_stage2(void)
 		  "PARange is %d bits, unsupported configuration!", parange);
 }
 
+/*
+ * All host FP/SIMD state is restored on guest exit, so nothing needs
+ * doing here except in the SVE case:
+*/
+static inline void kvm_fpsimd_flush_cpu_state(void)
+{
+	if (system_supports_sve())
+		sve_flush_cpu_state();
+}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit fb1522e099f0c69f36655af233a64e3f55941f5b
Author: Jrme Glisse <jglisse@redhat.com>
Date:   Thu Aug 31 17:17:37 2017 -0400

    KVM: update to new mmu_notifier semantic v2
    
    Calls to mmu_notifier_invalidate_page() were replaced by calls to
    mmu_notifier_invalidate_range() and are now bracketed by calls to
    mmu_notifier_invalidate_range_start()/end()
    
    Remove now useless invalidate_page callback.
    
    Changed since v1 (Linus Torvalds)
        - remove now useless kvm_arch_mmu_notifier_invalidate_page()
    
    Signed-off-by: Jrme Glisse <jglisse@redhat.com>
    Tested-by: Mike Galbraith <efault@gmx.de>
    Tested-by: Adam Borowski <kilobyte@angband.pl>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krm <rkrcmar@redhat.com>
    Cc: kvm@vger.kernel.org
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index d68630007b14..e923b58606e2 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -326,12 +326,6 @@ void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
 int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 
-/* We do not have shadow page tables, hence the empty hooks */
-static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
-							 unsigned long address)
-{
-}
-
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 void kvm_arm_halt_guest(struct kvm *kvm);

commit 04a7ea04d508b925e7f829305b358157d58b4f82
Merge: c853354429f7 d38338e396ee
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 30 12:38:26 2017 +0200

    Merge tag 'kvmarm-for-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM updates for 4.13
    
    - vcpu request overhaul
    - allow timer and PMU to have their interrupt number
      selected from userspace
    - workaround for Cavium erratum 30115
    - handling of memory poisonning
    - the usual crop of fixes and cleanups
    
    Conflicts:
            arch/s390/include/asm/kvm_host.h

commit 325f9c649c8a4e447e4d3babacc7a60b75012d5d
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:59 2017 +0200

    KVM: arm/arm64: use vcpu requests for irq injection
    
    Don't use request-less VCPU kicks when injecting IRQs, as a VCPU
    kick meant to trigger the interrupt injection could be sent while
    the VCPU is outside guest mode, which means no IPI is sent, and
    after it has called kvm_vgic_flush_hwstate(), meaning it won't see
    the updated GIC state until its next exit some time later for some
    other reason.  The receiving VCPU only needs to check this request
    in VCPU RUN to handle it.  By checking it, if it's pending, a
    memory barrier will be issued that ensures all state is visible.
    See "Ensuring Requests Are Seen" of
    Documentation/virtual/kvm/vcpu-requests.rst
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9bd0d1040de9..0c4fd1f46e10 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -43,6 +43,7 @@
 
 #define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_IRQ_PENDING	KVM_ARCH_REQ(1)
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit 7b244e2be654d90d77800015d23395357dbc82ba
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:58 2017 +0200

    KVM: arm/arm64: change exit request to sleep request
    
    A request called EXIT is too generic. All requests are meant to cause
    exits, but different requests have different flags. Let's not make
    it difficult to decide if the EXIT request is correct for some case
    by just always providing unique requests for each case. This patch
    changes EXIT to SLEEP, because that's what the request is asking the
    VCPU to do.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0ff991c9c66e..9bd0d1040de9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -41,7 +41,7 @@
 
 #define KVM_VCPU_MAX_FEATURES 4
 
-#define KVM_REQ_VCPU_EXIT \
+#define KVM_REQ_SLEEP \
 	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 
 int __attribute_const__ kvm_target_cpu(void);

commit 2387149eade25f32dcf1398811b3d0293181d005
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:51 2017 +0200

    KVM: improve arch vcpu request defining
    
    Marc Zyngier suggested that we define the arch specific VCPU request
    base, rather than requiring each arch to remember to start from 8.
    That suggestion, along with Radim Krcmar's recent VCPU request flag
    addition, snowballed into defining something of an arch VCPU request
    defining API.
    
    No functional change.
    
    (Looks like x86 is running out of arch VCPU request bits.  Maybe
     someday we'll need to extend to 64.)
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 32cbe8a3bb0d..0ff991c9c66e 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -41,7 +41,8 @@
 
 #define KVM_VCPU_MAX_FEATURES 4
 
-#define KVM_REQ_VCPU_EXIT	(8 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_VCPU_EXIT \
+	KVM_ARCH_REQ_FLAGS(0, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit abd7229626b9339e378a8cfcdebe0c0943b06a7f
Author: Christoffer Dall <cdall@linaro.org>
Date:   Sat May 6 20:01:24 2017 +0200

    KVM: arm/arm64: Simplify active_change_prepare and plug race
    
    We don't need to stop a specific VCPU when changing the active state,
    because private IRQs can only be modified by a running VCPU for the
    VCPU itself and it is therefore already stopped.
    
    However, it is also possible for two VCPUs to be modifying the active
    state of SPIs at the same time, which can cause the thread being stuck
    in the loop that checks other VCPU threads for a potentially very long
    time, or to modify the active state of a running VCPU.  Fix this by
    serializing all accesses to setting and clearing the active state of
    interrupts using the KVM mutex.
    
    Reported-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 5e19165c5fa8..32cbe8a3bb0d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -333,8 +333,6 @@ struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 void kvm_arm_halt_guest(struct kvm *kvm);
 void kvm_arm_resume_guest(struct kvm *kvm);
-void kvm_arm_halt_vcpu(struct kvm_vcpu *vcpu);
-void kvm_arm_resume_vcpu(struct kvm_vcpu *vcpu);
 
 u64 __kvm_call_hyp(void *hypfn, ...);
 #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)

commit 63a1e1c95e60e798fa09ab3c536fb555aa5bbf2b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 16 15:18:05 2017 +0100

    arm64/cpufeature: don't use mutex in bringup path
    
    Currently, cpus_set_cap() calls static_branch_enable_cpuslocked(), which
    must take the jump_label mutex.
    
    We call cpus_set_cap() in the secondary bringup path, from the idle
    thread where interrupts are disabled. Taking a mutex in this path "is a
    NONO" regardless of whether it's contended, and something we must avoid.
    We didn't spot this until recently, as ___might_sleep() won't warn for
    this case until all CPUs have been brought up.
    
    This patch avoids taking the mutex in the secondary bringup path. The
    poking of static keys is deferred until enable_cpu_capabilities(), which
    runs in a suitable context on the boot CPU. To account for the static
    keys being set later, cpus_have_const_cap() is updated to use another
    static key to check whether the const cap keys have been initialised,
    falling back to the caps bitmap until this is the case.
    
    This means that users of cpus_have_const_cap() gain should only gain a
    single additional NOP in the fast path once the const caps are
    initialised, but should always see the current cap value.
    
    The hyp code should never dereference the caps array, since the caps are
    initialized before we run the module initcall to initialise hyp. A check
    is added to the hyp init code to document this requirement.
    
    This change will sidestep a number of issues when the upcoming hotplug
    locking rework is merged.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Marc Zyniger <marc.zyngier@arm.com>
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 5e19165c5fa8..1f252a95bc02 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -24,6 +24,7 @@
 
 #include <linux/types.h>
 #include <linux/kvm_types.h>
+#include <asm/cpufeature.h>
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
@@ -355,9 +356,12 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 				       unsigned long vector_ptr)
 {
 	/*
-	 * Call initialization code, and switch to the full blown
-	 * HYP code.
+	 * Call initialization code, and switch to the full blown HYP code.
+	 * If the cpucaps haven't been finalized yet, something has gone very
+	 * wrong, and hyp will crash and burn when it uses any
+	 * cpus_have_const_cap() wrapper.
 	 */
+	BUG_ON(!static_branch_likely(&arm64_const_caps_ready));
 	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr);
 }
 

commit c24a7be2110ddac2ab75abcded76c62dccb6b1f0
Merge: 70f3aac964ae 1edb632133ef
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 27 17:33:14 2017 +0200

    Merge tag 'kvm-arm-for-v4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM Changes for v4.12.
    
    Changes include:
     - Using the common sysreg definitions between KVM and arm64
     - Improved hyp-stub implementation with support for kexec and kdump on the 32-bit side
     - Proper PMU exception handling
     - Performance improvements of our GIC handling
     - Support for irqchip in userspace with in-kernel arch-timers and PMU support
     - A fix for a race condition in our PSCI code
    
    Conflicts:
            Documentation/virtual/kvm/api.txt
            include/uapi/linux/kvm.h

commit 7a97cec26b94c909f4cbad2dc3186af3e457a522
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Apr 27 14:33:43 2017 +0200

    KVM: mark requests that need synchronization
    
    kvm_make_all_requests() provides a synchronization that waits until all
    kicked VCPUs have acknowledged the kick.  This is important for
    KVM_REQ_MMU_RELOAD as it prevents freeing while lockless paging is
    underway.
    
    This patch adds the synchronization property into all requests that are
    currently being used with kvm_make_all_requests() in order to preserve
    the current behavior and only introduce a new framework.  Removing it
    from requests where it is not necessary is left for future patches.
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 1c9458a7ec92..d239ae166c4e 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -41,7 +41,7 @@
 
 #define KVM_VCPU_MAX_FEATURES 4
 
-#define KVM_REQ_VCPU_EXIT	(8 | KVM_REQUEST_NO_WAKEUP)
+#define KVM_REQ_VCPU_EXIT	(8 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit 930f7fd6da77ed9476a538345513460fd304aaf5
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:22 2017 +0200

    KVM: mark requests that do not need a wakeup
    
    Some operations must ensure that the guest is not running with stale
    data, but if the guest is halted, then the update can wait until another
    event happens.  kvm_make_all_requests() currently doesn't wake up, so we
    can mark all requests used with it.
    
    First 8 bits were arbitrarily reserved for request numbers.
    
    Most uses of requests have the request type as a constant, so a compiler
    will optimize the '&'.
    
    An alternative would be to have an inline function that would return
    whether the request needs a wake-up or not, but I like this one better
    even though it might produce worse assembly.
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 522e4f60976e..1c9458a7ec92 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -41,7 +41,7 @@
 
 #define KVM_VCPU_MAX_FEATURES 4
 
-#define KVM_REQ_VCPU_EXIT	8
+#define KVM_REQ_VCPU_EXIT	(8 | KVM_REQUEST_NO_WAKEUP)
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit 0fb265930dbcdb6833094198b8b1b71b1d3ed88d
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:59 2017 +0100

    arm/arm64: KVM: Use __hyp_reset_vectors() directly
    
    __cpu_reset_hyp_mode doesn't need to be passed any argument now,
    as the hyp-stub implementations are self-contained, and is now
    reduced to just calling __hyp_reset_vectors(). Let's drop the
    wrapper and use the stub hypercall directly.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0355dd109956..578df18f66b7 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -362,12 +362,6 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr);
 }
 
-static inline void __cpu_reset_hyp_mode(unsigned long vector_ptr,
-					phys_addr_t phys_idmap_start)
-{
-	__hyp_reset_vectors();
-}
-
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}

commit 4adb1341c7ef68af54732ef11f69faedffa6acb7
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:43 2017 +0100

    arm64: KVM: Convert __cpu_reset_hyp_mode to using __hyp_reset_vectors
    
    We are now able to use the hyp stub to reset HYP mode. Time to
    kiss __kvm_hyp_reset goodbye, and use __hyp_reset_vectors.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e7705e7bb07b..0355dd109956 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -362,11 +362,10 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr);
 }
 
-void __kvm_hyp_teardown(void);
 static inline void __cpu_reset_hyp_mode(unsigned long vector_ptr,
 					phys_addr_t phys_idmap_start)
 {
-	kvm_call_hyp(__kvm_hyp_teardown, phys_idmap_start);
+	__hyp_reset_vectors();
 }
 
 static inline void kvm_arch_hardware_unsetup(void) {}

commit 4b4357e02523ec63ad853f927f5d93a25101a1d2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Mar 31 13:53:23 2017 +0200

    kvm: make KVM_COALESCED_MMIO_PAGE_OFFSET public
    
    Its value has never changed; we might as well make it part of the ABI instead
    of using the return value of KVM_CHECK_EXTENSION(KVM_CAP_COALESCED_MMIO).
    
    Because PPC does not always make MMIO available, the code has to be made
    dependent on CONFIG_KVM_MMIO rather than KVM_COALESCED_MMIO_PAGE_OFFSET.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e7705e7bb07b..522e4f60976e 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -31,7 +31,6 @@
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 
 #define KVM_USER_MEM_SLOTS 512
-#define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #define KVM_HALT_POLL_NS_DEFAULT 500000
 
 #include <kvm/arm_vgic.h>

commit 955a3fc6d2a1c11d6d00bce4f3816100ce0530cf
Author: Linu Cherian <linu.cherian@cavium.com>
Date:   Wed Mar 8 11:38:35 2017 +0530

    KVM: arm64: Increase number of user memslots to 512
    
    Having only 32 memslots is a real constraint for the maximum
    number of PCI devices that can be assigned to a single guest.
    Assuming each PCI device/virtual function having two memory BAR
    regions, we could assign only 15 devices/virtual functions to a
    guest.
    
    Hence increase KVM_USER_MEM_SLOTS to 512 as done in other archs like
    powerpc.
    
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Linu Cherian <linu.cherian@cavium.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 6ac17ee887c9..e7705e7bb07b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -30,7 +30,7 @@
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 
-#define KVM_USER_MEM_SLOTS 32
+#define KVM_USER_MEM_SLOTS 512
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #define KVM_HALT_POLL_NS_DEFAULT 500000
 

commit 3e92f94a3b8e925a6dd7ec88a5794b2084b5fb65
Author: Linu Cherian <linu.cherian@cavium.com>
Date:   Wed Mar 8 11:38:34 2017 +0530

    KVM: arm/arm64: Remove KVM_PRIVATE_MEM_SLOTS definition that are unused
    
    arm/arm64 architecture doesnt use private memslots, hence removing
    KVM_PRIVATE_MEM_SLOTS macro definition.
    
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Linu Cherian <linu.cherian@cavium.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f21fd3894370..6ac17ee887c9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -31,7 +31,6 @@
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 
 #define KVM_USER_MEM_SLOTS 32
-#define KVM_PRIVATE_MEM_SLOTS 4
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
 #define KVM_HALT_POLL_NS_DEFAULT 500000
 

commit fd7e9a88348472521d999434ee02f25735c7dadf
Merge: 5066e4a34081 dd0fd8bca185
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 18:22:53 2017 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "4.11 is going to be a relatively large release for KVM, with a little
      over 200 commits and noteworthy changes for most architectures.
    
      ARM:
       - GICv3 save/restore
       - cache flushing fixes
       - working MSI injection for GICv3 ITS
       - physical timer emulation
    
      MIPS:
       - various improvements under the hood
       - support for SMP guests
       - a large rewrite of MMU emulation. KVM MIPS can now use MMU
         notifiers to support copy-on-write, KSM, idle page tracking,
         swapping, ballooning and everything else. KVM_CAP_READONLY_MEM is
         also supported, so that writes to some memory regions can be
         treated as MMIO. The new MMU also paves the way for hardware
         virtualization support.
    
      PPC:
       - support for POWER9 using the radix-tree MMU for host and guest
       - resizable hashed page table
       - bugfixes.
    
      s390:
       - expose more features to the guest
       - more SIMD extensions
       - instruction execution protection
       - ESOP2
    
      x86:
       - improved hashing in the MMU
       - faster PageLRU tracking for Intel CPUs without EPT A/D bits
       - some refactoring of nested VMX entry/exit code, preparing for live
         migration support of nested hypervisors
       - expose yet another AVX512 CPUID bit
       - host-to-guest PTP support
       - refactoring of interrupt injection, with some optimizations thrown
         in and some duct tape removed.
       - remove lazy FPU handling
       - optimizations of user-mode exits
       - optimizations of vcpu_is_preempted() for KVM guests
    
      generic:
       - alternative signaling mechanism that doesn't pound on
         tsk->sighand->siglock"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (195 commits)
      x86/kvm: Provide optimized version of vcpu_is_preempted() for x86-64
      x86/paravirt: Change vcp_is_preempted() arg type to long
      KVM: VMX: use correct vmcs_read/write for guest segment selector/base
      x86/kvm/vmx: Defer TR reload after VM exit
      x86/asm/64: Drop __cacheline_aligned from struct x86_hw_tss
      x86/kvm/vmx: Simplify segment_base()
      x86/kvm/vmx: Get rid of segment_base() on 64-bit kernels
      x86/kvm/vmx: Don't fetch the TSS base from the GDT
      x86/asm: Define the kernel TSS limit in a macro
      kvm: fix page struct leak in handle_vmon
      KVM: PPC: Book3S HV: Disable HPT resizing on POWER9 for now
      KVM: Return an error code only as a constant in kvm_get_dirty_log()
      KVM: Return an error code only as a constant in kvm_get_dirty_log_protect()
      KVM: Return directly after a failed copy_from_user() in kvm_vm_compat_ioctl()
      KVM: x86: remove code for lazy FPU handling
      KVM: race-free exit from KVM_RUN without POSIX signals
      KVM: PPC: Book3S HV: Turn "KVM guest htab" message into a debug message
      KVM: PPC: Book3S PR: Ratelimit copy data failure error messages
      KVM: Support vCPU-based gfn->hva cache
      KVM: use separate generations for each address space
      ...

commit 90de943a430028ee389b22bf4a7ae5867c32ce0c
Author: Jintack Lim <jintack@cs.columbia.edu>
Date:   Fri Feb 3 10:20:00 2017 -0500

    KVM: arm/arm64: Move cntvoff to each timer context
    
    Make cntvoff per each timer context. This is helpful to abstract kvm
    timer functions to work with timer context without considering timer
    types (e.g. physical timer or virtual timer).
    
    This also would pave the way for ever doing adjustments of the cntvoff
    on a per-CPU basis if that should ever make sense.
    
    Signed-off-by: Jintack Lim <jintack@cs.columbia.edu>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e5050388e062..4a758cba1262 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -70,9 +70,6 @@ struct kvm_arch {
 
 	/* Interrupt controller */
 	struct vgic_dist	vgic;
-
-	/* Timer */
-	struct arch_timer_kvm	timer;
 };
 
 #define KVM_NR_MEM_OBJS     40

commit f85279b4bd481a1a0697c1d2a8a5f15de216b120
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 22 11:35:43 2016 +0100

    arm64: KVM: Save/restore the host SPE state when entering/leaving a VM
    
    The SPE buffer is virtually addressed, using the page tables of the CPU
    MMU. Unusually, this means that the EL0/1 page table may be live whilst
    we're executing at EL2 on non-VHE configurations. When VHE is in use,
    we can use the same property to profile the guest behind its back.
    
    This patch adds the relevant disabling and flushing code to KVM so that
    the host can make use of SPE without corrupting guest memory, and any
    attempts by a guest to use SPE will result in a trap.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alex Benne <alex.bennee@linaro.org>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e5050388e062..443b387021f2 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -229,7 +229,12 @@ struct kvm_vcpu_arch {
 
 	/* Pointer to host CPU context */
 	kvm_cpu_context_t *host_cpu_context;
-	struct kvm_guest_debug_arch host_debug_state;
+	struct {
+		/* {Break,watch}point registers */
+		struct kvm_guest_debug_arch regs;
+		/* Statistical profiling extension */
+		u64 pmscr_el1;
+	} host_debug_state;
 
 	/* VGIC state */
 	struct vgic_cpu vgic_cpu;

commit 94d0e5980d6791b9f98a9b6c586c1f7cb76b2178
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Oct 18 18:37:49 2016 +0100

    arm/arm64: KVM: Perform local TLB invalidation when multiplexing vcpus on a single CPU
    
    Architecturally, TLBs are private to the (physical) CPU they're
    associated with. But when multiple vcpus from the same VM are
    being multiplexed on the same CPU, the TLBs are not private
    to the vcpus (and are actually shared across the VMID).
    
    Let's consider the following scenario:
    
    - vcpu-0 maps PA to VA
    - vcpu-1 maps PA' to VA
    
    If run on the same physical CPU, vcpu-1 can hit TLB entries generated
    by vcpu-0 accesses, and access the wrong physical page.
    
    The solution to this is to keep a per-VM map of which vcpu ran last
    on each given physical CPU, and invalidate local TLBs when switching
    to a different vcpu from the same VM.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index bd94e6766759..e5050388e062 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -62,6 +62,9 @@ struct kvm_arch {
 	/* VTTBR value associated with above pgd and vmid */
 	u64    vttbr;
 
+	/* The last vcpu id that ran on each physical CPU */
+	int __percpu *last_vcpu_ran;
+
 	/* The maximum number of vCPUs depends on the used GIC model */
 	int max_vcpus;
 

commit 8a7e75d47b68193339f8727cf4503271d0a0b1d0
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Tue Aug 2 14:03:22 2016 +1000

    KVM: Add provisioning for ulong vm stats and u64 vcpu stats
    
    vms and vcpus have statistics associated with them which can be viewed
    within the debugfs. Currently it is assumed within the vcpu_stat_get() and
    vm_stat_get() functions that all of these statistics are represented as
    u32s, however the next patch adds some u64 vcpu statistics.
    
    Change all vcpu statistics to u64 and modify vcpu_stat_get() accordingly.
    Since vcpu statistics are per vcpu, they will only be updated by a single
    vcpu at a time so this shouldn't present a problem on 32-bit machines
    which can't atomically increment 64-bit numbers. However vm statistics
    could potentially be updated by multiple vcpus from that vm at a time.
    To avoid the overhead of atomics make all vm statistics ulong such that
    they are 64-bit on 64-bit systems where they can be atomically incremented
    and are 32-bit on 32-bit systems which may not be able to atomically
    increment 64-bit numbers. Modify vm_stat_get() to expect ulongs.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Reviewed-by: David Matlack <dmatlack@google.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 3eda975837d0..bd94e6766759 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -290,15 +290,15 @@ struct kvm_vcpu_arch {
 #endif
 
 struct kvm_vm_stat {
-	u32 remote_tlb_flush;
+	ulong remote_tlb_flush;
 };
 
 struct kvm_vcpu_stat {
-	u32 halt_successful_poll;
-	u32 halt_attempted_poll;
-	u32 halt_poll_invalid;
-	u32 halt_wakeup;
-	u32 hvc_exit_stat;
+	u64 halt_successful_poll;
+	u64 halt_attempted_poll;
+	u64 halt_poll_invalid;
+	u64 halt_wakeup;
+	u64 hvc_exit_stat;
 	u64 wfe_exit_stat;
 	u64 wfi_exit_stat;
 	u64 mmio_exit_user;

commit b46f01ce4dcfdce636588fe2ef5035724c77f266
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Jul 15 12:43:25 2016 +0100

    KVM: arm/arm64: Extend arch CAP checks to allow per-VM capabilities
    
    KVM capabilities can be a per-VM property, though ARM/ARM64 currently
    does not pass on the VM pointer to the architecture specific
    capability handlers.
    Add a "struct kvm*" parameter to those function to later allow proper
    per-VM capability reporting.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Eric Auger <eric.auger@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Tested-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 69d5cc2d2e17..3eda975837d0 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -47,7 +47,7 @@
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
-int kvm_arch_dev_ioctl_check_extension(long ext);
+int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
 struct kvm_arch {

commit e537ecd7efacaa7512e87ecb07c0c0335a902558
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:48 2016 +0100

    arm: KVM: Allow hyp teardown
    
    So far, KVM was getting in the way of kexec on 32bit (and the arm64
    kexec hackers couldn't be bothered to fix it on 32bit...).
    
    With simpler page tables, tearing KVM down becomes very easy, so
    let's just do it.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 6731d4e1c746..69d5cc2d2e17 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -359,7 +359,8 @@ static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 }
 
 void __kvm_hyp_teardown(void);
-static inline void __cpu_reset_hyp_mode(phys_addr_t phys_idmap_start)
+static inline void __cpu_reset_hyp_mode(unsigned long vector_ptr,
+					phys_addr_t phys_idmap_start)
 {
 	kvm_call_hyp(__kvm_hyp_teardown, phys_idmap_start);
 }

commit 12fda8123d74903d1f65fb006fe4964e23ede0d1
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:45 2016 +0100

    arm/arm64: KVM: Drop boot_pgd
    
    Since we now only have one set of page tables, the concept of
    boot_pgd is useless and can be removed. We still keep it as
    an element of the "extended idmap" thing.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 88462c307510..6731d4e1c746 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -347,8 +347,7 @@ int kvm_perf_teardown(void);
 
 struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
 
-static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
-				       phys_addr_t pgd_ptr,
+static inline void __cpu_init_hyp_mode(phys_addr_t pgd_ptr,
 				       unsigned long hyp_stack_ptr,
 				       unsigned long vector_ptr)
 {
@@ -360,8 +359,7 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 }
 
 void __kvm_hyp_teardown(void);
-static inline void __cpu_reset_hyp_mode(phys_addr_t boot_pgd_ptr,
-					phys_addr_t phys_idmap_start)
+static inline void __cpu_reset_hyp_mode(phys_addr_t phys_idmap_start)
 {
 	kvm_call_hyp(__kvm_hyp_teardown, phys_idmap_start);
 }

commit 3421e9d88d7ae70fbc8c903e44a5acace8ae2d29
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:44 2016 +0100

    arm64: KVM: Simplify HYP init/teardown
    
    Now that we only have the "merged page tables" case to deal with,
    there is a bunch of things we can simplify in the HYP code (both
    at init and teardown time).
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 49095fc4b482..88462c307510 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -48,7 +48,6 @@
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(long ext);
-unsigned long kvm_hyp_reset_entry(void);
 void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
 struct kvm_arch {
@@ -357,19 +356,14 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 	 * Call initialization code, and switch to the full blown
 	 * HYP code.
 	 */
-	__kvm_call_hyp((void *)boot_pgd_ptr, pgd_ptr,
-		       hyp_stack_ptr, vector_ptr);
+	__kvm_call_hyp((void *)pgd_ptr, hyp_stack_ptr, vector_ptr);
 }
 
+void __kvm_hyp_teardown(void);
 static inline void __cpu_reset_hyp_mode(phys_addr_t boot_pgd_ptr,
 					phys_addr_t phys_idmap_start)
 {
-	/*
-	 * Call reset code, and switch back to stub hyp vectors.
-	 * Uses __kvm_call_hyp() to avoid kaslr's kvm_ksym_ref() translation.
-	 */
-	__kvm_call_hyp((void *)kvm_hyp_reset_entry(),
-		       boot_pgd_ptr, phys_idmap_start);
+	kvm_call_hyp(__kvm_hyp_teardown, phys_idmap_start);
 }
 
 static inline void kvm_arch_hardware_unsetup(void) {}

commit e28e909c36bb5d6319953822d84df00fce7cbd18
Merge: dc03c0f9d12d fabc71286643
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 27 13:41:54 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull second batch of KVM updates from Radim Krm:
     "General:
    
       - move kvm_stat tool from QEMU repo into tools/kvm/kvm_stat (kvm_stat
         had nothing to do with QEMU in the first place -- the tool only
         interprets debugfs)
    
       - expose per-vm statistics in debugfs and support them in kvm_stat
         (KVM always collected per-vm statistics, but they were summarised
         into global statistics)
    
      x86:
    
       - fix dynamic APICv (VMX was improperly configured and a guest could
         access host's APIC MSRs, CVE-2016-4440)
    
       - minor fixes
    
      ARM changes from Christoffer Dall:
    
       - new vgic reimplementation of our horribly broken legacy vgic
         implementation.  The two implementations will live side-by-side
         (with the new being the configured default) for one kernel release
         and then we'll remove the legacy one.
    
       - fix for a non-critical issue with virtual abort injection to guests"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (70 commits)
      tools: kvm_stat: Add comments
      tools: kvm_stat: Introduce pid monitoring
      KVM: Create debugfs dir and stat files for each VM
      MAINTAINERS: Add kvm tools
      tools: kvm_stat: Powerpc related fixes
      tools: Add kvm_stat man page
      tools: Add kvm_stat vm monitor script
      kvm:vmx: more complete state update on APICv on/off
      KVM: SVM: Add more SVM_EXIT_REASONS
      KVM: Unify traced vector format
      svm: bitwise vs logical op typo
      KVM: arm/arm64: vgic-new: Synchronize changes to active state
      KVM: arm/arm64: vgic-new: enable build
      KVM: arm/arm64: vgic-new: implement mapped IRQ handling
      KVM: arm/arm64: vgic-new: Wire up irqfd injection
      KVM: arm/arm64: vgic-new: Add vgic_v2/v3_enable
      KVM: arm/arm64: vgic-new: vgic_init: implement map_resources
      KVM: arm/arm64: vgic-new: vgic_init: implement vgic_init
      KVM: arm/arm64: vgic-new: vgic_init: implement vgic_create
      KVM: arm/arm64: vgic-new: vgic_init: implement kvm_vgic_hyp_init
      ...

commit 44bcc922381e24c4f38dc5dfd8d34d60b2ede898
Merge: 9842df62004f 35a2d58588f0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 24 12:10:51 2016 +0200

    Merge tag 'kvm-arm-for-4-7-take2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into kvm-next
    
    KVM/ARM Changes for v4.7 take 2
    
    "The GIC is dead; Long live the GIC"
    
    This set of changes include the new vgic, which is a reimplementation of
    our horribly broken legacy vgic implementation.  The two implementations
    will live side-by-side (with the new being the configured default) for
    one kernel release and then we'll remove it.
    
    Also fixes a non-critical issue with virtual abort injection to guests.

commit 35a2d58588f0992627e74b447ccab21570544c86
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Fri May 20 15:25:28 2016 +0200

    KVM: arm/arm64: vgic-new: Synchronize changes to active state
    
    When modifying the active state of an interrupt via the MMIO interface,
    we should ensure that the write has the intended effect.
    
    If a guest sets an interrupt to active, but that interrupt is already
    flushed into a list register on a running VCPU, then that VCPU will
    write the active state back into the struct vgic_irq upon returning from
    the guest and syncing its state.  This is a non-benign race, because the
    guest can observe that an interrupt is not active, and it can have a
    reasonable expectations that other VCPUs will not ack any IRQs, and then
    set the state to active, and expect it to stay that way.  Currently we
    are not honoring this case.
    
    Thefore, change both the SACTIVE and CACTIVE mmio handlers to stop the
    world, change the irq state, potentially queue the irq if we're setting
    it to active, and then continue.
    
    We take this chance to slightly optimize these functions by not stopping
    the world when touching private interrupts where there is inherently no
    possible race.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index fa94f91812ab..38746d24466d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -329,6 +329,8 @@ struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 void kvm_arm_halt_guest(struct kvm *kvm);
 void kvm_arm_resume_guest(struct kvm *kvm);
+void kvm_arm_halt_vcpu(struct kvm_vcpu *vcpu);
+void kvm_arm_resume_vcpu(struct kvm_vcpu *vcpu);
 
 u64 __kvm_call_hyp(void *hypfn, ...);
 #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)

commit b13216cf6010ef482e69875e00711de7f9573c80
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Wed Apr 27 10:28:00 2016 +0100

    KVM: arm/arm64: Provide functionality to pause and resume a guest
    
    For some rare corner cases in our VGIC emulation later we have to stop
    the guest to make sure the VGIC state is consistent.
    Provide the necessary framework to pause and resume a guest.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4cd4196e94a9..fa94f91812ab 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -44,6 +44,8 @@
 
 #define KVM_VCPU_MAX_FEATURES 4
 
+#define KVM_REQ_VCPU_EXIT	8
+
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(long ext);
@@ -325,6 +327,8 @@ static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
+void kvm_arm_halt_guest(struct kvm *kvm);
+void kvm_arm_resume_guest(struct kvm *kvm);
 
 u64 __kvm_call_hyp(void *hypfn, ...);
 #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)

commit 7beaa24ba49717419e24d1f6321e8b3c265a719c
Merge: 07b75260ebc2 9842df62004f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 11:27:09 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small release overall.
    
      x86:
       - miscellaneous fixes
       - AVIC support (local APIC virtualization, AMD version)
    
      s390:
       - polling for interrupts after a VCPU goes to halted state is now
         enabled for s390
       - use hardware provided information about facility bits that do not
         need any hypervisor activity, and other fixes for cpu models and
         facilities
       - improve perf output
       - floating interrupt controller improvements.
    
      MIPS:
       - miscellaneous fixes
    
      PPC:
       - bugfixes only
    
      ARM:
       - 16K page size support
       - generic firmware probing layer for timer and GIC
    
      Christoffer Dall (KVM-ARM maintainer) says:
        "There are a few changes in this pull request touching things
         outside KVM, but they should all carry the necessary acks and it
         made the merge process much easier to do it this way."
    
      though actually the irqchip maintainers' acks didn't make it into the
      patches.  Marc Zyngier, who is both irqchip and KVM-ARM maintainer,
      later acked at http://mid.gmane.org/573351D1.4060303@arm.com ('more
      formally and for documentation purposes')"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (82 commits)
      KVM: MTRR: remove MSR 0x2f8
      KVM: x86: make hwapic_isr_update and hwapic_irr_update look the same
      svm: Manage vcpu load/unload when enable AVIC
      svm: Do not intercept CR8 when enable AVIC
      svm: Do not expose x2APIC when enable AVIC
      KVM: x86: Introducing kvm_x86_ops.apicv_post_state_restore
      svm: Add VMEXIT handlers for AVIC
      svm: Add interrupt injection via AVIC
      KVM: x86: Detect and Initialize AVIC support
      svm: Introduce new AVIC VMCB registers
      KVM: split kvm_vcpu_wake_up from kvm_vcpu_kick
      KVM: x86: Introducing kvm_x86_ops VCPU blocking/unblocking hooks
      KVM: x86: Introducing kvm_x86_ops VM init/destroy hooks
      KVM: x86: Rename kvm_apic_get_reg to kvm_lapic_get_reg
      KVM: x86: Misc LAPIC changes to expose helper functions
      KVM: shrink halt polling even more for invalid wakeups
      KVM: s390: set halt polling to 80 microseconds
      KVM: halt_polling: provide a way to qualify wakeups during poll
      KVM: PPC: Book3S HV: Re-enable XICS fast path for irqfd-generated interrupts
      kvm: Conditionally register IRQ bypass consumer
      ...

commit be092017b6ffbd013f481f915632db6aa9fc3ca3
Merge: fb6363e9f4ee e6d9a5254333
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 17:17:24 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
    
     - virt_to_page/page_address optimisations
    
     - support for NUMA systems described using device-tree
    
     - support for hibernate/suspend-to-disk
    
     - proper support for maxcpus= command line parameter
    
     - detection and graceful handling of AArch64-only CPUs
    
     - miscellaneous cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      arm64: do not enforce strict 16 byte alignment to stack pointer
      arm64: kernel: Fix incorrect brk randomization
      arm64: cpuinfo: Missing NULL terminator in compat_hwcap_str
      arm64: secondary_start_kernel: Remove unnecessary barrier
      arm64: Ensure pmd_present() returns false after pmd_mknotpresent()
      arm64: Replace hard-coded values in the pmd/pud_bad() macros
      arm64: Implement pmdp_set_access_flags() for hardware AF/DBM
      arm64: Fix typo in the pmdp_huge_get_and_clear() definition
      arm64: mm: remove unnecessary EXPORT_SYMBOL_GPL
      arm64: always use STRICT_MM_TYPECHECKS
      arm64: kvm: Fix kvm teardown for systems using the extended idmap
      arm64: kaslr: increase randomization granularity
      arm64: kconfig: drop CONFIG_RTC_LIB dependency
      arm64: make ARCH_SUPPORTS_DEBUG_PAGEALLOC depend on !HIBERNATION
      arm64: hibernate: Refuse to hibernate if the boot cpu is offline
      arm64: kernel: Add support for hibernate/suspend-to-disk
      PM / Hibernate: Call flush_icache_range() on pages restored in-place
      arm64: Add new asm macro copy_page
      arm64: Promote KERNEL_START/KERNEL_END definitions to a header file
      arm64: kernel: Include _AC definition in page.h
      ...

commit 3491caf2755e9f312666712510d80b00c81ff247
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri May 13 12:16:35 2016 +0200

    KVM: halt_polling: provide a way to qualify wakeups during poll
    
    Some wakeups should not be considered a sucessful poll. For example on
    s390 I/O interrupts are usually floating, which means that _ALL_ CPUs
    would be considered runnable - letting all vCPUs poll all the time for
    transactional like workload, even if one vCPU would be enough.
    This can result in huge CPU usage for large guests.
    This patch lets architectures provide a way to qualify wakeups if they
    should be considered a good/bad wakeups in regard to polls.
    
    For s390 the implementation will fence of halt polling for anything but
    known good, single vCPU events. The s390 implementation for floating
    interrupts does a wakeup for one vCPU, but the interrupt will be delivered
    by whatever CPU checks first for a pending interrupt. We prefer the
    woken up CPU by marking the poll of this CPU as "good" poll.
    This code will also mark several other wakeup reasons like IPI or
    expired timers as "good". This will of course also mark some events as
    not sucessful. As  KVM on z runs always as a 2nd level hypervisor,
    we prefer to not poll, unless we are really sure, though.
    
    This patch successfully limits the CPU usage for cases like uperf 1byte
    transactional ping pong workload or wakeup heavy workload like OLTP
    while still providing a proper speedup.
    
    This also introduced a new vcpu stat "halt_poll_no_tuning" that marks
    wakeups that are considered not good for polling.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Acked-by: Radim Krm <rkrcmar@redhat.com> (for an earlier version)
    Cc: David Matlack <dmatlack@google.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    [Rename config symbol. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f5c6bd2541ef..d49399d9890d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -293,6 +293,7 @@ struct kvm_vm_stat {
 struct kvm_vcpu_stat {
 	u32 halt_successful_poll;
 	u32 halt_attempted_poll;
+	u32 halt_poll_invalid;
 	u32 halt_wakeup;
 	u32 hvc_exit_stat;
 	u64 wfe_exit_stat;
@@ -357,6 +358,7 @@ static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
+static inline void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu) {}
 
 void kvm_arm_init_debug(void);
 void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);

commit c612505f860c6d4fac03924879982adcd042e239
Author: James Morse <james.morse@arm.com>
Date:   Fri Apr 29 18:27:03 2016 +0100

    arm64: kvm: Fix kvm teardown for systems using the extended idmap
    
    If memory is located above 1<<VA_BITS, kvm adds an extra level to its page
    tables, merging the runtime tables and boot tables that contain the idmap.
    This lets us avoid the trampoline dance during initialisation.
    
    This also means there is no trampoline page mapped, so
    __cpu_reset_hyp_mode() can't call __kvm_hyp_reset() in this page. The good
    news is the idmap is still mapped, so we don't need the trampoline page.
    The bad news is we can't call it directly as the idmap is above
    HYP_PAGE_OFFSET, so its address is masked by kvm_call_hyp.
    
    Add a function __extended_idmap_trampoline which will branch into
    __kvm_hyp_reset in the idmap, change kvm_hyp_reset_entry() to return
    this address if __kvm_cpu_uses_extended_idmap(). In this case
    __kvm_hyp_reset() will still switch to the boot tables (which are the
    merged tables that were already in use), and branch into the idmap (where
    it already was).
    
    This fixes boot failures on these systems, where we fail to execute the
    missing trampoline page when tearing down kvm in init_subsystems():
    [    2.508922] kvm [1]: 8-bit VMID
    [    2.512057] kvm [1]: Hyp mode initialized successfully
    [    2.517242] kvm [1]: interrupt-controller@e1140000 IRQ13
    [    2.522622] kvm [1]: timer IRQ3
    [    2.525783] Kernel panic - not syncing: HYP panic:
    [    2.525783] PS:200003c9 PC:0000007ffffff820 ESR:86000005
    [    2.525783] FAR:0000007ffffff820 HPFAR:00000000003ffff0 PAR:0000000000000000
    [    2.525783] VCPU:          (null)
    [    2.525783]
    [    2.547667] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G        W       4.6.0-rc5+ #1
    [    2.555137] Hardware name: Default string Default string/Default string, BIOS ROD0084E 09/03/2015
    [    2.563994] Call trace:
    [    2.566432] [<ffffff80080888d0>] dump_backtrace+0x0/0x240
    [    2.571818] [<ffffff8008088b24>] show_stack+0x14/0x20
    [    2.576858] [<ffffff80083423ac>] dump_stack+0x94/0xb8
    [    2.581899] [<ffffff8008152130>] panic+0x10c/0x250
    [    2.586677] [<ffffff8008152024>] panic+0x0/0x250
    [    2.591281] SMP: stopping secondary CPUs
    [    3.649692] SMP: failed to stop secondary CPUs 0-2,4-7
    [    3.654818] Kernel Offset: disabled
    [    3.658293] Memory Limit: none
    [    3.661337] ---[ end Kernel panic - not syncing: HYP panic:
    [    3.661337] PS:200003c9 PC:0000007ffffff820 ESR:86000005
    [    3.661337] FAR:0000007ffffff820 HPFAR:00000000003ffff0 PAR:0000000000000000
    [    3.661337] VCPU:          (null)
    [    3.661337]
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b6f194d9f6b2..88a34670ddef 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -46,7 +46,8 @@
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(long ext);
-phys_addr_t kvm_hyp_reset_entry(void);
+unsigned long kvm_hyp_reset_entry(void);
+void __extended_idmap_trampoline(phys_addr_t boot_pgd, phys_addr_t idmap_start);
 
 struct kvm_arch {
 	/* The VMID generation used for the virt. memory system */

commit 67f6919766620e7ea7aab11a6a3470dc7b451359
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 27 17:47:05 2016 +0100

    arm64: kvm: allows kvm cpu hotplug
    
    The current kvm implementation on arm64 does cpu-specific initialization
    at system boot, and has no way to gracefully shutdown a core in terms of
    kvm. This prevents kexec from rebooting the system at EL2.
    
    This patch adds a cpu tear-down function and also puts an existing cpu-init
    code into a separate function, kvm_arch_hardware_disable() and
    kvm_arch_hardware_enable() respectively.
    We don't need the arm64 specific cpu hotplug hook any more.
    
    Since this patch modifies common code between arm and arm64, one stub
    definition, __cpu_reset_hyp_mode(), is added on arm side to avoid
    compilation errors.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [Rebase, added separate VHE init/exit path, changed resets use of
     kvm_call_hyp() to the __version, en/disabled hardware in init_subsystems(),
     added icache maintenance to __kvm_hyp_reset() and removed lr restore, removed
     guest-enter after teardown handling]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b7e82a795ac9..b6f194d9f6b2 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -46,6 +46,7 @@
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(long ext);
+phys_addr_t kvm_hyp_reset_entry(void);
 
 struct kvm_arch {
 	/* The VMID generation used for the virt. memory system */
@@ -352,7 +353,17 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 		       hyp_stack_ptr, vector_ptr);
 }
 
-static inline void kvm_arch_hardware_disable(void) {}
+static inline void __cpu_reset_hyp_mode(phys_addr_t boot_pgd_ptr,
+					phys_addr_t phys_idmap_start)
+{
+	/*
+	 * Call reset code, and switch back to stub hyp vectors.
+	 * Uses __kvm_call_hyp() to avoid kaslr's kvm_ksym_ref() translation.
+	 */
+	__kvm_call_hyp((void *)kvm_hyp_reset_entry(),
+		       boot_pgd_ptr, phys_idmap_start);
+}
+
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}

commit 4a6cd3ba6fc4993f1805613098d7b032a7aa937b
Merge: 3d8e15dd6de6 06a71a24bae5
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Fri Apr 8 14:17:27 2016 +0200

    Merge tag 'kvm-arm-for-4.6-rc4' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm
    
    KVM/ARM Fixes for v4.6-rc4
    
    Addresses:
     - Wrong indentation in the PMU code from the merge window
     - A long-time bug occuring with running ntpd on the host, candidate for stable
     - Properly handle (and warn about) the unsupported configuration of running on
       systems with less than 40 bits of PA space
     - More fixes to the PM and hotplug notifier stuff from the merge window

commit 6141570c36f0c937d5deff20d9cf08cbd8d8ed48
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 5 16:11:47 2016 +0100

    arm64: KVM: Warn when PARange is less than 40 bits
    
    We always thought that 40bits of PA range would be the minimum people
    would actually build. Anything less is terrifyingly small.
    
    Turns out that we were both right and wrong. Nobody has ever built
    such a system, but the ARM Foundation Model has a PARange set to 36bits.
    Just because we can. Oh well. Now, the KVM API explicitely says that
    we offer a 40bit PA space to the VM, so we shouldn't run KVM on
    the Foundation Model at all.
    
    That being said, this patch offers a less agressive alternative, and
    loudly warns about the configuration being unsupported. You'll still
    be able to run VMs (at your own risks, though).
    
    This is just a workaround until we have a proper userspace API where
    we report the PARange to userspace.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 227ed475dbd3..4cd4196e94a9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -370,11 +370,12 @@ int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,
 int kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,
 			       struct kvm_device_attr *attr);
 
-/* #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__) */
-
 static inline void __cpu_init_stage2(void)
 {
-	kvm_call_hyp(__init_stage2_translation);
+	u32 parange = kvm_call_hyp(__init_stage2_translation);
+
+	WARN_ONCE(parange < 40,
+		  "PARange is %d bits, unsupported configuration!", parange);
 }
 
 #endif /* __ARM64_KVM_HOST_H__ */

commit b8cfadfcefdc8c306ca2c0b1bdbdd4e01f0155e3
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Thu Mar 24 16:01:16 2016 +0000

    arm64: perf: Move PMU register related defines to asm/perf_event.h
    
    To use the ARMv8 PMU related register defines from the KVM code, we move
    the relevant definitions to asm/perf_event.h header file and rename them
    with prefix ARMV8_PMU_. This allows us to get rid of kvm_perf_event.h.
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 227ed475dbd3..b7e82a795ac9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -27,7 +27,6 @@
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
-#include <asm/kvm_perf_event.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit ef769e320863a186e489e3f66ed8df60487fe9bf
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Wed Feb 24 09:52:41 2016 -0800

    arm64: Fix misspellings in comments.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e465b6d17dbe..f057404cf711 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -99,8 +99,8 @@ enum vcpu_sysreg {
 	TTBR1_EL1,	/* Translation Table Base Register 1 */
 	TCR_EL1,	/* Translation Control Register */
 	ESR_EL1,	/* Exception Syndrome Register */
-	AFSR0_EL1,	/* Auxilary Fault Status Register 0 */
-	AFSR1_EL1,	/* Auxilary Fault Status Register 1 */
+	AFSR0_EL1,	/* Auxiliary Fault Status Register 0 */
+	AFSR1_EL1,	/* Auxiliary Fault Status Register 1 */
 	FAR_EL1,	/* Fault Address Register */
 	MAIR_EL1,	/* Memory Attribute Indirection Register */
 	VBAR_EL1,	/* Vector Base Address Register */

commit 22b39ca3f22ac46a42731447aadc0a5109078241
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Mar 1 13:12:44 2016 +0000

    arm64: KVM: Move kvm_call_hyp back to its original localtion
    
    In order to reduce the risk of a bad merge, let's move the new
    kvm_call_hyp back to its original location in the file. This has
    zero impact from a code point of view.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e3d67ff8798b..e465b6d17dbe 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -308,6 +308,8 @@ struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 
 u64 __kvm_call_hyp(void *hypfn, ...);
+#define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)
+
 void force_vm_exit(const cpumask_t *mask);
 void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);
 
@@ -343,6 +345,4 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
 
-#define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)
-
 #endif /* __ARM64_KVM_HOST_H__ */

commit bb0c70bcca6ba3c84afc2da7426f3b923bbe6825
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Mon Jan 11 21:35:32 2016 +0800

    arm64: KVM: Add a new vcpu device control group for PMUv3
    
    To configure the virtual PMUv3 overflow interrupt number, we use the
    vcpu kvm_device ioctl, encapsulating the KVM_ARM_VCPU_PMU_V3_IRQ
    attribute within the KVM_ARM_VCPU_PMU_V3_CTRL group.
    
    After configuring the PMUv3, call the vcpu ioctl with attribute
    KVM_ARM_VCPU_PMU_V3_INIT to initialize the PMUv3.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Acked-by: Peter Maydell <peter.maydell@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b02ef0828f22..71fa6fe9d54a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -361,6 +361,12 @@ void kvm_arm_init_debug(void);
 void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
+int kvm_arm_vcpu_arch_set_attr(struct kvm_vcpu *vcpu,
+			       struct kvm_device_attr *attr);
+int kvm_arm_vcpu_arch_get_attr(struct kvm_vcpu *vcpu,
+			       struct kvm_device_attr *attr);
+int kvm_arm_vcpu_arch_has_attr(struct kvm_vcpu *vcpu,
+			       struct kvm_device_attr *attr);
 
 /* #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__) */
 

commit 808e738142e7086ef793ebf9797099c392894e65
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Mon Jan 11 22:46:15 2016 +0800

    arm64: KVM: Add a new feature bit for PMUv3
    
    To support guest PMUv3, use one bit of the VCPU INIT feature array.
    Initialize the PMU when initialzing the vcpu with that bit and PMU
    overflow interrupt set.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Acked-by: Peter Maydell <peter.maydell@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a819c6debce4..b02ef0828f22 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -42,7 +42,7 @@
 
 #define KVM_MAX_VCPUS VGIC_V3_MAX_CPUS
 
-#define KVM_VCPU_MAX_FEATURES 3
+#define KVM_VCPU_MAX_FEATURES 4
 
 int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit d692b8ad6ec4814ddd9a37ce5c9c9d971e741088
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Sep 8 15:15:56 2015 +0800

    arm64: KVM: Add access handler for PMUSERENR register
    
    This register resets as unknown in 64bit mode while it resets as zero
    in 32bit mode. Here we choose to reset it as zero for consistency.
    
    PMUSERENR_EL0 holds some bits which decide whether PMU registers can be
    accessed from EL0. Add some check helpers to handle the access from EL0.
    
    When these bits are zero, only reading PMUSERENR will trap to EL2 and
    writing PMUSERENR or reading/writing other PMU registers will trap to
    EL1 other than EL2 when HCR.TGE==0. To current KVM configuration
    (HCR.TGE==0) there is no way to get these traps. Here we write 0xf to
    physical PMUSERENR register on VM entry, so that it will trap PMU access
    from EL0 to EL2. Within the register access handler we check the real
    value of guest PMUSERENR register to decide whether this access is
    allowed. If not allowed, return false to inject UND to guest.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4001e85b4818..a819c6debce4 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -130,6 +130,7 @@ enum vcpu_sysreg {
 	PMINTENSET_EL1,	/* Interrupt Enable Set Register */
 	PMOVSSET_EL0,	/* Overflow Flag Status Set Register */
 	PMSWINC_EL0,	/* Software Increment Register */
+	PMUSERENR_EL0,	/* User Enable Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit 7a0adc7064b88609e2917446af8789fac1d4fdd1
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Sep 8 15:49:39 2015 +0800

    arm64: KVM: Add access handler for PMSWINC register
    
    Add access handler which emulates writing and reading PMSWINC
    register and add support for creating software increment event.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 6c61a2bda6de..4001e85b4818 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -129,6 +129,7 @@ enum vcpu_sysreg {
 	PMCNTENSET_EL0,	/* Count Enable Set Register */
 	PMINTENSET_EL1,	/* Interrupt Enable Set Register */
 	PMOVSSET_EL0,	/* Overflow Flag Status Set Register */
+	PMSWINC_EL0,	/* Software Increment Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit 76d883c4e6401b98ea26d40c437ff62719a517ad
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Sep 8 15:03:26 2015 +0800

    arm64: KVM: Add access handler for PMOVSSET and PMOVSCLR register
    
    Since the reset value of PMOVSSET and PMOVSCLR is UNKNOWN, use
    reset_unknown for its reset handler. Add a handler to emulate writing
    PMOVSSET or PMOVSCLR register.
    
    When writing non-zero value to PMOVSSET, the counter and its interrupt
    is enabled, kick this vcpu to sync PMU interrupt.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index da59f44f0c84..6c61a2bda6de 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -128,6 +128,7 @@ enum vcpu_sysreg {
 	PMCCFILTR_EL0,	/* Cycle Count Filter Register */
 	PMCNTENSET_EL0,	/* Count Enable Set Register */
 	PMINTENSET_EL1,	/* Interrupt Enable Set Register */
+	PMOVSSET_EL0,	/* Overflow Flag Status Set Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit 9db52c78cd43c7fe69992cb7d57cffa991b36ced
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Sep 8 14:40:20 2015 +0800

    arm64: KVM: Add access handler for PMINTENSET and PMINTENCLR register
    
    Since the reset value of PMINTENSET and PMINTENCLR is UNKNOWN, use
    reset_unknown for its reset handler. Add a handler to emulate writing
    PMINTENSET or PMINTENCLR register.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 121182dd0947..da59f44f0c84 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -127,6 +127,7 @@ enum vcpu_sysreg {
 	PMEVTYPER30_EL0 = PMEVTYPER0_EL0 + 30,
 	PMCCFILTR_EL0,	/* Cycle Count Filter Register */
 	PMCNTENSET_EL0,	/* Count Enable Set Register */
+	PMINTENSET_EL1,	/* Interrupt Enable Set Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit 9feb21ac57d53003557ddc01f9aee496269996c7
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Feb 23 11:11:27 2016 +0800

    arm64: KVM: Add access handler for event type register
    
    These kind of registers include PMEVTYPERn, PMCCFILTR and PMXEVTYPER
    which is mapped to PMEVTYPERn or PMCCFILTR.
    
    The access handler translates all aarch32 register offsets to aarch64
    ones and uses vcpu_sys_reg() to access their values to avoid taking care
    of big endian.
    
    When writing to these registers, create a perf_event for the selected
    event type.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 993793b422aa..121182dd0947 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -123,6 +123,9 @@ enum vcpu_sysreg {
 	PMEVCNTR0_EL0,	/* Event Counter Register (0-30) */
 	PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
 	PMCCNTR_EL0,	/* Cycle Counter Register */
+	PMEVTYPER0_EL0,	/* Event Type Register (0-30) */
+	PMEVTYPER30_EL0 = PMEVTYPER0_EL0 + 30,
+	PMCCFILTR_EL0,	/* Cycle Count Filter Register */
 	PMCNTENSET_EL0,	/* Count Enable Set Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */

commit 96b0eebcc6a14e3bdb9ff0e7176fbfc225bdde94
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Sep 8 12:26:13 2015 +0800

    arm64: KVM: Add access handler for PMCNTENSET and PMCNTENCLR register
    
    Since the reset value of PMCNTENSET and PMCNTENCLR is UNKNOWN, use
    reset_unknown for its reset handler. Add a handler to emulate writing
    PMCNTENSET or PMCNTENCLR register.
    
    When writing to PMCNTENSET, call perf_event_enable to enable the perf
    event. When writing to PMCNTENCLR, call perf_event_disable to disable
    the perf event.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4ae27fe34240..993793b422aa 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -123,6 +123,7 @@ enum vcpu_sysreg {
 	PMEVCNTR0_EL0,	/* Event Counter Register (0-30) */
 	PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
 	PMCCNTR_EL0,	/* Cycle Counter Register */
+	PMCNTENSET_EL0,	/* Count Enable Set Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit 051ff581ce70e822729e9474941f3c206cbf7436
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Tue Dec 8 15:29:06 2015 +0800

    arm64: KVM: Add access handler for event counter register
    
    These kind of registers include PMEVCNTRn, PMCCNTR and PMXEVCNTR which
    is mapped to PMEVCNTRn.
    
    The access handler translates all aarch32 register offsets to aarch64
    ones and uses vcpu_sys_reg() to access their values to avoid taking care
    of big endian.
    
    When reading these registers, return the sum of register value and the
    value perf event counts.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 57a2d8f76c2f..4ae27fe34240 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -120,6 +120,9 @@ enum vcpu_sysreg {
 	/* Performance Monitors Registers */
 	PMCR_EL0,	/* Control Register */
 	PMSELR_EL0,	/* Event Counter Selection Register */
+	PMEVCNTR0_EL0,	/* Event Counter Register (0-30) */
+	PMEVCNTR30_EL0 = PMEVCNTR0_EL0 + 30,
+	PMCCNTR_EL0,	/* Cycle Counter Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit 3965c3ce751ab5a97618a2818eec4497576f4654
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Mon Aug 31 17:20:22 2015 +0800

    arm64: KVM: Add access handler for PMSELR register
    
    Since the reset value of PMSELR_EL0 is UNKNOWN, use reset_unknown for
    its reset handler. When reading PMSELR, return the PMSELR.SEL field to
    guest.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 5def605b4525..57a2d8f76c2f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -119,6 +119,7 @@ enum vcpu_sysreg {
 
 	/* Performance Monitors Registers */
 	PMCR_EL0,	/* Control Register */
+	PMSELR_EL0,	/* Event Counter Selection Register */
 
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */

commit ab9468340d2bcc2a837b8b536fa819a0fc05a32e
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Thu Jun 18 16:01:53 2015 +0800

    arm64: KVM: Add access handler for PMCR register
    
    Add reset handler which gets host value of PMCR_EL0 and make writable
    bits architecturally UNKNOWN except PMCR.E which is zero. Add an access
    handler for PMCR.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index fb57fdc6a433..5def605b4525 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -117,6 +117,9 @@ enum vcpu_sysreg {
 	MDSCR_EL1,	/* Monitor Debug System Control Register */
 	MDCCINT_EL1,	/* Monitor Debug Comms Channel Interrupt Enable Reg */
 
+	/* Performance Monitors Registers */
+	PMCR_EL0,	/* Control Register */
+
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */
 	IFSR32_EL2,	/* Instruction Fault Status Register */

commit 04fe472615d0216ec0bdd66d9f3f1812b642ada6
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Fri Sep 11 09:38:32 2015 +0800

    arm64: KVM: Define PMU data structure for each vcpu
    
    Here we plan to support virtual PMU for guest by full software
    emulation, so define some basic structs and functions preparing for
    futher steps. Define struct kvm_pmc for performance monitor counter and
    struct kvm_pmu for performance monitor unit for each vcpu. According to
    ARMv8 spec, the PMU contains at most 32(ARMV8_PMU_MAX_COUNTERS)
    counters.
    
    Since this only supports ARM64 (or PMUv3), add a separate config symbol
    for it.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 15851f52096b..fb57fdc6a433 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -38,6 +38,7 @@
 
 #include <kvm/arm_vgic.h>
 #include <kvm/arm_arch_timer.h>
+#include <kvm/arm_pmu.h>
 
 #define KVM_MAX_VCPUS VGIC_V3_MAX_CPUS
 
@@ -213,6 +214,7 @@ struct kvm_vcpu_arch {
 	/* VGIC state */
 	struct vgic_cpu vgic_cpu;
 	struct arch_timer_cpu timer_cpu;
+	struct kvm_pmu pmu;
 
 	/*
 	 * Anything that is not used directly from assembly code goes

commit ad8821377384a2dfe3eae17dcf287b437f10ae03
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Feb 29 11:25:04 2016 +0000

    arm64: KVM: Add temporary kvm_perf_event.h
    
    In order to merge the KVM/ARM PMU patches without creating a
    conflict mess, let's have a temporary include file that won't
    conflict with anything. Subsequent patches will clean that up.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 31fe7d6f32de..15851f52096b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -27,6 +27,7 @@
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
+#include <asm/kvm_perf_event.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 

commit 21a4179ce0a127ea96c66d37ac571ac4ceeb992f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Feb 22 10:57:30 2016 +0000

    arm64: KVM: Move __cpu_init_stage2 after kvm_call_hyp
    
    In order to ease the merge with the rest of the arm64 tree, move the
    definition of __cpu_init_stage2() after what will be the new kvm_call_hyp.
    Hopefully the resolution of the merge conflict will be obvious.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 43688d93c756..31fe7d6f32de 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -333,11 +333,6 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 		     hyp_stack_ptr, vector_ptr);
 }
 
-static inline void __cpu_init_stage2(void)
-{
-	kvm_call_hyp(__init_stage2_translation);
-}
-
 static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
@@ -349,4 +344,11 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
 
+/* #define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__) */
+
+static inline void __cpu_init_stage2(void)
+{
+	kvm_call_hyp(__init_stage2_translation);
+}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 3a3604bc5eb4ae21ec95b13fdd15959e8f70c434
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jan 29 13:19:45 2015 +0000

    arm64: KVM: Switch to C-based stage2 init
    
    There is no real need to leave the stage2 initialization as part
    of the early HYP bootstrap, and we can easily postpone it to
    the point where we can safely run C code.
    
    This will help VHE, which doesn't need any of this bootstrap.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index fe86cf9f288b..43688d93c756 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -25,6 +25,7 @@
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <asm/kvm.h>
+#include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
@@ -334,6 +335,7 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 
 static inline void __cpu_init_stage2(void)
 {
+	kvm_call_hyp(__init_stage2_translation);
 }
 
 static inline void kvm_arch_hardware_disable(void) {}

commit 35a2491a624af1fa7ab6990639f5246cd5f12592
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Feb 1 17:54:35 2016 +0000

    arm/arm64: KVM: Add hook for C-based stage2 init
    
    As we're about to move the stage2 init to C code, introduce some
    C hooks that will later be populated with arch-specific implementations.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 689d4c95e12f..fe86cf9f288b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -332,6 +332,10 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 		     hyp_stack_ptr, vector_ptr);
 }
 
+static inline void __cpu_init_stage2(void)
+{
+}
+
 static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}

commit a0bf9776cd0be4490d4675d4108e13379849fc7f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:39 2016 +0100

    arm64: kvm: deal with kernel symbols outside of linear mapping
    
    KVM on arm64 uses a fixed offset between the linear mapping at EL1 and
    the HYP mapping at EL2. Before we can move the kernel virtual mapping
    out of the linear mapping, we have to make sure that references to kernel
    symbols that are accessed via the HYP mapping are translated to their
    linear equivalent.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 689d4c95e12f..e3d67ff8798b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -307,7 +307,7 @@ static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 
-u64 kvm_call_hyp(void *hypfn, ...);
+u64 __kvm_call_hyp(void *hypfn, ...);
 void force_vm_exit(const cpumask_t *mask);
 void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);
 
@@ -328,8 +328,8 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 	 * Call initialization code, and switch to the full blown
 	 * HYP code.
 	 */
-	kvm_call_hyp((void *)boot_pgd_ptr, pgd_ptr,
-		     hyp_stack_ptr, vector_ptr);
+	__kvm_call_hyp((void *)boot_pgd_ptr, pgd_ptr,
+		       hyp_stack_ptr, vector_ptr);
 }
 
 static inline void kvm_arch_hardware_disable(void) {}
@@ -343,4 +343,6 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
 
+#define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 9d8415d6c148a16b6d906a96f0596851d7e4d607
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sun Oct 25 19:57:11 2015 +0000

    arm64: KVM: Turn system register numbers to an enum
    
    Having the system register numbers as #defines has been a pain
    since day one, as the ordering is pretty fragile, and moving
    things around leads to renumbering and epic conflict resolutions.
    
    Now that we're mostly acessing the sysreg file in C, an enum is
    a much better type to use, and we can clean things up a bit.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 19504aa12459..689d4c95e12f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -25,7 +25,6 @@
 #include <linux/types.h>
 #include <linux/kvm_types.h>
 #include <asm/kvm.h>
-#include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
@@ -85,6 +84,86 @@ struct kvm_vcpu_fault_info {
 	u64 hpfar_el2;		/* Hyp IPA Fault Address Register */
 };
 
+/*
+ * 0 is reserved as an invalid value.
+ * Order should be kept in sync with the save/restore code.
+ */
+enum vcpu_sysreg {
+	__INVALID_SYSREG__,
+	MPIDR_EL1,	/* MultiProcessor Affinity Register */
+	CSSELR_EL1,	/* Cache Size Selection Register */
+	SCTLR_EL1,	/* System Control Register */
+	ACTLR_EL1,	/* Auxiliary Control Register */
+	CPACR_EL1,	/* Coprocessor Access Control */
+	TTBR0_EL1,	/* Translation Table Base Register 0 */
+	TTBR1_EL1,	/* Translation Table Base Register 1 */
+	TCR_EL1,	/* Translation Control Register */
+	ESR_EL1,	/* Exception Syndrome Register */
+	AFSR0_EL1,	/* Auxilary Fault Status Register 0 */
+	AFSR1_EL1,	/* Auxilary Fault Status Register 1 */
+	FAR_EL1,	/* Fault Address Register */
+	MAIR_EL1,	/* Memory Attribute Indirection Register */
+	VBAR_EL1,	/* Vector Base Address Register */
+	CONTEXTIDR_EL1,	/* Context ID Register */
+	TPIDR_EL0,	/* Thread ID, User R/W */
+	TPIDRRO_EL0,	/* Thread ID, User R/O */
+	TPIDR_EL1,	/* Thread ID, Privileged */
+	AMAIR_EL1,	/* Aux Memory Attribute Indirection Register */
+	CNTKCTL_EL1,	/* Timer Control Register (EL1) */
+	PAR_EL1,	/* Physical Address Register */
+	MDSCR_EL1,	/* Monitor Debug System Control Register */
+	MDCCINT_EL1,	/* Monitor Debug Comms Channel Interrupt Enable Reg */
+
+	/* 32bit specific registers. Keep them at the end of the range */
+	DACR32_EL2,	/* Domain Access Control Register */
+	IFSR32_EL2,	/* Instruction Fault Status Register */
+	FPEXC32_EL2,	/* Floating-Point Exception Control Register */
+	DBGVCR32_EL2,	/* Debug Vector Catch Register */
+
+	NR_SYS_REGS	/* Nothing after this line! */
+};
+
+/* 32bit mapping */
+#define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
+#define c0_CSSELR	(CSSELR_EL1 * 2)/* Cache Size Selection Register */
+#define c1_SCTLR	(SCTLR_EL1 * 2)	/* System Control Register */
+#define c1_ACTLR	(ACTLR_EL1 * 2)	/* Auxiliary Control Register */
+#define c1_CPACR	(CPACR_EL1 * 2)	/* Coprocessor Access Control */
+#define c2_TTBR0	(TTBR0_EL1 * 2)	/* Translation Table Base Register 0 */
+#define c2_TTBR0_high	(c2_TTBR0 + 1)	/* TTBR0 top 32 bits */
+#define c2_TTBR1	(TTBR1_EL1 * 2)	/* Translation Table Base Register 1 */
+#define c2_TTBR1_high	(c2_TTBR1 + 1)	/* TTBR1 top 32 bits */
+#define c2_TTBCR	(TCR_EL1 * 2)	/* Translation Table Base Control R. */
+#define c3_DACR		(DACR32_EL2 * 2)/* Domain Access Control Register */
+#define c5_DFSR		(ESR_EL1 * 2)	/* Data Fault Status Register */
+#define c5_IFSR		(IFSR32_EL2 * 2)/* Instruction Fault Status Register */
+#define c5_ADFSR	(AFSR0_EL1 * 2)	/* Auxiliary Data Fault Status R */
+#define c5_AIFSR	(AFSR1_EL1 * 2)	/* Auxiliary Instr Fault Status R */
+#define c6_DFAR		(FAR_EL1 * 2)	/* Data Fault Address Register */
+#define c6_IFAR		(c6_DFAR + 1)	/* Instruction Fault Address Register */
+#define c7_PAR		(PAR_EL1 * 2)	/* Physical Address Register */
+#define c7_PAR_high	(c7_PAR + 1)	/* PAR top 32 bits */
+#define c10_PRRR	(MAIR_EL1 * 2)	/* Primary Region Remap Register */
+#define c10_NMRR	(c10_PRRR + 1)	/* Normal Memory Remap Register */
+#define c12_VBAR	(VBAR_EL1 * 2)	/* Vector Base Address Register */
+#define c13_CID		(CONTEXTIDR_EL1 * 2)	/* Context ID Register */
+#define c13_TID_URW	(TPIDR_EL0 * 2)	/* Thread ID, User R/W */
+#define c13_TID_URO	(TPIDRRO_EL0 * 2)/* Thread ID, User R/O */
+#define c13_TID_PRIV	(TPIDR_EL1 * 2)	/* Thread ID, Privileged */
+#define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
+#define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
+#define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
+
+#define cp14_DBGDSCRext	(MDSCR_EL1 * 2)
+#define cp14_DBGBCR0	(DBGBCR0_EL1 * 2)
+#define cp14_DBGBVR0	(DBGBVR0_EL1 * 2)
+#define cp14_DBGBXVR0	(cp14_DBGBVR0 + 1)
+#define cp14_DBGWCR0	(DBGWCR0_EL1 * 2)
+#define cp14_DBGWVR0	(DBGWVR0_EL1 * 2)
+#define cp14_DBGDCCINT	(MDCCINT_EL1 * 2)
+
+#define NR_COPRO_REGS	(NR_SYS_REGS * 2)
+
 struct kvm_cpu_context {
 	struct kvm_regs	gp_regs;
 	union {

commit b19e6892a90e7c9d15fde0a08516ec891a4e7d54
Author: Amit Tomar <amittomer25@gmail.com>
Date:   Thu Nov 26 10:09:43 2015 +0000

    KVM: arm/arm64: Count guest exit due to various reasons
    
    It would add guest exit statistics to debugfs, this can be helpful
    while measuring KVM performance.
    
      [ Renamed some of the field names - Christoffer ]
    
    Signed-off-by: Amit Singh Tomar <amittomer25@gmail.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a35ce7266aac..19504aa12459 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -197,6 +197,12 @@ struct kvm_vcpu_stat {
 	u32 halt_successful_poll;
 	u32 halt_attempted_poll;
 	u32 halt_wakeup;
+	u32 hvc_exit_stat;
+	u64 wfe_exit_stat;
+	u64 wfi_exit_stat;
+	u64 mmio_exit_user;
+	u64 mmio_exit_kernel;
+	u64 exits;
 };
 
 int kvm_vcpu_preferred_target(struct kvm_vcpu_init *init);

commit 3b92830ad41b2fe377e0765322e8aefd0ab8388d
Author: Eric Auger <eric.auger@linaro.org>
Date:   Fri Sep 25 23:41:17 2015 +0200

    KVM: arm/arm64: implement kvm_arm_[halt,resume]_guest
    
    We introduce kvm_arm_halt_guest and resume functions. They
    will be used for IRQ forward state change.
    
    Halt is synchronous and prevents the guest from being re-entered.
    We use the same mechanism put in place for PSCI former pause,
    now renamed power_off. A new flag is introduced in arch vcpu state,
    pause, only meant to be used by those functions.
    
    Signed-off-by: Eric Auger <eric.auger@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4b4157b3b983..a35ce7266aac 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -152,6 +152,9 @@ struct kvm_vcpu_arch {
 	/* vcpu power-off state */
 	bool power_off;
 
+	/* Don't run the guest (internal implementation need) */
+	bool pause;
+
 	/* IO related fields */
 	struct kvm_decode mmio_decode;
 

commit 3781528e3045e7c9cc7c4846e0f675b1f353655f
Author: Eric Auger <eric.auger@linaro.org>
Date:   Fri Sep 25 23:41:14 2015 +0200

    KVM: arm/arm64: rename pause into power_off
    
    The kvm_vcpu_arch pause field is renamed into power_off to prepare
    for the introduction of a new pause field. Also vcpu_pause is renamed
    into vcpu_sleep since we will sleep until both power_off and pause are
    false.
    
    Signed-off-by: Eric Auger <eric.auger@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ed039688c221..4b4157b3b983 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -149,8 +149,8 @@ struct kvm_vcpu_arch {
 		u32	mdscr_el1;
 	} guest_debug_preserved;
 
-	/* Don't run the guest */
-	bool pause;
+	/* vcpu power-off state */
+	bool power_off;
 
 	/* IO related fields */
 	struct kvm_decode mmio_decode;

commit d35268da66870d733ae763fd7f9b06a1f63f395e
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Aug 25 19:48:21 2015 +0200

    arm/arm64: KVM: arch_timer: Only schedule soft timer on vcpu_block
    
    We currently schedule a soft timer every time we exit the guest if the
    timer did not expire while running the guest.  This is really not
    necessary, because the only work we do in the timer work function is to
    kick the vcpu.
    
    Kicking the vcpu does two things:
    (1) If the vpcu thread is on a waitqueue, make it runnable and remove it
    from the waitqueue.
    (2) If the vcpu is running on a different physical CPU from the one
    doing the kick, it sends a reschedule IPI.
    
    The second case cannot happen, because the soft timer is only ever
    scheduled when the vcpu is not running.  The first case is only relevant
    when the vcpu thread is on a waitqueue, which is only the case when the
    vcpu thread has called kvm_vcpu_block().
    
    Therefore, we only need to make sure a timer is scheduled for
    kvm_vcpu_block(), which we do by encapsulating all calls to
    kvm_vcpu_block() with kvm_timer_{un}schedule calls.
    
    Additionally, we only schedule a soft timer if the timer is enabled and
    unmasked, since it is useless otherwise.
    
    Note that theoretically userspace can use the SET_ONE_REG interface to
    change registers that should cause the timer to fire, even if the vcpu
    is blocked without a scheduled timer, but this case was not supported
    before this patch and we leave it for future work for now.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e4f4d65f7d2b..ed039688c221 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -255,7 +255,4 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
 
-static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
-static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
-
 #endif /* __ARM64_KVM_HOST_H__ */

commit 3217f7c25bca66eed9b07f0b8bfd1937169b0736
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Aug 27 16:41:15 2015 +0200

    KVM: Add kvm_arch_vcpu_{un}blocking callbacks
    
    Some times it is useful for architecture implementations of KVM to know
    when the VCPU thread is about to block or when it comes back from
    blocking (arm/arm64 needs to know this to properly implement timers, for
    example).
    
    Therefore provide a generic architecture callback function in line with
    what we do elsewhere for KVM generic-arch interactions.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ed039688c221..e4f4d65f7d2b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -255,4 +255,7 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
 
+static inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu) {}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 920552b213e3dc832a874b4e7ba29ecddbab31bc
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri Sep 18 12:34:53 2015 +0200

    KVM: disable halt_poll_ns as default for s390x
    
    We observed some performance degradation on s390x with dynamic
    halt polling. Until we can provide a proper fix, let's enable
    halt_poll_ns as default only for supported architectures.
    
    Architectures are now free to set their own halt_poll_ns
    default value.
    
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4562459456a6..ed039688c221 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -33,6 +33,7 @@
 #define KVM_USER_MEM_SLOTS 32
 #define KVM_PRIVATE_MEM_SLOTS 4
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
+#define KVM_HALT_POLL_NS_DEFAULT 500000
 
 #include <kvm/arm_vgic.h>
 #include <kvm/arm_arch_timer.h>

commit efe4d36a75d4dfd1a9c161b2bbf9b90beb4d9648
Merge: 9bf9fde2c98b ef748917b529
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Sep 17 16:51:59 2015 +0200

    Merge tag 'kvm-arm-for-4.3-rc2-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into kvm-master
    
    Second set of KVM/ARM changes for 4.3-rc2
    
    - Workaround for a Cortex-A57 erratum
    - Bug fix for the debugging infrastructure
    - Fix for 32bit guests with more than 4GB of address space
      on a 32bit host
    - A number of fixes for the (unusual) case when we don't use
      the in-kernel GIC emulation
    - Removal of ThumbEE handling on arm64, since these have been
      dropped from the architecture before anyone actually ever
      built a CPU
    - Remove the KVM_ARM_MAX_VCPUS limitation which has become
      fairly pointless

commit ef748917b529847277f07c98c55e1c0ce416449f
Author: Ming Lei <ming.lei@canonical.com>
Date:   Wed Sep 2 14:31:21 2015 +0800

    arm/arm64: KVM: Remove 'config KVM_ARM_MAX_VCPUS'
    
    This patch removes config option of KVM_ARM_MAX_VCPUS,
    and like other ARCHs, just choose the maximum allowed
    value from hardware, and follows the reasons:
    
    1) from distribution view, the option has to be
    defined as the max allowed value because it need to
    meet all kinds of virtulization applications and
    need to support most of SoCs;
    
    2) using a bigger value doesn't introduce extra memory
    consumption, and the help text in Kconfig isn't accurate
    because kvm_vpu structure isn't allocated until request
    of creating VCPU is sent from QEMU;
    
    3) the main effect is that the field of vcpus[] in 'struct kvm'
    becomes a bit bigger(sizeof(void *) per vcpu) and need more cache
    lines to hold the structure, but 'struct kvm' is one generic struct,
    and it has worked well on other ARCHs already in this way. Also,
    the world switch frequecy is often low, for example, it is ~2000
    when running kernel building load in VM from APM xgene KVM host,
    so the effect is very small, and the difference can't be observed
    in my test at all.
    
    Cc: Dann Frazier <dann.frazier@canonical.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 415938dc45cf..3fb58ea944eb 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -30,12 +30,6 @@
 
 #define __KVM_HAVE_ARCH_INTC_INITIALIZED
 
-#if defined(CONFIG_KVM_ARM_MAX_VCPUS)
-#define KVM_MAX_VCPUS CONFIG_KVM_ARM_MAX_VCPUS
-#else
-#define KVM_MAX_VCPUS 0
-#endif
-
 #define KVM_USER_MEM_SLOTS 32
 #define KVM_PRIVATE_MEM_SLOTS 4
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1
@@ -43,6 +37,8 @@
 #include <kvm/arm_vgic.h>
 #include <kvm/arm_arch_timer.h>
 
+#define KVM_MAX_VCPUS VGIC_V3_MAX_CPUS
+
 #define KVM_VCPU_MAX_FEATURES 3
 
 int __attribute_const__ kvm_target_cpu(void);

commit 62bea5bff486644ecf363fe8a1a2f6f32c614a49
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 15 18:27:57 2015 +0200

    KVM: add halt_attempted_poll to VCPU stats
    
    This new statistic can help diagnosing VCPUs that, for any reason,
    trigger bad behavior of halt_poll_ns autotuning.
    
    For example, say halt_poll_ns = 480000, and wakeups are spaced exactly
    like 479us, 481us, 479us, 481us. Then KVM always fails polling and wastes
    10+20+40+80+160+320+480 = 1110 microseconds out of every
    479+481+479+481+479+481+479 = 3359 microseconds. The VCPU then
    is consuming about 30% more CPU than it would use without
    polling.  This would show as an abnormally high number of
    attempted polling compared to the successful polls.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com<
    Reviewed-by: David Matlack <dmatlack@google.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 415938dc45cf..486594583cc6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -195,6 +195,7 @@ struct kvm_vm_stat {
 
 struct kvm_vcpu_stat {
 	u32 halt_successful_poll;
+	u32 halt_attempted_poll;
 	u32 halt_wakeup;
 };
 

commit 48f8bd57756573b08520629a2413227fcdf058f5
Author: Vladimir Murzin <Vladimir.Murzin@arm.com>
Date:   Tue Jul 28 10:42:23 2015 +0100

    arm64: KVM: remove remaining reference to vgic_sr_vectors
    
    Since commit 8a14849 (arm64: KVM: Switch vgic save/restore to
    alternative_insn) vgic_sr_vectors is not used anymore, so remove
    remaining leftovers and kill the structure.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 409217f48456..415938dc45cf 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -246,11 +246,6 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 		     hyp_stack_ptr, vector_ptr);
 }
 
-struct vgic_sr_vectors {
-	void	*save_vgic;
-	void	*restore_vgic;
-};
-
 static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}

commit 834bf88726f0f11ddc7ff9679fc9458654c01a12
Author: Alex Benne <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:30:02 2015 +0100

    KVM: arm64: enable KVM_CAP_SET_GUEST_DEBUG
    
    Finally advertise the KVM capability for SET_GUEST_DEBUG. Once arm
    support is added this check can be moved to the common
    kvm_vm_ioctl_check_extension() code.
    
    Signed-off-by: Alex Benne <alex.bennee@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9b99402b14df..409217f48456 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -116,13 +116,17 @@ struct kvm_vcpu_arch {
 	 * debugging the guest from the host and to maintain separate host and
 	 * guest state during world switches. vcpu_debug_state are the debug
 	 * registers of the vcpu as the guest sees them.  host_debug_state are
-	 * the host registers which are saved and restored during world switches.
+	 * the host registers which are saved and restored during
+	 * world switches. external_debug_state contains the debug
+	 * values we want to debug the guest. This is set via the
+	 * KVM_SET_GUEST_DEBUG ioctl.
 	 *
 	 * debug_ptr points to the set of debug registers that should be loaded
 	 * onto the hardware when running the guest.
 	 */
 	struct kvm_guest_debug_arch *debug_ptr;
 	struct kvm_guest_debug_arch vcpu_debug_state;
+	struct kvm_guest_debug_arch external_debug_state;
 
 	/* Pointer to host CPU context */
 	kvm_cpu_context_t *host_cpu_context;

commit 84e690bfbed1d1ecb45d8eccd4c7b6c8e878da1c
Author: Alex Benne <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:30:00 2015 +0100

    KVM: arm64: introduce vcpu->arch.debug_ptr
    
    This introduces a level of indirection for the debug registers. Instead
    of using the sys_regs[] directly we store registers in a structure in
    the vcpu. The new kvm_arm_reset_debug_ptr() sets the debug ptr to the
    guest context.
    
    Because we no longer give the sys_regs offset for the sys_reg_desc->reg
    field, but instead the index into a debug-specific struct we need to
    add a number of additional trap functions for each register. Also as the
    generic generic user-space access code no longer works we have
    introduced a new pair of function pointers to the sys_reg_desc structure
    to override the generic code when needed.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Alex Benne <alex.bennee@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index cfb675466e86..9b99402b14df 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -108,11 +108,25 @@ struct kvm_vcpu_arch {
 	/* Exception Information */
 	struct kvm_vcpu_fault_info fault;
 
-	/* Debug state */
+	/* Guest debug state */
 	u64 debug_flags;
 
+	/*
+	 * We maintain more than a single set of debug registers to support
+	 * debugging the guest from the host and to maintain separate host and
+	 * guest state during world switches. vcpu_debug_state are the debug
+	 * registers of the vcpu as the guest sees them.  host_debug_state are
+	 * the host registers which are saved and restored during world switches.
+	 *
+	 * debug_ptr points to the set of debug registers that should be loaded
+	 * onto the hardware when running the guest.
+	 */
+	struct kvm_guest_debug_arch *debug_ptr;
+	struct kvm_guest_debug_arch vcpu_debug_state;
+
 	/* Pointer to host CPU context */
 	kvm_cpu_context_t *host_cpu_context;
+	struct kvm_guest_debug_arch host_debug_state;
 
 	/* VGIC state */
 	struct vgic_cpu vgic_cpu;
@@ -242,5 +256,6 @@ static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 void kvm_arm_init_debug(void);
 void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
+void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
 
 #endif /* __ARM64_KVM_HOST_H__ */

commit 337b99bf7edfb2044781447e7ca386edb1fdba9d
Author: Alex Benne <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:29:58 2015 +0100

    KVM: arm64: guest debug, add support for single-step
    
    This adds support for single-stepping the guest. To do this we need to
    manipulate the guests PSTATE.SS and MDSCR_EL1.SS bits to trigger
    stepping. We take care to preserve MDSCR_EL1 and trap access to it to
    ensure we don't affect the apparent state of the guest.
    
    As we have to enable trapping of all software debug exceptions we
    suppress the ability of the guest to single-step itself. If we didn't we
    would have to deal with the exception arriving while the guest was in
    kernelspace when the guest is expecting to single-step userspace. This
    is something we don't want to unwind in the kernel. Once the host is no
    longer debugging the guest its ability to single-step userspace is
    restored.
    
    Signed-off-by: Alex Benne <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index c90c6a41c448..cfb675466e86 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -123,6 +123,17 @@ struct kvm_vcpu_arch {
 	 * here.
 	 */
 
+	/*
+	 * Guest registers we preserve during guest debugging.
+	 *
+	 * These shadow registers are updated by the kvm_handle_sys_reg
+	 * trap handler if the guest accesses or updates them while we
+	 * are using guest debug.
+	 */
+	struct {
+		u32	mdscr_el1;
+	} guest_debug_preserved;
+
 	/* Don't run the guest */
 	bool pause;
 

commit 56c7f5e77f797fd0dcf2376ce1496f4238e6be33
Author: Alex Benne <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:29:56 2015 +0100

    KVM: arm: introduce kvm_arm_init/setup/clear_debug
    
    This is a precursor for later patches which will need to do more to
    setup debug state before entering the hyp.S switch code. The existing
    functionality for setting mdcr_el2 has been moved out of hyp.S and now
    uses the value kept in vcpu->arch.mdcr_el2.
    
    As the assembler used to previously mask and preserve MDCR_EL2.HPMN I've
    had to add a mechanism to save the value of mdcr_el2 as a per-cpu
    variable during the initialisation code. The kernel never sets this
    number so we are assuming the bootcode has set up the correct value
    here.
    
    This also moves the conditional setting of the TDA bit from the hyp code
    into the C code which is currently used for the lazy debug register
    context switch code.
    
    Signed-off-by: Alex Benne <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 2709db2a7eac..c90c6a41c448 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -103,6 +103,7 @@ struct kvm_vcpu_arch {
 
 	/* HYP configuration */
 	u64 hcr_el2;
+	u32 mdcr_el2;
 
 	/* Exception Information */
 	struct kvm_vcpu_fault_info fault;
@@ -227,4 +228,8 @@ static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
 
+void kvm_arm_init_debug(void);
+void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
+void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 8a14849b4a355278f0b7baf6e2da7dc7144a23e8
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 12 12:06:37 2015 +0100

    arm64: KVM: Switch vgic save/restore to alternative_insn
    
    So far, we configured the world-switch by having a small array
    of pointers to the save and restore functions, depending on the
    GIC used on the platform.
    
    Loading these values each time is a bit silly (they never change),
    and it makes sense to rely on the instruction patching instead.
    
    This leads to a nice cleanup of the code.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index f0f58c9beec0..2709db2a7eac 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -221,29 +221,6 @@ struct vgic_sr_vectors {
 	void	*restore_vgic;
 };
 
-static inline void vgic_arch_setup(const struct vgic_params *vgic)
-{
-	extern struct vgic_sr_vectors __vgic_sr_vectors;
-
-	switch(vgic->type)
-	{
-	case VGIC_V2:
-		__vgic_sr_vectors.save_vgic	= __save_vgic_v2_state;
-		__vgic_sr_vectors.restore_vgic	= __restore_vgic_v2_state;
-		break;
-
-#ifdef CONFIG_ARM_GIC_V3
-	case VGIC_V3:
-		__vgic_sr_vectors.save_vgic	= __save_vgic_v3_state;
-		__vgic_sr_vectors.restore_vgic	= __restore_vgic_v3_state;
-		break;
-#endif
-
-	default:
-		BUG();
-	}
-}
-
 static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}

commit 35307b9a5f7ebcc8d8db41c73b69c131b48ace2b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Mar 12 18:16:51 2015 +0000

    arm/arm64: KVM: Implement Stage-2 page aging
    
    Until now, KVM/arm didn't care much for page aging (who was swapping
    anyway?), and simply provided empty hooks to the core KVM code. With
    server-type systems now being available, things are quite different.
    
    This patch implements very simple support for page aging, by clearing
    the Access flag in the Stage-2 page tables. On access fault, the current
    fault handling will write the PTE or PMD again, putting the Access flag
    back on.
    
    It should be possible to implement a much faster handling for Access
    faults, but that's left for a later patch.
    
    With this in place, performance in VMs is degraded much more gracefully.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 967fb1cee300..f0f58c9beec0 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -179,19 +179,10 @@ int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_unmap_hva_range(struct kvm *kvm,
 			unsigned long start, unsigned long end);
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
+int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
+int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
 
 /* We do not have shadow page tables, hence the empty hooks */
-static inline int kvm_age_hva(struct kvm *kvm, unsigned long start,
-			      unsigned long end)
-{
-	return 0;
-}
-
-static inline int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
-{
-	return 0;
-}
-
 static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
 							 unsigned long address)
 {

commit c1426e4c5add09042840013dfa5565e6be6d412e
Author: Eric Auger <eric.auger@linaro.org>
Date:   Wed Mar 4 11:14:34 2015 +0100

    KVM: arm/arm64: implement kvm_arch_intc_initialized
    
    On arm/arm64 the VGIC is dynamically instantiated and it is useful
    to expose its state, especially for irqfd setup.
    
    This patch defines __KVM_HAVE_ARCH_INTC_INITIALIZED and
    implements kvm_arch_intc_initialized.
    
    Signed-off-by: Eric Auger <eric.auger@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 8ac3c70fe3c6..967fb1cee300 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -28,6 +28,8 @@
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
 
+#define __KVM_HAVE_ARCH_INTC_INITIALIZED
+
 #if defined(CONFIG_KVM_ARM_MAX_VCPUS)
 #define KVM_MAX_VCPUS CONFIG_KVM_ARM_MAX_VCPUS
 #else

commit b9085bcbf5f43adf60533f9b635b2e7faeed0fe9
Merge: c7d7b9867155 6557bada461a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 13 09:55:09 2015 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM update from Paolo Bonzini:
     "Fairly small update, but there are some interesting new features.
    
      Common:
         Optional support for adding a small amount of polling on each HLT
         instruction executed in the guest (or equivalent for other
         architectures).  This can improve latency up to 50% on some
         scenarios (e.g. O_DSYNC writes or TCP_RR netperf tests).  This
         also has to be enabled manually for now, but the plan is to
         auto-tune this in the future.
    
      ARM/ARM64:
         The highlights are support for GICv3 emulation and dirty page
         tracking
    
      s390:
         Several optimizations and bugfixes.  Also a first: a feature
         exposed by KVM (UUID and long guest name in /proc/sysinfo) before
         it is available in IBM's hypervisor! :)
    
      MIPS:
         Bugfixes.
    
      x86:
         Support for PML (page modification logging, a new feature in
         Broadwell Xeons that speeds up dirty page tracking), nested
         virtualization improvements (nested APICv---a nice optimization),
         usual round of emulation fixes.
    
         There is also a new option to reduce latency of the TSC deadline
         timer in the guest; this needs to be tuned manually.
    
         Some commits are common between this pull and Catalin's; I see you
         have already included his tree.
    
      Powerpc:
         Nothing yet.
    
         The KVM/PPC changes will come in through the PPC maintainers,
         because I haven't received them yet and I might end up being
         offline for some part of next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: ia64: drop kvm.h from installed user headers
      KVM: x86: fix build with !CONFIG_SMP
      KVM: x86: emulate: correct page fault error code for NoWrite instructions
      KVM: Disable compat ioctl for s390
      KVM: s390: add cpu model support
      KVM: s390: use facilities and cpu_id per KVM
      KVM: s390/CPACF: Choose crypto control block format
      s390/kernel: Update /proc/sysinfo file with Extended Name and UUID
      KVM: s390: reenable LPP facility
      KVM: s390: floating irqs: fix user triggerable endless loop
      kvm: add halt_poll_ns module parameter
      kvm: remove KVM_MMIO_SIZE
      KVM: MIPS: Don't leak FPU/DSP to guest
      KVM: MIPS: Disable HTW while in guest
      KVM: nVMX: Enable nested posted interrupt processing
      KVM: nVMX: Enable nested virtual interrupt delivery
      KVM: nVMX: Enable nested apic register virtualization
      KVM: nVMX: Make nested control MSRs per-cpu
      KVM: nVMX: Enable nested virtualize x2apic mode
      KVM: nVMX: Prepare for using hardware MSR bitmap
      ...

commit f7819512996361280b86259222456fcf15aad926
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 4 18:20:58 2015 +0100

    kvm: add halt_poll_ns module parameter
    
    This patch introduces a new module parameter for the KVM module; when it
    is present, KVM attempts a bit of polling on every HLT before scheduling
    itself out via kvm_vcpu_block.
    
    This parameter helps a lot for latency-bound workloads---in particular
    I tested it with O_DSYNC writes with a battery-backed disk in the host.
    In this case, writes are fast (because the data doesn't have to go all
    the way to the platters) but they cannot be merged by either the host or
    the guest.  KVM's performance here is usually around 30% of bare metal,
    or 50% if you use cache=directsync or cache=writethrough (these
    parameters avoid that the guest sends pointless flush requests, and
    at the same time they are not slow because of the battery-backed cache).
    The bad performance happens because on every halt the host CPU decides
    to halt itself too.  When the interrupt comes, the vCPU thread is then
    migrated to a new physical CPU, and in general the latency is horrible
    because the vCPU thread has to be scheduled back in.
    
    With this patch performance reaches 60-65% of bare metal and, more
    important, 99% of what you get if you use idle=poll in the guest.  This
    means that the tunable gets rid of this particular bottleneck, and more
    work can be done to improve performance in the kernel or QEMU.
    
    Of course there is some price to pay; every time an otherwise idle vCPUs
    is interrupted by an interrupt, it will poll unnecessarily and thus
    impose a little load on the host.  The above results were obtained with
    a mostly random value of the parameter (500000), and the load was around
    1.5-2.5% CPU usage on one of the host's core for each idle guest vCPU.
    
    The patch also adds a new stat, /sys/kernel/debug/kvm/halt_successful_poll,
    that can be used to tune the parameter.  It counts how many HLT
    instructions received an interrupt during the polling period; each
    successful poll avoids that Linux schedules the VCPU thread out and back
    in, and may also avoid a likely trip to C1 and back for the physical CPU.
    
    While the VM is idle, a Linux 4 VCPU VM halts around 10 times per second.
    Of these halts, almost all are failed polls.  During the benchmark,
    instead, basically all halts end within the polling period, except a more
    or less constant stream of 50 per second coming from vCPUs that are not
    running the benchmark.  The wasted time is thus very low.  Things may
    be slightly different for Windows VMs, which have a ~10 ms timer tick.
    
    The effect is also visible on Marcelo's recently-introduced latency
    test for the TSC deadline timer.  Though of course a non-RT kernel has
    awful latency bounds, the latency of the timer is around 8000-10000 clock
    cycles compared to 20000-120000 without setting halt_poll_ns.  For the TSC
    deadline timer, thus, the effect is both a smaller average latency and
    a smaller variance.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 2c49aa4ac818..8efde89613f2 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -165,6 +165,7 @@ struct kvm_vm_stat {
 };
 
 struct kvm_vcpu_stat {
+	u32 halt_successful_poll;
 	u32 halt_wakeup;
 };
 

commit 3c1e716508335eb132c9349cb1a1716c8f7e3d2e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Dec 19 16:05:31 2014 +0000

    arm/arm64: KVM: Use set/way op trapping to track the state of the caches
    
    Trying to emulate the behaviour of set/way cache ops is fairly
    pointless, as there are too many ways we can end-up missing stuff.
    Also, there is some system caches out there that simply ignore
    set/way operations.
    
    So instead of trying to implement them, let's convert it to VA ops,
    and use them as a way to re-enable the trapping of VM ops. That way,
    we can detect the point when the MMU/caches are turned off, and do
    a full VM flush (which is what the guest was trying to do anyway).
    
    This allows a 32bit zImage to boot on the APM thingy, and will
    probably help bootloaders in general.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0b7dfdb931df..acd101a9014d 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -116,9 +116,6 @@ struct kvm_vcpu_arch {
 	 * Anything that is not used directly from assembly code goes
 	 * here.
 	 */
-	/* dcache set/way operation pending */
-	int last_pcpu;
-	cpumask_t require_dcache_flush;
 
 	/* Don't run the guest */
 	bool pause;

commit 3caa2d8c3b2d80f5e342fe8cec07c03c8147dcab
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Mon Jun 2 16:26:01 2014 +0200

    arm/arm64: KVM: make the maximum number of vCPUs a per-VM value
    
    Currently the maximum number of vCPUs supported is a global value
    limited by the used GIC model. GICv3 will lift this limit, but we
    still need to observe it for guests using GICv2.
    So the maximum number of vCPUs is per-VM value, depending on the
    GIC model the guest uses.
    Store and check the value in struct kvm_arch, but keep it down to
    8 for now.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index ff8ee3ec32f4..2c49aa4ac818 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -59,6 +59,9 @@ struct kvm_arch {
 	/* VTTBR value associated with above pgd and vmid */
 	u64    vttbr;
 
+	/* The maximum number of vCPUs depends on the used GIC model */
+	int max_vcpus;
+
 	/* Interrupt controller */
 	struct vgic_dist	vgic;
 

commit 4429fc64b90368e9bc93f933ea8b011d8db3a2f2
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Mon Jun 2 15:37:13 2014 +0200

    arm/arm64: KVM: rework MPIDR assignment and add accessors
    
    The virtual MPIDR registers (containing topology information) for the
    guest are currently mapped linearily to the vcpu_id. Improve this
    mapping for arm64 by using three levels to not artificially limit the
    number of vCPUs.
    To help this, change and rename the kvm_vcpu_get_mpidr() function to
    mask off the non-affinity bits in the MPIDR register.
    Also add an accessor to later allow easier access to a vCPU with a
    given MPIDR. Use this new accessor in the PSCI emulation.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 012af6ce9eed..ff8ee3ec32f4 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -207,6 +207,8 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 int kvm_perf_init(void);
 int kvm_perf_teardown(void);
 
+struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
+
 static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 				       phys_addr_t pgd_ptr,
 				       unsigned long hyp_stack_ptr,

commit 8199ed0e7c28ece79674a9fbba3208e93395a646
Author: Mario Smarduch <m.smarduch@samsung.com>
Date:   Thu Jan 15 15:58:59 2015 -0800

    KVM: arm64: ARMv8 header changes for page logging
    
    This patch adds arm64 helpers to write protect pmds/ptes and retrieve
    permissions while logging dirty pages. Also adds prototype to write protect
    a memory slot and adds a pmd define to check for read-only pmds.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mario Smarduch <m.smarduch@samsung.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0b7dfdb931df..012af6ce9eed 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -199,6 +199,7 @@ struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 
 u64 kvm_call_hyp(void *hypfn, ...);
 void force_vm_exit(const cpumask_t *mask);
+void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);
 
 int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		int exception_index);

commit cf5d318865e25f887d49a0c6083bbc6dcd1905b1
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Oct 16 17:00:18 2014 +0200

    arm/arm64: KVM: Turn off vcpus on PSCI shutdown/reboot
    
    When a vcpu calls SYSTEM_OFF or SYSTEM_RESET with PSCI v0.2, the vcpus
    should really be turned off for the VM adhering to the suggestions in
    the PSCI spec, and it's the sane thing to do.
    
    Also, clarify the behavior and expectations for exits to user space with
    the KVM_EXIT_SYSTEM_EVENT case.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 65c615294589..0b7dfdb931df 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -198,6 +198,7 @@ struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 
 u64 kvm_call_hyp(void *hypfn, ...);
+void force_vm_exit(const cpumask_t *mask);
 
 int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		int exception_index);

commit f7fa034dc8559c7d7326bfc8bd1a26175abd931a
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Oct 16 16:40:53 2014 +0200

    arm/arm64: KVM: Clarify KVM_ARM_VCPU_INIT ABI
    
    It is not clear that this ioctl can be called multiple times for a given
    vcpu.  Userspace already does this, so clarify the ABI.
    
    Also specify that userspace is expected to always make secondary and
    subsequent calls to the ioctl with the same parameters for the VCPU as
    the initial call (which userspace also already does).
    
    Add code to check that userspace doesn't violate that ABI in the future,
    and move the kvm_vcpu_set_target() function which is currently
    duplicated between the 32-bit and 64-bit versions in guest.c to a common
    static function in arm.c, shared between both architectures.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 2012c4ba8d67..65c615294589 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -165,8 +165,6 @@ struct kvm_vcpu_stat {
 	u32 halt_wakeup;
 };
 
-int kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
-			const struct kvm_vcpu_init *init);
 int kvm_vcpu_preferred_target(struct kvm_vcpu_init *init);
 unsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu);
 int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices);

commit e77d99d4a4ec761ad061f1ec890c71040a92efe3
Merge: bb0ca6acd466 0496daa5cf99
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Sep 27 11:03:33 2014 +0200

    Merge tag 'kvm-arm-for-3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into kvm-next
    
    Changes for KVM for arm/arm64 for 3.18
    
    This includes a bunch of changes:
     - Support read-only memory slots on arm/arm64
     - Various changes to fix Sparse warnings
     - Correctly detect write vs. read Stage-2 faults
     - Various VGIC cleanups and fixes
     - Dynamic VGIC data strcuture sizing
     - Fix SGI set_clear_pend offset bug
     - Fix VTTBR_BADDR Mask
     - Correctly report the FSC on Stage-2 faults
    
    Conflicts:
            virt/kvm/eventfd.c
            [duplicate, different patch where the kvm-arm version broke x86.
             The kvm tree instead has the right one]

commit fe71557afbec641fee73711e40602bed37f6f33b
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Wed Sep 24 15:57:57 2014 +0800

    kvm: Add arch specific mmu notifier for page invalidation
    
    This will be used to let the guest run while the APIC access page is
    not pinned.  Because subsequent patches will fill in the function
    for x86, place the (still empty) x86 implementation in the x86.c file
    instead of adding an inline function in kvm_host.h.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index a3c671b3acc9..992d9da88119 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -191,6 +191,11 @@ static inline int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
 	return 0;
 }
 
+static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+							 unsigned long address)
+{
+}
+
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
 struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
 

commit 57128468080a8b6ea452223036d3e417f748af55
Author: Andres Lagar-Cavilla <andreslc@google.com>
Date:   Mon Sep 22 14:54:42 2014 -0700

    kvm: Fix page ageing bugs
    
    1. We were calling clear_flush_young_notify in unmap_one, but we are
    within an mmu notifier invalidate range scope. The spte exists no more
    (due to range_start) and the accessed bit info has already been
    propagated (due to kvm_pfn_set_accessed). Simply call
    clear_flush_young.
    
    2. We clear_flush_young on a primary MMU PMD, but this may be mapped
    as a collection of PTEs by the secondary MMU (e.g. during log-dirty).
    This required expanding the interface of the clear_flush_young mmu
    notifier, so a lot of code has been trivially touched.
    
    3. In the absence of shadow_accessed_mask (e.g. EPT A bit), we emulate
    the access bit by blowing the spte. This requires proper synchronizing
    with MMU notifier consumers, like every other removal of spte's does.
    
    Signed-off-by: Andres Lagar-Cavilla <andreslc@google.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index be9970a59497..a3c671b3acc9 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -180,7 +180,8 @@ int kvm_unmap_hva_range(struct kvm *kvm,
 void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
 
 /* We do not have shadow page tables, hence the empty hooks */
-static inline int kvm_age_hva(struct kvm *kvm, unsigned long hva)
+static inline int kvm_age_hva(struct kvm *kvm, unsigned long start,
+			      unsigned long end)
 {
 	return 0;
 }

commit a875dafcf9b6b266c855e1f9b0aa060ef585d38a
Merge: 0ba09511ddc3 f51770ed465e
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Thu Sep 18 18:15:32 2014 -0700

    Merge remote-tracking branch 'kvm/next' into queue
    
    Conflicts:
            arch/arm64/include/asm/kvm_host.h
            virt/kvm/arm/vgic.c

commit 13a34e067eab24fec882e1834fbf2cc31911d474
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:03 2014 +0200

    KVM: remove garbage arg to *hardware_{en,dis}able
    
    In the beggining was on_each_cpu(), which required an unused argument to
    kvm_arch_ops.hardware_{en,dis}able, but this was soon forgotten.
    
    Remove unnecessary arguments that stem from this.
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index b5045e3e05f8..be9970a59497 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -242,7 +242,7 @@ static inline void vgic_arch_setup(const struct vgic_params *vgic)
 	}
 }
 
-static inline void kvm_arch_hardware_disable(void *garbage) {}
+static inline void kvm_arch_hardware_disable(void) {}
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}

commit 0865e636aef751966e6e0f8950a26bc7391e923c
Author: Radim Krm <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:02 2014 +0200

    KVM: static inline empty kvm_arch functions
    
    Using static inline is going to save few bytes and cycles.
    For example on powerpc, the difference is 700 B after stripping.
    (5 kB before)
    
    This patch also deals with two overlooked empty functions:
    kvm_arch_flush_shadow was not removed from arch/mips/kvm/mips.c
      2df72e9bc KVM: split kvm_arch_flush_shadow
    and kvm_arch_sched_in never made it into arch/ia64/kvm/kvm-ia64.c.
      e790d9ef6 KVM: add kvm_arch_sched_in
    
    Signed-off-by: Radim Krm <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 766147baae0b..b5045e3e05f8 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -242,4 +242,10 @@ static inline void vgic_arch_setup(const struct vgic_params *vgic)
 	}
 }
 
+static inline void kvm_arch_hardware_disable(void *garbage) {}
+static inline void kvm_arch_hardware_unsetup(void) {}
+static inline void kvm_arch_sync_events(struct kvm *kvm) {}
+static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}
+static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 656473003bc7e056c3bbd4a4d9832dad01e86f76
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Aug 29 14:01:17 2014 +0200

    KVM: forward declare structs in kvm_types.h
    
    Opaque KVM structs are useful for prototypes in asm/kvm_host.h, to avoid
    "'struct foo' declared inside parameter list" warnings (and consequent
    breakage due to conflicting types).
    
    Move them from individual files to a generic place in linux/kvm_types.h.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e10c45a578e3..766147baae0b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -22,6 +22,8 @@
 #ifndef __ARM64_KVM_HOST_H__
 #define __ARM64_KVM_HOST_H__
 
+#include <linux/types.h>
+#include <linux/kvm_types.h>
 #include <asm/kvm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
@@ -41,7 +43,6 @@
 
 #define KVM_VCPU_MAX_FEATURES 3
 
-struct kvm_vcpu;
 int kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(long ext);
@@ -164,18 +165,15 @@ struct kvm_vcpu_stat {
 	u32 halt_wakeup;
 };
 
-struct kvm_vcpu_init;
 int kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 			const struct kvm_vcpu_init *init);
 int kvm_vcpu_preferred_target(struct kvm_vcpu_init *init);
 unsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu);
 int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices);
-struct kvm_one_reg;
 int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
 int kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
 
 #define KVM_ARCH_WANT_MMU_NOTIFIER
-struct kvm;
 int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
 int kvm_unmap_hva_range(struct kvm *kvm,
 			unsigned long start, unsigned long end);

commit 4000be423cb01a8d09de878bb8184511c49d4238
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 26 15:13:21 2014 +0100

    KVM: ARM/arm64: fix broken __percpu annotation
    
    Running sparse results in a bunch of noisy address space mismatches
    thanks to the broken __percpu annotation on kvm_get_running_vcpus.
    
    This function returns a pcpu pointer to a pointer, not a pointer to a
    pcpu pointer. This patch fixes the annotation, which kills the warnings
    from sparse.
    
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 44094e559848..50431d36732b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -193,7 +193,7 @@ static inline int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
 }
 
 struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
-struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
+struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
 
 u64 kvm_call_hyp(void *hypfn, ...);
 

commit 6951e48bff0b55d2a8e825a953fc1f8e3a34bf1c
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 26 15:13:20 2014 +0100

    KVM: ARM/arm64: fix non-const declaration of function returning const
    
    Sparse kicks up about a type mismatch for kvm_target_cpu:
    
    arch/arm64/kvm/guest.c:271:25: error: symbol 'kvm_target_cpu' redeclared with different type (originally declared at ./arch/arm64/include/asm/kvm_host.h:45) - different modifiers
    
    so fix this by adding the missing const attribute to the function
    declaration.
    
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index e10c45a578e3..44094e559848 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -42,7 +42,7 @@
 #define KVM_VCPU_MAX_FEATURES 3
 
 struct kvm_vcpu;
-int kvm_target_cpu(void);
+int __attribute_const__ kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
 int kvm_arch_dev_ioctl_check_extension(long ext);
 

commit dedf97e8ff2c7513b1370e36b56e08b6bd0f0290
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Aug 1 12:00:36 2014 +0100

    arm64: KVM: fix 64bit CP15 VM access for 32bit guests
    
    Commit f0a3eaff71b8 (ARM64: KVM: fix big endian issue in
    access_vm_reg for 32bit guest) changed the way we handle CP15
    VM accesses, so that all 64bit accesses are done via vcpu_sys_reg.
    
    This looks like a good idea as it solves indianness issues in an
    elegant way, except for one small detail: the register index is
    doesn't refer to the same array! We end up corrupting some random
    data structure instead.
    
    Fix this by reverting to the original code, except for the introduction
    of a vcpu_cp15_64_high macro that deals with the endianness thing.
    
    Tested on Juno with 32bit SMP guests.
    
    Cc: Victor Kamensky <victor.kamensky@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 79812be4f25f..e10c45a578e3 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -149,9 +149,11 @@ struct kvm_vcpu_arch {
 #define vcpu_cp15(v,r)		((v)->arch.ctxt.copro[(r)])
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
-#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.copro[((r) + 1)])
+#define vcpu_cp15_64_high(v,r)	vcpu_cp15((v),(r))
+#define vcpu_cp15_64_low(v,r)	vcpu_cp15((v),(r) + 1)
 #else
-#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.copro[((r) + 0)])
+#define vcpu_cp15_64_high(v,r)	vcpu_cp15((v),(r) + 1)
+#define vcpu_cp15_64_low(v,r)	vcpu_cp15((v),(r))
 #endif
 
 struct kvm_vm_stat {

commit 72564016aae45f42e488f926bc803f9a2e1c771c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Apr 24 10:27:13 2014 +0100

    arm64: KVM: common infrastructure for handling AArch32 CP14/CP15
    
    As we're about to trap a bunch of CP14 registers, let's rework
    the CP15 handling so it can be generalized and work with multiple
    tables.
    
    Reviewed-by: Anup Patel <anup.patel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 8e410f761918..79812be4f25f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -86,7 +86,7 @@ struct kvm_cpu_context {
 	struct kvm_regs	gp_regs;
 	union {
 		u64 sys_regs[NR_SYS_REGS];
-		u32 cp15[NR_CP15_REGS];
+		u32 copro[NR_COPRO_REGS];
 	};
 };
 
@@ -141,12 +141,17 @@ struct kvm_vcpu_arch {
 
 #define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
 #define vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
-#define vcpu_cp15(v,r)		((v)->arch.ctxt.cp15[(r)])
+/*
+ * CP14 and CP15 live in the same array, as they are backed by the
+ * same system registers.
+ */
+#define vcpu_cp14(v,r)		((v)->arch.ctxt.copro[(r)])
+#define vcpu_cp15(v,r)		((v)->arch.ctxt.copro[(r)])
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
-#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.cp15[((r) + 1)])
+#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.copro[((r) + 1)])
 #else
-#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.cp15[((r) + 0)])
+#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.copro[((r) + 0)])
 #endif
 
 struct kvm_vm_stat {

commit 0c557ed4983b7abe152212b5b1726c2a789b2c61
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Apr 24 10:24:46 2014 +0100

    arm64: KVM: add trap handlers for AArch64 debug registers
    
    Add handlers for all the AArch64 debug registers that are accessible
    from EL0 or EL1. The trapping code keeps track of the state of the
    debug registers, allowing for the switch code to implement a lazy
    switching strategy.
    
    Reviewed-by: Anup Patel <anup.patel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 503c70661636..8e410f761918 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -101,6 +101,9 @@ struct kvm_vcpu_arch {
 	/* Exception Information */
 	struct kvm_vcpu_fault_info fault;
 
+	/* Debug state */
+	u64 debug_flags;
+
 	/* Pointer to host CPU context */
 	kvm_cpu_context_t *host_cpu_context;
 

commit f0a3eaff71b8bd5d5acfda1f0cf3eedf49755622
Author: Victor Kamensky <victor.kamensky@linaro.org>
Date:   Wed Jul 2 17:19:30 2014 +0100

    ARM64: KVM: fix big endian issue in access_vm_reg for 32bit guest
    
    Fix issue with 32bit guests running on top of BE KVM host.
    Indexes of high and low words of 64bit cp15 register are
    swapped in case of big endian code, since 64bit cp15 state is
    restored or saved with double word write or read instruction.
    
    Define helper macro to access low words of 64bit cp15 register.
    
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4ae9213aa997..503c70661636 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -140,6 +140,12 @@ struct kvm_vcpu_arch {
 #define vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
 #define vcpu_cp15(v,r)		((v)->arch.ctxt.cp15[(r)])
 
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.cp15[((r) + 1)])
+#else
+#define vcpu_cp15_64_low(v,r) ((v)->arch.ctxt.cp15[((r) + 0)])
+#endif
+
 struct kvm_vm_stat {
 	u32 remote_tlb_flush;
 };

commit 67b2abfedb7b861bead93400fa315c5c30879d51
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jul 9 10:45:49 2013 +0100

    arm64: KVM: vgic: enable GICv2 emulation on top on GICv3 hardware
    
    Add the last missing bits that enable GICv2 emulation on top of
    GICv3 hardware.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4c182d0aae70..4ae9213aa997 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -216,6 +216,13 @@ static inline void vgic_arch_setup(const struct vgic_params *vgic)
 		__vgic_sr_vectors.restore_vgic	= __restore_vgic_v2_state;
 		break;
 
+#ifdef CONFIG_ARM_GIC_V3
+	case VGIC_V3:
+		__vgic_sr_vectors.save_vgic	= __save_vgic_v3_state;
+		__vgic_sr_vectors.restore_vgic	= __restore_vgic_v3_state;
+		break;
+#endif
+
 	default:
 		BUG();
 	}

commit 1a9b13056dde7e3092304d6041ccc60a913042ea
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 21 11:57:56 2013 +0100

    arm64: KVM: split GICv2 world switch from hyp code
    
    Move the GICv2 world switch code into its own file, and add the
    necessary indirection to the arm64 switch code.
    
    Also introduce a new type field to the vgic_params structure.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 92242ce06309..4c182d0aae70 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -200,4 +200,25 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
 		     hyp_stack_ptr, vector_ptr);
 }
 
+struct vgic_sr_vectors {
+	void	*save_vgic;
+	void	*restore_vgic;
+};
+
+static inline void vgic_arch_setup(const struct vgic_params *vgic)
+{
+	extern struct vgic_sr_vectors __vgic_sr_vectors;
+
+	switch(vgic->type)
+	{
+	case VGIC_V2:
+		__vgic_sr_vectors.save_vgic	= __save_vgic_v2_state;
+		__vgic_sr_vectors.restore_vgic	= __restore_vgic_v2_state;
+		break;
+
+	default:
+		BUG();
+	}
+}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 7d0f84aae9e231930985eaff63ac91b61aaa15d6
Author: Anup Patel <anup.patel@linaro.org>
Date:   Tue Apr 29 11:24:16 2014 +0530

    ARM/ARM64: KVM: Add base for PSCI v0.2 emulation
    
    Currently, the in-kernel PSCI emulation provides PSCI v0.1 interface to
    VCPUs. This patch extends current in-kernel PSCI emulation to provide
    PSCI v0.2 interface to VCPUs.
    
    By default, ARM/ARM64 KVM will always provide PSCI v0.1 interface for
    keeping the ABI backward-compatible.
    
    To select PSCI v0.2 interface for VCPUs, the user space (i.e. QEMU or
    KVMTOOL) will have to set KVM_ARM_VCPU_PSCI_0_2 feature when doing VCPU
    init using KVM_ARM_VCPU_INIT ioctl.
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Pranavkumar Sawargaonkar <pranavkumar@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0a1d69751562..92242ce06309 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -39,7 +39,7 @@
 #include <kvm/arm_vgic.h>
 #include <kvm/arm_arch_timer.h>
 
-#define KVM_VCPU_MAX_FEATURES 2
+#define KVM_VCPU_MAX_FEATURES 3
 
 struct kvm_vcpu;
 int kvm_target_cpu(void);

commit da7814700a0c408bead58ce4714b7625ffbaade1
Author: Anup Patel <anup.patel@linaro.org>
Date:   Thu Dec 12 16:12:22 2013 +0000

    arm64: KVM: Add Kconfig option for max VCPUs per-Guest
    
    Current max VCPUs per-Guest is set to 4 which is preventing
    us from creating a Guest (or VM) with 8 VCPUs on Host (e.g.
    X-Gene Storm SOC) with 8 Host CPUs.
    
    The correct value of max VCPUs per-Guest should be same as
    the max CPUs supported by GICv2 which is 8 but, increasing
    value of max VCPUs per-Guest can make things slower hence
    we add Kconfig option to let KVM users select appropriate
    max VCPUs per-Guest.
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Pranavkumar Sawargaonkar <pranavkumar@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 5d85a02d1231..0a1d69751562 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -26,7 +26,12 @@
 #include <asm/kvm_asm.h>
 #include <asm/kvm_mmio.h>
 
-#define KVM_MAX_VCPUS 4
+#if defined(CONFIG_KVM_ARM_MAX_VCPUS)
+#define KVM_MAX_VCPUS CONFIG_KVM_ARM_MAX_VCPUS
+#else
+#define KVM_MAX_VCPUS 0
+#endif
+
 #define KVM_USER_MEM_SLOTS 32
 #define KVM_PRIVATE_MEM_SLOTS 4
 #define KVM_COALESCED_MMIO_PAGE_OFFSET 1

commit d570142674890fe10b3d7d86aa105e3dfce1ddfa
Merge: f2e106692d51 a7265fb1751f
Author: Gleb Natapov <gleb@redhat.com>
Date:   Wed Oct 16 15:30:32 2013 +0300

    Merge tag 'kvm-arm-for-3.13-1' of git://git.linaro.org/people/cdall/linux-kvm-arm into next
    
    Updates for KVM/ARM including cpu=host and Cortex-A7 support

commit ef0cfe71c2b1710cd4ae747537e36c56f9a26ccf
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Wed Oct 2 14:22:30 2013 -0700

    KVM: arm64: Get rid of KVM_HPAGE defines
    
    Now when the main kvm code relying on these defines has been moved to
    the x86 specific part of the world, we can get rid of these.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Gleb Natapov <gleb@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0859a4ddd1e7..54c064f622aa 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -36,11 +36,6 @@
 
 #define KVM_VCPU_MAX_FEATURES 2
 
-/* We don't currently support large pages. */
-#define KVM_HPAGE_GFN_SHIFT(x)	0
-#define KVM_NR_PAGE_SIZES	1
-#define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
-
 struct kvm_vcpu;
 int kvm_target_cpu(void);
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu);

commit 473bdc0e6565ebb22455657a40daa21b6b4ee16b
Author: Anup Patel <anup.patel@linaro.org>
Date:   Mon Sep 30 14:20:06 2013 +0530

    ARM64: KVM: Implement kvm_vcpu_preferred_target() function
    
    This patch implements kvm_vcpu_preferred_target() function for
    KVM ARM64 which will help us implement KVM_ARM_PREFERRED_TARGET
    ioctl for user space.
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Pranavkumar Sawargaonkar <pranavkumar@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 0859a4ddd1e7..4cc8c7078f39 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -151,6 +151,7 @@ struct kvm_vcpu_stat {
 struct kvm_vcpu_init;
 int kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
 			const struct kvm_vcpu_init *init);
+int kvm_vcpu_preferred_target(struct kvm_vcpu_init *init);
 unsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu);
 int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices);
 struct kvm_one_reg;

commit 6c8c0c4dc0e98ee2191211d66e9f876e95787073
Author: Chen Gang <gang.chen@asianux.com>
Date:   Mon Jul 22 04:40:38 2013 +0100

    arm64: KVM: use 'int' instead of 'u32' for variable 'target' in kvm_host.h.
    
    'target' will be set to '-1' in kvm_arch_vcpu_init(), and it need check
    'target' whether less than zero or not in kvm_vcpu_initialized().
    
    So need define target as 'int' instead of 'u32', just like ARM has done.
    
    The related warning:
    
      arch/arm64/kvm/../../../arch/arm/kvm/arm.c:497:2: warning: comparison of unsigned expression >= 0 is always true [-Wtype-limits]
    
    Signed-off-by: Chen Gang <gang.chen@asianux.com>
    [Marc: reformated the Subject line to fit the series]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 644d73956864..0859a4ddd1e7 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -129,7 +129,7 @@ struct kvm_vcpu_arch {
 	struct kvm_mmu_memory_cache mmu_page_cache;
 
 	/* Target CPU and feature flags */
-	u32 target;
+	int target;
 	DECLARE_BITMAP(features, KVM_VCPU_MAX_FEATURES);
 
 	/* Detect first run of a vcpu */

commit 0d854a60b1d7d39a37b25dd28f63cfa0df637b91
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Feb 7 10:46:46 2013 +0000

    arm64: KVM: enable initialization of a 32bit vcpu
    
    Wire the init of a 32bit vcpu by allowing 32bit modes in pstate,
    and providing sensible defaults out of reset state.
    
    This feature is of course conditioned by the presence of 32bit
    capability on the physical CPU, and is checked by the KVM_CAP_ARM_EL1_32BIT
    capability.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 3f5830b3ca3f..644d73956864 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -34,7 +34,7 @@
 #include <kvm/arm_vgic.h>
 #include <kvm/arm_arch_timer.h>
 
-#define KVM_VCPU_MAX_FEATURES 1
+#define KVM_VCPU_MAX_FEATURES 2
 
 /* We don't currently support large pages. */
 #define KVM_HPAGE_GFN_SHIFT(x)	0

commit 40033a614ea3db196d57c477ca328f44eb1e4df0
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Feb 6 19:17:50 2013 +0000

    arm64: KVM: define 32bit specific registers
    
    Define the 32bit specific registers (SPSRs, cp15...).
    
    Most CPU registers are directly mapped to a 64bit register
    (r0->x0...). Only the SPSRs have separate registers.
    
    cp15 registers are also mapped into their 64bit counterpart in most
    cases.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 2fdeb326c3ee..3f5830b3ca3f 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -84,7 +84,10 @@ struct kvm_vcpu_fault_info {
 
 struct kvm_cpu_context {
 	struct kvm_regs	gp_regs;
-	u64 sys_regs[NR_SYS_REGS];
+	union {
+		u64 sys_regs[NR_SYS_REGS];
+		u32 cp15[NR_CP15_REGS];
+	};
 };
 
 typedef struct kvm_cpu_context kvm_cpu_context_t;

commit dcd2e40c1e1cce302498d16d095b0f8a30326f74
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Dec 12 18:52:05 2012 +0000

    arm64: KVM: PSCI implementation
    
    Wire the PSCI backend into the exit handling code.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 2500eb6a4d2a..2fdeb326c3ee 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -34,7 +34,7 @@
 #include <kvm/arm_vgic.h>
 #include <kvm/arm_arch_timer.h>
 
-#define KVM_VCPU_MAX_FEATURES 0
+#define KVM_VCPU_MAX_FEATURES 1
 
 /* We don't currently support large pages. */
 #define KVM_HPAGE_GFN_SHIFT(x)	0

commit 092bd143cbb481b4ce1d55247a2987eaaf61f967
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Dec 17 17:07:52 2012 +0000

    arm64: KVM: hypervisor initialization code
    
    Provide EL2 with page tables and stack, and set the vectors
    to point to the full blown world-switch code.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 4a2622f5e81f..2500eb6a4d2a 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -183,4 +183,17 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 int kvm_perf_init(void);
 int kvm_perf_teardown(void);
 
+static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
+				       phys_addr_t pgd_ptr,
+				       unsigned long hyp_stack_ptr,
+				       unsigned long vector_ptr)
+{
+	/*
+	 * Call initialization code, and switch to the full blown
+	 * HYP code.
+	 */
+	kvm_call_hyp((void *)boot_pgd_ptr, pgd_ptr,
+		     hyp_stack_ptr, vector_ptr);
+}
+
 #endif /* __ARM64_KVM_HOST_H__ */

commit 4f8d6632ec71372a3b8dbb4775662c2c9025d173
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Dec 10 16:29:28 2012 +0000

    arm64: KVM: kvm_arch and kvm_vcpu_arch definitions
    
    Provide the architecture dependent structures for VM and
    vcpu abstractions.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
new file mode 100644
index 000000000000..4a2622f5e81f
--- /dev/null
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -0,0 +1,186 @@
+/*
+ * Copyright (C) 2012,2013 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * Derived from arch/arm/include/asm/kvm_host.h:
+ * Copyright (C) 2012 - Virtual Open Systems and Columbia University
+ * Author: Christoffer Dall <c.dall@virtualopensystems.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __ARM64_KVM_HOST_H__
+#define __ARM64_KVM_HOST_H__
+
+#include <asm/kvm.h>
+#include <asm/kvm_asm.h>
+#include <asm/kvm_mmio.h>
+
+#define KVM_MAX_VCPUS 4
+#define KVM_USER_MEM_SLOTS 32
+#define KVM_PRIVATE_MEM_SLOTS 4
+#define KVM_COALESCED_MMIO_PAGE_OFFSET 1
+
+#include <kvm/arm_vgic.h>
+#include <kvm/arm_arch_timer.h>
+
+#define KVM_VCPU_MAX_FEATURES 0
+
+/* We don't currently support large pages. */
+#define KVM_HPAGE_GFN_SHIFT(x)	0
+#define KVM_NR_PAGE_SIZES	1
+#define KVM_PAGES_PER_HPAGE(x)	(1UL<<31)
+
+struct kvm_vcpu;
+int kvm_target_cpu(void);
+int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
+int kvm_arch_dev_ioctl_check_extension(long ext);
+
+struct kvm_arch {
+	/* The VMID generation used for the virt. memory system */
+	u64    vmid_gen;
+	u32    vmid;
+
+	/* 1-level 2nd stage table and lock */
+	spinlock_t pgd_lock;
+	pgd_t *pgd;
+
+	/* VTTBR value associated with above pgd and vmid */
+	u64    vttbr;
+
+	/* Interrupt controller */
+	struct vgic_dist	vgic;
+
+	/* Timer */
+	struct arch_timer_kvm	timer;
+};
+
+#define KVM_NR_MEM_OBJS     40
+
+/*
+ * We don't want allocation failures within the mmu code, so we preallocate
+ * enough memory for a single page fault in a cache.
+ */
+struct kvm_mmu_memory_cache {
+	int nobjs;
+	void *objects[KVM_NR_MEM_OBJS];
+};
+
+struct kvm_vcpu_fault_info {
+	u32 esr_el2;		/* Hyp Syndrom Register */
+	u64 far_el2;		/* Hyp Fault Address Register */
+	u64 hpfar_el2;		/* Hyp IPA Fault Address Register */
+};
+
+struct kvm_cpu_context {
+	struct kvm_regs	gp_regs;
+	u64 sys_regs[NR_SYS_REGS];
+};
+
+typedef struct kvm_cpu_context kvm_cpu_context_t;
+
+struct kvm_vcpu_arch {
+	struct kvm_cpu_context ctxt;
+
+	/* HYP configuration */
+	u64 hcr_el2;
+
+	/* Exception Information */
+	struct kvm_vcpu_fault_info fault;
+
+	/* Pointer to host CPU context */
+	kvm_cpu_context_t *host_cpu_context;
+
+	/* VGIC state */
+	struct vgic_cpu vgic_cpu;
+	struct arch_timer_cpu timer_cpu;
+
+	/*
+	 * Anything that is not used directly from assembly code goes
+	 * here.
+	 */
+	/* dcache set/way operation pending */
+	int last_pcpu;
+	cpumask_t require_dcache_flush;
+
+	/* Don't run the guest */
+	bool pause;
+
+	/* IO related fields */
+	struct kvm_decode mmio_decode;
+
+	/* Interrupt related fields */
+	u64 irq_lines;		/* IRQ and FIQ levels */
+
+	/* Cache some mmu pages needed inside spinlock regions */
+	struct kvm_mmu_memory_cache mmu_page_cache;
+
+	/* Target CPU and feature flags */
+	u32 target;
+	DECLARE_BITMAP(features, KVM_VCPU_MAX_FEATURES);
+
+	/* Detect first run of a vcpu */
+	bool has_run_once;
+};
+
+#define vcpu_gp_regs(v)		(&(v)->arch.ctxt.gp_regs)
+#define vcpu_sys_reg(v,r)	((v)->arch.ctxt.sys_regs[(r)])
+#define vcpu_cp15(v,r)		((v)->arch.ctxt.cp15[(r)])
+
+struct kvm_vm_stat {
+	u32 remote_tlb_flush;
+};
+
+struct kvm_vcpu_stat {
+	u32 halt_wakeup;
+};
+
+struct kvm_vcpu_init;
+int kvm_vcpu_set_target(struct kvm_vcpu *vcpu,
+			const struct kvm_vcpu_init *init);
+unsigned long kvm_arm_num_regs(struct kvm_vcpu *vcpu);
+int kvm_arm_copy_reg_indices(struct kvm_vcpu *vcpu, u64 __user *indices);
+struct kvm_one_reg;
+int kvm_arm_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
+int kvm_arm_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg);
+
+#define KVM_ARCH_WANT_MMU_NOTIFIER
+struct kvm;
+int kvm_unmap_hva(struct kvm *kvm, unsigned long hva);
+int kvm_unmap_hva_range(struct kvm *kvm,
+			unsigned long start, unsigned long end);
+void kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
+
+/* We do not have shadow page tables, hence the empty hooks */
+static inline int kvm_age_hva(struct kvm *kvm, unsigned long hva)
+{
+	return 0;
+}
+
+static inline int kvm_test_age_hva(struct kvm *kvm, unsigned long hva)
+{
+	return 0;
+}
+
+struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
+struct kvm_vcpu __percpu **kvm_get_running_vcpus(void);
+
+u64 kvm_call_hyp(void *hypfn, ...);
+
+int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
+		int exception_index);
+
+int kvm_perf_init(void);
+int kvm_perf_teardown(void);
+
+#endif /* __ARM64_KVM_HOST_H__ */
