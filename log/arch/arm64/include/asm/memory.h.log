commit e16e65a02913d29a7b27c4e3a415ceec967b0629
Author: Ard Biesheuvel <ardb@kernel.org>
Date:   Sun Mar 29 16:12:58 2020 +0200

    arm64: remove CONFIG_DEBUG_ALIGN_RODATA feature
    
    When CONFIG_DEBUG_ALIGN_RODATA is enabled, kernel segments mapped with
    different permissions (r-x for .text, r-- for .rodata, rw- for .data,
    etc) are rounded up to 2 MiB so they can be mapped more efficiently.
    In particular, it permits the segments to be mapped using level 2
    block entries when using 4k pages, which is expected to result in less
    TLB pressure.
    
    However, the mappings for the bulk of the kernel will use level 2
    entries anyway, and the misaligned fringes are organized such that they
    can take advantage of the contiguous bit, and use far fewer level 3
    entries than would be needed otherwise.
    
    This makes the value of this feature dubious at best, and since it is not
    enabled in defconfig or in the distro configs, it does not appear to be
    in wide use either. So let's just remove it.
    
    Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Laura Abbott <labbott@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 2be67b232499..a1871bb32bb1 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -120,22 +120,12 @@
 
 /*
  * Alignment of kernel segments (e.g. .text, .data).
- */
-#if defined(CONFIG_DEBUG_ALIGN_RODATA)
-/*
- *  4 KB granule:   1 level 2 entry
- * 16 KB granule: 128 level 3 entries, with contiguous bit
- * 64 KB granule:  32 level 3 entries, with contiguous bit
- */
-#define SEGMENT_ALIGN		SZ_2M
-#else
-/*
+ *
  *  4 KB granule:  16 level 3 entries, with contiguous bit
  * 16 KB granule:   4 level 3 entries, without contiguous bit
  * 64 KB granule:   1 level 3 entry
  */
 #define SEGMENT_ALIGN		SZ_64K
-#endif
 
 /*
  * Memory types available.

commit bbd6ec605c0fc286c3f8ce60b4ed44635361d58b
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed Mar 4 09:58:43 2020 +0530

    arm64/mm: Enable memory hot remove
    
    The arch code for hot-remove must tear down portions of the linear map and
    vmemmap corresponding to memory being removed. In both cases the page
    tables mapping these regions must be freed, and when sparse vmemmap is in
    use the memory backing the vmemmap must also be freed.
    
    This patch adds unmap_hotplug_range() and free_empty_tables() helpers which
    can be used to tear down either region and calls it from vmemmap_free() and
    ___remove_pgd_mapping(). The free_mapped argument determines whether the
    backing memory will be freed.
    
    It makes two distinct passes over the kernel page table. In the first pass
    with unmap_hotplug_range() it unmaps, invalidates applicable TLB cache and
    frees backing memory if required (vmemmap) for each mapped leaf entry. In
    the second pass with free_empty_tables() it looks for empty page table
    sections whose page table page can be unmapped, TLB invalidated and freed.
    
    While freeing intermediate level page table pages bail out if any of its
    entries are still valid. This can happen for partially filled kernel page
    table either from a previously attempted failed memory hot add or while
    removing an address range which does not span the entire page table page
    range.
    
    The vmemmap region may share levels of table with the vmalloc region.
    There can be conflicts between hot remove freeing page table pages with
    a concurrent vmalloc() walking the kernel page table. This conflict can
    not just be solved by taking the init_mm ptl because of existing locking
    scheme in vmalloc(). So free_empty_tables() implements a floor and ceiling
    method which is borrowed from user page table tear with free_pgd_range()
    which skips freeing page table pages if intermediate address range is not
    aligned or maximum floor-ceiling might not own the entire page table page.
    
    Boot memory on arm64 cannot be removed. Hence this registers a new memory
    hotplug notifier which prevents boot memory offlining and it's removal.
    
    While here update arch_add_memory() to handle __add_pages() failures by
    just unmapping recently added kernel linear mapping. Now enable memory hot
    remove on arm64 platforms by default with ARCH_ENABLE_MEMORY_HOTREMOVE.
    
    This implementation is overall inspired from kernel page table tear down
    procedure on X86 architecture and user page table tear down method.
    
    [Mike and Catalin added P4D page table level support]
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 4d94676e5a8b..2be67b232499 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -54,6 +54,7 @@
 #define MODULES_VADDR		(BPF_JIT_REGION_END)
 #define MODULES_VSIZE		(SZ_128M)
 #define VMEMMAP_START		(-VMEMMAP_SIZE - SZ_2M)
+#define VMEMMAP_END		(VMEMMAP_START + VMEMMAP_SIZE)
 #define PCI_IO_END		(VMEMMAP_START - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)

commit d0022c0ef29b78bcbe8a5c5894bd2307143afce1
Author: Will Deacon <will@kernel.org>
Date:   Wed Feb 19 10:19:13 2020 +0000

    arm64: memory: Add missing brackets to untagged_addr() macro
    
    Add brackets around the evaluation of the 'addr' parameter to the
    untagged_addr() macro so that the cast to 'u64' applies to the result
    of the expression.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 597399d0cb91 ("arm64: tags: Preserve tags for addresses translated via TTBR1")
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index a4f9ca5479b0..4d94676e5a8b 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -213,7 +213,7 @@ static inline unsigned long kaslr_offset(void)
 	((__force __typeof__(addr))sign_extend64((__force u64)(addr), 55))
 
 #define untagged_addr(addr)	({					\
-	u64 __addr = (__force u64)addr;					\
+	u64 __addr = (__force u64)(addr);					\
 	__addr &= __untagged_addr(__addr);				\
 	(__force __typeof__(addr))__addr;				\
 })

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit 218564b164ad9d283d3cb3d5367705726123a610
Author: Bhupesh Sharma <bhsharma@redhat.com>
Date:   Tue Nov 5 03:26:46 2019 +0530

    arm64: mm: Remove MAX_USER_VA_BITS definition
    
    commit 9b31cf493ffa ("arm64: mm: Introduce MAX_USER_VA_BITS definition")
    introduced the MAX_USER_VA_BITS definition, which was used to support
    the arm64 mm use-cases where the user-space could use 52-bit virtual
    addresses whereas the kernel-space would still could a maximum of 48-bit
    virtual addressing.
    
    But, now with commit b6d00d47e81a ("arm64: mm: Introduce 52-bit Kernel
    VAs"), we removed the 52-bit user/48-bit kernel kconfig option and hence
    there is no longer any scenario where user VA != kernel VA size
    (even with CONFIG_ARM64_FORCE_52BIT enabled, the same is true).
    
    Hence we can do away with the MAX_USER_VA_BITS macro as it is equal to
    VA_BITS (maximum VA space size) in all possible use-cases. Note that
    even though the 'vabits_actual' value would be 48 for arm64 hardware
    which don't support LVA-8.2 extension (even when CONFIG_ARM64_VA_BITS_52
    is enabled), VA_BITS would still be set to a value 52. Hence this change
    would be safe in all possible VA address space combinations.
    
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Steve Capper <steve.capper@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: linux-kernel@vger.kernel.org
    Cc: kexec@lists.infradead.org
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Bhupesh Sharma <bhsharma@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b61b50bf68b1..4867e58dbc9c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -69,12 +69,6 @@
 #define KERNEL_START		_text
 #define KERNEL_END		_end
 
-#ifdef CONFIG_ARM64_VA_BITS_52
-#define MAX_USER_VA_BITS	52
-#else
-#define MAX_USER_VA_BITS	VA_BITS
-#endif
-
 /*
  * Generic and tag-based KASAN require 1/8th and 1/16th of the kernel virtual
  * address space for the shadow region respectively. They can bloat the stack

commit 597399d0cb91d049fcb78fb45c7694771b583bb7
Author: Will Deacon <will@kernel.org>
Date:   Tue Oct 15 21:04:18 2019 -0700

    arm64: tags: Preserve tags for addresses translated via TTBR1
    
    Sign-extending TTBR1 addresses when converting to an untagged address
    breaks the documented POSIX semantics for mlock() in some obscure error
    cases where we end up returning -EINVAL instead of -ENOMEM as a direct
    result of rewriting the upper address bits.
    
    Rework the untagged_addr() macro to preserve the upper address bits for
    TTBR1 addresses and only clear the tag bits for user addresses. This
    matches the behaviour of the 'clear_address_tag' assembly macro, so
    rename that and align the implementations at the same time so that they
    use the same instruction sequences for the tag manipulation.
    
    Link: https://lore.kernel.org/stable/20191014162651.GF19200@arrakis.emea.arm.com/
    Reported-by: Jan Stancek <jstancek@redhat.com>
    Tested-by: Jan Stancek <jstancek@redhat.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Tested-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b61b50bf68b1..c23c47360664 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -215,12 +215,18 @@ static inline unsigned long kaslr_offset(void)
  * up with a tagged userland pointer. Clear the tag to get a sane pointer to
  * pass on to access_ok(), for instance.
  */
-#define untagged_addr(addr)	\
+#define __untagged_addr(addr)	\
 	((__force __typeof__(addr))sign_extend64((__force u64)(addr), 55))
 
+#define untagged_addr(addr)	({					\
+	u64 __addr = (__force u64)addr;					\
+	__addr &= __untagged_addr(__addr);				\
+	(__force __typeof__(addr))__addr;				\
+})
+
 #ifdef CONFIG_KASAN_SW_TAGS
 #define __tag_shifted(tag)	((u64)(tag) << 56)
-#define __tag_reset(addr)	untagged_addr(addr)
+#define __tag_reset(addr)	__untagged_addr(addr)
 #define __tag_get(addr)		(__u8)((u64)(addr) >> 56)
 #else
 #define __tag_shifted(tag)	0UL

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit 77ad4ce69321abbe26ec92b2a2691a66531eb688
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Aug 14 14:28:48 2019 +0100

    arm64: memory: rename VA_START to PAGE_END
    
    Prior to commit:
    
      14c127c957c1c607 ("arm64: mm: Flip kernel VA space")
    
    ... VA_START described the start of the TTBR1 address space for a given
    VA size described by VA_BITS, where all kernel mappings began.
    
    Since that commit, VA_START described a portion midway through the
    address space, where the linear map ends and other kernel mappings
    begin.
    
    To avoid confusion, let's rename VA_START to PAGE_END, making it clear
    that it's not the start of the TTBR1 address space and implying that
    it's related to PAGE_OFFSET. Comments and other mnemonics are updated
    accordingly, along with a typo fix in the decription of VMEMMAP_SIZE.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index d69c2865ae40..a713bad71db5 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -28,20 +28,20 @@
  *                a struct page array
  *
  * If we are configured with a 52-bit kernel VA then our VMEMMAP_SIZE
- * neads to cover the memory region from the beginning of the 52-bit
- * PAGE_OFFSET all the way to VA_START for 48-bit. This allows us to
+ * needs to cover the memory region from the beginning of the 52-bit
+ * PAGE_OFFSET all the way to PAGE_END for 48-bit. This allows us to
  * keep a constant PAGE_OFFSET and "fallback" to using the higher end
  * of the VMEMMAP where 52-bit support is not available in hardware.
  */
-#define VMEMMAP_SIZE ((_VA_START(VA_BITS_MIN) - PAGE_OFFSET) \
+#define VMEMMAP_SIZE ((_PAGE_END(VA_BITS_MIN) - PAGE_OFFSET) \
 			>> (PAGE_SHIFT - STRUCT_PAGE_MAX_SHIFT))
 
 /*
- * PAGE_OFFSET - the virtual address of the start of the linear map (top
- *		 (VA_BITS - 1))
- * KIMAGE_VADDR - the virtual address of the start of the kernel image
+ * PAGE_OFFSET - the virtual address of the start of the linear map, at the
+ *               start of the TTBR1 address space.
+ * PAGE_END - the end of the linear map, where all other kernel mappings begin.
+ * KIMAGE_VADDR - the virtual address of the start of the kernel image.
  * VA_BITS - the maximum number of bits for virtual addresses.
- * VA_START - the first kernel virtual address.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
 #define _PAGE_OFFSET(va)	(-(UL(1) << (va)))
@@ -64,7 +64,7 @@
 #define VA_BITS_MIN		(VA_BITS)
 #endif
 
-#define _VA_START(va)		(-(UL(1) << ((va) - 1)))
+#define _PAGE_END(va)		(-(UL(1) << ((va) - 1)))
 
 #define KERNEL_START		_text
 #define KERNEL_END		_end
@@ -87,7 +87,7 @@
 #define KASAN_THREAD_SHIFT	1
 #else
 #define KASAN_THREAD_SHIFT	0
-#define KASAN_SHADOW_END	(_VA_START(VA_BITS_MIN))
+#define KASAN_SHADOW_END	(_PAGE_END(VA_BITS_MIN))
 #endif /* CONFIG_KASAN */
 
 #define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)
@@ -173,7 +173,7 @@
 
 #ifndef __ASSEMBLY__
 extern u64			vabits_actual;
-#define VA_START		(_VA_START(vabits_actual))
+#define PAGE_END		(_PAGE_END(vabits_actual))
 
 #include <linux/bitops.h>
 #include <linux/mmdebug.h>

commit d0b3c32ed9220616548ff63808751cf2f6608df1
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 17:22:51 2019 +0100

    arm64: memory: Cosmetic cleanups
    
    Cleanup memory.h so that the indentation is consistent, remove pointless
    line-wrapping and use consistent parameter names for different versions
    of the same macro.
    
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 27f35ce2e2ed..d69c2865ae40 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -12,10 +12,10 @@
 
 #include <linux/compiler.h>
 #include <linux/const.h>
+#include <linux/sizes.h>
 #include <linux/types.h>
 #include <asm/bug.h>
 #include <asm/page-def.h>
-#include <linux/sizes.h>
 
 /*
  * Size of the PCI I/O space. This must remain a power of two so that
@@ -66,8 +66,8 @@
 
 #define _VA_START(va)		(-(UL(1) << ((va) - 1)))
 
-#define KERNEL_START      _text
-#define KERNEL_END        _end
+#define KERNEL_START		_text
+#define KERNEL_END		_end
 
 #ifdef CONFIG_ARM64_VA_BITS_52
 #define MAX_USER_VA_BITS	52
@@ -132,14 +132,14 @@
  * 16 KB granule: 128 level 3 entries, with contiguous bit
  * 64 KB granule:  32 level 3 entries, with contiguous bit
  */
-#define SEGMENT_ALIGN			SZ_2M
+#define SEGMENT_ALIGN		SZ_2M
 #else
 /*
  *  4 KB granule:  16 level 3 entries, with contiguous bit
  * 16 KB granule:   4 level 3 entries, without contiguous bit
  * 64 KB granule:   1 level 3 entry
  */
-#define SEGMENT_ALIGN			SZ_64K
+#define SEGMENT_ALIGN		SZ_64K
 #endif
 
 /*
@@ -253,8 +253,7 @@ static inline const void *__tag_set(const void *addr, u8 tag)
 
 #define __virt_to_phys_nodebug(x) ({					\
 	phys_addr_t __x = (phys_addr_t)(__tag_reset(x));		\
-	__is_lm_address(__x) ? __lm_to_phys(__x) :			\
-			       __kimg_to_phys(__x);			\
+	__is_lm_address(__x) ? __lm_to_phys(__x) : __kimg_to_phys(__x);	\
 })
 
 #define __pa_symbol_nodebug(x)	__kimg_to_phys((phys_addr_t)(x))
@@ -301,17 +300,17 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __pa_nodebug(x)		__virt_to_phys_nodebug((unsigned long)(x))
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
-#define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))
-#define sym_to_pfn(x)	    __phys_to_pfn(__pa_symbol(x))
+#define virt_to_pfn(x)		__phys_to_pfn(__virt_to_phys((unsigned long)(x)))
+#define sym_to_pfn(x)		__phys_to_pfn(__pa_symbol(x))
 
 /*
- *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
- *  virt_addr_valid(k)	indicates whether a virtual address is valid
+ *  virt_to_page(x)	convert a _valid_ virtual address to struct page *
+ *  virt_addr_valid(x)	indicates whether a virtual address is valid
  */
 #define ARCH_PFN_OFFSET		((unsigned long)PHYS_PFN_OFFSET)
 
 #if !defined(CONFIG_SPARSEMEM_VMEMMAP) || defined(CONFIG_DEBUG_VIRTUAL)
-#define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
+#define virt_to_page(x)		pfn_to_page(virt_to_pfn(x))
 #else
 #define page_to_virt(x)	({						\
 	__typeof__(x) __page = x;					\

commit 68933aa973740796895e297e7dbf7baf3e9c51b1
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 17:06:29 2019 +0100

    arm64: memory: Add comments to end of non-trivial #ifdef blocks
    
    Commenting the #endif of a multi-statement #ifdef block with the
    condition which guards it is useful and can save having to scroll back
    through the file to figure out which set of Kconfig options apply to
    a particular piece of code.
    
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index fb0062555305..27f35ce2e2ed 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -57,11 +57,13 @@
 #define PCI_IO_END		(VMEMMAP_START - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
+
 #if VA_BITS > 48
 #define VA_BITS_MIN		(48)
 #else
 #define VA_BITS_MIN		(VA_BITS)
 #endif
+
 #define _VA_START(va)		(-(UL(1) << ((va) - 1)))
 
 #define KERNEL_START      _text
@@ -86,7 +88,7 @@
 #else
 #define KASAN_THREAD_SHIFT	0
 #define KASAN_SHADOW_END	(_VA_START(VA_BITS_MIN))
-#endif
+#endif /* CONFIG_KASAN */
 
 #define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)
 
@@ -224,7 +226,7 @@ static inline unsigned long kaslr_offset(void)
 #define __tag_shifted(tag)	0UL
 #define __tag_reset(addr)	(addr)
 #define __tag_get(addr)		0
-#endif
+#endif /* CONFIG_KASAN_SW_TAGS */
 
 static inline const void *__tag_set(const void *addr, u8 tag)
 {
@@ -263,7 +265,7 @@ extern phys_addr_t __phys_addr_symbol(unsigned long x);
 #else
 #define __virt_to_phys(x)	__virt_to_phys_nodebug(x)
 #define __phys_addr_symbol(x)	__pa_symbol_nodebug(x)
-#endif
+#endif /* CONFIG_DEBUG_VIRTUAL */
 
 #define __phys_to_virt(x)	((unsigned long)((x) - physvirt_offset))
 #define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
@@ -323,14 +325,14 @@ static inline void *phys_to_virt(phys_addr_t x)
 	u64 __addr = VMEMMAP_START + (__idx * sizeof(struct page));	\
 	(struct page *)__addr;						\
 })
-#endif
+#endif /* !CONFIG_SPARSEMEM_VMEMMAP || CONFIG_DEBUG_VIRTUAL */
 
 #define virt_addr_valid(addr)	({					\
 	__typeof__(addr) __addr = addr;					\
 	__is_lm_address(__addr) && pfn_valid(virt_to_pfn(__addr));	\
 })
 
-#endif
+#endif /* !ASSEMBLY */
 
 /*
  * Given that the GIC architecture permits ITS implementations that can only be
@@ -345,4 +347,4 @@ static inline void *phys_to_virt(phys_addr_t x)
 
 #include <asm-generic/memory_model.h>
 
-#endif
+#endif /* __ASM_MEMORY_H */

commit 6bbd497f027332b14cf2a6792c418c32286b66c2
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 17:01:05 2019 +0100

    arm64: memory: Implement __tag_set() as common function
    
    There's no need for __tag_set() to be a complicated macro when
    CONFIG_KASAN_SW_TAGS=y and a simple static inline otherwise. Rewrite
    the thing as a common static inline function.
    
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index a7a985602cba..fb0062555305 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -218,20 +218,20 @@ static inline unsigned long kaslr_offset(void)
 
 #ifdef CONFIG_KASAN_SW_TAGS
 #define __tag_shifted(tag)	((u64)(tag) << 56)
-#define __tag_set(addr, tag)	(__typeof__(addr))( \
-		((u64)(addr) & ~__tag_shifted(0xff)) | __tag_shifted(tag))
 #define __tag_reset(addr)	untagged_addr(addr)
 #define __tag_get(addr)		(__u8)((u64)(addr) >> 56)
 #else
-static inline const void *__tag_set(const void *addr, u8 tag)
-{
-	return addr;
-}
-
+#define __tag_shifted(tag)	0UL
 #define __tag_reset(addr)	(addr)
 #define __tag_get(addr)		0
 #endif
 
+static inline const void *__tag_set(const void *addr, u8 tag)
+{
+	u64 __addr = (u64)addr & ~__tag_shifted(0xff);
+	return (const void *)(__addr | __tag_shifted(tag));
+}
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit a5ac40f53bfa5e43d9c76e3c23415ee4afd71932
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 16:58:36 2019 +0100

    arm64: memory: Simplify _VA_START and _PAGE_OFFSET definitions
    
    Rather than subtracting from -1 and then adding 1, we can simply
    subtract from 0.
    
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index e6353d1a75fa..a7a985602cba 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -44,8 +44,7 @@
  * VA_START - the first kernel virtual address.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
-#define _PAGE_OFFSET(va)	(UL(0xffffffffffffffff) - \
-					(UL(1) << (va)) + 1)
+#define _PAGE_OFFSET(va)	(-(UL(1) << (va)))
 #define PAGE_OFFSET		(_PAGE_OFFSET(VA_BITS))
 #define KIMAGE_VADDR		(MODULES_END)
 #define BPF_JIT_REGION_START	(KASAN_SHADOW_END)
@@ -63,8 +62,7 @@
 #else
 #define VA_BITS_MIN		(VA_BITS)
 #endif
-#define _VA_START(va)		(UL(0xffffffffffffffff) - \
-				(UL(1) << ((va) - 1)) + 1)
+#define _VA_START(va)		(-(UL(1) << ((va) - 1)))
 
 #define KERNEL_START      _text
 #define KERNEL_END        _end

commit 9ba33dcc6bef4e56c762b446f0000f27ee737b8b
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 16:34:32 2019 +0100

    arm64: memory: Simplify virt_to_page() implementation
    
    Build virt_to_page() on top of virt_to_pfn() so we can avoid the need
    for explicit shifting.
    
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 636d414608cb..e6353d1a75fa 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -311,7 +311,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define ARCH_PFN_OFFSET		((unsigned long)PHYS_PFN_OFFSET)
 
 #if !defined(CONFIG_SPARSEMEM_VMEMMAP) || defined(CONFIG_DEBUG_VIRTUAL)
-#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #else
 #define page_to_virt(x)	({						\
 	__typeof__(x) __page = x;					\

commit 96628f0fb18080a4166fc9eab8f7fd062d860667
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 16:46:11 2019 +0100

    arm64: memory: Rewrite default page_to_virt()/virt_to_page()
    
    The default implementations of page_to_virt() and virt_to_page() are
    fairly confusing to read and the former evaluates its 'page' parameter
    twice in the macro
    
    Rewrite them so that the computation is expressed as 'base + index' in
    both cases and the parameter is always evaluated exactly once.
    
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 243e05ad4a67..636d414608cb 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -313,19 +313,18 @@ static inline void *phys_to_virt(phys_addr_t x)
 #if !defined(CONFIG_SPARSEMEM_VMEMMAP) || defined(CONFIG_DEBUG_VIRTUAL)
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #else
-#define __virt_to_pgoff(kaddr)	(((u64)(kaddr) - PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
-#define __page_to_voff(kaddr)	(((u64)(kaddr) - VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
-
-#define page_to_virt(page)	({					\
-	unsigned long __addr =						\
-		((__page_to_voff(page)) + PAGE_OFFSET);			\
-	const void *__addr_tag =					\
-		__tag_set((void *)__addr, page_kasan_tag(page));	\
-	((void *)__addr_tag);						\
+#define page_to_virt(x)	({						\
+	__typeof__(x) __page = x;					\
+	u64 __idx = ((u64)__page - VMEMMAP_START) / sizeof(struct page);\
+	u64 __addr = PAGE_OFFSET + (__idx * PAGE_SIZE);			\
+	(void *)__tag_set((const void *)__addr, page_kasan_tag(__page));\
 })
 
-#define virt_to_page(vaddr)	\
-	((struct page *)((__virt_to_pgoff(__tag_reset(vaddr))) + VMEMMAP_START))
+#define virt_to_page(x)	({						\
+	u64 __idx = (__tag_reset((u64)x) - PAGE_OFFSET) / PAGE_SIZE;	\
+	u64 __addr = VMEMMAP_START + (__idx * sizeof(struct page));	\
+	(struct page *)__addr;						\
+})
 #endif
 
 #define virt_addr_valid(addr)	({					\

commit 577c2b35283fbadcc9ce4b56304ccea3ec8a5ca1
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 16:26:54 2019 +0100

    arm64: memory: Ensure address tag is masked in conversion macros
    
    When converting a linear virtual address to a physical address, pfn or
    struct page *, we must make sure that the tag bits are masked before the
    calculation otherwise we end up with corrupt pointers when running with
    CONFIG_KASAN_SW_TAGS=y:
    
      | Unable to handle kernel paging request at virtual address 0037fe0007580d08
      | [0037fe0007580d08] address between user and kernel address ranges
    
    Mask out the tag in __virt_to_phys_nodebug() and virt_to_page().
    
    Reported-by: Qian Cai <cai@lca.pw>
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 9cb1c5ddd2c4 ("arm64: mm: Remove bit-masking optimisations for PAGE_OFFSET and VMEMMAP_START")
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 93ef8e5c6971..243e05ad4a67 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -252,7 +252,7 @@ static inline const void *__tag_set(const void *addr, u8 tag)
 #define __kimg_to_phys(addr)	((addr) - kimage_voffset)
 
 #define __virt_to_phys_nodebug(x) ({					\
-	phys_addr_t __x = (phys_addr_t)(x);				\
+	phys_addr_t __x = (phys_addr_t)(__tag_reset(x));		\
 	__is_lm_address(__x) ? __lm_to_phys(__x) :			\
 			       __kimg_to_phys(__x);			\
 })
@@ -324,7 +324,8 @@ static inline void *phys_to_virt(phys_addr_t x)
 	((void *)__addr_tag);						\
 })
 
-#define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) + VMEMMAP_START))
+#define virt_to_page(vaddr)	\
+	((struct page *)((__virt_to_pgoff(__tag_reset(vaddr))) + VMEMMAP_START))
 #endif
 
 #define virt_addr_valid(addr)	({					\

commit 68dd8ef321626f14ae9ef2039b7a03c707149489
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 13 15:52:23 2019 +0100

    arm64: memory: Fix virt_addr_valid() using __is_lm_address()
    
    virt_addr_valid() is intended to test whether or not the passed address
    is a valid linear map address. Unfortunately, it relies on
    _virt_addr_is_linear() which is broken because it assumes the linear
    map is at the top of the address space, which it no longer is.
    
    Reimplement virt_addr_valid() using __is_lm_address() and remove
    _virt_addr_is_linear() entirely. At the same time, ensure we evaluate
    the macro parameter only once and move it within the __ASSEMBLY__ block.
    
    Reported-by: Qian Cai <cai@lca.pw>
    Reported-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 14c127c957c1 ("arm64: mm: Flip kernel VA space")
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 2c3c4b145e95..93ef8e5c6971 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -242,11 +242,11 @@ static inline const void *__tag_set(const void *addr, u8 tag)
 
 
 /*
- * The linear kernel range starts in the middle of the virtual adddress
+ * The linear kernel range starts at the bottom of the virtual address
  * space. Testing the top bit for the start of the region is a
- * sufficient check.
+ * sufficient check and avoids having to worry about the tag.
  */
-#define __is_lm_address(addr)	(!((addr) & BIT(vabits_actual - 1)))
+#define __is_lm_address(addr)	(!(((u64)addr) & BIT(vabits_actual - 1)))
 
 #define __lm_to_phys(addr)	(((addr) + physvirt_offset))
 #define __kimg_to_phys(addr)	((addr) - kimage_voffset)
@@ -326,13 +326,13 @@ static inline void *phys_to_virt(phys_addr_t x)
 
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) + VMEMMAP_START))
 #endif
-#endif
 
-#define _virt_addr_is_linear(kaddr)	\
-	(__tag_reset((u64)(kaddr)) >= PAGE_OFFSET)
+#define virt_addr_valid(addr)	({					\
+	__typeof__(addr) __addr = addr;					\
+	__is_lm_address(__addr) && pfn_valid(virt_to_pfn(__addr));	\
+})
 
-#define virt_addr_valid(kaddr)		\
-	(_virt_addr_is_linear(kaddr) && pfn_valid(virt_to_pfn(kaddr)))
+#endif
 
 /*
  * Given that the GIC architecture permits ITS implementations that can only be

commit 9c1cac424c93d2b3122014e07a54b003ddedc168
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 9 15:39:37 2019 +0100

    arm64: mm: Really fix sparse warning in untagged_addr()
    
    untagged_addr() can be called with a '__user' pointer parameter and must
    therefore use '__force' casts both when passing this parameter through
    to sign_extend64() as a 'u64', but also when casting the 's64' return
    value back to the '__user' pointer type.
    
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 46c4c08a80a9..76e0b232a473 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -201,7 +201,7 @@ extern u64			vabits_user;
  * pass on to access_ok(), for instance.
  */
 #define untagged_addr(addr)	\
-	((__typeof__(addr))sign_extend64((__force u64)(addr), 55))
+	((__force __typeof__(addr))sign_extend64((__force u64)(addr), 55))
 
 #ifdef CONFIG_KASAN_SW_TAGS
 #define __tag_shifted(tag)	((u64)(tag) << 56)

commit d2d73d2fef421ca0d447946cc430fdf5c4c5b06a
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 9 15:27:32 2019 +0100

    arm64: mm: Simplify definition of virt_addr_valid()
    
    _virt_addr_valid() is defined as the same value in two places and rolls
    its own version of virt_to_pfn() in both cases.
    
    Consolidate these definitions by inlining a simplified version directly
    into virt_addr_valid().
    
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index ecc945ba8607..2c3c4b145e95 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -312,7 +312,6 @@ static inline void *phys_to_virt(phys_addr_t x)
 
 #if !defined(CONFIG_SPARSEMEM_VMEMMAP) || defined(CONFIG_DEBUG_VIRTUAL)
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #else
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) - PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(kaddr) - VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
@@ -326,15 +325,14 @@ static inline void *phys_to_virt(phys_addr_t x)
 })
 
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) + VMEMMAP_START))
-
-#define _virt_addr_valid(kaddr)	pfn_valid(__virt_to_phys((u64)(kaddr)) >> PAGE_SHIFT)
 #endif
 #endif
 
 #define _virt_addr_is_linear(kaddr)	\
 	(__tag_reset((u64)(kaddr)) >= PAGE_OFFSET)
+
 #define virt_addr_valid(kaddr)		\
-	(_virt_addr_is_linear(kaddr) && _virt_addr_valid(kaddr))
+	(_virt_addr_is_linear(kaddr) && pfn_valid(virt_to_pfn(kaddr)))
 
 /*
  * Given that the GIC architecture permits ITS implementations that can only be

commit 2c624fe68715e76eba1a7089f91e122310dc663c
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:23 2019 +0100

    arm64: mm: Remove vabits_user
    
    Previous patches have enabled 52-bit kernel + user VAs and there is no
    longer any scenario where user VA != kernel VA size.
    
    This patch removes the, now redundant, vabits_user variable and replaces
    usage with vabits_actual where appropriate.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index d911d0573460..ecc945ba8607 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -194,9 +194,6 @@ static inline unsigned long kaslr_offset(void)
 	return kimage_vaddr - KIMAGE_VADDR;
 }
 
-/* the actual size of a user virtual address */
-extern u64			vabits_user;
-
 /*
  * Allow all memory at the discovery stage. We will clip it later.
  */

commit b6d00d47e81a49f6cf462518c10408f37a3e6785
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:22 2019 +0100

    arm64: mm: Introduce 52-bit Kernel VAs
    
    Most of the machinery is now in place to enable 52-bit kernel VAs that
    are detectable at boot time.
    
    This patch adds a Kconfig option for 52-bit user and kernel addresses
    and plumbs in the requisite CONFIG_ macros as well as sets TCR.T1SZ,
    physvirt_offset and vmemmap at early boot.
    
    To simplify things this patch also removes the 52-bit user/48-bit kernel
    kconfig option.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 0204c2006c92..d911d0573460 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -44,8 +44,9 @@
  * VA_START - the first kernel virtual address.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
-#define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
-	(UL(1) << VA_BITS) + 1)
+#define _PAGE_OFFSET(va)	(UL(0xffffffffffffffff) - \
+					(UL(1) << (va)) + 1)
+#define PAGE_OFFSET		(_PAGE_OFFSET(VA_BITS))
 #define KIMAGE_VADDR		(MODULES_END)
 #define BPF_JIT_REGION_START	(KASAN_SHADOW_END)
 #define BPF_JIT_REGION_SIZE	(SZ_128M)
@@ -68,7 +69,7 @@
 #define KERNEL_START      _text
 #define KERNEL_END        _end
 
-#ifdef CONFIG_ARM64_USER_VA_BITS_52
+#ifdef CONFIG_ARM64_VA_BITS_52
 #define MAX_USER_VA_BITS	52
 #else
 #define MAX_USER_VA_BITS	VA_BITS

commit ce3aaed87344c83c77135f80e7b76e1da9c92ee6
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:21 2019 +0100

    arm64: mm: Modify calculation of VMEMMAP_SIZE
    
    In a later patch we will need to have a slightly larger VMEMMAP region
    to accommodate boot time selection between 48/52-bit kernel VAs.
    
    This patch modifies the formula for computing VMEMMAP_SIZE to depend
    explicitly on the PAGE_OFFSET and start of kernel addressable memory.
    (This allows for a slightly larger direct linear map in future).
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 364635b8370a..0204c2006c92 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -26,8 +26,15 @@
 /*
  * VMEMMAP_SIZE - allows the whole linear region to be covered by
  *                a struct page array
+ *
+ * If we are configured with a 52-bit kernel VA then our VMEMMAP_SIZE
+ * neads to cover the memory region from the beginning of the 52-bit
+ * PAGE_OFFSET all the way to VA_START for 48-bit. This allows us to
+ * keep a constant PAGE_OFFSET and "fallback" to using the higher end
+ * of the VMEMMAP where 52-bit support is not available in hardware.
  */
-#define VMEMMAP_SIZE (UL(1) << (VA_BITS - PAGE_SHIFT - 1 + STRUCT_PAGE_MAX_SHIFT))
+#define VMEMMAP_SIZE ((_VA_START(VA_BITS_MIN) - PAGE_OFFSET) \
+			>> (PAGE_SHIFT - STRUCT_PAGE_MAX_SHIFT))
 
 /*
  * PAGE_OFFSET - the virtual address of the start of the linear map (top

commit 5383cc6efed13784ddb3cff2cc183b6b8c50c8db
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:18 2019 +0100

    arm64: mm: Introduce vabits_actual
    
    In order to support 52-bit kernel addresses detectable at boot time, one
    needs to know the actual VA_BITS detected. A new variable vabits_actual
    is introduced in this commit and employed for the KVM hypervisor layout,
    KASAN, fault handling and phys-to/from-virt translation where there
    would normally be compile time constants.
    
    In order to maintain performance in phys_to_virt, another variable
    physvirt_offset is introduced.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 79e75e45d560..364635b8370a 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -37,8 +37,6 @@
  * VA_START - the first kernel virtual address.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
-#define VA_START		(UL(0xffffffffffffffff) - \
-	(UL(1) << (VA_BITS - 1)) + 1)
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
 	(UL(1) << VA_BITS) + 1)
 #define KIMAGE_VADDR		(MODULES_END)
@@ -166,10 +164,13 @@
 #endif
 
 #ifndef __ASSEMBLY__
+extern u64			vabits_actual;
+#define VA_START		(_VA_START(vabits_actual))
 
 #include <linux/bitops.h>
 #include <linux/mmdebug.h>
 
+extern s64			physvirt_offset;
 extern s64			memstart_addr;
 /* PHYS_OFFSET - the physical address of the start of memory. */
 #define PHYS_OFFSET		({ VM_BUG_ON(memstart_addr & 1); memstart_addr; })
@@ -240,9 +241,9 @@ static inline const void *__tag_set(const void *addr, u8 tag)
  * space. Testing the top bit for the start of the region is a
  * sufficient check.
  */
-#define __is_lm_address(addr)	(!((addr) & BIT(VA_BITS - 1)))
+#define __is_lm_address(addr)	(!((addr) & BIT(vabits_actual - 1)))
 
-#define __lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + PHYS_OFFSET)
+#define __lm_to_phys(addr)	(((addr) + physvirt_offset))
 #define __kimg_to_phys(addr)	((addr) - kimage_voffset)
 
 #define __virt_to_phys_nodebug(x) ({					\
@@ -261,7 +262,7 @@ extern phys_addr_t __phys_addr_symbol(unsigned long x);
 #define __phys_addr_symbol(x)	__pa_symbol_nodebug(x)
 #endif
 
-#define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
+#define __phys_to_virt(x)	((unsigned long)((x) - physvirt_offset))
 #define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
 
 /*

commit 90ec95cda91a021d82351c976896a63aa364ebf1
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:17 2019 +0100

    arm64: mm: Introduce VA_BITS_MIN
    
    In order to support 52-bit kernel addresses detectable at boot time, the
    kernel needs to know the most conservative VA_BITS possible should it
    need to fall back to this quantity due to lack of hardware support.
    
    A new compile time constant VA_BITS_MIN is introduced in this patch and
    it is employed in the KASAN end address, KASLR, and EFI stub.
    
    For Arm, if 52-bit VA support is unavailable the fallback is to 48-bits.
    
    In other words: VA_BITS_MIN = min (48, VA_BITS)
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 968659c90f5c..79e75e45d560 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -52,6 +52,13 @@
 #define PCI_IO_END		(VMEMMAP_START - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
+#if VA_BITS > 48
+#define VA_BITS_MIN		(48)
+#else
+#define VA_BITS_MIN		(VA_BITS)
+#endif
+#define _VA_START(va)		(UL(0xffffffffffffffff) - \
+				(UL(1) << ((va) - 1)) + 1)
 
 #define KERNEL_START      _text
 #define KERNEL_END        _end
@@ -74,7 +81,7 @@
 #define KASAN_THREAD_SHIFT	1
 #else
 #define KASAN_THREAD_SHIFT	0
-#define KASAN_SHADOW_END	(VA_START)
+#define KASAN_SHADOW_END	(_VA_START(VA_BITS_MIN))
 #endif
 
 #define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)

commit 6bd1d0be0e97936d15cdacc71f5c232fbf71293e
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:15 2019 +0100

    arm64: kasan: Switch to using KASAN_SHADOW_OFFSET
    
    KASAN_SHADOW_OFFSET is a constant that is supplied to gcc as a command
    line argument and affects the codegen of the inline address sanetiser.
    
    Essentially, for an example memory access:
        *ptr1 = val;
    The compiler will insert logic similar to the below:
        shadowValue = *(ptr1 >> KASAN_SHADOW_SCALE_SHIFT + KASAN_SHADOW_OFFSET)
        if (somethingWrong(shadowValue))
            flagAnError();
    
    This code sequence is inserted into many places, thus
    KASAN_SHADOW_OFFSET is essentially baked into many places in the kernel
    text.
    
    If we want to run a single kernel binary with multiple address spaces,
    then we need to do this with KASAN_SHADOW_OFFSET fixed.
    
    Thankfully, due to the way the KASAN_SHADOW_OFFSET is used to provide
    shadow addresses we know that the end of the shadow region is constant
    w.r.t. VA space size:
        KASAN_SHADOW_END = ~0 >> KASAN_SHADOW_SCALE_SHIFT + KASAN_SHADOW_OFFSET
    
    This means that if we increase the size of the VA space, the start of
    the KASAN region expands into lower addresses whilst the end of the
    KASAN region is fixed.
    
    Currently the arm64 code computes KASAN_SHADOW_OFFSET at build time via
    build scripts with the VA size used as a parameter. (There are build
    time checks in the C code too to ensure that expected values are being
    derived). It is sufficient, and indeed is a simplification, to remove
    the build scripts (and build time checks) entirely and instead provide
    KASAN_SHADOW_OFFSET values.
    
    This patch removes the logic to compute the KASAN_SHADOW_OFFSET in the
    arm64 Makefile, and instead we adopt the approach used by x86 to supply
    offset values in kConfig. To help debug/develop future VA space changes,
    the Makefile logic has been preserved in a script file in the arm64
    Documentation folder.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 380594b1a0ba..968659c90f5c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -42,7 +42,7 @@
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
 	(UL(1) << VA_BITS) + 1)
 #define KIMAGE_VADDR		(MODULES_END)
-#define BPF_JIT_REGION_START	(VA_START + KASAN_SHADOW_SIZE)
+#define BPF_JIT_REGION_START	(KASAN_SHADOW_END)
 #define BPF_JIT_REGION_SIZE	(SZ_128M)
 #define BPF_JIT_REGION_END	(BPF_JIT_REGION_START + BPF_JIT_REGION_SIZE)
 #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
@@ -68,11 +68,13 @@
  * significantly, so double the (minimum) stack size when they are in use.
  */
 #ifdef CONFIG_KASAN
-#define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
+#define KASAN_SHADOW_OFFSET	_AC(CONFIG_KASAN_SHADOW_OFFSET, UL)
+#define KASAN_SHADOW_END	((UL(1) << (64 - KASAN_SHADOW_SCALE_SHIFT)) \
+					+ KASAN_SHADOW_OFFSET)
 #define KASAN_THREAD_SHIFT	1
 #else
-#define KASAN_SHADOW_SIZE	(0)
 #define KASAN_THREAD_SHIFT	0
+#define KASAN_SHADOW_END	(VA_START)
 #endif
 
 #define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)

commit 14c127c957c1c6070647c171e72f06e0db275ebf
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:14 2019 +0100

    arm64: mm: Flip kernel VA space
    
    In order to allow for a KASAN shadow that changes size at boot time, one
    must fix the KASAN_SHADOW_END for both 48 & 52-bit VAs and "grow" the
    start address. Also, it is highly desirable to maintain the same
    function addresses in the kernel .text between VA sizes. Both of these
    requirements necessitate us to flip the kernel address space halves s.t.
    the direct linear map occupies the lower addresses.
    
    This patch puts the direct linear map in the lower addresses of the
    kernel VA range and everything else in the higher ranges.
    
    We need to adjust:
     *) KASAN shadow region placement logic,
     *) KASAN_SHADOW_OFFSET computation logic,
     *) virt_to_phys, phys_to_virt checks,
     *) page table dumper.
    
    These are all small changes, that need to take place atomically, so they
    are bundled into this commit.
    
    As part of the re-arrangement, a guard region of 2MB (to preserve
    alignment for fixed map) is added after the vmemmap. Otherwise the
    vmemmap could intersect with IS_ERR pointers.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 98fda92a2612..380594b1a0ba 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -38,9 +38,9 @@
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
 #define VA_START		(UL(0xffffffffffffffff) - \
-	(UL(1) << VA_BITS) + 1)
-#define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
 	(UL(1) << (VA_BITS - 1)) + 1)
+#define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
+	(UL(1) << VA_BITS) + 1)
 #define KIMAGE_VADDR		(MODULES_END)
 #define BPF_JIT_REGION_START	(VA_START + KASAN_SHADOW_SIZE)
 #define BPF_JIT_REGION_SIZE	(SZ_128M)
@@ -48,7 +48,7 @@
 #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
 #define MODULES_VADDR		(BPF_JIT_REGION_END)
 #define MODULES_VSIZE		(SZ_128M)
-#define VMEMMAP_START		(PAGE_OFFSET - VMEMMAP_SIZE)
+#define VMEMMAP_START		(-VMEMMAP_SIZE - SZ_2M)
 #define PCI_IO_END		(VMEMMAP_START - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
@@ -231,7 +231,7 @@ static inline const void *__tag_set(const void *addr, u8 tag)
  * space. Testing the top bit for the start of the region is a
  * sufficient check.
  */
-#define __is_lm_address(addr)	(!!((addr) & BIT(VA_BITS - 1)))
+#define __is_lm_address(addr)	(!((addr) & BIT(VA_BITS - 1)))
 
 #define __lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + PHYS_OFFSET)
 #define __kimg_to_phys(addr)	((addr) - kimage_voffset)

commit 9cb1c5ddd2c432834dd4f40c0170d6b639e8e5c3
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:13 2019 +0100

    arm64: mm: Remove bit-masking optimisations for PAGE_OFFSET and VMEMMAP_START
    
    Currently there are assumptions about the alignment of VMEMMAP_START
    and PAGE_OFFSET that won't be valid after this series is applied.
    
    These assumptions are in the form of bitwise operators being used
    instead of addition and subtraction when calculating addresses.
    
    This patch replaces these bitwise operators with addition/subtraction.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index fb04f10a78ab..98fda92a2612 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -299,21 +299,20 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #else
-#define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
-#define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
+#define __virt_to_pgoff(kaddr)	(((u64)(kaddr) - PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
+#define __page_to_voff(kaddr)	(((u64)(kaddr) - VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
 #define page_to_virt(page)	({					\
 	unsigned long __addr =						\
-		((__page_to_voff(page)) | PAGE_OFFSET);			\
+		((__page_to_voff(page)) + PAGE_OFFSET);			\
 	const void *__addr_tag =					\
 		__tag_set((void *)__addr, page_kasan_tag(page));	\
 	((void *)__addr_tag);						\
 })
 
-#define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
+#define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) + VMEMMAP_START))
 
-#define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
-					   + PHYS_OFFSET) >> PAGE_SHIFT)
+#define _virt_addr_valid(kaddr)	pfn_valid(__virt_to_phys((u64)(kaddr)) >> PAGE_SHIFT)
 #endif
 #endif
 

commit 2b835e24b5c6f9c633ff51973581ee7ca7b3e8ec
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Tue Jul 23 19:58:38 2019 +0200

    arm64: untag user pointers in access_ok and __uaccess_mask_ptr
    
    This patch is a part of a series that extends kernel ABI to allow to pass
    tagged user pointers (with the top byte set to something else other than
    0x00) as syscall arguments.
    
    copy_from_user (and a few other similar functions) are used to copy data
    from user memory into the kernel memory or vice versa. Since a user can
    provided a tagged pointer to one of the syscalls that use copy_from_user,
    we need to correctly handle such pointers.
    
    Do this by untagging user pointers in access_ok and in __uaccess_mask_ptr,
    before performing access validity checks.
    
    Note, that this patch only temporarily untags the pointers to perform the
    checks, but then passes them as is into the kernel internals.
    
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    [will: Add __force to casting in untagged_addr() to kill sparse warning]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index fb04f10a78ab..46c4c08a80a9 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -201,7 +201,7 @@ extern u64			vabits_user;
  * pass on to access_ok(), for instance.
  */
 #define untagged_addr(addr)	\
-	((__typeof__(addr))sign_extend64((u64)(addr), 55))
+	((__typeof__(addr))sign_extend64((__force u64)(addr), 55))
 
 #ifdef CONFIG_KASAN_SW_TAGS
 #define __tag_shifted(tag)	((u64)(tag) << 56)

commit 7732d20a160c76006c7fe7bca5178aea6af1d2e8
Author: Qian Cai <cai@lca.pw>
Date:   Thu Aug 1 10:47:05 2019 -0400

    arm64/mm: fix variable 'tag' set but not used
    
    When CONFIG_KASAN_SW_TAGS=n, set_tag() is compiled away. GCC throws a
    warning,
    
    mm/kasan/common.c: In function '__kasan_kmalloc':
    mm/kasan/common.c:464:5: warning: variable 'tag' set but not used
    [-Wunused-but-set-variable]
      u8 tag = 0xff;
         ^~~
    
    Fix it by making __tag_set() a static inline function the same as
    arch_kasan_set_tag() in mm/kasan/kasan.h for consistency because there
    is a macro in arch/arm64/include/asm/kasan.h,
    
     #define arch_kasan_set_tag(addr, tag) __tag_set(addr, tag)
    
    However, when CONFIG_DEBUG_VIRTUAL=n and CONFIG_SPARSEMEM_VMEMMAP=y,
    page_to_virt() will call __tag_set() with incorrect type of a
    parameter, so fix that as well. Also, still let page_to_virt() return
    "void *" instead of "const void *", so will not need to add a similar
    cast in lowmem_page_address().
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b7ba75809751..fb04f10a78ab 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -210,7 +210,11 @@ extern u64			vabits_user;
 #define __tag_reset(addr)	untagged_addr(addr)
 #define __tag_get(addr)		(__u8)((u64)(addr) >> 56)
 #else
-#define __tag_set(addr, tag)	(addr)
+static inline const void *__tag_set(const void *addr, u8 tag)
+{
+	return addr;
+}
+
 #define __tag_reset(addr)	(addr)
 #define __tag_get(addr)		0
 #endif
@@ -301,8 +305,8 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define page_to_virt(page)	({					\
 	unsigned long __addr =						\
 		((__page_to_voff(page)) | PAGE_OFFSET);			\
-	unsigned long __addr_tag =					\
-		 __tag_set(__addr, page_kasan_tag(page));		\
+	const void *__addr_tag =					\
+		__tag_set((void *)__addr, page_kasan_tag(page));	\
 	((void *)__addr_tag);						\
 })
 

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 8ffcf5a512bb..b7ba75809751 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -1,21 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Based on arch/arm/include/asm/memory.h
  *
  * Copyright (C) 2000-2002 Russell King
  * Copyright (C) 2012 ARM Ltd.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
- *
  * Note: this file should not be included by non-asm/.h files
  */
 #ifndef __ASM_MEMORY_H

commit 87dfb311b707cd4c4b666c9af0fa15acbe6eee99
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue May 14 15:46:51 2019 -0700

    treewide: replace #include <asm/sizes.h> with #include <linux/sizes.h>
    
    Since commit dccd2304cc90 ("ARM: 7430/1: sizes.h: move from asm-generic
    to <linux/sizes.h>"), <asm/sizes.h> and <asm-generic/sizes.h> are just
    wrappers of <linux/sizes.h>.
    
    This commit replaces all <asm/sizes.h> and <asm-generic/sizes.h> to
    prepare for the removal.
    
    Link: http://lkml.kernel.org/r/1553267665-27228-1-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 2cb8248fa2c8..8ffcf5a512bb 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -26,7 +26,7 @@
 #include <linux/types.h>
 #include <asm/bug.h>
 #include <asm/page-def.h>
-#include <asm/sizes.h>
+#include <linux/sizes.h>
 
 /*
  * Size of the PCI I/O space. This must remain a power of two so that

commit eea1bb2248691ed65c33daff94a253af44feb103
Author: Miles Chen <miles.chen@mediatek.com>
Date:   Tue Apr 16 01:36:36 2019 +0800

    arm64: mm: check virtual addr in virt_to_page() if CONFIG_DEBUG_VIRTUAL=y
    
    This change uses the original virt_to_page() (the one with __pa()) to
    check the given virtual address if CONFIG_DEBUG_VIRTUAL=y.
    
    Recently, I worked on a bug: a driver passes a symbol address to
    dma_map_single() and the virt_to_page() (called by dma_map_single())
    does not work for non-linear addresses after commit 9f2875912dac
    ("arm64: mm: restrict virt_to_page() to the linear mapping").
    
    I tried to trap the bug by enabling CONFIG_DEBUG_VIRTUAL but it
    did not work - bacause the commit removes the __pa() from
    virt_to_page() but CONFIG_DEBUG_VIRTUAL checks the virtual address
    in __pa()/__virt_to_phys().
    
    A simple solution is to use the original virt_to_page()
    (the one with__pa()) if CONFIG_DEBUG_VIRTUAL=y.
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Miles Chen <miles.chen@mediatek.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 290195168bb3..2cb8248fa2c8 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -302,7 +302,7 @@ static inline void *phys_to_virt(phys_addr_t x)
  */
 #define ARCH_PFN_OFFSET		((unsigned long)PHYS_PFN_OFFSET)
 
-#ifndef CONFIG_SPARSEMEM_VMEMMAP
+#if !defined(CONFIG_SPARSEMEM_VMEMMAP) || defined(CONFIG_DEBUG_VIRTUAL)
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #else

commit 3d8dfe75ef69f4dd4ba35c09b20a5aa58b4a5078
Merge: d60752629693 b855b58ac1b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 10:17:23 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Pseudo NMI support for arm64 using GICv3 interrupt priorities
    
     - uaccess macros clean-up (unsafe user accessors also merged but
       reverted, waiting for objtool support on arm64)
    
     - ptrace regsets for Pointer Authentication (ARMv8.3) key management
    
     - inX() ordering w.r.t. delay() on arm64 and riscv (acks in place by
       the riscv maintainers)
    
     - arm64/perf updates: PMU bindings converted to json-schema, unused
       variable and misleading comment removed
    
     - arm64/debug fixes to ensure checking of the triggering exception
       level and to avoid the propagation of the UNKNOWN FAR value into the
       si_code for debug signals
    
     - Workaround for Fujitsu A64FX erratum 010001
    
     - lib/raid6 ARM NEON optimisations
    
     - NR_CPUS now defaults to 256 on arm64
    
     - Minor clean-ups (documentation/comments, Kconfig warning, unused
       asm-offsets, clang warnings)
    
     - MAINTAINERS update for list information to the ARM64 ACPI entry
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      arm64: mmu: drop paging_init comments
      arm64: debug: Ensure debug handlers check triggering exception level
      arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
      Revert "arm64: uaccess: Implement unsafe accessors"
      arm64: avoid clang warning about self-assignment
      arm64: Kconfig.platforms: fix warning unmet direct dependencies
      lib/raid6: arm: optimize away a mask operation in NEON recovery routine
      lib/raid6: use vdupq_n_u8 to avoid endianness warnings
      arm64: io: Hook up __io_par() for inX() ordering
      riscv: io: Update __io_[p]ar() macros to take an argument
      asm-generic/io: Pass result of I/O accessor to __io_[p]ar()
      arm64: Add workaround for Fujitsu A64FX erratum 010001
      arm64: Rename get_thread_info()
      arm64: Remove documentation about TIF_USEDFPU
      arm64: irqflags: Fix clang build warnings
      arm64: Enable the support of pseudo-NMIs
      arm64: Skip irqflags tracing for NMI in IRQs disabled context
      arm64: Skip preemption when exiting an NMI
      arm64: Handle serror in NMI context
      irqchip/gic-v3: Allow interrupts to be set as pseudo-NMI
      ...

commit 7771bdbbfd3d6f204631b6fd9e1bbc30cd15918e
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Tue Mar 5 15:41:20 2019 -0800

    kasan: remove use after scope bugs detection.
    
    Use after scope bugs detector seems to be almost entirely useless for
    the linux kernel.  It exists over two years, but I've seen only one
    valid bug so far [1].  And the bug was fixed before it has been
    reported.  There were some other use-after-scope reports, but they were
    false-positives due to different reasons like incompatibility with
    structleak plugin.
    
    This feature significantly increases stack usage, especially with GCC <
    9 version, and causes a 32K stack overflow.  It probably adds
    performance penalty too.
    
    Given all that, let's remove use-after-scope detector entirely.
    
    While preparing this patch I've noticed that we mistakenly enable
    use-after-scope detection for clang compiler regardless of
    CONFIG_KASAN_EXTRA setting.  This is also fixed now.
    
    [1] http://lkml.kernel.org/r/<20171129052106.rhgbjhhis53hkgfn@wfg-t540p.sh.intel.com>
    
    Link: http://lkml.kernel.org/r/20190111185842.13978-1-aryabinin@virtuozzo.com
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Acked-by: Will Deacon <will.deacon@arm.com>             [arm64]
    Cc: Qian Cai <cai@lca.pw>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 0c656850eeea..b01ef0180a03 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -80,11 +80,7 @@
  */
 #ifdef CONFIG_KASAN
 #define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
-#ifdef CONFIG_KASAN_EXTRA
-#define KASAN_THREAD_SHIFT	2
-#else
 #define KASAN_THREAD_SHIFT	1
-#endif /* CONFIG_KASAN_EXTRA */
 #else
 #define KASAN_SHADOW_SIZE	(0)
 #define KASAN_THREAD_SHIFT	0

commit 366e37e4da23f9df498cc9577cadcb354f7bd431
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Feb 22 15:42:23 2019 +0100

    arm64: avoid clang warning about self-assignment
    
    Building a preprocessed source file for arm64 now always produces
    a warning with clang because of the page_to_virt() macro assigning
    a variable to itself.
    
    Adding a new temporary variable avoids this issue.
    
    Fixes: 2813b9c02962 ("kasan, mm, arm64: tag non slab memory allocated via pagealloc")
    Reviewed-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index e1ec947e7c0c..6340aa8350d9 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -316,8 +316,9 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define page_to_virt(page)	({					\
 	unsigned long __addr =						\
 		((__page_to_voff(page)) | PAGE_OFFSET);			\
-	__addr = __tag_set(__addr, page_kasan_tag(page));		\
-	((void *)__addr);						\
+	unsigned long __addr_tag =					\
+		 __tag_set(__addr, page_kasan_tag(page));		\
+	((void *)__addr_tag);						\
 })
 
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))

commit 8a5b403d71affa098009cc3dff1b2c45113021ad
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Feb 15 13:33:32 2019 +0100

    arm64, mm, efi: Account for GICv3 LPI tables in static memblock reserve table
    
    In the irqchip and EFI code, we have what basically amounts to a quirk
    to work around a peculiarity in the GICv3 architecture, which permits
    the system memory address of LPI tables to be programmable only once
    after a CPU reset. This means kexec kernels must use the same memory
    as the first kernel, and thus ensure that this memory has not been
    given out for other purposes by the time the ITS init code runs, which
    is not very early for secondary CPUs.
    
    On systems with many CPUs, these reservations could overflow the
    memblock reservation table, and this was addressed in commit:
    
      eff896288872 ("efi/arm: Defer persistent reservations until after paging_init()")
    
    However, this turns out to have made things worse, since the allocation
    of page tables and heap space for the resized memblock reservation table
    itself may overwrite the regions we are attempting to reserve, which may
    cause all kinds of corruption, also considering that the ITS will still
    be poking bits into that memory in response to incoming MSIs.
    
    So instead, let's grow the static memblock reservation table on such
    systems so it can accommodate these reservations at an earlier time.
    This will permit us to revert the above commit in a subsequent patch.
    
    [ mingo: Minor cleanups. ]
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-efi@vger.kernel.org
    Link: http://lkml.kernel.org/r/20190215123333.21209-2-ard.biesheuvel@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index e1ec947e7c0c..0c656850eeea 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -332,6 +332,17 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define virt_addr_valid(kaddr)		\
 	(_virt_addr_is_linear(kaddr) && _virt_addr_valid(kaddr))
 
+/*
+ * Given that the GIC architecture permits ITS implementations that can only be
+ * configured with a LPI table address once, GICv3 systems with many CPUs may
+ * end up reserving a lot of different regions after a kexec for their LPI
+ * tables (one per CPU), as we are forced to reuse the same memory after kexec
+ * (and thus reserve it persistently with EFI beforehand)
+ */
+#if defined(CONFIG_EFI) && defined(CONFIG_ARM_GIC_V3_ITS)
+# define INIT_MEMBLOCK_RESERVED_REGIONS	(INIT_MEMBLOCK_REGIONS + NR_CPUS + 1)
+#endif
+
 #include <asm-generic/memory_model.h>
 
 #endif

commit 030672aea826adf3dee9100ee8ac303b62c8fe7f
Merge: 24dc83635ffe 5801169a2ed2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 28 20:08:34 2018 -0800

    Merge tag 'devicetree-for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/robh/linux
    
    Pull Devicetree updates from Rob Herring:
     "The biggest highlight here is the start of using json-schema for DT
      bindings. Being able to validate bindings has been discussed for years
      with little progress.
    
       - Initial support for DT bindings using json-schema language. This is
         the start of converting DT bindings from free-form text to a
         structured format.
    
       - Reworking of initrd address initialization. This moves to using the
         phys address instead of virt addr in the DT parsing code. This
         rework was motivated by CONFIG_DEV_BLK_INITRD causing unnecessary
         rebuilding of lots of files.
    
       - Fix stale phandle entries in phandle cache
    
       - DT overlay validation improvements. This exposed several memory
         leak bugs which have been fixed.
    
       - Use node name and device_type helper functions in DT code
    
       - Last remaining conversions to using %pOFn printk specifier instead
         of device_node.name directly
    
       - Create new common RTC binding doc and move all trivial RTC devices
         out of trivial-devices.txt.
    
       - New bindings for Freescale MAG3110 magnetometer, Cadence Sierra
         PHY, and Xen shared memory
    
       - Update dtc to upstream version v1.4.7-57-gf267e674d145"
    
    * tag 'devicetree-for-4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/robh/linux: (68 commits)
      of: __of_detach_node() - remove node from phandle cache
      of: of_node_get()/of_node_put() nodes held in phandle cache
      gpio-omap.txt: add reg and interrupts properties
      dt-bindings: mrvl,intc: fix a trivial typo
      dt-bindings: iio: magnetometer: add dt-bindings for freescale mag3110
      dt-bindings: Convert trivial-devices.txt to json-schema
      dt-bindings: arm: mrvl: amend Browstone compatible string
      dt-bindings: arm: Convert Tegra board/soc bindings to json-schema
      dt-bindings: arm: Convert ZTE board/soc bindings to json-schema
      dt-bindings: arm: Add missing Xilinx boards
      dt-bindings: arm: Convert Xilinx board/soc bindings to json-schema
      dt-bindings: arm: Convert VIA board/soc bindings to json-schema
      dt-bindings: arm: Convert ST STi board/soc bindings to json-schema
      dt-bindings: arm: Convert SPEAr board/soc bindings to json-schema
      dt-bindings: arm: Convert CSR SiRF board/soc bindings to json-schema
      dt-bindings: arm: Convert QCom board/soc bindings to json-schema
      dt-bindings: arm: Convert TI nspire board/soc bindings to json-schema
      dt-bindings: arm: Convert TI davinci board/soc bindings to json-schema
      dt-bindings: arm: Convert Calxeda board/soc bindings to json-schema
      dt-bindings: arm: Convert Altera board/soc bindings to json-schema
      ...

commit 2813b9c0296259fb11e75c839bab2d958ba4f96c
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:57 2018 -0800

    kasan, mm, arm64: tag non slab memory allocated via pagealloc
    
    Tag-based KASAN doesn't check memory accesses through pointers tagged with
    0xff.  When page_address is used to get pointer to memory that corresponds
    to some page, the tag of the resulting pointer gets set to 0xff, even
    though the allocated memory might have been tagged differently.
    
    For slab pages it's impossible to recover the correct tag to return from
    page_address, since the page might contain multiple slab objects tagged
    with different values, and we can't know in advance which one of them is
    going to get accessed.  For non slab pages however, we can recover the tag
    in page_address, since the whole page was marked with the same tag.
    
    This patch adds tagging to non slab memory allocated with pagealloc.  To
    set the tag of the pointer returned from page_address, the tag gets stored
    to page->flags when the memory gets allocated.
    
    Link: http://lkml.kernel.org/r/d758ddcef46a5abc9970182b9137e2fbee202a2c.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 907946cc767c..2bb8721da7ef 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -321,7 +321,13 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
-#define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
+#define page_to_virt(page)	({					\
+	unsigned long __addr =						\
+		((__page_to_voff(page)) | PAGE_OFFSET);			\
+	__addr = __tag_set(__addr, page_kasan_tag(page));		\
+	((void *)__addr);						\
+})
+
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
 #define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \

commit e71fe3f921aeb27f0c65ee7ebfdde7f8c7d60b74
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:20 2018 -0800

    kasan, arm64: untag address in _virt_addr_is_linear
    
    virt_addr_is_linear (which is used by virt_addr_valid) assumes that the
    top byte of the address is 0xff, which isn't always the case with
    tag-based KASAN.
    
    This patch resets the tag in this macro.
    
    Link: http://lkml.kernel.org/r/df73a37dd5ed37f4deaf77bc718e9f2e590e69b1.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 25b46f88726c..907946cc767c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -329,9 +329,10 @@ static inline void *phys_to_virt(phys_addr_t x)
 #endif
 #endif
 
-#define _virt_addr_is_linear(kaddr)	(((u64)(kaddr)) >= PAGE_OFFSET)
-#define virt_addr_valid(kaddr)		(_virt_addr_is_linear(kaddr) && \
-					 _virt_addr_valid(kaddr))
+#define _virt_addr_is_linear(kaddr)	\
+	(__tag_reset((u64)(kaddr)) >= PAGE_OFFSET)
+#define virt_addr_valid(kaddr)		\
+	(_virt_addr_is_linear(kaddr) && _virt_addr_valid(kaddr))
 
 #include <asm-generic/memory_model.h>
 

commit 3c9e3aa11094e821aff4a8f6812a6e032293dbc0
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:16 2018 -0800

    kasan: add tag related helper functions
    
    This commit adds a few helper functions, that are meant to be used to work
    with tags embedded in the top byte of kernel pointers: to set, to get or
    to reset the top byte.
    
    Link: http://lkml.kernel.org/r/f6c6437bb8e143bc44f42c3c259c62e734be7935.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index e73bb89d6141..25b46f88726c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -226,6 +226,18 @@ extern u64			vabits_user;
 #define untagged_addr(addr)	\
 	((__typeof__(addr))sign_extend64((u64)(addr), 55))
 
+#ifdef CONFIG_KASAN_SW_TAGS
+#define __tag_shifted(tag)	((u64)(tag) << 56)
+#define __tag_set(addr, tag)	(__typeof__(addr))( \
+		((u64)(addr) & ~__tag_shifted(0xff)) | __tag_shifted(tag))
+#define __tag_reset(addr)	untagged_addr(addr)
+#define __tag_get(addr)		(__u8)((u64)(addr) >> 56)
+#else
+#define __tag_set(addr, tag)	(addr)
+#define __tag_reset(addr)	(addr)
+#define __tag_get(addr)		0
+#endif
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit 9c23f84723d2bb5611a973f56f0952fa74f048f3
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:12 2018 -0800

    arm64: move untagged_addr macro from uaccess.h to memory.h
    
    Move the untagged_addr() macro from arch/arm64/include/asm/uaccess.h
    to arch/arm64/include/asm/memory.h to be later reused by KASAN.
    
    Also make the untagged_addr() macro accept all kinds of address types
    (void *, unsigned long, etc.). This allows not to specify type casts in
    each place where the macro is used. This is done by using __typeof__.
    
    Link: http://lkml.kernel.org/r/2e9ef8d2ed594106eca514b268365b5419113f6a.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 7640feed268d..e73bb89d6141 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -218,6 +218,14 @@ extern u64			vabits_user;
  */
 #define PHYS_PFN_OFFSET	(PHYS_OFFSET >> PAGE_SHIFT)
 
+/*
+ * When dealing with data aborts, watchpoints, or instruction traps we may end
+ * up with a tagged userland pointer. Clear the tag to get a sane pointer to
+ * pass on to access_ok(), for instance.
+ */
+#define untagged_addr(addr)	\
+	((__typeof__(addr))sign_extend64((u64)(addr), 55))
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit b2f557eae9ed0ab2b612ce9ce7e3f06174a83e76
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:29:57 2018 -0800

    kasan, arm64: adjust shadow size for tag-based mode
    
    Tag-based KASAN uses 1 shadow byte for 16 bytes of kernel memory, so it
    requires 1/16th of the kernel virtual address space for the shadow memory.
    
    This commit sets KASAN_SHADOW_SCALE_SHIFT to 4 when the tag-based KASAN
    mode is enabled.
    
    Link: http://lkml.kernel.org/r/308b6bd49f756bb5e533be93c6f085ba99b30339.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 0385752bd079..7640feed268d 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -74,13 +74,11 @@
 #endif
 
 /*
- * KASAN requires 1/8th of the kernel virtual address space for the shadow
- * region. KASAN can bloat the stack significantly, so double the (minimum)
- * stack size when KASAN is in use, and then double it again if KASAN_EXTRA is
- * on.
+ * Generic and tag-based KASAN require 1/8th and 1/16th of the kernel virtual
+ * address space for the shadow region respectively. They can bloat the stack
+ * significantly, so double the (minimum) stack size when they are in use.
  */
 #ifdef CONFIG_KASAN
-#define KASAN_SHADOW_SCALE_SHIFT 3
 #define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
 #ifdef CONFIG_KASAN_EXTRA
 #define KASAN_THREAD_SHIFT	2

commit e0c38a4d1f196a4b17d2eba36afff8f656a4f1de
Merge: 7f9f852c75e7 90cadbbf341d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 27 13:04:52 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) New ipset extensions for matching on destination MAC addresses, from
        Stefano Brivio.
    
     2) Add ipv4 ttl and tos, plus ipv6 flow label and hop limit offloads to
        nfp driver. From Stefano Brivio.
    
     3) Implement GRO for plain UDP sockets, from Paolo Abeni.
    
     4) Lots of work from Michał Mirosław to eliminate the VLAN_TAG_PRESENT
        bit so that we could support the entire vlan_tci value.
    
     5) Rework the IPSEC policy lookups to better optimize more usecases,
        from Florian Westphal.
    
     6) Infrastructure changes eliminating direct manipulation of SKB lists
        wherever possible, and to always use the appropriate SKB list
        helpers. This work is still ongoing...
    
     7) Lots of PHY driver and state machine improvements and
        simplifications, from Heiner Kallweit.
    
     8) Various TSO deferral refinements, from Eric Dumazet.
    
     9) Add ntuple filter support to aquantia driver, from Dmitry Bogdanov.
    
    10) Batch dropping of XDP packets in tuntap, from Jason Wang.
    
    11) Lots of cleanups and improvements to the r8169 driver from Heiner
        Kallweit, including support for ->xmit_more. This driver has been
        getting some much needed love since he started working on it.
    
    12) Lots of new forwarding selftests from Petr Machata.
    
    13) Enable VXLAN learning in mlxsw driver, from Ido Schimmel.
    
    14) Packed ring support for virtio, from Tiwei Bie.
    
    15) Add new Aquantia AQtion USB driver, from Dmitry Bezrukov.
    
    16) Add XDP support to dpaa2-eth driver, from Ioana Ciocoi Radulescu.
    
    17) Implement coalescing on TCP backlog queue, from Eric Dumazet.
    
    18) Implement carrier change in tun driver, from Nicolas Dichtel.
    
    19) Support msg_zerocopy in UDP, from Willem de Bruijn.
    
    20) Significantly improve garbage collection of neighbor objects when
        the table has many PERMANENT entries, from David Ahern.
    
    21) Remove egdev usage from nfp and mlx5, and remove the facility
        completely from the tree as it no longer has any users. From Oz
        Shlomo and others.
    
    22) Add a NETDEV_PRE_CHANGEADDR so that drivers can veto the change and
        therefore abort the operation before the commit phase (which is the
        NETDEV_CHANGEADDR event). From Petr Machata.
    
    23) Add indirect call wrappers to avoid retpoline overhead, and use them
        in the GRO code paths. From Paolo Abeni.
    
    24) Add support for netlink FDB get operations, from Roopa Prabhu.
    
    25) Support bloom filter in mlxsw driver, from Nir Dotan.
    
    26) Add SKB extension infrastructure. This consolidates the handling of
        the auxiliary SKB data used by IPSEC and bridge netfilter, and is
        designed to support the needs to MPTCP which could be integrated in
        the future.
    
    27) Lots of XDP TX optimizations in mlx5 from Tariq Toukan.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1845 commits)
      net: dccp: fix kernel crash on module load
      drivers/net: appletalk/cops: remove redundant if statement and mask
      bnx2x: Fix NULL pointer dereference in bnx2x_del_all_vlans() on some hw
      net/net_namespace: Check the return value of register_pernet_subsys()
      net/netlink_compat: Fix a missing check of nla_parse_nested
      ieee802154: lowpan_header_create check must check daddr
      net/mlx4_core: drop useless LIST_HEAD
      mlxsw: spectrum: drop useless LIST_HEAD
      net/mlx5e: drop useless LIST_HEAD
      iptunnel: Set tun_flags in the iptunnel_metadata_reply from src
      net/mlx5e: fix semicolon.cocci warnings
      staging: octeon: fix build failure with XFRM enabled
      net: Revert recent Spectre-v1 patches.
      can: af_can: Fix Spectre v1 vulnerability
      packet: validate address length if non-zero
      nfc: af_nfc: Fix Spectre v1 vulnerability
      phonet: af_phonet: Fix Spectre v1 vulnerability
      net: core: Fix Spectre v1 vulnerability
      net: minor cleanup in skb_ext_add()
      net: drop the unused helper skb_ext_get()
      ...

commit 5694cecdb092656a822287a6691aa7ce668c8160
Merge: 13e1ad2be3a8 12f799c8c739
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 25 17:41:56 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 festive updates from Will Deacon:
     "In the end, we ended up with quite a lot more than I expected:
    
       - Support for ARMv8.3 Pointer Authentication in userspace (CRIU and
         kernel-side support to come later)
    
       - Support for per-thread stack canaries, pending an update to GCC
         that is currently undergoing review
    
       - Support for kexec_file_load(), which permits secure boot of a kexec
         payload but also happens to improve the performance of kexec
         dramatically because we can avoid the sucky purgatory code from
         userspace. Kdump will come later (requires updates to libfdt).
    
       - Optimisation of our dynamic CPU feature framework, so that all
         detected features are enabled via a single stop_machine()
         invocation
    
       - KPTI whitelisting of Cortex-A CPUs unaffected by Meltdown, so that
         they can benefit from global TLB entries when KASLR is not in use
    
       - 52-bit virtual addressing for userspace (kernel remains 48-bit)
    
       - Patch in LSE atomics for per-cpu atomic operations
    
       - Custom preempt.h implementation to avoid unconditional calls to
         preempt_schedule() from preempt_enable()
    
       - Support for the new 'SB' Speculation Barrier instruction
    
       - Vectorised implementation of XOR checksumming and CRC32
         optimisations
    
       - Workaround for Cortex-A76 erratum #1165522
    
       - Improved compatibility with Clang/LLD
    
       - Support for TX2 system PMUS for profiling the L3 cache and DMC
    
       - Reflect read-only permissions in the linear map by default
    
       - Ensure MMIO reads are ordered with subsequent calls to Xdelay()
    
       - Initial support for memory hotplug
    
       - Tweak the threshold when we invalidate the TLB by-ASID, so that
         mremap() performance is improved for ranges spanning multiple PMDs.
    
       - Minor refactoring and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (125 commits)
      arm64: kaslr: print PHYS_OFFSET in dump_kernel_offset()
      arm64: sysreg: Use _BITUL() when defining register bits
      arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
      arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
      arm64: docs: document pointer authentication
      arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
      arm64: enable pointer authentication
      arm64: add prctl control for resetting ptrauth keys
      arm64: perf: strip PAC when unwinding userspace
      arm64: expose user PAC bit positions via ptrace
      arm64: add basic pointer authentication support
      arm64/cpufeature: detect pointer authentication
      arm64: Don't trap host pointer auth use to EL2
      arm64/kvm: hide ptrauth from guests
      arm64/kvm: consistently handle host HCR_EL2 flags
      arm64: add pointer authentication register bits
      arm64: add comments about EC exception levels
      arm64: perf: Treat EXCLUDE_EL* bit definitions as unsigned
      arm64: kpti: Whitelist Cortex-A CPUs that don't implement the CSV3 field
      arm64: enable per-task stack canaries
      ...

commit 2be09de7d6a06f58e768de1255a687c9aaa66606
Merge: 44a7b3b6e3a4 1d51b4b1d3f2
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Dec 20 10:53:28 2018 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Lots of conflicts, by happily all cases of overlapping
    changes, parallel adds, things of that nature.
    
    Thanks to Stephen Rothwell, Saeed Mahameed, and others
    for their guidance in these resolutions.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d1402fc708e4c355813e49df6d15bc3466ba5114
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Dec 14 14:16:53 2018 -0800

    mm: introduce common STRUCT_PAGE_MAX_SHIFT define
    
    This define is used by arm64 to calculate the size of the vmemmap
    region.  It is defined as the log2 of the upper bound on the size of a
    struct page.
    
    We move it into mm_types.h so it can be defined properly instead of set
    and checked with a build bug.  This also allows us to use the same
    define for riscv.
    
    Link: http://lkml.kernel.org/r/20181107205433.3875-2-logang@deltatee.com
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b96442960aea..f0a5c9531e8b 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -34,15 +34,6 @@
  */
 #define PCI_IO_SIZE		SZ_16M
 
-/*
- * Log2 of the upper bound of the size of a struct page. Used for sizing
- * the vmemmap region only, does not affect actual memory footprint.
- * We don't use sizeof(struct page) directly since taking its size here
- * requires its definition to be available at this point in the inclusion
- * chain, and it may not be a power of 2 in the first place.
- */
-#define STRUCT_PAGE_MAX_SHIFT	6
-
 /*
  * VMEMMAP_SIZE - allows the whole linear region to be covered by
  *                a struct page array

commit ec6e822d1a22d0eef1d1fa260dff751dba9a4258
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:26 2018 +0000

    arm64: expose user PAC bit positions via ptrace
    
    When pointer authentication is in use, data/instruction pointers have a
    number of PAC bits inserted into them. The number and position of these
    bits depends on the configured TCR_ELx.TxSZ and whether tagging is
    enabled. ARMv8.3 allows tagging to differ for instruction and data
    pointers.
    
    For userspace debuggers to unwind the stack and/or to follow pointer
    chains, they need to be able to remove the PAC bits before attempting to
    use a pointer.
    
    This patch adds a new structure with masks describing the location of
    the PAC bits in userspace instruction and data pointers (i.e. those
    addressable via TTBR0), which userspace can query via PTRACE_GETREGSET.
    By clearing these bits from pointers (and replacing them with the value
    of bit 55), userspace can acquire the PAC-less versions.
    
    This new regset is exposed when the kernel is built with (user) pointer
    authentication support, and the address authentication feature is
    enabled. Otherwise, the regset is hidden.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [will: Fix to use vabits_user instead of VA_BITS and rename macro]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 6747a3eddeb1..bd749033bc88 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -207,6 +207,9 @@ static inline unsigned long kaslr_offset(void)
 	return kimage_vaddr - KIMAGE_VADDR;
 }
 
+/* the actual size of a user virtual address */
+extern u64			vabits_user;
+
 /*
  * Allow all memory at the discovery stage. We will clip it later.
  */

commit 9b31cf493ffa40914e02998381993116e574c651
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 11:51:40 2018 +0000

    arm64: mm: Introduce MAX_USER_VA_BITS definition
    
    With the introduction of 52-bit virtual addressing for userspace, we are
    now in a position where the virtual addressing capability of userspace
    may exceed that of the kernel. Consequently, the VA_BITS definition
    cannot be used blindly, since it reflects only the size of kernel
    virtual addresses.
    
    This patch introduces MAX_USER_VA_BITS which is either VA_BITS or 52
    depending on whether 52-bit virtual addressing has been configured at
    build time, removing a few places where the 52 is open-coded based on
    explicit CONFIG_ guards.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 56562ff01076..6747a3eddeb1 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -73,6 +73,12 @@
 #define KERNEL_START      _text
 #define KERNEL_END        _end
 
+#ifdef CONFIG_ARM64_USER_VA_BITS_52
+#define MAX_USER_VA_BITS	52
+#else
+#define MAX_USER_VA_BITS	VA_BITS
+#endif
+
 /*
  * KASAN requires 1/8th of the kernel virtual address space for the shadow
  * region. KASAN can bloat the stack significantly, so double the (minimum)

commit 6e8830674ea77f57d57a33cca09083b117a71f41
Author: Qian Cai <cai@lca.pw>
Date:   Fri Dec 7 17:34:49 2018 -0500

    arm64: kasan: Increase stack size for KASAN_EXTRA
    
    If the kernel is configured with KASAN_EXTRA, the stack size is
    increased significantly due to setting the GCC -fstack-reuse option to
    "none" [1]. As a result, it can trigger a stack overrun quite often with
    32k stack size compiled using GCC 8. For example, this reproducer
    
      https://github.com/linux-test-project/ltp/blob/master/testcases/kernel/syscalls/madvise/madvise06.c
    
    can trigger a "corrupted stack end detected inside scheduler" very
    reliably with CONFIG_SCHED_STACK_END_CHECK enabled. There are other
    reports at:
    
      https://lore.kernel.org/lkml/1542144497.12945.29.camel@gmx.us/
      https://lore.kernel.org/lkml/721E7B42-2D55-4866-9C1A-3E8D64F33F9C@gmx.us/
    
    There are just too many functions that could have a large stack with
    KASAN_EXTRA due to large local variables that have been called over and
    over again without being able to reuse the stacks. Some noticiable ones
    are,
    
    size
    7536 shrink_inactive_list
    7440 shrink_page_list
    6560 fscache_stats_show
    3920 jbd2_journal_commit_transaction
    3216 try_to_unmap_one
    3072 migrate_page_move_mapping
    3584 migrate_misplaced_transhuge_page
    3920 ip_vs_lblcr_schedule
    4304 lpfc_nvme_info_show
    3888 lpfc_debugfs_nvmestat_data.constprop
    
    There are other 49 functions over 2k in size while compiling kernel with
    "-Wframe-larger-than=" on this machine. Hence, it is too much work to
    change Makefiles for each object to compile without
    -fsanitize-address-use-after-scope individually.
    
    [1] https://gcc.gnu.org/bugzilla/show_bug.cgi?id=81715#c23
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b96442960aea..56562ff01076 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -76,12 +76,17 @@
 /*
  * KASAN requires 1/8th of the kernel virtual address space for the shadow
  * region. KASAN can bloat the stack significantly, so double the (minimum)
- * stack size when KASAN is in use.
+ * stack size when KASAN is in use, and then double it again if KASAN_EXTRA is
+ * on.
  */
 #ifdef CONFIG_KASAN
 #define KASAN_SHADOW_SCALE_SHIFT 3
 #define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
+#ifdef CONFIG_KASAN_EXTRA
+#define KASAN_THREAD_SHIFT	2
+#else
 #define KASAN_THREAD_SHIFT	1
+#endif /* CONFIG_KASAN_EXTRA */
 #else
 #define KASAN_SHADOW_SIZE	(0)
 #define KASAN_THREAD_SHIFT	0

commit 91fc957c9b1d6c55168df15a397cbd1af16bc00f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Nov 23 23:18:04 2018 +0100

    arm64/bpf: don't allocate BPF JIT programs in module memory
    
    The arm64 module region is a 128 MB region that is kept close to
    the core kernel, in order to ensure that relative branches are
    always in range. So using the same region for programs that do
    not have this restriction is wasteful, and preferably avoided.
    
    Now that the core BPF JIT code permits the alloc/free routines to
    be overridden, implement them by vmalloc()/vfree() calls from a
    dedicated 128 MB region set aside for BPF programs. This ensures
    that BPF programs are still in branching range of each other, which
    is something the JIT currently depends upon (and is not guaranteed
    when using module_alloc() on KASLR kernels like we do currently).
    It also ensures that placement of BPF programs does not correlate
    with the placement of the core kernel or modules, making it less
    likely that leaking the former will reveal the latter.
    
    This also solves an issue under KASAN, where shadow memory is
    needlessly allocated for all BPF programs (which don't require KASAN
    shadow pages since they are not KASAN instrumented)
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b96442960aea..ee20fc63899c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -62,8 +62,11 @@
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
 	(UL(1) << (VA_BITS - 1)) + 1)
 #define KIMAGE_VADDR		(MODULES_END)
+#define BPF_JIT_REGION_START	(VA_START + KASAN_SHADOW_SIZE)
+#define BPF_JIT_REGION_SIZE	(SZ_128M)
+#define BPF_JIT_REGION_END	(BPF_JIT_REGION_START + BPF_JIT_REGION_SIZE)
 #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
-#define MODULES_VADDR		(VA_START + KASAN_SHADOW_SIZE)
+#define MODULES_VADDR		(BPF_JIT_REGION_END)
 #define MODULES_VSIZE		(SZ_128M)
 #define VMEMMAP_START		(PAGE_OFFSET - VMEMMAP_SIZE)
 #define PCI_IO_END		(VMEMMAP_START - SZ_2M)

commit cdbc848b03414c75b7badccd8ada29deba7ec6e4
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Mon Nov 5 14:54:30 2018 -0800

    of/fdt: Remove custom __early_init_dt_declare_initrd() implementation
    
    Now that ARM64 uses phys_initrd_start/phys_initrd_size, we can get rid
    of its custom __early_init_dt_declare_initrd() which causes a fair
    amount of objects rebuild when changing CONFIG_BLK_DEV_INITRD. In order
    to make sure ARM64 does not produce a BUG() when VM debugging is turned
    on though, we must avoid early calls to __va() which is what
    __early_init_dt_declare_initrd() does and wrap this around to avoid
    running that code on ARM64.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b96442960aea..dc3ca21ba240 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -168,14 +168,6 @@
 #define IOREMAP_MAX_ORDER	(PMD_SHIFT)
 #endif
 
-#ifdef CONFIG_BLK_DEV_INITRD
-#define __early_init_dt_declare_initrd(__start, __end)			\
-	do {								\
-		initrd_start = (__start);				\
-		initrd_end = (__end);					\
-	} while (0)
-#endif
-
 #ifndef __ASSEMBLY__
 
 #include <linux/bitops.h>

commit e48d53a91f6e90873e21a5ca5e8c0d7a9f8936a4
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 6 12:27:28 2018 +0100

    arm64: KVM: Add support for Stage-2 control of memory types and cacheability
    
    Up to ARMv8.3, the combinaison of Stage-1 and Stage-2 attributes
    results in the strongest attribute of the two stages.  This means
    that the hypervisor has to perform quite a lot of cache maintenance
    just in case the guest has some non-cacheable mappings around.
    
    ARMv8.4 solves this problem by offering a different mode (FWB) where
    Stage-2 has total control over the memory attribute (this is limited
    to systems where both I/O and instruction fetches are coherent with
    the dcache). This is achieved by having a different set of memory
    attributes in the page tables, and a new bit set in HCR_EL2.
    
    On such a system, we can then safely sidestep any form of dcache
    management.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 49d99214f43c..b96442960aea 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -155,6 +155,13 @@
 #define MT_S2_NORMAL		0xf
 #define MT_S2_DEVICE_nGnRE	0x1
 
+/*
+ * Memory types for Stage-2 translation when ID_AA64MMFR2_EL1.FWB is 0001
+ * Stage-2 enforces Normal-WB and Device-nGnRE
+ */
+#define MT_S2_FWB_NORMAL	6
+#define MT_S2_FWB_DEVICE_nGnRE	1
+
 #ifdef CONFIG_ARM64_4K_PAGES
 #define IOREMAP_MAX_ORDER	(PUD_SHIFT)
 #else

commit 2dd8a62c647691161a2346546834262597739872
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Apr 10 16:36:19 2018 -0700

    linux/const.h: move UL() macro to include/linux/const.h
    
    ARM, ARM64 and UniCore32 duplicate the definition of UL():
    
      #define UL(x) _AC(x, UL)
    
    This is not actually arch-specific, so it will be useful to move it to a
    common header.  Currently, we only have the uapi variant for
    linux/const.h, so I am creating include/linux/const.h.
    
    I also added _UL(), _ULL() and ULL() because _AC() is mostly used in
    the form either _AC(..., UL) or _AC(..., ULL).  I expect they will be
    replaced in follow-up cleanups.  The underscore-prefixed ones should
    be used for exported headers.
    
    Link: http://lkml.kernel.org/r/1519301715-31798-4-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 50fa96a49792..49d99214f43c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -28,12 +28,6 @@
 #include <asm/page-def.h>
 #include <asm/sizes.h>
 
-/*
- * Allow for constants defined here to be used from assembly code
- * by prepending the UL suffix only with actual C code compilation.
- */
-#define UL(x) _AC(x, UL)
-
 /*
  * Size of the PCI I/O space. This must remain a power of two so that
  * IO_SPACE_LIMIT acts as a mask for the low bits of I/O addresses.

commit 917538e212a2c080af95ccb4376c5387fac08176
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Tue Feb 6 15:36:44 2018 -0800

    kasan: clean up KASAN_SHADOW_SCALE_SHIFT usage
    
    Right now the fact that KASAN uses a single shadow byte for 8 bytes of
    memory is scattered all over the code.
    
    This change defines KASAN_SHADOW_SCALE_SHIFT early in asm include files
    and makes use of this constant where necessary.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Link: http://lkml.kernel.org/r/34937ca3b90736eaad91b568edf5684091f662e3.1515775666.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Acked-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index d4bae7d6e0d8..50fa96a49792 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -85,7 +85,8 @@
  * stack size when KASAN is in use.
  */
 #ifdef CONFIG_KASAN
-#define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - 3))
+#define KASAN_SHADOW_SCALE_SHIFT 3
+#define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - KASAN_SHADOW_SCALE_SHIFT))
 #define KASAN_THREAD_SHIFT	1
 #else
 #define KASAN_SHADOW_SIZE	(0)

commit c9b012e5f4a1d01dfa8abc6318211a67ba7d5db2
Merge: b293fca43be5 6cfa7cc46b1a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 10:56:56 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "The big highlight is support for the Scalable Vector Extension (SVE)
      which required extensive ABI work to ensure we don't break existing
      applications by blowing away their signal stack with the rather large
      new vector context (<= 2 kbit per vector register). There's further
      work to be done optimising things like exception return, but the ABI
      is solid now.
    
      Much of the line count comes from some new PMU drivers we have, but
      they're pretty self-contained and I suspect we'll have more of them in
      future.
    
      Plenty of acronym soup here:
    
       - initial support for the Scalable Vector Extension (SVE)
    
       - improved handling for SError interrupts (required to handle RAS
         events)
    
       - enable GCC support for 128-bit integer types
    
       - remove kernel text addresses from backtraces and register dumps
    
       - use of WFE to implement long delay()s
    
       - ACPI IORT updates from Lorenzo Pieralisi
    
       - perf PMU driver for the Statistical Profiling Extension (SPE)
    
       - perf PMU driver for Hisilicon's system PMUs
    
       - misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (97 commits)
      arm64: Make ARMV8_DEPRECATED depend on SYSCTL
      arm64: Implement __lshrti3 library function
      arm64: support __int128 on gcc 5+
      arm64/sve: Add documentation
      arm64/sve: Detect SVE and activate runtime support
      arm64/sve: KVM: Hide SVE from CPU features exposed to guests
      arm64/sve: KVM: Treat guest SVE use as undefined instruction execution
      arm64/sve: KVM: Prevent guests from using SVE
      arm64/sve: Add sysctl to set the default vector length for new processes
      arm64/sve: Add prctl controls for userspace vector length management
      arm64/sve: ptrace and ELF coredump support
      arm64/sve: Preserve SVE registers around EFI runtime service calls
      arm64/sve: Preserve SVE registers around kernel-mode NEON use
      arm64/sve: Probe SVE capabilities and usable vector lengths
      arm64: cpufeature: Move sys_caps_initialised declarations
      arm64/sve: Backend logic for setting the vector length
      arm64/sve: Signal handling support
      arm64/sve: Support vector length resetting for new processes
      arm64/sve: Core task context handling
      arm64/sve: Low-level CPU setup
      ...

commit b02faed15d86f846b0f23f47b92e0782baa873ed
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Oct 3 18:25:46 2017 +0100

    arm64: Use larger stacks when KASAN is selected
    
    AddressSanitizer instrumentation can significantly bloat the stack, and
    with GCC 7 this can result in stack overflows at boot time in some
    configurations.
    
    We can avoid this by doubling our stack size when KASAN is in use, as is
    already done on x86 (and has been since KASAN was introduced).
    Regardless of other patches to decrease KASAN's stack utilization,
    kernels built with KASAN will always require more stack space than those
    built without, and we should take this into account.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 3585a5e26151..f7c4d2146aed 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -95,16 +95,19 @@
 #define KERNEL_END        _end
 
 /*
- * The size of the KASAN shadow region. This should be 1/8th of the
- * size of the entire kernel virtual address space.
+ * KASAN requires 1/8th of the kernel virtual address space for the shadow
+ * region. KASAN can bloat the stack significantly, so double the (minimum)
+ * stack size when KASAN is in use.
  */
 #ifdef CONFIG_KASAN
 #define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - 3))
+#define KASAN_THREAD_SHIFT	1
 #else
 #define KASAN_SHADOW_SIZE	(0)
+#define KASAN_THREAD_SHIFT	0
 #endif
 
-#define MIN_THREAD_SHIFT	14
+#define MIN_THREAD_SHIFT	(14 + KASAN_THREAD_SHIFT)
 
 /*
  * VMAP'd stacks are allocated at page granularity, so we must ensure that such

commit eef94a3d09aab437c8c254de942d8b1aa76455e2
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Thu Aug 31 11:30:50 2017 +0300

    arm64: move TASK_* definitions to <asm/processor.h>
    
    ILP32 series [1] introduces the dependency on <asm/is_compat.h> for
    TASK_SIZE macro. Which in turn requires <asm/thread_info.h>, and
    <asm/thread_info.h> include <asm/memory.h>, giving a circular dependency,
    because TASK_SIZE is currently located in <asm/memory.h>.
    
    In other architectures, TASK_SIZE is defined in <asm/processor.h>, and
    moving TASK_SIZE there fixes the problem.
    
    Discussion: https://patchwork.kernel.org/patch/9929107/
    
    [1] https://github.com/norov/linux/tree/ilp32-next
    
    CC: Will Deacon <will.deacon@arm.com>
    CC: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 3585a5e26151..87296a6ed1b3 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -61,8 +61,6 @@
  * KIMAGE_VADDR - the virtual address of the start of the kernel image
  * VA_BITS - the maximum number of bits for virtual addresses.
  * VA_START - the first kernel virtual address.
- * TASK_SIZE - the maximum size of a user space task.
- * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
 #define VA_START		(UL(0xffffffffffffffff) - \
@@ -77,19 +75,6 @@
 #define PCI_IO_END		(VMEMMAP_START - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
-#define TASK_SIZE_64		(UL(1) << VA_BITS)
-
-#ifdef CONFIG_COMPAT
-#define TASK_SIZE_32		UL(0x100000000)
-#define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
-				TASK_SIZE_32 : TASK_SIZE_64)
-#define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_32BIT) ? \
-				TASK_SIZE_32 : TASK_SIZE_64)
-#else
-#define TASK_SIZE		TASK_SIZE_64
-#endif /* CONFIG_COMPAT */
-
-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
 
 #define KERNEL_START      _text
 #define KERNEL_END        _end

commit df5b95bee1ed7009a2090e9924e7a96e14850d56
Merge: 969ff73e72fe 872d8327ce89
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 15 18:40:58 2017 +0100

    Merge branch 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: add VMAP_STACK overflow detection
      arm64: add on_accessible_stack()
      arm64: add basic VMAP_STACK support
      arm64: use an irq stack pointer
      arm64: assembler: allow adr_this_cpu to use the stack pointer
      arm64: factor out entry stack manipulation
      efi/arm64: add EFI_KIMG_ALIGN
      arm64: move SEGMENT_ALIGN to <asm/memory.h>
      arm64: clean up irq stack definitions
      arm64: clean up THREAD_* definitions
      arm64: factor out PAGE_* and CONT_* definitions
      arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
      fork: allow arch-override of VMAP stack alignment
      arm64: remove __die()'s stack dump

commit 872d8327ce8982883b8237b2c320c8666f14e561
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 20:30:35 2017 +0100

    arm64: add VMAP_STACK overflow detection
    
    This patch adds stack overflow detection to arm64, usable when vmap'd stacks
    are in use.
    
    Overflow is detected in a small preamble executed for each exception entry,
    which checks whether there is enough space on the current stack for the general
    purpose registers to be saved. If there is not enough space, the overflow
    handler is invoked on a per-cpu overflow stack. This approach preserves the
    original exception information in ESR_EL1 (and where appropriate, FAR_EL1).
    
    Task and IRQ stacks are aligned to double their size, enabling overflow to be
    detected with a single bit test. For example, a 16K stack is aligned to 32K,
    ensuring that bit 14 of the SP must be zero. On an overflow (or underflow),
    this bit is flipped. Thus, overflow (of less than the size of the stack) can be
    detected by testing whether this bit is set.
    
    The overflow check is performed before any attempt is made to access the
    stack, avoiding recursive faults (and the loss of exception information
    these would entail). As logical operations cannot be performed on the SP
    directly, the SP is temporarily swapped with a general purpose register
    using arithmetic operations to enable the test to be performed.
    
    This gives us a useful error message on stack overflow, as can be trigger with
    the LKDTM overflow test:
    
    [  305.388749] lkdtm: Performing direct entry OVERFLOW
    [  305.395444] Insufficient stack space to handle exception!
    [  305.395482] ESR: 0x96000047 -- DABT (current EL)
    [  305.399890] FAR: 0xffff00000a5e7f30
    [  305.401315] Task stack:     [0xffff00000a5e8000..0xffff00000a5ec000]
    [  305.403815] IRQ stack:      [0xffff000008000000..0xffff000008004000]
    [  305.407035] Overflow stack: [0xffff80003efce4e0..0xffff80003efcf4e0]
    [  305.409622] CPU: 0 PID: 1219 Comm: sh Not tainted 4.13.0-rc3-00021-g9636aea #5
    [  305.412785] Hardware name: linux,dummy-virt (DT)
    [  305.415756] task: ffff80003d051c00 task.stack: ffff00000a5e8000
    [  305.419221] PC is at recursive_loop+0x10/0x48
    [  305.421637] LR is at recursive_loop+0x38/0x48
    [  305.423768] pc : [<ffff00000859f330>] lr : [<ffff00000859f358>] pstate: 40000145
    [  305.428020] sp : ffff00000a5e7f50
    [  305.430469] x29: ffff00000a5e8350 x28: ffff80003d051c00
    [  305.433191] x27: ffff000008981000 x26: ffff000008f80400
    [  305.439012] x25: ffff00000a5ebeb8 x24: ffff00000a5ebeb8
    [  305.440369] x23: ffff000008f80138 x22: 0000000000000009
    [  305.442241] x21: ffff80003ce65000 x20: ffff000008f80188
    [  305.444552] x19: 0000000000000013 x18: 0000000000000006
    [  305.446032] x17: 0000ffffa2601280 x16: ffff0000081fe0b8
    [  305.448252] x15: ffff000008ff546d x14: 000000000047a4c8
    [  305.450246] x13: ffff000008ff7872 x12: 0000000005f5e0ff
    [  305.452953] x11: ffff000008ed2548 x10: 000000000005ee8d
    [  305.454824] x9 : ffff000008545380 x8 : ffff00000a5e8770
    [  305.457105] x7 : 1313131313131313 x6 : 00000000000000e1
    [  305.459285] x5 : 0000000000000000 x4 : 0000000000000000
    [  305.461781] x3 : 0000000000000000 x2 : 0000000000000400
    [  305.465119] x1 : 0000000000000013 x0 : 0000000000000012
    [  305.467724] Kernel panic - not syncing: kernel stack overflow
    [  305.470561] CPU: 0 PID: 1219 Comm: sh Not tainted 4.13.0-rc3-00021-g9636aea #5
    [  305.473325] Hardware name: linux,dummy-virt (DT)
    [  305.475070] Call trace:
    [  305.476116] [<ffff000008088ad8>] dump_backtrace+0x0/0x378
    [  305.478991] [<ffff000008088e64>] show_stack+0x14/0x20
    [  305.481237] [<ffff00000895a178>] dump_stack+0x98/0xb8
    [  305.483294] [<ffff0000080c3288>] panic+0x118/0x280
    [  305.485673] [<ffff0000080c2e9c>] nmi_panic+0x6c/0x70
    [  305.486216] [<ffff000008089710>] handle_bad_stack+0x118/0x128
    [  305.486612] Exception stack(0xffff80003efcf3a0 to 0xffff80003efcf4e0)
    [  305.487334] f3a0: 0000000000000012 0000000000000013 0000000000000400 0000000000000000
    [  305.488025] f3c0: 0000000000000000 0000000000000000 00000000000000e1 1313131313131313
    [  305.488908] f3e0: ffff00000a5e8770 ffff000008545380 000000000005ee8d ffff000008ed2548
    [  305.489403] f400: 0000000005f5e0ff ffff000008ff7872 000000000047a4c8 ffff000008ff546d
    [  305.489759] f420: ffff0000081fe0b8 0000ffffa2601280 0000000000000006 0000000000000013
    [  305.490256] f440: ffff000008f80188 ffff80003ce65000 0000000000000009 ffff000008f80138
    [  305.490683] f460: ffff00000a5ebeb8 ffff00000a5ebeb8 ffff000008f80400 ffff000008981000
    [  305.491051] f480: ffff80003d051c00 ffff00000a5e8350 ffff00000859f358 ffff00000a5e7f50
    [  305.491444] f4a0: ffff00000859f330 0000000040000145 0000000000000000 0000000000000000
    [  305.492008] f4c0: 0001000000000000 0000000000000000 ffff00000a5e8350 ffff00000859f330
    [  305.493063] [<ffff00000808205c>] __bad_stack+0x88/0x8c
    [  305.493396] [<ffff00000859f330>] recursive_loop+0x10/0x48
    [  305.493731] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494088] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494425] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494649] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494898] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495205] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495453] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495708] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496000] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496302] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496644] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496894] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.497138] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.497325] [<ffff00000859f3dc>] lkdtm_OVERFLOW+0x14/0x20
    [  305.497506] [<ffff00000859f314>] lkdtm_do_action+0x1c/0x28
    [  305.497786] [<ffff00000859f178>] direct_entry+0xe0/0x170
    [  305.498095] [<ffff000008345568>] full_proxy_write+0x60/0xa8
    [  305.498387] [<ffff0000081fb7f4>] __vfs_write+0x1c/0x128
    [  305.498679] [<ffff0000081fcc68>] vfs_write+0xa0/0x1b0
    [  305.498926] [<ffff0000081fe0fc>] SyS_write+0x44/0xa0
    [  305.499182] Exception stack(0xffff00000a5ebec0 to 0xffff00000a5ec000)
    [  305.499429] bec0: 0000000000000001 000000001c4cf5e0 0000000000000009 000000001c4cf5e0
    [  305.499674] bee0: 574f4c465245564f 0000000000000000 0000000000000000 8000000080808080
    [  305.499904] bf00: 0000000000000040 0000000000000038 fefefeff1b4bc2ff 7f7f7f7f7f7fff7f
    [  305.500189] bf20: 0101010101010101 0000000000000000 000000000047a4c8 0000000000000038
    [  305.500712] bf40: 0000000000000000 0000ffffa2601280 0000ffffc63f6068 00000000004b5000
    [  305.501241] bf60: 0000000000000001 000000001c4cf5e0 0000000000000009 000000001c4cf5e0
    [  305.501791] bf80: 0000000000000020 0000000000000000 00000000004b5000 000000001c4cc458
    [  305.502314] bfa0: 0000000000000000 0000ffffc63f7950 000000000040a3c4 0000ffffc63f70e0
    [  305.502762] bfc0: 0000ffffa2601268 0000000080000000 0000000000000001 0000000000000040
    [  305.503207] bfe0: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [  305.503680] [<ffff000008082fb0>] el0_svc_naked+0x24/0x28
    [  305.504720] Kernel Offset: disabled
    [  305.505189] CPU features: 0x002082
    [  305.505473] Memory Limit: none
    [  305.506181] ---[ end Kernel panic - not syncing: kernel stack overflow
    
    This patch was co-authored by Ard Biesheuvel and Mark Rutland.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index c5cd2c599b24..1a025b744107 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -133,6 +133,8 @@
 
 #define IRQ_STACK_SIZE		THREAD_SIZE
 
+#define OVERFLOW_STACK_SIZE	SZ_4K
+
 /*
  * Alignment of kernel segments (e.g. .text, .data).
  */

commit e3067861ba6650a566a6273738c23c956ad55c02
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 21 14:25:33 2017 +0100

    arm64: add basic VMAP_STACK support
    
    This patch enables arm64 to be built with vmap'd task and IRQ stacks.
    
    As vmap'd stacks are mapped at page granularity, stacks must be a multiple of
    PAGE_SIZE. This means that a 64K page kernel must use stacks of at least 64K in
    size.
    
    To minimize the increase in Image size, IRQ stacks are dynamically allocated at
    boot time, rather than embedding the boot CPU's IRQ stack in the kernel image.
    
    This patch was co-authored by Ard Biesheuvel and Mark Rutland.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 7fa6ad48d574..c5cd2c599b24 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -102,7 +102,17 @@
 #define KASAN_SHADOW_SIZE	(0)
 #endif
 
-#define THREAD_SHIFT		14
+#define MIN_THREAD_SHIFT	14
+
+/*
+ * VMAP'd stacks are allocated at page granularity, so we must ensure that such
+ * stacks are a multiple of page size.
+ */
+#if defined(CONFIG_VMAP_STACK) && (MIN_THREAD_SHIFT < PAGE_SHIFT)
+#define THREAD_SHIFT		PAGE_SHIFT
+#else
+#define THREAD_SHIFT		MIN_THREAD_SHIFT
+#endif
 
 #if THREAD_SHIFT >= PAGE_SHIFT
 #define THREAD_SIZE_ORDER	(THREAD_SHIFT - PAGE_SHIFT)
@@ -110,6 +120,17 @@
 
 #define THREAD_SIZE		(UL(1) << THREAD_SHIFT)
 
+/*
+ * By aligning VMAP'd stacks to 2 * THREAD_SIZE, we can detect overflow by
+ * checking sp & (1 << THREAD_SHIFT), which we can do cheaply in the entry
+ * assembly.
+ */
+#ifdef CONFIG_VMAP_STACK
+#define THREAD_ALIGN		(2 * THREAD_SIZE)
+#else
+#define THREAD_ALIGN		THREAD_SIZE
+#endif
+
 #define IRQ_STACK_SIZE		THREAD_SIZE
 
 /*

commit 8018ba4edfd3a8b46f876c65988bd0d8e35c32a6
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 15:38:43 2017 +0100

    arm64: move SEGMENT_ALIGN to <asm/memory.h>
    
    Currently we define SEGMENT_ALIGN directly in our vmlinux.lds.S.
    
    This is unfortunate, as the EFI stub currently open-codes the same
    number, and in future we'll want to fiddle with this.
    
    This patch moves the definition to our <asm/memory.h>, where it can be
    used by both vmlinux.lds.S and the EFI stub code.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 1fc24532987e..7fa6ad48d574 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -112,6 +112,25 @@
 
 #define IRQ_STACK_SIZE		THREAD_SIZE
 
+/*
+ * Alignment of kernel segments (e.g. .text, .data).
+ */
+#if defined(CONFIG_DEBUG_ALIGN_RODATA)
+/*
+ *  4 KB granule:   1 level 2 entry
+ * 16 KB granule: 128 level 3 entries, with contiguous bit
+ * 64 KB granule:  32 level 3 entries, with contiguous bit
+ */
+#define SEGMENT_ALIGN			SZ_2M
+#else
+/*
+ *  4 KB granule:  16 level 3 entries, with contiguous bit
+ * 16 KB granule:   4 level 3 entries, without contiguous bit
+ * 64 KB granule:   1 level 3 entry
+ */
+#define SEGMENT_ALIGN			SZ_64K
+#endif
+
 /*
  * Memory types available.
  */

commit f60ad4edcf07238a3d2646d65d8d217032452550
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 20 12:26:48 2017 +0100

    arm64: clean up irq stack definitions
    
    Before we add yet another stack to the kernel, it would be nice to
    ensure that we consistently organise stack definitions and related
    helper functions.
    
    This patch moves the basic IRQ stack defintions to <asm/memory.h> to
    live with their task stack counterparts. Helpers used for unwinding are
    moved into <asm/stacktrace.h>, where subsequent patches will add helpers
    for other stacks. Includes are fixed up accordingly.
    
    This patch is a pure refactoring -- there should be no functional
    changes as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 8ab4774e2616..1fc24532987e 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -110,6 +110,8 @@
 
 #define THREAD_SIZE		(UL(1) << THREAD_SHIFT)
 
+#define IRQ_STACK_SIZE		THREAD_SIZE
+
 /*
  * Memory types available.
  */

commit dbc9344a68e506f19f80a9affc8fe7023a9cdc4c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 16:39:21 2017 +0100

    arm64: clean up THREAD_* definitions
    
    Currently we define THREAD_SIZE and THREAD_SIZE_ORDER separately, with
    the latter dependent on particular CONFIG_ARM64_*K_PAGES definitions.
    This is somewhat opaque, and will get in the way of future modifications
    to THREAD_SIZE.
    
    This patch cleans this up, defining both in terms of a common
    THREAD_SHIFT, and using PAGE_SHIFT to calculate THREAD_SIZE_ORDER,
    rather than using a number of definitions dependent on config symbols.
    Subsequent patches will make use of this to alter the stack size used in
    some configurations.
    
    At the same time, these are moved into <asm/memory.h>, which will avoid
    circular include issues in subsequent patches. To ensure that existing
    code isn't adversely affected, <asm/thread_info.h> is updated to
    transitively include these definitions.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 77d55dcfb86c..8ab4774e2616 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -102,6 +102,14 @@
 #define KASAN_SHADOW_SIZE	(0)
 #endif
 
+#define THREAD_SHIFT		14
+
+#if THREAD_SHIFT >= PAGE_SHIFT
+#define THREAD_SIZE_ORDER	(THREAD_SHIFT - PAGE_SHIFT)
+#endif
+
+#define THREAD_SIZE		(UL(1) << THREAD_SHIFT)
+
 /*
  * Memory types available.
  */

commit b6531456ba279bb7cea5dd2e7b3ec6a3ed0f4668
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 19:43:56 2017 +0100

    arm64: factor out PAGE_* and CONT_* definitions
    
    Some headers rely on PAGE_* definitions from <asm/page.h>, but cannot
    include this due to potential circular includes. For example, a number
    of definitions in <asm/memory.h> rely on PAGE_SHIFT, and <asm/page.h>
    includes <asm/memory.h>.
    
    This requires users of these definitions to include both headers, which
    is fragile and error-prone.
    
    This patch ameliorates matters by moving the basic definitions out to a
    new header, <asm/page-def.h>. Both <asm/page.h> and <asm/memory.h> are
    updated to include this, avoiding this fragility, and avoiding the
    possibility of circular include dependencies.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 32f82723338a..77d55dcfb86c 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -25,6 +25,7 @@
 #include <linux/const.h>
 #include <linux/types.h>
 #include <asm/bug.h>
+#include <asm/page-def.h>
 #include <asm/sizes.h>
 
 /*

commit 82cd588052815eb4146f9f7c5347ca5e32c56360
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Thu Aug 3 11:03:58 2017 -0700

    arm64: avoid overflow in VA_START and PAGE_OFFSET
    
    The bitmask used to define these values produces overflow, as seen by
    this compiler warning:
    
    arch/arm64/kernel/head.S:47:8: warning:
          integer overflow in preprocessor expression
      #elif (PAGE_OFFSET & 0x1fffff) != 0
             ^~~~~~~~~~~
    arch/arm64/include/asm/memory.h:52:46: note:
          expanded from macro 'PAGE_OFFSET'
      #define PAGE_OFFSET             (UL(0xffffffffffffffff) << (VA_BITS -
    1))
                                          ~~~~~~~~~~~~~~~~~~  ^
    
    It would be preferrable to use GENMASK_ULL() instead, but it's not set
    up to be used from assembly (the UL() macro token pastes UL suffixes
    when not included in assembly sources).
    
    Suggested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Suggested-by: Yury Norov <ynorov@caviumnetworks.com>
    Suggested-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 32f82723338a..ef39dcb9ca6a 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -64,8 +64,10 @@
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
-#define VA_START		(UL(0xffffffffffffffff) << VA_BITS)
-#define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
+#define VA_START		(UL(0xffffffffffffffff) - \
+	(UL(1) << VA_BITS) + 1)
+#define PAGE_OFFSET		(UL(0xffffffffffffffff) - \
+	(UL(1) << (VA_BITS - 1)) + 1)
 #define KIMAGE_VADDR		(MODULES_END)
 #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
 #define MODULES_VADDR		(VA_START + KASAN_SHADOW_SIZE)

commit ca78d3173cff3503bcd15723b049757f75762d15
Merge: a4ee7bacd6c0 ffe7afd17135
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:46:44 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     - Errata workarounds for Qualcomm's Falkor CPU
     - Qualcomm L2 Cache PMU driver
     - Qualcomm SMCCC firmware quirk
     - Support for DEBUG_VIRTUAL
     - CPU feature detection for userspace via MRS emulation
     - Preliminary work for the Statistical Profiling Extension
     - Misc cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (74 commits)
      arm64/kprobes: consistently handle MRS/MSR with XZR
      arm64: cpufeature: correctly handle MRS to XZR
      arm64: traps: correctly handle MRS/MSR with XZR
      arm64: ptrace: add XZR-safe regs accessors
      arm64: include asm/assembler.h in entry-ftrace.S
      arm64: fix warning about swapper_pg_dir overflow
      arm64: Work around Falkor erratum 1003
      arm64: head.S: Enable EL1 (host) access to SPE when entered at EL2
      arm64: arch_timer: document Hisilicon erratum 161010101
      arm64: use is_vmalloc_addr
      arm64: use linux/sizes.h for constants
      arm64: uaccess: consistently check object sizes
      perf: add qcom l2 cache perf events driver
      arm64: remove wrong CONFIG_PROC_SYSCTL ifdef
      ARM: smccc: Update HVC comment to describe new quirk parameter
      arm64: do not trace atomic operations
      ACPI/IORT: Fix the error return code in iort_add_smmu_platform_device()
      ACPI/IORT: Fix iort_node_get_id() mapping entries indexing
      arm64: mm: enable CONFIG_HOLES_IN_ZONE for NUMA
      perf: xgene: Include module.h
      ...

commit 1c8a946bf3754a59cba1fc373949a8114bfe5aaa
Author: Oleksandr Andrushchenko <Oleksandr_Andrushchenko@epam.com>
Date:   Wed Jan 18 09:09:25 2017 +0200

    arm64: mm: avoid name clash in __page_to_voff()
    
    The arm64 __page_to_voff() macro takes a parameter called 'page', and
    also refers to 'struct page'. Thus, if the value passed in is not
    called 'page', we'll refer to the wrong struct name (which might not
    exist).
    
    Fixes: 3fa72fe9c614 ("arm64: mm: fix __page_to_voff definition")
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Suggested-by: Volodymyr Babchuk <Volodymyr_Babchuk@epam.com>
    Signed-off-by: Oleksandr Andrushchenko <Oleksandr_Andrushchenko@epam.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index bfe632808d77..90c39a662379 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -222,7 +222,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #else
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
-#define __page_to_voff(page)	(((u64)(page) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
+#define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
 #define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))

commit ec6d06efb0bac6cd92846e42d1afe5b98b57e7c2
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:50 2017 -0800

    arm64: Add support for CONFIG_DEBUG_VIRTUAL
    
    x86 has an option CONFIG_DEBUG_VIRTUAL to do additional checks
    on virt_to_phys calls. The goal is to catch users who are calling
    virt_to_phys on non-linear addresses immediately. This inclues callers
    using virt_to_phys on image addresses instead of __pa_symbol. As features
    such as CONFIG_VMAP_STACK get enabled for arm64, this becomes increasingly
    important. Add checks to catch bad virt_to_phys usage.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 0ff237a75c05..7011f0857629 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -172,10 +172,33 @@ static inline unsigned long kaslr_offset(void)
  * private definitions which should NOT be used outside memory.h
  * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
  */
-#define __virt_to_phys(x) ({						\
+
+
+/*
+ * The linear kernel range starts in the middle of the virtual adddress
+ * space. Testing the top bit for the start of the region is a
+ * sufficient check.
+ */
+#define __is_lm_address(addr)	(!!((addr) & BIT(VA_BITS - 1)))
+
+#define __lm_to_phys(addr)	(((addr) & ~PAGE_OFFSET) + PHYS_OFFSET)
+#define __kimg_to_phys(addr)	((addr) - kimage_voffset)
+
+#define __virt_to_phys_nodebug(x) ({					\
 	phys_addr_t __x = (phys_addr_t)(x);				\
-	__x & BIT(VA_BITS - 1) ? (__x & ~PAGE_OFFSET) + PHYS_OFFSET :	\
-				 (__x - kimage_voffset); })
+	__is_lm_address(__x) ? __lm_to_phys(__x) :			\
+			       __kimg_to_phys(__x);			\
+})
+
+#define __pa_symbol_nodebug(x)	__kimg_to_phys((phys_addr_t)(x))
+
+#ifdef CONFIG_DEBUG_VIRTUAL
+extern phys_addr_t __virt_to_phys(unsigned long x);
+extern phys_addr_t __phys_addr_symbol(unsigned long x);
+#else
+#define __virt_to_phys(x)	__virt_to_phys_nodebug(x)
+#define __phys_addr_symbol(x)	__pa_symbol_nodebug(x)
+#endif
 
 #define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
 #define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
@@ -207,6 +230,8 @@ static inline void *phys_to_virt(phys_addr_t x)
  * Drivers should NOT use these either.
  */
 #define __pa(x)			__virt_to_phys((unsigned long)(x))
+#define __pa_symbol(x)		__phys_addr_symbol(RELOC_HIDE((unsigned long)(x), 0))
+#define __pa_nodebug(x)		__virt_to_phys_nodebug((unsigned long)(x))
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 #define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))

commit 2077be6783b5936c3daa838d8addbb635667927f
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:49 2017 -0800

    arm64: Use __pa_symbol for kernel symbols
    
    __pa_symbol is technically the marcro that should be used for kernel
    symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which
    will do bounds checking.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index cd6e3eec0efe..0ff237a75c05 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -210,6 +210,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 #define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))
+#define sym_to_pfn(x)	    __phys_to_pfn(__pa_symbol(x))
 
 /*
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *

commit 869dcfd10dfe59484cf14acddf264656d5ee94ea
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:48 2017 -0800

    arm64: Add cast for virt_to_pfn
    
    virt_to_pfn lacks a cast at the top level. Don't rely on __virt_to_phys
    and explicitly cast to unsigned long.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index f80a8e403847..cd6e3eec0efe 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -209,7 +209,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __pa(x)			__virt_to_phys((unsigned long)(x))
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
-#define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys(x))
+#define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys((unsigned long)(x)))
 
 /*
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *

commit 9e22eb616f861a25e3123d1221bad61f8f132913
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:47 2017 -0800

    arm64: Move some macros under #ifndef __ASSEMBLY__
    
    Several macros for various x_to_y exist outside the bounds of an
    __ASSEMBLY__ guard. Move them in preparation for support for
    CONFIG_DEBUG_VIRTUAL.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index bfe632808d77..f80a8e403847 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -101,25 +101,6 @@
 #define KASAN_SHADOW_SIZE	(0)
 #endif
 
-/*
- * Physical vs virtual RAM address space conversion.  These are
- * private definitions which should NOT be used outside memory.h
- * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
- */
-#define __virt_to_phys(x) ({						\
-	phys_addr_t __x = (phys_addr_t)(x);				\
-	__x & BIT(VA_BITS - 1) ? (__x & ~PAGE_OFFSET) + PHYS_OFFSET :	\
-				 (__x - kimage_voffset); })
-
-#define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
-#define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
-
-/*
- * Convert a page to/from a physical address
- */
-#define page_to_phys(page)	(__pfn_to_phys(page_to_pfn(page)))
-#define phys_to_page(phys)	(pfn_to_page(__phys_to_pfn(phys)))
-
 /*
  * Memory types available.
  */
@@ -186,6 +167,25 @@ static inline unsigned long kaslr_offset(void)
  */
 #define PHYS_PFN_OFFSET	(PHYS_OFFSET >> PAGE_SHIFT)
 
+/*
+ * Physical vs virtual RAM address space conversion.  These are
+ * private definitions which should NOT be used outside memory.h
+ * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
+ */
+#define __virt_to_phys(x) ({						\
+	phys_addr_t __x = (phys_addr_t)(x);				\
+	__x & BIT(VA_BITS - 1) ? (__x & ~PAGE_OFFSET) + PHYS_OFFSET :	\
+				 (__x - kimage_voffset); })
+
+#define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
+#define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
+
+/*
+ * Convert a page to/from a physical address
+ */
+#define page_to_phys(page)	(__pfn_to_phys(page_to_pfn(page)))
+#define phys_to_page(phys)	(pfn_to_page(__phys_to_pfn(phys)))
+
 /*
  * Note: Drivers should NOT use these.  They are the wrong
  * translation for translating DMA addresses.  Use the driver

commit 7ede8665f27cde7da69e8b2fbeaa1ed0664879c5
Author: Alexander Popov <alex.popov@linux.com>
Date:   Mon Dec 19 16:23:06 2016 -0800

    arm64: setup: introduce kaslr_offset()
    
    Introduce kaslr_offset() similar to x86_64 to fix kcov.
    
    [ Updated by Will Deacon ]
    
    Link: http://lkml.kernel.org/r/1481417456-28826-2-git-send-email-alex.popov@linux.com
    Signed-off-by: Alexander Popov <alex.popov@linux.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Kefeng Wang <wangkefeng.wang@huawei.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Nicolai Stange <nicstange@gmail.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Alexander Popov <alex.popov@linux.com>
    Cc: syzkaller <syzkaller@googlegroups.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index b71086d25195..bfe632808d77 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -165,6 +165,11 @@ extern u64			kimage_vaddr;
 /* the offset between the kernel virtual and physical mappings */
 extern u64			kimage_voffset;
 
+static inline unsigned long kaslr_offset(void)
+{
+	return kimage_vaddr - KIMAGE_VADDR;
+}
+
 /*
  * Allow all memory at the discovery stage. We will clip it later.
  */

commit 3fa72fe9c614717d22ae75b84d45f41da65c10fe
Author: Neeraj Upadhyay <neeraju@codeaurora.org>
Date:   Fri Oct 21 14:28:46 2016 +0530

    arm64: mm: fix __page_to_voff definition
    
    Fix parameter name for __page_to_voff, to match its definition.
    At present, we don't see any issue, as page_to_virt's caller
    declares 'page'.
    
    Fixes: 9f2875912dac ("arm64: mm: restrict virt_to_page() to the linear mapping")
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Neeraj Upadhyay <neeraju@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index ba62df8c6e35..b71086d25195 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -217,7 +217,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #else
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
-#define __page_to_voff(kaddr)	(((u64)(page) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
+#define __page_to_voff(page)	(((u64)(page) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
 
 #define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))

commit ca219452c6b8a6cd1369b6a78b1cf069d0386865
Author: Laura Abbott <labbott@redhat.com>
Date:   Wed Sep 21 15:25:04 2016 -0700

    arm64: Correctly bounds check virt_addr_valid
    
    virt_addr_valid is supposed to return true if and only if virt_to_page
    returns a valid page structure. The current macro does math on whatever
    address is given and passes that to pfn_valid to verify. vmalloc and
    module addresses can happen to generate a pfn that 'happens' to be
    valid. Fix this by only performing the pfn_valid check on addresses that
    have the potential to be valid.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 31b73227b41f..ba62df8c6e35 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -214,7 +214,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 
 #ifndef CONFIG_SPARSEMEM_VMEMMAP
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 #else
 #define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
 #define __page_to_voff(kaddr)	(((u64)(page) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
@@ -222,11 +222,15 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
 #define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
-#define virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
+#define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
 					   + PHYS_OFFSET) >> PAGE_SHIFT)
 #endif
 #endif
 
+#define _virt_addr_is_linear(kaddr)	(((u64)(kaddr)) >= PAGE_OFFSET)
+#define virt_addr_valid(kaddr)		(_virt_addr_is_linear(kaddr) && \
+					 _virt_addr_valid(kaddr))
+
 #include <asm-generic/memory_model.h>
 
 #endif

commit a13e3a5b54e59ffdeba738ea6cb57a7856425206
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jun 1 12:07:17 2016 +0100

    arm64: update stale PAGE_OFFSET comment
    
    Commit ab893fb9f1b17f02 ("arm64: introduce KIMAGE_VADDR as the virtual
    base of the kernel region") logically split KIMAGE_VADDR from
    PAGE_OFFSET, and since commit f9040773b7bbbd9e ("arm64: move kernel
    image to base of vmalloc area") the two have been distinct values.
    
    Unfortunately, neither commit updated the comment above these
    definitions, which now erroneously states that PAGE_OFFSET is the start
    of the kernel image rather than the start of the linear mapping.
    
    This patch fixes said comment, and introduces an explanation of
    KIMAGE_VADDR.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 72a3025bb583..31b73227b41f 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -55,8 +55,9 @@
 #define VMEMMAP_SIZE (UL(1) << (VA_BITS - PAGE_SHIFT - 1 + STRUCT_PAGE_MAX_SHIFT))
 
 /*
- * PAGE_OFFSET - the virtual address of the start of the kernel image (top
+ * PAGE_OFFSET - the virtual address of the start of the linear map (top
  *		 (VA_BITS - 1))
+ * KIMAGE_VADDR - the virtual address of the start of the kernel image
  * VA_BITS - the maximum number of bits for virtual addresses.
  * VA_START - the first kernel virtual address.
  * TASK_SIZE - the maximum size of a user space task.

commit 28c7258330ee4ce701a4da7af96d6605d1a0b3bd
Author: James Morse <james.morse@arm.com>
Date:   Wed Apr 27 17:47:09 2016 +0100

    arm64: Promote KERNEL_START/KERNEL_END definitions to a header file
    
    KERNEL_START and KERNEL_END are useful outside head.S, move them to a
    header file.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index f412f502ccdd..72a3025bb583 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -87,6 +87,9 @@
 
 #define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
 
+#define KERNEL_START      _text
+#define KERNEL_END        _end
+
 /*
  * The size of the KASAN shadow region. This should be 1/8th of the
  * size of the entire kernel virtual address space.

commit 9f2875912dac35d9272a82ea9eec9e5884b42cd2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 16:46:01 2016 +0200

    arm64: mm: restrict virt_to_page() to the linear mapping
    
    Now that the vmemmap region has been redefined to cover the linear region
    rather than the entire physical address space, we no longer need to
    perform a virtual-to-physical translation in the implementaion of
    virt_to_page(). This restricts virt_to_page() translations to the linear
    region, so redefine virt_addr_valid() as well.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 8a2ab195ca77..f412f502ccdd 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -208,9 +208,19 @@ static inline void *phys_to_virt(phys_addr_t x)
  */
 #define ARCH_PFN_OFFSET		((unsigned long)PHYS_PFN_OFFSET)
 
+#ifndef CONFIG_SPARSEMEM_VMEMMAP
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define	virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#else
+#define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
+#define __page_to_voff(kaddr)	(((u64)(page) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
+
+#define page_to_virt(page)	((void *)((__page_to_voff(page)) | PAGE_OFFSET))
+#define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
 
+#define virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
+					   + PHYS_OFFSET) >> PAGE_SHIFT)
+#endif
 #endif
 
 #include <asm-generic/memory_model.h>

commit 3e1907d5bf5a1e0b182ee599f92586f0165029e2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 16:46:00 2016 +0200

    arm64: mm: move vmemmap region right below the linear region
    
    This moves the vmemmap region right below PAGE_OFFSET, aka the start
    of the linear region, and redefines its size to be a power of two.
    Due to the placement of PAGE_OFFSET in the middle of the address space,
    whose size is a power of two as well, this guarantees that virt to
    page conversions and vice versa can be implemented efficiently, by
    masking and shifting rather than ordinary arithmetic.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 12f8a00fb3f1..8a2ab195ca77 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -39,6 +39,21 @@
  */
 #define PCI_IO_SIZE		SZ_16M
 
+/*
+ * Log2 of the upper bound of the size of a struct page. Used for sizing
+ * the vmemmap region only, does not affect actual memory footprint.
+ * We don't use sizeof(struct page) directly since taking its size here
+ * requires its definition to be available at this point in the inclusion
+ * chain, and it may not be a power of 2 in the first place.
+ */
+#define STRUCT_PAGE_MAX_SHIFT	6
+
+/*
+ * VMEMMAP_SIZE - allows the whole linear region to be covered by
+ *                a struct page array
+ */
+#define VMEMMAP_SIZE (UL(1) << (VA_BITS - PAGE_SHIFT - 1 + STRUCT_PAGE_MAX_SHIFT))
+
 /*
  * PAGE_OFFSET - the virtual address of the start of the kernel image (top
  *		 (VA_BITS - 1))
@@ -54,7 +69,8 @@
 #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
 #define MODULES_VADDR		(VA_START + KASAN_SHADOW_SIZE)
 #define MODULES_VSIZE		(SZ_128M)
-#define PCI_IO_END		(PAGE_OFFSET - SZ_2M)
+#define VMEMMAP_START		(PAGE_OFFSET - VMEMMAP_SIZE)
+#define PCI_IO_END		(VMEMMAP_START - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
 #define TASK_SIZE_64		(UL(1) << VA_BITS)

commit 020d044f66874eba058ce8264fc550f3eca67879
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Feb 26 17:57:14 2016 +0100

    arm64: mm: treat memstart_addr as a signed quantity
    
    Commit c031a4213c11 ("arm64: kaslr: randomize the linear region")
    implements randomization of the linear region, by subtracting a random
    multiple of PUD_SIZE from memstart_addr. This causes the virtual mapping
    of system RAM to move upwards in the linear region, and at the same time
    causes memstart_addr to assume a value which may be negative if the offset
    of system RAM in the physical space is smaller than its offset relative to
    PAGE_OFFSET in the virtual space.
    
    Since memstart_addr is effectively an offset now, redefine its type as s64
    so that expressions involving shifting or division preserve its sign.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 5f8667a99e41..12f8a00fb3f1 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -135,7 +135,7 @@
 #include <linux/bitops.h>
 #include <linux/mmdebug.h>
 
-extern phys_addr_t		memstart_addr;
+extern s64			memstart_addr;
 /* PHYS_OFFSET - the physical address of the start of memory. */
 #define PHYS_OFFSET		({ VM_BUG_ON(memstart_addr & 1); memstart_addr; })
 

commit f80fb3a3d50843a401dac4b566b3b131da8077a2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Jan 26 14:12:01 2016 +0100

    arm64: add support for kernel ASLR
    
    This adds support for KASLR is implemented, based on entropy provided by
    the bootloader in the /chosen/kaslr-seed DT property. Depending on the size
    of the address space (VA_BITS) and the page size, the entropy in the
    virtual displacement is up to 13 bits (16k/2 levels) and up to 25 bits (all
    4 levels), with the sidenote that displacements that result in the kernel
    image straddling a 1GB/32MB/512MB alignment boundary (for 4KB/16KB/64KB
    granule kernels, respectively) are not allowed, and will be rounded up to
    an acceptable value.
    
    If CONFIG_RANDOMIZE_MODULE_REGION_FULL is enabled, the module region is
    randomized independently from the core kernel. This makes it less likely
    that the location of core kernel data structures can be determined by an
    adversary, but causes all function calls from modules into the core kernel
    to be resolved via entries in the module PLTs.
    
    If CONFIG_RANDOMIZE_MODULE_REGION_FULL is not enabled, the module region is
    randomized by choosing a page aligned 128 MB region inside the interval
    [_etext - 128 MB, _stext + 128 MB). This gives between 10 and 14 bits of
    entropy (depending on page size), independently of the kernel randomization,
    but still guarantees that modules are within the range of relative branch
    and jump instructions (with the caveat that, since the module region is
    shared with other uses of the vmalloc area, modules may need to be loaded
    further away if the module region is exhausted)
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index eb798156cf56..5f8667a99e41 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -53,7 +53,7 @@
 #define KIMAGE_VADDR		(MODULES_END)
 #define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
 #define MODULES_VADDR		(VA_START + KASAN_SHADOW_SIZE)
-#define MODULES_VSIZE		(SZ_64M)
+#define MODULES_VSIZE		(SZ_128M)
 #define PCI_IO_END		(PAGE_OFFSET - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
@@ -139,6 +139,9 @@ extern phys_addr_t		memstart_addr;
 /* PHYS_OFFSET - the physical address of the start of memory. */
 #define PHYS_OFFSET		({ VM_BUG_ON(memstart_addr & 1); memstart_addr; })
 
+/* the virtual base of the kernel image (minus TEXT_OFFSET) */
+extern u64			kimage_vaddr;
+
 /* the offset between the kernel virtual and physical mappings */
 extern u64			kimage_voffset;
 

commit 8439e62a15614e8fcd43835d57b7245cd9870dc5
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Feb 22 18:46:04 2016 +0100

    arm64: mm: use bit ops rather than arithmetic in pa/va translations
    
    Since PAGE_OFFSET is chosen such that it cuts the kernel VA space right
    in half, and since the size of the kernel VA space itself is always a
    power of 2, we can treat PAGE_OFFSET as a bitmask and replace the
    additions/subtractions with 'or' and 'and-not' operations.
    
    For the comparison against PAGE_OFFSET, a mov/cmp/branch sequence ends
    up getting replaced with a single tbz instruction. For the additions and
    subtractions, we save a mov instruction since the mask is folded into the
    instruction's immediate field.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 460d09bf9442..eb798156cf56 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -88,10 +88,10 @@
  */
 #define __virt_to_phys(x) ({						\
 	phys_addr_t __x = (phys_addr_t)(x);				\
-	__x >= PAGE_OFFSET ? (__x - PAGE_OFFSET + PHYS_OFFSET) :	\
-			     (__x - kimage_voffset); })
+	__x & BIT(VA_BITS - 1) ? (__x & ~PAGE_OFFSET) + PHYS_OFFSET :	\
+				 (__x - kimage_voffset); })
 
-#define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
+#define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET) | PAGE_OFFSET)
 #define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
 
 /*
@@ -132,6 +132,7 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/bitops.h>
 #include <linux/mmdebug.h>
 
 extern phys_addr_t		memstart_addr;

commit a92405f082d43267575444a6927085e4c8a69e4e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Feb 22 18:46:03 2016 +0100

    arm64: mm: only perform memstart_addr sanity check if DEBUG_VM
    
    Checking whether memstart_addr has been assigned every time it is
    referenced adds a branch instruction that may hurt performance if
    the reference in question occurs on a hot path. So only perform the
    check if CONFIG_DEBUG_VM=y.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [catalin.marinas@arm.com: replaced #ifdef with VM_BUG_ON]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 3239e4d78e0d..460d09bf9442 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -132,9 +132,11 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/mmdebug.h>
+
 extern phys_addr_t		memstart_addr;
 /* PHYS_OFFSET - the physical address of the start of memory. */
-#define PHYS_OFFSET		({ BUG_ON(memstart_addr & 1); memstart_addr; })
+#define PHYS_OFFSET		({ VM_BUG_ON(memstart_addr & 1); memstart_addr; })
 
 /* the offset between the kernel virtual and physical mappings */
 extern u64			kimage_voffset;

commit a7f8de168ace487fa7b88cb154e413cf40e87fc6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:42 2016 +0100

    arm64: allow kernel Image to be loaded anywhere in physical memory
    
    This relaxes the kernel Image placement requirements, so that it
    may be placed at any 2 MB aligned offset in physical memory.
    
    This is accomplished by ignoring PHYS_OFFSET when installing
    memblocks, and accounting for the apparent virtual offset of
    the kernel Image. As a result, virtual address references
    below PAGE_OFFSET are correctly mapped onto physical references
    into the kernel Image regardless of where it sits in memory.
    
    Special care needs to be taken for dealing with memory limits passed
    via mem=, since the generic implementation clips memory top down, which
    may clip the kernel image itself if it is loaded high up in memory. To
    deal with this case, we simply add back the memory covering the kernel
    image, which may result in more memory to be retained than was passed
    as a mem= parameter.
    
    Since mem= should not be considered a production feature, a panic notifier
    handler is installed that dumps the memory limit at panic time if one was
    set.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 18b7e77c7495..3239e4d78e0d 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -24,6 +24,7 @@
 #include <linux/compiler.h>
 #include <linux/const.h>
 #include <linux/types.h>
+#include <asm/bug.h>
 #include <asm/sizes.h>
 
 /*
@@ -88,10 +89,10 @@
 #define __virt_to_phys(x) ({						\
 	phys_addr_t __x = (phys_addr_t)(x);				\
 	__x >= PAGE_OFFSET ? (__x - PAGE_OFFSET + PHYS_OFFSET) :	\
-			     (__x - KIMAGE_VADDR + PHYS_OFFSET); })
+			     (__x - kimage_voffset); })
 
 #define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
-#define __phys_to_kimg(x)	((unsigned long)((x) - PHYS_OFFSET + KIMAGE_VADDR))
+#define __phys_to_kimg(x)	((unsigned long)((x) + kimage_voffset))
 
 /*
  * Convert a page to/from a physical address
@@ -133,15 +134,16 @@
 
 extern phys_addr_t		memstart_addr;
 /* PHYS_OFFSET - the physical address of the start of memory. */
-#define PHYS_OFFSET		({ memstart_addr; })
+#define PHYS_OFFSET		({ BUG_ON(memstart_addr & 1); memstart_addr; })
+
+/* the offset between the kernel virtual and physical mappings */
+extern u64			kimage_voffset;
 
 /*
- * The maximum physical address that the linear direct mapping
- * of system RAM can cover. (PAGE_OFFSET can be interpreted as
- * a 2's complement signed quantity and negated to derive the
- * maximum size of the linear mapping.)
+ * Allow all memory at the discovery stage. We will clip it later.
  */
-#define MAX_MEMBLOCK_ADDR	({ memstart_addr - PAGE_OFFSET - 1; })
+#define MIN_MEMBLOCK_ADDR	0
+#define MAX_MEMBLOCK_ADDR	U64_MAX
 
 /*
  * PFNs are used to describe any physical page; this means

commit a89dea585371a9d5d85499db47c93f129be8e0c4
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:41 2016 +0100

    arm64: defer __va translation of initrd_start and initrd_end
    
    Before deferring the assignment of memstart_addr in a subsequent patch, to
    the moment where all memory has been discovered and possibly clipped based
    on the size of the linear region and the presence of a mem= command line
    parameter, we need to ensure that memstart_addr is not used to perform __va
    translations before it is assigned.
    
    One such use is in the generic early DT discovery of the initrd location,
    which is recorded as a virtual address in the globals initrd_start and
    initrd_end. So wire up the generic support to declare the initrd addresses,
    and implement it without __va() translations, and perform the translation
    after memstart_addr has been assigned.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 4388651d1f0d..18b7e77c7495 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -121,6 +121,14 @@
 #define IOREMAP_MAX_ORDER	(PMD_SHIFT)
 #endif
 
+#ifdef CONFIG_BLK_DEV_INITRD
+#define __early_init_dt_declare_initrd(__start, __end)			\
+	do {								\
+		initrd_start = (__start);				\
+		initrd_end = (__end);					\
+	} while (0)
+#endif
+
 #ifndef __ASSEMBLY__
 
 extern phys_addr_t		memstart_addr;

commit f9040773b7bbbd9e98eb6184a263512a7cfc133f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:40 2016 +0100

    arm64: move kernel image to base of vmalloc area
    
    This moves the module area to right before the vmalloc area, and moves
    the kernel image to the base of the vmalloc area. This is an intermediate
    step towards implementing KASLR, which allows the kernel image to be
    located anywhere in the vmalloc area.
    
    Since other subsystems such as hibernate may still need to refer to the
    kernel text or data segments via their linears addresses, both are mapped
    in the linear region as well. The linear alias of the text region is
    mapped read-only/non-executable to prevent inadvertent modification or
    execution.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index aebc739f5a11..4388651d1f0d 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -45,16 +45,15 @@
  * VA_START - the first kernel virtual address.
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
- * The module space lives between the addresses given by TASK_SIZE
- * and PAGE_OFFSET - it must be within 128MB of the kernel text.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
 #define VA_START		(UL(0xffffffffffffffff) << VA_BITS)
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
-#define KIMAGE_VADDR		(PAGE_OFFSET)
-#define MODULES_END		(KIMAGE_VADDR)
-#define MODULES_VADDR		(MODULES_END - SZ_64M)
-#define PCI_IO_END		(MODULES_VADDR - SZ_2M)
+#define KIMAGE_VADDR		(MODULES_END)
+#define MODULES_END		(MODULES_VADDR + MODULES_VSIZE)
+#define MODULES_VADDR		(VA_START + KASAN_SHADOW_SIZE)
+#define MODULES_VSIZE		(SZ_64M)
+#define PCI_IO_END		(PAGE_OFFSET - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
 #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
 #define TASK_SIZE_64		(UL(1) << VA_BITS)
@@ -71,6 +70,16 @@
 
 #define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
 
+/*
+ * The size of the KASAN shadow region. This should be 1/8th of the
+ * size of the entire kernel virtual address space.
+ */
+#ifdef CONFIG_KASAN
+#define KASAN_SHADOW_SIZE	(UL(1) << (VA_BITS - 3))
+#else
+#define KASAN_SHADOW_SIZE	(0)
+#endif
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit ab893fb9f1b17f02139bce547bb4b69e96b9ae16
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:36 2016 +0100

    arm64: introduce KIMAGE_VADDR as the virtual base of the kernel region
    
    This introduces the preprocessor symbol KIMAGE_VADDR which will serve as
    the symbolic virtual base of the kernel region, i.e., the kernel's virtual
    offset will be KIMAGE_VADDR + TEXT_OFFSET. For now, we define it as being
    equal to PAGE_OFFSET, but in the future, it will be moved below it once
    we move the kernel virtual mapping out of the linear mapping.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index c65aad7b13dc..aebc739f5a11 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -51,7 +51,8 @@
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
 #define VA_START		(UL(0xffffffffffffffff) << VA_BITS)
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
-#define MODULES_END		(PAGE_OFFSET)
+#define KIMAGE_VADDR		(PAGE_OFFSET)
+#define MODULES_END		(KIMAGE_VADDR)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)
 #define PCI_IO_END		(MODULES_VADDR - SZ_2M)
 #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
@@ -75,8 +76,13 @@
  * private definitions which should NOT be used outside memory.h
  * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
  */
-#define __virt_to_phys(x)	(((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))
+#define __virt_to_phys(x) ({						\
+	phys_addr_t __x = (phys_addr_t)(x);				\
+	__x >= PAGE_OFFSET ? (__x - PAGE_OFFSET + PHYS_OFFSET) :	\
+			     (__x - KIMAGE_VADDR + PHYS_OFFSET); })
+
 #define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
+#define __phys_to_kimg(x)	((unsigned long)((x) - PHYS_OFFSET + KIMAGE_VADDR))
 
 /*
  * Convert a page to/from a physical address

commit 324420bf91f60582bb481133db9547111768ef17
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:35 2016 +0100

    arm64: add support for ioremap() block mappings
    
    This wires up the existing generic huge-vmap feature, which allows
    ioremap() to use PMD or PUD sized block mappings. It also adds support
    to the unmap path for dealing with block mappings, which will allow us
    to unmap the __init region using unmap_kernel_range() in a subsequent
    patch.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 853953cd1f08..c65aad7b13dc 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -100,6 +100,12 @@
 #define MT_S2_NORMAL		0xf
 #define MT_S2_DEVICE_nGnRE	0x1
 
+#ifdef CONFIG_ARM64_4K_PAGES
+#define IOREMAP_MAX_ORDER	(PUD_SHIFT)
+#else
+#define IOREMAP_MAX_ORDER	(PMD_SHIFT)
+#endif
+
 #ifndef __ASSEMBLY__
 
 extern phys_addr_t		memstart_addr;

commit 2dc10ad81fc017837037e60439662e1b16bdffb9
Merge: e627078a0cbd f8f8bdc48851
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 14:47:13 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - "genirq: Introduce generic irq migration for cpu hotunplugged" patch
       merged from tip/irq/for-arm to allow the arm64-specific part to be
       upstreamed via the arm64 tree
    
     - CPU feature detection reworked to cope with heterogeneous systems
       where CPUs may not have exactly the same features.  The features
       reported by the kernel via internal data structures or ELF_HWCAP are
       delayed until all the CPUs are up (and before user space starts)
    
     - Support for 16KB pages, with the additional bonus of a 36-bit VA
       space, though the latter only depending on EXPERT
    
     - Implement native {relaxed, acquire, release} atomics for arm64
    
     - New ASID allocation algorithm which avoids IPI on roll-over, together
       with TLB invalidation optimisations (using local vs global where
       feasible)
    
     - KASan support for arm64
    
     - EFI_STUB clean-up and isolation for the kernel proper (required by
       KASan)
    
     - copy_{to,from,in}_user optimisations (sharing the memcpy template)
    
     - perf: moving arm64 to the arm32/64 shared PMU framework
    
     - L1_CACHE_BYTES increased to 128 to accommodate Cavium hardware
    
     - Support for the contiguous PTE hint on kernel mapping (16 consecutive
       entries may be able to use a single TLB entry)
    
     - Generic CONFIG_HZ now used on arm64
    
     - defconfig updates
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (91 commits)
      arm64/efi: fix libstub build under CONFIG_MODVERSIONS
      ARM64: Enable multi-core scheduler support by default
      arm64/efi: move arm64 specific stub C code to libstub
      arm64: page-align sections for DEBUG_RODATA
      arm64: Fix build with CONFIG_ZONE_DMA=n
      arm64: Fix compat register mappings
      arm64: Increase the max granular size
      arm64: remove bogus TASK_SIZE_64 check
      arm64: make Timer Interrupt Frequency selectable
      arm64/mm: use PAGE_ALIGNED instead of IS_ALIGNED
      arm64: cachetype: fix definitions of ICACHEF_* flags
      arm64: cpufeature: declare enable_cpu_capabilities as static
      genirq: Make the cpuhotplug migration code less noisy
      arm64: Constify hwcap name string arrays
      arm64/kvm: Make use of the system wide safe values
      arm64/debug: Make use of the system wide safe value
      arm64: Move FP/ASIMD hwcap handling to common code
      arm64/HWCAP: Use system wide safe values
      arm64/capabilities: Make use of system wide safe value
      arm64: Delay cpu feature capability checks
      ...

commit 6e4a0f2b5c56af6be43b546df16b1ece7df537d8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Oct 26 12:53:17 2015 +0900

    arm64: remove bogus TASK_SIZE_64 check
    
    The comparison between TASK_SIZE_64 and MODULES_VADDR does not
    make any sense on arm64, it is simply something that has been
    carried over from the ARM port which arm64 is based on. So drop it.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 11ccf6c09533..37f3538a2924 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -70,10 +70,6 @@
 
 #define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
 
-#if TASK_SIZE_64 > MODULES_VADDR
-#error Top of 64-bit user space clashes with start of module space
-#endif
-
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit c7d77a7980e434c3af17de19e3348157f9b9ccce
Merge: 0ce423b6492a 8a53554e12e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Oct 14 16:05:18 2015 +0200

    Merge branch 'x86/urgent' into core/efi, to pick up a pending EFI fix
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 127db024a7baee9874014dac33628253f438b4da
Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
Date:   Thu Sep 17 12:38:07 2015 +0300

    arm64: introduce VA_START macro - the first kernel virtual address.
    
    In order to not use lengthy (UL(0xffffffffffffffff) << VA_BITS) everywhere,
    replace it with VA_START.
    
    Signed-off-by: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 6b4c3ad75a2a..11ccf6c09533 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -42,12 +42,14 @@
  * PAGE_OFFSET - the virtual address of the start of the kernel image (top
  *		 (VA_BITS - 1))
  * VA_BITS - the maximum number of bits for virtual addresses.
+ * VA_START - the first kernel virtual address.
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 128MB of the kernel text.
  */
 #define VA_BITS			(CONFIG_ARM64_VA_BITS)
+#define VA_START		(UL(0xffffffffffffffff) << VA_BITS)
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
 #define MODULES_END		(PAGE_OFFSET)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)

commit 12f03ee606914317e7e6a0815e53a48205c31dae
Merge: d9241b22b58e 004f1afbe199
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 8 14:35:59 2015 -0700

    Merge tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "This update has successfully completed a 0day-kbuild run and has
      appeared in a linux-next release.  The changes outside of the typical
      drivers/nvdimm/ and drivers/acpi/nfit.[ch] paths are related to the
      removal of IORESOURCE_CACHEABLE, the introduction of memremap(), and
      the introduction of ZONE_DEVICE + devm_memremap_pages().
    
      Summary:
    
       - Introduce ZONE_DEVICE and devm_memremap_pages() as a generic
         mechanism for adding device-driver-discovered memory regions to the
         kernel's direct map.
    
         This facility is used by the pmem driver to enable pfn_to_page()
         operations on the page frames returned by DAX ('direct_access' in
         'struct block_device_operations').
    
         For now, the 'memmap' allocation for these "device" pages comes
         from "System RAM".  Support for allocating the memmap from device
         memory will arrive in a later kernel.
    
       - Introduce memremap() to replace usages of ioremap_cache() and
         ioremap_wt().  memremap() drops the __iomem annotation for these
         mappings to memory that do not have i/o side effects.  The
         replacement of ioremap_cache() with memremap() is limited to the
         pmem driver to ease merging the api change in v4.3.
    
         Completion of the conversion is targeted for v4.4.
    
       - Similar to the usage of memcpy_to_pmem() + wmb_pmem() in the pmem
         driver, update the VFS DAX implementation and PMEM api to provide
         persistence guarantees for kernel operations on a DAX mapping.
    
       - Convert the ACPI NFIT 'BLK' driver to map the block apertures as
         cacheable to improve performance.
    
       - Miscellaneous updates and fixes to libnvdimm including support for
         issuing "address range scrub" commands, clarifying the optimal
         'sector size' of pmem devices, a clarification of the usage of the
         ACPI '_STA' (status) property for DIMM devices, and other minor
         fixes"
    
    * tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (34 commits)
      libnvdimm, pmem: direct map legacy pmem by default
      libnvdimm, pmem: 'struct page' for pmem
      libnvdimm, pfn: 'struct page' provider infrastructure
      x86, pmem: clarify that ARCH_HAS_PMEM_API implies PMEM mapped WB
      add devm_memremap_pages
      mm: ZONE_DEVICE for "device memory"
      mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
      dax: drop size parameter to ->direct_access()
      nd_blk: change aperture mapping from WC to WB
      nvdimm: change to use generic kvfree()
      pmem, dax: have direct_access use __pmem annotation
      dax: update I/O path to do proper PMEM flushing
      pmem: add copy_from_iter_pmem() and clear_pmem()
      pmem, x86: clean up conditional pmem includes
      pmem: remove layer when calling arch_has_wmb_pmem()
      pmem, x86: move x86 PMEM API to new pmem.h header
      libnvdimm, e820: make CONFIG_X86_PMEM_LEGACY a tristate option
      pmem: switch to devm_ allocations
      devres: add devm_memremap
      libnvdimm, btt: write and validate parent_uuid
      ...

commit 012dcef3f058385268630c0003e9b7f8dcafbeb4
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 7 17:41:01 2015 -0400

    mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
    
    Three architectures already define these, and we'll need them genericly
    soon.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index f800d45ea226..d808bb688751 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -80,12 +80,6 @@
 #define __virt_to_phys(x)	(((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))
 #define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
 
-/*
- * Convert a physical address to a Page Frame Number and back
- */
-#define	__phys_to_pfn(paddr)	((unsigned long)((paddr) >> PAGE_SHIFT))
-#define	__pfn_to_phys(pfn)	((phys_addr_t)(pfn) << PAGE_SHIFT)
-
 /*
  * Convert a page to/from a physical address
  */

commit 34ba2c4247e5c4b1542b1106e156af324660c4f0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Aug 18 10:34:42 2015 +0100

    arm64: set MAX_MEMBLOCK_ADDR according to linear region size
    
    The linear region size of a 39-bit VA kernel is only 256 GB, which
    may be insufficient to cover all of system RAM, even on platforms
    that have much less than 256 GB of memory but which is laid out
    very sparsely.
    
    So make sure we clip the memory we will not be able to map before
    installing it into the memblock memory table, by setting
    MAX_MEMBLOCK_ADDR accordingly.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stuart Yoder <stuart.yoder@freescale.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index f800d45ea226..44a59c20e773 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -113,6 +113,14 @@ extern phys_addr_t		memstart_addr;
 /* PHYS_OFFSET - the physical address of the start of memory. */
 #define PHYS_OFFSET		({ memstart_addr; })
 
+/*
+ * The maximum physical address that the linear direct mapping
+ * of system RAM can cover. (PAGE_OFFSET can be interpreted as
+ * a 2's complement signed quantity and negated to derive the
+ * maximum size of the linear mapping.)
+ */
+#define MAX_MEMBLOCK_ADDR	({ memstart_addr - PAGE_OFFSET - 1; })
+
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.

commit 8d446c8647c9ab8fcb45a8fc7dbbafe1f83aa2f3
Author: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
Date:   Fri Aug 7 09:36:59 2015 +0100

    arm64/mm: Add PROT_DEVICE_nGnRnE and PROT_NORMAL_WT
    
    UEFI spec 2.5 section 2.3.6.1 defines that
    EFI_MEMORY_[UC|WC|WT|WB] are possible EFI memory types for
    AArch64.
    
    Each of those EFI memory types is mapped to a corresponding
    AArch64 memory type. So we need to define PROT_DEVICE_nGnRnE
    and PROT_NORMWL_WT additionaly.
    
    MT_NORMAL_WT is defined, and its encoding is added to MAIR_EL1
    when initializing the CPU.
    
    Signed-off-by: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438936621-5215-6-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index f800d45ea226..4112b3d7468e 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -100,6 +100,7 @@
 #define MT_DEVICE_GRE		2
 #define MT_NORMAL_NC		3
 #define MT_NORMAL		4
+#define MT_NORMAL_WT		5
 
 /*
  * Memory types for Stage-2 translation

commit aa03c428e67881795f4190f9ffdb62fc14978608
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 22 18:20:35 2015 +0000

    arm64: Fix overlapping VA allocations
    
    PCI IO space was intended to be 16MiB, at 32MiB below MODULES_VADDR, but
    commit d1e6dc91b532d3d3 ("arm64: Add architectural support for PCI")
    extended this to cover the full 32MiB. The final 8KiB of this 32MiB is
    also allocated for the fixmap, allowing for potential clashes between
    the two.
    
    This change was masked by assumptions in mem_init and the page table
    dumping code, which assumed the I/O space to be 16MiB long through
    seaparte hard-coded definitions.
    
    This patch changes the definition of the PCI I/O space allocation to
    live in asm/memory.h, along with the other VA space allocations. As the
    fixmap allocation depends on the number of fixmap entries, this is moved
    below the PCI I/O space allocation. Both the fixmap and PCI I/O space
    are guarded with 2MB of padding. Sites assuming the I/O space was 16MiB
    are moved over use new PCI_IO_{START,END} definitions, which will keep
    in sync with the size of the IO space (now restored to 16MiB).
    
    As a useful side effect, the use of the new PCI_IO_{START,END}
    definitions prevents a build issue in the dumping code due to a (now
    redundant) missing include of io.h for PCI_IOBASE.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Liviu Dudau <liviu.dudau@arm.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: reorder FIXADDR and PCI_IO address_markers_idx enum]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 6486b2bfd562..f800d45ea226 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -32,6 +32,12 @@
  */
 #define UL(x) _AC(x, UL)
 
+/*
+ * Size of the PCI I/O space. This must remain a power of two so that
+ * IO_SPACE_LIMIT acts as a mask for the low bits of I/O addresses.
+ */
+#define PCI_IO_SIZE		SZ_16M
+
 /*
  * PAGE_OFFSET - the virtual address of the start of the kernel image (top
  *		 (VA_BITS - 1))
@@ -45,7 +51,9 @@
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
 #define MODULES_END		(PAGE_OFFSET)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)
-#define FIXADDR_TOP		(MODULES_VADDR - SZ_2M - PAGE_SIZE)
+#define PCI_IO_END		(MODULES_VADDR - SZ_2M)
+#define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
+#define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
 #define TASK_SIZE_64		(UL(1) << VA_BITS)
 
 #ifdef CONFIG_COMPAT

commit a0e4467726cd26bacb16f13d207ffcfa82ffc07d
Merge: ed8efd2de754 cb61f6769b88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 9 17:25:00 2014 -0800

    Merge tag 'asm-generic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic
    
    Pull asm-generic asm/io.h rewrite from Arnd Bergmann:
     "While there normally is no reason to have a pull request for
      asm-generic but have all changes get merged through whichever tree
      needs them, I do have a series for 3.19.
    
      There are two sets of patches that change significant portions of
      asm/io.h, and this branch contains both in order to resolve the
      conflicts:
    
       - Will Deacon has done a set of patches to ensure that all
         architectures define {read,write}{b,w,l,q}_relaxed() functions or
         get them by including asm-generic/io.h.
    
         These functions are commonly used on ARM specific drivers to avoid
         expensive L2 cache synchronization implied by the normal
         {read,write}{b,w,l,q}, but we need to define them on all
         architectures in order to share the drivers across architectures
         and to enable CONFIG_COMPILE_TEST configurations for them
    
       - Thierry Reding has done an unrelated set of patches that extends
         the asm-generic/io.h file to the degree necessary to make it useful
         on ARM64 and potentially other architectures"
    
    * tag 'asm-generic-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/asm-generic: (29 commits)
      ARM64: use GENERIC_PCI_IOMAP
      sparc: io: remove duplicate relaxed accessors on sparc32
      ARM: sa11x0: Use void __iomem * in MMIO accessors
      arm64: Use include/asm-generic/io.h
      ARM: Use include/asm-generic/io.h
      asm-generic/io.h: Implement generic {read,write}s*()
      asm-generic/io.h: Reconcile I/O accessor overrides
      /dev/mem: Use more consistent data types
      Change xlate_dev_{kmem,mem}_ptr() prototypes
      ARM: ixp4xx: Properly override I/O accessors
      ARM: ixp4xx: Fix build with IXP4XX_INDIRECT_PCI
      ARM: ebsa110: Properly override I/O accessors
      ARC: Remove redundant PCI_IOBASE declaration
      documentation: memory-barriers: clarify relaxed io accessor semantics
      x86: io: implement dummy relaxed accessor macros for writes
      tile: io: implement dummy relaxed accessor macros for writes
      sparc: io: implement dummy relaxed accessor macros for writes
      powerpc: io: implement dummy relaxed accessor macros for writes
      parisc: io: implement dummy relaxed accessor macros for writes
      mn10300: io: implement dummy relaxed accessor macros for writes
      ...

commit 5fd6690c8fb2c3012012979dd8ce7492c69c5d86
Author: Neil Zhang <zhangwm@marvell.com>
Date:   Tue Oct 28 05:44:01 2014 +0000

    arm64: ARCH_PFN_OFFSET should be unsigned long
    
    pfns are unsigned long, but PHYS_PFN_OFFSET is phys_addr_t. This leads
    to page_to_pfn() returning phys_addr_t which cause type mismatches in
    some print statements.
    
    Signed-off-by: Neil Zhang <zhangwm@marvell.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index ccc7087d3c4e..a62cd077457b 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -142,7 +142,7 @@ static inline void *phys_to_virt(phys_addr_t x)
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
  *  virt_addr_valid(k)	indicates whether a virtual address is valid
  */
-#define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
+#define ARCH_PFN_OFFSET		((unsigned long)PHYS_PFN_OFFSET)
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define	virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)

commit 09a5723983e383e7d627fe3191366761722695bc
Author: Thierry Reding <treding@nvidia.com>
Date:   Mon Jul 28 17:25:48 2014 +0200

    arm64: Use include/asm-generic/io.h
    
    Include the generic I/O header file so that duplicate implementations
    can be removed. This will also help to establish consistency across more
    architectures regarding which accessors they support.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Thierry Reding <treding@nvidia.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index ccc7087d3c4e..a40a4d7e830f 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -120,11 +120,13 @@ extern phys_addr_t		memstart_addr;
  * translation for translating DMA addresses.  Use the driver
  * DMA support - see dma-mapping.h.
  */
+#define virt_to_phys virt_to_phys
 static inline phys_addr_t virt_to_phys(const volatile void *x)
 {
 	return __virt_to_phys((unsigned long)(x));
 }
 
+#define phys_to_virt phys_to_virt
 static inline void *phys_to_virt(phys_addr_t x)
 {
 	return (void *)(__phys_to_virt(x));

commit 5167d09ffad5b16b574d35ce3047ed34caf1e837
Merge: 8533ce727188 ea1719672f59
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 12:31:53 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Once again, Catalin's off on holiday and I'm looking after the arm64
      tree.  Please can you pull the following arm64 updates for 3.17?
    
      Note that this branch also includes the new GICv3 driver (merged via a
      stable tag from Jason's irqchip tree), since there is a fix for older
      binutils on top.
    
      Changes include:
       - context tracking support (NO_HZ_FULL) which narrowly missed 3.16
       - vDSO layout rework following Andy's work on x86
       - TEXT_OFFSET fuzzing for bootloader testing
       - /proc/cpuinfo tidy-up
       - preliminary work to support 48-bit virtual addresses, but this is
         currently disabled until KVM has been ported to use it (the patches
         do, however, bring some nice clean-up)
       - boot-time CPU sanity checks (especially useful on heterogenous
         systems)
       - support for syscall auditing
       - support for CC_STACKPROTECTOR
       - defconfig updates"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (55 commits)
      arm64: add newline to I-cache policy string
      Revert "arm64: dmi: Add SMBIOS/DMI support"
      arm64: fpsimd: fix a typo in fpsimd_save_partial_state ENDPROC
      arm64: don't call break hooks for BRK exceptions from EL0
      arm64: defconfig: enable devtmpfs mount option
      arm64: vdso: fix build error when switching from LE to BE
      arm64: defconfig: add virtio support for running as a kvm guest
      arm64: gicv3: Allow GICv3 compilation with older binutils
      arm64: fix soft lockup due to large tlb flush range
      arm64/crypto: fix makefile rule for aes-glue-%.o
      arm64: Do not invoke audit_syscall_* functions if !CONFIG_AUDIT_SYSCALL
      arm64: Fix barriers used for page table modifications
      arm64: Add support for 48-bit VA space with 64KB page configuration
      arm64: asm/pgtable.h pmd/pud definitions clean-up
      arm64: Determine the vmalloc/vmemmap space at build time based on VA_BITS
      arm64: Clean up the initial page table creation in head.S
      arm64: Remove asm/pgtable-*level-types.h files
      arm64: Remove asm/pgtable-*level-hwdef.h files
      arm64: Convert bool ARM64_x_LEVELS to int ARM64_PGTABLE_LEVELS
      arm64: mm: Implement 4 levels of translation tables
      ...

commit e41ceed035966d593ae34c3de33924965a6b9fba
Author: Jungseok Lee <jays.lee@samsung.com>
Date:   Mon May 12 10:40:38 2014 +0100

    arm64: Introduce VA_BITS and translation level options
    
    This patch adds virtual address space size and a level of translation
    tables to kernel configuration. It facilicates introduction of
    different MMU options, such as 4KB + 4 levels, 16KB + 4 levels and
    64KB + 3 levels, easily.
    
    The idea is based on the discussion with Catalin Marinas:
    http://www.spinics.net/linux/lists/arm-kernel/msg319552.html
    
    Signed-off-by: Jungseok Lee <jays.lee@samsung.com>
    Reviewed-by: Sungjinn Chung <sungjinn.chung@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 993bce527b85..45ad6cf678dd 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -41,11 +41,7 @@
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 128MB of the kernel text.
  */
-#ifdef CONFIG_ARM64_64K_PAGES
-#define VA_BITS			(42)
-#else
-#define VA_BITS			(39)
-#endif
+#define VA_BITS			(CONFIG_ARM64_VA_BITS)
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
 #define MODULES_END		(PAGE_OFFSET)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)

commit fa2ec3ea10bd377f9d55772b1dab65178425a1a2
Author: Colin Cross <ccross@android.com>
Date:   Wed Jun 18 21:10:09 2014 +0100

    arm64: implement TASK_SIZE_OF
    
    include/linux/sched.h implements TASK_SIZE_OF as TASK_SIZE if it
    is not set by the architecture headers.  TASK_SIZE uses the
    current task to determine the size of the virtual address space.
    On a 64-bit kernel this will cause reading /proc/pid/pagemap of a
    64-bit process from a 32-bit process to return EOF when it reads
    past 0xffffffff.
    
    Implement TASK_SIZE_OF exactly the same as TASK_SIZE with
    test_tsk_thread_flag instead of test_thread_flag.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Colin Cross <ccross@android.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 993bce527b85..902eb708804a 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -56,6 +56,8 @@
 #define TASK_SIZE_32		UL(0x100000000)
 #define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
 				TASK_SIZE_32 : TASK_SIZE_64)
+#define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_32BIT) ? \
+				TASK_SIZE_32 : TASK_SIZE_64)
 #else
 #define TASK_SIZE		TASK_SIZE_64
 #endif /* CONFIG_COMPAT */

commit 619b5891903936f3e493bf8cda18a8b6664fcdd7
Merge: 19726630c69e 1f53ba6e8174
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 13 11:21:01 2014 +0900

    Merge tag 'stable/for-linus-3.15-rc5-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen bug fixes from David Vrabel:
     - Fix arm64 crash on boot.
     - Quiet a noisy arm build warning (virt_to_pfn() redefined).
    
    * tag 'stable/for-linus-3.15-rc5-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      arm64: introduce virt_to_pfn
      xen/events/fifo: correctly align bitops
      arm/xen: Remove definiition of virt_to_pfn in asm/xen/page.h

commit 1f53ba6e81749a420226e5502c49ab83ba85c81d
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu May 8 15:48:13 2014 +0000

    arm64: introduce virt_to_pfn
    
    virt_to_pfn has been defined in arch/arm/include/asm/memory.h by commit
    e26a9e0 "ARM: Better virt_to_page() handling" and Xen has come to rely
    on it.  Introduce virt_to_pfn on arm64 too.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 9dc5dc39fded..d79a79e7dd04 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -138,6 +138,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __pa(x)			__virt_to_phys((unsigned long)(x))
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+#define virt_to_pfn(x)      __phys_to_pfn(__virt_to_phys(x))
 
 /*
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *

commit bf4b558eba920a38f91beb5ee62a8ce2628c92f7
Author: Mark Salter <msalter@redhat.com>
Date:   Mon Apr 7 15:39:52 2014 -0700

    arm64: add early_ioremap support
    
    Add support for early IO or memory mappings which are needed before the
    normal ioremap() is usable.  This also adds fixmap support for permanent
    fixed mappings such as that used by the earlyprintk device register
    region.
    
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 9dc5dc39fded..e94f9458aa6f 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -49,7 +49,7 @@
 #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
 #define MODULES_END		(PAGE_OFFSET)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)
-#define EARLYCON_IOBASE		(MODULES_VADDR - SZ_4M)
+#define FIXADDR_TOP		(MODULES_VADDR - SZ_2M - PAGE_SIZE)
 #define TASK_SIZE_64		(UL(1) << VA_BITS)
 
 #ifdef CONFIG_COMPAT

commit e26db3f3d9c9592256c1f8c4b5c0ed0a7664c561
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Dec 11 01:23:02 2013 +0000

    arm64: Correct virt_addr_valid
    
    The definition of virt_addr_valid is that virt_addr_valid should
    return true if and only if virt_to_page returns a valid pointer.
    The current definition of virt_addr_valid only checks against the
    virtual address range. There's no guarantee that just because a
    virtual address falls bewteen PAGE_OFFSET and high_memory the
    associated physical memory has a valid backing struct page. Follow
    the example of other architectures and convert to pfn_valid to
    verify that the virtual address is actually valid.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 37762175896f..9dc5dc39fded 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -146,8 +146,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define	virt_addr_valid(kaddr)	(((void *)(kaddr) >= (void *)PAGE_OFFSET) && \
-				 ((void *)(kaddr) < (void *)high_memory))
+#define	virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
 
 #endif
 

commit 847264fb7e73ade5b5e4b6eea3daa243a1f5217e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Oct 23 16:50:07 2013 +0100

    arm64: Use 42-bit address space with 64K pages
    
    This patch expands the VA_BITS to 42 when the 64K page configuration is
    enabled allowing 2TB kernel linear mapping. Linux still uses 2 levels of
    page tables in this configuration with pgd now being a full page.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 20925bcf4e2a..37762175896f 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -33,18 +33,23 @@
 #define UL(x) _AC(x, UL)
 
 /*
- * PAGE_OFFSET - the virtual address of the start of the kernel image.
+ * PAGE_OFFSET - the virtual address of the start of the kernel image (top
+ *		 (VA_BITS - 1))
  * VA_BITS - the maximum number of bits for virtual addresses.
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 128MB of the kernel text.
  */
-#define PAGE_OFFSET		UL(0xffffffc000000000)
+#ifdef CONFIG_ARM64_64K_PAGES
+#define VA_BITS			(42)
+#else
+#define VA_BITS			(39)
+#endif
+#define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
 #define MODULES_END		(PAGE_OFFSET)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)
 #define EARLYCON_IOBASE		(MODULES_VADDR - SZ_4M)
-#define VA_BITS			(39)
 #define TASK_SIZE_64		(UL(1) << VA_BITS)
 
 #ifdef CONFIG_COMPAT

commit 363116073a26dbc2903d8417047597eebcc05273
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Dec 7 18:35:41 2012 +0000

    arm64: KVM: define HYP and Stage-2 translation page flags
    
    Add HYP and S2 page flags, for both normal and device memory.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 381f556b664e..20925bcf4e2a 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -90,6 +90,12 @@
 #define MT_NORMAL_NC		3
 #define MT_NORMAL		4
 
+/*
+ * Memory types for Stage-2 translation
+ */
+#define MT_S2_NORMAL		0xf
+#define MT_S2_DEVICE_nGnRE	0x1
+
 #ifndef __ASSEMBLY__
 
 extern phys_addr_t		memstart_addr;

commit 2475ff9d2c6ea3bbfed55c4635426c371f9ad327
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Oct 23 14:55:08 2012 +0100

    arm64: Add simple earlyprintk support
    
    This patch adds support for "earlyprintk=" parameter on the kernel
    command line. The format is:
    
      earlyprintk=<name>[,<addr>][,<options>]
    
    where <name> is the name of the (UART) device, e.g. "pl011", <addr> is
    the I/O address. The <options> aren't currently used.
    
    The mapping of the earlyprintk device is done very early during kernel
    boot and there are restrictions on which functions it can call. A
    special early_io_map() function is added which creates the mapping from
    the pre-defined EARLY_IOBASE to the device I/O address passed via the
    kernel parameter. The pgd entry corresponding to EARLY_IOBASE is
    pre-populated in head.S during kernel boot.
    
    Only PL011 is currently supported and it is assumed that the interface
    is already initialised by the boot loader before the kernel is started.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
index 1cac16a001cb..381f556b664e 100644
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@ -43,6 +43,7 @@
 #define PAGE_OFFSET		UL(0xffffffc000000000)
 #define MODULES_END		(PAGE_OFFSET)
 #define MODULES_VADDR		(MODULES_END - SZ_64M)
+#define EARLYCON_IOBASE		(MODULES_VADDR - SZ_4M)
 #define VA_BITS			(39)
 #define TASK_SIZE_64		(UL(1) << VA_BITS)
 

commit 4f04d8f00545110a0e525ae2fb62ab38cb417236
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: MMU definitions
    
    The virtual memory layout is described in
    Documentation/arm64/memory.txt. This patch adds the MMU definitions for
    the 4KB and 64KB translation table configurations. The SECTION_SIZE is
    2MB with 4KB page and 512MB with 64KB page configuration.
    
    PHYS_OFFSET is calculated at run-time and stored in a variable (no
    run-time code patching at this stage).
    
    On the current implementation, both user and kernel address spaces are
    512G (39-bit) each with a maximum of 256G for the RAM linear mapping.
    Linux uses 3 levels of translation tables with the 4K page configuration
    and 2 levels with the 64K configuration. Extending the memory space
    beyond 39-bit with the 4K pages or 42-bit with 64K pages requires an
    additional level of translation tables.
    
    The SPARSEMEM configuration is global to all AArch64 platforms and
    allows for 1GB sections with SPARSEMEM_VMEMMAP enabled by default.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
new file mode 100644
index 000000000000..1cac16a001cb
--- /dev/null
+++ b/arch/arm64/include/asm/memory.h
@@ -0,0 +1,144 @@
+/*
+ * Based on arch/arm/include/asm/memory.h
+ *
+ * Copyright (C) 2000-2002 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ *
+ * Note: this file should not be included by non-asm/.h files
+ */
+#ifndef __ASM_MEMORY_H
+#define __ASM_MEMORY_H
+
+#include <linux/compiler.h>
+#include <linux/const.h>
+#include <linux/types.h>
+#include <asm/sizes.h>
+
+/*
+ * Allow for constants defined here to be used from assembly code
+ * by prepending the UL suffix only with actual C code compilation.
+ */
+#define UL(x) _AC(x, UL)
+
+/*
+ * PAGE_OFFSET - the virtual address of the start of the kernel image.
+ * VA_BITS - the maximum number of bits for virtual addresses.
+ * TASK_SIZE - the maximum size of a user space task.
+ * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
+ * The module space lives between the addresses given by TASK_SIZE
+ * and PAGE_OFFSET - it must be within 128MB of the kernel text.
+ */
+#define PAGE_OFFSET		UL(0xffffffc000000000)
+#define MODULES_END		(PAGE_OFFSET)
+#define MODULES_VADDR		(MODULES_END - SZ_64M)
+#define VA_BITS			(39)
+#define TASK_SIZE_64		(UL(1) << VA_BITS)
+
+#ifdef CONFIG_COMPAT
+#define TASK_SIZE_32		UL(0x100000000)
+#define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
+				TASK_SIZE_32 : TASK_SIZE_64)
+#else
+#define TASK_SIZE		TASK_SIZE_64
+#endif /* CONFIG_COMPAT */
+
+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
+
+#if TASK_SIZE_64 > MODULES_VADDR
+#error Top of 64-bit user space clashes with start of module space
+#endif
+
+/*
+ * Physical vs virtual RAM address space conversion.  These are
+ * private definitions which should NOT be used outside memory.h
+ * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
+ */
+#define __virt_to_phys(x)	(((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))
+#define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
+
+/*
+ * Convert a physical address to a Page Frame Number and back
+ */
+#define	__phys_to_pfn(paddr)	((unsigned long)((paddr) >> PAGE_SHIFT))
+#define	__pfn_to_phys(pfn)	((phys_addr_t)(pfn) << PAGE_SHIFT)
+
+/*
+ * Convert a page to/from a physical address
+ */
+#define page_to_phys(page)	(__pfn_to_phys(page_to_pfn(page)))
+#define phys_to_page(phys)	(pfn_to_page(__phys_to_pfn(phys)))
+
+/*
+ * Memory types available.
+ */
+#define MT_DEVICE_nGnRnE	0
+#define MT_DEVICE_nGnRE		1
+#define MT_DEVICE_GRE		2
+#define MT_NORMAL_NC		3
+#define MT_NORMAL		4
+
+#ifndef __ASSEMBLY__
+
+extern phys_addr_t		memstart_addr;
+/* PHYS_OFFSET - the physical address of the start of memory. */
+#define PHYS_OFFSET		({ memstart_addr; })
+
+/*
+ * PFNs are used to describe any physical page; this means
+ * PFN 0 == physical address 0.
+ *
+ * This is the PFN of the first RAM page in the kernel
+ * direct-mapped view.  We assume this is the first page
+ * of RAM in the mem_map as well.
+ */
+#define PHYS_PFN_OFFSET	(PHYS_OFFSET >> PAGE_SHIFT)
+
+/*
+ * Note: Drivers should NOT use these.  They are the wrong
+ * translation for translating DMA addresses.  Use the driver
+ * DMA support - see dma-mapping.h.
+ */
+static inline phys_addr_t virt_to_phys(const volatile void *x)
+{
+	return __virt_to_phys((unsigned long)(x));
+}
+
+static inline void *phys_to_virt(phys_addr_t x)
+{
+	return (void *)(__phys_to_virt(x));
+}
+
+/*
+ * Drivers should NOT use these either.
+ */
+#define __pa(x)			__virt_to_phys((unsigned long)(x))
+#define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
+#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+
+/*
+ *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
+ *  virt_addr_valid(k)	indicates whether a virtual address is valid
+ */
+#define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
+
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define	virt_addr_valid(kaddr)	(((void *)(kaddr) >= (void *)PAGE_OFFSET) && \
+				 ((void *)(kaddr) < (void *)high_memory))
+
+#endif
+
+#include <asm-generic/memory_model.h>
+
+#endif
