commit 25b92693a1b67a47b0c64a3410009d09e9658412
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Feb 13 12:14:52 2020 +0000

    arm64: mm: convert cpu_do_switch_mm() to C
    
    There's no reason that cpu_do_switch_mm() needs to be written as an
    assembly function, and having it as a C function would make it easier to
    maintain.
    
    This patch converts cpu_do_switch_mm() to C, removing code that this
    change makes redundant (e.g. the mmid macro). Since the header comment
    was stale and the prototype now implies all the necessary information,
    this comment is removed. The 'pgd_phys' argument is made a phys_addr_t
    to match the return type of virt_to_phys().
    
    At the same time, post_ttbr_update_workaround() is updated to use
    IS_ENABLED(), which allows the compiler to figure out it can elide calls
    for !CONFIG_CAVIUM_ERRATUM_27456 builds.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will@kernel.org>
    [catalin.marinas@arm.com: change comments from asm-style to C-style]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index a2ce65a0c1fa..0d5d1f0525eb 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -13,11 +13,9 @@
 
 #include <asm/page.h>
 
-struct mm_struct;
 struct cpu_suspend_ctx;
 
 extern void cpu_do_idle(void);
-extern void cpu_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
 extern void cpu_do_suspend(struct cpu_suspend_ctx *ptr);
 extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 

commit b907b80d7ae7b2b65ef9f534f3e9a32ce6a4b539
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jul 8 17:36:40 2019 +0100

    arm64: remove pointless __KERNEL__ guards
    
    For a number of years, UAPI headers have been split from kernel-internal
    headers. The latter are never exposed to userspace, and always built
    with __KERNEL__ defined.
    
    Most headers under arch/arm64 don't have __KERNEL__ guards, but there
    are a few stragglers lying around. To make things more consistent, and
    to set a good example going forward, let's remove these redundant
    __KERNEL__ guards.
    
    In a couple of cases, a trailing #endif lacked a comment describing its
    corresponding #if or #ifdef, so these are fixes up at the same time.
    
    Guards in auto-generated crypto code are left as-is, as these guards are
    generated by scripting imported from the upstream openssl project
    scripts. Guards in UAPI headers are left as-is, as these can be included
    by userspace or the kernel.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 368d90a9d0e5..a2ce65a0c1fa 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -9,7 +9,6 @@
 #ifndef __ASM_PROCFNS_H
 #define __ASM_PROCFNS_H
 
-#ifdef __KERNEL__
 #ifndef __ASSEMBLY__
 
 #include <asm/page.h>
@@ -25,5 +24,4 @@ extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 #include <asm/memory.h>
 
 #endif /* __ASSEMBLY__ */
-#endif /* __KERNEL__ */
 #endif /* __ASM_PROCFNS_H */

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 16cef2e8449e..368d90a9d0e5 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -1,21 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Based on arch/arm/include/asm/proc-fns.h
  *
  * Copyright (C) 1997-1999 Russell King
  * Copyright (C) 2000 Deep Blue Solutions Ltd
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_PROCFNS_H
 #define __ASM_PROCFNS_H

commit 7655abb953860485940d4de74fb45a8192149bb6
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 10 13:19:09 2017 +0100

    arm64: mm: Move ASID from TTBR0 to TTBR1
    
    In preparation for mapping kernelspace and userspace with different
    ASIDs, move the ASID to TTBR1 and update switch_mm to context-switch
    TTBR0 via an invalid mapping (the zero page).
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 14ad6e4e87d1..16cef2e8449e 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -35,12 +35,6 @@ extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 
 #include <asm/memory.h>
 
-#define cpu_switch_mm(pgd,mm)				\
-do {							\
-	BUG_ON(pgd == swapper_pg_dir);			\
-	cpu_do_switch_mm(virt_to_phys(pgd),mm);		\
-} while (0)
-
 #endif /* __ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* __ASM_PROCFNS_H */

commit 68234df4ea7939f98431aa81113fbdce10c4a84b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Apr 20 10:24:35 2015 +0100

    arm64: kill flush_cache_all()
    
    The documented semantics of flush_cache_all are not possible to provide
    for arm64 (short of flushing the entire physical address space by VA),
    and there are currently no users; KVM uses VA maintenance exclusively,
    cpu_reset is never called, and the only two users outside of arch code
    cannot be built for arm64.
    
    While cpu_soft_reset and related functions (which call flush_cache_all)
    were thought to be useful for kexec, their current implementations only
    serve to mask bugs. For correctness kexec will need to perform
    maintenance by VA anyway to account for system caches, line migration,
    and other subtleties of the cache architecture. As the extent of this
    cache maintenance will be kexec-specific, it should probably live in the
    kexec code.
    
    This patch removes flush_cache_all, and related unused components,
    preventing further abuse.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 220633b791b8..14ad6e4e87d1 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -28,12 +28,8 @@
 struct mm_struct;
 struct cpu_suspend_ctx;
 
-extern void cpu_cache_off(void);
 extern void cpu_do_idle(void);
 extern void cpu_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
-extern void cpu_reset(unsigned long addr) __attribute__((noreturn));
-void cpu_soft_restart(phys_addr_t cpu_reset,
-		unsigned long addr) __attribute__((noreturn));
 extern void cpu_do_suspend(struct cpu_suspend_ctx *ptr);
 extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 

commit 714d8e7e27197dd39b2550e762a6a6fcf397a471
Merge: d19d5efd8c88 6d1966dfd6e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 16 13:58:29 2015 -0500

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Here are the core arm64 updates for 4.1.
    
      Highlights include a significant rework to head.S (allowing us to boot
      on machines with physical memory at a really high address), an AES
      performance boost on Cortex-A57 and the ability to run a 32-bit
      userspace with 64k pages (although this requires said userspace to be
      built with a recent binutils).
    
      The head.S rework spilt over into KVM, so there are some changes under
      arch/arm/ which have been acked by Marc Zyngier (KVM co-maintainer).
      In particular, the linker script changes caused us some issues in
      -next, so there are a few merge commits where we had to apply fixes on
      top of a stable branch.
    
      Other changes include:
    
       - AES performance boost for Cortex-A57
       - AArch32 (compat) userspace with 64k pages
       - Cortex-A53 erratum workaround for #845719
       - defconfig updates (new platforms, PCI, ...)"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (39 commits)
      arm64: fix midr range for Cortex-A57 erratum 832075
      arm64: errata: add workaround for cortex-a53 erratum #845719
      arm64: Use bool function return values of true/false not 1/0
      arm64: defconfig: updates for 4.1
      arm64: Extract feature parsing code from cpu_errata.c
      arm64: alternative: Allow immediate branch as alternative instruction
      arm64: insn: Add aarch64_insn_decode_immediate
      ARM: kvm: round HYP section to page size instead of log2 upper bound
      ARM: kvm: assert on HYP section boundaries not actual code size
      arm64: head.S: ensure idmap_t0sz is visible
      arm64: pmu: add support for interrupt-affinity property
      dt: pmu: extend ARM PMU binding to allow for explicit interrupt affinity
      arm64: head.S: ensure visibility of page tables
      arm64: KVM: use ID map with increased VA range if required
      arm64: mm: increase VA range of identity map
      ARM: kvm: implement replacement for ld's LOG2CEIL()
      arm64: proc: remove unused cpu_get_pgd macro
      arm64: enforce x1|x2|x3 == 0 upon kernel entry as per boot protocol
      arm64: remove __calc_phys_offset
      arm64: merge __enable_mmu and __turn_mmu_on
      ...

commit 130c93fd10c4d150e39d8879420c1351aa207fa9
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Mar 19 15:43:00 2015 +0000

    arm64: efi: don't restore TTBR0 if active_mm points at init_mm
    
    init_mm isn't a normal mm: it has swapper_pg_dir as its pgd (which
    contains kernel mappings) and is used as the active_mm for the idle
    thread.
    
    When restoring the pgd after an EFI call, we write current->active_mm
    into TTBR0. If the current task is actually the idle thread (e.g. when
    initialising the EFI RTC before entering userspace), then the TLB can
    erroneously populate itself with junk global entries as a result of
    speculative table walks.
    
    When we do eventually return to userspace, the task can end up hitting
    these junk mappings leading to lockups, corruption or crashes.
    
    This patch fixes the problem in the same way as the CPU suspend code by
    ensuring that we never switch to the init_mm in efi_set_pgd and instead
    point TTBR0 at the zero page. A check is also added to cpu_switch_mm to
    BUG if we get passed swapper_pg_dir.
    
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Fixes: f3cdfd239da5 ("arm64/efi: move SetVirtualAddressMap() to UEFI stub")
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 9a8fd84f8fb2..941c375616e2 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -39,7 +39,11 @@ extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 
 #include <asm/memory.h>
 
-#define cpu_switch_mm(pgd,mm) cpu_do_switch_mm(virt_to_phys(pgd),mm)
+#define cpu_switch_mm(pgd,mm)				\
+do {							\
+	BUG_ON(pgd == swapper_pg_dir);			\
+	cpu_do_switch_mm(virt_to_phys(pgd),mm);		\
+} while (0)
 
 #define cpu_get_pgd()					\
 ({							\

commit ce47fbb7c8956742a6de06b9706b1c6236339f51
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Mar 19 19:19:40 2015 +0000

    arm64: proc: remove unused cpu_get_pgd macro
    
    cpu_get_pgd isn't used anywhere and is Probably Not What You Want.
    Remove it before anybody decides to use it.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 9a8fd84f8fb2..4d9ede7b6361 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -41,15 +41,6 @@ extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 
 #define cpu_switch_mm(pgd,mm) cpu_do_switch_mm(virt_to_phys(pgd),mm)
 
-#define cpu_get_pgd()					\
-({							\
-	unsigned long pg;				\
-	asm("mrs	%0, ttbr0_el1\n"		\
-	    : "=r" (pg));				\
-	pg &= ~0xffff000000003ffful;			\
-	(pgd_t *)phys_to_virt(pg);			\
-})
-
 #endif /* __ASSEMBLY__ */
 #endif /* __KERNEL__ */
 #endif /* __ASM_PROCFNS_H */

commit 5e051531447259e5df95c44bccb69979537c19e4
Author: Arun Chandran <achandran@mvista.com>
Date:   Mon Aug 18 10:06:58 2014 +0100

    arm64: convert part of soft_restart() to assembly
    
    The current soft_restart() and setup_restart implementations incorrectly
    assume that compiler will not spill/fill values to/from stack. However
    this assumption seems to be wrong, revealed by the disassembly of the
    currently existing code (v3.16) built with Linaro GCC 4.9-2014.05.
    
    ffffffc000085224 <soft_restart>:
    ffffffc000085224:  a9be7bfd  stp    x29, x30, [sp,#-32]!
    ffffffc000085228:  910003fd  mov    x29, sp
    ffffffc00008522c:  f9000fa0  str    x0, [x29,#24]
    ffffffc000085230:  94003d21  bl     ffffffc0000946b4 <setup_mm_for_reboot>
    ffffffc000085234:  94003b33  bl     ffffffc000093f00 <flush_cache_all>
    ffffffc000085238:  94003dfa  bl     ffffffc000094a20 <cpu_cache_off>
    ffffffc00008523c:  94003b31  bl     ffffffc000093f00 <flush_cache_all>
    ffffffc000085240:  b0003321  adrp   x1, ffffffc0006ea000 <reset_devices>
    
    ffffffc000085244:  f9400fa0  ldr    x0, [x29,#24] ----> spilled addr
    ffffffc000085248:  f942fc22  ldr    x2, [x1,#1528] ----> global memstart_addr
    
    ffffffc00008524c:  f0000061  adrp   x1, ffffffc000094000 <__inval_cache_range+0x40>
    ffffffc000085250:  91290021  add    x1, x1, #0xa40
    ffffffc000085254:  8b010041  add    x1, x2, x1
    ffffffc000085258:  d2c00802  mov    x2, #0x4000000000           // #274877906944
    ffffffc00008525c:  8b020021  add    x1, x1, x2
    ffffffc000085260:  d63f0020  blr    x1
    ...
    
    Here the compiler generates memory accesses after the cache is disabled,
    loading stale values for the spilled value and global variable. As we cannot
    control when the compiler will access memory we must rewrite the
    functions in assembly to stash values we need in registers prior to
    disabling the cache, avoiding the use of memory.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Arun Chandran <achandran@mvista.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 0c657bb54597..9a8fd84f8fb2 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -32,6 +32,8 @@ extern void cpu_cache_off(void);
 extern void cpu_do_idle(void);
 extern void cpu_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
 extern void cpu_reset(unsigned long addr) __attribute__((noreturn));
+void cpu_soft_restart(phys_addr_t cpu_reset,
+		unsigned long addr) __attribute__((noreturn));
 extern void cpu_do_suspend(struct cpu_suspend_ctx *ptr);
 extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 

commit 6732bc65c277b697f6d8b645b15f63d1558c0cc4
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed Jul 17 10:14:45 2013 +0100

    arm64: kernel: suspend/resume registers save/restore
    
    Power management software requires the kernel to save and restore
    CPU registers while going through suspend and resume operations
    triggered by kernel subsystems like CPU idle and suspend to RAM.
    
    This patch implements code that provides save and restore mechanism
    for the arm v8 implementation. Memory for the context is passed as
    parameter to both cpu_do_suspend and cpu_do_resume functions, and allows
    the callers to implement context allocation as they deem fit.
    
    The registers that are saved and restored correspond to the registers set
    actually required by the kernel to be up and running which represents a
    subset of v8 ISA.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
index 7cdf466fd0c5..0c657bb54597 100644
--- a/arch/arm64/include/asm/proc-fns.h
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -26,11 +26,14 @@
 #include <asm/page.h>
 
 struct mm_struct;
+struct cpu_suspend_ctx;
 
 extern void cpu_cache_off(void);
 extern void cpu_do_idle(void);
 extern void cpu_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
 extern void cpu_reset(unsigned long addr) __attribute__((noreturn));
+extern void cpu_do_suspend(struct cpu_suspend_ctx *ptr);
+extern u64 cpu_do_resume(phys_addr_t ptr, u64 idmap_ttbr);
 
 #include <asm/memory.h>
 

commit 9cce7a435f89c9e60f244d44da2cf1cf4ed094ac
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: CPU support
    
    This patch adds AArch64 CPU specific functionality. It assumes that the
    implementation is generic to AArch64 and does not require specific
    identification. Different CPU implementations may require the setting of
    various ACTLR_EL1 bits but such information is not currently available
    and it should ideally be pushed to firmware.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm64/include/asm/proc-fns.h b/arch/arm64/include/asm/proc-fns.h
new file mode 100644
index 000000000000..7cdf466fd0c5
--- /dev/null
+++ b/arch/arm64/include/asm/proc-fns.h
@@ -0,0 +1,50 @@
+/*
+ * Based on arch/arm/include/asm/proc-fns.h
+ *
+ * Copyright (C) 1997-1999 Russell King
+ * Copyright (C) 2000 Deep Blue Solutions Ltd
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_PROCFNS_H
+#define __ASM_PROCFNS_H
+
+#ifdef __KERNEL__
+#ifndef __ASSEMBLY__
+
+#include <asm/page.h>
+
+struct mm_struct;
+
+extern void cpu_cache_off(void);
+extern void cpu_do_idle(void);
+extern void cpu_do_switch_mm(unsigned long pgd_phys, struct mm_struct *mm);
+extern void cpu_reset(unsigned long addr) __attribute__((noreturn));
+
+#include <asm/memory.h>
+
+#define cpu_switch_mm(pgd,mm) cpu_do_switch_mm(virt_to_phys(pgd),mm)
+
+#define cpu_get_pgd()					\
+({							\
+	unsigned long pg;				\
+	asm("mrs	%0, ttbr0_el1\n"		\
+	    : "=r" (pg));				\
+	pg &= ~0xffff000000003ffful;			\
+	(pgd_t *)phys_to_virt(pg);			\
+})
+
+#endif /* __ASSEMBLY__ */
+#endif /* __KERNEL__ */
+#endif /* __ASM_PROCFNS_H */
