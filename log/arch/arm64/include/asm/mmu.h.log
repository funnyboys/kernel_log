commit a39060b009ca0b5b5fe0c0dab85ed437531aab52
Author: Will Deacon <will@kernel.org>
Date:   Mon Jun 22 12:35:41 2020 +0100

    arm64: compat: Allow 32-bit vdso and sigpage to co-exist
    
    In preparation for removing the signal trampoline from the compat vDSO,
    allow the sigpage and the compat vDSO to co-exist.
    
    For the moment the vDSO signal trampoline will still be used when built.
    Subsequent patches will move to the sigpage consistently.
    
    Acked-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 68140fdd89d6..8444df000181 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -19,6 +19,9 @@
 
 typedef struct {
 	atomic64_t	id;
+#ifdef CONFIG_COMPAT
+	void		*sigpage;
+#endif
 	void		*vdso;
 	unsigned long	flags;
 } mm_context_t;

commit 3cd86a58f7734bf9cef38f6f899608ebcaa3da13
Merge: a8222fd5b80c b2a84de2a2de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 10:05:01 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The bulk is in-kernel pointer authentication, activity monitors and
      lots of asm symbol annotations. I also queued the sys_mremap() patch
      commenting the asymmetry in the address untagging.
    
      Summary:
    
       - In-kernel Pointer Authentication support (previously only offered
         to user space).
    
       - ARM Activity Monitors (AMU) extension support allowing better CPU
         utilisation numbers for the scheduler (frequency invariance).
    
       - Memory hot-remove support for arm64.
    
       - Lots of asm annotations (SYM_*) in preparation for the in-kernel
         Branch Target Identification (BTI) support.
    
       - arm64 perf updates: ARMv8.5-PMU 64-bit counters, refactoring the
         PMU init callbacks, support for new DT compatibles.
    
       - IPv6 header checksum optimisation.
    
       - Fixes: SDEI (software delegated exception interface) double-lock on
         hibernate with shared events.
    
       - Minor clean-ups and refactoring: cpu_ops accessor,
         cpu_do_switch_mm() converted to C, cpufeature finalisation helper.
    
       - sys_mremap() comment explaining the asymmetric address untagging
         behaviour"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (81 commits)
      mm/mremap: Add comment explaining the untagging behaviour of mremap()
      arm64: head: Convert install_el2_stub to SYM_INNER_LABEL
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
      arm64: move kimage_vaddr to .rodata
      arm64: use mov_q instead of literal ldr
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      ...

commit 0829a076958ddd203cf4824dd330c93ba4662815
Merge: da12d2739fb6 d4abd29d6775
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:10:46 2020 +0000

    Merge branch 'for-next/asm-annotations' into for-next/core
    
    * for-next/asm-annotations:
      : Modernise arm64 assembly annotations
      arm64: head: Convert install_el2_stub to SYM_INNER_LABEL
      arm64: Mark call_smc_arch_workaround_1 as __maybe_unused
      arm64: entry-ftrace.S: Fix missing argument for CONFIG_FUNCTION_GRAPH_TRACER=y
      arm64: vdso32: Convert to modern assembler annotations
      arm64: vdso: Convert to modern assembler annotations
      arm64: sdei: Annotate SDEI entry points using new style annotations
      arm64: kvm: Modernize __smccc_workaround_1_smc_start annotations
      arm64: kvm: Modernize annotation for __bp_harden_hyp_vecs
      arm64: kvm: Annotate assembly using modern annoations
      arm64: kernel: Convert to modern annotations for assembly data
      arm64: head: Annotate stext and preserve_boot_args as code
      arm64: head.S: Convert to modern annotations for assembly functions
      arm64: ftrace: Modernise annotation of return_to_handler
      arm64: ftrace: Correct annotation of ftrace_caller assembly
      arm64: entry-ftrace.S: Convert to modern annotations for assembly functions
      arm64: entry: Additional annotation conversions for entry.S
      arm64: entry: Annotate ret_from_fork as code
      arm64: entry: Annotate vector table and handlers as code
      arm64: crypto: Modernize names for AES function macros
      arm64: crypto: Modernize some extra assembly annotations

commit c83557859eaa1286330a4d3d2e1ea0c0988c4604
Author: Will Deacon <will@kernel.org>
Date:   Wed Mar 18 20:38:29 2020 +0000

    arm64: kpti: Fix "kpti=off" when KASLR is enabled
    
    Enabling KASLR forces the use of non-global page-table entries for kernel
    mappings, as this is a decision that we have to make very early on before
    mapping the kernel proper. When used in conjunction with the "kpti=off"
    command-line option, it is possible to use non-global kernel mappings but
    with the kpti trampoline disabled.
    
    Since commit 09e3c22a86f6 ("arm64: Use a variable to store non-global
    mappings decision"), arm64_kernel_unmapped_at_el0() reflects only the use of
    non-global mappings and does not take into account whether the kpti
    trampoline is enabled. This breaks context switching of the TPIDRRO_EL0
    register for 64-bit tasks, where the clearing of the register is deferred to
    the ret-to-user code, but it also breaks the ARM SPE PMU driver which
    helpfully recommends passing "kpti=off" on the command line!
    
    Report whether or not KPTI is actually enabled in
    arm64_kernel_unmapped_at_el0() and check the 'arm64_use_ng_mappings' global
    variable directly when determining the protection flags for kernel mappings.
    
    Cc: Mark Brown <broonie@kernel.org>
    Reported-by: Hongbo Yao <yaohongbo@huawei.com>
    Tested-by: Hongbo Yao <yaohongbo@huawei.com>
    Fixes: 09e3c22a86f6 ("arm64: Use a variable to store non-global mappings decision")
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index e4d862420bb4..d79ce6df9e12 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -29,11 +29,9 @@ typedef struct {
  */
 #define ASID(mm)	((mm)->context.id.counter & 0xffff)
 
-extern bool arm64_use_ng_mappings;
-
 static inline bool arm64_kernel_unmapped_at_el0(void)
 {
-	return arm64_use_ng_mappings;
+	return cpus_have_const_cap(ARM64_UNMAP_KERNEL_AT_EL0);
 }
 
 typedef void (*bp_hardening_cb_t)(void);

commit 6e52aab9015277c39c6f03a76a7e9487370013f8
Author: Mark Brown <broonie@kernel.org>
Date:   Tue Feb 18 19:58:38 2020 +0000

    arm64: kvm: Modernize annotation for __bp_harden_hyp_vecs
    
    We have recently introduced new macros for annotating assembly symbols
    for things that aren't C functions, SYM_CODE_START() and SYM_CODE_END(),
    in an effort to clarify and simplify our annotations of assembly files.
    
    Using these for __bp_harden_hyp_vecs is more involved than for most symbols
    as this symbol is annotated quite unusually as rather than just have the
    explicit symbol we define _start and _end symbols which we then use to
    compute the length. This does not play at all nicely with the new style
    macros. Since the size of the vectors is a known constant which won't vary
    the simplest thing to do is simply to drop the separate _start and _end
    symbols and just use a #define for the size.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index e4d862420bb4..a3324d6ccbfe 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -13,6 +13,7 @@
 #define TTBR_ASID_MASK	(UL(0xffff) << 48)
 
 #define BP_HARDEN_EL2_SLOTS 4
+#define __BP_HARDEN_HYP_VECS_SZ (BP_HARDEN_EL2_SLOTS * SZ_2K)
 
 #ifndef __ASSEMBLY__
 
@@ -45,7 +46,8 @@ struct bp_hardening_data {
 
 #if (defined(CONFIG_HARDEN_BRANCH_PREDICTOR) ||	\
      defined(CONFIG_HARDEN_EL2_VECTORS))
-extern char __bp_harden_hyp_vecs_start[], __bp_harden_hyp_vecs_end[];
+
+extern char __bp_harden_hyp_vecs[];
 extern atomic_t arm64_el2_vector_last_slot;
 #endif  /* CONFIG_HARDEN_BRANCH_PREDICTOR || CONFIG_HARDEN_EL2_VECTORS */
 

commit 90765f745b08fdf069e4562f6985bdba0fefad8d
Author: Will Deacon <will@kernel.org>
Date:   Fri Feb 28 12:43:55 2020 +0000

    arm64: Update comment for ASID() macro
    
    Commit 25b92693a1b6 ("arm64: mm: convert cpu_do_switch_mm() to C") added
    a new use of the ASID() macro, so update the comment in asm/mmu.h which
    reasons about why an atomic reload of 'mm->context.id.counter' is not
    required.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index e4d862420bb4..21a4bcfdb378 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -23,9 +23,9 @@ typedef struct {
 } mm_context_t;
 
 /*
- * This macro is only used by the TLBI code, which cannot race with an
- * ASID change and therefore doesn't need to reload the counter using
- * atomic64_read.
+ * This macro is only used by the TLBI and low-level switch_mm() code,
+ * neither of which can race with an ASID change. We therefore don't
+ * need to reload the counter using atomic64_read().
  */
 #define ASID(mm)	((mm)->context.id.counter & 0xffff)
 

commit 09e3c22a86f6889db0e93fb29d9255081a126f64
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Dec 9 18:12:17 2019 +0000

    arm64: Use a variable to store non-global mappings decision
    
    Refactor the code which checks to see if we need to use non-global
    mappings to use a variable instead of checking with the CPU capabilities
    each time, doing the initial check for KPTI early in boot before we
    start allocating memory so we still avoid transitioning to non-global
    mappings in common cases.
    
    Since this variable always matches our decision about non-global
    mappings this means we can also combine arm64_kernel_use_ng_mappings()
    and arm64_unmap_kernel_at_el0() into a single function, the variable
    simply stores the result and the decision code is elsewhere. We could
    just have the users check the variable directly but having a function
    makes it clear that these uses are read-only.
    
    The result is that we simplify the code a bit and reduces the amount of
    code executed at runtime.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 1eec3971f0a9..e4d862420bb4 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -29,91 +29,11 @@ typedef struct {
  */
 #define ASID(mm)	((mm)->context.id.counter & 0xffff)
 
-static inline bool arm64_kernel_unmapped_at_el0(void)
-{
-	return IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0) &&
-	       cpus_have_const_cap(ARM64_UNMAP_KERNEL_AT_EL0);
-}
+extern bool arm64_use_ng_mappings;
 
-/*
- * This check is triggered during the early boot before the cpufeature
- * is initialised. Checking the status on the local CPU allows the boot
- * CPU to detect the need for non-global mappings and thus avoiding a
- * pagetable re-write after all the CPUs are booted. This check will be
- * anyway run on individual CPUs, allowing us to get the consistent
- * state once the SMP CPUs are up and thus make the switch to non-global
- * mappings if required.
- */
-static inline bool kaslr_requires_kpti(void)
-{
-	bool tx1_bug;
-	u64 ftr;
-
-	if (!IS_ENABLED(CONFIG_RANDOMIZE_BASE))
-		return false;
-
-	/*
-	 * E0PD does a similar job to KPTI so can be used instead
-	 * where available.
-	 */
-	if (IS_ENABLED(CONFIG_ARM64_E0PD)) {
-		ftr = read_sysreg_s(SYS_ID_AA64MMFR2_EL1);
-		if ((ftr >> ID_AA64MMFR2_E0PD_SHIFT) & 0xf)
-			return false;
-	}
-
-	/*
-	 * Systems affected by Cavium erratum 24756 are incompatible
-	 * with KPTI.
-	 */
-	if (!IS_ENABLED(CONFIG_CAVIUM_ERRATUM_27456)) {
-		tx1_bug = false;
-#ifndef MODULE
-	} else if (!static_branch_likely(&arm64_const_caps_ready)) {
-		extern const struct midr_range cavium_erratum_27456_cpus[];
-
-		tx1_bug = is_midr_in_range_list(read_cpuid_id(),
-						cavium_erratum_27456_cpus);
-#endif
-	} else {
-		tx1_bug = __cpus_have_const_cap(ARM64_WORKAROUND_CAVIUM_27456);
-	}
-	if (tx1_bug)
-		return false;
-
-	return kaslr_offset() > 0;
-}
-
-static inline bool arm64_kernel_use_ng_mappings(void)
+static inline bool arm64_kernel_unmapped_at_el0(void)
 {
-	/* What's a kpti? Use global mappings if we don't know. */
-	if (!IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0))
-		return false;
-
-	/*
-	 * Note: this function is called before the CPU capabilities have
-	 * been configured, so our early mappings will be global. If we
-	 * later determine that kpti is required, then
-	 * kpti_install_ng_mappings() will make them non-global.
-	 */
-	if (arm64_kernel_unmapped_at_el0())
-		return true;
-
-	/*
-	 * Once we are far enough into boot for capabilities to be
-	 * ready we will have confirmed if we are using non-global
-	 * mappings so don't need to consider anything else here.
-	 */
-	if (static_branch_likely(&arm64_const_caps_ready))
-		return false;
-
-	/*
-	 * KASLR is enabled so we're going to be enabling kpti on non-broken
-	 * CPUs regardless of their susceptibility to Meltdown. Rather
-	 * than force everybody to go through the G -> nG dance later on,
-	 * just put down non-global mappings from the beginning
-	 */
-	return kaslr_requires_kpti();
+	return arm64_use_ng_mappings;
 }
 
 typedef void (*bp_hardening_cb_t)(void);
@@ -167,6 +87,7 @@ extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       pgprot_t prot, bool page_mappings_only);
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys, int *size, pgprot_t prot);
 extern void mark_linear_text_alias_ro(void);
+extern bool kaslr_requires_kpti(void);
 
 #define INIT_MM_CONTEXT(name)	\
 	.pgd = init_pg_dir,

commit 92ac6fd162b42628ebe50cc2f08d6a77759e7911
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Dec 9 18:12:16 2019 +0000

    arm64: Don't use KPTI where we have E0PD
    
    Since E0PD is intended to fulfil the same role as KPTI we don't need to
    use KPTI on CPUs where E0PD is available, we can rely on E0PD instead.
    Change the check that forces KPTI on when KASLR is enabled to check for
    E0PD before doing so, CPUs with E0PD are not expected to be affected by
    meltdown so should not need to enable KPTI for other reasons.
    
    Since E0PD is a system capability we will still enable KPTI if any of
    the CPUs in the system lacks E0PD, this will rewrite any global mappings
    that were established in systems where some but not all CPUs support
    E0PD.  We may transiently have a mix of global and non-global mappings
    while booting since we use the local CPU when deciding if KPTI will be
    required prior to completing CPU enumeration but any global mappings
    will be converted to non-global ones when KPTI is applied.
    
    KPTI can still be forced on from the command line if required.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 2a93d34cc0ca..1eec3971f0a9 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -47,10 +47,21 @@ static inline bool arm64_kernel_unmapped_at_el0(void)
 static inline bool kaslr_requires_kpti(void)
 {
 	bool tx1_bug;
+	u64 ftr;
 
 	if (!IS_ENABLED(CONFIG_RANDOMIZE_BASE))
 		return false;
 
+	/*
+	 * E0PD does a similar job to KPTI so can be used instead
+	 * where available.
+	 */
+	if (IS_ENABLED(CONFIG_ARM64_E0PD)) {
+		ftr = read_sysreg_s(SYS_ID_AA64MMFR2_EL1);
+		if ((ftr >> ID_AA64MMFR2_E0PD_SHIFT) & 0xf)
+			return false;
+	}
+
 	/*
 	 * Systems affected by Cavium erratum 24756 are incompatible
 	 * with KPTI.

commit c2d92353b28f8542c6b3150900b38c94b1d61480
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Dec 9 18:12:15 2019 +0000

    arm64: Factor out checks for KASLR in KPTI code into separate function
    
    In preparation for integrating E0PD support with KASLR factor out the
    checks for interaction between KASLR and KPTI done in boot context into
    a new function kaslr_requires_kpti(), in the process clarifying the
    distinction between what we do in boot context and what we do at
    runtime.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index f217e3292919..2a93d34cc0ca 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -35,10 +35,46 @@ static inline bool arm64_kernel_unmapped_at_el0(void)
 	       cpus_have_const_cap(ARM64_UNMAP_KERNEL_AT_EL0);
 }
 
-static inline bool arm64_kernel_use_ng_mappings(void)
+/*
+ * This check is triggered during the early boot before the cpufeature
+ * is initialised. Checking the status on the local CPU allows the boot
+ * CPU to detect the need for non-global mappings and thus avoiding a
+ * pagetable re-write after all the CPUs are booted. This check will be
+ * anyway run on individual CPUs, allowing us to get the consistent
+ * state once the SMP CPUs are up and thus make the switch to non-global
+ * mappings if required.
+ */
+static inline bool kaslr_requires_kpti(void)
 {
 	bool tx1_bug;
 
+	if (!IS_ENABLED(CONFIG_RANDOMIZE_BASE))
+		return false;
+
+	/*
+	 * Systems affected by Cavium erratum 24756 are incompatible
+	 * with KPTI.
+	 */
+	if (!IS_ENABLED(CONFIG_CAVIUM_ERRATUM_27456)) {
+		tx1_bug = false;
+#ifndef MODULE
+	} else if (!static_branch_likely(&arm64_const_caps_ready)) {
+		extern const struct midr_range cavium_erratum_27456_cpus[];
+
+		tx1_bug = is_midr_in_range_list(read_cpuid_id(),
+						cavium_erratum_27456_cpus);
+#endif
+	} else {
+		tx1_bug = __cpus_have_const_cap(ARM64_WORKAROUND_CAVIUM_27456);
+	}
+	if (tx1_bug)
+		return false;
+
+	return kaslr_offset() > 0;
+}
+
+static inline bool arm64_kernel_use_ng_mappings(void)
+{
 	/* What's a kpti? Use global mappings if we don't know. */
 	if (!IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0))
 		return false;
@@ -52,29 +88,21 @@ static inline bool arm64_kernel_use_ng_mappings(void)
 	if (arm64_kernel_unmapped_at_el0())
 		return true;
 
-	if (!IS_ENABLED(CONFIG_RANDOMIZE_BASE))
+	/*
+	 * Once we are far enough into boot for capabilities to be
+	 * ready we will have confirmed if we are using non-global
+	 * mappings so don't need to consider anything else here.
+	 */
+	if (static_branch_likely(&arm64_const_caps_ready))
 		return false;
 
 	/*
 	 * KASLR is enabled so we're going to be enabling kpti on non-broken
 	 * CPUs regardless of their susceptibility to Meltdown. Rather
 	 * than force everybody to go through the G -> nG dance later on,
-	 * just put down non-global mappings from the beginning.
+	 * just put down non-global mappings from the beginning
 	 */
-	if (!IS_ENABLED(CONFIG_CAVIUM_ERRATUM_27456)) {
-		tx1_bug = false;
-#ifndef MODULE
-	} else if (!static_branch_likely(&arm64_const_caps_ready)) {
-		extern const struct midr_range cavium_erratum_27456_cpus[];
-
-		tx1_bug = is_midr_in_range_list(read_cpuid_id(),
-						cavium_erratum_27456_cpus);
-#endif
-	} else {
-		tx1_bug = __cpus_have_const_cap(ARM64_WORKAROUND_CAVIUM_27456);
-	}
-
-	return !tx1_bug && kaslr_offset() > 0;
+	return kaslr_requires_kpti();
 }
 
 typedef void (*bp_hardening_cb_t)(void);

commit e112b032a72c78f15d0c803c5dc6be444c2e6c66
Author: Hsin-Yi Wang <hsinyi@chromium.org>
Date:   Fri Aug 23 14:24:50 2019 +0800

    arm64: map FDT as RW for early_init_dt_scan()
    
    Currently in arm64, FDT is mapped to RO before it's passed to
    early_init_dt_scan(). However, there might be some codes
    (eg. commit "fdt: add support for rng-seed") that need to modify FDT
    during init. Map FDT to RO after early fixups are done.
    
    Signed-off-by: Hsin-Yi Wang <hsinyi@chromium.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index fd6161336653..f217e3292919 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -126,7 +126,7 @@ extern void init_mem_pgprot(void);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       unsigned long virt, phys_addr_t size,
 			       pgprot_t prot, bool page_mappings_only);
-extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
+extern void *fixmap_remap_fdt(phys_addr_t dt_phys, int *size, pgprot_t prot);
 extern void mark_linear_text_alias_ro(void);
 
 #define INIT_MM_CONTEXT(name)	\

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 67ef25d037ea..fd6161336653 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_MMU_H
 #define __ASM_MMU_H

commit 83504032e6ddcc8b0942aa24dfad5db849090c9f
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jan 14 14:22:24 2019 +0000

    arm64: Remove asm/memblock.h
    
    The arm64 asm/memblock.h header exists only to provide a function
    prototype for arm64_memblock_init(), which is called only from
    setup_arch().
    
    Move the declaration into mmu.h, where it can live alongside other
    init functions such as paging_init() and bootmem_init() without the
    need for its own special header file.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 3e8063f4f9d3..67ef25d037ea 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -129,6 +129,7 @@ static inline struct bp_hardening_data *arm64_get_bp_hardening_data(void)
 static inline void arm64_apply_bp_hardening(void)	{ }
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
+extern void arm64_memblock_init(void);
 extern void paging_init(void);
 extern void bootmem_init(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);

commit 2f97967503df8e45bc256a348b6f050abd2a38ed
Author: James Morse <james.morse@arm.com>
Date:   Tue Jan 15 18:49:17 2019 +0000

    arm64: kpti: Update arm64_kernel_use_ng_mappings() when forced on
    
    Since commit b89d82ef01b3 ("arm64: kpti: Avoid rewriting early page
    tables when KASLR is enabled"), a kernel built with CONFIG_RANDOMIZE_BASE
    can decide early whether to use non-global mappings by checking the
    kaslr_offset().
    
    A kernel built without CONFIG_RANDOMIZE_BASE, instead checks the
    cpufeature static-key.
    
    This leaves a gap where CONFIG_RANDOMIZE_BASE was enabled, no
    kaslr seed was provided, but kpti was forced on using the cmdline
    option.
    
    When the decision is made late, kpti_install_ng_mappings() will re-write
    the page tables, but arm64_kernel_use_ng_mappings()'s value does not
    change as it only tests the cpufeature static-key if
    CONFIG_RANDOMIZE_BASE is disabled.
    This function influences PROT_DEFAULT via PTE_MAYBE_NG, and causes
    pgattr_change_is_safe() to catch nG->G transitions when the unchanged
    PROT_DEFAULT is used as part of PAGE_KERNEL_RO:
    [    1.942255] alternatives: patching kernel code
    [    1.998288] ------------[ cut here ]------------
    [    2.000693] kernel BUG at arch/arm64/mm/mmu.c:165!
    [    2.019215] Internal error: Oops - BUG: 0 [#1] PREEMPT SMP
    [    2.020257] Modules linked in:
    [    2.020807] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 5.0.0-rc2 #51
    [    2.021917] Hardware name: linux,dummy-virt (DT)
    [    2.022790] pstate: 40000005 (nZcv daif -PAN -UAO)
    [    2.023742] pc : __create_pgd_mapping+0x508/0x6d0
    [    2.024671] lr : __create_pgd_mapping+0x500/0x6d0
    
    [    2.058059] Process swapper/0 (pid: 1, stack limit = 0x(____ptrval____))
    [    2.059369] Call trace:
    [    2.059845]  __create_pgd_mapping+0x508/0x6d0
    [    2.060684]  update_mapping_prot+0x48/0xd0
    [    2.061477]  mark_linear_text_alias_ro+0xdc/0xe4
    [    2.070502]  smp_cpus_done+0x90/0x98
    [    2.071216]  smp_init+0x100/0x114
    [    2.071878]  kernel_init_freeable+0xd4/0x220
    [    2.072750]  kernel_init+0x10/0x100
    [    2.073455]  ret_from_fork+0x10/0x18
    
    [    2.075414] ---[ end trace 3572f3a7782292de ]---
    [    2.076389] Kernel panic - not syncing: Attempted to kill init! exitcode=0x0000000b
    
    If arm64_kernel_unmapped_at_el0() is true, arm64_kernel_use_ng_mappings()
    should also be true.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    CC: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    CC: John Garry <john.garry@huawei.com>
    CC: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index ac352accb3d9..3e8063f4f9d3 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -60,8 +60,11 @@ static inline bool arm64_kernel_use_ng_mappings(void)
 	 * later determine that kpti is required, then
 	 * kpti_install_ng_mappings() will make them non-global.
 	 */
+	if (arm64_kernel_unmapped_at_el0())
+		return true;
+
 	if (!IS_ENABLED(CONFIG_RANDOMIZE_BASE))
-		return arm64_kernel_unmapped_at_el0();
+		return false;
 
 	/*
 	 * KASLR is enabled so we're going to be enabling kpti on non-broken

commit b89d82ef01b33bc50cbaa8ff05607879b40d0704
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jan 8 16:19:01 2019 +0000

    arm64: kpti: Avoid rewriting early page tables when KASLR is enabled
    
    A side effect of commit c55191e96caa ("arm64: mm: apply r/o permissions
    of VM areas to its linear alias as well") is that the linear map is
    created with page granularity, which means that transitioning the early
    page table from global to non-global mappings when enabling kpti can
    take a significant amount of time during boot.
    
    Given that most CPU implementations do not require kpti, this mainly
    impacts KASLR builds where kpti is forcefully enabled. However, in these
    situations we know early on that non-global mappings are required and
    can avoid the use of global mappings from the beginning. The only gotcha
    is Cavium erratum #27456, which we must detect based on the MIDR value
    of the boot CPU.
    
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reported-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 7689c7aa1d77..ac352accb3d9 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -16,6 +16,8 @@
 #ifndef __ASM_MMU_H
 #define __ASM_MMU_H
 
+#include <asm/cputype.h>
+
 #define MMCF_AARCH32	0x1	/* mm context flag for AArch32 executables */
 #define USER_ASID_BIT	48
 #define USER_ASID_FLAG	(UL(1) << USER_ASID_BIT)
@@ -44,6 +46,45 @@ static inline bool arm64_kernel_unmapped_at_el0(void)
 	       cpus_have_const_cap(ARM64_UNMAP_KERNEL_AT_EL0);
 }
 
+static inline bool arm64_kernel_use_ng_mappings(void)
+{
+	bool tx1_bug;
+
+	/* What's a kpti? Use global mappings if we don't know. */
+	if (!IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0))
+		return false;
+
+	/*
+	 * Note: this function is called before the CPU capabilities have
+	 * been configured, so our early mappings will be global. If we
+	 * later determine that kpti is required, then
+	 * kpti_install_ng_mappings() will make them non-global.
+	 */
+	if (!IS_ENABLED(CONFIG_RANDOMIZE_BASE))
+		return arm64_kernel_unmapped_at_el0();
+
+	/*
+	 * KASLR is enabled so we're going to be enabling kpti on non-broken
+	 * CPUs regardless of their susceptibility to Meltdown. Rather
+	 * than force everybody to go through the G -> nG dance later on,
+	 * just put down non-global mappings from the beginning.
+	 */
+	if (!IS_ENABLED(CONFIG_CAVIUM_ERRATUM_27456)) {
+		tx1_bug = false;
+#ifndef MODULE
+	} else if (!static_branch_likely(&arm64_const_caps_ready)) {
+		extern const struct midr_range cavium_erratum_27456_cpus[];
+
+		tx1_bug = is_midr_in_range_list(read_cpuid_id(),
+						cavium_erratum_27456_cpus);
+#endif
+	} else {
+		tx1_bug = __cpus_have_const_cap(ARM64_WORKAROUND_CAVIUM_27456);
+	}
+
+	return !tx1_bug && kaslr_offset() > 0;
+}
+
 typedef void (*bp_hardening_cb_t)(void);
 
 struct bp_hardening_data {

commit 2b5548b68199c17c1466d5798cf2c9cd806bdaa9
Author: Jun Yao <yaojun8558363@gmail.com>
Date:   Mon Sep 24 15:47:49 2018 +0100

    arm64/mm: Separate boot-time page tables from swapper_pg_dir
    
    Since the address of swapper_pg_dir is fixed for a given kernel image,
    it is an attractive target for manipulation via an arbitrary write. To
    mitigate this we'd like to make it read-only by moving it into the
    rodata section.
    
    We require that swapper_pg_dir is at a fixed offset from tramp_pg_dir
    and reserved_ttbr0, so these will also need to move into rodata.
    However, swapper_pg_dir is allocated along with some transient page
    tables used for boot which we do not want to move into rodata.
    
    As a step towards this, this patch separates the boot-time page tables
    into a new init_pg_dir, and reduces swapper_pg_dir to the single page it
    needs to be. This allows us to retain the relationship between
    swapper_pg_dir, tramp_pg_dir, and swapper_pg_dir, while cleanly
    separating these from the boot-time page tables.
    
    The init_pg_dir holds all of the pgd/pud/pmd/pte levels needed during
    boot, and all of these levels will be freed when we switch to the
    swapper_pg_dir, which is initialized by the existing code in
    paging_init(). Since we start off on the init_pg_dir, we no longer need
    to allocate a transient page table in paging_init() in order to ensure
    that swapper_pg_dir isn't live while we initialize it.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Jun Yao <yaojun8558363@gmail.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    [Mark: place init_pg_dir after BSS, fold mm changes, commit message]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index dd320df0d026..7689c7aa1d77 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -95,5 +95,8 @@ extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
 extern void mark_linear_text_alias_ro(void);
 
+#define INIT_MM_CONTEXT(name)	\
+	.pgd = init_pg_dir,
+
 #endif	/* !__ASSEMBLY__ */
 #endif

commit dee39247dc75465a24990cb1772c6aaced5fd910
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Feb 15 11:47:14 2018 +0000

    arm64: KVM: Allow mapping of vectors outside of the RAM region
    
    We're now ready to map our vectors in weird and wonderful locations.
    On enabling ARM64_HARDEN_EL2_VECTORS, a vector slot gets allocated
    if this hasn't been already done via ARM64_HARDEN_BRANCH_PREDICTOR
    and gets mapped outside of the normal RAM region, next to the
    idmap.
    
    That way, being able to obtain VBAR_EL2 doesn't reveal the mapping
    of the rest of the hypervisor code.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 3baf010fe883..dd320df0d026 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -51,10 +51,13 @@ struct bp_hardening_data {
 	bp_hardening_cb_t	fn;
 };
 
-#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
+#if (defined(CONFIG_HARDEN_BRANCH_PREDICTOR) ||	\
+     defined(CONFIG_HARDEN_EL2_VECTORS))
 extern char __bp_harden_hyp_vecs_start[], __bp_harden_hyp_vecs_end[];
 extern atomic_t arm64_el2_vector_last_slot;
+#endif  /* CONFIG_HARDEN_BRANCH_PREDICTOR || CONFIG_HARDEN_EL2_VECTORS */
 
+#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 DECLARE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 static inline struct bp_hardening_data *arm64_get_bp_hardening_data(void)

commit 4205a89b8060141ac0216a507b9f70728f056a10
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Mar 13 12:40:39 2018 +0000

    arm64: Make BP hardening slot counter available
    
    We're about to need to allocate hardening slots from other parts
    of the kernel (in order to support ARM64_HARDEN_EL2_VECTORS).
    
    Turn the counter into an atomic_t and make it available to the
    rest of the kernel. Also add BP_HARDEN_EL2_SLOTS as the number of
    slots instead of the hardcoded 4...
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index a050d4f3615d..3baf010fe883 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -21,6 +21,8 @@
 #define USER_ASID_FLAG	(UL(1) << USER_ASID_BIT)
 #define TTBR_ASID_MASK	(UL(0xffff) << 48)
 
+#define BP_HARDEN_EL2_SLOTS 4
+
 #ifndef __ASSEMBLY__
 
 typedef struct {
@@ -51,6 +53,7 @@ struct bp_hardening_data {
 
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 extern char __bp_harden_hyp_vecs_start[], __bp_harden_hyp_vecs_end[];
+extern atomic_t arm64_el2_vector_last_slot;
 
 DECLARE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 

commit 79e9aa59dc29a995921fb01e64cd36b73cf5abe0
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:18 2018 +0000

    arm64: sdei: Add trampoline code for remapping the kernel
    
    When CONFIG_UNMAP_KERNEL_AT_EL0 is set the SDEI entry point and the rest
    of the kernel may be unmapped when we take an event. If this may be the
    case, use an entry trampoline that can switch to the kernel page tables.
    
    We can't use the provided PSTATE to determine whether to switch page
    tables as we may have interrupted the kernel's entry trampoline, (or a
    normal-priority event that interrupted the kernel's entry trampoline).
    Instead test for a user ASID in ttbr1_el1.
    
    Save a value in regs->addr_limit to indicate whether we need to restore
    the original ASID when returning from this event. This value is only used
    by do_page_fault(), which we don't call with the SDEI regs.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 6dd83d75b82a..a050d4f3615d 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -17,7 +17,8 @@
 #define __ASM_MMU_H
 
 #define MMCF_AARCH32	0x1	/* mm context flag for AArch32 executables */
-#define USER_ASID_FLAG	(UL(1) << 48)
+#define USER_ASID_BIT	48
+#define USER_ASID_FLAG	(UL(1) << USER_ASID_BIT)
 #define TTBR_ASID_MASK	(UL(0xffff) << 48)
 
 #ifndef __ASSEMBLY__

commit 0f15adbb2861ce6f75ccfc5a92b19eae0ef327d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 3 11:17:58 2018 +0000

    arm64: Add skeleton to harden the branch predictor against aliasing attacks
    
    Aliasing attacks against CPU branch predictors can allow an attacker to
    redirect speculative control flow on some CPUs and potentially divulge
    information from one context to another.
    
    This patch adds initial skeleton code behind a new Kconfig option to
    enable implementation-specific mitigations against these attacks for
    CPUs that are affected.
    
    Co-developed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 6f7bdb89817f..6dd83d75b82a 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -41,6 +41,43 @@ static inline bool arm64_kernel_unmapped_at_el0(void)
 	       cpus_have_const_cap(ARM64_UNMAP_KERNEL_AT_EL0);
 }
 
+typedef void (*bp_hardening_cb_t)(void);
+
+struct bp_hardening_data {
+	int			hyp_vectors_slot;
+	bp_hardening_cb_t	fn;
+};
+
+#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
+extern char __bp_harden_hyp_vecs_start[], __bp_harden_hyp_vecs_end[];
+
+DECLARE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
+
+static inline struct bp_hardening_data *arm64_get_bp_hardening_data(void)
+{
+	return this_cpu_ptr(&bp_hardening_data);
+}
+
+static inline void arm64_apply_bp_hardening(void)
+{
+	struct bp_hardening_data *d;
+
+	if (!cpus_have_const_cap(ARM64_HARDEN_BRANCH_PREDICTOR))
+		return;
+
+	d = arm64_get_bp_hardening_data();
+	if (d->fn)
+		d->fn();
+}
+#else
+static inline struct bp_hardening_data *arm64_get_bp_hardening_data(void)
+{
+	return NULL;
+}
+
+static inline void arm64_apply_bp_hardening(void)	{ }
+#endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
+
 extern void paging_init(void);
 extern void bootmem_init(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);

commit b519538dfefc2f8478a1bcb458459c861d431784
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Dec 1 17:33:48 2017 +0000

    arm64: mm: Introduce TTBR_ASID_MASK for getting at the ASID in the TTBR
    
    There are now a handful of open-coded masks to extract the ASID from a
    TTBR value, so introduce a TTBR_ASID_MASK and use that instead.
    
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index da6f12e40714..6f7bdb89817f 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -18,6 +18,7 @@
 
 #define MMCF_AARCH32	0x1	/* mm context flag for AArch32 executables */
 #define USER_ASID_FLAG	(UL(1) << 48)
+#define TTBR_ASID_MASK	(UL(0xffff) << 48)
 
 #ifndef __ASSEMBLY__
 

commit ea1e3de85e94d711f63437c04624aa0e8de5c8b3
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 14 14:38:19 2017 +0000

    arm64: entry: Add fake CPU feature for unmapping the kernel at EL0
    
    Allow explicit disabling of the entry trampoline on the kernel command
    line (kpti=off) by adding a fake CPU feature (ARM64_UNMAP_KERNEL_AT_EL0)
    that can be used to toggle the alternative sequences in our entry code and
    avoid use of the trampoline altogether if desired. This also allows us to
    make use of a static key in arm64_kernel_unmapped_at_el0().
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index c07954638658..da6f12e40714 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -36,7 +36,8 @@ typedef struct {
 
 static inline bool arm64_kernel_unmapped_at_el0(void)
 {
-	return IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0);
+	return IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0) &&
+	       cpus_have_const_cap(ARM64_UNMAP_KERNEL_AT_EL0);
 }
 
 extern void paging_init(void);

commit fc0e1299da548b32440051f58f08e0c1eb7edd0b
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 14 13:58:08 2017 +0000

    arm64: mm: Add arm64_kernel_unmapped_at_el0 helper
    
    In order for code such as TLB invalidation to operate efficiently when
    the decision to map the kernel at EL0 is determined at runtime, this
    patch introduces a helper function, arm64_kernel_unmapped_at_el0, to
    determine whether or not the kernel is mapped whilst running in userspace.
    
    Currently, this just reports the value of CONFIG_UNMAP_KERNEL_AT_EL0,
    but will later be hooked up to a fake CPU capability using a static key.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 01bfb184f2a8..c07954638658 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -19,6 +19,8 @@
 #define MMCF_AARCH32	0x1	/* mm context flag for AArch32 executables */
 #define USER_ASID_FLAG	(UL(1) << 48)
 
+#ifndef __ASSEMBLY__
+
 typedef struct {
 	atomic64_t	id;
 	void		*vdso;
@@ -32,6 +34,11 @@ typedef struct {
  */
 #define ASID(mm)	((mm)->context.id.counter & 0xffff)
 
+static inline bool arm64_kernel_unmapped_at_el0(void)
+{
+	return IS_ENABLED(CONFIG_UNMAP_KERNEL_AT_EL0);
+}
+
 extern void paging_init(void);
 extern void bootmem_init(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
@@ -42,4 +49,5 @@ extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
 extern void mark_linear_text_alias_ro(void);
 
+#endif	/* !__ASSEMBLY__ */
 #endif

commit 0c8ea531b7740754cf374ca8b7510655f569c5e3
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 10 14:10:28 2017 +0100

    arm64: mm: Allocate ASIDs in pairs
    
    In preparation for separate kernel/user ASIDs, allocate them in pairs
    for each mm_struct. The bottom bit distinguishes the two: if it is set,
    then the ASID will map only userspace.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 0d34bf0a89c7..01bfb184f2a8 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -17,6 +17,7 @@
 #define __ASM_MMU_H
 
 #define MMCF_AARCH32	0x1	/* mm context flag for AArch32 executables */
+#define USER_ASID_FLAG	(UL(1) << 48)
 
 typedef struct {
 	atomic64_t	id;

commit 5ce93ab624cee4ed68086c946bd6d18b9b3f64aa
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Sun Aug 20 13:20:47 2017 +0300

    arm64: introduce separated bits for mm_context_t flags
    
    Currently mm->context.flags field uses thread_info flags which is not
    the best idea for many reasons. For example, mm_context_t doesn't need
    most of thread_info flags. And it would be difficult to add new mm-related
    flag if needed because it may easily interfere with TIF ones.
    
    To deal with it, the new MMCF_AARCH32 flag is introduced for
    mm_context_t->flags, where MMCF prefix stands for mm_context_t flags.
    Also, mm_context_t flag doesn't require atomicity and ordering of the
    access, so using set/clear_bit() is replaced with simple masks.
    
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 5468c834b072..0d34bf0a89c7 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -16,6 +16,8 @@
 #ifndef __ASM_MMU_H
 #define __ASM_MMU_H
 
+#define MMCF_AARCH32	0x1	/* mm context flag for AArch32 executables */
+
 typedef struct {
 	atomic64_t	id;
 	void		*vdso;

commit 5ea5306c3235a157f06040c59730b1133115ed26
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Mar 9 21:52:01 2017 +0100

    arm64: alternatives: apply boot time fixups via the linear mapping
    
    One important rule of thumb when desiging a secure software system is
    that memory should never be writable and executable at the same time.
    We mostly adhere to this rule in the kernel, except at boot time, when
    regions may be mapped RWX until after we are done applying alternatives
    or making other one-off changes.
    
    For the alternative patching, we can improve the situation by applying
    the fixups via the linear mapping, which is never mapped with executable
    permissions. So map the linear alias of .text with RW- permissions
    initially, and remove the write permissions as soon as alternative
    patching has completed.
    
    Reviewed-by: Laura Abbott <labbott@redhat.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 47619411f0ff..5468c834b072 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -37,5 +37,6 @@ extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       unsigned long virt, phys_addr_t size,
 			       pgprot_t prot, bool page_mappings_only);
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
+extern void mark_linear_text_alias_ro(void);
 
 #endif

commit 06beb72fbe23e348cc423dd9310c6fc02cd7c7da
Author: Pratyush Anand <panand@redhat.com>
Date:   Wed Nov 2 14:40:45 2016 +0530

    arm64: introduce mm context flag to keep 32 bit task information
    
    We need to decide in some cases like uprobe instruction analysis that
    whether the current mm context belongs to a 32 bit task or 64 bit.
    
    This patch has introduced an unsigned flag variable in mm_context_t.
    Currently, we set and clear TIF_32BIT depending on the condition that
    whether an elf binary load sets personality for 32 bit or 64 bit
    respectively.
    
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index a81454ad5455..47619411f0ff 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -19,6 +19,7 @@
 typedef struct {
 	atomic64_t	id;
 	void		*vdso;
+	unsigned long	flags;
 } mm_context_t;
 
 /*

commit f14c66ce81b5595a483bd83df151539dbe1058fa
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Oct 21 12:22:57 2016 +0100

    arm64: mm: replace 'block_mappings_allowed' with 'page_mappings_only'
    
    In preparation of adding support for contiguous PTE and PMD mappings,
    let's replace 'block_mappings_allowed' with 'page_mappings_only', which
    will be a more accurate description of the nature of the setting once we
    add such contiguous mappings into the mix.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 8d9fce037b2f..a81454ad5455 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -34,7 +34,7 @@ extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       unsigned long virt, phys_addr_t size,
-			       pgprot_t prot, bool allow_block_mappings);
+			       pgprot_t prot, bool page_mappings_only);
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
 
 #endif

commit 53e1b32910a3bc94d9f122321442b79b314219f8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Jun 29 14:51:26 2016 +0200

    arm64: mm: add param to force create_pgd_mapping() to use page mappings
    
    Add a bool parameter 'allow_block_mappings' to create_pgd_mapping() and
    the various helper functions that it descends into, to give the caller
    control over whether block entries may be used to create the mapping.
    
    The UEFI runtime mapping routines will use this to avoid creating block
    entries that would need to split up into page entries when applying the
    permissions listed in the Memory Attributes firmware table.
    
    This also replaces the block_mappings_allowed() helper function that was
    added for DEBUG_PAGEALLOC functionality, but the resulting code is
    functionally equivalent (given that debug_page_alloc does not operate on
    EFI page table entries anyway)
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 97b1d8f26b9c..8d9fce037b2f 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -34,7 +34,7 @@ extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       unsigned long virt, phys_addr_t size,
-			       pgprot_t prot);
+			       pgprot_t prot, bool allow_block_mappings);
 extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
 
 #endif

commit 3194ac6e66cc7a00c1fa9fecf33a7c376b489497
Author: David Daney <david.daney@cavium.com>
Date:   Fri Apr 8 15:50:26 2016 -0700

    arm64: Move unflatten_device_tree() call earlier.
    
    In order to extract NUMA information from the device tree, we need to
    have the tree in its unflattened form.
    
    Move the call to bootmem_init() in the tail of paging_init() into
    setup_arch, and adjust header files so that its declaration is
    visible.
    
    Move the unflatten_device_tree() call between the calls to
    paging_init() and bootmem_init().  Follow on patches add NUMA handling
    to bootmem_init().
    
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 990124a67eeb..97b1d8f26b9c 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -29,6 +29,7 @@ typedef struct {
 #define ASID(mm)	((mm)->context.id.counter & 0xffff)
 
 extern void paging_init(void);
+extern void bootmem_init(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,

commit 5aec715d7d3122f77cabaa7578d9d25a0c1ed20e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:24 2015 +0100

    arm64: mm: rewrite ASID allocator and MM context-switching code
    
    Our current switch_mm implementation suffers from a number of problems:
    
      (1) The ASID allocator relies on IPIs to synchronise the CPUs on a
          rollover event
    
      (2) Because of (1), we cannot allocate ASIDs with interrupts disabled
          and therefore make use of a TIF_SWITCH_MM flag to postpone the
          actual switch to finish_arch_post_lock_switch
    
      (3) We run context switch with a reserved (invalid) TTBR0 value, even
          though the ASID and pgd are updated atomically
    
      (4) We take a global spinlock (cpu_asid_lock) during context-switch
    
      (5) We use h/w broadcast TLB operations when they are not required
          (e.g. in flush_context)
    
    This patch addresses these problems by rewriting the ASID algorithm to
    match the bitmap-based arch/arm/ implementation more closely. This in
    turn allows us to remove much of the complications surrounding switch_mm,
    including the ugly thread flag.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 030208767185..990124a67eeb 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -17,15 +17,16 @@
 #define __ASM_MMU_H
 
 typedef struct {
-	unsigned int id;
-	raw_spinlock_t id_lock;
-	void *vdso;
+	atomic64_t	id;
+	void		*vdso;
 } mm_context_t;
 
-#define INIT_MM_CONTEXT(name) \
-	.context.id_lock = __RAW_SPIN_LOCK_UNLOCKED(name.context.id_lock),
-
-#define ASID(mm)	((mm)->context.id & 0xffff)
+/*
+ * This macro is only used by the TLBI code, which cannot race with an
+ * ASID change and therefore doesn't need to reload the counter using
+ * atomic64_read.
+ */
+#define ASID(mm)	((mm)->context.id.counter & 0xffff)
 
 extern void paging_init(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);

commit b08d4640a3dca68670fc5af2fe9205b395a02388
Author: Mark Salter <msalter@redhat.com>
Date:   Thu Jul 16 18:58:53 2015 +0100

    arm64: remove dead code
    
    Commit 68234df4ea79 ("arm64: kill flush_cache_all()") removed
    soft_reset() from the kernel. This was the only caller of
    setup_mm_for_reboot(), so remove that also.
    
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 79fcfb048884..030208767185 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -28,7 +28,6 @@ typedef struct {
 #define ASID(mm)	((mm)->context.id & 0xffff)
 
 extern void paging_init(void);
-extern void setup_mm_for_reboot(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,

commit 61bd93ce801bb6df36eda257a9d2d16c02863cdd
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Jun 1 13:40:32 2015 +0200

    arm64: use fixmap region for permanent FDT mapping
    
    Currently, the FDT blob needs to be in the same 512 MB region as
    the kernel, so that it can be mapped into the kernel virtual memory
    space very early on using a minimal set of statically allocated
    translation tables.
    
    Now that we have early fixmap support, we can relax this restriction,
    by moving the permanent FDT mapping to the fixmap region instead.
    This way, the FDT blob may be anywhere in memory.
    
    This also moves the vetting of the FDT to mmu.c, since the early
    init code in head.S does not handle mapping of the FDT anymore.
    At the same time, fix up some comments in head.S that have gone stale.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 3d311761e3c2..79fcfb048884 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -34,5 +34,6 @@ extern void init_mem_pgprot(void);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       unsigned long virt, phys_addr_t size,
 			       pgprot_t prot);
+extern void *fixmap_remap_fdt(phys_addr_t dt_phys);
 
 #endif

commit 9679be103108926cfe9e6fd2f6829cefa77e47b0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Oct 20 16:41:38 2014 +0200

    arm64/efi: remove idmap manipulations from UEFI code
    
    Now that we have moved the call to SetVirtualAddressMap() to the stub,
    UEFI has no use for the ID map, so we can drop the code that installs
    ID mappings for UEFI memory regions.
    
    Acked-by: Leif Lindholm <leif.lindholm@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Leif Lindholm <leif.lindholm@linaro.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 5fd40c43be80..3d311761e3c2 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -31,8 +31,6 @@ extern void paging_init(void);
 extern void setup_mm_for_reboot(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
-/* create an identity mapping for memory (or io if map_io is true) */
-extern void create_id_mapping(phys_addr_t addr, phys_addr_t size, int map_io);
 extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
 			       unsigned long virt, phys_addr_t size,
 			       pgprot_t prot);

commit 8ce837cee8f51fb0eacb32c85461ea2f0fafc9f8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Oct 20 15:42:07 2014 +0200

    arm64/mm: add create_pgd_mapping() to create private page tables
    
    For UEFI, we need to install the memory mappings used for Runtime Services
    in a dedicated set of page tables. Add create_pgd_mapping(), which allows
    us to allocate and install those page table entries early.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Leif Lindholm <leif.lindholm@linaro.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index c2f006c48bdb..5fd40c43be80 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -33,5 +33,8 @@ extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
 /* create an identity mapping for memory (or io if map_io is true) */
 extern void create_id_mapping(phys_addr_t addr, phys_addr_t size, int map_io);
+extern void create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
+			       unsigned long virt, phys_addr_t size,
+			       pgprot_t prot);
 
 #endif

commit c3c55a07203947f72afa50a3218460b27307c47d
Merge: 046f153343e3 74bcc2499291
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 5 13:15:32 2014 -0700

    Merge branch 'arm64-efi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip into next
    
    Pull ARM64 EFI update from Peter Anvin:
     "By agreement with the ARM64 EFI maintainers, we have agreed to make
      -tip the upstream for all EFI patches.  That is why this patchset
      comes from me :)
    
      This patchset enables EFI stub support for ARM64, like we already have
      on x86"
    
    * 'arm64-efi-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      arm64: efi: only attempt efi map setup if booting via EFI
      efi/arm64: ignore dtb= when UEFI SecureBoot is enabled
      doc: arm64: add description of EFI stub support
      arm64: efi: add EFI stub
      doc: arm: add UEFI support documentation
      arm64: add EFI runtime services
      efi: Add shared FDT related functions for ARM/ARM64
      arm64: Add function to create identity mappings
      efi: add helper function to get UEFI params from FDT
      doc: efi-stub.txt updates for ARM
      lib: add fdt_empty_tree.c

commit d7ecbddf4caefbac1b99478dd2b679f83dfc2545
Author: Mark Salter <msalter@redhat.com>
Date:   Wed Mar 12 12:28:06 2014 -0400

    arm64: Add function to create identity mappings
    
    At boot time, before switching to a virtual UEFI memory map, firmware
    expects UEFI memory and IO regions to be identity mapped whenever
    kernel makes runtime services calls. The existing early boot code
    creates an identity map of kernel text/data but this is not sufficient
    for UEFI. This patch adds a create_id_mapping() function which reuses
    the core code of the existing create_mapping().
    
    Signed-off-by: Mark Salter <msalter@redhat.com>
    [ Fixed error message formatting (%pa). ]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Leif Lindholm <leif.lindholm@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index f600d400c07d..29ed1d865e13 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -28,5 +28,7 @@ extern void paging_init(void);
 extern void setup_mm_for_reboot(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 extern void init_mem_pgprot(void);
+/* create an identity mapping for memory (or io if map_io is true) */
+extern void create_id_mapping(phys_addr_t addr, phys_addr_t size, int map_io);
 
 #endif

commit 8f0712037b4ed63dfce844939ac9866054f15ca0
Author: Leo Yan <leoy@marvell.com>
Date:   Wed Apr 16 13:26:35 2014 +0100

    arm64: initialize spinlock for init_mm's context
    
    ARM64 has defined the spinlock for init_mm's context, so need initialize
    the spinlock structure; otherwise during the suspend flow it will dump
    the info for spinlock's bad magic warning as below:
    
    [   39.084394] Disabling non-boot CPUs ...
    [   39.092871] BUG: spinlock bad magic on CPU#1, swapper/1/0
    [   39.092896]  lock: init_mm+0x338/0x3e0, .magic: 00000000, .owner: <none>/-1, .owner_cpu: 0
    [   39.092907] CPU: 1 PID: 0 Comm: swapper/1 Tainted: G           O 3.10.33 #125
    [   39.092912] Call trace:
    [   39.092927] [<ffffffc000087e64>] dump_backtrace+0x0/0x16c
    [   39.092934] [<ffffffc000087fe0>] show_stack+0x10/0x1c
    [   39.092947] [<ffffffc000765334>] dump_stack+0x1c/0x28
    [   39.092953] [<ffffffc0007653b8>] spin_dump+0x78/0x88
    [   39.092960] [<ffffffc0007653ec>] spin_bug+0x24/0x34
    [   39.092971] [<ffffffc000300a28>] do_raw_spin_lock+0x98/0x17c
    [   39.092979] [<ffffffc00076cf08>] _raw_spin_lock_irqsave+0x4c/0x60
    [   39.092990] [<ffffffc000094044>] set_mm_context+0x1c/0x6c
    [   39.092996] [<ffffffc0000941c8>] __new_context+0x94/0x10c
    [   39.093007] [<ffffffc0000d63d4>] idle_task_exit+0x104/0x1b0
    [   39.093014] [<ffffffc00008d91c>] cpu_die+0x14/0x74
    [   39.093021] [<ffffffc000084f74>] arch_cpu_idle_dead+0x8/0x14
    [   39.093030] [<ffffffc0000e7f18>] cpu_startup_entry+0x1ec/0x258
    [   39.093036] [<ffffffc00008d810>] secondary_start_kernel+0x114/0x124
    
    Signed-off-by: Leo Yan <leoy@marvell.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index f600d400c07d..aff0292c8f4d 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -22,6 +22,9 @@ typedef struct {
 	void *vdso;
 } mm_context_t;
 
+#define INIT_MM_CONTEXT(name) \
+	.context.id_lock = __RAW_SPIN_LOCK_UNLOCKED(name.context.id_lock),
+
 #define ASID(mm)	((mm)->context.id & 0xffff)
 
 extern void paging_init(void);

commit 0bf757c73d6612d3d279de3f61b35062aa9c8b1d
Author: Mark Salter <msalter@redhat.com>
Date:   Mon Apr 7 15:39:51 2014 -0700

    arm64: initialize pgprot info earlier in boot
    
    Presently, paging_init() calls init_mem_pgprot() to initialize pgprot
    values used by macros such as PAGE_KERNEL, PAGE_KERNEL_EXEC, etc.
    
    The new fixmap and early_ioremap support also needs to use these macros
    before paging_init() is called.  This patch moves the init_mem_pgprot()
    call out of paging_init() and into setup_arch() so that pgprot_default
    gets initialized in time for fixmap and early_ioremap.
    
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Borislav Petkov <borislav.petkov@amd.com>
    Cc: Dave Young <dyoung@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index 2494fc01896a..f600d400c07d 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -27,5 +27,6 @@ typedef struct {
 extern void paging_init(void);
 extern void setup_mm_for_reboot(void);
 extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
+extern void init_mem_pgprot(void);
 
 #endif

commit 2475ff9d2c6ea3bbfed55c4635426c371f9ad327
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Oct 23 14:55:08 2012 +0100

    arm64: Add simple earlyprintk support
    
    This patch adds support for "earlyprintk=" parameter on the kernel
    command line. The format is:
    
      earlyprintk=<name>[,<addr>][,<options>]
    
    where <name> is the name of the (UART) device, e.g. "pl011", <addr> is
    the I/O address. The <options> aren't currently used.
    
    The mapping of the earlyprintk device is done very early during kernel
    boot and there are restrictions on which functions it can call. A
    special early_io_map() function is added which creates the mapping from
    the pre-defined EARLY_IOBASE to the device I/O address passed via the
    kernel parameter. The pgd entry corresponding to EARLY_IOBASE is
    pre-populated in head.S during kernel boot.
    
    Only PL011 is currently supported and it is assumed that the interface
    is already initialised by the boot loader before the kernel is started.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
index d4f7fd5b9e33..2494fc01896a 100644
--- a/arch/arm64/include/asm/mmu.h
+++ b/arch/arm64/include/asm/mmu.h
@@ -26,5 +26,6 @@ typedef struct {
 
 extern void paging_init(void);
 extern void setup_mm_for_reboot(void);
+extern void __iomem *early_io_map(phys_addr_t phys, unsigned long virt);
 
 #endif

commit 4f04d8f00545110a0e525ae2fb62ab38cb417236
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: MMU definitions
    
    The virtual memory layout is described in
    Documentation/arm64/memory.txt. This patch adds the MMU definitions for
    the 4KB and 64KB translation table configurations. The SECTION_SIZE is
    2MB with 4KB page and 512MB with 64KB page configuration.
    
    PHYS_OFFSET is calculated at run-time and stored in a variable (no
    run-time code patching at this stage).
    
    On the current implementation, both user and kernel address spaces are
    512G (39-bit) each with a maximum of 256G for the RAM linear mapping.
    Linux uses 3 levels of translation tables with the 4K page configuration
    and 2 levels with the 64K configuration. Extending the memory space
    beyond 39-bit with the 4K pages or 42-bit with 64K pages requires an
    additional level of translation tables.
    
    The SPARSEMEM configuration is global to all AArch64 platforms and
    allows for 1GB sections with SPARSEMEM_VMEMMAP enabled by default.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
new file mode 100644
index 000000000000..d4f7fd5b9e33
--- /dev/null
+++ b/arch/arm64/include/asm/mmu.h
@@ -0,0 +1,30 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_MMU_H
+#define __ASM_MMU_H
+
+typedef struct {
+	unsigned int id;
+	raw_spinlock_t id_lock;
+	void *vdso;
+} mm_context_t;
+
+#define ASID(mm)	((mm)->context.id & 0xffff)
+
+extern void paging_init(void);
+extern void setup_mm_for_reboot(void);
+
+#endif
