commit 034aa9cd698e315c767af1bac3fd1ff8898d2cd7
Author: Will Deacon <will@kernel.org>
Date:   Mon Jun 15 16:27:43 2020 +0100

    arm64: pgtable: Clear the GP bit for non-executable kernel pages
    
    Commit cca98e9f8b5e ("mm: enforce that vmap can't map pages executable")
    introduced 'pgprot_nx(prot)' for arm64 but collided silently with the
    BTI support during the merge window, which endeavours to clear the GP
    bit for non-executable kernel mappings in set_memory_nx().
    
    For consistency between the two APIs, clear the GP bit in pgprot_nx().
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mark Brown <broonie@kernel.org>
    Link: https://lore.kernel.org/r/20200615154642.3579-1-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 6dbd267ab931..758e2d1577d0 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -416,7 +416,7 @@ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
 	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
 
 #define pgprot_nx(prot) \
-	__pgprot_modify(prot, 0, PTE_PXN)
+	__pgprot_modify(prot, PTE_MAYBE_GP, PTE_PXN)
 
 /*
  * Mark the prot value as uncacheable and unbufferable.

commit 974b9b2c68f3d35a65e80af9657fe378d2439b60
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:33:10 2020 -0700

    mm: consolidate pte_index() and pte_offset_*() definitions
    
    All architectures define pte_index() as
    
            (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1)
    
    and all architectures define pte_offset_kernel() as an entry in the array
    of PTEs indexed by the pte_index().
    
    For the most architectures the pte_offset_kernel() implementation relies
    on the availability of pmd_page_vaddr() that converts a PMD entry value to
    the virtual address of the page containing PTEs array.
    
    Let's move x86 definitions of the PTE accessors to the generic place in
    <linux/pgtable.h> and then simply drop the respective definitions from the
    other architectures.
    
    The architectures that didn't provide pmd_page_vaddr() are updated to have
    that defined.
    
    The generic implementation of pte_offset_kernel() can be overridden by an
    architecture and alpha makes use of this because it has special ordering
    requirements for its version of pte_offset_kernel().
    
    [rppt@linux.ibm.com: v2]
      Link: http://lkml.kernel.org/r/20200514170327.31389-11-rppt@kernel.org
    [rppt@linux.ibm.com: update]
      Link: http://lkml.kernel.org/r/20200514170327.31389-12-rppt@kernel.org
    [rppt@linux.ibm.com: update]
      Link: http://lkml.kernel.org/r/20200514170327.31389-13-rppt@kernel.org
    [akpm@linux-foundation.org: fix x86 warning]
    [sfr@canb.auug.org.au: fix powerpc build]
      Link: http://lkml.kernel.org/r/20200607153443.GB738695@linux.ibm.com
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-10-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 437c92ab2081..6dbd267ab931 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -506,15 +506,13 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 	return __pmd_to_phys(pmd);
 }
 
-static inline void pte_unmap(pte_t *pte) { }
+static inline unsigned long pmd_page_vaddr(pmd_t pmd)
+{
+	return (unsigned long)__va(pmd_page_paddr(pmd));
+}
 
 /* Find an entry in the third-level page table. */
-#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
-
 #define pte_offset_phys(dir,addr)	(pmd_page_paddr(READ_ONCE(*(dir))) + pte_index(addr) * sizeof(pte_t))
-#define pte_offset_kernel(dir,addr)	((pte_t *)__va(pte_offset_phys((dir), (addr))))
-
-#define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
 
 #define pte_set_fixmap(addr)		((pte_t *)set_fixmap_offset(FIX_PTE, addr))
 #define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))
@@ -568,11 +566,13 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 	return __pud_to_phys(pud);
 }
 
-/* Find an entry in the second-level page table. */
-#define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
+static inline unsigned long pud_page_vaddr(pud_t pud)
+{
+	return (unsigned long)__va(pud_page_paddr(pud));
+}
 
+/* Find an entry in the second-level page table. */
 #define pmd_offset_phys(dir, addr)	(pud_page_paddr(READ_ONCE(*(dir))) + pmd_index(addr) * sizeof(pmd_t))
-#define pmd_offset(dir, addr)		((pmd_t *)__va(pmd_offset_phys((dir), (addr))))
 
 #define pmd_set_fixmap(addr)		((pmd_t *)set_fixmap_offset(FIX_PMD, addr))
 #define pmd_set_fixmap_offset(pud, addr)	pmd_set_fixmap(pmd_offset_phys(pud, addr))
@@ -626,11 +626,13 @@ static inline phys_addr_t p4d_page_paddr(p4d_t p4d)
 	return __p4d_to_phys(p4d);
 }
 
-/* Find an entry in the frst-level page table. */
-#define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
+static inline unsigned long p4d_page_vaddr(p4d_t p4d)
+{
+	return (unsigned long)__va(p4d_page_paddr(p4d));
+}
 
+/* Find an entry in the frst-level page table. */
 #define pud_offset_phys(dir, addr)	(p4d_page_paddr(READ_ONCE(*(dir))) + pud_index(addr) * sizeof(pud_t))
-#define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
 
 #define pud_set_fixmap(addr)		((pud_t *)set_fixmap_offset(FIX_PUD, addr))
 #define pud_set_fixmap_offset(p4d, addr)	pud_set_fixmap(pud_offset_phys(p4d, addr))
@@ -657,16 +659,6 @@ static inline phys_addr_t p4d_page_paddr(p4d_t p4d)
 
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
 
-/* to find an entry in a page-table-directory */
-#define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
-
-#define pgd_offset_raw(pgd, addr)	((pgd) + pgd_index(addr))
-
-#define pgd_offset(mm, addr)	(pgd_offset_raw((mm)->pgd, (addr)))
-
-/* to find an entry in a kernel page-table-directory */
-#define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
-
 #define pgd_set_fixmap(addr)	((pgd_t *)set_fixmap_offset(FIX_PGD, addr))
 #define pgd_clear_fixmap()	clear_fixmap(FIX_PGD)
 

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 1f3218fc52fc..437c92ab2081 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -340,7 +340,7 @@ static inline pgprot_t mk_pmd_sect_prot(pgprot_t prot)
 
 #ifdef CONFIG_NUMA_BALANCING
 /*
- * See the comment in include/asm-generic/pgtable.h
+ * See the comment in include/linux/pgtable.h
  */
 static inline int pte_protnone(pte_t pte)
 {
@@ -853,8 +853,6 @@ static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
 
 extern int kern_addr_valid(unsigned long addr);
 
-#include <asm-generic/pgtable.h>
-
 /*
  * On AArch64, the cache coherency is handled via the set_pte_at() function.
  */

commit e9f6376858b9799148d07e58b72b681d4b8fa4c7
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Jun 4 16:46:23 2020 -0700

    arm64: add support for folded p4d page tables
    
    Implement primitives necessary for the 4th level folding, add walks of p4d
    level where appropriate, replace 5level-fixup.h with pgtable-nop4d.h and
    remove __ARCH_USE_5LEVEL_HACK.
    
    [arnd@arndb.de: fix gcc-10 shift warning]
      Link: http://lkml.kernel.org/r/20200429185657.4085975-1-arnd@arndb.de
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200414153455.21744-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 9ce000f22d9e..1f3218fc52fc 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -298,6 +298,11 @@ static inline pte_t pgd_pte(pgd_t pgd)
 	return __pte(pgd_val(pgd));
 }
 
+static inline pte_t p4d_pte(p4d_t p4d)
+{
+	return __pte(p4d_val(p4d));
+}
+
 static inline pte_t pud_pte(pud_t pud)
 {
 	return __pte(pud_val(pud));
@@ -401,6 +406,9 @@ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 
+#define __p4d_to_phys(p4d)	__pte_to_phys(p4d_pte(p4d))
+#define __phys_to_p4d_val(phys)	__phys_to_pte_val(phys)
+
 #define __pgd_to_phys(pgd)	__pte_to_phys(pgd_pte(pgd))
 #define __phys_to_pgd_val(phys)	__phys_to_pte_val(phys)
 
@@ -592,49 +600,50 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 
 #define pud_ERROR(pud)		__pud_error(__FILE__, __LINE__, pud_val(pud))
 
-#define pgd_none(pgd)		(!pgd_val(pgd))
-#define pgd_bad(pgd)		(!(pgd_val(pgd) & 2))
-#define pgd_present(pgd)	(pgd_val(pgd))
+#define p4d_none(p4d)		(!p4d_val(p4d))
+#define p4d_bad(p4d)		(!(p4d_val(p4d) & 2))
+#define p4d_present(p4d)	(p4d_val(p4d))
 
-static inline void set_pgd(pgd_t *pgdp, pgd_t pgd)
+static inline void set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
-	if (in_swapper_pgdir(pgdp)) {
-		set_swapper_pgd(pgdp, pgd);
+	if (in_swapper_pgdir(p4dp)) {
+		set_swapper_pgd((pgd_t *)p4dp, __pgd(p4d_val(p4d)));
 		return;
 	}
 
-	WRITE_ONCE(*pgdp, pgd);
+	WRITE_ONCE(*p4dp, p4d);
 	dsb(ishst);
 	isb();
 }
 
-static inline void pgd_clear(pgd_t *pgdp)
+static inline void p4d_clear(p4d_t *p4dp)
 {
-	set_pgd(pgdp, __pgd(0));
+	set_p4d(p4dp, __p4d(0));
 }
 
-static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
+static inline phys_addr_t p4d_page_paddr(p4d_t p4d)
 {
-	return __pgd_to_phys(pgd);
+	return __p4d_to_phys(p4d);
 }
 
 /* Find an entry in the frst-level page table. */
 #define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
 
-#define pud_offset_phys(dir, addr)	(pgd_page_paddr(READ_ONCE(*(dir))) + pud_index(addr) * sizeof(pud_t))
+#define pud_offset_phys(dir, addr)	(p4d_page_paddr(READ_ONCE(*(dir))) + pud_index(addr) * sizeof(pud_t))
 #define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
 
 #define pud_set_fixmap(addr)		((pud_t *)set_fixmap_offset(FIX_PUD, addr))
-#define pud_set_fixmap_offset(pgd, addr)	pud_set_fixmap(pud_offset_phys(pgd, addr))
+#define pud_set_fixmap_offset(p4d, addr)	pud_set_fixmap(pud_offset_phys(p4d, addr))
 #define pud_clear_fixmap()		clear_fixmap(FIX_PUD)
 
-#define pgd_page(pgd)			phys_to_page(__pgd_to_phys(pgd))
+#define p4d_page(p4d)		pfn_to_page(__phys_to_pfn(__p4d_to_phys(p4d)))
 
 /* use ONLY for statically allocated translation tables */
 #define pud_offset_kimg(dir,addr)	((pud_t *)__phys_to_kimg(pud_offset_phys((dir), (addr))))
 
 #else
 
+#define p4d_page_paddr(p4d)	({ BUILD_BUG(); 0;})
 #define pgd_page_paddr(pgd)	({ BUILD_BUG(); 0;})
 
 /* Match pud_offset folding in <asm/generic/pgtable-nopud.h> */

commit 86ec2da037b85436b63afe3df43ed48fa0e52b0e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed Jun 3 16:03:45 2020 -0700

    mm/thp: rename pmd_mknotpresent() as pmd_mkinvalid()
    
    pmd_present() is expected to test positive after pmdp_mknotpresent() as
    the PMD entry still points to a valid huge page in memory.
    pmdp_mknotpresent() implies that given PMD entry is just invalidated from
    MMU perspective while still holding on to pmd_page() referred valid huge
    page thus also clearing pmd_present() test.  This creates the following
    situation which is counter intuitive.
    
    [pmd_present(pmd_mknotpresent(pmd)) = true]
    
    This renames pmd_mknotpresent() as pmd_mkinvalid() reflecting the helper's
    functionality more accurately while changing the above mentioned situation
    as follows.  This does not create any functional change.
    
    [pmd_present(pmd_mkinvalid(pmd)) = true]
    
    This is not applicable for platforms that define own pmdp_invalidate() via
    __HAVE_ARCH_PMDP_INVALIDATE.  Suggestion for renaming came during a
    previous discussion here.
    
    https://patchwork.kernel.org/patch/11019637/
    
    [anshuman.khandual@arm.com: change pmd_mknotvalid() to pmd_mkinvalid() per Will]
      Link: http://lkml.kernel.org/r/1587520326-10099-3-git-send-email-anshuman.khandual@arm.com
    Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Link: http://lkml.kernel.org/r/1584680057-13753-3-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index dae0466d19d6..9ce000f22d9e 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -366,7 +366,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pmd_mkclean(pmd)	pte_pmd(pte_mkclean(pmd_pte(pmd)))
 #define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))
 #define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
-#define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_SECT_VALID))
+#define pmd_mkinvalid(pmd)	(__pmd(pmd_val(pmd) & ~PMD_SECT_VALID))
 
 #define pmd_thp_or_huge(pmd)	(pmd_huge(pmd) || pmd_trans_huge(pmd))
 

commit 94709049fb8442fb2f7b91fbec3c2897a75e18df
Merge: 17839856fd58 4fba37586e4e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 2 12:21:36 2020 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
     "A few little subsystems and a start of a lot of MM patches.
    
      Subsystems affected by this patch series: squashfs, ocfs2, parisc,
      vfs. With mm subsystems: slab-generic, slub, debug, pagecache, gup,
      swap, memcg, pagemap, memory-failure, vmalloc, kasan"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (128 commits)
      kasan: move kasan_report() into report.c
      mm/mm_init.c: report kasan-tag information stored in page->flags
      ubsan: entirely disable alignment checks under UBSAN_TRAP
      kasan: fix clang compilation warning due to stack protector
      x86/mm: remove vmalloc faulting
      mm: remove vmalloc_sync_(un)mappings()
      x86/mm/32: implement arch_sync_kernel_mappings()
      x86/mm/64: implement arch_sync_kernel_mappings()
      mm/ioremap: track which page-table levels were modified
      mm/vmalloc: track which page-table levels were modified
      mm: add functions to track page directory modifications
      s390: use __vmalloc_node in stack_alloc
      powerpc: use __vmalloc_node in alloc_vm_stack
      arm64: use __vmalloc_node in arch_alloc_vmap_stack
      mm: remove vmalloc_user_node_flags
      mm: switch the test_vmalloc module to use __vmalloc_node
      mm: remove __vmalloc_node_flags_caller
      mm: remove both instances of __vmalloc_node_flags
      mm: remove the prot argument to __vmalloc_node
      mm: remove the pgprot argument to __vmalloc
      ...

commit cca98e9f8b5ebcd9640846a675172578249b11a0
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jun 1 21:51:32 2020 -0700

    mm: enforce that vmap can't map pages executable
    
    To help enforcing the W^X protection don't allow remapping existing pages
    as executable.
    
    x86 bits from Peter Zijlstra, arm64 bits from Mark Rutland.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Mark Rutland <mark.rutland@arm.com>.
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: David Airlie <airlied@linux.ie>
    Cc: Gao Xiang <xiang@kernel.org>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Haiyang Zhang <haiyangz@microsoft.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "K. Y. Srinivasan" <kys@microsoft.com>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Michael Kelley <mikelley@microsoft.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Nitin Gupta <ngupta@vflare.org>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Sakari Ailus <sakari.ailus@linux.intel.com>
    Cc: Stephen Hemminger <sthemmin@microsoft.com>
    Cc: Sumit Semwal <sumit.semwal@linaro.org>
    Cc: Wei Liu <wei.liu@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200414131348.444715-20-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 538c85e62f86..47095216d6a8 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -407,6 +407,9 @@ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
 #define __pgprot_modify(prot,mask,bits) \
 	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
 
+#define pgprot_nx(prot) \
+	__pgprot_modify(prot, 0, PTE_PXN)
+
 /*
  * Mark the prot value as uncacheable and unbufferable.
  */

commit d27865279f12035c730818aa1a0280fada866a37
Merge: 342403bcb4df a4eb355a3fda
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:00:51 2020 +0100

    Merge branch 'for-next/bti' into for-next/core
    
    Support for Branch Target Identification (BTI) in user and kernel
    (Mark Brown and others)
    * for-next/bti: (39 commits)
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      arm64: bti: Fix support for userspace only BTI
      arm64: kconfig: Update and comment GCC version check for kernel BTI
      arm64: vdso: Map the vDSO text with guarded pages when built for BTI
      arm64: vdso: Force the vDSO to be linked as BTI when built for BTI
      arm64: vdso: Annotate for BTI
      arm64: asm: Provide a mechanism for generating ELF note for BTI
      arm64: bti: Provide Kconfig for kernel mode BTI
      arm64: mm: Mark executable text as guarded pages
      arm64: bpf: Annotate JITed code for BTI
      arm64: Set GP bit in kernel page tables to enable BTI for the kernel
      arm64: asm: Override SYM_FUNC_START when building the kernel with BTI
      arm64: bti: Support building kernel C code using BTI
      arm64: Document why we enable PAC support for leaf functions
      arm64: insn: Report PAC and BTI instructions as skippable
      arm64: insn: Don't assume unrecognized HINTs are skippable
      arm64: insn: Provide a better name for aarch64_insn_is_nop()
      arm64: insn: Add constants for new HINT instruction decode
      arm64: Disable old style assembly annotations
      ...

commit 9d2d75ede59bc1edd8561f2ee9d4702a5ea0ae30
Author: Gavin Shan <gshan@redhat.com>
Date:   Tue Apr 28 09:57:00 2020 +1000

    arm64/kernel: Fix range on invalidating dcache for boot page tables
    
    Prior to commit 8eb7e28d4c642c31 ("arm64/mm: move runtime pgds to
    rodata"), idmap_pgd_dir, tramp_pg_dir, reserved_ttbr0, swapper_pg_dir,
    and init_pg_dir were contiguous at the end of the kernel image. The
    maintenance at the end of __create_page_tables assumed these were
    contiguous, and affected everything from the start of idmap_pg_dir
    to the end of init_pg_dir.
    
    That commit moved all but init_pg_dir into the .rodata section, with
    other data placed between idmap_pg_dir and init_pg_dir, but did not
    update the maintenance. Hence the maintenance is performed on much
    more data than necessary (but as the bootloader previously made this
    clean to the PoC there is no functional problem).
    
    As we only alter idmap_pg_dir, and init_pg_dir, we only need to perform
    maintenance for these. As the other dirs are in .rodata, the bootloader
    will have initialised them as expected and cleaned them to the PoC. The
    kernel will initialize them as necessary after enabling the MMU.
    
    This patch reworks the maintenance to only cover the idmap_pg_dir and
    init_pg_dir to avoid this unnecessary work.
    
    Signed-off-by: Gavin Shan <gshan@redhat.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Link: https://lore.kernel.org/r/20200427235700.112220-1-gshan@redhat.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 8c20e2bd6287..5caff09c6a3a 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -457,6 +457,7 @@ extern pgd_t init_pg_dir[PTRS_PER_PGD];
 extern pgd_t init_pg_end[];
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
+extern pgd_t idmap_pg_end[];
 extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
 
 extern void set_swapper_pgd(pgd_t *pgdp, pgd_t pgd);

commit 68ecabd0e680a4ceaf950ae189a55d4730d10c64
Author: Gavin Shan <gshan@redhat.com>
Date:   Tue Apr 28 09:46:55 2020 +1000

    arm64/mm: Use phys_to_page() to access pgtable memory
    
    The macros {pgd, pud, pmd}_page() retrieves the page struct of the
    corresponding page frame, which is reserved as page table. There
    is already a macro (phys_to_page), defined in memory.h as below,
    to convert the physical address to the page struct. Also, the header
    file (memory.h) has been included by pgtable.h.
    
       #define phys_to_page(phys)      (pfn_to_page(__phys_to_pfn(phys)))
    
    So it's reasonable to use the macro in pgtable.h.
    
    Signed-off-by: Gavin Shan <gshan@redhat.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Link: https://lore.kernel.org/r/20200427234655.111847-1-gshan@redhat.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 538c85e62f86..8c20e2bd6287 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -508,7 +508,7 @@ static inline void pte_unmap(pte_t *pte) { }
 #define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))
 #define pte_clear_fixmap()		clear_fixmap(FIX_PTE)
 
-#define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(__pmd_to_phys(pmd)))
+#define pmd_page(pmd)			phys_to_page(__pmd_to_phys(pmd))
 
 /* use ONLY for statically allocated translation tables */
 #define pte_offset_kimg(dir,addr)	((pte_t *)__phys_to_kimg(pte_offset_phys((dir), (addr))))
@@ -566,7 +566,7 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 #define pmd_set_fixmap_offset(pud, addr)	pmd_set_fixmap(pmd_offset_phys(pud, addr))
 #define pmd_clear_fixmap()		clear_fixmap(FIX_PMD)
 
-#define pud_page(pud)		pfn_to_page(__phys_to_pfn(__pud_to_phys(pud)))
+#define pud_page(pud)			phys_to_page(__pud_to_phys(pud))
 
 /* use ONLY for statically allocated translation tables */
 #define pmd_offset_kimg(dir,addr)	((pmd_t *)__phys_to_kimg(pmd_offset_phys((dir), (addr))))
@@ -624,7 +624,7 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 #define pud_set_fixmap_offset(pgd, addr)	pud_set_fixmap(pud_offset_phys(pgd, addr))
 #define pud_clear_fixmap()		clear_fixmap(FIX_PUD)
 
-#define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(__pgd_to_phys(pgd)))
+#define pgd_page(pgd)			phys_to_page(__pgd_to_phys(pgd))
 
 /* use ONLY for statically allocated translation tables */
 #define pud_offset_kimg(dir,addr)	((pud_t *)__phys_to_kimg(pud_offset_phys((dir), (addr))))

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 538c85e62f86..4fbf516d8cb2 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -660,7 +660,7 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
-			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
+			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE | PTE_GP;
 	/* preserve the hardware dirty information */
 	if (pte_hw_dirty(pte))
 		pte = pte_mkdirty(pte);

commit 8aa82df3c123129025a364d8f823929cc488b834
Author: Steven Price <steven.price@arm.com>
Date:   Mon Feb 3 17:35:14 2020 -0800

    arm64: mm: add p?d_leaf() definitions
    
    walk_page_range() is going to be allowed to walk page tables other than
    those of user space.  For this it needs to know when it has reached a
    'leaf' entry in the page tables.  This information will be provided by the
    p?d_leaf() functions/macros.
    
    For arm64, we already have p?d_sect() macros which we can reuse for
    p?d_leaf().
    
    pud_sect() is defined as a dummy function when CONFIG_PGTABLE_LEVELS < 3
    or CONFIG_ARM64_64K_PAGES is defined.  However when the kernel is
    configured this way then architecturally it isn't allowed to have a large
    page at this level, and any code using these page walking macros is
    implicitly relying on the page size/number of levels being the same as the
    kernel.  So it is safe to reuse this for p?d_leaf() as it is an
    architectural restriction.
    
    Link: http://lkml.kernel.org/r/20191218162402.45610-5-steven.price@arm.com
    Signed-off-by: Steven Price <steven.price@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Alexandre Ghiti <alex@ghiti.fr>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: "Liang, Kan" <kan.liang@linux.intel.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Zong Li <zong.li@sifive.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index cd5de0e40bfa..538c85e62f86 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -441,6 +441,7 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 				 PMD_TYPE_TABLE)
 #define pmd_sect(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
 				 PMD_TYPE_SECT)
+#define pmd_leaf(pmd)		pmd_sect(pmd)
 
 #if defined(CONFIG_ARM64_64K_PAGES) || CONFIG_PGTABLE_LEVELS < 3
 static inline bool pud_sect(pud_t pud) { return false; }
@@ -525,6 +526,7 @@ static inline void pte_unmap(pte_t *pte) { }
 #define pud_none(pud)		(!pud_val(pud))
 #define pud_bad(pud)		(!(pud_val(pud) & PUD_TABLE_BIT))
 #define pud_present(pud)	pte_present(pud_pte(pud))
+#define pud_leaf(pud)		pud_sect(pud)
 #define pud_valid(pud)		pte_valid(pud_pte(pud))
 
 static inline void set_pud(pud_t *pudp, pud_t pud)

commit 24cecc37746393432d994c0dbc251fb9ac7c5d72
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jan 6 14:35:39 2020 +0000

    arm64: Revert support for execute-only user mappings
    
    The ARMv8 64-bit architecture supports execute-only user permissions by
    clearing the PTE_USER and PTE_UXN bits, practically making it a mostly
    privileged mapping but from which user running at EL0 can still execute.
    
    The downside, however, is that the kernel at EL1 inadvertently reading
    such mapping would not trip over the PAN (privileged access never)
    protection.
    
    Revert the relevant bits from commit cab15ce604e5 ("arm64: Introduce
    execute-only page access permissions") so that PROT_EXEC implies
    PROT_READ (and therefore PTE_USER) until the architecture gains proper
    support for execute-only user mappings.
    
    Fixes: cab15ce604e5 ("arm64: Introduce execute-only page access permissions")
    Cc: <stable@vger.kernel.org> # 4.9.x-
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5d15b4735a0e..cd5de0e40bfa 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -96,12 +96,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
 
 #define pte_valid(pte)		(!!(pte_val(pte) & PTE_VALID))
-/*
- * Execute-only user mappings do not have the PTE_USER bit set. All valid
- * kernel mappings have the PTE_UXN bit set.
- */
 #define pte_valid_not_user(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_UXN)) == (PTE_VALID | PTE_UXN))
+	((pte_val(pte) & (PTE_VALID | PTE_USER)) == PTE_VALID)
 #define pte_valid_young(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_AF)) == (PTE_VALID | PTE_AF))
 #define pte_valid_user(pte) \
@@ -117,8 +113,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 
 /*
  * p??_access_permitted() is true for valid user mappings (subject to the
- * write permission check) other than user execute-only which do not have the
- * PTE_USER bit set. PROT_NONE mappings do not have the PTE_VALID bit set.
+ * write permission check). PROT_NONE mappings do not have the PTE_VALID bit
+ * set.
  */
 #define pte_access_permitted(pte, write) \
 	(pte_valid_user(pte) && (!(write) || pte_write(pte)))

commit 4ba380f61624113395bebdc2f9f6da990a0738f9
Merge: e25645b181ae d8e85e144bbe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 15:39:19 2019 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Apart from the arm64-specific bits (core arch and perf, new arm64
      selftests), it touches the generic cow_user_page() (reviewed by
      Kirill) together with a macro for x86 to preserve the existing
      behaviour on this architecture.
    
      Summary:
    
       - On ARMv8 CPUs without hardware updates of the access flag, avoid
         failing cow_user_page() on PFN mappings if the pte is old. The
         patches introduce an arch_faults_on_old_pte() macro, defined as
         false on x86. When true, cow_user_page() makes the pte young before
         attempting __copy_from_user_inatomic().
    
       - Covert the synchronous exception handling paths in
         arch/arm64/kernel/entry.S to C.
    
       - FTRACE_WITH_REGS support for arm64.
    
       - ZONE_DMA re-introduced on arm64 to support Raspberry Pi 4
    
       - Several kselftest cases specific to arm64, together with a
         MAINTAINERS update for these files (moved to the ARM64 PORT entry).
    
       - Workaround for a Neoverse-N1 erratum where the CPU may fetch stale
         instructions under certain conditions.
    
       - Workaround for Cortex-A57 and A72 errata where the CPU may
         speculatively execute an AT instruction and associate a VMID with
         the wrong guest page tables (corrupting the TLB).
    
       - Perf updates for arm64: additional PMU topologies on HiSilicon
         platforms, support for CCN-512 interconnect, AXI ID filtering in
         the IMX8 DDR PMU, support for the CCPI2 uncore PMU in ThunderX2.
    
       - GICv3 optimisation to avoid a heavy barrier when accessing the
         ICC_PMR_EL1 register.
    
       - ELF HWCAP documentation updates and clean-up.
    
       - SMC calling convention conduit code clean-up.
    
       - KASLR diagnostics printed during boot
    
       - NVIDIA Carmel CPU added to the KPTI whitelist
    
       - Some arm64 mm clean-ups: use generic free_initrd_mem(), remove
         stale macro, simplify calculation in __create_pgd_mapping(), typos.
    
       - Kconfig clean-ups: CMDLINE_FORCE to depend on CMDLINE, choice for
         endinanness to help with allmodconfig"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (93 commits)
      arm64: Kconfig: add a choice for endianness
      kselftest: arm64: fix spelling mistake "contiguos" -> "contiguous"
      arm64: Kconfig: make CMDLINE_FORCE depend on CMDLINE
      MAINTAINERS: Add arm64 selftests to the ARM64 PORT entry
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
      drivers/perf: hisi: update the sccl_id/ccl_id for certain HiSilicon platform
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      ...

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit 6767df245f4736d0cf0c6fb7cf9cf94b27414245
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Nov 6 15:41:05 2019 +0000

    arm64: Do not mask out PTE_RDONLY in pte_same()
    
    Following commit 73e86cb03cf2 ("arm64: Move PTE_RDONLY bit handling out
    of set_pte_at()"), the PTE_RDONLY bit is no longer managed by
    set_pte_at() but built into the PAGE_* attribute definitions.
    Consequently, pte_same() must include this bit when checking two PTEs
    for equality.
    
    Remove the arm64-specific pte_same() function, practically reverting
    commit 747a70e60b72 ("arm64: Fix copy-on-write referencing in HugeTLB")
    
    Fixes: 73e86cb03cf2 ("arm64: Move PTE_RDONLY bit handling out of set_pte_at()")
    Cc: <stable@vger.kernel.org> # 4.14.x-
    Cc: Will Deacon <will@kernel.org>
    Cc: Steve Capper <steve.capper@arm.com>
    Reported-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 8330810f699e..565aa45ef134 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -283,23 +283,6 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	set_pte(ptep, pte);
 }
 
-#define __HAVE_ARCH_PTE_SAME
-static inline int pte_same(pte_t pte_a, pte_t pte_b)
-{
-	pteval_t lhs, rhs;
-
-	lhs = pte_val(pte_a);
-	rhs = pte_val(pte_b);
-
-	if (pte_present(pte_a))
-		lhs &= ~PTE_RDONLY;
-
-	if (pte_present(pte_b))
-		rhs &= ~PTE_RDONLY;
-
-	return (lhs == rhs);
-}
-
 /*
  * Huge pte definitions.
  */

commit a5315819c5e7e50b2b457b60aaf2cc61d76888a2
Author: Mark Brown <broonie@kernel.org>
Date:   Thu Oct 24 13:01:43 2019 +0100

    arm64: pgtable: Correct typo in comment
    
    vmmemmap -> vmemmap
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7576df00eb50..4438a23f969c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -17,7 +17,7 @@
  * VMALLOC range.
  *
  * VMALLOC_START: beginning of the kernel vmalloc space
- * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space
+ * VMALLOC_END: extends to the available space below vmemmap, PCI I/O space
  *	and fixed mappings
  */
 #define VMALLOC_START		(MODULES_END)

commit 6af31226d0394691f5562eca0134262bb935fa9c
Author: Jia He <justin.he@arm.com>
Date:   Fri Oct 11 22:09:37 2019 +0800

    arm64: mm: implement arch_faults_on_old_pte() on arm64
    
    On arm64 without hardware Access Flag, copying from user will fail because
    the pte is old and cannot be marked young. So we always end up with zeroed
    page after fork() + CoW for pfn mappings. We don't always have a
    hardware-managed Access Flag on arm64.
    
    Hence implement arch_faults_on_old_pte on arm64 to indicate that it might
    cause page fault when accessing old pte.
    
    Signed-off-by: Jia He <justin.he@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7576df00eb50..e96fb82f62de 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -885,6 +885,20 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 #define phys_to_ttbr(addr)	(addr)
 #endif
 
+/*
+ * On arm64 without hardware Access Flag, copying from user will fail because
+ * the pte is old and cannot be marked young. So we always end up with zeroed
+ * page after fork() + CoW for pfn mappings. We don't always have a
+ * hardware-managed access flag on arm64.
+ */
+static inline bool arch_faults_on_old_pte(void)
+{
+	WARN_ON(preemptible());
+
+	return !cpu_has_hw_af();
+}
+#define arch_faults_on_old_pte arch_faults_on_old_pte
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */

commit 86109a691a454e08cbe0356400268cb2a81f1997
Author: Chris von Recklinghausen <crecklin@redhat.com>
Date:   Thu Oct 10 13:22:47 2019 -0400

    arm64: Fix kcore macros after 52-bit virtual addressing fallout
    
    We export the entire kernel address space (i.e. the whole of the TTBR1
    address range) via /proc/kcore. The kc_vaddr_to_offset() and
    kc_offset_to_vaddr() macros are intended to convert between a kernel
    virtual address and its offset relative to the start of the TTBR1
    address space.
    
    Prior to commit:
    
      14c127c957c1c607 ("arm64: mm: Flip kernel VA space")
    
    ... the offset was calculated relative to VA_START, which at the time
    was the start of the TTBR1 address space. At this time, PAGE_OFFSET
    pointed to the high half of the TTBR1 address space where arm64's
    linear map lived.
    
    That commit swapped the position of VA_START and PAGE_OFFSET, but
    failed to update kc_vaddr_to_offset() or kc_offset_to_vaddr(), so
    since then the two macros behave incorrectly.
    
    Note that VA_START was subsequently renamed to PAGE_END in commit:
    
      77ad4ce69321abbe ("arm64: memory: rename VA_START to PAGE_END")
    
    As the generic implementations of the two macros calculate the offset
    relative to PAGE_OFFSET (which is now the start of the TTBR1 address
    space), we can delete the arm64 implementation and use those.
    
    Fixes: 14c127c957c1c607 ("arm64: mm: Flip kernel VA space")
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Chris von Recklinghausen <crecklin@redhat.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7576df00eb50..8330810f699e 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -876,9 +876,6 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 
 #define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)
 
-#define kc_vaddr_to_offset(v)	((v) & ~PAGE_END)
-#define kc_offset_to_vaddr(o)	((o) | PAGE_END)
-
 #ifdef CONFIG_ARM64_PA_BITS_52
 #define phys_to_ttbr(addr)	(((addr) | ((addr) >> 46)) & TTBR_BADDR_MASK_52)
 #else

commit 782de70c42930baae55234f3df0dc90774924447
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Sep 23 15:35:31 2019 -0700

    mm: consolidate pgtable_cache_init() and pgd_cache_init()
    
    Both pgtable_cache_init() and pgd_cache_init() are used to initialize kmem
    cache for page table allocations on several architectures that do not use
    PAGE_SIZE tables for one or more levels of the page table hierarchy.
    
    Most architectures do not implement these functions and use __weak default
    NOP implementation of pgd_cache_init().  Since there is no such default
    for pgtable_cache_init(), its empty stub is duplicated among most
    architectures.
    
    Rename the definitions of pgd_cache_init() to pgtable_cache_init() and
    drop empty stubs of pgtable_cache_init().
    
    Link: http://lkml.kernel.org/r/1566457046-22637-1-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Will Deacon <will@kernel.org>         [arm64]
    Acked-by: Thomas Gleixner <tglx@linutronix.de>  [x86]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 57427d17580e..7576df00eb50 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -861,8 +861,6 @@ extern int kern_addr_valid(unsigned long addr);
 
 #include <asm-generic/pgtable.h>
 
-static inline void pgtable_cache_init(void) { }
-
 /*
  * On AArch64, the cache coherency is handled via the set_pte_at() function.
  */

commit 671df189537883f36cf9c7d4f9495bfac0f86627
Merge: c9fe5630dae1 c7d9eccb3c1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 19 13:27:23 2019 -0700

    Merge tag 'dma-mapping-5.4' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - add dma-mapping and block layer helpers to take care of IOMMU merging
       for mmc plus subsequent fixups (Yoshihiro Shimoda)
    
     - rework handling of the pgprot bits for remapping (me)
    
     - take care of the dma direct infrastructure for swiotlb-xen (me)
    
     - improve the dma noncoherent remapping infrastructure (me)
    
     - better defaults for ->mmap, ->get_sgtable and ->get_required_mask
       (me)
    
     - cleanup mmaping of coherent DMA allocations (me)
    
     - various misc cleanups (Andy Shevchenko, me)
    
    * tag 'dma-mapping-5.4' of git://git.infradead.org/users/hch/dma-mapping: (41 commits)
      mmc: renesas_sdhi_internal_dmac: Add MMC_CAP2_MERGE_CAPABLE
      mmc: queue: Fix bigger segments usage
      arm64: use asm-generic/dma-mapping.h
      swiotlb-xen: merge xen_unmap_single into xen_swiotlb_unmap_page
      swiotlb-xen: simplify cache maintainance
      swiotlb-xen: use the same foreign page check everywhere
      swiotlb-xen: remove xen_swiotlb_dma_mmap and xen_swiotlb_dma_get_sgtable
      xen: remove the exports for xen_{create,destroy}_contiguous_region
      xen/arm: remove xen_dma_ops
      xen/arm: simplify dma_cache_maint
      xen/arm: use dev_is_dma_coherent
      xen/arm: consolidate page-coherent.h
      xen/arm: use dma-noncoherent.h calls for xen-swiotlb cache maintainance
      arm: remove wrappers for the generic dma remap helpers
      dma-mapping: introduce a dma_common_find_pages helper
      dma-mapping: always use VM_DMA_COHERENT for generic DMA remap
      vmalloc: lift the arm flag for coherent mappings to common code
      dma-mapping: provide a better default ->get_required_mask
      dma-mapping: remove the dma_declare_coherent_memory export
      remoteproc: don't allow modular build
      ...

commit e77fafe9afb53b7f4d8176c5cd5c10c43a905bc8
Merge: 52a5525214d0 e376897f424a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 14:31:40 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Although there isn't tonnes of code in terms of line count, there are
      a fair few headline features which I've noted both in the tag and also
      in the merge commits when I pulled everything together.
    
      The part I'm most pleased with is that we had 35 contributors this
      time around, which feels like a big jump from the usual small group of
      core arm64 arch developers. Hopefully they all enjoyed it so much that
      they'll continue to contribute, but we'll see.
    
      It's probably worth highlighting that we've pulled in a branch from
      the risc-v folks which moves our CPU topology code out to where it can
      be shared with others.
    
      Summary:
    
       - 52-bit virtual addressing in the kernel
    
       - New ABI to allow tagged user pointers to be dereferenced by
         syscalls
    
       - Early RNG seeding by the bootloader
    
       - Improve robustness of SMP boot
    
       - Fix TLB invalidation in light of recent architectural
         clarifications
    
       - Support for i.MX8 DDR PMU
    
       - Remove direct LSE instruction patching in favour of static keys
    
       - Function error injection using kprobes
    
       - Support for the PPTT "thread" flag introduced by ACPI 6.3
    
       - Move PSCI idle code into proper cpuidle driver
    
       - Relaxation of implicit I/O memory barriers
    
       - Build with RELR relocations when toolchain supports them
    
       - Numerous cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (114 commits)
      arm64: remove __iounmap
      arm64: atomics: Use K constraint when toolchain appears to support it
      arm64: atomics: Undefine internal macros after use
      arm64: lse: Make ARM64_LSE_ATOMICS depend on JUMP_LABEL
      arm64: asm: Kill 'asm/atomic_arch.h'
      arm64: lse: Remove unused 'alt_lse' assembly macro
      arm64: atomics: Remove atomic_ll_sc compilation unit
      arm64: avoid using hard-coded registers for LSE atomics
      arm64: atomics: avoid out-of-line ll/sc atomics
      arm64: Use correct ll/sc atomic constraints
      jump_label: Don't warn on __exit jump entries
      docs/perf: Add documentation for the i.MX8 DDR PMU
      perf/imx_ddr: Add support for AXI ID filtering
      arm64: kpti: ensure patched kernel text is fetched from PoU
      arm64: fix fixmap copy for 16K pages and 48-bit VA
      perf/smmuv3: Validate groups for global filtering
      perf/smmuv3: Validate group size
      arm64: Relax Documentation/arm64/tagged-pointers.rst
      arm64: kvm: Replace hardcoded '1' with SYS_PAR_EL1_F
      arm64: mm: Ignore spurious translation faults taken from the kernel
      ...

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit 3e4e1d3fb89193cc072858e1469d6f2926c603f7
Author: Christoph Hellwig <hch@lst.de>
Date:   Sat Aug 3 12:38:31 2019 +0300

    arm64: document the choice of page attributes for pgprot_dmacoherent
    
    Based on an email from Will Deacon.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 6700371227d1..fd40fb05eb51 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -435,6 +435,14 @@ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
 #define pgprot_device(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRE) | PTE_PXN | PTE_UXN)
+/*
+ * DMA allocations for non-coherent devices use what the Arm architecture calls
+ * "Normal non-cacheable" memory, which permits speculation, unaligned accesses
+ * and merging of writes.  This is different from "Device-nGnR[nE]" memory which
+ * is intended for MMIO and thus forbids speculation, preserves access size,
+ * requires strict alignment and can also force write responses to come from the
+ * endpoint.
+ */
 #define pgprot_dmacoherent(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, \
 			PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)

commit 419e2f1838819e954071dfa1d1f820ab3386ada1
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Aug 26 09:03:44 2019 +0200

    dma-mapping: remove arch_dma_mmap_pgprot
    
    arch_dma_mmap_pgprot is used for two things:
    
     1) to override the "normal" uncached page attributes for mapping
        memory coherent to devices that can't snoop the CPU caches
     2) to provide the special DMA_ATTR_WRITE_COMBINE semantics on older
        arm systems and some mips platforms
    
    Replace one with the pgprot_dmacoherent macro that is already provided
    by arm and much simpler to use, and lift the DMA_ATTR_WRITE_COMBINE
    handling to common code with an explicit arch opt-in.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>     # m68k
    Acked-by: Paul Burton <paul.burton@mips.com>            # mips

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e09760ece844..6700371227d1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -435,6 +435,10 @@ static inline pmd_t pmd_mkdevmap(pmd_t pmd)
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
 #define pgprot_device(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRE) | PTE_PXN | PTE_UXN)
+#define pgprot_dmacoherent(prot) \
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, \
+			PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
+
 #define __HAVE_PHYS_MEM_ACCESS_PROT
 struct file;
 extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,

commit eb6a4dcce33925ac95023bbe5199474f8db40ba7
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 23 13:03:55 2019 +0100

    arm64: mm: Add ISB instruction to set_pgd()
    
    Commit 6a4cbd63c25a ("Revert "arm64: Remove unnecessary ISBs from
    set_{pte,pmd,pud}"") reintroduced ISB instructions to some of our
    page table setter functions in light of a recent clarification to the
    Armv8 architecture. Although 'set_pgd()' isn't currently used to update
    a live page table, add the ISB instruction there too for consistency
    with the other macros and to provide some future-proofing if we use it
    on live tables in the future.
    
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index feda7294320c..2faa77635942 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -605,6 +605,7 @@ static inline void set_pgd(pgd_t *pgdp, pgd_t pgd)
 
 	WRITE_ONCE(*pgdp, pgd);
 	dsb(ishst);
+	isb();
 }
 
 static inline void pgd_clear(pgd_t *pgdp)

commit d0b7a302d58abe24ed0f32a0672dd4c356bb73db
Author: Will Deacon <will@kernel.org>
Date:   Thu Aug 22 14:58:37 2019 +0100

    Revert "arm64: Remove unnecessary ISBs from set_{pte,pmd,pud}"
    
    This reverts commit 24fe1b0efad4fcdd32ce46cffeab297f22581707.
    
    Commit 24fe1b0efad4fcdd ("arm64: Remove unnecessary ISBs from
    set_{pte,pmd,pud}") removed ISB instructions immediately following updates
    to the page table, on the grounds that they are not required by the
    architecture and a DSB alone is sufficient to ensure that subsequent data
    accesses use the new translation:
    
      DDI0487E_a, B2-128:
    
      | ... no instruction that appears in program order after the DSB
      | instruction can alter any state of the system or perform any part of
      | its functionality until the DSB completes other than:
      |
      | * Being fetched from memory and decoded
      | * Reading the general-purpose, SIMD and floating-point,
      |   Special-purpose, or System registers that are directly or indirectly
      |   read without causing side-effects.
    
    However, the same document also states the following:
    
      DDI0487E_a, B2-125:
    
      | DMB and DSB instructions affect reads and writes to the memory system
      | generated by Load/Store instructions and data or unified cache
      | maintenance instructions being executed by the PE. Instruction fetches
      | or accesses caused by a hardware translation table access are not
      | explicit accesses.
    
    which appears to claim that the DSB alone is insufficient.  Unfortunately,
    some CPU designers have followed the second clause above, whereas in Linux
    we've been relying on the first. This means that our mapping sequence:
    
            MOV     X0, <valid pte>
            STR     X0, [Xptep]     // Store new PTE to page table
            DSB     ISHST
            LDR     X1, [X2]        // Translates using the new PTE
    
    can actually raise a translation fault on the load instruction because the
    translation can be performed speculatively before the page table update and
    then marked as "faulting" by the CPU. For user PTEs, this is ok because we
    can handle the spurious fault, but for kernel PTEs and intermediate table
    entries this results in a panic().
    
    Revert the offending commit to reintroduce the missing barriers.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 24fe1b0efad4fcdd ("arm64: Remove unnecessary ISBs from set_{pte,pmd,pud}")
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5fdcfe237338..feda7294320c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -220,8 +220,10 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
 	 * or update_mmu_cache() have the necessary barriers.
 	 */
-	if (pte_valid_not_user(pte))
+	if (pte_valid_not_user(pte)) {
 		dsb(ishst);
+		isb();
+	}
 }
 
 extern void __sync_icache_dcache(pte_t pteval);
@@ -481,8 +483,10 @@ static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 
 	WRITE_ONCE(*pmdp, pmd);
 
-	if (pmd_valid(pmd))
+	if (pmd_valid(pmd)) {
 		dsb(ishst);
+		isb();
+	}
 }
 
 static inline void pmd_clear(pmd_t *pmdp)
@@ -540,8 +544,10 @@ static inline void set_pud(pud_t *pudp, pud_t pud)
 
 	WRITE_ONCE(*pudp, pud);
 
-	if (pud_valid(pud))
+	if (pud_valid(pud)) {
 		dsb(ishst);
+		isb();
+	}
 }
 
 static inline void pud_clear(pud_t *pudp)

commit 77ad4ce69321abbe26ec92b2a2691a66531eb688
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Aug 14 14:28:48 2019 +0100

    arm64: memory: rename VA_START to PAGE_END
    
    Prior to commit:
    
      14c127c957c1c607 ("arm64: mm: Flip kernel VA space")
    
    ... VA_START described the start of the TTBR1 address space for a given
    VA size described by VA_BITS, where all kernel mappings began.
    
    Since that commit, VA_START described a portion midway through the
    address space, where the linear map ends and other kernel mappings
    begin.
    
    To avoid confusion, let's rename VA_START to PAGE_END, making it clear
    that it's not the start of the TTBR1 address space and implying that
    it's related to PAGE_OFFSET. Comments and other mnemonics are updated
    accordingly, along with a typo fix in the decription of VMEMMAP_SIZE.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 4a695b9ee0f0..979e24fadf35 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -856,8 +856,8 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 
 #define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)
 
-#define kc_vaddr_to_offset(v)	((v) & ~VA_START)
-#define kc_offset_to_vaddr(o)	((o) | VA_START)
+#define kc_vaddr_to_offset(v)	((v) & ~PAGE_END)
+#define kc_offset_to_vaddr(o)	((o) | PAGE_END)
 
 #ifdef CONFIG_ARM64_PA_BITS_52
 #define phys_to_ttbr(addr)	(((addr) | ((addr) >> 46)) & TTBR_BADDR_MASK_52)

commit c8b6d2ccf9b10ce872cdea037f9685804440bb7e
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:20 2019 +0100

    arm64: mm: Separate out vmemmap
    
    vmemmap is a preprocessor definition that depends on a variable,
    memstart_addr. In a later patch we will need to expand the size of
    the VMEMMAP region and optionally modify vmemmap depending upon
    whether or not hardware support is available for 52-bit virtual
    addresses.
    
    This patch changes vmemmap to be a variable. As the old definition
    depended on a variable load, this should not affect performance
    noticeably.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 046b811309bb..4a695b9ee0f0 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -23,8 +23,6 @@
 #define VMALLOC_START		(MODULES_END)
 #define VMALLOC_END		(- PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
-#define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
-
 #define FIRST_USER_ADDRESS	0UL
 
 #ifndef __ASSEMBLY__
@@ -35,6 +33,8 @@
 #include <linux/mm_types.h>
 #include <linux/sched.h>
 
+extern struct page *vmemmap;
+
 extern void __pte_error(const char *file, int line, unsigned long val);
 extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pud_error(const char *file, int line, unsigned long val);

commit 14c127c957c1c6070647c171e72f06e0db275ebf
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:14 2019 +0100

    arm64: mm: Flip kernel VA space
    
    In order to allow for a KASAN shadow that changes size at boot time, one
    must fix the KASAN_SHADOW_END for both 48 & 52-bit VAs and "grow" the
    start address. Also, it is highly desirable to maintain the same
    function addresses in the kernel .text between VA sizes. Both of these
    requirements necessitate us to flip the kernel address space halves s.t.
    the direct linear map occupies the lower addresses.
    
    This patch puts the direct linear map in the lower addresses of the
    kernel VA range and everything else in the higher ranges.
    
    We need to adjust:
     *) KASAN shadow region placement logic,
     *) KASAN_SHADOW_OFFSET computation logic,
     *) virt_to_phys, phys_to_virt checks,
     *) page table dumper.
    
    These are all small changes, that need to take place atomically, so they
    are bundled into this commit.
    
    As part of the re-arrangement, a guard region of 2MB (to preserve
    alignment for fixed map) is added after the vmemmap. Otherwise the
    vmemmap could intersect with IS_ERR pointers.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5fdcfe237338..046b811309bb 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -21,7 +21,7 @@
  *	and fixed mappings
  */
 #define VMALLOC_START		(MODULES_END)
-#define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
+#define VMALLOC_END		(- PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
 

commit 30e235389faadb9e3d918887b1f126155d7d761d
Author: Jia He <justin.he@arm.com>
Date:   Wed Aug 7 12:58:51 2019 +0800

    arm64: mm: add missing PTE_SPECIAL in pte_mkdevmap on arm64
    
    Without this patch, the MAP_SYNC test case will cause a print_bad_pte
    warning on arm64 as follows:
    
    [   25.542693] BUG: Bad page map in process mapdax333 pte:2e8000448800f53 pmd:41ff5f003
    [   25.546360] page:ffff7e0010220000 refcount:1 mapcount:-1 mapping:ffff8003e29c7440 index:0x0
    [   25.550281] ext4_dax_aops
    [   25.550282] name:"__aaabbbcccddd__"
    [   25.551553] flags: 0x3ffff0000001002(referenced|reserved)
    [   25.555802] raw: 03ffff0000001002 ffff8003dfffa908 0000000000000000 ffff8003e29c7440
    [   25.559446] raw: 0000000000000000 0000000000000000 00000001fffffffe 0000000000000000
    [   25.563075] page dumped because: bad pte
    [   25.564938] addr:0000ffffbe05b000 vm_flags:208000fb anon_vma:0000000000000000 mapping:ffff8003e29c7440 index:0
    [   25.574272] file:__aaabbbcccddd__ fault:ext4_dax_fault mmmmap:ext4_file_mmap readpage:0x0
    [   25.578799] CPU: 1 PID: 1180 Comm: mapdax333 Not tainted 5.2.0+ #21
    [   25.581702] Hardware name: QEMU KVM Virtual Machine, BIOS 0.0.0 02/06/2015
    [   25.585624] Call trace:
    [   25.587008]  dump_backtrace+0x0/0x178
    [   25.588799]  show_stack+0x24/0x30
    [   25.590328]  dump_stack+0xa8/0xcc
    [   25.591901]  print_bad_pte+0x18c/0x218
    [   25.593628]  unmap_page_range+0x778/0xc00
    [   25.595506]  unmap_single_vma+0x94/0xe8
    [   25.597304]  unmap_vmas+0x90/0x108
    [   25.598901]  unmap_region+0xc0/0x128
    [   25.600566]  __do_munmap+0x284/0x3f0
    [   25.602245]  __vm_munmap+0x78/0xe0
    [   25.603820]  __arm64_sys_munmap+0x34/0x48
    [   25.605709]  el0_svc_common.constprop.0+0x78/0x168
    [   25.607956]  el0_svc_handler+0x34/0x90
    [   25.609698]  el0_svc+0x8/0xc
    [...]
    
    The root cause is in _vm_normal_page, without the PTE_SPECIAL bit,
    the return value will be incorrectly set to pfn_to_page(pfn) instead
    of NULL. Besides, this patch also rewrite the pmd_mkdevmap to avoid
    setting PTE_SPECIAL for pmd
    
    The MAP_SYNC test case is as follows(Provided by Yibo Cai)
    $#include <stdio.h>
    $#include <string.h>
    $#include <unistd.h>
    $#include <sys/file.h>
    $#include <sys/mman.h>
    
    $#ifndef MAP_SYNC
    $#define MAP_SYNC 0x80000
    $#endif
    
    /* mount -o dax /dev/pmem0 /mnt */
    $#define F "/mnt/__aaabbbcccddd__"
    
    int main(void)
    {
        int fd;
        char buf[4096];
        void *addr;
    
        if ((fd = open(F, O_CREAT|O_TRUNC|O_RDWR, 0644)) < 0) {
            perror("open1");
            return 1;
        }
    
        if (write(fd, buf, 4096) != 4096) {
            perror("lseek");
            return 1;
        }
    
        addr = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_SYNC, fd, 0);
        if (addr == MAP_FAILED) {
            perror("mmap");
            printf("did you mount with '-o dax'?\n");
            return 1;
        }
    
        memset(addr, 0x55, 4096);
    
        if (munmap(addr, 4096) == -1) {
            perror("munmap");
            return 1;
        }
    
        close(fd);
    
        return 0;
    }
    
    Fixes: 73b20c84d42d ("arm64: mm: implement pte_devmap support")
    Reported-by: Yibo Cai <Yibo.Cai@arm.com>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Robin Murphy <Robin.Murphy@arm.com>
    Signed-off-by: Jia He <justin.he@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5fdcfe237338..e09760ece844 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -209,7 +209,7 @@ static inline pmd_t pmd_mkcont(pmd_t pmd)
 
 static inline pte_t pte_mkdevmap(pte_t pte)
 {
-	return set_pte_bit(pte, __pgprot(PTE_DEVMAP));
+	return set_pte_bit(pte, __pgprot(PTE_DEVMAP | PTE_SPECIAL));
 }
 
 static inline void set_pte(pte_t *ptep, pte_t pte)
@@ -396,7 +396,10 @@ static inline int pmd_protnone(pmd_t pmd)
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define pmd_devmap(pmd)		pte_devmap(pmd_pte(pmd))
 #endif
-#define pmd_mkdevmap(pmd)	pte_pmd(pte_mkdevmap(pmd_pte(pmd)))
+static inline pmd_t pmd_mkdevmap(pmd_t pmd)
+{
+	return pte_pmd(set_pte_bit(pmd_pte(pmd), __pgprot(PTE_DEVMAP)));
+}
 
 #define __pmd_to_phys(pmd)	__pte_to_phys(pmd_pte(pmd))
 #define __phys_to_pmd_val(phys)	__phys_to_pte_val(phys)

commit 7d4e2dcf311d3b98421d1f119efe5964cafa32fc
Author: Qian Cai <cai@lca.pw>
Date:   Wed Jul 31 16:05:45 2019 -0400

    arm64/mm: fix variable 'pud' set but not used
    
    GCC throws a warning,
    
    arch/arm64/mm/mmu.c: In function 'pud_free_pmd_page':
    arch/arm64/mm/mmu.c:1033:8: warning: variable 'pud' set but not used
    [-Wunused-but-set-variable]
      pud_t pud;
            ^~~
    
    because pud_table() is a macro and compiled away. Fix it by making it a
    static inline function and for pud_sect() as well.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3f5461f7b560..5fdcfe237338 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -447,8 +447,8 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 				 PMD_TYPE_SECT)
 
 #if defined(CONFIG_ARM64_64K_PAGES) || CONFIG_PGTABLE_LEVELS < 3
-#define pud_sect(pud)		(0)
-#define pud_table(pud)		(1)
+static inline bool pud_sect(pud_t pud) { return false; }
+static inline bool pud_table(pud_t pud) { return true; }
 #else
 #define pud_sect(pud)		((pud_val(pud) & PUD_TYPE_MASK) == \
 				 PUD_TYPE_SECT)

commit 5a9060e9437be47f92f85a2b5c7cd73314d080e8
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Jul 2 11:02:55 2019 +0530

    arm64: mm: Drop pte_huge()
    
    This helper is required from generic huge_pte_alloc() which is available
    when arch subscribes ARCH_WANT_GENERAL_HUGETLB. arm64 implements it's own
    huge_pte_alloc() and does not depend on the generic definition. Drop this
    helper which is redundant on arm64.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Steve Capper <Steve.Capper@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 87a4b2ddc1a1..3f5461f7b560 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -301,7 +301,6 @@ static inline int pte_same(pte_t pte_a, pte_t pte_b)
 /*
  * Huge pte definitions.
  */
-#define pte_huge(pte)		(!(pte_val(pte) & PTE_TABLE_BIT))
 #define pte_mkhuge(pte)		(__pte(pte_val(pte) & ~PTE_TABLE_BIT))
 
 /*

commit 73b20c84d42de14673a987816dd4d132c7b1f801
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 16 16:30:51 2019 -0700

    arm64: mm: implement pte_devmap support
    
    In order for things like get_user_pages() to work on ZONE_DEVICE memory,
    we need a software PTE bit to identify device-backed PFNs.  Hook this up
    along with the relevant helpers to join in with ARCH_HAS_PTE_DEVMAP.
    
    [robin.murphy@arm.com: build fixes]
      Link: http://lkml.kernel.org/r/13026c4e64abc17133bbfa07d7731ec6691c0bcd.1559050949.git.robin.murphy@arm.com
    Link: http://lkml.kernel.org/r/817d92886fc3b33bcbf6e105ee83a74babb3a5aa.1558547956.git.robin.murphy@arm.com
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3052381baaeb..87a4b2ddc1a1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -79,6 +79,7 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_user_exec(pte)	(!(pte_val(pte) & PTE_UXN))
 #define pte_cont(pte)		(!!(pte_val(pte) & PTE_CONT))
+#define pte_devmap(pte)		(!!(pte_val(pte) & PTE_DEVMAP))
 
 #define pte_cont_addr_end(addr, end)						\
 ({	unsigned long __boundary = ((addr) + CONT_PTE_SIZE) & CONT_PTE_MASK;	\
@@ -206,6 +207,11 @@ static inline pmd_t pmd_mkcont(pmd_t pmd)
 	return __pmd(pmd_val(pmd) | PMD_SECT_CONT);
 }
 
+static inline pte_t pte_mkdevmap(pte_t pte)
+{
+	return set_pte_bit(pte, __pgprot(PTE_DEVMAP));
+}
+
 static inline void set_pte(pte_t *ptep, pte_t pte)
 {
 	WRITE_ONCE(*ptep, pte);
@@ -388,6 +394,11 @@ static inline int pmd_protnone(pmd_t pmd)
 
 #define pmd_mkhuge(pmd)		(__pmd(pmd_val(pmd) & ~PMD_TABLE_BIT))
 
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#define pmd_devmap(pmd)		pte_devmap(pmd_pte(pmd))
+#endif
+#define pmd_mkdevmap(pmd)	pte_pmd(pte_mkdevmap(pmd_pte(pmd)))
+
 #define __pmd_to_phys(pmd)	__pte_to_phys(pmd_pte(pmd))
 #define __phys_to_pmd_val(phys)	__phys_to_pte_val(phys)
 #define pmd_pfn(pmd)		((__pmd_to_phys(pmd) & PMD_MASK) >> PAGE_SHIFT)
@@ -673,6 +684,16 @@ static inline int pmdp_set_access_flags(struct vm_area_struct *vma,
 {
 	return ptep_set_access_flags(vma, address, (pte_t *)pmdp, pmd_pte(entry), dirty);
 }
+
+static inline int pud_devmap(pud_t pud)
+{
+	return 0;
+}
+
+static inline int pgd_devmap(pgd_t pgd)
+{
+	return 0;
+}
 #endif
 
 /*

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit c884d8ac7ffccc094e9674a3eb3be90d3b296c0a
Merge: 05512b0f4652 c891f3b97964
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 21 09:58:42 2019 -0700

    Merge tag 'spdx-5.2-rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/spdx
    
    Pull still more SPDX updates from Greg KH:
     "Another round of SPDX updates for 5.2-rc6
    
      Here is what I am guessing is going to be the last "big" SPDX update
      for 5.2. It contains all of the remaining GPLv2 and GPLv2+ updates
      that were "easy" to determine by pattern matching. The ones after this
      are going to be a bit more difficult and the people on the spdx list
      will be discussing them on a case-by-case basis now.
    
      Another 5000+ files are fixed up, so our overall totals are:
            Files checked:            64545
            Files with SPDX:          45529
    
      Compared to the 5.1 kernel which was:
            Files checked:            63848
            Files with SPDX:          22576
    
      This is a huge improvement.
    
      Also, we deleted another 20000 lines of boilerplate license crud,
      always nice to see in a diffstat"
    
    * tag 'spdx-5.2-rc6' of git://git.kernel.org/pub/scm/linux/kernel/git/gregkh/spdx: (65 commits)
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 507
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 506
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 505
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 504
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 503
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 502
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 501
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 499
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 498
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 497
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 496
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 495
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 491
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 490
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 489
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 488
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 487
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 486
      treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 485
      ...

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2c41b04708fe..36e4302483a0 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_PGTABLE_H
 #define __ASM_PGTABLE_H

commit 615c48ad8f4275b4d39fa57df68d4015078be201
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Tue Jun 18 10:32:29 2019 +0300

    arm64/mm: don't initialize pgd_cache twice
    
    When PGD_SIZE != PAGE_SIZE, arm64 uses kmem_cache for allocation of PGD
    memory. That cache was initialized twice: first through
    pgtable_cache_init() alias and then as an override for weak
    pgd_cache_init().
    
    Remove the alias from pgtable_cache_init() and keep the only pgd_cache
    initialization in pgd_cache_init().
    
    Fixes: caa841360134 ("x86/mm: Initialize PGD cache during mm initialization")
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2c41b04708fe..851c68dc6d61 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -812,8 +812,7 @@ extern int kern_addr_valid(unsigned long addr);
 
 #include <asm-generic/pgtable.h>
 
-void pgd_cache_init(void);
-#define pgtable_cache_init	pgd_cache_init
+static inline void pgtable_cache_init(void) { }
 
 /*
  * On AArch64, the cache coherency is handled via the set_pte_at() function.

commit 9b604722059039a1a3ff69fb8dfd024264046024
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jun 10 13:41:07 2019 +0100

    arm64: mm: avoid redundant READ_ONCE(*ptep)
    
    In set_pte_at(), we read the old pte value so that it can be passed into
    checks for racy hw updates. These checks are only performed for
    CONFIG_DEBUG_VM, and the value is not used otherwise.
    
    Since we read the pte value with READ_ONCE(), the compiler cannot elide
    the redundant read for !CONFIG_DEBUG_VM kernels.
    
    Let's ameliorate matters by moving the read and the checks into a
    helper, __check_racy_pte_update(), which only performs the read when the
    value will be used. This also allows us to reformat the conditions for
    clarity.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 6478dd121228..2023eab19d73 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -246,29 +246,42 @@ extern void __sync_icache_dcache(pte_t pteval);
  *
  *   PTE_DIRTY || (PTE_WRITE && !PTE_RDONLY)
  */
-static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
-			      pte_t *ptep, pte_t pte)
+
+static inline void __check_racy_pte_update(struct mm_struct *mm, pte_t *ptep,
+					   pte_t pte)
 {
 	pte_t old_pte;
 
-	if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
-		__sync_icache_dcache(pte);
+	if (!IS_ENABLED(CONFIG_DEBUG_VM))
+		return;
+
+	old_pte = READ_ONCE(*ptep);
+
+	if (!pte_valid(old_pte) || !pte_valid(pte))
+		return;
+	if (mm != current->active_mm && atomic_read(&mm->mm_users) <= 1)
+		return;
 
 	/*
-	 * If the existing pte is valid, check for potential race with
-	 * hardware updates of the pte (ptep_set_access_flags safely changes
-	 * valid ptes without going through an invalid entry).
+	 * Check for potential race with hardware updates of the pte
+	 * (ptep_set_access_flags safely changes valid ptes without going
+	 * through an invalid entry).
 	 */
-	old_pte = READ_ONCE(*ptep);
-	if (IS_ENABLED(CONFIG_DEBUG_VM) && pte_valid(old_pte) && pte_valid(pte) &&
-	   (mm == current->active_mm || atomic_read(&mm->mm_users) > 1)) {
-		VM_WARN_ONCE(!pte_young(pte),
-			     "%s: racy access flag clearing: 0x%016llx -> 0x%016llx",
-			     __func__, pte_val(old_pte), pte_val(pte));
-		VM_WARN_ONCE(pte_write(old_pte) && !pte_dirty(pte),
-			     "%s: racy dirty state clearing: 0x%016llx -> 0x%016llx",
-			     __func__, pte_val(old_pte), pte_val(pte));
-	}
+	VM_WARN_ONCE(!pte_young(pte),
+		     "%s: racy access flag clearing: 0x%016llx -> 0x%016llx",
+		     __func__, pte_val(old_pte), pte_val(pte));
+	VM_WARN_ONCE(pte_write(old_pte) && !pte_dirty(pte),
+		     "%s: racy dirty state clearing: 0x%016llx -> 0x%016llx",
+		     __func__, pte_val(old_pte), pte_val(pte));
+}
+
+static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep, pte_t pte)
+{
+	if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
+		__sync_icache_dcache(pte);
+
+	__check_racy_pte_update(mm, ptep, pte);
 
 	set_pte(ptep, pte);
 }

commit f7f0097af67c3c119f6dc7046234630e77f4877e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon May 27 09:28:15 2019 +0530

    arm64/mm: Simplify protection flag creation for kernel huge mappings
    
    Even though they have got the same value, PMD_TYPE_SECT and PUD_TYPE_SECT
    get used for kernel huge mappings. But before that first the table bit gets
    cleared using leaf level PTE_TABLE_BIT. Though functionally they are same,
    we should use page table level specific macros to be consistent as per the
    MMU specifications. Create page table level specific wrappers for kernel
    huge mapping entries and just drop mk_sect_prot() which does not have any
    other user.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2c41b04708fe..6478dd121228 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -335,9 +335,14 @@ static inline pmd_t pte_pmd(pte_t pte)
 	return __pmd(pte_val(pte));
 }
 
-static inline pgprot_t mk_sect_prot(pgprot_t prot)
+static inline pgprot_t mk_pud_sect_prot(pgprot_t prot)
 {
-	return __pgprot(pgprot_val(prot) & ~PTE_TABLE_BIT);
+	return __pgprot((pgprot_val(prot) & ~PUD_TABLE_BIT) | PUD_TYPE_SECT);
+}
+
+static inline pgprot_t mk_pmd_sect_prot(pgprot_t prot)
+{
+	return __pgprot((pgprot_val(prot) & ~PMD_TABLE_BIT) | PMD_TYPE_SECT);
 }
 
 #ifdef CONFIG_NUMA_BALANCING

commit 5fbbeedb9a8f039e0a531435c7894905c1d51e77
Author: Qian Cai <cai@lca.pw>
Date:   Mon Apr 29 13:37:02 2019 -0400

    arm64: mm: Remove pte_unmap_nested()
    
    As of commit ece0e2b6406a ("mm: remove pte_*map_nested()"),
    pte_unmap_nested() is no longer used and can be removed from the arm64
    code.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    [will: also remove pte_offset_map_nested()]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 74ebe9693714..2c41b04708fe 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -487,8 +487,6 @@ static inline void pte_unmap(pte_t *pte) { }
 #define pte_offset_kernel(dir,addr)	((pte_t *)__va(pte_offset_phys((dir), (addr))))
 
 #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
-#define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
-#define pte_unmap_nested(pte)		do { } while (0)
 
 #define pte_set_fixmap(addr)		((pte_t *)set_fixmap_offset(FIX_PTE, addr))
 #define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))

commit 74dd022f9e6260c3b5b8d15901d27ebcc5f21eda
Author: Qian Cai <cai@lca.pw>
Date:   Mon Apr 29 13:37:01 2019 -0400

    arm64: Fix compiler warning from pte_unmap() with -Wunused-but-set-variable
    
    When building with -Wunused-but-set-variable, the compiler shouts about
    a number of pte_unmap() users, since this expands to an empty macro on
    arm64:
    
      | mm/gup.c: In function 'gup_pte_range':
      | mm/gup.c:1727:16: warning: variable 'ptem' set but not used
      | [-Wunused-but-set-variable]
      | mm/gup.c: At top level:
      | mm/memory.c: In function 'copy_pte_range':
      | mm/memory.c:821:24: warning: variable 'orig_dst_pte' set but not used
      | [-Wunused-but-set-variable]
      | mm/memory.c:821:9: warning: variable 'orig_src_pte' set but not used
      | [-Wunused-but-set-variable]
      | mm/swap_state.c: In function 'swap_ra_info':
      | mm/swap_state.c:641:15: warning: variable 'orig_pte' set but not used
      | [-Wunused-but-set-variable]
      | mm/madvise.c: In function 'madvise_free_pte_range':
      | mm/madvise.c:318:9: warning: variable 'orig_pte' set but not used
      | [-Wunused-but-set-variable]
    
    Rewrite pte_unmap() as a static inline function, which silences the
    warnings.
    
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index de70c1eabf33..74ebe9693714 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -478,6 +478,8 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 	return __pmd_to_phys(pmd);
 }
 
+static inline void pte_unmap(pte_t *pte) { }
+
 /* Find an entry in the third-level page table. */
 #define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
 
@@ -486,7 +488,6 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 
 #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
 #define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
-#define pte_unmap(pte)			do { } while (0)
 #define pte_unmap_nested(pte)		do { } while (0)
 
 #define pte_set_fixmap(addr)		((pte_t *)set_fixmap_offset(FIX_PTE, addr))

commit 42b00f122cfbfed79fc29b0b3610f3abbb1e3864
Merge: 460023a5d1d2 a0aea130afeb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 11:46:28 2018 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - selftests improvements
       - large PUD support for HugeTLB
       - single-stepping fixes
       - improved tracing
       - various timer and vGIC fixes
    
      x86:
       - Processor Tracing virtualization
       - STIBP support
       - some correctness fixes
       - refactorings and splitting of vmx.c
       - use the Hyper-V range TLB flush hypercall
       - reduce order of vcpu struct
       - WBNOINVD support
       - do not use -ftrace for __noclone functions
       - nested guest support for PAUSE filtering on AMD
       - more Hyper-V enlightenments (direct mode for synthetic timers)
    
      PPC:
       -  nested VFIO
    
      s390:
       - bugfixes only this time"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (171 commits)
      KVM: x86: Add CPUID support for new instruction WBNOINVD
      kvm: selftests: ucall: fix exit mmio address guessing
      Revert "compiler-gcc: disable -ftracer for __noclone functions"
      KVM: VMX: Move VM-Enter + VM-Exit handling to non-inline sub-routines
      KVM: VMX: Explicitly reference RCX as the vmx_vcpu pointer in asm blobs
      KVM: x86: Use jmp to invoke kvm_spurious_fault() from .fixup
      MAINTAINERS: Add arch/x86/kvm sub-directories to existing KVM/x86 entry
      KVM/x86: Use SVM assembly instruction mnemonics instead of .byte streams
      KVM/MMU: Flush tlb directly in the kvm_zap_gfn_range()
      KVM/MMU: Flush tlb directly in kvm_set_pte_rmapp()
      KVM/MMU: Move tlb flush in kvm_set_pte_rmapp() to kvm_mmu_notifier_change_pte()
      KVM: Make kvm_set_spte_hva() return int
      KVM: Replace old tlb flush function with new one to flush a specified range.
      KVM/MMU: Add tlb flush with range helper function
      KVM/VMX: Add hv tlb range flush support
      x86/hyper-v: Add HvFlushGuestAddressList hypercall support
      KVM: Add tlb_remote_flush_with_range callback in kvm_x86_ops
      KVM: x86: Disable Intel PT when VMXON in L1 guest
      KVM: x86: Set intercept for Intel PT MSRs read/write
      KVM: x86: Implement Intel PT MSRs read/write emulation
      ...

commit b8e0ba7c8bea994011aff3b4c35256b180fab874
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Tue Dec 11 17:10:41 2018 +0000

    KVM: arm64: Add support for creating PUD hugepages at stage 2
    
    KVM only supports PMD hugepages at stage 2. Now that the various page
    handling routines are updated, extend the stage 2 fault handling to
    map in PUD hugepages.
    
    Addition of PUD hugepage support enables additional page sizes (e.g.,
    1G with 4K granule) which can be useful on cores that support mapping
    larger block sizes in the TLB entries.
    
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [ Replace BUG() => WARN_ON(1) for arm32 PUD helpers ]
    Signed-off-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index bb0f3f17a7a9..576128635f3c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -390,6 +390,8 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pud_mkyoung(pud)	pte_pud(pte_mkyoung(pud_pte(pud)))
 #define pud_write(pud)		pte_write(pud_pte(pud))
 
+#define pud_mkhuge(pud)		(__pud(pud_val(pud) & ~PUD_TABLE_BIT))
+
 #define __pud_to_phys(pud)	__pte_to_phys(pud_pte(pud))
 #define __phys_to_pud_val(phys)	__phys_to_pte_val(phys)
 #define pud_pfn(pud)		((__pud_to_phys(pud) & PUD_MASK) >> PAGE_SHIFT)

commit 35a63966194dd994f44150f07398c62f8dca011e
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Tue Dec 11 17:10:40 2018 +0000

    KVM: arm64: Update age handlers to support PUD hugepages
    
    In preparation for creating larger hugepages at Stage 2, add support
    to the age handling notifiers for PUD hugepages when encountered.
    
    Provide trivial helpers for arm32 to allow sharing code.
    
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [ Replaced BUG() => WARN_ON(1) for arm32 PUD helpers ]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index f51e2271e6a3..bb0f3f17a7a9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -386,6 +386,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pfn_pmd(pfn,prot)	__pmd(__phys_to_pmd_val((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
+#define pud_young(pud)		pte_young(pud_pte(pud))
 #define pud_mkyoung(pud)	pte_pud(pte_mkyoung(pud_pte(pud)))
 #define pud_write(pud)		pte_write(pud_pte(pud))
 

commit eb3f0624ea082def887acc79e97934e27d0188b7
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Tue Dec 11 17:10:39 2018 +0000

    KVM: arm64: Support handling access faults for PUD hugepages
    
    In preparation for creating larger hugepages at Stage 2, extend the
    access fault handling at Stage 2 to support PUD hugepages when
    encountered.
    
    Provide trivial helpers for arm32 to allow sharing of code.
    
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [ Replaced BUG() => WARN_ON(1) in PUD helpers ]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 50b1ef8584c0..f51e2271e6a3 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -314,6 +314,11 @@ static inline pte_t pud_pte(pud_t pud)
 	return __pte(pud_val(pud));
 }
 
+static inline pud_t pte_pud(pte_t pte)
+{
+	return __pud(pte_val(pte));
+}
+
 static inline pmd_t pud_pmd(pud_t pud)
 {
 	return __pmd(pud_val(pud));
@@ -381,6 +386,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pfn_pmd(pfn,prot)	__pmd(__phys_to_pmd_val((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
+#define pud_mkyoung(pud)	pte_pud(pte_mkyoung(pud_pte(pud)))
 #define pud_write(pud)		pte_write(pud_pte(pud))
 
 #define __pud_to_phys(pud)	__pte_to_phys(pud_pte(pud))

commit 3403e56b41c176f6531a2a6d77d85b46fa34169c
Author: Alex Van Brunt <avanbrunt@nvidia.com>
Date:   Mon Oct 29 14:55:58 2018 +0530

    arm64: mm: Don't wait for completion of TLB invalidation when page aging
    
    When transitioning a PTE from young to old as part of page aging, we
    can avoid waiting for the TLB invalidation to complete and therefore
    drop the subsequent DSB instruction. Whilst this opens up a race with
    page reclaim, where a PTE in active use via a stale, young TLB entry
    does not update the underlying descriptor, the worst thing that happens
    is that the page is reclaimed and then immediately faulted back in.
    
    Given that we have a DSB in our context-switch path, the window for a
    spurious reclaim is fairly limited and eliding the barrier claims to
    boost NVMe/SSD accesses by over 10% on some platforms.
    
    A similar optimisation was made for x86 in commit b13b1d2d8692 ("x86/mm:
    In the PTE swapout page reclaim case clear the accessed bit instead of
    flushing the TLB").
    
    Signed-off-by: Alex Van Brunt <avanbrunt@nvidia.com>
    Signed-off-by: Ashish Mhetre <amhetre@nvidia.com>
    [will: rewrote patch]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 50b1ef8584c0..5bbb59c81920 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -22,6 +22,7 @@
 #include <asm/memory.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/pgtable-prot.h>
+#include <asm/tlbflush.h>
 
 /*
  * VMALLOC range.
@@ -685,6 +686,27 @@ static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
 	return __ptep_test_and_clear_young(ptep);
 }
 
+#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH
+static inline int ptep_clear_flush_young(struct vm_area_struct *vma,
+					 unsigned long address, pte_t *ptep)
+{
+	int young = ptep_test_and_clear_young(vma, address, ptep);
+
+	if (young) {
+		/*
+		 * We can elide the trailing DSB here since the worst that can
+		 * happen is that a CPU continues to use the young entry in its
+		 * TLB and we mistakenly reclaim the associated page. The
+		 * window for such an event is bounded by the next
+		 * context-switch, which provides a DSB to complete the TLB
+		 * invalidation.
+		 */
+		flush_tlb_page_nosync(vma, address);
+	}
+
+	return young;
+}
+
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG
 static inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,

commit e9ed821be48600ea3ec7f7b76e478c769729f83c
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 5 14:49:16 2018 +0100

    arm64: mm: Use #ifdef for the __PAGETABLE_P?D_FOLDED defines
    
    __is_defined(__PAGETABLE_P?D_FOLDED) doesn't quite work as intended
    as these symbols are internal to asm-generic and aren't defined in the
    way kconfig expects. This makes them always evaluate to false.
    Switch to #ifdef.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b58f764babf8..50b1ef8584c0 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -445,10 +445,12 @@ static inline bool in_swapper_pgdir(void *addr)
 
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
-	if (__is_defined(__PAGETABLE_PMD_FOLDED) && in_swapper_pgdir(pmdp)) {
+#ifdef __PAGETABLE_PMD_FOLDED
+	if (in_swapper_pgdir(pmdp)) {
 		set_swapper_pgd((pgd_t *)pmdp, __pgd(pmd_val(pmd)));
 		return;
 	}
+#endif /* __PAGETABLE_PMD_FOLDED */
 
 	WRITE_ONCE(*pmdp, pmd);
 
@@ -503,10 +505,12 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 
 static inline void set_pud(pud_t *pudp, pud_t pud)
 {
-	if (__is_defined(__PAGETABLE_PUD_FOLDED) && in_swapper_pgdir(pudp)) {
+#ifdef __PAGETABLE_PUD_FOLDED
+	if (in_swapper_pgdir(pudp)) {
 		set_swapper_pgd((pgd_t *)pudp, __pgd(pud_val(pud)));
 		return;
 	}
+#endif /* __PAGETABLE_PUD_FOLDED */
 
 	WRITE_ONCE(*pudp, pud);
 

commit 2330b7ca78350efcb1a3b919ea4b3e0e4c57d99f
Author: Jun Yao <yaojun8558363@gmail.com>
Date:   Mon Sep 24 17:15:02 2018 +0100

    arm64/mm: use fixmap to modify swapper_pg_dir
    
    Once swapper_pg_dir is in the rodata section, it will not be possible to
    modify it directly, but we will need to modify it in some cases.
    
    To enable this, we can use the fixmap when deliberately modifying
    swapper_pg_dir. As the pgd is only transiently mapped, this provides
    some resilience against illicit modification of the pgd, e.g. for
    Kernel Space Mirror Attack (KSMA).
    
    Signed-off-by: Jun Yao <yaojun8558363@gmail.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    [Mark: simplify ifdeffery, commit message]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 92bcc50b0cc2..b58f764babf8 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -429,8 +429,27 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 				 PUD_TYPE_TABLE)
 #endif
 
+extern pgd_t init_pg_dir[PTRS_PER_PGD];
+extern pgd_t init_pg_end[];
+extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
+extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
+extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
+
+extern void set_swapper_pgd(pgd_t *pgdp, pgd_t pgd);
+
+static inline bool in_swapper_pgdir(void *addr)
+{
+	return ((unsigned long)addr & PAGE_MASK) ==
+	        ((unsigned long)swapper_pg_dir & PAGE_MASK);
+}
+
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
+	if (__is_defined(__PAGETABLE_PMD_FOLDED) && in_swapper_pgdir(pmdp)) {
+		set_swapper_pgd((pgd_t *)pmdp, __pgd(pmd_val(pmd)));
+		return;
+	}
+
 	WRITE_ONCE(*pmdp, pmd);
 
 	if (pmd_valid(pmd))
@@ -484,6 +503,11 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 
 static inline void set_pud(pud_t *pudp, pud_t pud)
 {
+	if (__is_defined(__PAGETABLE_PUD_FOLDED) && in_swapper_pgdir(pudp)) {
+		set_swapper_pgd((pgd_t *)pudp, __pgd(pud_val(pud)));
+		return;
+	}
+
 	WRITE_ONCE(*pudp, pud);
 
 	if (pud_valid(pud))
@@ -538,6 +562,11 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 
 static inline void set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
+	if (in_swapper_pgdir(pgdp)) {
+		set_swapper_pgd(pgdp, pgd);
+		return;
+	}
+
 	WRITE_ONCE(*pgdp, pgd);
 	dsb(ishst);
 }
@@ -718,12 +747,6 @@ static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
 }
 #endif
 
-extern pgd_t init_pg_dir[PTRS_PER_PGD];
-extern pgd_t init_pg_end[];
-extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
-extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
-extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
-
 /*
  * Encode and decode a swap entry:
  *	bits 0-1:	present (must be zero)

commit 2b5548b68199c17c1466d5798cf2c9cd806bdaa9
Author: Jun Yao <yaojun8558363@gmail.com>
Date:   Mon Sep 24 15:47:49 2018 +0100

    arm64/mm: Separate boot-time page tables from swapper_pg_dir
    
    Since the address of swapper_pg_dir is fixed for a given kernel image,
    it is an attractive target for manipulation via an arbitrary write. To
    mitigate this we'd like to make it read-only by moving it into the
    rodata section.
    
    We require that swapper_pg_dir is at a fixed offset from tramp_pg_dir
    and reserved_ttbr0, so these will also need to move into rodata.
    However, swapper_pg_dir is allocated along with some transient page
    tables used for boot which we do not want to move into rodata.
    
    As a step towards this, this patch separates the boot-time page tables
    into a new init_pg_dir, and reduces swapper_pg_dir to the single page it
    needs to be. This allows us to retain the relationship between
    swapper_pg_dir, tramp_pg_dir, and swapper_pg_dir, while cleanly
    separating these from the boot-time page tables.
    
    The init_pg_dir holds all of the pgd/pud/pmd/pte levels needed during
    boot, and all of these levels will be freed when we switch to the
    swapper_pg_dir, which is initialized by the existing code in
    paging_init(). Since we start off on the init_pg_dir, we no longer need
    to allocate a transient page table in paging_init() in order to ensure
    that swapper_pg_dir isn't live while we initialize it.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Jun Yao <yaojun8558363@gmail.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    [Mark: place init_pg_dir after BSS, fold mm changes, commit message]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2ab2031b778c..92bcc50b0cc2 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -718,8 +718,9 @@ static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
 }
 #endif
 
+extern pgd_t init_pg_dir[PTRS_PER_PGD];
+extern pgd_t init_pg_end[];
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
-extern pgd_t swapper_pg_end[];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
 

commit 0795edaf3f1ff1ea58048515211280db004bbd68
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 22 21:36:31 2018 +0100

    arm64: pgtable: Implement p[mu]d_valid() and check in set_p[mu]d()
    
    Now that our walk-cache invalidation routines imply a DSB before the
    invalidation, we no longer need one when we are clearing an entry during
    unmap.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 1bdeca8918a6..2ab2031b778c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -360,6 +360,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pmd_present(pmd)	pte_present(pmd_pte(pmd))
 #define pmd_dirty(pmd)		pte_dirty(pmd_pte(pmd))
 #define pmd_young(pmd)		pte_young(pmd_pte(pmd))
+#define pmd_valid(pmd)		pte_valid(pmd_pte(pmd))
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
 #define pmd_mkold(pmd)		pte_pmd(pte_mkold(pmd_pte(pmd)))
 #define pmd_mkwrite(pmd)	pte_pmd(pte_mkwrite(pmd_pte(pmd)))
@@ -431,7 +432,9 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	WRITE_ONCE(*pmdp, pmd);
-	dsb(ishst);
+
+	if (pmd_valid(pmd))
+		dsb(ishst);
 }
 
 static inline void pmd_clear(pmd_t *pmdp)
@@ -477,11 +480,14 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 #define pud_none(pud)		(!pud_val(pud))
 #define pud_bad(pud)		(!(pud_val(pud) & PUD_TABLE_BIT))
 #define pud_present(pud)	pte_present(pud_pte(pud))
+#define pud_valid(pud)		pte_valid(pud_pte(pud))
 
 static inline void set_pud(pud_t *pudp, pud_t pud)
 {
 	WRITE_ONCE(*pudp, pud);
-	dsb(ishst);
+
+	if (pud_valid(pud))
+		dsb(ishst);
 }
 
 static inline void pud_clear(pud_t *pudp)

commit 24fe1b0efad4fcdd32ce46cffeab297f22581707
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jun 22 09:31:16 2018 +0100

    arm64: Remove unnecessary ISBs from set_{pte,pmd,pud}
    
    Commit 7f0b1bf04511 ("arm64: Fix barriers used for page table modifications")
    fixed a reported issue with fixmap page-table entries not being visible
    to the walker due to a missing DSB instruction. At the same time, it added
    ISB instructions to the arm64 set_{pte,pmd,pud} functions, which are not
    required by the architecture and make little sense in isolation.
    
    Remove the redundant ISBs.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 9f82d6b53851..1bdeca8918a6 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -224,10 +224,8 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
 	 * or update_mmu_cache() have the necessary barriers.
 	 */
-	if (pte_valid_not_user(pte)) {
+	if (pte_valid_not_user(pte))
 		dsb(ishst);
-		isb();
-	}
 }
 
 extern void __sync_icache_dcache(pte_t pteval);
@@ -434,7 +432,6 @@ static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	WRITE_ONCE(*pmdp, pmd);
 	dsb(ishst);
-	isb();
 }
 
 static inline void pmd_clear(pmd_t *pmdp)
@@ -485,7 +482,6 @@ static inline void set_pud(pud_t *pudp, pud_t pud)
 {
 	WRITE_ONCE(*pudp, pud);
 	dsb(ishst);
-	isb();
 }
 
 static inline void pud_clear(pud_t *pudp)

commit 3010a5ea665a089361e435093bd737399123fcc4
Author: Laurent Dufour <ldufour@linux.vnet.ibm.com>
Date:   Thu Jun 7 17:06:08 2018 -0700

    mm: introduce ARCH_HAS_PTE_SPECIAL
    
    Currently the PTE special supports is turned on in per architecture
    header files.  Most of the time, it is defined in
    arch/*/include/asm/pgtable.h depending or not on some other per
    architecture static definition.
    
    This patch introduce a new configuration variable to manage this
    directly in the Kconfig files.  It would later replace
    __HAVE_ARCH_PTE_SPECIAL.
    
    Here notes for some architecture where the definition of
    __HAVE_ARCH_PTE_SPECIAL is not obvious:
    
    arm
     __HAVE_ARCH_PTE_SPECIAL which is currently defined in
    arch/arm/include/asm/pgtable-3level.h which is included by
    arch/arm/include/asm/pgtable.h when CONFIG_ARM_LPAE is set.
    So select ARCH_HAS_PTE_SPECIAL if ARM_LPAE.
    
    powerpc
    __HAVE_ARCH_PTE_SPECIAL is defined in 2 files:
     - arch/powerpc/include/asm/book3s/64/pgtable.h
     - arch/powerpc/include/asm/pte-common.h
    The first one is included if (PPC_BOOK3S & PPC64) while the second is
    included in all the other cases.
    So select ARCH_HAS_PTE_SPECIAL all the time.
    
    sparc:
    __HAVE_ARCH_PTE_SPECIAL is defined if defined(__sparc__) &&
    defined(__arch64__) which are defined through the compiler in
    sparc/Makefile if !SPARC32 which I assume to be if SPARC64.
    So select ARCH_HAS_PTE_SPECIAL if SPARC64
    
    There is no functional change introduced by this patch.
    
    Link: http://lkml.kernel.org/r/1523433816-14460-2-git-send-email-ldufour@linux.vnet.ibm.com
    Signed-off-by: Laurent Dufour <ldufour@linux.vnet.ibm.com>
    Suggested-by: Jerome Glisse <jglisse@redhat.com>
    Reviewed-by: Jerome Glisse <jglisse@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Aneesh Kumar K . V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Albert Ou <albert@sifive.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Christophe LEROY <christophe.leroy@c-s.fr>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7c4c8f318ba9..9f82d6b53851 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -306,8 +306,6 @@ static inline int pte_same(pte_t pte_a, pte_t pte_b)
 #define HPAGE_MASK		(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
 
-#define __HAVE_ARCH_PTE_SPECIAL
-
 static inline pte_t pgd_pte(pgd_t pgd)
 {
 	return __pte(pgd_val(pgd));

commit 907e21c15c883c2c15d1e5ee3cdbb7824ab1da59
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Tue Apr 17 20:03:09 2018 +0800

    arm64: mm: drop addr parameter from sync icache and dcache
    
    The addr parameter isn't used for anything. Let's simplify and get rid of
    it, like arm.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7e2c27e63cd8..7c4c8f318ba9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -230,7 +230,7 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	}
 }
 
-extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
+extern void __sync_icache_dcache(pte_t pteval);
 
 /*
  * PTE bits configuration in the presence of hardware Dirty Bit Management
@@ -253,7 +253,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	pte_t old_pte;
 
 	if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
-		__sync_icache_dcache(pte, addr);
+		__sync_icache_dcache(pte);
 
 	/*
 	 * If the existing pte is valid, check for potential race with

commit 20a004e7b017cce282a46ac5d02c2b9c6b9bb1fa
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Feb 15 11:14:56 2018 +0000

    arm64: mm: Use READ_ONCE/WRITE_ONCE when accessing page tables
    
    In many cases, page tables can be accessed concurrently by either another
    CPU (due to things like fast gup) or by the hardware page table walker
    itself, which may set access/dirty bits. In such cases, it is important
    to use READ_ONCE/WRITE_ONCE when accessing page table entries so that
    entries cannot be torn, merged or subject to apparent loss of coherence
    due to compiler transformations.
    
    Whilst there are some scenarios where this cannot happen (e.g. pinned
    kernel mappings for the linear region), the overhead of using READ_ONCE
    /WRITE_ONCE everywhere is minimal and makes the code an awful lot easier
    to reason about. This patch consistently uses these macros in the arch
    code, as well as explicitly namespacing pointers to page table entries
    from the entries themselves by using adopting a 'p' suffix for the former
    (as is sometimes used elsewhere in the kernel source).
    
    Tested-by: Yury Norov <ynorov@caviumnetworks.com>
    Tested-by: Richard Ruigrok <rruigrok@codeaurora.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 094374c82db0..7e2c27e63cd8 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -218,7 +218,7 @@ static inline pmd_t pmd_mkcont(pmd_t pmd)
 
 static inline void set_pte(pte_t *ptep, pte_t pte)
 {
-	*ptep = pte;
+	WRITE_ONCE(*ptep, pte);
 
 	/*
 	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
@@ -250,6 +250,8 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
+	pte_t old_pte;
+
 	if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
 		__sync_icache_dcache(pte, addr);
 
@@ -258,14 +260,15 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	 * hardware updates of the pte (ptep_set_access_flags safely changes
 	 * valid ptes without going through an invalid entry).
 	 */
-	if (IS_ENABLED(CONFIG_DEBUG_VM) && pte_valid(*ptep) && pte_valid(pte) &&
+	old_pte = READ_ONCE(*ptep);
+	if (IS_ENABLED(CONFIG_DEBUG_VM) && pte_valid(old_pte) && pte_valid(pte) &&
 	   (mm == current->active_mm || atomic_read(&mm->mm_users) > 1)) {
 		VM_WARN_ONCE(!pte_young(pte),
 			     "%s: racy access flag clearing: 0x%016llx -> 0x%016llx",
-			     __func__, pte_val(*ptep), pte_val(pte));
-		VM_WARN_ONCE(pte_write(*ptep) && !pte_dirty(pte),
+			     __func__, pte_val(old_pte), pte_val(pte));
+		VM_WARN_ONCE(pte_write(old_pte) && !pte_dirty(pte),
 			     "%s: racy dirty state clearing: 0x%016llx -> 0x%016llx",
-			     __func__, pte_val(*ptep), pte_val(pte));
+			     __func__, pte_val(old_pte), pte_val(pte));
 	}
 
 	set_pte(ptep, pte);
@@ -431,7 +434,7 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
-	*pmdp = pmd;
+	WRITE_ONCE(*pmdp, pmd);
 	dsb(ishst);
 	isb();
 }
@@ -482,7 +485,7 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 
 static inline void set_pud(pud_t *pudp, pud_t pud)
 {
-	*pudp = pud;
+	WRITE_ONCE(*pudp, pud);
 	dsb(ishst);
 	isb();
 }
@@ -500,7 +503,7 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 /* Find an entry in the second-level page table. */
 #define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
 
-#define pmd_offset_phys(dir, addr)	(pud_page_paddr(*(dir)) + pmd_index(addr) * sizeof(pmd_t))
+#define pmd_offset_phys(dir, addr)	(pud_page_paddr(READ_ONCE(*(dir))) + pmd_index(addr) * sizeof(pmd_t))
 #define pmd_offset(dir, addr)		((pmd_t *)__va(pmd_offset_phys((dir), (addr))))
 
 #define pmd_set_fixmap(addr)		((pmd_t *)set_fixmap_offset(FIX_PMD, addr))
@@ -535,7 +538,7 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 
 static inline void set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
-	*pgdp = pgd;
+	WRITE_ONCE(*pgdp, pgd);
 	dsb(ishst);
 }
 
@@ -552,7 +555,7 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 /* Find an entry in the frst-level page table. */
 #define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
 
-#define pud_offset_phys(dir, addr)	(pgd_page_paddr(*(dir)) + pud_index(addr) * sizeof(pud_t))
+#define pud_offset_phys(dir, addr)	(pgd_page_paddr(READ_ONCE(*(dir))) + pud_index(addr) * sizeof(pud_t))
 #define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
 
 #define pud_set_fixmap(addr)		((pud_t *)set_fixmap_offset(FIX_PUD, addr))

commit 1d78a62cb3bb2bd95d00149daaa144f1fe0a77df
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jan 31 16:17:55 2018 -0800

    arm64: provide pmdp_establish() helper
    
    We need an atomic way to setup pmd page table entry, avoiding races with
    CPU setting dirty/accessed bits.  This is required to implement
    pmdp_invalidate() that doesn't lose these bits.
    
    Link: http://lkml.kernel.org/r/20171213105756.69879-5-kirill.shutemov@linux.intel.com
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 89167c43ebb5..094374c82db0 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -706,6 +706,13 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 {
 	ptep_set_wrprotect(mm, address, (pte_t *)pmdp);
 }
+
+#define pmdp_establish pmdp_establish
+static inline pmd_t pmdp_establish(struct vm_area_struct *vma,
+		unsigned long address, pmd_t *pmdp, pmd_t pmd)
+{
+	return __pmd(xchg_relaxed(&pmd_val(*pmdp), pmd_val(pmd)));
+}
 #endif
 
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];

commit 0aebc6a440b942df6221a7765f077f02217e0114
Merge: 72906f38934a ec89ab50a03a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 30 13:57:43 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The main theme of this pull request is security covering variants 2
      and 3 for arm64. I expect to send additional patches next week
      covering an improved firmware interface (requires firmware changes)
      for variant 2 and way for KPTI to be disabled on unaffected CPUs
      (Cavium's ThunderX doesn't work properly with KPTI enabled because of
      a hardware erratum).
    
      Summary:
    
       - Security mitigations:
          - variant 2: invalidate the branch predictor with a call to
            secure firmware
          - variant 3: implement KPTI for arm64
    
       - 52-bit physical address support for arm64 (ARMv8.2)
    
       - arm64 support for RAS (firmware first only) and SDEI (software
         delegated exception interface; allows firmware to inject a RAS
         error into the OS)
    
       - perf support for the ARM DynamIQ Shared Unit PMU
    
       - CPUID and HWCAP bits updated for new floating point multiplication
         instructions in ARMv8.4
    
       - remove some virtual memory layout printks during boot
    
       - fix initial page table creation to cope with larger than 32M kernel
         images when 16K pages are enabled"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (104 commits)
      arm64: Fix TTBR + PAN + 52-bit PA logic in cpu_do_switch_mm
      arm64: Turn on KPTI only on CPUs that need it
      arm64: Branch predictor hardening for Cavium ThunderX2
      arm64: Run enable method for errata work arounds on late CPUs
      arm64: Move BP hardening to check_and_switch_context
      arm64: mm: ignore memory above supported physical address size
      arm64: kpti: Fix the interaction between ASID switching and software PAN
      KVM: arm64: Emulate RAS error registers and set HCR_EL2's TERR & TEA
      KVM: arm64: Handle RAS SErrors from EL2 on guest exit
      KVM: arm64: Handle RAS SErrors from EL1 on guest exit
      KVM: arm64: Save ESR_EL2 on guest SError
      KVM: arm64: Save/Restore guest DISR_EL1
      KVM: arm64: Set an impdef ESR for Virtual-SError using VSESR_EL2.
      KVM: arm/arm64: mask/unmask daif around VHE guests
      arm64: kernel: Prepare for a DISR user
      arm64: Unconditionally enable IESB on exception entry/return for firmware-first
      arm64: kernel: Survive corrected RAS errors notified by SError
      arm64: cpufeature: Detect CPU RAS Extentions
      arm64: sysreg: Move to use definitions for all the SCTLR bits
      arm64: cpufeature: __this_cpu_has_cap() shouldn't stop early
      ...

commit 0370b31e48454d8cf11120664aedd1c51b3004cb
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Jan 11 10:11:59 2018 +0000

    arm64: Extend early page table code to allow for larger kernels
    
    Currently the early assembler page table code assumes that precisely
    1xpgd, 1xpud, 1xpmd are sufficient to represent the early kernel text
    mappings.
    
    Unfortunately this is rarely the case when running with a 16KB granule,
    and we also run into limits with 4KB granule when building much larger
    kernels.
    
    This patch re-writes the early page table logic to compute indices of
    mappings for each level of page table, and if multiple indices are
    required, the next-level page table is scaled up accordingly.
    
    Also the required size of the swapper_pg_dir is computed at link time
    to cover the mapping [KIMAGE_ADDR + VOFFSET, _end]. When KASLR is
    enabled, an extra page is set aside for each level that may require extra
    entries at runtime.
    
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index bfa237e892f1..54b0a8398055 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -706,6 +706,7 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 #endif
 
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
+extern pgd_t swapper_pg_end[];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
 

commit 1f911c3a1140e1668e68791fb6dd07757e2f3956
Merge: 6aef0fdd35ea f77d281713d4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Dec 22 17:40:58 2017 +0000

    Merge branch 'for-next/52-bit-pa' into for-next/core
    
    * for-next/52-bit-pa:
      arm64: enable 52-bit physical address support
      arm64: allow ID map to be extended to 52 bits
      arm64: handle 52-bit physical addresses in page table entries
      arm64: don't open code page table entry creation
      arm64: head.S: handle 52-bit PAs in PTEs in early page table setup
      arm64: handle 52-bit addresses in TTBR
      arm64: limit PA size to supported range
      arm64: add kconfig symbol to configure physical address size

commit 75387b92635e7dca410c1ef92cfe510019248f76
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:21 2017 +0000

    arm64: handle 52-bit physical addresses in page table entries
    
    The top 4 bits of a 52-bit physical address are positioned at bits
    12..15 of a page table entry. Introduce macros to convert between a
    physical address and its placement in a table entry, and change all
    macros/functions that access PTEs to use them.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [catalin.marinas@arm.com: some long lines wrapped]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5d9554fb2692..4fd8af303e2c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -57,9 +57,22 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
 
-#define pte_pfn(pte)		((pte_val(pte) & PHYS_MASK) >> PAGE_SHIFT)
+/*
+ * Macros to convert between a physical address and its placement in a
+ * page table entry, taking care of 52-bit addresses.
+ */
+#ifdef CONFIG_ARM64_PA_BITS_52
+#define __pte_to_phys(pte)	\
+	((pte_val(pte) & PTE_ADDR_LOW) | ((pte_val(pte) & PTE_ADDR_HIGH) << 36))
+#define __phys_to_pte_val(phys)	(((phys) | ((phys) >> 36)) & PTE_ADDR_MASK)
+#else
+#define __pte_to_phys(pte)	(pte_val(pte) & PTE_ADDR_MASK)
+#define __phys_to_pte_val(phys)	(phys)
+#endif
 
-#define pfn_pte(pfn,prot)	(__pte(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
+#define pte_pfn(pte)		(__pte_to_phys(pte) >> PAGE_SHIFT)
+#define pfn_pte(pfn,prot)	\
+	__pte(__phys_to_pte_val((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot))
 
 #define pte_none(pte)		(!pte_val(pte))
 #define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
@@ -284,6 +297,11 @@ static inline int pte_same(pte_t pte_a, pte_t pte_b)
 
 #define __HAVE_ARCH_PTE_SPECIAL
 
+static inline pte_t pgd_pte(pgd_t pgd)
+{
+	return __pte(pgd_val(pgd));
+}
+
 static inline pte_t pud_pte(pud_t pud)
 {
 	return __pte(pud_val(pud));
@@ -349,16 +367,24 @@ static inline int pmd_protnone(pmd_t pmd)
 
 #define pmd_mkhuge(pmd)		(__pmd(pmd_val(pmd) & ~PMD_TABLE_BIT))
 
-#define pmd_pfn(pmd)		(((pmd_val(pmd) & PMD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
-#define pfn_pmd(pfn,prot)	(__pmd(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
+#define __pmd_to_phys(pmd)	__pte_to_phys(pmd_pte(pmd))
+#define __phys_to_pmd_val(phys)	__phys_to_pte_val(phys)
+#define pmd_pfn(pmd)		((__pmd_to_phys(pmd) & PMD_MASK) >> PAGE_SHIFT)
+#define pfn_pmd(pfn,prot)	__pmd(__phys_to_pmd_val((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot))
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
 #define pud_write(pud)		pte_write(pud_pte(pud))
-#define pud_pfn(pud)		(((pud_val(pud) & PUD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
-#define pfn_pud(pfn,prot)	(__pud(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
+
+#define __pud_to_phys(pud)	__pte_to_phys(pud_pte(pud))
+#define __phys_to_pud_val(phys)	__phys_to_pte_val(phys)
+#define pud_pfn(pud)		((__pud_to_phys(pud) & PUD_MASK) >> PAGE_SHIFT)
+#define pfn_pud(pfn,prot)	__pud(__phys_to_pud_val((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot))
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 
+#define __pgd_to_phys(pgd)	__pte_to_phys(pgd_pte(pgd))
+#define __phys_to_pgd_val(phys)	__phys_to_pte_val(phys)
+
 #define __pgprot_modify(prot,mask,bits) \
 	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
 
@@ -409,7 +435,7 @@ static inline void pmd_clear(pmd_t *pmdp)
 
 static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 {
-	return pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK;
+	return __pmd_to_phys(pmd);
 }
 
 /* Find an entry in the third-level page table. */
@@ -427,7 +453,7 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 #define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))
 #define pte_clear_fixmap()		clear_fixmap(FIX_PTE)
 
-#define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+#define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(__pmd_to_phys(pmd)))
 
 /* use ONLY for statically allocated translation tables */
 #define pte_offset_kimg(dir,addr)	((pte_t *)__phys_to_kimg(pte_offset_phys((dir), (addr))))
@@ -460,7 +486,7 @@ static inline void pud_clear(pud_t *pudp)
 
 static inline phys_addr_t pud_page_paddr(pud_t pud)
 {
-	return pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK;
+	return __pud_to_phys(pud);
 }
 
 /* Find an entry in the second-level page table. */
@@ -473,7 +499,7 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 #define pmd_set_fixmap_offset(pud, addr)	pmd_set_fixmap(pmd_offset_phys(pud, addr))
 #define pmd_clear_fixmap()		clear_fixmap(FIX_PMD)
 
-#define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
+#define pud_page(pud)		pfn_to_page(__phys_to_pfn(__pud_to_phys(pud)))
 
 /* use ONLY for statically allocated translation tables */
 #define pmd_offset_kimg(dir,addr)	((pmd_t *)__phys_to_kimg(pmd_offset_phys((dir), (addr))))
@@ -512,7 +538,7 @@ static inline void pgd_clear(pgd_t *pgdp)
 
 static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 {
-	return pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK;
+	return __pgd_to_phys(pgd);
 }
 
 /* Find an entry in the frst-level page table. */
@@ -525,7 +551,7 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 #define pud_set_fixmap_offset(pgd, addr)	pud_set_fixmap(pud_offset_phys(pgd, addr))
 #define pud_clear_fixmap()		clear_fixmap(FIX_PUD)
 
-#define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
+#define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(__pgd_to_phys(pgd)))
 
 /* use ONLY for statically allocated translation tables */
 #define pud_offset_kimg(dir,addr)	((pud_t *)__phys_to_kimg(pud_offset_phys((dir), (addr))))

commit 193383043f14a398393dc18bae8380f7fe665ec3
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:20 2017 +0000

    arm64: don't open code page table entry creation
    
    Instead of open coding the generation of page table entries, use the
    macros/functions that exist for this - pfn_p*d and p*d_populate. Most
    code in the kernel already uses these macros, this patch tries to fix
    up the few places that don't. This is useful for the next patch in this
    series, which needs to change the page table entry logic, and it's
    better to have that logic in one place.
    
    The KVM extended ID map is special, since we're creating a level above
    CONFIG_PGTABLE_LEVELS and the required function isn't available. Leave
    it as is and add a comment to explain it. (The normal kernel ID map code
    doesn't need this change because its page tables are created in assembly
    (__create_page_tables)).
    
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 93677b9db947..5d9554fb2692 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -355,6 +355,7 @@ static inline int pmd_protnone(pmd_t pmd)
 
 #define pud_write(pud)		pte_write(pud_pte(pud))
 #define pud_pfn(pud)		(((pud_val(pud) & PUD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
+#define pfn_pud(pfn,prot)	(__pud(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 

commit 529c4b05a3cb2f324aac347042ee6d641478e946
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:18 2017 +0000

    arm64: handle 52-bit addresses in TTBR
    
    The top 4 bits of a 52-bit physical address are positioned at bits 2..5
    in the TTBR registers. Introduce a couple of macros to move the bits
    there, and change all TTBR writers to use them.
    
    Leave TTBR0 PAN code unchanged, to avoid complicating it. A system with
    52-bit PA will have PAN anyway (because it's ARMv8.1 or later), and a
    system without 52-bit PA can only use up to 48-bit PAs. A later patch in
    this series will add a kconfig dependency to ensure PAN is configured.
    
    In addition, when using 52-bit PA there is a special alignment
    requirement on the top-level table. We don't currently have any VA_BITS
    configuration that would violate the requirement, but one could be added
    in the future, so add a compile-time BUG_ON to check for it.
    
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [catalin.marinas@arm.com: added TTBR_BADD_MASK_52 comment]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 149d05fb9421..93677b9db947 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -733,6 +733,12 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 #define kc_vaddr_to_offset(v)	((v) & ~VA_START)
 #define kc_offset_to_vaddr(o)	((o) | VA_START)
 
+#ifdef CONFIG_ARM64_PA_BITS_52
+#define phys_to_ttbr(addr)	(((addr) | ((addr) >> 46)) & TTBR_BADDR_MASK_52)
+#else
+#define phys_to_ttbr(addr)	(addr)
+#endif
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */

commit 86c9e8126e9fbcbf06c36e285168b880369a537c
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 12 10:48:54 2017 +0000

    arm64: mm: Fix false positives in set_pte_at access/dirty race detection
    
    Jiankang reports that our race detection in set_pte_at is firing when
    copying the page tables in dup_mmap as a result of a fork(). In this
    situation, the page table isn't actually live and so there is no way
    that we can race with a concurrent update from the hardware page table
    walker.
    
    This patch reworks the race detection so that we require either the
    mm to match the current active_mm (i.e. currently installed in our TTBR0)
    or the mm_users count to be greater than 1, implying that the page table
    could be live in another CPU. The mm_users check might still be racy,
    but we'll avoid false positives and it's not realistic to validate that
    all the necessary locks are held as part of this assertion.
    
    Cc: Yisheng Xie <xieyisheng1@huawei.com>
    Reported-by: Jiankang Chen <chenjiankang1@huawei.com>
    Tested-by: Jiankang Chen <chenjiankang1@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3ff03a755c32..bdcc7f1c9d06 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -42,6 +42,8 @@
 #include <asm/cmpxchg.h>
 #include <asm/fixmap.h>
 #include <linux/mmdebug.h>
+#include <linux/mm_types.h>
+#include <linux/sched.h>
 
 extern void __pte_error(const char *file, int line, unsigned long val);
 extern void __pmd_error(const char *file, int line, unsigned long val);
@@ -215,9 +217,6 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	}
 }
 
-struct mm_struct;
-struct vm_area_struct;
-
 extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 
 /*
@@ -246,7 +245,8 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	 * hardware updates of the pte (ptep_set_access_flags safely changes
 	 * valid ptes without going through an invalid entry).
 	 */
-	if (pte_valid(*ptep) && pte_valid(pte)) {
+	if (IS_ENABLED(CONFIG_DEBUG_VM) && pte_valid(*ptep) && pte_valid(pte) &&
+	   (mm == current->active_mm || atomic_read(&mm->mm_users) > 1)) {
 		VM_WARN_ONCE(!pte_young(pte),
 			     "%s: racy access flag clearing: 0x%016llx -> 0x%016llx",
 			     __func__, pte_val(*ptep), pte_val(pte));

commit 8781bcbc5e69d7da69e84c7044ca0284848d5d01
Author: Steve Capper <steve.capper@arm.com>
Date:   Fri Dec 1 17:22:14 2017 +0000

    arm64: mm: Fix pte_mkclean, pte_mkdirty semantics
    
    On systems with hardware dirty bit management, the ltp madvise09 unit
    test fails due to dirty bit information being lost and pages being
    incorrectly freed.
    
    This was bisected to:
            arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()
    
    Reverting this commit leads to a separate problem, that the unit test
    retains pages that should have been dropped due to the function
    madvise_free_pte_range(.) not cleaning pte's properly.
    
    Currently pte_mkclean only clears the software dirty bit, thus the
    following code sequence can appear:
    
            pte = pte_mkclean(pte);
            if (pte_dirty(pte))
                    // this condition can return true with HW DBM!
    
    This patch also adjusts pte_mkclean to set PTE_RDONLY thus effectively
    clearing both the SW and HW dirty information.
    
    In order for this to function on systems without HW DBM, we need to
    also adjust pte_mkdirty to remove the read only bit from writable pte's
    to avoid infinite fault loops.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 64c26841b349 ("arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()")
    Reported-by: Bhupinder Thakur <bhupinder.thakur@linaro.org>
    Tested-by: Bhupinder Thakur <bhupinder.thakur@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 149d05fb9421..3ff03a755c32 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -149,12 +149,20 @@ static inline pte_t pte_mkwrite(pte_t pte)
 
 static inline pte_t pte_mkclean(pte_t pte)
 {
-	return clear_pte_bit(pte, __pgprot(PTE_DIRTY));
+	pte = clear_pte_bit(pte, __pgprot(PTE_DIRTY));
+	pte = set_pte_bit(pte, __pgprot(PTE_RDONLY));
+
+	return pte;
 }
 
 static inline pte_t pte_mkdirty(pte_t pte)
 {
-	return set_pte_bit(pte, __pgprot(PTE_DIRTY));
+	pte = set_pte_bit(pte, __pgprot(PTE_DIRTY));
+
+	if (pte_write(pte))
+		pte = clear_pte_bit(pte, __pgprot(PTE_RDONLY));
+
+	return pte;
 }
 
 static inline pte_t pte_mkold(pte_t pte)
@@ -641,28 +649,23 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 /*
- * ptep_set_wrprotect - mark read-only while preserving the hardware update of
- * the Access Flag.
+ * ptep_set_wrprotect - mark read-only while trasferring potential hardware
+ * dirty status (PTE_DBM && !PTE_RDONLY) to the software PTE_DIRTY bit.
  */
 #define __HAVE_ARCH_PTEP_SET_WRPROTECT
 static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)
 {
 	pte_t old_pte, pte;
 
-	/*
-	 * ptep_set_wrprotect() is only called on CoW mappings which are
-	 * private (!VM_SHARED) with the pte either read-only (!PTE_WRITE &&
-	 * PTE_RDONLY) or writable and software-dirty (PTE_WRITE &&
-	 * !PTE_RDONLY && PTE_DIRTY); see is_cow_mapping() and
-	 * protection_map[]. There is no race with the hardware update of the
-	 * dirty state: clearing of PTE_RDONLY when PTE_WRITE (a.k.a. PTE_DBM)
-	 * is set.
-	 */
-	VM_WARN_ONCE(pte_write(*ptep) && !pte_dirty(*ptep),
-		     "%s: potential race with hardware DBM", __func__);
 	pte = READ_ONCE(*ptep);
 	do {
 		old_pte = pte;
+		/*
+		 * If hardware-dirty (PTE_WRITE/DBM bit set and PTE_RDONLY
+		 * clear), set the PTE_DIRTY bit.
+		 */
+		if (pte_hw_dirty(pte))
+			pte = pte_mkdirty(pte);
 		pte = pte_wrprotect(pte);
 		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
 					       pte_val(old_pte), pte_val(pte));

commit 51a0048beb449682d632d0af52a515adb9f9882e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 14 14:14:17 2017 +0000

    arm64: mm: Map entry trampoline into trampoline and kernel page tables
    
    The exception entry trampoline needs to be mapped at the same virtual
    address in both the trampoline page table (which maps nothing else)
    and also the kernel page table, so that we can swizzle TTBR1_EL1 on
    exceptions from and return to EL0.
    
    This patch maps the trampoline at a fixed virtual address in the fixmap
    area of the kernel virtual address space, which allows the kernel proper
    to be randomized with respect to the trampoline when KASLR is enabled.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 149d05fb9421..774003b247ad 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -680,6 +680,7 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
+extern pgd_t tramp_pg_dir[PTRS_PER_PGD];
 
 /*
  * Encode and decode a swap entry:

commit e4e40e0263ea6a3bfefbfd15d1b6ff5c03f2b95e
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Nov 29 16:10:10 2017 -0800

    mm: switch to 'define pmd_write' instead of __HAVE_ARCH_PMD_WRITE
    
    In response to compile breakage introduced by a series that added the
    pud_write helper to x86, Stephen notes:
    
        did you consider using the other paradigm:
    
        In arch include files:
        #define pud_write       pud_write
        static inline int pud_write(pud_t pud)
         .....
    
        Then in include/asm-generic/pgtable.h:
    
        #ifndef pud_write
        tatic inline int pud_write(pud_t pud)
        {
                ....
        }
        #endif
    
        If you had, then the powerpc code would have worked ... ;-) and many
        of the other interfaces in include/asm-generic/pgtable.h are
        protected that way ...
    
    Given that some architecture already define pmd_write() as a macro, it's
    a net reduction to drop the definition of __HAVE_ARCH_PMD_WRITE.
    
    Link: http://lkml.kernel.org/r/151129126721.37405.13339850900081557813.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Suggested-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Oliver OHalloran <oliveroh@au1.ibm.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index c9530b5b5ca8..149d05fb9421 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -345,7 +345,6 @@ static inline int pmd_protnone(pmd_t pmd)
 
 #define pmd_thp_or_huge(pmd)	(pmd_huge(pmd) || pmd_trans_huge(pmd))
 
-#define __HAVE_ARCH_PMD_WRITE
 #define pmd_write(pmd)		pte_write(pmd_pte(pmd))
 
 #define pmd_mkhuge(pmd)		(__pmd(pmd_val(pmd) & ~PMD_TABLE_BIT))

commit 6218f96c58dbf44a06aeaf767aab1f54fc397838
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Oct 26 18:36:47 2017 +0100

    arm64: Implement arch-specific pte_access_permitted()
    
    The generic pte_access_permitted() implementation only checks for
    pte_present() (together with the write permission where applicable).
    However, for both kernel ptes and PROT_NONE mappings pte_present() also
    returns true on arm64 even though such mappings are not user accessible.
    Additionally, arm64 now supports execute-only user permission
    (PROT_EXEC) which is implemented by clearing the PTE_USER bit.
    
    With this patch the arm64 implementation of pte_access_permitted()
    checks for the PTE_VALID and PTE_USER bits together with writable access
    if applicable.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b46e54c2399b..c9530b5b5ca8 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -98,6 +98,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_UXN)) == (PTE_VALID | PTE_UXN))
 #define pte_valid_young(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_AF)) == (PTE_VALID | PTE_AF))
+#define pte_valid_user(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 
 /*
  * Could the pte be present in the TLB? We must check mm_tlb_flush_pending
@@ -107,6 +109,18 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_accessible(mm, pte)	\
 	(mm_tlb_flush_pending(mm) ? pte_present(pte) : pte_valid_young(pte))
 
+/*
+ * p??_access_permitted() is true for valid user mappings (subject to the
+ * write permission check) other than user execute-only which do not have the
+ * PTE_USER bit set. PROT_NONE mappings do not have the PTE_VALID bit set.
+ */
+#define pte_access_permitted(pte, write) \
+	(pte_valid_user(pte) && (!(write) || pte_write(pte)))
+#define pmd_access_permitted(pmd, write) \
+	(pte_access_permitted(pmd_pte(pmd), (write)))
+#define pud_access_permitted(pud, write) \
+	(pte_access_permitted(pud_pte(pud), (write)))
+
 static inline pte_t clear_pte_bit(pte_t pte, pgprot_t prot)
 {
 	pte_val(pte) &= ~pgprot_val(prot);

commit f069faba688701c4d56b6c3452a130f97bf02e95
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Sep 29 11:29:55 2017 +0100

    arm64: mm: Use READ_ONCE when dereferencing pointer to pte table
    
    On kernels built with support for transparent huge pages, different CPUs
    can access the PMD concurrently due to e.g. fast GUP or page_vma_mapped_walk
    and they must take care to use READ_ONCE to avoid value tearing or caching
    of stale values by the compiler. Unfortunately, these functions call into
    our pgtable macros, which don't use READ_ONCE, and compiler caching has
    been observed to cause the following crash during ext4 writeback:
    
    PC is at check_pte+0x20/0x170
    LR is at page_vma_mapped_walk+0x2e0/0x540
    [...]
    Process doio (pid: 2463, stack limit = 0xffff00000f2e8000)
    Call trace:
    [<ffff000008233328>] check_pte+0x20/0x170
    [<ffff000008233758>] page_vma_mapped_walk+0x2e0/0x540
    [<ffff000008234adc>] page_mkclean_one+0xac/0x278
    [<ffff000008234d98>] rmap_walk_file+0xf0/0x238
    [<ffff000008236e74>] rmap_walk+0x64/0xa0
    [<ffff0000082370c8>] page_mkclean+0x90/0xa8
    [<ffff0000081f3c64>] clear_page_dirty_for_io+0x84/0x2a8
    [<ffff00000832f984>] mpage_submit_page+0x34/0x98
    [<ffff00000832fb4c>] mpage_process_page_bufs+0x164/0x170
    [<ffff00000832fc8c>] mpage_prepare_extent_to_map+0x134/0x2b8
    [<ffff00000833530c>] ext4_writepages+0x484/0xe30
    [<ffff0000081f6ab4>] do_writepages+0x44/0xe8
    [<ffff0000081e5bd4>] __filemap_fdatawrite_range+0xbc/0x110
    [<ffff0000081e5e68>] file_write_and_wait_range+0x48/0xd8
    [<ffff000008324310>] ext4_sync_file+0x80/0x4b8
    [<ffff0000082bd434>] vfs_fsync_range+0x64/0xc0
    [<ffff0000082332b4>] SyS_msync+0x194/0x1e8
    
    This is because page_vma_mapped_walk loads the PMD twice before calling
    pte_offset_map: the first time without READ_ONCE (where it gets all zeroes
    due to a concurrent pmdp_invalidate) and the second time with READ_ONCE
    (where it sees a valid table pointer due to a concurrent pmd_populate).
    However, the compiler inlines everything and caches the first value in
    a register, which is subsequently used in pte_offset_phys which returns
    a junk pointer that is later dereferenced when attempting to access the
    relevant pte.
    
    This patch fixes the issue by using READ_ONCE in pte_offset_phys to ensure
    that a stale value is not used. Whilst this is a point fix for a known
    failure (and simple to backport), a full fix moving all of our page table
    accessors over to {READ,WRITE}_ONCE and consistently using READ_ONCE in
    page_vma_mapped_walk is in the works for a future kernel release.
    
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Timur Tabi <timur@codeaurora.org>
    Cc: <stable@vger.kernel.org>
    Fixes: f27176cfc363 ("mm: convert page_mkclean_one() to use page_vma_mapped_walk()")
    Tested-by: Richard Ruigrok <rruigrok@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index bc4e92337d16..b46e54c2399b 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -401,7 +401,7 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 /* Find an entry in the third-level page table. */
 #define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
 
-#define pte_offset_phys(dir,addr)	(pmd_page_paddr(*(dir)) + pte_index(addr) * sizeof(pte_t))
+#define pte_offset_phys(dir,addr)	(pmd_page_paddr(READ_ONCE(*(dir))) + pte_index(addr) * sizeof(pte_t))
 #define pte_offset_kernel(dir,addr)	((pte_t *)__va(pte_offset_phys((dir), (addr))))
 
 #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))

commit af29678fe785ad79e7386e97b57093482f0dd7c4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jul 6 11:53:08 2017 +0100

    arm64: Remove the !CONFIG_ARM64_HW_AFDBM alternative code paths
    
    Since the pte handling for hardware AF/DBM works even when the hardware
    feature is not present, make the pte accessors implementation permanent
    and remove the corresponding #ifdefs. The Kconfig option is kept as it
    can still be used to disable the feature at the hardware level.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 0117cbcd62d4..bc4e92337d16 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -85,11 +85,7 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 	(__boundary - 1 < (end) - 1) ? __boundary : (end);			\
 })
 
-#ifdef CONFIG_ARM64_HW_AFDBM
 #define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
-#else
-#define pte_hw_dirty(pte)	(0)
-#endif
 #define pte_sw_dirty(pte)	(!!(pte_val(pte) & PTE_DIRTY))
 #define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
 
@@ -228,8 +224,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	 * hardware updates of the pte (ptep_set_access_flags safely changes
 	 * valid ptes without going through an invalid entry).
 	 */
-	if (IS_ENABLED(CONFIG_ARM64_HW_AFDBM) &&
-	    pte_valid(*ptep) && pte_valid(pte)) {
+	if (pte_valid(*ptep) && pte_valid(pte)) {
 		VM_WARN_ONCE(!pte_young(pte),
 			     "%s: racy access flag clearing: 0x%016llx -> 0x%016llx",
 			     __func__, pte_val(*ptep), pte_val(pte));
@@ -565,7 +560,6 @@ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
 	return pte_pmd(pte_modify(pmd_pte(pmd), newprot));
 }
 
-#ifdef CONFIG_ARM64_HW_AFDBM
 #define __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS
 extern int ptep_set_access_flags(struct vm_area_struct *vma,
 				 unsigned long address, pte_t *ptep,
@@ -670,7 +664,6 @@ static inline void pmdp_set_wrprotect(struct mm_struct *mm,
 	ptep_set_wrprotect(mm, address, (pte_t *)pmdp);
 }
 #endif
-#endif	/* CONFIG_ARM64_HW_AFDBM */
 
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];

commit 64c26841b34957ef8f33f7a9e8663aeee59c3ded
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jul 5 10:59:42 2017 +0100

    arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()
    
    ptep_set_wrprotect() is only called on CoW mappings which are private
    (!VM_SHARED) with the pte either read-only (!PTE_WRITE && PTE_RDONLY) or
    writable and software-dirty (PTE_WRITE && !PTE_RDONLY && PTE_DIRTY).
    There is no race with the hardware update of the dirty state: clearing
    of PTE_RDONLY when PTE_WRITE (a.k.a. PTE_DBM) is set. This patch removes
    the code setting the software PTE_DIRTY bit in ptep_set_wrprotect() as
    superfluous. A VM_WARN_ONCE is introduced in case the above logic is
    wrong or the core mm code changes its use of ptep_set_wrprotect().
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index a04bfb869a80..0117cbcd62d4 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -634,23 +634,28 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 /*
- * ptep_set_wrprotect - mark read-only while trasferring potential hardware
- * dirty status (PTE_DBM && !PTE_RDONLY) to the software PTE_DIRTY bit.
+ * ptep_set_wrprotect - mark read-only while preserving the hardware update of
+ * the Access Flag.
  */
 #define __HAVE_ARCH_PTEP_SET_WRPROTECT
 static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)
 {
 	pte_t old_pte, pte;
 
+	/*
+	 * ptep_set_wrprotect() is only called on CoW mappings which are
+	 * private (!VM_SHARED) with the pte either read-only (!PTE_WRITE &&
+	 * PTE_RDONLY) or writable and software-dirty (PTE_WRITE &&
+	 * !PTE_RDONLY && PTE_DIRTY); see is_cow_mapping() and
+	 * protection_map[]. There is no race with the hardware update of the
+	 * dirty state: clearing of PTE_RDONLY when PTE_WRITE (a.k.a. PTE_DBM)
+	 * is set.
+	 */
+	VM_WARN_ONCE(pte_write(*ptep) && !pte_dirty(*ptep),
+		     "%s: potential race with hardware DBM", __func__);
 	pte = READ_ONCE(*ptep);
 	do {
 		old_pte = pte;
-		/*
-		 * If hardware-dirty (PTE_WRITE/DBM bit set and PTE_RDONLY
-		 * clear), set the PTE_DIRTY bit.
-		 */
-		if (pte_hw_dirty(pte))
-			pte = pte_mkdirty(pte);
 		pte = pte_wrprotect(pte);
 		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
 					       pte_val(old_pte), pte_val(pte));

commit 73e86cb03cf2ec0aa3789dc8621c6d53619cac5e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 4 19:04:18 2017 +0100

    arm64: Move PTE_RDONLY bit handling out of set_pte_at()
    
    Currently PTE_RDONLY is treated as a hardware only bit and not handled
    by the pte_mkwrite(), pte_wrprotect() or the user PAGE_* definitions.
    The set_pte_at() function is responsible for setting this bit based on
    the write permission or dirty state. This patch moves the PTE_RDONLY
    handling out of set_pte_at into the pte_mkwrite()/pte_wrprotect()
    functions. The PAGE_* definitions to need to be updated to explicitly
    include PTE_RDONLY when !PTE_WRITE.
    
    The patch also removes the redundant PAGE_COPY(_EXEC) definitions as
    they are identical to the corresponding PAGE_READONLY(_EXEC).
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 9127688ae775..a04bfb869a80 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -125,12 +125,16 @@ static inline pte_t set_pte_bit(pte_t pte, pgprot_t prot)
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
-	return clear_pte_bit(pte, __pgprot(PTE_WRITE));
+	pte = clear_pte_bit(pte, __pgprot(PTE_WRITE));
+	pte = set_pte_bit(pte, __pgprot(PTE_RDONLY));
+	return pte;
 }
 
 static inline pte_t pte_mkwrite(pte_t pte)
 {
-	return set_pte_bit(pte, __pgprot(PTE_WRITE));
+	pte = set_pte_bit(pte, __pgprot(PTE_WRITE));
+	pte = clear_pte_bit(pte, __pgprot(PTE_RDONLY));
+	return pte;
 }
 
 static inline pte_t pte_mkclean(pte_t pte)
@@ -169,16 +173,6 @@ static inline pte_t pte_mknoncont(pte_t pte)
 	return clear_pte_bit(pte, __pgprot(PTE_CONT));
 }
 
-static inline pte_t pte_clear_rdonly(pte_t pte)
-{
-	return clear_pte_bit(pte, __pgprot(PTE_RDONLY));
-}
-
-static inline pte_t pte_set_rdonly(pte_t pte)
-{
-	return set_pte_bit(pte, __pgprot(PTE_RDONLY));
-}
-
 static inline pte_t pte_mkpresent(pte_t pte)
 {
 	return set_pte_bit(pte, __pgprot(PTE_VALID));
@@ -226,14 +220,8 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_present(pte)) {
-		if (pte_sw_dirty(pte) && pte_write(pte))
-			pte_val(pte) &= ~PTE_RDONLY;
-		else
-			pte_val(pte) |= PTE_RDONLY;
-		if (pte_user_exec(pte) && !pte_special(pte))
-			__sync_icache_dcache(pte, addr);
-	}
+	if (pte_present(pte) && pte_user_exec(pte) && !pte_special(pte))
+		__sync_icache_dcache(pte, addr);
 
 	/*
 	 * If the existing pte is valid, check for potential race with
@@ -659,12 +647,10 @@ static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long addres
 		old_pte = pte;
 		/*
 		 * If hardware-dirty (PTE_WRITE/DBM bit set and PTE_RDONLY
-		 * clear), set the PTE_DIRTY and PTE_RDONLY bits.
+		 * clear), set the PTE_DIRTY bit.
 		 */
-		if (pte_hw_dirty(pte)) {
+		if (pte_hw_dirty(pte))
 			pte = pte_mkdirty(pte);
-			pte = pte_set_rdonly(pte);
-		}
 		pte = pte_wrprotect(pte);
 		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
 					       pte_val(old_pte), pte_val(pte));

commit 3bbf7157ac66a88d94b291d4d5e2b2a9319a0f90
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jun 26 14:27:36 2017 +0100

    arm64: Convert pte handling from inline asm to using (cmp)xchg
    
    With the support for hardware updates of the access and dirty states,
    the following pte handling functions had to be implemented using
    exclusives: __ptep_test_and_clear_young(), ptep_get_and_clear(),
    ptep_set_wrprotect() and ptep_set_access_flags(). To take advantage of
    the LSE atomic instructions and also make the code cleaner, convert
    these pte functions to use the more generic cmpxchg()/xchg().
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 6eae342ced6b..9127688ae775 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -39,6 +39,7 @@
 
 #ifndef __ASSEMBLY__
 
+#include <asm/cmpxchg.h>
 #include <asm/fixmap.h>
 #include <linux/mmdebug.h>
 
@@ -173,6 +174,11 @@ static inline pte_t pte_clear_rdonly(pte_t pte)
 	return clear_pte_bit(pte, __pgprot(PTE_RDONLY));
 }
 
+static inline pte_t pte_set_rdonly(pte_t pte)
+{
+	return set_pte_bit(pte, __pgprot(PTE_RDONLY));
+}
+
 static inline pte_t pte_mkpresent(pte_t pte)
 {
 	return set_pte_bit(pte, __pgprot(PTE_VALID));
@@ -593,20 +599,17 @@ static inline int pmdp_set_access_flags(struct vm_area_struct *vma,
 #define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
 static inline int __ptep_test_and_clear_young(pte_t *ptep)
 {
-	pteval_t pteval;
-	unsigned int tmp, res;
+	pte_t old_pte, pte;
 
-	asm volatile("//	__ptep_test_and_clear_young\n"
-	"	prfm	pstl1strm, %2\n"
-	"1:	ldxr	%0, %2\n"
-	"	ubfx	%w3, %w0, %5, #1	// extract PTE_AF (young)\n"
-	"	and	%0, %0, %4		// clear PTE_AF\n"
-	"	stxr	%w1, %0, %2\n"
-	"	cbnz	%w1, 1b\n"
-	: "=&r" (pteval), "=&r" (tmp), "+Q" (pte_val(*ptep)), "=&r" (res)
-	: "L" (~PTE_AF), "I" (ilog2(PTE_AF)));
+	pte = READ_ONCE(*ptep);
+	do {
+		old_pte = pte;
+		pte = pte_mkold(pte);
+		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
+					       pte_val(old_pte), pte_val(pte));
+	} while (pte_val(pte) != pte_val(old_pte));
 
-	return res;
+	return pte_young(pte);
 }
 
 static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
@@ -630,17 +633,7 @@ static inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm,
 				       unsigned long address, pte_t *ptep)
 {
-	pteval_t old_pteval;
-	unsigned int tmp;
-
-	asm volatile("//	ptep_get_and_clear\n"
-	"	prfm	pstl1strm, %2\n"
-	"1:	ldxr	%0, %2\n"
-	"	stxr	%w1, xzr, %2\n"
-	"	cbnz	%w1, 1b\n"
-	: "=&r" (old_pteval), "=&r" (tmp), "+Q" (pte_val(*ptep)));
-
-	return __pte(old_pteval);
+	return __pte(xchg_relaxed(&pte_val(*ptep), 0));
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
@@ -659,21 +652,23 @@ static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
 #define __HAVE_ARCH_PTEP_SET_WRPROTECT
 static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)
 {
-	pteval_t pteval;
-	unsigned long tmp;
-
-	asm volatile("//	ptep_set_wrprotect\n"
-	"	prfm	pstl1strm, %2\n"
-	"1:	ldxr	%0, %2\n"
-	"	tst	%0, %4			// check for hw dirty (!PTE_RDONLY)\n"
-	"	csel	%1, %3, xzr, eq		// set PTE_DIRTY|PTE_RDONLY if dirty\n"
-	"	orr	%0, %0, %1		// if !dirty, PTE_RDONLY is already set\n"
-	"	and	%0, %0, %5		// clear PTE_WRITE/PTE_DBM\n"
-	"	stxr	%w1, %0, %2\n"
-	"	cbnz	%w1, 1b\n"
-	: "=&r" (pteval), "=&r" (tmp), "+Q" (pte_val(*ptep))
-	: "r" (PTE_DIRTY|PTE_RDONLY), "L" (PTE_RDONLY), "L" (~PTE_WRITE)
-	: "cc");
+	pte_t old_pte, pte;
+
+	pte = READ_ONCE(*ptep);
+	do {
+		old_pte = pte;
+		/*
+		 * If hardware-dirty (PTE_WRITE/DBM bit set and PTE_RDONLY
+		 * clear), set the PTE_DIRTY and PTE_RDONLY bits.
+		 */
+		if (pte_hw_dirty(pte)) {
+			pte = pte_mkdirty(pte);
+			pte = pte_set_rdonly(pte);
+		}
+		pte = pte_wrprotect(pte);
+		pte_val(pte) = cmpxchg_relaxed(&pte_val(*ptep),
+					       pte_val(old_pte), pte_val(pte));
+	} while (pte_val(pte) != pte_val(old_pte));
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE

commit f02ab08afbe76ee7b0b2a34a9970e7dd200d8b01
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Thu Jun 8 18:25:26 2017 +0100

    arm64: hugetlb: Fix huge_pte_offset to return poisoned page table entries
    
    When memory failure is enabled, a poisoned hugepage pte is marked as a
    swap entry. huge_pte_offset() does not return the poisoned page table
    entries when it encounters PUD/PMD hugepages.
    
    This behaviour of huge_pte_offset() leads to error such as below when
    munmap is called on poisoned hugepages.
    
    [  344.165544] mm/pgtable-generic.c:33: bad pmd 000000083af00074.
    
    Fix huge_pte_offset() to return the poisoned pte which is then
    appropriately handled by the generic layer code.
    
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: David Woods <dwoods@mellanox.com>
    Tested-by: Manoj Iyer <manoj.iyer@canonical.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index c213fdbd056c..6eae342ced6b 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -441,7 +441,7 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 
 #define pud_none(pud)		(!pud_val(pud))
 #define pud_bad(pud)		(!(pud_val(pud) & PUD_TABLE_BIT))
-#define pud_present(pud)	(pud_val(pud))
+#define pud_present(pud)	pte_present(pud_pte(pud))
 
 static inline void set_pud(pud_t *pudp, pud_t pud)
 {

commit d27cfa1fc823d35a6cf45ba51f5623db8a14a9b9
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Mar 9 21:52:09 2017 +0100

    arm64: mm: set the contiguous bit for kernel mappings where appropriate
    
    This is the third attempt at enabling the use of contiguous hints for
    kernel mappings. The most recent attempt 0bfc445dec9d was reverted after
    it turned out that updating permission attributes on live contiguous ranges
    may result in TLB conflicts. So this time, the contiguous hint is not set
    for .rodata or for the linear alias of .text/.rodata, both of which are
    mapped read-write initially, and remapped read-only at a later stage.
    (Note that the latter region could also be unmapped and remapped again
    with updated permission attributes, given that the region, while live, is
    only mapped for the convenience of the hibernation code, but that also
    means the TLB footprint is negligible anyway, so why bother)
    
    This enables the following contiguous range sizes for the virtual mapping
    of the kernel image, and for the linear mapping:
    
              granule size |  cont PTE  |  cont PMD  |
              -------------+------------+------------+
                   4 KB    |    64 KB   |   32 MB    |
                  16 KB    |     2 MB   |    1 GB*   |
                  64 KB    |     2 MB   |   16 GB*   |
    
    * Only when built for 3 or more levels of translation. This is due to the
      fact that a 2 level configuration only consists of PGDs and PTEs, and the
      added complexity of dealing with folded PMDs is not justified considering
      that 16 GB contiguous ranges are likely to be ignored by the hardware (and
      16k/2 levels is a niche configuration)
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 0eef6064bf3b..c213fdbd056c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -74,6 +74,16 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_user_exec(pte)	(!(pte_val(pte) & PTE_UXN))
 #define pte_cont(pte)		(!!(pte_val(pte) & PTE_CONT))
 
+#define pte_cont_addr_end(addr, end)						\
+({	unsigned long __boundary = ((addr) + CONT_PTE_SIZE) & CONT_PTE_MASK;	\
+	(__boundary - 1 < (end) - 1) ? __boundary : (end);			\
+})
+
+#define pmd_cont_addr_end(addr, end)						\
+({	unsigned long __boundary = ((addr) + CONT_PMD_SIZE) & CONT_PMD_MASK;	\
+	(__boundary - 1 < (end) - 1) ? __boundary : (end);			\
+})
+
 #ifdef CONFIG_ARM64_HW_AFDBM
 #define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
 #else

commit ec663d967b2276448a416406ca59ff247c0c80c5
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jan 27 10:54:12 2017 +0000

    arm64: Improve detection of user/non-user mappings in set_pte(_at)
    
    Commit cab15ce604e5 ("arm64: Introduce execute-only page access
    permissions") allowed a valid user PTE to have the PTE_USER bit clear.
    As a consequence, the pte_valid_not_user() macro in set_pte() was
    replaced with pte_valid_global() under the assumption that only user
    pages have the nG bit set. EFI mappings, however, also have the nG bit
    set and set_pte() wrongly ignores issuing the DSB+ISB.
    
    This patch reinstates the pte_valid_not_user() macro and adds the
    PTE_UXN bit check since all kernel mappings have this bit set. For
    clarity, pte_exec() is renamed to pte_user_exec() as it only checks for
    the absence of PTE_UXN. Consequently, the user executable check in
    set_pte_at() drops the pte_ng() test since pte_user_exec() is
    sufficient.
    
    Fixes: cab15ce604e5 ("arm64: Introduce execute-only page access permissions")
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 090134c346db..0eef6064bf3b 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -71,9 +71,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_young(pte)		(!!(pte_val(pte) & PTE_AF))
 #define pte_special(pte)	(!!(pte_val(pte) & PTE_SPECIAL))
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
-#define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
+#define pte_user_exec(pte)	(!(pte_val(pte) & PTE_UXN))
 #define pte_cont(pte)		(!!(pte_val(pte) & PTE_CONT))
-#define pte_ng(pte)		(!!(pte_val(pte) & PTE_NG))
 
 #ifdef CONFIG_ARM64_HW_AFDBM
 #define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
@@ -84,8 +83,12 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
 
 #define pte_valid(pte)		(!!(pte_val(pte) & PTE_VALID))
-#define pte_valid_global(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_NG)) == PTE_VALID)
+/*
+ * Execute-only user mappings do not have the PTE_USER bit set. All valid
+ * kernel mappings have the PTE_UXN bit set.
+ */
+#define pte_valid_not_user(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_UXN)) == (PTE_VALID | PTE_UXN))
 #define pte_valid_young(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_AF)) == (PTE_VALID | PTE_AF))
 
@@ -178,7 +181,7 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
 	 * or update_mmu_cache() have the necessary barriers.
 	 */
-	if (pte_valid_global(pte)) {
+	if (pte_valid_not_user(pte)) {
 		dsb(ishst);
 		isb();
 	}
@@ -212,7 +215,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			pte_val(pte) &= ~PTE_RDONLY;
 		else
 			pte_val(pte) |= PTE_RDONLY;
-		if (pte_ng(pte) && pte_exec(pte) && !pte_special(pte))
+		if (pte_user_exec(pte) && !pte_special(pte))
 			__sync_icache_dcache(pte, addr);
 	}
 

commit 2077be6783b5936c3daa838d8addbb635667927f
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:49 2017 -0800

    arm64: Use __pa_symbol for kernel symbols
    
    __pa_symbol is technically the marcro that should be used for kernel
    symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which
    will do bounds checking.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index ffbb9a520563..090134c346db 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -52,7 +52,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
  * for zero-mapped memory areas etc..
  */
 extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
-#define ZERO_PAGE(vaddr)	pfn_to_page(PHYS_PFN(__pa(empty_zero_page)))
+#define ZERO_PAGE(vaddr)	phys_to_page(__pa_symbol(empty_zero_page))
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
 

commit 5ebe3a44cc744d11cb60d8438106a9322b7c04dc
Author: James Morse <james.morse@arm.com>
Date:   Wed Aug 24 18:27:30 2016 +0100

    arm64: hibernate: Support DEBUG_PAGEALLOC
    
    DEBUG_PAGEALLOC removes the valid bit of page table entries to prevent
    any access to unallocated memory. Hibernate uses this as a hint that those
    pages don't need to be saved/restored. This patch adds the
    kernel_page_present() function it uses.
    
    hibernate.c copies the resume kernel's linear map for use during restore.
    Add _copy_pte() to fill-in the holes made by DEBUG_PAGEALLOC in the resume
    kernel, so we can restore data the original kernel had at these addresses.
    
    Finally, DEBUG_PAGEALLOC means the linear-map alias of KERNEL_START to
    KERNEL_END may have holes in it, so we can't lazily clean this whole
    area to the PoC. Only clean the new mmuoff region, and the kernel/kvm
    idmaps.
    
    This reverts commit da24eb1f3f9e2c7b75c5f8c40d8e48e2c4789596.
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7ba1cebb64d9..ffbb9a520563 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -155,6 +155,16 @@ static inline pte_t pte_mknoncont(pte_t pte)
 	return clear_pte_bit(pte, __pgprot(PTE_CONT));
 }
 
+static inline pte_t pte_clear_rdonly(pte_t pte)
+{
+	return clear_pte_bit(pte, __pgprot(PTE_RDONLY));
+}
+
+static inline pte_t pte_mkpresent(pte_t pte)
+{
+	return set_pte_bit(pte, __pgprot(PTE_VALID));
+}
+
 static inline pmd_t pmd_mkcont(pmd_t pmd)
 {
 	return __pmd(pmd_val(pmd) | PMD_SECT_CONT);

commit cab15ce604e550020bb7115b779013b91bcdbc21
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Aug 11 18:44:50 2016 +0100

    arm64: Introduce execute-only page access permissions
    
    The ARMv8 architecture allows execute-only user permissions by clearing
    the PTE_UXN and PTE_USER bits. However, the kernel running on a CPU
    implementation without User Access Override (ARMv8.2 onwards) can still
    access such page, so execute-only page permission does not protect
    against read(2)/write(2) etc. accesses. Systems requiring such
    protection must enable features like SECCOMP.
    
    This patch changes the arm64 __P100 and __S100 protection_map[] macros
    to the new __PAGE_EXECONLY attributes. A side effect is that
    pte_user() no longer triggers for __PAGE_EXECONLY since PTE_USER isn't
    set. To work around this, the check is done on the PTE_NG bit via the
    pte_ng() macro. VM_READ is also checked now for page faults.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e20bd431184a..7ba1cebb64d9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -73,7 +73,7 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 #define pte_cont(pte)		(!!(pte_val(pte) & PTE_CONT))
-#define pte_user(pte)		(!!(pte_val(pte) & PTE_USER))
+#define pte_ng(pte)		(!!(pte_val(pte) & PTE_NG))
 
 #ifdef CONFIG_ARM64_HW_AFDBM
 #define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
@@ -84,8 +84,8 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
 
 #define pte_valid(pte)		(!!(pte_val(pte) & PTE_VALID))
-#define pte_valid_not_user(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_USER)) == PTE_VALID)
+#define pte_valid_global(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_NG)) == PTE_VALID)
 #define pte_valid_young(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_AF)) == (PTE_VALID | PTE_AF))
 
@@ -168,7 +168,7 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
 	 * or update_mmu_cache() have the necessary barriers.
 	 */
-	if (pte_valid_not_user(pte)) {
+	if (pte_valid_global(pte)) {
 		dsb(ishst);
 		isb();
 	}
@@ -202,7 +202,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			pte_val(pte) &= ~PTE_RDONLY;
 		else
 			pte_val(pte) |= PTE_RDONLY;
-		if (pte_user(pte) && pte_exec(pte) && !pte_special(pte))
+		if (pte_ng(pte) && pte_exec(pte) && !pte_special(pte))
 			__sync_icache_dcache(pte, addr);
 	}
 

commit 747a70e60b7234e8fd9c35dd2f2db10ac1db231d
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 3 15:15:55 2016 +0100

    arm64: Fix copy-on-write referencing in HugeTLB
    
    set_pte_at(.) will set or unset the PTE_RDONLY hardware bit before
    writing the entry to the table.
    
    This can cause problems with the copy-on-write logic in hugetlb_cow:
     *) hugetlb_cow(.) called to handle a write fault on read only pte,
     *) Before the copy-on-write updates the new page table a call is
        made to pte_same(huge_ptep_get(ptep), pte)), to check for a race,
     *) Because set_pte_at(.) changed the pte, *ptep != pte, and the
        hugetlb_cow(.) code erroneously assumes that it lost the race,
     *) The new page is subsequently freed without being used.
    
    On arm64 this problem only becomes apparent when we apply:
    67961f9 mm/hugetlb: fix huge page reserve accounting for private
    mappings
    
    When one runs the libhugetlbfs test suite, there are allocation errors
    and hugetlbfs pages become erroneously locked in memory as reserved.
    (There is a high HugePages_Rsvd: count).
    
    In this patch we introduce pte_same which ignores the PTE_RDONLY bit,
    allowing for the libhugetlbfs test suite to pass as expected and
    without leaking any reserved HugeTLB pages.
    
    Reported-by: Huang Shijie <shijie.huang@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 46472a91b6df..e20bd431184a 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -224,6 +224,23 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	set_pte(ptep, pte);
 }
 
+#define __HAVE_ARCH_PTE_SAME
+static inline int pte_same(pte_t pte_a, pte_t pte_b)
+{
+	pteval_t lhs, rhs;
+
+	lhs = pte_val(pte_a);
+	rhs = pte_val(pte_b);
+
+	if (pte_present(pte_a))
+		lhs &= ~PTE_RDONLY;
+
+	if (pte_present(pte_b))
+		rhs &= ~PTE_RDONLY;
+
+	return (lhs == rhs);
+}
+
 /*
  * Huge pte definitions.
  */

commit a05a70db34ba24ca009e1c9cedaef26fd17d5470
Merge: 03b979dd0323 4741526b83c5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 20:00:06 2016 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge updates from Andrew Morton:
    
     - fsnotify fix
    
     - poll() timeout fix
    
     - a few scripts/ tweaks
    
     - debugobjects updates
    
     - the (small) ocfs2 queue
    
     - Minor fixes to kernel/padata.c
    
     - Maybe half of the MM queue
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (117 commits)
      mm, page_alloc: restore the original nodemask if the fast path allocation failed
      mm, page_alloc: uninline the bad page part of check_new_page()
      mm, page_alloc: don't duplicate code in free_pcp_prepare
      mm, page_alloc: defer debugging checks of pages allocated from the PCP
      mm, page_alloc: defer debugging checks of freed pages until a PCP drain
      cpuset: use static key better and convert to new API
      mm, page_alloc: inline pageblock lookup in page free fast paths
      mm, page_alloc: remove unnecessary variable from free_pcppages_bulk
      mm, page_alloc: pull out side effects from free_pages_check
      mm, page_alloc: un-inline the bad part of free_pages_check
      mm, page_alloc: check multiple page fields with a single branch
      mm, page_alloc: remove field from alloc_context
      mm, page_alloc: avoid looking up the first zone in a zonelist twice
      mm, page_alloc: shortcut watermark checks for order-0 pages
      mm, page_alloc: reduce cost of fair zone allocation policy retry
      mm, page_alloc: shorten the page allocator fast path
      mm, page_alloc: check once if a zone has isolated pageblocks
      mm, page_alloc: move __GFP_HARDWALL modifications out of the fastpath
      mm, page_alloc: simplify last cpupid reset
      mm, page_alloc: remove unnecessary initialisation from __alloc_pages_nodemask()
      ...

commit fd8cfd3000191cb7f5b9ea8640bd46181f6b4b74
Author: Hugh Dickins <hughd@google.com>
Date:   Thu May 19 17:13:00 2016 -0700

    arch: fix has_transparent_hugepage()
    
    I've just discovered that the useful-sounding has_transparent_hugepage()
    is actually an architecture-dependent minefield: on some arches it only
    builds if CONFIG_TRANSPARENT_HUGEPAGE=y, on others it's also there when
    not, but on some of those (arm and arm64) it then gives the wrong
    answer; and on mips alone it's marked __init, which would crash if
    called later (but so far it has not been called later).
    
    Straighten this out: make it available to all configs, with a sensible
    default in asm-generic/pgtable.h, removing its definitions from those
    arches (arc, arm, arm64, sparc, tile) which are served by the default,
    adding #define has_transparent_hugepage has_transparent_hugepage to
    those (mips, powerpc, s390, x86) which need to override the default at
    runtime, and removing the __init from mips (but maybe that kind of code
    should be avoided after init: set a static variable the first time it's
    called).
    
    Signed-off-by: Hugh Dickins <hughd@google.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andres Lagar-Cavilla <andreslc@google.com>
    Cc: Yang Shi <yang.shi@linaro.org>
    Cc: Ning Qu <quning@gmail.com>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>            [arch/arc]
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>  [arch/s390]
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2da46ae9c991..a7ac45a03dd0 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -314,11 +314,6 @@ static inline int pmd_protnone(pmd_t pmd)
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 
-static inline int has_transparent_hugepage(void)
-{
-	return 1;
-}
-
 #define __pgprot_modify(prot,mask,bits) \
 	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
 

commit 7beaa24ba49717419e24d1f6321e8b3c265a719c
Merge: 07b75260ebc2 9842df62004f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 11:27:09 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small release overall.
    
      x86:
       - miscellaneous fixes
       - AVIC support (local APIC virtualization, AMD version)
    
      s390:
       - polling for interrupts after a VCPU goes to halted state is now
         enabled for s390
       - use hardware provided information about facility bits that do not
         need any hypervisor activity, and other fixes for cpu models and
         facilities
       - improve perf output
       - floating interrupt controller improvements.
    
      MIPS:
       - miscellaneous fixes
    
      PPC:
       - bugfixes only
    
      ARM:
       - 16K page size support
       - generic firmware probing layer for timer and GIC
    
      Christoffer Dall (KVM-ARM maintainer) says:
        "There are a few changes in this pull request touching things
         outside KVM, but they should all carry the necessary acks and it
         made the merge process much easier to do it this way."
    
      though actually the irqchip maintainers' acks didn't make it into the
      patches.  Marc Zyngier, who is both irqchip and KVM-ARM maintainer,
      later acked at http://mid.gmane.org/573351D1.4060303@arm.com ('more
      formally and for documentation purposes')"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (82 commits)
      KVM: MTRR: remove MSR 0x2f8
      KVM: x86: make hwapic_isr_update and hwapic_irr_update look the same
      svm: Manage vcpu load/unload when enable AVIC
      svm: Do not intercept CR8 when enable AVIC
      svm: Do not expose x2APIC when enable AVIC
      KVM: x86: Introducing kvm_x86_ops.apicv_post_state_restore
      svm: Add VMEXIT handlers for AVIC
      svm: Add interrupt injection via AVIC
      KVM: x86: Detect and Initialize AVIC support
      svm: Introduce new AVIC VMCB registers
      KVM: split kvm_vcpu_wake_up from kvm_vcpu_kick
      KVM: x86: Introducing kvm_x86_ops VCPU blocking/unblocking hooks
      KVM: x86: Introducing kvm_x86_ops VM init/destroy hooks
      KVM: x86: Rename kvm_apic_get_reg to kvm_lapic_get_reg
      KVM: x86: Misc LAPIC changes to expose helper functions
      KVM: shrink halt polling even more for invalid wakeups
      KVM: s390: set halt polling to 80 microseconds
      KVM: halt_polling: provide a way to qualify wakeups during poll
      KVM: PPC: Book3S HV: Re-enable XICS fast path for irqfd-generated interrupts
      kvm: Conditionally register IRQ bypass consumer
      ...

commit 06485053244480f5f403d8f89b8617bd7d549113
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 13 17:57:37 2016 +0100

    kvm: arm64: Enable hardware updates of the Access Flag for Stage 2 page tables
    
    The ARMv8.1 architecture extensions introduce support for hardware
    updates of the access and dirty information in page table entries. With
    VTCR_EL2.HA enabled (bit 21), when the CPU accesses an IPA with the
    PTE_AF bit cleared in the stage 2 page table, instead of raising an
    Access Flag fault to EL2 the CPU sets the actual page table entry bit
    (10). To ensure that kernel modifications to the page table do not
    inadvertently revert a bit set by hardware updates, certain Stage 2
    software pte/pmd operations must be performed atomically.
    
    The main user of the AF bit is the kvm_age_hva() mechanism. The
    kvm_age_hva_handler() function performs a "test and clear young" action
    on the pte/pmd. This needs to be atomic in respect of automatic hardware
    updates of the AF bit. Since the AF bit is in the same position for both
    Stage 1 and Stage 2, the patch reuses the existing
    ptep_test_and_clear_young() functionality if
    __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG is defined. Otherwise, the
    existing pte_young/pte_mkold mechanism is preserved.
    
    The kvm_set_s2pte_readonly() (and the corresponding pmd equivalent) have
    to perform atomic modifications in order to avoid a race with updates of
    the AF bit. The arm64 implementation has been re-written using
    exclusives.
    
    Currently, kvm_set_s2pte_writable() (and pmd equivalent) take a pointer
    argument and modify the pte/pmd in place. However, these functions are
    only used on local variables rather than actual page table entries, so
    it makes more sense to follow the pte_mkwrite() approach for stage 1
    attributes. The change to kvm_s2pte_mkwrite() makes it clear that these
    functions do not modify the actual page table entries.
    
    The (pte|pmd)_mkyoung() uses on Stage 2 entries (setting the AF bit
    explicitly) do not need to be modified since hardware updates of the
    dirty status are not supported by KVM, so there is no possibility of
    losing such information.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index dda4aa9ba3f8..f1d5afdb12db 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -532,14 +532,12 @@ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
  * Atomic pte/pmd modifications.
  */
 #define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
-static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
-					    unsigned long address,
-					    pte_t *ptep)
+static inline int __ptep_test_and_clear_young(pte_t *ptep)
 {
 	pteval_t pteval;
 	unsigned int tmp, res;
 
-	asm volatile("//	ptep_test_and_clear_young\n"
+	asm volatile("//	__ptep_test_and_clear_young\n"
 	"	prfm	pstl1strm, %2\n"
 	"1:	ldxr	%0, %2\n"
 	"	ubfx	%w3, %w0, %5, #1	// extract PTE_AF (young)\n"
@@ -552,6 +550,13 @@ static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
 	return res;
 }
 
+static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
+					    unsigned long address,
+					    pte_t *ptep)
+{
+	return __ptep_test_and_clear_young(ptep);
+}
+
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG
 static inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,

commit 5bb1cc0ff9a6b68871970737e6c4c16919928d8b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu May 5 10:44:02 2016 +0100

    arm64: Ensure pmd_present() returns false after pmd_mknotpresent()
    
    Currently, pmd_present() only checks for a non-zero value, returning
    true even after pmd_mknotpresent() (which only clears the type bits).
    This patch converts pmd_present() to using pte_present(), similar to the
    other pmd_*() checks. As a side effect, it will return true for
    PROT_NONE mappings, though they are not yet used by the kernel with
    transparent huge pages.
    
    For consistency, also change pmd_mknotpresent() to only clear the
    PMD_SECT_VALID bit, even though the PMD_TABLE_BIT is already 0 for block
    mappings (no functional change). The unused PMD_SECT_PROT_NONE
    definition is removed as transparent huge pages use the pte page prot
    values.
    
    Fixes: 9c7e535fcc17 ("arm64: mm: Route pmd thp functions through pte equivalents")
    Cc: <stable@vger.kernel.org> # 3.15+
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b90e4adfcf5e..2da46ae9c991 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -289,6 +289,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pmd_trans_huge(pmd)	(pmd_val(pmd) && !(pmd_val(pmd) & PMD_TABLE_BIT))
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
+#define pmd_present(pmd)	pte_present(pmd_pte(pmd))
 #define pmd_dirty(pmd)		pte_dirty(pmd_pte(pmd))
 #define pmd_young(pmd)		pte_young(pmd_pte(pmd))
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
@@ -297,7 +298,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pmd_mkclean(pmd)	pte_pmd(pte_mkclean(pmd_pte(pmd)))
 #define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))
 #define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
-#define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_TYPE_MASK))
+#define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_SECT_VALID))
 
 #define __HAVE_ARCH_PMD_WRITE
 #define pmd_write(pmd)		pte_write(pmd_pte(pmd))
@@ -336,7 +337,6 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 				     unsigned long size, pgprot_t vma_prot);
 
 #define pmd_none(pmd)		(!pmd_val(pmd))
-#define pmd_present(pmd)	(pmd_val(pmd))
 
 #define pmd_bad(pmd)		(!(pmd_val(pmd) & PMD_TABLE_BIT))
 

commit ab4db1f2244dfc5ecd7adf9927c29cd654cc14c6
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu May 5 10:44:01 2016 +0100

    arm64: Replace hard-coded values in the pmd/pud_bad() macros
    
    This patch replaces the hard-coded value 2 with PMD_TABLE_BIT in the
    pmd/pud_bad() macros. Note that using these macros on pmd_trans_huge()
    entries is giving incorrect results
    (pmd_none_or_trans_huge_or_clear_bad() correctly checks for
    pmd_trans_huge before pmd_bad).
    
    Additionally, white-space clean-up for pmd_mkclean().
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 072c335d4786..b90e4adfcf5e 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -294,7 +294,7 @@ static inline int pmd_protnone(pmd_t pmd)
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
 #define pmd_mkold(pmd)		pte_pmd(pte_mkold(pmd_pte(pmd)))
 #define pmd_mkwrite(pmd)	pte_pmd(pte_mkwrite(pmd_pte(pmd)))
-#define pmd_mkclean(pmd)       pte_pmd(pte_mkclean(pmd_pte(pmd)))
+#define pmd_mkclean(pmd)	pte_pmd(pte_mkclean(pmd_pte(pmd)))
 #define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))
 #define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
 #define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_TYPE_MASK))
@@ -338,7 +338,7 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 #define pmd_none(pmd)		(!pmd_val(pmd))
 #define pmd_present(pmd)	(pmd_val(pmd))
 
-#define pmd_bad(pmd)		(!(pmd_val(pmd) & 2))
+#define pmd_bad(pmd)		(!(pmd_val(pmd) & PMD_TABLE_BIT))
 
 #define pmd_table(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
 				 PMD_TYPE_TABLE)
@@ -403,7 +403,7 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 #define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
 
 #define pud_none(pud)		(!pud_val(pud))
-#define pud_bad(pud)		(!(pud_val(pud) & 2))
+#define pud_bad(pud)		(!(pud_val(pud) & PUD_TABLE_BIT))
 #define pud_present(pud)	(pud_val(pud))
 
 static inline void set_pud(pud_t *pudp, pud_t pud)

commit 282aa7051b0169991b34716f0f22d9c2f59c46c4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu May 5 10:44:00 2016 +0100

    arm64: Implement pmdp_set_access_flags() for hardware AF/DBM
    
    The update to the accessed or dirty states for block mappings must be
    done atomically on hardware with support for automatic AF/DBM. The
    ptep_set_access_flags() function has been fixed as part of commit
    66dbd6e61a52 ("arm64: Implement ptep_set_access_flags() for hardware
    AF/DBM"). This patch brings pmdp_set_access_flags() in line with the pte
    counterpart.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Cc: <stable@vger.kernel.org> # 4.4.x: 66dbd6e61a52: arm64: Implement ptep_set_access_flags() for hardware AF/DBM
    Cc: <stable@vger.kernel.org> # 4.3+
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b0ad16f05bcc..072c335d4786 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -540,6 +540,16 @@ extern int ptep_set_access_flags(struct vm_area_struct *vma,
 				 unsigned long address, pte_t *ptep,
 				 pte_t entry, int dirty);
 
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#define __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS
+static inline int pmdp_set_access_flags(struct vm_area_struct *vma,
+					unsigned long address, pmd_t *pmdp,
+					pmd_t entry, int dirty)
+{
+	return ptep_set_access_flags(vma, address, (pte_t *)pmdp, pmd_pte(entry), dirty);
+}
+#endif
+
 /*
  * Atomic pte/pmd modifications.
  */

commit 911f56eeb87ee378f5e215469268a7a2f68a5a8a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu May 5 10:43:59 2016 +0100

    arm64: Fix typo in the pmdp_huge_get_and_clear() definition
    
    With hardware AF/DBM support, pmd modifications (transparent huge pages)
    should be performed atomically using load/store exclusive. The initial
    patches defined the get-and-clear function and __HAVE_ARCH_* macro
    without the "huge" word, leaving the pmdp_huge_get_and_clear() to the
    default, non-atomic implementation.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Cc: <stable@vger.kernel.org> # 4.3+
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3eefc181d00b..b0ad16f05bcc 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -592,9 +592,9 @@ static inline pte_t ptep_get_and_clear(struct mm_struct *mm,
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
-#define __HAVE_ARCH_PMDP_GET_AND_CLEAR
-static inline pmd_t pmdp_get_and_clear(struct mm_struct *mm,
-				       unsigned long address, pmd_t *pmdp)
+#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
+static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
+					    unsigned long address, pmd_t *pmdp)
 {
 	return pte_pmd(ptep_get_and_clear(mm, address, (pte_t *)pmdp));
 }

commit 0dbd3b18c63c81dec1a8c47667d89c54ade9b52a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Mar 15 10:46:34 2016 +0000

    arm64: Introduce pmd_thp_or_huge
    
    Add a helper to determine if a given pmd represents a huge page
    either by hugetlb or thp, as we have for arm. This will be used
    by KVM MMU code.
    
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 989fef16d461..dda4aa9ba3f8 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -290,6 +290,8 @@ static inline pgprot_t mk_sect_prot(pgprot_t prot)
 #define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
 #define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_TYPE_MASK))
 
+#define pmd_thp_or_huge(pmd)	(pmd_huge(pmd) || pmd_trans_huge(pmd))
+
 #define __HAVE_ARCH_PMD_WRITE
 #define pmd_write(pmd)		pte_write(pmd_pte(pmd))
 

commit 66dbd6e61a526ae7d11a208238ae2c17e5cacb6b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 13 16:01:22 2016 +0100

    arm64: Implement ptep_set_access_flags() for hardware AF/DBM
    
    When hardware updates of the access and dirty states are enabled, the
    default ptep_set_access_flags() implementation based on calling
    set_pte_at() directly is potentially racy. This triggers the "racy dirty
    state clearing" warning in set_pte_at() because an existing writable PTE
    is overridden with a clean entry.
    
    There are two main scenarios for this situation:
    
    1. The CPU getting an access fault does not support hardware updates of
       the access/dirty flags. However, a different agent in the system
       (e.g. SMMU) can do this, therefore overriding a writable entry with a
       clean one could potentially lose the automatically updated dirty
       status
    
    2. A more complex situation is possible when all CPUs support hardware
       AF/DBM:
    
       a) Initial state: shareable + writable vma and pte_none(pte)
       b) Read fault taken by two threads of the same process on different
          CPUs
       c) CPU0 takes the mmap_sem and proceeds to handling the fault. It
          eventually reaches do_set_pte() which sets a writable + clean pte.
          CPU0 releases the mmap_sem
       d) CPU1 acquires the mmap_sem and proceeds to handle_pte_fault(). The
          pte entry it reads is present, writable and clean and it continues
          to pte_mkyoung()
       e) CPU1 calls ptep_set_access_flags()
    
       If between (d) and (e) the hardware (another CPU) updates the dirty
       state (clears PTE_RDONLY), CPU1 will override the PTR_RDONLY bit
       marking the entry clean again.
    
    This patch implements an arm64-specific ptep_set_access_flags() function
    to perform an atomic update of the PTE flags.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Ming Lei <tom.leiming@gmail.com>
    Tested-by: Julien Grall <julien.grall@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org> # 4.3+
    [will: reworded comment]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3c91b179089a..3eefc181d00b 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -535,6 +535,11 @@ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
 }
 
 #ifdef CONFIG_ARM64_HW_AFDBM
+#define __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS
+extern int ptep_set_access_flags(struct vm_area_struct *vma,
+				 unsigned long address, pte_t *ptep,
+				 pte_t entry, int dirty);
+
 /*
  * Atomic pte/pmd modifications.
  */

commit 561662301eb6a40d569453b3555fc8cfce094b93
Author: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
Date:   Fri Apr 8 15:50:28 2016 -0700

    arm64, mm, numa: Add NUMA balancing support for arm64.
    
    Enable NUMA balancing for arm64 platforms.
    Add pte, pmd protnone helpers for use by automatic NUMA balancing.
    
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Robert Richter <rrichter@cavium.com>
    Signed-off-by: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2abfa4d09e65..3c91b179089a 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -266,6 +266,21 @@ static inline pgprot_t mk_sect_prot(pgprot_t prot)
 	return __pgprot(pgprot_val(prot) & ~PTE_TABLE_BIT);
 }
 
+#ifdef CONFIG_NUMA_BALANCING
+/*
+ * See the comment in include/asm-generic/pgtable.h
+ */
+static inline int pte_protnone(pte_t pte)
+{
+	return (pte_val(pte) & (PTE_VALID | PTE_PROT_NONE)) == PTE_PROT_NONE;
+}
+
+static inline int pmd_protnone(pmd_t pmd)
+{
+	return pte_protnone(pmd_pte(pmd));
+}
+#endif
+
 /*
  * THP definitions.
  */

commit 3e1907d5bf5a1e0b182ee599f92586f0165029e2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 16:46:00 2016 +0200

    arm64: mm: move vmemmap region right below the linear region
    
    This moves the vmemmap region right below PAGE_OFFSET, aka the start
    of the linear region, and redefines its size to be a power of two.
    Due to the placement of PAGE_OFFSET in the middle of the address space,
    whose size is a power of two as well, this guarantees that virt to
    page conversions and vice versa can be implemented efficiently, by
    masking and shifting rather than ordinary arithmetic.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 377257a8d393..2abfa4d09e65 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -24,20 +24,15 @@
 #include <asm/pgtable-prot.h>
 
 /*
- * VMALLOC and SPARSEMEM_VMEMMAP ranges.
+ * VMALLOC range.
  *
- * VMEMAP_SIZE: allows the whole linear region to be covered by a struct page array
- *	(rounded up to PUD_SIZE).
  * VMALLOC_START: beginning of the kernel vmalloc space
- * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space,
- *	fixed mappings and modules
+ * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space
+ *	and fixed mappings
  */
-#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT - 1)) * sizeof(struct page), PUD_SIZE)
-
 #define VMALLOC_START		(MODULES_END)
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
-#define VMEMMAP_START		(VMALLOC_END + SZ_64K)
 #define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
 
 #define FIRST_USER_ADDRESS	0UL

commit 22b6f3b0549be61a2d5fe8210ff7628e6d2d8185
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 16:45:58 2016 +0200

    arm64: mm: avoid virt_to_page() translation for the zero page
    
    The zero page is statically allocated, so grab its struct page pointer
    without using virt_to_page(), which will be restricted to the linear
    mapping later.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index aa6106ac050c..377257a8d393 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -57,7 +57,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
  * for zero-mapped memory areas etc..
  */
 extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
-#define ZERO_PAGE(vaddr)	virt_to_page(empty_zero_page)
+#define ZERO_PAGE(vaddr)	pfn_to_page(PHYS_PFN(__pa(empty_zero_page)))
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
 

commit 3bab79edc67152cb6cadb12773b7c7d05228eb77
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 14:25:48 2016 +0200

    Revert "arm64: account for sparsemem section alignment when choosing vmemmap offset"
    
    This reverts commit 36e5cd6b897e17d03008f81e075625d8e43e52d0, since the
    section alignment is now guaranteed by construction when choosing the
    value of memstart_addr.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 989fef16d461..aa6106ac050c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -32,14 +32,13 @@
  * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space,
  *	fixed mappings and modules
  */
-#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
+#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT - 1)) * sizeof(struct page), PUD_SIZE)
 
 #define VMALLOC_START		(MODULES_END)
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define VMEMMAP_START		(VMALLOC_END + SZ_64K)
-#define vmemmap			((struct page *)VMEMMAP_START - \
-				 SECTION_ALIGN_DOWN(memstart_addr >> PAGE_SHIFT))
+#define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
 
 #define FIRST_USER_ADDRESS	0UL
 

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit fdc69e7df3cb24f18a93192641786e5b7ecd1dfe
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 9 16:31:29 2016 +0000

    arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
    
    The set_pte_at() function must update the hardware PTE_RDONLY bit
    depending on the state of the PTE_WRITE and PTE_DIRTY bits of the given
    entry value. However, it currently only performs this for pte_valid()
    entries, ignoring PTE_PROT_NONE. The side-effect is that PROT_NONE
    mappings would not have the PTE_RDONLY bit set. Without
    CONFIG_ARM64_HW_AFDBM, this is not an issue since such PROT_NONE pages
    are not accessible anyway.
    
    With commit 2f4b829c625e ("arm64: Add support for hardware updates of
    the access and dirty pte bits"), the ptep_set_wrprotect() function was
    re-written to cope with automatic hardware updates of the dirty state.
    As an optimisation, only PTE_RDONLY is checked to assess the "dirty"
    status. Since set_pte_at() does not set this bit for PROT_NONE mappings,
    such pages may be considered "dirty" as a result of
    ptep_set_wrprotect().
    
    This patch updates the pte_valid() check to pte_present() in
    set_pte_at(). It also adds PTE_PROT_NONE to the swap entry bits comment.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Tested-by: Ganapatrao Kulkarni <gkulkarni@cavium.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7c73b365fcfa..e308807105e2 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -201,7 +201,7 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_valid(pte)) {
+	if (pte_present(pte)) {
 		if (pte_sw_dirty(pte) && pte_write(pte))
 			pte_val(pte) &= ~PTE_RDONLY;
 		else
@@ -626,6 +626,7 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
  *	bits 0-1:	present (must be zero)
  *	bits 2-7:	swap type
  *	bits 8-57:	swap offset
+ *	bit  58:	PTE_PROT_NONE (must be zero)
  */
 #define __SWP_TYPE_SHIFT	2
 #define __SWP_TYPE_BITS		6

commit 36e5cd6b897e17d03008f81e075625d8e43e52d0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 8 21:09:29 2016 +0700

    arm64: account for sparsemem section alignment when choosing vmemmap offset
    
    Commit dfd55ad85e4a ("arm64: vmemmap: use virtual projection of linear
    region") fixed an issue where the struct page array would overflow into the
    adjacent virtual memory region if system RAM was placed so high up in
    physical memory that its addresses were not representable in the build time
    configured virtual address size.
    
    However, the fix failed to take into account that the vmemmap region needs
    to be relatively aligned with respect to the sparsemem section size, so that
    a sequence of page structs corresponding with a sparsemem section in the
    linear region appears naturally aligned in the vmemmap region.
    
    So round up vmemmap to sparsemem section size. Since this essentially moves
    the projection of the linear region up in memory, also revert the reduction
    of the size of the vmemmap region.
    
    Cc: <stable@vger.kernel.org>
    Fixes: dfd55ad85e4a ("arm64: vmemmap: use virtual projection of linear region")
    Tested-by: Mark Langsdorf <mlangsdo@redhat.com>
    Tested-by: David Daney <david.daney@cavium.com>
    Tested-by: Robert Richter <rrichter@cavium.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index f50608674580..819aff5d593f 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -40,7 +40,7 @@
  * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space,
  *	fixed mappings and modules
  */
-#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT - 1)) * sizeof(struct page), PUD_SIZE)
+#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
 
 #ifndef CONFIG_KASAN
 #define VMALLOC_START		(VA_START)
@@ -52,7 +52,8 @@
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define VMEMMAP_START		(VMALLOC_END + SZ_64K)
-#define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
+#define vmemmap			((struct page *)VMEMMAP_START - \
+				 SECTION_ALIGN_DOWN(memstart_addr >> PAGE_SHIFT))
 
 #define FIRST_USER_ADDRESS	0UL
 

commit dfd55ad85e4a7fbaa82df12467515ac3c81e8a3e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Feb 26 17:57:13 2016 +0100

    arm64: vmemmap: use virtual projection of linear region
    
    Commit dd006da21646 ("arm64: mm: increase VA range of identity map") made
    some changes to the memory mapping code to allow physical memory to reside
    at an offset that exceeds the size of the virtual mapping.
    
    However, since the size of the vmemmap area is proportional to the size of
    the VA area, but it is populated relative to the physical space, we may
    end up with the struct page array being mapped outside of the vmemmap
    region. For instance, on my Seattle A0 box, I can see the following output
    in the dmesg log.
    
       vmemmap : 0xffffffbdc0000000 - 0xffffffbfc0000000   (     8 GB maximum)
                 0xffffffbfc0000000 - 0xffffffbfd0000000   (   256 MB actual)
    
    We can fix this by deciding that the vmemmap region is not a projection of
    the physical space, but of the virtual space above PAGE_OFFSET, i.e., the
    linear region. This way, we are guaranteed that the vmemmap region is of
    sufficient size, and we can even reduce the size by half.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index bf464de33f52..f50608674580 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -34,13 +34,13 @@
 /*
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
  *
- * VMEMAP_SIZE: allows the whole VA space to be covered by a struct page array
+ * VMEMAP_SIZE: allows the whole linear region to be covered by a struct page array
  *	(rounded up to PUD_SIZE).
  * VMALLOC_START: beginning of the kernel VA space
  * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space,
  *	fixed mappings and modules
  */
-#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
+#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT - 1)) * sizeof(struct page), PUD_SIZE)
 
 #ifndef CONFIG_KASAN
 #define VMALLOC_START		(VA_START)
@@ -51,7 +51,8 @@
 
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
-#define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))
+#define VMEMMAP_START		(VMALLOC_END + SZ_64K)
+#define vmemmap			((struct page *)VMEMMAP_START - (memstart_addr >> PAGE_SHIFT))
 
 #define FIRST_USER_ADDRESS	0UL
 

commit 3eca86e75ec7a7d4b9a9c8091b11676f7bd2a39f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Feb 26 14:31:32 2016 +0000

    arm64: Remove fixmap include fragility
    
    The asm-generic fixmap.h depends on each architecture's fixmap.h to pull
    in the definition of PAGE_KERNEL_RO, if this exists. In the absence of
    this, FIXMAP_PAGE_RO will not be defined. In mm/early_ioremap.c the
    definition of early_memremap_ro is predicated on FIXMAP_PAGE_RO being
    defined.
    
    Currently, the arm64 fixmap.h doesn't include pgtable.h for the
    definition of PAGE_KERNEL_RO, and as a knock-on effect early_memremap_ro
    is not always defined, leading to link-time failures when it is used.
    This has been observed with defconfig on next-20160226.
    
    Unfortunately, as pgtable.h includes fixmap.h, adding the include
    introduces a circular dependency, which is just as fragile.
    
    Instead, this patch factors out PAGE_KERNEL_RO and other prot
    definitions into a new pgtable-prot header which can be included by poth
    pgtable.h and fixmap.h, avoiding the  circular dependency, and ensuring
    that early_memremap_ro is alwyas defined where it is used.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 16438dd8916a..7c73b365fcfa 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -21,15 +21,7 @@
 
 #include <asm/memory.h>
 #include <asm/pgtable-hwdef.h>
-
-/*
- * Software defined PTE bits definition.
- */
-#define PTE_VALID		(_AT(pteval_t, 1) << 0)
-#define PTE_WRITE		(PTE_DBM)		 /* same as DBM (51) */
-#define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
-#define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
-#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 58) /* only when !PTE_VALID */
+#include <asm/pgtable-prot.h>
 
 /*
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
@@ -59,59 +51,6 @@ extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pud_error(const char *file, int line, unsigned long val);
 extern void __pgd_error(const char *file, int line, unsigned long val);
 
-#define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
-#define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
-
-#define PROT_DEVICE_nGnRnE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_DEVICE_nGnRnE))
-#define PROT_DEVICE_nGnRE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_DEVICE_nGnRE))
-#define PROT_NORMAL_NC		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL_NC))
-#define PROT_NORMAL_WT		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL_WT))
-#define PROT_NORMAL		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL))
-
-#define PROT_SECT_DEVICE_nGnRE	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_DEVICE_nGnRE))
-#define PROT_SECT_NORMAL	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
-#define PROT_SECT_NORMAL_EXEC	(PROT_SECT_DEFAULT | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
-
-#define _PAGE_DEFAULT		(PROT_DEFAULT | PTE_ATTRINDX(MT_NORMAL))
-
-#define PAGE_KERNEL		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
-#define PAGE_KERNEL_RO		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
-#define PAGE_KERNEL_ROX		__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
-#define PAGE_KERNEL_EXEC	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
-#define PAGE_KERNEL_EXEC_CONT	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_CONT)
-
-#define PAGE_HYP		__pgprot(_PAGE_DEFAULT | PTE_HYP)
-#define PAGE_HYP_DEVICE		__pgprot(PROT_DEVICE_nGnRE | PTE_HYP)
-
-#define PAGE_S2			__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
-#define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDONLY | PTE_UXN)
-
-#define PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_VALID) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
-#define PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
-#define PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
-#define PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define PAGE_COPY_EXEC		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
-#define PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
-
-#define __P000  PAGE_NONE
-#define __P001  PAGE_READONLY
-#define __P010  PAGE_COPY
-#define __P011  PAGE_COPY
-#define __P100  PAGE_READONLY_EXEC
-#define __P101  PAGE_READONLY_EXEC
-#define __P110  PAGE_COPY_EXEC
-#define __P111  PAGE_COPY_EXEC
-
-#define __S000  PAGE_NONE
-#define __S001  PAGE_READONLY
-#define __S010  PAGE_SHARED
-#define __S011  PAGE_SHARED
-#define __S100  PAGE_READONLY_EXEC
-#define __S101  PAGE_READONLY_EXEC
-#define __S110  PAGE_SHARED_EXEC
-#define __S111  PAGE_SHARED_EXEC
-
 /*
  * ZERO_PAGE is a global shared page that is always zero: used
  * for zero-mapped memory areas etc..

commit cac4b8cdf5a20a11d1725b832350c044d9e13d29
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 25 15:53:44 2016 +0000

    arm64: Fix building error with 16KB pages and 36-bit VA
    
    In such configuration, Linux uses only two pages of page tables and
    __pud_populate() should not be used. However, the BUILD_BUG() triggers
    since pud_sect() is still defined and the compiler cannot eliminate such
    code, even though at run-time it should not be triggered. This patch
    extends the #ifdef ARM64_64K_PAGES condition for pud_sect to include
    PGTABLE_LEVELS < 3.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index a440f5a85d08..16438dd8916a 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -395,7 +395,7 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 #define pmd_sect(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
 				 PMD_TYPE_SECT)
 
-#ifdef CONFIG_ARM64_64K_PAGES
+#if defined(CONFIG_ARM64_64K_PAGES) || CONFIG_PGTABLE_LEVELS < 3
 #define pud_sect(pud)		(0)
 #define pud_table(pud)		(1)
 #else

commit f9040773b7bbbd9e98eb6184a263512a7cfc133f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:40 2016 +0100

    arm64: move kernel image to base of vmalloc area
    
    This moves the module area to right before the vmalloc area, and moves
    the kernel image to the base of the vmalloc area. This is an intermediate
    step towards implementing KASLR, which allows the kernel image to be
    located anywhere in the vmalloc area.
    
    Since other subsystems such as hibernate may still need to refer to the
    kernel text or data segments via their linears addresses, both are mapped
    in the linear region as well. The linear alias of the text region is
    mapped read-only/non-executable to prevent inadvertent modification or
    execution.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 87355408d448..a440f5a85d08 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -36,19 +36,13 @@
  *
  * VMEMAP_SIZE: allows the whole VA space to be covered by a struct page array
  *	(rounded up to PUD_SIZE).
- * VMALLOC_START: beginning of the kernel VA space
+ * VMALLOC_START: beginning of the kernel vmalloc space
  * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space,
  *	fixed mappings and modules
  */
 #define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
 
-#ifndef CONFIG_KASAN
-#define VMALLOC_START		(VA_START)
-#else
-#include <asm/kasan.h>
-#define VMALLOC_START		(KASAN_SHADOW_END + SZ_64K)
-#endif
-
+#define VMALLOC_START		(MODULES_END)
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))

commit 6533945a32c762c5db70d7a3ec251a040b2d9661
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:37 2016 +0100

    arm64: pgtable: implement static [pte|pmd|pud]_offset variants
    
    The page table accessors pte_offset(), pud_offset() and pmd_offset()
    rely on __va translations, so they can only be used after the linear
    mapping has been installed. For the early fixmap and kasan init routines,
    whose page tables are allocated statically in the kernel image, these
    functions will return bogus values. So implement pte_offset_kimg(),
    pmd_offset_kimg() and pud_offset_kimg(), which can be used instead
    before any page tables have been allocated dynamically.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 4229f75fd145..87355408d448 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -445,6 +445,9 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 
 #define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
 
+/* use ONLY for statically allocated translation tables */
+#define pte_offset_kimg(dir,addr)	((pte_t *)__phys_to_kimg(pte_offset_phys((dir), (addr))))
+
 /*
  * Conversion functions: convert a page and protection to a page entry,
  * and a page entry and page directory to the page they refer to.
@@ -488,6 +491,9 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 
 #define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
 
+/* use ONLY for statically allocated translation tables */
+#define pmd_offset_kimg(dir,addr)	((pmd_t *)__phys_to_kimg(pmd_offset_phys((dir), (addr))))
+
 #else
 
 #define pud_page_paddr(pud)	({ BUILD_BUG(); 0; })
@@ -497,6 +503,8 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 #define pmd_set_fixmap_offset(pudp, addr)	((pmd_t *)pudp)
 #define pmd_clear_fixmap()
 
+#define pmd_offset_kimg(dir,addr)	((pmd_t *)dir)
+
 #endif	/* CONFIG_PGTABLE_LEVELS > 2 */
 
 #if CONFIG_PGTABLE_LEVELS > 3
@@ -535,6 +543,9 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 
 #define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
 
+/* use ONLY for statically allocated translation tables */
+#define pud_offset_kimg(dir,addr)	((pud_t *)__phys_to_kimg(pud_offset_phys((dir), (addr))))
+
 #else
 
 #define pgd_page_paddr(pgd)	({ BUILD_BUG(); 0;})
@@ -544,6 +555,8 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 #define pud_set_fixmap_offset(pgdp, addr)	((pud_t *)pgdp)
 #define pud_clear_fixmap()
 
+#define pud_offset_kimg(dir,addr)	((pud_t *)dir)
+
 #endif  /* CONFIG_PGTABLE_LEVELS > 3 */
 
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))

commit 961faac114819a01e627fe9c9c82b830bb3849d4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 25 11:45:07 2016 +0000

    arm64: mm: add functions to walk tables in fixmap
    
    As a preparatory step to allow us to allocate early page tables from
    unmapped memory using memblock_alloc, add new p??_{set,clear}_fixmap*
    functions which can be used to walk page tables outside of the linear
    mapping by using fixmap slots.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 842c192a1ca1..4229f75fd145 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -57,6 +57,7 @@
 
 #ifndef __ASSEMBLY__
 
+#include <asm/fixmap.h>
 #include <linux/mmdebug.h>
 
 extern void __pte_error(const char *file, int line, unsigned long val);
@@ -438,6 +439,10 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 #define pte_unmap(pte)			do { } while (0)
 #define pte_unmap_nested(pte)		do { } while (0)
 
+#define pte_set_fixmap(addr)		((pte_t *)set_fixmap_offset(FIX_PTE, addr))
+#define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))
+#define pte_clear_fixmap()		clear_fixmap(FIX_PTE)
+
 #define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
 
 /*
@@ -477,12 +482,21 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
 #define pmd_offset_phys(dir, addr)	(pud_page_paddr(*(dir)) + pmd_index(addr) * sizeof(pmd_t))
 #define pmd_offset(dir, addr)		((pmd_t *)__va(pmd_offset_phys((dir), (addr))))
 
+#define pmd_set_fixmap(addr)		((pmd_t *)set_fixmap_offset(FIX_PMD, addr))
+#define pmd_set_fixmap_offset(pud, addr)	pmd_set_fixmap(pmd_offset_phys(pud, addr))
+#define pmd_clear_fixmap()		clear_fixmap(FIX_PMD)
+
 #define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
 
 #else
 
 #define pud_page_paddr(pud)	({ BUILD_BUG(); 0; })
 
+/* Match pmd_offset folding in <asm/generic/pgtable-nopmd.h> */
+#define pmd_set_fixmap(addr)		NULL
+#define pmd_set_fixmap_offset(pudp, addr)	((pmd_t *)pudp)
+#define pmd_clear_fixmap()
+
 #endif	/* CONFIG_PGTABLE_LEVELS > 2 */
 
 #if CONFIG_PGTABLE_LEVELS > 3
@@ -515,12 +529,21 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 #define pud_offset_phys(dir, addr)	(pgd_page_paddr(*(dir)) + pud_index(addr) * sizeof(pud_t))
 #define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
 
+#define pud_set_fixmap(addr)		((pud_t *)set_fixmap_offset(FIX_PUD, addr))
+#define pud_set_fixmap_offset(pgd, addr)	pud_set_fixmap(pud_offset_phys(pgd, addr))
+#define pud_clear_fixmap()		clear_fixmap(FIX_PUD)
+
 #define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
 
 #else
 
 #define pgd_page_paddr(pgd)	({ BUILD_BUG(); 0;})
 
+/* Match pud_offset folding in <asm/generic/pgtable-nopud.h> */
+#define pud_set_fixmap(addr)		NULL
+#define pud_set_fixmap_offset(pgdp, addr)	((pud_t *)pgdp)
+#define pud_clear_fixmap()
+
 #endif  /* CONFIG_PGTABLE_LEVELS > 3 */
 
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
@@ -535,6 +558,9 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 /* to find an entry in a kernel page-table-directory */
 #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
 
+#define pgd_set_fixmap(addr)	((pgd_t *)set_fixmap_offset(FIX_PGD, addr))
+#define pgd_clear_fixmap()	clear_fixmap(FIX_PGD)
+
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |

commit dca56dca7124709f3dfca81afe61b4d98eb9cacf
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 25 11:45:04 2016 +0000

    arm64: mm: add functions to walk page tables by PA
    
    To allow us to walk tables allocated into the fixmap, we need to acquire
    the physical address of a page, rather than the virtual address in the
    linear map.
    
    This patch adds new p??_page_paddr and p??_offset_phys functions to
    acquire the physical address of a next-level table, and changes
    p??_offset* into macros which simply convert this to a linear map VA.
    This renders p??_page_vaddr unused, and hence they are removed.
    
    At the pgd level, a new pgd_offset_raw function is added to find the
    relevant PGD entry given the base of a PGD and a virtual address.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e68ef89a7020..842c192a1ca1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -422,15 +422,16 @@ static inline void pmd_clear(pmd_t *pmdp)
 	set_pmd(pmdp, __pmd(0));
 }
 
-static inline pte_t *pmd_page_vaddr(pmd_t pmd)
+static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
 {
-	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
+	return pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK;
 }
 
 /* Find an entry in the third-level page table. */
 #define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
 
-#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
+#define pte_offset_phys(dir,addr)	(pmd_page_paddr(*(dir)) + pte_index(addr) * sizeof(pte_t))
+#define pte_offset_kernel(dir,addr)	((pte_t *)__va(pte_offset_phys((dir), (addr))))
 
 #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
 #define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
@@ -465,21 +466,23 @@ static inline void pud_clear(pud_t *pudp)
 	set_pud(pudp, __pud(0));
 }
 
-static inline pmd_t *pud_page_vaddr(pud_t pud)
+static inline phys_addr_t pud_page_paddr(pud_t pud)
 {
-	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
+	return pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK;
 }
 
 /* Find an entry in the second-level page table. */
 #define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
 
-static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
-{
-	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
-}
+#define pmd_offset_phys(dir, addr)	(pud_page_paddr(*(dir)) + pmd_index(addr) * sizeof(pmd_t))
+#define pmd_offset(dir, addr)		((pmd_t *)__va(pmd_offset_phys((dir), (addr))))
 
 #define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
 
+#else
+
+#define pud_page_paddr(pud)	({ BUILD_BUG(); 0; })
+
 #endif	/* CONFIG_PGTABLE_LEVELS > 2 */
 
 #if CONFIG_PGTABLE_LEVELS > 3
@@ -501,21 +504,23 @@ static inline void pgd_clear(pgd_t *pgdp)
 	set_pgd(pgdp, __pgd(0));
 }
 
-static inline pud_t *pgd_page_vaddr(pgd_t pgd)
+static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
 {
-	return __va(pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK);
+	return pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK;
 }
 
 /* Find an entry in the frst-level page table. */
 #define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
 
-static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
-{
-	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(addr);
-}
+#define pud_offset_phys(dir, addr)	(pgd_page_paddr(*(dir)) + pud_index(addr) * sizeof(pud_t))
+#define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
 
 #define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
 
+#else
+
+#define pgd_page_paddr(pgd)	({ BUILD_BUG(); 0;})
+
 #endif  /* CONFIG_PGTABLE_LEVELS > 3 */
 
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
@@ -523,7 +528,9 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 /* to find an entry in a page-table-directory */
 #define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
 
-#define pgd_offset(mm, addr)	((mm)->pgd+pgd_index(addr))
+#define pgd_offset_raw(pgd, addr)	((pgd) + pgd_index(addr))
+
+#define pgd_offset(mm, addr)	(pgd_offset_raw((mm)->pgd, (addr)))
 
 /* to find an entry in a kernel page-table-directory */
 #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)

commit 053520f7d3923cc6d37afb28f9887cb1e7d77454
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 25 11:45:03 2016 +0000

    arm64: mm: move pte_* macros
    
    For pmd, pud, and pgd levels of table, functions including p?d_index and
    p?d_offset are defined after the p?d_page_vaddr function for the
    immediately higher level of table.
    
    The pte functions however are defined much earlier, even though several
    rely on the later definition of pmd_page_vaddr. While this isn't
    currently a problem as these are macros, it prevents the logical
    grouping of later C functions (which cannot rely on prototypes for
    functions not yet defined).
    
    Move these definitions after pmd_page_vaddr, for consistency with the
    placement of these functions for other levels of table.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 310710f45aea..e68ef89a7020 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -134,16 +134,6 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
 #define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
 #define pte_page(pte)		(pfn_to_page(pte_pfn(pte)))
 
-/* Find an entry in the third-level page table. */
-#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
-
-#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
-
-#define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
-#define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
-#define pte_unmap(pte)			do { } while (0)
-#define pte_unmap_nested(pte)		do { } while (0)
-
 /*
  * The following only work if pte_present(). Undefined behaviour otherwise.
  */
@@ -437,6 +427,16 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
 	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
 }
 
+/* Find an entry in the third-level page table. */
+#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+
+#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
+
+#define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
+#define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
+#define pte_unmap(pte)			do { } while (0)
+#define pte_unmap_nested(pte)		do { } while (0)
+
 #define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
 
 /*

commit 5227cfa71f9e8574373f4d0e9e754942d76cdf67
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 25 11:44:57 2016 +0000

    arm64: mm: place empty_zero_page in bss
    
    Currently the zero page is set up in paging_init, and thus we cannot use
    the zero page earlier. We use the zero page as a reserved TTBR value
    from which no TLB entries may be allocated (e.g. when uninstalling the
    idmap). To enable such usage earlier (as may be required for invasive
    changes to the kernel page tables), and to minimise the time that the
    idmap is active, we need to be able to use the zero page before
    paging_init.
    
    This patch follows the example set by x86, by allocating the zero page
    at compile time, in .bss. This means that the zero page itself is
    available immediately upon entry to start_kernel (as we zero .bss before
    this), and also means that the zero page takes up no space in the raw
    Image binary. The associated struct page is allocated in bootmem_init,
    and remains unavailable until this time.
    
    Outside of arch code, the only users of empty_zero_page assume that the
    empty_zero_page symbol refers to the zeroed memory itself, and that
    ZERO_PAGE(x) must be used to acquire the associated struct page,
    following the example of x86. This patch also brings arm64 inline with
    these assumptions.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index bf464de33f52..310710f45aea 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -121,8 +121,8 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
  * ZERO_PAGE is a global shared page that is always zero: used
  * for zero-mapped memory areas etc..
  */
-extern struct page *empty_zero_page;
-#define ZERO_PAGE(vaddr)	(empty_zero_page)
+extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
+#define ZERO_PAGE(vaddr)	virt_to_page(empty_zero_page)
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
 

commit ac15bd63bbb24238f763ec5b24ee175ec301e8cd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jan 7 16:07:20 2016 +0000

    arm64: Honour !PTE_WRITE in set_pte_at() for kernel mappings
    
    Currently, set_pte_at() only checks the software PTE_WRITE bit for user
    mappings when it sets or clears the hardware PTE_RDONLY accordingly. The
    kernel ptes are written directly without any modification, relying
    solely on the protection bits in macros like PAGE_KERNEL. However,
    modifying kernel pte attributes via pte_wrprotect() would be ignored by
    set_pte_at(). Since pte_wrprotect() does not set PTE_RDONLY (it only
    clears PTE_WRITE), the new permission is not taken into account.
    
    This patch changes set_pte_at() to adjust the read-only permission for
    kernel ptes as well. As a side effect, existing PROT_* definitions used
    for kernel ioremap*() need to include PTE_DIRTY | PTE_WRITE.
    
    (additionally, white space fix for PTE_KERNEL_ROX)
    
    Acked-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2d545d7aa80b..bf464de33f52 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -67,11 +67,11 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
 #define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
 
-#define PROT_DEVICE_nGnRnE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRnE))
-#define PROT_DEVICE_nGnRE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRE))
-#define PROT_NORMAL_NC		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_NC))
-#define PROT_NORMAL_WT		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_WT))
-#define PROT_NORMAL		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL))
+#define PROT_DEVICE_nGnRnE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_DEVICE_nGnRnE))
+#define PROT_DEVICE_nGnRE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_DEVICE_nGnRE))
+#define PROT_NORMAL_NC		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL_NC))
+#define PROT_NORMAL_WT		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL_WT))
+#define PROT_NORMAL		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_ATTRINDX(MT_NORMAL))
 
 #define PROT_SECT_DEVICE_nGnRE	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_DEVICE_nGnRE))
 #define PROT_SECT_NORMAL	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
@@ -81,7 +81,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 
 #define PAGE_KERNEL		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 #define PAGE_KERNEL_RO		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
-#define PAGE_KERNEL_ROX	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
+#define PAGE_KERNEL_ROX		__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
 #define PAGE_KERNEL_EXEC	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 #define PAGE_KERNEL_EXEC_CONT	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_CONT)
 
@@ -153,6 +153,7 @@ extern struct page *empty_zero_page;
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 #define pte_cont(pte)		(!!(pte_val(pte) & PTE_CONT))
+#define pte_user(pte)		(!!(pte_val(pte) & PTE_USER))
 
 #ifdef CONFIG_ARM64_HW_AFDBM
 #define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
@@ -163,8 +164,6 @@ extern struct page *empty_zero_page;
 #define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
 
 #define pte_valid(pte)		(!!(pte_val(pte) & PTE_VALID))
-#define pte_valid_user(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 #define pte_valid_not_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == PTE_VALID)
 #define pte_valid_young(pte) \
@@ -278,13 +277,13 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_valid_user(pte)) {
-		if (!pte_special(pte) && pte_exec(pte))
-			__sync_icache_dcache(pte, addr);
+	if (pte_valid(pte)) {
 		if (pte_sw_dirty(pte) && pte_write(pte))
 			pte_val(pte) &= ~PTE_RDONLY;
 		else
 			pte_val(pte) |= PTE_RDONLY;
+		if (pte_user(pte) && pte_exec(pte) && !pte_special(pte))
+			__sync_icache_dcache(pte, addr);
 	}
 
 	/*

commit 05ee26d9e7e29ab026995eab79be3c6e8351908c
Author: Minchan Kim <minchan@kernel.org>
Date:   Fri Jan 15 16:55:37 2016 -0800

    arch/arm64/include/asm/pgtable.h: add pmd_mkclean for THP
    
    MADV_FREE needs pmd_dirty and pmd_mkclean for detecting recent overwrite
    of the contents since MADV_FREE syscall is called for THP page.
    
    This patch adds pmd_mkclean for THP page MADV_FREE support.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: <yalin.wang2010@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Gang <gang.chen.5i5j@gmail.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Jason Evans <je@fb.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mika Penttil <mika.penttila@nextfour.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 64dc55a2f7bc..2d545d7aa80b 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -360,6 +360,7 @@ static inline pgprot_t mk_sect_prot(pgprot_t prot)
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
 #define pmd_mkold(pmd)		pte_pmd(pte_mkold(pmd_pte(pmd)))
 #define pmd_mkwrite(pmd)	pte_pmd(pte_mkwrite(pmd_pte(pmd)))
+#define pmd_mkclean(pmd)       pte_pmd(pte_mkclean(pmd_pte(pmd)))
 #define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))
 #define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
 #define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_TYPE_MASK))

commit b7ed934a7c3fd4d2651eae81cb658cf1926ff54f
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Fri Jan 15 16:53:10 2016 -0800

    arm64, thp: remove infrastructure for handling splitting PMDs
    
    With new refcounting we don't need to mark PMDs splitting.  Let's drop
    code to handle this.
    
    pmdp_splitting_flush() is not needed too: on splitting PMD we will do
    pmdp_clear_flush() + set_pte_at().  pmdp_clear_flush() will do IPI as
    needed for fast_gup.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 69d2e2f86bce..64dc55a2f7bc 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -353,19 +353,11 @@ static inline pgprot_t mk_sect_prot(pgprot_t prot)
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define pmd_trans_huge(pmd)	(pmd_val(pmd) && !(pmd_val(pmd) & PMD_TABLE_BIT))
-#define pmd_trans_splitting(pmd)	pte_special(pmd_pte(pmd))
-#ifdef CONFIG_HAVE_RCU_TABLE_FREE
-#define __HAVE_ARCH_PMDP_SPLITTING_FLUSH
-struct vm_area_struct;
-void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
-			  pmd_t *pmdp);
-#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 #define pmd_dirty(pmd)		pte_dirty(pmd_pte(pmd))
 #define pmd_young(pmd)		pte_young(pmd_pte(pmd))
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
-#define pmd_mksplitting(pmd)	pte_pmd(pte_mkspecial(pmd_pte(pmd)))
 #define pmd_mkold(pmd)		pte_pmd(pte_mkold(pmd_pte(pmd)))
 #define pmd_mkwrite(pmd)	pte_pmd(pte_mkwrite(pmd_pte(pmd)))
 #define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))

commit fa5fd7c628412ee09ccf5e1d6eebe1dba916b8ee
Merge: 19e2fc4066d7 2a803c4db615
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 12:23:33 2016 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Here is the core arm64 queue for 4.5.  As you might expect, the
      Christmas break resulted in a number of patches not making the final
      cut, so 4.6 is likely to be larger than usual.  There's still some
      useful stuff here, however, and it's detailed below.
    
      The EFI changes have been Reviewed-by Matt and the memblock change got
      an "OK" from akpm.
    
      Summary:
    
       - Support for a separate IRQ stack, although we haven't reduced the
         size of our thread stack just yet since we don't have enough data
         to determine a safe value
    
       - Refactoring of our EFI initialisation and runtime code into
         drivers/firmware/efi/ so that it can be reused by arch/arm/.
    
       - Ftrace improvements when unwinding in the function graph tracer
    
       - Document our silicon errata handling process
    
       - Cache flushing optimisation when mapping executable pages
    
       - Support for hugetlb mappings using the contiguous hint in the pte"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (45 commits)
      arm64: head.S: use memset to clear BSS
      efi: stub: define DISABLE_BRANCH_PROFILING for all architectures
      arm64: entry: remove pointless SPSR mode check
      arm64: mm: move pgd_cache initialisation to pgtable_cache_init
      arm64: module: avoid undefined shift behavior in reloc_data()
      arm64: module: fix relocation of movz instruction with negative immediate
      arm64: traps: address fallout from printk -> pr_* conversion
      arm64: ftrace: fix a stack tracer's output under function graph tracer
      arm64: pass a task parameter to unwind_frame()
      arm64: ftrace: modify a stack frame in a safe way
      arm64: remove irq_count and do_softirq_own_stack()
      arm64: hugetlb: add support for PTE contiguous bit
      arm64: Use PoU cache instr for I/D coherency
      arm64: Defer dcache flush in __cpu_copy_user_page
      arm64: reduce stack use in irq_handler
      arm64: mm: ensure that the zero page is visible to the page table walker
      arm64: Documentation: add list of software workarounds for errata
      arm64: mm: place __cpu_setup in .text
      arm64: cmpxchg: Don't incldue linux/mmdebug.h
      arm64: mm: fold alternatives into .init
      ...

commit 39b5be9b4233a9f212b98242bddf008f379b5122
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jan 5 15:36:59 2016 +0000

    arm64: mm: move pgd_cache initialisation to pgtable_cache_init
    
    Initialising the suppport for EFI runtime services requires us to
    allocate a pgd off the back of an early_initcall. On systems where the
    PGD_SIZE is smaller than PAGE_SIZE (e.g. 64k pages and 48-bit VA), the
    pgd_cache isn't initialised at this stage, and we panic with a NULL
    dereference during boot:
    
      Unable to handle kernel NULL pointer dereference at virtual address 00000000
    
      __create_mapping.isra.5+0x84/0x350
      create_pgd_mapping+0x20/0x28
      efi_create_mapping+0x5c/0x6c
      arm_enable_runtime_services+0x154/0x1e4
      do_one_initcall+0x8c/0x190
      kernel_init_freeable+0x84/0x1ec
      kernel_init+0x10/0xe0
      ret_from_fork+0x10/0x50
    
    This patch fixes the problem by initialising the pgd_cache earlier, in
    the pgtable_cache_init callback, which sounds suspiciously like what it
    was intended for.
    
    Reported-by: Dennis Chen <dennis.chen@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 35a318c2fd87..a87e964d2791 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -676,7 +676,8 @@ extern int kern_addr_valid(unsigned long addr);
 
 #include <asm-generic/pgtable.h>
 
-#define pgtable_cache_init() do { } while (0)
+void pgd_cache_init(void);
+#define pgtable_cache_init	pgd_cache_init
 
 /*
  * On AArch64, the cache coherency is handled via the set_pte_at() function.

commit 66b3923a1a0f77a563b43f43f6ad091354abbfe9
Author: David Woods <dwoods@ezchip.com>
Date:   Thu Dec 17 14:31:26 2015 -0500

    arm64: hugetlb: add support for PTE contiguous bit
    
    The arm64 MMU supports a Contiguous bit which is a hint that the TTE
    is one of a set of contiguous entries which can be cached in a single
    TLB entry.  Supporting this bit adds new intermediate huge page sizes.
    
    The set of huge page sizes available depends on the base page size.
    Without using contiguous pages the huge page sizes are as follows.
    
     4KB:   2MB  1GB
    64KB: 512MB
    
    With a 4KB granule, the contiguous bit groups together sets of 16 pages
    and with a 64KB granule it groups sets of 32 pages.  This enables two new
    huge page sizes in each case, so that the full set of available sizes
    is as follows.
    
     4KB:  64KB   2MB  32MB  1GB
    64KB:   2MB 512MB  16GB
    
    If a 16KB granule is used then the contiguous bit groups 128 pages
    at the PTE level and 32 pages at the PMD level.
    
    If the base page size is set to 64KB then 2MB pages are enabled by
    default.  It is possible in the future to make 2MB the default huge
    page size for both 4KB and 64KB granules.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Reviewed-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: David Woods <dwoods@ezchip.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 450b355f3f49..35a318c2fd87 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -227,7 +227,8 @@ static inline pte_t pte_mkspecial(pte_t pte)
 
 static inline pte_t pte_mkcont(pte_t pte)
 {
-	return set_pte_bit(pte, __pgprot(PTE_CONT));
+	pte = set_pte_bit(pte, __pgprot(PTE_CONT));
+	return set_pte_bit(pte, __pgprot(PTE_TYPE_PAGE));
 }
 
 static inline pte_t pte_mknoncont(pte_t pte)
@@ -235,6 +236,11 @@ static inline pte_t pte_mknoncont(pte_t pte)
 	return clear_pte_bit(pte, __pgprot(PTE_CONT));
 }
 
+static inline pmd_t pmd_mkcont(pmd_t pmd)
+{
+	return __pmd(pmd_val(pmd) | PMD_SECT_CONT);
+}
+
 static inline void set_pte(pte_t *ptep, pte_t pte)
 {
 	*ptep = pte;
@@ -304,7 +310,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 /*
  * Hugetlb definitions.
  */
-#define HUGE_MAX_HSTATE		2
+#define HUGE_MAX_HSTATE		4
 #define HPAGE_SHIFT		PMD_SHIFT
 #define HPAGE_SIZE		(_AC(1, UL) << HPAGE_SHIFT)
 #define HPAGE_MASK		(~(HPAGE_SIZE - 1))

commit 82d340081b6f71237373d1452e3573a5a122794c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Dec 8 17:39:15 2015 +0000

    arm64: Improve error reporting on set_pte_at() checks
    
    Currently the BUG_ON() checks do not give enough information about the
    PTEs being set. This patch changes BUG_ON to WARN_ONCE and dumps the
    values of the old and new PTEs. In addition, the checks are only made if
    the new PTE entry is valid.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Ming Lei <tom.leiming@gmail.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7e074f93f383..63f52b55defe 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -276,10 +276,14 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	 * hardware updates of the pte (ptep_set_access_flags safely changes
 	 * valid ptes without going through an invalid entry).
 	 */
-	if (IS_ENABLED(CONFIG_DEBUG_VM) && IS_ENABLED(CONFIG_ARM64_HW_AFDBM) &&
-	    pte_valid(*ptep)) {
-		BUG_ON(!pte_young(pte));
-		BUG_ON(pte_write(*ptep) && !pte_dirty(pte));
+	if (IS_ENABLED(CONFIG_ARM64_HW_AFDBM) &&
+	    pte_valid(*ptep) && pte_valid(pte)) {
+		VM_WARN_ONCE(!pte_young(pte),
+			     "%s: racy access flag clearing: 0x%016llx -> 0x%016llx",
+			     __func__, pte_val(*ptep), pte_val(pte));
+		VM_WARN_ONCE(pte_write(*ptep) && !pte_dirty(pte),
+			     "%s: racy dirty state clearing: 0x%016llx -> 0x%016llx",
+			     __func__, pte_val(*ptep), pte_val(pte));
 	}
 
 	set_pte(ptep, pte);

commit 76c714be0e5e60c935a53b31be58939510ba1d0f
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Oct 30 18:56:19 2015 +0000

    arm64: pgtable: implement pte_accessible()
    
    This patch implements the pte_accessible() macro, which can be used to
    test whether or not a given pte is a candidate for allocation in the
    TLB.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7e074f93f383..450b355f3f49 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -167,6 +167,16 @@ extern struct page *empty_zero_page;
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 #define pte_valid_not_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == PTE_VALID)
+#define pte_valid_young(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_AF)) == (PTE_VALID | PTE_AF))
+
+/*
+ * Could the pte be present in the TLB? We must check mm_tlb_flush_pending
+ * so that we don't erroneously return false for pages that have been
+ * remapped as PROT_NONE but are yet to be flushed from the TLB.
+ */
+#define pte_accessible(mm, pte)	\
+	(mm_tlb_flush_pending(mm) ? pte_present(pte) : pte_valid_young(pte))
 
 static inline pte_t clear_pte_bit(pte_t pte, pgprot_t prot)
 {

commit 0b2aa5b80bbf4d0fb5daa1fb83ff637daa12d552
Author: Laura Abbott <labbott@fedoraproject.org>
Date:   Thu Nov 12 12:21:10 2015 -0800

    arm64: Fix R/O permissions in mark_rodata_ro
    
    The permissions in mark_rodata_ro trigger a build error
    with STRICT_MM_TYPECHECKS. Fix this by introducing
    PAGE_KERNEL_ROX for the same reasons as PAGE_KERNEL_RO.
    From Ard:
    
    "PAGE_KERNEL_EXEC has PTE_WRITE set as well, making the range
    writeable under the ARMv8.1 DBM feature, that manages the
    dirty bit in hardware (writing to a page with the PTE_RDONLY
    and PTE_WRITE bits both set will clear the PTE_RDONLY bit in that case)"
    
    Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 9819a9426b69..7e074f93f383 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -81,6 +81,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 
 #define PAGE_KERNEL		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 #define PAGE_KERNEL_RO		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
+#define PAGE_KERNEL_ROX	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
 #define PAGE_KERNEL_EXEC	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 #define PAGE_KERNEL_EXEC_CONT	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_CONT)
 

commit a18e2fa5e670a1b84e66522b221c42875b02028a
Merge: 7dac7102afbe 01b305a23494
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 12 15:33:11 2015 -0800

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes and clean-ups from Catalin Marinas:
     "Here's a second pull request for this merging window with some
      fixes/clean-ups:
    
       - __cmpxchg_double*() return type fix to avoid truncation of a long
         to int and subsequent logical "not" in cmpxchg_double()
         misinterpreting the operation success/failure
    
       - BPF fixes for mod and div by zero
    
       - Fix compilation with STRICT_MM_TYPECHECKS enabled
    
       - VDSO build fix without libgcov
    
       - Some static and __maybe_unused annotations
    
       - Kconfig clean-up (FRAME_POINTER)
    
       - defconfig update for CRYPTO_CRC32_ARM64"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: suspend: make hw_breakpoint_restore static
      arm64: mmu: make split_pud and fixup_executable static
      arm64: smp: make of_parse_and_init_cpus static
      arm64: use linux/types.h in kvm.h
      arm64: build vdso without libgcov
      arm64: mark cpus_have_hwcap as __maybe_unused
      arm64: remove redundant FRAME_POINTER kconfig option and force to select it
      arm64: fix R/O permissions of FDT mapping
      arm64: fix STRICT_MM_TYPECHECKS issue in PTE_CONT manipulation
      arm64: bpf: fix mod-by-zero case
      arm64: bpf: fix div-by-zero case
      arm64: Enable CRYPTO_CRC32_ARM64 in defconfig
      arm64: cmpxchg_dbl: fix return value type

commit fb226c3d7c77b4f99cee675795cc0e70937c56ee
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Nov 9 09:55:46 2015 +0100

    arm64: fix R/O permissions of FDT mapping
    
    The mapping permissions of the FDT are set to 'PAGE_KERNEL | PTE_RDONLY'
    in an attempt to map the FDT as read-only. However, not only does this
    break at build time under STRICT_MM_TYPECHECKS (since the two terms are
    of different types in that case), it also results in both the PTE_WRITE
    and PTE_RDONLY attributes to be set, which means the region is still
    writable under ARMv8.1 DBM (and an attempted write will simply clear the
    PT_RDONLY bit).
    
    So instead, define PAGE_KERNEL_RO (which already has an established
    meaning across architectures) and use that instead.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index c3d22a507648..1c99d5610bc7 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -78,6 +78,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define _PAGE_DEFAULT		(PROT_DEFAULT | PTE_ATTRINDX(MT_NORMAL))
 
 #define PAGE_KERNEL		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
+#define PAGE_KERNEL_RO		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_RDONLY)
 #define PAGE_KERNEL_EXEC	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 #define PAGE_KERNEL_EXEC_CONT	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_CONT)
 

commit 2dc10ad81fc017837037e60439662e1b16bdffb9
Merge: e627078a0cbd f8f8bdc48851
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 14:47:13 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - "genirq: Introduce generic irq migration for cpu hotunplugged" patch
       merged from tip/irq/for-arm to allow the arm64-specific part to be
       upstreamed via the arm64 tree
    
     - CPU feature detection reworked to cope with heterogeneous systems
       where CPUs may not have exactly the same features.  The features
       reported by the kernel via internal data structures or ELF_HWCAP are
       delayed until all the CPUs are up (and before user space starts)
    
     - Support for 16KB pages, with the additional bonus of a 36-bit VA
       space, though the latter only depending on EXPERT
    
     - Implement native {relaxed, acquire, release} atomics for arm64
    
     - New ASID allocation algorithm which avoids IPI on roll-over, together
       with TLB invalidation optimisations (using local vs global where
       feasible)
    
     - KASan support for arm64
    
     - EFI_STUB clean-up and isolation for the kernel proper (required by
       KASan)
    
     - copy_{to,from,in}_user optimisations (sharing the memcpy template)
    
     - perf: moving arm64 to the arm32/64 shared PMU framework
    
     - L1_CACHE_BYTES increased to 128 to accommodate Cavium hardware
    
     - Support for the contiguous PTE hint on kernel mapping (16 consecutive
       entries may be able to use a single TLB entry)
    
     - Generic CONFIG_HZ now used on arm64
    
     - defconfig updates
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (91 commits)
      arm64/efi: fix libstub build under CONFIG_MODVERSIONS
      ARM64: Enable multi-core scheduler support by default
      arm64/efi: move arm64 specific stub C code to libstub
      arm64: page-align sections for DEBUG_RODATA
      arm64: Fix build with CONFIG_ZONE_DMA=n
      arm64: Fix compat register mappings
      arm64: Increase the max granular size
      arm64: remove bogus TASK_SIZE_64 check
      arm64: make Timer Interrupt Frequency selectable
      arm64/mm: use PAGE_ALIGNED instead of IS_ALIGNED
      arm64: cachetype: fix definitions of ICACHEF_* flags
      arm64: cpufeature: declare enable_cpu_capabilities as static
      genirq: Make the cpuhotplug migration code less noisy
      arm64: Constify hwcap name string arrays
      arm64/kvm: Make use of the system wide safe values
      arm64/debug: Make use of the system wide safe value
      arm64: Move FP/ASIMD hwcap handling to common code
      arm64/HWCAP: Use system wide safe values
      arm64/capabilities: Make use of system wide safe value
      arm64: Delay cpu feature capability checks
      ...

commit 7db743c6d8bb7a08396a0ea3084fb9f8ebccc577
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Oct 16 14:34:50 2015 +0100

    arm64: Minor coding style fixes for kc_offset_to_vaddr and kc_vaddr_to_offset
    
    These were introduced by commit 03875ad52fdd (arm64: add
    kc_offset_to_vaddr and kc_vaddr_to_offset macro).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 83965dd63d2f..c3d22a507648 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -673,8 +673,9 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 
 #define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)
 
-#define	kc_vaddr_to_offset(v) ((v) & ~VA_START)
-#define	kc_offset_to_vaddr(o) ((o) | VA_START)
+#define kc_vaddr_to_offset(v)	((v) & ~VA_START)
+#define kc_offset_to_vaddr(o)	((o) | VA_START)
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */

commit c7d77a7980e434c3af17de19e3348157f9b9ccce
Merge: 0ce423b6492a 8a53554e12e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Oct 14 16:05:18 2015 +0200

    Merge branch 'x86/urgent' into core/efi, to pick up a pending EFI fix
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 03875ad52fdde1f110f663470a45d3dcc34b8fef
Author: yalin wang <yalin.wang2010@gmail.com>
Date:   Mon Oct 12 14:52:59 2015 +0800

    arm64: add kc_offset_to_vaddr and kc_vaddr_to_offset macro
    
    This patch add kc_offset_to_vaddr() and kc_vaddr_to_offset(),
    the default version doesn't work on arm64, because arm64 kernel address
    is below the PAGE_OFFSET, like module address and vmemmap address are
    all below PAGE_OFFSET address.
    
    Signed-off-by: yalin wang <yalin.wang2010@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e3b515fbf0c2..83965dd63d2f 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -673,6 +673,8 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 
 #define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)
 
+#define	kc_vaddr_to_offset(v) ((v) & ~VA_START)
+#define	kc_offset_to_vaddr(o) ((o) | VA_START)
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */

commit 39d114ddc68223022c12ae3a1573912bc4b585e5
Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
Date:   Mon Oct 12 18:52:58 2015 +0300

    arm64: add KASAN support
    
    This patch adds arch specific code for kernel address sanitizer
    (see Documentation/kasan.txt).
    
    1/8 of kernel addresses reserved for shadow memory. There was no
    big enough hole for this, so virtual addresses for shadow were
    stolen from vmalloc area.
    
    At early boot stage the whole shadow region populated with just
    one physical page (kasan_zero_page). Later, this page reused
    as readonly zero shadow for some memory that KASan currently
    don't track (vmalloc).
    After mapping the physical memory, pages for shadow memory are
    allocated and mapped.
    
    Functions like memset/memmove/memcpy do a lot of memory accesses.
    If bad pointer passed to one of these function it is important
    to catch this. Compiler's instrumentation cannot do this since
    these functions are written in assembly.
    KASan replaces memory functions with manually instrumented variants.
    Original functions declared as weak symbols so strong definitions
    in mm/kasan/kasan.c could replace them. Original functions have aliases
    with '__' prefix in name, so we could call non-instrumented variant
    if needed.
    Some files built without kasan instrumentation (e.g. mm/slub.c).
    Original mem* function replaced (via #define) with prefixed variants
    to disable memory access checks for such files.
    
    Signed-off-by: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3f481ef42c07..e3b515fbf0c2 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -41,7 +41,14 @@
  *	fixed mappings and modules
  */
 #define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
+
+#ifndef CONFIG_KASAN
 #define VMALLOC_START		(VA_START)
+#else
+#include <asm/kasan.h>
+#define VMALLOC_START		(KASAN_SHADOW_END + SZ_64K)
+#endif
+
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))

commit 06f90d2527845c4767207f54280df2c5ca11e82b
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed Oct 7 12:00:22 2015 -0500

    arm64: Default kernel pages should be contiguous
    
    The default page attributes for a PMD being broken should have the CONT bit
    set. Create a new definition for an early boot range of PTE's that are
    contiguous.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 1a1a6efa75e5..3f481ef42c07 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -72,6 +72,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 
 #define PAGE_KERNEL		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 #define PAGE_KERNEL_EXEC	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
+#define PAGE_KERNEL_EXEC_CONT	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE | PTE_CONT)
 
 #define PAGE_HYP		__pgprot(_PAGE_DEFAULT | PTE_HYP)
 #define PAGE_HYP_DEVICE		__pgprot(PROT_DEVICE_nGnRE | PTE_HYP)

commit 93ef666a094ff9c9fc8d7cf1774ef0b92e270a75
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed Oct 7 12:00:21 2015 -0500

    arm64: Macros to check/set/unset the contiguous bit
    
    Add the supporting macros to check if the contiguous bit
    is set, set the bit, or clear it in a PTE entry.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5043a84e724e..1a1a6efa75e5 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -140,6 +140,7 @@ extern struct page *empty_zero_page;
 #define pte_special(pte)	(!!(pte_val(pte) & PTE_SPECIAL))
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
+#define pte_cont(pte)		(!!(pte_val(pte) & PTE_CONT))
 
 #ifdef CONFIG_ARM64_HW_AFDBM
 #define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
@@ -202,6 +203,16 @@ static inline pte_t pte_mkspecial(pte_t pte)
 	return set_pte_bit(pte, __pgprot(PTE_SPECIAL));
 }
 
+static inline pte_t pte_mkcont(pte_t pte)
+{
+	return set_pte_bit(pte, __pgprot(PTE_CONT));
+}
+
+static inline pte_t pte_mknoncont(pte_t pte)
+{
+	return clear_pte_bit(pte, __pgprot(PTE_CONT));
+}
+
 static inline void set_pte(pte_t *ptep, pte_t pte)
 {
 	*ptep = pte;

commit 120798d2e7d1ac87365fe5ea91b074bb42ca1eff
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:30 2015 +0100

    arm64: mm: remove dsb from update_mmu_cache
    
    update_mmu_cache() consists of a dsb(ishst) instruction so that new user
    mappings are guaranteed to be visible to the page table walker on
    exception return.
    
    In reality this can be a very expensive operation which is rarely needed.
    Removing this barrier shows a modest improvement in hackbench scores and
    , in the worst case, we re-take the user fault and establish that there
    was nothing to do.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 581d1406ba28..5043a84e724e 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -646,10 +646,10 @@ static inline void update_mmu_cache(struct vm_area_struct *vma,
 				    unsigned long addr, pte_t *ptep)
 {
 	/*
-	 * set_pte() does not have a DSB for user mappings, so make sure that
-	 * the page table write is visible.
+	 * We don't do anything here, so there's a very small chance of
+	 * us retaking a user fault which we just fixed up. The alternative
+	 * is doing a dsb(ishst), but that penalises the fastpath.
 	 */
-	dsb(ishst);
 }
 
 #define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)

commit 127db024a7baee9874014dac33628253f438b4da
Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
Date:   Thu Sep 17 12:38:07 2015 +0300

    arm64: introduce VA_START macro - the first kernel virtual address.
    
    In order to not use lengthy (UL(0xffffffffffffffff) << VA_BITS) everywhere,
    replace it with VA_START.
    
    Signed-off-by: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 26b066690593..581d1406ba28 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -41,7 +41,7 @@
  *	fixed mappings and modules
  */
 #define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
-#define VMALLOC_START		(UL(0xffffffffffffffff) << VA_BITS)
+#define VMALLOC_START		(VA_START)
 #define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))

commit 1a541b4e3cd6f5795022514114854b3e1345f24e
Author: Steve Capper <steve.capper@linaro.org>
Date:   Thu Oct 1 13:06:07 2015 +0100

    arm64: Fix THP protection change logic
    
    6910fa1 ("arm64: enable PTE type bit in the mask for pte_modify") fixes
    a problem whereby a large block of PROT_NONE mapped memory is
    incorrectly mapped as block descriptors when mprotect is called.
    
    Unfortunately, a subtle bug was introduced by this fix to the THP logic.
    
    If one mmaps a large block of memory, then faults it such that it is
    collapsed into THPs; resulting calls to mprotect on this area of memory
    will lead to incorrect table descriptors being written instead of block
    descriptors. This is because pmd_modify calls pte_modify which is now
    allowed to modify the type of the page table entry.
    
    This patch reverts commit 6910fa16dbe142f6a0fd0fd7c249f9883ff7fc8a, and
    fixes the problem it was trying to address by adjusting PAGE_NONE to
    represent a table entry. Thus no change in pte type is required when
    moving from PROT_NONE to a different protection.
    
    Fixes: 6910fa16dbe1 ("arm64: enable PTE type bit in the mask for pte_modify")
    Cc: <stable@vger.kernel.org> # 4.0+
    Cc: Feng Kan <fkan@apm.com>
    Reported-by: Ganapatrao Kulkarni <Ganapatrao.Kulkarni@caviumnetworks.com>
    Tested-by: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b0329be95cb1..26b066690593 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -79,7 +79,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define PAGE_S2			__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
 #define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDONLY | PTE_UXN)
 
-#define PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
+#define PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_VALID) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
 #define PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
 #define PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
 #define PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
@@ -496,7 +496,7 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
-			      PTE_PROT_NONE | PTE_WRITE | PTE_TYPE_MASK;
+			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
 	/* preserve the hardware dirty information */
 	if (pte_hw_dirty(pte))
 		pte = pte_mkdirty(pte);

commit bf950040a53da35522e38066d9eb6ab7a1c9d136
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Sep 11 18:22:02 2015 +0100

    arm64: pgtable: use a single bit for PTE_WRITE regardless of DBM
    
    Depending on CONFIG_ARM64_HW_AFDBM, we use either bit 57 or 51 of the
    pte to represent PTE_WRITE. Given that bit 51 is reserved prior to
    ARMv8.1, we can just use that bit regardless of the config option. That
    also matches what happens if a kernel configured with ARM64_HW_AFDBM=y
    is run on a CPU without the DBM functionality.
    
    Cc: Julien Grall <julien.grall@citrix.com>
    Tested-by: Julien Grall <julien.grall@citrix.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 31df98adf005..b0329be95cb1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -26,13 +26,9 @@
  * Software defined PTE bits definition.
  */
 #define PTE_VALID		(_AT(pteval_t, 1) << 0)
+#define PTE_WRITE		(PTE_DBM)		 /* same as DBM (51) */
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
-#ifdef CONFIG_ARM64_HW_AFDBM
-#define PTE_WRITE		(PTE_DBM)		 /* same as DBM */
-#else
-#define PTE_WRITE		(_AT(pteval_t, 1) << 57)
-#endif
 #define PTE_PROT_NONE		(_AT(pteval_t, 1) << 58) /* only when !PTE_VALID */
 
 /*

commit 62d96c71d248834af2891293dc23cc344ae2ec36
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Sep 11 18:22:01 2015 +0100

    arm64: Fix pte_modify() to preserve the hardware dirty information
    
    The pte_modify() function with hardware AF/DBM enabled must transfer the
    hardware dirty information to the software PTE_DIRTY bit. However, it
    was setting this bit in newprot and the mask does not cover such bit.
    This patch sets PTE_DIRTY on the original pte which will be preserved in
    the returned value.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Cc: Julien Grall <julien.grall@citrix.com>
    Tested-by: Julien Grall <julien.grall@citrix.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 69207f016891..31df98adf005 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -503,7 +503,7 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 			      PTE_PROT_NONE | PTE_WRITE | PTE_TYPE_MASK;
 	/* preserve the hardware dirty information */
 	if (pte_hw_dirty(pte))
-		newprot |= PTE_DIRTY;
+		pte = pte_mkdirty(pte);
 	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
 	return pte;
 }

commit b847415ce96efef819534b230d84695b1bc6d36b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Sep 11 18:22:00 2015 +0100

    arm64: Fix the pte_hw_dirty() check when AF/DBM is enabled
    
    Commit 2f4b829c625e ("arm64: Add support for hardware updates of the
    access and dirty pte bits") introduced support for handling hardware
    updates of the access flag and dirty status. The PTE is automatically
    dirtied in hardware (if supported) by clearing the PTE_RDONLY bit when
    the PTE_DBM/PTE_WRITE bit is set. The pte_hw_dirty() macro was added to
    detect a hardware dirtied pte. The pte_dirty() macro checks for both
    software PTE_DIRTY and pte_hw_dirty().
    
    Functions like pte_modify() clear the PTE_RDONLY bit since it is meant
    to be set in set_pte_at() when written to memory. In such cases,
    pte_hw_dirty() would return true even though such pte is clean. This
    patch changes pte_hw_dirty() to test the PTE_DBM/PTE_WRITE bit together
    with PTE_RDONLY.
    
    Fixes: 2f4b829c625e ("arm64: Add support for hardware updates of the access and dirty pte bits")
    Reported-by: Julien Grall <julien.grall@citrix.com>
    Tested-by: Julien Grall <julien.grall@citrix.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 6900b2d95371..69207f016891 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -146,7 +146,7 @@ extern struct page *empty_zero_page;
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
 #ifdef CONFIG_ARM64_HW_AFDBM
-#define pte_hw_dirty(pte)	(!(pte_val(pte) & PTE_RDONLY))
+#define pte_hw_dirty(pte)	(pte_write(pte) && !(pte_val(pte) & PTE_RDONLY))
 #else
 #define pte_hw_dirty(pte)	(0)
 #endif
@@ -238,7 +238,7 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
  * When hardware DBM is not present, the sofware PTE_DIRTY bit is updated via
  * the page fault mechanism. Checking the dirty status of a pte becomes:
  *
- *   PTE_DIRTY || !PTE_RDONLY
+ *   PTE_DIRTY || (PTE_WRITE && !PTE_RDONLY)
  */
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)

commit 8d446c8647c9ab8fcb45a8fc7dbbafe1f83aa2f3
Author: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
Date:   Fri Aug 7 09:36:59 2015 +0100

    arm64/mm: Add PROT_DEVICE_nGnRnE and PROT_NORMAL_WT
    
    UEFI spec 2.5 section 2.3.6.1 defines that
    EFI_MEMORY_[UC|WC|WT|WB] are possible EFI memory types for
    AArch64.
    
    Each of those EFI memory types is mapped to a corresponding
    AArch64 memory type. So we need to define PROT_DEVICE_nGnRnE
    and PROT_NORMWL_WT additionaly.
    
    MT_NORMAL_WT is defined, and its encoding is added to MAIR_EL1
    when initializing the CPU.
    
    Signed-off-by: Jonathan (Zhixiong) Zhang <zjzhang@codeaurora.org>
    Signed-off-by: Matt Fleming <matt.fleming@intel.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1438936621-5215-6-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 56283f8a675c..0a105e3254a1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -61,8 +61,10 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF)
 #endif
 
+#define PROT_DEVICE_nGnRnE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRnE))
 #define PROT_DEVICE_nGnRE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRE))
 #define PROT_NORMAL_NC		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_NC))
+#define PROT_NORMAL_WT		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_WT))
 #define PROT_NORMAL		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL))
 
 #define PROT_SECT_DEVICE_nGnRE	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_DEVICE_nGnRE))

commit 766ffb69803943c2b580a44ac14a189b875d21f6
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jul 28 16:14:03 2015 +0100

    arm64: pgtable: fix definition of pte_valid
    
    pte_valid should check if the PTE_VALID bit (1 << 0) is set in the pte,
    so fix the macro definition to use bitwise & instead of logical &&.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index d001846c13ac..6900b2d95371 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -153,7 +153,7 @@ extern struct page *empty_zero_page;
 #define pte_sw_dirty(pte)	(!!(pte_val(pte) & PTE_DIRTY))
 #define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
 
-#define pte_valid(pte)		(!!(pte_val(pte) && PTE_VALID))
+#define pte_valid(pte)		(!!(pte_val(pte) & PTE_VALID))
 #define pte_valid_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 #define pte_valid_not_user(pte) \

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 8212e6aa0fb1..d001846c13ac 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -61,13 +61,8 @@ extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pud_error(const char *file, int line, unsigned long val);
 extern void __pgd_error(const char *file, int line, unsigned long val);
 
-#ifdef CONFIG_SMP
 #define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
 #define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
-#else
-#define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF)
-#define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF)
-#endif
 
 #define PROT_DEVICE_nGnRE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRE))
 #define PROT_NORMAL_NC		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_NC))

commit 2f4b829c625ec36c2d80bef6395c7b74cea8aac0
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 10 17:24:28 2015 +0100

    arm64: Add support for hardware updates of the access and dirty pte bits
    
    The ARMv8.1 architecture extensions introduce support for hardware
    updates of the access and dirty information in page table entries. With
    TCR_EL1.HA enabled, when the CPU accesses an address with the PTE_AF bit
    cleared in the page table, instead of raising an access flag fault the
    CPU sets the actual page table entry bit. To ensure that kernel
    modifications to the page tables do not inadvertently revert a change
    introduced by hardware updates, the exclusive monitor (ldxr/stxr) is
    adopted in the pte accessors.
    
    When TCR_EL1.HD is enabled, a write access to a memory location with the
    DBM (Dirty Bit Management) bit set in the corresponding pte
    automatically clears the read-only bit (AP[2]). Such DBM bit maps onto
    the Linux PTE_WRITE bit and to check whether a writable (DBM set) page
    is dirty, the kernel tests the PTE_RDONLY bit. In order to allow
    read-only and dirty pages, the kernel needs to preserve the software
    dirty bit. The hardware dirty status is transferred to the software
    dirty bit in ptep_set_wrprotect() (using load/store exclusive loop) and
    pte_modify().
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 4d5c812847e9..8212e6aa0fb1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -16,6 +16,7 @@
 #ifndef __ASM_PGTABLE_H
 #define __ASM_PGTABLE_H
 
+#include <asm/bug.h>
 #include <asm/proc-fns.h>
 
 #include <asm/memory.h>
@@ -27,7 +28,11 @@
 #define PTE_VALID		(_AT(pteval_t, 1) << 0)
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
+#ifdef CONFIG_ARM64_HW_AFDBM
+#define PTE_WRITE		(PTE_DBM)		 /* same as DBM */
+#else
 #define PTE_WRITE		(_AT(pteval_t, 1) << 57)
+#endif
 #define PTE_PROT_NONE		(_AT(pteval_t, 1) << 58) /* only when !PTE_VALID */
 
 /*
@@ -48,6 +53,9 @@
 #define FIRST_USER_ADDRESS	0UL
 
 #ifndef __ASSEMBLY__
+
+#include <linux/mmdebug.h>
+
 extern void __pte_error(const char *file, int line, unsigned long val);
 extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pud_error(const char *file, int line, unsigned long val);
@@ -137,12 +145,20 @@ extern struct page *empty_zero_page;
  * The following only work if pte_present(). Undefined behaviour otherwise.
  */
 #define pte_present(pte)	(!!(pte_val(pte) & (PTE_VALID | PTE_PROT_NONE)))
-#define pte_dirty(pte)		(!!(pte_val(pte) & PTE_DIRTY))
 #define pte_young(pte)		(!!(pte_val(pte) & PTE_AF))
 #define pte_special(pte)	(!!(pte_val(pte) & PTE_SPECIAL))
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
+#ifdef CONFIG_ARM64_HW_AFDBM
+#define pte_hw_dirty(pte)	(!(pte_val(pte) & PTE_RDONLY))
+#else
+#define pte_hw_dirty(pte)	(0)
+#endif
+#define pte_sw_dirty(pte)	(!!(pte_val(pte) & PTE_DIRTY))
+#define pte_dirty(pte)		(pte_sw_dirty(pte) || pte_hw_dirty(pte))
+
+#define pte_valid(pte)		(!!(pte_val(pte) && PTE_VALID))
 #define pte_valid_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 #define pte_valid_not_user(pte) \
@@ -209,20 +225,49 @@ static inline void set_pte(pte_t *ptep, pte_t pte)
 	}
 }
 
+struct mm_struct;
+struct vm_area_struct;
+
 extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 
+/*
+ * PTE bits configuration in the presence of hardware Dirty Bit Management
+ * (PTE_WRITE == PTE_DBM):
+ *
+ * Dirty  Writable | PTE_RDONLY  PTE_WRITE  PTE_DIRTY (sw)
+ *   0      0      |   1           0          0
+ *   0      1      |   1           1          0
+ *   1      0      |   1           0          1
+ *   1      1      |   0           1          x
+ *
+ * When hardware DBM is not present, the sofware PTE_DIRTY bit is updated via
+ * the page fault mechanism. Checking the dirty status of a pte becomes:
+ *
+ *   PTE_DIRTY || !PTE_RDONLY
+ */
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
 	if (pte_valid_user(pte)) {
 		if (!pte_special(pte) && pte_exec(pte))
 			__sync_icache_dcache(pte, addr);
-		if (pte_dirty(pte) && pte_write(pte))
+		if (pte_sw_dirty(pte) && pte_write(pte))
 			pte_val(pte) &= ~PTE_RDONLY;
 		else
 			pte_val(pte) |= PTE_RDONLY;
 	}
 
+	/*
+	 * If the existing pte is valid, check for potential race with
+	 * hardware updates of the pte (ptep_set_access_flags safely changes
+	 * valid ptes without going through an invalid entry).
+	 */
+	if (IS_ENABLED(CONFIG_DEBUG_VM) && IS_ENABLED(CONFIG_ARM64_HW_AFDBM) &&
+	    pte_valid(*ptep)) {
+		BUG_ON(!pte_young(pte));
+		BUG_ON(pte_write(*ptep) && !pte_dirty(pte));
+	}
+
 	set_pte(ptep, pte);
 }
 
@@ -461,6 +506,9 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
 			      PTE_PROT_NONE | PTE_WRITE | PTE_TYPE_MASK;
+	/* preserve the hardware dirty information */
+	if (pte_hw_dirty(pte))
+		newprot |= PTE_DIRTY;
 	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
 	return pte;
 }
@@ -470,6 +518,101 @@ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
 	return pte_pmd(pte_modify(pmd_pte(pmd), newprot));
 }
 
+#ifdef CONFIG_ARM64_HW_AFDBM
+/*
+ * Atomic pte/pmd modifications.
+ */
+#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
+static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,
+					    unsigned long address,
+					    pte_t *ptep)
+{
+	pteval_t pteval;
+	unsigned int tmp, res;
+
+	asm volatile("//	ptep_test_and_clear_young\n"
+	"	prfm	pstl1strm, %2\n"
+	"1:	ldxr	%0, %2\n"
+	"	ubfx	%w3, %w0, %5, #1	// extract PTE_AF (young)\n"
+	"	and	%0, %0, %4		// clear PTE_AF\n"
+	"	stxr	%w1, %0, %2\n"
+	"	cbnz	%w1, 1b\n"
+	: "=&r" (pteval), "=&r" (tmp), "+Q" (pte_val(*ptep)), "=&r" (res)
+	: "L" (~PTE_AF), "I" (ilog2(PTE_AF)));
+
+	return res;
+}
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG
+static inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,
+					    unsigned long address,
+					    pmd_t *pmdp)
+{
+	return ptep_test_and_clear_young(vma, address, (pte_t *)pmdp);
+}
+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+
+#define __HAVE_ARCH_PTEP_GET_AND_CLEAR
+static inline pte_t ptep_get_and_clear(struct mm_struct *mm,
+				       unsigned long address, pte_t *ptep)
+{
+	pteval_t old_pteval;
+	unsigned int tmp;
+
+	asm volatile("//	ptep_get_and_clear\n"
+	"	prfm	pstl1strm, %2\n"
+	"1:	ldxr	%0, %2\n"
+	"	stxr	%w1, xzr, %2\n"
+	"	cbnz	%w1, 1b\n"
+	: "=&r" (old_pteval), "=&r" (tmp), "+Q" (pte_val(*ptep)));
+
+	return __pte(old_pteval);
+}
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#define __HAVE_ARCH_PMDP_GET_AND_CLEAR
+static inline pmd_t pmdp_get_and_clear(struct mm_struct *mm,
+				       unsigned long address, pmd_t *pmdp)
+{
+	return pte_pmd(ptep_get_and_clear(mm, address, (pte_t *)pmdp));
+}
+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+
+/*
+ * ptep_set_wrprotect - mark read-only while trasferring potential hardware
+ * dirty status (PTE_DBM && !PTE_RDONLY) to the software PTE_DIRTY bit.
+ */
+#define __HAVE_ARCH_PTEP_SET_WRPROTECT
+static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)
+{
+	pteval_t pteval;
+	unsigned long tmp;
+
+	asm volatile("//	ptep_set_wrprotect\n"
+	"	prfm	pstl1strm, %2\n"
+	"1:	ldxr	%0, %2\n"
+	"	tst	%0, %4			// check for hw dirty (!PTE_RDONLY)\n"
+	"	csel	%1, %3, xzr, eq		// set PTE_DIRTY|PTE_RDONLY if dirty\n"
+	"	orr	%0, %0, %1		// if !dirty, PTE_RDONLY is already set\n"
+	"	and	%0, %0, %5		// clear PTE_WRITE/PTE_DBM\n"
+	"	stxr	%w1, %0, %2\n"
+	"	cbnz	%w1, 1b\n"
+	: "=&r" (pteval), "=&r" (tmp), "+Q" (pte_val(*ptep))
+	: "r" (PTE_DIRTY|PTE_RDONLY), "L" (PTE_RDONLY), "L" (~PTE_WRITE)
+	: "cc");
+}
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#define __HAVE_ARCH_PMDP_SET_WRPROTECT
+static inline void pmdp_set_wrprotect(struct mm_struct *mm,
+				      unsigned long address, pmd_t *pmdp)
+{
+	ptep_set_wrprotect(mm, address, (pte_t *)pmdp);
+}
+#endif
+#endif	/* CONFIG_ARM64_HW_AFDBM */
+
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 

commit cba3574fd56be8132a19e4aa6b1d41a12c56d990
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jul 16 19:26:02 2015 +0100

    arm64: move update_mmu_cache() into asm/pgtable.h
    
    Mark Brown reported an allnoconfig build failure in -next:
    
      Today's linux-next fails to build an arm64 allnoconfig due to "mm:
      make GUP handle pfn mapping unless FOLL_GET is requested" which
      causes:
    
      >       arm64-allnoconfig
      > ../mm/gup.c:51:4: error: implicit declaration of function
        'update_mmu_cache' [-Werror=implicit-function-declaration]
    
    Fix the error by moving the function to asm/pgtable.h, as is the case
    for most other architectures.
    
    Reported-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 56283f8a675c..4d5c812847e9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -505,6 +505,21 @@ extern int kern_addr_valid(unsigned long addr);
 
 #define pgtable_cache_init() do { } while (0)
 
+/*
+ * On AArch64, the cache coherency is handled via the set_pte_at() function.
+ */
+static inline void update_mmu_cache(struct vm_area_struct *vma,
+				    unsigned long addr, pte_t *ptep)
+{
+	/*
+	 * set_pte() does not have a DSB for user mappings, so make sure that
+	 * the page table write is visible.
+	 */
+	dsb(ishst);
+}
+
+#define update_mmu_cache_pmd(vma, address, pmd) do { } while (0)
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */

commit 9f25e6ad58e1fb3b4d441e4c55635c4598a6fa94
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Apr 14 15:45:39 2015 -0700

    arm64: expose number of page table levels on Kconfig level
    
    We would want to use number of page table level to define mm_struct.
    Let's expose it as CONFIG_PGTABLE_LEVELS.
    
    ARM64_PGTABLE_LEVELS is renamed to PGTABLE_LEVELS and defined before
    sourcing init/Kconfig: arch/Kconfig will define default value and it's
    sourced from init/Kconfig.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Tested-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 800ec0e87ed9..56283f8a675c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -374,7 +374,7 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
  */
 #define mk_pte(page,prot)	pfn_pte(page_to_pfn(page),prot)
 
-#if CONFIG_ARM64_PGTABLE_LEVELS > 2
+#if CONFIG_PGTABLE_LEVELS > 2
 
 #define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
 
@@ -409,9 +409,9 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 
 #define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
 
-#endif	/* CONFIG_ARM64_PGTABLE_LEVELS > 2 */
+#endif	/* CONFIG_PGTABLE_LEVELS > 2 */
 
-#if CONFIG_ARM64_PGTABLE_LEVELS > 3
+#if CONFIG_PGTABLE_LEVELS > 3
 
 #define pud_ERROR(pud)		__pud_error(__FILE__, __LINE__, pud_val(pud))
 
@@ -445,7 +445,7 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 
 #define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
 
-#endif  /* CONFIG_ARM64_PGTABLE_LEVELS > 3 */
+#endif  /* CONFIG_PGTABLE_LEVELS > 3 */
 
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
 

commit 6910fa16dbe142f6a0fd0fd7c249f9883ff7fc8a
Author: Feng Kan <fkan@apm.com>
Date:   Tue Feb 24 15:40:21 2015 -0800

    arm64: enable PTE type bit in the mask for pte_modify
    
    Caught during Trinity testing. The pte_modify does not allow
    modification for PTE type bit. This cause the test to hang
    the system. It is found that the PTE can't transit from an
    inaccessible page (b00) to a valid page (b11) because the mask
    does not allow it. This happens when a big block of mmaped
    memory is set the PROT_NONE, then the a small piece is broken
    off and set to PROT_WRITE | PROT_READ cause a huge page split.
    
    Signed-off-by: Feng Kan <fkan@apm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 16449c535e50..800ec0e87ed9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -460,7 +460,7 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
-			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
+			      PTE_PROT_NONE | PTE_WRITE | PTE_TYPE_MASK;
 	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
 	return pte;
 }

commit 59d53737a8640482995fea13c6e2c0fd016115d6
Merge: d3f180ea1a44 8138a67a5557
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 18:23:28 2015 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge second set of updates from Andrew Morton:
     "More of MM"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (83 commits)
      mm/nommu.c: fix arithmetic overflow in __vm_enough_memory()
      mm/mmap.c: fix arithmetic overflow in __vm_enough_memory()
      vmstat: Reduce time interval to stat update on idle cpu
      mm/page_owner.c: remove unnecessary stack_trace field
      Documentation/filesystems/proc.txt: describe /proc/<pid>/map_files
      mm: incorporate read-only pages into transparent huge pages
      vmstat: do not use deferrable delayed work for vmstat_update
      mm: more aggressive page stealing for UNMOVABLE allocations
      mm: always steal split buddies in fallback allocations
      mm: when stealing freepages, also take pages created by splitting buddy page
      mincore: apply page table walker on do_mincore()
      mm: /proc/pid/clear_refs: avoid split_huge_page()
      mm: pagewalk: fix misbehavior of walk_page_range for vma(VM_PFNMAP)
      mempolicy: apply page table walker on queue_pages_range()
      arch/powerpc/mm/subpage-prot.c: use walk->vma and walk_page_vma()
      memcg: cleanup preparation for page table walk
      numa_maps: remove numa_maps->vma
      numa_maps: fix typo in gather_hugetbl_stats
      pagemap: use walk->vma instead of calling find_vma()
      clear_refs: remove clear_refs_private->vma and introduce clear_refs_test_walk()
      ...

commit 6b00f7efb5303418c231994c91fb8239f5ada260
Merge: b3d6524ff795 d476d94f180a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 11 18:03:54 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "arm64 updates for 3.20:
    
       - reimplementation of the virtual remapping of UEFI Runtime Services
         in a way that is stable across kexec
       - emulation of the "setend" instruction for 32-bit tasks (user
         endianness switching trapped in the kernel, SCTLR_EL1.E0E bit set
         accordingly)
       - compat_sys_call_table implemented in C (from asm) and made it a
         constant array together with sys_call_table
       - export CPU cache information via /sys (like other architectures)
       - DMA API implementation clean-up in preparation for IOMMU support
       - macros clean-up for KVM
       - dropped some unnecessary cache+tlb maintenance
       - CONFIG_ARM64_CPU_SUSPEND clean-up
       - defconfig update (CPU_IDLE)
    
      The EFI changes going via the arm64 tree have been acked by Matt
      Fleming.  There is also a patch adding sys_*stat64 prototypes to
      include/linux/syscalls.h, acked by Andrew Morton"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (47 commits)
      arm64: compat: Remove incorrect comment in compat_siginfo
      arm64: Fix section mismatch on alloc_init_p[mu]d()
      arm64: Avoid breakage caused by .altmacro in fpsimd save/restore macros
      arm64: mm: use *_sect to check for section maps
      arm64: drop unnecessary cache+tlb maintenance
      arm64:mm: free the useless initial page table
      arm64: Enable CPU_IDLE in defconfig
      arm64: kernel: remove ARM64_CPU_SUSPEND config option
      arm64: make sys_call_table const
      arm64: Remove asm/syscalls.h
      arm64: Implement the compat_sys_call_table in C
      syscalls: Declare sys_*stat64 prototypes if __ARCH_WANT_(COMPAT_)STAT64
      compat: Declare compat_sys_sigpending and compat_sys_sigprocmask prototypes
      arm64: uapi: expose our struct ucontext to the uapi headers
      smp, ARM64: Kill SMP single function call interrupt
      arm64: Emulate SETEND for AArch32 tasks
      arm64: Consolidate hotplug notifier for instruction emulation
      arm64: Track system support for mixed endian EL0
      arm64: implement generic IOMMU configuration
      arm64: Combine coherent and non-coherent swiotlb dma_ops
      ...

commit d016bf7ece53b2b947bfd769e0842fd2feb7556b
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Wed Feb 11 15:26:41 2015 -0800

    mm: make FIRST_USER_ADDRESS unsigned long on all archs
    
    LKP has triggered a compiler warning after my recent patch "mm: account
    pmd page tables to the process":
    
        mm/mmap.c: In function 'exit_mmap':
     >> mm/mmap.c:2857:2: warning: right shift count >= width of type [enabled by default]
    
    The code:
    
     > 2857                WARN_ON(mm_nr_pmds(mm) >
       2858                                round_up(FIRST_USER_ADDRESS, PUD_SIZE) >> PUD_SHIFT);
    
    In this, on tile, we have FIRST_USER_ADDRESS defined as 0.  round_up() has
    the same type -- int.  PUD_SHIFT.
    
    I think the best way to fix it is to define FIRST_USER_ADDRESS as unsigned
    long.  On every arch for consistency.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Reported-by: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 4c445057169d..3e4d3c43632a 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -45,7 +45,7 @@
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))
 
-#define FIRST_USER_ADDRESS	0
+#define FIRST_USER_ADDRESS	0UL
 
 #ifndef __ASSEMBLY__
 extern void __pte_error(const char *file, int line, unsigned long val);

commit 9b3e661e58b90b0c2d5c2168c23408f1e59e9e35
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Tue Feb 10 14:10:15 2015 -0800

    arm64: drop PTE_FILE and pte_file()-related helpers
    
    We've replaced remap_file_pages(2) implementation with emulation.  Nobody
    creates non-linear mapping anymore.
    
    This patch also adjust __SWP_TYPE_SHIFT and increase number of bits
    availble for swap offset.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 210d632aa5ad..4c445057169d 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -25,7 +25,6 @@
  * Software defined PTE bits definition.
  */
 #define PTE_VALID		(_AT(pteval_t, 1) << 0)
-#define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
 #define PTE_WRITE		(_AT(pteval_t, 1) << 57)
@@ -469,13 +468,12 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 /*
  * Encode and decode a swap entry:
  *	bits 0-1:	present (must be zero)
- *	bit  2:		PTE_FILE
- *	bits 3-8:	swap type
- *	bits 9-57:	swap offset
+ *	bits 2-7:	swap type
+ *	bits 8-57:	swap offset
  */
-#define __SWP_TYPE_SHIFT	3
+#define __SWP_TYPE_SHIFT	2
 #define __SWP_TYPE_BITS		6
-#define __SWP_OFFSET_BITS	49
+#define __SWP_OFFSET_BITS	50
 #define __SWP_TYPE_MASK		((1 << __SWP_TYPE_BITS) - 1)
 #define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)
 #define __SWP_OFFSET_MASK	((1UL << __SWP_OFFSET_BITS) - 1)
@@ -493,18 +491,6 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
  */
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > __SWP_TYPE_BITS)
 
-/*
- * Encode and decode a file entry:
- *	bits 0-1:	present (must be zero)
- *	bit  2:		PTE_FILE
- *	bits 3-57:	file offset / PAGE_SIZE
- */
-#define pte_file(pte)		(pte_val(pte) & PTE_FILE)
-#define pte_to_pgoff(x)		(pte_val(x) >> 3)
-#define pgoff_to_pte(x)		__pte(((x) << 3) | PTE_FILE)
-
-#define PTE_FILE_MAX_BITS	55
-
 extern int kern_addr_valid(unsigned long addr);
 
 #include <asm-generic/pgtable.h>

commit 523d6e9fae9333a0e2a7baf4d11c8bcca544790e
Author: zhichang.yuan <zhichang.yuan@linaro.org>
Date:   Tue Dec 9 07:26:47 2014 +0000

    arm64:mm: free the useless initial page table
    
    For 64K page system, after mapping a PMD section, the corresponding initial
    page table is not needed any more. That page can be freed.
    
    Signed-off-by: Zhichang Yuan <zhichang.yuan@linaro.org>
    [catalin.marinas@arm.com: added BUG_ON() to catch late memblock freeing]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 59079248529d..67f6ede39474 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -342,9 +342,12 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 
 #ifdef CONFIG_ARM64_64K_PAGES
 #define pud_sect(pud)		(0)
+#define pud_table(pud)		(1)
 #else
 #define pud_sect(pud)		((pud_val(pud) & PUD_TYPE_MASK) == \
 				 PUD_TYPE_SECT)
+#define pud_table(pud)		((pud_val(pud) & PUD_TYPE_MASK) == \
+				 PUD_TYPE_TABLE)
 #endif
 
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)

commit 8ce837cee8f51fb0eacb32c85461ea2f0fafc9f8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Oct 20 15:42:07 2014 +0200

    arm64/mm: add create_pgd_mapping() to create private page tables
    
    For UEFI, we need to install the memory mappings used for Runtime Services
    in a dedicated set of page tables. Add create_pgd_mapping(), which allows
    us to allocate and install those page table entries early.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Leif Lindholm <leif.lindholm@linaro.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 210d632aa5ad..59079248529d 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -264,6 +264,11 @@ static inline pmd_t pte_pmd(pte_t pte)
 	return __pmd(pte_val(pte));
 }
 
+static inline pgprot_t mk_sect_prot(pgprot_t prot)
+{
+	return __pgprot(pgprot_val(prot) & ~PTE_TABLE_BIT);
+}
+
 /*
  * THP definitions.
  */

commit 5d96e0cba26323c3daeb9f7cdfd4efe70985e2c6
Author: Jungseok Lee <jungseoklee85@gmail.com>
Date:   Sat Dec 20 00:49:40 2014 +0000

    arm64: mm: Add pgd_page to support RCU fast_gup
    
    This patch adds pgd_page definition in order to keep supporting
    HAVE_GENERIC_RCU_GUP configuration. In addition, it changes pud_page
    expression to align with pmd_page for readability.
    
    An introduction of pgd_page resolves the following build breakage
    under 4KB + 4Level memory management combo.
    
    mm/gup.c: In function 'gup_huge_pgd':
    mm/gup.c:889:2: error: implicit declaration of function 'pgd_page' [-Werror=implicit-function-declaration]
      head = pgd_page(orig);
      ^
    mm/gup.c:889:7: warning: assignment makes pointer from integer without a cast
      head = pgd_page(orig);
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Jungseok Lee <jungseoklee85@gmail.com>
    [catalin.marinas@arm.com: remove duplicate pmd_page definition]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index df22314f57cf..210d632aa5ad 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -298,7 +298,6 @@ void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
 #define pfn_pmd(pfn,prot)	(__pmd(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
-#define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
 #define pud_write(pud)		pte_write(pud_pte(pud))
 #define pud_pfn(pud)		(((pud_val(pud) & PUD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
 
@@ -401,7 +400,7 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
 }
 
-#define pud_page(pud)           pmd_page(pud_pmd(pud))
+#define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
 
 #endif	/* CONFIG_ARM64_PGTABLE_LEVELS > 2 */
 
@@ -437,6 +436,8 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(addr);
 }
 
+#define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
+
 #endif  /* CONFIG_ARM64_PGTABLE_LEVELS > 3 */
 
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))

commit c164e038eee805147e95789dddb88ae3b3aca11c
Author: Kirill A. Shutemov <kirill@shutemov.name>
Date:   Wed Dec 10 15:44:36 2014 -0800

    mm: fix huge zero page accounting in smaps report
    
    As a small zero page, huge zero page should not be accounted in smaps
    report as normal page.
    
    For small pages we rely on vm_normal_page() to filter out zero page, but
    vm_normal_page() is not designed to handle pmds.  We only get here due
    hackish cast pmd to pte in smaps_pte_range() -- pte and pmd format is not
    necessary compatible on each and every architecture.
    
    Let's add separate codepath to handle pmds.  follow_trans_huge_pmd() will
    detect huge zero page for us.
    
    We would need pmd_dirty() helper to do this properly.  The patch adds it
    to THP-enabled architectures which don't yet have one.
    
    [akpm@linux-foundation.org: use do_div to fix 32-bit build]
    Signed-off-by: "Kirill A. Shutemov" <kirill@shutemov.name>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Tested-by: Fengwei Yin <yfw.kernel@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 41a43bf26492..df22314f57cf 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -279,6 +279,7 @@ void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
 #endif /* CONFIG_HAVE_RCU_TABLE_FREE */
 #endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
+#define pmd_dirty(pmd)		pte_dirty(pmd_pte(pmd))
 #define pmd_young(pmd)		pte_young(pmd_pte(pmd))
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
 #define pmd_mksplitting(pmd)	pte_pmd(pte_mkspecial(pmd_pte(pmd)))

commit 8a5de18239e418fe7b1f36504834689f754d8ccc
Merge: 857b50f5d0ee 2df36a5dd679
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 18 14:32:31 2014 -0700

    Merge tag 'kvm-arm-for-3.18-take-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm
    
    Pull second batch of changes for KVM/{arm,arm64} from Marc Zyngier:
     "The most obvious thing is the sizeable MMU changes to support 48bit
      VAs on arm64.
    
      Summary:
    
       - support for 48bit IPA and VA (EL2)
       - a number of fixes for devices mapped into guests
       - yet another VGIC fix for BE
       - a fix for CPU hotplug
       - a few compile fixes (disabled VGIC, strict mm checks)"
    
    [ I'm pulling directly from Marc at the request of Paolo Bonzini, whose
      backpack was stolen at Dsseldorf airport and will do new keys and
      rebuild his web of trust.    - Linus ]
    
    * tag 'kvm-arm-for-3.18-take-2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm:
      arm/arm64: KVM: Fix BE accesses to GICv2 EISR and ELRSR regs
      arm: kvm: STRICT_MM_TYPECHECKS fix for user_mem_abort
      arm/arm64: KVM: Ensure memslots are within KVM_PHYS_SIZE
      arm64: KVM: Implement 48 VA support for KVM EL2 and Stage-2
      arm/arm64: KVM: map MMIO regions at creation time
      arm64: kvm: define PAGE_S2_DEVICE as read-only by default
      ARM: kvm: define PAGE_S2_DEVICE as read-only by default
      arm/arm64: KVM: add 'writable' parameter to kvm_phys_addr_ioremap
      arm/arm64: KVM: fix potential NULL dereference in user_mem_abort()
      arm/arm64: KVM: use __GFP_ZERO not memset() to get zeroed pages
      ARM: KVM: fix vgic-disabled build
      arm: kvm: fix CPU hotplug

commit 4a513fb009b96cf3d86491e00565367ceec29073
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Sep 17 14:56:20 2014 -0700

    arm64: kvm: define PAGE_S2_DEVICE as read-only by default
    
    Now that we support read-only memslots, we need to make sure that
    pass-through device mappings are not mapped writable if the guest
    has requested them to be read-only. The existing implementation
    already honours this by calling kvm_set_s2pte_writable() on the new
    pte in case of writable mappings, so all we need to do is define
    the default pgprot_t value used for devices to be PTE_S2_RDONLY.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index ffe1ba0506d1..51f6f5284ce5 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -79,7 +79,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define PAGE_HYP_DEVICE		__pgprot(PROT_DEVICE_nGnRE | PTE_HYP)
 
 #define PAGE_S2			__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
-#define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDWR | PTE_UXN)
+#define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDONLY | PTE_UXN)
 
 #define PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
 #define PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)

commit 0cf744bc7ae8e0072159a901f6e1a159bbc30ffa
Merge: b52839266941 7f8998c7aef3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 9 22:26:14 2014 -0400

    Merge branch 'akpm' (fixes from Andrew Morton)
    
    Merge patch-bomb from Andrew Morton:
     - part of OCFS2 (review is laggy again)
     - procfs
     - slab
     - all of MM
     - zram, zbud
     - various other random things: arch, filesystems.
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (164 commits)
      nosave: consolidate __nosave_{begin,end} in <asm/sections.h>
      include/linux/screen_info.h: remove unused ORIG_* macros
      kernel/sys.c: compat sysinfo syscall: fix undefined behavior
      kernel/sys.c: whitespace fixes
      acct: eliminate compile warning
      kernel/async.c: switch to pr_foo()
      include/linux/blkdev.h: use NULL instead of zero
      include/linux/kernel.h: deduplicate code implementing clamp* macros
      include/linux/kernel.h: rewrite min3, max3 and clamp using min and max
      alpha: use Kbuild logic to include <asm-generic/sections.h>
      frv: remove deprecated IRQF_DISABLED
      frv: remove unused cpuinfo_frv and friends to fix future build error
      zbud: avoid accessing last unused freelist
      zsmalloc: simplify init_zspage free obj linking
      mm/zsmalloc.c: correct comment for fullness group computation
      zram: use notify_free to account all free notifications
      zram: report maximum used memory
      zram: zram memory size limitation
      zsmalloc: change return value unit of zs_get_total_size_bytes
      zsmalloc: move pages_allocated to zs_pool
      ...

commit 29e5694054149acd25b0d5538c95fb6d64478315
Author: Steve Capper <steve.capper@linaro.org>
Date:   Thu Oct 9 15:29:25 2014 -0700

    arm64: mm: enable RCU fast_gup
    
    Activate the RCU fast_gup for ARM64.  We also need to force THP splits to
    broadcast an IPI s.t.  we block in the fast_gup page walker.  As THP
    splits are comparatively rare, this should not lead to a noticeable
    performance degradation.
    
    Some pre-requisite functions pud_write and pud_page are also added.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Tested-by: Dann Frazier <dann.frazier@canonical.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index d58e40cde88e..464c5cecdd15 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -244,6 +244,16 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 
 #define __HAVE_ARCH_PTE_SPECIAL
 
+static inline pte_t pud_pte(pud_t pud)
+{
+	return __pte(pud_val(pud));
+}
+
+static inline pmd_t pud_pmd(pud_t pud)
+{
+	return __pmd(pud_val(pud));
+}
+
 static inline pte_t pmd_pte(pmd_t pmd)
 {
 	return __pte(pmd_val(pmd));
@@ -261,7 +271,13 @@ static inline pmd_t pte_pmd(pte_t pte)
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define pmd_trans_huge(pmd)	(pmd_val(pmd) && !(pmd_val(pmd) & PMD_TABLE_BIT))
 #define pmd_trans_splitting(pmd)	pte_special(pmd_pte(pmd))
-#endif
+#ifdef CONFIG_HAVE_RCU_TABLE_FREE
+#define __HAVE_ARCH_PMDP_SPLITTING_FLUSH
+struct vm_area_struct;
+void pmdp_splitting_flush(struct vm_area_struct *vma, unsigned long address,
+			  pmd_t *pmdp);
+#endif /* CONFIG_HAVE_RCU_TABLE_FREE */
+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
 
 #define pmd_young(pmd)		pte_young(pmd_pte(pmd))
 #define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
@@ -282,6 +298,7 @@ static inline pmd_t pte_pmd(pte_t pte)
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
 #define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+#define pud_write(pud)		pte_write(pud_pte(pud))
 #define pud_pfn(pud)		(((pud_val(pud) & PUD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
@@ -381,6 +398,8 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
 }
 
+#define pud_page(pud)           pmd_page(pud_pmd(pud))
+
 #endif	/* CONFIG_ARM64_PGTABLE_LEVELS > 2 */
 
 #if CONFIG_ARM64_PGTABLE_LEVELS > 3

commit 80213c03c4151d900cf293ef0fc51f8d88495e14
Merge: ea584595fc85 f92d9ee3ab39
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 9 15:03:49 2014 -0400

    Merge tag 'pci-v3.18-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci
    
    Pull PCI updates from Bjorn Helgaas:
     "The interesting things here are:
    
       - Turn on Config Request Retry Status Software Visibility.  This
         caused hangs last time, but we included a fix this time.
       - Rework PCI device configuration to use _HPP/_HPX more aggressively
       - Allow PCI devices to be put into D3cold during system suspend
       - Add arm64 PCI support
       - Add APM X-Gene host bridge driver
       - Add TI Keystone host bridge driver
       - Add Xilinx AXI host bridge driver
    
      More detailed summary:
    
      Enumeration
        - Check Vendor ID only for Config Request Retry Status (Rajat Jain)
        - Enable Config Request Retry Status when supported (Rajat Jain)
        - Add generic domain handling (Catalin Marinas)
        - Generate uppercase hex for modalias interface class (Ricardo Ribalda Delgado)
    
      Resource management
        - Add missing MEM_64 mask in pci_assign_unassigned_bridge_resources() (Yinghai Lu)
        - Increase IBM ipr SAS Crocodile BARs to at least system page size (Douglas Lehr)
    
      PCI device hotplug
        - Prevent NULL dereference during pciehp probe (Andreas Noever)
        - Move _HPP & _HPX handling into core (Bjorn Helgaas)
        - Apply _HPP to PCIe devices as well as PCI (Bjorn Helgaas)
        - Apply _HPP/_HPX to display devices (Bjorn Helgaas)
        - Preserve SERR & PARITY settings when applying _HPP/_HPX (Bjorn Helgaas)
        - Preserve MPS and MRRS settings when applying _HPP/_HPX (Bjorn Helgaas)
        - Apply _HPP/_HPX to all devices, not just hot-added ones (Bjorn Helgaas)
        - Fix wait time in pciehp timeout message (Yinghai Lu)
        - Add more pciehp Slot Control debug output (Yinghai Lu)
        - Stop disabling pciehp notifications during init (Yinghai Lu)
    
      MSI
        - Remove arch_msi_check_device() (Alexander Gordeev)
        - Rename pci_msi_check_device() to pci_msi_supported() (Alexander Gordeev)
        - Move D0 check into pci_msi_check_device() (Alexander Gordeev)
        - Remove unused kobject from struct msi_desc (Yijing Wang)
        - Remove "pos" from the struct msi_desc msi_attrib (Yijing Wang)
        - Add "msi_bus" sysfs MSI/MSI-X control for endpoints (Yijing Wang)
        - Use __get_cached_msi_msg() instead of get_cached_msi_msg() (Yijing Wang)
        - Use __read_msi_msg() instead of read_msi_msg() (Yijing Wang)
        - Use __write_msi_msg() instead of write_msi_msg() (Yijing Wang)
    
      Power management
        - Drop unused runtime PM support code for PCIe ports (Rafael J.  Wysocki)
        - Allow PCI devices to be put into D3cold during system suspend (Rafael J. Wysocki)
    
      AER
        - Add additional AER error strings (Gong Chen)
        - Make <linux/aer.h> standalone includable (Thierry Reding)
    
      Virtualization
        - Add ACS quirk for Solarflare SFC9120 & SFC9140 (Alex Williamson)
        - Add ACS quirk for Intel 10G NICs (Alex Williamson)
        - Add ACS quirk for AMD A88X southbridge (Marti Raudsepp)
        - Remove unused pci_find_upstream_pcie_bridge(), pci_get_dma_source() (Alex Williamson)
        - Add device flag helpers (Ethan Zhao)
        - Assume all Mellanox devices have broken INTx masking (Gavin Shan)
    
      Generic host bridge driver
        - Fix ioport_map() for !CONFIG_GENERIC_IOMAP (Liviu Dudau)
        - Add pci_register_io_range() and pci_pio_to_address() (Liviu Dudau)
        - Define PCI_IOBASE as the base of virtual PCI IO space (Liviu Dudau)
        - Fix the conversion of IO ranges into IO resources (Liviu Dudau)
        - Add pci_get_new_domain_nr() and of_get_pci_domain_nr() (Liviu Dudau)
        - Add support for parsing PCI host bridge resources from DT (Liviu Dudau)
        - Add pci_remap_iospace() to map bus I/O resources (Liviu Dudau)
        - Add arm64 architectural support for PCI (Liviu Dudau)
    
      APM X-Gene
        - Add APM X-Gene PCIe driver (Tanmay Inamdar)
        - Add arm64 DT APM X-Gene PCIe device tree nodes (Tanmay Inamdar)
    
      Freescale i.MX6
        - Probe in module_init(), not fs_initcall() (Lucas Stach)
        - Delay enabling reference clock for SS until it stabilizes (Tim Harvey)
    
      Marvell MVEBU
        - Fix uninitialized variable in mvebu_get_tgt_attr() (Thomas Petazzoni)
    
      NVIDIA Tegra
        - Make sure the PCIe PLL is really reset (Eric Yuen)
        - Add error path tegra_msi_teardown_irq() cleanup (Jisheng Zhang)
        - Fix extended configuration space mapping (Peter Daifuku)
        - Implement resource hierarchy (Thierry Reding)
        - Clear CLKREQ# enable on port disable (Thierry Reding)
        - Add Tegra124 support (Thierry Reding)
    
      ST Microelectronics SPEAr13xx
        - Pass config resource through reg property (Pratyush Anand)
    
      Synopsys DesignWare
        - Use NULL instead of false (Fabio Estevam)
        - Parse bus-range property from devicetree (Lucas Stach)
        - Use pci_create_root_bus() instead of pci_scan_root_bus() (Lucas Stach)
        - Remove pci_assign_unassigned_resources() (Lucas Stach)
        - Check private_data validity in single place (Lucas Stach)
        - Setup and clear exactly one MSI at a time (Lucas Stach)
        - Remove open-coded bitmap operations (Lucas Stach)
        - Fix configuration base address when using 'reg' (Minghuan Lian)
        - Fix IO resource end address calculation (Minghuan Lian)
        - Rename get_msi_data() to get_msi_addr() (Minghuan Lian)
        - Add get_msi_data() to pcie_host_ops (Minghuan Lian)
        - Add support for v3.65 hardware (Murali Karicheri)
        - Fold struct pcie_port_info into struct pcie_port (Pratyush Anand)
    
      TI Keystone
        - Add TI Keystone PCIe driver (Murali Karicheri)
        - Limit MRSS for all downstream devices (Murali Karicheri)
        - Assume controller is already in RC mode (Murali Karicheri)
        - Set device ID based on SoC to support multiple ports (Murali Karicheri)
    
      Xilinx AXI
        - Add Xilinx AXI PCIe driver (Srikanth Thokala)
        - Fix xilinx_pcie_assign_msi() return value test (Dan Carpenter)
    
      Miscellaneous
        - Clean up whitespace (Quentin Lambert)
        - Remove assignments from "if" conditions (Quentin Lambert)
        - Move PCI_VENDOR_ID_VMWARE to pci_ids.h (Francesco Ruggeri)
        - x86: Mark DMI tables as initialization data (Mathias Krause)
        - x86: Move __init annotation to the correct place (Mathias Krause)
        - x86: Mark constants of pci_mmcfg_nvidia_mcp55() as __initconst (Mathias Krause)
        - x86: Constify pci_mmcfg_probes[] array (Mathias Krause)
        - x86: Mark PCI BIOS initialization code as such (Mathias Krause)
        - Parenthesize PCI_DEVID and PCI_VPD_LRDT_ID parameters (Megan Kamiya)
        - Remove unnecessary variable in pci_add_dynid() (Tobias Klauser)"
    
    * tag 'pci-v3.18-changes' of git://git.kernel.org/pub/scm/linux/kernel/git/helgaas/pci: (109 commits)
      arm64: dts: Add APM X-Gene PCIe device tree nodes
      PCI: Add ACS quirk for AMD A88X southbridge devices
      PCI: xgene: Add APM X-Gene PCIe driver
      PCI: designware: Remove open-coded bitmap operations
      PCI/MSI: Remove unnecessary temporary variable
      PCI/MSI: Use __write_msi_msg() instead of write_msi_msg()
      MSI/powerpc: Use __read_msi_msg() instead of read_msi_msg()
      PCI/MSI: Use __get_cached_msi_msg() instead of get_cached_msi_msg()
      PCI/MSI: Add "msi_bus" sysfs MSI/MSI-X control for endpoints
      PCI/MSI: Remove "pos" from the struct msi_desc msi_attrib
      PCI/MSI: Remove unused kobject from struct msi_desc
      PCI/MSI: Rename pci_msi_check_device() to pci_msi_supported()
      PCI/MSI: Move D0 check into pci_msi_check_device()
      PCI/MSI: Remove arch_msi_check_device()
      irqchip: armada-370-xp: Remove arch_msi_check_device()
      PCI/MSI/PPC: Remove arch_msi_check_device()
      arm64: Add architectural support for PCI
      PCI: Add pci_remap_iospace() to map bus I/O resources
      of/pci: Add support for parsing PCI host bridge resources from DT
      of/pci: Add pci_get_new_domain_nr() and of_get_pci_domain_nr()
      ...
    
    Conflicts:
            arch/arm64/boot/dts/apm-storm.dtsi

commit d1e6dc91b532d3d3dbbd0fa356b775ca320dc2c2
Author: Liviu Dudau <Liviu.Dudau@arm.com>
Date:   Mon Sep 29 15:29:31 2014 +0100

    arm64: Add architectural support for PCI
    
    Use the generic PCI domain and OF functions to provide support for PCI
    on arm64.
    
    [bhelgaas: Change comments to use generic PCI, not just PCIe.  Nothing at
    this level is PCIe-specific.]
    Signed-off-by: Liviu Dudau <Liviu.Dudau@arm.com>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index ffe1ba0506d1..a968523adf56 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -296,6 +296,8 @@ static inline int has_transparent_hugepage(void)
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRnE) | PTE_PXN | PTE_UXN)
 #define pgprot_writecombine(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
+#define pgprot_device(prot) \
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRE) | PTE_PXN | PTE_UXN)
 #define __HAVE_PHYS_MEM_ACCESS_PROT
 struct file;
 extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,

commit b6d4f2800b7bad654caf00654f4bff21594ef838
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Aug 19 20:41:42 2014 +0100

    arm64: Introduce {set,clear}_pte_bit
    
    It's useful to be able to change individual bits in ptes at times.
    Introduce functions for this and update existing pte_mk* functions
    to use these primatives.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    [will: added missing inline keyword for new header functions]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index ffe1ba0506d1..d58e40cde88e 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -149,46 +149,51 @@ extern struct page *empty_zero_page;
 #define pte_valid_not_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == PTE_VALID)
 
-static inline pte_t pte_wrprotect(pte_t pte)
+static inline pte_t clear_pte_bit(pte_t pte, pgprot_t prot)
 {
-	pte_val(pte) &= ~PTE_WRITE;
+	pte_val(pte) &= ~pgprot_val(prot);
 	return pte;
 }
 
-static inline pte_t pte_mkwrite(pte_t pte)
+static inline pte_t set_pte_bit(pte_t pte, pgprot_t prot)
 {
-	pte_val(pte) |= PTE_WRITE;
+	pte_val(pte) |= pgprot_val(prot);
 	return pte;
 }
 
+static inline pte_t pte_wrprotect(pte_t pte)
+{
+	return clear_pte_bit(pte, __pgprot(PTE_WRITE));
+}
+
+static inline pte_t pte_mkwrite(pte_t pte)
+{
+	return set_pte_bit(pte, __pgprot(PTE_WRITE));
+}
+
 static inline pte_t pte_mkclean(pte_t pte)
 {
-	pte_val(pte) &= ~PTE_DIRTY;
-	return pte;
+	return clear_pte_bit(pte, __pgprot(PTE_DIRTY));
 }
 
 static inline pte_t pte_mkdirty(pte_t pte)
 {
-	pte_val(pte) |= PTE_DIRTY;
-	return pte;
+	return set_pte_bit(pte, __pgprot(PTE_DIRTY));
 }
 
 static inline pte_t pte_mkold(pte_t pte)
 {
-	pte_val(pte) &= ~PTE_AF;
-	return pte;
+	return clear_pte_bit(pte, __pgprot(PTE_AF));
 }
 
 static inline pte_t pte_mkyoung(pte_t pte)
 {
-	pte_val(pte) |= PTE_AF;
-	return pte;
+	return set_pte_bit(pte, __pgprot(PTE_AF));
 }
 
 static inline pte_t pte_mkspecial(pte_t pte)
 {
-	pte_val(pte) |= PTE_SPECIAL;
-	return pte;
+	return set_pte_bit(pte, __pgprot(PTE_SPECIAL));
 }
 
 static inline void set_pte(pte_t *ptep, pte_t pte)

commit 7f0b1bf04511348995d6fce38c87c98a3b5cb781
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jun 9 11:55:03 2014 +0100

    arm64: Fix barriers used for page table modifications
    
    The architecture specification states that both DSB and ISB are required
    between page table modifications and subsequent memory accesses using the
    corresponding virtual address. When TLB invalidation takes place, the
    tlb_flush_* functions already have the necessary barriers. However, there are
    other functions like create_mapping() for which this is not the case.
    
    The patch adds the DSB+ISB instructions in the set_pte() function for
    valid kernel mappings. The invalid pte case is handled by tlb_flush_*
    and the user mappings in general have a corresponding update_mmu_cache()
    call containing a DSB. Even when update_mmu_cache() isn't called, the
    kernel can still cope with an unlikely spurious page fault by
    re-executing the instruction.
    
    In addition, the set_pmd, set_pud() functions gain an ISB for
    architecture compliance when block mappings are created.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Leif Lindholm <leif.lindholm@linaro.org>
    Acked-by: Steve Capper <steve.capper@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 73968c2180aa..ffe1ba0506d1 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -146,6 +146,8 @@ extern struct page *empty_zero_page;
 
 #define pte_valid_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
+#define pte_valid_not_user(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_USER)) == PTE_VALID)
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
@@ -192,6 +194,15 @@ static inline pte_t pte_mkspecial(pte_t pte)
 static inline void set_pte(pte_t *ptep, pte_t pte)
 {
 	*ptep = pte;
+
+	/*
+	 * Only if the new pte is valid and kernel, otherwise TLB maintenance
+	 * or update_mmu_cache() have the necessary barriers.
+	 */
+	if (pte_valid_not_user(pte)) {
+		dsb(ishst);
+		isb();
+	}
 }
 
 extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
@@ -311,6 +322,7 @@ static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	*pmdp = pmd;
 	dsb(ishst);
+	isb();
 }
 
 static inline void pmd_clear(pmd_t *pmdp)
@@ -343,6 +355,7 @@ static inline void set_pud(pud_t *pudp, pud_t pud)
 {
 	*pudp = pud;
 	dsb(ishst);
+	isb();
 }
 
 static inline void pud_clear(pud_t *pudp)

commit 7078db46215f9137801a122f87ac31c504220a94
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jul 21 14:52:49 2014 +0100

    arm64: asm/pgtable.h pmd/pud definitions clean-up
    
    Non-functional change to group together the pmd/pud definitions and
    reduce the amount of #if CONFIG_ARM64_PGTABLE_LEVELS.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index ec82789d03c3..73968c2180aa 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -54,15 +54,6 @@ extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pud_error(const char *file, int line, unsigned long val);
 extern void __pgd_error(const char *file, int line, unsigned long val);
 
-#define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
-#if CONFIG_ARM64_PGTABLE_LEVELS > 2
-#define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
-#endif
-#if CONFIG_ARM64_PGTABLE_LEVELS > 3
-#define pud_ERROR(pud)		__pud_error(__FILE__, __LINE__, pud_val(pud))
-#endif
-#define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
-
 #ifdef CONFIG_SMP
 #define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
 #define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
@@ -123,6 +114,8 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 extern struct page *empty_zero_page;
 #define ZERO_PAGE(vaddr)	(empty_zero_page)
 
+#define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
+
 #define pte_pfn(pte)		((pte_val(pte) & PHYS_MASK) >> PAGE_SHIFT)
 
 #define pfn_pte(pfn,prot)	(__pte(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
@@ -130,6 +123,10 @@ extern struct page *empty_zero_page;
 #define pte_none(pte)		(!pte_val(pte))
 #define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
 #define pte_page(pte)		(pfn_to_page(pte_pfn(pte)))
+
+/* Find an entry in the third-level page table. */
+#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+
 #define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
 
 #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
@@ -336,6 +333,8 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
 
 #if CONFIG_ARM64_PGTABLE_LEVELS > 2
 
+#define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
+
 #define pud_none(pud)		(!pud_val(pud))
 #define pud_bad(pud)		(!(pud_val(pud) & 2))
 #define pud_present(pud)	(pud_val(pud))
@@ -356,10 +355,20 @@ static inline pmd_t *pud_page_vaddr(pud_t pud)
 	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
 }
 
+/* Find an entry in the second-level page table. */
+#define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
+
+static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
+{
+	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
+}
+
 #endif	/* CONFIG_ARM64_PGTABLE_LEVELS > 2 */
 
 #if CONFIG_ARM64_PGTABLE_LEVELS > 3
 
+#define pud_ERROR(pud)		__pud_error(__FILE__, __LINE__, pud_val(pud))
+
 #define pgd_none(pgd)		(!pgd_val(pgd))
 #define pgd_bad(pgd)		(!(pgd_val(pgd) & 2))
 #define pgd_present(pgd)	(pgd_val(pgd))
@@ -380,8 +389,18 @@ static inline pud_t *pgd_page_vaddr(pgd_t pgd)
 	return __va(pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK);
 }
 
+/* Find an entry in the frst-level page table. */
+#define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
+
+static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
+{
+	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(addr);
+}
+
 #endif  /* CONFIG_ARM64_PGTABLE_LEVELS > 3 */
 
+#define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
+
 /* to find an entry in a page-table-directory */
 #define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
 
@@ -390,26 +409,6 @@ static inline pud_t *pgd_page_vaddr(pgd_t pgd)
 /* to find an entry in a kernel page-table-directory */
 #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
 
-#if CONFIG_ARM64_PGTABLE_LEVELS > 3
-#define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
-static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
-{
-	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(addr);
-}
-#endif
-
-/* Find an entry in the second-level page table.. */
-#if CONFIG_ARM64_PGTABLE_LEVELS > 2
-#define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
-static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
-{
-	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
-}
-#endif
-
-/* Find an entry in the third-level page table.. */
-#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
-
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |

commit 08375198b01001c0e43bdd580104b16b019a3754
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jul 16 17:42:43 2014 +0100

    arm64: Determine the vmalloc/vmemmap space at build time based on VA_BITS
    
    Rather than guessing what the maximum vmmemap space should be, this
    patch allows the calculation based on the VA_BITS and sizeof(struct
    page). The vmalloc space extends to the beginning of the vmemmap space.
    
    Since the virtual kernel memory layout now depends on the build
    configuration, this patch removes the detailed description in
    Documentation/arm64/memory.txt in favour of information printed during
    kernel booting.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 9f862e6e9286..ec82789d03c3 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -33,13 +33,16 @@
 
 /*
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
+ *
+ * VMEMAP_SIZE: allows the whole VA space to be covered by a struct page array
+ *	(rounded up to PUD_SIZE).
+ * VMALLOC_START: beginning of the kernel VA space
+ * VMALLOC_END: extends to the available space below vmmemmap, PCI I/O space,
+ *	fixed mappings and modules
  */
+#define VMEMMAP_SIZE		ALIGN((1UL << (VA_BITS - PAGE_SHIFT)) * sizeof(struct page), PUD_SIZE)
 #define VMALLOC_START		(UL(0xffffffffffffffff) << VA_BITS)
-#if CONFIG_ARM64_PGTABLE_LEVELS != 4
-#define VMALLOC_END		(PAGE_OFFSET - UL(0x400000000) - SZ_64K)
-#else
-#define VMALLOC_END		(PAGE_OFFSET - UL(0x40000000000) - SZ_64K)
-#endif
+#define VMALLOC_END		(PAGE_OFFSET - PUD_SIZE - VMEMMAP_SIZE - SZ_64K)
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))
 

commit abe669d7e1a8f9163eb7e8e153e7257d38c1ba3e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 15 15:37:21 2014 +0100

    arm64: Convert bool ARM64_x_LEVELS to int ARM64_PGTABLE_LEVELS
    
    Rather than having several Kconfig options, define int
    ARM64_PGTABLE_LEVELS which will be also useful in converting some of the
    pgtable macros.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index d9b23efdaded..9f862e6e9286 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -35,7 +35,7 @@
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
  */
 #define VMALLOC_START		(UL(0xffffffffffffffff) << VA_BITS)
-#ifndef CONFIG_ARM64_4_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS != 4
 #define VMALLOC_END		(PAGE_OFFSET - UL(0x400000000) - SZ_64K)
 #else
 #define VMALLOC_END		(PAGE_OFFSET - UL(0x40000000000) - SZ_64K)
@@ -52,10 +52,10 @@ extern void __pud_error(const char *file, int line, unsigned long val);
 extern void __pgd_error(const char *file, int line, unsigned long val);
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
-#ifndef CONFIG_ARM64_2_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS > 2
 #define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
 #endif
-#ifdef CONFIG_ARM64_4_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS > 3
 #define pud_ERROR(pud)		__pud_error(__FILE__, __LINE__, pud_val(pud))
 #endif
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
@@ -331,7 +331,7 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
  */
 #define mk_pte(page,prot)	pfn_pte(page_to_pfn(page),prot)
 
-#ifndef CONFIG_ARM64_2_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS > 2
 
 #define pud_none(pud)		(!pud_val(pud))
 #define pud_bad(pud)		(!(pud_val(pud) & 2))
@@ -353,9 +353,9 @@ static inline pmd_t *pud_page_vaddr(pud_t pud)
 	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
 }
 
-#endif	/* CONFIG_ARM64_2_LEVELS */
+#endif	/* CONFIG_ARM64_PGTABLE_LEVELS > 2 */
 
-#ifdef CONFIG_ARM64_4_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS > 3
 
 #define pgd_none(pgd)		(!pgd_val(pgd))
 #define pgd_bad(pgd)		(!(pgd_val(pgd) & 2))
@@ -377,7 +377,7 @@ static inline pud_t *pgd_page_vaddr(pgd_t pgd)
 	return __va(pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK);
 }
 
-#endif  /* CONFIG_ARM64_4_LEVELS */
+#endif  /* CONFIG_ARM64_PGTABLE_LEVELS > 3 */
 
 /* to find an entry in a page-table-directory */
 #define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
@@ -387,7 +387,7 @@ static inline pud_t *pgd_page_vaddr(pgd_t pgd)
 /* to find an entry in a kernel page-table-directory */
 #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
 
-#ifdef CONFIG_ARM64_4_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS > 3
 #define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
 static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 {
@@ -396,7 +396,7 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
 #endif
 
 /* Find an entry in the second-level page table.. */
-#ifndef CONFIG_ARM64_2_LEVELS
+#if CONFIG_ARM64_PGTABLE_LEVELS > 2
 #define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
 static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 {

commit c79b954bf6c006f2d3dd9d01f231abeead13a410
Author: Jungseok Lee <jays.lee@samsung.com>
Date:   Mon May 12 18:40:51 2014 +0900

    arm64: mm: Implement 4 levels of translation tables
    
    This patch implements 4 levels of translation tables since 3 levels
    of page tables with 4KB pages cannot support 40-bit physical address
    space described in [1] due to the following issue.
    
    It is a restriction that kernel logical memory map with 4KB + 3 levels
    (0xffffffc000000000-0xffffffffffffffff) cannot cover RAM region from
    544GB to 1024GB in [1]. Specifically, ARM64 kernel fails to create
    mapping for this region in map_mem function since __phys_to_virt for
    this region reaches to address overflow.
    
    If SoC design follows the document, [1], over 32GB RAM would be placed
    from 544GB. Even 64GB system is supposed to use the region from 544GB
    to 576GB for only 32GB RAM. Naturally, it would reach to enable 4 levels
    of page tables to avoid hacking __virt_to_phys and __phys_to_virt.
    
    However, it is recommended 4 levels of page table should be only enabled
    if memory map is too sparse or there is about 512GB RAM.
    
    References
    ----------
    [1]: Principles of ARM Memory Maps, White Paper, Issue C
    
    Signed-off-by: Jungseok Lee <jays.lee@samsung.com>
    Reviewed-by: Sungjinn Chung <sungjinn.chung@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Steve Capper <steve.capper@linaro.org>
    [catalin.marinas@arm.com: MEMBLOCK_INITIAL_LIMIT removed, same as PUD_SIZE]
    [catalin.marinas@arm.com: early_ioremap_init() updated for 4 levels]
    [catalin.marinas@arm.com: 48-bit VA depends on BROKEN until KVM is fixed]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 6d5854972a77..d9b23efdaded 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -35,7 +35,11 @@
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
  */
 #define VMALLOC_START		(UL(0xffffffffffffffff) << VA_BITS)
+#ifndef CONFIG_ARM64_4_LEVELS
 #define VMALLOC_END		(PAGE_OFFSET - UL(0x400000000) - SZ_64K)
+#else
+#define VMALLOC_END		(PAGE_OFFSET - UL(0x40000000000) - SZ_64K)
+#endif
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))
 
@@ -44,12 +48,16 @@
 #ifndef __ASSEMBLY__
 extern void __pte_error(const char *file, int line, unsigned long val);
 extern void __pmd_error(const char *file, int line, unsigned long val);
+extern void __pud_error(const char *file, int line, unsigned long val);
 extern void __pgd_error(const char *file, int line, unsigned long val);
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
 #ifndef CONFIG_ARM64_2_LEVELS
 #define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
 #endif
+#ifdef CONFIG_ARM64_4_LEVELS
+#define pud_ERROR(pud)		__pud_error(__FILE__, __LINE__, pud_val(pud))
+#endif
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
 
 #ifdef CONFIG_SMP
@@ -347,6 +355,30 @@ static inline pmd_t *pud_page_vaddr(pud_t pud)
 
 #endif	/* CONFIG_ARM64_2_LEVELS */
 
+#ifdef CONFIG_ARM64_4_LEVELS
+
+#define pgd_none(pgd)		(!pgd_val(pgd))
+#define pgd_bad(pgd)		(!(pgd_val(pgd) & 2))
+#define pgd_present(pgd)	(pgd_val(pgd))
+
+static inline void set_pgd(pgd_t *pgdp, pgd_t pgd)
+{
+	*pgdp = pgd;
+	dsb(ishst);
+}
+
+static inline void pgd_clear(pgd_t *pgdp)
+{
+	set_pgd(pgdp, __pgd(0));
+}
+
+static inline pud_t *pgd_page_vaddr(pgd_t pgd)
+{
+	return __va(pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK);
+}
+
+#endif  /* CONFIG_ARM64_4_LEVELS */
+
 /* to find an entry in a page-table-directory */
 #define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
 
@@ -355,6 +387,14 @@ static inline pmd_t *pud_page_vaddr(pud_t pud)
 /* to find an entry in a kernel page-table-directory */
 #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
 
+#ifdef CONFIG_ARM64_4_LEVELS
+#define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
+static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
+{
+	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(addr);
+}
+#endif
+
 /* Find an entry in the second-level page table.. */
 #ifndef CONFIG_ARM64_2_LEVELS
 #define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))

commit e41ceed035966d593ae34c3de33924965a6b9fba
Author: Jungseok Lee <jays.lee@samsung.com>
Date:   Mon May 12 10:40:38 2014 +0100

    arm64: Introduce VA_BITS and translation level options
    
    This patch adds virtual address space size and a level of translation
    tables to kernel configuration. It facilicates introduction of
    different MMU options, such as 4KB + 4 levels, 16KB + 4 levels and
    64KB + 3 levels, easily.
    
    The idea is based on the discussion with Catalin Marinas:
    http://www.spinics.net/linux/lists/arm-kernel/msg319552.html
    
    Signed-off-by: Jungseok Lee <jays.lee@samsung.com>
    Reviewed-by: Sungjinn Chung <sungjinn.chung@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index d7455fa83bc7..6d5854972a77 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -47,7 +47,7 @@ extern void __pmd_error(const char *file, int line, unsigned long val);
 extern void __pgd_error(const char *file, int line, unsigned long val);
 
 #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
-#ifndef CONFIG_ARM64_64K_PAGES
+#ifndef CONFIG_ARM64_2_LEVELS
 #define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
 #endif
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
@@ -323,7 +323,7 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
  */
 #define mk_pte(page,prot)	pfn_pte(page_to_pfn(page),prot)
 
-#ifndef CONFIG_ARM64_64K_PAGES
+#ifndef CONFIG_ARM64_2_LEVELS
 
 #define pud_none(pud)		(!pud_val(pud))
 #define pud_bad(pud)		(!(pud_val(pud) & 2))
@@ -345,7 +345,7 @@ static inline pmd_t *pud_page_vaddr(pud_t pud)
 	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
 }
 
-#endif	/* CONFIG_ARM64_64K_PAGES */
+#endif	/* CONFIG_ARM64_2_LEVELS */
 
 /* to find an entry in a page-table-directory */
 #define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
@@ -356,7 +356,7 @@ static inline pmd_t *pud_page_vaddr(pud_t pud)
 #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
 
 /* Find an entry in the second-level page table.. */
-#ifndef CONFIG_ARM64_64K_PAGES
+#ifndef CONFIG_ARM64_2_LEVELS
 #define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
 static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 {

commit b2f8c07bcb7d1a3575f41444d2d8048d0c922762
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 15 15:46:02 2014 +0100

    arm64: Remove duplicate (SWAPPER|IDMAP)_DIR_SIZE definitions
    
    Just keep the asm/page.h definition as this is included in vmlinux.lds.S
    as well.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e0ccceb317d9..d7455fa83bc7 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -383,9 +383,6 @@ static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 
-#define SWAPPER_DIR_SIZE	(3 * PAGE_SIZE)
-#define IDMAP_DIR_SIZE		(2 * PAGE_SIZE)
-
 /*
  * Encode and decode a swap entry:
  *	bits 0-1:	present (must be zero)

commit f3b766a26dd490026b9eb91a9136ade9f49fc674
Author: Steve Capper <steve.capper@linaro.org>
Date:   Wed Jun 25 08:41:45 2014 +0100

    arm64: mm: Fix horrendous config typo
    
    The define ARM64_64K_PAGES is tested for rather than
    CONFIG_ARM64_64K_PAGES. Correct that typo here.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 579702086488..e0ccceb317d9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -292,7 +292,7 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 #define pmd_sect(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
 				 PMD_TYPE_SECT)
 
-#ifdef ARM64_64K_PAGES
+#ifdef CONFIG_ARM64_64K_PAGES
 #define pud_sect(pud)		(0)
 #else
 #define pud_sect(pud)		((pud_val(pud) & PUD_TYPE_MASK) == \

commit e3a920afc3482e954834a4ed95908c4bc5e4c000
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jun 18 14:06:27 2014 +0100

    arm64: mm: remove broken &= operator from pmd_mknotpresent
    
    This should be a plain old '&' and could easily lead to undefined
    behaviour if the target of a pmd_mknotpresent invocation was the same
    as the parameter.
    
    Fixes: 9c7e535fcc17 (arm64: mm: Route pmd thp functions through pte equivalents)
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org> # v3.15
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 598cc384fc1c..579702086488 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -246,7 +246,7 @@ static inline pmd_t pte_pmd(pte_t pte)
 #define pmd_mkwrite(pmd)	pte_pmd(pte_mkwrite(pmd_pte(pmd)))
 #define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))
 #define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
-#define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) &= ~PMD_TYPE_MASK))
+#define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) & ~PMD_TYPE_MASK))
 
 #define __HAVE_ARCH_PMD_WRITE
 #define pmd_write(pmd)		pte_write(pmd_pte(pmd))

commit cc07aabc53978ae09a1d539237189f7c9841060a
Merge: 9e47aaef0bd3 9358d755bd5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 6 10:43:28 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into next
    
    Pull arm64 updates from Catalin Marinas:
     - Optimised assembly string/memory routines (based on the AArch64
       Cortex Strings library contributed to glibc but re-licensed under
       GPLv2)
     - Optimised crypto algorithms making use of the ARMv8 crypto extensions
       (together with kernel API for using FPSIMD instructions in interrupt
       context)
     - Ftrace support
     - CPU topology parsing from DT
     - ESR_EL1 (Exception Syndrome Register) exposed to user space signal
       handlers for SIGSEGV/SIGBUS (useful to emulation tools like Qemu)
     - 1GB section linear mapping if applicable
     - Barriers usage clean-up
     - Default pgprot clean-up
    
    Conflicts as per Catalin.
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (57 commits)
      arm64: kernel: initialize broadcast hrtimer based clock event device
      arm64: ftrace: Add system call tracepoint
      arm64: ftrace: Add CALLER_ADDRx macros
      arm64: ftrace: Add dynamic ftrace support
      arm64: Add ftrace support
      ftrace: Add arm64 support to recordmcount
      arm64: Add 'notrace' attribute to unwind_frame() for ftrace
      arm64: add __ASSEMBLY__ in asm/insn.h
      arm64: Fix linker script entry point
      arm64: lib: Implement optimized string length routines
      arm64: lib: Implement optimized string compare routines
      arm64: lib: Implement optimized memcmp routine
      arm64: lib: Implement optimized memset routine
      arm64: lib: Implement optimized memmove routine
      arm64: lib: Implement optimized memcpy routine
      arm64: defconfig: enable a few more common/useful options in defconfig
      ftrace: Make CALLER_ADDRx macros more generic
      arm64: Fix deadlock scenario with smp_send_stop()
      arm64: Fix machine_shutdown() definition
      arm64: Support arch_irq_work_raise() via self IPIs
      ...

commit 1aacb90eaaac057c10fd746e189553e04cfeb291
Merge: b05d59dfceae 31789538e3ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 4 08:50:34 2014 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial into next
    
    Pull trivial tree changes from Jiri Kosina:
     "Usual pile of patches from trivial tree that make the world go round"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (23 commits)
      staging: go7007: remove reference to CONFIG_KMOD
      aic7xxx: Remove obsolete preprocessor define
      of: dma: doc fixes
      doc: fix incorrect formula to calculate CommitLimit value
      doc: Note need of bc in the kernel build from 3.10 onwards
      mm: Fix printk typo in dmapool.c
      modpost: Fix comment typo "Modules.symvers"
      Kconfig.debug: Grammar s/addition/additional/
      wimax: Spelling s/than/that/, wording s/destinatary/recipient/
      aic7xxx: Spelling s/termnation/termination/
      arm64: mm: Remove superfluous "the" in comment
      of: Spelling s/anonymouns/anonymous/
      dma: imx-sdma: Spelling s/determnine/determine/
      ath10k: Improve grammar in comments
      ath6kl: Spelling s/determnine/determine/
      of: Improve grammar for of_alias_get_id() documentation
      drm/exynos: Spelling s/contro/control/
      radio-bcm2048.c: fix wrong overflow check
      doc: printk-formats: do not mention casts for u64/s64
      doc: spelling error changes
      ...

commit ceb218359de22e70980801d4fa04fffbfc44adb8
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue May 27 19:11:58 2014 +0100

    arm64: mm: fix pmd_write CoW brokenness
    
    Commit 9c7e535fcc17 ("arm64: mm: Route pmd thp functions through pte
    equivalents") changed the pmd manipulator and accessor functions to
    convert the target pmd to a pte, process it with the pte functions, then
    convert it back. Along the way, we gained support for PTE_WRITE, however
    this is completely ignored by set_pmd_at, and so we fail to set the
    PMD_SECT_RDONLY for PMDs, resulting in all sorts of lovely failures (like
    CoW not working).
    
    Partially reverting the offending commit (by making use of
    PMD_SECT_RDONLY explicitly for pmd_{write,wrprotect,mkwrite} functions)
    leads to further issues because pmd_write can then return potentially
    incorrect values for page table entries marked as RDONLY, leading to
    BUG_ON(pmd_write(entry)) tripping under some THP workloads.
    
    This patch fixes the issue by routing set_pmd_at through set_pte_at,
    which correctly takes the PTE_WRITE flag into account. Given that
    THP mappings are always anonymous, the additional cache-flushing code
    in __sync_icache_dcache won't impose any significant overhead as the
    flush will be skipped.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Steve Capper <steve.capper@arm.com>
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 90c811f05a2e..7b1c67a0b485 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -266,7 +266,7 @@ static inline pmd_t pte_pmd(pte_t pte)
 
 #define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
 
-#define set_pmd_at(mm, addr, pmdp, pmd)	set_pmd(pmdp, pmd)
+#define set_pmd_at(mm, addr, pmdp, pmd)	set_pte_at(mm, addr, (pte_t *)pmdp, pmd_pte(pmd))
 
 static inline int has_transparent_hugepage(void)
 {

commit 5a0fdfada3a2aa50d7b947a2e958bf00cbe0d830
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 16 16:44:32 2014 +0100

    Revert "arm64: Introduce execute-only page access permissions"
    
    This reverts commit bc07c2c6e9ed125d362af0214b6313dca180cb08.
    
    While the aim is increased security for --x memory maps, it does not
    protect against kernel level reads. Until SECCOMP is implemented for
    arm64, revert this patch to avoid giving a false idea of execute-only
    mappings.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e4c60d6e18b8..aa150ed99f22 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -86,13 +86,12 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define PAGE_COPY_EXEC		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
 #define PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
-#define PAGE_EXECONLY		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)
 
 #define __P000  PAGE_NONE
 #define __P001  PAGE_READONLY
 #define __P010  PAGE_COPY
 #define __P011  PAGE_COPY
-#define __P100  PAGE_EXECONLY
+#define __P100  PAGE_READONLY_EXEC
 #define __P101  PAGE_READONLY_EXEC
 #define __P110  PAGE_COPY_EXEC
 #define __P111  PAGE_COPY_EXEC
@@ -101,7 +100,7 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #define __S001  PAGE_READONLY
 #define __S010  PAGE_SHARED
 #define __S011  PAGE_SHARED
-#define __S100  PAGE_EXECONLY
+#define __S100  PAGE_READONLY_EXEC
 #define __S101  PAGE_READONLY_EXEC
 #define __S110  PAGE_SHARED_EXEC
 #define __S111  PAGE_SHARED_EXEC
@@ -137,8 +136,8 @@ extern struct page *empty_zero_page;
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
-#define pte_valid_ng(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_NG)) == (PTE_VALID | PTE_NG))
+#define pte_valid_user(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
@@ -192,7 +191,7 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_valid_ng(pte)) {
+	if (pte_valid_user(pte)) {
 		if (!pte_special(pte) && pte_exec(pte))
 			__sync_icache_dcache(pte, addr);
 		if (pte_dirty(pte) && pte_write(pte))

commit 98f7685ee69f871ba991089cb9685f0da07517ea
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 2 16:24:10 2014 +0100

    arm64: barriers: make use of barrier options with explicit barriers
    
    When calling our low-level barrier macros directly, we can often suffice
    with more relaxed behaviour than the default "all accesses, full system"
    option.
    
    This patch updates the users of dsb() to specify the option which they
    actually require.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 3de4ef8bfd82..e4c60d6e18b8 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -303,7 +303,7 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	*pmdp = pmd;
-	dsb();
+	dsb(ishst);
 }
 
 static inline void pmd_clear(pmd_t *pmdp)
@@ -333,7 +333,7 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
 static inline void set_pud(pud_t *pudp, pud_t pud)
 {
 	*pudp = pud;
-	dsb();
+	dsb(ishst);
 }
 
 static inline void pud_clear(pud_t *pudp)

commit 206a2a73a62d37c8b8f6ddd3180c202b2e7298ab
Author: Steve Capper <steve.capper@linaro.org>
Date:   Tue May 6 14:02:27 2014 +0100

    arm64: mm: Create gigabyte kernel logical mappings where possible
    
    We have the capability to map 1GB level 1 blocks when using a 4K
    granule.
    
    This patch adjusts the create_mapping logic s.t. when mapping physical
    memory on boot, we attempt to use a 1GB block if both the VA and PA
    start and end are 1GB aligned. This both reduces the levels of lookup
    required to resolve a kernel logical address, as well as reduces TLB
    pressure on cores that support 1GB TLB entries.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Tested-by: Jungseok Lee <jays.lee@samsung.com>
    [catalin.marinas@arm.com: s/prot_sect_kernel/PROT_SECT_NORMAL_EXEC/]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 752348dbb4f3..3de4ef8bfd82 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -259,6 +259,7 @@ static inline pmd_t pte_pmd(pte_t pte)
 #define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
 
 #define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+#define pud_pfn(pud)		(((pud_val(pud) & PUD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
 
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pmd(pmdp, pmd)
 
@@ -292,6 +293,12 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 #define pmd_sect(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
 				 PMD_TYPE_SECT)
 
+#ifdef ARM64_64K_PAGES
+#define pud_sect(pud)		(0)
+#else
+#define pud_sect(pud)		((pud_val(pud) & PUD_TYPE_MASK) == \
+				 PUD_TYPE_SECT)
+#endif
 
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {

commit a501e32430d4232012ab708b8f0ce841f29e0f02
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Apr 3 15:57:15 2014 +0100

    arm64: Clean up the default pgprot setting
    
    The primary aim of this patchset is to remove the pgprot_default and
    prot_sect_default global variables and rely strictly on predefined
    values. The original goal was to be able to run SMP kernels on UP
    hardware by not setting the Shareability bit. However, it is unlikely to
    see UP ARMv8 hardware and even if we do, the Shareability bit is no
    longer assumed to disable cacheable accesses.
    
    A side effect is that the device mappings now have the Shareability
    attribute set. The hardware, however, should ignore it since Device
    accesses are always Outer Shareable.
    
    Following the removal of the two global variables, there is some PROT_*
    macro reshuffling and cleanup, including the __PAGE_* macros (replaced
    by PAGE_*).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e50bb3cbd8f2..752348dbb4f3 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -52,67 +52,60 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 #endif
 #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
 
-/*
- * The pgprot_* and protection_map entries will be fixed up at runtime to
- * include the cachable and bufferable bits based on memory policy, as well as
- * any architecture dependent bits like global/ASID and SMP shared mapping
- * bits.
- */
-#define _PAGE_DEFAULT		PTE_TYPE_PAGE | PTE_AF
+#ifdef CONFIG_SMP
+#define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF | PTE_SHARED)
+#define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF | PMD_SECT_S)
+#else
+#define PROT_DEFAULT		(PTE_TYPE_PAGE | PTE_AF)
+#define PROT_SECT_DEFAULT	(PMD_TYPE_SECT | PMD_SECT_AF)
+#endif
 
-extern pgprot_t pgprot_default;
+#define PROT_DEVICE_nGnRE	(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_DEVICE_nGnRE))
+#define PROT_NORMAL_NC		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL_NC))
+#define PROT_NORMAL		(PROT_DEFAULT | PTE_PXN | PTE_UXN | PTE_ATTRINDX(MT_NORMAL))
 
-#define __pgprot_modify(prot,mask,bits) \
-	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
+#define PROT_SECT_DEVICE_nGnRE	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_DEVICE_nGnRE))
+#define PROT_SECT_NORMAL	(PROT_SECT_DEFAULT | PMD_SECT_PXN | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
+#define PROT_SECT_NORMAL_EXEC	(PROT_SECT_DEFAULT | PMD_SECT_UXN | PMD_ATTRINDX(MT_NORMAL))
 
-#define _MOD_PROT(p, b)		__pgprot_modify(p, 0, b)
+#define _PAGE_DEFAULT		(PROT_DEFAULT | PTE_ATTRINDX(MT_NORMAL))
 
-#define PAGE_NONE		__pgprot_modify(pgprot_default, PTE_TYPE_MASK, PTE_PROT_NONE | PTE_PXN | PTE_UXN)
-#define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
-#define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
-#define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define PAGE_COPY_EXEC		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
-#define PAGE_READONLY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define PAGE_READONLY_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
-#define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
-#define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY | PTE_WRITE)
+#define PAGE_KERNEL		__pgprot(_PAGE_DEFAULT | PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
+#define PAGE_KERNEL_EXEC	__pgprot(_PAGE_DEFAULT | PTE_UXN | PTE_DIRTY | PTE_WRITE)
 
-#define PAGE_HYP		_MOD_PROT(pgprot_default, PTE_HYP)
+#define PAGE_HYP		__pgprot(_PAGE_DEFAULT | PTE_HYP)
 #define PAGE_HYP_DEVICE		__pgprot(PROT_DEVICE_nGnRE | PTE_HYP)
 
-#define PAGE_S2			__pgprot_modify(pgprot_default, PTE_S2_MEMATTR_MASK, PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
+#define PAGE_S2			__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
 #define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDWR | PTE_UXN)
 
-#define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
-#define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
-#define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
-#define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
-#define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
-#define __PAGE_EXECONLY		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)
-
-#endif /* __ASSEMBLY__ */
-
-#define __P000  __PAGE_NONE
-#define __P001  __PAGE_READONLY
-#define __P010  __PAGE_COPY
-#define __P011  __PAGE_COPY
-#define __P100  __PAGE_EXECONLY
-#define __P101  __PAGE_READONLY_EXEC
-#define __P110  __PAGE_COPY_EXEC
-#define __P111  __PAGE_COPY_EXEC
-
-#define __S000  __PAGE_NONE
-#define __S001  __PAGE_READONLY
-#define __S010  __PAGE_SHARED
-#define __S011  __PAGE_SHARED
-#define __S100  __PAGE_EXECONLY
-#define __S101  __PAGE_READONLY_EXEC
-#define __S110  __PAGE_SHARED_EXEC
-#define __S111  __PAGE_SHARED_EXEC
+#define PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
+#define PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
+#define PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
+#define PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define PAGE_COPY_EXEC		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
+#define PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
+#define PAGE_EXECONLY		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)
+
+#define __P000  PAGE_NONE
+#define __P001  PAGE_READONLY
+#define __P010  PAGE_COPY
+#define __P011  PAGE_COPY
+#define __P100  PAGE_EXECONLY
+#define __P101  PAGE_READONLY_EXEC
+#define __P110  PAGE_COPY_EXEC
+#define __P111  PAGE_COPY_EXEC
+
+#define __S000  PAGE_NONE
+#define __S001  PAGE_READONLY
+#define __S010  PAGE_SHARED
+#define __S011  PAGE_SHARED
+#define __S100  PAGE_EXECONLY
+#define __S101  PAGE_READONLY_EXEC
+#define __S110  PAGE_SHARED_EXEC
+#define __S111  PAGE_SHARED_EXEC
 
-#ifndef __ASSEMBLY__
 /*
  * ZERO_PAGE is a global shared page that is always zero: used
  * for zero-mapped memory areas etc..
@@ -274,6 +267,9 @@ static inline int has_transparent_hugepage(void)
 	return 1;
 }
 
+#define __pgprot_modify(prot,mask,bits) \
+	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
+
 /*
  * Mark the prot value as uncacheable and unbufferable.
  */

commit bc07c2c6e9ed125d362af0214b6313dca180cb08
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Apr 3 16:17:32 2014 +0100

    arm64: Introduce execute-only page access permissions
    
    The ARMv8 architecture allows execute-only user permissions by clearing
    the PTE_UXN and PTE_USER bits. The kernel, however, can still access
    such page, so execute-only page permission does not protect against
    read(2)/write(2) etc. accesses. Systems requiring such protection must
    implement/enable features like SECCOMP.
    
    This patch changes the arm64 __P100 and __S100 protection_map[] macros
    to the new __PAGE_EXECONLY attributes. A side effect is that
    pte_valid_user() no longer triggers for __PAGE_EXECONLY since PTE_USER
    isn't set. To work around this, the check is done on the PTE_NG bit via
    the pte_valid_ng() macro. VM_READ is also checked now for page faults.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 90c811f05a2e..e50bb3cbd8f2 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -90,6 +90,7 @@ extern pgprot_t pgprot_default;
 #define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
 #define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
+#define __PAGE_EXECONLY		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)
 
 #endif /* __ASSEMBLY__ */
 
@@ -97,7 +98,7 @@ extern pgprot_t pgprot_default;
 #define __P001  __PAGE_READONLY
 #define __P010  __PAGE_COPY
 #define __P011  __PAGE_COPY
-#define __P100  __PAGE_READONLY_EXEC
+#define __P100  __PAGE_EXECONLY
 #define __P101  __PAGE_READONLY_EXEC
 #define __P110  __PAGE_COPY_EXEC
 #define __P111  __PAGE_COPY_EXEC
@@ -106,7 +107,7 @@ extern pgprot_t pgprot_default;
 #define __S001  __PAGE_READONLY
 #define __S010  __PAGE_SHARED
 #define __S011  __PAGE_SHARED
-#define __S100  __PAGE_READONLY_EXEC
+#define __S100  __PAGE_EXECONLY
 #define __S101  __PAGE_READONLY_EXEC
 #define __S110  __PAGE_SHARED_EXEC
 #define __S111  __PAGE_SHARED_EXEC
@@ -143,8 +144,8 @@ extern struct page *empty_zero_page;
 #define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
-#define pte_valid_user(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
+#define pte_valid_ng(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_NG)) == (PTE_VALID | PTE_NG))
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
@@ -198,7 +199,7 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_valid_user(pte)) {
+	if (pte_valid_ng(pte)) {
 		if (!pte_special(pte) && pte_exec(pte))
 			__sync_icache_dcache(pte, addr);
 		if (pte_dirty(pte) && pte_write(pte))

commit aad9061bf37e05d29a2a94ae8fe1e12d8808a0dd
Author: Geert Uytterhoeven <geert+renesas@linux-m68k.org>
Date:   Tue Mar 11 11:23:39 2014 +0100

    arm64: mm: Remove superfluous "the" in comment
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@linux-m68k.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 90c811f05a2e..97c39bab06be 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -406,7 +406,7 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 
 /*
  * Ensure that there are not more swap files than can be encoded in the kernel
- * the PTEs.
+ * PTEs.
  */
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > __SWP_TYPE_BITS)
 

commit 1ce235faa8fefa4eb7199cad890944c1d2ba1b3e
Merge: e38be1b10666 196adf2f3015
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 31 15:01:45 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull ARM64 updates from Catalin Marinas:
     - KGDB support for arm64
     - PCI I/O space extended to 16M (in preparation of PCIe support
       patches)
     - Dropping ZONE_DMA32 in favour of ZONE_DMA (we only need one for the
       time being), together with swiotlb late initialisation to correctly
       setup the bounce buffer
     - DMA API cache maintenance support (not all ARMv8 platforms have
       hardware cache coherency)
     - Crypto extensions advertising via ELF_HWCAP2 for compat user space
     - Perf support for dwarf unwinding in compat mode
     - asm/tlb.h converted to the generic mmu_gather code
     - asm-generic rwsem implementation
     - Code clean-up
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (42 commits)
      arm64: Remove pgprot_dmacoherent()
      arm64: Support DMA_ATTR_WRITE_COMBINE
      arm64: Implement custom mmap functions for dma mapping
      arm64: Fix __range_ok macro
      arm64: Fix duplicated Kconfig entries
      arm64: mm: Route pmd thp functions through pte equivalents
      arm64: rwsem: use asm-generic rwsem implementation
      asm-generic: rwsem: de-PPCify rwsem.h
      arm64: enable generic CPU feature modalias matching for this architecture
      arm64: smp: make local symbol static
      arm64: debug: make local symbols static
      ARM64: perf: support dwarf unwinding in compat mode
      ARM64: perf: add support for frame pointer unwinding in compat mode
      ARM64: perf: add support for perf registers API
      arm64: Add boot time configuration of Intermediate Physical Address size
      arm64: Do not synchronise I and D caches for special ptes
      arm64: Make DMA coherent and strongly ordered mappings not executable
      arm64: barriers: add dmb barrier
      arm64: topology: Implement basic CPU topology support
      arm64: advertise ARMv8 extensions to 32-bit compat ELF binaries
      ...

commit 196adf2f3015eacac0567278ba538e3ffdd16d0e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 24 10:35:35 2014 +0000

    arm64: Remove pgprot_dmacoherent()
    
    Since this macro is identical to pgprot_writecombine() and is only used
    in a single place, remove it completely to avoid confusion. On ARMv7+
    processors, the coherent DMA mapping must be Normal NonCacheable (a.k.a.
    writecombine) to avoid mismatched hardware attribute aliases (with the
    kernel linear mapping as Normal Cacheable).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index ae10350f75ec..da92265bd798 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -280,8 +280,6 @@ static inline int has_transparent_hugepage(void)
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRnE) | PTE_PXN | PTE_UXN)
 #define pgprot_writecombine(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
-#define pgprot_dmacoherent(prot) \
-	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
 #define __HAVE_PHYS_MEM_ACCESS_PROT
 struct file;
 extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,

commit 9c7e535fcc1725fc2e2d4f0d9dd14137f0243e23
Author: Steve Capper <steve.capper@linaro.org>
Date:   Tue Feb 25 10:02:13 2014 +0000

    arm64: mm: Route pmd thp functions through pte equivalents
    
    Rather than have separate hugetlb and transparent huge page pmd
    manipulation functions, re-wire our thp functions to simply call the
    pte equivalents.
    
    This allows THP to take advantage of the new PTE_WRITE logic introduced
    in:
      c2c93e5 arm64: mm: Introduce PTE_WRITE
    
    To represent splitting THPs we use the PTE_SPECIAL bit as this is not
    used for pmds.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 72c9ac38cdd9..ae10350f75ec 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -227,36 +227,36 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 
 #define __HAVE_ARCH_PTE_SPECIAL
 
-/*
- * Software PMD bits for THP
- */
+static inline pte_t pmd_pte(pmd_t pmd)
+{
+	return __pte(pmd_val(pmd));
+}
 
-#define PMD_SECT_DIRTY		(_AT(pmdval_t, 1) << 55)
-#define PMD_SECT_SPLITTING	(_AT(pmdval_t, 1) << 57)
+static inline pmd_t pte_pmd(pte_t pte)
+{
+	return __pmd(pte_val(pte));
+}
 
 /*
  * THP definitions.
  */
-#define pmd_young(pmd)		(pmd_val(pmd) & PMD_SECT_AF)
-
-#define __HAVE_ARCH_PMD_WRITE
-#define pmd_write(pmd)		(!(pmd_val(pmd) & PMD_SECT_RDONLY))
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 #define pmd_trans_huge(pmd)	(pmd_val(pmd) && !(pmd_val(pmd) & PMD_TABLE_BIT))
-#define pmd_trans_splitting(pmd) (pmd_val(pmd) & PMD_SECT_SPLITTING)
+#define pmd_trans_splitting(pmd)	pte_special(pmd_pte(pmd))
 #endif
 
-#define PMD_BIT_FUNC(fn,op) \
-static inline pmd_t pmd_##fn(pmd_t pmd) { pmd_val(pmd) op; return pmd; }
+#define pmd_young(pmd)		pte_young(pmd_pte(pmd))
+#define pmd_wrprotect(pmd)	pte_pmd(pte_wrprotect(pmd_pte(pmd)))
+#define pmd_mksplitting(pmd)	pte_pmd(pte_mkspecial(pmd_pte(pmd)))
+#define pmd_mkold(pmd)		pte_pmd(pte_mkold(pmd_pte(pmd)))
+#define pmd_mkwrite(pmd)	pte_pmd(pte_mkwrite(pmd_pte(pmd)))
+#define pmd_mkdirty(pmd)	pte_pmd(pte_mkdirty(pmd_pte(pmd)))
+#define pmd_mkyoung(pmd)	pte_pmd(pte_mkyoung(pmd_pte(pmd)))
+#define pmd_mknotpresent(pmd)	(__pmd(pmd_val(pmd) &= ~PMD_TYPE_MASK))
 
-PMD_BIT_FUNC(wrprotect,	|= PMD_SECT_RDONLY);
-PMD_BIT_FUNC(mkold,	&= ~PMD_SECT_AF);
-PMD_BIT_FUNC(mksplitting, |= PMD_SECT_SPLITTING);
-PMD_BIT_FUNC(mkwrite,   &= ~PMD_SECT_RDONLY);
-PMD_BIT_FUNC(mkdirty,   |= PMD_SECT_DIRTY);
-PMD_BIT_FUNC(mkyoung,   |= PMD_SECT_AF);
-PMD_BIT_FUNC(mknotpresent, &= ~PMD_TYPE_MASK);
+#define __HAVE_ARCH_PMD_WRITE
+#define pmd_write(pmd)		pte_write(pmd_pte(pmd))
 
 #define pmd_mkhuge(pmd)		(__pmd(pmd_val(pmd) & ~PMD_TABLE_BIT))
 
@@ -266,15 +266,6 @@ PMD_BIT_FUNC(mknotpresent, &= ~PMD_TYPE_MASK);
 
 #define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
 
-static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
-{
-	const pmdval_t mask = PMD_SECT_USER | PMD_SECT_PXN | PMD_SECT_UXN |
-			      PMD_SECT_RDONLY | PMD_SECT_PROT_NONE |
-			      PMD_SECT_VALID;
-	pmd_val(pmd) = (pmd_val(pmd) & ~mask) | (pgprot_val(newprot) & mask);
-	return pmd;
-}
-
 #define set_pmd_at(mm, addr, pmdp, pmd)	set_pmd(pmdp, pmd)
 
 static inline int has_transparent_hugepage(void)
@@ -383,6 +374,11 @@ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 	return pte;
 }
 
+static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
+{
+	return pte_pmd(pte_modify(pmd_pte(pmd), newprot));
+}
+
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 

commit 71fdb6bf61bf0692f004f9daf5650392c0cfe300
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 12 16:28:09 2014 +0000

    arm64: Do not synchronise I and D caches for special ptes
    
    Special pte mappings are not intended to be executable and do not even
    have an associated struct page. This patch ensures that we do not call
    __sync_icache_dcache() on such ptes.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Steve Capper <Steve.Capper@arm.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Tested-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2d3cede62709..72c9ac38cdd9 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -199,7 +199,7 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
 	if (pte_valid_user(pte)) {
-		if (pte_exec(pte))
+		if (!pte_special(pte) && pte_exec(pte))
 			__sync_icache_dcache(pte, addr);
 		if (pte_dirty(pte) && pte_write(pte))
 			pte_val(pte) &= ~PTE_RDONLY;

commit de2db7432917a82b62d55bb59635586eeca6d1bd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 12 16:07:06 2014 +0000

    arm64: Make DMA coherent and strongly ordered mappings not executable
    
    pgprot_{dmacoherent,writecombine,noncached} don't need to generate
    executable mappings with side-effects like __sync_icache_dcache() being
    called when the mapping is in user space.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Tested-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b524dcd17243..2d3cede62709 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -286,11 +286,11 @@ static inline int has_transparent_hugepage(void)
  * Mark the prot value as uncacheable and unbufferable.
  */
 #define pgprot_noncached(prot) \
-	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRnE))
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRnE) | PTE_PXN | PTE_UXN)
 #define pgprot_writecombine(prot) \
-	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC))
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
 #define pgprot_dmacoherent(prot) \
-	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC))
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC) | PTE_PXN | PTE_UXN)
 #define __HAVE_PHYS_MEM_ACCESS_PROT
 struct file;
 extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,

commit 84fe6826c28f69d8708bd575faed7f75e6b6f57f
Author: Steve Capper <steve.capper@linaro.org>
Date:   Tue Feb 25 11:38:53 2014 +0000

    arm64: mm: Add double logical invert to pte accessors
    
    Page table entries on ARM64 are 64 bits, and some pte functions such as
    pte_dirty return a bitwise-and of a flag with the pte value. If the
    flag to be tested resides in the upper 32 bits of the pte, then we run
    into the danger of the result being dropped if downcast.
    
    For example:
            gather_stats(page, md, pte_dirty(*pte), 1);
    where pte_dirty(*pte) is downcast to an int.
    
    This patch adds a double logical invert to all the pte_ accessors to
    ensure predictable downcasting.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index b524dcd17243..aa3917c8b623 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -136,11 +136,11 @@ extern struct page *empty_zero_page;
 /*
  * The following only work if pte_present(). Undefined behaviour otherwise.
  */
-#define pte_present(pte)	(pte_val(pte) & (PTE_VALID | PTE_PROT_NONE))
-#define pte_dirty(pte)		(pte_val(pte) & PTE_DIRTY)
-#define pte_young(pte)		(pte_val(pte) & PTE_AF)
-#define pte_special(pte)	(pte_val(pte) & PTE_SPECIAL)
-#define pte_write(pte)		(pte_val(pte) & PTE_WRITE)
+#define pte_present(pte)	(!!(pte_val(pte) & (PTE_VALID | PTE_PROT_NONE)))
+#define pte_dirty(pte)		(!!(pte_val(pte) & PTE_DIRTY))
+#define pte_young(pte)		(!!(pte_val(pte) & PTE_AF))
+#define pte_special(pte)	(!!(pte_val(pte) & PTE_SPECIAL))
+#define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
 #define pte_valid_user(pte) \

commit c2c93e5b7f3f42277ec25ff3746096abc0c0d0f7
Author: Steve Capper <Steve.Capper@arm.com>
Date:   Wed Jan 15 14:07:13 2014 +0000

    arm64: mm: Introduce PTE_WRITE
    
    We have the following means for encoding writable or dirty ptes:
    
                                    PTE_DIRTY       PTE_RDONLY
    !pte_dirty && !pte_write        0               1
    !pte_dirty && pte_write         0               1
    pte_dirty && !pte_write         1               1
    pte_dirty && pte_write          1               0
    
    So we can't distinguish between writable clean ptes and read only
    ptes. This can cause problems with ptes being incorrectly flagged as
    read only when they are writable but not dirty.
    
    This patch introduces a new software bit PTE_WRITE which allows us to
    correctly identify writable ptes. PTE_RDONLY is now only clear for
    valid ptes where a page is both writable and dirty.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 27c78395ae76..b524dcd17243 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -28,7 +28,7 @@
 #define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
-				/* bit 57 for PMD_SECT_SPLITTING */
+#define PTE_WRITE		(_AT(pteval_t, 1) << 57)
 #define PTE_PROT_NONE		(_AT(pteval_t, 1) << 58) /* only when !PTE_VALID */
 
 /*
@@ -67,15 +67,15 @@ extern pgprot_t pgprot_default;
 
 #define _MOD_PROT(p, b)		__pgprot_modify(p, 0, b)
 
-#define PAGE_NONE		__pgprot_modify(pgprot_default, PTE_TYPE_MASK, PTE_PROT_NONE | PTE_RDONLY | PTE_PXN | PTE_UXN)
-#define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
-#define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
-#define PAGE_COPY_EXEC		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
-#define PAGE_READONLY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
-#define PAGE_READONLY_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
-#define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY)
-#define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY)
+#define PAGE_NONE		__pgprot_modify(pgprot_default, PTE_TYPE_MASK, PTE_PROT_NONE | PTE_PXN | PTE_UXN)
+#define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
+#define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
+#define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define PAGE_COPY_EXEC		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
+#define PAGE_READONLY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define PAGE_READONLY_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
+#define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY | PTE_WRITE)
+#define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY | PTE_WRITE)
 
 #define PAGE_HYP		_MOD_PROT(pgprot_default, PTE_HYP)
 #define PAGE_HYP_DEVICE		__pgprot(PROT_DEVICE_nGnRE | PTE_HYP)
@@ -83,13 +83,13 @@ extern pgprot_t pgprot_default;
 #define PAGE_S2			__pgprot_modify(pgprot_default, PTE_S2_MEMATTR_MASK, PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
 #define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDWR | PTE_UXN)
 
-#define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_RDONLY | PTE_PXN | PTE_UXN)
-#define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
-#define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
-#define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
-#define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
-#define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
-#define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
+#define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_PXN | PTE_UXN)
+#define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_WRITE)
+#define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_WRITE)
+#define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
+#define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
 
 #endif /* __ASSEMBLY__ */
 
@@ -140,7 +140,7 @@ extern struct page *empty_zero_page;
 #define pte_dirty(pte)		(pte_val(pte) & PTE_DIRTY)
 #define pte_young(pte)		(pte_val(pte) & PTE_AF)
 #define pte_special(pte)	(pte_val(pte) & PTE_SPECIAL)
-#define pte_write(pte)		(!(pte_val(pte) & PTE_RDONLY))
+#define pte_write(pte)		(pte_val(pte) & PTE_WRITE)
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
 #define pte_valid_user(pte) \
@@ -148,13 +148,13 @@ extern struct page *empty_zero_page;
 
 static inline pte_t pte_wrprotect(pte_t pte)
 {
-	pte_val(pte) |= PTE_RDONLY;
+	pte_val(pte) &= ~PTE_WRITE;
 	return pte;
 }
 
 static inline pte_t pte_mkwrite(pte_t pte)
 {
-	pte_val(pte) &= ~PTE_RDONLY;
+	pte_val(pte) |= PTE_WRITE;
 	return pte;
 }
 
@@ -201,8 +201,10 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 	if (pte_valid_user(pte)) {
 		if (pte_exec(pte))
 			__sync_icache_dcache(pte, addr);
-		if (!pte_dirty(pte))
-			pte = pte_wrprotect(pte);
+		if (pte_dirty(pte) && pte_write(pte))
+			pte_val(pte) &= ~PTE_RDONLY;
+		else
+			pte_val(pte) |= PTE_RDONLY;
 	}
 
 	set_pte(ptep, pte);
@@ -376,7 +378,7 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
-			      PTE_PROT_NONE | PTE_VALID;
+			      PTE_PROT_NONE | PTE_VALID | PTE_WRITE;
 	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
 	return pte;
 }

commit 44b6dfc556811c0e6981329e29e1865fc7a8847d
Author: Steve Capper <Steve.Capper@arm.com>
Date:   Wed Jan 15 14:07:12 2014 +0000

    arm64: mm: Remove PTE_BIT_FUNC macro
    
    Expand out the pte manipulation functions. This makes our life easier
    when using things like tags and cscope.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7f2b60affbb4..27c78395ae76 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -146,16 +146,47 @@ extern struct page *empty_zero_page;
 #define pte_valid_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 
-#define PTE_BIT_FUNC(fn,op) \
-static inline pte_t pte_##fn(pte_t pte) { pte_val(pte) op; return pte; }
-
-PTE_BIT_FUNC(wrprotect, |= PTE_RDONLY);
-PTE_BIT_FUNC(mkwrite,   &= ~PTE_RDONLY);
-PTE_BIT_FUNC(mkclean,   &= ~PTE_DIRTY);
-PTE_BIT_FUNC(mkdirty,   |= PTE_DIRTY);
-PTE_BIT_FUNC(mkold,     &= ~PTE_AF);
-PTE_BIT_FUNC(mkyoung,   |= PTE_AF);
-PTE_BIT_FUNC(mkspecial, |= PTE_SPECIAL);
+static inline pte_t pte_wrprotect(pte_t pte)
+{
+	pte_val(pte) |= PTE_RDONLY;
+	return pte;
+}
+
+static inline pte_t pte_mkwrite(pte_t pte)
+{
+	pte_val(pte) &= ~PTE_RDONLY;
+	return pte;
+}
+
+static inline pte_t pte_mkclean(pte_t pte)
+{
+	pte_val(pte) &= ~PTE_DIRTY;
+	return pte;
+}
+
+static inline pte_t pte_mkdirty(pte_t pte)
+{
+	pte_val(pte) |= PTE_DIRTY;
+	return pte;
+}
+
+static inline pte_t pte_mkold(pte_t pte)
+{
+	pte_val(pte) &= ~PTE_AF;
+	return pte;
+}
+
+static inline pte_t pte_mkyoung(pte_t pte)
+{
+	pte_val(pte) |= PTE_AF;
+	return pte;
+}
+
+static inline pte_t pte_mkspecial(pte_t pte)
+{
+	pte_val(pte) |= PTE_SPECIAL;
+	return pte;
+}
 
 static inline void set_pte(pte_t *ptep, pte_t pte)
 {

commit 3676f9ef5481d614f8c5c857f5319755be248268
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Nov 27 16:59:27 2013 +0000

    arm64: Move PTE_PROT_NONE higher up
    
    PTE_PROT_NONE means that a pte is present but does not have any
    read/write attributes. However, setting the memory type like
    pgprot_writecombine() is allowed and such bits overlap with
    PTE_PROT_NONE. This causes mmap/munmap issues in drivers that change the
    vma->vm_pg_prot on PROT_NONE mappings.
    
    This patch reverts the PTE_FILE/PTE_PROT_NONE shift in commit
    59911ca4325d (ARM64: mm: Move PTE_PROT_NONE bit) and moves PTE_PROT_NONE
    together with the other software bits.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Steve Capper <steve.capper@linaro.org>
    Cc: <stable@vger.kernel.org> # 3.11+

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 5347b39fdb35..7f2b60affbb4 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -25,10 +25,11 @@
  * Software defined PTE bits definition.
  */
 #define PTE_VALID		(_AT(pteval_t, 1) << 0)
-#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 2)	/* only when !PTE_VALID */
-#define PTE_FILE		(_AT(pteval_t, 1) << 3)	/* only when !pte_present() */
+#define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
+				/* bit 57 for PMD_SECT_SPLITTING */
+#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 58) /* only when !PTE_VALID */
 
 /*
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
@@ -357,18 +358,20 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 
 /*
  * Encode and decode a swap entry:
- *	bits 0, 2:	present (must both be zero)
- *	bit  3:		PTE_FILE
- *	bits 4-8:	swap type
- *	bits 9-63:	swap offset
+ *	bits 0-1:	present (must be zero)
+ *	bit  2:		PTE_FILE
+ *	bits 3-8:	swap type
+ *	bits 9-57:	swap offset
  */
-#define __SWP_TYPE_SHIFT	4
+#define __SWP_TYPE_SHIFT	3
 #define __SWP_TYPE_BITS		6
+#define __SWP_OFFSET_BITS	49
 #define __SWP_TYPE_MASK		((1 << __SWP_TYPE_BITS) - 1)
 #define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)
+#define __SWP_OFFSET_MASK	((1UL << __SWP_OFFSET_BITS) - 1)
 
 #define __swp_type(x)		(((x).val >> __SWP_TYPE_SHIFT) & __SWP_TYPE_MASK)
-#define __swp_offset(x)		((x).val >> __SWP_OFFSET_SHIFT)
+#define __swp_offset(x)		(((x).val >> __SWP_OFFSET_SHIFT) & __SWP_OFFSET_MASK)
 #define __swp_entry(type,offset) ((swp_entry_t) { ((type) << __SWP_TYPE_SHIFT) | ((offset) << __SWP_OFFSET_SHIFT) })
 
 #define __pte_to_swp_entry(pte)	((swp_entry_t) { pte_val(pte) })
@@ -382,15 +385,15 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 
 /*
  * Encode and decode a file entry:
- *	bits 0, 2:	present (must both be zero)
- *	bit  3:		PTE_FILE
- *	bits 4-63:	file offset / PAGE_SIZE
+ *	bits 0-1:	present (must be zero)
+ *	bit  2:		PTE_FILE
+ *	bits 3-57:	file offset / PAGE_SIZE
  */
 #define pte_file(pte)		(pte_val(pte) & PTE_FILE)
-#define pte_to_pgoff(x)		(pte_val(x) >> 4)
-#define pgoff_to_pte(x)		__pte(((x) << 4) | PTE_FILE)
+#define pte_to_pgoff(x)		(pte_val(x) >> 3)
+#define pgoff_to_pte(x)		__pte(((x) << 3) | PTE_FILE)
 
-#define PTE_FILE_MAX_BITS	60
+#define PTE_FILE_MAX_BITS	55
 
 extern int kern_addr_valid(unsigned long addr);
 

commit 4f00130b70e5eee813cc7bc298e0f3fdf79673cc
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 29 10:56:14 2013 +0000

    arm64: Use Normal NonCacheable memory for writecombine
    
    This provides better performance compared to Device GRE and also allows
    unaligned accesses. Such memory is intended to be used with standard RAM
    (e.g. framebuffers) and not I/O.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 17bd3af0a117..5347b39fdb35 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -254,7 +254,7 @@ static inline int has_transparent_hugepage(void)
 #define pgprot_noncached(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRnE))
 #define pgprot_writecombine(prot) \
-	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_GRE))
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC))
 #define pgprot_dmacoherent(prot) \
 	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC))
 #define __HAVE_PHYS_MEM_ACCESS_PROT

commit 847264fb7e73ade5b5e4b6eea3daa243a1f5217e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Oct 23 16:50:07 2013 +0100

    arm64: Use 42-bit address space with 64K pages
    
    This patch expands the VA_BITS to 42 when the 64K page configuration is
    enabled allowing 2TB kernel linear mapping. Linux still uses 2 levels of
    page tables in this configuration with pgd now being a full page.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index f0bebc5e22cd..17bd3af0a117 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -33,7 +33,7 @@
 /*
  * VMALLOC and SPARSEMEM_VMEMMAP ranges.
  */
-#define VMALLOC_START		UL(0xffffff8000000000)
+#define VMALLOC_START		(UL(0xffffffffffffffff) << VA_BITS)
 #define VMALLOC_END		(PAGE_OFFSET - UL(0x400000000) - SZ_64K)
 
 #define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))

commit 1873e50028ce87dd9014049c86d71a898fa02166
Merge: fb2af0020a51 aa729dccb5e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 10:31:38 2013 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull ARM64 updates from Catalin Marinas:
     "Main features:
       - KVM and Xen ports to AArch64
       - Hugetlbfs and transparent huge pages support for arm64
       - Applied Micro X-Gene Kconfig entry and dts file
       - Cache flushing improvements
    
      For arm64 huge pages support, there are x86 changes moving part of
      arch/x86/mm/hugetlbpage.c into mm/hugetlb.c to be re-used by arm64"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (66 commits)
      arm64: Add initial DTS for APM X-Gene Storm SOC and APM Mustang board
      arm64: Add defines for APM ARMv8 implementation
      arm64: Enable APM X-Gene SOC family in the defconfig
      arm64: Add Kconfig option for APM X-Gene SOC family
      arm64/Makefile: provide vdso_install target
      ARM64: mm: THP support.
      ARM64: mm: Raise MAX_ORDER for 64KB pages and THP.
      ARM64: mm: HugeTLB support.
      ARM64: mm: Move PTE_PROT_NONE bit.
      ARM64: mm: Make PAGE_NONE pages read only and no-execute.
      ARM64: mm: Restore memblock limit when map_mem finished.
      mm: thp: Correct the HPAGE_PMD_ORDER check.
      x86: mm: Remove general hugetlb code from x86.
      mm: hugetlb: Copy general hugetlb code from x86 to mm.
      x86: mm: Remove x86 version of huge_pmd_share.
      mm: hugetlb: Copy huge_pmd_share from x86 to mm.
      arm64: KVM: document kernel object mappings in HYP
      arm64: KVM: MAINTAINERS update
      arm64: KVM: userspace API documentation
      arm64: KVM: enable initialization of a 32bit vcpu
      ...

commit aa729dccb5e8dfbc78e2e235b8754d6acccee731
Merge: ee877b5321c4 af07484863e0
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jul 1 11:20:58 2013 +0100

    Merge branch 'for-next/hugepages' of git://git.linaro.org/people/stevecapper/linux into upstream-hugepages
    
    * 'for-next/hugepages' of git://git.linaro.org/people/stevecapper/linux:
      ARM64: mm: THP support.
      ARM64: mm: Raise MAX_ORDER for 64KB pages and THP.
      ARM64: mm: HugeTLB support.
      ARM64: mm: Move PTE_PROT_NONE bit.
      ARM64: mm: Make PAGE_NONE pages read only and no-execute.
      ARM64: mm: Restore memblock limit when map_mem finished.
      mm: thp: Correct the HPAGE_PMD_ORDER check.
      x86: mm: Remove general hugetlb code from x86.
      mm: hugetlb: Copy general hugetlb code from x86 to mm.
      x86: mm: Remove x86 version of huge_pmd_share.
      mm: hugetlb: Copy huge_pmd_share from x86 to mm.
    
    Conflicts:
            arch/arm64/Kconfig
            arch/arm64/include/asm/pgtable-hwdef.h
            arch/arm64/include/asm/pgtable.h

commit 40d158e61840fbbe23be3f37302a3ca237c15491
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat May 11 12:13:10 2013 -0400

    consolidate io_remap_pfn_range definitions
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e333a243bfcc..3a768e96cf0e 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -320,13 +320,6 @@ extern int kern_addr_valid(unsigned long addr);
 
 #include <asm-generic/pgtable.h>
 
-/*
- * remap a physical page `pfn' of size `size' with page protection `prot'
- * into virtual address `from'
- */
-#define io_remap_pfn_range(vma,from,pfn,size,prot) \
-		remap_pfn_range(vma, from, pfn, size, prot)
-
 #define pgtable_cache_init() do { } while (0)
 
 #endif /* !__ASSEMBLY__ */

commit af07484863e0c20796081e57093886c22dc16705
Author: Steve Capper <steve.capper@linaro.org>
Date:   Fri Apr 19 16:23:57 2013 +0100

    ARM64: mm: THP support.
    
    Bring Transparent HugePage support to ARM. The size of a
    transparent huge page depends on the normal page size. A
    transparent huge page is always represented as a pmd.
    
    If PAGE_SIZE is 4KB, THPs are 2MB.
    If PAGE_SIZE is 64KB, THPs are 512MB.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 21771330f809..720fc4a2be49 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -187,6 +187,61 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 
 #define __HAVE_ARCH_PTE_SPECIAL
 
+/*
+ * Software PMD bits for THP
+ */
+
+#define PMD_SECT_DIRTY		(_AT(pmdval_t, 1) << 55)
+#define PMD_SECT_SPLITTING	(_AT(pmdval_t, 1) << 57)
+
+/*
+ * THP definitions.
+ */
+#define pmd_young(pmd)		(pmd_val(pmd) & PMD_SECT_AF)
+
+#define __HAVE_ARCH_PMD_WRITE
+#define pmd_write(pmd)		(!(pmd_val(pmd) & PMD_SECT_RDONLY))
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+#define pmd_trans_huge(pmd)	(pmd_val(pmd) && !(pmd_val(pmd) & PMD_TABLE_BIT))
+#define pmd_trans_splitting(pmd) (pmd_val(pmd) & PMD_SECT_SPLITTING)
+#endif
+
+#define PMD_BIT_FUNC(fn,op) \
+static inline pmd_t pmd_##fn(pmd_t pmd) { pmd_val(pmd) op; return pmd; }
+
+PMD_BIT_FUNC(wrprotect,	|= PMD_SECT_RDONLY);
+PMD_BIT_FUNC(mkold,	&= ~PMD_SECT_AF);
+PMD_BIT_FUNC(mksplitting, |= PMD_SECT_SPLITTING);
+PMD_BIT_FUNC(mkwrite,   &= ~PMD_SECT_RDONLY);
+PMD_BIT_FUNC(mkdirty,   |= PMD_SECT_DIRTY);
+PMD_BIT_FUNC(mkyoung,   |= PMD_SECT_AF);
+PMD_BIT_FUNC(mknotpresent, &= ~PMD_TYPE_MASK);
+
+#define pmd_mkhuge(pmd)		(__pmd(pmd_val(pmd) & ~PMD_TABLE_BIT))
+
+#define pmd_pfn(pmd)		(((pmd_val(pmd) & PMD_MASK) & PHYS_MASK) >> PAGE_SHIFT)
+#define pfn_pmd(pfn,prot)	(__pmd(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
+#define mk_pmd(page,prot)	pfn_pmd(page_to_pfn(page),prot)
+
+#define pmd_page(pmd)           pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+
+static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
+{
+	const pmdval_t mask = PMD_SECT_USER | PMD_SECT_PXN | PMD_SECT_UXN |
+			      PMD_SECT_RDONLY | PMD_SECT_PROT_NONE |
+			      PMD_SECT_VALID;
+	pmd_val(pmd) = (pmd_val(pmd) & ~mask) | (pgprot_val(newprot) & mask);
+	return pmd;
+}
+
+#define set_pmd_at(mm, addr, pmdp, pmd)	set_pmd(pmdp, pmd)
+
+static inline int has_transparent_hugepage(void)
+{
+	return 1;
+}
+
 /*
  * Mark the prot value as uncacheable and unbufferable.
  */

commit 084bd29810a5689e423d2f085255a3200a03a06e
Author: Steve Capper <steve.capper@linaro.org>
Date:   Wed Apr 10 13:48:00 2013 +0100

    ARM64: mm: HugeTLB support.
    
    Add huge page support to ARM64, different huge page sizes are
    supported depending on the size of normal pages:
    
    PAGE_SIZE is 4KB:
       2MB - (pmds) these can be allocated at any time.
    1024MB - (puds) usually allocated on bootup with the command line
             with something like: hugepagesz=1G hugepages=6
    
    PAGE_SIZE is 64KB:
     512MB - (pmds) usually allocated on bootup via command line.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 2291de0258ed..21771330f809 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -173,8 +173,17 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 /*
  * Huge pte definitions.
  */
-#define pte_huge(pte)		((pte_val(pte) & PTE_TYPE_MASK) == PTE_TYPE_HUGEPAGE)
-#define pte_mkhuge(pte)		(__pte((pte_val(pte) & ~PTE_TYPE_MASK) | PTE_TYPE_HUGEPAGE))
+#define pte_huge(pte)		(!(pte_val(pte) & PTE_TABLE_BIT))
+#define pte_mkhuge(pte)		(__pte(pte_val(pte) & ~PTE_TABLE_BIT))
+
+/*
+ * Hugetlb definitions.
+ */
+#define HUGE_MAX_HSTATE		2
+#define HPAGE_SHIFT		PMD_SHIFT
+#define HPAGE_SIZE		(_AC(1, UL) << HPAGE_SHIFT)
+#define HPAGE_MASK		(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
 
 #define __HAVE_ARCH_PTE_SPECIAL
 

commit 59911ca4325dc7bd95e05c988fef3593b694e62c
Author: Steve Capper <steve.capper@linaro.org>
Date:   Tue May 28 13:35:51 2013 +0100

    ARM64: mm: Move PTE_PROT_NONE bit.
    
    Under ARM64, PTEs can be broadly categorised as follows:
       - Present and valid: Bit #0 is set. The PTE is valid and memory
         access to the region may fault.
    
       - Present and invalid: Bit #0 is clear and bit #1 is set.
         Represents present memory with PROT_NONE protection. The PTE
         is an invalid entry, and the user fault handler will raise a
         SIGSEGV.
    
       - Not present (file or swap): Bits #0 and #1 are clear.
         Memory represented has been paged out. The PTE is an invalid
         entry, and the fault handler will try and re-populate the
         memory where necessary.
    
    Huge PTEs are block descriptors that have bit #1 clear. If we wish
    to represent PROT_NONE huge PTEs we then run into a problem as
    there is no way to distinguish between regular and huge PTEs if we
    set bit #1.
    
    To resolve this ambiguity this patch moves PTE_PROT_NONE from
    bit #1 to bit #2 and moves PTE_FILE from bit #2 to bit #3. The
    number of swap/file bits is reduced by 1 as a consequence, leaving
    60 bits for file and swap entries.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 77b09d6fee23..2291de0258ed 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -25,8 +25,8 @@
  * Software defined PTE bits definition.
  */
 #define PTE_VALID		(_AT(pteval_t, 1) << 0)
-#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 1)	/* only when !PTE_VALID */
-#define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
+#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 2)	/* only when !PTE_VALID */
+#define PTE_FILE		(_AT(pteval_t, 1) << 3)	/* only when !pte_present() */
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
 
@@ -281,12 +281,12 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 
 /*
  * Encode and decode a swap entry:
- *	bits 0-1:	present (must be zero)
- *	bit  2:		PTE_FILE
- *	bits 3-8:	swap type
+ *	bits 0, 2:	present (must both be zero)
+ *	bit  3:		PTE_FILE
+ *	bits 4-8:	swap type
  *	bits 9-63:	swap offset
  */
-#define __SWP_TYPE_SHIFT	3
+#define __SWP_TYPE_SHIFT	4
 #define __SWP_TYPE_BITS		6
 #define __SWP_TYPE_MASK		((1 << __SWP_TYPE_BITS) - 1)
 #define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)
@@ -306,15 +306,15 @@ extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
 
 /*
  * Encode and decode a file entry:
- *	bits 0-1:	present (must be zero)
- *	bit  2:		PTE_FILE
- *	bits 3-63:	file offset / PAGE_SIZE
+ *	bits 0, 2:	present (must both be zero)
+ *	bit  3:		PTE_FILE
+ *	bits 4-63:	file offset / PAGE_SIZE
  */
 #define pte_file(pte)		(pte_val(pte) & PTE_FILE)
-#define pte_to_pgoff(x)		(pte_val(x) >> 3)
-#define pgoff_to_pte(x)		__pte(((x) << 3) | PTE_FILE)
+#define pte_to_pgoff(x)		(pte_val(x) >> 4)
+#define pgoff_to_pte(x)		__pte(((x) << 4) | PTE_FILE)
 
-#define PTE_FILE_MAX_BITS	61
+#define PTE_FILE_MAX_BITS	60
 
 extern int kern_addr_valid(unsigned long addr);
 

commit 072b1b62a6436b71ab951faae4500db2fbed63de
Author: Steve Capper <steve.capper@linaro.org>
Date:   Thu May 2 16:25:42 2013 +0100

    ARM64: mm: Make PAGE_NONE pages read only and no-execute.
    
    If we consider the following code sequence:
    
            my_pte = pte_modify(entry, myprot);
            x = pte_write(my_pte);
            y = pte_exec(my_pte);
    
    If myprot comes from a PROT_NONE page, then x and y will both be
    true which is undesireable behaviour.
    
    This patch sets the no-execute and read-only bits for PAGE_NONE
    such that the code above will return false for both x and y.
    
    Signed-off-by: Steve Capper <steve.capper@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e333a243bfcc..77b09d6fee23 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -66,7 +66,7 @@ extern pgprot_t pgprot_default;
 
 #define _MOD_PROT(p, b)		__pgprot_modify(p, 0, b)
 
-#define PAGE_NONE		__pgprot_modify(pgprot_default, PTE_TYPE_MASK, PTE_PROT_NONE)
+#define PAGE_NONE		__pgprot_modify(pgprot_default, PTE_TYPE_MASK, PTE_PROT_NONE | PTE_RDONLY | PTE_PXN | PTE_UXN)
 #define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
 #define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
@@ -76,7 +76,7 @@ extern pgprot_t pgprot_default;
 #define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY)
 #define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY)
 
-#define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE)
+#define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE | PTE_RDONLY | PTE_PXN | PTE_UXN)
 #define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
 #define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)

commit 63917f0b5ba2a932d4fca7f67d1a1eae9034269e
Merge: d822d2a1e331 aa4a73a0a23a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jun 12 16:48:38 2013 +0100

    Merge branch 'kvm-arm64/kvm-for-3.11' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms into upstream
    
    * 'kvm-arm64/kvm-for-3.11' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms: (33 commits)
      arm64: KVM: document kernel object mappings in HYP
      arm64: KVM: MAINTAINERS update
      arm64: KVM: userspace API documentation
      arm64: KVM: enable initialization of a 32bit vcpu
      arm64: KVM: 32bit guest fault injection
      arm64: KVM: 32bit specific register world switch
      arm64: KVM: CPU specific 32bit coprocessor access
      arm64: KVM: 32bit handling of coprocessor traps
      arm64: KVM: 32bit conditional execution emulation
      arm64: KVM: 32bit GP register access
      arm64: KVM: define 32bit specific registers
      arm64: KVM: Build system integration
      arm64: KVM: PSCI implementation
      arm64: KVM: Plug the arch timer
      ARM: KVM: timer: allow DT matching for ARMv8 cores
      arm64: KVM: Plug the VGIC
      arm64: KVM: Exit handling
      arm64: KVM: HYP mode world switch implementation
      arm64: KVM: hypervisor initialization code
      arm64: KVM: guest one-reg interface
      ...
    
    Conflicts:
            arch/arm64/Makefile

commit 9ab6d02fddc6831b166812956ff387d7112ff626
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 10 19:34:41 2013 +0100

    arm64: pgtable: use pte_index instead of __pte_index
    
    pte_index is a useful helper outside of arch/arm64, for things like the
    ARM SMMU driver, so rename __pte_index to pte_index to be consistent
    with both arch/arm/ and also the definitions of pmd_index and pgd_index.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e333a243bfcc..b93bc2326f56 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -119,7 +119,7 @@ extern struct page *empty_zero_page;
 #define pte_none(pte)		(!pte_val(pte))
 #define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
 #define pte_page(pte)		(pfn_to_page(pte_pfn(pte)))
-#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + __pte_index(addr))
+#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
 
 #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
 #define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
@@ -263,7 +263,7 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 #endif
 
 /* Find an entry in the third-level page table.. */
-#define __pte_index(addr)	(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
 
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {

commit 363116073a26dbc2903d8417047597eebcc05273
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Dec 7 18:35:41 2012 +0000

    arm64: KVM: define HYP and Stage-2 translation page flags
    
    Add HYP and S2 page flags, for both normal and device memory.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index e333a243bfcc..fc2915a35c52 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -76,6 +76,12 @@ extern pgprot_t pgprot_default;
 #define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY)
 #define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY)
 
+#define PAGE_HYP		_MOD_PROT(pgprot_default, PTE_HYP)
+#define PAGE_HYP_DEVICE		__pgprot(PROT_DEVICE_nGnRE | PTE_HYP)
+
+#define PAGE_S2			__pgprot_modify(pgprot_default, PTE_S2_MEMATTR_MASK, PTE_S2_MEMATTR(MT_S2_NORMAL) | PTE_S2_RDONLY)
+#define PAGE_S2_DEVICE		__pgprot(PROT_DEFAULT | PTE_S2_MEMATTR(MT_S2_DEVICE_nGnRE) | PTE_S2_RDWR | PTE_UXN)
+
 #define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE)
 #define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
@@ -197,6 +203,12 @@ extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 
 #define pmd_bad(pmd)		(!(pmd_val(pmd) & 2))
 
+#define pmd_table(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
+				 PMD_TYPE_TABLE)
+#define pmd_sect(pmd)		((pmd_val(pmd) & PMD_TYPE_MASK) == \
+				 PMD_TYPE_SECT)
+
+
 static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
 {
 	*pmdp = pmd;

commit a6fadf7e67d3794aae40244f435d281a62736c93
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 18 14:15:15 2012 +0000

    arm64: mm: introduce present, faulting entries for PAGE_NONE
    
    This is mostly a port of dbf62d50067e ("ARM: mm: introduce L_PTE_VALID
    for page table entries") and 26ffd0d43b18 ("ARM: mm: introduce present,
    faulting entries for PAGE_NONE") from ARM, which makes use of present,
    faulting page table entries for page table entries mapped as PROT_NONE.
    
    The main difference with this implementation is that we can make use of
    the two pte type bits in order to avoid allocating a software bit for
    identifying PROT_NONE pages, instead reserving the 10b suffix for these
    types of mappings.
    
    This is required to prevent users from accessing such pages via syscalls
    such as read/write over a pipe.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 7adf4142a85c..e333a243bfcc 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -24,7 +24,8 @@
 /*
  * Software defined PTE bits definition.
  */
-#define PTE_VALID		(_AT(pteval_t, 1) << 0)	/* pte_present() check */
+#define PTE_VALID		(_AT(pteval_t, 1) << 0)
+#define PTE_PROT_NONE		(_AT(pteval_t, 1) << 1)	/* only when !PTE_VALID */
 #define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
 #define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
 #define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
@@ -60,9 +61,12 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
 
 extern pgprot_t pgprot_default;
 
-#define _MOD_PROT(p, b)	__pgprot(pgprot_val(p) | (b))
+#define __pgprot_modify(prot,mask,bits) \
+	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
+
+#define _MOD_PROT(p, b)		__pgprot_modify(p, 0, b)
 
-#define PAGE_NONE		_MOD_PROT(pgprot_default, PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define PAGE_NONE		__pgprot_modify(pgprot_default, PTE_TYPE_MASK, PTE_PROT_NONE)
 #define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
 #define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
@@ -72,7 +76,7 @@ extern pgprot_t pgprot_default;
 #define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY)
 #define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY)
 
-#define __PAGE_NONE		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define __PAGE_NONE		__pgprot(((_PAGE_DEFAULT) & ~PTE_TYPE_MASK) | PTE_PROT_NONE)
 #define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
 #define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
 #define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
@@ -125,14 +129,14 @@ extern struct page *empty_zero_page;
 /*
  * The following only work if pte_present(). Undefined behaviour otherwise.
  */
-#define pte_present(pte)	(pte_val(pte) & PTE_VALID)
+#define pte_present(pte)	(pte_val(pte) & (PTE_VALID | PTE_PROT_NONE))
 #define pte_dirty(pte)		(pte_val(pte) & PTE_DIRTY)
 #define pte_young(pte)		(pte_val(pte) & PTE_AF)
 #define pte_special(pte)	(pte_val(pte) & PTE_SPECIAL)
 #define pte_write(pte)		(!(pte_val(pte) & PTE_RDONLY))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
-#define pte_present_user(pte) \
+#define pte_valid_user(pte) \
 	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 
 #define PTE_BIT_FUNC(fn,op) \
@@ -156,7 +160,7 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_present_user(pte)) {
+	if (pte_valid_user(pte)) {
 		if (pte_exec(pte))
 			__sync_icache_dcache(pte, addr);
 		if (!pte_dirty(pte))
@@ -172,9 +176,6 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 #define pte_huge(pte)		((pte_val(pte) & PTE_TYPE_MASK) == PTE_TYPE_HUGEPAGE)
 #define pte_mkhuge(pte)		(__pte((pte_val(pte) & ~PTE_TYPE_MASK) | PTE_TYPE_HUGEPAGE))
 
-#define __pgprot_modify(prot,mask,bits)		\
-	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
-
 #define __HAVE_ARCH_PTE_SPECIAL
 
 /*
@@ -266,7 +267,8 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
-	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY;
+	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
+			      PTE_PROT_NONE | PTE_VALID;
 	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
 	return pte;
 }

commit 02522463c84748b3b8ad770f9424bcfa70a5b4c4
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 9 11:08:10 2013 +0000

    arm64: mm: only wrprotect clean ptes if they are present
    
    Marking non-present ptes as read-only can corrupt file ptes, breaking
    things like swap and file mappings.
    
    This patch ensures that we only manipulate user pte bits when the pte
    is marked present.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 64b133949502..7adf4142a85c 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -132,9 +132,8 @@ extern struct page *empty_zero_page;
 #define pte_write(pte)		(!(pte_val(pte) & PTE_RDONLY))
 #define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
-#define pte_present_exec_user(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_UXN)) == \
-	 (PTE_VALID | PTE_USER))
+#define pte_present_user(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))
 
 #define PTE_BIT_FUNC(fn,op) \
 static inline pte_t pte_##fn(pte_t pte) { pte_val(pte) op; return pte; }
@@ -157,10 +156,13 @@ extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
 static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 			      pte_t *ptep, pte_t pte)
 {
-	if (pte_present_exec_user(pte))
-		__sync_icache_dcache(pte, addr);
-	if (!pte_dirty(pte))
-		pte = pte_wrprotect(pte);
+	if (pte_present_user(pte)) {
+		if (pte_exec(pte))
+			__sync_icache_dcache(pte, addr);
+		if (!pte_dirty(pte))
+			pte = pte_wrprotect(pte);
+	}
+
 	set_pte(ptep, pte);
 }
 

commit 97ebe8f55ae99059c0ad3d3be5c0417647f5e3e0
Merge: d07e43d70eef d19766ec5221
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 07:49:02 2012 -0800

    Merge tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull ARM64 updates from Catalin Marinas:
    
     - Generic execve, kernel_thread, fork/vfork/clone.
    
     - Preparatory patches for KVM support (initialising EL2 mode for later
       installing KVM support, hypervisor stub).
    
     - Signal handling corner case fix (alternative signal stack set up for
       a SEGV handler, which is raised in response to RLIMIT_STACK being
       reached).
    
     - Sub-nanosecond timer error fix.
    
    * tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (30 commits)
      arm64: Update the MAINTAINERS entry
      arm64: compat for clock_adjtime(2) is miswired
      arm64: move FP-SIMD save/restore code to a macro
      arm64: hyp: initialize vttbr_el2 to zero
      arm64: add hypervisor stub
      arm64: record boot mode when entering the kernel
      arm64: move vector entry macro to assembler.h
      arm64: add AArch32 execution modes to ptrace.h
      arm64: expand register mapping between AArch32 and AArch64
      arm64: generic timer: use virtual counter instead of physical at EL0
      arm64: vdso: defer shifting of nanosecond component of timespec
      arm64: vdso: rework __do_get_tspec register allocation and return shift
      arm64: vdso: check sequence counter even for coarse realtime operations
      arm64: vdso: fix clocksource mask when extracting bottom 56 bits
      ARM64: Remove incorrect Kconfig symbol HAVE_SPARSE_IRQ
      Documentation: Fixes a word in Documentation/arm64/memory.txt
      arm64: Make !dirty ptes read-only
      arm64: Convert empty flush_cache_{mm,page} functions to static inline
      arm64: signal: let the compiler inline compat_get_sigframe
      arm64: signal: return struct rt_sigframe from get_sigframe
      ...
    
    Conflicts:
            arch/arm64/include/asm/unistd32.h

commit 33eaa58f854770dc9c98411a356c98e3a53edfda
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Nov 28 17:06:05 2012 +0000

    arm64: Make !dirty ptes read-only
    
    The AArch64 Linux port relies on the mm code to wrprotect clean ptes.
    This however is not the case with newly created ptes and
    PAGE_SHARED(_EXEC) is writable but !dirty.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 8960239be722..937ae2064682 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -159,6 +159,8 @@ static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 {
 	if (pte_present_exec_user(pte))
 		__sync_icache_dcache(pte, addr);
+	if (!pte_dirty(pte))
+		pte = pte_wrprotect(pte);
 	set_pte(ptep, pte);
 }
 

commit 8e620b0476696e9428442d3551f3dad47df0e28f
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 15 17:21:16 2012 +0000

    arm64: Distinguish between user and kernel XN bits
    
    On AArch64, the meaning of the XN bit has changed to UXN (user). The PXN
    (privileged) bit must be set to prevent kernel execution. Without the
    PXN bit set, the CPU may speculatively access device memory. This patch
    ensures that all the mappings that the kernel must not execute from
    (including user mappings) have the PXN bit set.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
index 8960239be722..14aba2db6776 100644
--- a/arch/arm64/include/asm/pgtable.h
+++ b/arch/arm64/include/asm/pgtable.h
@@ -62,23 +62,23 @@ extern pgprot_t pgprot_default;
 
 #define _MOD_PROT(p, b)	__pgprot(pgprot_val(p) | (b))
 
-#define PAGE_NONE		_MOD_PROT(pgprot_default, PTE_NG | PTE_XN | PTE_RDONLY)
-#define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_XN)
-#define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG)
-#define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
-#define PAGE_COPY_EXEC		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_RDONLY)
-#define PAGE_READONLY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
-#define PAGE_READONLY_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_RDONLY)
-#define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_XN | PTE_DIRTY)
-#define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_DIRTY)
-
-#define __PAGE_NONE		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_XN | PTE_RDONLY)
-#define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_XN)
-#define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG)
-#define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
-#define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_RDONLY)
-#define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
-#define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_RDONLY)
+#define PAGE_NONE		_MOD_PROT(pgprot_default, PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN)
+#define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define PAGE_COPY_EXEC		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
+#define PAGE_READONLY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define PAGE_READONLY_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
+#define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_PXN | PTE_UXN | PTE_DIRTY)
+#define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_UXN | PTE_DIRTY)
+
+#define __PAGE_NONE		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
+#define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
+#define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
+#define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN | PTE_RDONLY)
+#define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_RDONLY)
 
 #endif /* __ASSEMBLY__ */
 
@@ -130,10 +130,10 @@ extern struct page *empty_zero_page;
 #define pte_young(pte)		(pte_val(pte) & PTE_AF)
 #define pte_special(pte)	(pte_val(pte) & PTE_SPECIAL)
 #define pte_write(pte)		(!(pte_val(pte) & PTE_RDONLY))
-#define pte_exec(pte)		(!(pte_val(pte) & PTE_XN))
+#define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))
 
 #define pte_present_exec_user(pte) \
-	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_XN)) == \
+	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_UXN)) == \
 	 (PTE_VALID | PTE_USER))
 
 #define PTE_BIT_FUNC(fn,op) \
@@ -262,7 +262,7 @@ static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
 
 static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
 {
-	const pteval_t mask = PTE_USER | PTE_XN | PTE_RDONLY;
+	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY;
 	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
 	return pte;
 }

commit 4f04d8f00545110a0e525ae2fb62ab38cb417236
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: MMU definitions
    
    The virtual memory layout is described in
    Documentation/arm64/memory.txt. This patch adds the MMU definitions for
    the 4KB and 64KB translation table configurations. The SECTION_SIZE is
    2MB with 4KB page and 512MB with 64KB page configuration.
    
    PHYS_OFFSET is calculated at run-time and stored in a variable (no
    run-time code patching at this stage).
    
    On the current implementation, both user and kernel address spaces are
    512G (39-bit) each with a maximum of 256G for the RAM linear mapping.
    Linux uses 3 levels of translation tables with the 4K page configuration
    and 2 levels with the 64K configuration. Extending the memory space
    beyond 39-bit with the 4K pages or 42-bit with 64K pages requires an
    additional level of translation tables.
    
    The SPARSEMEM configuration is global to all AArch64 platforms and
    allows for 1GB sections with SPARSEMEM_VMEMMAP enabled by default.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
new file mode 100644
index 000000000000..8960239be722
--- /dev/null
+++ b/arch/arm64/include/asm/pgtable.h
@@ -0,0 +1,328 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_PGTABLE_H
+#define __ASM_PGTABLE_H
+
+#include <asm/proc-fns.h>
+
+#include <asm/memory.h>
+#include <asm/pgtable-hwdef.h>
+
+/*
+ * Software defined PTE bits definition.
+ */
+#define PTE_VALID		(_AT(pteval_t, 1) << 0)	/* pte_present() check */
+#define PTE_FILE		(_AT(pteval_t, 1) << 2)	/* only when !pte_present() */
+#define PTE_DIRTY		(_AT(pteval_t, 1) << 55)
+#define PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
+
+/*
+ * VMALLOC and SPARSEMEM_VMEMMAP ranges.
+ */
+#define VMALLOC_START		UL(0xffffff8000000000)
+#define VMALLOC_END		(PAGE_OFFSET - UL(0x400000000) - SZ_64K)
+
+#define vmemmap			((struct page *)(VMALLOC_END + SZ_64K))
+
+#define FIRST_USER_ADDRESS	0
+
+#ifndef __ASSEMBLY__
+extern void __pte_error(const char *file, int line, unsigned long val);
+extern void __pmd_error(const char *file, int line, unsigned long val);
+extern void __pgd_error(const char *file, int line, unsigned long val);
+
+#define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
+#ifndef CONFIG_ARM64_64K_PAGES
+#define pmd_ERROR(pmd)		__pmd_error(__FILE__, __LINE__, pmd_val(pmd))
+#endif
+#define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
+
+/*
+ * The pgprot_* and protection_map entries will be fixed up at runtime to
+ * include the cachable and bufferable bits based on memory policy, as well as
+ * any architecture dependent bits like global/ASID and SMP shared mapping
+ * bits.
+ */
+#define _PAGE_DEFAULT		PTE_TYPE_PAGE | PTE_AF
+
+extern pgprot_t pgprot_default;
+
+#define _MOD_PROT(p, b)	__pgprot(pgprot_val(p) | (b))
+
+#define PAGE_NONE		_MOD_PROT(pgprot_default, PTE_NG | PTE_XN | PTE_RDONLY)
+#define PAGE_SHARED		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_XN)
+#define PAGE_SHARED_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG)
+#define PAGE_COPY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
+#define PAGE_COPY_EXEC		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_RDONLY)
+#define PAGE_READONLY		_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
+#define PAGE_READONLY_EXEC	_MOD_PROT(pgprot_default, PTE_USER | PTE_NG | PTE_RDONLY)
+#define PAGE_KERNEL		_MOD_PROT(pgprot_default, PTE_XN | PTE_DIRTY)
+#define PAGE_KERNEL_EXEC	_MOD_PROT(pgprot_default, PTE_DIRTY)
+
+#define __PAGE_NONE		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_XN | PTE_RDONLY)
+#define __PAGE_SHARED		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_XN)
+#define __PAGE_SHARED_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG)
+#define __PAGE_COPY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
+#define __PAGE_COPY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_RDONLY)
+#define __PAGE_READONLY		__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_XN | PTE_RDONLY)
+#define __PAGE_READONLY_EXEC	__pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_RDONLY)
+
+#endif /* __ASSEMBLY__ */
+
+#define __P000  __PAGE_NONE
+#define __P001  __PAGE_READONLY
+#define __P010  __PAGE_COPY
+#define __P011  __PAGE_COPY
+#define __P100  __PAGE_READONLY_EXEC
+#define __P101  __PAGE_READONLY_EXEC
+#define __P110  __PAGE_COPY_EXEC
+#define __P111  __PAGE_COPY_EXEC
+
+#define __S000  __PAGE_NONE
+#define __S001  __PAGE_READONLY
+#define __S010  __PAGE_SHARED
+#define __S011  __PAGE_SHARED
+#define __S100  __PAGE_READONLY_EXEC
+#define __S101  __PAGE_READONLY_EXEC
+#define __S110  __PAGE_SHARED_EXEC
+#define __S111  __PAGE_SHARED_EXEC
+
+#ifndef __ASSEMBLY__
+/*
+ * ZERO_PAGE is a global shared page that is always zero: used
+ * for zero-mapped memory areas etc..
+ */
+extern struct page *empty_zero_page;
+#define ZERO_PAGE(vaddr)	(empty_zero_page)
+
+#define pte_pfn(pte)		((pte_val(pte) & PHYS_MASK) >> PAGE_SHIFT)
+
+#define pfn_pte(pfn,prot)	(__pte(((phys_addr_t)(pfn) << PAGE_SHIFT) | pgprot_val(prot)))
+
+#define pte_none(pte)		(!pte_val(pte))
+#define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
+#define pte_page(pte)		(pfn_to_page(pte_pfn(pte)))
+#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + __pte_index(addr))
+
+#define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
+#define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
+#define pte_unmap(pte)			do { } while (0)
+#define pte_unmap_nested(pte)		do { } while (0)
+
+/*
+ * The following only work if pte_present(). Undefined behaviour otherwise.
+ */
+#define pte_present(pte)	(pte_val(pte) & PTE_VALID)
+#define pte_dirty(pte)		(pte_val(pte) & PTE_DIRTY)
+#define pte_young(pte)		(pte_val(pte) & PTE_AF)
+#define pte_special(pte)	(pte_val(pte) & PTE_SPECIAL)
+#define pte_write(pte)		(!(pte_val(pte) & PTE_RDONLY))
+#define pte_exec(pte)		(!(pte_val(pte) & PTE_XN))
+
+#define pte_present_exec_user(pte) \
+	((pte_val(pte) & (PTE_VALID | PTE_USER | PTE_XN)) == \
+	 (PTE_VALID | PTE_USER))
+
+#define PTE_BIT_FUNC(fn,op) \
+static inline pte_t pte_##fn(pte_t pte) { pte_val(pte) op; return pte; }
+
+PTE_BIT_FUNC(wrprotect, |= PTE_RDONLY);
+PTE_BIT_FUNC(mkwrite,   &= ~PTE_RDONLY);
+PTE_BIT_FUNC(mkclean,   &= ~PTE_DIRTY);
+PTE_BIT_FUNC(mkdirty,   |= PTE_DIRTY);
+PTE_BIT_FUNC(mkold,     &= ~PTE_AF);
+PTE_BIT_FUNC(mkyoung,   |= PTE_AF);
+PTE_BIT_FUNC(mkspecial, |= PTE_SPECIAL);
+
+static inline void set_pte(pte_t *ptep, pte_t pte)
+{
+	*ptep = pte;
+}
+
+extern void __sync_icache_dcache(pte_t pteval, unsigned long addr);
+
+static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep, pte_t pte)
+{
+	if (pte_present_exec_user(pte))
+		__sync_icache_dcache(pte, addr);
+	set_pte(ptep, pte);
+}
+
+/*
+ * Huge pte definitions.
+ */
+#define pte_huge(pte)		((pte_val(pte) & PTE_TYPE_MASK) == PTE_TYPE_HUGEPAGE)
+#define pte_mkhuge(pte)		(__pte((pte_val(pte) & ~PTE_TYPE_MASK) | PTE_TYPE_HUGEPAGE))
+
+#define __pgprot_modify(prot,mask,bits)		\
+	__pgprot((pgprot_val(prot) & ~(mask)) | (bits))
+
+#define __HAVE_ARCH_PTE_SPECIAL
+
+/*
+ * Mark the prot value as uncacheable and unbufferable.
+ */
+#define pgprot_noncached(prot) \
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_nGnRnE))
+#define pgprot_writecombine(prot) \
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_DEVICE_GRE))
+#define pgprot_dmacoherent(prot) \
+	__pgprot_modify(prot, PTE_ATTRINDX_MASK, PTE_ATTRINDX(MT_NORMAL_NC))
+#define __HAVE_PHYS_MEM_ACCESS_PROT
+struct file;
+extern pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
+				     unsigned long size, pgprot_t vma_prot);
+
+#define pmd_none(pmd)		(!pmd_val(pmd))
+#define pmd_present(pmd)	(pmd_val(pmd))
+
+#define pmd_bad(pmd)		(!(pmd_val(pmd) & 2))
+
+static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)
+{
+	*pmdp = pmd;
+	dsb();
+}
+
+static inline void pmd_clear(pmd_t *pmdp)
+{
+	set_pmd(pmdp, __pmd(0));
+}
+
+static inline pte_t *pmd_page_vaddr(pmd_t pmd)
+{
+	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
+}
+
+#define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+
+/*
+ * Conversion functions: convert a page and protection to a page entry,
+ * and a page entry and page directory to the page they refer to.
+ */
+#define mk_pte(page,prot)	pfn_pte(page_to_pfn(page),prot)
+
+#ifndef CONFIG_ARM64_64K_PAGES
+
+#define pud_none(pud)		(!pud_val(pud))
+#define pud_bad(pud)		(!(pud_val(pud) & 2))
+#define pud_present(pud)	(pud_val(pud))
+
+static inline void set_pud(pud_t *pudp, pud_t pud)
+{
+	*pudp = pud;
+	dsb();
+}
+
+static inline void pud_clear(pud_t *pudp)
+{
+	set_pud(pudp, __pud(0));
+}
+
+static inline pmd_t *pud_page_vaddr(pud_t pud)
+{
+	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
+}
+
+#endif	/* CONFIG_ARM64_64K_PAGES */
+
+/* to find an entry in a page-table-directory */
+#define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
+
+#define pgd_offset(mm, addr)	((mm)->pgd+pgd_index(addr))
+
+/* to find an entry in a kernel page-table-directory */
+#define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
+
+/* Find an entry in the second-level page table.. */
+#ifndef CONFIG_ARM64_64K_PAGES
+#define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
+static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
+{
+	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
+}
+#endif
+
+/* Find an entry in the third-level page table.. */
+#define __pte_index(addr)	(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+
+static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+{
+	const pteval_t mask = PTE_USER | PTE_XN | PTE_RDONLY;
+	pte_val(pte) = (pte_val(pte) & ~mask) | (pgprot_val(newprot) & mask);
+	return pte;
+}
+
+extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
+extern pgd_t idmap_pg_dir[PTRS_PER_PGD];
+
+#define SWAPPER_DIR_SIZE	(3 * PAGE_SIZE)
+#define IDMAP_DIR_SIZE		(2 * PAGE_SIZE)
+
+/*
+ * Encode and decode a swap entry:
+ *	bits 0-1:	present (must be zero)
+ *	bit  2:		PTE_FILE
+ *	bits 3-8:	swap type
+ *	bits 9-63:	swap offset
+ */
+#define __SWP_TYPE_SHIFT	3
+#define __SWP_TYPE_BITS		6
+#define __SWP_TYPE_MASK		((1 << __SWP_TYPE_BITS) - 1)
+#define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)
+
+#define __swp_type(x)		(((x).val >> __SWP_TYPE_SHIFT) & __SWP_TYPE_MASK)
+#define __swp_offset(x)		((x).val >> __SWP_OFFSET_SHIFT)
+#define __swp_entry(type,offset) ((swp_entry_t) { ((type) << __SWP_TYPE_SHIFT) | ((offset) << __SWP_OFFSET_SHIFT) })
+
+#define __pte_to_swp_entry(pte)	((swp_entry_t) { pte_val(pte) })
+#define __swp_entry_to_pte(swp)	((pte_t) { (swp).val })
+
+/*
+ * Ensure that there are not more swap files than can be encoded in the kernel
+ * the PTEs.
+ */
+#define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT > __SWP_TYPE_BITS)
+
+/*
+ * Encode and decode a file entry:
+ *	bits 0-1:	present (must be zero)
+ *	bit  2:		PTE_FILE
+ *	bits 3-63:	file offset / PAGE_SIZE
+ */
+#define pte_file(pte)		(pte_val(pte) & PTE_FILE)
+#define pte_to_pgoff(x)		(pte_val(x) >> 3)
+#define pgoff_to_pte(x)		__pte(((x) << 3) | PTE_FILE)
+
+#define PTE_FILE_MAX_BITS	61
+
+extern int kern_addr_valid(unsigned long addr);
+
+#include <asm-generic/pgtable.h>
+
+/*
+ * remap a physical page `pfn' of size `size' with page protection `prot'
+ * into virtual address `from'
+ */
+#define io_remap_pfn_range(vma,from,pfn,size,prot) \
+		remap_pfn_range(vma, from, pfn, size, prot)
+
+#define pgtable_cache_init() do { } while (0)
+
+#endif /* !__ASSEMBLY__ */
+
+#endif /* __ASM_PGTABLE_H */
