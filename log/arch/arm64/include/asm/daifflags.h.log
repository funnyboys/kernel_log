commit e533dbe9dcb199bb637a2c465f3a6e70564994fe
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jan 22 12:45:46 2020 +0000

    arm64: acpi: fix DAIF manipulation with pNMI
    
    Since commit:
    
      d44f1b8dd7e66d80 ("arm64: KVM/mm: Move SEA handling behind a single 'claim' interface")
    
    ... the top-level APEI SEA handler has the shape:
    
    1. current_flags = arch_local_save_flags()
    2. local_daif_restore(DAIF_ERRCTX)
    3. <GHES handler>
    4. local_daif_restore(current_flags)
    
    However, since commit:
    
      4a503217ce37e1f4 ("arm64: irqflags: Use ICC_PMR_EL1 for interrupt masking")
    
    ... when pseudo-NMIs (pNMIs) are in use, arch_local_save_flags() will save
    the PMR value rather than the DAIF flags.
    
    The combination of these two commits means that the APEI SEA handler will
    erroneously attempt to restore the PMR value into DAIF. Fix this by
    factoring local_daif_save_flags() out of local_daif_save(), so that we
    can consistently save DAIF in step #1, regardless of whether pNMIs are in
    use.
    
    Both commits were introduced concurrently in v5.0.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 4a503217ce37e1f4 ("arm64: irqflags: Use ICC_PMR_EL1 for interrupt masking")
    Fixes: d44f1b8dd7e66d80 ("arm64: KVM/mm: Move SEA handling behind a single 'claim' interface")
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 72acd2db167f..ec213b4a1650 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -38,7 +38,7 @@ static inline void local_daif_mask(void)
 	trace_hardirqs_off();
 }
 
-static inline unsigned long local_daif_save(void)
+static inline unsigned long local_daif_save_flags(void)
 {
 	unsigned long flags;
 
@@ -50,6 +50,15 @@ static inline unsigned long local_daif_save(void)
 			flags |= PSR_I_BIT;
 	}
 
+	return flags;
+}
+
+static inline unsigned long local_daif_save(void)
+{
+	unsigned long flags;
+
+	flags = local_daif_save_flags();
+
 	local_daif_mask();
 
 	return flags;

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit 51077e03b8cef2a24d6582b8c54b718fced6878c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Oct 25 17:42:12 2019 +0100

    arm64: add local_daif_inherit()
    
    Some synchronous exceptions can be taken from a number of contexts,
    e.g. where IRQs may or may not be masked. In the entry assembly for
    these exceptions, we use the inherit_daif assembly macro to ensure
    that we only mask those exceptions which were masked when the exception
    was taken.
    
    So that we can do the same from C code, this patch adds a new
    local_daif_inherit() function, following the existing local_daif_*()
    naming scheme.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    [moved away from local_daif_restore()]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 063c964af705..9207cd5aa39e 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -9,6 +9,7 @@
 
 #include <asm/arch_gicv3.h>
 #include <asm/cpufeature.h>
+#include <asm/ptrace.h>
 
 #define DAIF_PROCCTX		0
 #define DAIF_PROCCTX_NOIRQ	PSR_I_BIT
@@ -109,4 +110,19 @@ static inline void local_daif_restore(unsigned long flags)
 		trace_hardirqs_off();
 }
 
+/*
+ * Called by synchronous exception handlers to restore the DAIF bits that were
+ * modified by taking an exception.
+ */
+static inline void local_daif_inherit(struct pt_regs *regs)
+{
+	unsigned long flags = regs->pstate & DAIF_MASK;
+
+	/*
+	 * We can't use local_daif_restore(regs->pstate) here as
+	 * system_has_prio_mask_debugging() won't restore the I bit if it can
+	 * use the pmr instead.
+	 */
+	write_sysreg(flags, daif);
+}
 #endif

commit f226650494c6aa87526d12135b7de8b8c074f3de
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Oct 2 10:06:12 2019 +0100

    arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    The GICv3 architecture specification is incredibly misleading when it
    comes to PMR and the requirement for a DSB. It turns out that this DSB
    is only required if the CPU interface sends an Upstream Control
    message to the redistributor in order to update the RD's view of PMR.
    
    This message is only sent when ICC_CTLR_EL1.PMHE is set, which isn't
    the case in Linux. It can still be set from EL3, so some special care
    is required. But the upshot is that in the (hopefuly large) majority
    of the cases, we can drop the DSB altogether.
    
    This relies on a new static key being set if the boot CPU has PMHE
    set. The drawback is that this static key has to be exported to
    modules.
    
    Cc: Will Deacon <will@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 063c964af705..53cd5fab79a8 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -8,6 +8,7 @@
 #include <linux/irqflags.h>
 
 #include <asm/arch_gicv3.h>
+#include <asm/barrier.h>
 #include <asm/cpufeature.h>
 
 #define DAIF_PROCCTX		0
@@ -65,7 +66,7 @@ static inline void local_daif_restore(unsigned long flags)
 
 		if (system_uses_irq_prio_masking()) {
 			gic_write_pmr(GIC_PRIO_IRQON);
-			dsb(sy);
+			pmr_sync();
 		}
 	} else if (system_uses_irq_prio_masking()) {
 		u64 pmr;

commit b3980e48528c4d2a9e70b145a5bba328b73a0f93
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Thu Aug 1 23:25:49 2019 +0900

    arm64: kprobes: Recover pstate.D in single-step exception handler
    
    kprobes manipulates the interrupted PSTATE for single step, and
    doesn't restore it. Thus, if we put a kprobe where the pstate.D
    (debug) masked, the mask will be cleared after the kprobe hits.
    
    Moreover, in the most complicated case, this can lead a kernel
    crash with below message when a nested kprobe hits.
    
    [  152.118921] Unexpected kernel single-step exception at EL1
    
    When the 1st kprobe hits, do_debug_exception() will be called.
    At this point, debug exception (= pstate.D) must be masked (=1).
    But if another kprobes hits before single-step of the first kprobe
    (e.g. inside user pre_handler), it unmask the debug exception
    (pstate.D = 0) and return.
    Then, when the 1st kprobe setting up single-step, it saves current
    DAIF, mask DAIF, enable single-step, and restore DAIF.
    However, since "D" flag in DAIF is cleared by the 2nd kprobe, the
    single-step exception happens soon after restoring DAIF.
    
    This has been introduced by commit 7419333fa15e ("arm64: kprobe:
    Always clear pstate.D in breakpoint exception handler")
    
    To solve this issue, this stores all DAIF bits and restore it
    after single stepping.
    
    Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
    Fixes: 7419333fa15e ("arm64: kprobe: Always clear pstate.D in breakpoint exception handler")
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 987926ed535e..063c964af705 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -13,6 +13,8 @@
 #define DAIF_PROCCTX		0
 #define DAIF_PROCCTX_NOIRQ	PSR_I_BIT
 #define DAIF_ERRCTX		(PSR_I_BIT | PSR_A_BIT)
+#define DAIF_MASK		(PSR_D_BIT | PSR_A_BIT | PSR_I_BIT | PSR_F_BIT)
+
 
 /* mask/save/unmask/restore all exceptions, including interrupts. */
 static inline void local_daif_mask(void)

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit 48ce8f80f5901f1f031b00be66d659d39f33b0a1
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:11 2019 +0100

    arm64: irqflags: Introduce explicit debugging for IRQ priorities
    
    Using IRQ priority masking to enable/disable interrupts is a bit
    sensitive as it requires to deal with both ICC_PMR_EL1 and PSR.I.
    
    Introduce some validity checks to both highlight the states in which
    functions dealing with IRQ enabling/disabling can (not) be called, and
    bark a warning when called in an unexpected state.
    
    Since these checks are done on hotpaths, introduce a build option to
    choose whether to do the checking.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index f93204f319da..eca5bee1d85b 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -28,6 +28,10 @@
 /* mask/save/unmask/restore all exceptions, including interrupts. */
 static inline void local_daif_mask(void)
 {
+	WARN_ON(system_has_prio_mask_debugging() &&
+		(read_sysreg_s(SYS_ICC_PMR_EL1) == (GIC_PRIO_IRQOFF |
+						    GIC_PRIO_PSR_I_SET)));
+
 	asm volatile(
 		"msr	daifset, #0xf		// local_daif_mask\n"
 		:
@@ -62,6 +66,9 @@ static inline void local_daif_restore(unsigned long flags)
 {
 	bool irq_disabled = flags & PSR_I_BIT;
 
+	WARN_ON(system_has_prio_mask_debugging() &&
+		!(read_sysreg(daif) & PSR_I_BIT));
+
 	if (!irq_disabled) {
 		trace_hardirqs_on();
 

commit bd82d4bd21880b7c4d5f5756be435095d6ae07b5
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:10 2019 +0100

    arm64: Fix incorrect irqflag restore for priority masking
    
    When using IRQ priority masking to disable interrupts, in order to deal
    with the PSR.I state, local_irq_save() would convert the I bit into a
    PMR value (GIC_PRIO_IRQOFF). This resulted in local_irq_restore()
    potentially modifying the value of PMR in undesired location due to the
    state of PSR.I upon flag saving [1].
    
    In an attempt to solve this issue in a less hackish manner, introduce
    a bit (GIC_PRIO_IGNORE_PMR) for the PMR values that can represent
    whether PSR.I is being used to disable interrupts, in which case it
    takes precedence of the status of interrupt masking via PMR.
    
    GIC_PRIO_PSR_I_SET is chosen such that (<pmr_value> |
    GIC_PRIO_PSR_I_SET) does not mask more interrupts than <pmr_value> as
    some sections (e.g. arch_cpu_idle(), interrupt acknowledge path)
    requires PMR not to mask interrupts that could be signaled to the
    CPU when using only PSR.I.
    
    [1] https://www.spinics.net/lists/arm-kernel/msg716956.html
    
    Fixes: 4a503217ce37 ("arm64: irqflags: Use ICC_PMR_EL1 for interrupt masking")
    Cc: <stable@vger.kernel.org> # 5.1.x-
    Reported-by: Zenghui Yu <yuzenghui@huawei.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Wei Li <liwei391@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Pouloze <suzuki.poulose@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index db452aa9e651..f93204f319da 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -18,6 +18,7 @@
 
 #include <linux/irqflags.h>
 
+#include <asm/arch_gicv3.h>
 #include <asm/cpufeature.h>
 
 #define DAIF_PROCCTX		0
@@ -32,6 +33,11 @@ static inline void local_daif_mask(void)
 		:
 		:
 		: "memory");
+
+	/* Don't really care for a dsb here, we don't intend to enable IRQs */
+	if (system_uses_irq_prio_masking())
+		gic_write_pmr(GIC_PRIO_IRQON | GIC_PRIO_PSR_I_SET);
+
 	trace_hardirqs_off();
 }
 
@@ -43,7 +49,7 @@ static inline unsigned long local_daif_save(void)
 
 	if (system_uses_irq_prio_masking()) {
 		/* If IRQs are masked with PMR, reflect it in the flags */
-		if (read_sysreg_s(SYS_ICC_PMR_EL1) <= GIC_PRIO_IRQOFF)
+		if (read_sysreg_s(SYS_ICC_PMR_EL1) != GIC_PRIO_IRQON)
 			flags |= PSR_I_BIT;
 	}
 
@@ -59,36 +65,44 @@ static inline void local_daif_restore(unsigned long flags)
 	if (!irq_disabled) {
 		trace_hardirqs_on();
 
-		if (system_uses_irq_prio_masking())
-			arch_local_irq_enable();
-	} else if (!(flags & PSR_A_BIT)) {
-		/*
-		 * If interrupts are disabled but we can take
-		 * asynchronous errors, we can take NMIs
-		 */
 		if (system_uses_irq_prio_masking()) {
-			flags &= ~PSR_I_BIT;
+			gic_write_pmr(GIC_PRIO_IRQON);
+			dsb(sy);
+		}
+	} else if (system_uses_irq_prio_masking()) {
+		u64 pmr;
+
+		if (!(flags & PSR_A_BIT)) {
 			/*
-			 * There has been concern that the write to daif
-			 * might be reordered before this write to PMR.
-			 * From the ARM ARM DDI 0487D.a, section D1.7.1
-			 * "Accessing PSTATE fields":
-			 *   Writes to the PSTATE fields have side-effects on
-			 *   various aspects of the PE operation. All of these
-			 *   side-effects are guaranteed:
-			 *     - Not to be visible to earlier instructions in
-			 *       the execution stream.
-			 *     - To be visible to later instructions in the
-			 *       execution stream
-			 *
-			 * Also, writes to PMR are self-synchronizing, so no
-			 * interrupts with a lower priority than PMR is signaled
-			 * to the PE after the write.
-			 *
-			 * So we don't need additional synchronization here.
+			 * If interrupts are disabled but we can take
+			 * asynchronous errors, we can take NMIs
 			 */
-			arch_local_irq_disable();
+			flags &= ~PSR_I_BIT;
+			pmr = GIC_PRIO_IRQOFF;
+		} else {
+			pmr = GIC_PRIO_IRQON | GIC_PRIO_PSR_I_SET;
 		}
+
+		/*
+		 * There has been concern that the write to daif
+		 * might be reordered before this write to PMR.
+		 * From the ARM ARM DDI 0487D.a, section D1.7.1
+		 * "Accessing PSTATE fields":
+		 *   Writes to the PSTATE fields have side-effects on
+		 *   various aspects of the PE operation. All of these
+		 *   side-effects are guaranteed:
+		 *     - Not to be visible to earlier instructions in
+		 *       the execution stream.
+		 *     - To be visible to later instructions in the
+		 *       execution stream
+		 *
+		 * Also, writes to PMR are self-synchronizing, so no
+		 * interrupts with a lower priority than PMR is signaled
+		 * to the PE after the write.
+		 *
+		 * So we don't need additional synchronization here.
+		 */
+		gic_write_pmr(pmr);
 	}
 
 	write_sysreg(flags, daif);

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index db452aa9e651..6dd8a8723525 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2017 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_DAIFFLAGS_H
 #define __ASM_DAIFFLAGS_H

commit 3d8dfe75ef69f4dd4ba35c09b20a5aa58b4a5078
Merge: d60752629693 b855b58ac1b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 10:17:23 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Pseudo NMI support for arm64 using GICv3 interrupt priorities
    
     - uaccess macros clean-up (unsafe user accessors also merged but
       reverted, waiting for objtool support on arm64)
    
     - ptrace regsets for Pointer Authentication (ARMv8.3) key management
    
     - inX() ordering w.r.t. delay() on arm64 and riscv (acks in place by
       the riscv maintainers)
    
     - arm64/perf updates: PMU bindings converted to json-schema, unused
       variable and misleading comment removed
    
     - arm64/debug fixes to ensure checking of the triggering exception
       level and to avoid the propagation of the UNKNOWN FAR value into the
       si_code for debug signals
    
     - Workaround for Fujitsu A64FX erratum 010001
    
     - lib/raid6 ARM NEON optimisations
    
     - NR_CPUS now defaults to 256 on arm64
    
     - Minor clean-ups (documentation/comments, Kconfig warning, unused
       asm-offsets, clang warnings)
    
     - MAINTAINERS update for list information to the ARM64 ACPI entry
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      arm64: mmu: drop paging_init comments
      arm64: debug: Ensure debug handlers check triggering exception level
      arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
      Revert "arm64: uaccess: Implement unsafe accessors"
      arm64: avoid clang warning about self-assignment
      arm64: Kconfig.platforms: fix warning unmet direct dependencies
      lib/raid6: arm: optimize away a mask operation in NEON recovery routine
      lib/raid6: use vdupq_n_u8 to avoid endianness warnings
      arm64: io: Hook up __io_par() for inX() ordering
      riscv: io: Update __io_[p]ar() macros to take an argument
      asm-generic/io: Pass result of I/O accessor to __io_[p]ar()
      arm64: Add workaround for Fujitsu A64FX erratum 010001
      arm64: Rename get_thread_info()
      arm64: Remove documentation about TIF_USEDFPU
      arm64: irqflags: Fix clang build warnings
      arm64: Enable the support of pseudo-NMIs
      arm64: Skip irqflags tracing for NMI in IRQs disabled context
      arm64: Skip preemption when exiting an NMI
      arm64: Handle serror in NMI context
      irqchip/gic-v3: Allow interrupts to be set as pseudo-NMI
      ...

commit d44f1b8dd7e66d80cc4205809e5ace866bd851da
Author: James Morse <james.morse@arm.com>
Date:   Tue Jan 29 18:48:50 2019 +0000

    arm64: KVM/mm: Move SEA handling behind a single 'claim' interface
    
    To split up APEIs in_nmi() path, the caller needs to always be
    in_nmi(). Add a helper to do the work and claim the notification.
    
    When KVM or the arch code takes an exception that might be a RAS
    notification, it asks the APEI firmware-first code whether it wants
    to claim the exception. A future kernel-first mechanism may be queried
    afterwards, and claim the notification, otherwise we fall through
    to the existing default behaviour.
    
    The NOTIFY_SEA code was merged before considering multiple, possibly
    interacting, NMI-like notifications and the need to consider kernel
    first in the future. Make the 'claiming' behaviour explicit.
    
    Restructuring the APEI code to allow multiple NMI-like notifications
    means any notification that might interrupt interrupts-masked
    code must always be wrapped in nmi_enter()/nmi_exit(). This will
    allow APEI to use in_nmi() to use the right fixmap entries.
    
    Mask SError over this window to prevent an asynchronous RAS error
    arriving and tripping 'nmi_enter()'s BUG_ON(in_nmi()).
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Tyler Baicar <tbaicar@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 8d91f2233135..fa90779fc752 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -20,6 +20,7 @@
 
 #define DAIF_PROCCTX		0
 #define DAIF_PROCCTX_NOIRQ	PSR_I_BIT
+#define DAIF_ERRCTX		(PSR_I_BIT | PSR_A_BIT)
 
 /* mask/save/unmask/restore all exceptions, including interrupts. */
 static inline void local_daif_mask(void)

commit 8cb7eff32cc00697d4a37b1ed569c72ee2039ca4
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:51 2019 +0000

    arm64: daifflags: Include PMR in daifflags restore operations
    
    The addition of PMR should not bypass the semantics of daifflags.
    
    When DA_F are set, I bit is also set as no interrupts (even of higher
    priority) is allowed.
    
    When DA_F are cleared, I bit is cleared and interrupt enabling/disabling
    goes through ICC_PMR_EL1.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 546bc398553e..1dd3d7a38d34 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -18,6 +18,8 @@
 
 #include <linux/irqflags.h>
 
+#include <asm/cpufeature.h>
+
 #define DAIF_PROCCTX		0
 #define DAIF_PROCCTX_NOIRQ	PSR_I_BIT
 
@@ -36,7 +38,13 @@ static inline unsigned long local_daif_save(void)
 {
 	unsigned long flags;
 
-	flags = arch_local_save_flags();
+	flags = read_sysreg(daif);
+
+	if (system_uses_irq_prio_masking()) {
+		/* If IRQs are masked with PMR, reflect it in the flags */
+		if (read_sysreg_s(SYS_ICC_PMR_EL1) <= GIC_PRIO_IRQOFF)
+			flags |= PSR_I_BIT;
+	}
 
 	local_daif_mask();
 
@@ -45,12 +53,46 @@ static inline unsigned long local_daif_save(void)
 
 static inline void local_daif_restore(unsigned long flags)
 {
-	if (!arch_irqs_disabled_flags(flags))
+	bool irq_disabled = flags & PSR_I_BIT;
+
+	if (!irq_disabled) {
 		trace_hardirqs_on();
 
-	arch_local_irq_restore(flags);
+		if (system_uses_irq_prio_masking())
+			arch_local_irq_enable();
+	} else if (!(flags & PSR_A_BIT)) {
+		/*
+		 * If interrupts are disabled but we can take
+		 * asynchronous errors, we can take NMIs
+		 */
+		if (system_uses_irq_prio_masking()) {
+			flags &= ~PSR_I_BIT;
+			/*
+			 * There has been concern that the write to daif
+			 * might be reordered before this write to PMR.
+			 * From the ARM ARM DDI 0487D.a, section D1.7.1
+			 * "Accessing PSTATE fields":
+			 *   Writes to the PSTATE fields have side-effects on
+			 *   various aspects of the PE operation. All of these
+			 *   side-effects are guaranteed:
+			 *     - Not to be visible to earlier instructions in
+			 *       the execution stream.
+			 *     - To be visible to later instructions in the
+			 *       execution stream
+			 *
+			 * Also, writes to PMR are self-synchronizing, so no
+			 * interrupts with a lower priority than PMR is signaled
+			 * to the PE after the write.
+			 *
+			 * So we don't need additional synchronization here.
+			 */
+			arch_local_irq_disable();
+		}
+	}
+
+	write_sysreg(flags, daif);
 
-	if (arch_irqs_disabled_flags(flags))
+	if (irq_disabled)
 		trace_hardirqs_off();
 }
 

commit a82785a953e03444fe38616aed4d27b01da79a97
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:40 2019 +0000

    arm64: Remove unused daif related functions/macros
    
    There are some helpers to modify PSR.[DAIF] bits that are not referenced
    anywhere. The less these bits are available outside of local_irq_*
    functions the better.
    
    Get rid of those unused helpers.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 8d91f2233135..546bc398553e 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -43,16 +43,6 @@ static inline unsigned long local_daif_save(void)
 	return flags;
 }
 
-static inline void local_daif_unmask(void)
-{
-	trace_hardirqs_on();
-	asm volatile(
-		"msr	daifclr, #0xf		// local_daif_unmask"
-		:
-		:
-		: "memory");
-}
-
 static inline void local_daif_restore(unsigned long flags)
 {
 	if (!arch_irqs_disabled_flags(flags))

commit f05692919bdd44ad2a1a1fe85222dec16fec7e80
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Aug 28 16:51:14 2018 +0100

    arm64: daifflags: Use irqflags functions for daifflags
    
    Some of the work done in daifflags save/restore is already provided
    by irqflags functions. Daifflags should always be a superset of irqflags
    (it handles irq status + status of other flags). Modifying behaviour of
    irqflags should alter the behaviour of daifflags.
    
    Use irqflags_save/restore functions for the corresponding daifflags
    operation.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 22e4c83de5a5..8d91f2233135 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -36,11 +36,8 @@ static inline unsigned long local_daif_save(void)
 {
 	unsigned long flags;
 
-	asm volatile(
-		"mrs	%0, daif		// local_daif_save\n"
-		: "=r" (flags)
-		:
-		: "memory");
+	flags = arch_local_save_flags();
+
 	local_daif_mask();
 
 	return flags;
@@ -60,11 +57,9 @@ static inline void local_daif_restore(unsigned long flags)
 {
 	if (!arch_irqs_disabled_flags(flags))
 		trace_hardirqs_on();
-	asm volatile(
-		"msr	daif, %0		// local_daif_restore"
-		:
-		: "r" (flags)
-		: "memory");
+
+	arch_local_irq_restore(flags);
+
 	if (arch_irqs_disabled_flags(flags))
 		trace_hardirqs_off();
 }

commit 41bd5b5d22b77c7300df2a2fa5397cbe785189b4
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:36 2017 +0000

    arm64: Move the async/fiq helpers to explicitly set process context flags
    
    Remove the local_{async,fiq}_{en,dis}able macros as they don't respect
    our newly defined order and are only used to set the flags for process
    context when we bring CPUs online.
    
    Add a helper to do this. The IRQ flag varies as we want it masked on
    the boot CPU until we are ready to handle interrupts.
    The boot CPU unmasks SError during early boot once it can print an error
    message. If we can print an error message about SError, we can do the
    same for FIQ. Debug exceptions are already enabled by __cpu_setup(),
    which has also configured MDSCR_EL1 to disable MDE and KDE.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
index 55e2598a8c4c..22e4c83de5a5 100644
--- a/arch/arm64/include/asm/daifflags.h
+++ b/arch/arm64/include/asm/daifflags.h
@@ -18,6 +18,9 @@
 
 #include <linux/irqflags.h>
 
+#define DAIF_PROCCTX		0
+#define DAIF_PROCCTX_NOIRQ	PSR_I_BIT
+
 /* mask/save/unmask/restore all exceptions, including interrupts. */
 static inline void local_daif_mask(void)
 {

commit 0fbeb318754860b37150fd42c2058d636a431426
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:34 2017 +0000

    arm64: explicitly mask all exceptions
    
    There are a few places where we want to mask all exceptions. Today we
    do this in a piecemeal fashion, typically we expect the caller to
    have masked irqs and the arch code masks debug exceptions, ignoring
    serror which is probably masked.
    
    Make it clear that 'mask all exceptions' is the intention by adding
    helpers to do exactly that.
    
    This will let us unmask SError without having to add 'oh and SError'
    to these paths.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/daifflags.h b/arch/arm64/include/asm/daifflags.h
new file mode 100644
index 000000000000..55e2598a8c4c
--- /dev/null
+++ b/arch/arm64/include/asm/daifflags.h
@@ -0,0 +1,69 @@
+/*
+ * Copyright (C) 2017 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_DAIFFLAGS_H
+#define __ASM_DAIFFLAGS_H
+
+#include <linux/irqflags.h>
+
+/* mask/save/unmask/restore all exceptions, including interrupts. */
+static inline void local_daif_mask(void)
+{
+	asm volatile(
+		"msr	daifset, #0xf		// local_daif_mask\n"
+		:
+		:
+		: "memory");
+	trace_hardirqs_off();
+}
+
+static inline unsigned long local_daif_save(void)
+{
+	unsigned long flags;
+
+	asm volatile(
+		"mrs	%0, daif		// local_daif_save\n"
+		: "=r" (flags)
+		:
+		: "memory");
+	local_daif_mask();
+
+	return flags;
+}
+
+static inline void local_daif_unmask(void)
+{
+	trace_hardirqs_on();
+	asm volatile(
+		"msr	daifclr, #0xf		// local_daif_unmask"
+		:
+		:
+		: "memory");
+}
+
+static inline void local_daif_restore(unsigned long flags)
+{
+	if (!arch_irqs_disabled_flags(flags))
+		trace_hardirqs_on();
+	asm volatile(
+		"msr	daif, %0		// local_daif_restore"
+		:
+		: "r" (flags)
+		: "memory");
+	if (arch_irqs_disabled_flags(flags))
+		trace_hardirqs_off();
+}
+
+#endif
