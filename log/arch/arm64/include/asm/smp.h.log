commit 62a679cb2825488387f458c16dff32be41eb3d32
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Apr 23 11:16:06 2020 +0100

    arm64: simplify ptrauth initialization
    
    Currently __cpu_setup conditionally initializes the address
    authentication keys and enables them in SCTLR_EL1, doing so differently
    for the primary CPU and secondary CPUs, and skipping this work for CPUs
    returning from an idle state. For the latter case, cpu_do_resume
    restores the keys and SCTLR_EL1 value after the MMU has been enabled.
    
    This flow is rather difficult to follow, so instead let's move the
    primary and secondary CPU initialization into their respective boot
    paths. By following the example of cpu_do_resume and doing so once the
    MMU is enabled, we can always initialize the keys from the values in
    thread_struct, and avoid the machinery necessary to pass the keys in
    secondary_data or open-coding initialization for the boot CPU.
    
    This means we perform an additional RMW of SCTLR_EL1, but we already do
    this in the cpu_do_resume path, and for other features in cpufeature.c,
    so this isn't a major concern in a bringup path. Note that even while
    the enable bits are clear, the key registers are accessible.
    
    As this now renders the argument to __cpu_setup redundant, let's also
    remove that entirely. Future extensions can follow a similar approach to
    initialize values that differ for primary/secondary CPUs.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200423101606.37601-3-mark.rutland@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 40d5ba029615..ea268d88b6f7 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -23,14 +23,6 @@
 #define CPU_STUCK_REASON_52_BIT_VA	(UL(1) << CPU_STUCK_REASON_SHIFT)
 #define CPU_STUCK_REASON_NO_GRAN	(UL(2) << CPU_STUCK_REASON_SHIFT)
 
-/* Possible options for __cpu_setup */
-/* Option to setup primary cpu */
-#define ARM64_CPU_BOOT_PRIMARY		(1)
-/* Option to setup secondary cpus */
-#define ARM64_CPU_BOOT_SECONDARY	(2)
-/* Option to setup cpus for different cpu run time services */
-#define ARM64_CPU_RUNTIME		(3)
-
 #ifndef __ASSEMBLY__
 
 #include <asm/percpu.h>
@@ -96,9 +88,6 @@ asmlinkage void secondary_start_kernel(void);
 struct secondary_data {
 	void *stack;
 	struct task_struct *task;
-#ifdef CONFIG_ARM64_PTR_AUTH
-	struct ptrauth_keys_kernel ptrauth_key;
-#endif
 	long status;
 };
 

commit 33e45234987ea3ed4b05fc512f4441696478f12d
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:56 2020 +0530

    arm64: initialize and switch ptrauth kernel keys
    
    Set up keys to use pointer authentication within the kernel. The kernel
    will be compiled with APIAKey instructions, the other keys are currently
    unused. Each task is given its own APIAKey, which is initialized during
    fork. The key is changed during context switch and on kernel entry from
    EL0.
    
    The keys for idle threads need to be set before calling any C functions,
    because it is not possible to enter and exit a function with different
    keys.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Modified secondary cores key structure, comments]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 8d66497d8157..40d5ba029615 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -38,6 +38,7 @@
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/thread_info.h>
+#include <asm/pointer_auth.h>
 
 DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
 
@@ -95,6 +96,9 @@ asmlinkage void secondary_start_kernel(void);
 struct secondary_data {
 	void *stack;
 	struct task_struct *task;
+#ifdef CONFIG_ARM64_PTR_AUTH
+	struct ptrauth_keys_kernel ptrauth_key;
+#endif
 	long status;
 };
 

commit df3551011b8188e7a9291a66c2c0a04c4eb9d8eb
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Fri Mar 13 14:34:52 2020 +0530

    arm64: ptrauth: Add bootup/runtime flags for __cpu_setup
    
    This patch allows __cpu_setup to be invoked with one of these flags,
    ARM64_CPU_BOOT_PRIMARY, ARM64_CPU_BOOT_SECONDARY or ARM64_CPU_RUNTIME.
    This is required as some cpufeatures need different handling during
    different scenarios.
    
    The input parameter in x0 is preserved till the end to be used inside
    this function.
    
    There should be no functional change with this patch and is useful
    for the subsequent ptrauth patch which utilizes it. Some upcoming
    arm cpufeatures can also utilize these flags.
    
    Suggested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index a0c8a0b65259..8d66497d8157 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -23,6 +23,14 @@
 #define CPU_STUCK_REASON_52_BIT_VA	(UL(1) << CPU_STUCK_REASON_SHIFT)
 #define CPU_STUCK_REASON_NO_GRAN	(UL(2) << CPU_STUCK_REASON_SHIFT)
 
+/* Possible options for __cpu_setup */
+/* Option to setup primary cpu */
+#define ARM64_CPU_BOOT_PRIMARY		(1)
+/* Option to setup secondary cpus */
+#define ARM64_CPU_BOOT_SECONDARY	(2)
+/* Option to setup cpus for different cpu run time services */
+#define ARM64_CPU_RUNTIME		(3)
+
 #ifndef __ASSEMBLY__
 
 #include <asm/percpu.h>

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index eae2d6c01262..a0c8a0b65259 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_SMP_H
 #define __ASM_SMP_H

commit 262afe92fa8c7d64118948d98667c102c7d16174
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Mon Jun 3 16:18:29 2019 -0700

    arm64: smp: Moved cpu_logical_map[] to smp.h
    
    asm/smp.h is included by linux/smp.h and some drivers, in particular
    irqchip drivers can access cpu_logical_map[] in order to perform SMP
    affinity tasks. Make arm64 consistent with other architectures here.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 18553f399e08..eae2d6c01262 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -53,6 +53,12 @@ DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
  */
 #define raw_smp_processor_id() (*raw_cpu_ptr(&cpu_number))
 
+/*
+ * Logical CPU mapping.
+ */
+extern u64 __cpu_logical_map[NR_CPUS];
+#define cpu_logical_map(cpu)    __cpu_logical_map[cpu]
+
 struct seq_file;
 
 /*

commit 1236cd2bad8bc9038af90830cf017c770fdba03c
Author: Shaokun Zhang <zhangshaokun@hisilicon.com>
Date:   Sat Dec 29 09:43:17 2018 +0800

    arm64: smp: Fix compilation error
    
    For arm64: updates for 4.21, there is a compilation error:
    arch/arm64/kernel/head.S: Assembler messages:
    arch/arm64/kernel/head.S:824: Error: missing ')'
    arch/arm64/kernel/head.S:824: Error: missing ')'
    arch/arm64/kernel/head.S:824: Error: missing ')'
    arch/arm64/kernel/head.S:824: Error: unexpected characters following instruction at operand 2 -- `mov x2,#(2)|(2U<<(8))'
    scripts/Makefile.build:391: recipe for target 'arch/arm64/kernel/head.o' failed
    make[1]: *** [arch/arm64/kernel/head.o] Error 1
    GCC version is gcc (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
    
    Let's fix it using the UL() macro.
    
    Fixes: 66f16a24512f ("arm64: smp: Rework early feature mismatched detection")
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Tested-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Shaokun Zhang <zhangshaokun@hisilicon.com>
    [will: consistent use of UL() for all shifts in asm constants]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 1895561839a9..18553f399e08 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -16,9 +16,11 @@
 #ifndef __ASM_SMP_H
 #define __ASM_SMP_H
 
+#include <linux/const.h>
+
 /* Values for secondary_data.status */
 #define CPU_STUCK_REASON_SHIFT		(8)
-#define CPU_BOOT_STATUS_MASK		((1U << CPU_STUCK_REASON_SHIFT) - 1)
+#define CPU_BOOT_STATUS_MASK		((UL(1) << CPU_STUCK_REASON_SHIFT) - 1)
 
 #define CPU_MMU_OFF			(-1)
 #define CPU_BOOT_SUCCESS		(0)
@@ -29,8 +31,8 @@
 /* Fatal system error detected by secondary CPU, crash the system */
 #define CPU_PANIC_KERNEL		(3)
 
-#define CPU_STUCK_REASON_52_BIT_VA	(1U << CPU_STUCK_REASON_SHIFT)
-#define CPU_STUCK_REASON_NO_GRAN	(2U << CPU_STUCK_REASON_SHIFT)
+#define CPU_STUCK_REASON_52_BIT_VA	(UL(1) << CPU_STUCK_REASON_SHIFT)
+#define CPU_STUCK_REASON_NO_GRAN	(UL(2) << CPU_STUCK_REASON_SHIFT)
 
 #ifndef __ASSEMBLY__
 

commit 66f16a24512fa44680504effe908df8326885594
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 14:21:13 2018 +0000

    arm64: smp: Rework early feature mismatched detection
    
    Rather than add additional variables to detect specific early feature
    mismatches with secondary CPUs, we can instead dedicate the upper bits
    of the CPU boot status word to flag specific mismatches.
    
    This allows us to communicate both granule and VA-size mismatches back
    to the primary CPU without the need for additional book-keeping.
    
    Tested-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index f82b447bd34f..1895561839a9 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -17,15 +17,20 @@
 #define __ASM_SMP_H
 
 /* Values for secondary_data.status */
+#define CPU_STUCK_REASON_SHIFT		(8)
+#define CPU_BOOT_STATUS_MASK		((1U << CPU_STUCK_REASON_SHIFT) - 1)
 
-#define CPU_MMU_OFF		(-1)
-#define CPU_BOOT_SUCCESS	(0)
+#define CPU_MMU_OFF			(-1)
+#define CPU_BOOT_SUCCESS		(0)
 /* The cpu invoked ops->cpu_die, synchronise it with cpu_kill */
-#define CPU_KILL_ME		(1)
+#define CPU_KILL_ME			(1)
 /* The cpu couldn't die gracefully and is looping in the kernel */
-#define CPU_STUCK_IN_KERNEL	(2)
+#define CPU_STUCK_IN_KERNEL		(2)
 /* Fatal system error detected by secondary CPU, crash the system */
-#define CPU_PANIC_KERNEL	(3)
+#define CPU_PANIC_KERNEL		(3)
+
+#define CPU_STUCK_REASON_52_BIT_VA	(1U << CPU_STUCK_REASON_SHIFT)
+#define CPU_STUCK_REASON_NO_GRAN	(2U << CPU_STUCK_REASON_SHIFT)
 
 #ifndef __ASSEMBLY__
 

commit a88ce63b642cf8cd82cbc278429ccd9de4455a07
Author: Hoeun Ryu <hoeun.ryu@gmail.com>
Date:   Thu Aug 17 11:24:27 2017 +0900

    arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
    
     Commit 0ee5941 : (x86/panic: replace smp_send_stop() with kdump friendly
    version in panic path) introduced crash_smp_send_stop() which is a weak
    function and can be overridden by architecture codes to fix the side effect
    caused by commit f06e515 : (kernel/panic.c: add "crash_kexec_post_
    notifiers" option).
    
     ARM64 architecture uses the weak version function and the problem is that
    the weak function simply calls smp_send_stop() which makes other CPUs
    offline and takes away the chance to save crash information for nonpanic
    CPUs in machine_crash_shutdown() when crash_kexec_post_notifiers kernel
    option is enabled.
    
     Calling smp_send_crash_stop() in machine_crash_shutdown() is useless
    because all nonpanic CPUs are already offline by smp_send_stop() in this
    case and smp_send_crash_stop() only works against online CPUs.
    
     The result is that secondary CPUs registers are not saved by
    crash_save_cpu() and the vmcore file misreports these CPUs as being
    offline.
    
     crash_smp_send_stop() is implemented to fix this problem by replacing the
    existing smp_send_crash_stop() and adding a check for multiple calling to
    the function. The function (strong symbol version) saves crash information
    for nonpanic CPUs and machine_crash_shutdown() tries to save crash
    information for nonpanic CPUs only when crash_kexec_post_notifiers kernel
    option is disabled.
    
    * crash_kexec_post_notifiers : false
    
      panic()
        __crash_kexec()
          machine_crash_shutdown()
            crash_smp_send_stop()    <= save crash dump for nonpanic cores
    
    * crash_kexec_post_notifiers : true
    
      panic()
        crash_smp_send_stop()        <= save crash dump for nonpanic cores
        __crash_kexec()
          machine_crash_shutdown()
            crash_smp_send_stop()    <= just return.
    
    Signed-off-by: Hoeun Ryu <hoeun.ryu@gmail.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 55f08c5acfad..f82b447bd34f 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -148,7 +148,7 @@ static inline void cpu_panic_kernel(void)
  */
 bool cpus_are_stuck_in_kernel(void);
 
-extern void smp_send_crash_stop(void);
+extern void crash_smp_send_stop(void);
 extern bool smp_crash_stop_failed(void);
 
 #endif /* ifndef __ASSEMBLY__ */

commit 78fd584cdec0518075cf3aa75e5ec491cc8f3ff3
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:36 2017 +0900

    arm64: kdump: implement machine_crash_shutdown()
    
    Primary kernel calls machine_crash_shutdown() to shut down non-boot cpus
    and save registers' status in per-cpu ELF notes before starting crash
    dump kernel. See kernel_kexec().
    Even if not all secondary cpus have shut down, we do kdump anyway.
    
    As we don't have to make non-boot(crashed) cpus offline (to preserve
    correct status of cpus at crash dump) before shutting down, this patch
    also adds a variant of smp_send_stop().
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index d050d720a1b4..55f08c5acfad 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -148,6 +148,9 @@ static inline void cpu_panic_kernel(void)
  */
 bool cpus_are_stuck_in_kernel(void);
 
+extern void smp_send_crash_stop(void);
+extern bool smp_crash_stop_failed(void);
+
 #endif /* ifndef __ASSEMBLY__ */
 
 #endif /* ifndef __ASM_SMP_H */

commit 34a6980c82fb1342e7064844c95aa4cf933e5ecc
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Thu Dec 1 15:55:13 2016 +0000

    arm64: smp: Prevent raw_smp_processor_id() recursion
    
    Under CONFIG_DEBUG_PREEMPT=y, this_cpu_ptr() ends up calling back into
    raw_smp_processor_id(), resulting in some hilariously catastrophic
    infinite recursion. In the normal case, we have:
    
      #define this_cpu_ptr(ptr) raw_cpu_ptr(ptr)
    
    and everything is dandy. However for CONFIG_DEBUG_PREEMPT, this_cpu_ptr()
    is defined in terms of my_cpu_offset, wherein the fun begins:
    
      #define my_cpu_offset per_cpu_offset(smp_processor_id())
      ...
      #define smp_processor_id() debug_smp_processor_id()
      ...
      notrace unsigned int debug_smp_processor_id(void)
      {
            return check_preemption_disabled("smp_processor_id", "");
      ...
      notrace static unsigned int check_preemption_disabled(const char *what1,
                                                            const char *what2)
      {
            int this_cpu = raw_smp_processor_id();
    
    and bang. Use raw_cpu_ptr() directly to avoid that.
    
    Fixes: 57c82954e77f ("arm64: make cpu number a percpu variable")
    Reported-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index a62db952ffcb..d050d720a1b4 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -41,8 +41,10 @@ DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
  * We don't use this_cpu_read(cpu_number) as that has implicit writes to
  * preempt_count, and associated (compiler) barriers, that we'd like to avoid
  * the expense of. If we're preemptible, the value can be stale at use anyway.
+ * And we can't use this_cpu_ptr() either, as that winds up recursing back
+ * here under CONFIG_DEBUG_PREEMPT=y.
  */
-#define raw_smp_processor_id() (*this_cpu_ptr(&cpu_number))
+#define raw_smp_processor_id() (*raw_cpu_ptr(&cpu_number))
 
 struct seq_file;
 

commit c02433dd6de32f042cf3ffe476746b1115b8c096
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:13 2016 +0000

    arm64: split thread_info from task stack
    
    This patch moves arm64's struct thread_info from the task stack into
    task_struct. This protects thread_info from corruption in the case of
    stack overflows, and makes its address harder to determine if stack
    addresses are leaked, making a number of attacks more difficult. Precise
    detection and handling of overflow is left for subsequent patches.
    
    Largely, this involves changing code to store the task_struct in sp_el0,
    and acquire the thread_info from the task struct. Core code now
    implements current_thread_info(), and as noted in <linux/sched.h> this
    relies on offsetof(task_struct, thread_info) == 0, enforced by core
    code.
    
    This change means that the 'tsk' register used in entry.S now points to
    a task_struct, rather than a thread_info as it used to. To make this
    clear, the TI_* field offsets are renamed to TSK_TI_*, with asm-offsets
    appropriately updated to account for the structural change.
    
    Userspace clobbers sp_el0, and we can no longer restore this from the
    stack. Instead, the current task is cached in a per-cpu variable that we
    can safely access from early assembly as interrupts are disabled (and we
    are thus not preemptible).
    
    Both secondary entry and idle are updated to stash the sp and task
    pointer separately.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 968b08de820d..a62db952ffcb 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -82,6 +82,7 @@ asmlinkage void secondary_start_kernel(void);
  */
 struct secondary_data {
 	void *stack;
+	struct task_struct *task;
 	long status;
 };
 

commit 57c82954e77fa12c1023e87210d2ede77aaa0058
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:11 2016 +0000

    arm64: make cpu number a percpu variable
    
    In the absence of CONFIG_THREAD_INFO_IN_TASK, core code maintains
    thread_info::cpu, and low-level architecture code can access this to
    build raw_smp_processor_id(). With CONFIG_THREAD_INFO_IN_TASK, core code
    maintains task_struct::cpu, which for reasons of hte header soup is not
    accessible to low-level arch code.
    
    Instead, we can maintain a percpu variable containing the cpu number.
    
    For both the old and new implementation of raw_smp_processor_id(), we
    read a syreg into a GPR, add an offset, and load the result. As the
    offset is now larger, it may not be folded into the load, but otherwise
    the assembly shouldn't change much.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 022644704a93..968b08de820d 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -29,11 +29,20 @@
 
 #ifndef __ASSEMBLY__
 
+#include <asm/percpu.h>
+
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/thread_info.h>
 
-#define raw_smp_processor_id() (current_thread_info()->cpu)
+DECLARE_PER_CPU_READ_MOSTLY(int, cpu_number);
+
+/*
+ * We don't use this_cpu_read(cpu_number) as that has implicit writes to
+ * preempt_count, and associated (compiler) barriers, that we'd like to avoid
+ * the expense of. If we're preemptible, the value can be stale at use anyway.
+ */
+#define raw_smp_processor_id() (*this_cpu_ptr(&cpu_number))
 
 struct seq_file;
 

commit 5c492c3f5255bd34f7ff8867515ecf98dcba2a2e
Author: James Morse <james.morse@arm.com>
Date:   Wed Jun 22 10:06:12 2016 +0100

    arm64: smp: Add function to determine if cpus are stuck in the kernel
    
    kernel/smp.c has a fancy counter that keeps track of the number of CPUs
    it marked as not-present and left in cpu_park_loop(). If there are any
    CPUs spinning in here, features like kexec or hibernate may release them
    by overwriting this memory.
    
    This problem also occurs on machines using spin-tables to release
    secondary cores.
    After commit 44dbcc93ab67 ("arm64: Fix behavior of maxcpus=N")
    we bring all known cpus into the secondary holding pen, meaning this
    memory can't be re-used by kexec or hibernate.
    
    Add a function cpus_are_stuck_in_kernel() to determine if either of these
    cases have occurred.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 433e50405274..022644704a93 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -124,6 +124,18 @@ static inline void cpu_panic_kernel(void)
 	cpu_park_loop();
 }
 
+/*
+ * If a secondary CPU enters the kernel but fails to come online,
+ * (e.g. due to mismatched features), and cannot exit the kernel,
+ * we increment cpus_stuck_in_kernel and leave the CPU in a
+ * quiesecent loop within the kernel text. The memory containing
+ * this loop must not be re-used for anything else as the 'stuck'
+ * core is executing it.
+ *
+ * This function is used to inhibit features like kexec and hibernate.
+ */
+bool cpus_are_stuck_in_kernel(void);
+
 #endif /* ifndef __ASSEMBLY__ */
 
 #endif /* ifndef __ASM_SMP_H */

commit 17eebd1a435c8616c47b715e3447f4a9c15b741f
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Apr 12 15:46:00 2016 +0100

    arm64: Add cpu_panic_kernel helper
    
    During the activation of a secondary CPU, we could report serious
    configuration issues and hence request to crash the kernel. We do
    this for CPU ASID bit check now. We will need it also for handling
    mismatched exception levels for the CPUs with VHE. Hence, add a
    helper to do the same for reusability.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 817a067ba058..433e50405274 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -113,6 +113,17 @@ static inline void update_cpu_boot_status(int val)
 	dsb(ishst);
 }
 
+/*
+ * The calling secondary CPU has detected serious configuration mismatch,
+ * which calls for a kernel panic. Update the boot status and park the calling
+ * CPU.
+ */
+static inline void cpu_panic_kernel(void)
+{
+	update_cpu_boot_status(CPU_PANIC_KERNEL);
+	cpu_park_loop();
+}
+
 #endif /* ifndef __ASSEMBLY__ */
 
 #endif /* ifndef __ASM_SMP_H */

commit bb9052744f4b7ae11d0061ac9492dd2949981b49
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Feb 23 10:31:42 2016 +0000

    arm64: Handle early CPU boot failures
    
    A secondary CPU could fail to come online due to insufficient
    capabilities and could simply die or loop in the kernel.
    e.g, a CPU with no support for the selected kernel PAGE_SIZE
    loops in kernel with MMU turned off.
    or a hotplugged CPU which doesn't have one of the advertised
    system capability will die during the activation.
    
    There is no way to synchronise the status of the failing CPU
    back to the master. This patch solves the issue by adding a
    field to the secondary_data which can be updated by the failing
    CPU. If the secondary CPU fails even before turning the MMU on,
    it updates the status in a special variable reserved in the head.txt
    section to make sure that the update can be cache invalidated safely
    without possible sharing of cache write back granule.
    
    Here are the possible states :
    
     -1. CPU_MMU_OFF - Initial value set by the master CPU, this value
    indicates that the CPU could not turn the MMU on, hence the status
    could not be reliably updated in the secondary_data. Instead, the
    CPU has updated the status @ __early_cpu_boot_status.
    
     0. CPU_BOOT_SUCCESS - CPU has booted successfully.
    
     1. CPU_KILL_ME - CPU has invoked cpu_ops->die, indicating the
    master CPU to synchronise by issuing a cpu_ops->cpu_kill.
    
     2. CPU_STUCK_IN_KERNEL - CPU couldn't invoke die(), instead is
    looping in the kernel. This information could be used by say,
    kexec to check if it is really safe to do a kexec reboot.
    
     3. CPU_PANIC_KERNEL - CPU detected some serious issues which
    requires kernel to crash immediately. The secondary CPU cannot
    call panic() until it has initialised the GIC. This flag can
    be used to instruct the master to do so.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [catalin.marinas@arm.com: conflict resolution]
    [catalin.marinas@arm.com: converted "status" from int to long]
    [catalin.marinas@arm.com: updated update_early_cpu_boot_status to use str_l]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 51913be35a0a..817a067ba058 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -16,6 +16,19 @@
 #ifndef __ASM_SMP_H
 #define __ASM_SMP_H
 
+/* Values for secondary_data.status */
+
+#define CPU_MMU_OFF		(-1)
+#define CPU_BOOT_SUCCESS	(0)
+/* The cpu invoked ops->cpu_die, synchronise it with cpu_kill */
+#define CPU_KILL_ME		(1)
+/* The cpu couldn't die gracefully and is looping in the kernel */
+#define CPU_STUCK_IN_KERNEL	(2)
+/* Fatal system error detected by secondary CPU, crash the system */
+#define CPU_PANIC_KERNEL	(3)
+
+#ifndef __ASSEMBLY__
+
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/thread_info.h>
@@ -54,11 +67,17 @@ asmlinkage void secondary_start_kernel(void);
 
 /*
  * Initial data for bringing up a secondary CPU.
+ * @stack  - sp for the secondary CPU
+ * @status - Result passed back from the secondary CPU to
+ *           indicate failure.
  */
 struct secondary_data {
 	void *stack;
+	long status;
 };
+
 extern struct secondary_data secondary_data;
+extern long __early_cpu_boot_status;
 extern void secondary_entry(void);
 
 extern void arch_send_call_function_single_ipi(int cpu);
@@ -87,4 +106,13 @@ static inline void cpu_park_loop(void)
 	}
 }
 
+static inline void update_cpu_boot_status(int val)
+{
+	WRITE_ONCE(secondary_data.status, val);
+	/* Ensure the visibility of the status update */
+	dsb(ishst);
+}
+
+#endif /* ifndef __ASSEMBLY__ */
+
 #endif /* ifndef __ASM_SMP_H */

commit fce6361fe9b0caeba0c05b7d72ceda406f8780df
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Feb 23 10:31:41 2016 +0000

    arm64: Move cpu_die_early to smp.c
    
    This patch moves cpu_die_early to smp.c, where it fits better.
    No functional changes, except for adding the necessary checks
    for CONFIG_HOTPLUG_CPU.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index b93eb3303131..51913be35a0a 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -77,6 +77,7 @@ extern int __cpu_disable(void);
 
 extern void __cpu_die(unsigned int cpu);
 extern void cpu_die(void);
+extern void cpu_die_early(void);
 
 static inline void cpu_park_loop(void)
 {

commit c4bc34d20273db698c51951a1951dba0a722e162
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Feb 23 10:31:39 2016 +0000

    arm64: Add a helper for parking CPUs in a loop
    
    Adds a routine which can be used to park CPUs (spinning in kernel)
    when they can't be killed.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 2013a4dc5124..b93eb3303131 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -78,4 +78,12 @@ extern int __cpu_disable(void);
 extern void __cpu_die(unsigned int cpu);
 extern void cpu_die(void);
 
+static inline void cpu_park_loop(void)
+{
+	for (;;) {
+		wfe();
+		wfi();
+	}
+}
+
 #endif /* ifndef __ASM_SMP_H */

commit 5e89c55e4ed81d7abb1ce8828db35fa389dc0e90
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Tue Jan 26 11:10:38 2016 +0000

    arm64: kernel: implement ACPI parking protocol
    
    The SBBR and ACPI specifications allow ACPI based systems that do not
    implement PSCI (eg systems with no EL3) to boot through the ACPI parking
    protocol specification[1].
    
    This patch implements the ACPI parking protocol CPU operations, and adds
    code that eases parsing the parking protocol data structures to the
    ARM64 SMP initializion carried out at the same time as cpus enumeration.
    
    To wake-up the CPUs from the parked state, this patch implements a
    wakeup IPI for ARM64 (ie arch_send_wakeup_ipi_mask()) that mirrors the
    ARM one, so that a specific IPI is sent for wake-up purpose in order
    to distinguish it from other IPI sources.
    
    Given the current ACPI MADT parsing API, the patch implements a glue
    layer that helps passing MADT GICC data structure from SMP initialization
    code to the parking protocol implementation somewhat overriding the CPU
    operations interfaces. This to avoid creating a completely trasparent
    DT/ACPI CPU operations layer that would require creating opaque
    structure handling for CPUs data (DT represents CPU through DT nodes, ACPI
    through static MADT table entries), which seems overkill given that ACPI
    on ARM64 mandates only two booting protocols (PSCI and parking protocol),
    so there is no need for further protocol additions.
    
    Based on the original work by Mark Salter <msalter@redhat.com>
    
    [1] https://acpica.org/sites/acpica/files/MP%20Startup%20for%20ARM%20platforms.docx
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Loc Ho <lho@apm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Al Stone <ahs3@redhat.com>
    [catalin.marinas@arm.com: Added WARN_ONCE(!acpi_parking_protocol_valid() on the IPI]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index d9c3d6a6100a..2013a4dc5124 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -64,6 +64,15 @@ extern void secondary_entry(void);
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
 
+#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
+extern void arch_send_wakeup_ipi_mask(const struct cpumask *mask);
+#else
+static inline void arch_send_wakeup_ipi_mask(const struct cpumask *mask)
+{
+	BUILD_BUG();
+}
+#endif
+
 extern int __cpu_disable(void);
 
 extern void __cpu_die(unsigned int cpu);

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index db02be81b90a..d9c3d6a6100a 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -20,10 +20,6 @@
 #include <linux/cpumask.h>
 #include <linux/thread_info.h>
 
-#ifndef CONFIG_SMP
-# error "<asm/smp.h> included in non-SMP build"
-#endif
-
 #define raw_smp_processor_id() (current_thread_info()->cpu)
 
 struct seq_file;

commit 0f0783365cbb7ec13a8f02198f6e1a146d94a5a9
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed May 13 14:12:47 2015 +0100

    ARM64: kernel: unify ACPI and DT cpus initialization
    
    The code that initializes cpus on arm64 is currently split in two
    different code paths that carry out DT and ACPI cpus initialization.
    
    Most of the code executing SMP initialization is common and should
    be merged to reduce discrepancies between ACPI and DT initialization
    and to have code initializing cpus in a single common place in the
    kernel.
    
    This patch refactors arm64 SMP cpus initialization code to merge
    ACPI and DT boot paths in a common file and to create sanity
    checks that can be reused by both boot methods.
    
    Current code assumes PSCI is the only available boot method
    when arm64 boots with ACPI; this can be easily extended if/when
    the ACPI parking protocol is merged into the kernel.
    
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: Mark Rutland <mark.rutland@arm.com> [DT]
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index bf22650b1a78..db02be81b90a 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -42,7 +42,7 @@ extern void handle_IPI(int ipinr, struct pt_regs *regs);
  * Discover the set of possible CPUs and determine their
  * SMP operations.
  */
-extern void of_smp_init_cpus(void);
+extern void smp_init_cpus(void);
 
 /*
  * Provide a function to raise an IPI cross call on CPUs in callmap.

commit fccb9a81fd08b61bed91ddef88341694f8ecbfd1
Author: Hanjun Guo <hanjun.guo@linaro.org>
Date:   Tue Mar 24 22:02:45 2015 +0800

    ARM64 / ACPI: Parse MADT for SMP initialization
    
    MADT contains the information for MPIDR which is essential for
    SMP initialization, parse the GIC cpu interface structures to
    get the MPIDR value and map it to cpu_logical_map(), and add
    enabled cpu with valid MPIDR into cpu_possible_map.
    
    ACPI 5.1 only has two explicit methods to boot up SMP, PSCI and
    Parking protocol, but the Parking protocol is only specified for
    ARMv7 now, so make PSCI as the only way for the SMP boot protocol
    before some updates for the ACPI spec or the Parking protocol spec.
    
    Parking protocol patches for SMP boot will be sent to upstream when
    the new version of Parking protocol is ready.
    
    CC: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    CC: Catalin Marinas <catalin.marinas@arm.com>
    CC: Will Deacon <will.deacon@arm.com>
    CC: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
    Tested-by: Yijing Wang <wangyijing@huawei.com>
    Tested-by: Mark Langsdorf <mlangsdo@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>
    Tested-by: Timur Tabi <timur@codeaurora.org>
    Tested-by: Robert Richter <rrichter@cavium.com>
    Acked-by: Robert Richter <rrichter@cavium.com>
    Acked-by: Olof Johansson <olof@lixom.net>
    Reviewed-by: Grant Likely <grant.likely@linaro.org>
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Tomasz Nowicki <tomasz.nowicki@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 780f82c827b6..bf22650b1a78 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -39,9 +39,10 @@ extern void show_ipi_list(struct seq_file *p, int prec);
 extern void handle_IPI(int ipinr, struct pt_regs *regs);
 
 /*
- * Setup the set of possible CPUs (via set_cpu_possible)
+ * Discover the set of possible CPUs and determine their
+ * SMP operations.
  */
-extern void smp_init_cpus(void);
+extern void of_smp_init_cpus(void);
 
 /*
  * Provide a function to raise an IPI cross call on CPUs in callmap.

commit 3631073659d0aafeaa52227bb61a100efaf901dc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Aug 16 18:48:05 2014 +0200

    arm64: Tell irq work about self IPI support
    
    ARM64 irq work self-IPI support depends on __smp_cross_call to point to
    some relevant IRQ controller operations. This information should be
    available after the call to init_IRQ().
    
    Lets implement arch_irq_work_has_interrupt() accordingly.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index a498f2cd2c2a..780f82c827b6 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -48,6 +48,8 @@ extern void smp_init_cpus(void);
  */
 extern void set_smp_cross_call(void (*)(const struct cpumask *, unsigned int));
 
+extern void (*__smp_cross_call)(const struct cpumask *, unsigned int);
+
 /*
  * Called from the secondary holding pen, this is the secondary CPU entry point.
  */

commit 9327e2c6bb8cb0131b38a07847cd58c78dc095e9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:18 2013 +0100

    arm64: add CPU_HOTPLUG infrastructure
    
    This patch adds the basic infrastructure necessary to support
    CPU_HOTPLUG on arm64, based on the arm implementation. Actual hotplug
    support will depend on an implementation's cpu_operations (e.g. PSCI).
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index d64187ce69a2..a498f2cd2c2a 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -65,4 +65,9 @@ extern void secondary_entry(void);
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
 
+extern int __cpu_disable(void);
+
+extern void __cpu_die(unsigned int cpu);
+extern void cpu_die(void);
+
 #endif /* ifndef __ASM_SMP_H */

commit 652af899799354049b273af897b798b8f03fdd88
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:16 2013 +0100

    arm64: factor out spin-table boot method
    
    The arm64 kernel has an internal holding pen, which is necessary for
    some systems where we can't bring CPUs online individually and must hold
    multiple CPUs in a safe area until the kernel is able to handle them.
    The current SMP infrastructure for arm64 is closely coupled to this
    holding pen, and alternative boot methods must launch CPUs into the pen,
    where they sit before they are launched into the kernel proper.
    
    With PSCI (and possibly other future boot methods), we can bring CPUs
    online individually, and need not perform the secondary_holding_pen
    dance. Instead, this patch factors the holding pen management code out
    to the spin-table boot method code, as it is the only boot method
    requiring the pen.
    
    A new entry point for secondaries, secondary_entry is added for other
    boot methods to use, which bypasses the holding pen and its associated
    overhead when bringing CPUs online. The smp.pen.text section is also
    removed, as the pen can live in head.text without problem.
    
    The cpu_operations structure is extended with two new functions,
    cpu_boot and cpu_postboot, for bringing a cpu into the kernel and
    performing any post-boot cleanup required by a bootmethod (e.g.
    resetting the secondary_holding_pen_release to INVALID_HWID).
    Documentation is added for cpu_operations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 7e34295f78e3..d64187ce69a2 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -60,8 +60,7 @@ struct secondary_data {
 	void *stack;
 };
 extern struct secondary_data secondary_data;
-extern void secondary_holding_pen(void);
-extern volatile unsigned long secondary_holding_pen_release;
+extern void secondary_entry(void);
 
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);

commit cd1aebf5277a3a154a9e4c0ea4b3acabb62e5cab
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:15 2013 +0100

    arm64: reorganise smp_enable_ops
    
    For hotplug support, we're going to want a place to store operations
    that do more than bring CPUs online, and it makes sense to group these
    with our current smp_enable_ops. For cpuidle support, we'll want to
    group additional functions, and we may want them even for UP kernels.
    
    This patch renames smp_enable_ops to the more general cpu_operations,
    and pulls the definitions out of smp code such that they can be used in
    UP kernels. While we're at it, fix up instances of the cpu parameter to
    be an unsigned int, drop the init markings and rename the *_cpu
    functions to cpu_* to reduce future churn when cpu_operations is
    extended.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 4b8023c5d146..7e34295f78e3 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -66,15 +66,4 @@ extern volatile unsigned long secondary_holding_pen_release;
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
 
-struct device_node;
-
-struct smp_enable_ops {
-	const char	*name;
-	int		(*init_cpu)(struct device_node *, int);
-	int		(*prepare_cpu)(int);
-};
-
-extern const struct smp_enable_ops smp_spin_table_ops;
-extern const struct smp_enable_ops smp_psci_ops;
-
 #endif /* ifndef __ASM_SMP_H */

commit 0459ca9b7a9f86d0523e008b79c1fcf1afbd3635
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jan 2 15:34:50 2013 +0000

    arm64: SMP: enable PSCI boot method
    
    Wire the PSCI implementation into the SMP secondary startup
    code.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index b1f68c19d170..4b8023c5d146 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -75,5 +75,6 @@ struct smp_enable_ops {
 };
 
 extern const struct smp_enable_ops smp_spin_table_ops;
+extern const struct smp_enable_ops smp_psci_ops;
 
 #endif /* ifndef __ASM_SMP_H */

commit d329de3f2ada413c7cd16e1dc1d70d4abc7309e9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jan 2 15:24:22 2013 +0000

    arm64: SMP: rework the SMP code to be enabling method agnostic
    
    In order to introduce PSCI support, let the SMP code handle
    multiple enabling methods. This also allow CPUs to be booted
    using different methods (though this feels a bit weird...).
    
    In the process, move the spin-table code to its own file.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
index 7e34295f78e3..b1f68c19d170 100644
--- a/arch/arm64/include/asm/smp.h
+++ b/arch/arm64/include/asm/smp.h
@@ -66,4 +66,14 @@ extern volatile unsigned long secondary_holding_pen_release;
 extern void arch_send_call_function_single_ipi(int cpu);
 extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
 
+struct device_node;
+
+struct smp_enable_ops {
+	const char	*name;
+	int		(*init_cpu)(struct device_node *, int);
+	int		(*prepare_cpu)(int);
+};
+
+extern const struct smp_enable_ops smp_spin_table_ops;
+
 #endif /* ifndef __ASM_SMP_H */

commit 08e875c16a16c950e1e6d85755df5f3440844675
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:30 2012 +0000

    arm64: SMP support
    
    This patch adds SMP initialisation and spinlocks implementation for
    AArch64. The spinlock support uses the new load-acquire/store-release
    instructions to avoid explicit barriers. The architecture also specifies
    that an event is automatically generated when clearing the exclusive
    monitor state to wake up processors in WFE, so there is no need for an
    explicit DSB/SEV instruction sequence. The SEVL instruction is used to
    set the exclusive monitor locally as there is no conditional WFE and a
    branch is more expensive.
    
    For the SMP booting protocol, see Documentation/arm64/booting.txt.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
new file mode 100644
index 000000000000..7e34295f78e3
--- /dev/null
+++ b/arch/arm64/include/asm/smp.h
@@ -0,0 +1,69 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_SMP_H
+#define __ASM_SMP_H
+
+#include <linux/threads.h>
+#include <linux/cpumask.h>
+#include <linux/thread_info.h>
+
+#ifndef CONFIG_SMP
+# error "<asm/smp.h> included in non-SMP build"
+#endif
+
+#define raw_smp_processor_id() (current_thread_info()->cpu)
+
+struct seq_file;
+
+/*
+ * generate IPI list text
+ */
+extern void show_ipi_list(struct seq_file *p, int prec);
+
+/*
+ * Called from C code, this handles an IPI.
+ */
+extern void handle_IPI(int ipinr, struct pt_regs *regs);
+
+/*
+ * Setup the set of possible CPUs (via set_cpu_possible)
+ */
+extern void smp_init_cpus(void);
+
+/*
+ * Provide a function to raise an IPI cross call on CPUs in callmap.
+ */
+extern void set_smp_cross_call(void (*)(const struct cpumask *, unsigned int));
+
+/*
+ * Called from the secondary holding pen, this is the secondary CPU entry point.
+ */
+asmlinkage void secondary_start_kernel(void);
+
+/*
+ * Initial data for bringing up a secondary CPU.
+ */
+struct secondary_data {
+	void *stack;
+};
+extern struct secondary_data secondary_data;
+extern void secondary_holding_pen(void);
+extern volatile unsigned long secondary_holding_pen_release;
+
+extern void arch_send_call_function_single_ipi(int cpu);
+extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
+
+#endif /* ifndef __ASM_SMP_H */
