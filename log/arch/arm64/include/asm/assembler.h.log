commit 3a9b136c998fc990cbde1a3cbc343050de704d6b
Author: Mark Brown <broonie@kernel.org>
Date:   Wed May 6 20:51:35 2020 +0100

    arm64: asm: Provide a mechanism for generating ELF note for BTI
    
    ELF files built for BTI should have a program property note section which
    identifies them as such. The linker expects to find this note in all
    object files it is linking into a BTI annotated output, the compiler will
    ensure that this happens for C files but for assembler files we need to do
    this in the source so provide a macro which can be used for this purpose.
    To support likely future requirements for additional notes we split the
    defininition of the flags to set for BTI code from the macro that creates
    the note itself.
    
    This is mainly for use in the vDSO which should be a normal ELF shared
    library and should therefore include BTI annotations when built for BTI.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Link: https://lore.kernel.org/r/20200506195138.22086-9-broonie@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 0bff325117b4..54d181177656 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -736,4 +736,54 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 .Lyield_out_\@ :
 	.endm
 
+/*
+ * This macro emits a program property note section identifying
+ * architecture features which require special handling, mainly for
+ * use in assembly files included in the VDSO.
+ */
+
+#define NT_GNU_PROPERTY_TYPE_0  5
+#define GNU_PROPERTY_AARCH64_FEATURE_1_AND      0xc0000000
+
+#define GNU_PROPERTY_AARCH64_FEATURE_1_BTI      (1U << 0)
+#define GNU_PROPERTY_AARCH64_FEATURE_1_PAC      (1U << 1)
+
+#ifdef CONFIG_ARM64_BTI_KERNEL
+#define GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT		\
+		((GNU_PROPERTY_AARCH64_FEATURE_1_BTI |	\
+		  GNU_PROPERTY_AARCH64_FEATURE_1_PAC))
+#endif
+
+#ifdef GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT
+.macro emit_aarch64_feature_1_and, feat=GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT
+	.pushsection .note.gnu.property, "a"
+	.align  3
+	.long   2f - 1f
+	.long   6f - 3f
+	.long   NT_GNU_PROPERTY_TYPE_0
+1:      .string "GNU"
+2:
+	.align  3
+3:      .long   GNU_PROPERTY_AARCH64_FEATURE_1_AND
+	.long   5f - 4f
+4:
+	/*
+	 * This is described with an array of char in the Linux API
+	 * spec but the text and all other usage (including binutils,
+	 * clang and GCC) treat this as a 32 bit value so no swizzling
+	 * is required for big endian.
+	 */
+	.long   \feat
+5:
+	.align  3
+6:
+	.popsection
+.endm
+
+#else
+.macro emit_aarch64_feature_1_and, feat=0
+.endm
+
+#endif /* GNU_PROPERTY_AARCH64_FEATURE_1_DEFAULT */
+
 #endif	/* __ASM_ASSEMBLER_H */

commit da12d2739fb69531bf6bb6eb7e46d73d1dabc814
Merge: bbd6ec605c0f f7d5ef0c654e c265861af2af b5475d8caedb de58ed5e16e6 c17a290f7e7e 8673e02e5841
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:10:32 2020 +0000

    Merge branches 'for-next/memory-hotremove', 'for-next/arm_sdei', 'for-next/amu', 'for-next/final-cap-helper', 'for-next/cpu_ops-cleanup', 'for-next/misc' and 'for-next/perf' into for-next/core
    
    * for-next/memory-hotremove:
      : Memory hot-remove support for arm64
      arm64/mm: Enable memory hot remove
      arm64/mm: Hold memory hotplug lock while walking for kernel page table dump
    
    * for-next/arm_sdei:
      : SDEI: fix double locking on return from hibernate and clean-up
      firmware: arm_sdei: clean up sdei_event_create()
      firmware: arm_sdei: Use cpus_read_lock() to avoid races with cpuhp
      firmware: arm_sdei: fix possible double-lock on hibernate error path
      firmware: arm_sdei: fix double-lock on hibernate with shared events
    
    * for-next/amu:
      : ARMv8.4 Activity Monitors support
      clocksource/drivers/arm_arch_timer: validate arch_timer_rate
      arm64: use activity monitors for frequency invariance
      cpufreq: add function to get the hardware max frequency
      Documentation: arm64: document support for the AMU extension
      arm64/kvm: disable access to AMU registers from kvm guests
      arm64: trap to EL1 accesses to AMU counters from EL0
      arm64: add support for the AMU extension v1
    
    * for-next/final-cap-helper:
      : Introduce cpus_have_final_cap_helper(), migrate arm64 KVM to it
      arm64: kvm: hyp: use cpus_have_final_cap()
      arm64: cpufeature: add cpus_have_final_cap()
    
    * for-next/cpu_ops-cleanup:
      : cpu_ops[] access code clean-up
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: define __alloc_zeroed_user_highpage
      arm64/kernel: Simplify __cpu_up() by bailing out early
      arm64: remove redundant blank for '=' operator
      arm64: kexec_file: Fixed code style.
      arm64: add blank after 'if'
      arm64: fix spelling mistake "ca not" -> "cannot"
      arm64: entry: unmask IRQ in el0_sp()
      arm64: efi: add efi-entry.o to targets instead of extra-$(CONFIG_EFI)
      arm64: csum: Optimise IPv6 header checksum
      arch/arm64: fix typo in a comment
      arm64: remove gratuitious/stray .ltorg stanzas
      arm64: Update comment for ASID() macro
      arm64: mm: convert cpu_do_switch_mm() to C
      arm64: fix NUMA Kconfig typos
    
    * for-next/perf:
      : arm64 perf updates
      arm64: perf: Add support for ARMv8.5-PMU 64-bit counters
      KVM: arm64: limit PMU version to PMUv3 for ARMv8.1
      arm64: cpufeature: Extract capped perfmon fields
      arm64: perf: Clean up enable/disable calls
      perf: arm-ccn: Use scnprintf() for robustness
      arm64: perf: Support new DT compatibles
      arm64: perf: Refactor PMU init callbacks
      perf: arm_spe: Remove unnecessary zero check on 'nr_pages'

commit 87a1f063464afd934f0f22aac710ca65bef77af3
Author: Ionela Voinescu <ionela.voinescu@arm.com>
Date:   Thu Mar 5 09:06:22 2020 +0000

    arm64: trap to EL1 accesses to AMU counters from EL0
    
    The activity monitors extension is an optional extension introduced
    by the ARMv8.4 CPU architecture. In order to access the activity
    monitors counters safely, if desired, the kernel should detect the
    presence of the extension through the feature register, and mediate
    the access.
    
    Therefore, disable direct accesses to activity monitors counters
    from EL0 (userspace) and trap them to EL1 (kernel).
    
    To be noted that the ARM64_AMU_EXTN kernel config does not have an
    effect on this code. Given that the amuserenr_el0 resets to an
    UNKNOWN value, setting the trap of EL0 accesses to EL1 is always
    attempted for safety and security considerations. Therefore firmware
    should still ensure accesses to AMU registers are not trapped in
    EL2/EL3 as this code cannot be bypassed if the CPU implements the
    Activity Monitors Unit.
    
    Signed-off-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Steve Capper <steve.capper@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index aca337d79d12..c5487806273f 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -430,6 +430,16 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 9000:
 	.endm
 
+/*
+ * reset_amuserenr_el0 - reset AMUSERENR_EL0 if AMUv1 present
+ */
+	.macro	reset_amuserenr_el0, tmpreg
+	mrs	\tmpreg, id_aa64pfr0_el1	// Check ID_AA64PFR0_EL1
+	ubfx	\tmpreg, \tmpreg, #ID_AA64PFR0_AMU_SHIFT, #4
+	cbz	\tmpreg, .Lskip_\@		// Skip if no AMU present
+	msr_s	SYS_AMUSERENR_EL0, xzr		// Disable AMU access from EL0
+.Lskip_\@:
+	.endm
 /*
  * copy_page - copy src to dest using temp registers t1-t8
  */

commit 25b92693a1b67a47b0c64a3410009d09e9658412
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Feb 13 12:14:52 2020 +0000

    arm64: mm: convert cpu_do_switch_mm() to C
    
    There's no reason that cpu_do_switch_mm() needs to be written as an
    assembly function, and having it as a C function would make it easier to
    maintain.
    
    This patch converts cpu_do_switch_mm() to C, removing code that this
    change makes redundant (e.g. the mmid macro). Since the header comment
    was stale and the prototype now implies all the necessary information,
    this comment is removed. The 'pgd_phys' argument is made a phys_addr_t
    to match the return type of virt_to_phys().
    
    At the same time, post_ttbr_update_workaround() is updated to use
    IS_ENABLED(), which allows the compiler to figure out it can elide calls
    for !CONFIG_CAVIUM_ERRATUM_27456 builds.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will@kernel.org>
    [catalin.marinas@arm.com: change comments from asm-style to C-style]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index aca337d79d12..af03001293c6 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -256,12 +256,6 @@ alternative_endif
 	ldr	\rd, [\rn, #VMA_VM_MM]
 	.endm
 
-/*
- * mmid - get context id from mm pointer (mm->context.id)
- */
-	.macro	mmid, rd, rn
-	ldr	\rd, [\rn, #MM_CONTEXT_ID]
-	.endm
 /*
  * read_ctr - read CTR_EL0. If the system has mismatched register fields,
  * provide the system wide safe value from arm64_ftr_reg_ctrel0.sys_val

commit c677124e631d97130e4ff7db6e10acdfb7a82321
Merge: c0e809e24480 afa70d941f66
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 28 10:07:09 2020 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "These were the main changes in this cycle:
    
       - More -rt motivated separation of CONFIG_PREEMPT and
         CONFIG_PREEMPTION.
    
       - Add more low level scheduling topology sanity checks and warnings
         to filter out nonsensical topologies that break scheduling.
    
       - Extend uclamp constraints to influence wakeup CPU placement
    
       - Make the RT scheduler more aware of asymmetric topologies and CPU
         capacities, via uclamp metrics, if CONFIG_UCLAMP_TASK=y
    
       - Make idle CPU selection more consistent
    
       - Various fixes, smaller cleanups, updates and enhancements - please
         see the git log for details"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (58 commits)
      sched/fair: Define sched_idle_cpu() only for SMP configurations
      sched/topology: Assert non-NUMA topology masks don't (partially) overlap
      idle: fix spelling mistake "iterrupts" -> "interrupts"
      sched/fair: Remove redundant call to cpufreq_update_util()
      sched/psi: create /proc/pressure and /proc/pressure/{io|memory|cpu} only when psi enabled
      sched/fair: Fix sgc->{min,max}_capacity calculation for SD_OVERLAP
      sched/fair: calculate delta runnable load only when it's needed
      sched/cputime: move rq parameter in irqtime_account_process_tick
      stop_machine: Make stop_cpus() static
      sched/debug: Reset watchdog on all CPUs while processing sysrq-t
      sched/core: Fix size of rq::uclamp initialization
      sched/uclamp: Fix a bug in propagating uclamp value in new cgroups
      sched/fair: Load balance aggressively for SCHED_IDLE CPUs
      sched/fair : Improve update_sd_pick_busiest for spare capacity case
      watchdog: Remove soft_lockup_hrtimer_cnt and related code
      sched/rt: Make RT capacity-aware
      sched/fair: Make EAS wakeup placement consider uclamp restrictions
      sched/fair: Make task_fits_capacity() consider uclamp restrictions
      sched/uclamp: Rename uclamp_util_with() into uclamp_rq_util_with()
      sched/uclamp: Make uclamp util helpers use and return UL values
      ...

commit aa246c056c43d41140c26706e519b498f056de8a
Merge: 4f6cdf296cc4 73d6890fe8ff
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:34:21 2020 +0000

    Merge branch 'for-next/asm-annotations' into for-next/core
    
    * for-next/asm-annotations: (6 commits)
      arm64: kernel: Correct annotation of end of el0_sync
      ...

commit ddb953f86cfb1ec450eff16a94230fc6d9901552
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 16 18:35:45 2020 +0000

    arm64: assembler: remove smp_dmb macro
    
    These days arm64 kernels are always SMP, and thus smp_dmb is an
    overly-long way of writing dmb. Naturally, no-one uses it.
    
    Remove the unused macro.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 5f8a2772baeb..995362adaac0 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -79,13 +79,6 @@
 9990:
 	.endm
 
-/*
- * SMP data memory barrier
- */
-	.macro	smp_dmb, opt
-	dmb	\opt
-	.endm
-
 /*
  * RAS Error Synchronization barrier
  */

commit 170b25fa6aab84dc9bdba88d4c64af5fbb789b6b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 16 18:35:44 2020 +0000

    arm64: assembler: remove inherit_daif macro
    
    We haven't needed the inherit_daif macro since commit:
    
      ed3768db588291dd ("arm64: entry: convert el1_sync to C")
    
    ... which converted all callers to C and the local_daif_inherit
    function.
    
    Remove the unused macro.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index b8cf7c85ffa2..5f8a2772baeb 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -40,12 +40,6 @@
 	msr	daif, \flags
 	.endm
 
-	/* Only on aarch64 pstate, PSR_D_BIT is different for aarch32 */
-	.macro	inherit_daif, pstate:req, tmp:req
-	and	\tmp, \pstate, #(PSR_D_BIT | PSR_A_BIT | PSR_I_BIT | PSR_F_BIT)
-	msr	daif, \tmp
-	.endm
-
 	/* IRQ is the lowest priority flag, unconditionally unmask the rest. */
 	.macro enable_da_f
 	msr	daifclr, #(8 | 4 | 1)

commit f7ef82c22fd76c300ae71c6dd19d9b03f4ab8253
Author: Mark Brown <broonie@kernel.org>
Date:   Wed Jan 8 16:57:52 2020 +0000

    arm64: asm: Remove ENDPIPROC()
    
    Now all the users have been removed delete the definition of ENDPIPROC()
    to ensure we don't acquire any new users.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 4a4258f17c86..e318d31bb979 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -461,18 +461,6 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	b.ne	9998b
 	.endm
 
-/*
- * Deprecated! Use SYM_FUNC_{START,START_WEAK,END}_PI instead.
- * Annotate a function as position independent, i.e., safe to be called before
- * the kernel virtual mapping is activated.
- */
-#define ENDPIPROC(x)			\
-	.globl	__pi_##x;		\
-	.type 	__pi_##x, %function;	\
-	.set	__pi_##x, x;		\
-	.size	__pi_##x, . - x;	\
-	ENDPROC(x)
-
 /*
  * Annotate a function as being unsuitable for kprobes.
  */

commit 35e61c77ef386555f3df1bc2057098c6997ca10b
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Jan 6 19:58:16 2020 +0000

    arm64: asm: Add new-style position independent function annotations
    
    As part of an effort to make the annotations in assembly code clearer and
    more consistent new macros have been introduced, including replacements
    for ENTRY() and ENDPROC().
    
    On arm64 we have ENDPIPROC(), a custom version of ENDPROC() which is
    used for code that will need to run in position independent environments
    like EFI, it creates an alias for the function with the prefix __pi_ and
    then emits the standard ENDPROC. Add new-style macros to replace this
    which expand to the standard SYM_FUNC_*() and SYM_FUNC_ALIAS_*(),
    resulting in the same object code. These are added in linkage.h for
    consistency with where the generic assembler code has its macros.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    [will: Rename 'WEAK' macro, use ';' instead of ASM_NL, deprecate ENDPIPROC]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index b8cf7c85ffa2..4a4258f17c86 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -462,6 +462,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	.endm
 
 /*
+ * Deprecated! Use SYM_FUNC_{START,START_WEAK,END}_PI instead.
  * Annotate a function as position independent, i.e., safe to be called before
  * the kernel virtual mapping is activated.
  */

commit 7ef858dad9fa6cabfe3b78997c3114cd641de6e3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 15 21:17:49 2019 +0200

    sched/rt, arm64: Use CONFIG_PREEMPTION
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by CONFIG_PREEMPT_RT.
    Both PREEMPT and PREEMPT_RT require the same functionality which today
    depends on CONFIG_PREEMPT.
    
    Switch the Kconfig dependency, entry code and preemption handling over
    to use CONFIG_PREEMPTION. Add PREEMPT_RT output in show_stack().
    
    [bigeasy: +traps.c, Kconfig]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: https://lore.kernel.org/r/20191015191821.11479-3-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index b8cf7c85ffa2..2cc0dd8bd9f7 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -699,8 +699,8 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * where <label> is optional, and marks the point where execution will resume
  * after a yield has been performed. If omitted, execution resumes right after
  * the endif_yield_neon invocation. Note that the entire sequence, including
- * the provided patchup code, will be omitted from the image if CONFIG_PREEMPT
- * is not defined.
+ * the provided patchup code, will be omitted from the image if
+ * CONFIG_PREEMPTION is not defined.
  *
  * As a convenience, in the case where no patchup code is required, the above
  * sequence may be abbreviated to
@@ -728,7 +728,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	.endm
 
 	.macro		if_will_cond_yield_neon
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 	get_current_task	x0
 	ldr		x0, [x0, #TSK_TI_PREEMPT]
 	sub		x0, x0, #PREEMPT_DISABLE_OFFSET

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit b6d00d47e81a49f6cf462518c10408f37a3e6785
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:22 2019 +0100

    arm64: mm: Introduce 52-bit Kernel VAs
    
    Most of the machinery is now in place to enable 52-bit kernel VAs that
    are detectable at boot time.
    
    This patch adds a Kconfig option for 52-bit user and kernel addresses
    and plumbs in the requisite CONFIG_ macros as well as sets TCR.T1SZ,
    physvirt_offset and vmemmap at early boot.
    
    To simplify things this patch also removes the 52-bit user/48-bit kernel
    kconfig option.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index ede368bafa2c..c066fc4976cd 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -349,6 +349,13 @@ alternative_endif
 	bfi	\valreg, \t0sz, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
 	.endm
 
+/*
+ * tcr_set_t1sz - update TCR.T1SZ
+ */
+	.macro	tcr_set_t1sz, valreg, t1sz
+	bfi	\valreg, \t1sz, #TCR_T1SZ_OFFSET, #TCR_TxSZ_WIDTH
+	.endm
+
 /*
  * tcr_compute_pa_size - set TCR.(I)PS to the highest supported
  * ID_AA64MMFR0_EL1.PARange value
@@ -539,10 +546,6 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * 	ttbr: Value of ttbr to set, modified.
  */
 	.macro	offset_ttbr1, ttbr, tmp
-#ifdef CONFIG_ARM64_USER_VA_BITS_52
-	orr	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
-#endif
-
 #ifdef CONFIG_ARM64_VA_BITS_52
 	mrs_s	\tmp, SYS_ID_AA64MMFR2_EL1
 	and	\tmp, \tmp, #(0xf << ID_AA64MMFR2_LVA_SHIFT)
@@ -558,7 +561,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * to be nop'ed out when dealing with 52-bit kernel VAs.
  */
 	.macro	restore_ttbr1, ttbr
-#if defined(CONFIG_ARM64_USER_VA_BITS_52) || defined(CONFIG_ARM64_VA_BITS_52)
+#ifdef CONFIG_ARM64_VA_BITS_52
 	bic	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
 #endif
 	.endm

commit c812026c54cfaec23fa1d78cdbfd0e56e787470a
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:19 2019 +0100

    arm64: mm: Logic to make offset_ttbr1 conditional
    
    When running with a 52-bit userspace VA and a 48-bit kernel VA we offset
    ttbr1_el1 to allow the kernel pagetables with a 52-bit PTRS_PER_PGD to
    be used for both userspace and kernel.
    
    Moving on to a 52-bit kernel VA we no longer require this offset to
    ttbr1_el1 should we be running on a system with HW support for 52-bit
    VAs.
    
    This patch introduces conditional logic to offset_ttbr1 to query
    SYS_ID_AA64MMFR2_EL1 whenever 52-bit VAs are selected. If there is HW
    support for 52-bit VAs then the ttbr1 offset is skipped.
    
    We choose to read a system register rather than vabits_actual because
    offset_ttbr1 can be called in places where the kernel data is not
    actually mapped.
    
    Calls to offset_ttbr1 appear to be made from rarely called code paths so
    this extra logic is not expected to adversely affect performance.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index e3a15c751b13..ede368bafa2c 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -538,9 +538,17 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * In future this may be nop'ed out when dealing with 52-bit kernel VAs.
  * 	ttbr: Value of ttbr to set, modified.
  */
-	.macro	offset_ttbr1, ttbr
+	.macro	offset_ttbr1, ttbr, tmp
 #ifdef CONFIG_ARM64_USER_VA_BITS_52
 	orr	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
+#endif
+
+#ifdef CONFIG_ARM64_VA_BITS_52
+	mrs_s	\tmp, SYS_ID_AA64MMFR2_EL1
+	and	\tmp, \tmp, #(0xf << ID_AA64MMFR2_LVA_SHIFT)
+	cbnz	\tmp, .Lskipoffs_\@
+	orr	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
+.Lskipoffs_\@ :
 #endif
 	.endm
 
@@ -550,7 +558,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * to be nop'ed out when dealing with 52-bit kernel VAs.
  */
 	.macro	restore_ttbr1, ttbr
-#ifdef CONFIG_ARM64_USER_VA_BITS_52
+#if defined(CONFIG_ARM64_USER_VA_BITS_52) || defined(CONFIG_ARM64_VA_BITS_52)
 	bic	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
 #endif
 	.endm

commit c87857945b0e61c46222798b56fb1b0f4868088b
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jul 4 14:44:04 2019 +0100

    arm64: Remove unused assembly macro
    
    As of commit 4141c857fd09dbed480f021b3eece4f46c653161 ("arm64: convert
    raw syscall invocation to C"), moving syscall handling from assembly to
    C, the macro mask_nospec64 is no longer referenced.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index e3a15c751b13..9f7395a1177f 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -123,17 +123,6 @@ alternative_else
 alternative_endif
 	.endm
 
-/*
- * Sanitise a 64-bit bounded index wrt speculation, returning zero if out
- * of bounds.
- */
-	.macro	mask_nospec64, idx, limit, tmp
-	sub	\tmp, \idx, \limit
-	bic	\tmp, \tmp, \idx
-	and	\idx, \idx, \tmp, asr #63
-	csdb
-	.endm
-
 /*
  * NOP sequence
  */

commit 2b68a2a963a157f024c67c0697b16f5f792c8a35
Author: James Morse <james.morse@arm.com>
Date:   Tue Jun 18 16:17:33 2019 +0100

    arm64: assembler: Switch ESB-instruction with a vanilla nop if !ARM64_HAS_RAS
    
    The ESB-instruction is a nop on CPUs that don't implement the RAS
    extensions. This lets us use it in places like the vectors without
    having to use alternatives.
    
    If someone disables CONFIG_ARM64_RAS_EXTN, this instruction still has
    its RAS extensions behaviour, but we no longer read DISR_EL1 as this
    register does depend on alternatives.
    
    This could go wrong if we want to synchronize an SError from a KVM
    guest. On a CPU that has the RAS extensions, but the KConfig option
    was disabled, we consume the pending SError with no chance of ever
    reading it.
    
    Hide the ESB-instruction behind the CONFIG_ARM64_RAS_EXTN option,
    outputting a regular nop if the feature has been disabled.
    
    Reported-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 570d195a184d..e3a15c751b13 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -96,7 +96,11 @@
  * RAS Error Synchronization barrier
  */
 	.macro  esb
+#ifdef CONFIG_ARM64_RAS_EXTN
 	hint    #16
+#else
+	nop
+#endif
 	.endm
 
 /*

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 92b6b7cf67dd..570d195a184d 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -1,20 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Based on arch/arm/include/asm/assembler.h, arch/arm/mm/proc-macros.S
  *
  * Copyright (C) 1996-2000 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASSEMBLY__
 #error "Only include this from assembly code"

commit 0e4add4ae79e4466ae2a7e7772dfbe39fb28787a
Author: Hillf Danton <hdanton@sina.com>
Date:   Tue May 14 16:34:19 2019 +0800

    arm64: assembler: Update comment above cond_yield_neon() macro
    
    Since commit 7faa313f05ca ("arm64: preempt: Fix big-endian when checking
    preempt count in assembly") both the preempt count and the 'need_resched'
    flag are checked as part of a single 64-bit load in cond_yield_neon(),
    so update the stale comment to reflect reality.
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Dave Martin <Dave.Martin@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Hillf Danton <hdanton@sina.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 039fbd822ec6..92b6b7cf67dd 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -718,12 +718,11 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * the output section, any use of such directives is undefined.
  *
  * The yield itself consists of the following:
- * - Check whether the preempt count is exactly 1, in which case disabling
- *   preemption once will make the task preemptible. If this is not the case,
- *   yielding is pointless.
- * - Check whether TIF_NEED_RESCHED is set, and if so, disable and re-enable
- *   kernel mode NEON (which will trigger a reschedule), and branch to the
- *   yield fixup code.
+ * - Check whether the preempt count is exactly 1 and a reschedule is also
+ *   needed. If so, calling of preempt_enable() in kernel_neon_end() will
+ *   trigger a reschedule. If it is not the case, yielding is pointless.
+ * - Disable and re-enable kernel mode NEON, and branch to the yield fixup
+ *   code.
  *
  * This macro sequence may clobber all CPU state that is not guaranteed by the
  * AAPCS to be preserved across an ordinary function call.

commit 04a1438e5660ae44ecebd6c870fbcc140dd883a7
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 10:52:44 2019 +0100

    arm64: add CVADP support to the cache maintenance helper
    
    Allow users of dcache_by_line_op to specify cvadp as an op.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 621212196871..039fbd822ec6 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -407,10 +407,14 @@ alternative_endif
 	.ifc	\op, cvap
 	sys	3, c7, c12, 1, \kaddr	// dc cvap
 	.else
+	.ifc	\op, cvadp
+	sys	3, c7, c13, 1, \kaddr	// dc cvadp
+	.else
 	dc	\op, \kaddr
 	.endif
 	.endif
 	.endif
+	.endif
 	add	\kaddr, \kaddr, \tmp1
 	cmp	\kaddr, \size
 	b.lo	9998b

commit f6e564354a01dd943ba92c514ddf386080d7baa4
Author: Alexandru Elisei <alexandru.elisei@arm.com>
Date:   Fri Apr 5 11:20:12 2019 +0100

    arm64: Use defines instead of magic numbers
    
    Following assembly code is not trivial; make it slightly easier to read by
    replacing some of the magic numbers with the defines which are already
    present in sysreg.h.
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Alexandru Elisei <alexandru.elisei@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index c5308d01e228..621212196871 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -442,8 +442,8 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * reset_pmuserenr_el0 - reset PMUSERENR_EL0 if PMUv3 present
  */
 	.macro	reset_pmuserenr_el0, tmpreg
-	mrs	\tmpreg, id_aa64dfr0_el1	// Check ID_AA64DFR0_EL1 PMUVer
-	sbfx	\tmpreg, \tmpreg, #8, #4
+	mrs	\tmpreg, id_aa64dfr0_el1
+	sbfx	\tmpreg, \tmpreg, #ID_AA64DFR0_PMUVER_SHIFT, #4
 	cmp	\tmpreg, #1			// Skip if no PMU present
 	b.lt	9000f
 	msr	pmuserenr_el0, xzr		// Disable PMU access from EL0

commit 3e32131abc311a5cb9fddecc72cbd0b95ffcc10d
Author: Zhang Lei <zhang.lei@jp.fujitsu.com>
Date:   Tue Feb 26 18:43:41 2019 +0000

    arm64: Add workaround for Fujitsu A64FX erratum 010001
    
    On the Fujitsu-A64FX cores ver(1.0, 1.1), memory access may cause
    an undefined fault (Data abort, DFSC=0b111111). This fault occurs under
    a specific hardware condition when a load/store instruction performs an
    address translation. Any load/store instruction, except non-fault access
    including Armv8 and SVE might cause this undefined fault.
    
    The TCR_ELx.NFD1 bit is used by the kernel when CONFIG_RANDOMIZE_BASE
    is enabled to mitigate timing attacks against KASLR where the kernel
    address space could be probed using the FFR and suppressed fault on
    SVE loads.
    
    Since this erratum causes spurious exceptions, which may corrupt
    the exception registers, we clear the TCR_ELx.NFDx=1 bits when
    booting on an affected CPU.
    
    Signed-off-by: Zhang Lei <zhang.lei@jp.fujitsu.com>
    [Generated MIDR value/mask for __cpu_setup(), removed spurious-fault handler
     and always disabled the NFDx bits on affected CPUs]
    Signed-off-by: James Morse <james.morse@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 9c5c876a9ff2..c5308d01e228 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -27,6 +27,7 @@
 
 #include <asm/asm-offsets.h>
 #include <asm/cpufeature.h>
+#include <asm/cputype.h>
 #include <asm/debug-monitors.h>
 #include <asm/page.h>
 #include <asm/pgtable-hwdef.h>
@@ -596,6 +597,25 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 #endif
 	.endm
 
+/*
+ * tcr_clear_errata_bits - Clear TCR bits that trigger an errata on this CPU.
+ */
+	.macro	tcr_clear_errata_bits, tcr, tmp1, tmp2
+#ifdef CONFIG_FUJITSU_ERRATUM_010001
+	mrs	\tmp1, midr_el1
+
+	mov_q	\tmp2, MIDR_FUJITSU_ERRATUM_010001_MASK
+	and	\tmp1, \tmp1, \tmp2
+	mov_q	\tmp2, MIDR_FUJITSU_ERRATUM_010001
+	cmp	\tmp1, \tmp2
+	b.ne	10f
+
+	mov_q	\tmp2, TCR_CLEAR_FUJITSU_ERRATUM_010001
+	bic	\tcr, \tcr, \tmp2
+10:
+#endif /* CONFIG_FUJITSU_ERRATUM_010001 */
+	.endm
+
 /**
  * Errata workaround prior to disable MMU. Insert an ISB immediately prior
  * to executing the MSR that will change SCTLR_ELn[M] from a value of 1 to 0.

commit 4caf8758b60b6f7f9773fd1d265cb5a7cf935c97
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Fri Feb 22 09:32:50 2019 +0000

    arm64: Rename get_thread_info()
    
    The assembly macro get_thread_info() actually returns a task_struct and is
    analogous to the current/get_current macro/function.
    
    While it could be argued that thread_info sits at the start of
    task_struct and the intention could have been to return a thread_info,
    instances of loads from/stores to the address obtained from
    get_thread_info() use offsets that are generated with
    offsetof(struct task_struct, [...]).
    
    Rename get_thread_info() to state it returns a task_struct.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 7acf2436b578..9c5c876a9ff2 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -528,9 +528,9 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	.endm
 
 /*
- * Return the current thread_info.
+ * Return the current task_struct.
  */
-	.macro	get_thread_info, rd
+	.macro	get_current_task, rd
 	mrs	\rd, sp_el0
 	.endm
 
@@ -713,7 +713,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 
 	.macro		if_will_cond_yield_neon
 #ifdef CONFIG_PREEMPT
-	get_thread_info	x0
+	get_current_task	x0
 	ldr		x0, [x0, #TSK_TI_PREEMPT]
 	sub		x0, x0, #PREEMPT_DISABLE_OFFSET
 	cbz		x0, .Lyield_\@

commit a82785a953e03444fe38616aed4d27b01da79a97
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:40 2019 +0000

    arm64: Remove unused daif related functions/macros
    
    There are some helpers to modify PSR.[DAIF] bits that are not referenced
    anywhere. The less these bits are available outside of local_irq_*
    functions the better.
    
    Get rid of those unused helpers.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 4feb6119c3c9..7acf2436b578 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -62,16 +62,8 @@
 	.endm
 
 /*
- * Enable and disable interrupts.
+ * Save/restore interrupts.
  */
-	.macro	disable_irq
-	msr	daifset, #2
-	.endm
-
-	.macro	enable_irq
-	msr	daifclr, #2
-	.endm
-
 	.macro	save_and_disable_irq, flags
 	mrs	\flags, daif
 	msr	daifset, #2

commit 7faa313f05cad184e8b17750f0cbe5216ac6debb
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 11 13:41:32 2018 +0000

    arm64: preempt: Fix big-endian when checking preempt count in assembly
    
    Commit 396244692232 ("arm64: preempt: Provide our own implementation of
    asm/preempt.h") extended the preempt count field in struct thread_info
    to 64 bits, so that it consists of a 32-bit count plus a 32-bit flag
    indicating whether or not the current task needs rescheduling.
    
    Whilst the asm-offsets definition of TSK_TI_PREEMPT was updated to point
    to this new field, the assembly usage was left untouched meaning that a
    32-bit load from TSK_TI_PREEMPT on a big-endian machine actually returns
    the reschedule flag instead of the count.
    
    Whilst we could fix this by pointing TSK_TI_PREEMPT at the count field,
    we're actually better off reworking the two assembly users so that they
    operate on the whole 64-bit value in favour of inspecting the thread
    flags separately in order to determine whether a reschedule is needed.
    
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reported-by: "kernelci.org bot" <bot@kernelci.org>
    Tested-by: Kevin Hilman <khilman@baylibre.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index ce985f13dce5..4feb6119c3c9 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -722,11 +722,9 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	.macro		if_will_cond_yield_neon
 #ifdef CONFIG_PREEMPT
 	get_thread_info	x0
-	ldr		w1, [x0, #TSK_TI_PREEMPT]
-	ldr		x0, [x0, #TSK_TI_FLAGS]
-	cmp		w1, #PREEMPT_DISABLE_OFFSET
-	csel		x0, x0, xzr, eq
-	tbnz		x0, #TIF_NEED_RESCHED, .Lyield_\@	// needs rescheduling?
+	ldr		x0, [x0, #TSK_TI_PREEMPT]
+	sub		x0, x0, #PREEMPT_DISABLE_OFFSET
+	cbz		x0, .Lyield_\@
 	/* fall through to endif_yield_neon */
 	.subsection	1
 .Lyield_\@ :

commit 68d23da4373aba76f5300017c4746440f276698e
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 14:15:15 2018 +0000

    arm64: Kconfig: Re-jig CONFIG options for 52-bit VA
    
    Enabling 52-bit VAs for userspace is pretty confusing, since it requires
    you to select "48-bit" virtual addressing in the Kconfig.
    
    Rework the logic so that 52-bit user virtual addressing is advertised in
    the "Virtual address space size" choice, along with some help text to
    describe its interaction with Pointer Authentication. The EXPERT-only
    option to force all user mappings to the 52-bit range is then made
    available immediately below the VA size selection.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 122d91d4097a..ce985f13dce5 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -549,7 +549,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * 	ttbr: Value of ttbr to set, modified.
  */
 	.macro	offset_ttbr1, ttbr
-#ifdef CONFIG_ARM64_52BIT_VA
+#ifdef CONFIG_ARM64_USER_VA_BITS_52
 	orr	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
 #endif
 	.endm
@@ -560,7 +560,7 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
  * to be nop'ed out when dealing with 52-bit kernel VAs.
  */
 	.macro	restore_ttbr1, ttbr
-#ifdef CONFIG_ARM64_52BIT_VA
+#ifdef CONFIG_ARM64_USER_VA_BITS_52
 	bic	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
 #endif
 	.endm

commit 67e7fdfcc6824a4f768d76d89377b33baad58fad
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:41 2018 +0000

    arm64: mm: introduce 52-bit userspace support
    
    On arm64 there is optional support for a 52-bit virtual address space.
    To exploit this one has to be running with a 64KB page size and be
    running on hardware that supports this.
    
    For an arm64 kernel supporting a 48 bit VA with a 64KB page size,
    some changes are needed to support a 52-bit userspace:
     * TCR_EL1.T0SZ needs to be 12 instead of 16,
     * TASK_SIZE needs to reflect the new size.
    
    This patch implements the above when the support for 52-bit VAs is
    detected at early boot time.
    
    On arm64 userspace addresses translation is controlled by TTBR0_EL1. As
    well as userspace, TTBR0_EL1 controls:
     * The identity mapping,
     * EFI runtime code.
    
    It is possible to run a kernel with an identity mapping that has a
    larger VA size than userspace (and for this case __cpu_set_tcr_t0sz()
    would set TCR_EL1.T0SZ as appropriate). However, when the conditions for
    52-bit userspace are met; it is possible to keep TCR_EL1.T0SZ fixed at
    12. Thus in this patch, the TCR_EL1.T0SZ size changing logic is
    disabled.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index ba609e0439e8..122d91d4097a 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -357,11 +357,10 @@ alternative_endif
 	.endm
 
 /*
- * tcr_set_idmap_t0sz - update TCR.T0SZ so that we can load the ID map
+ * tcr_set_t0sz - update TCR.T0SZ so that we can load the ID map
  */
-	.macro	tcr_set_idmap_t0sz, valreg, tmpreg
-	ldr_l	\tmpreg, idmap_t0sz
-	bfi	\valreg, \tmpreg, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
+	.macro	tcr_set_t0sz, valreg, t0sz
+	bfi	\valreg, \t0sz, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
 	.endm
 
 /*

commit e842dfb5a2d3b4c43766508ef89a4eb67471d53a
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:39 2018 +0000

    arm64: mm: Offset TTBR1 to allow 52-bit PTRS_PER_PGD
    
    Enabling 52-bit VAs on arm64 requires that the PGD table expands from 64
    entries (for the 48-bit case) to 1024 entries. This quantity,
    PTRS_PER_PGD is used as follows to compute which PGD entry corresponds
    to a given virtual address, addr:
    
    pgd_index(addr) -> (addr >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1)
    
    Userspace addresses are prefixed by 0's, so for a 48-bit userspace
    address, uva, the following is true:
    (uva >> PGDIR_SHIFT) & (1024 - 1) == (uva >> PGDIR_SHIFT) & (64 - 1)
    
    In other words, a 48-bit userspace address will have the same pgd_index
    when using PTRS_PER_PGD = 64 and 1024.
    
    Kernel addresses are prefixed by 1's so, given a 48-bit kernel address,
    kva, we have the following inequality:
    (kva >> PGDIR_SHIFT) & (1024 - 1) != (kva >> PGDIR_SHIFT) & (64 - 1)
    
    In other words a 48-bit kernel virtual address will have a different
    pgd_index when using PTRS_PER_PGD = 64 and 1024.
    
    If, however, we note that:
    kva = 0xFFFF << 48 + lower (where lower[63:48] == 0b)
    and, PGDIR_SHIFT = 42 (as we are dealing with 64KB PAGE_SIZE)
    
    We can consider:
    (kva >> PGDIR_SHIFT) & (1024 - 1) - (kva >> PGDIR_SHIFT) & (64 - 1)
     = (0xFFFF << 6) & 0x3FF - (0xFFFF << 6) & 0x3F // "lower" cancels out
     = 0x3C0
    
    In other words, one can switch PTRS_PER_PGD to the 52-bit value globally
    provided that they increment ttbr1_el1 by 0x3C0 * 8 = 0x1E00 bytes when
    running with 48-bit kernel VAs (TCR_EL1.T1SZ = 16).
    
    For kernel configuration where 52-bit userspace VAs are possible, this
    patch offsets ttbr1_el1 and sets PTRS_PER_PGD corresponding to the
    52-bit value.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    [will: added comment to TTBR1_BADDR_4852_OFFSET calculation]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index d103c3ee7335..ba609e0439e8 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -543,6 +543,29 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	mrs	\rd, sp_el0
 	.endm
 
+/*
+ * Offset ttbr1 to allow for 48-bit kernel VAs set with 52-bit PTRS_PER_PGD.
+ * orr is used as it can cover the immediate value (and is idempotent).
+ * In future this may be nop'ed out when dealing with 52-bit kernel VAs.
+ * 	ttbr: Value of ttbr to set, modified.
+ */
+	.macro	offset_ttbr1, ttbr
+#ifdef CONFIG_ARM64_52BIT_VA
+	orr	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
+#endif
+	.endm
+
+/*
+ * Perform the reverse of offset_ttbr1.
+ * bic is used as it can cover the immediate value and, in future, won't need
+ * to be nop'ed out when dealing with 52-bit kernel VAs.
+ */
+	.macro	restore_ttbr1, ttbr
+#ifdef CONFIG_ARM64_52BIT_VA
+	bic	\ttbr, \ttbr, #TTBR1_BADDR_4852_OFFSET
+#endif
+	.endm
+
 /*
  * Arrange a physical address in a TTBR register, taking care of 52-bit
  * addresses.

commit 33309ecda0070506c49182530abe7728850ebe78
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 13:39:48 2018 +0000

    arm64: Fix minor issues with the dcache_by_line_op macro
    
    The dcache_by_line_op macro suffers from a couple of small problems:
    
    First, the GAS directives that are currently being used rely on
    assembler behavior that is not documented, and probably not guaranteed
    to produce the correct behavior going forward. As a result, we end up
    with some undefined symbols in cache.o:
    
    $ nm arch/arm64/mm/cache.o
             ...
             U civac
             ...
             U cvac
             U cvap
             U cvau
    
    This is due to the fact that the comparisons used to select the
    operation type in the dcache_by_line_op macro are comparing symbols
    not strings, and even though it seems that GAS is doing the right
    thing here (undefined symbols by the same name are equal to each
    other), it seems unwise to rely on this.
    
    Second, when patching in a DC CVAP instruction on CPUs that support it,
    the fallback path consists of a DC CVAU instruction which may be
    affected by CPU errata that require ARM64_WORKAROUND_CLEAN_CACHE.
    
    Solve these issues by unrolling the various maintenance routines and
    using the conditional directives that are documented as operating on
    strings. To avoid the complexity of nested alternatives, we move the
    DC CVAP patching to __clean_dcache_area_pop, falling back to a branch
    to __clean_dcache_area_poc if DCPOP is not supported by the CPU.
    
    Reported-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Suggested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index d8b9d3a427d1..d103c3ee7335 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -392,27 +392,33 @@ alternative_endif
  * 	size:		size of the region
  * 	Corrupts:	kaddr, size, tmp1, tmp2
  */
+	.macro __dcache_op_workaround_clean_cache, op, kaddr
+alternative_if_not ARM64_WORKAROUND_CLEAN_CACHE
+	dc	\op, \kaddr
+alternative_else
+	dc	civac, \kaddr
+alternative_endif
+	.endm
+
 	.macro dcache_by_line_op op, domain, kaddr, size, tmp1, tmp2
 	dcache_line_size \tmp1, \tmp2
 	add	\size, \kaddr, \size
 	sub	\tmp2, \tmp1, #1
 	bic	\kaddr, \kaddr, \tmp2
 9998:
-	.if	(\op == cvau || \op == cvac)
-alternative_if_not ARM64_WORKAROUND_CLEAN_CACHE
-	dc	\op, \kaddr
-alternative_else
-	dc	civac, \kaddr
-alternative_endif
-	.elseif	(\op == cvap)
-alternative_if ARM64_HAS_DCPOP
-	sys 3, c7, c12, 1, \kaddr	// dc cvap
-alternative_else
-	dc	cvac, \kaddr
-alternative_endif
+	.ifc	\op, cvau
+	__dcache_op_workaround_clean_cache \op, \kaddr
+	.else
+	.ifc	\op, cvac
+	__dcache_op_workaround_clean_cache \op, \kaddr
+	.else
+	.ifc	\op, cvap
+	sys	3, c7, c12, 1, \kaddr	// dc cvap
 	.else
 	dc	\op, \kaddr
 	.endif
+	.endif
+	.endif
 	add	\kaddr, \kaddr, \tmp1
 	cmp	\kaddr, \size
 	b.lo	9998b

commit 386b3c7bdafcc67aaf4168e9627b381370b6538a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:08:16 2018 +0000

    arm64: add EXPORT_SYMBOL_NOKASAN()
    
    So that we can export symbols directly from assembly files, let's make
    use of the generic <asm/export.h>. We have a few symbols that we'll want
    to conditionally export for !KASAN kernel builds, so we add a helper for
    that in <asm/assembler.h>.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 953208267252..d8b9d3a427d1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -23,6 +23,8 @@
 #ifndef __ASM_ASSEMBLER_H
 #define __ASM_ASSEMBLER_H
 
+#include <asm-generic/export.h>
+
 #include <asm/asm-offsets.h>
 #include <asm/cpufeature.h>
 #include <asm/debug-monitors.h>
@@ -490,6 +492,13 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 #else
 #define NOKPROBE(x)
 #endif
+
+#ifdef CONFIG_KASAN
+#define EXPORT_SYMBOL_NOKASAN(name)
+#else
+#define EXPORT_SYMBOL_NOKASAN(name)	EXPORT_SYMBOL(name)
+#endif
+
 	/*
 	 * Emit a 64-bit absolute little endian symbol reference in a way that
 	 * ensures that it will be resolved at build time, even when building a

commit bd4fb6d270bc423a9a4098108784f7f9254c4e6d
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jun 14 11:21:34 2018 +0100

    arm64: Add support for SB barrier and patch in over DSB; ISB sequences
    
    We currently use a DSB; ISB sequence to inhibit speculation in set_fs().
    Whilst this works for current CPUs, future CPUs may implement a new SB
    barrier instruction which acts as an architected speculation barrier.
    
    On CPUs that support it, patch in an SB; NOP sequence over the DSB; ISB
    sequence and advertise the presence of the new instruction to userspace.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 6142402c2eb4..953208267252 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -122,6 +122,19 @@
 	hint	#20
 	.endm
 
+/*
+ * Speculation barrier
+ */
+	.macro	sb
+alternative_if_not ARM64_HAS_SB
+	dsb	nsh
+	isb
+alternative_else
+	SB_BARRIER_INSN
+	nop
+alternative_endif
+	.endm
+
 /*
  * Sanitise a 64-bit bounded index wrt speculation, returning zero if out
  * of bounds.

commit 880f7cc47265e7b195781dfa9a0cd62ef78304e3
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Sep 19 11:41:21 2018 +0100

    arm64: cpu_errata: Remove ARM64_MISMATCHED_CACHE_LINE_SIZE
    
    There's no need to treat mismatched cache-line sizes reported by CTR_EL0
    differently to any other mismatched fields that we treat as "STRICT" in
    the cpufeature code. In both cases we need to trap and emulate EL0
    accesses to the register, so drop ARM64_MISMATCHED_CACHE_LINE_SIZE and
    rely on ARM64_MISMATCHED_CACHE_TYPE instead.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: move ARM64_HAS_CNP in the empty cpucaps.h slot]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 0bcc98dbba56..6142402c2eb4 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -286,12 +286,11 @@ alternative_endif
 	ldr	\rd, [\rn, #MM_CONTEXT_ID]
 	.endm
 /*
- * read_ctr - read CTR_EL0. If the system has mismatched
- * cache line sizes, provide the system wide safe value
- * from arm64_ftr_reg_ctrel0.sys_val
+ * read_ctr - read CTR_EL0. If the system has mismatched register fields,
+ * provide the system wide safe value from arm64_ftr_reg_ctrel0.sys_val
  */
 	.macro	read_ctr, reg
-alternative_if_not ARM64_MISMATCHED_CACHE_LINE_SIZE
+alternative_if_not ARM64_MISMATCHED_CACHE_TYPE
 	mrs	\reg, ctr_el0			// read CTR
 	nop
 alternative_else

commit 24534b3511828c66215fdf1533d77a7bf2e1fdb2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Mar 29 15:13:23 2018 +0200

    arm64: assembler: add macros to conditionally yield the NEON under PREEMPT
    
    Add support macros to conditionally yield the NEON (and thus the CPU)
    that may be called from the assembler code.
    
    In some cases, yielding the NEON involves saving and restoring a non
    trivial amount of context (especially in the CRC folding algorithms),
    and so the macro is split into three, and the code in between is only
    executed when the yield path is taken, allowing the context to be preserved.
    The third macro takes an optional label argument that marks the resume
    path after a yield has been performed.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index fe2ff3efe1f0..0bcc98dbba56 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -628,4 +628,77 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 	.endif
 	.endm
 
+/*
+ * Check whether to yield to another runnable task from kernel mode NEON code
+ * (which runs with preemption disabled).
+ *
+ * if_will_cond_yield_neon
+ *        // pre-yield patchup code
+ * do_cond_yield_neon
+ *        // post-yield patchup code
+ * endif_yield_neon    <label>
+ *
+ * where <label> is optional, and marks the point where execution will resume
+ * after a yield has been performed. If omitted, execution resumes right after
+ * the endif_yield_neon invocation. Note that the entire sequence, including
+ * the provided patchup code, will be omitted from the image if CONFIG_PREEMPT
+ * is not defined.
+ *
+ * As a convenience, in the case where no patchup code is required, the above
+ * sequence may be abbreviated to
+ *
+ * cond_yield_neon <label>
+ *
+ * Note that the patchup code does not support assembler directives that change
+ * the output section, any use of such directives is undefined.
+ *
+ * The yield itself consists of the following:
+ * - Check whether the preempt count is exactly 1, in which case disabling
+ *   preemption once will make the task preemptible. If this is not the case,
+ *   yielding is pointless.
+ * - Check whether TIF_NEED_RESCHED is set, and if so, disable and re-enable
+ *   kernel mode NEON (which will trigger a reschedule), and branch to the
+ *   yield fixup code.
+ *
+ * This macro sequence may clobber all CPU state that is not guaranteed by the
+ * AAPCS to be preserved across an ordinary function call.
+ */
+
+	.macro		cond_yield_neon, lbl
+	if_will_cond_yield_neon
+	do_cond_yield_neon
+	endif_yield_neon	\lbl
+	.endm
+
+	.macro		if_will_cond_yield_neon
+#ifdef CONFIG_PREEMPT
+	get_thread_info	x0
+	ldr		w1, [x0, #TSK_TI_PREEMPT]
+	ldr		x0, [x0, #TSK_TI_FLAGS]
+	cmp		w1, #PREEMPT_DISABLE_OFFSET
+	csel		x0, x0, xzr, eq
+	tbnz		x0, #TIF_NEED_RESCHED, .Lyield_\@	// needs rescheduling?
+	/* fall through to endif_yield_neon */
+	.subsection	1
+.Lyield_\@ :
+#else
+	.section	".discard.cond_yield_neon", "ax"
+#endif
+	.endm
+
+	.macro		do_cond_yield_neon
+	bl		kernel_neon_end
+	bl		kernel_neon_begin
+	.endm
+
+	.macro		endif_yield_neon, lbl
+	.ifnb		\lbl
+	b		\lbl
+	.else
+	b		.Lyield_out_\@
+	.endif
+	.previous
+.Lyield_out_\@ :
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 0f468e221c3ae89d2fbe611a1a69ee554188519a
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Mar 29 15:13:22 2018 +0200

    arm64: assembler: add utility macros to push/pop stack frames
    
    We are going to add code to all the NEON crypto routines that will
    turn them into non-leaf functions, so we need to manage the stack
    frames. To make this less tedious and error prone, add some macros
    that take the number of callee saved registers to preserve and the
    extra size to allocate in the stack frame (for locals) and emit
    the ldp/stp sequences.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 053d83e8db6f..fe2ff3efe1f0 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -565,4 +565,67 @@ USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
 #endif
 	.endm
 
+	/*
+	 * frame_push - Push @regcount callee saved registers to the stack,
+	 *              starting at x19, as well as x29/x30, and set x29 to
+	 *              the new value of sp. Add @extra bytes of stack space
+	 *              for locals.
+	 */
+	.macro		frame_push, regcount:req, extra
+	__frame		st, \regcount, \extra
+	.endm
+
+	/*
+	 * frame_pop  - Pop the callee saved registers from the stack that were
+	 *              pushed in the most recent call to frame_push, as well
+	 *              as x29/x30 and any extra stack space that may have been
+	 *              allocated.
+	 */
+	.macro		frame_pop
+	__frame		ld
+	.endm
+
+	.macro		__frame_regs, reg1, reg2, op, num
+	.if		.Lframe_regcount == \num
+	\op\()r		\reg1, [sp, #(\num + 1) * 8]
+	.elseif		.Lframe_regcount > \num
+	\op\()p		\reg1, \reg2, [sp, #(\num + 1) * 8]
+	.endif
+	.endm
+
+	.macro		__frame, op, regcount, extra=0
+	.ifc		\op, st
+	.if		(\regcount) < 0 || (\regcount) > 10
+	.error		"regcount should be in the range [0 ... 10]"
+	.endif
+	.if		((\extra) % 16) != 0
+	.error		"extra should be a multiple of 16 bytes"
+	.endif
+	.ifdef		.Lframe_regcount
+	.if		.Lframe_regcount != -1
+	.error		"frame_push/frame_pop may not be nested"
+	.endif
+	.endif
+	.set		.Lframe_regcount, \regcount
+	.set		.Lframe_extra, \extra
+	.set		.Lframe_local_offset, ((\regcount + 3) / 2) * 16
+	stp		x29, x30, [sp, #-.Lframe_local_offset - .Lframe_extra]!
+	mov		x29, sp
+	.endif
+
+	__frame_regs	x19, x20, \op, 1
+	__frame_regs	x21, x22, \op, 3
+	__frame_regs	x23, x24, \op, 5
+	__frame_regs	x25, x26, \op, 7
+	__frame_regs	x27, x28, \op, 9
+
+	.ifc		\op, ld
+	.if		.Lframe_regcount == -1
+	.error		"frame_push/frame_pop may not be nested"
+	.endif
+	ldp		x29, x30, [sp], #.Lframe_local_offset + .Lframe_extra
+	.set		.Lframe_regcount, -1
+	.endif
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 350e1dad0dd8c55750f9d4fa6b19cea1a0037ace
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Mar 10 14:59:29 2018 +0000

    arm64: asm: drop special versions of adr_l/ldr_l/str_l for modules
    
    Now that we started keeping modules within 4 GB of the core kernel
    in all cases, we no longer need to special case the adr_l/ldr_l/str_l
    macros for modules to deal with them being loaded farther away.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 3c78835bba94..053d83e8db6f 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -202,25 +202,15 @@ lr	.req	x30		// link register
 
 /*
  * Pseudo-ops for PC-relative adr/ldr/str <reg>, <symbol> where
- * <symbol> is within the range +/- 4 GB of the PC when running
- * in core kernel context. In module context, a movz/movk sequence
- * is used, since modules may be loaded far away from the kernel
- * when KASLR is in effect.
+ * <symbol> is within the range +/- 4 GB of the PC.
  */
 	/*
 	 * @dst: destination register (64 bit wide)
 	 * @sym: name of the symbol
 	 */
 	.macro	adr_l, dst, sym
-#ifndef MODULE
 	adrp	\dst, \sym
 	add	\dst, \dst, :lo12:\sym
-#else
-	movz	\dst, #:abs_g3:\sym
-	movk	\dst, #:abs_g2_nc:\sym
-	movk	\dst, #:abs_g1_nc:\sym
-	movk	\dst, #:abs_g0_nc:\sym
-#endif
 	.endm
 
 	/*
@@ -231,7 +221,6 @@ lr	.req	x30		// link register
 	 *       the address
 	 */
 	.macro	ldr_l, dst, sym, tmp=
-#ifndef MODULE
 	.ifb	\tmp
 	adrp	\dst, \sym
 	ldr	\dst, [\dst, :lo12:\sym]
@@ -239,15 +228,6 @@ lr	.req	x30		// link register
 	adrp	\tmp, \sym
 	ldr	\dst, [\tmp, :lo12:\sym]
 	.endif
-#else
-	.ifb	\tmp
-	adr_l	\dst, \sym
-	ldr	\dst, [\dst]
-	.else
-	adr_l	\tmp, \sym
-	ldr	\dst, [\tmp]
-	.endif
-#endif
 	.endm
 
 	/*
@@ -257,28 +237,18 @@ lr	.req	x30		// link register
 	 *       while <src> needs to be preserved.
 	 */
 	.macro	str_l, src, sym, tmp
-#ifndef MODULE
 	adrp	\tmp, \sym
 	str	\src, [\tmp, :lo12:\sym]
-#else
-	adr_l	\tmp, \sym
-	str	\src, [\tmp]
-#endif
 	.endm
 
 	/*
-	 * @dst: Result of per_cpu(sym, smp_processor_id()), can be SP for
-	 *       non-module code
+	 * @dst: Result of per_cpu(sym, smp_processor_id()) (can be SP)
 	 * @sym: The name of the per-cpu variable
 	 * @tmp: scratch register
 	 */
 	.macro adr_this_cpu, dst, sym, tmp
-#ifndef MODULE
 	adrp	\tmp, \sym
 	add	\dst, \tmp, #:lo12:\sym
-#else
-	adr_l	\dst, \sym
-#endif
 alternative_if_not ARM64_HAS_VIRT_HOST_EXTN
 	mrs	\tmp, tpidr_el1
 alternative_else

commit 15303ba5d1cd9b28d03a980456c0978c0ea3b208
Merge: 9a61df9e5f74 1ab03c072feb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 10 13:16:35 2018 -0800

    Merge tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
    
       - icache invalidation optimizations, improving VM startup time
    
       - support for forwarded level-triggered interrupts, improving
         performance for timers and passthrough platform devices
    
       - a small fix for power-management notifiers, and some cosmetic
         changes
    
      PPC:
    
       - add MMIO emulation for vector loads and stores
    
       - allow HPT guests to run on a radix host on POWER9 v2.2 CPUs without
         requiring the complex thread synchronization of older CPU versions
    
       - improve the handling of escalation interrupts with the XIVE
         interrupt controller
    
       - support decrement register migration
    
       - various cleanups and bugfixes.
    
      s390:
    
       - Cornelia Huck passed maintainership to Janosch Frank
    
       - exitless interrupts for emulated devices
    
       - cleanup of cpuflag handling
    
       - kvm_stat counter improvements
    
       - VSIE improvements
    
       - mm cleanup
    
      x86:
    
       - hypervisor part of SEV
    
       - UMIP, RDPID, and MSR_SMI_COUNT emulation
    
       - paravirtualized TLB shootdown using the new KVM_VCPU_PREEMPTED bit
    
       - allow guests to see TOPOEXT, GFNI, VAES, VPCLMULQDQ, and more
         AVX512 features
    
       - show vcpu id in its anonymous inode name
    
       - many fixes and cleanups
    
       - per-VCPU MSR bitmaps (already merged through x86/pti branch)
    
       - stable KVM clock when nesting on Hyper-V (merged through
         x86/hyperv)"
    
    * tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (197 commits)
      KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
      KVM: PPC: Book3S HV: Branch inside feature section
      KVM: PPC: Book3S HV: Make HPT resizing work on POWER9
      KVM: PPC: Book3S HV: Fix handling of secondary HPTEG in HPT resizing code
      KVM: PPC: Book3S PR: Fix broken select due to misspelling
      KVM: x86: don't forget vcpu_put() in kvm_arch_vcpu_ioctl_set_sregs()
      KVM: PPC: Book3S PR: Fix svcpu copying with preemption enabled
      KVM: PPC: Book3S HV: Drop locks before reading guest memory
      kvm: x86: remove efer_reload entry in kvm_vcpu_stat
      KVM: x86: AMD Processor Topology Information
      x86/kvm/vmx: do not use vm-exit instruction length for fast MMIO when running nested
      kvm: embed vcpu id to dentry of vcpu anon inode
      kvm: Map PFN-type memory regions as writable (if possible)
      x86/kvm: Make it compile on 32bit and with HYPYERVISOR_GUEST=n
      KVM: arm/arm64: Fixup userspace irqchip static key optimization
      KVM: arm/arm64: Fix userspace_irqchip_in_use counting
      KVM: arm/arm64: Fix incorrect timer_is_pending logic
      MAINTAINERS: update KVM/s390 maintainers
      MAINTAINERS: add Halil as additional vfio-ccw maintainer
      MAINTAINERS: add David as a reviewer for KVM/s390
      ...

commit 6314d90e64936c584f300a52ef173603fb2461b5
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 5 15:34:20 2018 +0000

    arm64: entry: Ensure branch through syscall table is bounded under speculation
    
    In a similar manner to array_index_mask_nospec, this patch introduces an
    assembly macro (mask_nospec64) which can be used to bound a value under
    speculation. This macro is then used to ensure that the indirect branch
    through the syscall table is bounded under speculation, with out-of-range
    addresses speculating as calls to sys_io_setup (0).
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 1f44b4255a28..1241fb211293 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -122,6 +122,17 @@
 	hint	#20
 	.endm
 
+/*
+ * Sanitise a 64-bit bounded index wrt speculation, returning zero if out
+ * of bounds.
+ */
+	.macro	mask_nospec64, idx, limit, tmp
+	sub	\tmp, \idx, \limit
+	bic	\tmp, \tmp, \idx
+	and	\idx, \idx, \tmp, asr #63
+	csdb
+	.endm
+
 /*
  * NOP sequence
  */

commit 669474e772b952b14f4de4845a1558fd4c0414a4
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 5 15:34:16 2018 +0000

    arm64: barrier: Add CSDB macros to control data-value prediction
    
    For CPUs capable of data value prediction, CSDB waits for any outstanding
    predictions to architecturally resolve before allowing speculative execution
    to continue. Provide macros to expose it to the arch code.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 544878a9f29e..1f44b4255a28 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -115,6 +115,13 @@
 	hint    #16
 	.endm
 
+/*
+ * Value prediction barrier
+ */
+	.macro	csdb
+	hint	#20
+	.endm
+
 /*
  * NOP sequence
  */

commit 79ddab3b05ca903f552fd5bf920efa055210322b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jan 29 11:59:59 2018 +0000

    arm64: assembler: Align phys_to_pte with pte_to_phys
    
    pte_to_phys lives in assembler.h and takes its destination register as
    the first argument. Move phys_to_pte out of head.S to sit with its
    counterpart and rejig it to follow the same calling convention.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 47555f632ffd..544878a9f29e 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -523,6 +523,19 @@ alternative_endif
 #endif
 	.endm
 
+	.macro	phys_to_pte, pte, phys
+#ifdef CONFIG_ARM64_PA_BITS_52
+	/*
+	 * We assume \phys is 64K aligned and this is guaranteed by only
+	 * supporting this configuration with 64K pages.
+	 */
+	orr	\pte, \phys, \phys, lsr #36
+	and	\pte, \pte, #PTE_ADDR_MASK
+#else
+	mov	\pte, \phys
+#endif
+	.endm
+
 	.macro	pte_to_phys, phys, pte
 #ifdef CONFIG_ARM64_PA_BITS_52
 	ubfiz	\phys, \pte, #(48 - 16 - 12), #16

commit fa0465fc07c2f9f47bd1198ab368d341bd7c7e4e
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jan 29 11:59:57 2018 +0000

    arm64: assembler: Change order of macro arguments in phys_to_ttbr
    
    Since AArch64 assembly instructions take the destination register as
    their first operand, do the same thing for the phys_to_ttbr macro.
    
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 5aae7a6e8ab9..47555f632ffd 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -514,7 +514,7 @@ alternative_endif
  * 	phys:	physical address, preserved
  * 	ttbr:	returns the TTBR value
  */
-	.macro	phys_to_ttbr, phys, ttbr
+	.macro	phys_to_ttbr, ttbr, phys
 #ifdef CONFIG_ARM64_PA_BITS_52
 	orr	\ttbr, \phys, \phys, lsr #46
 	and	\ttbr, \ttbr, #TTBR_BADDR_MASK_52

commit f992b4dfd58be07e31a42bc940a53b3e4b282616
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 6 22:22:50 2018 +0000

    arm64: kpti: Add ->enable callback to remap swapper using nG mappings
    
    Defaulting to global mappings for kernel space is generally good for
    performance and appears to be necessary for Cavium ThunderX. If we
    subsequently decide that we need to enable kpti, then we need to rewrite
    our existing page table entries to be non-global. This is fiddly, and
    made worse by the possible use of contiguous mappings, which require
    a strict break-before-make sequence.
    
    Since the enable callback runs on each online CPU from stop_machine
    context, we can have all CPUs enter the idmap, where secondaries can
    wait for the primary CPU to rewrite swapper with its MMU off. It's all
    fairly horrible, but at least it only runs once.
    
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 3873dd7b5a32..5aae7a6e8ab9 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -523,6 +523,16 @@ alternative_endif
 #endif
 	.endm
 
+	.macro	pte_to_phys, phys, pte
+#ifdef CONFIG_ARM64_PA_BITS_52
+	ubfiz	\phys, \pte, #(48 - 16 - 12), #16
+	bfxil	\phys, \pte, #16, #32
+	lsl	\phys, \phys, #16
+#else
+	and	\phys, \pte, #PTE_ADDR_MASK
+#endif
+	.endm
+
 /**
  * Errata workaround prior to disable MMU. Insert an ISB immediately prior
  * to executing the MSR that will change SCTLR_ELn[M] from a value of 1 to 0.

commit 3060e9f0d14b46eb1949aaf9a31519ef75e59fda
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Mon Jan 29 11:59:52 2018 +0000

    arm64: Add software workaround for Falkor erratum 1041
    
    The ARM architecture defines the memory locations that are permitted
    to be accessed as the result of a speculative instruction fetch from
    an exception level for which all stages of translation are disabled.
    Specifically, the core is permitted to speculatively fetch from the
    4KB region containing the current program counter 4K and next 4K.
    
    When translation is changed from enabled to disabled for the running
    exception level (SCTLR_ELn[M] changed from a value of 1 to 0), the
    Falkor core may errantly speculatively access memory locations outside
    of the 4KB region permitted by the architecture. The errant memory
    access may lead to one of the following unexpected behaviors.
    
    1) A System Error Interrupt (SEI) being raised by the Falkor core due
       to the errant memory access attempting to access a region of memory
       that is protected by a slave-side memory protection unit.
    2) Unpredictable device behavior due to a speculative read from device
       memory. This behavior may only occur if the instruction cache is
       disabled prior to or coincident with translation being changed from
       enabled to disabled.
    
    The conditions leading to this erratum will not occur when either of the
    following occur:
     1) A higher exception level disables translation of a lower exception level
       (e.g. EL2 changing SCTLR_EL1[M] from a value of 1 to 0).
     2) An exception level disabling its stage-1 translation if its stage-2
        translation is enabled (e.g. EL1 changing SCTLR_EL1[M] from a value of 1
        to 0 when HCR_EL2[VM] has a value of 1).
    
    To avoid the errant behavior, software must execute an ISB immediately
    prior to executing the MSR that will change SCTLR_ELn[M] from 1 to 0.
    
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 794fe8122602..3873dd7b5a32 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -523,4 +523,14 @@ alternative_endif
 #endif
 	.endm
 
+/**
+ * Errata workaround prior to disable MMU. Insert an ISB immediately prior
+ * to executing the MSR that will change SCTLR_ELn[M] from a value of 1 to 0.
+ */
+	.macro pre_disable_mmu_workaround
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_E1041
+	isb
+#endif
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 7bf14c28ee776be567855bd39ed8ff795ea19f55
Merge: 87cedc6be559 5fa4ec9cb2e6
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 1 15:04:17 2018 +0100

    Merge branch 'x86/hyperv' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Topic branch for stable KVM clockource under Hyper-V.
    
    Thanks to Christoffer Dall for resolving the ARM conflict.

commit 68ddbf09ec5a888ec850edd7e7438d2daf069c56
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:38:59 2018 +0000

    arm64: kernel: Prepare for a DISR user
    
    KVM would like to consume any pending SError (or RAS error) after guest
    exit. Today it has to unmask SError and use dsb+isb to synchronise the
    CPU. With the RAS extensions we can use ESB to synchronise any pending
    SError.
    
    Add the necessary macros to allow DISR to be read and converted to an
    ESR.
    
    We clear the DISR register when we enable the RAS cpufeature, and the
    kernel has not executed any ESB instructions. Any value we find in DISR
    must have belonged to firmware. Executing an ESB instruction is the
    only way to update DISR, so we can expect firmware to have handled
    any deferred SError. By the same logic we clear DISR in the idle path.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 1645e0d3a2c1..794fe8122602 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -108,6 +108,13 @@
 	dmb	\opt
 	.endm
 
+/*
+ * RAS Error Synchronization barrier
+ */
+	.macro  esb
+	hint    #16
+	.endm
+
 /*
  * NOP sequence
  */

commit 39610a68d9a9a9c6e71be731d071c405e4f2434b
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Mon Jan 15 15:23:50 2018 +0000

    arm64: fix comment above tcr_compute_pa_size
    
    The 'pos' argument is used to select where in TCR to write the value:
    the IPS or PS bitfield.
    
    Fixes: 787fd1d019b2 ("arm64: limit PA size to supported range")
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 5dc4856f3bb9..1645e0d3a2c1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -360,7 +360,7 @@ alternative_endif
  * ID_AA64MMFR0_EL1.PARange value
  *
  *	tcr:		register with the TCR_ELx value to be updated
- *	pos:		PARange bitfield position
+ *	pos:		IPS or PS bitfield position
  *	tmp{0,1}:	temporary registers
  */
 	.macro	tcr_compute_pa_size, tcr, pos, tmp0, tmp1

commit 6d99b68933fbcf51f84fcbba49246ce1209ec193
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:06 2018 +0000

    arm64: alternatives: use tpidr_el2 on VHE hosts
    
    Now that KVM uses tpidr_el2 in the same way as Linux's cpu_offset in
    tpidr_el1, merge the two. This saves KVM from save/restoring tpidr_el1
    on VHE hosts, and allows future code to blindly access per-cpu variables
    without triggering world-switch.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index a6f90b648655..5dc4856f3bb9 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -254,7 +254,11 @@ lr	.req	x30		// link register
 #else
 	adr_l	\dst, \sym
 #endif
+alternative_if_not ARM64_HAS_VIRT_HOST_EXTN
 	mrs	\tmp, tpidr_el1
+alternative_else
+	mrs	\tmp, tpidr_el2
+alternative_endif
 	add	\dst, \dst, \tmp
 	.endm
 
@@ -265,7 +269,11 @@ lr	.req	x30		// link register
 	 */
 	.macro ldr_this_cpu dst, sym, tmp
 	adr_l	\dst, \sym
+alternative_if_not ARM64_HAS_VIRT_HOST_EXTN
 	mrs	\tmp, tpidr_el1
+alternative_else
+	mrs	\tmp, tpidr_el2
+alternative_endif
 	ldr	\dst, [\dst, \tmp]
 	.endm
 

commit 95e3de3590e3f2358bb13f013911bc1bfa5d3f53
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jan 2 18:19:39 2018 +0000

    arm64: Move post_ttbr_update_workaround to C code
    
    We will soon need to invoke a CPU-specific function pointer after changing
    page tables, so move post_ttbr_update_workaround out into C code to make
    this possible.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 215a49213507..a6f90b648655 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -492,19 +492,6 @@ alternative_endif
 	mrs	\rd, sp_el0
 	.endm
 
-/*
- * Errata workaround post TTBRx_EL1 update.
- */
-	.macro	post_ttbr_update_workaround
-#ifdef CONFIG_CAVIUM_ERRATUM_27456
-alternative_if ARM64_WORKAROUND_CAVIUM_27456
-	ic	iallu
-	dsb	nsh
-	isb
-alternative_else_nop_endif
-#endif
-	.endm
-
 /*
  * Arrange a physical address in a TTBR register, taking care of 52-bit
  * addresses.

commit 4fee94736603cd6fd83c1ea1ee0388d1d2dbe11b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 23 17:11:16 2017 +0100

    arm64: KVM: Add invalidate_icache_range helper
    
    We currently tightly couple dcache clean with icache invalidation,
    but KVM could do without the initial flush to PoU, as we've
    already flushed things to PoC.
    
    Let's introduce invalidate_icache_range which is limited to
    invalidating the icache from the linear mapping (and thus
    has none of the userspace fault handling complexity), and
    wire it in KVM instead of flush_icache_range.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index aef72d886677..0884e1fdfd30 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -387,6 +387,27 @@ alternative_endif
 	dsb	\domain
 	.endm
 
+/*
+ * Macro to perform an instruction cache maintenance for the interval
+ * [start, end)
+ *
+ * 	start, end:	virtual addresses describing the region
+ *	label:		A label to branch to on user fault.
+ * 	Corrupts:	tmp1, tmp2
+ */
+	.macro invalidate_icache_by_line start, end, tmp1, tmp2, label
+	icache_line_size \tmp1, \tmp2
+	sub	\tmp2, \tmp1, #1
+	bic	\tmp2, \start, \tmp2
+9997:
+USER(\label, ic	ivau, \tmp2)			// invalidate I line PoU
+	add	\tmp2, \tmp2, \tmp1
+	cmp	\tmp2, \end
+	b.lo	9997b
+	dsb	ish
+	isb
+	.endm
+
 /*
  * reset_pmuserenr_el0 - reset PMUSERENR_EL0 if PMUv3 present
  */

commit 1f911c3a1140e1668e68791fb6dd07757e2f3956
Merge: 6aef0fdd35ea f77d281713d4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Dec 22 17:40:58 2017 +0000

    Merge branch 'for-next/52-bit-pa' into for-next/core
    
    * for-next/52-bit-pa:
      arm64: enable 52-bit physical address support
      arm64: allow ID map to be extended to 52 bits
      arm64: handle 52-bit physical addresses in page table entries
      arm64: don't open code page table entry creation
      arm64: head.S: handle 52-bit PAs in PTEs in early page table setup
      arm64: handle 52-bit addresses in TTBR
      arm64: limit PA size to supported range
      arm64: add kconfig symbol to configure physical address size

commit fa2a8445b1d3810c52f2a6b3a006456bd1aacb7e
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:24 2017 +0000

    arm64: allow ID map to be extended to 52 bits
    
    Currently, when using VA_BITS < 48, if the ID map text happens to be
    placed in physical memory above VA_BITS, we increase the VA size (up to
    48) and create a new table level, in order to map in the ID map text.
    This is okay because the system always supports 48 bits of VA.
    
    This patch extends the code such that if the system supports 52 bits of
    VA, and the ID map text is placed that high up, then we increase the VA
    size accordingly, up to 52.
    
    One difference from the current implementation is that so far the
    condition of VA_BITS < 48 has meant that the top level table is always
    "full", with the maximum number of entries, and an extra table level is
    always needed. Now, when VA_BITS = 48 (and using 64k pages), the top
    level table is not full, and we simply need to increase the number of
    entries in it, instead of creating a new table level.
    
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [catalin.marinas@arm.com: reduce arguments to __create_hyp_mappings()]
    [catalin.marinas@arm.com: reworked/renamed __cpu_uses_extended_idmap_level()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 49ea3def4bd1..942fdb5ef0ad 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -344,10 +344,8 @@ alternative_endif
  * tcr_set_idmap_t0sz - update TCR.T0SZ so that we can load the ID map
  */
 	.macro	tcr_set_idmap_t0sz, valreg, tmpreg
-#ifndef CONFIG_ARM64_VA_BITS_48
 	ldr_l	\tmpreg, idmap_t0sz
 	bfi	\valreg, \tmpreg, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
-#endif
 	.endm
 
 /*

commit 529c4b05a3cb2f324aac347042ee6d641478e946
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:18 2017 +0000

    arm64: handle 52-bit addresses in TTBR
    
    The top 4 bits of a 52-bit physical address are positioned at bits 2..5
    in the TTBR registers. Introduce a couple of macros to move the bits
    there, and change all TTBR writers to use them.
    
    Leave TTBR0 PAN code unchanged, to avoid complicating it. A system with
    52-bit PA will have PAN anyway (because it's ARMv8.1 or later), and a
    system without 52-bit PA can only use up to 48-bit PAs. A later patch in
    this series will add a kconfig dependency to ensure PAN is configured.
    
    In addition, when using 52-bit PA there is a special alignment
    requirement on the top-level table. We don't currently have any VA_BITS
    configuration that would violate the requirement, but one could be added
    in the future, so add a compile-time BUG_ON to check for it.
    
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [catalin.marinas@arm.com: added TTBR_BADD_MASK_52 comment]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 04a92307e6c1..49ea3def4bd1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -530,4 +530,20 @@ alternative_else_nop_endif
 #endif
 	.endm
 
+/*
+ * Arrange a physical address in a TTBR register, taking care of 52-bit
+ * addresses.
+ *
+ * 	phys:	physical address, preserved
+ * 	ttbr:	returns the TTBR value
+ */
+	.macro	phys_to_ttbr, phys, ttbr
+#ifdef CONFIG_ARM64_PA_BITS_52
+	orr	\ttbr, \phys, \phys, lsr #46
+	and	\ttbr, \ttbr, #TTBR_BADDR_MASK_52
+#else
+	mov	\ttbr, \phys
+#endif
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 787fd1d019b269af7912249231dfe34a5fe3e7c8
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:17 2017 +0000

    arm64: limit PA size to supported range
    
    We currently copy the physical address size from
    ID_AA64MMFR0_EL1.PARange directly into TCR.(I)PS. This will not work for
    4k and 16k granule kernels on systems that support 52-bit physical
    addresses, since 52-bit addresses are only permitted with the 64k
    granule.
    
    To fix this, fall back to 48 bits when configuring the PA size when the
    kernel does not support 52-bit PAs. When it does, fall back to 52, to
    avoid similar problems in the future if the PA size is ever increased
    above 52.
    
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [catalin.marinas@arm.com: tcr_set_pa_size macro renamed to tcr_compute_pa_size]
    [catalin.marinas@arm.com: comments added to tcr_compute_pa_size]
    [catalin.marinas@arm.com: definitions added for TCR_*PS_SHIFT]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index aef72d886677..04a92307e6c1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -350,6 +350,24 @@ alternative_endif
 #endif
 	.endm
 
+/*
+ * tcr_compute_pa_size - set TCR.(I)PS to the highest supported
+ * ID_AA64MMFR0_EL1.PARange value
+ *
+ *	tcr:		register with the TCR_ELx value to be updated
+ *	pos:		PARange bitfield position
+ *	tmp{0,1}:	temporary registers
+ */
+	.macro	tcr_compute_pa_size, tcr, pos, tmp0, tmp1
+	mrs	\tmp0, ID_AA64MMFR0_EL1
+	// Narrow PARange to fit the PS field in TCR_ELx
+	ubfx	\tmp0, \tmp0, #ID_AA64MMFR0_PARANGE_SHIFT, #3
+	mov	\tmp1, #ID_AA64MMFR0_PARANGE_MAX
+	cmp	\tmp0, \tmp1
+	csel	\tmp0, \tmp1, \tmp0, hi
+	bfi	\tcr, \tmp0, \pos, #3
+	.endm
+
 /*
  * Macro to perform a data cache maintenance for the interval
  * [kaddr, kaddr + size)

commit 932b50c7c1c65e6f23002e075b97ee083c4a9e71
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Mon Dec 11 16:42:32 2017 -0600

    arm64: Add software workaround for Falkor erratum 1041
    
    The ARM architecture defines the memory locations that are permitted
    to be accessed as the result of a speculative instruction fetch from
    an exception level for which all stages of translation are disabled.
    Specifically, the core is permitted to speculatively fetch from the
    4KB region containing the current program counter 4K and next 4K.
    
    When translation is changed from enabled to disabled for the running
    exception level (SCTLR_ELn[M] changed from a value of 1 to 0), the
    Falkor core may errantly speculatively access memory locations outside
    of the 4KB region permitted by the architecture. The errant memory
    access may lead to one of the following unexpected behaviors.
    
    1) A System Error Interrupt (SEI) being raised by the Falkor core due
       to the errant memory access attempting to access a region of memory
       that is protected by a slave-side memory protection unit.
    2) Unpredictable device behavior due to a speculative read from device
       memory. This behavior may only occur if the instruction cache is
       disabled prior to or coincident with translation being changed from
       enabled to disabled.
    
    The conditions leading to this erratum will not occur when either of the
    following occur:
     1) A higher exception level disables translation of a lower exception level
       (e.g. EL2 changing SCTLR_EL1[M] from a value of 1 to 0).
     2) An exception level disabling its stage-1 translation if its stage-2
        translation is enabled (e.g. EL1 changing SCTLR_EL1[M] from a value of 1
        to 0 when HCR_EL2[VM] has a value of 1).
    
    To avoid the errant behavior, software must execute an ISB immediately
    prior to executing the MSR that will change SCTLR_ELn[M] from 1 to 0.
    
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index aef72d886677..8b168280976f 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -512,4 +512,14 @@ alternative_else_nop_endif
 #endif
 	.endm
 
+/**
+ * Errata workaround prior to disable MMU. Insert an ISB immediately prior
+ * to executing the MSR that will change SCTLR_ELn[M] from a value of 1 to 0.
+ */
+	.macro pre_disable_mmu_workaround
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_E1041
+	isb
+#endif
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 158d495899ce55db453f682a8ac8390d5a426578
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 10 13:34:30 2017 +0100

    arm64: mm: Rename post_ttbr0_update_workaround
    
    The post_ttbr0_update_workaround hook applies to any change to TTBRx_EL1.
    Since we're using TTBR1 for the ASID, rename the hook to make it clearer
    as to what it's doing.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index e1fa5db858b7..c45bc94f15d0 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -477,10 +477,9 @@ alternative_endif
 	.endm
 
 /*
-/*
- * Errata workaround post TTBR0_EL1 update.
+ * Errata workaround post TTBRx_EL1 update.
  */
-	.macro	post_ttbr0_update_workaround
+	.macro	post_ttbr_update_workaround
 #ifdef CONFIG_CAVIUM_ERRATUM_27456
 alternative_if ARM64_WORKAROUND_CAVIUM_27456
 	ic	iallu

commit 85d13c001497b481b4a8cc5d7db243cc44ac2bd8
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 10 13:29:06 2017 +0100

    arm64: mm: Remove pre_ttbr0_update_workaround for Falkor erratum #E1003
    
    The pre_ttbr0_update_workaround hook is called prior to context-switching
    TTBR0 because Falkor erratum E1003 can cause TLB allocation with the wrong
    ASID if both the ASID and the base address of the TTBR are updated at
    the same time.
    
    With the ASID sitting safely in TTBR1, we no longer update things
    atomically, so we can remove the pre_ttbr0_update_workaround macro as
    it's no longer required. The erratum infrastructure and documentation
    is left around for #E1003, as it will be required by the entry
    trampoline code in a future patch.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index aef72d886677..e1fa5db858b7 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -26,7 +26,6 @@
 #include <asm/asm-offsets.h>
 #include <asm/cpufeature.h>
 #include <asm/debug-monitors.h>
-#include <asm/mmu_context.h>
 #include <asm/page.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
@@ -478,27 +477,6 @@ alternative_endif
 	.endm
 
 /*
- * Errata workaround prior to TTBR0_EL1 update
- *
- * 	val:	TTBR value with new BADDR, preserved
- * 	tmp0:	temporary register, clobbered
- * 	tmp1:	other temporary register, clobbered
- */
-	.macro	pre_ttbr0_update_workaround, val, tmp0, tmp1
-#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
-alternative_if ARM64_WORKAROUND_QCOM_FALKOR_E1003
-	mrs	\tmp0, ttbr0_el1
-	mov	\tmp1, #FALKOR_RESERVED_ASID
-	bfi	\tmp0, \tmp1, #48, #16		// reserved ASID + old BADDR
-	msr	ttbr0_el1, \tmp0
-	isb
-	bfi	\tmp0, \val, #0, #48		// reserved ASID + new BADDR
-	msr	ttbr0_el1, \tmp0
-	isb
-alternative_else_nop_endif
-#endif
-	.endm
-
 /*
  * Errata workaround post TTBR0_EL1 update.
  */

commit b282e1ce29bb677224ba8fb38e94f5e94e2656d5
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:41 2017 +0000

    arm64: entry.S: convert elX_irq
    
    Following our 'dai' order, irqs should be processed with debug and
    serror exceptions unmasked.
    
    Add a helper to unmask these two, (and fiq for good measure).
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 90d1f01d602f..aef72d886677 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -55,6 +55,11 @@
 	msr	daif, \tmp
 	.endm
 
+	/* IRQ is the lowest priority flag, unconditionally unmask the rest. */
+	.macro enable_da_f
+	msr	daifclr, #(8 | 4 | 1)
+	.endm
+
 /*
  * Enable and disable interrupts.
  */

commit 746647c75afb5a1706426c2563ff02884a15530d
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:40 2017 +0000

    arm64: entry.S convert el0_sync
    
    el0_sync also unmasks exceptions on a case-by-case basis, debug exceptions
    are enabled, unless this was a debug exception. Irqs are unmasked for
    some exception types but not for others.
    
    el0_dbg should run with everything masked to prevent us taking a debug
    exception from do_debug_exception. For the other cases we can unmask
    everything. This changes the behaviour of fpsimd_{acc,exc} and el0_inv
    which previously ran with irqs masked.
    
    This patch removed the last user of enable_dbg_and_irq, remove it.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 1886b8b44672..90d1f01d602f 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -97,15 +97,6 @@
 9990:
 	.endm
 
-/*
- * Enable both debug exceptions and interrupts. This is likely to be
- * faster than two daifclr operations, since writes to this register
- * are self-synchronising.
- */
-	.macro	enable_dbg_and_irq
-	msr	daifclr, #(8 | 2)
-	.endm
-
 /*
  * SMP data memory barrier
  */

commit b55a5a1b0a7d4f51b6c8eec0d4d78ace8f5fa2b3
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:39 2017 +0000

    arm64: entry.S: convert el1_sync
    
    el1_sync unmasks exceptions on a case-by-case basis, debug exceptions
    are unmasked, unless this was a debug exception. IRQs are unmasked
    for instruction and data aborts only if the interupted context had
    irqs unmasked.
    
    Following our 'dai' order, el1_dbg should run with everything masked.
    For the other cases we can inherit whatever we interrupted.
    
    Add a macro inherit_daif to set daif based on the interrupted pstate.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 8bc63fc98752..1886b8b44672 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -49,6 +49,12 @@
 	msr	daif, \flags
 	.endm
 
+	/* Only on aarch64 pstate, PSR_D_BIT is different for aarch32 */
+	.macro	inherit_daif, pstate:req, tmp:req
+	and	\tmp, \pstate, #(PSR_D_BIT | PSR_A_BIT | PSR_I_BIT | PSR_F_BIT)
+	msr	daif, \tmp
+	.endm
+
 /*
  * Enable and disable interrupts.
  */

commit 84d0fb1bb6257d5d5d45f5e3554ab441b4ff5a74
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:38 2017 +0000

    arm64: entry.S: Remove disable_dbg
    
    enable_step_tsk is the only user of disable_dbg, which doesn't respect
    our 'dai' order for exception masking. enable_step_tsk may enable
    single-step, so previously needed to mask debug exceptions to prevent us
    from single-stepping kernel_exit. enable_step_tsk is called at the end
    of the ret_to_user loop, which has already masked all exceptions so this
    is no longer needed.
    
    Remove disable_dbg, add a comment that enable_step_tsk's caller should
    have masked debug.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index a4aa22241dd1..8bc63fc98752 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -69,13 +69,6 @@
 	msr	daif, \flags
 	.endm
 
-/*
- * Enable and disable debug exceptions.
- */
-	.macro	disable_dbg
-	msr	daifset, #8
-	.endm
-
 	.macro	enable_dbg
 	msr	daifclr, #8
 	.endm
@@ -89,9 +82,9 @@
 9990:
 	.endm
 
+	/* call with daif masked */
 	.macro	enable_step_tsk, flgs, tmp
 	tbz	\flgs, #TIF_SINGLESTEP, 9990f
-	disable_dbg
 	mrs	\tmp, mdscr_el1
 	orr	\tmp, \tmp, #DBG_MDSCR_SS
 	msr	mdscr_el1, \tmp

commit 0fbeb318754860b37150fd42c2058d636a431426
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:34 2017 +0000

    arm64: explicitly mask all exceptions
    
    There are a few places where we want to mask all exceptions. Today we
    do this in a piecemeal fashion, typically we expect the caller to
    have masked irqs and the arch code masks debug exceptions, ignoring
    serror which is probably masked.
    
    Make it clear that 'mask all exceptions' is the intention by adding
    helpers to do exactly that.
    
    This will let us unmask SError without having to add 'oh and SError'
    to these paths.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 3128a9ca5701..a4aa22241dd1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -32,6 +32,23 @@
 #include <asm/ptrace.h>
 #include <asm/thread_info.h>
 
+	.macro save_and_disable_daif, flags
+	mrs	\flags, daif
+	msr	daifset, #0xf
+	.endm
+
+	.macro disable_daif
+	msr	daifset, #0xf
+	.endm
+
+	.macro enable_daif
+	msr	daifclr, #0xf
+	.endm
+
+	.macro	restore_daif, flags:req
+	msr	daif, \flags
+	.endm
+
 /*
  * Enable and disable interrupts.
  */

commit e28cc0255997e9b770039d87276c1b9fd8bc231d
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Wed Oct 25 10:04:32 2017 +0100

    arm64: Use existing defines for mdscr
    
    Literal values are being used to set single stepping in mdscr from assembly
    code. There are already existing defines representing those values, use
    those instead of the literal values.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index d58a6253c6ab..3128a9ca5701 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -25,6 +25,7 @@
 
 #include <asm/asm-offsets.h>
 #include <asm/cpufeature.h>
+#include <asm/debug-monitors.h>
 #include <asm/mmu_context.h>
 #include <asm/page.h>
 #include <asm/pgtable-hwdef.h>
@@ -65,7 +66,7 @@
 	.macro	disable_step_tsk, flgs, tmp
 	tbz	\flgs, #TIF_SINGLESTEP, 9990f
 	mrs	\tmp, mdscr_el1
-	bic	\tmp, \tmp, #1
+	bic	\tmp, \tmp, #DBG_MDSCR_SS
 	msr	mdscr_el1, \tmp
 	isb	// Synchronise with enable_dbg
 9990:
@@ -75,7 +76,7 @@
 	tbz	\flgs, #TIF_SINGLESTEP, 9990f
 	disable_dbg
 	mrs	\tmp, mdscr_el1
-	orr	\tmp, \tmp, #1
+	orr	\tmp, \tmp, #DBG_MDSCR_SS
 	msr	mdscr_el1, \tmp
 9990:
 	.endm

commit df5b95bee1ed7009a2090e9924e7a96e14850d56
Merge: 969ff73e72fe 872d8327ce89
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 15 18:40:58 2017 +0100

    Merge branch 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: add VMAP_STACK overflow detection
      arm64: add on_accessible_stack()
      arm64: add basic VMAP_STACK support
      arm64: use an irq stack pointer
      arm64: assembler: allow adr_this_cpu to use the stack pointer
      arm64: factor out entry stack manipulation
      efi/arm64: add EFI_KIMG_ALIGN
      arm64: move SEGMENT_ALIGN to <asm/memory.h>
      arm64: clean up irq stack definitions
      arm64: clean up THREAD_* definitions
      arm64: factor out PAGE_* and CONT_* definitions
      arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
      fork: allow arch-override of VMAP stack alignment
      arm64: remove __die()'s stack dump

commit 8ea41b11ef746e1ac97f8c90911e5c61f8bd5cc0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Jul 15 17:23:13 2017 +0100

    arm64: assembler: allow adr_this_cpu to use the stack pointer
    
    Given that adr_this_cpu already requires a temp register in addition
    to the destination register, tweak the instruction sequence so that sp
    may be used as well.
    
    This will simplify switching to per-cpu stacks in subsequent patches. While
    this limits the range of adr_this_cpu, to +/-4GiB, we don't currently use
    adr_this_cpu in modules, and this is not problematic for the main kernel image.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [Mark: add more commit text]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 610a42018241..2f2bd5192b5e 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -230,12 +230,18 @@ lr	.req	x30		// link register
 	.endm
 
 	/*
-	 * @dst: Result of per_cpu(sym, smp_processor_id())
+	 * @dst: Result of per_cpu(sym, smp_processor_id()), can be SP for
+	 *       non-module code
 	 * @sym: The name of the per-cpu variable
 	 * @tmp: scratch register
 	 */
 	.macro adr_this_cpu, dst, sym, tmp
+#ifndef MODULE
+	adrp	\tmp, \sym
+	add	\dst, \tmp, #:lo12:\sym
+#else
 	adr_l	\dst, \sym
+#endif
 	mrs	\tmp, tpidr_el1
 	add	\dst, \dst, \tmp
 	.endm

commit 0553896787353e2526078064ff1cf21ff7bc34ce
Merge: 739586951b8a 31e43ad3b74a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Aug 9 15:37:49 2017 +0100

    Merge branch 'arm64/exception-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/exception-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: unwind: remove sp from struct stackframe
      arm64: unwind: reference pt_regs via embedded stack frame
      arm64: unwind: disregard frame.sp when validating frame pointer
      arm64: unwind: avoid percpu indirection for irq stack
      arm64: move non-entry code out of .entry.text
      arm64: consistently use bl for C exception entry
      arm64: Add ASM_BUG()

commit d50e071fdaa33c1b399c764c44fa1ce879881185
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:42 2017 +0100

    arm64: Implement pmem API support
    
    Add a clean-to-point-of-persistence cache maintenance helper, and wire
    up the basic architectural support for the pmem driver based on it.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    [catalin.marinas@arm.com: move arch_*_pmem() functions to arch/arm64/mm/flush.c]
    [catalin.marinas@arm.com: change dmb(sy) to dmb(osh)]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 1b67c3782d00..5d8903c45031 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -352,6 +352,12 @@ alternative_if_not ARM64_WORKAROUND_CLEAN_CACHE
 	dc	\op, \kaddr
 alternative_else
 	dc	civac, \kaddr
+alternative_endif
+	.elseif	(\op == cvap)
+alternative_if ARM64_HAS_DCPOP
+	sys 3, c7, c12, 1, \kaddr	// dc cvap
+alternative_else
+	dc	cvac, \kaddr
 alternative_endif
 	.else
 	dc	\op, \kaddr

commit ed84b4e9582bdfeffc617589fe17dddfc5fe6672
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 26 16:05:20 2017 +0100

    arm64: move non-entry code out of .entry.text
    
    Currently, cpu_switch_to and ret_from_fork both live in .entry.text,
    though neither form the critical path for an exception entry.
    
    In subsequent patches, we will require that code in .entry.text is part
    of the critical path for exception entry, for which we can assume
    certain properties (e.g. the presence of exception regs on the stack).
    
    Neither cpu_switch_to nor ret_from_fork will meet these requirements, so
    we must move them out of .entry.text. To ensure that neither are kprobed
    after being moved out of .entry.text, we must explicitly blacklist them,
    requiring a new NOKPROBE() asm helper.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 1b67c3782d00..610a42018241 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -403,6 +403,17 @@ alternative_endif
 	.size	__pi_##x, . - x;	\
 	ENDPROC(x)
 
+/*
+ * Annotate a function as being unsuitable for kprobes.
+ */
+#ifdef CONFIG_KPROBES
+#define NOKPROBE(x)				\
+	.pushsection "_kprobe_blacklist", "aw";	\
+	.quad	x;				\
+	.popsection;
+#else
+#define NOKPROBE(x)
+#endif
 	/*
 	 * Emit a 64-bit absolute little endian symbol reference in a way that
 	 * ensures that it will be resolved at build time, even when building a

commit ca78d3173cff3503bcd15723b049757f75762d15
Merge: a4ee7bacd6c0 ffe7afd17135
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:46:44 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     - Errata workarounds for Qualcomm's Falkor CPU
     - Qualcomm L2 Cache PMU driver
     - Qualcomm SMCCC firmware quirk
     - Support for DEBUG_VIRTUAL
     - CPU feature detection for userspace via MRS emulation
     - Preliminary work for the Statistical Profiling Extension
     - Misc cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (74 commits)
      arm64/kprobes: consistently handle MRS/MSR with XZR
      arm64: cpufeature: correctly handle MRS to XZR
      arm64: traps: correctly handle MRS/MSR with XZR
      arm64: ptrace: add XZR-safe regs accessors
      arm64: include asm/assembler.h in entry-ftrace.S
      arm64: fix warning about swapper_pg_dir overflow
      arm64: Work around Falkor erratum 1003
      arm64: head.S: Enable EL1 (host) access to SPE when entered at EL2
      arm64: arch_timer: document Hisilicon erratum 161010101
      arm64: use is_vmalloc_addr
      arm64: use linux/sizes.h for constants
      arm64: uaccess: consistently check object sizes
      perf: add qcom l2 cache perf events driver
      arm64: remove wrong CONFIG_PROC_SYSCTL ifdef
      ARM: smccc: Update HVC comment to describe new quirk parameter
      arm64: do not trace atomic operations
      ACPI/IORT: Fix the error return code in iort_add_smmu_platform_device()
      ACPI/IORT: Fix iort_node_get_id() mapping entries indexing
      arm64: mm: enable CONFIG_HOLES_IN_ZONE for NUMA
      perf: xgene: Include module.h
      ...

commit 38fd94b0275c91071157a03cc27676909b23dcde
Author: Christopher Covington <cov@codeaurora.org>
Date:   Wed Feb 8 15:08:37 2017 -0500

    arm64: Work around Falkor erratum 1003
    
    The Qualcomm Datacenter Technologies Falkor v1 CPU may allocate TLB entries
    using an incorrect ASID when TTBRx_EL1 is being updated. When the erratum
    is triggered, page table entries using the new translation table base
    address (BADDR) will be allocated into the TLB using the old ASID. All
    circumstances leading to the incorrect ASID being cached in the TLB arise
    when software writes TTBRx_EL1[ASID] and TTBRx_EL1[BADDR], a memory
    operation is in the process of performing a translation using the specific
    TTBRx_EL1 being written, and the memory operation uses a translation table
    descriptor designated as non-global. EL2 and EL3 code changing the EL1&0
    ASID is not subject to this erratum because hardware is prohibited from
    performing translations from an out-of-context translation regime.
    
    Consider the following pseudo code.
    
      write new BADDR and ASID values to TTBRx_EL1
    
    Replacing the above sequence with the one below will ensure that no TLB
    entries with an incorrect ASID are used by software.
    
      write reserved value to TTBRx_EL1[ASID]
      ISB
      write new value to TTBRx_EL1[BADDR]
      ISB
      write new value to TTBRx_EL1[ASID]
      ISB
    
    When the above sequence is used, page table entries using the new BADDR
    value may still be incorrectly allocated into the TLB using the reserved
    ASID. Yet this will not reduce functionality, since TLB entries incorrectly
    tagged with the reserved ASID will never be hit by a later instruction.
    
    Based on work by Shanker Donthineni <shankerd@codeaurora.org>
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Christopher Covington <cov@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 446f6c46d4b1..33b20c075fb3 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -25,6 +25,7 @@
 
 #include <asm/asm-offsets.h>
 #include <asm/cpufeature.h>
+#include <asm/mmu_context.h>
 #include <asm/page.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
@@ -422,6 +423,28 @@ alternative_endif
 	mrs	\rd, sp_el0
 	.endm
 
+/*
+ * Errata workaround prior to TTBR0_EL1 update
+ *
+ * 	val:	TTBR value with new BADDR, preserved
+ * 	tmp0:	temporary register, clobbered
+ * 	tmp1:	other temporary register, clobbered
+ */
+	.macro	pre_ttbr0_update_workaround, val, tmp0, tmp1
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
+alternative_if ARM64_WORKAROUND_QCOM_FALKOR_E1003
+	mrs	\tmp0, ttbr0_el1
+	mov	\tmp1, #FALKOR_RESERVED_ASID
+	bfi	\tmp0, \tmp1, #48, #16		// reserved ASID + old BADDR
+	msr	ttbr0_el1, \tmp0
+	isb
+	bfi	\tmp0, \val, #0, #48		// reserved ASID + new BADDR
+	msr	ttbr0_el1, \tmp0
+	isb
+alternative_else_nop_endif
+#endif
+	.endm
+
 /*
  * Errata workaround post TTBR0_EL1 update.
  */

commit 41c066f2c4d436c535616fe182331766c57838f0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Jan 11 14:54:53 2017 +0000

    arm64: assembler: make adr_l work in modules under KASLR
    
    When CONFIG_RANDOMIZE_MODULE_REGION_FULL=y, the offset between loaded
    modules and the core kernel may exceed 4 GB, putting symbols exported
    by the core kernel out of the reach of the ordinary adrp/add instruction
    pairs used to generate relative symbol references. So make the adr_l
    macro emit a movz/movk sequence instead when executing in module context.
    
    While at it, remove the pointless special case for the stack pointer.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 446f6c46d4b1..3a4301163e04 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -164,22 +164,25 @@ lr	.req	x30		// link register
 
 /*
  * Pseudo-ops for PC-relative adr/ldr/str <reg>, <symbol> where
- * <symbol> is within the range +/- 4 GB of the PC.
+ * <symbol> is within the range +/- 4 GB of the PC when running
+ * in core kernel context. In module context, a movz/movk sequence
+ * is used, since modules may be loaded far away from the kernel
+ * when KASLR is in effect.
  */
 	/*
 	 * @dst: destination register (64 bit wide)
 	 * @sym: name of the symbol
-	 * @tmp: optional scratch register to be used if <dst> == sp, which
-	 *       is not allowed in an adrp instruction
 	 */
-	.macro	adr_l, dst, sym, tmp=
-	.ifb	\tmp
+	.macro	adr_l, dst, sym
+#ifndef MODULE
 	adrp	\dst, \sym
 	add	\dst, \dst, :lo12:\sym
-	.else
-	adrp	\tmp, \sym
-	add	\dst, \tmp, :lo12:\sym
-	.endif
+#else
+	movz	\dst, #:abs_g3:\sym
+	movk	\dst, #:abs_g2_nc:\sym
+	movk	\dst, #:abs_g1_nc:\sym
+	movk	\dst, #:abs_g0_nc:\sym
+#endif
 	.endm
 
 	/*
@@ -190,6 +193,7 @@ lr	.req	x30		// link register
 	 *       the address
 	 */
 	.macro	ldr_l, dst, sym, tmp=
+#ifndef MODULE
 	.ifb	\tmp
 	adrp	\dst, \sym
 	ldr	\dst, [\dst, :lo12:\sym]
@@ -197,6 +201,15 @@ lr	.req	x30		// link register
 	adrp	\tmp, \sym
 	ldr	\dst, [\tmp, :lo12:\sym]
 	.endif
+#else
+	.ifb	\tmp
+	adr_l	\dst, \sym
+	ldr	\dst, [\dst]
+	.else
+	adr_l	\tmp, \sym
+	ldr	\dst, [\tmp]
+	.endif
+#endif
 	.endm
 
 	/*
@@ -206,8 +219,13 @@ lr	.req	x30		// link register
 	 *       while <src> needs to be preserved.
 	 */
 	.macro	str_l, src, sym, tmp
+#ifndef MODULE
 	adrp	\tmp, \sym
 	str	\src, [\tmp, :lo12:\sym]
+#else
+	adr_l	\tmp, \sym
+	str	\src, [\tmp]
+#endif
 	.endm
 
 	/*

commit 4b65a5db362783ab4b04ca1c1d2ad70ed9b0ba2a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 1 16:53:00 2016 +0100

    arm64: Introduce uaccess_{disable,enable} functionality based on TTBR0_EL1
    
    This patch adds the uaccess macros/functions to disable access to user
    space by setting TTBR0_EL1 to a reserved zeroed page. Since the value
    written to TTBR0_EL1 must be a physical address, for simplicity this
    patch introduces a reserved_ttbr0 page at a constant offset from
    swapper_pg_dir. The uaccess_disable code uses the ttbr1_el1 value
    adjusted by the reserved_ttbr0 offset.
    
    Enabling access to user is done by restoring TTBR0_EL1 with the value
    from the struct thread_info ttbr0 variable. Interrupts must be disabled
    during the uaccess_ttbr0_enable code to ensure the atomicity of the
    thread_info.ttbr0 read and TTBR0_EL1 write. This patch also moves the
    get_thread_info asm macro from entry.S to assembler.h for reuse in the
    uaccess_ttbr0_* macros.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 55752231055c..446f6c46d4b1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -41,6 +41,15 @@
 	msr	daifclr, #2
 	.endm
 
+	.macro	save_and_disable_irq, flags
+	mrs	\flags, daif
+	msr	daifset, #2
+	.endm
+
+	.macro	restore_irq, flags
+	msr	daif, \flags
+	.endm
+
 /*
  * Enable and disable debug exceptions.
  */
@@ -406,6 +415,13 @@ alternative_endif
 	movk	\reg, :abs_g0_nc:\val
 	.endm
 
+/*
+ * Return the current thread_info.
+ */
+	.macro	get_thread_info, rd
+	mrs	\rd, sp_el0
+	.endm
+
 /*
  * Errata workaround post TTBR0_EL1 update.
  */

commit f33bcf03e6079668da6bf4eec4a7dcf9289131d0
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 1 15:48:55 2016 +0100

    arm64: Factor out TTBR0_EL1 post-update workaround into a specific asm macro
    
    This patch takes the errata workaround code out of cpu_do_switch_mm into
    a dedicated post_ttbr0_update_workaround macro which will be reused in a
    subsequent patch.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 128a9ca06c8a..55752231055c 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -406,4 +406,17 @@ alternative_endif
 	movk	\reg, :abs_g0_nc:\val
 	.endm
 
+/*
+ * Errata workaround post TTBR0_EL1 update.
+ */
+	.macro	post_ttbr0_update_workaround
+#ifdef CONFIG_CAVIUM_ERRATUM_27456
+alternative_if ARM64_WORKAROUND_CAVIUM_27456
+	ic	iallu
+	dsb	nsh
+	isb
+alternative_else_nop_endif
+#endif
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 1b7e2296a822dfd2349960addc42a139360ce769
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:12 2016 +0000

    arm64: assembler: introduce ldr_this_cpu
    
    Shortly we will want to load a percpu variable in the return from
    userspace path. We can save an instruction by folding the addition of
    the percpu offset into the load instruction, and this patch adds a new
    helper to do so.
    
    At the same time, we clean up this_cpu_ptr for consistency. As with
    {adr,ldr,str}_l, we change the template to take the destination register
    first, and name this dst. Secondly, we rename the macro to adr_this_cpu,
    following the scheme of adr_l, and matching the newly added
    ldr_this_cpu.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 28bfe6132eb6..128a9ca06c8a 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -202,14 +202,25 @@ lr	.req	x30		// link register
 	.endm
 
 	/*
+	 * @dst: Result of per_cpu(sym, smp_processor_id())
 	 * @sym: The name of the per-cpu variable
-	 * @reg: Result of per_cpu(sym, smp_processor_id())
 	 * @tmp: scratch register
 	 */
-	.macro this_cpu_ptr, sym, reg, tmp
-	adr_l	\reg, \sym
+	.macro adr_this_cpu, dst, sym, tmp
+	adr_l	\dst, \sym
 	mrs	\tmp, tpidr_el1
-	add	\reg, \reg, \tmp
+	add	\dst, \dst, \tmp
+	.endm
+
+	/*
+	 * @dst: Result of READ_ONCE(per_cpu(sym, smp_processor_id()))
+	 * @sym: The name of the per-cpu variable
+	 * @tmp: scratch register
+	 */
+	.macro ldr_this_cpu dst, sym, tmp
+	adr_l	\dst, \sym
+	mrs	\tmp, tpidr_el1
+	ldr	\dst, [\dst, \tmp]
 	.endm
 
 /*

commit f99a250cb6a3b301b101b4c0f5fcb80593bba6dc
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 6 16:40:23 2016 +0100

    arm64: barriers: introduce nops and __nops macros for NOP sequences
    
    NOP sequences tend to get used for padding out alternative sections
    and uarch-specific pipeline flushes in errata workarounds.
    
    This patch adds macros for generating these sequences as both inline
    asm blocks, but also as strings suitable for embedding in other asm
    blocks directly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index f09a5ae48a44..28bfe6132eb6 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -86,6 +86,15 @@
 	dmb	\opt
 	.endm
 
+/*
+ * NOP sequence
+ */
+	.macro	nops, num
+	.rept	\num
+	nop
+	.endr
+	.endm
+
 /*
  * Emit an entry into the exception table
  */

commit 116c81f427ff6c5380850963e3fb8798cc821d2b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:16 2016 +0100

    arm64: Work around systems with mismatched cache line sizes
    
    Systems with differing CPU i-cache/d-cache line sizes can cause
    problems with the cache management by software when the execution
    is migrated from one to another. Usually, the application reads
    the cache size on a CPU and then uses that length to perform cache
    operations. However, if it gets migrated to another CPU with a smaller
    cache line size, things could go completely wrong. To prevent such
    cases, always use the smallest cache line size among the CPUs. The
    kernel CPU feature infrastructure already keeps track of the safe
    value for all CPUID registers including CTR. This patch works around
    the problem by :
    
    For kernel, dynamically patch the kernel to read the cache size
    from the system wide copy of CTR_EL0.
    
    For applications, trap read accesses to CTR_EL0 (by clearing the SCTLR.UCT)
    and emulate the mrs instruction to return the system wide safe value
    of CTR_EL0.
    
    For faster access (i.e, avoiding to lookup the system wide value of CTR_EL0
    via read_system_reg), we keep track of the pointer to table entry for
    CTR_EL0 in the CPU feature infrastructure.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index a4bb3f52d9ef..f09a5ae48a44 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -216,6 +216,20 @@ lr	.req	x30		// link register
 	.macro	mmid, rd, rn
 	ldr	\rd, [\rn, #MM_CONTEXT_ID]
 	.endm
+/*
+ * read_ctr - read CTR_EL0. If the system has mismatched
+ * cache line sizes, provide the system wide safe value
+ * from arm64_ftr_reg_ctrel0.sys_val
+ */
+	.macro	read_ctr, reg
+alternative_if_not ARM64_MISMATCHED_CACHE_LINE_SIZE
+	mrs	\reg, ctr_el0			// read CTR
+	nop
+alternative_else
+	ldr_l	\reg, arm64_ftr_reg_ctrel0 + ARM64_FTR_SYSVAL
+alternative_endif
+	.endm
+
 
 /*
  * raw_dcache_line_size - get the minimum D-cache line size on this CPU
@@ -232,7 +246,10 @@ lr	.req	x30		// link register
  * dcache_line_size - get the safe D-cache line size across all CPUs
  */
 	.macro	dcache_line_size, reg, tmp
-	raw_dcache_line_size	\reg, \tmp
+	read_ctr	\tmp
+	ubfm		\tmp, \tmp, #16, #19	// cache line size encoding
+	mov		\reg, #4		// bytes per word
+	lsl		\reg, \reg, \tmp	// actual cache line size
 	.endm
 
 /*
@@ -250,7 +267,10 @@ lr	.req	x30		// link register
  * icache_line_size - get the safe I-cache line size across all CPUs
  */
 	.macro	icache_line_size, reg, tmp
-	raw_icache_line_size	\reg, \tmp
+	read_ctr	\tmp
+	and		\tmp, \tmp, #0xf	// cache line size encoding
+	mov		\reg, #4		// bytes per word
+	lsl		\reg, \reg, \tmp	// actual cache line size
 	.endm
 
 /*

commit 072f0a633838aca13b5a8b211eb64f5c445cfd7c
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:14 2016 +0100

    arm64: Introduce raw_{d,i}cache_line_size
    
    On systems with mismatched i/d cache min line sizes, we need to use
    the smallest size possible across all CPUs. This will be done by fetching
    the system wide safe value from CPU feature infrastructure.
    However the some special users(e.g kexec, hibernate) would need the line
    size on the CPU (rather than the system wide), when either the system
    wide feature may not be accessible or it is guranteed that the caller
    executes with a gurantee of no migration.
    Provide another helper which will fetch cache line size on the current CPU.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: James Morse <james.morse@arm.com>
    Reviewed-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index d5025c69ca81..a4bb3f52d9ef 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -218,9 +218,10 @@ lr	.req	x30		// link register
 	.endm
 
 /*
- * dcache_line_size - get the minimum D-cache line size from the CTR register.
+ * raw_dcache_line_size - get the minimum D-cache line size on this CPU
+ * from the CTR register.
  */
-	.macro	dcache_line_size, reg, tmp
+	.macro	raw_dcache_line_size, reg, tmp
 	mrs	\tmp, ctr_el0			// read CTR
 	ubfm	\tmp, \tmp, #16, #19		// cache line size encoding
 	mov	\reg, #4			// bytes per word
@@ -228,15 +229,30 @@ lr	.req	x30		// link register
 	.endm
 
 /*
- * icache_line_size - get the minimum I-cache line size from the CTR register.
+ * dcache_line_size - get the safe D-cache line size across all CPUs
  */
-	.macro	icache_line_size, reg, tmp
+	.macro	dcache_line_size, reg, tmp
+	raw_dcache_line_size	\reg, \tmp
+	.endm
+
+/*
+ * raw_icache_line_size - get the minimum I-cache line size on this CPU
+ * from the CTR register.
+ */
+	.macro	raw_icache_line_size, reg, tmp
 	mrs	\tmp, ctr_el0			// read CTR
 	and	\tmp, \tmp, #0xf		// cache line size encoding
 	mov	\reg, #4			// bytes per word
 	lsl	\reg, \reg, \tmp		// actual cache line size
 	.endm
 
+/*
+ * icache_line_size - get the safe I-cache line size across all CPUs
+ */
+	.macro	icache_line_size, reg, tmp
+	raw_icache_line_size	\reg, \tmp
+	.endm
+
 /*
  * tcr_set_idmap_t0sz - update TCR.T0SZ so that we can load the ID map
  */

commit 823066d9edcdfe4cedb06216c2b1f91efaf68a87
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:29 2016 +0100

    arm64: include alternative handling in dcache_by_line_op
    
    The newly introduced dcache_by_line_op macro is used at least in
    one occassion at the moment to issue a "dc cvau" instruction,
    which is affected by ARM errata 819472, 826319, 827319 and 824069.
    Change the macro to allow for alternative patching in there to
    protect affected Cortex-A53 cores.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [catalin.marinas@arm.com: indentation fixups]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 10b017c4bdd8..d5025c69ca81 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -24,6 +24,7 @@
 #define __ASM_ASSEMBLER_H
 
 #include <asm/asm-offsets.h>
+#include <asm/cpufeature.h>
 #include <asm/page.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
@@ -261,7 +262,16 @@ lr	.req	x30		// link register
 	add	\size, \kaddr, \size
 	sub	\tmp2, \tmp1, #1
 	bic	\kaddr, \kaddr, \tmp2
-9998:	dc	\op, \kaddr
+9998:
+	.if	(\op == cvau || \op == cvac)
+alternative_if_not ARM64_WORKAROUND_CLEAN_CACHE
+	dc	\op, \kaddr
+alternative_else
+	dc	civac, \kaddr
+alternative_endif
+	.else
+	dc	\op, \kaddr
+	.endif
 	add	\kaddr, \kaddr, \tmp1
 	cmp	\kaddr, \size
 	b.lo	9998b

commit 5003dbde45961dd7ab3d8a09ab9ad8bcb604db40
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Apr 27 17:47:10 2016 +0100

    arm64: Add new asm macro copy_page
    
    Kexec and hibernate need to copy pages of memory, but may not have all
    of the kernel mapped, and are unable to call copy_page().
    
    Add a simplistic copy_page() macro, that can be inlined in these
    situations. lib/copy_page.S provides a bigger better version, but
    uses more registers.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    [Changed asm label to 9998, added commit message]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 3b3812a3e8b9..10b017c4bdd8 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -24,6 +24,7 @@
 #define __ASM_ASSEMBLER_H
 
 #include <asm/asm-offsets.h>
+#include <asm/page.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
 #include <asm/thread_info.h>
@@ -279,6 +280,24 @@ lr	.req	x30		// link register
 9000:
 	.endm
 
+/*
+ * copy_page - copy src to dest using temp registers t1-t8
+ */
+	.macro copy_page dest:req src:req t1:req t2:req t3:req t4:req t5:req t6:req t7:req t8:req
+9998:	ldp	\t1, \t2, [\src]
+	ldp	\t3, \t4, [\src, #16]
+	ldp	\t5, \t6, [\src, #32]
+	ldp	\t7, \t8, [\src, #48]
+	add	\src, \src, #64
+	stnp	\t1, \t2, [\dest]
+	stnp	\t3, \t4, [\dest, #16]
+	stnp	\t5, \t6, [\dest, #32]
+	stnp	\t7, \t8, [\dest, #48]
+	add	\dest, \dest, #64
+	tst	\src, #(PAGE_SIZE - 1)
+	b.ne	9998b
+	.endm
+
 /*
  * Annotate a function as position independent, i.e., safe to be called before
  * the kernel virtual mapping is activated.

commit 7b7293ae3dbd0a1965bf310b77fed5f9bb37bb93
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Apr 27 17:47:00 2016 +0100

    arm64: Fold proc-macros.S into assembler.h
    
    To allow the assembler macros defined in arch/arm64/mm/proc-macros.S to
    be used outside the mm code move the contents of proc-macros.S to
    asm/assembler.h.  Also, delete proc-macros.S, and fix up all references
    to proc-macros.S.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Acked-by: Pavel Machek <pavel@ucw.cz>
    [rebased, included dcache_by_line_op]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index f9b6f7af75ac..3b3812a3e8b9 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -1,5 +1,5 @@
 /*
- * Based on arch/arm/include/asm/assembler.h
+ * Based on arch/arm/include/asm/assembler.h, arch/arm/mm/proc-macros.S
  *
  * Copyright (C) 1996-2000 Russell King
  * Copyright (C) 2012 ARM Ltd.
@@ -23,6 +23,8 @@
 #ifndef __ASM_ASSEMBLER_H
 #define __ASM_ASSEMBLER_H
 
+#include <asm/asm-offsets.h>
+#include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
 #include <asm/thread_info.h>
 
@@ -199,6 +201,84 @@ lr	.req	x30		// link register
 	add	\reg, \reg, \tmp
 	.endm
 
+/*
+ * vma_vm_mm - get mm pointer from vma pointer (vma->vm_mm)
+ */
+	.macro	vma_vm_mm, rd, rn
+	ldr	\rd, [\rn, #VMA_VM_MM]
+	.endm
+
+/*
+ * mmid - get context id from mm pointer (mm->context.id)
+ */
+	.macro	mmid, rd, rn
+	ldr	\rd, [\rn, #MM_CONTEXT_ID]
+	.endm
+
+/*
+ * dcache_line_size - get the minimum D-cache line size from the CTR register.
+ */
+	.macro	dcache_line_size, reg, tmp
+	mrs	\tmp, ctr_el0			// read CTR
+	ubfm	\tmp, \tmp, #16, #19		// cache line size encoding
+	mov	\reg, #4			// bytes per word
+	lsl	\reg, \reg, \tmp		// actual cache line size
+	.endm
+
+/*
+ * icache_line_size - get the minimum I-cache line size from the CTR register.
+ */
+	.macro	icache_line_size, reg, tmp
+	mrs	\tmp, ctr_el0			// read CTR
+	and	\tmp, \tmp, #0xf		// cache line size encoding
+	mov	\reg, #4			// bytes per word
+	lsl	\reg, \reg, \tmp		// actual cache line size
+	.endm
+
+/*
+ * tcr_set_idmap_t0sz - update TCR.T0SZ so that we can load the ID map
+ */
+	.macro	tcr_set_idmap_t0sz, valreg, tmpreg
+#ifndef CONFIG_ARM64_VA_BITS_48
+	ldr_l	\tmpreg, idmap_t0sz
+	bfi	\valreg, \tmpreg, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
+#endif
+	.endm
+
+/*
+ * Macro to perform a data cache maintenance for the interval
+ * [kaddr, kaddr + size)
+ *
+ * 	op:		operation passed to dc instruction
+ * 	domain:		domain used in dsb instruciton
+ * 	kaddr:		starting virtual address of the region
+ * 	size:		size of the region
+ * 	Corrupts:	kaddr, size, tmp1, tmp2
+ */
+	.macro dcache_by_line_op op, domain, kaddr, size, tmp1, tmp2
+	dcache_line_size \tmp1, \tmp2
+	add	\size, \kaddr, \size
+	sub	\tmp2, \tmp1, #1
+	bic	\kaddr, \kaddr, \tmp2
+9998:	dc	\op, \kaddr
+	add	\kaddr, \kaddr, \tmp1
+	cmp	\kaddr, \size
+	b.lo	9998b
+	dsb	\domain
+	.endm
+
+/*
+ * reset_pmuserenr_el0 - reset PMUSERENR_EL0 if PMUv3 present
+ */
+	.macro	reset_pmuserenr_el0, tmpreg
+	mrs	\tmpreg, id_aa64dfr0_el1	// Check ID_AA64DFR0_EL1 PMUVer
+	sbfx	\tmpreg, \tmpreg, #8, #4
+	cmp	\tmpreg, #1			// Skip if no PMU present
+	b.lt	9000f
+	msr	pmuserenr_el0, xzr		// Disable PMU access from EL0
+9000:
+	.endm
+
 /*
  * Annotate a function as position independent, i.e., safe to be called before
  * the kernel virtual mapping is activated.

commit 30b5ba5cf333cc650e474eaf2cc1ae91bc7cf89f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Apr 18 17:09:44 2016 +0200

    arm64: introduce mov_q macro to move a constant into a 64-bit register
    
    Implement a macro mov_q that can be used to move an immediate constant
    into a 64-bit register, using between 2 and 4 movz/movk instructions
    (depending on the operand)
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 972fb55af9f1..f9b6f7af75ac 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -221,4 +221,24 @@ lr	.req	x30		// link register
 	.long	\sym\()_hi32
 	.endm
 
+	/*
+	 * mov_q - move an immediate constant into a 64-bit register using
+	 *         between 2 and 4 movz/movk instructions (depending on the
+	 *         magnitude and sign of the operand)
+	 */
+	.macro	mov_q, reg, val
+	.if (((\val) >> 31) == 0 || ((\val) >> 31) == 0x1ffffffff)
+	movz	\reg, :abs_g1_s:\val
+	.else
+	.if (((\val) >> 47) == 0 || ((\val) >> 47) == 0x1ffff)
+	movz	\reg, :abs_g2_s:\val
+	.else
+	movz	\reg, :abs_g3:\val
+	movk	\reg, :abs_g2_nc:\val
+	.endif
+	movk	\reg, :abs_g1_nc:\val
+	.endif
+	movk	\reg, :abs_g0_nc:\val
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 2ff4936c1d68060b080aac49ec622b047f9e6c45
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 19 10:31:19 2016 +0100

    arm64: asm: remove unused push/pop macros
    
    We haven't used the push/pop macros for a while now, as it's typically
    better to use immediate offsets for batches of accesses to the stack, as
    we now do in the entry assembly for the kernel and hyp code.
    
    Remove the unused macros.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 70f7b9e04598..972fb55af9f1 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -26,18 +26,6 @@
 #include <asm/ptrace.h>
 #include <asm/thread_info.h>
 
-/*
- * Stack pushing/popping (register pairs only). Equivalent to store decrement
- * before, load increment after.
- */
-	.macro	push, xreg1, xreg2
-	stp	\xreg1, \xreg2, [sp, #-16]!
-	.endm
-
-	.macro	pop, xreg1, xreg2
-	ldp	\xreg1, \xreg2, [sp], #16
-	.endm
-
 /*
  * Enable and disable interrupts.
  */

commit 6c94f27ac847ff8ef15b3da5b200574923bd6287
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Jan 1 15:02:12 2016 +0100

    arm64: switch to relative exception tables
    
    Instead of using absolute addresses for both the exception location
    and the fixup, use offsets relative to the exception table entry values.
    Not only does this cut the size of the exception table in half, it is
    also a prerequisite for KASLR, since absolute exception table entries
    are subject to dynamic relocation, which is incompatible with the sorting
    of the exception table that occurs at build time.
    
    This patch also introduces the _ASM_EXTABLE preprocessor macro (which
    exists on x86 as well) and its _asm_extable assembly counterpart, as
    shorthands to emit exception table entries.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index ba5aff6c830e..70f7b9e04598 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -94,12 +94,19 @@
 	dmb	\opt
 	.endm
 
+/*
+ * Emit an entry into the exception table
+ */
+	.macro		_asm_extable, from, to
+	.pushsection	__ex_table, "a"
+	.align		3
+	.long		(\from - .), (\to - .)
+	.popsection
+	.endm
+
 #define USER(l, x...)				\
 9999:	x;					\
-	.section __ex_table,"a";		\
-	.align	3;				\
-	.quad	9999b,l;			\
-	.previous
+	_asm_extable	9999b, l
 
 /*
  * Register aliases.

commit 6ad1fe5d9077a1ab40bf74b61994d2e770b00b14
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Dec 26 13:48:02 2015 +0100

    arm64: avoid R_AARCH64_ABS64 relocations for Image header fields
    
    Unfortunately, the current way of using the linker to emit build time
    constants into the Image header will no longer work once we switch to
    the use of PIE executables. The reason is that such constants are emitted
    into the binary using R_AARCH64_ABS64 relocations, which are resolved at
    runtime, not at build time, and the places targeted by those relocations
    will contain zeroes before that.
    
    So refactor the endian swapping linker script constant generation code so
    that it emits the upper and lower 32-bit words separately.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index bb7b72734c24..ba5aff6c830e 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -215,4 +215,15 @@ lr	.req	x30		// link register
 	.size	__pi_##x, . - x;	\
 	ENDPROC(x)
 
+	/*
+	 * Emit a 64-bit absolute little endian symbol reference in a way that
+	 * ensures that it will be resolved at build time, even when building a
+	 * PIE binary. This requires cooperation from the linker script, which
+	 * must emit the lo32/hi32 halves individually.
+	 */
+	.macro	le64sym, sym
+	.long	\sym\()_lo32
+	.long	\sym\()_hi32
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit aa4d5d3cbc258c355151a3903211b27359390ec5
Author: James Morse <james.morse@arm.com>
Date:   Thu Dec 10 10:22:39 2015 +0000

    arm64: Add this_cpu_ptr() assembler macro for use in entry.S
    
    irq_stack is a per_cpu variable, that needs to be access from entry.S.
    Use an assembler macro instead of the unreadable details.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 12eff928ef8b..bb7b72734c24 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -193,6 +193,17 @@ lr	.req	x30		// link register
 	str	\src, [\tmp, :lo12:\sym]
 	.endm
 
+	/*
+	 * @sym: The name of the per-cpu variable
+	 * @reg: Result of per_cpu(sym, smp_processor_id())
+	 * @tmp: scratch register
+	 */
+	.macro this_cpu_ptr, sym, reg, tmp
+	adr_l	\reg, \sym
+	mrs	\tmp, tpidr_el1
+	add	\reg, \reg, \tmp
+	.endm
+
 /*
  * Annotate a function as position independent, i.e., safe to be called before
  * the kernel virtual mapping is activated.

commit 207918461eb0aca720fddec5da79bc71c133b9f1
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Oct 8 20:02:03 2015 +0100

    arm64: use ENDPIPROC() to annotate position independent assembler routines
    
    For more control over which functions are called with the MMU off or
    with the UEFI 1:1 mapping active, annotate some assembler routines as
    position independent. This is done by introducing ENDPIPROC(), which
    replaces the ENDPROC() declaration of those routines.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index b51f2cc22ca9..12eff928ef8b 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -193,4 +193,15 @@ lr	.req	x30		// link register
 	str	\src, [\tmp, :lo12:\sym]
 	.endm
 
+/*
+ * Annotate a function as position independent, i.e., safe to be called before
+ * the kernel virtual mapping is activated.
+ */
+#define ENDPIPROC(x)			\
+	.globl	__pi_##x;		\
+	.type 	__pi_##x, %function;	\
+	.set	__pi_##x, x;		\
+	.size	__pi_##x, . - x;	\
+	ENDPROC(x)
+
 #endif	/* __ASM_ASSEMBLER_H */

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index e10516bbe833..b51f2cc22ca9 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -91,9 +91,7 @@
  * SMP data memory barrier
  */
 	.macro	smp_dmb, opt
-#ifdef CONFIG_SMP
 	dmb	\opt
-#endif
 	.endm
 
 #define USER(l, x...)				\

commit d3127afa712321a2b297cfee358be2cb223f933c
Author: Daniel Thompson <daniel.thompson@linaro.org>
Date:   Fri Jul 10 14:58:00 2015 +0100

    arm64: Remove unused macros from assembler.h
    
    Commit 68234df4ea79 ("arm64: kill flush_cache_all()") removed the
    only users of these macros.
    
    Signed-off-by: Daniel Thompson <daniel.thompson@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 144b64ad96c3..e10516bbe833 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -49,18 +49,6 @@
 	msr	daifclr, #2
 	.endm
 
-/*
- * Save/disable and restore interrupts.
- */
-	.macro	save_and_disable_irqs, olddaif
-	mrs	\olddaif, daif
-	disable_irq
-	.endm
-
-	.macro	restore_irqs, olddaif
-	msr	daif, \olddaif
-	.endm
-
 /*
  * Enable and disable debug exceptions.
  */

commit b784a5d97d0af4835dd0125a3e0e5d0fd48128d6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 4 19:45:38 2015 +0100

    arm64: add macros for common adrp usages
    
    The adrp instruction is mostly used in combination with either
    an add, a ldr or a str instruction with the low bits of the
    referenced symbol in the 12-bit immediate of the followup
    instruction.
    
    Introduce the macros adr_l, ldr_l and str_l that encapsulate
    these common patterns.
    
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 750bac4e637e..144b64ad96c3 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -159,4 +159,52 @@ lr	.req	x30		// link register
 	orr	\rd, \lbits, \hbits, lsl #32
 	.endm
 
+/*
+ * Pseudo-ops for PC-relative adr/ldr/str <reg>, <symbol> where
+ * <symbol> is within the range +/- 4 GB of the PC.
+ */
+	/*
+	 * @dst: destination register (64 bit wide)
+	 * @sym: name of the symbol
+	 * @tmp: optional scratch register to be used if <dst> == sp, which
+	 *       is not allowed in an adrp instruction
+	 */
+	.macro	adr_l, dst, sym, tmp=
+	.ifb	\tmp
+	adrp	\dst, \sym
+	add	\dst, \dst, :lo12:\sym
+	.else
+	adrp	\tmp, \sym
+	add	\dst, \tmp, :lo12:\sym
+	.endif
+	.endm
+
+	/*
+	 * @dst: destination register (32 or 64 bit wide)
+	 * @sym: name of the symbol
+	 * @tmp: optional 64-bit scratch register to be used if <dst> is a
+	 *       32-bit wide register, in which case it cannot be used to hold
+	 *       the address
+	 */
+	.macro	ldr_l, dst, sym, tmp=
+	.ifb	\tmp
+	adrp	\dst, \sym
+	ldr	\dst, [\dst, :lo12:\sym]
+	.else
+	adrp	\tmp, \sym
+	ldr	\dst, [\tmp, :lo12:\sym]
+	.endif
+	.endm
+
+	/*
+	 * @src: source register (32 or 64 bit wide)
+	 * @sym: name of the symbol
+	 * @tmp: mandatory 64-bit scratch register to calculate the address
+	 *       while <src> needs to be preserved.
+	 */
+	.macro	str_l, src, sym, tmp
+	adrp	\tmp, \sym
+	str	\src, [\tmp, :lo12:\sym]
+	.endm
+
 #endif	/* __ASM_ASSEMBLER_H */

commit f3e39273e0a9a5c9dc78cd667ec3663e97e0e989
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Feb 20 13:53:13 2015 +0000

    arm64: guard asm/assembler.h against multiple inclusions
    
    asm/assembler.h lacks the usual guard against multiple inclusion,
    leading to a compilation failure if it is accidentally included
    twice.
    
    Using the classic #ifndef/#define/#endif construct solves the issue.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 5901480bfdca..750bac4e637e 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -20,6 +20,9 @@
 #error "Only include this from assembly code"
 #endif
 
+#ifndef __ASM_ASSEMBLER_H
+#define __ASM_ASSEMBLER_H
+
 #include <asm/ptrace.h>
 #include <asm/thread_info.h>
 
@@ -155,3 +158,5 @@ lr	.req	x30		// link register
 #endif
 	orr	\rd, \lbits, \hbits, lsl #32
 	.endm
+
+#endif	/* __ASM_ASSEMBLER_H */

commit 2a2830703a2371b47f7b50b1d35cb15dc0e2b717
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Apr 29 19:04:06 2014 +0100

    arm64: debug: avoid accessing mdscr_el1 on fault paths where possible
    
    Since mdscr_el1 is part of the debug register group, it is highly likely
    to be trapped by a hypervisor to prevent virtual machines from debugging
    (buggering?) each other. Unfortunately, this absolutely destroys our
    performance, since we access the register on many of our low-level
    fault handling paths to keep track of the various debug state machines.
    
    This patch removes our dependency on mdscr_el1 in the case that debugging
    is not being used. More specifically we:
    
      - Use TIF_SINGLESTEP to indicate that a task is stepping at EL0 and
        avoid disabling step in the MDSCR when we don't need to.
        MDSCR_EL1.SS handling is moved to kernel_entry, when trapping from
        userspace.
    
      - Ensure debug exceptions are re-enabled on *all* exception entry
        paths, even the debug exception handling path (where we re-enable
        exceptions after invoking the handler). Since we can now rely on
        MDSCR_EL1.SS being cleared by the entry code, exception handlers can
        usually enable debug immediately before enabling interrupts.
    
      - Remove all debug exception unmasking from ret_to_user and
        el1_preempt, since we will never get here with debug exceptions
        masked.
    
    This results in a slight change to kernel debug behaviour, where we now
    step into interrupt handlers and data aborts from EL1 when debugging the
    kernel, which is actually a useful thing to do. A side-effect of this is
    that it *does* potentially prevent stepping off {break,watch}points when
    there is a high-frequency interrupt source (e.g. a timer), so a debugger
    would need to use either breakpoints or manually disable interrupts to
    get around this issue.
    
    With this patch applied, guest performance is restored under KVM when
    debug register accesses are trapped (and we get a measurable performance
    increase on the host on Cortex-A57 too).
    
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index fd3e3924041b..5901480bfdca 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -21,6 +21,7 @@
 #endif
 
 #include <asm/ptrace.h>
+#include <asm/thread_info.h>
 
 /*
  * Stack pushing/popping (register pairs only). Equivalent to store decrement
@@ -68,23 +69,31 @@
 	msr	daifclr, #8
 	.endm
 
-	.macro	disable_step, tmp
+	.macro	disable_step_tsk, flgs, tmp
+	tbz	\flgs, #TIF_SINGLESTEP, 9990f
 	mrs	\tmp, mdscr_el1
 	bic	\tmp, \tmp, #1
 	msr	mdscr_el1, \tmp
+	isb	// Synchronise with enable_dbg
+9990:
 	.endm
 
-	.macro	enable_step, tmp
+	.macro	enable_step_tsk, flgs, tmp
+	tbz	\flgs, #TIF_SINGLESTEP, 9990f
+	disable_dbg
 	mrs	\tmp, mdscr_el1
 	orr	\tmp, \tmp, #1
 	msr	mdscr_el1, \tmp
+9990:
 	.endm
 
-	.macro	enable_dbg_if_not_stepping, tmp
-	mrs	\tmp, mdscr_el1
-	tbnz	\tmp, #0, 9990f
-	enable_dbg
-9990:
+/*
+ * Enable both debug exceptions and interrupts. This is likely to be
+ * faster than two daifclr operations, since writes to this register
+ * are self-synchronising.
+ */
+	.macro	enable_dbg_and_irq
+	msr	daifclr, #(8 | 2)
 	.endm
 
 /*

commit e68bedaa03c950ae8045e7899e7a6b2a97d1bf41
Author: Matthew Leach <matthew.leach@arm.com>
Date:   Fri Oct 11 14:52:15 2013 +0100

    arm64: asm: add CPU_LE & CPU_BE assembler helpers
    
    Add CPU_LE and CPU_BE to select assembler code in little and big
    endian configurations respectively.
    
    Signed-off-by: Matthew Leach <matthew.leach@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 381b935e74cd..fd3e3924041b 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -115,6 +115,25 @@ lr	.req	x30		// link register
 	.align	7
 	b	\label
 	.endm
+
+/*
+ * Select code when configured for BE.
+ */
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define CPU_BE(code...) code
+#else
+#define CPU_BE(code...)
+#endif
+
+/*
+ * Select code when configured for LE.
+ */
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define CPU_LE(code...)
+#else
+#define CPU_LE(code...) code
+#endif
+
 /*
  * Define a macro that constructs a 64-bit value by concatenating two
  * 32-bit registers. Note that on big endian systems the order of the

commit 55b89540b0d8d031f90e3d711ec0df3f797ecc61
Author: Matthew Leach <matthew.leach@arm.com>
Date:   Fri Oct 11 14:52:13 2013 +0100

    arm64: compat: correct register concatenation for syscall wrappers
    
    The arm64 port contains wrappers for arm32 syscalls that pass 64-bit
    values. These wrappers concatenate the two registers to hold a 64-bit
    value in a single X register. On BE, however, the lower and higher
    words are swapped.
    
    Create a new assembler macro, regs_to_64, that when on BE systems
    swaps the registers in the orr instruction.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Matthew Leach <matthew.leach@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index 5aceb83b3f5c..381b935e74cd 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -115,3 +115,15 @@ lr	.req	x30		// link register
 	.align	7
 	b	\label
 	.endm
+/*
+ * Define a macro that constructs a 64-bit value by concatenating two
+ * 32-bit registers. Note that on big endian systems the order of the
+ * registers is swapped.
+ */
+#ifndef CONFIG_CPU_BIG_ENDIAN
+	.macro	regs_to_64, rd, lbits, hbits
+#else
+	.macro	regs_to_64, rd, hbits, lbits
+#endif
+	orr	\rd, \lbits, \hbits, lsl #32
+	.endm

commit 3126976be64bfb4c87297cb022ca815212079aec
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 17 17:41:22 2013 +0100

    arm64: debug: fix mdscr.ss check when enabling debug exceptions
    
    When we take an exception at EL1, we only want to enable debug
    exceptions if we're not currently stepping, otherwise we can easily get
    stuck in a loop stepping into interrupt handlers.
    
    Unfortunately, the current code tests the wrong bit in the mdscr, so fix
    that.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index c8eedc604984..5aceb83b3f5c 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -82,7 +82,7 @@
 
 	.macro	enable_dbg_if_not_stepping, tmp
 	mrs	\tmp, mdscr_el1
-	tbnz	\tmp, #1, 9990f
+	tbnz	\tmp, #0, 9990f
 	enable_dbg
 9990:
 	.endm

commit dc637f1fdaa6f335271a0341fef3914b80ab929c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Oct 19 17:37:35 2012 +0100

    arm64: move vector entry macro to assembler.h
    
    This macro is also useful to other bits defining vectors (hypervisor
    stub, KVM...).
    
    Move it to a common location.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index da2a13e8f1e6..c8eedc604984 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -107,3 +107,11 @@
  * Register aliases.
  */
 lr	.req	x30		// link register
+
+/*
+ * Vector entry
+ */
+	 .macro	ventry	label
+	.align	7
+	b	\label
+	.endm

commit 0be7320a635c2e434e8b67e0e9474a85ceb421c4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:26 2012 +0000

    arm64: Assembly macros and definitions
    
    This patch introduces several assembly macros and definitions used in
    the .S files across arch/arm64/ like IRQ disabling/enabling, together
    with asm-offsets.c.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
new file mode 100644
index 000000000000..da2a13e8f1e6
--- /dev/null
+++ b/arch/arm64/include/asm/assembler.h
@@ -0,0 +1,109 @@
+/*
+ * Based on arch/arm/include/asm/assembler.h
+ *
+ * Copyright (C) 1996-2000 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASSEMBLY__
+#error "Only include this from assembly code"
+#endif
+
+#include <asm/ptrace.h>
+
+/*
+ * Stack pushing/popping (register pairs only). Equivalent to store decrement
+ * before, load increment after.
+ */
+	.macro	push, xreg1, xreg2
+	stp	\xreg1, \xreg2, [sp, #-16]!
+	.endm
+
+	.macro	pop, xreg1, xreg2
+	ldp	\xreg1, \xreg2, [sp], #16
+	.endm
+
+/*
+ * Enable and disable interrupts.
+ */
+	.macro	disable_irq
+	msr	daifset, #2
+	.endm
+
+	.macro	enable_irq
+	msr	daifclr, #2
+	.endm
+
+/*
+ * Save/disable and restore interrupts.
+ */
+	.macro	save_and_disable_irqs, olddaif
+	mrs	\olddaif, daif
+	disable_irq
+	.endm
+
+	.macro	restore_irqs, olddaif
+	msr	daif, \olddaif
+	.endm
+
+/*
+ * Enable and disable debug exceptions.
+ */
+	.macro	disable_dbg
+	msr	daifset, #8
+	.endm
+
+	.macro	enable_dbg
+	msr	daifclr, #8
+	.endm
+
+	.macro	disable_step, tmp
+	mrs	\tmp, mdscr_el1
+	bic	\tmp, \tmp, #1
+	msr	mdscr_el1, \tmp
+	.endm
+
+	.macro	enable_step, tmp
+	mrs	\tmp, mdscr_el1
+	orr	\tmp, \tmp, #1
+	msr	mdscr_el1, \tmp
+	.endm
+
+	.macro	enable_dbg_if_not_stepping, tmp
+	mrs	\tmp, mdscr_el1
+	tbnz	\tmp, #1, 9990f
+	enable_dbg
+9990:
+	.endm
+
+/*
+ * SMP data memory barrier
+ */
+	.macro	smp_dmb, opt
+#ifdef CONFIG_SMP
+	dmb	\opt
+#endif
+	.endm
+
+#define USER(l, x...)				\
+9999:	x;					\
+	.section __ex_table,"a";		\
+	.align	3;				\
+	.quad	9999b,l;			\
+	.previous
+
+/*
+ * Register aliases.
+ */
+lr	.req	x30		// link register
