commit 3cd86a58f7734bf9cef38f6f899608ebcaa3da13
Merge: a8222fd5b80c b2a84de2a2de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 10:05:01 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The bulk is in-kernel pointer authentication, activity monitors and
      lots of asm symbol annotations. I also queued the sys_mremap() patch
      commenting the asymmetry in the address untagging.
    
      Summary:
    
       - In-kernel Pointer Authentication support (previously only offered
         to user space).
    
       - ARM Activity Monitors (AMU) extension support allowing better CPU
         utilisation numbers for the scheduler (frequency invariance).
    
       - Memory hot-remove support for arm64.
    
       - Lots of asm annotations (SYM_*) in preparation for the in-kernel
         Branch Target Identification (BTI) support.
    
       - arm64 perf updates: ARMv8.5-PMU 64-bit counters, refactoring the
         PMU init callbacks, support for new DT compatibles.
    
       - IPv6 header checksum optimisation.
    
       - Fixes: SDEI (software delegated exception interface) double-lock on
         hibernate with shared events.
    
       - Minor clean-ups and refactoring: cpu_ops accessor,
         cpu_do_switch_mm() converted to C, cpufeature finalisation helper.
    
       - sys_mremap() comment explaining the asymmetric address untagging
         behaviour"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (81 commits)
      mm/mremap: Add comment explaining the untagging behaviour of mremap()
      arm64: head: Convert install_el2_stub to SYM_INNER_LABEL
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
      arm64: move kimage_vaddr to .rodata
      arm64: use mov_q instead of literal ldr
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      ...

commit f511e079177a9b97175a9a3b0ee2374d55682403
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Mar 20 14:53:44 2020 +0000

    arm64: Introduce asm/vdso/processor.h
    
    The vDSO library should only include the necessary headers required for
    a userspace library (UAPI and a minimal set of kernel headers). To make
    this possible it is necessary to isolate from the kernel headers the
    common parts that are strictly necessary to build the library.
    
    Introduce asm/vdso/processor.h to contain all the arm64 specific
    functions that are suitable for vDSO inclusion.
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200320145351.32292-20-vincenzo.frascino@arm.com

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5ba63204d078..e51ef2dc5749 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -28,6 +28,8 @@
 #include <linux/string.h>
 #include <linux/thread_info.h>
 
+#include <vdso/processor.h>
+
 #include <asm/alternative.h>
 #include <asm/cpufeature.h>
 #include <asm/hw_breakpoint.h>
@@ -256,11 +258,6 @@ extern void release_thread(struct task_struct *);
 
 unsigned long get_wchan(struct task_struct *p);
 
-static inline void cpu_relax(void)
-{
-	asm volatile("yield" ::: "memory");
-}
-
 /* Thread switching */
 extern struct task_struct *cpu_switch_to(struct task_struct *prev,
 					 struct task_struct *next);

commit 33e45234987ea3ed4b05fc512f4441696478f12d
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:56 2020 +0530

    arm64: initialize and switch ptrauth kernel keys
    
    Set up keys to use pointer authentication within the kernel. The kernel
    will be compiled with APIAKey instructions, the other keys are currently
    unused. Each task is given its own APIAKey, which is initialized during
    fork. The key is changed during context switch and on kernel entry from
    EL0.
    
    The keys for idle threads need to be set before calling any C functions,
    because it is not possible to enter and exit a function with different
    keys.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Modified secondary cores key structure, comments]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 496a92873290..4c77da5dc819 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -147,6 +147,7 @@ struct thread_struct {
 	struct debug_info	debug;		/* debugging */
 #ifdef CONFIG_ARM64_PTR_AUTH
 	struct ptrauth_keys_user	keys_user;
+	struct ptrauth_keys_kernel	keys_kernel;
 #endif
 };
 

commit 91a1b6ccff323e60615e3118eceb2d8cbc4f69ab
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:50 2020 +0530

    arm64: rename ptrauth key structures to be user-specific
    
    We currently enable ptrauth for userspace, but do not use it within the
    kernel. We're going to enable it for the kernel, and will need to manage
    a separate set of ptrauth keys for the kernel.
    
    We currently keep all 5 keys in struct ptrauth_keys. However, as the
    kernel will only need to use 1 key, it is a bit wasteful to allocate a
    whole ptrauth_keys struct for every thread.
    
    Therefore, a subsequent patch will define a separate struct, with only 1
    key, for the kernel. In preparation for that, rename the existing struct
    (and associated macros and functions) to reflect that they are specific
    to userspace.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Re-positioned the patch to reduce the diff]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5ba63204d078..496a92873290 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -146,7 +146,7 @@ struct thread_struct {
 	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */
 #ifdef CONFIG_ARM64_PTR_AUTH
-	struct ptrauth_keys	keys_user;
+	struct ptrauth_keys_user	keys_user;
 #endif
 };
 

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit 218564b164ad9d283d3cb3d5367705726123a610
Author: Bhupesh Sharma <bhsharma@redhat.com>
Date:   Tue Nov 5 03:26:46 2019 +0530

    arm64: mm: Remove MAX_USER_VA_BITS definition
    
    commit 9b31cf493ffa ("arm64: mm: Introduce MAX_USER_VA_BITS definition")
    introduced the MAX_USER_VA_BITS definition, which was used to support
    the arm64 mm use-cases where the user-space could use 52-bit virtual
    addresses whereas the kernel-space would still could a maximum of 48-bit
    virtual addressing.
    
    But, now with commit b6d00d47e81a ("arm64: mm: Introduce 52-bit Kernel
    VAs"), we removed the 52-bit user/48-bit kernel kconfig option and hence
    there is no longer any scenario where user VA != kernel VA size
    (even with CONFIG_ARM64_FORCE_52BIT enabled, the same is true).
    
    Hence we can do away with the MAX_USER_VA_BITS macro as it is equal to
    VA_BITS (maximum VA space size) in all possible use-cases. Note that
    even though the 'vabits_actual' value would be 48 for arm64 hardware
    which don't support LVA-8.2 extension (even when CONFIG_ARM64_VA_BITS_52
    is enabled), VA_BITS would still be set to a value 52. Hence this change
    would be safe in all possible VA address space combinations.
    
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Steve Capper <steve.capper@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: linux-kernel@vger.kernel.org
    Cc: kexec@lists.infradead.org
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Bhupesh Sharma <bhsharma@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5623685c7d13..586fcd4b1965 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -9,7 +9,7 @@
 #define __ASM_PROCESSOR_H
 
 #define KERNEL_DS		UL(-1)
-#define USER_DS			((UL(1) << MAX_USER_VA_BITS) - 1)
+#define USER_DS			((UL(1) << VA_BITS) - 1)
 
 /*
  * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is

commit bfe298745afc9548ad9344a9a3f26c81fd1a76c4
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:16 2019 +0100

    arm64: entry-common: don't touch daif before bp-hardening
    
    The previous patches mechanically transformed the assembly version of
    entry.S to entry-common.c for synchronous exceptions.
    
    The C version of local_daif_restore() doesn't quite do the same thing
    as the assembly versions if pseudo-NMI is in use. In particular,
    | local_daif_restore(DAIF_PROCCTX_NOIRQ)
    will still allow pNMI to be delivered. This is not the behaviour
    do_el0_ia_bp_hardening() and do_sp_pc_abort() want as it should not
    be possible for the PMU handler to run as an NMI until the bp-hardening
    sequence has run.
    
    The bp-hardening calls were placed where they are because this was the
    first C code to run after the relevant exceptions. As we've now moved
    that point earlier, move the checks and calls earlier too.
    
    This makes it clearer that this stuff runs before any kind of exception,
    and saves modifying PSTATE twice.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5623685c7d13..8899d26f73ff 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -26,10 +26,12 @@
 #include <linux/init.h>
 #include <linux/stddef.h>
 #include <linux/string.h>
+#include <linux/thread_info.h>
 
 #include <asm/alternative.h>
 #include <asm/cpufeature.h>
 #include <asm/hw_breakpoint.h>
+#include <asm/kasan.h>
 #include <asm/lse.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/pointer_auth.h>
@@ -214,6 +216,18 @@ static inline void start_thread(struct pt_regs *regs, unsigned long pc,
 	regs->sp = sp;
 }
 
+static inline bool is_ttbr0_addr(unsigned long addr)
+{
+	/* entry assembly clears tags for TTBR0 addrs */
+	return addr < TASK_SIZE;
+}
+
+static inline bool is_ttbr1_addr(unsigned long addr)
+{
+	/* TTBR1 addresses may have a tag if KASAN_SW_TAGS is in use */
+	return arch_kasan_reset_tag(addr) >= PAGE_OFFSET;
+}
+
 #ifdef CONFIG_COMPAT
 static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
 				       unsigned long sp)

commit 67f3977f805b34cf0e41090679800d2091d41d49
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Mon Sep 23 15:38:47 2019 -0700

    arm64, mm: move generic mmap layout functions to mm
    
    arm64 handles top-down mmap layout in a way that can be easily reused by
    other architectures, so make it available in mm.  It then introduces a new
    config ARCH_WANT_DEFAULT_TOPDOWN_MMAP_LAYOUT that can be set by other
    architectures to benefit from those functions.  Note that this new config
    depends on MMU being enabled, if selected without MMU support, a warning
    will be thrown.
    
    Link: http://lkml.kernel.org/r/20190730055113.23635-5-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Luis Chamberlain <mcgrof@kernel.org>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index c67848c55009..5623685c7d13 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -280,8 +280,6 @@ static inline void spin_lock_prefetch(const void *ptr)
 		     "nop") : : "p" (ptr));
 }
 
-#define HAVE_ARCH_PICK_MMAP_LAYOUT
-
 extern unsigned long __ro_after_init signal_minsigstksz; /* sigframe size */
 extern void __init minsigstksz_setup(void);
 

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit 2c624fe68715e76eba1a7089f91e122310dc663c
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:23 2019 +0100

    arm64: mm: Remove vabits_user
    
    Previous patches have enabled 52-bit kernel + user VAs and there is no
    longer any scenario where user VA != kernel VA size.
    
    This patch removes the, now redundant, vabits_user variable and replaces
    usage with vabits_actual where appropriate.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 0e1f2770192a..e4c93945e477 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -43,7 +43,7 @@
  */
 
 #define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS_MIN)
-#define TASK_SIZE_64		(UL(1) << vabits_user)
+#define TASK_SIZE_64		(UL(1) << vabits_actual)
 
 #ifdef CONFIG_COMPAT
 #if defined(CONFIG_ARM64_64K_PAGES) && defined(CONFIG_KUSER_HELPERS)

commit 90ec95cda91a021d82351c976896a63aa364ebf1
Author: Steve Capper <steve.capper@arm.com>
Date:   Wed Aug 7 16:55:17 2019 +0100

    arm64: mm: Introduce VA_BITS_MIN
    
    In order to support 52-bit kernel addresses detectable at boot time, the
    kernel needs to know the most conservative VA_BITS possible should it
    need to fall back to this quantity due to lack of hardware support.
    
    A new compile time constant VA_BITS_MIN is introduced in this patch and
    it is employed in the KASAN end address, KASLR, and EFI stub.
    
    For Arm, if 52-bit VA support is unavailable the fallback is to 48-bits.
    
    In other words: VA_BITS_MIN = min (48, VA_BITS)
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 844e2964b0f5..0e1f2770192a 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -42,7 +42,7 @@
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
  */
 
-#define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS)
+#define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS_MIN)
 #define TASK_SIZE_64		(UL(1) << vabits_user)
 
 #ifdef CONFIG_COMPAT

commit 63f0c60379650d82250f22e4cf4137ef3dc4f43d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 23 19:58:39 2019 +0200

    arm64: Introduce prctl() options to control the tagged user addresses ABI
    
    It is not desirable to relax the ABI to allow tagged user addresses into
    the kernel indiscriminately. This patch introduces a prctl() interface
    for enabling or disabling the tagged ABI with a global sysctl control
    for preventing applications from enabling the relaxed ABI (meant for
    testing user-space prctl() return error checking without reconfiguring
    the kernel). The ABI properties are inherited by threads of the same
    application and fork()'ed children but cleared on execve(). A Kconfig
    option allows the overall disabling of the relaxed ABI.
    
    The PR_SET_TAGGED_ADDR_CTRL will be expanded in the future to handle
    MTE-specific settings like imprecise vs precise exceptions.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 844e2964b0f5..28eed40ffc12 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -306,6 +306,14 @@ extern void __init minsigstksz_setup(void);
 /* PR_PAC_RESET_KEYS prctl */
 #define PAC_RESET_KEYS(tsk, arg)	ptrauth_prctl_reset_keys(tsk, arg)
 
+#ifdef CONFIG_ARM64_TAGGED_ADDR_ABI
+/* PR_{SET,GET}_TAGGED_ADDR_CTRL prctl */
+long set_tagged_addr_ctrl(unsigned long arg);
+long get_tagged_addr_ctrl(void);
+#define SET_TAGGED_ADDR_CTRL(arg)	set_tagged_addr_ctrl(arg)
+#define GET_TAGGED_ADDR_CTRL()		get_tagged_addr_ctrl()
+#endif
+
 /*
  * For CONFIG_GCC_PLUGIN_STACKLEAK
  *

commit b907b80d7ae7b2b65ef9f534f3e9a32ce6a4b539
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jul 8 17:36:40 2019 +0100

    arm64: remove pointless __KERNEL__ guards
    
    For a number of years, UAPI headers have been split from kernel-internal
    headers. The latter are never exposed to userspace, and always built
    with __KERNEL__ defined.
    
    Most headers under arch/arm64 don't have __KERNEL__ guards, but there
    are a few stragglers lying around. To make things more consistent, and
    to set a good example going forward, let's remove these redundant
    __KERNEL__ guards.
    
    In a couple of cases, a trailing #endif lacked a comment describing its
    corresponding #if or #ifdef, so these are fixes up at the same time.
    
    Guards in auto-generated crypto code are left as-is, as these guards are
    generated by scripting imported from the upstream openssl project
    scripts. Guards in UAPI headers are left as-is, as these can be included
    by userspace or the kernel.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 844e2964b0f5..fa7a6f63308f 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -20,7 +20,6 @@
 #define NET_IP_ALIGN	0
 
 #ifndef __ASSEMBLY__
-#ifdef __KERNEL__
 
 #include <linux/build_bug.h>
 #include <linux/cache.h>
@@ -283,8 +282,6 @@ static inline void spin_lock_prefetch(const void *ptr)
 
 #define HAVE_ARCH_PICK_MMAP_LAYOUT
 
-#endif
-
 extern unsigned long __ro_after_init signal_minsigstksz; /* sigframe size */
 extern void __init minsigstksz_setup(void);
 

commit cbdf8a189a66001c36007bf0f5c975d0376c5c3a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Jul 22 14:53:09 2019 +0100

    arm64: Force SSBS on context switch
    
    On a CPU that doesn't support SSBS, PSTATE[12] is RES0.  In a system
    where only some of the CPUs implement SSBS, we end-up losing track of
    the SSBS bit across task migration.
    
    To address this issue, let's force the SSBS bit on context switch.
    
    Fixes: 8f04e8e6e29c ("arm64: ssbd: Add support for PSTATE.SSBS rather than trapping to EL3")
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    [will: inverted logic and added comments]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index fd5b1a4efc70..844e2964b0f5 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -193,6 +193,16 @@ static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
 		regs->pmr_save = GIC_PRIO_IRQON;
 }
 
+static inline void set_ssbs_bit(struct pt_regs *regs)
+{
+	regs->pstate |= PSR_SSBS_BIT;
+}
+
+static inline void set_compat_ssbs_bit(struct pt_regs *regs)
+{
+	regs->pstate |= PSR_AA32_SSBS_BIT;
+}
+
 static inline void start_thread(struct pt_regs *regs, unsigned long pc,
 				unsigned long sp)
 {
@@ -200,7 +210,7 @@ static inline void start_thread(struct pt_regs *regs, unsigned long pc,
 	regs->pstate = PSR_MODE_EL0t;
 
 	if (arm64_get_ssbd_state() != ARM64_SSBD_FORCE_ENABLE)
-		regs->pstate |= PSR_SSBS_BIT;
+		set_ssbs_bit(regs);
 
 	regs->sp = sp;
 }
@@ -219,7 +229,7 @@ static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
 #endif
 
 	if (arm64_get_ssbd_state() != ARM64_SSBD_FORCE_ENABLE)
-		regs->pstate |= PSR_AA32_SSBS_BIT;
+		set_compat_ssbs_bit(regs);
 
 	regs->compat_sp = sp;
 }

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index fcd0e691b1ea..fd5b1a4efc70 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -1,20 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Based on arch/arm/include/asm/processor.h
  *
  * Copyright (C) 1995-1999 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_PROCESSOR_H
 #define __ASM_PROCESSOR_H

commit 359db57c34af15edf35492944af4d4417f59886c
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Mon Apr 29 18:27:13 2019 +0100

    arm64: compat: Reduce address limit for 64K pages
    
    With the introduction of the config option that allows to enable kuser
    helpers, it is now possible to reduce TASK_SIZE_32 when these are
    disabled and 64K pages are enabled. This extends the compliance with
    the section 6.5.8 of the C standard (C99).
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 228af56ece48..fcd0e691b1ea 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -57,7 +57,7 @@
 #define TASK_SIZE_64		(UL(1) << vabits_user)
 
 #ifdef CONFIG_COMPAT
-#ifdef CONFIG_ARM64_64K_PAGES
+#if defined(CONFIG_ARM64_64K_PAGES) && defined(CONFIG_KUSER_HELPERS)
 /*
  * With CONFIG_ARM64_64K_PAGES enabled, the last page is occupied
  * by the compat vectors page.

commit d263119387de9975d2acba1dfd3392f7c5979c18
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Mon Apr 1 12:30:14 2019 +0100

    arm64: compat: Reduce address limit
    
    Currently, compat tasks running on arm64 can allocate memory up to
    TASK_SIZE_32 (UL(0x100000000)).
    
    This means that mmap() allocations, if we treat them as returning an
    array, are not compliant with the sections 6.5.8 of the C standard
    (C99) which states that: "If the expression P points to an element of
    an array object and the expression Q points to the last element of the
    same array object, the pointer expression Q+1 compares greater than P".
    
    Redefine TASK_SIZE_32 to address the issue.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: <stable@vger.kernel.org>
    Reported-by: Jann Horn <jannh@google.com>
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    [will: fixed typo in comment]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5d9ce62bdebd..228af56ece48 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -57,7 +57,15 @@
 #define TASK_SIZE_64		(UL(1) << vabits_user)
 
 #ifdef CONFIG_COMPAT
+#ifdef CONFIG_ARM64_64K_PAGES
+/*
+ * With CONFIG_ARM64_64K_PAGES enabled, the last page is occupied
+ * by the compat vectors page.
+ */
 #define TASK_SIZE_32		UL(0x100000000)
+#else
+#define TASK_SIZE_32		(UL(0x100000000) - PAGE_SIZE)
+#endif /* CONFIG_ARM64_64K_PAGES */
 #define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
 				TASK_SIZE_32 : TASK_SIZE_64)
 #define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_32BIT) ? \

commit 133d05186325ce04494ea6488a6b86e50a446c12
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:46 2019 +0000

    arm64: Make PMR part of task context
    
    In order to replace PSR.I interrupt disabling/enabling with ICC_PMR_EL1
    interrupt masking, ICC_PMR_EL1 needs to be saved/restored when
    taking/returning from an exception. This mimics the way hardware saves
    and restores PSR.I bit in spsr_el1 for exceptions and ERET.
    
    Add PMR to the registers to save in the pt_regs struct upon kernel entry,
    and restore it before ERET. Also, initialize it to a sane value when
    creating new tasks.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index f1a7ab18faf3..5d9ce62bdebd 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -191,6 +191,9 @@ static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
 	memset(regs, 0, sizeof(*regs));
 	forget_syscall(regs);
 	regs->pc = pc;
+
+	if (system_uses_irq_prio_masking())
+		regs->pmr_save = GIC_PRIO_IRQON;
 }
 
 static inline void start_thread(struct pt_regs *regs, unsigned long pc,

commit 84931327a807a4dd65d0d6b53a8ae47845c91f79
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Dec 13 13:14:06 2018 +0000

    arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
    
    We don't need to get at the per-thread keys from assembly at all, so
    they can live alongside the rest of the per-thread register state in
    thread_struct instead of thread_info.
    
    This will also allow straighforward whitelisting of the keys for
    hardened usercopy should we expose them via a ptrace request later on.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 142c708cb429..f1a7ab18faf3 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -147,6 +147,9 @@ struct thread_struct {
 	unsigned long		fault_address;	/* fault info */
 	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */
+#ifdef CONFIG_ARM64_PTR_AUTH
+	struct ptrauth_keys	keys_user;
+#endif
 };
 
 static inline void arch_thread_struct_whitelist(unsigned long *offset,

commit ba830885656414101b2f8ca88786524d4bb5e8c1
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Dec 7 18:39:28 2018 +0000

    arm64: add prctl control for resetting ptrauth keys
    
    Add an arm64-specific prctl to allow a thread to reinitialize its
    pointer authentication keys to random values. This can be useful when
    exec() is not used for starting new processes, to ensure that different
    processes still have different keys.
    
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index f4b8e09aff56..142c708cb429 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -44,6 +44,7 @@
 #include <asm/hw_breakpoint.h>
 #include <asm/lse.h>
 #include <asm/pgtable-hwdef.h>
+#include <asm/pointer_auth.h>
 #include <asm/ptrace.h>
 #include <asm/types.h>
 
@@ -289,6 +290,9 @@ extern void __init minsigstksz_setup(void);
 #define SVE_SET_VL(arg)	sve_set_current_vl(arg)
 #define SVE_GET_VL()	sve_get_current_vl()
 
+/* PR_PAC_RESET_KEYS prctl */
+#define PAC_RESET_KEYS(tsk, arg)	ptrauth_prctl_reset_keys(tsk, arg)
+
 /*
  * For CONFIG_GCC_PLUGIN_STACKLEAK
  *

commit ec6e822d1a22d0eef1d1fa260dff751dba9a4258
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:26 2018 +0000

    arm64: expose user PAC bit positions via ptrace
    
    When pointer authentication is in use, data/instruction pointers have a
    number of PAC bits inserted into them. The number and position of these
    bits depends on the configured TCR_ELx.TxSZ and whether tagging is
    enabled. ARMv8.3 allows tagging to differ for instruction and data
    pointers.
    
    For userspace debuggers to unwind the stack and/or to follow pointer
    chains, they need to be able to remove the PAC bits before attempting to
    use a pointer.
    
    This patch adds a new structure with masks describing the location of
    the PAC bits in userspace instruction and data pointers (i.e. those
    addressable via TTBR0), which userspace can query via PTRACE_GETREGSET.
    By clearing these bits from pointers (and replacing them with the value
    of bit 55), userspace can acquire the PAC-less versions.
    
    This new regset is exposed when the kernel is built with (user) pointer
    authentication support, and the address authentication feature is
    enabled. Otherwise, the regset is hidden.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [will: Fix to use vabits_user instead of VA_BITS and rename macro]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index bbecc6fe3e5b..f4b8e09aff56 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -53,8 +53,6 @@
  */
 
 #define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS)
-
-extern u64 vabits_user;
 #define TASK_SIZE_64		(UL(1) << vabits_user)
 
 #ifdef CONFIG_COMPAT

commit 9b31cf493ffa40914e02998381993116e574c651
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 11:51:40 2018 +0000

    arm64: mm: Introduce MAX_USER_VA_BITS definition
    
    With the introduction of 52-bit virtual addressing for userspace, we are
    now in a position where the virtual addressing capability of userspace
    may exceed that of the kernel. Consequently, the VA_BITS definition
    cannot be used blindly, since it reflects only the size of kernel
    virtual addresses.
    
    This patch introduces MAX_USER_VA_BITS which is either VA_BITS or 52
    depending on whether 52-bit virtual addressing has been configured at
    build time, removing a few places where the 52 is open-coded based on
    explicit CONFIG_ guards.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 538ecbc15067..bbecc6fe3e5b 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -20,11 +20,7 @@
 #define __ASM_PROCESSOR_H
 
 #define KERNEL_DS		UL(-1)
-#ifdef CONFIG_ARM64_USER_VA_BITS_52
-#define USER_DS			((UL(1) << 52) - 1)
-#else
-#define USER_DS			((UL(1) << VA_BITS) - 1)
-#endif /* CONFIG_ARM64_USER_VA_BITS_52 */
+#define USER_DS			((UL(1) << MAX_USER_VA_BITS) - 1)
 
 /*
  * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is

commit 68d23da4373aba76f5300017c4746440f276698e
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 14:15:15 2018 +0000

    arm64: Kconfig: Re-jig CONFIG options for 52-bit VA
    
    Enabling 52-bit VAs for userspace is pretty confusing, since it requires
    you to select "48-bit" virtual addressing in the Kconfig.
    
    Rework the logic so that 52-bit user virtual addressing is advertised in
    the "Virtual address space size" choice, along with some help text to
    describe its interaction with Pointer Authentication. The EXPERT-only
    option to force all user mappings to the 52-bit range is then made
    available immediately below the VA size selection.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index efa0210cf927..538ecbc15067 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -20,11 +20,11 @@
 #define __ASM_PROCESSOR_H
 
 #define KERNEL_DS		UL(-1)
-#ifdef CONFIG_ARM64_52BIT_VA
+#ifdef CONFIG_ARM64_USER_VA_BITS_52
 #define USER_DS			((UL(1) << 52) - 1)
 #else
 #define USER_DS			((UL(1) << VA_BITS) - 1)
-#endif /* CONFIG_ARM64_52BIT_VA */
+#endif /* CONFIG_ARM64_USER_VA_BITS_52 */
 
 /*
  * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is

commit b9567720a1b8e739380e0241413606c056c57859
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:42 2018 +0000

    arm64: mm: Allow forcing all userspace addresses to 52-bit
    
    On arm64 52-bit VAs are provided to userspace when a hint is supplied to
    mmap. This helps maintain compatibility with software that expects at
    most 48-bit VAs to be returned.
    
    In order to help identify software that has 48-bit VA assumptions, this
    patch allows one to compile a kernel where 52-bit VAs are returned by
    default on HW that supports it.
    
    This feature is intended to be for development systems only.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 7ff75e52b762..efa0210cf927 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -74,8 +74,13 @@ extern u64 vabits_user;
 #define DEFAULT_MAP_WINDOW	DEFAULT_MAP_WINDOW_64
 #endif /* CONFIG_COMPAT */
 
-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(DEFAULT_MAP_WINDOW / 4))
+#ifdef CONFIG_ARM64_FORCE_52BIT
+#define STACK_TOP_MAX		TASK_SIZE_64
+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
+#else
 #define STACK_TOP_MAX		DEFAULT_MAP_WINDOW_64
+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(DEFAULT_MAP_WINDOW / 4))
+#endif /* CONFIG_ARM64_FORCE_52BIT */
 
 #ifdef CONFIG_COMPAT
 #define AARCH32_VECTORS_BASE	0xffff0000
@@ -85,12 +90,14 @@ extern u64 vabits_user;
 #define STACK_TOP		STACK_TOP_MAX
 #endif /* CONFIG_COMPAT */
 
+#ifndef CONFIG_ARM64_FORCE_52BIT
 #define arch_get_mmap_end(addr) ((addr > DEFAULT_MAP_WINDOW) ? TASK_SIZE :\
 				DEFAULT_MAP_WINDOW)
 
 #define arch_get_mmap_base(addr, base) ((addr > DEFAULT_MAP_WINDOW) ? \
 					base + TASK_SIZE - DEFAULT_MAP_WINDOW :\
 					base)
+#endif /* CONFIG_ARM64_FORCE_52BIT */
 
 extern phys_addr_t arm64_dma_phys_limit;
 #define ARCH_LOW_ADDRESS_LIMIT	(arm64_dma_phys_limit - 1)

commit 67e7fdfcc6824a4f768d76d89377b33baad58fad
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:41 2018 +0000

    arm64: mm: introduce 52-bit userspace support
    
    On arm64 there is optional support for a 52-bit virtual address space.
    To exploit this one has to be running with a 64KB page size and be
    running on hardware that supports this.
    
    For an arm64 kernel supporting a 48 bit VA with a 64KB page size,
    some changes are needed to support a 52-bit userspace:
     * TCR_EL1.T0SZ needs to be 12 instead of 16,
     * TASK_SIZE needs to reflect the new size.
    
    This patch implements the above when the support for 52-bit VAs is
    detected at early boot time.
    
    On arm64 userspace addresses translation is controlled by TTBR0_EL1. As
    well as userspace, TTBR0_EL1 controls:
     * The identity mapping,
     * EFI runtime code.
    
    It is possible to run a kernel with an identity mapping that has a
    larger VA size than userspace (and for this case __cpu_set_tcr_t0sz()
    would set TCR_EL1.T0SZ as appropriate). However, when the conditions for
    52-bit userspace are met; it is possible to keep TCR_EL1.T0SZ fixed at
    12. Thus in this patch, the TCR_EL1.T0SZ size changing logic is
    disabled.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 759927faf7f6..7ff75e52b762 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -19,10 +19,12 @@
 #ifndef __ASM_PROCESSOR_H
 #define __ASM_PROCESSOR_H
 
-#define TASK_SIZE_64		(UL(1) << VA_BITS)
-
-#define KERNEL_DS	UL(-1)
-#define USER_DS		(TASK_SIZE_64 - 1)
+#define KERNEL_DS		UL(-1)
+#ifdef CONFIG_ARM64_52BIT_VA
+#define USER_DS			((UL(1) << 52) - 1)
+#else
+#define USER_DS			((UL(1) << VA_BITS) - 1)
+#endif /* CONFIG_ARM64_52BIT_VA */
 
 /*
  * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is
@@ -56,6 +58,9 @@
 
 #define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS)
 
+extern u64 vabits_user;
+#define TASK_SIZE_64		(UL(1) << vabits_user)
+
 #ifdef CONFIG_COMPAT
 #define TASK_SIZE_32		UL(0x100000000)
 #define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \

commit e5d99157459f79ed26953e8a77addb9d3e0122b7
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:38 2018 +0000

    arm64: mm: Define arch_get_mmap_end, arch_get_mmap_base
    
    Now that we have DEFAULT_MAP_WINDOW defined, we can arch_get_mmap_end
    and arch_get_mmap_base helpers to allow for high addresses in mmap.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 102bab4344b7..759927faf7f6 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -80,6 +80,13 @@
 #define STACK_TOP		STACK_TOP_MAX
 #endif /* CONFIG_COMPAT */
 
+#define arch_get_mmap_end(addr) ((addr > DEFAULT_MAP_WINDOW) ? TASK_SIZE :\
+				DEFAULT_MAP_WINDOW)
+
+#define arch_get_mmap_base(addr, base) ((addr > DEFAULT_MAP_WINDOW) ? \
+					base + TASK_SIZE - DEFAULT_MAP_WINDOW :\
+					base)
+
 extern phys_addr_t arm64_dma_phys_limit;
 #define ARCH_LOW_ADDRESS_LIMIT	(arm64_dma_phys_limit - 1)
 

commit 363524d2b12270d86677e1154ecc1c5061f43219
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:37 2018 +0000

    arm64: mm: Introduce DEFAULT_MAP_WINDOW
    
    We wish to introduce a 52-bit virtual address space for userspace but
    maintain compatibility with software that assumes the maximum VA space
    size is 48 bit.
    
    In order to achieve this, on 52-bit VA systems, we make mmap behave as
    if it were running on a 48-bit VA system (unless userspace explicitly
    requests a VA where addr[51:48] != 0).
    
    On a system running a 52-bit userspace we need TASK_SIZE to represent
    the 52-bit limit as it is used in various places to distinguish between
    kernelspace and userspace addresses.
    
    Thus we need a new limit for mmap, stack, ELF loader and EFI (which uses
    TTBR0) to represent the non-extended VA space.
    
    This patch introduces DEFAULT_MAP_WINDOW and DEFAULT_MAP_WINDOW_64 and
    switches the appropriate logic to use that instead of TASK_SIZE.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 6b0d4dff5012..102bab4344b7 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -53,19 +53,25 @@
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
  */
+
+#define DEFAULT_MAP_WINDOW_64	(UL(1) << VA_BITS)
+
 #ifdef CONFIG_COMPAT
 #define TASK_SIZE_32		UL(0x100000000)
 #define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
 				TASK_SIZE_32 : TASK_SIZE_64)
 #define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_32BIT) ? \
 				TASK_SIZE_32 : TASK_SIZE_64)
+#define DEFAULT_MAP_WINDOW	(test_thread_flag(TIF_32BIT) ? \
+				TASK_SIZE_32 : DEFAULT_MAP_WINDOW_64)
 #else
 #define TASK_SIZE		TASK_SIZE_64
+#define DEFAULT_MAP_WINDOW	DEFAULT_MAP_WINDOW_64
 #endif /* CONFIG_COMPAT */
 
-#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(DEFAULT_MAP_WINDOW / 4))
+#define STACK_TOP_MAX		DEFAULT_MAP_WINDOW_64
 
-#define STACK_TOP_MAX		TASK_SIZE_64
 #ifdef CONFIG_COMPAT
 #define AARCH32_VECTORS_BASE	0xffff0000
 #define STACK_TOP		(test_thread_flag(TIF_32BIT) ? \

commit 26a4676faa1ad5d99317e0cd701e5d6f3e716b77
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Nov 7 18:10:38 2018 +0100

    arm64: mm: define NET_IP_ALIGN to 0
    
    On arm64, there is no need to add 2 bytes of padding to the start of
    each network buffer just to make the IP header appear 32-bit aligned.
    
    Since this might actually adversely affect DMA performance some
    platforms, let's override NET_IP_ALIGN to 0 to get rid of this
    padding.
    
    Acked-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Tested-by: Ilias Apalodimas <ilias.apalodimas@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 3e2091708b8e..6b0d4dff5012 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -24,6 +24,14 @@
 #define KERNEL_DS	UL(-1)
 #define USER_DS		(TASK_SIZE_64 - 1)
 
+/*
+ * On arm64 systems, unaligned accesses by the CPU are cheap, and so there is
+ * no point in shifting all network buffers by 2 bytes just to make some IP
+ * header fields appear aligned in memory, potentially sacrificing some DMA
+ * performance on some platforms.
+ */
+#define NET_IP_ALIGN	0
+
 #ifndef __ASSEMBLY__
 #ifdef __KERNEL__
 

commit de0d22e50cd3d57277f073ccf65d57aa519d6888
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Tue Oct 30 15:04:47 2018 -0700

    treewide: remove current_text_addr
    
    Prefer _THIS_IP_ defined in linux/kernel.h.
    
    Most definitions of current_text_addr were the same as _THIS_IP_, but
    a few archs had inline assembly instead.
    
    This patch removes the final call site of current_text_addr, making all
    of the definitions dead code.
    
    [akpm@linux-foundation.org: fix arch/csky/include/asm/processor.h]
    Link: http://lkml.kernel.org/r/20180911182413.180715-1-ndesaulniers@google.com
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 2bf6691371c2..3e2091708b8e 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -25,13 +25,6 @@
 #define USER_DS		(TASK_SIZE_64 - 1)
 
 #ifndef __ASSEMBLY__
-
-/*
- * Default implementation of macro that returns current
- * instruction pointer ("program counter").
- */
-#define current_text_addr() ({ __label__ _l; _l: &&_l;})
-
 #ifdef __KERNEL__
 
 #include <linux/build_bug.h>

commit b8925ee2e12d1cb9a11d6f28b5814f2bfa59dce1
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:53:41 2018 +0100

    arm64: cpu: Move errata and feature enable callbacks closer to callers
    
    The cpu errata and feature enable callbacks are only called via their
    respective arm64_cpu_capabilities structure and therefore shouldn't
    exist in the global namespace.
    
    Move the PAN, RAS and cache maintenance emulation enable callbacks into
    the same files as their corresponding arm64_cpu_capabilities structures,
    making them static in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index f6835374ed9f..2bf6691371c2 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -251,10 +251,6 @@ static inline void spin_lock_prefetch(const void *ptr)
 
 #endif
 
-void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused);
-void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused);
-void cpu_clear_disr(const struct arm64_cpu_capabilities *__unused);
-
 extern unsigned long __ro_after_init signal_minsigstksz; /* sigframe size */
 extern void __init minsigstksz_setup(void);
 

commit 8f04e8e6e29c93421a95b61cad62e3918425eac7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:47:06 2018 +0100

    arm64: ssbd: Add support for PSTATE.SSBS rather than trapping to EL3
    
    On CPUs with support for PSTATE.SSBS, the kernel can toggle the SSBD
    state without needing to call into firmware.
    
    This patch hooks into the existing SSBD infrastructure so that SSBS is
    used on CPUs that support it, but it's all made horribly complicated by
    the very real possibility of big/little systems that don't uniformly
    provide the new capability.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 79657ad91397..f6835374ed9f 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -174,6 +174,10 @@ static inline void start_thread(struct pt_regs *regs, unsigned long pc,
 {
 	start_thread_common(regs, pc);
 	regs->pstate = PSR_MODE_EL0t;
+
+	if (arm64_get_ssbd_state() != ARM64_SSBD_FORCE_ENABLE)
+		regs->pstate |= PSR_SSBS_BIT;
+
 	regs->sp = sp;
 }
 
@@ -190,6 +194,9 @@ static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
 	regs->pstate |= PSR_AA32_E_BIT;
 #endif
 
+	if (arm64_get_ssbd_state() != ARM64_SSBD_FORCE_ENABLE)
+		regs->pstate |= PSR_AA32_SSBS_BIT;
+
 	regs->compat_sp = sp;
 }
 #endif

commit 0b3e336601b82c6afa0e9cf21db9cb8793e25399
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jul 20 14:41:54 2018 -0700

    arm64: Add support for STACKLEAK gcc plugin
    
    This adds support for the STACKLEAK gcc plugin to arm64 by implementing
    stackleak_check_alloca(), based heavily on the x86 version, and adding the
    two helpers used by the stackleak common code: current_top_of_stack() and
    on_thread_stack(). The stack erasure calls are made at syscall returns.
    Additionally, this disables the plugin in hypervisor and EFI stub code,
    which are out of scope for the protection.
    
    Acked-by: Alexander Popov <alex.popov@linux.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index e02612105d78..79657ad91397 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -266,5 +266,20 @@ extern void __init minsigstksz_setup(void);
 #define SVE_SET_VL(arg)	sve_set_current_vl(arg)
 #define SVE_GET_VL()	sve_get_current_vl()
 
+/*
+ * For CONFIG_GCC_PLUGIN_STACKLEAK
+ *
+ * These need to be macros because otherwise we get stuck in a nightmare
+ * of header definitions for the use of task_stack_page.
+ */
+
+#define current_top_of_stack()							\
+({										\
+	struct stack_info _info;						\
+	BUG_ON(!on_accessible_stack(current, current_stack_pointer, &_info));	\
+	_info.high;								\
+})
+#define on_thread_stack()	(on_task_stack(current, current_stack_pointer, NULL))
+
 #endif /* __ASSEMBLY__ */
 #endif /* __ASM_PROCESSOR_H */

commit d64567f67835736d65086e9bfc41a19b2863c32e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 5 15:16:52 2018 +0100

    arm64: use PSR_AA32 definitions
    
    Some code cares about the SPSR_ELx format for exceptions taken from
    AArch32 to inspect or manipulate the SPSR_ELx value, which is already in
    the SPSR_ELx format, and not in the AArch32 PSR format.
    
    To separate these from cases where we care about the AArch32 PSR format,
    migrate these cases to use the PSR_AA32_* definitions rather than
    COMPAT_PSR_*.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index a73ae1e49200..e02612105d78 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -182,12 +182,12 @@ static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
 				       unsigned long sp)
 {
 	start_thread_common(regs, pc);
-	regs->pstate = COMPAT_PSR_MODE_USR;
+	regs->pstate = PSR_AA32_MODE_USR;
 	if (pc & 1)
-		regs->pstate |= COMPAT_PSR_T_BIT;
+		regs->pstate |= PSR_AA32_T_BIT;
 
 #ifdef __AARCH64EB__
-	regs->pstate |= COMPAT_PSR_E_BIT;
+	regs->pstate |= PSR_AA32_E_BIT;
 #endif
 
 	regs->compat_sp = sp;

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit 94b07c1f8c39c6d839df35fa28ffd1785d385897
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Jun 1 11:10:14 2018 +0100

    arm64: signal: Report signal frame size to userspace via auxv
    
    Stateful CPU architecture extensions may require the signal frame
    to grow to a size that exceeds the arch's MINSIGSTKSZ #define.
    However, changing this #define is an ABI break.
    
    To allow userspace the option of determining the signal frame size
    in a more forwards-compatible way, this patch adds a new auxv entry
    tagged with AT_MINSIGSTKSZ, which provides the maximum signal frame
    size that the process can observe during its lifetime.
    
    If AT_MINSIGSTKSZ is absent from the aux vector, the caller can
    assume that the MINSIGSTKSZ #define is sufficient.  This allows for
    a consistent interface with older kernels that do not provide
    AT_MINSIGSTKSZ.
    
    The idea is that libc could expose this via sysconf() or some
    similar mechanism.
    
    There is deliberately no AT_SIGSTKSZ.  The kernel knows nothing
    about userspace's own stack overheads and should not pretend to
    know.
    
    For arm64:
    
    The primary motivation for this interface is the Scalable Vector
    Extension, which can require at least 4KB or so of extra space
    in the signal frame for the largest hardware implementations.
    
    To determine the correct value, a "Christmas tree" mode (via the
    add_all argument) is added to setup_sigframe_layout(), to simulate
    addition of all possible records to the signal frame at maximum
    possible size.
    
    If this procedure goes wrong somehow, resulting in a stupidly large
    frame layout and hence failure of sigframe_alloc() to allocate a
    record to the frame, then this is indicative of a kernel bug.  In
    this case, we WARN() and no attempt is made to populate
    AT_MINSIGSTKSZ for userspace.
    
    For arm64 SVE:
    
    The SVE context block in the signal frame needs to be considered
    too when computing the maximum possible signal frame size.
    
    Because the size of this block depends on the vector length, this
    patch computes the size based not on the thread's current vector
    length but instead on the maximum possible vector length: this
    determines the maximum size of SVE context block that can be
    observed in any signal frame for the lifetime of the process.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 767598932549..65ab83e8926e 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -35,6 +35,8 @@
 #ifdef __KERNEL__
 
 #include <linux/build_bug.h>
+#include <linux/cache.h>
+#include <linux/init.h>
 #include <linux/stddef.h>
 #include <linux/string.h>
 
@@ -244,6 +246,9 @@ void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused);
 void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused);
 void cpu_clear_disr(const struct arm64_cpu_capabilities *__unused);
 
+extern unsigned long __ro_after_init signal_minsigstksz; /* sigframe size */
+extern void __init minsigstksz_setup(void);
+
 /* Userspace interface for PR_SVE_{SET,GET}_VL prctl()s: */
 #define SVE_SET_VL(arg)	sve_set_current_vl(arg)
 #define SVE_GET_VL()	sve_get_current_vl()

commit 9a6e594869b29ccec4f99db83c071e4f2dbfc11f
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Apr 12 17:32:35 2018 +0100

    arm64/sve: Move sve_pffr() to fpsimd.h and make inline
    
    In order to make sve_save_state()/sve_load_state() more easily
    reusable and to get rid of a potential branch on context switch
    critical paths, this patch makes sve_pffr() inline and moves it to
    fpsimd.h.
    
    <asm/processor.h> must be included in fpsimd.h in order to make
    this work, and this creates an #include cycle that is tricky to
    avoid without modifying core code, due to the way the PR_SVE_*()
    prctl helpers are included in the core prctl implementation.
    
    Instead of breaking the cycle, this patch defers inclusion of
    <asm/fpsimd.h> in <asm/processor.h> until the point where it is
    actually needed: i.e., immediately before the prctl definitions.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 9231b8762ca6..c99e657fdd57 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -40,7 +40,6 @@
 
 #include <asm/alternative.h>
 #include <asm/cpufeature.h>
-#include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
 #include <asm/lse.h>
 #include <asm/pgtable-hwdef.h>
@@ -247,6 +246,17 @@ void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused);
 void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused);
 void cpu_clear_disr(const struct arm64_cpu_capabilities *__unused);
 
+/*
+ * Not at the top of the file due to a direct #include cycle between
+ * <asm/fpsimd.h> and <asm/processor.h>.  Deferring this #include
+ * ensures that contents of processor.h are visible to fpsimd.h even if
+ * processor.h is included first.
+ *
+ * These prctl helpers are the only things in this file that require
+ * fpsimd.h.  The core code expects them to be in this header.
+ */
+#include <asm/fpsimd.h>
+
 /* Userspace interface for PR_SVE_{SET,GET}_VL prctl()s: */
 #define SVE_SET_VL(arg)	sve_set_current_vl(arg)
 #define SVE_GET_VL()	sve_get_current_vl()

commit 31dc52b3c8faf47bf3ff5ced661488a20e5d1811
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Apr 12 16:47:20 2018 +0100

    arm64/sve: Move read_zcr_features() out of cpufeature.h
    
    Having read_zcr_features() inline in cpufeature.h results in that
    header requiring #includes which make it hard to include
    <asm/fpsimd.h> elsewhere without triggering header inclusion
    cycles.
    
    This is not a hot-path function and arguably should not be in
    cpufeature.h in the first place, so this patch moves it to
    fpsimd.c, compiled conditionally if CONFIG_ARM64_SVE=y.
    
    This allows some SVE-related #includes to be dropped from
    cpufeature.h, which will ease future maintenance.
    
    A couple of missing #includes of <asm/fpsimd.h> are exposed by this
    change under arch/arm64/.  This patch adds the missing #includes as
    necessary.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 36d64f83cdfb..9231b8762ca6 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -40,6 +40,7 @@
 
 #include <asm/alternative.h>
 #include <asm/cpufeature.h>
+#include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
 #include <asm/lse.h>
 #include <asm/pgtable-hwdef.h>

commit df3fb96820455ef70a51630d1be336d4f2602111
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon May 21 19:08:15 2018 +0100

    arm64: fpsimd: Eliminate task->mm checks
    
    Currently the FPSIMD handling code uses the condition task->mm ==
    NULL as a hint that task has no FPSIMD register context.
    
    The ->mm check is only there to filter out tasks that cannot
    possibly have FPSIMD context loaded, for optimisation purposes.
    Also, TIF_FOREIGN_FPSTATE must always be checked anyway before
    saving FPSIMD context back to memory.  For these reasons, the ->mm
    checks are not useful, providing that TIF_FOREIGN_FPSTATE is
    maintained in a consistent way for all threads.
    
    The context switch logic is already deliberately optimised to defer
    reloads of the regs until ret_to_user (or sigreturn as a special
    case), and save them only if they have been previously loaded.
    These paths are the only places where the wrong_task and wrong_cpu
    conditions can be made false, by calling fpsimd_bind_task_to_cpu().
    Kernel threads by definition never reach these paths.  As a result,
    the wrong_task and wrong_cpu tests in fpsimd_thread_switch() will
    always yield true for kernel threads.
    
    This patch removes the redundant checks and special-case code,
    ensuring that TIF_FOREIGN_FPSTATE is set whenever a kernel thread
    is scheduled in, and ensures that this flag is set for the init
    task.  The fpsimd_flush_task_state() call already present in
    copy_thread() ensures the same for any new task.
    
    With TIF_FOREIGN_FPSTATE always set for kernel threads, this patch
    ensures that no extra context save work is added for kernel
    threads, and eliminates the redundant context saving that may
    currently occur for kernel threads that have acquired an mm via
    use_mm().
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 767598932549..36d64f83cdfb 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -156,7 +156,9 @@ static inline void arch_thread_struct_whitelist(unsigned long *offset,
 /* Sync TPIDR_EL0 back to thread_struct for current */
 void tls_preserve_current_state(void);
 
-#define INIT_THREAD  {	}
+#define INIT_THREAD {				\
+	.fpsimd_cpu = NR_CPUS,			\
+}
 
 static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
 {

commit 65896545b69ffaac947c12e11d3dcc57fd1fb772
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Mar 28 10:50:49 2018 +0100

    arm64: uaccess: Fix omissions from usercopy whitelist
    
    When the hardend usercopy support was added for arm64, it was
    concluded that all cases of usercopy into and out of thread_struct
    were statically sized and so didn't require explicit whitelisting
    of the appropriate fields in thread_struct.
    
    Testing with usercopy hardening enabled has revealed that this is
    not the case for certain ptrace regset manipulation calls on arm64.
    This occurs because the sizes of usercopies associated with the
    regset API are dynamic by construction, and because arm64 does not
    always stage such copies via the stack: indeed the regset API is
    designed to avoid the need for that by adding some bounds checking.
    
    This is currently believed to affect only the fpsimd and TLS
    registers.
    
    Because the whitelisted fields in thread_struct must be contiguous,
    this patch groups them together in a nested struct.  It is also
    necessary to be able to determine the location and size of that
    struct, so rather than making the struct anonymous (which would
    save on edits elsewhere) or adding an anonymous union containing
    named and unnamed instances of the same struct (gross), this patch
    gives the struct a name and makes the necessary edits to code that
    references it (noisy but simple).
    
    Care is needed to ensure that the new struct does not contain
    padding (which the usercopy hardening would fail to protect).
    
    For this reason, the presence of tp2_value is made unconditional,
    since a padding field would be needed there in any case.  This pads
    up to the 16-byte alignment required by struct user_fpsimd_state.
    
    Acked-by: Kees Cook <keescook@chromium.org>
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 9e8084d3f761 ("arm64: Implement thread_struct whitelist for hardened usercopy")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 63d3850db224..767598932549 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -34,6 +34,8 @@
 
 #ifdef __KERNEL__
 
+#include <linux/build_bug.h>
+#include <linux/stddef.h>
 #include <linux/string.h>
 
 #include <asm/alternative.h>
@@ -103,11 +105,18 @@ struct cpu_context {
 
 struct thread_struct {
 	struct cpu_context	cpu_context;	/* cpu context */
-	unsigned long		tp_value;	/* TLS register */
-#ifdef CONFIG_COMPAT
-	unsigned long		tp2_value;
-#endif
-	struct user_fpsimd_state fpsimd_state;
+
+	/*
+	 * Whitelisted fields for hardened usercopy:
+	 * Maintainers must ensure manually that this contains no
+	 * implicit padding.
+	 */
+	struct {
+		unsigned long	tp_value;	/* TLS register */
+		unsigned long	tp2_value;
+		struct user_fpsimd_state fpsimd_state;
+	} uw;
+
 	unsigned int		fpsimd_cpu;
 	void			*sve_state;	/* SVE registers, if any */
 	unsigned int		sve_vl;		/* SVE vector length */
@@ -117,14 +126,17 @@ struct thread_struct {
 	struct debug_info	debug;		/* debugging */
 };
 
-/*
- * Everything usercopied to/from thread_struct is statically-sized, so
- * no hardened usercopy whitelist is needed.
- */
 static inline void arch_thread_struct_whitelist(unsigned long *offset,
 						unsigned long *size)
 {
-	*offset = *size = 0;
+	/* Verify that there is no padding among the whitelisted fields: */
+	BUILD_BUG_ON(sizeof_field(struct thread_struct, uw) !=
+		     sizeof_field(struct thread_struct, uw.tp_value) +
+		     sizeof_field(struct thread_struct, uw.tp2_value) +
+		     sizeof_field(struct thread_struct, uw.fpsimd_state));
+
+	*offset = offsetof(struct thread_struct, uw);
+	*size = sizeof_field(struct thread_struct, uw);
 }
 
 #ifdef CONFIG_COMPAT
@@ -132,13 +144,13 @@ static inline void arch_thread_struct_whitelist(unsigned long *offset,
 ({									\
 	unsigned long *__tls;						\
 	if (is_compat_thread(task_thread_info(t)))			\
-		__tls = &(t)->thread.tp2_value;				\
+		__tls = &(t)->thread.uw.tp2_value;			\
 	else								\
-		__tls = &(t)->thread.tp_value;				\
+		__tls = &(t)->thread.uw.tp_value;			\
 	__tls;								\
  })
 #else
-#define task_user_tls(t)	(&(t)->thread.tp_value)
+#define task_user_tls(t)	(&(t)->thread.uw.tp_value)
 #endif
 
 /* Sync TPIDR_EL0 back to thread_struct for current */

commit 20b8547277a6e8ee1d928792c1b2782c9a2a6cf5
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Mar 28 10:50:48 2018 +0100

    arm64: fpsimd: Split cpu field out from struct fpsimd_state
    
    In preparation for using a common representation of the FPSIMD
    state for tasks and KVM vcpus, this patch separates out the "cpu"
    field that is used to track the cpu on which the state was most
    recently loaded.
    
    This will allow common code to operate on task and vcpu contexts
    without requiring the cpu field to be stored at the same offset
    from the FPSIMD register data in both cases.  This should avoid the
    need for messing with the definition of those parts of struct
    vcpu_arch that are exposed in the KVM user ABI.
    
    The resulting change is also convenient for grouping and defining
    the set of thread_struct fields that are supposed to be accessible
    to copy_{to,from}_user(), which includes user_fpsimd_state but
    should exclude the cpu field.  This patch does not amend the
    usercopy whitelist to match: that will be addressed in a subsequent
    patch.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    [will: inline fpsimd_flush_state for now]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 4fc8867fde4d..63d3850db224 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -38,7 +38,6 @@
 
 #include <asm/alternative.h>
 #include <asm/cpufeature.h>
-#include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
 #include <asm/lse.h>
 #include <asm/pgtable-hwdef.h>
@@ -108,7 +107,8 @@ struct thread_struct {
 #ifdef CONFIG_COMPAT
 	unsigned long		tp2_value;
 #endif
-	struct fpsimd_state	fpsimd_state;
+	struct user_fpsimd_state fpsimd_state;
+	unsigned int		fpsimd_cpu;
 	void			*sve_state;	/* SVE registers, if any */
 	unsigned int		sve_vl;		/* SVE vector length */
 	unsigned int		sve_vl_onexec;	/* SVE vl after next exec */

commit c0cda3b8ee6b4b6851b2fd8b6db91fd7b0e2524a
Author: Dave Martin <dave.martin@arm.com>
Date:   Mon Mar 26 15:12:28 2018 +0100

    arm64: capabilities: Update prototype for enable call back
    
    We issue the enable() call back for all CPU hwcaps capabilities
    available on the system, on all the CPUs. So far we have ignored
    the argument passed to the call back, which had a prototype to
    accept a "void *" for use with on_each_cpu() and later with
    stop_machine(). However, with commit 0a0d111d40fd1
    ("arm64: cpufeature: Pass capability structure to ->enable callback"),
    there are some users of the argument who wants the matching capability
    struct pointer where there are multiple matching criteria for a single
    capability. Clean up the declaration of the call back to make it clear.
    
     1) Renamed to cpu_enable(), to imply taking necessary actions on the
        called CPU for the entry.
     2) Pass const pointer to the capability, to allow the call back to
        check the entry. (e.,g to check if any action is needed on the CPU)
     3) We don't care about the result of the call back, turning this to
        a void.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [suzuki: convert more users, rename call back and drop results]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index fce604e3e599..4fc8867fde4d 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -37,6 +37,7 @@
 #include <linux/string.h>
 
 #include <asm/alternative.h>
+#include <asm/cpufeature.h>
 #include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
 #include <asm/lse.h>
@@ -227,9 +228,9 @@ static inline void spin_lock_prefetch(const void *ptr)
 
 #endif
 
-int cpu_enable_pan(void *__unused);
-int cpu_enable_cache_maint_trap(void *__unused);
-int cpu_clear_disr(void *__unused);
+void cpu_enable_pan(const struct arm64_cpu_capabilities *__unused);
+void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused);
+void cpu_clear_disr(const struct arm64_cpu_capabilities *__unused);
 
 /* Userspace interface for PR_SVE_{SET,GET}_VL prctl()s: */
 #define SVE_SET_VL(arg)	sve_set_current_vl(arg)

commit c0136321924dd338bb8fc5661c4b0e27441a8d04
Merge: 846ade7dd2e6 3a0a397ff5ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 8 10:44:25 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull more arm64 updates from Catalin Marinas:
     "As I mentioned in the last pull request, there's a second batch of
      security updates for arm64 with mitigations for Spectre/v1 and an
      improved one for Spectre/v2 (via a newly defined firmware interface
      API).
    
      Spectre v1 mitigation:
    
       - back-end version of array_index_mask_nospec()
    
       - masking of the syscall number to restrict speculation through the
         syscall table
    
       - masking of __user pointers prior to deference in uaccess routines
    
      Spectre v2 mitigation update:
    
       - using the new firmware SMC calling convention specification update
    
       - removing the current PSCI GET_VERSION firmware call mitigation as
         vendors are deploying new SMCCC-capable firmware
    
       - additional branch predictor hardening for synchronous exceptions
         and interrupts while in user mode
    
      Meltdown v3 mitigation update:
    
        - Cavium Thunder X is unaffected but a hardware erratum gets in the
          way. The kernel now starts with the page tables mapped as global
          and switches to non-global if kpti needs to be enabled.
    
      Other:
    
       - Theoretical trylock bug fixed"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (38 commits)
      arm64: Kill PSCI_GET_VERSION as a variant-2 workaround
      arm64: Add ARM_SMCCC_ARCH_WORKAROUND_1 BP hardening support
      arm/arm64: smccc: Implement SMCCC v1.1 inline primitive
      arm/arm64: smccc: Make function identifiers an unsigned quantity
      firmware/psci: Expose SMCCC version through psci_ops
      firmware/psci: Expose PSCI conduit
      arm64: KVM: Add SMCCC_ARCH_WORKAROUND_1 fast handling
      arm64: KVM: Report SMCCC_ARCH_WORKAROUND_1 BP hardening support
      arm/arm64: KVM: Turn kvm_psci_version into a static inline
      arm/arm64: KVM: Advertise SMCCC v1.1
      arm/arm64: KVM: Implement PSCI 1.0 support
      arm/arm64: KVM: Add smccc accessors to PSCI code
      arm/arm64: KVM: Add PSCI_VERSION helper
      arm/arm64: KVM: Consolidate the PSCI include files
      arm64: KVM: Increment PC after handling an SMC trap
      arm: KVM: Fix SMCCC handling of unimplemented SMC/HVC calls
      arm64: KVM: Fix SMCCC handling of unimplemented SMC/HVC calls
      arm64: entry: Apply BP hardening for suspicious interrupts from EL0
      arm64: entry: Apply BP hardening for high-priority synchronous exceptions
      arm64: futex: Mask __user pointers prior to dereference
      ...

commit 51369e398d0d33e8f524314e672b07e8cf870e79
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Feb 5 15:34:18 2018 +0000

    arm64: Make USER_DS an inclusive limit
    
    Currently, USER_DS represents an exclusive limit while KERNEL_DS is
    inclusive. In order to do some clever trickery for speculation-safe
    masking, we need them both to behave equivalently - there aren't enough
    bits to make KERNEL_DS exclusive, so we have precisely one option. This
    also happens to correct a longstanding false negative for a range
    ending on the very top byte of kernel memory.
    
    Mark Rutland points out that we've actually got the semantics of
    addresses vs. segments muddled up in most of the places we need to
    amend, so shuffle the {USER,KERNEL}_DS definitions around such that we
    can correct those properly instead of just pasting "-1"s everywhere.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index cee4ae25a5d1..dfefbf3fd9e3 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -21,6 +21,9 @@
 
 #define TASK_SIZE_64		(UL(1) << VA_BITS)
 
+#define KERNEL_DS	UL(-1)
+#define USER_DS		(TASK_SIZE_64 - 1)
+
 #ifndef __ASSEMBLY__
 
 /*

commit 617aebe6a97efa539cc4b8a52adccd89596e6be0
Merge: 0771ad44a20b e47e311843de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 3 16:25:42 2018 -0800

    Merge tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull hardened usercopy whitelisting from Kees Cook:
     "Currently, hardened usercopy performs dynamic bounds checking on slab
      cache objects. This is good, but still leaves a lot of kernel memory
      available to be copied to/from userspace in the face of bugs.
    
      To further restrict what memory is available for copying, this creates
      a way to whitelist specific areas of a given slab cache object for
      copying to/from userspace, allowing much finer granularity of access
      control.
    
      Slab caches that are never exposed to userspace can declare no
      whitelist for their objects, thereby keeping them unavailable to
      userspace via dynamic copy operations. (Note, an implicit form of
      whitelisting is the use of constant sizes in usercopy operations and
      get_user()/put_user(); these bypass all hardened usercopy checks since
      these sizes cannot change at runtime.)
    
      This new check is WARN-by-default, so any mistakes can be found over
      the next several releases without breaking anyone's system.
    
      The series has roughly the following sections:
       - remove %p and improve reporting with offset
       - prepare infrastructure and whitelist kmalloc
       - update VFS subsystem with whitelists
       - update SCSI subsystem with whitelists
       - update network subsystem with whitelists
       - update process memory with whitelists
       - update per-architecture thread_struct with whitelists
       - update KVM with whitelists and fix ioctl bug
       - mark all other allocations as not whitelisted
       - update lkdtm for more sensible test overage"
    
    * tag 'usercopy-v4.16-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux: (38 commits)
      lkdtm: Update usercopy tests for whitelisting
      usercopy: Restrict non-usercopy caches to size 0
      kvm: x86: fix KVM_XEN_HVM_CONFIG ioctl
      kvm: whitelist struct kvm_vcpu_arch
      arm: Implement thread_struct whitelist for hardened usercopy
      arm64: Implement thread_struct whitelist for hardened usercopy
      x86: Implement thread_struct whitelist for hardened usercopy
      fork: Provide usercopy whitelisting for task_struct
      fork: Define usercopy region in thread_stack slab caches
      fork: Define usercopy region in mm_struct slab caches
      net: Restrict unwhitelisted proto caches to size 0
      sctp: Copy struct sctp_sock.autoclose to userspace using put_user()
      sctp: Define usercopy region in SCTP proto slab cache
      caif: Define usercopy region in caif proto slab cache
      ip: Define usercopy region in IP proto slab cache
      net: Define usercopy region in struct proto slab cache
      scsi: Define usercopy region in scsi_sense_cache slab cache
      cifs: Define usercopy region in cifs_request slab cache
      vxfs: Define usercopy region in vxfs_inode slab cache
      ufs: Define usercopy region in ufs_inode_cache slab cache
      ...

commit 68ddbf09ec5a888ec850edd7e7438d2daf069c56
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:38:59 2018 +0000

    arm64: kernel: Prepare for a DISR user
    
    KVM would like to consume any pending SError (or RAS error) after guest
    exit. Today it has to unmask SError and use dsb+isb to synchronise the
    CPU. With the RAS extensions we can use ESB to synchronise any pending
    SError.
    
    Add the necessary macros to allow DISR to be read and converted to an
    ESR.
    
    We clear the DISR register when we enable the RAS cpufeature, and the
    kernel has not executed any ESB instructions. Any value we find in DISR
    must have belonged to firmware. Executing an ESB instruction is the
    only way to update DISR, so we can expect firmware to have handled
    any deferred SError. By the same logic we clear DISR in the idle path.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 023cacb946c3..cee4ae25a5d1 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -216,6 +216,7 @@ static inline void spin_lock_prefetch(const void *ptr)
 
 int cpu_enable_pan(void *__unused);
 int cpu_enable_cache_maint_trap(void *__unused);
+int cpu_clear_disr(void *__unused);
 
 /* Userspace interface for PR_SVE_{SET,GET}_VL prctl()s: */
 #define SVE_SET_VL(arg)	sve_set_current_vl(arg)

commit 9e8084d3f761413b2d58b2625bc6e332eab2bfce
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Aug 16 14:05:09 2017 -0700

    arm64: Implement thread_struct whitelist for hardened usercopy
    
    While ARM64 carries FPU state in the thread structure that is saved and
    restored during signal handling, it doesn't need to declare a usercopy
    whitelist, since existing accessors are all either using a bounce buffer
    (for which whitelisting isn't checking the slab), are statically sized
    (which will bypass the hardened usercopy check), or both.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Cc: Dave Martin <Dave.Martin@arm.com>
    Cc: zijun_hu <zijun_hu@htc.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 023cacb946c3..ba6c9ce6cc75 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -113,6 +113,16 @@ struct thread_struct {
 	struct debug_info	debug;		/* debugging */
 };
 
+/*
+ * Everything usercopied to/from thread_struct is statically-sized, so
+ * no hardened usercopy whitelist is needed.
+ */
+static inline void arch_thread_struct_whitelist(unsigned long *offset,
+						unsigned long *size)
+{
+	*offset = *size = 0;
+}
+
 #ifdef CONFIG_COMPAT
 #define task_user_tls(t)						\
 ({									\

commit 2d2123bc7c7f843aa9db87720de159a049839862
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:14 2017 +0000

    arm64/sve: Add prctl controls for userspace vector length management
    
    This patch adds two arm64-specific prctls, to permit userspace to
    control its vector length:
    
     * PR_SVE_SET_VL: set the thread's SVE vector length and vector
       length inheritance mode.
    
     * PR_SVE_GET_VL: get the same information.
    
    Although these prctls resemble instruction set features in the SVE
    architecture, they provide additional control: the vector length
    inheritance mode is Linux-specific and nothing to do with the
    architecture, and the architecture does not permit EL0 to set its
    own vector length directly.  Both can be used in portable tools
    without requiring the use of SVE instructions.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    [will: Fixed up prctl constants to avoid clash with PDEATHSIG]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index c6fddb005dc2..023cacb946c3 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -217,5 +217,9 @@ static inline void spin_lock_prefetch(const void *ptr)
 int cpu_enable_pan(void *__unused);
 int cpu_enable_cache_maint_trap(void *__unused);
 
+/* Userspace interface for PR_SVE_{SET,GET}_VL prctl()s: */
+#define SVE_SET_VL(arg)	sve_set_current_vl(arg)
+#define SVE_GET_VL()	sve_get_current_vl()
+
 #endif /* __ASSEMBLY__ */
 #endif /* __ASM_PROCESSOR_H */

commit 79ab047c75d6a9f95d8840d94f405e20cbacac4b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:06 2017 +0000

    arm64/sve: Support vector length resetting for new processes
    
    It's desirable to be able to reset the vector length to some sane
    default for new processes, since the new binary and its libraries
    may or may not be SVE-aware.
    
    This patch tracks the desired post-exec vector length (if any) in a
    new thread member sve_vl_onexec, and adds a new thread flag
    TIF_SVE_VL_INHERIT to control whether to inherit or reset the
    vector length.  Currently these are inactive.  Subsequent patches
    will provide the capability to configure them.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index e2f575dbdddd..c6fddb005dc2 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -107,6 +107,7 @@ struct thread_struct {
 	struct fpsimd_state	fpsimd_state;
 	void			*sve_state;	/* SVE registers, if any */
 	unsigned int		sve_vl;		/* SVE vector length */
+	unsigned int		sve_vl_onexec;	/* SVE vl after next exec */
 	unsigned long		fault_address;	/* fault info */
 	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */

commit bc0ee476036478a85beeed51f0d94c8729fd0544
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:05 2017 +0000

    arm64/sve: Core task context handling
    
    This patch adds the core support for switching and managing the SVE
    architectural state of user tasks.
    
    Calls to the existing FPSIMD low-level save/restore functions are
    factored out as new functions task_fpsimd_{save,load}(), since SVE
    now dynamically may or may not need to be handled at these points
    depending on the kernel configuration, hardware features discovered
    at boot, and the runtime state of the task.  To make these
    decisions as fast as possible, const cpucaps are used where
    feasible, via the system_supports_sve() helper.
    
    The SVE registers are only tracked for threads that have explicitly
    used SVE, indicated by the new thread flag TIF_SVE.  Otherwise, the
    FPSIMD view of the architectural state is stored in
    thread.fpsimd_state as usual.
    
    When in use, the SVE registers are not stored directly in
    thread_struct due to their potentially large and variable size.
    Because the task_struct slab allocator must be configured very
    early during kernel boot, it is also tricky to configure it
    correctly to match the maximum vector length provided by the
    hardware, since this depends on examining secondary CPUs as well as
    the primary.  Instead, a pointer sve_state in thread_struct points
    to a dynamically allocated buffer containing the SVE register data,
    and code is added to allocate and free this buffer at appropriate
    times.
    
    TIF_SVE is set when taking an SVE access trap from userspace, if
    suitable hardware support has been detected.  This enables SVE for
    the thread: a subsequent return to userspace will disable the trap
    accordingly.  If such a trap is taken without sufficient system-
    wide hardware support, SIGILL is sent to the thread instead as if
    an undefined instruction had been executed: this may happen if
    userspace tries to use SVE in a system where not all CPUs support
    it for example.
    
    The kernel will clear TIF_SVE and disable SVE for the thread
    whenever an explicit syscall is made by userspace.  For backwards
    compatibility reasons and conformance with the spirit of the base
    AArch64 procedure call standard, the subset of the SVE register
    state that aliases the FPSIMD registers is still preserved across a
    syscall even if this happens.  The remainder of the SVE register
    state logically becomes zero at syscall entry, though the actual
    zeroing work is currently deferred until the thread next tries to
    use SVE, causing another trap to the kernel.  This implementation
    is suboptimal: in the future, the fastpath case may be optimised
    to zero the registers in-place and leave SVE enabled for the task,
    where beneficial.
    
    TIF_SVE is also cleared in the following slowpath cases, which are
    taken as reasonable hints that the task may no longer use SVE:
     * exec
     * fork and clone
    
    Code is added to sync data between thread.fpsimd_state and
    thread.sve_state whenever enabling/disabling SVE, in a manner
    consistent with the SVE architectural programmer's model.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    [will: added #include to fix allnoconfig build]
    [will: use enable_daif in do_sve_acc]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 7dddca21fc64..e2f575dbdddd 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -105,6 +105,8 @@ struct thread_struct {
 	unsigned long		tp2_value;
 #endif
 	struct fpsimd_state	fpsimd_state;
+	void			*sve_state;	/* SVE registers, if any */
+	unsigned int		sve_vl;		/* SVE vector length */
 	unsigned long		fault_address;	/* fault info */
 	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */

commit eef94a3d09aab437c8c254de942d8b1aa76455e2
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Thu Aug 31 11:30:50 2017 +0300

    arm64: move TASK_* definitions to <asm/processor.h>
    
    ILP32 series [1] introduces the dependency on <asm/is_compat.h> for
    TASK_SIZE macro. Which in turn requires <asm/thread_info.h>, and
    <asm/thread_info.h> include <asm/memory.h>, giving a circular dependency,
    because TASK_SIZE is currently located in <asm/memory.h>.
    
    In other architectures, TASK_SIZE is defined in <asm/processor.h>, and
    moving TASK_SIZE there fixes the problem.
    
    Discussion: https://patchwork.kernel.org/patch/9929107/
    
    [1] https://github.com/norov/linux/tree/ilp32-next
    
    CC: Will Deacon <will.deacon@arm.com>
    CC: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 29adab8138c3..7dddca21fc64 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -19,6 +19,10 @@
 #ifndef __ASM_PROCESSOR_H
 #define __ASM_PROCESSOR_H
 
+#define TASK_SIZE_64		(UL(1) << VA_BITS)
+
+#ifndef __ASSEMBLY__
+
 /*
  * Default implementation of macro that returns current
  * instruction pointer ("program counter").
@@ -37,6 +41,22 @@
 #include <asm/ptrace.h>
 #include <asm/types.h>
 
+/*
+ * TASK_SIZE - the maximum size of a user space task.
+ * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area.
+ */
+#ifdef CONFIG_COMPAT
+#define TASK_SIZE_32		UL(0x100000000)
+#define TASK_SIZE		(test_thread_flag(TIF_32BIT) ? \
+				TASK_SIZE_32 : TASK_SIZE_64)
+#define TASK_SIZE_OF(tsk)	(test_tsk_thread_flag(tsk, TIF_32BIT) ? \
+				TASK_SIZE_32 : TASK_SIZE_64)
+#else
+#define TASK_SIZE		TASK_SIZE_64
+#endif /* CONFIG_COMPAT */
+
+#define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
+
 #define STACK_TOP_MAX		TASK_SIZE_64
 #ifdef CONFIG_COMPAT
 #define AARCH32_VECTORS_BASE	0xffff0000
@@ -194,4 +214,5 @@ static inline void spin_lock_prefetch(const void *ptr)
 int cpu_enable_pan(void *__unused);
 int cpu_enable_cache_maint_trap(void *__unused);
 
+#endif /* __ASSEMBLY__ */
 #endif /* __ASM_PROCESSOR_H */

commit df5b95bee1ed7009a2090e9924e7a96e14850d56
Merge: 969ff73e72fe 872d8327ce89
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 15 18:40:58 2017 +0100

    Merge branch 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: add VMAP_STACK overflow detection
      arm64: add on_accessible_stack()
      arm64: add basic VMAP_STACK support
      arm64: use an irq stack pointer
      arm64: assembler: allow adr_this_cpu to use the stack pointer
      arm64: factor out entry stack manipulation
      efi/arm64: add EFI_KIMG_ALIGN
      arm64: move SEGMENT_ALIGN to <asm/memory.h>
      arm64: clean up irq stack definitions
      arm64: clean up THREAD_* definitions
      arm64: factor out PAGE_* and CONT_* definitions
      arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
      fork: allow arch-override of VMAP stack alignment
      arm64: remove __die()'s stack dump

commit 34be98f4944f99076f049a6806fc5f5207a755d3
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Jul 20 17:15:45 2017 +0100

    arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
    
    For historical reasons, we leave the top 16 bytes of our task and IRQ
    stacks unused, a practice used to ensure that the SP can always be
    masked to find the base of the current stack (historically, where
    thread_info could be found).
    
    However, this is not necessary, as:
    
    * When an exception is taken from a task stack, we decrement the SP by
      S_FRAME_SIZE and stash the exception registers before we compare the
      SP against the task stack. In such cases, the SP must be at least
      S_FRAME_SIZE below the limit, and can be safely masked to determine
      whether the task stack is in use.
    
    * When transitioning to an IRQ stack, we'll place a dummy frame onto the
      IRQ stack before enabling asynchronous exceptions, or executing code
      we expect to trigger faults. Thus, if an exception is taken from the
      IRQ stack, the SP must be at least 16 bytes below the limit.
    
    * We no longer mask the SP to find the thread_info, which is now found
      via sp_el0. Note that historically, the offset was critical to ensure
      that cpu_switch_to() found the correct stack for new threads that
      hadn't yet executed ret_from_fork().
    
    Given that, this initial offset serves no purpose, and can be removed.
    This brings us in-line with other architectures (e.g. x86) which do not
    rely on this masking.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [Mark: rebase, kill THREAD_START_SP, commit msg additions]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 64c9e78f9882..6687dd29f7e0 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -159,7 +159,7 @@ extern struct task_struct *cpu_switch_to(struct task_struct *prev,
 					 struct task_struct *next);
 
 #define task_pt_regs(p) \
-	((struct pt_regs *)(THREAD_START_SP + task_stack_page(p)) - 1)
+	((struct pt_regs *)(THREAD_SIZE + task_stack_page(p)) - 1)
 
 #define KSTK_EIP(tsk)	((unsigned long)task_pt_regs(tsk)->pc)
 #define KSTK_ESP(tsk)	user_stack_pointer(task_pt_regs(tsk))

commit 17c28958600928109049a3bcc814b0d5bfb1ff3a
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Aug 1 15:35:54 2017 +0100

    arm64: Abstract syscallno manipulation
    
    The -1 "no syscall" value is written in various ways, shared with
    the user ABI in some places, and generally obscure.
    
    This patch attempts to make things a little more consistent and
    readable by replacing all these uses with a single #define.  A
    couple of symbolic helpers are provided to clarify the intent
    further.
    
    Because the in-syscall check in do_signal() is changed from >= 0 to
    != NO_SYSCALL by this patch, different behaviour may be observable
    if syscallno is set to values less than -1 by a tracer.  However,
    this is not different from the behaviour that is already observable
    if a tracer sets syscallno to a value >= __NR_(compat_)syscalls.
    
    It appears that this can cause spurious syscall restarting, but
    that is not a new behaviour either, and does not appear harmful.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 379def1d6b67..b7334f11c4cf 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -112,7 +112,7 @@ void tls_preserve_current_state(void);
 static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
 {
 	memset(regs, 0, sizeof(*regs));
-	regs->syscallno = ~0;
+	forget_syscall(regs);
 	regs->pc = pc;
 }
 

commit 35d0e6fb4d219d64ab3b7cffef7a11a0662140f5
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Aug 1 15:35:53 2017 +0100

    arm64: syscallno is secretly an int, make it official
    
    The upper 32 bits of the syscallno field in thread_struct are
    handled inconsistently, being sometimes zero extended and sometimes
    sign-extended.  In fact, only the lower 32 bits seem to have any
    real significance for the behaviour of the code: it's been OK to
    handle the upper bits inconsistently because they don't matter.
    
    Currently, the only place I can find where those bits are
    significant is in calling trace_sys_enter(), which may be
    unintentional: for example, if a compat tracer attempts to cancel a
    syscall by passing -1 to (COMPAT_)PTRACE_SET_SYSCALL at the
    syscall-enter-stop, it will be traced as syscall 4294967295
    rather than -1 as might be expected (and as occurs for a native
    tracer doing the same thing).  Elsewhere, reads of syscallno cast
    it to an int or truncate it.
    
    There's also a conspicuous amount of code and casting to bodge
    around the fact that although semantically an int, syscallno is
    stored as a u64.
    
    Let's not pretend any more.
    
    In order to preserve the stp x instruction that stores the syscall
    number in entry.S, this patch special-cases the layout of struct
    pt_regs for big endian so that the newly 32-bit syscallno field
    maps onto the low bits of the stored value.  This is not beautiful,
    but benchmarking of the getpid syscall on Juno suggests indicates a
    minor slowdown if the stp is split into an stp x and stp w.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 64c9e78f9882..379def1d6b67 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -112,7 +112,7 @@ void tls_preserve_current_state(void);
 static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
 {
 	memset(regs, 0, sizeof(*regs));
-	regs->syscallno = ~0UL;
+	regs->syscallno = ~0;
 	regs->pc = pc;
 }
 

commit 936eb65ca22ad856cb3a995e8cd742e982dc2dd0
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jun 21 16:00:44 2017 +0100

    arm64: ptrace: Flush user-RW TLS reg to thread_struct before reading
    
    When reading current's user-writable TLS register (which occurs
    when dumping core for native tasks), it is possible that userspace
    has modified it since the time the task was last scheduled out.
    The new TLS register value is not guaranteed to have been written
    immediately back to thread_struct in this case.
    
    As a result, a coredump can capture stale data for this register.
    Reading the register for a stopped task via ptrace is unaffected.
    
    For native tasks, this patch explicitly flushes the TPIDR_EL0
    register back to thread_struct before dumping when operating on
    current, thus ensuring that coredump contents are up to date.  For
    compat tasks, the TLS register is not user-writable and so cannot
    be out of sync, so no flush is required in compat_tls_get().
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 9428b93fefb2..64c9e78f9882 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -104,6 +104,9 @@ struct thread_struct {
 #define task_user_tls(t)	(&(t)->thread.tp_value)
 #endif
 
+/* Sync TPIDR_EL0 back to thread_struct for current */
+void tls_preserve_current_state(void);
+
 #define INIT_THREAD  {	}
 
 static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)

commit fda89d9efcabaafcbbb189e969d46dc634574b67
Author: Chris Redmon <credmonster@gmail.com>
Date:   Thu Mar 16 18:10:43 2017 -0400

    arm64: struct debug_info: Check CONFIG_HAVE_HW_BREAKPOINT
    
    Check if CONFIG_HAVE_HW_BREAKPOINT is enabled before compiling in extra
    data required for hardware breakpoints. Compiling out this code when hw
    breakpoints are disabled saves about 272 bytes per struct task_struct.
    
    Signed-off-by: Chris Redmon <credmonster@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index c97b8bd2acba..9428b93fefb2 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -50,6 +50,7 @@ extern phys_addr_t arm64_dma_phys_limit;
 #define ARCH_LOW_ADDRESS_LIMIT	(arm64_dma_phys_limit - 1)
 
 struct debug_info {
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
 	/* Have we suspended stepping by a debugger? */
 	int			suspended_step;
 	/* Allow breakpoints and watchpoints to be disabled for this thread. */
@@ -58,6 +59,7 @@ struct debug_info {
 	/* Hardware breakpoints pinned to this task. */
 	struct perf_event	*hbp_break[ARM_MAX_BRP];
 	struct perf_event	*hbp_watch[ARM_MAX_WRP];
+#endif
 };
 
 struct cpu_context {

commit c8b06e3fddddaae1a87ed479edcb8b3d85caecc7
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 9 18:14:02 2017 +0000

    arm64: Remove useless UAO IPI and describe how this gets enabled
    
    Since its introduction, the UAO enable call was broken, and useless.
    commit 2a6dcb2b5f3e ("arm64: cpufeature: Schedule enable() calls instead
    of calling them via IPI"), fixed the framework so that these calls
    are scheduled, so that they can modify PSTATE.
    
    Now it is just useless. Remove it. UAO is enabled by the code patching
    which causes get_user() and friends to use the 'ldtr' family of
    instructions. This relies on the PSTATE.UAO bit being set to match
    addr_limit, which we do in uao_thread_switch() called via __switch_to().
    
    All that is needed to enable UAO is patch the code, and call schedule().
    __apply_alternatives_multi_stop() calls stop_machine() when it modifies
    the kernel text to enable the alternatives, (including the UAO code in
    uao_thread_switch()). Once stop_machine() has finished __switch_to() is
    called to reschedule the original task, this causes PSTATE.UAO to be set
    appropriately. An explicit enable() call is not needed.
    
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 747c65a616ed..c97b8bd2acba 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -187,7 +187,6 @@ static inline void spin_lock_prefetch(const void *ptr)
 #endif
 
 int cpu_enable_pan(void *__unused);
-int cpu_enable_uao(void *__unused);
 int cpu_enable_cache_maint_trap(void *__unused);
 
 #endif /* __ASM_PROCESSOR_H */

commit 6d0d287891a022ebba572327cbd70b5de69a63a2
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Wed Nov 16 13:23:05 2016 +0100

    locking/core: Provide common cpu_relax_yield() definition
    
    No need to duplicate the same define everywhere. Since
    the only user is stop-machine and the only provider is
    s390, we can use a default implementation of cpu_relax_yield()
    in sched.h.
    
    Suggested-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvm@vger.kernel.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-s390 <linux-s390@vger.kernel.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: sparclinux@vger.kernel.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1479298985-191589-1-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 6132f64af68d..747c65a616ed 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -149,8 +149,6 @@ static inline void cpu_relax(void)
 	asm volatile("yield" ::: "memory");
 }
 
-#define cpu_relax_yield()                     cpu_relax()
-
 /* Thread switching */
 extern struct task_struct *cpu_switch_to(struct task_struct *prev,
 					 struct task_struct *next);

commit 5bd0b85ba8bb9de6f61f33f3752fc85f4c87fc22
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:15 2016 +0200

    locking/core, arch: Remove cpu_relax_lowlatency()
    
    As there are no users left, we can remove cpu_relax_lowlatency()
    implementations from every architecture.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Cc: <linux-arch@vger.kernel.org>
    Link: http://lkml.kernel.org/r/1477386195-32736-6-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 3f9b0e54dae3..6132f64af68d 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -150,7 +150,6 @@ static inline void cpu_relax(void)
 }
 
 #define cpu_relax_yield()                     cpu_relax()
-#define cpu_relax_lowlatency()                cpu_relax()
 
 /* Thread switching */
 extern struct task_struct *cpu_switch_to(struct task_struct *prev,

commit 79ab11cdb90d8536817ab7357ecb6b1ff76be26c
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 25 11:03:11 2016 +0200

    locking/core: Introduce cpu_relax_yield()
    
    For spinning loops people do often use barrier() or cpu_relax().
    For most architectures cpu_relax and barrier are the same, but on
    some architectures cpu_relax can add some latency.
    For example on power,sparc64 and arc, cpu_relax can shift the CPU
    towards other hardware threads in an SMT environment.
    On s390 cpu_relax does even more, it uses an hypercall to the
    hypervisor to give up the timeslice.
    In contrast to the SMT yielding this can result in larger latencies.
    In some places this latency is unwanted, so another variant
    "cpu_relax_lowlatency" was introduced. Before this is used in more
    and more places, lets revert the logic and provide a cpu_relax_yield
    that can be called in places where yielding is more important than
    latency. By default this is the same as cpu_relax on all architectures.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Noam Camus <noamc@ezchip.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: virtualization@lists.linux-foundation.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/1477386195-32736-2-git-send-email-borntraeger@de.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 60e34824e18c..3f9b0e54dae3 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -149,6 +149,7 @@ static inline void cpu_relax(void)
 	asm volatile("yield" ::: "memory");
 }
 
+#define cpu_relax_yield()                     cpu_relax()
 #define cpu_relax_lowlatency()                cpu_relax()
 
 /* Thread switching */

commit 2a6dcb2b5f3e21592ca8dfa198dcce7bec09b020
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:46 2016 +0100

    arm64: cpufeature: Schedule enable() calls instead of calling them via IPI
    
    The enable() call for a cpufeature/errata is called using on_each_cpu().
    This issues a cross-call IPI to get the work done. Implicitly, this
    stashes the running PSTATE in SPSR when the CPU receives the IPI, and
    restores it when we return. This means an enable() call can never modify
    PSTATE.
    
    To allow PAN to do this, change the on_each_cpu() call to use
    stop_machine(). This schedules the work on each CPU which allows
    us to modify PSTATE.
    
    This involves changing the protype of all the enable() functions.
    
    enable_cpu_capabilities() is called during boot and enables the feature
    on all online CPUs. This path now uses stop_machine(). CPU features for
    hotplug'd CPUs are enabled by verify_local_cpu_features() which only
    acts on the local CPU, and can already modify the running PSTATE as it
    is called from secondary_start_kernel().
    
    Reported-by: Tony Thompson <anthony.thompson@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index df2e53d3a969..60e34824e18c 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -188,8 +188,8 @@ static inline void spin_lock_prefetch(const void *ptr)
 
 #endif
 
-void cpu_enable_pan(void *__unused);
-void cpu_enable_uao(void *__unused);
-void cpu_enable_cache_maint_trap(void *__unused);
+int cpu_enable_pan(void *__unused);
+int cpu_enable_uao(void *__unused);
+int cpu_enable_cache_maint_trap(void *__unused);
 
 #endif /* __ASM_PROCESSOR_H */

commit a842789837c0e3734357c6b4c54d39d60a1d24b1
Author: zijun_hu <zijun_hu@htc.com>
Date:   Thu Sep 1 18:51:19 2016 +0800

    arm64: remove duplicate macro __KERNEL__ check
    
    remove duplicate macro __KERNEL__ check
    
    Signed-off-by: zijun_hu <zijun_hu@htc.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index ace0a96e7d6e..df2e53d3a969 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -37,7 +37,6 @@
 #include <asm/ptrace.h>
 #include <asm/types.h>
 
-#ifdef __KERNEL__
 #define STACK_TOP_MAX		TASK_SIZE_64
 #ifdef CONFIG_COMPAT
 #define AARCH32_VECTORS_BASE	0xffff0000
@@ -49,7 +48,6 @@
 
 extern phys_addr_t arm64_dma_phys_limit;
 #define ARCH_LOW_ADDRESS_LIMIT	(arm64_dma_phys_limit - 1)
-#endif /* __KERNEL__ */
 
 struct debug_info {
 	/* Have we suspended stepping by a debugger? */

commit 7dd01aef055792260287c6708daf75aac3918f66
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:32 2016 +0100

    arm64: trap userspace "dc cvau" cache operation on errata-affected core
    
    The ARM errata 819472, 826319, 827319 and 824069 for affected
    Cortex-A53 cores demand to promote "dc cvau" instructions to
    "dc civac". Since we allow userspace to also emit those instructions,
    we should make sure that "dc cvau" gets promoted there too.
    So lets grasp the nettle here and actually trap every userland cache
    maintenance instruction once we detect at least one affected core in
    the system.
    We then emulate the instruction by executing it on behalf of userland,
    promoting "dc cvau" to "dc civac" on the way and injecting access
    fault back into userspace.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [catalin.marinas@arm.com: s/set_segfault/arm64_notify_segfault/]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index cef1cf398356..ace0a96e7d6e 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -192,5 +192,6 @@ static inline void spin_lock_prefetch(const void *ptr)
 
 void cpu_enable_pan(void *__unused);
 void cpu_enable_uao(void *__unused);
+void cpu_enable_cache_maint_trap(void *__unused);
 
 #endif /* __ASM_PROCESSOR_H */

commit 57f4959bad0a154aeca125b7d38d1d9471a12422
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:48 2016 +0000

    arm64: kernel: Add support for User Access Override
    
    'User Access Override' is a new ARMv8.2 feature which allows the
    unprivileged load and store instructions to be overridden to behave in
    the normal way.
    
    This patch converts {get,put}_user() and friends to use ldtr*/sttr*
    instructions - so that they can only access EL0 memory, then enables
    UAO when fs==KERNEL_DS so that these functions can access kernel memory.
    
    This allows user space's read/write permissions to be checked against the
    page tables, instead of testing addr<USER_DS, then using the kernel's
    read/write permissions.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: move uao_thread_switch() above dsb()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5bb1d763d17a..cef1cf398356 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -191,5 +191,6 @@ static inline void spin_lock_prefetch(const void *ptr)
 #endif
 
 void cpu_enable_pan(void *__unused);
+void cpu_enable_uao(void *__unused);
 
 #endif /* __ASM_PROCESSOR_H */

commit afb83cc3f0e4f86ea0e1cc3db7a90f58f1abd4d5
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Feb 10 10:07:30 2016 +0000

    arm64: prefetch: add missing #include for spin_lock_prefetch
    
    As of 52e662326e1e ("arm64: prefetch: don't provide spin_lock_prefetch
    with LSE"), spin_lock_prefetch is patched at runtime when the LSE atomics
    are in use. This relies on the ARM64_LSE_ATOMIC_INSN macro to drive
    the alternatives framework, but that macro is only available via
    asm/lse.h, which isn't explicitly included in processor.h. Consequently,
    drivers can run into build failures such as:
    
       In file included from include/linux/prefetch.h:14:0,
                        from drivers/net/ethernet/intel/i40e/i40e_txrx.c:27:
       arch/arm64/include/asm/processor.h: In function 'spin_lock_prefetch':
       arch/arm64/include/asm/processor.h:183:15: error: expected string literal before 'ARM64_LSE_ATOMIC_INSN'
         asm volatile(ARM64_LSE_ATOMIC_INSN(
    
    This patch add the missing include and gets things building again.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 31b76fce4477..5bb1d763d17a 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -32,6 +32,7 @@
 #include <asm/alternative.h>
 #include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
+#include <asm/lse.h>
 #include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
 #include <asm/types.h>

commit cd5e10bdf3795d22f10787bb1991c43798c885d5
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 2 12:46:23 2016 +0000

    arm64: prefetch: don't provide spin_lock_prefetch with LSE
    
    The LSE atomics rely on us not dirtying data at L1 if we can avoid it,
    otherwise many of the potential scalability benefits are lost.
    
    This patch replaces spin_lock_prefetch with a nop when the LSE atomics
    are in use, so that users don't shoot themselves in the foot by causing
    needless coherence traffic at L1.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Andrew Pinski <apinski@cavium.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 4acb7ca94fcd..31b76fce4477 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -29,6 +29,7 @@
 
 #include <linux/string.h>
 
+#include <asm/alternative.h>
 #include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
 #include <asm/pgtable-hwdef.h>
@@ -177,9 +178,11 @@ static inline void prefetchw(const void *ptr)
 }
 
 #define ARCH_HAS_SPINLOCK_PREFETCH
-static inline void spin_lock_prefetch(const void *x)
+static inline void spin_lock_prefetch(const void *ptr)
 {
-	prefetchw(x);
+	asm volatile(ARM64_LSE_ATOMIC_INSN(
+		     "prfm pstl1strm, %a0",
+		     "nop") : : "p" (ptr));
 }
 
 #define HAVE_ARCH_PICK_MMAP_LAYOUT

commit dbb4e152b8da1f977d9d8cd7e494ab4ee3622f72
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:50 2015 +0100

    arm64: Delay cpu feature capability checks
    
    At the moment we run through the arm64_features capability list for
    each CPU and set the capability if one of the CPU supports it. This
    could be problematic in a heterogeneous system with differing capabilities.
    Delay the CPU feature checks until all the enabled CPUs are up(i.e,
    smp_cpus_done(), so that we can make better decisions based on the
    overall system capability. Once we decide and advertise the capabilities
    the alternatives can be applied. From this state, we cannot roll back
    a feature to disabled based on the values from a new hotplugged CPU,
    due to the runtime patching and other reasons. So, for all new CPUs,
    we need to make sure that they have the established system capabilities.
    Failing which, we bring the CPU down, preventing it from turning online.
    Once the capabilities are decided, any new CPU booting up goes through
    verification to ensure that it has all the enabled capabilities and also
    invokes the respective enable() method on the CPU.
    
    The CPU errata checks are not delayed and is still executed per-CPU
    to detect the respective capabilities. If we ever come across a non-errata
    capability that needs to be checked on each-CPU, we could introduce them via
    a new capability table(or introduce a flag), which can be processed per CPU.
    
    The next patch will make the feature checks use the system wide
    safe value of a feature register.
    
    NOTE: The enable() methods associated with the capability is scheduled
    on all the CPUs (which is the only use case at the moment). If we need
    a different type of 'enable()' which only needs to be run once on any CPU,
    we should be able to handle that when needed.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    [catalin.marinas@arm.com: static variable and coding style fixes]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 98f32355dc97..4acb7ca94fcd 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -186,6 +186,6 @@ static inline void spin_lock_prefetch(const void *x)
 
 #endif
 
-void cpu_enable_pan(void);
+void cpu_enable_pan(void *__unused);
 
 #endif /* __ASM_PROCESSOR_H */

commit 338d4f49d6f7114a017d294ccf7374df4f998edc
Author: James Morse <james.morse@arm.com>
Date:   Wed Jul 22 19:05:54 2015 +0100

    arm64: kernel: Add support for Privileged Access Never
    
    'Privileged Access Never' is a new arm8.1 feature which prevents
    privileged code from accessing any virtual address where read or write
    access is also permitted at EL0.
    
    This patch enables the PAN feature on all CPUs, and modifies {get,put}_user
    helpers temporarily to permit access.
    
    This will catch kernel bugs where user memory is accessed directly.
    'Unprivileged loads and stores' using ldtrb et al are unaffected by PAN.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [will: use ALTERNATIVE in asm and tidy up pan_enable check]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index e4c893e54f01..98f32355dc97 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -186,4 +186,6 @@ static inline void spin_lock_prefetch(const void *x)
 
 #endif
 
+void cpu_enable_pan(void);
+
 #endif /* __ASM_PROCESSOR_H */

commit d00a3810c16207d2541b7796a73cca5a24ea3742
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed May 27 15:39:40 2015 +0100

    arm64: context-switch user tls register tpidr_el0 for compat tasks
    
    Since commit a4780adeefd0 ("ARM: 7735/2: Preserve the user r/w register
    TPIDRURW on context switch and fork"), arch/arm/ has context switched
    the user-writable TLS register, so do the same for compat tasks running
    under the arm64 kernel.
    
    Reported-by: André Hentschel <nerv@dawncrow.de>
    Tested-by: André Hentschel <nerv@dawncrow.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index d2c37a1df0eb..e4c893e54f01 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -78,13 +78,30 @@ struct cpu_context {
 
 struct thread_struct {
 	struct cpu_context	cpu_context;	/* cpu context */
-	unsigned long		tp_value;
+	unsigned long		tp_value;	/* TLS register */
+#ifdef CONFIG_COMPAT
+	unsigned long		tp2_value;
+#endif
 	struct fpsimd_state	fpsimd_state;
 	unsigned long		fault_address;	/* fault info */
 	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */
 };
 
+#ifdef CONFIG_COMPAT
+#define task_user_tls(t)						\
+({									\
+	unsigned long *__tls;						\
+	if (is_compat_thread(task_thread_info(t)))			\
+		__tls = &(t)->thread.tp2_value;				\
+	else								\
+		__tls = &(t)->thread.tp_value;				\
+	__tls;								\
+ })
+#else
+#define task_user_tls(t)	(&(t)->thread.tp_value)
+#endif
+
 #define INIT_THREAD  {	}
 
 static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)

commit 1baa82f48030f38d1895301f1ec93acbcb3d15db
Author: Peter Crosthwaite <peter.crosthwaite@xilinx.com>
Date:   Mon Mar 2 19:19:14 2015 +0000

    arm64: Implement cpu_relax as yield
    
    ARM64 has the yield nop hint which has the intended semantics of
    cpu_relax. Implement.
    
    The immediate application is ARM CPU emulators. An emulator can take
    advantage of the yield hint to de-prioritise an emulated CPU in favor
    of other emulation tasks. QEMU A64 SMP emulation has yield awareness,
    and sees a significant boot time performance increase with this change.
    
    Signed-off-by: Peter Crosthwaite <peter.crosthwaite@xilinx.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 20e9591a60cf..d2c37a1df0eb 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -127,7 +127,11 @@ extern void release_thread(struct task_struct *);
 
 unsigned long get_wchan(struct task_struct *p);
 
-#define cpu_relax()			barrier()
+static inline void cpu_relax(void)
+{
+	asm volatile("yield" ::: "memory");
+}
+
 #define cpu_relax_lowlatency()                cpu_relax()
 
 /* Thread switching */

commit a1e50a82256ed2f1312e70c52a84323e2e378f49
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 5 18:01:53 2015 +0000

    arm64: Increase the swiotlb buffer size 64MB
    
    With commit 3690951fc6d4 (arm64: Use swiotlb late initialisation), the
    swiotlb buffer size is limited to MAX_ORDER_NR_PAGES. However, there are
    platforms with 32-bit only devices that require bounce buffering via
    swiotlb. This patch changes the swiotlb initialisation to an early 64MB
    memblock allocation. In order to get the swiotlb buffer correctly
    allocated (via memblock_virt_alloc_low_nopanic), this patch also defines
    ARCH_LOW_ADDRESS_LIMIT to the maximum physical address capable of 32-bit
    DMA.
    
    Reported-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Tested-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index f9be30ea1cbd..20e9591a60cf 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -45,7 +45,8 @@
 #define STACK_TOP		STACK_TOP_MAX
 #endif /* CONFIG_COMPAT */
 
-#define ARCH_LOW_ADDRESS_LIMIT	PHYS_MASK
+extern phys_addr_t arm64_dma_phys_limit;
+#define ARCH_LOW_ADDRESS_LIMIT	(arm64_dma_phys_limit - 1)
 #endif /* __KERNEL__ */
 
 struct debug_info {

commit 2ec4560b7c73e6c9febc4fb2a3e6af257c904979
Author: Paul Walmsley <pwalmsley@nvidia.com>
Date:   Mon Jan 5 17:38:41 2015 -0700

    arm64: fix missing asm/pgtable-hwdef.h include in asm/processor.h
    
    On next-20150105, defconfig compilation breaks with:
    
    ./arch/arm64/include/asm/processor.h:47:32: error: ‘PHYS_MASK’ undeclared (first use in this function)
    
    Fix by including asm/pgtable-hwdef.h, where PHYS_MASK is defined.
    
    This second version incorporates a comment from Mark Rutland
    <mark.rutland@arm.com> to keep the includes in alphabetical order
    by filename.
    
    Signed-off-by: Paul Walmsley <paul@pwsan.com>
    Cc: Paul Walmsley <pwalmsley@nvidia.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index f131a98e1e64..f9be30ea1cbd 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -31,6 +31,7 @@
 
 #include <asm/fpsimd.h>
 #include <asm/hw_breakpoint.h>
+#include <asm/pgtable-hwdef.h>
 #include <asm/ptrace.h>
 #include <asm/types.h>
 

commit 3efcb7a44bb75bd94d889245ba82e2195a7ab0a2
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Mon Jan 5 14:23:48 2015 +0000

    arm64: Remove unused prepare_to_copy()
    
    prepare_to_copy() was removed from all architectures supported at that
    time in commit 55ccf3fe3f9a ("fork: move the real prepare_to_copy()
    users to arch_dup_task_struct()"). Remove it from arm64 as well.
    
    Signed-off-by: Tobias Klauser <tklauser@distanz.ch>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 286b1bec547c..f131a98e1e64 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -123,9 +123,6 @@ struct task_struct;
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 
-/* Prepare to copy thread state - unlazy all lazy status */
-#define prepare_to_copy(tsk)	do { } while (0)
-
 unsigned long get_wchan(struct task_struct *p);
 
 #define cpu_relax()			barrier()

commit 3168a743461ecf86adf3e7dcfcd79271828fb263
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Aug 29 16:11:10 2014 +0100

    arm64: report correct stack pointer in KSTK_ESP for compat tasks
    
    The KSTK_ESP macro is used to determine the user stack pointer for a
    given task. In particular, this is used to to report the '[stack]' VMA
    in /proc/self/maps, which is used by Android to determine the stack
    location for children of the main thread.
    
    This patch fixes the macro to use user_stack_pointer instead of directly
    returning sp. This means that we report w13 instead of sp, since the
    former is used as the stack pointer when executing in AArch32 state.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Serban Constantinescu <Serban.Constantinescu@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 3df21feeabdd..286b1bec547c 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -139,7 +139,7 @@ extern struct task_struct *cpu_switch_to(struct task_struct *prev,
 	((struct pt_regs *)(THREAD_START_SP + task_stack_page(p)) - 1)
 
 #define KSTK_EIP(tsk)	((unsigned long)task_pt_regs(tsk)->pc)
-#define KSTK_ESP(tsk)	((unsigned long)task_pt_regs(tsk)->sp)
+#define KSTK_ESP(tsk)	user_stack_pointer(task_pt_regs(tsk))
 
 /*
  * Prefetching support

commit 8efb90cf1e80129fad197b916714e1d01ee183d2
Merge: 5bda4f638f36 3a6bfbc91df0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 16:09:06 2014 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - big rtmutex and futex cleanup and robustification from Thomas
         Gleixner
       - mutex optimizations and refinements from Jason Low
       - arch_mutex_cpu_relax() removal and related cleanups
       - smaller lockdep tweaks"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (23 commits)
      arch, locking: Ciao arch_mutex_cpu_relax()
      locking/lockdep: Only ask for /proc/lock_stat output when available
      locking/mutexes: Optimize mutex trylock slowpath
      locking/mutexes: Try to acquire mutex only if it is unlocked
      locking/mutexes: Delete the MUTEX_SHOW_NO_WAITER macro
      locking/mutexes: Correct documentation on mutex optimistic spinning
      rtmutex: Make the rtmutex tester depend on BROKEN
      futex: Simplify futex_lock_pi_atomic() and make it more robust
      futex: Split out the first waiter attachment from lookup_pi_state()
      futex: Split out the waiter check from lookup_pi_state()
      futex: Use futex_top_waiter() in lookup_pi_state()
      futex: Make unlock_pi more robust
      rtmutex: Avoid pointless requeueing in the deadlock detection chain walk
      rtmutex: Cleanup deadlock detector debug logic
      rtmutex: Confine deadlock logic to futex
      rtmutex: Simplify remove_waiter()
      rtmutex: Document pi chain walk
      rtmutex: Clarify the boost/deboost part
      rtmutex: No need to keep task ref for lock owner check
      rtmutex: Simplify and document try_to_take_rtmutex()
      ...

commit 3a6bfbc91df04b081a44d419e0260bad54abddf7
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Sun Jun 29 15:09:33 2014 -0700

    arch, locking: Ciao arch_mutex_cpu_relax()
    
    The arch_mutex_cpu_relax() function, introduced by 34b133f, is
    hacky and ugly. It was added a few years ago to address the fact
    that common cpu_relax() calls include yielding on s390, and thus
    impact the optimistic spinning functionality of mutexes. Nowadays
    we use this function well beyond mutexes: rwsem, qrwlock, mcs and
    lockref. Since the macro that defines the call is in the mutex header,
    any users must include mutex.h and the naming is misleading as well.
    
    This patch (i) renames the call to cpu_relax_lowlatency  ("relax, but
    only if you can do it with very low latency") and (ii) defines it in
    each arch's asm/processor.h local header, just like for regular cpu_relax
    functions. On all archs, except s390, cpu_relax_lowlatency is simply cpu_relax,
    and thus we can take it out of mutex.h. While this can seem redundant,
    I believe it is a good choice as it allows us to move out arch specific
    logic from generic locking primitives and enables future(?) archs to
    transparently define it, similarly to System Z.
    
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Bharat Bhushan <r65777@freescale.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jason Wang <jasowang@redhat.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Joe Perches <joe@perches.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Joseph Myers <joseph@codesourcery.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Qiaowei Ren <qiaowei.ren@intel.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Stratos Karafotis <stratosk@semaphore.gr>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vasily Kulikov <segoon@openwall.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Vineet Gupta <Vineet.Gupta1@synopsys.com>
    Cc: Waiman Long <Waiman.Long@hp.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wolfram Sang <wsa@the-dreams.de>
    Cc: adi-buildroot-devel@lists.sourceforge.net
    Cc: linux390@de.ibm.com
    Cc: linux-alpha@vger.kernel.org
    Cc: linux-am33-list@redhat.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-cris-kernel@axis.com
    Cc: linux-hexagon@vger.kernel.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux@lists.openrisc.net
    Cc: linux-m32r-ja@ml.linux-m32r.org
    Cc: linux-m32r@ml.linux-m32r.org
    Cc: linux-m68k@lists.linux-m68k.org
    Cc: linux-metag@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linux-xtensa@linux-xtensa.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1404079773.2619.4.camel@buesod1.americas.hpqcorp.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 34de2a8f7d93..4610b0daf1bf 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -129,6 +129,7 @@ extern void release_thread(struct task_struct *);
 unsigned long get_wchan(struct task_struct *p);
 
 #define cpu_relax()			barrier()
+#define cpu_relax_lowlatency()                cpu_relax()
 
 /* Thread switching */
 extern struct task_struct *cpu_switch_to(struct task_struct *prev,

commit ebe6152e722b9df6b487a0d9464aeff216b6d364
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jul 10 11:37:40 2014 +0100

    arm64: Cast KSTK_(EIP|ESP) to unsigned long
    
    This is for similarity with thread_saved_(pc|sp) and to avoid some
    compiler warnings in the audit code.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 34de2a8f7d93..77712454486b 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -137,8 +137,8 @@ extern struct task_struct *cpu_switch_to(struct task_struct *prev,
 #define task_pt_regs(p) \
 	((struct pt_regs *)(THREAD_START_SP + task_stack_page(p)) - 1)
 
-#define KSTK_EIP(tsk)	task_pt_regs(tsk)->pc
-#define KSTK_ESP(tsk)	task_pt_regs(tsk)->sp
+#define KSTK_EIP(tsk)	((unsigned long)task_pt_regs(tsk)->pc)
+#define KSTK_ESP(tsk)	((unsigned long)task_pt_regs(tsk)->sp)
 
 /*
  * Prefetching support

commit 9141300a5884b57cea6d32c4e3fd16a337cfc99a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sun Apr 6 23:04:12 2014 +0100

    arm64: Provide read/write fault information in compat signal handlers
    
    For AArch32, bit 11 (WnR) of the FSR/ESR register is set when the fault
    was caused by a write access and applications like Qemu rely on such
    information being provided in sigcontext. This patch introduces the
    ESR_EL1 tracking for the arm64 kernel faults and sets bit 11 accordingly
    in compat sigcontext.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 45b20cd6cbca..34de2a8f7d93 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -79,6 +79,7 @@ struct thread_struct {
 	unsigned long		tp_value;
 	struct fpsimd_state	fpsimd_state;
 	unsigned long		fault_address;	/* fault info */
+	unsigned long		fault_code;	/* ESR_EL1 value */
 	struct debug_info	debug;		/* debugging */
 };
 

commit a795a38eb91cf72c4a05e72a9c84e317ee179a48
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Oct 11 14:52:12 2013 +0100

    arm64: compat: add support for big-endian (BE8) AArch32 binaries
    
    This patch adds support for BE8 AArch32 tasks to the compat layer.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index ab239b2c456f..45b20cd6cbca 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -107,6 +107,11 @@ static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
 	regs->pstate = COMPAT_PSR_MODE_USR;
 	if (pc & 1)
 		regs->pstate |= COMPAT_PSR_T_BIT;
+
+#ifdef __AARCH64EB__
+	regs->pstate |= COMPAT_PSR_E_BIT;
+#endif
+
 	regs->compat_sp = sp;
 }
 #endif

commit 6097a07411005c0184cf90256743c784079198fc
Merge: 6a872777ffff 77b67063bb6b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Nov 13 17:36:07 2012 +0000

    Merge tag 'v3.7-rc5' into execve
    
    Linux 3.7-rc5
    
    Conflicts:
            arch/arm64/kernel/process.c

commit f483a853b0b932a1d75eb27a1dcbd732862260db
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Nov 8 16:00:16 2012 +0000

    arm64: mm: fix booting on systems with no memory below 4GB
    
    Booting on a system with all of its memory above the 4GB boundary breaks
    for two reasons:
    
            (1) We still try to create a non-empty DMA32 zone
            (2) no-bootmem limits allocations to 0xffffffff
    
    This patch fixes these issues for ARM64.
    
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 5d810044feda..77f696c14339 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -43,6 +43,8 @@
 #else
 #define STACK_TOP		STACK_TOP_MAX
 #endif /* CONFIG_COMPAT */
+
+#define ARCH_LOW_ADDRESS_LIMIT	PHYS_MASK
 #endif /* __KERNEL__ */
 
 struct debug_info {

commit 16dd46bb781a1d37eeb2377e8e48276e9d14d15d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Oct 16 17:07:46 2012 +0100

    arm64: No need to set the x0-x2 registers in start_thread()
    
    For historical reasons, ARM used to set r0-r2 in start_thread() to the
    first values on the user stack when starting a new user application. The
    same logic has been inherited in AArch64. The x0 register is overridden
    by the sys_execve() return value so it's always zero on success. The x1
    and x2 registers are ignored by AArch64 and EABI AArch32 applications,
    so we can safely remove the register setting for both native and compat
    user space.
    
    This also fixes a potential fault with the kernel accessing user space
    stack directly.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 39a208a392f7..5d810044feda 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -92,30 +92,20 @@ static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
 static inline void start_thread(struct pt_regs *regs, unsigned long pc,
 				unsigned long sp)
 {
-	unsigned long *stack = (unsigned long *)sp;
-
 	start_thread_common(regs, pc);
 	regs->pstate = PSR_MODE_EL0t;
 	regs->sp = sp;
-	regs->regs[2] = stack[2];	/* x2 (envp) */
-	regs->regs[1] = stack[1];	/* x1 (argv) */
-	regs->regs[0] = stack[0];	/* x0 (argc) */
 }
 
 #ifdef CONFIG_COMPAT
 static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
 				       unsigned long sp)
 {
-	unsigned int *stack = (unsigned int *)sp;
-
 	start_thread_common(regs, pc);
 	regs->pstate = COMPAT_PSR_MODE_USR;
 	if (pc & 1)
 		regs->pstate |= COMPAT_PSR_T_BIT;
 	regs->compat_sp = sp;
-	regs->regs[2] = stack[2];	/* x2 (envp) */
-	regs->regs[1] = stack[1];	/* x1 (argv) */
-	regs->regs[0] = stack[0];	/* x0 (argc) */
 }
 #endif
 

commit c34501d21b005a6e363386a19519bd11cf92a67c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Oct 5 12:31:20 2012 +0100

    arm64: Use generic kernel_thread() implementation
    
    This patch enables CONFIG_GENERIC_KERNEL_THREAD on arm64, changes
    copy_threads to cope with kernel threads creation and adapts
    ret_from_fork accordingly. The arm64-specific kernel_thread
    implementation is no longer needed.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
index 39a208a392f7..d6331acaf64e 100644
--- a/arch/arm64/include/asm/processor.h
+++ b/arch/arm64/include/asm/processor.h
@@ -136,11 +136,6 @@ unsigned long get_wchan(struct task_struct *p);
 extern struct task_struct *cpu_switch_to(struct task_struct *prev,
 					 struct task_struct *next);
 
-/*
- * Create a new kernel thread
- */
-extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
-
 #define task_pt_regs(p) \
 	((struct pt_regs *)(THREAD_START_SP + task_stack_page(p)) - 1)
 

commit 9cce7a435f89c9e60f244d44da2cf1cf4ed094ac
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: CPU support
    
    This patch adds AArch64 CPU specific functionality. It assumes that the
    implementation is generic to AArch64 and does not require specific
    identification. Different CPU implementations may require the setting of
    various ACTLR_EL1 bits but such information is not currently available
    and it should ideally be pushed to firmware.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
new file mode 100644
index 000000000000..39a208a392f7
--- /dev/null
+++ b/arch/arm64/include/asm/processor.h
@@ -0,0 +1,175 @@
+/*
+ * Based on arch/arm/include/asm/processor.h
+ *
+ * Copyright (C) 1995-1999 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_PROCESSOR_H
+#define __ASM_PROCESSOR_H
+
+/*
+ * Default implementation of macro that returns current
+ * instruction pointer ("program counter").
+ */
+#define current_text_addr() ({ __label__ _l; _l: &&_l;})
+
+#ifdef __KERNEL__
+
+#include <linux/string.h>
+
+#include <asm/fpsimd.h>
+#include <asm/hw_breakpoint.h>
+#include <asm/ptrace.h>
+#include <asm/types.h>
+
+#ifdef __KERNEL__
+#define STACK_TOP_MAX		TASK_SIZE_64
+#ifdef CONFIG_COMPAT
+#define AARCH32_VECTORS_BASE	0xffff0000
+#define STACK_TOP		(test_thread_flag(TIF_32BIT) ? \
+				AARCH32_VECTORS_BASE : STACK_TOP_MAX)
+#else
+#define STACK_TOP		STACK_TOP_MAX
+#endif /* CONFIG_COMPAT */
+#endif /* __KERNEL__ */
+
+struct debug_info {
+	/* Have we suspended stepping by a debugger? */
+	int			suspended_step;
+	/* Allow breakpoints and watchpoints to be disabled for this thread. */
+	int			bps_disabled;
+	int			wps_disabled;
+	/* Hardware breakpoints pinned to this task. */
+	struct perf_event	*hbp_break[ARM_MAX_BRP];
+	struct perf_event	*hbp_watch[ARM_MAX_WRP];
+};
+
+struct cpu_context {
+	unsigned long x19;
+	unsigned long x20;
+	unsigned long x21;
+	unsigned long x22;
+	unsigned long x23;
+	unsigned long x24;
+	unsigned long x25;
+	unsigned long x26;
+	unsigned long x27;
+	unsigned long x28;
+	unsigned long fp;
+	unsigned long sp;
+	unsigned long pc;
+};
+
+struct thread_struct {
+	struct cpu_context	cpu_context;	/* cpu context */
+	unsigned long		tp_value;
+	struct fpsimd_state	fpsimd_state;
+	unsigned long		fault_address;	/* fault info */
+	struct debug_info	debug;		/* debugging */
+};
+
+#define INIT_THREAD  {	}
+
+static inline void start_thread_common(struct pt_regs *regs, unsigned long pc)
+{
+	memset(regs, 0, sizeof(*regs));
+	regs->syscallno = ~0UL;
+	regs->pc = pc;
+}
+
+static inline void start_thread(struct pt_regs *regs, unsigned long pc,
+				unsigned long sp)
+{
+	unsigned long *stack = (unsigned long *)sp;
+
+	start_thread_common(regs, pc);
+	regs->pstate = PSR_MODE_EL0t;
+	regs->sp = sp;
+	regs->regs[2] = stack[2];	/* x2 (envp) */
+	regs->regs[1] = stack[1];	/* x1 (argv) */
+	regs->regs[0] = stack[0];	/* x0 (argc) */
+}
+
+#ifdef CONFIG_COMPAT
+static inline void compat_start_thread(struct pt_regs *regs, unsigned long pc,
+				       unsigned long sp)
+{
+	unsigned int *stack = (unsigned int *)sp;
+
+	start_thread_common(regs, pc);
+	regs->pstate = COMPAT_PSR_MODE_USR;
+	if (pc & 1)
+		regs->pstate |= COMPAT_PSR_T_BIT;
+	regs->compat_sp = sp;
+	regs->regs[2] = stack[2];	/* x2 (envp) */
+	regs->regs[1] = stack[1];	/* x1 (argv) */
+	regs->regs[0] = stack[0];	/* x0 (argc) */
+}
+#endif
+
+/* Forward declaration, a strange C thing */
+struct task_struct;
+
+/* Free all resources held by a thread. */
+extern void release_thread(struct task_struct *);
+
+/* Prepare to copy thread state - unlazy all lazy status */
+#define prepare_to_copy(tsk)	do { } while (0)
+
+unsigned long get_wchan(struct task_struct *p);
+
+#define cpu_relax()			barrier()
+
+/* Thread switching */
+extern struct task_struct *cpu_switch_to(struct task_struct *prev,
+					 struct task_struct *next);
+
+/*
+ * Create a new kernel thread
+ */
+extern int kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
+
+#define task_pt_regs(p) \
+	((struct pt_regs *)(THREAD_START_SP + task_stack_page(p)) - 1)
+
+#define KSTK_EIP(tsk)	task_pt_regs(tsk)->pc
+#define KSTK_ESP(tsk)	task_pt_regs(tsk)->sp
+
+/*
+ * Prefetching support
+ */
+#define ARCH_HAS_PREFETCH
+static inline void prefetch(const void *ptr)
+{
+	asm volatile("prfm pldl1keep, %a0\n" : : "p" (ptr));
+}
+
+#define ARCH_HAS_PREFETCHW
+static inline void prefetchw(const void *ptr)
+{
+	asm volatile("prfm pstl1keep, %a0\n" : : "p" (ptr));
+}
+
+#define ARCH_HAS_SPINLOCK_PREFETCH
+static inline void spin_lock_prefetch(const void *x)
+{
+	prefetchw(x);
+}
+
+#define HAVE_ARCH_PICK_MMAP_LAYOUT
+
+#endif
+
+#endif /* __ASM_PROCESSOR_H */
