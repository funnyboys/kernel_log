commit a7ba121215faad444f94db5e89eb5d7dfc58ecb2
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Jun 7 21:41:51 2020 -0700

    arm64: use asm-generic/cacheflush.h
    
    ARM64 needs almost no cache flushing routines of its own.  Rely on
    asm-generic/cacheflush.h for the defaults.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200515143646.3857579-10-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index ce50c1f1f1ea..9384fd8fc13c 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -94,20 +94,7 @@ static inline void flush_icache_range(unsigned long start, unsigned long end)
 
 	kick_all_cpus_sync();
 }
-
-static inline void flush_cache_mm(struct mm_struct *mm)
-{
-}
-
-static inline void flush_cache_page(struct vm_area_struct *vma,
-				    unsigned long user_addr, unsigned long pfn)
-{
-}
-
-static inline void flush_cache_range(struct vm_area_struct *vma,
-				     unsigned long start, unsigned long end)
-{
-}
+#define flush_icache_range flush_icache_range
 
 /*
  * Cache maintenance functions used by the DMA API. No to be used directly.
@@ -123,12 +110,7 @@ extern void __dma_flush_area(const void *, size_t);
  */
 extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 	unsigned long, void *, const void *, unsigned long);
-#define copy_from_user_page(vma, page, vaddr, dst, src, len) \
-	do {							\
-		memcpy(dst, src, len);				\
-	} while (0)
-
-#define flush_cache_dup_mm(mm) flush_cache_mm(mm)
+#define copy_to_user_page copy_to_user_page
 
 /*
  * flush_dcache_page is used when the kernel has written to the page
@@ -154,29 +136,11 @@ static __always_inline void __flush_icache_all(void)
 	dsb(ish);
 }
 
-#define flush_dcache_mmap_lock(mapping)		do { } while (0)
-#define flush_dcache_mmap_unlock(mapping)	do { } while (0)
-
-/*
- * We don't appear to need to do anything here.  In fact, if we did, we'd
- * duplicate cache flushing elsewhere performed by flush_dcache_page().
- */
-#define flush_icache_page(vma,page)	do { } while (0)
-
-/*
- * Not required on AArch64 (PIPT or VIPT non-aliasing D-cache).
- */
-static inline void flush_cache_vmap(unsigned long start, unsigned long end)
-{
-}
-
-static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
-{
-}
-
 int set_memory_valid(unsigned long addr, int numpages, int enable);
 
 int set_direct_map_invalid_noflush(struct page *page);
 int set_direct_map_default_noflush(struct page *page);
 
-#endif
+#include <asm-generic/cacheflush.h>
+
+#endif /* __ASM_CACHEFLUSH_H */

commit ab8ad279ceac4fc78ae4dcf1a26326e05695e537
Author: Daniel Thompson <daniel.thompson@linaro.org>
Date:   Mon May 4 18:05:18 2020 +0100

    arm64: cacheflush: Fix KGDB trap detection
    
    flush_icache_range() contains a bodge to avoid issuing IPIs when the kgdb
    trap handler is running because issuing IPIs is unsafe (and not needed)
    in this execution context. However the current test, based on
    kgdb_connected is flawed: it both over-matches and under-matches.
    
    The over match occurs because kgdb_connected is set when gdb attaches
    to the stub and remains set during normal running. This is relatively
    harmelss because in almost all cases irq_disabled() will be false.
    
    The under match is more serious. When kdb is used instead of kgdb to access
    the debugger then kgdb_connected is not set in all the places that the
    debug core updates sw breakpoints (and hence flushes the icache). This
    can lead to deadlock.
    
    Fix by replacing the ad-hoc check with the proper kgdb macro. This also
    allows us to drop the #ifdef wrapper.
    
    Fixes: 3b8c9f1cdfc5 ("arm64: IPI each CPU after invalidating the I-cache for kernel mappings")
    Signed-off-by: Daniel Thompson <daniel.thompson@linaro.org>
    Reviewed-by: Douglas Anderson <dianders@chromium.org>
    Link: https://lore.kernel.org/r/20200504170518.2959478-1-daniel.thompson@linaro.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index e6cca3d4acf7..ce50c1f1f1ea 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -79,7 +79,7 @@ static inline void flush_icache_range(unsigned long start, unsigned long end)
 	 * IPI all online CPUs so that they undergo a context synchronization
 	 * event and are forced to refetch the new instructions.
 	 */
-#ifdef CONFIG_KGDB
+
 	/*
 	 * KGDB performs cache maintenance with interrupts disabled, so we
 	 * will deadlock trying to IPI the secondary CPUs. In theory, we can
@@ -89,9 +89,9 @@ static inline void flush_icache_range(unsigned long start, unsigned long end)
 	 * the patching operation, so we don't need extra IPIs here anyway.
 	 * In which case, add a KGDB-specific bodge and return early.
 	 */
-	if (kgdb_connected && irqs_disabled())
+	if (in_dbg_master())
 		return;
-#endif
+
 	kick_all_cpus_sync();
 }
 

commit e43f1331e2ef913b8c566920c9af75e0ccdd1d3f
Author: James Morse <james.morse@arm.com>
Date:   Thu Feb 20 16:58:39 2020 +0000

    arm64: Ask the compiler to __always_inline functions used by KVM at HYP
    
    KVM uses some of the static-inline helpers like icache_is_vipt() from
    its HYP code. This assumes the function is inlined so that the code is
    mapped to EL2. The compiler may decide not to inline these, and the
    out-of-line version may not be in the __hyp_text section.
    
    Add the additional __always_ hint to these static-inlines that are used
    by KVM.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200220165839.256881-4-james.morse@arm.com

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 665c78e0665a..e6cca3d4acf7 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -145,7 +145,7 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 #define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
 extern void flush_dcache_page(struct page *);
 
-static inline void __flush_icache_all(void)
+static __always_inline void __flush_icache_all(void)
 {
 	if (cpus_have_const_cap(ARM64_HAS_CACHE_DIC))
 		return;

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit 4739d53fcd1df8a9f6f72bb02a3a1d852ad252b3
Author: Ard Biesheuvel <ard.biesheuvel@arm.com>
Date:   Thu May 23 11:22:54 2019 +0100

    arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
    
    Wire up the special helper functions to manipulate aliases of vmalloc
    regions in the linear map.
    
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 19844211a4e6..b9ee5510067f 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -187,4 +187,7 @@ static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
 
 int set_memory_valid(unsigned long addr, int numpages, int enable);
 
+int set_direct_map_invalid_noflush(struct page *page);
+int set_direct_map_default_noflush(struct page *page);
+
 #endif

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 19844211a4e6..1fe4467442aa 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -1,20 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Based on arch/arm/include/asm/cacheflush.h
  *
  * Copyright (C) 1999-2002 Russell King.
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_CACHEFLUSH_H
 #define __ASM_CACHEFLUSH_H

commit 3b8c9f1cdfc506e94e992ae66b68bbe416f89610
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 11 14:22:09 2018 +0100

    arm64: IPI each CPU after invalidating the I-cache for kernel mappings
    
    When invalidating the instruction cache for a kernel mapping via
    flush_icache_range(), it is also necessary to flush the pipeline for
    other CPUs so that instructions fetched into the pipeline before the
    I-cache invalidation are discarded. For example, if module 'foo' is
    unloaded and then module 'bar' is loaded into the same area of memory,
    a CPU could end up executing instructions from 'foo' when branching into
    'bar' if these instructions were fetched into the pipeline before 'foo'
    was unloaded.
    
    Whilst this is highly unlikely to occur in practice, particularly as
    any exception acts as a context-synchronizing operation, following the
    letter of the architecture requires us to execute an ISB on each CPU
    in order for the new instruction stream to be visible.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index d264a7274811..19844211a4e6 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -19,6 +19,7 @@
 #ifndef __ASM_CACHEFLUSH_H
 #define __ASM_CACHEFLUSH_H
 
+#include <linux/kgdb.h>
 #include <linux/mm.h>
 
 /*
@@ -71,7 +72,7 @@
  *		- kaddr  - page address
  *		- size   - region size
  */
-extern void flush_icache_range(unsigned long start, unsigned long end);
+extern void __flush_icache_range(unsigned long start, unsigned long end);
 extern int  invalidate_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
 extern void __inval_dcache_area(void *addr, size_t len);
@@ -81,6 +82,30 @@ extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);
 extern void sync_icache_aliases(void *kaddr, unsigned long len);
 
+static inline void flush_icache_range(unsigned long start, unsigned long end)
+{
+	__flush_icache_range(start, end);
+
+	/*
+	 * IPI all online CPUs so that they undergo a context synchronization
+	 * event and are forced to refetch the new instructions.
+	 */
+#ifdef CONFIG_KGDB
+	/*
+	 * KGDB performs cache maintenance with interrupts disabled, so we
+	 * will deadlock trying to IPI the secondary CPUs. In theory, we can
+	 * set CACHE_FLUSH_IS_SAFE to 0 to avoid this known issue, but that
+	 * just means that KGDB will elide the maintenance altogether! As it
+	 * turns out, KGDB uses IPIs to round-up the secondary CPUs during
+	 * the patching operation, so we don't need extra IPIs here anyway.
+	 * In which case, add a KGDB-specific bodge and return early.
+	 */
+	if (kgdb_connected && irqs_disabled())
+		return;
+#endif
+	kick_all_cpus_sync();
+}
+
 static inline void flush_cache_mm(struct mm_struct *mm)
 {
 }

commit 5fb94e9ca333f0fe1d96de06704a79942b3832c3
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Tue May 8 15:14:57 2018 -0300

    docs: Fix some broken references
    
    As we move stuff around, some doc references are broken. Fix some of
    them via this script:
            ./scripts/documentation-file-ref-check --fix
    
    Manually checked if the produced result is valid, removing a few
    false-positives.
    
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Stephen Boyd <sboyd@kernel.org>
    Acked-by: Charles Keepax <ckeepax@opensource.wolfsonmicro.com>
    Acked-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Acked-by: Jonathan Corbet <corbet@lwn.net>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 0094c6653b06..d264a7274811 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -36,7 +36,7 @@
  *	Start addresses are inclusive and end addresses are exclusive; start
  *	addresses should be rounded down, end addresses up.
  *
- *	See Documentation/cachetlb.txt for more information. Please note that
+ *	See Documentation/core-api/cachetlb.rst for more information. Please note that
  *	the implementation assumes non-aliasing VIPT D-cache and (aliasing)
  *	VIPT I-cache.
  *

commit 427c896f262ad70d1e6b04890945675df13bb031
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Tue Apr 10 16:36:36 2018 -0700

    arm64: turn flush_dcache_mmap_lock into a no-op
    
    ARM64 doesn't walk the VMA tree in its flush_dcache_page()
    implementation, so has no need to take the tree_lock.
    
    Link: http://lkml.kernel.org/r/20180313132639.17387-4-willy@infradead.org
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jeff Layton <jlayton@kernel.org>
    Cc: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 7dfcec4700fe..0094c6653b06 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -140,10 +140,8 @@ static inline void __flush_icache_all(void)
 	dsb(ish);
 }
 
-#define flush_dcache_mmap_lock(mapping) \
-	spin_lock_irq(&(mapping)->tree_lock)
-#define flush_dcache_mmap_unlock(mapping) \
-	spin_unlock_irq(&(mapping)->tree_lock)
+#define flush_dcache_mmap_lock(mapping)		do { } while (0)
+#define flush_dcache_mmap_unlock(mapping)	do { } while (0)
 
 /*
  * We don't appear to need to do anything here.  In fact, if we did, we'd

commit 6ae4b6e0578886eb36cedbf99f04031d93f9e315
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Wed Mar 7 09:00:08 2018 -0600

    arm64: Add support for new control bits CTR_EL0.DIC and CTR_EL0.IDC
    
    The DCache clean & ICache invalidation requirements for instructions
    to be data coherence are discoverable through new fields in CTR_EL0.
    The following two control bits DIC and IDC were defined for this
    purpose. No need to perform point of unification cache maintenance
    operations from software on systems where CPU caches are transparent.
    
    This patch optimize the three functions __flush_cache_user_range(),
    clean_dcache_area_pou() and invalidate_icache_range() if the hardware
    reports CTR_EL0.IDC and/or CTR_EL0.IDC. Basically it skips the two
    instructions 'DC CVAU' and 'IC IVAU', and the associated loop logic
    in order to avoid the unnecessary overhead.
    
    CTR_EL0.DIC: Instruction cache invalidation requirements for
     instruction to data coherence. The meaning of this bit[29].
      0: Instruction cache invalidation to the point of unification
         is required for instruction to data coherence.
      1: Instruction cache cleaning to the point of unification is
          not required for instruction to data coherence.
    
    CTR_EL0.IDC: Data cache clean requirements for instruction to data
     coherence. The meaning of this bit[28].
      0: Data cache clean to the point of unification is required for
         instruction to data coherence, unless CLIDR_EL1.LoC == 0b000
         or (CLIDR_EL1.LoUIS == 0b000 && CLIDR_EL1.LoUU == 0b000).
      1: Data cache clean to the point of unification is not required
         for instruction to data coherence.
    
    Co-authored-by: Philip Elcan <pelcan@codeaurora.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index bef9f418f089..7dfcec4700fe 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -133,6 +133,9 @@ extern void flush_dcache_page(struct page *);
 
 static inline void __flush_icache_all(void)
 {
+	if (cpus_have_const_cap(ARM64_HAS_CACHE_DIC))
+		return;
+
 	asm("ic	ialluis");
 	dsb(ish);
 }

commit 4fee94736603cd6fd83c1ea1ee0388d1d2dbe11b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 23 17:11:16 2017 +0100

    arm64: KVM: Add invalidate_icache_range helper
    
    We currently tightly couple dcache clean with icache invalidation,
    but KVM could do without the initial flush to PoU, as we've
    already flushed things to PoC.
    
    Let's introduce invalidate_icache_range which is limited to
    invalidating the icache from the linear mapping (and thus
    has none of the userspace fault handling complexity), and
    wire it in KVM instead of flush_icache_range.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 955130762a3c..bef9f418f089 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -52,6 +52,12 @@
  *		- start  - virtual start address
  *		- end    - virtual end address
  *
+ *	invalidate_icache_range(start, end)
+ *
+ *		Invalidate the I-cache in the region described by start, end.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
  *	__flush_cache_user_range(start, end)
  *
  *		Ensure coherency between the I-cache and the D-cache in the
@@ -66,6 +72,7 @@
  *		- size   - region size
  */
 extern void flush_icache_range(unsigned long start, unsigned long end);
+extern int  invalidate_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
 extern void __inval_dcache_area(void *addr, size_t len);
 extern void __clean_dcache_area_poc(void *addr, size_t len);

commit f81a348728ec5ac43f3bbcf81c97d52baba253f7
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Nov 21 11:59:13 2017 +0000

    arm64: mm: cleanup stale AIVIVT references
    
    Since commit:
    
      155433cb365ee466 ("arm64: cache: Remove support for ASID-tagged VIVT I-caches")
    
    ... the kernel no longer cares about AIVIVT I-caches, as these were
    removed from the architecture.
    
    This patch removes the stale references to such I-caches.
    
    The comment in flush_context() is also updated to clarify when and where
    the TLB invalidation occurs.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 76d1cc85d5b1..955130762a3c 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -38,7 +38,7 @@
  *
  *	See Documentation/cachetlb.txt for more information. Please note that
  *	the implementation assumes non-aliasing VIPT D-cache and (aliasing)
- *	VIPT or ASID-tagged VIVT I-cache.
+ *	VIPT I-cache.
  *
  *	flush_cache_mm(mm)
  *

commit d50e071fdaa33c1b399c764c44fa1ce879881185
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:42 2017 +0100

    arm64: Implement pmem API support
    
    Add a clean-to-point-of-persistence cache maintenance helper, and wire
    up the basic architectural support for the pmem driver based on it.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    [catalin.marinas@arm.com: move arch_*_pmem() functions to arch/arm64/mm/flush.c]
    [catalin.marinas@arm.com: change dmb(sy) to dmb(osh)]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index b4b43a94dffd..76d1cc85d5b1 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -69,6 +69,7 @@ extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
 extern void __inval_dcache_area(void *addr, size_t len);
 extern void __clean_dcache_area_poc(void *addr, size_t len);
+extern void __clean_dcache_area_pop(void *addr, size_t len);
 extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);
 extern void sync_icache_aliases(void *kaddr, unsigned long len);

commit d46befef4c03fb61a62b3319ff5265aaac7bc465
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:39 2017 +0100

    arm64: Convert __inval_cache_range() to area-based
    
    __inval_cache_range() is already the odd one out among our data cache
    maintenance routines as the only remaining range-based one; as we're
    going to want an invalidation routine to call from C code for the pmem
    API, let's tweak the prototype and name to bring it in line with the
    clean operations, and to make its relationship with __dma_inv_area()
    neatly mirror that of __clean_dcache_area_poc() and __dma_clean_area().
    The loop clearing the early page tables gets mildly massaged in the
    process for the sake of consistency.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 4d4f650c290e..b4b43a94dffd 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -67,6 +67,7 @@
  */
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
+extern void __inval_dcache_area(void *addr, size_t len);
 extern void __clean_dcache_area_poc(void *addr, size_t len);
 extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);

commit 09c2a7dc4ca2755892b74858fe3bf62b652ff9d0
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:38 2017 +0100

    arm64: mm: Fix set_memory_valid() declaration
    
    Clearly, set_memory_valid() has never been seen in the same room as its
    declaration... Whilst the type mismatch is such that kexec probably
    wasn't broken in practice, fix it to match the definition as it should.
    
    Fixes: 9b0aa14e3155 ("arm64: mm: add set_memory_valid()")
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index d74a284abdc2..4d4f650c290e 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -150,6 +150,6 @@ static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
 {
 }
 
-int set_memory_valid(unsigned long addr, unsigned long size, int enable);
+int set_memory_valid(unsigned long addr, int numpages, int enable);
 
 #endif

commit e6ccbff0e90cf4bf012bf369dbdaf84c6faaedaa
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:58:47 2017 -0700

    treewide: decouple cacheflush.h and set_memory.h
    
    Now that all call sites, completely decouple cacheflush.h and
    set_memory.h
    
    [sfr@canb.auug.org.au: kprobes/x86: merge fix for set_memory.h decoupling]
      Link: http://lkml.kernel.org/r/20170418180903.10300fd3@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1488920133-27229-17-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 0927f47607e2..d74a284abdc2 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -20,7 +20,6 @@
 #define __ASM_CACHEFLUSH_H
 
 #include <linux/mm.h>
-#include <asm/set_memory.h>
 
 /*
  * This flag is used to indicate that the page pointed to by a pte is clean

commit 299878bac326c890699c696ebba26f56fe93fc75
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:57:59 2017 -0700

    treewide: move set_memory_* functions away from cacheflush.h
    
    Patch series "set_memory_* functions header refactor", v3.
    
    The set_memory_* APIs came out of a desire to have a better way to
    change memory attributes.  Many of these attributes were linked to cache
    functionality so the prototypes were put in cacheflush.h.  These days,
    the APIs have grown and have a much wider use than just cache APIs.  To
    support this growth, split off set_memory_* and friends into a separate
    header file to avoid growing cacheflush.h for APIs that have nothing to
    do with caches.
    
    Link: http://lkml.kernel.org/r/1488920133-27229-2-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 728f933cef8c..0927f47607e2 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -20,6 +20,7 @@
 #define __ASM_CACHEFLUSH_H
 
 #include <linux/mm.h>
+#include <asm/set_memory.h>
 
 /*
  * This flag is used to indicate that the page pointed to by a pte is clean
@@ -150,10 +151,6 @@ static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
 {
 }
 
-int set_memory_ro(unsigned long addr, int numpages);
-int set_memory_rw(unsigned long addr, int numpages);
-int set_memory_x(unsigned long addr, int numpages);
-int set_memory_nx(unsigned long addr, int numpages);
 int set_memory_valid(unsigned long addr, unsigned long size, int enable);
 
 #endif

commit 9b0aa14e31555b64460c1ee323d0e19bf891958b
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:33 2017 +0900

    arm64: mm: add set_memory_valid()
    
    This function validates and invalidates PTE entries, and will be utilized
    in kdump to protect loaded crash dump kernel image.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 5a2a6ee65f65..728f933cef8c 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -154,5 +154,6 @@ int set_memory_ro(unsigned long addr, int numpages);
 int set_memory_rw(unsigned long addr, int numpages);
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
+int set_memory_valid(unsigned long addr, unsigned long size, int enable);
 
 #endif

commit ee6a7fce8e5ecd90794ad7c9f62518c753fb3cb6
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Nov 23 18:05:52 2016 +0000

    arm64: Remove I-cache invalidation from flush_cache_range()
    
    The flush_cache_range() function (similarly for flush_cache_page()) is
    called when the kernel is changing an existing VA->PA mapping range to
    either a new PA or to different attributes. Since ARMv8 has PIPT-like
    D-caches, this function does not need to perform any D-cache
    maintenance. The I-cache maintenance is already handled via set_pte_at()
    and flush_cache_range() cannot anyway guarantee that there are no cache
    lines left after invalidation due to the speculative loads.
    
    This patch makes flush_cache_range() a no-op.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index e9f64ecb75ce..5a2a6ee65f65 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -65,7 +65,6 @@
  *		- kaddr  - page address
  *		- size   - region size
  */
-extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
 extern void __clean_dcache_area_poc(void *addr, size_t len);
@@ -82,6 +81,11 @@ static inline void flush_cache_page(struct vm_area_struct *vma,
 {
 }
 
+static inline void flush_cache_range(struct vm_area_struct *vma,
+				     unsigned long start, unsigned long end)
+{
+}
+
 /*
  * Cache maintenance functions used by the DMA API. No to be used directly.
  */

commit 9842ceae9fa8deae141533d52a6ead7666962c09
Author: Pratyush Anand <panand@redhat.com>
Date:   Wed Nov 2 14:40:46 2016 +0530

    arm64: Add uprobe support
    
    This patch adds support for uprobe on ARM64 architecture.
    
    Unit tests for following have been done so far and they have been found
    working
        1. Step-able instructions, like sub, ldr, add etc.
        2. Simulation-able like ret, cbnz, cbz etc.
        3. uretprobe
        4. Reject-able instructions like sev, wfe etc.
        5. trapped and abort xol path
        6. probe at unaligned user address.
        7. longjump test cases
    
    Currently it does not support aarch32 instruction probing.
    
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 2e5fb976a572..e9f64ecb75ce 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -71,6 +71,7 @@ extern void __flush_dcache_area(void *addr, size_t len);
 extern void __clean_dcache_area_poc(void *addr, size_t len);
 extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);
+extern void sync_icache_aliases(void *kaddr, unsigned long len);
 
 static inline void flush_cache_mm(struct mm_struct *mm)
 {

commit d34fdb7081394cbf93fa6571d990086356f4ea9d
Author: Kwangwoo Lee <kwangwoo.lee@sk.com>
Date:   Tue Aug 2 09:50:50 2016 +0900

    arm64: mm: convert __dma_* routines to use start, size
    
    __dma_* routines have been converted to use start and size instread of
    start and end addresses. The patch was origianlly for adding
    __clean_dcache_area_poc() which will be used in pmem driver to clean
    dcache to the PoC(Point of Coherency) in arch_wb_cache_pmem().
    
    The functionality of __clean_dcache_area_poc()  was equivalent to
    __dma_clean_range(). The difference was __dma_clean_range() uses the end
    address, but __clean_dcache_area_poc() uses the size to clean.
    
    Thus, __clean_dcache_area_poc() has been revised with a fallthrough
    function of __dma_clean_range() after the change that __dma_* routines
    use start and size instead of using start and end.
    
    As a consequence of using start and size, the name of __dma_* routines
    has also been altered following the terminology below:
        area: takes a start and size
        range: takes a start and end
    
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Kwangwoo Lee <kwangwoo.lee@sk.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index c64268dbff64..2e5fb976a572 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -68,6 +68,7 @@
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
+extern void __clean_dcache_area_poc(void *addr, size_t len);
 extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);
 
@@ -85,7 +86,7 @@ static inline void flush_cache_page(struct vm_area_struct *vma,
  */
 extern void __dma_map_area(const void *, size_t, int);
 extern void __dma_unmap_area(const void *, size_t, int);
-extern void __dma_flush_range(const void *, const void *);
+extern void __dma_flush_area(const void *, size_t);
 
 /*
  * Copy user data from/to a page which is mapped into a different

commit 9d854607f9005c593dca9672b708f28e6ef96fe4
Merge: 8a20a04bda46 691b1e2ebf72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 19:13:59 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull second set of arm64 updates from Catalin Marinas:
    
     - KASLR bug fixes: use callee-saved register, boot-time I-cache
       maintenance
    
     - inv_entry asm macro fix (EL0 check typo)
    
     - pr_notice("Virtual kernel memory layout...") splitting
    
     - Clean-ups: use p?d_set_huge consistently, allow preemption around
       copy_to_user_page, remove unused __local_flush_icache_all()
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: mm: allow preemption in copy_to_user_page
      arm64: consistently use p?d_set_huge
      arm64: kaslr: use callee saved register to preserve SCTLR across C call
      arm64: Split pr_notice("Virtual kernel memory layout...") into multiple pr_cont()
      arm64: drop unused __local_flush_icache_all()
      arm64: fix KASLR boot-time I-cache maintenance
      arm64/kernel: fix incorrect EL0 check in inv_entry macro

commit cc7c0cda8f8c03d6867408e3df953b2fa67d704c
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Tue Mar 15 17:47:10 2016 +0800

    arm64: drop unused __local_flush_icache_all()
    
    After commit 65da0a8e34a8 ("arm64: use non-global mappings for UEFI
    runtime regions"), nobody use __local_flush_icache_all() anymore,
    so drop it.
    
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 7fc294c3bc5b..6a5ecbddcace 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -116,13 +116,6 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 #define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
 extern void flush_dcache_page(struct page *);
 
-static inline void __local_flush_icache_all(void)
-{
-	asm("ic iallu");
-	dsb(nsh);
-	isb();
-}
-
 static inline void __flush_icache_all(void)
 {
 	asm("ic	ialluis");

commit e267d97b83d9cecc16c54825f9f3ac7f72dc1e1e
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Feb 17 14:41:12 2016 -0800

    asm-generic: Consolidate mark_rodata_ro()
    
    Instead of defining mark_rodata_ro() in each architecture, consolidate it.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Gross <agross@codeaurora.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Ashok Kumar <ashoks@broadcom.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Brown <david.brown@linaro.org>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Emese Revfy <re.emese@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: kernel-hardening@lists.openwall.com
    Cc: linux-arch <linux-arch@vger.kernel.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-parisc@vger.kernel.org
    Link: http://lkml.kernel.org/r/1455748879-21872-2-git-send-email-keescook@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 7fc294c3bc5b..22dda613f9c9 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -156,8 +156,4 @@ int set_memory_rw(unsigned long addr, int numpages);
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
 
-#ifdef CONFIG_DEBUG_RODATA
-void mark_rodata_ro(void);
-#endif
-
 #endif

commit 0a28714c53fd4f7aea709be7577dfbe0095c8c3e
Author: Ashok Kumar <ashoks@broadcom.com>
Date:   Thu Dec 17 01:38:32 2015 -0800

    arm64: Use PoU cache instr for I/D coherency
    
    In systems with three levels of cache(PoU at L1 and PoC at L3),
    PoC cache flush instructions flushes L2 and L3 caches which could affect
    performance.
    For cache flushes for I and D coherency, PoU should suffice.
    So changing all I and D coherency related cache flushes to PoU.
    
    Introduced a new __clean_dcache_area_pou API for dcache flush till PoU
    and provided a common macro for __flush_dcache_area and
    __clean_dcache_area_pou.
    
    Also, now in __sync_icache_dcache, icache invalidation for non-aliasing
    VIPT icache is done only for that particular page instead of the earlier
    __flush_icache_all.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ashok Kumar <ashoks@broadcom.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 54efedaf331f..7fc294c3bc5b 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -68,6 +68,7 @@
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
+extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);
 
 static inline void flush_cache_mm(struct mm_struct *mm)

commit 8e63d38876691756f9bc6930850f1fb77809be1b
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:23 2015 +0100

    arm64: flush: use local TLB and I-cache invalidation
    
    There are a number of places where a single CPU is running with a
    private page-table and we need to perform maintenance on the TLB and
    I-cache in order to ensure correctness, but do not require the operation
    to be broadcast to other CPUs.
    
    This patch adds local variants of tlb_flush_all and __flush_icache_all
    to support these use-cases and updates the callers respectively.
    __local_flush_icache_all also implies an isb, since it is intended to be
    used synchronously.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index c75b8d027eb1..54efedaf331f 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -115,6 +115,13 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 #define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
 extern void flush_dcache_page(struct page *);
 
+static inline void __local_flush_icache_all(void)
+{
+	asm("ic iallu");
+	dsb(nsh);
+	isb();
+}
+
 static inline void __flush_icache_all(void)
 {
 	asm("ic	ialluis");

commit 68234df4ea7939f98431aa81113fbdce10c4a84b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Apr 20 10:24:35 2015 +0100

    arm64: kill flush_cache_all()
    
    The documented semantics of flush_cache_all are not possible to provide
    for arm64 (short of flushing the entire physical address space by VA),
    and there are currently no users; KVM uses VA maintenance exclusively,
    cpu_reset is never called, and the only two users outside of arch code
    cannot be built for arm64.
    
    While cpu_soft_reset and related functions (which call flush_cache_all)
    were thought to be useful for kexec, their current implementations only
    serve to mask bugs. For correctness kexec will need to perform
    maintenance by VA anyway to account for system caches, line migration,
    and other subtleties of the cache architecture. As the extent of this
    cache maintenance will be kexec-specific, it should probably live in the
    kexec code.
    
    This patch removes flush_cache_all, and related unused components,
    preventing further abuse.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 67d309cc3b6b..c75b8d027eb1 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -40,10 +40,6 @@
  *	the implementation assumes non-aliasing VIPT D-cache and (aliasing)
  *	VIPT or ASID-tagged VIVT I-cache.
  *
- *	flush_cache_all()
- *
- *		Unconditionally clean and invalidate the entire cache.
- *
  *	flush_cache_mm(mm)
  *
  *		Clean and invalidate all user space cache entries
@@ -69,7 +65,6 @@
  *		- kaddr  - page address
  *		- size   - region size
  */
-extern void flush_cache_all(void);
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);

commit da141706aea52c1a9fbd28cb8d289b78819f5436
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jan 21 17:36:06 2015 -0800

    arm64: add better page protections to arm64
    
    Add page protections for arm64 similar to those in arm.
    This is for security reasons to prevent certain classes
    of exploits. The current method:
    
    - Map all memory as either RWX or RW. We round to the nearest
      section to avoid creating page tables before everything is mapped
    - Once everything is mapped, if either end of the RWX section should
      not be X, we split the PMD and remap as necessary
    - When initmem is to be freed, we change the permissions back to
      RW (using stop machine if necessary to flush the TLB)
    - If CONFIG_DEBUG_RODATA is set, the read only sections are set
      read only.
    
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Kees Cook <keescook@chromium.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 7ae31a2cc6c0..67d309cc3b6b 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -152,4 +152,9 @@ int set_memory_ro(unsigned long addr, int numpages);
 int set_memory_rw(unsigned long addr, int numpages);
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
+
+#ifdef CONFIG_DEBUG_RODATA
+void mark_rodata_ro(void);
+#endif
+
 #endif

commit a2d25a5391ca219f196f9fee7b535c40d201c6bf
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Dec 1 10:53:08 2014 +0000

    arm64: compat: align cacheflush syscall with arch/arm
    
    Update handling of cacheflush syscall with changes made in arch/arm
    counterpart:
     - return error to userspace when flushing syscall fails
     - split user cache-flushing into interruptible chunks
     - don't bother rounding to nearest vma
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    [will: changed internal return value from -EINTR to 0 to match arch/arm/]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 689b6379188c..7ae31a2cc6c0 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -73,7 +73,7 @@ extern void flush_cache_all(void);
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
-extern void __flush_cache_user_range(unsigned long start, unsigned long end);
+extern long __flush_cache_user_range(unsigned long start, unsigned long end);
 
 static inline void flush_cache_mm(struct mm_struct *mm)
 {

commit 11d91a770f1fff44dafdf88d6089a3451f99c9b6
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Tue Aug 19 20:41:43 2014 +0100

    arm64: Add CONFIG_DEBUG_SET_MODULE_RONX support
    
    In a similar fashion to other architecture, add the infrastructure
    and Kconfig to enable DEBUG_SET_MODULE_RONX support. When
    enabled, module ranges will be marked read-only/no-execute as
    appropriate.
    
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    [will: fixed off-by-one in module end check]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index f2defe1c380c..689b6379188c 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -148,4 +148,8 @@ static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
 {
 }
 
+int set_memory_ro(unsigned long addr, int numpages);
+int set_memory_rw(unsigned long addr, int numpages);
+int set_memory_x(unsigned long addr, int numpages);
+int set_memory_nx(unsigned long addr, int numpages);
 #endif

commit 7f0b1bf04511348995d6fce38c87c98a3b5cb781
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jun 9 11:55:03 2014 +0100

    arm64: Fix barriers used for page table modifications
    
    The architecture specification states that both DSB and ISB are required
    between page table modifications and subsequent memory accesses using the
    corresponding virtual address. When TLB invalidation takes place, the
    tlb_flush_* functions already have the necessary barriers. However, there are
    other functions like create_mapping() for which this is not the case.
    
    The patch adds the DSB+ISB instructions in the set_pte() function for
    valid kernel mappings. The invalid pte case is handled by tlb_flush_*
    and the user mappings in general have a corresponding update_mmu_cache()
    call containing a DSB. Even when update_mmu_cache() isn't called, the
    kernel can still cope with an unlikely spurious page fault by
    re-executing the instruction.
    
    In addition, the set_pmd, set_pud() functions gain an ISB for
    architecture compliance when block mappings are created.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Leif Lindholm <leif.lindholm@linaro.org>
    Acked-by: Steve Capper <steve.capper@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index a5176cf32dad..f2defe1c380c 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -138,19 +138,10 @@ static inline void __flush_icache_all(void)
 #define flush_icache_page(vma,page)	do { } while (0)
 
 /*
- * flush_cache_vmap() is used when creating mappings (eg, via vmap,
- * vmalloc, ioremap etc) in kernel space for pages.  On non-VIPT
- * caches, since the direct-mappings of these pages may contain cached
- * data, we need to do a full cache flush to ensure that writebacks
- * don't corrupt data placed into these pages via the new mappings.
+ * Not required on AArch64 (PIPT or VIPT non-aliasing D-cache).
  */
 static inline void flush_cache_vmap(unsigned long start, unsigned long end)
 {
-	/*
-	 * set_pte_at() called from vmap_pte_range() does not
-	 * have a DSB after cleaning the cache line.
-	 */
-	dsb(ish);
 }
 
 static inline void flush_cache_vunmap(unsigned long start, unsigned long end)

commit 98f7685ee69f871ba991089cb9685f0da07517ea
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 2 16:24:10 2014 +0100

    arm64: barriers: make use of barrier options with explicit barriers
    
    When calling our low-level barrier macros directly, we can often suffice
    with more relaxed behaviour than the default "all accesses, full system"
    option.
    
    This patch updates the users of dsb() to specify the option which they
    actually require.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 4c60e64a801c..a5176cf32dad 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -123,7 +123,7 @@ extern void flush_dcache_page(struct page *);
 static inline void __flush_icache_all(void)
 {
 	asm("ic	ialluis");
-	dsb();
+	dsb(ish);
 }
 
 #define flush_dcache_mmap_lock(mapping) \
@@ -150,7 +150,7 @@ static inline void flush_cache_vmap(unsigned long start, unsigned long end)
 	 * set_pte_at() called from vmap_pte_range() does not
 	 * have a DSB after cleaning the cache line.
 	 */
-	dsb();
+	dsb(ish);
 }
 
 static inline void flush_cache_vunmap(unsigned long start, unsigned long end)

commit 7363590d2c4691593fd280f94b3deaeb5e83dbbd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue May 21 17:35:19 2013 +0100

    arm64: Implement coherent DMA API based on swiotlb
    
    This patch adds support for DMA API cache maintenance on SoCs without
    hardware device cache coherency.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 889324981aa4..4c60e64a801c 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -84,6 +84,13 @@ static inline void flush_cache_page(struct vm_area_struct *vma,
 {
 }
 
+/*
+ * Cache maintenance functions used by the DMA API. No to be used directly.
+ */
+extern void __dma_map_area(const void *, size_t, int);
+extern void __dma_unmap_area(const void *, size_t, int);
+extern void __dma_flush_range(const void *, const void *);
+
 /*
  * Copy user data from/to a page which is mapped into a different
  * processes address space.  Really, we want to allow our "user

commit 5044bad43ee573d0b6d90e3ccb7a40c2c7d25eb4
Author: Vinayak Kale <vkale@apm.com>
Date:   Wed Feb 5 09:34:36 2014 +0000

    arm64: add DSB after icache flush in __flush_icache_all()
    
    Add DSB after icache flush to complete the cache maintenance operation.
    The function __flush_icache_all() is used only for user space mappings
    and an ISB is not required because of an exception return before executing
    user instructions. An exception return would behave like an ISB.
    
    Signed-off-by: Vinayak Kale <vkale@apm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index fea9ee327206..889324981aa4 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -116,6 +116,7 @@ extern void flush_dcache_page(struct page *);
 static inline void __flush_icache_all(void)
 {
 	asm("ic	ialluis");
+	dsb();
 }
 
 #define flush_dcache_mmap_lock(mapping) \

commit ebd88367de80f9509bd30a09342d0a19c925b23e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed May 1 16:38:23 2013 +0100

    arm64: Remove __flush_dcache_page()
    
    This function is only used in __sync_icache_dcache(), so remove it and
    call __flush_dcache_area() directly. The flush_icache_user_range()
    function is not used in the arm64 kernel.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 3300cbd18a89..fea9ee327206 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -123,9 +123,6 @@ static inline void __flush_icache_all(void)
 #define flush_dcache_mmap_unlock(mapping) \
 	spin_unlock_irq(&(mapping)->tree_lock)
 
-#define flush_icache_user_range(vma,page,addr,len) \
-	flush_dcache_page(page)
-
 /*
  * We don't appear to need to do anything here.  In fact, if we did, we'd
  * duplicate cache flushing elsewhere performed by flush_dcache_page().

commit 8f3bfa584ed05e9e7d290707c48eee026fb94ece
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 23 18:15:32 2012 +0000

    arm64: Convert empty flush_cache_{mm,page} functions to static inline
    
    These functions are empty, just make them static inline in the header.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index aa3132ab7f29..3300cbd18a89 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -70,13 +70,20 @@
  *		- size   - region size
  */
 extern void flush_cache_all(void);
-extern void flush_cache_mm(struct mm_struct *mm);
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
-extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn);
 extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
 extern void __flush_cache_user_range(unsigned long start, unsigned long end);
 
+static inline void flush_cache_mm(struct mm_struct *mm)
+{
+}
+
+static inline void flush_cache_page(struct vm_area_struct *vma,
+				    unsigned long user_addr, unsigned long pfn)
+{
+}
+
 /*
  * Copy user data from/to a page which is mapped into a different
  * processes address space.  Really, we want to allow our "user

commit f1a0c4aa0937975b53991842a494f741d7769b02
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: Cache maintenance routines
    
    The patch adds functionality required for cache maintenance. The AArch64
    architecture mandates non-aliasing VIPT or PIPT D-cache and VIPT (may
    have aliases) or ASID-tagged VIVT I-cache. Cache maintenance operations
    are automatically broadcast in hardware between CPUs.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
new file mode 100644
index 000000000000..aa3132ab7f29
--- /dev/null
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -0,0 +1,148 @@
+/*
+ * Based on arch/arm/include/asm/cacheflush.h
+ *
+ * Copyright (C) 1999-2002 Russell King.
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_CACHEFLUSH_H
+#define __ASM_CACHEFLUSH_H
+
+#include <linux/mm.h>
+
+/*
+ * This flag is used to indicate that the page pointed to by a pte is clean
+ * and does not require cleaning before returning it to the user.
+ */
+#define PG_dcache_clean PG_arch_1
+
+/*
+ *	MM Cache Management
+ *	===================
+ *
+ *	The arch/arm64/mm/cache.S implements these methods.
+ *
+ *	Start addresses are inclusive and end addresses are exclusive; start
+ *	addresses should be rounded down, end addresses up.
+ *
+ *	See Documentation/cachetlb.txt for more information. Please note that
+ *	the implementation assumes non-aliasing VIPT D-cache and (aliasing)
+ *	VIPT or ASID-tagged VIVT I-cache.
+ *
+ *	flush_cache_all()
+ *
+ *		Unconditionally clean and invalidate the entire cache.
+ *
+ *	flush_cache_mm(mm)
+ *
+ *		Clean and invalidate all user space cache entries
+ *		before a change of page tables.
+ *
+ *	flush_icache_range(start, end)
+ *
+ *		Ensure coherency between the I-cache and the D-cache in the
+ *		region described by start, end.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	__flush_cache_user_range(start, end)
+ *
+ *		Ensure coherency between the I-cache and the D-cache in the
+ *		region described by start, end.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	__flush_dcache_area(kaddr, size)
+ *
+ *		Ensure that the data held in page is written back.
+ *		- kaddr  - page address
+ *		- size   - region size
+ */
+extern void flush_cache_all(void);
+extern void flush_cache_mm(struct mm_struct *mm);
+extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
+extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn);
+extern void flush_icache_range(unsigned long start, unsigned long end);
+extern void __flush_dcache_area(void *addr, size_t len);
+extern void __flush_cache_user_range(unsigned long start, unsigned long end);
+
+/*
+ * Copy user data from/to a page which is mapped into a different
+ * processes address space.  Really, we want to allow our "user
+ * space" model to handle this.
+ */
+extern void copy_to_user_page(struct vm_area_struct *, struct page *,
+	unsigned long, void *, const void *, unsigned long);
+#define copy_from_user_page(vma, page, vaddr, dst, src, len) \
+	do {							\
+		memcpy(dst, src, len);				\
+	} while (0)
+
+#define flush_cache_dup_mm(mm) flush_cache_mm(mm)
+
+/*
+ * flush_dcache_page is used when the kernel has written to the page
+ * cache page at virtual address page->virtual.
+ *
+ * If this page isn't mapped (ie, page_mapping == NULL), or it might
+ * have userspace mappings, then we _must_ always clean + invalidate
+ * the dcache entries associated with the kernel mapping.
+ *
+ * Otherwise we can defer the operation, and clean the cache when we are
+ * about to change to user space.  This is the same method as used on SPARC64.
+ * See update_mmu_cache for the user space part.
+ */
+#define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
+extern void flush_dcache_page(struct page *);
+
+static inline void __flush_icache_all(void)
+{
+	asm("ic	ialluis");
+}
+
+#define flush_dcache_mmap_lock(mapping) \
+	spin_lock_irq(&(mapping)->tree_lock)
+#define flush_dcache_mmap_unlock(mapping) \
+	spin_unlock_irq(&(mapping)->tree_lock)
+
+#define flush_icache_user_range(vma,page,addr,len) \
+	flush_dcache_page(page)
+
+/*
+ * We don't appear to need to do anything here.  In fact, if we did, we'd
+ * duplicate cache flushing elsewhere performed by flush_dcache_page().
+ */
+#define flush_icache_page(vma,page)	do { } while (0)
+
+/*
+ * flush_cache_vmap() is used when creating mappings (eg, via vmap,
+ * vmalloc, ioremap etc) in kernel space for pages.  On non-VIPT
+ * caches, since the direct-mappings of these pages may contain cached
+ * data, we need to do a full cache flush to ensure that writebacks
+ * don't corrupt data placed into these pages via the new mappings.
+ */
+static inline void flush_cache_vmap(unsigned long start, unsigned long end)
+{
+	/*
+	 * set_pte_at() called from vmap_pte_range() does not
+	 * have a DSB after cleaning the cache line.
+	 */
+	dsb();
+}
+
+static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
+{
+}
+
+#endif
