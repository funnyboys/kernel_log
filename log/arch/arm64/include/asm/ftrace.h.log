commit 3b23e4991fb66f6d152f9055ede271a726ef9f21
Author: Torsten Duwe <duwe@lst.de>
Date:   Fri Feb 8 16:10:19 2019 +0100

    arm64: implement ftrace with regs
    
    This patch implements FTRACE_WITH_REGS for arm64, which allows a traced
    function's arguments (and some other registers) to be captured into a
    struct pt_regs, allowing these to be inspected and/or modified. This is
    a building block for live-patching, where a function's arguments may be
    forwarded to another function. This is also necessary to enable ftrace
    and in-kernel pointer authentication at the same time, as it allows the
    LR value to be captured and adjusted prior to signing.
    
    Using GCC's -fpatchable-function-entry=N option, we can have the
    compiler insert a configurable number of NOPs between the function entry
    point and the usual prologue. This also ensures functions are AAPCS
    compliant (e.g. disabling inter-procedural register allocation).
    
    For example, with -fpatchable-function-entry=2, GCC 8.1.0 compiles the
    following:
    
    | unsigned long bar(void);
    |
    | unsigned long foo(void)
    | {
    |         return bar() + 1;
    | }
    
    ... to:
    
    | <foo>:
    |         nop
    |         nop
    |         stp     x29, x30, [sp, #-16]!
    |         mov     x29, sp
    |         bl      0 <bar>
    |         add     x0, x0, #0x1
    |         ldp     x29, x30, [sp], #16
    |         ret
    
    This patch builds the kernel with -fpatchable-function-entry=2,
    prefixing each function with two NOPs. To trace a function, we replace
    these NOPs with a sequence that saves the LR into a GPR, then calls an
    ftrace entry assembly function which saves this and other relevant
    registers:
    
    | mov   x9, x30
    | bl    <ftrace-entry>
    
    Since patchable functions are AAPCS compliant (and the kernel does not
    use x18 as a platform register), x9-x18 can be safely clobbered in the
    patched sequence and the ftrace entry code.
    
    There are now two ftrace entry functions, ftrace_regs_entry (which saves
    all GPRs), and ftrace_entry (which saves the bare minimum). A PLT is
    allocated for each within modules.
    
    Signed-off-by: Torsten Duwe <duwe@suse.de>
    [Mark: rework asm, comments, PLTs, initialization, commit message]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Torsten Duwe <duwe@suse.de>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Tested-by: Torsten Duwe <duwe@suse.de>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Julien Thierry <jthierry@redhat.com>
    Cc: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index d48667b04c41..91fa4baa1a93 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -11,9 +11,20 @@
 #include <asm/insn.h>
 
 #define HAVE_FUNCTION_GRAPH_FP_TEST
+
+#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS
+#define ARCH_SUPPORTS_FTRACE_OPS 1
+#else
 #define MCOUNT_ADDR		((unsigned long)_mcount)
+#endif
+
+/* The BL at the callsite's adjusted rec->ip */
 #define MCOUNT_INSN_SIZE	AARCH64_INSN_SIZE
 
+#define FTRACE_PLT_IDX		0
+#define FTRACE_REGS_PLT_IDX	1
+#define NR_FTRACE_PLTS		2
+
 /*
  * Currently, gcc tends to save the link register after the local variables
  * on the stack. This causes the max stack tracer to report the function
@@ -43,6 +54,12 @@ extern void return_to_handler(void);
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
+	/*
+	 * Adjust addr to point at the BL in the callsite.
+	 * See ftrace_init_nop() for the callsite sequence.
+	 */
+	if (IS_ENABLED(CONFIG_DYNAMIC_FTRACE_WITH_REGS))
+		return addr + AARCH64_INSN_SIZE;
 	/*
 	 * addr is the address of the mcount call instruction.
 	 * recordmcount does the necessary offset calculation.
@@ -50,6 +67,12 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 	return addr;
 }
 
+#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS
+struct dyn_ftrace;
+int ftrace_init_nop(struct module *mod, struct dyn_ftrace *rec);
+#define ftrace_init_nop ftrace_init_nop
+#endif
+
 #define ftrace_return_address(n) return_address(n)
 
 /*

commit f7edb451fa51e44e62177347ea7850aa0e901ea5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Aug 7 11:28:59 2019 -0400

    tracing/arm64: Have max stack tracer handle the case of return address after data
    
    Most archs (well at least x86) store the function call return address on the
    stack before storing the local variables for the function. The max stack
    tracer depends on this in its algorithm to display the stack size of each
    function it finds in the back trace.
    
    Some archs (arm64), may store the return address (from its link register)
    just before calling a nested function. There's no reason to save the link
    register on leaf functions, as it wont be updated. This breaks the algorithm
    of the max stack tracer.
    
    Add a new define ARCH_FTRACE_SHIFT_STACK_TRACER that an architecture may set
    if it stores the return address (link register) after it stores the
    function's local variables, and have the stack trace shift the values of the
    mapped stack size to the appropriate functions.
    
    Link: 20190802094103.163576-1-jiping.ma2@windriver.com
    
    Reported-by: Jiping Ma <jiping.ma2@windriver.com>
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index 5ab5200b2bdc..d48667b04c41 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -14,6 +14,19 @@
 #define MCOUNT_ADDR		((unsigned long)_mcount)
 #define MCOUNT_INSN_SIZE	AARCH64_INSN_SIZE
 
+/*
+ * Currently, gcc tends to save the link register after the local variables
+ * on the stack. This causes the max stack tracer to report the function
+ * frame sizes for the wrong functions. By defining
+ * ARCH_FTRACE_SHIFT_STACK_TRACER, it will tell the stack tracer to expect
+ * to find the return address on the stack after the local variables have
+ * been set up.
+ *
+ * Note, this may change in the future, and we will need to deal with that
+ * if it were to happen.
+ */
+#define ARCH_FTRACE_SHIFT_STACK_TRACER 1
+
 #ifndef __ASSEMBLY__
 #include <linux/compat.h>
 

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index 15a6587e12f9..5ab5200b2bdc 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -1,12 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * arch/arm64/include/asm/ftrace.h
  *
  * Copyright (C) 2013 Linaro Limited
  * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #ifndef __ASM_FTRACE_H
 #define __ASM_FTRACE_H

commit 5694cecdb092656a822287a6691aa7ce668c8160
Merge: 13e1ad2be3a8 12f799c8c739
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 25 17:41:56 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 festive updates from Will Deacon:
     "In the end, we ended up with quite a lot more than I expected:
    
       - Support for ARMv8.3 Pointer Authentication in userspace (CRIU and
         kernel-side support to come later)
    
       - Support for per-thread stack canaries, pending an update to GCC
         that is currently undergoing review
    
       - Support for kexec_file_load(), which permits secure boot of a kexec
         payload but also happens to improve the performance of kexec
         dramatically because we can avoid the sucky purgatory code from
         userspace. Kdump will come later (requires updates to libfdt).
    
       - Optimisation of our dynamic CPU feature framework, so that all
         detected features are enabled via a single stop_machine()
         invocation
    
       - KPTI whitelisting of Cortex-A CPUs unaffected by Meltdown, so that
         they can benefit from global TLB entries when KASLR is not in use
    
       - 52-bit virtual addressing for userspace (kernel remains 48-bit)
    
       - Patch in LSE atomics for per-cpu atomic operations
    
       - Custom preempt.h implementation to avoid unconditional calls to
         preempt_schedule() from preempt_enable()
    
       - Support for the new 'SB' Speculation Barrier instruction
    
       - Vectorised implementation of XOR checksumming and CRC32
         optimisations
    
       - Workaround for Cortex-A76 erratum #1165522
    
       - Improved compatibility with Clang/LLD
    
       - Support for TX2 system PMUS for profiling the L3 cache and DMC
    
       - Reflect read-only permissions in the linear map by default
    
       - Ensure MMIO reads are ordered with subsequent calls to Xdelay()
    
       - Initial support for memory hotplug
    
       - Tweak the threshold when we invalidate the TLB by-ASID, so that
         mremap() performance is improved for ranges spanning multiple PMDs.
    
       - Minor refactoring and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (125 commits)
      arm64: kaslr: print PHYS_OFFSET in dump_kernel_offset()
      arm64: sysreg: Use _BITUL() when defining register bits
      arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
      arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
      arm64: docs: document pointer authentication
      arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
      arm64: enable pointer authentication
      arm64: add prctl control for resetting ptrauth keys
      arm64: perf: strip PAC when unwinding userspace
      arm64: expose user PAC bit positions via ptrace
      arm64: add basic pointer authentication support
      arm64/cpufeature: detect pointer authentication
      arm64: Don't trap host pointer auth use to EL2
      arm64/kvm: hide ptrauth from guests
      arm64/kvm: consistently handle host HCR_EL2 flags
      arm64: add pointer authentication register bits
      arm64: add comments about EC exception levels
      arm64: perf: Treat EXCLUDE_EL* bit definitions as unsigned
      arm64: kpti: Whitelist Cortex-A CPUs that don't implement the CSV3 field
      arm64: enable per-task stack canaries
      ...

commit 5c176aff5b5a7027840c37da9d48a8f9cedb08b9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 15 22:42:00 2018 +0000

    arm64: ftrace: enable graph FP test
    
    The core frace code has an optional sanity check on the frame pointer
    passed by ftrace_graph_caller and return_to_handler. This is cheap,
    useful, and enabled unconditionally on x86, sparc, and riscv.
    
    Let's do the same on arm64, so that we can catch any problems early.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Torsten Duwe <duwe@suse.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index caa955f10e19..6795c147cbcc 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -13,6 +13,7 @@
 
 #include <asm/insn.h>
 
+#define HAVE_FUNCTION_GRAPH_FP_TEST
 #define MCOUNT_ADDR		((unsigned long)_mcount)
 #define MCOUNT_INSN_SIZE	AARCH64_INSN_SIZE
 

commit 874bfc6e5422d2421f7e4d5ea318d30e91679dfe
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Thu Nov 29 14:39:33 2018 +0900

    arm64: ftrace: Fix to enable syscall events on arm64
    
    Since commit 4378a7d4be30 ("arm64: implement syscall wrappers")
    introduced "__arm64_" prefix to all syscall wrapper symbols in
    sys_call_table, syscall tracer can not find corresponding
    metadata from syscall name. In the result, we have no syscall
    ftrace events on arm64 kernel, and some bpf testcases are failed
    on arm64.
    
    To fix this issue, this introduces custom
    arch_syscall_match_sym_name() which skips first 8 bytes when
    comparing the syscall and symbol names.
    
    Fixes: 4378a7d4be30 ("arm64: implement syscall wrappers")
    Reported-by: Naresh Kamboju <naresh.kamboju@linaro.org>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Naresh Kamboju <naresh.kamboju@linaro.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index caa955f10e19..fac54fb050d0 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -56,6 +56,19 @@ static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
 {
 	return is_compat_task();
 }
+
+#define ARCH_HAS_SYSCALL_MATCH_SYM_NAME
+
+static inline bool arch_syscall_match_sym_name(const char *sym,
+					       const char *name)
+{
+	/*
+	 * Since all syscall functions have __arm64_ prefix, we must skip it.
+	 * However, as we described above, we decided to ignore compat
+	 * syscalls, so we don't care about __arm64_compat_ prefix here.
+	 */
+	return !strcmp(sym + 8, name);
+}
 #endif /* ifndef __ASSEMBLY__ */
 
 #endif /* __ASM_FTRACE_H */

commit ef769e320863a186e489e3f66ed8df60487fe9bf
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Wed Feb 24 09:52:41 2016 -0800

    arm64: Fix misspellings in comments.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index 3c60f37e48ab..caa955f10e19 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -48,7 +48,7 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
  * See kernel/trace/trace_syscalls.c
  *
  * x86 code says:
- * If the user realy wants these, then they should use the
+ * If the user really wants these, then they should use the
  * raw syscall tracepoints with filtering.
  */
 #define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS

commit 20380bb390a443b2c5c8800cec59743faf8151b4
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:41 2015 +0900

    arm64: ftrace: fix a stack tracer's output under function graph tracer
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function return. This will result in many useless entries
    (return_to_handler) showing up in
     a) a stack tracer's output
     b) perf call graph (with perf record -g)
     c) dump_backtrace (at panic et al.)
    
    For example, in case of a),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo 1 > /proc/sys/kernel/stack_trace_enabled
      $ cat /sys/kernel/debug/tracing/stack_trace
            Depth    Size   Location    (54 entries)
            -----    ----   --------
      0)     4504      16   gic_raise_softirq+0x28/0x150
      1)     4488      80   smp_cross_call+0x38/0xb8
      2)     4408      48   return_to_handler+0x0/0x40
      3)     4360      32   return_to_handler+0x0/0x40
      ...
    
    In case of b),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ perf record -e mem:XXX:x -ag -- sleep 10
      $ perf report
                      ...
                      |          |          |--0.22%-- 0x550f8
                      |          |          |          0x10888
                      |          |          |          el0_svc_naked
                      |          |          |          sys_openat
                      |          |          |          return_to_handler
                      |          |          |          return_to_handler
                      ...
    
    In case of c),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo c > /proc/sysrq-trigger
      ...
      Call trace:
      [<ffffffc00044d3ac>] sysrq_handle_crash+0x24/0x30
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      ...
    
    This patch replaces such entries with real addresses preserved in
    current->ret_stack[] at unwind_frame(). This way, we can cover all
    the cases.
    
    Reviewed-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [will: fixed minor context changes conflicting with irq stack bits]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index c5534facf941..3c60f37e48ab 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -28,6 +28,8 @@ struct dyn_arch_ftrace {
 
 extern unsigned long ftrace_graph_call;
 
+extern void return_to_handler(void);
+
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
 	/*

commit 055b1212d141f1f398fca548f8147787c0b6253f
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:54:36 2014 +0100

    arm64: ftrace: Add system call tracepoint
    
    This patch allows system call entry or exit to be traced as ftrace events,
    ie. sys_enter_*/sys_exit_*, if CONFIG_FTRACE_SYSCALLS is enabled.
    Those events appear and can be controlled under
        ${sysfs}/tracing/events/syscalls/
    
    Please note that we can't trace compat system calls here because
    AArch32 mode does not share the same syscall table with AArch64.
    Just define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS in order to avoid unexpected
    results (bogus syscalls reported or even hang-up).
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index 41e8670db20f..c5534facf941 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -17,6 +17,8 @@
 #define MCOUNT_INSN_SIZE	AARCH64_INSN_SIZE
 
 #ifndef __ASSEMBLY__
+#include <linux/compat.h>
+
 extern void _mcount(unsigned long);
 extern void *return_address(unsigned int);
 
@@ -36,6 +38,22 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 }
 
 #define ftrace_return_address(n) return_address(n)
+
+/*
+ * Because AArch32 mode does not share the same syscall table with AArch64,
+ * tracing compat syscalls may result in reporting bogus syscalls or even
+ * hang-up, so just do not trace them.
+ * See kernel/trace/trace_syscalls.c
+ *
+ * x86 code says:
+ * If the user realy wants these, then they should use the
+ * raw syscall tracepoints with filtering.
+ */
+#define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS
+static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
+{
+	return is_compat_task();
+}
 #endif /* ifndef __ASSEMBLY__ */
 
 #endif /* __ASM_FTRACE_H */

commit 3711784ece66d39352a0dbb6da40e097a77da057
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:54:35 2014 +0100

    arm64: ftrace: Add CALLER_ADDRx macros
    
    CALLER_ADDRx returns caller's address at specified level in call stacks.
    They are used for several tracers like irqsoff and preemptoff.
    Strange to say, however, they are refered even without FTRACE.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index ed5c448ece99..41e8670db20f 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -18,6 +18,7 @@
 
 #ifndef __ASSEMBLY__
 extern void _mcount(unsigned long);
+extern void *return_address(unsigned int);
 
 struct dyn_arch_ftrace {
 	/* No extra data needed for arm64 */
@@ -33,6 +34,8 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 	 */
 	return addr;
 }
-#endif /* __ASSEMBLY__ */
+
+#define ftrace_return_address(n) return_address(n)
+#endif /* ifndef __ASSEMBLY__ */
 
 #endif /* __ASM_FTRACE_H */

commit bd7d38dbdf356e75eb3b1699158c9b8021fd6784
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:54:34 2014 +0100

    arm64: ftrace: Add dynamic ftrace support
    
    This patch allows "dynamic ftrace" if CONFIG_DYNAMIC_FTRACE is enabled.
    Here we can turn on and off tracing dynamically per-function base.
    
    On arm64, this is done by patching single branch instruction to _mcount()
    inserted by gcc -pg option. The branch is replaced to NOP initially at
    kernel start up, and later on, NOP to branch to ftrace_caller() when
    enabled or branch to NOP when disabled.
    Please note that ftrace_caller() is a counterpart of _mcount() in case of
    'static' ftrace.
    
    More details on architecture specific requirements are described in
    Documentation/trace/ftrace-design.txt.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
index 58ea5951b198..ed5c448ece99 100644
--- a/arch/arm64/include/asm/ftrace.h
+++ b/arch/arm64/include/asm/ftrace.h
@@ -18,6 +18,21 @@
 
 #ifndef __ASSEMBLY__
 extern void _mcount(unsigned long);
+
+struct dyn_arch_ftrace {
+	/* No extra data needed for arm64 */
+};
+
+extern unsigned long ftrace_graph_call;
+
+static inline unsigned long ftrace_call_adjust(unsigned long addr)
+{
+	/*
+	 * addr is the address of the mcount call instruction.
+	 * recordmcount does the necessary offset calculation.
+	 */
+	return addr;
+}
 #endif /* __ASSEMBLY__ */
 
 #endif /* __ASM_FTRACE_H */

commit 819e50e25d0ce8a75f5cba815416a6a8573655c4
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 18:54:33 2014 +0900

    arm64: Add ftrace support
    
    This patch implements arm64 specific part to support function tracers,
    such as function (CONFIG_FUNCTION_TRACER), function_graph
    (CONFIG_FUNCTION_GRAPH_TRACER) and function profiler
    (CONFIG_FUNCTION_PROFILER).
    
    With 'function' tracer, all the functions in the kernel are traced with
    timestamps in ${sysfs}/tracing/trace. If function_graph tracer is
    specified, call graph is generated.
    
    The kernel must be compiled with -pg option so that _mcount() is inserted
    at the beginning of functions. This function is called on every function's
    entry as long as tracing is enabled.
    In addition, function_graph tracer also needs to be able to probe function's
    exit. ftrace_graph_caller() & return_to_handler do this by faking link
    register's value to intercept function's return path.
    
    More details on architecture specific requirements are described in
    Documentation/trace/ftrace-design.txt.
    
    Reviewed-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/ftrace.h b/arch/arm64/include/asm/ftrace.h
new file mode 100644
index 000000000000..58ea5951b198
--- /dev/null
+++ b/arch/arm64/include/asm/ftrace.h
@@ -0,0 +1,23 @@
+/*
+ * arch/arm64/include/asm/ftrace.h
+ *
+ * Copyright (C) 2013 Linaro Limited
+ * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __ASM_FTRACE_H
+#define __ASM_FTRACE_H
+
+#include <asm/insn.h>
+
+#define MCOUNT_ADDR		((unsigned long)_mcount)
+#define MCOUNT_INSN_SIZE	AARCH64_INSN_SIZE
+
+#ifndef __ASSEMBLY__
+extern void _mcount(unsigned long);
+#endif /* __ASSEMBLY__ */
+
+#endif /* __ASM_FTRACE_H */
