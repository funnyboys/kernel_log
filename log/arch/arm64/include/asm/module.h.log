commit 62d0fd591db1f9dcf68fb963b3a94af085a6b166
Author: Masahiro Yamada <masahiroy@kernel.org>
Date:   Wed Apr 22 01:13:55 2020 +0900

    arch: split MODULE_ARCH_VERMAGIC definitions out to <asm/vermagic.h>
    
    As the bug report [1] pointed out, <linux/vermagic.h> must be included
    after <linux/module.h>.
    
    I believe we should not impose any include order restriction. We often
    sort include directives alphabetically, but it is just coding style
    convention. Technically, we can include header files in any order by
    making every header self-contained.
    
    Currently, arch-specific MODULE_ARCH_VERMAGIC is defined in
    <asm/module.h>, which is not included from <linux/vermagic.h>.
    
    Hence, the straight-forward fix-up would be as follows:
    
    |--- a/include/linux/vermagic.h
    |+++ b/include/linux/vermagic.h
    |@@ -1,5 +1,6 @@
    | /* SPDX-License-Identifier: GPL-2.0 */
    | #include <generated/utsrelease.h>
    |+#include <linux/module.h>
    |
    | /* Simply sanity version stamp for modules. */
    | #ifdef CONFIG_SMP
    
    This works enough, but for further cleanups, I split MODULE_ARCH_VERMAGIC
    definitions into <asm/vermagic.h>.
    
    With this, <linux/module.h> and <linux/vermagic.h> will be orthogonal,
    and the location of MODULE_ARCH_VERMAGIC definitions will be consistent.
    
    For arc and ia64, MODULE_PROC_FAMILY is only used for defining
    MODULE_ARCH_VERMAGIC. I squashed it.
    
    For hexagon, nds32, and xtensa, I removed <asm/modules.h> entirely
    because they contained nothing but MODULE_ARCH_VERMAGIC definition.
    Kbuild will automatically generate <asm/modules.h> at build-time,
    wrapping <asm-generic/module.h>.
    
    [1] https://lore.kernel.org/lkml/20200411155623.GA22175@zn.tnic
    
    Reported-by: Borislav Petkov <bp@suse.de>
    Signed-off-by: Masahiro Yamada <masahiroy@kernel.org>
    Acked-by: Jessica Yu <jeyu@kernel.org>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 1e93de68c044..4e7fa2623896 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -7,8 +7,6 @@
 
 #include <asm-generic/module.h>
 
-#define MODULE_ARCH_VERMAGIC	"aarch64"
-
 #ifdef CONFIG_ARM64_MODULE_PLTS
 struct mod_plt_sec {
 	int			plt_shndx;

commit 3b23e4991fb66f6d152f9055ede271a726ef9f21
Author: Torsten Duwe <duwe@lst.de>
Date:   Fri Feb 8 16:10:19 2019 +0100

    arm64: implement ftrace with regs
    
    This patch implements FTRACE_WITH_REGS for arm64, which allows a traced
    function's arguments (and some other registers) to be captured into a
    struct pt_regs, allowing these to be inspected and/or modified. This is
    a building block for live-patching, where a function's arguments may be
    forwarded to another function. This is also necessary to enable ftrace
    and in-kernel pointer authentication at the same time, as it allows the
    LR value to be captured and adjusted prior to signing.
    
    Using GCC's -fpatchable-function-entry=N option, we can have the
    compiler insert a configurable number of NOPs between the function entry
    point and the usual prologue. This also ensures functions are AAPCS
    compliant (e.g. disabling inter-procedural register allocation).
    
    For example, with -fpatchable-function-entry=2, GCC 8.1.0 compiles the
    following:
    
    | unsigned long bar(void);
    |
    | unsigned long foo(void)
    | {
    |         return bar() + 1;
    | }
    
    ... to:
    
    | <foo>:
    |         nop
    |         nop
    |         stp     x29, x30, [sp, #-16]!
    |         mov     x29, sp
    |         bl      0 <bar>
    |         add     x0, x0, #0x1
    |         ldp     x29, x30, [sp], #16
    |         ret
    
    This patch builds the kernel with -fpatchable-function-entry=2,
    prefixing each function with two NOPs. To trace a function, we replace
    these NOPs with a sequence that saves the LR into a GPR, then calls an
    ftrace entry assembly function which saves this and other relevant
    registers:
    
    | mov   x9, x30
    | bl    <ftrace-entry>
    
    Since patchable functions are AAPCS compliant (and the kernel does not
    use x18 as a platform register), x9-x18 can be safely clobbered in the
    patched sequence and the ftrace entry code.
    
    There are now two ftrace entry functions, ftrace_regs_entry (which saves
    all GPRs), and ftrace_entry (which saves the bare minimum). A PLT is
    allocated for each within modules.
    
    Signed-off-by: Torsten Duwe <duwe@suse.de>
    [Mark: rework asm, comments, PLTs, initialization, commit message]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Torsten Duwe <duwe@suse.de>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Tested-by: Torsten Duwe <duwe@suse.de>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Julien Thierry <jthierry@redhat.com>
    Cc: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index f80e13cbf8ec..1e93de68c044 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -21,7 +21,7 @@ struct mod_arch_specific {
 	struct mod_plt_sec	init;
 
 	/* for CONFIG_DYNAMIC_FTRACE */
-	struct plt_entry 	*ftrace_trampoline;
+	struct plt_entry	*ftrace_trampolines;
 };
 #endif
 

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index cd9f4e9d04d3..f80e13cbf8ec 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_MODULE_H
 #define __ASM_MODULE_H

commit 5a3ae7b314a2259b1188b22b392f5eba01e443ee
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sun Apr 7 21:06:16 2019 +0200

    arm64/ftrace: fix inadvertent BUG() in trampoline check
    
    The ftrace trampoline code (which deals with modules loaded out of
    BL range of the core kernel) uses plt_entries_equal() to check whether
    the per-module trampoline equals a zero buffer, to decide whether the
    trampoline has already been initialized.
    
    This triggers a BUG() in the opcode manipulation code, since we end
    up checking the ADRP offset of a 0x0 opcode, which is not an ADRP
    instruction.
    
    So instead, add a helper to check whether a PLT is initialized, and
    call that from the frace code.
    
    Cc: <stable@vger.kernel.org> # v5.0
    Fixes: bdb85cd1d206 ("arm64/module: switch to ADRP/ADD sequences for PLT entries")
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 905e1bb0e7bd..cd9f4e9d04d3 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -73,4 +73,9 @@ static inline bool is_forbidden_offset_for_adrp(void *place)
 struct plt_entry get_plt_entry(u64 dst, void *pc);
 bool plt_entries_equal(const struct plt_entry *a, const struct plt_entry *b);
 
+static inline bool plt_entry_is_initialized(const struct plt_entry *e)
+{
+	return e->adrp || e->add || e->br;
+}
+
 #endif /* __ASM_MODULE_H */

commit bdb85cd1d20669dfae813555dddb745ad09323ba
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Nov 22 09:46:46 2018 +0100

    arm64/module: switch to ADRP/ADD sequences for PLT entries
    
    Now that we have switched to the small code model entirely, and
    reduced the extended KASLR range to 4 GB, we can be sure that the
    targets of relative branches that are out of range are in range
    for a ADRP/ADD pair, which is one instruction shorter than our
    current MOVN/MOVK/MOVK sequence, and is more idiomatic and so it
    is more likely to be implemented efficiently by micro-architectures.
    
    So switch over the ordinary PLT code and the special handling of
    the Cortex-A53 ADRP errata, as well as the ftrace trampline
    handling.
    
    Reviewed-by: Torsten Duwe <duwe@lst.de>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [will: Added a couple of comments in the plt equality check]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index f81c1847312d..905e1bb0e7bd 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -58,39 +58,19 @@ struct plt_entry {
 	 * is exactly what we are dealing with here, we are free to use x16
 	 * as a scratch register in the PLT veneers.
 	 */
-	__le32	mov0;	/* movn	x16, #0x....			*/
-	__le32	mov1;	/* movk	x16, #0x...., lsl #16		*/
-	__le32	mov2;	/* movk	x16, #0x...., lsl #32		*/
+	__le32	adrp;	/* adrp	x16, ....			*/
+	__le32	add;	/* add	x16, x16, #0x....		*/
 	__le32	br;	/* br	x16				*/
 };
 
-static inline struct plt_entry get_plt_entry(u64 val)
+static inline bool is_forbidden_offset_for_adrp(void *place)
 {
-	/*
-	 * MOVK/MOVN/MOVZ opcode:
-	 * +--------+------------+--------+-----------+-------------+---------+
-	 * | sf[31] | opc[30:29] | 100101 | hw[22:21] | imm16[20:5] | Rd[4:0] |
-	 * +--------+------------+--------+-----------+-------------+---------+
-	 *
-	 * Rd     := 0x10 (x16)
-	 * hw     := 0b00 (no shift), 0b01 (lsl #16), 0b10 (lsl #32)
-	 * opc    := 0b11 (MOVK), 0b00 (MOVN), 0b10 (MOVZ)
-	 * sf     := 1 (64-bit variant)
-	 */
-	return (struct plt_entry){
-		cpu_to_le32(0x92800010 | (((~val      ) & 0xffff)) << 5),
-		cpu_to_le32(0xf2a00010 | ((( val >> 16) & 0xffff)) << 5),
-		cpu_to_le32(0xf2c00010 | ((( val >> 32) & 0xffff)) << 5),
-		cpu_to_le32(0xd61f0200)
-	};
+	return IS_ENABLED(CONFIG_ARM64_ERRATUM_843419) &&
+	       cpus_have_const_cap(ARM64_WORKAROUND_843419) &&
+	       ((u64)place & 0xfff) >= 0xff8;
 }
 
-static inline bool plt_entries_equal(const struct plt_entry *a,
-				     const struct plt_entry *b)
-{
-	return a->mov0 == b->mov0 &&
-	       a->mov1 == b->mov1 &&
-	       a->mov2 == b->mov2;
-}
+struct plt_entry get_plt_entry(u64 dst, void *pc);
+bool plt_entries_equal(const struct plt_entry *a, const struct plt_entry *b);
 
 #endif /* __ASM_MODULE_H */

commit c8ebf64eab743130fe404dc6679c2ff0cbc01615
Author: Jessica Yu <jeyu@kernel.org>
Date:   Mon Nov 5 19:53:23 2018 +0100

    arm64/module: use plt section indices for relocations
    
    Instead of saving a pointer to the .plt and .init.plt sections to apply
    plt-based relocations, save and use their section indices instead.
    
    The mod->arch.{core,init}.plt pointers were problematic for livepatch
    because they pointed within temporary section headers (provided by the
    module loader via info->sechdrs) that would be freed after module load.
    Since livepatch modules may need to apply relocations post-module-load
    (for example, to patch a module that is loaded later), using section
    indices to offset into the section headers (instead of accessing them
    through a saved pointer) allows livepatch modules on arm64 to pass in
    their own copy of the section headers to apply_relocate_add() to apply
    delayed relocations.
    
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Miroslav Benes <mbenes@suse.cz>
    Signed-off-by: Jessica Yu <jeyu@kernel.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 97d0ef12e2ff..f81c1847312d 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -22,7 +22,7 @@
 
 #ifdef CONFIG_ARM64_MODULE_PLTS
 struct mod_plt_sec {
-	struct elf64_shdr	*plt;
+	int			plt_shndx;
 	int			plt_num_entries;
 	int			plt_max_entries;
 };
@@ -36,10 +36,12 @@ struct mod_arch_specific {
 };
 #endif
 
-u64 module_emit_plt_entry(struct module *mod, void *loc, const Elf64_Rela *rela,
+u64 module_emit_plt_entry(struct module *mod, Elf64_Shdr *sechdrs,
+			  void *loc, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
-u64 module_emit_veneer_for_adrp(struct module *mod, void *loc, u64 val);
+u64 module_emit_veneer_for_adrp(struct module *mod, Elf64_Shdr *sechdrs,
+				void *loc, u64 val);
 
 #ifdef CONFIG_RANDOMIZE_BASE
 extern u64 module_alloc_base;

commit ed231ae384fdfcb546b63b2fe7add65029e3a94c
Author: Kim Phillips <kim.phillips@arm.com>
Date:   Tue Apr 24 10:39:43 2018 -0500

    arm64/kernel: rename module_emit_adrp_veneer->module_emit_veneer_for_adrp
    
    Commit a257e02579e ("arm64/kernel: don't ban ADRP to work around
    Cortex-A53 erratum #843419") introduced a function whose name ends with
    "_veneer".
    
    This clashes with commit bd8b22d2888e ("Kbuild: kallsyms: ignore veneers
    emitted by the ARM linker"), which removes symbols ending in "_veneer"
    from kallsyms.
    
    The problem was manifested as 'perf test -vvvvv vmlinux' failed,
    correctly claiming the symbol 'module_emit_adrp_veneer' was present in
    vmlinux, but not in kallsyms.
    
    ...
        ERR : 0xffff00000809aa58: module_emit_adrp_veneer not on kallsyms
    ...
        test child finished with -1
        ---- end ----
        vmlinux symtab matches kallsyms: FAILED!
    
    Fix the problem by renaming module_emit_adrp_veneer to
    module_emit_veneer_for_adrp.  Now the test passes.
    
    Fixes: a257e02579e ("arm64/kernel: don't ban ADRP to work around Cortex-A53 erratum #843419")
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Michal Marek <mmarek@suse.cz>
    Signed-off-by: Kim Phillips <kim.phillips@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index b6dbbe3123a9..97d0ef12e2ff 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -39,7 +39,7 @@ struct mod_arch_specific {
 u64 module_emit_plt_entry(struct module *mod, void *loc, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
-u64 module_emit_adrp_veneer(struct module *mod, void *loc, u64 val);
+u64 module_emit_veneer_for_adrp(struct module *mod, void *loc, u64 val);
 
 #ifdef CONFIG_RANDOMIZE_BASE
 extern u64 module_alloc_base;

commit a257e02579e42703de1b7825cbd56cd7191f97b0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 6 17:15:33 2018 +0000

    arm64/kernel: don't ban ADRP to work around Cortex-A53 erratum #843419
    
    Working around Cortex-A53 erratum #843419 involves special handling of
    ADRP instructions that end up in the last two instruction slots of a
    4k page, or whose output register gets overwritten without having been
    read. (Note that the latter instruction sequence is never emitted by
    a properly functioning compiler, which is why it is disregarded by the
    handling of the same erratum in the bfd.ld linker which we rely on for
    the core kernel)
    
    Normally, this gets taken care of by the linker, which can spot such
    sequences at final link time, and insert a veneer if the ADRP ends up
    at a vulnerable offset. However, linux kernel modules are partially
    linked ELF objects, and so there is no 'final link time' other than the
    runtime loading of the module, at which time all the static relocations
    are resolved.
    
    For this reason, we have implemented the #843419 workaround for modules
    by avoiding ADRP instructions altogether, by using the large C model,
    and by passing -mpc-relative-literal-loads to recent versions of GCC
    that may emit adrp/ldr pairs to perform literal loads. However, this
    workaround forces us to keep literal data mixed with the instructions
    in the executable .text segment, and literal data may inadvertently
    turn into an exploitable speculative gadget depending on the relative
    offsets of arbitrary symbols.
    
    So let's reimplement this workaround in a way that allows us to switch
    back to the small C model, and to drop the -mpc-relative-literal-loads
    GCC switch, by patching affected ADRP instructions at runtime:
    - ADRP instructions that do not appear at 4k relative offset 0xff8 or
      0xffc are ignored
    - ADRP instructions that are within 1 MB of their target symbol are
      converted into ADR instructions
    - remaining ADRP instructions are redirected via a veneer that performs
      the load using an unaffected movn/movk sequence.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [will: tidied up ADRP -> ADR instruction patching.]
    [will: use ULL suffix for 64-bit immediate]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 4f766178fa6f..b6dbbe3123a9 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -39,6 +39,8 @@ struct mod_arch_specific {
 u64 module_emit_plt_entry(struct module *mod, void *loc, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
+u64 module_emit_adrp_veneer(struct module *mod, void *loc, u64 val);
+
 #ifdef CONFIG_RANDOMIZE_BASE
 extern u64 module_alloc_base;
 #else

commit be0f272bfc83797f70d44faca86954df62e2bbc0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Nov 20 17:41:30 2017 +0000

    arm64: ftrace: emit ftrace-mod.o contents through code
    
    When building the arm64 kernel with both CONFIG_ARM64_MODULE_PLTS and
    CONFIG_DYNAMIC_FTRACE enabled, the ftrace-mod.o object file is built
    with the kernel and contains a trampoline that is linked into each
    module, so that modules can be loaded far away from the kernel and
    still reach the ftrace entry point in the core kernel with an ordinary
    relative branch, as is emitted by the compiler instrumentation code
    dynamic ftrace relies on.
    
    In order to be able to build out of tree modules, this object file
    needs to be included into the linux-headers or linux-devel packages,
    which is undesirable, as it makes arm64 a special case (although a
    precedent does exist for 32-bit PPC).
    
    Given that the trampoline essentially consists of a PLT entry, let's
    not bother with a source or object file for it, and simply patch it
    in whenever the trampoline is being populated, using the existing
    PLT support routines.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 11d4aaee82e1..4f766178fa6f 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -32,7 +32,7 @@ struct mod_arch_specific {
 	struct mod_plt_sec	init;
 
 	/* for CONFIG_DYNAMIC_FTRACE */
-	void			*ftrace_trampoline;
+	struct plt_entry 	*ftrace_trampoline;
 };
 #endif
 

commit 7e8b9c1d2e2f5f45db7d40b50d14f606097c25de
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Nov 20 17:41:29 2017 +0000

    arm64: module-plts: factor out PLT generation code for ftrace
    
    To allow the ftrace trampoline code to reuse the PLT entry routines,
    factor it out and move it into asm/module.h.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 19bd97671bb8..11d4aaee82e1 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -45,4 +45,48 @@ extern u64 module_alloc_base;
 #define module_alloc_base	((u64)_etext - MODULES_VSIZE)
 #endif
 
+struct plt_entry {
+	/*
+	 * A program that conforms to the AArch64 Procedure Call Standard
+	 * (AAPCS64) must assume that a veneer that alters IP0 (x16) and/or
+	 * IP1 (x17) may be inserted at any branch instruction that is
+	 * exposed to a relocation that supports long branches. Since that
+	 * is exactly what we are dealing with here, we are free to use x16
+	 * as a scratch register in the PLT veneers.
+	 */
+	__le32	mov0;	/* movn	x16, #0x....			*/
+	__le32	mov1;	/* movk	x16, #0x...., lsl #16		*/
+	__le32	mov2;	/* movk	x16, #0x...., lsl #32		*/
+	__le32	br;	/* br	x16				*/
+};
+
+static inline struct plt_entry get_plt_entry(u64 val)
+{
+	/*
+	 * MOVK/MOVN/MOVZ opcode:
+	 * +--------+------------+--------+-----------+-------------+---------+
+	 * | sf[31] | opc[30:29] | 100101 | hw[22:21] | imm16[20:5] | Rd[4:0] |
+	 * +--------+------------+--------+-----------+-------------+---------+
+	 *
+	 * Rd     := 0x10 (x16)
+	 * hw     := 0b00 (no shift), 0b01 (lsl #16), 0b10 (lsl #32)
+	 * opc    := 0b11 (MOVK), 0b00 (MOVN), 0b10 (MOVZ)
+	 * sf     := 1 (64-bit variant)
+	 */
+	return (struct plt_entry){
+		cpu_to_le32(0x92800010 | (((~val      ) & 0xffff)) << 5),
+		cpu_to_le32(0xf2a00010 | ((( val >> 16) & 0xffff)) << 5),
+		cpu_to_le32(0xf2c00010 | ((( val >> 32) & 0xffff)) << 5),
+		cpu_to_le32(0xd61f0200)
+	};
+}
+
+static inline bool plt_entries_equal(const struct plt_entry *a,
+				     const struct plt_entry *b)
+{
+	return a->mov0 == b->mov0 &&
+	       a->mov1 == b->mov1 &&
+	       a->mov2 == b->mov2;
+}
+
 #endif /* __ASM_MODULE_H */

commit e71a4e1bebaf7fd990efbdc04b38e5526914f0f1
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Jun 6 17:00:22 2017 +0000

    arm64: ftrace: add support for far branches to dynamic ftrace
    
    Currently, dynamic ftrace support in the arm64 kernel assumes that all
    core kernel code is within range of ordinary branch instructions that
    occur in module code, which is usually the case, but is no longer
    guaranteed now that we have support for module PLTs and address space
    randomization.
    
    Since on arm64, all patching of branch instructions involves function
    calls to the same entry point [ftrace_caller()], we can emit the modules
    with a trampoline that has unlimited range, and patch both the trampoline
    itself and the branch instruction to redirect the call via the trampoline.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [will: minor clarification to smp_wmb() comment]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index d57693f5d4ec..19bd97671bb8 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -30,6 +30,9 @@ struct mod_plt_sec {
 struct mod_arch_specific {
 	struct mod_plt_sec	core;
 	struct mod_plt_sec	init;
+
+	/* for CONFIG_DYNAMIC_FTRACE */
+	void			*ftrace_trampoline;
 };
 #endif
 

commit 24af6c4e4e0f6e9803bec8dca0f7748afbb2bbf0
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 21 22:12:57 2017 +0000

    arm64: module: split core and init PLT sections
    
    The arm64 module PLT code allocates all PLT entries in a single core
    section, since the overhead of having a separate init PLT section is
    not justified by the small number of PLT entries usually required for
    init code.
    
    However, the core and init module regions are allocated independently,
    and there is a corner case where the core region may be allocated from
    the VMALLOC region if the dedicated module region is exhausted, but the
    init region, being much smaller, can still be allocated from the module
    region. This leads to relocation failures if the distance between those
    regions exceeds 128 MB. (In fact, this corner case is highly unlikely to
    occur on arm64, but the issue has been observed on ARM, whose module
    region is much smaller).
    
    So split the core and init PLT regions, and name the latter ".init.plt"
    so it gets allocated along with (and sufficiently close to) the .init
    sections that it serves. Also, given that init PLT entries may need to
    be emitted for branches that target the core module, modify the logic
    that disregards defined symbols to only disregard symbols that are
    defined in the same section as the relocated branch instruction.
    
    Since there may now be two PLT entries associated with each entry in
    the symbol table, we can no longer hijack the symbol::st_size fields
    to record the addresses of PLT entries as we emit them for zero-addend
    relocations. So instead, perform an explicit comparison to check for
    duplicate entries.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index e12af6754634..d57693f5d4ec 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -21,14 +21,19 @@
 #define MODULE_ARCH_VERMAGIC	"aarch64"
 
 #ifdef CONFIG_ARM64_MODULE_PLTS
-struct mod_arch_specific {
+struct mod_plt_sec {
 	struct elf64_shdr	*plt;
 	int			plt_num_entries;
 	int			plt_max_entries;
 };
+
+struct mod_arch_specific {
+	struct mod_plt_sec	core;
+	struct mod_plt_sec	init;
+};
 #endif
 
-u64 module_emit_plt_entry(struct module *mod, const Elf64_Rela *rela,
+u64 module_emit_plt_entry(struct module *mod, void *loc, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
 #ifdef CONFIG_RANDOMIZE_BASE

commit 3b3c6c24de7f2de213e5e1e04a0c4b9082d61650
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Feb 6 15:02:42 2017 +0000

    arm64: Revert "arm64: kaslr: fix breakage with CONFIG_MODVERSIONS=y"
    
    This reverts commit 9c0e83c371cf4696926c95f9c8c77cd6ea803426, which
    is no longer needed now that the modversions code plays nice with
    relocatable PIE kernels.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 06ff7fd9e81f..e12af6754634 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -17,7 +17,6 @@
 #define __ASM_MODULE_H
 
 #include <asm-generic/module.h>
-#include <asm/memory.h>
 
 #define MODULE_ARCH_VERMAGIC	"aarch64"
 
@@ -33,10 +32,6 @@ u64 module_emit_plt_entry(struct module *mod, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
 #ifdef CONFIG_RANDOMIZE_BASE
-#ifdef CONFIG_MODVERSIONS
-#define ARCH_RELOCATES_KCRCTAB
-#define reloc_start 		(kimage_vaddr - KIMAGE_VADDR)
-#endif
 extern u64 module_alloc_base;
 #else
 #define module_alloc_base	((u64)_etext - MODULES_VSIZE)

commit 9c0e83c371cf4696926c95f9c8c77cd6ea803426
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Oct 13 17:42:09 2016 +0100

    arm64: kaslr: fix breakage with CONFIG_MODVERSIONS=y
    
    As it turns out, the KASLR code breaks CONFIG_MODVERSIONS, since the
    kcrctab has an absolute address field that is relocated at runtime
    when the kernel offset is randomized.
    
    This has been fixed already for PowerPC in the past, so simply wire up
    the existing code dealing with this issue.
    
    Cc: <stable@vger.kernel.org>
    Fixes: f80fb3a3d508 ("arm64: add support for kernel ASLR")
    Tested-by: Timur Tabi <timur@codeaurora.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index e12af6754634..06ff7fd9e81f 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -17,6 +17,7 @@
 #define __ASM_MODULE_H
 
 #include <asm-generic/module.h>
+#include <asm/memory.h>
 
 #define MODULE_ARCH_VERMAGIC	"aarch64"
 
@@ -32,6 +33,10 @@ u64 module_emit_plt_entry(struct module *mod, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
 #ifdef CONFIG_RANDOMIZE_BASE
+#ifdef CONFIG_MODVERSIONS
+#define ARCH_RELOCATES_KCRCTAB
+#define reloc_start 		(kimage_vaddr - KIMAGE_VADDR)
+#endif
 extern u64 module_alloc_base;
 #else
 #define module_alloc_base	((u64)_etext - MODULES_VSIZE)

commit f80fb3a3d50843a401dac4b566b3b131da8077a2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Jan 26 14:12:01 2016 +0100

    arm64: add support for kernel ASLR
    
    This adds support for KASLR is implemented, based on entropy provided by
    the bootloader in the /chosen/kaslr-seed DT property. Depending on the size
    of the address space (VA_BITS) and the page size, the entropy in the
    virtual displacement is up to 13 bits (16k/2 levels) and up to 25 bits (all
    4 levels), with the sidenote that displacements that result in the kernel
    image straddling a 1GB/32MB/512MB alignment boundary (for 4KB/16KB/64KB
    granule kernels, respectively) are not allowed, and will be rounded up to
    an acceptable value.
    
    If CONFIG_RANDOMIZE_MODULE_REGION_FULL is enabled, the module region is
    randomized independently from the core kernel. This makes it less likely
    that the location of core kernel data structures can be determined by an
    adversary, but causes all function calls from modules into the core kernel
    to be resolved via entries in the module PLTs.
    
    If CONFIG_RANDOMIZE_MODULE_REGION_FULL is not enabled, the module region is
    randomized by choosing a page aligned 128 MB region inside the interval
    [_etext - 128 MB, _stext + 128 MB). This gives between 10 and 14 bits of
    entropy (depending on page size), independently of the kernel randomization,
    but still guarantees that modules are within the range of relative branch
    and jump instructions (with the caveat that, since the module region is
    shared with other uses of the vmalloc area, modules may need to be loaded
    further away if the module region is exhausted)
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index 8652fb613304..e12af6754634 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -31,4 +31,10 @@ struct mod_arch_specific {
 u64 module_emit_plt_entry(struct module *mod, const Elf64_Rela *rela,
 			  Elf64_Sym *sym);
 
+#ifdef CONFIG_RANDOMIZE_BASE
+extern u64 module_alloc_base;
+#else
+#define module_alloc_base	((u64)_etext - MODULES_VSIZE)
+#endif
+
 #endif /* __ASM_MODULE_H */

commit fd045f6cd98ec4953147b318418bd45e441e52a3
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Nov 24 12:37:35 2015 +0100

    arm64: add support for module PLTs
    
    This adds support for emitting PLTs at module load time for relative
    branches that are out of range. This is a prerequisite for KASLR, which
    may place the kernel and the modules anywhere in the vmalloc area,
    making it more likely that branch target offsets exceed the maximum
    range of +/- 128 MB.
    
    In this version, I removed the distinction between relocations against
    .init executable sections and ordinary executable sections. The reason
    is that it is hardly worth the trouble, given that .init.text usually
    does not contain that many far branches, and this version now only
    reserves PLT entry space for jump and call relocations against undefined
    symbols (since symbols defined in the same module can be assumed to be
    within +/- 128 MB)
    
    For example, the mac80211.ko module (which is fairly sizable at ~400 KB)
    built with -mcmodel=large gives the following relocation counts:
    
                        relocs    branches   unique     !local
      .text              3925       3347       518        219
      .init.text           11          8         7          1
      .exit.text            4          4         4          1
      .text.unlikely       81         67        36         17
    
    ('unique' means branches to unique type/symbol/addend combos, of which
    !local is the subset referring to undefined symbols)
    
    IOW, we are only emitting a single PLT entry for the .init sections, and
    we are better off just adding it to the core PLT section instead.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
index e80e232b730e..8652fb613304 100644
--- a/arch/arm64/include/asm/module.h
+++ b/arch/arm64/include/asm/module.h
@@ -20,4 +20,15 @@
 
 #define MODULE_ARCH_VERMAGIC	"aarch64"
 
+#ifdef CONFIG_ARM64_MODULE_PLTS
+struct mod_arch_specific {
+	struct elf64_shdr	*plt;
+	int			plt_num_entries;
+	int			plt_max_entries;
+};
+#endif
+
+u64 module_emit_plt_entry(struct module *mod, const Elf64_Rela *rela,
+			  Elf64_Sym *sym);
+
 #endif /* __ASM_MODULE_H */

commit 257cb251925f854da435cbf79b140984413871ac
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Mar 5 11:49:33 2012 +0000

    arm64: Loadable modules
    
    This patch adds support for loadable modules. Loadable modules are
    loaded 64MB below the kernel image due to branch relocation restrictions
    (see Documentation/arm64/memory.txt).
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/module.h b/arch/arm64/include/asm/module.h
new file mode 100644
index 000000000000..e80e232b730e
--- /dev/null
+++ b/arch/arm64/include/asm/module.h
@@ -0,0 +1,23 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_MODULE_H
+#define __ASM_MODULE_H
+
+#include <asm-generic/module.h>
+
+#define MODULE_ARCH_VERMAGIC	"aarch64"
+
+#endif /* __ASM_MODULE_H */
