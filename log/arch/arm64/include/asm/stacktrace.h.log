commit c76898373f9b71586edaf150190c493ae9ed3e77
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:30:23 2020 -0700

    arm64: add loglvl to dump_backtrace()
    
    Currently, the log-level of show_stack() depends on a platform
    realization.  It creates situations where the headers are printed with
    lower log level or higher than the stacktrace (depending on a platform or
    user).
    
    Furthermore, it forces the logic decision from user to an architecture
    side.  In result, some users as sysrq/kdb/etc are doing tricks with
    temporary rising console_loglevel while printing their messages.  And in
    result it not only may print unwanted messages from other CPUs, but also
    omit printing at all in the unlucky case where the printk() was deferred.
    
    Introducing log-level parameter and KERN_UNSUPPRESSED [1] seems an easier
    approach than introducing more printk buffers.  Also, it will consolidate
    printings with headers.
    
    Add log level argument to dump_backtrace() as a preparation for
    introducing show_stack_loglvl().
    
    [1]: https://lore.kernel.org/lkml/20190528002412.1625-1-dima@arista.com/T/#u
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-10-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 5017b531a415..fc7613023c19 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -64,7 +64,8 @@ struct stackframe {
 extern int unwind_frame(struct task_struct *tsk, struct stackframe *frame);
 extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
-extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk);
+extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk,
+			   const char *loglvl);
 
 DECLARE_PER_CPU(unsigned long *, irq_stack_ptr);
 

commit bd4298c72b56d7faf0ee3671739f3a704a962d0f
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Fri May 8 11:15:45 2020 +0800

    arm64: stacktrace: Factor out some common code into on_stack()
    
    There are some common codes for stack checking, so factors it out into
    the function on_stack().
    
    No functional change.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Link: https://lore.kernel.org/r/07b3b0e6-3f58-4fed-07ea-7d17b7508948@huawei.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 4d9b1f48dc39..5017b531a415 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -68,12 +68,10 @@ extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk);
 
 DECLARE_PER_CPU(unsigned long *, irq_stack_ptr);
 
-static inline bool on_irq_stack(unsigned long sp,
+static inline bool on_stack(unsigned long sp, unsigned long low,
+				unsigned long high, enum stack_type type,
 				struct stack_info *info)
 {
-	unsigned long low = (unsigned long)raw_cpu_read(irq_stack_ptr);
-	unsigned long high = low + IRQ_STACK_SIZE;
-
 	if (!low)
 		return false;
 
@@ -83,12 +81,20 @@ static inline bool on_irq_stack(unsigned long sp,
 	if (info) {
 		info->low = low;
 		info->high = high;
-		info->type = STACK_TYPE_IRQ;
+		info->type = type;
 	}
-
 	return true;
 }
 
+static inline bool on_irq_stack(unsigned long sp,
+				struct stack_info *info)
+{
+	unsigned long low = (unsigned long)raw_cpu_read(irq_stack_ptr);
+	unsigned long high = low + IRQ_STACK_SIZE;
+
+	return on_stack(sp, low, high, STACK_TYPE_IRQ, info);
+}
+
 static inline bool on_task_stack(const struct task_struct *tsk,
 				 unsigned long sp,
 				 struct stack_info *info)
@@ -96,16 +102,7 @@ static inline bool on_task_stack(const struct task_struct *tsk,
 	unsigned long low = (unsigned long)task_stack_page(tsk);
 	unsigned long high = low + THREAD_SIZE;
 
-	if (sp < low || sp >= high)
-		return false;
-
-	if (info) {
-		info->low = low;
-		info->high = high;
-		info->type = STACK_TYPE_TASK;
-	}
-
-	return true;
+	return on_stack(sp, low, high, STACK_TYPE_TASK, info);
 }
 
 #ifdef CONFIG_VMAP_STACK
@@ -117,16 +114,7 @@ static inline bool on_overflow_stack(unsigned long sp,
 	unsigned long low = (unsigned long)raw_cpu_ptr(overflow_stack);
 	unsigned long high = low + OVERFLOW_STACK_SIZE;
 
-	if (sp < low || sp >= high)
-		return false;
-
-	if (info) {
-		info->low = low;
-		info->high = high;
-		info->type = STACK_TYPE_OVERFLOW;
-	}
-
-	return true;
+	return on_stack(sp, low, high, STACK_TYPE_OVERFLOW, info);
 }
 #else
 static inline bool on_overflow_stack(unsigned long sp,

commit 592700f094be229b5c9cc1192d5cea46eb4c7afc
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jul 2 14:07:29 2019 +0100

    arm64: stacktrace: Better handle corrupted stacks
    
    The arm64 stacktrace code is careful to only dereference frame records
    in valid stack ranges, ensuring that a corrupted frame record won't
    result in a faulting access.
    
    However, it's still possible for corrupt frame records to result in
    infinite loops in the stacktrace code, which is also undesirable.
    
    This patch ensures that we complete a stacktrace in finite time, by
    keeping track of which stacks we have already completed unwinding, and
    verifying that if the next frame record is on the same stack, it is at a
    higher address.
    
    As this has turned out to be particularly subtle, comments are added to
    explain the procedure.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Acked-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Tengfei Fan <tengfeif@codeaurora.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 7fa0dfedb8e9..4d9b1f48dc39 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -8,19 +8,12 @@
 #include <linux/percpu.h>
 #include <linux/sched.h>
 #include <linux/sched/task_stack.h>
+#include <linux/types.h>
 
 #include <asm/memory.h>
 #include <asm/ptrace.h>
 #include <asm/sdei.h>
 
-struct stackframe {
-	unsigned long fp;
-	unsigned long pc;
-#ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	int graph;
-#endif
-};
-
 enum stack_type {
 	STACK_TYPE_UNKNOWN,
 	STACK_TYPE_TASK,
@@ -28,6 +21,7 @@ enum stack_type {
 	STACK_TYPE_OVERFLOW,
 	STACK_TYPE_SDEI_NORMAL,
 	STACK_TYPE_SDEI_CRITICAL,
+	__NR_STACK_TYPES
 };
 
 struct stack_info {
@@ -36,6 +30,37 @@ struct stack_info {
 	enum stack_type type;
 };
 
+/*
+ * A snapshot of a frame record or fp/lr register values, along with some
+ * accounting information necessary for robust unwinding.
+ *
+ * @fp:          The fp value in the frame record (or the real fp)
+ * @pc:          The fp value in the frame record (or the real lr)
+ *
+ * @stacks_done: Stacks which have been entirely unwound, for which it is no
+ *               longer valid to unwind to.
+ *
+ * @prev_fp:     The fp that pointed to this frame record, or a synthetic value
+ *               of 0. This is used to ensure that within a stack, each
+ *               subsequent frame record is at an increasing address.
+ * @prev_type:   The type of stack this frame record was on, or a synthetic
+ *               value of STACK_TYPE_UNKNOWN. This is used to detect a
+ *               transition from one stack to another.
+ *
+ * @graph:       When FUNCTION_GRAPH_TRACER is selected, holds the index of a
+ *               replacement lr value in the ftrace graph stack.
+ */
+struct stackframe {
+	unsigned long fp;
+	unsigned long pc;
+	DECLARE_BITMAP(stacks_done, __NR_STACK_TYPES);
+	unsigned long prev_fp;
+	enum stack_type prev_type;
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	int graph;
+#endif
+};
+
 extern int unwind_frame(struct task_struct *tsk, struct stackframe *frame);
 extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
@@ -117,6 +142,9 @@ static inline bool on_accessible_stack(const struct task_struct *tsk,
 				       unsigned long sp,
 				       struct stack_info *info)
 {
+	if (info)
+		info->type = STACK_TYPE_UNKNOWN;
+
 	if (on_task_stack(tsk, sp, info))
 		return true;
 	if (tsk != current || preemptible())
@@ -139,6 +167,19 @@ static inline void start_backtrace(struct stackframe *frame,
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	frame->graph = 0;
 #endif
+
+	/*
+	 * Prime the first unwind.
+	 *
+	 * In unwind_frame() we'll check that the FP points to a valid stack,
+	 * which can't be STACK_TYPE_UNKNOWN, and the first unwind will be
+	 * treated as a transition to whichever stack that happens to be. The
+	 * prev_fp value won't be used, but we set it to 0 such that it is
+	 * definitely not an accessible stack address.
+	 */
+	bitmap_zero(frame->stacks_done, __NR_STACK_TYPES);
+	frame->prev_fp = 0;
+	frame->prev_type = STACK_TYPE_UNKNOWN;
 }
 
 #endif	/* __ASM_STACKTRACE_H */

commit f3dcbe67ed424f1cf92065f9ad0cc647f2b44eac
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jul 2 14:07:28 2019 +0100

    arm64: stacktrace: Factor out backtrace initialisation
    
    Some common code is required by each stacktrace user to initialise
    struct stackframe before the first call to unwind_frame().
    
    In preparation for adding to the common code, this patch factors it
    out into a separate function start_backtrace(), and modifies the
    stacktrace callers appropriately.
    
    No functional change.
    
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [Mark: drop tsk argument, update more callsites]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 1e0c5a7cdce5..7fa0dfedb8e9 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -131,4 +131,14 @@ static inline bool on_accessible_stack(const struct task_struct *tsk,
 	return false;
 }
 
+static inline void start_backtrace(struct stackframe *frame,
+				   unsigned long fp, unsigned long pc)
+{
+	frame->fp = fp;
+	frame->pc = pc;
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	frame->graph = 0;
+#endif
+}
+
 #endif	/* __ASM_STACKTRACE_H */

commit 8caa6e2be72313c170f2b30e8475323526dd7ed1
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jul 2 14:07:27 2019 +0100

    arm64: stacktrace: Constify stacktrace.h functions
    
    on_accessible_stack() and on_task_stack() shouldn't (and don't)
    modify their task argument, so it can be const.
    
    This patch adds the appropriate modifiers. Whitespace violations in the
    parameter lists are fixed at the same time.
    
    No functional change.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [Mark: fixup const location, whitespace]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index df45af931459..1e0c5a7cdce5 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -64,8 +64,9 @@ static inline bool on_irq_stack(unsigned long sp,
 	return true;
 }
 
-static inline bool on_task_stack(struct task_struct *tsk, unsigned long sp,
-				struct stack_info *info)
+static inline bool on_task_stack(const struct task_struct *tsk,
+				 unsigned long sp,
+				 struct stack_info *info)
 {
 	unsigned long low = (unsigned long)task_stack_page(tsk);
 	unsigned long high = low + THREAD_SIZE;
@@ -112,9 +113,9 @@ static inline bool on_overflow_stack(unsigned long sp,
  * We can only safely access per-cpu stacks from current in a non-preemptible
  * context.
  */
-static inline bool on_accessible_stack(struct task_struct *tsk,
-					unsigned long sp,
-					struct stack_info *info)
+static inline bool on_accessible_stack(const struct task_struct *tsk,
+				       unsigned long sp,
+				       struct stack_info *info)
 {
 	if (on_task_stack(tsk, sp, info))
 		return true;

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index e86737b7c924..df45af931459 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_STACKTRACE_H
 #define __ASM_STACKTRACE_H

commit 8a1ccfbc9e0256baafbbce85ccdb72ec89af2aab
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jul 20 14:41:53 2018 -0700

    arm64: Add stack information to on_accessible_stack
    
    In preparation for enabling the stackleak plugin on arm64,
    we need a way to get the bounds of the current stack. Extend
    on_accessible_stack to get this information.
    
    Acked-by: Alexander Popov <alex.popov@linux.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    [will: folded in fix for allmodconfig build breakage w/ sdei]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 902f9edacbea..e86737b7c924 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -32,6 +32,21 @@ struct stackframe {
 #endif
 };
 
+enum stack_type {
+	STACK_TYPE_UNKNOWN,
+	STACK_TYPE_TASK,
+	STACK_TYPE_IRQ,
+	STACK_TYPE_OVERFLOW,
+	STACK_TYPE_SDEI_NORMAL,
+	STACK_TYPE_SDEI_CRITICAL,
+};
+
+struct stack_info {
+	unsigned long low;
+	unsigned long high;
+	enum stack_type type;
+};
+
 extern int unwind_frame(struct task_struct *tsk, struct stackframe *frame);
 extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
@@ -39,7 +54,8 @@ extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk);
 
 DECLARE_PER_CPU(unsigned long *, irq_stack_ptr);
 
-static inline bool on_irq_stack(unsigned long sp)
+static inline bool on_irq_stack(unsigned long sp,
+				struct stack_info *info)
 {
 	unsigned long low = (unsigned long)raw_cpu_read(irq_stack_ptr);
 	unsigned long high = low + IRQ_STACK_SIZE;
@@ -47,46 +63,79 @@ static inline bool on_irq_stack(unsigned long sp)
 	if (!low)
 		return false;
 
-	return (low <= sp && sp < high);
+	if (sp < low || sp >= high)
+		return false;
+
+	if (info) {
+		info->low = low;
+		info->high = high;
+		info->type = STACK_TYPE_IRQ;
+	}
+
+	return true;
 }
 
-static inline bool on_task_stack(struct task_struct *tsk, unsigned long sp)
+static inline bool on_task_stack(struct task_struct *tsk, unsigned long sp,
+				struct stack_info *info)
 {
 	unsigned long low = (unsigned long)task_stack_page(tsk);
 	unsigned long high = low + THREAD_SIZE;
 
-	return (low <= sp && sp < high);
+	if (sp < low || sp >= high)
+		return false;
+
+	if (info) {
+		info->low = low;
+		info->high = high;
+		info->type = STACK_TYPE_TASK;
+	}
+
+	return true;
 }
 
 #ifdef CONFIG_VMAP_STACK
 DECLARE_PER_CPU(unsigned long [OVERFLOW_STACK_SIZE/sizeof(long)], overflow_stack);
 
-static inline bool on_overflow_stack(unsigned long sp)
+static inline bool on_overflow_stack(unsigned long sp,
+				struct stack_info *info)
 {
 	unsigned long low = (unsigned long)raw_cpu_ptr(overflow_stack);
 	unsigned long high = low + OVERFLOW_STACK_SIZE;
 
-	return (low <= sp && sp < high);
+	if (sp < low || sp >= high)
+		return false;
+
+	if (info) {
+		info->low = low;
+		info->high = high;
+		info->type = STACK_TYPE_OVERFLOW;
+	}
+
+	return true;
 }
 #else
-static inline bool on_overflow_stack(unsigned long sp) { return false; }
+static inline bool on_overflow_stack(unsigned long sp,
+			struct stack_info *info) { return false; }
 #endif
 
+
 /*
  * We can only safely access per-cpu stacks from current in a non-preemptible
  * context.
  */
-static inline bool on_accessible_stack(struct task_struct *tsk, unsigned long sp)
+static inline bool on_accessible_stack(struct task_struct *tsk,
+					unsigned long sp,
+					struct stack_info *info)
 {
-	if (on_task_stack(tsk, sp))
+	if (on_task_stack(tsk, sp, info))
 		return true;
 	if (tsk != current || preemptible())
 		return false;
-	if (on_irq_stack(sp))
+	if (on_irq_stack(sp, info))
 		return true;
-	if (on_overflow_stack(sp))
+	if (on_overflow_stack(sp, info))
 		return true;
-	if (on_sdei_stack(sp))
+	if (on_sdei_stack(sp, info))
 		return true;
 
 	return false;

commit 9f416319f40cd857d2bb517630e5855a905ef3fb
Author: Pratyush Anand <panand@redhat.com>
Date:   Mon Feb 5 14:28:01 2018 +0100

    arm64: fix unwind_frame() for filtered out fn for function graph tracing
    
    do_task_stat() calls get_wchan(), which further does unwind_frame().
    unwind_frame() restores frame->pc to original value in case function
    graph tracer has modified a return address (LR) in a stack frame to hook
    a function return. However, if function graph tracer has hit a filtered
    function, then we can't unwind it as ftrace_push_return_trace() has
    biased the index(frame->graph) with a 'huge negative'
    offset(-FTRACE_NOTRACE_DEPTH).
    
    Moreover, arm64 stack walker defines index(frame->graph) as unsigned
    int, which can not compare a -ve number.
    
    Similar problem we can have with calling of walk_stackframe() from
    save_stack_trace_tsk() or dump_backtrace().
    
    This patch fixes unwind_frame() to test the index for -ve value and
    restore index accordingly before we can restore frame->pc.
    
    Reproducer:
    
    cd /sys/kernel/debug/tracing/
    echo schedule > set_graph_notrace
    echo 1 > options/display-graph
    echo wakeup > current_tracer
    ps -ef | grep -i agent
    
    Above commands result in:
    Unable to handle kernel paging request at virtual address ffff801bd3d1e000
    pgd = ffff8003cbe97c00
    [ffff801bd3d1e000] *pgd=0000000000000000, *pud=0000000000000000
    Internal error: Oops: 96000006 [#1] SMP
    [...]
    CPU: 5 PID: 11696 Comm: ps Not tainted 4.11.0+ #33
    [...]
    task: ffff8003c21ba000 task.stack: ffff8003cc6c0000
    PC is at unwind_frame+0x12c/0x180
    LR is at get_wchan+0xd4/0x134
    pc : [<ffff00000808892c>] lr : [<ffff0000080860b8>] pstate: 60000145
    sp : ffff8003cc6c3ab0
    x29: ffff8003cc6c3ab0 x28: 0000000000000001
    x27: 0000000000000026 x26: 0000000000000026
    x25: 00000000000012d8 x24: 0000000000000000
    x23: ffff8003c1c04000 x22: ffff000008c83000
    x21: ffff8003c1c00000 x20: 000000000000000f
    x19: ffff8003c1bc0000 x18: 0000fffffc593690
    x17: 0000000000000000 x16: 0000000000000001
    x15: 0000b855670e2b60 x14: 0003e97f22cf1d0f
    x13: 0000000000000001 x12: 0000000000000000
    x11: 00000000e8f4883e x10: 0000000154f47ec8
    x9 : 0000000070f367c0 x8 : 0000000000000000
    x7 : 00008003f7290000 x6 : 0000000000000018
    x5 : 0000000000000000 x4 : ffff8003c1c03cb0
    x3 : ffff8003c1c03ca0 x2 : 00000017ffe80000
    x1 : ffff8003cc6c3af8 x0 : ffff8003d3e9e000
    
    Process ps (pid: 11696, stack limit = 0xffff8003cc6c0000)
    Stack: (0xffff8003cc6c3ab0 to 0xffff8003cc6c4000)
    [...]
    [<ffff00000808892c>] unwind_frame+0x12c/0x180
    [<ffff000008305008>] do_task_stat+0x864/0x870
    [<ffff000008305c44>] proc_tgid_stat+0x3c/0x48
    [<ffff0000082fde0c>] proc_single_show+0x5c/0xb8
    [<ffff0000082b27e0>] seq_read+0x160/0x414
    [<ffff000008289e6c>] __vfs_read+0x58/0x164
    [<ffff00000828b164>] vfs_read+0x88/0x144
    [<ffff00000828c2e8>] SyS_read+0x60/0xc0
    [<ffff0000080834a0>] __sys_trace_return+0x0/0x4
    
    Fixes: 20380bb390a4 (arm64: ftrace: fix a stack tracer's output under function graph tracer)
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Jerome Marchand <jmarchan@redhat.com>
    [catalin.marinas@arm.com: replace WARN_ON with WARN_ON_ONCE]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 472ef944e932..902f9edacbea 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -28,7 +28,7 @@ struct stackframe {
 	unsigned long fp;
 	unsigned long pc;
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	unsigned int graph;
+	int graph;
 #endif
 };
 

commit f5df26961853d6809d704cedcaf082c57f635a64
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:12 2018 +0000

    arm64: kernel: Add arch-specific SDEI entry code and CPU masking
    
    The Software Delegated Exception Interface (SDEI) is an ARM standard
    for registering callbacks from the platform firmware into the OS.
    This is typically used to implement RAS notifications.
    
    Such notifications enter the kernel at the registered entry-point
    with the register values of the interrupted CPU context. Because this
    is not a CPU exception, it cannot reuse the existing entry code.
    (crucially we don't implicitly know which exception level we interrupted),
    
    Add the entry point to entry.S to set us up for calling into C code. If
    the event interrupted code that had interrupts masked, we always return
    to that location. Otherwise we pretend this was an IRQ, and use SDEI's
    complete_and_resume call to return to vbar_el1 + offset.
    
    This allows the kernel to deliver signals to user space processes. For
    KVM this triggers the world switch, a quick spin round vcpu_run, then
    back into the guest, unless there are pending signals.
    
    Add sdei_mask_local_cpu() calls to the smp_send_stop() code, this covers
    the panic() code-path, which doesn't invoke cpuhotplug notifiers.
    
    Because we can interrupt entry-from/exit-to another EL, we can't trust the
    value in sp_el0 or x29, even if we interrupted the kernel, in this case
    the code in entry.S will save/restore sp_el0 and use the value in
    __entry_task.
    
    When we have VMAP stacks we can interrupt the stack-overflow test, which
    stirs x0 into sp, meaning we have to have our own VMAP stacks. For now
    these are allocated when we probe the interface. Future patches will add
    refcounting hooks to allow the arch code to allocate them lazily.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 6ad30776e984..472ef944e932 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -22,6 +22,7 @@
 
 #include <asm/memory.h>
 #include <asm/ptrace.h>
+#include <asm/sdei.h>
 
 struct stackframe {
 	unsigned long fp;
@@ -85,6 +86,8 @@ static inline bool on_accessible_stack(struct task_struct *tsk, unsigned long sp
 		return true;
 	if (on_overflow_stack(sp))
 		return true;
+	if (on_sdei_stack(sp))
+		return true;
 
 	return false;
 }

commit 872d8327ce8982883b8237b2c320c8666f14e561
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 20:30:35 2017 +0100

    arm64: add VMAP_STACK overflow detection
    
    This patch adds stack overflow detection to arm64, usable when vmap'd stacks
    are in use.
    
    Overflow is detected in a small preamble executed for each exception entry,
    which checks whether there is enough space on the current stack for the general
    purpose registers to be saved. If there is not enough space, the overflow
    handler is invoked on a per-cpu overflow stack. This approach preserves the
    original exception information in ESR_EL1 (and where appropriate, FAR_EL1).
    
    Task and IRQ stacks are aligned to double their size, enabling overflow to be
    detected with a single bit test. For example, a 16K stack is aligned to 32K,
    ensuring that bit 14 of the SP must be zero. On an overflow (or underflow),
    this bit is flipped. Thus, overflow (of less than the size of the stack) can be
    detected by testing whether this bit is set.
    
    The overflow check is performed before any attempt is made to access the
    stack, avoiding recursive faults (and the loss of exception information
    these would entail). As logical operations cannot be performed on the SP
    directly, the SP is temporarily swapped with a general purpose register
    using arithmetic operations to enable the test to be performed.
    
    This gives us a useful error message on stack overflow, as can be trigger with
    the LKDTM overflow test:
    
    [  305.388749] lkdtm: Performing direct entry OVERFLOW
    [  305.395444] Insufficient stack space to handle exception!
    [  305.395482] ESR: 0x96000047 -- DABT (current EL)
    [  305.399890] FAR: 0xffff00000a5e7f30
    [  305.401315] Task stack:     [0xffff00000a5e8000..0xffff00000a5ec000]
    [  305.403815] IRQ stack:      [0xffff000008000000..0xffff000008004000]
    [  305.407035] Overflow stack: [0xffff80003efce4e0..0xffff80003efcf4e0]
    [  305.409622] CPU: 0 PID: 1219 Comm: sh Not tainted 4.13.0-rc3-00021-g9636aea #5
    [  305.412785] Hardware name: linux,dummy-virt (DT)
    [  305.415756] task: ffff80003d051c00 task.stack: ffff00000a5e8000
    [  305.419221] PC is at recursive_loop+0x10/0x48
    [  305.421637] LR is at recursive_loop+0x38/0x48
    [  305.423768] pc : [<ffff00000859f330>] lr : [<ffff00000859f358>] pstate: 40000145
    [  305.428020] sp : ffff00000a5e7f50
    [  305.430469] x29: ffff00000a5e8350 x28: ffff80003d051c00
    [  305.433191] x27: ffff000008981000 x26: ffff000008f80400
    [  305.439012] x25: ffff00000a5ebeb8 x24: ffff00000a5ebeb8
    [  305.440369] x23: ffff000008f80138 x22: 0000000000000009
    [  305.442241] x21: ffff80003ce65000 x20: ffff000008f80188
    [  305.444552] x19: 0000000000000013 x18: 0000000000000006
    [  305.446032] x17: 0000ffffa2601280 x16: ffff0000081fe0b8
    [  305.448252] x15: ffff000008ff546d x14: 000000000047a4c8
    [  305.450246] x13: ffff000008ff7872 x12: 0000000005f5e0ff
    [  305.452953] x11: ffff000008ed2548 x10: 000000000005ee8d
    [  305.454824] x9 : ffff000008545380 x8 : ffff00000a5e8770
    [  305.457105] x7 : 1313131313131313 x6 : 00000000000000e1
    [  305.459285] x5 : 0000000000000000 x4 : 0000000000000000
    [  305.461781] x3 : 0000000000000000 x2 : 0000000000000400
    [  305.465119] x1 : 0000000000000013 x0 : 0000000000000012
    [  305.467724] Kernel panic - not syncing: kernel stack overflow
    [  305.470561] CPU: 0 PID: 1219 Comm: sh Not tainted 4.13.0-rc3-00021-g9636aea #5
    [  305.473325] Hardware name: linux,dummy-virt (DT)
    [  305.475070] Call trace:
    [  305.476116] [<ffff000008088ad8>] dump_backtrace+0x0/0x378
    [  305.478991] [<ffff000008088e64>] show_stack+0x14/0x20
    [  305.481237] [<ffff00000895a178>] dump_stack+0x98/0xb8
    [  305.483294] [<ffff0000080c3288>] panic+0x118/0x280
    [  305.485673] [<ffff0000080c2e9c>] nmi_panic+0x6c/0x70
    [  305.486216] [<ffff000008089710>] handle_bad_stack+0x118/0x128
    [  305.486612] Exception stack(0xffff80003efcf3a0 to 0xffff80003efcf4e0)
    [  305.487334] f3a0: 0000000000000012 0000000000000013 0000000000000400 0000000000000000
    [  305.488025] f3c0: 0000000000000000 0000000000000000 00000000000000e1 1313131313131313
    [  305.488908] f3e0: ffff00000a5e8770 ffff000008545380 000000000005ee8d ffff000008ed2548
    [  305.489403] f400: 0000000005f5e0ff ffff000008ff7872 000000000047a4c8 ffff000008ff546d
    [  305.489759] f420: ffff0000081fe0b8 0000ffffa2601280 0000000000000006 0000000000000013
    [  305.490256] f440: ffff000008f80188 ffff80003ce65000 0000000000000009 ffff000008f80138
    [  305.490683] f460: ffff00000a5ebeb8 ffff00000a5ebeb8 ffff000008f80400 ffff000008981000
    [  305.491051] f480: ffff80003d051c00 ffff00000a5e8350 ffff00000859f358 ffff00000a5e7f50
    [  305.491444] f4a0: ffff00000859f330 0000000040000145 0000000000000000 0000000000000000
    [  305.492008] f4c0: 0001000000000000 0000000000000000 ffff00000a5e8350 ffff00000859f330
    [  305.493063] [<ffff00000808205c>] __bad_stack+0x88/0x8c
    [  305.493396] [<ffff00000859f330>] recursive_loop+0x10/0x48
    [  305.493731] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494088] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494425] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494649] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494898] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495205] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495453] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495708] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496000] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496302] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496644] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496894] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.497138] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.497325] [<ffff00000859f3dc>] lkdtm_OVERFLOW+0x14/0x20
    [  305.497506] [<ffff00000859f314>] lkdtm_do_action+0x1c/0x28
    [  305.497786] [<ffff00000859f178>] direct_entry+0xe0/0x170
    [  305.498095] [<ffff000008345568>] full_proxy_write+0x60/0xa8
    [  305.498387] [<ffff0000081fb7f4>] __vfs_write+0x1c/0x128
    [  305.498679] [<ffff0000081fcc68>] vfs_write+0xa0/0x1b0
    [  305.498926] [<ffff0000081fe0fc>] SyS_write+0x44/0xa0
    [  305.499182] Exception stack(0xffff00000a5ebec0 to 0xffff00000a5ec000)
    [  305.499429] bec0: 0000000000000001 000000001c4cf5e0 0000000000000009 000000001c4cf5e0
    [  305.499674] bee0: 574f4c465245564f 0000000000000000 0000000000000000 8000000080808080
    [  305.499904] bf00: 0000000000000040 0000000000000038 fefefeff1b4bc2ff 7f7f7f7f7f7fff7f
    [  305.500189] bf20: 0101010101010101 0000000000000000 000000000047a4c8 0000000000000038
    [  305.500712] bf40: 0000000000000000 0000ffffa2601280 0000ffffc63f6068 00000000004b5000
    [  305.501241] bf60: 0000000000000001 000000001c4cf5e0 0000000000000009 000000001c4cf5e0
    [  305.501791] bf80: 0000000000000020 0000000000000000 00000000004b5000 000000001c4cc458
    [  305.502314] bfa0: 0000000000000000 0000ffffc63f7950 000000000040a3c4 0000ffffc63f70e0
    [  305.502762] bfc0: 0000ffffa2601268 0000000080000000 0000000000000001 0000000000000040
    [  305.503207] bfe0: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [  305.503680] [<ffff000008082fb0>] el0_svc_naked+0x24/0x28
    [  305.504720] Kernel Offset: disabled
    [  305.505189] CPU features: 0x002082
    [  305.505473] Memory Limit: none
    [  305.506181] ---[ end Kernel panic - not syncing: kernel stack overflow
    
    This patch was co-authored by Ard Biesheuvel and Mark Rutland.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 92ddb6d25cf3..6ad30776e984 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -57,6 +57,20 @@ static inline bool on_task_stack(struct task_struct *tsk, unsigned long sp)
 	return (low <= sp && sp < high);
 }
 
+#ifdef CONFIG_VMAP_STACK
+DECLARE_PER_CPU(unsigned long [OVERFLOW_STACK_SIZE/sizeof(long)], overflow_stack);
+
+static inline bool on_overflow_stack(unsigned long sp)
+{
+	unsigned long low = (unsigned long)raw_cpu_ptr(overflow_stack);
+	unsigned long high = low + OVERFLOW_STACK_SIZE;
+
+	return (low <= sp && sp < high);
+}
+#else
+static inline bool on_overflow_stack(unsigned long sp) { return false; }
+#endif
+
 /*
  * We can only safely access per-cpu stacks from current in a non-preemptible
  * context.
@@ -69,6 +83,8 @@ static inline bool on_accessible_stack(struct task_struct *tsk, unsigned long sp
 		return false;
 	if (on_irq_stack(sp))
 		return true;
+	if (on_overflow_stack(sp))
+		return true;
 
 	return false;
 }

commit 12964443e8d1914010f9269f9f9abc4e122bc6ca
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Aug 1 18:51:15 2017 +0100

    arm64: add on_accessible_stack()
    
    Both unwind_frame() and dump_backtrace() try to check whether a stack
    address is sane to access, with very similar logic. Both will need
    updating in order to handle overflow stacks.
    
    Factor out this logic into a helper, so that we can avoid further
    duplication when we add overflow stacks.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 4c68d8a81988..92ddb6d25cf3 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -57,4 +57,20 @@ static inline bool on_task_stack(struct task_struct *tsk, unsigned long sp)
 	return (low <= sp && sp < high);
 }
 
+/*
+ * We can only safely access per-cpu stacks from current in a non-preemptible
+ * context.
+ */
+static inline bool on_accessible_stack(struct task_struct *tsk, unsigned long sp)
+{
+	if (on_task_stack(tsk, sp))
+		return true;
+	if (tsk != current || preemptible())
+		return false;
+	if (on_irq_stack(sp))
+		return true;
+
+	return false;
+}
+
 #endif	/* __ASM_STACKTRACE_H */

commit f60fe78f133243e6de0f05fdefc3ed2f3c5085ca
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jul 31 21:17:03 2017 +0100

    arm64: use an irq stack pointer
    
    We allocate our IRQ stacks using a percpu array. This allows us to generate our
    IRQ stack pointers with adr_this_cpu, but bloats the kernel Image with the boot
    CPU's IRQ stack. Additionally, these are packed with other percpu variables,
    and aren't guaranteed to have guard pages.
    
    When we enable VMAP_STACK we'll want to vmap our IRQ stacks also, in order to
    provide guard pages and to permit more stringent alignment requirements. Doing
    so will require that we use a percpu pointer to each IRQ stack, rather than
    allocating a percpu IRQ stack in the kernel image.
    
    This patch updates our IRQ stack code to use a percpu pointer to the base of
    each IRQ stack. This will allow us to change the way the stack is allocated
    with minimal changes elsewhere. In some cases we may try to backtrace before
    the IRQ stack pointers are initialised, so on_irq_stack() is updated to account
    for this.
    
    In testing with cyclictest, there was no measureable difference between using
    adr_this_cpu (for irq_stack) and ldr_this_cpu (for irq_stack_ptr) in the IRQ
    entry path.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 000e24182a5c..4c68d8a81988 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -36,13 +36,16 @@ extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
 extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk);
 
-DECLARE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack);
+DECLARE_PER_CPU(unsigned long *, irq_stack_ptr);
 
 static inline bool on_irq_stack(unsigned long sp)
 {
-	unsigned long low = (unsigned long)raw_cpu_ptr(irq_stack);
+	unsigned long low = (unsigned long)raw_cpu_read(irq_stack_ptr);
 	unsigned long high = low + IRQ_STACK_SIZE;
 
+	if (!low)
+		return false;
+
 	return (low <= sp && sp < high);
 }
 

commit f60ad4edcf07238a3d2646d65d8d217032452550
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 20 12:26:48 2017 +0100

    arm64: clean up irq stack definitions
    
    Before we add yet another stack to the kernel, it would be nice to
    ensure that we consistently organise stack definitions and related
    helper functions.
    
    This patch moves the basic IRQ stack defintions to <asm/memory.h> to
    live with their task stack counterparts. Helpers used for unwinding are
    moved into <asm/stacktrace.h>, where subsequent patches will add helpers
    for other stacks. Includes are fixed up accordingly.
    
    This patch is a pure refactoring -- there should be no functional
    changes as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 3bebab378c72..000e24182a5c 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -16,7 +16,12 @@
 #ifndef __ASM_STACKTRACE_H
 #define __ASM_STACKTRACE_H
 
-struct task_struct;
+#include <linux/percpu.h>
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+
+#include <asm/memory.h>
+#include <asm/ptrace.h>
 
 struct stackframe {
 	unsigned long fp;
@@ -31,4 +36,22 @@ extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
 extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk);
 
+DECLARE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack);
+
+static inline bool on_irq_stack(unsigned long sp)
+{
+	unsigned long low = (unsigned long)raw_cpu_ptr(irq_stack);
+	unsigned long high = low + IRQ_STACK_SIZE;
+
+	return (low <= sp && sp < high);
+}
+
+static inline bool on_task_stack(struct task_struct *tsk, unsigned long sp)
+{
+	unsigned long low = (unsigned long)task_stack_page(tsk);
+	unsigned long high = low + THREAD_SIZE;
+
+	return (low <= sp && sp < high);
+}
+
 #endif	/* __ASM_STACKTRACE_H */

commit 31e43ad3b74a5d7b282023b72f25fc677c14c727
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sun Jul 23 09:05:38 2017 +0100

    arm64: unwind: remove sp from struct stackframe
    
    The unwind code sets the sp member of struct stackframe to
    'frame pointer + 0x10' unconditionally, without regard for whether
    doing so produces a legal value. So let's simply remove it now that
    we have stopped using it anyway.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 5b6eafccc5d8..3bebab378c72 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -20,7 +20,6 @@ struct task_struct;
 
 struct stackframe {
 	unsigned long fp;
-	unsigned long sp;
 	unsigned long pc;
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	unsigned int graph;

commit 1149aad10b1e2f2cf1ea023889ac8604ae869c5a
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Tue May 9 09:53:37 2017 +0800

    arm64: Add dump_backtrace() in show_regs
    
    Generic code expects show_regs() to dump the stack, but arm64's
    show_regs() does not. This makes it hard to debug softlockups and
    other issues that result in show_regs() being called.
    
    This patch updates arm64's show_regs() to dump the stack, as common
    code expects.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    [will: folded in bug_handler fix from mrutland]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 801a16dbbdf6..5b6eafccc5d8 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -30,5 +30,6 @@ struct stackframe {
 extern int unwind_frame(struct task_struct *tsk, struct stackframe *frame);
 extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
+extern void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk);
 
 #endif	/* __ASM_STACKTRACE_H */

commit 20380bb390a443b2c5c8800cec59743faf8151b4
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:41 2015 +0900

    arm64: ftrace: fix a stack tracer's output under function graph tracer
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function return. This will result in many useless entries
    (return_to_handler) showing up in
     a) a stack tracer's output
     b) perf call graph (with perf record -g)
     c) dump_backtrace (at panic et al.)
    
    For example, in case of a),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo 1 > /proc/sys/kernel/stack_trace_enabled
      $ cat /sys/kernel/debug/tracing/stack_trace
            Depth    Size   Location    (54 entries)
            -----    ----   --------
      0)     4504      16   gic_raise_softirq+0x28/0x150
      1)     4488      80   smp_cross_call+0x38/0xb8
      2)     4408      48   return_to_handler+0x0/0x40
      3)     4360      32   return_to_handler+0x0/0x40
      ...
    
    In case of b),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ perf record -e mem:XXX:x -ag -- sleep 10
      $ perf report
                      ...
                      |          |          |--0.22%-- 0x550f8
                      |          |          |          0x10888
                      |          |          |          el0_svc_naked
                      |          |          |          sys_openat
                      |          |          |          return_to_handler
                      |          |          |          return_to_handler
                      ...
    
    In case of c),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo c > /proc/sysrq-trigger
      ...
      Call trace:
      [<ffffffc00044d3ac>] sysrq_handle_crash+0x24/0x30
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      ...
    
    This patch replaces such entries with real addresses preserved in
    current->ret_stack[] at unwind_frame(). This way, we can cover all
    the cases.
    
    Reviewed-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [will: fixed minor context changes conflicting with irq stack bits]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 6fb61c5090b4..801a16dbbdf6 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -22,6 +22,9 @@ struct stackframe {
 	unsigned long fp;
 	unsigned long sp;
 	unsigned long pc;
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	unsigned int graph;
+#endif
 };
 
 extern int unwind_frame(struct task_struct *tsk, struct stackframe *frame);

commit fe13f95b720075327a761fe6ddb45b0c90cab504
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:40 2015 +0900

    arm64: pass a task parameter to unwind_frame()
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function's return. This will result in many useless entries
    (return_to_handler) showing up in a call stack list.
    We will fix this problem in a later patch ("arm64: ftrace: fix a stack
    tracer's output under function graph tracer"). But since real return
    addresses are saved in ret_stack[] array in struct task_struct,
    unwind functions need to be notified of, in addition to a stack pointer
    address, which task is being traced in order to find out real return
    addresses.
    
    This patch extends unwind functions' interfaces by adding an extra
    argument of a pointer to task_struct.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
index 7318f6d54aa9..6fb61c5090b4 100644
--- a/arch/arm64/include/asm/stacktrace.h
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -16,14 +16,16 @@
 #ifndef __ASM_STACKTRACE_H
 #define __ASM_STACKTRACE_H
 
+struct task_struct;
+
 struct stackframe {
 	unsigned long fp;
 	unsigned long sp;
 	unsigned long pc;
 };
 
-extern int unwind_frame(struct stackframe *frame);
-extern void walk_stackframe(struct stackframe *frame,
+extern int unwind_frame(struct task_struct *tsk, struct stackframe *frame);
+extern void walk_stackframe(struct task_struct *tsk, struct stackframe *frame,
 			    int (*fn)(struct stackframe *, void *), void *data);
 
 #endif	/* __ASM_STACKTRACE_H */

commit 60ffc30d5652810dd34ea2eec41504222f5d5791
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: Exception handling
    
    The patch contains the exception entry code (kernel/entry.S), pt_regs
    structure and related accessors, undefined instruction trapping and
    stack tracing.
    
    AArch64 Linux kernel (including kernel threads) runs in EL1 mode using
    the SP1 stack. The vectors don't have a fixed address, only alignment
    (2^11) requirements.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/stacktrace.h b/arch/arm64/include/asm/stacktrace.h
new file mode 100644
index 000000000000..7318f6d54aa9
--- /dev/null
+++ b/arch/arm64/include/asm/stacktrace.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_STACKTRACE_H
+#define __ASM_STACKTRACE_H
+
+struct stackframe {
+	unsigned long fp;
+	unsigned long sp;
+	unsigned long pc;
+};
+
+extern int unwind_frame(struct stackframe *frame);
+extern void walk_stackframe(struct stackframe *frame,
+			    int (*fn)(struct stackframe *, void *), void *data);
+
+#endif	/* __ASM_STACKTRACE_H */
