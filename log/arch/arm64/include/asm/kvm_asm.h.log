commit 304e2989c93e941fa55b38c59c975d4acfb6e4a2
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Jun 10 16:27:46 2020 +0100

    KVM: arm64: Move hyp_symbol_addr() to kvm_asm.h
    
    Recent refactoring of the arm64 code make it awkward to have
    hyp_symbol_addr() in kvm_mmu.h. Instead, move it next to its
    main user, which is __hyp_this_cpu_ptr().
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index d9b7da15dbca..352aaebf4198 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -81,6 +81,26 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern char __smccc_workaround_1_smc[__SMCCC_WORKAROUND_1_SMC_SZ];
 
+/*
+ * Obtain the PC-relative address of a kernel symbol
+ * s: symbol
+ *
+ * The goal of this macro is to return a symbol's address based on a
+ * PC-relative computation, as opposed to a loading the VA from a
+ * constant pool or something similar. This works well for HYP, as an
+ * absolute VA is guaranteed to be wrong. Only use this if trying to
+ * obtain the address of a symbol (i.e. not something you obtained by
+ * following a pointer).
+ */
+#define hyp_symbol_addr(s)						\
+	({								\
+		typeof(s) *addr;					\
+		asm("adrp	%0, %1\n"				\
+		    "add	%0, %0, :lo12:%1\n"			\
+		    : "=r" (addr) : "S" (&s));				\
+		addr;							\
+	})
+
 /*
  * Home-grown __this_cpu_{ptr,read} variants that always work at HYP,
  * provided that sym is really a *symbol* and not a pointer obtained from

commit b990d37fdf6781fd0907ffd14d0dff16b5d58ffa
Author: Marc Zyngier <maz@kernel.org>
Date:   Sun Jun 7 10:55:28 2020 +0100

    KVM: arm64: Stop sparse from moaning at __hyp_this_cpu_ptr
    
    Sparse complains that __hyp_this_cpu_ptr() returns something
    that is flagged noderef and not in the correct address space
    (both being the result of the __percpu annotation).
    
    Pretend that __hyp_this_cpu_ptr() knows what it is doing by
    forcefully casting the pointer with __kernel __force.
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 0c9b5fc4ba0a..d9b7da15dbca 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -81,12 +81,19 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern char __smccc_workaround_1_smc[__SMCCC_WORKAROUND_1_SMC_SZ];
 
-/* Home-grown __this_cpu_{ptr,read} variants that always work at HYP */
+/*
+ * Home-grown __this_cpu_{ptr,read} variants that always work at HYP,
+ * provided that sym is really a *symbol* and not a pointer obtained from
+ * a data structure. As for SHIFT_PERCPU_PTR(), the creative casting keeps
+ * sparse quiet.
+ */
 #define __hyp_this_cpu_ptr(sym)						\
 	({								\
-		void *__ptr = hyp_symbol_addr(sym);			\
+		void *__ptr;						\
+		__verify_pcpu_ptr(&sym);				\
+		__ptr = hyp_symbol_addr(sym);				\
 		__ptr += read_sysreg(tpidr_el2);			\
-		(typeof(&sym))__ptr;					\
+		(typeof(sym) __kernel __force *)__ptr;			\
 	 })
 
 #define __hyp_this_cpu_read(sym)					\

commit 71b3ec5f221b8b3ff545639be83ddfcd5d7c9800
Author: David Brazdil <dbrazdil@google.com>
Date:   Fri May 15 16:20:56 2020 +0100

    KVM: arm64: Clean up cpu_init_hyp_mode()
    
    Pull bits of code to the only place where it is used. Remove empty function
    __cpu_init_stage2(). Remove redundant has_vhe() check since this function is
    nVHE-only. No functional changes intended.
    
    Signed-off-by: David Brazdil <dbrazdil@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200515152056.83158-1-dbrazdil@google.com

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 59e314f38e43..0c9b5fc4ba0a 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -70,6 +70,8 @@ extern int kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu);
 
 extern int __kvm_vcpu_run_nvhe(struct kvm_vcpu *vcpu);
 
+extern void __kvm_enable_ssbs(void);
+
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
 extern u64 __vgic_v3_read_vmcr(void);
 extern void __vgic_v3_write_vmcr(u32 vmcr);

commit c6fe89ff8b250ad4dc4bed7bd5877bfbc35f4aba
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed May 13 11:58:29 2020 +0100

    KVM: arm64: Simplify __kvm_timer_set_cntvoff implementation
    
    Now that this function isn't constrained by the 32bit PCS,
    let's simplify it by taking a single 64bit offset instead
    of two 32bit parameters.
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 7c7eeeaab9fa..59e314f38e43 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -64,7 +64,7 @@ extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 extern void __kvm_tlb_flush_vmid(struct kvm *kvm);
 extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);
 
-extern void __kvm_timer_set_cntvoff(u32 cntvoff_low, u32 cntvoff_high);
+extern void __kvm_timer_set_cntvoff(u64 cntvoff);
 
 extern int kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu);
 

commit 4db61fef16a104f94bde24fe163064b98cee6b7c
Author: Mark Brown <broonie@kernel.org>
Date:   Tue Feb 18 19:58:39 2020 +0000

    arm64: kvm: Modernize __smccc_workaround_1_smc_start annotations
    
    In an effort to clarify and simplify the annotation of assembly functions
    in the kernel new macros have been introduced. These replace ENTRY and
    ENDPROC with separate annotations for standard C callable functions,
    data and code with different calling conventions.
    
    Using these for __smccc_workaround_1_smc is more involved than for most
    symbols as this symbol is annotated quite unusually, rather than just have
    the explicit symbol we define _start and _end symbols which we then use to
    compute the length. This does not play at all nicely with the new style
    macros. Instead define a constant for the size of the function and use that
    in both the C code and for .org based size checks in the assembly code.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 44a243754c1b..7c7eeeaab9fa 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -36,6 +36,8 @@
  */
 #define KVM_VECTOR_PREAMBLE	(2 * AARCH64_INSN_SIZE)
 
+#define __SMCCC_WORKAROUND_1_SMC_SZ 36
+
 #ifndef __ASSEMBLY__
 
 #include <linux/mm.h>
@@ -75,6 +77,8 @@ extern void __vgic_v3_init_lrs(void);
 
 extern u32 __kvm_get_mdcr_el2(void);
 
+extern char __smccc_workaround_1_smc[__SMCCC_WORKAROUND_1_SMC_SZ];
+
 /* Home-grown __this_cpu_{ptr,read} variants that always work at HYP */
 #define __hyp_this_cpu_ptr(sym)						\
 	({								\

commit 0e5b9c085dcef61163f3f277964c1a1623043f67
Author: James Morse <james.morse@arm.com>
Date:   Tue Jun 18 16:17:36 2019 +0100

    KVM: arm64: Consume pending SError as early as possible
    
    On systems with v8.2 we switch the 'vaxorcism' of guest SError with an
    alternative sequence that uses the ESB-instruction, then reads DISR_EL1.
    This saves the unmasking and remasking of asynchronous exceptions.
    
    We do this after we've saved the guest registers and restored the
    host's. Any SError that becomes pending due to this will be accounted
    to the guest, when it actually occurred during host-execution.
    
    Move the ESB-instruction as early as possible. Any guest SError
    will become pending due to this ESB-instruction and then consumed to
    DISR_EL1 before the host touches anything.
    
    This lets us account for host/guest SError precisely on the guest
    exit exception boundary.
    
    Because the ESB-instruction now lands in the preamble section of
    the vectors, we need to add it to the unpatched indirect vectors
    too, and to any sequence that may be patched in over the top.
    
    The ESB-instruction always lives in the head of the vectors,
    to be before any memory write. Whereas the register-store always
    lives in the tail.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 388e1b520618..44a243754c1b 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -34,7 +34,7 @@
  * Size of the HYP vectors preamble. kvm_patch_vector_branch() generates code
  * that jumps over this.
  */
-#define KVM_VECTOR_PREAMBLE	(1 * AARCH64_INSN_SIZE)
+#define KVM_VECTOR_PREAMBLE	(2 * AARCH64_INSN_SIZE)
 
 #ifndef __ASSEMBLY__
 

commit 3dbf100b0b10e91d65bd83b91cee3ef61f1b96c4
Author: James Morse <james.morse@arm.com>
Date:   Tue Jun 18 16:17:34 2019 +0100

    KVM: arm64: Abstract the size of the HYP vectors pre-amble
    
    The EL2 vector hardening feature causes KVM to generate vectors for
    each type of CPU present in the system. The generated sequences already
    do some of the early guest-exit work (i.e. saving registers). To avoid
    duplication the generated vectors branch to the original vector just
    after the preamble. This size is hard coded.
    
    Adding new instructions to the HYP vector causes strange side effects,
    which are difficult to debug as the affected code is patched in at
    runtime.
    
    Add KVM_VECTOR_PREAMBLE to tell kvm_patch_vector_branch() how big
    the preamble is. The valid_vect macro can then validate this at
    build time.
    
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 2ca437ef59fa..388e1b520618 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -30,6 +30,12 @@
 	{ARM_EXCEPTION_TRAP, 		"TRAP"		},	\
 	{ARM_EXCEPTION_HYP_GONE,	"HYP_GONE"	}
 
+/*
+ * Size of the HYP vectors preamble. kvm_patch_vector_branch() generates code
+ * that jumps over this.
+ */
+#define KVM_VECTOR_PREAMBLE	(1 * AARCH64_INSN_SIZE)
+
 #ifndef __ASSEMBLY__
 
 #include <linux/mm.h>

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index ff73f5462aca..2ca437ef59fa 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -1,18 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012,2013 - ARM Ltd
  * Author: Marc Zyngier <marc.zyngier@arm.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #ifndef __ARM_KVM_ASM_H__

commit 630a16854d2d28d13e96ff27ab43cc5caa4609fc
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 20:22:11 2019 +0100

    arm64: KVM: Encapsulate kvm_cpu_context in kvm_host_data
    
    The virt/arm core allocates a kvm_cpu_context_t percpu, at present this is
    a typedef to kvm_cpu_context and is used to store host cpu context. The
    kvm_cpu_context structure is also used elsewhere to hold vcpu context.
    In order to use the percpu to hold additional future host information we
    encapsulate kvm_cpu_context in a new structure and rename the typedef and
    percpu to match.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index f5b79e995f40..ff73f5462aca 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -108,7 +108,8 @@ extern u32 __kvm_get_mdcr_el2(void);
 .endm
 
 .macro get_host_ctxt reg, tmp
-	hyp_adr_this_cpu \reg, kvm_host_cpu_state, \tmp
+	hyp_adr_this_cpu \reg, kvm_host_data, \tmp
+	add	\reg, \reg, #HOST_DATA_CONTEXT
 .endm
 
 .macro get_vcpu_ptr vcpu, ctxt

commit 58466766cd35754a061414c0c93225db2962948e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Dec 19 08:28:38 2018 +0000

    arm/arm64: KVM: Add ARM_EXCEPTION_IS_TRAP macro
    
    32 and 64bit use different symbols to identify the traps.
    32bit has a fine grained approach (prefetch abort, data abort and HVC),
    while 64bit is pretty happy with just "trap".
    
    This has been fine so far, except that we now need to decode some
    of that in tracepoints that are common to both architectures.
    
    Introduce ARM_EXCEPTION_IS_TRAP which abstracts the trap symbols
    and make the tracepoint use it.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index b2e12c99db7d..f5b79e995f40 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -25,6 +25,7 @@
 
 #define ARM_EXIT_WITH_SERROR_BIT  31
 #define ARM_EXCEPTION_CODE(x)	  ((x) & ~(1U << ARM_EXIT_WITH_SERROR_BIT))
+#define ARM_EXCEPTION_IS_TRAP(x)  (ARM_EXCEPTION_CODE((x)) == ARM_EXCEPTION_TRAP)
 #define ARM_SERROR_PENDING(x)	  !!((x) & (1U << ARM_EXIT_WITH_SERROR_BIT))
 
 #define ARM_EXCEPTION_IRQ	  0

commit 71a7e47f39a2fb971ef9934e98ba089581eff641
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Mon Dec 3 21:31:24 2018 +0100

    KVM: arm/arm64: Fixup the kvm_exit tracepoint
    
    The kvm_exit tracepoint strangely always reported exits as being IRQs.
    This seems to be because either the __print_symbolic or the tracepoint
    macros use a variable named idx.
    
    Take this chance to update the fields in the tracepoint to reflect the
    concepts in the arm64 architecture that we pass to the tracepoint and
    move the exception type table to the same location and header files as
    the exits code.
    
    We also clear out the exception code to 0 for IRQ exits (which
    translates to UNKNOWN in text) to make it slighyly less confusing to
    parse the trace output.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index aea01a09eb94..b2e12c99db7d 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -34,6 +34,12 @@
 /* The hyp-stub will return this for any kvm_call_hyp() call */
 #define ARM_EXCEPTION_HYP_GONE	  HVC_STUB_ERR
 
+#define kvm_arm_exception_type					\
+	{ARM_EXCEPTION_IRQ,		"IRQ"		},	\
+	{ARM_EXCEPTION_EL1_SERROR, 	"SERROR"	},	\
+	{ARM_EXCEPTION_TRAP, 		"TRAP"		},	\
+	{ARM_EXCEPTION_HYP_GONE,	"HYP_GONE"	}
+
 #ifndef __ASSEMBLY__
 
 #include <linux/mm.h>

commit e4e11cc0f81ee7be17d6f6fb96128a6d51c0e838
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Wed Oct 17 20:21:16 2018 +0200

    KVM: arm64: Safety check PSTATE when entering guest and handle IL
    
    This commit adds a paranoid check when entering the guest to make sure
    we don't attempt running guest code in an equally or more privilged mode
    than the hypervisor.  We also catch other accidental programming of the
    SPSR_EL2 which results in an illegal exception return and report this
    safely back to the user.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 0b53c72e7591..aea01a09eb94 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -30,6 +30,7 @@
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_EL1_SERROR  1
 #define ARM_EXCEPTION_TRAP	  2
+#define ARM_EXCEPTION_IL	  3
 /* The hyp-stub will return this for any kvm_call_hyp() call */
 #define ARM_EXCEPTION_HYP_GONE	  HVC_STUB_ERR
 

commit 7665f3a8491b0ed3c6f65c0bc3a5424ea8f87731
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:43 2018 +0100

    kvm: arm64: Configure VTCR_EL2 per VM
    
    Add support for setting the VTCR_EL2 per VM, rather than hard
    coding a value at boot time per CPU. This would allow us to tune
    the stage2 page table parameters per VM in the later changes.
    
    We compute the VTCR fields based on the system wide sanitised
    feature registers, except for the hardware management of Access
    Flags (VTCR_EL2.HA). It is fine to run a system with a mix of
    CPUs that may or may not update the page table Access Flags.
    Since the bit is RES0 on CPUs that don't support it, the bit
    should be ignored on them.
    
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 102b5a5c47b6..0b53c72e7591 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -72,8 +72,6 @@ extern void __vgic_v3_init_lrs(void);
 
 extern u32 __kvm_get_mdcr_el2(void);
 
-extern u32 __init_stage2_translation(void);
-
 /* Home-grown __this_cpu_{ptr,read} variants that always work at HYP */
 #define __hyp_this_cpu_ptr(sym)						\
 	({								\

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit 38500be10e46c09e20b1a0e9c6cb94e1f0a86610
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sat Jun 2 10:42:54 2018 +0100

    arm64: KVM: Move VCPU_WORKAROUND_2_FLAG macros to the top of the file
    
    This is to avoid potential merging conflicts between commit 55e3748e8902
    ("arm64: KVM: Add ARCH_WORKAROUND_2 support for guests") and the KVM
    tree.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index d4fbb1356c4c..951b2076a5e2 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -20,6 +20,9 @@
 
 #include <asm/virt.h>
 
+#define	VCPU_WORKAROUND_2_FLAG_SHIFT	0
+#define	VCPU_WORKAROUND_2_FLAG		(_AC(1, UL) << VCPU_WORKAROUND_2_FLAG_SHIFT)
+
 #define ARM_EXIT_WITH_SERROR_BIT  31
 #define ARM_EXCEPTION_CODE(x)	  ((x) & ~(1U << ARM_EXIT_WITH_SERROR_BIT))
 #define ARM_SERROR_PENDING(x)	  !!((x) & (1U << ARM_EXIT_WITH_SERROR_BIT))
@@ -33,9 +36,6 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
-#define	VCPU_WORKAROUND_2_FLAG_SHIFT	0
-#define	VCPU_WORKAROUND_2_FLAG		(_AC(1, UL) << VCPU_WORKAROUND_2_FLAG_SHIFT)
-
 /* Translate a kernel address of @sym into its equivalent linear mapping */
 #define kvm_ksym_ref(sym)						\
 	({								\

commit 55e3748e8902ff641e334226bdcb432f9a5d78d3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:16 2018 +0100

    arm64: KVM: Add ARCH_WORKAROUND_2 support for guests
    
    In order to offer ARCH_WORKAROUND_2 support to guests, we need
    a bit of infrastructure.
    
    Let's add a flag indicating whether or not the guest uses
    SSBD mitigation. Depending on the state of this flag, allow
    KVM to disable ARCH_WORKAROUND_2 before entering the guest,
    and enable it when exiting it.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index fefd8cf42c35..d4fbb1356c4c 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -33,6 +33,9 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
+#define	VCPU_WORKAROUND_2_FLAG_SHIFT	0
+#define	VCPU_WORKAROUND_2_FLAG		(_AC(1, UL) << VCPU_WORKAROUND_2_FLAG_SHIFT)
+
 /* Translate a kernel address of @sym into its equivalent linear mapping */
 #define kvm_ksym_ref(sym)						\
 	({								\

commit 85478bab409171de501b719971fd25a3d5d639f9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:15 2018 +0100

    arm64: KVM: Add HYP per-cpu accessors
    
    As we're going to require to access per-cpu variables at EL2,
    let's craft the minimum set of accessors required to implement
    reading a per-cpu variable, relying on tpidr_el2 to contain the
    per-cpu offset.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index f6648a3e4152..fefd8cf42c35 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -71,14 +71,37 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern u32 __init_stage2_translation(void);
 
+/* Home-grown __this_cpu_{ptr,read} variants that always work at HYP */
+#define __hyp_this_cpu_ptr(sym)						\
+	({								\
+		void *__ptr = hyp_symbol_addr(sym);			\
+		__ptr += read_sysreg(tpidr_el2);			\
+		(typeof(&sym))__ptr;					\
+	 })
+
+#define __hyp_this_cpu_read(sym)					\
+	({								\
+		*__hyp_this_cpu_ptr(sym);				\
+	 })
+
 #else /* __ASSEMBLY__ */
 
-.macro get_host_ctxt reg, tmp
-	adr_l	\reg, kvm_host_cpu_state
+.macro hyp_adr_this_cpu reg, sym, tmp
+	adr_l	\reg, \sym
 	mrs	\tmp, tpidr_el2
 	add	\reg, \reg, \tmp
 .endm
 
+.macro hyp_ldr_this_cpu reg, sym, tmp
+	adr_l	\reg, \sym
+	mrs	\tmp, tpidr_el2
+	ldr	\reg,  [\reg, \tmp]
+.endm
+
+.macro get_host_ctxt reg, tmp
+	hyp_adr_this_cpu \reg, kvm_host_cpu_state, \tmp
+.endm
+
 .macro get_vcpu_ptr vcpu, ctxt
 	get_host_ctxt \ctxt, \vcpu
 	ldr	\vcpu, [\ctxt, #HOST_CONTEXT_VCPU]

commit fa89d31c53061139bd66f9de6e55340ac7bd5480
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue May 8 14:47:23 2018 +0100

    KVM: arm64: Repurpose vcpu_arch.debug_flags for general-purpose flags
    
    In struct vcpu_arch, the debug_flags field is used to store
    debug-related flags about the vcpu state.
    
    Since we are about to add some more flags related to FPSIMD and
    SVE, it makes sense to add them to the existing flags field rather
    than adding new fields.  Since there is only one debug_flags flag
    defined so far, there is plenty of free space for expansion.
    
    In preparation for adding more flags, this patch renames the
    debug_flags field to simply "flags", and updates comments
    appropriately.
    
    The flag definitions are also moved to <asm/kvm_host.h>, since
    their presence in <asm/kvm_asm.h> was for purely historical
    reasons:  these definitions are not used from asm any more, and not
    very likely to be as more Hyp asm is migrated to C.
    
    KVM_ARM64_DEBUG_DIRTY_SHIFT has not been used since commit
    1ea66d27e7b0 ("arm64: KVM: Move away from the assembly version of
    the world switch"), so this patch gets rid of that too.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Alex Benn√©e <alex.bennee@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    [maz: fixed minor conflict]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index a9ceeec5a76f..821a7032c0f7 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -30,9 +30,6 @@
 /* The hyp-stub will return this for any kvm_call_hyp() call */
 #define ARM_EXCEPTION_HYP_GONE	  HVC_STUB_ERR
 
-#define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
-#define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
-
 #ifndef __ASSEMBLY__
 
 #include <linux/mm.h>

commit 46c4a30b0b1638c9b87dfea41436c5699e9fe24e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu May 10 12:13:47 2018 +0100

    arm64: KVM: Use lm_alias() for kvm_ksym_ref()
    
    For historical reasons, we open-code lm_alias() in kvm_ksym_ref().
    
    Let's use lm_alias() to avoid duplication and make things clearer.
    
    As we have to pull this from <linux/mm.h> (which is not safe for
    inclusion in assembly), we may as well move the kvm_ksym_ref()
    definition into the existing !__ASSEMBLY__ block.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index f6648a3e4152..a9ceeec5a76f 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -33,16 +33,19 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
+#ifndef __ASSEMBLY__
+
+#include <linux/mm.h>
+
 /* Translate a kernel address of @sym into its equivalent linear mapping */
 #define kvm_ksym_ref(sym)						\
 	({								\
 		void *val = &sym;					\
 		if (!is_kernel_in_hyp_mode())				\
-			val = phys_to_virt((u64)&sym - kimage_voffset);	\
+			val = lm_alias(&sym);				\
 		val;							\
 	 })
 
-#ifndef __ASSEMBLY__
 struct kvm;
 struct kvm_vcpu;
 

commit 4bc352ffb39e4eec253e70f8c076f2f48a6c1926
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Tue Apr 10 11:36:42 2018 +0100

    arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening
    
    The function SMCCC_ARCH_WORKAROUND_1 was introduced as part of SMC
    V1.1 Calling Convention to mitigate CVE-2017-5715. This patch uses
    the standard call SMCCC_ARCH_WORKAROUND_1 for Falkor chips instead
    of Silicon provider service ID 0xC2001700.
    
    Cc: <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    [maz: reworked errata framework integration]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index d53d40704416..f6648a3e4152 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -71,8 +71,6 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern u32 __init_stage2_translation(void);
 
-extern void __qcom_hyp_sanitize_btac_predictors(void);
-
 #else /* __ASSEMBLY__ */
 
 .macro get_host_ctxt reg, tmp

commit adc91ab7854195f107c137aa197ddfe8b82a2331
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Mar 28 11:59:13 2018 +0100

    Revert "arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening"
    
    Creates far too many conflicts with arm64/for-next/core, to be
    resent post -rc1.
    
    This reverts commit f9f5dc19509bbef6f5e675346f1a7d7b846bdb12.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index f6648a3e4152..d53d40704416 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -71,6 +71,8 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern u32 __init_stage2_translation(void);
 
+extern void __qcom_hyp_sanitize_btac_predictors(void);
+
 #else /* __ASSEMBLY__ */
 
 .macro get_host_ctxt reg, tmp

commit f9f5dc19509bbef6f5e675346f1a7d7b846bdb12
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Mon Mar 5 11:06:43 2018 -0600

    arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening
    
    The function SMCCC_ARCH_WORKAROUND_1 was introduced as part of SMC
    V1.1 Calling Convention to mitigate CVE-2017-5715. This patch uses
    the standard call SMCCC_ARCH_WORKAROUND_1 for Falkor chips instead
    of Silicon provider service ID 0xC2001700.
    
    Cc: <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index d53d40704416..f6648a3e4152 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -71,8 +71,6 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern u32 __init_stage2_translation(void);
 
-extern void __qcom_hyp_sanitize_btac_predictors(void);
-
 #else /* __ASSEMBLY__ */
 
 .macro get_host_ctxt reg, tmp

commit 3f5c90b890acfa7ad0b817a67cfc5eaaf0e21f7d
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Oct 3 14:02:12 2017 +0200

    KVM: arm64: Introduce VHE-specific kvm_vcpu_run
    
    So far this is mostly (see below) a copy of the legacy non-VHE switch
    function, but we will start reworking these functions in separate
    directions to work on VHE and non-VHE in the most optimal way in later
    patches.
    
    The only difference after this patch between the VHE and non-VHE run
    functions is that we omit the branch-predictor variant-2 hardening for
    QC Falkor CPUs, because this workaround is specific to a series of
    non-VHE ARMv8.0 CPUs.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 7149f1520382..d53d40704416 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -58,7 +58,9 @@ extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);
 
 extern void __kvm_timer_set_cntvoff(u32 cntvoff_low, u32 cntvoff_high);
 
-extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
+extern int kvm_vcpu_run_vhe(struct kvm_vcpu *vcpu);
+
+extern int __kvm_vcpu_run_nvhe(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
 extern u64 __vgic_v3_read_vmcr(void);

commit 4464e210de9e80e38de59df052fe09ea2ff80b1b
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Sun Oct 8 17:01:56 2017 +0200

    KVM: arm64: Avoid storing the vcpu pointer on the stack
    
    We already have the percpu area for the host cpu state, which points to
    the VCPU, so there's no need to store the VCPU pointer on the stack on
    every context switch.  We can be a little more clever and just use
    tpidr_el2 for the percpu offset and load the VCPU pointer from the host
    context.
    
    This has the benefit of being able to retrieve the host context even
    when our stack is corrupted, and it has a potential performance benefit
    because we trade a store plus a load for an mrs and a load on a round
    trip to the guest.
    
    This does require us to calculate the percpu offset without including
    the offset from the kernel mapping of the percpu array to the linear
    mapping of the array (which is what we store in tpidr_el1), because a
    PC-relative generated address in EL2 is already giving us the hyp alias
    of the linear mapping of a kernel address.  We do this in
    __cpu_init_hyp_mode() by using kvm_ksym_ref().
    
    The code that accesses ESR_EL2 was previously using an alternative to
    use the _EL1 accessor on VHE systems, but this was actually unnecessary
    as the _EL1 accessor aliases the ESR_EL2 register on VHE, and the _EL2
    accessor does the same thing on both systems.
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 24961b732e65..7149f1520382 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -33,6 +33,7 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
+/* Translate a kernel address of @sym into its equivalent linear mapping */
 #define kvm_ksym_ref(sym)						\
 	({								\
 		void *val = &sym;					\
@@ -70,6 +71,20 @@ extern u32 __init_stage2_translation(void);
 
 extern void __qcom_hyp_sanitize_btac_predictors(void);
 
+#else /* __ASSEMBLY__ */
+
+.macro get_host_ctxt reg, tmp
+	adr_l	\reg, kvm_host_cpu_state
+	mrs	\tmp, tpidr_el2
+	add	\reg, \reg, \tmp
+.endm
+
+.macro get_vcpu_ptr vcpu, ctxt
+	get_host_ctxt \ctxt, \vcpu
+	ldr	\vcpu, [\ctxt, #HOST_CONTEXT_VCPU]
+	kern_hyp_va	\vcpu
+.endm
+
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */

commit ec82b567a74fbdffdf418d4bb381d55f6a9096af
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Fri Jan 5 14:28:59 2018 -0600

    arm64: Implement branch predictor hardening for Falkor
    
    Falkor is susceptible to branch predictor aliasing and can
    theoretically be attacked by malicious code. This patch
    implements a mitigation for these attacks, preventing any
    malicious entries from affecting other victim contexts.
    
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    [will: fix label name when !CONFIG_KVM and remove references to MIDR_FALKOR]
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index ab4d0a926043..24961b732e65 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -68,6 +68,8 @@ extern u32 __kvm_get_mdcr_el2(void);
 
 extern u32 __init_stage2_translation(void);
 
+extern void __qcom_hyp_sanitize_btac_predictors(void);
+
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */

commit 688c50aa72f64ca21767486e5eef876ec23e418c
Author: Christoffer Dall <cdall@linaro.org>
Date:   Wed Jan 4 16:10:28 2017 +0100

    KVM: arm/arm64: Move timer save/restore out of the hyp code
    
    As we are about to be lazy with saving and restoring the timer
    registers, we prepare by moving all possible timer configuration logic
    out of the hyp code.  All virtual timer registers can be programmed from
    EL1 and since the arch timer is always a level triggered interrupt we
    can safely do this with interrupts disabled in the host kernel on the
    way to the guest without taking vtimer interrupts in the host kernel
    (yet).
    
    The downside is that the cntvoff register can only be programmed from
    hyp mode, so we jump into hyp mode and back to program it.  This is also
    safe, because the host kernel doesn't use the virtual timer in the KVM
    code.  It may add a little performance performance penalty, but only
    until following commits where we move this operation to vcpu load/put.
    
    Signed-off-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 26a64d0f9ab9..ab4d0a926043 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -55,6 +55,8 @@ extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 extern void __kvm_tlb_flush_vmid(struct kvm *kvm);
 extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);
 
+extern void __kvm_timer_set_cntvoff(u32 cntvoff_low, u32 cntvoff_high);
+
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);

commit 4adb1341c7ef68af54732ef11f69faedffa6acb7
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:43 2017 +0100

    arm64: KVM: Convert __cpu_reset_hyp_mode to using __hyp_reset_vectors
    
    We are now able to use the hyp stub to reset HYP mode. Time to
    kiss __kvm_hyp_reset goodbye, and use __hyp_reset_vectors.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index b7e4ef5fcc41..26a64d0f9ab9 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -47,7 +47,6 @@ struct kvm_vcpu;
 
 extern char __kvm_hyp_init[];
 extern char __kvm_hyp_init_end[];
-extern char __kvm_hyp_reset[];
 
 extern char __kvm_hyp_vector[];
 

commit 4993fdcf399f59ed56a16ecfedf8a61066198816
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:37 2017 +0100

    arm64: hyp-stub: Define a return value for failed stub calls
    
    Define a standard return value to be returned when a hyp stub
    call fails, and make KVM use it for ARM_EXCEPTION_HYP_GONE
    (instead of using a KVM-specific value).
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 49f99cd02613..b7e4ef5fcc41 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -28,7 +28,7 @@
 #define ARM_EXCEPTION_EL1_SERROR  1
 #define ARM_EXCEPTION_TRAP	  2
 /* The hyp-stub will return this for any kvm_call_hyp() call */
-#define ARM_EXCEPTION_HYP_GONE	  3
+#define ARM_EXCEPTION_HYP_GONE	  HVC_STUB_ERR
 
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)

commit 328e566479449194979d64685ae6d74c989599bb
Author: Christoffer Dall <cdall@cs.columbia.edu>
Date:   Thu Mar 24 11:21:04 2016 +0100

    KVM: arm/arm64: vgic: Defer touching GICH_VMCR to vcpu_load/put
    
    We don't have to save/restore the VMCR on every entry to/from the guest,
    since on GICv2 we can access the control interface from EL1 and on VHE
    systems with GICv3 we can access the control interface from KVM running
    in EL2.
    
    GICv3 systems without VHE becomes the rare case, which has to
    save/restore the register on each round trip.
    
    Note that userspace accesses may see out-of-date values if the VCPU is
    running while accessing the VGIC state via the KVM device API, but this
    is already the case and it is up to userspace to quiesce the CPUs before
    reading the CPU registers from the GIC for an up-to-date view.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@cs.columbia.edu>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index ec3553eb9349..49f99cd02613 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -59,6 +59,8 @@ extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
+extern u64 __vgic_v3_read_vmcr(void);
+extern void __vgic_v3_write_vmcr(u32 vmcr);
 extern void __vgic_v3_init_lrs(void);
 
 extern u32 __kvm_get_mdcr_el2(void);

commit 94d0e5980d6791b9f98a9b6c586c1f7cb76b2178
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Oct 18 18:37:49 2016 +0100

    arm/arm64: KVM: Perform local TLB invalidation when multiplexing vcpus on a single CPU
    
    Architecturally, TLBs are private to the (physical) CPU they're
    associated with. But when multiple vcpus from the same VM are
    being multiplexed on the same CPU, the TLBs are not private
    to the vcpus (and are actually shared across the VMID).
    
    Let's consider the following scenario:
    
    - vcpu-0 maps PA to VA
    - vcpu-1 maps PA' to VA
    
    If run on the same physical CPU, vcpu-1 can hit TLB entries generated
    by vcpu-0 accesses, and access the wrong physical page.
    
    The solution to this is to keep a per-VM map of which vcpu ran last
    on each given physical CPU, and invalidate local TLBs when switching
    to a different vcpu from the same VM.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 18f746551bf6..ec3553eb9349 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -54,6 +54,7 @@ extern char __kvm_hyp_vector[];
 extern void __kvm_flush_vm_context(void);
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 extern void __kvm_tlb_flush_vmid(struct kvm *kvm);
+extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 

commit 20163403a1f2fe3f00f59df2b45ea5cb74b85d97
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 14:02:05 2016 +0100

    arm64: KVM: Allow an exit code to be tagged with an SError
    
    Similarily to EL1, an asynchronous abort can be triggered whilst
    running at EL2. But instead of making that a new error code,
    we need to communicate it to the rest of KVM together with
    the exit reason. So let's hijack a single bit that allows the
    exception code to be tagged with a "pending SError" information.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index d177e7e115f4..18f746551bf6 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -20,6 +20,10 @@
 
 #include <asm/virt.h>
 
+#define ARM_EXIT_WITH_SERROR_BIT  31
+#define ARM_EXCEPTION_CODE(x)	  ((x) & ~(1U << ARM_EXIT_WITH_SERROR_BIT))
+#define ARM_SERROR_PENDING(x)	  !!((x) & (1U << ARM_EXIT_WITH_SERROR_BIT))
+
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_EL1_SERROR  1
 #define ARM_EXCEPTION_TRAP	  2

commit 9aecafc86ca2f4f4d7f5cbaf674f5df244221c51
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 14:02:02 2016 +0100

    arm64: KVM: Add exception code to report EL1 asynchronous aborts
    
    So far, we don't have a code to indicate that we've taken an
    asynchronous abort from EL1. Let's add one.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 7561f63f1c28..d177e7e115f4 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -21,9 +21,10 @@
 #include <asm/virt.h>
 
 #define ARM_EXCEPTION_IRQ	  0
-#define ARM_EXCEPTION_TRAP	  1
+#define ARM_EXCEPTION_EL1_SERROR  1
+#define ARM_EXCEPTION_TRAP	  2
 /* The hyp-stub will return this for any kvm_call_hyp() call */
-#define ARM_EXCEPTION_HYP_GONE	  2
+#define ARM_EXCEPTION_HYP_GONE	  3
 
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)

commit be092017b6ffbd013f481f915632db6aa9fc3ca3
Merge: fb6363e9f4ee e6d9a5254333
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 17:17:24 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
    
     - virt_to_page/page_address optimisations
    
     - support for NUMA systems described using device-tree
    
     - support for hibernate/suspend-to-disk
    
     - proper support for maxcpus= command line parameter
    
     - detection and graceful handling of AArch64-only CPUs
    
     - miscellaneous cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      arm64: do not enforce strict 16 byte alignment to stack pointer
      arm64: kernel: Fix incorrect brk randomization
      arm64: cpuinfo: Missing NULL terminator in compat_hwcap_str
      arm64: secondary_start_kernel: Remove unnecessary barrier
      arm64: Ensure pmd_present() returns false after pmd_mknotpresent()
      arm64: Replace hard-coded values in the pmd/pud_bad() macros
      arm64: Implement pmdp_set_access_flags() for hardware AF/DBM
      arm64: Fix typo in the pmdp_huge_get_and_clear() definition
      arm64: mm: remove unnecessary EXPORT_SYMBOL_GPL
      arm64: always use STRICT_MM_TYPECHECKS
      arm64: kvm: Fix kvm teardown for systems using the extended idmap
      arm64: kaslr: increase randomization granularity
      arm64: kconfig: drop CONFIG_RTC_LIB dependency
      arm64: make ARCH_SUPPORTS_DEBUG_PAGEALLOC depend on !HIBERNATION
      arm64: hibernate: Refuse to hibernate if the boot cpu is offline
      arm64: kernel: Add support for hibernate/suspend-to-disk
      PM / Hibernate: Call flush_icache_range() on pages restored in-place
      arm64: Add new asm macro copy_page
      arm64: Promote KERNEL_START/KERNEL_END definitions to a header file
      arm64: kernel: Include _AC definition in page.h
      ...

commit 67f6919766620e7ea7aab11a6a3470dc7b451359
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 27 17:47:05 2016 +0100

    arm64: kvm: allows kvm cpu hotplug
    
    The current kvm implementation on arm64 does cpu-specific initialization
    at system boot, and has no way to gracefully shutdown a core in terms of
    kvm. This prevents kexec from rebooting the system at EL2.
    
    This patch adds a cpu tear-down function and also puts an existing cpu-init
    code into a separate function, kvm_arch_hardware_disable() and
    kvm_arch_hardware_enable() respectively.
    We don't need the arm64 specific cpu hotplug hook any more.
    
    Since this patch modifies common code between arm and arm64, one stub
    definition, __cpu_reset_hyp_mode(), is added on arm side to avoid
    compilation errors.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [Rebase, added separate VHE init/exit path, changed resets use of
     kvm_call_hyp() to the __version, en/disabled hardware in init_subsystems(),
     added icache maintenance to __kvm_hyp_reset() and removed lr restore, removed
     guest-enter after teardown handling]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index dbfbc5fb5d35..a88da136f332 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -42,6 +42,7 @@ struct kvm_vcpu;
 
 extern char __kvm_hyp_init[];
 extern char __kvm_hyp_init_end[];
+extern char __kvm_hyp_reset[];
 
 extern char __kvm_hyp_vector[];
 

commit c94b0cf28281d483c8b43b4874fcb7ab14ade1b1
Author: James Morse <james.morse@arm.com>
Date:   Wed Apr 27 17:47:04 2016 +0100

    arm64: hyp/kvm: Make hyp-stub reject kvm_call_hyp()
    
    A later patch implements kvm_arch_hardware_disable(), to remove kvm
    from el2, and re-instate the hyp-stub.
    
    This can happen while guests are running, particularly when kvm_reboot()
    calls kvm_arch_hardware_disable() on each cpu. This can interrupt a guest,
    remove kvm, then allow the guest to be scheduled again. This causes
    kvm_call_hyp() to be run against the hyp-stub.
    
    Change the hyp-stub to return a new exception type when this happens,
    and add code to kvm's handle_exit() to tell userspace we failed to
    enter the guest.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index eb7490d232a0..dbfbc5fb5d35 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -22,6 +22,8 @@
 
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_TRAP	  1
+/* The hyp-stub will return this for any kvm_call_hyp() call */
+#define ARM_EXCEPTION_HYP_GONE	  2
 
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)

commit 6141570c36f0c937d5deff20d9cf08cbd8d8ed48
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 5 16:11:47 2016 +0100

    arm64: KVM: Warn when PARange is less than 40 bits
    
    We always thought that 40bits of PA range would be the minimum people
    would actually build. Anything less is terrifyingly small.
    
    Turns out that we were both right and wrong. Nobody has ever built
    such a system, but the ARM Foundation Model has a PARange set to 36bits.
    Just because we can. Oh well. Now, the KVM API explicitely says that
    we offer a 40bit PA space to the VM, so we shouldn't run KVM on
    the Foundation Model at all.
    
    That being said, this patch offers a less agressive alternative, and
    loudly warns about the configuration being unsupported. You'll still
    be able to run VMs (at your own risks, though).
    
    This is just a workaround until we have a proper userspace API where
    we report the PARange to userspace.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index eb7490d232a0..40a0a24e6c98 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -54,7 +54,7 @@ extern void __vgic_v3_init_lrs(void);
 
 extern u32 __kvm_get_mdcr_el2(void);
 
-extern void __init_stage2_translation(void);
+extern u32 __init_stage2_translation(void);
 
 #endif
 

commit 2510ffe17f9707eb96cf286cf5d11ad372ff679f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Mar 18 17:25:59 2016 +0000

    arm64: KVM: Turn kvm_ksym_ref into a NOP on VHE
    
    When running with VHE, there is no need to translate kernel pointers
    to the EL2 memory space, since we're already there (and we have a much
    saner memory map to start with).
    
    Unfortunately, kvm_ksym_ref is getting in the way, and the first
    call into the "hypervisor" section is going to end up in fireworks,
    since we're now branching into nowhereland. Meh.
    
    A potential solution is to test if VHE is engaged or not, and only
    perform the translation in the negative case. With this in place,
    VHE is able to run again.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 226f49d69ea9..eb7490d232a0 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -26,7 +26,13 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
-#define kvm_ksym_ref(sym)		phys_to_virt((u64)&sym - kimage_voffset)
+#define kvm_ksym_ref(sym)						\
+	({								\
+		void *val = &sym;					\
+		if (!is_kernel_in_hyp_mode())				\
+			val = phys_to_virt((u64)&sym - kimage_voffset);	\
+		val;							\
+	 })
 
 #ifndef __ASSEMBLY__
 struct kvm;

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit 0d98d00b8d80bfdee95cf7e85f20f107377e2662
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Mar 3 15:43:58 2016 +0000

    arm64: KVM: vgic-v3: Reset LRs at boot time
    
    In order to let the GICv3 code be more lazy in the way it
    accesses the LRs, it is necessary to start with a clean slate.
    
    Let's reset the LRs on each CPU when the vgic is probed (which
    includes a round trip to EL2...).
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 1037392ae134..2d02ba67478c 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -42,6 +42,7 @@ extern void __kvm_tlb_flush_vmid(struct kvm *kvm);
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
+extern void __vgic_v3_init_lrs(void);
 
 extern u32 __kvm_get_mdcr_el2(void);
 

commit 3a3604bc5eb4ae21ec95b13fdd15959e8f70c434
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jan 29 13:19:45 2015 +0000

    arm64: KVM: Switch to C-based stage2 init
    
    There is no real need to leave the stage2 initialization as part
    of the early HYP bootstrap, and we can easily postpone it to
    the point where we can safely run C code.
    
    This will help VHE, which doesn't need any of this bootstrap.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 2ad8930e7eb3..1037392ae134 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -45,6 +45,8 @@ extern u64 __vgic_v3_get_ich_vtr_el2(void);
 
 extern u32 __kvm_get_mdcr_el2(void);
 
+extern void __init_stage2_translation(void);
+
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */

commit 42428525a9eefea9dda68de684381ce9f3dc4266
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sat Jan 2 14:04:48 2016 +0000

    ARM: KVM: Remove __kvm_hyp_code_start/__kvm_hyp_code_end
    
    Now that we've unified the way we refer to the HYP text between
    arm and arm64, drop __kvm_hyp_code_start/end, and just use the
    __hyp_text_start/end symbols.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 52b777b7d407..2ad8930e7eb3 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -35,9 +35,6 @@ extern char __kvm_hyp_init_end[];
 
 extern char __kvm_hyp_vector[];
 
-#define	__kvm_hyp_code_start	__hyp_text_start
-#define	__kvm_hyp_code_end	__hyp_text_end
-
 extern void __kvm_flush_vm_context(void);
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 extern void __kvm_tlb_flush_vmid(struct kvm *kvm);

commit a7f8de168ace487fa7b88cb154e413cf40e87fc6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:42 2016 +0100

    arm64: allow kernel Image to be loaded anywhere in physical memory
    
    This relaxes the kernel Image placement requirements, so that it
    may be placed at any 2 MB aligned offset in physical memory.
    
    This is accomplished by ignoring PHYS_OFFSET when installing
    memblocks, and accounting for the apparent virtual offset of
    the kernel Image. As a result, virtual address references
    below PAGE_OFFSET are correctly mapped onto physical references
    into the kernel Image regardless of where it sits in memory.
    
    Special care needs to be taken for dealing with memory limits passed
    via mem=, since the generic implementation clips memory top down, which
    may clip the kernel image itself if it is loaded high up in memory. To
    deal with this case, we simply add back the memory covering the kernel
    image, which may result in more memory to be retained than was passed
    as a mem= parameter.
    
    Since mem= should not be considered a production feature, a panic notifier
    handler is installed that dumps the memory limit at panic time if one was
    set.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 31b56008f412..054ac25e7c2e 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -26,24 +26,9 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
-#define kvm_ksym_ref(sym)		((void *)&sym + kvm_ksym_shift)
+#define kvm_ksym_ref(sym)		phys_to_virt((u64)&sym - kimage_voffset)
 
 #ifndef __ASSEMBLY__
-#if __GNUC__ > 4
-#define kvm_ksym_shift			(PAGE_OFFSET - KIMAGE_VADDR)
-#else
-/*
- * GCC versions 4.9 and older will fold the constant below into the addend of
- * the reference to 'sym' above if kvm_ksym_shift is declared static or if the
- * constant is used directly. However, since we use the small code model for
- * the core kernel, the reference to 'sym' will be emitted as a adrp/add pair,
- * with a +/- 4 GB range, resulting in linker relocation errors if the shift
- * is sufficiently large. So prevent the compiler from folding the shift into
- * the addend, by making the shift a variable with external linkage.
- */
-__weak u64 kvm_ksym_shift = PAGE_OFFSET - KIMAGE_VADDR;
-#endif
-
 struct kvm;
 struct kvm_vcpu;
 

commit a0bf9776cd0be4490d4675d4108e13379849fc7f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:39 2016 +0100

    arm64: kvm: deal with kernel symbols outside of linear mapping
    
    KVM on arm64 uses a fixed offset between the linear mapping at EL1 and
    the HYP mapping at EL2. Before we can move the kernel virtual mapping
    out of the linear mapping, we have to make sure that references to kernel
    symbols that are accessed via the HYP mapping are translated to their
    linear equivalent.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 52b777b7d407..31b56008f412 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -26,7 +26,24 @@
 #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
 #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
 
+#define kvm_ksym_ref(sym)		((void *)&sym + kvm_ksym_shift)
+
 #ifndef __ASSEMBLY__
+#if __GNUC__ > 4
+#define kvm_ksym_shift			(PAGE_OFFSET - KIMAGE_VADDR)
+#else
+/*
+ * GCC versions 4.9 and older will fold the constant below into the addend of
+ * the reference to 'sym' above if kvm_ksym_shift is declared static or if the
+ * constant is used directly. However, since we use the small code model for
+ * the core kernel, the reference to 'sym' will be emitted as a adrp/add pair,
+ * with a +/- 4 GB range, resulting in linker relocation errors if the shift
+ * is sufficiently large. So prevent the compiler from folding the shift into
+ * the addend, by making the shift a variable with external linkage.
+ */
+__weak u64 kvm_ksym_shift = PAGE_OFFSET - KIMAGE_VADDR;
+#endif
+
 struct kvm;
 struct kvm_vcpu;
 

commit 9d8415d6c148a16b6d906a96f0596851d7e4d607
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sun Oct 25 19:57:11 2015 +0000

    arm64: KVM: Turn system register numbers to an enum
    
    Having the system register numbers as #defines has been a pain
    since day one, as the ordering is pretty fragile, and moving
    things around leads to renumbering and epic conflict resolutions.
    
    Now that we're mostly acessing the sysreg file in C, an enum is
    a much better type to use, and we can clean things up a bit.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 5e377101f919..52b777b7d407 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -20,82 +20,6 @@
 
 #include <asm/virt.h>
 
-/*
- * 0 is reserved as an invalid value.
- * Order *must* be kept in sync with the hyp switch code.
- */
-#define	MPIDR_EL1	1	/* MultiProcessor Affinity Register */
-#define	CSSELR_EL1	2	/* Cache Size Selection Register */
-#define	SCTLR_EL1	3	/* System Control Register */
-#define	ACTLR_EL1	4	/* Auxiliary Control Register */
-#define	CPACR_EL1	5	/* Coprocessor Access Control */
-#define	TTBR0_EL1	6	/* Translation Table Base Register 0 */
-#define	TTBR1_EL1	7	/* Translation Table Base Register 1 */
-#define	TCR_EL1		8	/* Translation Control Register */
-#define	ESR_EL1		9	/* Exception Syndrome Register */
-#define	AFSR0_EL1	10	/* Auxilary Fault Status Register 0 */
-#define	AFSR1_EL1	11	/* Auxilary Fault Status Register 1 */
-#define	FAR_EL1		12	/* Fault Address Register */
-#define	MAIR_EL1	13	/* Memory Attribute Indirection Register */
-#define	VBAR_EL1	14	/* Vector Base Address Register */
-#define	CONTEXTIDR_EL1	15	/* Context ID Register */
-#define	TPIDR_EL0	16	/* Thread ID, User R/W */
-#define	TPIDRRO_EL0	17	/* Thread ID, User R/O */
-#define	TPIDR_EL1	18	/* Thread ID, Privileged */
-#define	AMAIR_EL1	19	/* Aux Memory Attribute Indirection Register */
-#define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
-#define	PAR_EL1		21	/* Physical Address Register */
-#define MDSCR_EL1	22	/* Monitor Debug System Control Register */
-#define MDCCINT_EL1	23	/* Monitor Debug Comms Channel Interrupt Enable Reg */
-
-/* 32bit specific registers. Keep them at the end of the range */
-#define	DACR32_EL2	24	/* Domain Access Control Register */
-#define	IFSR32_EL2	25	/* Instruction Fault Status Register */
-#define	FPEXC32_EL2	26	/* Floating-Point Exception Control Register */
-#define	DBGVCR32_EL2	27	/* Debug Vector Catch Register */
-#define	NR_SYS_REGS	28
-
-/* 32bit mapping */
-#define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
-#define c0_CSSELR	(CSSELR_EL1 * 2)/* Cache Size Selection Register */
-#define c1_SCTLR	(SCTLR_EL1 * 2)	/* System Control Register */
-#define c1_ACTLR	(ACTLR_EL1 * 2)	/* Auxiliary Control Register */
-#define c1_CPACR	(CPACR_EL1 * 2)	/* Coprocessor Access Control */
-#define c2_TTBR0	(TTBR0_EL1 * 2)	/* Translation Table Base Register 0 */
-#define c2_TTBR0_high	(c2_TTBR0 + 1)	/* TTBR0 top 32 bits */
-#define c2_TTBR1	(TTBR1_EL1 * 2)	/* Translation Table Base Register 1 */
-#define c2_TTBR1_high	(c2_TTBR1 + 1)	/* TTBR1 top 32 bits */
-#define c2_TTBCR	(TCR_EL1 * 2)	/* Translation Table Base Control R. */
-#define c3_DACR		(DACR32_EL2 * 2)/* Domain Access Control Register */
-#define c5_DFSR		(ESR_EL1 * 2)	/* Data Fault Status Register */
-#define c5_IFSR		(IFSR32_EL2 * 2)/* Instruction Fault Status Register */
-#define c5_ADFSR	(AFSR0_EL1 * 2)	/* Auxiliary Data Fault Status R */
-#define c5_AIFSR	(AFSR1_EL1 * 2)	/* Auxiliary Instr Fault Status R */
-#define c6_DFAR		(FAR_EL1 * 2)	/* Data Fault Address Register */
-#define c6_IFAR		(c6_DFAR + 1)	/* Instruction Fault Address Register */
-#define c7_PAR		(PAR_EL1 * 2)	/* Physical Address Register */
-#define c7_PAR_high	(c7_PAR + 1)	/* PAR top 32 bits */
-#define c10_PRRR	(MAIR_EL1 * 2)	/* Primary Region Remap Register */
-#define c10_NMRR	(c10_PRRR + 1)	/* Normal Memory Remap Register */
-#define c12_VBAR	(VBAR_EL1 * 2)	/* Vector Base Address Register */
-#define c13_CID		(CONTEXTIDR_EL1 * 2)	/* Context ID Register */
-#define c13_TID_URW	(TPIDR_EL0 * 2)	/* Thread ID, User R/W */
-#define c13_TID_URO	(TPIDRRO_EL0 * 2)/* Thread ID, User R/O */
-#define c13_TID_PRIV	(TPIDR_EL1 * 2)	/* Thread ID, Privileged */
-#define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
-#define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
-#define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
-
-#define cp14_DBGDSCRext	(MDSCR_EL1 * 2)
-#define cp14_DBGBCR0	(DBGBCR0_EL1 * 2)
-#define cp14_DBGBVR0	(DBGBVR0_EL1 * 2)
-#define cp14_DBGBXVR0	(cp14_DBGBVR0 + 1)
-#define cp14_DBGWCR0	(DBGWCR0_EL1 * 2)
-#define cp14_DBGWVR0	(DBGWVR0_EL1 * 2)
-#define cp14_DBGDCCINT	(MDCCINT_EL1 * 2)
-
-#define NR_COPRO_REGS	(NR_SYS_REGS * 2)
-
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_TRAP	  1
 

commit 34c3faa353db8f5d3ce9966cf854d5643c64c4db
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 15 17:15:33 2015 +0100

    arm64: KVM: Remove all traces of the ThumbEE registers
    
    Although the ThumbEE registers and traps were present in earlier
    versions of the v8 architecture, it was retrospectively removed and so
    we can do the same.
    
    Whilst this breaks migrating a guest started on a previous version of
    the kernel, it is much better to kill these (non existent) registers
    as soon as possible.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [maz: added commend about migration]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 67fa0de3d483..5e377101f919 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -53,9 +53,7 @@
 #define	IFSR32_EL2	25	/* Instruction Fault Status Register */
 #define	FPEXC32_EL2	26	/* Floating-Point Exception Control Register */
 #define	DBGVCR32_EL2	27	/* Debug Vector Catch Register */
-#define	TEECR32_EL1	28	/* ThumbEE Configuration Register */
-#define	TEEHBR32_EL1	29	/* ThumbEE Handler Base Register */
-#define	NR_SYS_REGS	30
+#define	NR_SYS_REGS	28
 
 /* 32bit mapping */
 #define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */

commit 84e690bfbed1d1ecb45d8eccd4c7b6c8e878da1c
Author: Alex Benn√©e <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:30:00 2015 +0100

    KVM: arm64: introduce vcpu->arch.debug_ptr
    
    This introduces a level of indirection for the debug registers. Instead
    of using the sys_regs[] directly we store registers in a structure in
    the vcpu. The new kvm_arm_reset_debug_ptr() sets the debug ptr to the
    guest context.
    
    Because we no longer give the sys_regs offset for the sys_reg_desc->reg
    field, but instead the index into a debug-specific struct we need to
    add a number of additional trap functions for each register. Also as the
    generic generic user-space access code no longer works we have
    introduced a new pair of function pointers to the sys_reg_desc structure
    to override the generic code when needed.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Alex Benn√©e <alex.bennee@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index f5e40dae291a..67fa0de3d483 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -46,24 +46,16 @@
 #define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
 #define	PAR_EL1		21	/* Physical Address Register */
 #define MDSCR_EL1	22	/* Monitor Debug System Control Register */
-#define DBGBCR0_EL1	23	/* Debug Breakpoint Control Registers (0-15) */
-#define DBGBCR15_EL1	38
-#define DBGBVR0_EL1	39	/* Debug Breakpoint Value Registers (0-15) */
-#define DBGBVR15_EL1	54
-#define DBGWCR0_EL1	55	/* Debug Watchpoint Control Registers (0-15) */
-#define DBGWCR15_EL1	70
-#define DBGWVR0_EL1	71	/* Debug Watchpoint Value Registers (0-15) */
-#define DBGWVR15_EL1	86
-#define MDCCINT_EL1	87	/* Monitor Debug Comms Channel Interrupt Enable Reg */
+#define MDCCINT_EL1	23	/* Monitor Debug Comms Channel Interrupt Enable Reg */
 
 /* 32bit specific registers. Keep them at the end of the range */
-#define	DACR32_EL2	88	/* Domain Access Control Register */
-#define	IFSR32_EL2	89	/* Instruction Fault Status Register */
-#define	FPEXC32_EL2	90	/* Floating-Point Exception Control Register */
-#define	DBGVCR32_EL2	91	/* Debug Vector Catch Register */
-#define	TEECR32_EL1	92	/* ThumbEE Configuration Register */
-#define	TEEHBR32_EL1	93	/* ThumbEE Handler Base Register */
-#define	NR_SYS_REGS	94
+#define	DACR32_EL2	24	/* Domain Access Control Register */
+#define	IFSR32_EL2	25	/* Instruction Fault Status Register */
+#define	FPEXC32_EL2	26	/* Floating-Point Exception Control Register */
+#define	DBGVCR32_EL2	27	/* Debug Vector Catch Register */
+#define	TEECR32_EL1	28	/* ThumbEE Configuration Register */
+#define	TEEHBR32_EL1	29	/* ThumbEE Handler Base Register */
+#define	NR_SYS_REGS	30
 
 /* 32bit mapping */
 #define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */

commit 56c7f5e77f797fd0dcf2376ce1496f4238e6be33
Author: Alex Benn√©e <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:29:56 2015 +0100

    KVM: arm: introduce kvm_arm_init/setup/clear_debug
    
    This is a precursor for later patches which will need to do more to
    setup debug state before entering the hyp.S switch code. The existing
    functionality for setting mdcr_el2 has been moved out of hyp.S and now
    uses the value kept in vcpu->arch.mdcr_el2.
    
    As the assembler used to previously mask and preserve MDCR_EL2.HPMN I've
    had to add a mechanism to save the value of mdcr_el2 as a per-cpu
    variable during the initialisation code. The kernel never sets this
    number so we are assuming the bootcode has set up the correct value
    here.
    
    This also moves the conditional setting of the TDA bit from the hyp code
    into the C code which is currently used for the lazy debug register
    context switch code.
    
    Signed-off-by: Alex Benn√©e <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 3c5fe685a2d6..f5e40dae291a 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -132,6 +132,8 @@ extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
 
+extern u32 __kvm_get_mdcr_el2(void);
+
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */

commit e3d8238d7f5c3f539a29f5ac596cd342d847e099
Merge: 4e241557fc1c 86dca36e6ba0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 10:02:15 2015 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Mostly refactoring/clean-up:
    
       - CPU ops and PSCI (Power State Coordination Interface) refactoring
         following the merging of the arm64 ACPI support, together with
         handling of Trusted (secure) OS instances
    
       - Using fixmap for permanent FDT mapping, removing the initial dtb
         placement requirements (within 512MB from the start of the kernel
         image).  This required moving the FDT self reservation out of the
         memreserve processing
    
       - Idmap (1:1 mapping used for MMU on/off) handling clean-up
    
       - Removing flush_cache_all() - not safe on ARM unless the MMU is off.
         Last stages of CPU power down/up are handled by firmware already
    
       - "Alternatives" (run-time code patching) refactoring and support for
         immediate branch patching, GICv3 CPU interface access
    
       - User faults handling clean-up
    
      And some fixes:
    
       - Fix for VDSO building with broken ELF toolchains
    
       - Fix another case of init_mm.pgd usage for user mappings (during
         ASID roll-over broadcasting)
    
       - Fix for FPSIMD reloading after CPU hotplug
    
       - Fix for missing syscall trace exit
    
       - Workaround for .inst asm bug
    
       - Compat fix for switching the user tls tpidr_el0 register"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (42 commits)
      arm64: use private ratelimit state along with show_unhandled_signals
      arm64: show unhandled SP/PC alignment faults
      arm64: vdso: work-around broken ELF toolchains in Makefile
      arm64: kernel: rename __cpu_suspend to keep it aligned with arm
      arm64: compat: print compat_sp instead of sp
      arm64: mm: Fix freeing of the wrong memmap entries with !SPARSEMEM_VMEMMAP
      arm64: entry: fix context tracking for el0_sp_pc
      arm64: defconfig: enable memtest
      arm64: mm: remove reference to tlb.S from comment block
      arm64: Do not attempt to use init_mm in reset_context()
      arm64: KVM: Switch vgic save/restore to alternative_insn
      arm64: alternative: Introduce feature for GICv3 CPU interface
      arm64: psci: fix !CONFIG_HOTPLUG_CPU build warning
      arm64: fix bug for reloading FPSIMD state after CPU hotplug.
      arm64: kernel thread don't need to save fpsimd context.
      arm64: fix missing syscall trace exit
      arm64: alternative: Work around .inst assembler bugs
      arm64: alternative: Merge alternative-asm.h into alternative.h
      arm64: alternative: Allow immediate branch as alternative instruction
      arm64: Rework alternate sequence for ARM erratum 845719
      ...

commit cb8a4deaf9b2778653c4391d8ccb24c5ab159f9d
Merge: 0faef837e431 79ce48df755e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 23 14:08:54 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial tree updates from Jiri Kosina:
     "As usual, mostly comment, kerneldoc and printk() fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial:
      lpfc: Grammar s/an negative/a negative/
      ARM: lib/lib1funcs.S: fix typo s/substractions/subtractions/
      cx25821: cx25821-medusa-reg.h: fix 0x0x prefix
      lib: crc-itu-t.[ch] fix 0x0x prefix in integer constants
      rapidio: Fix kerneldoc and comment
      qla4xxx: Fix printk() in qla4_83xx_read_reset_template() and qla4_83xx_pre_loopback_config()
      treewide: Kconfig: fix wording / spelling
      usb/serial: fix grammar in Kconfig help text for FTDI_SIO
      megaraid_sas: fix kerneldoc
      netfilter: ebtables: fix comment grammar
      drm/radeon: fix comment
      isdn: fix grammar in comment
      ARM: KVM: fix comment

commit 8a14849b4a355278f0b7baf6e2da7dc7144a23e8
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 12 12:06:37 2015 +0100

    arm64: KVM: Switch vgic save/restore to alternative_insn
    
    So far, we configured the world-switch by having a small array
    of pointers to the save and restore functions, depending on the
    GIC used on the platform.
    
    Loading these values each time is a bit silly (they never change),
    and it makes sense to rely on the instruction patching instead.
    
    This leads to a nice cleanup of the code.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 4f7310fa77f0..d13bc5eb688c 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -132,11 +132,6 @@ extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
 
-extern char __save_vgic_v2_state[];
-extern char __restore_vgic_v2_state[];
-extern char __save_vgic_v3_state[];
-extern char __restore_vgic_v3_state[];
-
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */

commit 47b2d14294e04918205950f824fe099505cd4a03
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Tue Mar 3 11:58:06 2015 +0100

    ARM: KVM: fix comment
    
    Fix spelling in comment.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 483842180f8f..7e04f6ab3d5e 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -27,7 +27,7 @@
 #define	MPIDR_EL1	1	/* MultiProcessor Affinity Register */
 #define	CSSELR_EL1	2	/* Cache Size Selection Register */
 #define	SCTLR_EL1	3	/* System Control Register */
-#define	ACTLR_EL1	4	/* Auxilliary Control Register */
+#define	ACTLR_EL1	4	/* Auxiliary Control Register */
 #define	CPACR_EL1	5	/* Coprocessor Access Control */
 #define	TTBR0_EL1	6	/* Translation Table Base Register 0 */
 #define	TTBR1_EL1	7	/* Translation Table Base Register 1 */

commit 8199ed0e7c28ece79674a9fbba3208e93395a646
Author: Mario Smarduch <m.smarduch@samsung.com>
Date:   Thu Jan 15 15:58:59 2015 -0800

    KVM: arm64: ARMv8 header changes for page logging
    
    This patch adds arm64 helpers to write protect pmds/ptes and retrieve
    permissions while logging dirty pages. Also adds prototype to write protect
    a memory slot and adds a pmd define to check for read-only pmds.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mario Smarduch <m.smarduch@samsung.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 483842180f8f..4f7310fa77f0 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -126,6 +126,7 @@ extern char __kvm_hyp_vector[];
 
 extern void __kvm_flush_vm_context(void);
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
+extern void __kvm_tlb_flush_vmid(struct kvm *kvm);
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 

commit bdfb4b389c8d8f07e2d5b8e1291e01c789ba4aad
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Apr 24 10:31:37 2014 +0100

    arm64: KVM: add trap handlers for AArch32 debug registers
    
    Add handlers for all the AArch32 debug registers that are accessible
    from EL0 or EL1. The code follow the same strategy as the AArch64
    counterpart with regards to tracking the dirty state of the debug
    registers.
    
    Reviewed-by: Anup Patel <anup.patel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 69027ded5006..483842180f8f 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -95,6 +95,15 @@
 #define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
 #define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
 #define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
+
+#define cp14_DBGDSCRext	(MDSCR_EL1 * 2)
+#define cp14_DBGBCR0	(DBGBCR0_EL1 * 2)
+#define cp14_DBGBVR0	(DBGBVR0_EL1 * 2)
+#define cp14_DBGBXVR0	(cp14_DBGBVR0 + 1)
+#define cp14_DBGWCR0	(DBGWCR0_EL1 * 2)
+#define cp14_DBGWVR0	(DBGWVR0_EL1 * 2)
+#define cp14_DBGDCCINT	(MDCCINT_EL1 * 2)
+
 #define NR_COPRO_REGS	(NR_SYS_REGS * 2)
 
 #define ARM_EXCEPTION_IRQ	  0

commit 72564016aae45f42e488f926bc803f9a2e1c771c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Apr 24 10:27:13 2014 +0100

    arm64: KVM: common infrastructure for handling AArch32 CP14/CP15
    
    As we're about to trap a bunch of CP14 registers, let's rework
    the CP15 handling so it can be generalized and work with multiple
    tables.
    
    Reviewed-by: Anup Patel <anup.patel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 660f75c48bbb..69027ded5006 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -95,7 +95,7 @@
 #define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
 #define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
 #define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
-#define NR_CP15_REGS	(NR_SYS_REGS * 2)
+#define NR_COPRO_REGS	(NR_SYS_REGS * 2)
 
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_TRAP	  1

commit 0c557ed4983b7abe152212b5b1726c2a789b2c61
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Apr 24 10:24:46 2014 +0100

    arm64: KVM: add trap handlers for AArch64 debug registers
    
    Add handlers for all the AArch64 debug registers that are accessible
    from EL0 or EL1. The trapping code keeps track of the state of the
    debug registers, allowing for the switch code to implement a lazy
    switching strategy.
    
    Reviewed-by: Anup Patel <anup.patel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index a28c35b337ec..660f75c48bbb 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -45,14 +45,25 @@
 #define	AMAIR_EL1	19	/* Aux Memory Attribute Indirection Register */
 #define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
 #define	PAR_EL1		21	/* Physical Address Register */
+#define MDSCR_EL1	22	/* Monitor Debug System Control Register */
+#define DBGBCR0_EL1	23	/* Debug Breakpoint Control Registers (0-15) */
+#define DBGBCR15_EL1	38
+#define DBGBVR0_EL1	39	/* Debug Breakpoint Value Registers (0-15) */
+#define DBGBVR15_EL1	54
+#define DBGWCR0_EL1	55	/* Debug Watchpoint Control Registers (0-15) */
+#define DBGWCR15_EL1	70
+#define DBGWVR0_EL1	71	/* Debug Watchpoint Value Registers (0-15) */
+#define DBGWVR15_EL1	86
+#define MDCCINT_EL1	87	/* Monitor Debug Comms Channel Interrupt Enable Reg */
+
 /* 32bit specific registers. Keep them at the end of the range */
-#define	DACR32_EL2	22	/* Domain Access Control Register */
-#define	IFSR32_EL2	23	/* Instruction Fault Status Register */
-#define	FPEXC32_EL2	24	/* Floating-Point Exception Control Register */
-#define	DBGVCR32_EL2	25	/* Debug Vector Catch Register */
-#define	TEECR32_EL1	26	/* ThumbEE Configuration Register */
-#define	TEEHBR32_EL1	27	/* ThumbEE Handler Base Register */
-#define	NR_SYS_REGS	28
+#define	DACR32_EL2	88	/* Domain Access Control Register */
+#define	IFSR32_EL2	89	/* Instruction Fault Status Register */
+#define	FPEXC32_EL2	90	/* Floating-Point Exception Control Register */
+#define	DBGVCR32_EL2	91	/* Debug Vector Catch Register */
+#define	TEECR32_EL1	92	/* ThumbEE Configuration Register */
+#define	TEEHBR32_EL1	93	/* ThumbEE Handler Base Register */
+#define	NR_SYS_REGS	94
 
 /* 32bit mapping */
 #define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
@@ -89,6 +100,9 @@
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_TRAP	  1
 
+#define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
+#define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
+
 #ifndef __ASSEMBLY__
 struct kvm;
 struct kvm_vcpu;

commit 754d37726010d872f1f714a8ce8920acdfa4978c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jul 9 10:45:49 2013 +0100

    arm64: KVM: vgic: add GICv3 world switch
    
    Introduce the GICv3 world switch code used to save/restore the
    GICv3 context.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index ed4987bf9ac7..a28c35b337ec 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -110,6 +110,8 @@ extern u64 __vgic_v3_get_ich_vtr_el2(void);
 
 extern char __save_vgic_v2_state[];
 extern char __restore_vgic_v2_state[];
+extern char __save_vgic_v3_state[];
+extern char __restore_vgic_v3_state[];
 
 #endif
 

commit b2fb1c0d378399e1427a91bb991c094f2ca09a2f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jul 12 15:15:23 2013 +0100

    KVM: ARM: vgic: add the GICv3 backend
    
    Introduce the support code for emulating a GICv2 on top of GICv3
    hardware.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 6252264341c8..ed4987bf9ac7 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -106,6 +106,8 @@ extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
+extern u64 __vgic_v3_get_ich_vtr_el2(void);
+
 extern char __save_vgic_v2_state[];
 extern char __restore_vgic_v2_state[];
 

commit 1a9b13056dde7e3092304d6041ccc60a913042ea
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 21 11:57:56 2013 +0100

    arm64: KVM: split GICv2 world switch from hyp code
    
    Move the GICv2 world switch code into its own file, and add the
    necessary indirection to the arm64 switch code.
    
    Also introduce a new type field to the vgic_params structure.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index d0bfc4ba82c0..6252264341c8 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -105,6 +105,10 @@ extern void __kvm_flush_vm_context(void);
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
+
+extern char __save_vgic_v2_state[];
+extern char __restore_vgic_v2_state[];
+
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */

commit 45451914c875bba44903ce4f1445e047b7992bf7
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jun 26 15:16:40 2013 +0100

    arm64: KVM: remove __kvm_hyp_code_{start,end} from hyp.S
    
    We already have __hyp_text_{start,end} to express the boundaries
    of the HYP text section, and __kvm_hyp_code_{start,end} are getting
    in the way of a more modular world switch code.
    
    Just turn __kvm_hyp_code_{start,end} into #defines mapping the
    linker-emited symbols.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 9fcd54b1e16d..d0bfc4ba82c0 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -18,6 +18,8 @@
 #ifndef __ARM_KVM_ASM_H__
 #define __ARM_KVM_ASM_H__
 
+#include <asm/virt.h>
+
 /*
  * 0 is reserved as an invalid value.
  * Order *must* be kept in sync with the hyp switch code.
@@ -96,8 +98,8 @@ extern char __kvm_hyp_init_end[];
 
 extern char __kvm_hyp_vector[];
 
-extern char __kvm_hyp_code_start[];
-extern char __kvm_hyp_code_end[];
+#define	__kvm_hyp_code_start	__hyp_text_start
+#define	__kvm_hyp_code_end	__hyp_text_end
 
 extern void __kvm_flush_vm_context(void);
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);

commit 4d44923b17bff283c002ed961373848284aaff1b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jan 14 18:00:55 2014 +0000

    arm64: KVM: trap VM system registers until MMU and caches are ON
    
    In order to be able to detect the point where the guest enables
    its MMU and caches, trap all the VM related system registers.
    
    Once we see the guest enabling both the MMU and the caches, we
    can go back to a saner mode of operation, which is to leave these
    registers in complete control of the guest.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index b25763bc0ec4..9fcd54b1e16d 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -79,7 +79,8 @@
 #define c13_TID_URW	(TPIDR_EL0 * 2)	/* Thread ID, User R/W */
 #define c13_TID_URO	(TPIDRRO_EL0 * 2)/* Thread ID, User R/O */
 #define c13_TID_PRIV	(TPIDR_EL1 * 2)	/* Thread ID, Privileged */
-#define c10_AMAIR	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
+#define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
+#define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
 #define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
 #define NR_CP15_REGS	(NR_SYS_REGS * 2)
 

commit 1bbd80549810637b7381ab0649ba7c7d62f1342a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 7 11:02:34 2013 +0100

    arm64: KVM: perform save/restore of PAR_EL1
    
    Not saving PAR_EL1 is an unfortunate oversight. If the guest
    performs an AT* operation and gets scheduled out before reading
    the result of the translation from PAREL1, it could become
    corrupted by another guest or the host.
    
    Saving this register is made slightly more complicated as KVM also
    uses it on the permission fault handling path, leading to an ugly
    "stash and restore" sequence. Fortunately, this is already a slow
    path so we don't really care. Also, Linux doesn't do any AT*
    operation, so Linux guests are not impacted by this bug.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index c92de4163eba..b25763bc0ec4 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -42,14 +42,15 @@
 #define	TPIDR_EL1	18	/* Thread ID, Privileged */
 #define	AMAIR_EL1	19	/* Aux Memory Attribute Indirection Register */
 #define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
+#define	PAR_EL1		21	/* Physical Address Register */
 /* 32bit specific registers. Keep them at the end of the range */
-#define	DACR32_EL2	21	/* Domain Access Control Register */
-#define	IFSR32_EL2	22	/* Instruction Fault Status Register */
-#define	FPEXC32_EL2	23	/* Floating-Point Exception Control Register */
-#define	DBGVCR32_EL2	24	/* Debug Vector Catch Register */
-#define	TEECR32_EL1	25	/* ThumbEE Configuration Register */
-#define	TEEHBR32_EL1	26	/* ThumbEE Handler Base Register */
-#define	NR_SYS_REGS	27
+#define	DACR32_EL2	22	/* Domain Access Control Register */
+#define	IFSR32_EL2	23	/* Instruction Fault Status Register */
+#define	FPEXC32_EL2	24	/* Floating-Point Exception Control Register */
+#define	DBGVCR32_EL2	25	/* Debug Vector Catch Register */
+#define	TEECR32_EL1	26	/* ThumbEE Configuration Register */
+#define	TEEHBR32_EL1	27	/* ThumbEE Handler Base Register */
+#define	NR_SYS_REGS	28
 
 /* 32bit mapping */
 #define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
@@ -69,6 +70,8 @@
 #define c5_AIFSR	(AFSR1_EL1 * 2)	/* Auxiliary Instr Fault Status R */
 #define c6_DFAR		(FAR_EL1 * 2)	/* Data Fault Address Register */
 #define c6_IFAR		(c6_DFAR + 1)	/* Instruction Fault Address Register */
+#define c7_PAR		(PAR_EL1 * 2)	/* Physical Address Register */
+#define c7_PAR_high	(c7_PAR + 1)	/* PAR top 32 bits */
 #define c10_PRRR	(MAIR_EL1 * 2)	/* Primary Region Remap Register */
 #define c10_NMRR	(c10_PRRR + 1)	/* Normal Memory Remap Register */
 #define c12_VBAR	(VBAR_EL1 * 2)	/* Vector Base Address Register */

commit 40033a614ea3db196d57c477ca328f44eb1e4df0
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Feb 6 19:17:50 2013 +0000

    arm64: KVM: define 32bit specific registers
    
    Define the 32bit specific registers (SPSRs, cp15...).
    
    Most CPU registers are directly mapped to a 64bit register
    (r0->x0...). Only the SPSRs have separate registers.
    
    cp15 registers are also mapped into their 64bit counterpart in most
    cases.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index 591ac219964a..c92de4163eba 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -42,7 +42,43 @@
 #define	TPIDR_EL1	18	/* Thread ID, Privileged */
 #define	AMAIR_EL1	19	/* Aux Memory Attribute Indirection Register */
 #define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
-#define	NR_SYS_REGS	21
+/* 32bit specific registers. Keep them at the end of the range */
+#define	DACR32_EL2	21	/* Domain Access Control Register */
+#define	IFSR32_EL2	22	/* Instruction Fault Status Register */
+#define	FPEXC32_EL2	23	/* Floating-Point Exception Control Register */
+#define	DBGVCR32_EL2	24	/* Debug Vector Catch Register */
+#define	TEECR32_EL1	25	/* ThumbEE Configuration Register */
+#define	TEEHBR32_EL1	26	/* ThumbEE Handler Base Register */
+#define	NR_SYS_REGS	27
+
+/* 32bit mapping */
+#define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
+#define c0_CSSELR	(CSSELR_EL1 * 2)/* Cache Size Selection Register */
+#define c1_SCTLR	(SCTLR_EL1 * 2)	/* System Control Register */
+#define c1_ACTLR	(ACTLR_EL1 * 2)	/* Auxiliary Control Register */
+#define c1_CPACR	(CPACR_EL1 * 2)	/* Coprocessor Access Control */
+#define c2_TTBR0	(TTBR0_EL1 * 2)	/* Translation Table Base Register 0 */
+#define c2_TTBR0_high	(c2_TTBR0 + 1)	/* TTBR0 top 32 bits */
+#define c2_TTBR1	(TTBR1_EL1 * 2)	/* Translation Table Base Register 1 */
+#define c2_TTBR1_high	(c2_TTBR1 + 1)	/* TTBR1 top 32 bits */
+#define c2_TTBCR	(TCR_EL1 * 2)	/* Translation Table Base Control R. */
+#define c3_DACR		(DACR32_EL2 * 2)/* Domain Access Control Register */
+#define c5_DFSR		(ESR_EL1 * 2)	/* Data Fault Status Register */
+#define c5_IFSR		(IFSR32_EL2 * 2)/* Instruction Fault Status Register */
+#define c5_ADFSR	(AFSR0_EL1 * 2)	/* Auxiliary Data Fault Status R */
+#define c5_AIFSR	(AFSR1_EL1 * 2)	/* Auxiliary Instr Fault Status R */
+#define c6_DFAR		(FAR_EL1 * 2)	/* Data Fault Address Register */
+#define c6_IFAR		(c6_DFAR + 1)	/* Instruction Fault Address Register */
+#define c10_PRRR	(MAIR_EL1 * 2)	/* Primary Region Remap Register */
+#define c10_NMRR	(c10_PRRR + 1)	/* Normal Memory Remap Register */
+#define c12_VBAR	(VBAR_EL1 * 2)	/* Vector Base Address Register */
+#define c13_CID		(CONTEXTIDR_EL1 * 2)	/* Context ID Register */
+#define c13_TID_URW	(TPIDR_EL0 * 2)	/* Thread ID, User R/W */
+#define c13_TID_URO	(TPIDRRO_EL0 * 2)/* Thread ID, User R/O */
+#define c13_TID_PRIV	(TPIDR_EL1 * 2)	/* Thread ID, Privileged */
+#define c10_AMAIR	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
+#define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
+#define NR_CP15_REGS	(NR_SYS_REGS * 2)
 
 #define ARM_EXCEPTION_IRQ	  0
 #define ARM_EXCEPTION_TRAP	  1

commit fd9fc9f73cc2070d2637a7ee082800a817fd45f3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Dec 10 11:16:40 2012 +0000

    arm64: KVM: system register definitions for 64bit guests
    
    Define the saved/restored registers for 64bit guests.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
new file mode 100644
index 000000000000..591ac219964a
--- /dev/null
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright (C) 2012,2013 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __ARM_KVM_ASM_H__
+#define __ARM_KVM_ASM_H__
+
+/*
+ * 0 is reserved as an invalid value.
+ * Order *must* be kept in sync with the hyp switch code.
+ */
+#define	MPIDR_EL1	1	/* MultiProcessor Affinity Register */
+#define	CSSELR_EL1	2	/* Cache Size Selection Register */
+#define	SCTLR_EL1	3	/* System Control Register */
+#define	ACTLR_EL1	4	/* Auxilliary Control Register */
+#define	CPACR_EL1	5	/* Coprocessor Access Control */
+#define	TTBR0_EL1	6	/* Translation Table Base Register 0 */
+#define	TTBR1_EL1	7	/* Translation Table Base Register 1 */
+#define	TCR_EL1		8	/* Translation Control Register */
+#define	ESR_EL1		9	/* Exception Syndrome Register */
+#define	AFSR0_EL1	10	/* Auxilary Fault Status Register 0 */
+#define	AFSR1_EL1	11	/* Auxilary Fault Status Register 1 */
+#define	FAR_EL1		12	/* Fault Address Register */
+#define	MAIR_EL1	13	/* Memory Attribute Indirection Register */
+#define	VBAR_EL1	14	/* Vector Base Address Register */
+#define	CONTEXTIDR_EL1	15	/* Context ID Register */
+#define	TPIDR_EL0	16	/* Thread ID, User R/W */
+#define	TPIDRRO_EL0	17	/* Thread ID, User R/O */
+#define	TPIDR_EL1	18	/* Thread ID, Privileged */
+#define	AMAIR_EL1	19	/* Aux Memory Attribute Indirection Register */
+#define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
+#define	NR_SYS_REGS	21
+
+#define ARM_EXCEPTION_IRQ	  0
+#define ARM_EXCEPTION_TRAP	  1
+
+#ifndef __ASSEMBLY__
+struct kvm;
+struct kvm_vcpu;
+
+extern char __kvm_hyp_init[];
+extern char __kvm_hyp_init_end[];
+
+extern char __kvm_hyp_vector[];
+
+extern char __kvm_hyp_code_start[];
+extern char __kvm_hyp_code_end[];
+
+extern void __kvm_flush_vm_context(void);
+extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
+
+extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
+#endif
+
+#endif /* __ARM_KVM_ASM_H__ */
