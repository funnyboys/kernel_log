commit ce6f8f02f9f6786355fa6c79d88b839639dd75d8
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed May 13 11:38:28 2020 +0100

    KVM: arm64: Use cpus_have_final_cap for has_vhe()
    
    By the time we start using the has_vhe() helper, we have long
    discovered whether we are running VHE or not. It thus makes
    sense to use cpus_have_final_cap() instead of cpus_have_const_cap(),
    which leads to a small text size reduction.
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: David Brazdil <dbrazdil@google.com>
    Link: https://lore.kernel.org/r/20200513103828.74580-1-maz@kernel.org

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 61fd26752adc..5051b388c654 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -85,7 +85,7 @@ static inline bool is_kernel_in_hyp_mode(void)
 
 static __always_inline bool has_vhe(void)
 {
-	if (cpus_have_const_cap(ARM64_HAS_VIRT_HOST_EXTN))
+	if (cpus_have_final_cap(ARM64_HAS_VIRT_HOST_EXTN))
 		return true;
 
 	return false;

commit 5c37f1ae1c335800d16b207cb578009c695dcd39
Author: James Morse <james.morse@arm.com>
Date:   Thu Feb 20 16:58:37 2020 +0000

    KVM: arm64: Ask the compiler to __always_inline functions used at HYP
    
    On non VHE CPUs, KVM's __hyp_text contains code run at EL2 while the rest
    of the kernel runs at EL1. This code lives in its own section with start
    and end markers so we can map it to EL2.
    
    The compiler may decide not to inline static-inline functions from the
    header file. It may also decide not to put these out-of-line functions
    in the same section, meaning they aren't mapped when called at EL2.
    
    Clang-9 does exactly this with __kern_hyp_va() and a few others when
    x18 is reserved for the shadow call stack. Add the additional __always_
    hint to all the static-inlines that are called from a hyp file.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200220165839.256881-2-james.morse@arm.com
    
    ----
    kvm_get_hyp_vector() pulls in all the regular per-cpu accessors
    and this_cpu_has_cap(), fortunately its only called for VHE.

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 0958ed6191aa..61fd26752adc 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -83,7 +83,7 @@ static inline bool is_kernel_in_hyp_mode(void)
 	return read_sysreg(CurrentEL) == CurrentEL_EL2;
 }
 
-static inline bool has_vhe(void)
+static __always_inline bool has_vhe(void)
 {
 	if (cpus_have_const_cap(ARM64_HAS_VIRT_HOST_EXTN))
 		return true;

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 9d1e24e030b3..0958ed6191aa 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -1,18 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
  * Author: Marc Zyngier <marc.zyngier@arm.com>
- *
- * This program is free software: you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #ifndef __ASM__VIRT_H

commit 830dcc9f9a7cd26a812522a26efaacf7df6fc365
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:42 2018 +0100

    arm64: capabilities: Change scope of VHE to Boot CPU feature
    
    We expect all CPUs to be running at the same EL inside the kernel
    with or without VHE enabled and we have strict checks to ensure
    that any mismatch triggers a kernel panic. If VHE is enabled,
    we use the feature based on the boot CPU and all other CPUs
    should follow. This makes it a perfect candidate for a capability
    based on the boot CPU,  which should be matched by all the CPUs
    (both when is ON and OFF). This saves us some not-so-pretty
    hooks and special code, just for verifying the conflict.
    
    The patch also makes the VHE capability entry depend on
    CONFIG_ARM64_VHE.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index c5f89442785c..9d1e24e030b3 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -102,12 +102,6 @@ static inline bool has_vhe(void)
 	return false;
 }
 
-#ifdef CONFIG_ARM64_VHE
-extern void verify_cpu_run_el(void);
-#else
-static inline void verify_cpu_run_el(void) {}
-#endif
-
 #endif /* __ASSEMBLY__ */
 
 #endif /* ! __ASM__VIRT_H */

commit 0b51c547fdb7ee05b6d65471785a1d899c8a667c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:38:04 2017 +0100

    arm64: hyp-stub/KVM: Kill __hyp_get_vectors
    
    Nobody is using __hyp_get_vectors anymore, so let's remove both
    implementations (hyp-stub and KVM).
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 435514c52b0f..c5f89442785c 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -29,28 +29,25 @@
  * indirection of a function call (as implemented in hyp-stub.S).
  */
 
-/* HVC_GET_VECTORS - Return the value of the vbar_el2 register. */
-#define HVC_GET_VECTORS 0
-
 /*
  * HVC_SET_VECTORS - Set the value of the vbar_el2 register.
  *
  * @x1: Physical address of the new vector table.
  */
-#define HVC_SET_VECTORS 1
+#define HVC_SET_VECTORS 0
 
 /*
  * HVC_SOFT_RESTART - CPU soft reset, used by the cpu_soft_restart routine.
  */
-#define HVC_SOFT_RESTART 2
+#define HVC_SOFT_RESTART 1
 
 /*
  * HVC_RESET_VECTORS - Restore the vectors to the original HYP stubs
  */
-#define HVC_RESET_VECTORS 3
+#define HVC_RESET_VECTORS 2
 
 /* Max number of HYP stub hypercalls */
-#define HVC_STUB_HCALL_NR 4
+#define HVC_STUB_HCALL_NR 3
 
 /* Error returned when an invalid stub number is passed into x0 */
 #define HVC_STUB_ERR	0xbadca11
@@ -77,7 +74,6 @@
 extern u32 __boot_cpu_mode[2];
 
 void __hyp_set_vectors(phys_addr_t phys_vector_base);
-phys_addr_t __hyp_get_vectors(void);
 void __hyp_reset_vectors(void);
 
 /* Reports the availability of HYP mode */

commit fd0e0c617089e7ba8333c55bd2e318dfc128bf33
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:39 2017 +0100

    arm64: hyp-stub: Implement HVC_RESET_VECTORS stub hypercall
    
    Let's define a new stub hypercall that resets the HYP configuration
    to its default: hyp-stub vectors, and MMU disabled.
    
    Of course, for the hyp-stub itself, this is a trivial no-op.
    Hypervisors will have a bit more work to do.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 1569c3a0d794..435514c52b0f 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -44,6 +44,14 @@
  */
 #define HVC_SOFT_RESTART 2
 
+/*
+ * HVC_RESET_VECTORS - Restore the vectors to the original HYP stubs
+ */
+#define HVC_RESET_VECTORS 3
+
+/* Max number of HYP stub hypercalls */
+#define HVC_STUB_HCALL_NR 4
+
 /* Error returned when an invalid stub number is passed into x0 */
 #define HVC_STUB_ERR	0xbadca11
 
@@ -70,6 +78,7 @@ extern u32 __boot_cpu_mode[2];
 
 void __hyp_set_vectors(phys_addr_t phys_vector_base);
 phys_addr_t __hyp_get_vectors(void);
+void __hyp_reset_vectors(void);
 
 /* Reports the availability of HYP mode */
 static inline bool is_hyp_mode_available(void)

commit 810c86ee4ee70928952fe17bc593d569052d8198
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:38 2017 +0100

    arm64: hyp-stub: Update documentation in asm/virt.h
    
    Comments in asm/virt.h are slightly out of date, so let's align
    them with the new behaviour of the code.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 1466d14c664d..1569c3a0d794 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -19,9 +19,14 @@
 #define __ASM__VIRT_H
 
 /*
- * The arm64 hcall implementation uses x0 to specify the hcall type. A value
- * less than 0xfff indicates a special hcall, such as get/set vector.
- * Any other value is used as a pointer to the function to call.
+ * The arm64 hcall implementation uses x0 to specify the hcall
+ * number. A value less than HVC_STUB_HCALL_NR indicates a special
+ * hcall, such as set vector. Any other value is handled in a
+ * hypervisor specific way.
+ *
+ * The hypercall is allowed to clobber any of the caller-saved
+ * registers (x0-x18), so it is advisable to use it through the
+ * indirection of a function call (as implemented in hyp-stub.S).
  */
 
 /* HVC_GET_VECTORS - Return the value of the vbar_el2 register. */

commit 4993fdcf399f59ed56a16ecfedf8a61066198816
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:37 2017 +0100

    arm64: hyp-stub: Define a return value for failed stub calls
    
    Define a standard return value to be returned when a hyp stub
    call fails, and make KVM use it for ARM_EXCEPTION_HYP_GONE
    (instead of using a KVM-specific value).
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 439f6b5d31f6..1466d14c664d 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -39,6 +39,9 @@
  */
 #define HVC_SOFT_RESTART 2
 
+/* Error returned when an invalid stub number is passed into x0 */
+#define HVC_STUB_ERR	0xbadca11
+
 #define BOOT_CPU_MODE_EL1	(0xe11)
 #define BOOT_CPU_MODE_EL2	(0xe12)
 

commit 488f94d7212b00a2ec72fb886b155f1b04c5aa98
Author: Jintack Lim <jintack@cs.columbia.edu>
Date:   Thu Dec 1 14:32:05 2016 -0500

    KVM: arm64: Access CNTHCTL_EL2 bit fields correctly on VHE systems
    
    Current KVM world switch code is unintentionally setting wrong bits to
    CNTHCTL_EL2 when E2H == 1, which may allow guest OS to access physical
    timer.  Bit positions of CNTHCTL_EL2 are changing depending on
    HCR_EL2.E2H bit.  EL1PCEN and EL1PCTEN are 1st and 0th bits when E2H is
    not set, but they are 11th and 10th bits respectively when E2H is set.
    
    In fact, on VHE we only need to set those bits once, not for every world
    switch. This is because the host kernel runs in EL2 with HCR_EL2.TGE ==
    1, which makes those bits have no effect for the host kernel execution.
    So we just set those bits once for guests, and that's it.
    
    Signed-off-by: Jintack Lim <jintack@cs.columbia.edu>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index fea10736b11f..439f6b5d31f6 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -47,6 +47,7 @@
 #include <asm/ptrace.h>
 #include <asm/sections.h>
 #include <asm/sysreg.h>
+#include <asm/cpufeature.h>
 
 /*
  * __boot_cpu_mode records what mode CPUs were booted in.
@@ -80,6 +81,14 @@ static inline bool is_kernel_in_hyp_mode(void)
 	return read_sysreg(CurrentEL) == CurrentEL_EL2;
 }
 
+static inline bool has_vhe(void)
+{
+	if (cpus_have_const_cap(ARM64_HAS_VIRT_HOST_EXTN))
+		return true;
+
+	return false;
+}
+
 #ifdef CONFIG_ARM64_VHE
 extern void verify_cpu_run_el(void);
 #else

commit 1f3d8699be82583c713e2a1099c597a740ebaf4d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Sep 8 13:55:37 2016 +0100

    arm64/kvm: use {read,write}_sysreg()
    
    A while back we added {read,write}_sysreg accessors to handle accesses
    to system registers, without the usual boilerplate asm volatile,
    temporary variable, etc.
    
    This patch makes use of these in the arm64 KVM code to make the code
    shorter and clearer.
    
    At the same time, a comment style violation next to a system register
    access is fixed up in reset_pmcr, and comments describing whether
    operations are reads or writes are removed as this is now painfully
    obvious.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index db5739413677..fea10736b11f 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -46,6 +46,7 @@
 
 #include <asm/ptrace.h>
 #include <asm/sections.h>
+#include <asm/sysreg.h>
 
 /*
  * __boot_cpu_mode records what mode CPUs were booted in.
@@ -76,10 +77,7 @@ static inline bool is_hyp_mode_mismatched(void)
 
 static inline bool is_kernel_in_hyp_mode(void)
 {
-	u64 el;
-
-	asm("mrs %0, CurrentEL" : "=r" (el));
-	return el == CurrentEL_EL2;
+	return read_sysreg(CurrentEL) == CurrentEL_EL2;
 }
 
 #ifdef CONFIG_ARM64_VHE

commit ee78fdc71db1ce9a437b9ca17e31063996b71ec1
Author: James Morse <james.morse@arm.com>
Date:   Wed Aug 24 18:27:28 2016 +0100

    arm64: Create sections.h
    
    Each time new section markers are added, kernel/vmlinux.ld.S is updated,
    and new extern char __start_foo[] definitions are scattered through the
    tree.
    
    Create asm/include/sections.h to collect these definitions (and include
    the existing asm-generic version).
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 1788545f25bc..db5739413677 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -45,6 +45,7 @@
 #ifndef __ASSEMBLY__
 
 #include <asm/ptrace.h>
+#include <asm/sections.h>
 
 /*
  * __boot_cpu_mode records what mode CPUs were booted in.
@@ -87,14 +88,6 @@ extern void verify_cpu_run_el(void);
 static inline void verify_cpu_run_el(void) {}
 #endif
 
-/* The section containing the hypervisor idmap text */
-extern char __hyp_idmap_text_start[];
-extern char __hyp_idmap_text_end[];
-
-/* The section containing the hypervisor text */
-extern char __hyp_text_start[];
-extern char __hyp_text_end[];
-
 #endif /* __ASSEMBLY__ */
 
 #endif /* ! __ASM__VIRT_H */

commit 221bb8a46e230b9824204ae86537183d9991ff2a
Merge: f7b32e4c021f 23528bb21ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:11:27 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
    
     - ARM: GICv3 ITS emulation and various fixes.  Removal of the
       old VGIC implementation.
    
     - s390: support for trapping software breakpoints, nested
       virtualization (vSIE), the STHYI opcode, initial extensions
       for CPU model support.
    
     - MIPS: support for MIPS64 hosts (32-bit guests only) and lots
       of cleanups, preliminary to this and the upcoming support for
       hardware virtualization extensions.
    
     - x86: support for execute-only mappings in nested EPT; reduced
       vmexit latency for TSC deadline timer (by about 30%) on Intel
       hosts; support for more than 255 vCPUs.
    
     - PPC: bugfixes.
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (302 commits)
      KVM: PPC: Introduce KVM_CAP_PPC_HTM
      MIPS: Select HAVE_KVM for MIPS64_R{2,6}
      MIPS: KVM: Reset CP0_PageMask during host TLB flush
      MIPS: KVM: Fix ptr->int cast via KVM_GUEST_KSEGX()
      MIPS: KVM: Sign extend MFC0/RDHWR results
      MIPS: KVM: Fix 64-bit big endian dynamic translation
      MIPS: KVM: Fail if ebase doesn't fit in CP0_EBase
      MIPS: KVM: Use 64-bit CP0_EBase when appropriate
      MIPS: KVM: Set CP0_Status.KX on MIPS64
      MIPS: KVM: Make entry code MIPS64 friendly
      MIPS: KVM: Use kmap instead of CKSEG0ADDR()
      MIPS: KVM: Use virt_to_phys() to get commpage PFN
      MIPS: Fix definition of KSEGX() for 64-bit
      KVM: VMX: Add VMCS to CPU's loaded VMCSs before VMPTRLD
      kvm: x86: nVMX: maintain internal copy of current VMCS
      KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE
      KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures
      KVM: arm64: vgic-its: Simplify MAPI error handling
      KVM: arm64: vgic-its: Make vgic_its_cmd_handle_mapi similar to other handlers
      KVM: arm64: vgic-its: Turn device_id validation into generic ID validation
      ...

commit 1df3e2347a432fec7ec4aea67161986e116f68eb
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:41 2016 +0100

    arm/arm64: KVM: Export __hyp_text_start/end symbols
    
    Declare the __hyp_text_start/end symbols in asm/virt.h so that
    they can be reused without having to declare them locally.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index dcbcf8dcbefb..88aa8ec784f6 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -82,6 +82,10 @@ extern void verify_cpu_run_el(void);
 static inline void verify_cpu_run_el(void) {}
 #endif
 
+/* The section containing the hypervisor idmap text */
+extern char __hyp_idmap_text_start[];
+extern char __hyp_idmap_text_end[];
+
 /* The section containing the hypervisor text */
 extern char __hyp_text_start[];
 extern char __hyp_text_end[];

commit f9076ecfb1216a478312b1c078d04792df6d4477
Author: Geoff Levand <geoff@infradead.org>
Date:   Thu Jun 23 17:54:48 2016 +0000

    arm64: Add back cpu reset routines
    
    Commit 68234df4ea79 ("arm64: kill flush_cache_all()") removed the global
    arm64 routines cpu_reset() and cpu_soft_restart() needed by the arm64
    kexec and kdump support.  Add back a simplified version of
    cpu_soft_restart() with some changes needed for kexec in the new files
    cpu_reset.S, and cpu_reset.h.
    
    When a CPU is reset it needs to be put into the exception level it had when
    it entered the kernel. Update cpu_soft_restart() to accept an argument
    which signals if the reset address should be entered at EL1 or EL2, and
    add a new hypercall HVC_SOFT_RESTART which is used for the EL2 switch.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index dcbcf8dcbefb..bbc6a8cf83f1 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -34,6 +34,11 @@
  */
 #define HVC_SET_VECTORS 1
 
+/*
+ * HVC_SOFT_RESTART - CPU soft reset, used by the cpu_soft_restart routine.
+ */
+#define HVC_SOFT_RESTART 2
+
 #define BOOT_CPU_MODE_EL1	(0xe11)
 #define BOOT_CPU_MODE_EL2	(0xe12)
 

commit ad72e59ff2bad55f6b9e7ac1fe5d824831ea2550
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Apr 27 17:47:03 2016 +0100

    arm64: hyp/kvm: Make hyp-stub extensible
    
    The existing arm64 hcall implementations are limited in that they only
    allow for two distinct hcalls; with the x0 register either zero or not
    zero.  Also, the API of the hyp-stub exception vector routines and the
    KVM exception vector routines differ; hyp-stub uses a non-zero value in
    x0 to implement __hyp_set_vectors, whereas KVM uses it to implement
    kvm_call_hyp.
    
    To allow for additional hcalls to be defined and to make the arm64 hcall
    API more consistent across exception vector routines, change the hcall
    implementations to reserve all x0 values below 0xfff for hcalls such
    as {s,g}et_vectors().
    
    Define two new preprocessor macros HVC_GET_VECTORS, and HVC_SET_VECTORS
    to be used as hcall type specifiers and convert the existing
    __hyp_get_vectors() and __hyp_set_vectors() routines to use these new
    macros when executing an HVC call.  Also, change the corresponding
    hyp-stub and KVM el1_sync exception vector routines to use these new
    macros.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    [Merged two hcall patches, moved immediate value from esr to x0, use lr
     as a scratch register, changed limit to 0xfff]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index a5c2b4852154..dcbcf8dcbefb 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -18,6 +18,22 @@
 #ifndef __ASM__VIRT_H
 #define __ASM__VIRT_H
 
+/*
+ * The arm64 hcall implementation uses x0 to specify the hcall type. A value
+ * less than 0xfff indicates a special hcall, such as get/set vector.
+ * Any other value is used as a pointer to the function to call.
+ */
+
+/* HVC_GET_VECTORS - Return the value of the vbar_el2 register. */
+#define HVC_GET_VECTORS 0
+
+/*
+ * HVC_SET_VECTORS - Set the value of the vbar_el2 register.
+ *
+ * @x1: Physical address of the new vector table.
+ */
+#define HVC_SET_VECTORS 1
+
 #define BOOT_CPU_MODE_EL1	(0xe11)
 #define BOOT_CPU_MODE_EL2	(0xe12)
 

commit ac1ad20f9ed73a22b0a72eb83206302f5fbff55c
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Apr 13 14:41:33 2016 +0100

    arm64: vhe: Verify CPU Exception Levels
    
    With a VHE capable CPU, kernel can run at EL2 and is a decided at early
    boot. If some of the CPUs didn't start it EL2 or doesn't have VHE, we
    could have CPUs running at different exception levels, all in the same
    kernel! This patch adds an early check for the secondary CPUs to detect
    such situations.
    
    For each non-boot CPU add a sanity check to make sure we don't have
    different run levels w.r.t the boot CPU. We save the information on
    whether the boot CPU is running in hyp mode or not and ensure the
    remaining CPUs match it.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: made boot_cpu_hyp_mode static]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 9f22dd607958..a5c2b4852154 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -60,6 +60,12 @@ static inline bool is_kernel_in_hyp_mode(void)
 	return el == CurrentEL_EL2;
 }
 
+#ifdef CONFIG_ARM64_VHE
+extern void verify_cpu_run_el(void);
+#else
+static inline void verify_cpu_run_el(void) {}
+#endif
+
 /* The section containing the hypervisor text */
 extern char __hyp_text_start[];
 extern char __hyp_text_end[];

commit 82deae0fc8ba256c1061dd4de42f0ef6cb9f954f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Jun 9 19:47:09 2014 +0100

    arm/arm64: Add new is_kernel_in_hyp_mode predicate
    
    With ARMv8.1 VHE extension, it will be possible to run the kernel
    at EL2 (aka HYP mode). In order for the kernel to easily find out
    where it is running, add a new predicate that returns whether or
    not the kernel is in HYP mode.
    
    For completeness, the 32bit code also get such a predicate (always
    returning false) so that code common to both architecture (timers,
    KVM) can use it transparently.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 7a5df5252dd7..9f22dd607958 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -23,6 +23,8 @@
 
 #ifndef __ASSEMBLY__
 
+#include <asm/ptrace.h>
+
 /*
  * __boot_cpu_mode records what mode CPUs were booted in.
  * A correctly-implemented bootloader must start all CPUs in the same mode:
@@ -50,6 +52,14 @@ static inline bool is_hyp_mode_mismatched(void)
 	return __boot_cpu_mode[0] != __boot_cpu_mode[1];
 }
 
+static inline bool is_kernel_in_hyp_mode(void)
+{
+	u64 el;
+
+	asm("mrs %0, CurrentEL" : "=r" (el));
+	return el == CurrentEL_EL2;
+}
+
 /* The section containing the hypervisor text */
 extern char __hyp_text_start[];
 extern char __hyp_text_end[];

commit 45451914c875bba44903ce4f1445e047b7992bf7
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jun 26 15:16:40 2013 +0100

    arm64: KVM: remove __kvm_hyp_code_{start,end} from hyp.S
    
    We already have __hyp_text_{start,end} to express the boundaries
    of the HYP text section, and __kvm_hyp_code_{start,end} are getting
    in the way of a more modular world switch code.
    
    Just turn __kvm_hyp_code_{start,end} into #defines mapping the
    linker-emited symbols.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 215ad4649dd7..7a5df5252dd7 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -50,6 +50,10 @@ static inline bool is_hyp_mode_mismatched(void)
 	return __boot_cpu_mode[0] != __boot_cpu_mode[1];
 }
 
+/* The section containing the hypervisor text */
+extern char __hyp_text_start[];
+extern char __hyp_text_end[];
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* ! __ASM__VIRT_H */

commit 0a997ecc08e0b551119c56d52a591d9e5b38a7cd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Mar 28 09:49:13 2014 +0000

    Revert "arm64: virt: ensure visibility of __boot_cpu_mode"
    
    This reverts commit 82b2f495fba338d1e3098dde1df54944a9c19751. The
    __boot_cpu_mode variable is flushed in head.S after being written,
    therefore the additional cache flushing is no longer required.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 130e2be952cf..215ad4649dd7 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -22,7 +22,6 @@
 #define BOOT_CPU_MODE_EL2	(0xe12)
 
 #ifndef __ASSEMBLY__
-#include <asm/cacheflush.h>
 
 /*
  * __boot_cpu_mode records what mode CPUs were booted in.
@@ -38,20 +37,9 @@ extern u32 __boot_cpu_mode[2];
 void __hyp_set_vectors(phys_addr_t phys_vector_base);
 phys_addr_t __hyp_get_vectors(void);
 
-static inline void sync_boot_mode(void)
-{
-	/*
-	 * As secondaries write to __boot_cpu_mode with caches disabled, we
-	 * must flush the corresponding cache entries to ensure the visibility
-	 * of their writes.
-	 */
-	__flush_dcache_area(__boot_cpu_mode, sizeof(__boot_cpu_mode));
-}
-
 /* Reports the availability of HYP mode */
 static inline bool is_hyp_mode_available(void)
 {
-	sync_boot_mode();
 	return (__boot_cpu_mode[0] == BOOT_CPU_MODE_EL2 &&
 		__boot_cpu_mode[1] == BOOT_CPU_MODE_EL2);
 }
@@ -59,7 +47,6 @@ static inline bool is_hyp_mode_available(void)
 /* Check if the bootloader has booted CPUs in different modes */
 static inline bool is_hyp_mode_mismatched(void)
 {
-	sync_boot_mode();
 	return __boot_cpu_mode[0] != __boot_cpu_mode[1];
 }
 

commit 828e9834e9a5b7e61046aa3c5f603a4fecba2fb4
Author: Matthew Leach <matthew.leach@arm.com>
Date:   Fri Oct 11 14:52:16 2013 +0100

    arm64: head: create a new function for setting the boot_cpu_mode flag
    
    Currently, the code for setting the __cpu_boot_mode flag is munged in
    with el2_setup. This makes things difficult on a BE bringup as a
    memory access has to have occurred before el2_setup which is the place
    that we'd like to set the endianess on the current EL.
    
    Create a new function for setting __cpu_boot_mode and have el2_setup
    return the mode the CPU. Also define a new constant in virt.h,
    BOOT_CPU_MODE_EL1, for readability.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Matthew Leach <matthew.leach@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 26e310c54344..130e2be952cf 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -18,7 +18,8 @@
 #ifndef __ASM__VIRT_H
 #define __ASM__VIRT_H
 
-#define BOOT_CPU_MODE_EL2	(0x0e12b007)
+#define BOOT_CPU_MODE_EL1	(0xe11)
+#define BOOT_CPU_MODE_EL2	(0xe12)
 
 #ifndef __ASSEMBLY__
 #include <asm/cacheflush.h>

commit 82b2f495fba338d1e3098dde1df54944a9c19751
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jul 9 15:16:06 2013 +0100

    arm64: virt: ensure visibility of __boot_cpu_mode
    
    Secondary CPUs write to __boot_cpu_mode with caches disabled, and thus a
    cached value of __boot_cpu_mode may be incoherent with that in memory.
    This could lead to a failure to detect mismatched boot modes.
    
    This patch adds flushing to ensure that writes by secondaries to
    __boot_cpu_mode are made visible before we test against it.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@cs.columbia.edu>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index 439827271e3d..26e310c54344 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -21,6 +21,7 @@
 #define BOOT_CPU_MODE_EL2	(0x0e12b007)
 
 #ifndef __ASSEMBLY__
+#include <asm/cacheflush.h>
 
 /*
  * __boot_cpu_mode records what mode CPUs were booted in.
@@ -36,9 +37,20 @@ extern u32 __boot_cpu_mode[2];
 void __hyp_set_vectors(phys_addr_t phys_vector_base);
 phys_addr_t __hyp_get_vectors(void);
 
+static inline void sync_boot_mode(void)
+{
+	/*
+	 * As secondaries write to __boot_cpu_mode with caches disabled, we
+	 * must flush the corresponding cache entries to ensure the visibility
+	 * of their writes.
+	 */
+	__flush_dcache_area(__boot_cpu_mode, sizeof(__boot_cpu_mode));
+}
+
 /* Reports the availability of HYP mode */
 static inline bool is_hyp_mode_available(void)
 {
+	sync_boot_mode();
 	return (__boot_cpu_mode[0] == BOOT_CPU_MODE_EL2 &&
 		__boot_cpu_mode[1] == BOOT_CPU_MODE_EL2);
 }
@@ -46,6 +58,7 @@ static inline bool is_hyp_mode_available(void)
 /* Check if the bootloader has booted CPUs in different modes */
 static inline bool is_hyp_mode_mismatched(void)
 {
+	sync_boot_mode();
 	return __boot_cpu_mode[0] != __boot_cpu_mode[1];
 }
 

commit 712c6ff4dba4917a440be601dc312506322bffe8
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Oct 19 17:46:27 2012 +0100

    arm64: add hypervisor stub
    
    If booted in EL2, install an dummy hypervisor whose only purpose
    is to be replaced by a full fledged one.
    
    A minimal API allows to:
    - obtain the current HYP vectors (__hyp_get_vectors)
    - set new HYP vectors (__hyp_set_vectors)
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
index f28547d9edfa..439827271e3d 100644
--- a/arch/arm64/include/asm/virt.h
+++ b/arch/arm64/include/asm/virt.h
@@ -33,6 +33,9 @@
  */
 extern u32 __boot_cpu_mode[2];
 
+void __hyp_set_vectors(phys_addr_t phys_vector_base);
+phys_addr_t __hyp_get_vectors(void);
+
 /* Reports the availability of HYP mode */
 static inline bool is_hyp_mode_available(void)
 {

commit f35a92053b45cf8154db5558ede3ba5245c9dc7e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Oct 26 15:40:05 2012 +0100

    arm64: record boot mode when entering the kernel
    
    To be able to signal the availability of EL2 to other parts of
    the kernel, record the boot mode.
    
    Once booted, two predicates indicate if HYP mode is available,
    and if not, whether this is due to a boot mode mismatch or not.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
new file mode 100644
index 000000000000..f28547d9edfa
--- /dev/null
+++ b/arch/arm64/include/asm/virt.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __ASM__VIRT_H
+#define __ASM__VIRT_H
+
+#define BOOT_CPU_MODE_EL2	(0x0e12b007)
+
+#ifndef __ASSEMBLY__
+
+/*
+ * __boot_cpu_mode records what mode CPUs were booted in.
+ * A correctly-implemented bootloader must start all CPUs in the same mode:
+ * In this case, both 32bit halves of __boot_cpu_mode will contain the
+ * same value (either 0 if booted in EL1, BOOT_CPU_MODE_EL2 if booted in EL2).
+ *
+ * Should the bootloader fail to do this, the two values will be different.
+ * This allows the kernel to flag an error when the secondaries have come up.
+ */
+extern u32 __boot_cpu_mode[2];
+
+/* Reports the availability of HYP mode */
+static inline bool is_hyp_mode_available(void)
+{
+	return (__boot_cpu_mode[0] == BOOT_CPU_MODE_EL2 &&
+		__boot_cpu_mode[1] == BOOT_CPU_MODE_EL2);
+}
+
+/* Check if the bootloader has booted CPUs in different modes */
+static inline bool is_hyp_mode_mismatched(void)
+{
+	return __boot_cpu_mode[0] != __boot_cpu_mode[1];
+}
+
+#endif /* __ASSEMBLY__ */
+
+#endif /* ! __ASM__VIRT_H */
