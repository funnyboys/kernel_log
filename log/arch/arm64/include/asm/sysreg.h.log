commit d27865279f12035c730818aa1a0280fada866a37
Merge: 342403bcb4df a4eb355a3fda
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:00:51 2020 +0100

    Merge branch 'for-next/bti' into for-next/core
    
    Support for Branch Target Identification (BTI) in user and kernel
    (Mark Brown and others)
    * for-next/bti: (39 commits)
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      arm64: bti: Fix support for userspace only BTI
      arm64: kconfig: Update and comment GCC version check for kernel BTI
      arm64: vdso: Map the vDSO text with guarded pages when built for BTI
      arm64: vdso: Force the vDSO to be linked as BTI when built for BTI
      arm64: vdso: Annotate for BTI
      arm64: asm: Provide a mechanism for generating ELF note for BTI
      arm64: bti: Provide Kconfig for kernel mode BTI
      arm64: mm: Mark executable text as guarded pages
      arm64: bpf: Annotate JITed code for BTI
      arm64: Set GP bit in kernel page tables to enable BTI for the kernel
      arm64: asm: Override SYM_FUNC_START when building the kernel with BTI
      arm64: bti: Support building kernel C code using BTI
      arm64: Document why we enable PAC support for leaf functions
      arm64: insn: Report PAC and BTI instructions as skippable
      arm64: insn: Don't assume unrecognized HINTs are skippable
      arm64: insn: Provide a better name for aarch64_insn_is_nop()
      arm64: insn: Add constants for new HINT instruction decode
      arm64: Disable old style assembly annotations
      ...

commit b130a8f70cbbf907c399799bd1073a78763ca0e7
Author: Marc Zyngier <maz@kernel.org>
Date:   Thu May 28 14:12:58 2020 +0100

    KVM: arm64: Check advertised Stage-2 page size capability
    
    With ARMv8.5-GTG, the hardware (or more likely a hypervisor) can
    advertise the supported Stage-2 page sizes.
    
    Let's check this at boot time.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Alexandru Elisei <alexandru.elisei@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index fa9d02ca4b25..efe368ee4996 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -703,6 +703,9 @@
 #define ID_AA64ZFR0_SVEVER_SVE2		0x1
 
 /* id_aa64mmfr0 */
+#define ID_AA64MMFR0_TGRAN4_2_SHIFT	40
+#define ID_AA64MMFR0_TGRAN64_2_SHIFT	36
+#define ID_AA64MMFR0_TGRAN16_2_SHIFT	32
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28
 #define ID_AA64MMFR0_TGRAN64_SHIFT	24
 #define ID_AA64MMFR0_TGRAN16_SHIFT	20

commit 14e270fa5c4cde8f6cc0240a2ab007b58a367f16
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:48 2020 +0530

    arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
    
    Enable the following features bits in ID_AA64PFR1 register as per ARM DDI
    0487F.a specification.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Will Deacon <will@kernel.org>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-12-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 638f6108860f..fa9d02ca4b25 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -670,7 +670,11 @@
 #define ID_AA64PFR0_EL0_32BIT_64BIT	0x2
 
 /* id_aa64pfr1 */
+#define ID_AA64PFR1_MPAMFRAC_SHIFT	16
+#define ID_AA64PFR1_RASFRAC_SHIFT	12
+#define ID_AA64PFR1_MTE_SHIFT		8
 #define ID_AA64PFR1_SSBS_SHIFT		4
+#define ID_AA64PFR1_BT_SHIFT		0
 
 #define ID_AA64PFR1_SSBS_PSTATE_NI	0
 #define ID_AA64PFR1_SSBS_PSTATE_ONLY	1

commit 011e5f5bf529f8ec2988ef7667d1a52f83273c36
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:47 2020 +0530

    arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
    
    Enable MPAM and SEL2 features bits in ID_AA64PFR0 register as per ARM DDI
    0487F.a specification.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Will Deacon <will@kernel.org>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-11-git-send-email-anshuman.khandual@arm.com
    [will: Make SEL2 a NONSTRICT feature per Suzuki]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ea075cc08c8f..638f6108860f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -645,6 +645,8 @@
 #define ID_AA64PFR0_CSV2_SHIFT		56
 #define ID_AA64PFR0_DIT_SHIFT		48
 #define ID_AA64PFR0_AMU_SHIFT		44
+#define ID_AA64PFR0_MPAM_SHIFT		40
+#define ID_AA64PFR0_SEL2_SHIFT		36
 #define ID_AA64PFR0_SVE_SHIFT		32
 #define ID_AA64PFR0_RAS_SHIFT		28
 #define ID_AA64PFR0_GIC_SHIFT		24

commit 7cd51a5a84d115cd49c43e90b083ca60873874e5
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:46 2020 +0530

    arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
    
    Enable TLB features bit in ID_AA64ISAR0 register as per ARM DDI 0487F.a
    specification.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Will Deacon <will@kernel.org>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-10-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 0a0cbb3add89..ea075cc08c8f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -601,6 +601,7 @@
 
 /* id_aa64isar0 */
 #define ID_AA64ISAR0_RNDR_SHIFT		60
+#define ID_AA64ISAR0_TLB_SHIFT		56
 #define ID_AA64ISAR0_TS_SHIFT		52
 #define ID_AA64ISAR0_FHM_SHIFT		48
 #define ID_AA64ISAR0_DP_SHIFT		44

commit fcd6535322cccf21830031f389c302346f767c47
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:45 2020 +0530

    arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
    
    Enable all remaining feature bits like EVT, CCIDX, LSM, HPDS, CnP, XNX,
    SpecSEI in ID_MMFR4 register per ARM DDI 0487F.a.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-9-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 02b1246e7dbf..0a0cbb3add89 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -794,6 +794,14 @@
 #define ID_ISAR6_DP_SHIFT		4
 #define ID_ISAR6_JSCVT_SHIFT		0
 
+#define ID_MMFR4_EVT_SHIFT		28
+#define ID_MMFR4_CCIDX_SHIFT		24
+#define ID_MMFR4_LSM_SHIFT		20
+#define ID_MMFR4_HPDS_SHIFT		16
+#define ID_MMFR4_CNP_SHIFT		12
+#define ID_MMFR4_XNX_SHIFT		8
+#define ID_MMFR4_SPECSEI_SHIFT		0
+
 #define ID_MMFR5_ETS_SHIFT		0
 
 #define ID_PFR0_DIT_SHIFT		24

commit 0ae43a99fe91399d9e5b2d0756e16e97b21cc187
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:44 2020 +0530

    arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
    
    Enable DIT and CSV2 feature bits in ID_PFR0 register as per ARM DDI 0487F.a
    specification. Except RAS and AMU, all other feature bits are now enabled.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-8-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b7f549d09c1c..02b1246e7dbf 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -796,6 +796,9 @@
 
 #define ID_MMFR5_ETS_SHIFT		0
 
+#define ID_PFR0_DIT_SHIFT		24
+#define ID_PFR0_CSV2_SHIFT		16
+
 #define ID_PFR2_SSBS_SHIFT		4
 #define ID_PFR2_CSV3_SHIFT		0
 

commit 152accf8476f0447de57d9ffaf0c5ab578ed3d40
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:43 2020 +0530

    arm64/cpufeature: Introduce ID_MMFR5 CPU register
    
    This adds basic building blocks required for ID_MMFR5 CPU register which
    provides information about the implemented memory model and memory
    management support in AArch32 state. This is added per ARM DDI 0487F.a
    specification.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Will Deacon <will@kernel.org>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-7-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index c1c97e08a799..b7f549d09c1c 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -147,6 +147,7 @@
 #define SYS_ID_MMFR2_EL1		sys_reg(3, 0, 0, 1, 6)
 #define SYS_ID_MMFR3_EL1		sys_reg(3, 0, 0, 1, 7)
 #define SYS_ID_MMFR4_EL1		sys_reg(3, 0, 0, 2, 6)
+#define SYS_ID_MMFR5_EL1		sys_reg(3, 0, 0, 3, 6)
 
 #define SYS_ID_ISAR0_EL1		sys_reg(3, 0, 0, 2, 0)
 #define SYS_ID_ISAR1_EL1		sys_reg(3, 0, 0, 2, 1)
@@ -793,6 +794,8 @@
 #define ID_ISAR6_DP_SHIFT		4
 #define ID_ISAR6_JSCVT_SHIFT		0
 
+#define ID_MMFR5_ETS_SHIFT		0
+
 #define ID_PFR2_SSBS_SHIFT		4
 #define ID_PFR2_CSV3_SHIFT		0
 

commit dd35ec07045753adcc86f89d485e589d860e298f
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:42 2020 +0530

    arm64/cpufeature: Introduce ID_DFR1 CPU register
    
    This adds basic building blocks required for ID_DFR1 CPU register which
    provides top level information about the debug system in AArch32 state.
    We hide the register from KVM guests, as we don't emulate the 'MTPMU'
    feature.
    
    This is added per ARM DDI 0487F.a specification.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Will Deacon <will@kernel.org>
    Reviewed-by : Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-6-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index d0ea916b8528..c1c97e08a799 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -140,6 +140,7 @@
 #define SYS_ID_PFR1_EL1			sys_reg(3, 0, 0, 1, 1)
 #define SYS_ID_PFR2_EL1			sys_reg(3, 0, 0, 3, 4)
 #define SYS_ID_DFR0_EL1			sys_reg(3, 0, 0, 1, 2)
+#define SYS_ID_DFR1_EL1			sys_reg(3, 0, 0, 3, 5)
 #define SYS_ID_AFR0_EL1			sys_reg(3, 0, 0, 1, 3)
 #define SYS_ID_MMFR0_EL1		sys_reg(3, 0, 0, 1, 4)
 #define SYS_ID_MMFR1_EL1		sys_reg(3, 0, 0, 1, 5)
@@ -767,6 +768,8 @@
 #define ID_ISAR4_WITHSHIFTS_SHIFT	4
 #define ID_ISAR4_UNPRIV_SHIFT		0
 
+#define ID_DFR1_MTPMU_SHIFT		0
+
 #define ID_ISAR0_DIVIDE_SHIFT		24
 #define ID_ISAR0_DEBUG_SHIFT		20
 #define ID_ISAR0_COPROC_SHIFT		16

commit 16824085a7dd426408d4403284374da90671e749
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:41 2020 +0530

    arm64/cpufeature: Introduce ID_PFR2 CPU register
    
    This adds basic building blocks required for ID_PFR2 CPU register which
    provides information about the AArch32 programmers model which must be
    interpreted along with ID_PFR0 and ID_PFR1 CPU registers. This is added
    per ARM DDI 0487F.a specification.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-5-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ea55fe5925c4..d0ea916b8528 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -138,6 +138,7 @@
 
 #define SYS_ID_PFR0_EL1			sys_reg(3, 0, 0, 1, 0)
 #define SYS_ID_PFR1_EL1			sys_reg(3, 0, 0, 1, 1)
+#define SYS_ID_PFR2_EL1			sys_reg(3, 0, 0, 3, 4)
 #define SYS_ID_DFR0_EL1			sys_reg(3, 0, 0, 1, 2)
 #define SYS_ID_AFR0_EL1			sys_reg(3, 0, 0, 1, 3)
 #define SYS_ID_MMFR0_EL1		sys_reg(3, 0, 0, 1, 4)
@@ -789,6 +790,9 @@
 #define ID_ISAR6_DP_SHIFT		4
 #define ID_ISAR6_JSCVT_SHIFT		0
 
+#define ID_PFR2_SSBS_SHIFT		4
+#define ID_PFR2_CSV3_SHIFT		0
+
 #define MVFR0_FPROUND_SHIFT		28
 #define MVFR0_FPSHVEC_SHIFT		24
 #define MVFR0_FPSQRT_SHIFT		20

commit 2a5bc6c47bc3b1bcdab5bef7e74fbb74d17dc618
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 19 15:10:38 2020 +0530

    arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
    
    ID_ISAR0[31..28] bits are RES0 in ARMv8, Reserved/UNK in ARMv7. Currently
    these bits get exposed through generic_id_ftr32[] which is not desirable.
    Hence define an explicit ftr_id_isar0[] array for ID_ISAR0 register where
    those bits can be hidden.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/1589881254-10082-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 194684301df0..ea55fe5925c4 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -766,6 +766,14 @@
 #define ID_ISAR4_WITHSHIFTS_SHIFT	4
 #define ID_ISAR4_UNPRIV_SHIFT		0
 
+#define ID_ISAR0_DIVIDE_SHIFT		24
+#define ID_ISAR0_DEBUG_SHIFT		20
+#define ID_ISAR0_COPROC_SHIFT		16
+#define ID_ISAR0_CMPBRANCH_SHIFT	12
+#define ID_ISAR0_BITFIELD_SHIFT		8
+#define ID_ISAR0_BITCOUNT_SHIFT		4
+#define ID_ISAR0_SWAP_SHIFT		0
+
 #define ID_ISAR5_RDM_SHIFT		24
 #define ID_ISAR5_CRC32_SHIFT		16
 #define ID_ISAR5_SHA2_SHIFT		12

commit 80e4e561321595d2e5f4a173e8cf8d8432078995
Merge: 6a8b55ed4056 5d1b631c773f
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 15:15:58 2020 +0100

    Merge branch 'for-next/bti-user' into for-next/bti
    
    Merge in user support for Branch Target Identification, which narrowly
    missed the cut for 5.7 after a late ABI concern.
    
    * for-next/bti-user:
      arm64: bti: Document behaviour for dynamically linked binaries
      arm64: elf: Fix allnoconfig kernel build with !ARCH_USE_GNU_PROPERTY
      arm64: BTI: Add Kconfig entry for userspace BTI
      mm: smaps: Report arm64 guarded pages in smaps
      arm64: mm: Display guarded pages in ptdump
      KVM: arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: traps: Shuffle code to eliminate forward declarations
      arm64: unify native/compat instruction skipping
      arm64: BTI: Decode BYTPE bits when printing PSTATE
      arm64: elf: Enable BTI at exec based on ELF program properties
      elf: Allow arch to tweak initial mmap prot flags
      arm64: Basic Branch Target Identification support
      ELF: Add ELF program property parsing support
      ELF: UAPI and Kconfig additions for ELF program properties

commit eeb2d87ece1219cff33e5107c2a6d57803354d15
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 13:08:02 2020 +0100

    arm64: cpufeature: Group indexed system register definitions by name
    
    Some system registers contain an index in the name (e.g. ID_MMFR<n>_EL1)
    and, while this index often follows the register encoding, newer additions
    to the architecture are necessarily tacked on the end. Sorting these
    registers by encoding therefore becomes a bit of a mess.
    
    Group the indexed system register definitions by name so that it's easier to
    read and will hopefully reduce the chance of us accidentally introducing
    duplicate definitions in the future.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 2dd3f4ca9780..194684301df0 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -105,6 +105,10 @@
 #define SYS_DC_CSW			sys_insn(1, 0, 7, 10, 2)
 #define SYS_DC_CISW			sys_insn(1, 0, 7, 14, 2)
 
+/*
+ * System registers, organised loosely by encoding but grouped together
+ * where the architected name contains an index. e.g. ID_MMFR<n>_EL1.
+ */
 #define SYS_OSDTRRX_EL1			sys_reg(2, 0, 0, 0, 2)
 #define SYS_MDCCINT_EL1			sys_reg(2, 0, 0, 2, 0)
 #define SYS_MDSCR_EL1			sys_reg(2, 0, 0, 2, 2)
@@ -140,6 +144,7 @@
 #define SYS_ID_MMFR1_EL1		sys_reg(3, 0, 0, 1, 5)
 #define SYS_ID_MMFR2_EL1		sys_reg(3, 0, 0, 1, 6)
 #define SYS_ID_MMFR3_EL1		sys_reg(3, 0, 0, 1, 7)
+#define SYS_ID_MMFR4_EL1		sys_reg(3, 0, 0, 2, 6)
 
 #define SYS_ID_ISAR0_EL1		sys_reg(3, 0, 0, 2, 0)
 #define SYS_ID_ISAR1_EL1		sys_reg(3, 0, 0, 2, 1)
@@ -147,7 +152,6 @@
 #define SYS_ID_ISAR3_EL1		sys_reg(3, 0, 0, 2, 3)
 #define SYS_ID_ISAR4_EL1		sys_reg(3, 0, 0, 2, 4)
 #define SYS_ID_ISAR5_EL1		sys_reg(3, 0, 0, 2, 5)
-#define SYS_ID_MMFR4_EL1		sys_reg(3, 0, 0, 2, 6)
 #define SYS_ID_ISAR6_EL1		sys_reg(3, 0, 0, 2, 7)
 
 #define SYS_MVFR0_EL1			sys_reg(3, 0, 0, 3, 0)

commit 9b5aaec441d3c76d6255049e99c446f14c89a284
Author: Ard Biesheuvel <ardb@kernel.org>
Date:   Tue Apr 14 15:47:06 2020 +0200

    arm64: drop duplicate definitions of ID_AA64MMFR0_TGRAN constants
    
    A bunch of ID_AA64MMFR0_TGRAN_* constant definitions appear twice
    in arch/arm64/include/asm/sysreg.h, so drop the duplicates.
    
    Signed-off-by: Ard Biesheuvel <ardb@kernel.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/20200414134706.8435-1-ardb@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index c4e896bf77f3..2dd3f4ca9780 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -804,17 +804,6 @@
 #define ID_PFR1_SECURITY_SHIFT		4
 #define ID_PFR1_PROGMOD_SHIFT		0
 
-#define ID_AA64MMFR0_TGRAN4_SHIFT	28
-#define ID_AA64MMFR0_TGRAN64_SHIFT	24
-#define ID_AA64MMFR0_TGRAN16_SHIFT	20
-
-#define ID_AA64MMFR0_TGRAN4_NI		0xf
-#define ID_AA64MMFR0_TGRAN4_SUPPORTED	0x0
-#define ID_AA64MMFR0_TGRAN64_NI		0xf
-#define ID_AA64MMFR0_TGRAN64_SUPPORTED	0x0
-#define ID_AA64MMFR0_TGRAN16_NI		0x0
-#define ID_AA64MMFR0_TGRAN16_SUPPORTED	0x1
-
 #if defined(CONFIG_ARM64_4K_PAGES)
 #define ID_AA64MMFR0_TGRAN_SHIFT	ID_AA64MMFR0_TGRAN4_SHIFT
 #define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN4_SUPPORTED

commit 540f76d12c662d3da2ebdf0086ee289123fcd120
Author: Will Deacon <will@kernel.org>
Date:   Tue Apr 21 15:29:17 2020 +0100

    arm64: cpufeature: Add CPU capability for AArch32 EL1 support
    
    Although we emit a "SANITY CHECK" warning and taint the kernel if we
    detect a CPU mismatch for AArch32 support at EL1, we still online the
    CPU with disastrous consequences for any running 32-bit VMs.
    
    Introduce a capability for AArch32 support at EL1 so that late onlining
    of incompatible CPUs is forbidden.
    
    Tested-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200421142922.18950-4-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index d7181972d28d..c4e896bf77f3 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -655,6 +655,7 @@
 #define ID_AA64PFR0_ASIMD_NI		0xf
 #define ID_AA64PFR0_ASIMD_SUPPORTED	0x0
 #define ID_AA64PFR0_EL1_64BIT_ONLY	0x1
+#define ID_AA64PFR0_EL1_32BIT_64BIT	0x2
 #define ID_AA64PFR0_EL0_64BIT_ONLY	0x1
 #define ID_AA64PFR0_EL0_32BIT_64BIT	0x2
 

commit 0113340e6e83f8710b216f72b826499fc0151c29
Author: Will Deacon <will@kernel.org>
Date:   Tue Apr 21 15:29:16 2020 +0100

    arm64: cpufeature: Spell out register fields for ID_ISAR4 and ID_PFR1
    
    In preparation for runtime updates to the strictness of some AArch32
    features, spell out the register fields for ID_ISAR4 and ID_PFR1 to make
    things clearer to read. Note that this isn't functionally necessary, as
    the feature arrays themselves are not modified dynamically and remain
    'const'.
    
    Tested-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/20200421142922.18950-3-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index c4ac0ac25a00..d7181972d28d 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -752,6 +752,15 @@
 
 #define ID_DFR0_PERFMON_8_1		0x4
 
+#define ID_ISAR4_SWP_FRAC_SHIFT		28
+#define ID_ISAR4_PSR_M_SHIFT		24
+#define ID_ISAR4_SYNCH_PRIM_FRAC_SHIFT	20
+#define ID_ISAR4_BARRIER_SHIFT		16
+#define ID_ISAR4_SMC_SHIFT		12
+#define ID_ISAR4_WRITEBACK_SHIFT	8
+#define ID_ISAR4_WITHSHIFTS_SHIFT	4
+#define ID_ISAR4_UNPRIV_SHIFT		0
+
 #define ID_ISAR5_RDM_SHIFT		24
 #define ID_ISAR5_CRC32_SHIFT		16
 #define ID_ISAR5_SHA2_SHIFT		12
@@ -785,6 +794,14 @@
 #define MVFR1_FPDNAN_SHIFT		4
 #define MVFR1_FPFTZ_SHIFT		0
 
+#define ID_PFR1_GIC_SHIFT		28
+#define ID_PFR1_VIRT_FRAC_SHIFT		24
+#define ID_PFR1_SEC_FRAC_SHIFT		20
+#define ID_PFR1_GENTIMER_SHIFT		16
+#define ID_PFR1_VIRTUALIZATION_SHIFT	12
+#define ID_PFR1_MPROGMOD_SHIFT		8
+#define ID_PFR1_SECURITY_SHIFT		4
+#define ID_PFR1_PROGMOD_SHIFT		0
 
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28
 #define ID_AA64MMFR0_TGRAN64_SHIFT	24

commit c9a4ef66450145a356a626c833d3d7b1668b3ded
Author: Fangrui Song <maskray@google.com>
Date:   Tue Apr 14 09:32:55 2020 -0700

    arm64: Delete the space separator in __emit_inst
    
    In assembly, many instances of __emit_inst(x) expand to a directive. In
    a few places __emit_inst(x) is used as an assembler macro argument. For
    example, in arch/arm64/kvm/hyp/entry.S
    
      ALTERNATIVE(nop, SET_PSTATE_PAN(1), ARM64_HAS_PAN, CONFIG_ARM64_PAN)
    
    expands to the following by the C preprocessor:
    
      alternative_insn nop, .inst (0xd500401f | ((0) << 16 | (4) << 5) | ((!!1) << 8)), 4, 1
    
    Both comma and space are separators, with an exception that content
    inside a pair of parentheses/quotes is not split, so the clang
    integrated assembler splits the arguments to:
    
       nop, .inst, (0xd500401f | ((0) << 16 | (4) << 5) | ((!!1) << 8)), 4, 1
    
    GNU as preprocesses the input with do_scrub_chars(). Its arm64 backend
    (along with many other non-x86 backends) sees:
    
      alternative_insn nop,.inst(0xd500401f|((0)<<16|(4)<<5)|((!!1)<<8)),4,1
      # .inst(...) is parsed as one argument
    
    while its x86 backend sees:
    
      alternative_insn nop,.inst (0xd500401f|((0)<<16|(4)<<5)|((!!1)<<8)),4,1
      # The extra space before '(' makes the whole .inst (...) parsed as two arguments
    
    The non-x86 backend's behavior is considered unintentional
    (https://sourceware.org/bugzilla/show_bug.cgi?id=25750).
    So drop the space separator inside `.inst (...)` to make the clang
    integrated assembler work.
    
    Suggested-by: Ilie Halip <ilie.halip@gmail.com>
    Signed-off-by: Fangrui Song <maskray@google.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Link: https://github.com/ClangBuiltLinux/linux/issues/939
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ebc622432831..c4ac0ac25a00 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -49,7 +49,9 @@
 #ifndef CONFIG_BROKEN_GAS_INST
 
 #ifdef __ASSEMBLY__
-#define __emit_inst(x)			.inst (x)
+// The space separator is omitted so that __emit_inst(x) can be parsed as
+// either an assembler directive or an assembler macro argument.
+#define __emit_inst(x)			.inst(x)
 #else
 #define __emit_inst(x)			".inst " __stringify((x)) "\n\t"
 #endif

commit da12d2739fb69531bf6bb6eb7e46d73d1dabc814
Merge: bbd6ec605c0f f7d5ef0c654e c265861af2af b5475d8caedb de58ed5e16e6 c17a290f7e7e 8673e02e5841
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:10:32 2020 +0000

    Merge branches 'for-next/memory-hotremove', 'for-next/arm_sdei', 'for-next/amu', 'for-next/final-cap-helper', 'for-next/cpu_ops-cleanup', 'for-next/misc' and 'for-next/perf' into for-next/core
    
    * for-next/memory-hotremove:
      : Memory hot-remove support for arm64
      arm64/mm: Enable memory hot remove
      arm64/mm: Hold memory hotplug lock while walking for kernel page table dump
    
    * for-next/arm_sdei:
      : SDEI: fix double locking on return from hibernate and clean-up
      firmware: arm_sdei: clean up sdei_event_create()
      firmware: arm_sdei: Use cpus_read_lock() to avoid races with cpuhp
      firmware: arm_sdei: fix possible double-lock on hibernate error path
      firmware: arm_sdei: fix double-lock on hibernate with shared events
    
    * for-next/amu:
      : ARMv8.4 Activity Monitors support
      clocksource/drivers/arm_arch_timer: validate arch_timer_rate
      arm64: use activity monitors for frequency invariance
      cpufreq: add function to get the hardware max frequency
      Documentation: arm64: document support for the AMU extension
      arm64/kvm: disable access to AMU registers from kvm guests
      arm64: trap to EL1 accesses to AMU counters from EL0
      arm64: add support for the AMU extension v1
    
    * for-next/final-cap-helper:
      : Introduce cpus_have_final_cap_helper(), migrate arm64 KVM to it
      arm64: kvm: hyp: use cpus_have_final_cap()
      arm64: cpufeature: add cpus_have_final_cap()
    
    * for-next/cpu_ops-cleanup:
      : cpu_ops[] access code clean-up
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: define __alloc_zeroed_user_highpage
      arm64/kernel: Simplify __cpu_up() by bailing out early
      arm64: remove redundant blank for '=' operator
      arm64: kexec_file: Fixed code style.
      arm64: add blank after 'if'
      arm64: fix spelling mistake "ca not" -> "cannot"
      arm64: entry: unmask IRQ in el0_sp()
      arm64: efi: add efi-entry.o to targets instead of extra-$(CONFIG_EFI)
      arm64: csum: Optimise IPv6 header checksum
      arch/arm64: fix typo in a comment
      arm64: remove gratuitious/stray .ltorg stanzas
      arm64: Update comment for ASID() macro
      arm64: mm: convert cpu_do_switch_mm() to C
      arm64: fix NUMA Kconfig typos
    
    * for-next/perf:
      : arm64 perf updates
      arm64: perf: Add support for ARMv8.5-PMU 64-bit counters
      KVM: arm64: limit PMU version to PMUv3 for ARMv8.1
      arm64: cpufeature: Extract capped perfmon fields
      arm64: perf: Clean up enable/disable calls
      perf: arm-ccn: Use scnprintf() for robustness
      arm64: perf: Support new DT compatibles
      arm64: perf: Refactor PMU init callbacks
      perf: arm_spe: Remove unnecessary zero check on 'nr_pages'

commit 8673e02e58410e6c4cefa499efa846286e45a991
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Mon Mar 2 18:17:52 2020 +0000

    arm64: perf: Add support for ARMv8.5-PMU 64-bit counters
    
    At present ARMv8 event counters are limited to 32-bits, though by
    using the CHAIN event it's possible to combine adjacent counters to
    achieve 64-bits. The perf config1:0 bit can be set to use such a
    configuration.
    
    With the introduction of ARMv8.5-PMU support, all event counters can
    now be used as 64-bit counters.
    
    Let's enable 64-bit event counters where support exists. Unless the
    user sets config1:0 we will adjust the counter value such that it
    overflows upon 32-bit overflow. This follows the same behaviour as
    the cycle counter which has always been (and remains) 64-bits.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [Mark: fix ID field names, compare with 8.5 value]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index d8f1eed070f0..9b66c5b5b36f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -702,7 +702,11 @@
 #define ID_AA64DFR0_TRACEVER_SHIFT	4
 #define ID_AA64DFR0_DEBUGVER_SHIFT	0
 
+#define ID_AA64DFR0_PMUVER_8_0		0x1
 #define ID_AA64DFR0_PMUVER_8_1		0x4
+#define ID_AA64DFR0_PMUVER_8_4		0x5
+#define ID_AA64DFR0_PMUVER_8_5		0x6
+#define ID_AA64DFR0_PMUVER_IMP_DEF	0xf
 
 #define ID_DFR0_PERFMON_SHIFT		24
 

commit c854188ea01062f5a5fd7f05658feb1863774eaa
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Mon Mar 2 18:17:51 2020 +0000

    KVM: arm64: limit PMU version to PMUv3 for ARMv8.1
    
    We currently expose the PMU version of the host to the guest via
    emulation of the DFR0_EL1 and AA64DFR0_EL1 debug feature registers.
    However many of the features offered beyond PMUv3 for 8.1 are not
    supported in KVM. Examples of this include support for the PMMIR
    registers (added in PMUv3 for ARMv8.4) and 64-bit event counters
    added in (PMUv3 for ARMv8.5).
    
    Let's trap the Debug Feature Registers in order to limit
    PMUVer/PerfMon in the Debug Feature Registers to PMUv3 for ARMv8.1
    to avoid unexpected behaviour.
    
    Both ID_AA64DFR0.PMUVer and ID_DFR0.PerfMon follow the "Alternative ID
    scheme used for the Performance Monitors Extension version" where 0xF
    means an IMPLEMENTATION DEFINED PMU is implemented, and values 0x0-0xE
    are treated as with an unsigned field (with 0x0 meaning no PMU is
    present). As we don't expect to expose an IMPLEMENTATION DEFINED PMU,
    and our cap is below 0xF, we can treat these fields as unsigned when
    applying the cap.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [Mark: make field names consistent, use perfmon cap]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b91570ff9db1..d8f1eed070f0 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -702,6 +702,12 @@
 #define ID_AA64DFR0_TRACEVER_SHIFT	4
 #define ID_AA64DFR0_DEBUGVER_SHIFT	0
 
+#define ID_AA64DFR0_PMUVER_8_1		0x4
+
+#define ID_DFR0_PERFMON_SHIFT		24
+
+#define ID_DFR0_PERFMON_8_1		0x4
+
 #define ID_ISAR5_RDM_SHIFT		24
 #define ID_ISAR5_CRC32_SHIFT		16
 #define ID_ISAR5_SHA2_SHIFT		12

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b91570ff9db1..db08ceb4cc9a 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -514,6 +514,8 @@
 #endif
 
 /* SCTLR_EL1 specific flags. */
+#define SCTLR_EL1_BT1		(BIT(36))
+#define SCTLR_EL1_BT0		(BIT(35))
 #define SCTLR_EL1_UCI		(BIT(26))
 #define SCTLR_EL1_E0E		(BIT(24))
 #define SCTLR_EL1_SPAN		(BIT(23))
@@ -620,10 +622,12 @@
 
 /* id_aa64pfr1 */
 #define ID_AA64PFR1_SSBS_SHIFT		4
+#define ID_AA64PFR1_BT_SHIFT		0
 
 #define ID_AA64PFR1_SSBS_PSTATE_NI	0
 #define ID_AA64PFR1_SSBS_PSTATE_ONLY	1
 #define ID_AA64PFR1_SSBS_PSTATE_INSNS	2
+#define ID_AA64PFR1_BT_BTI		0x1
 
 /* id_aa64zfr0 */
 #define ID_AA64ZFR0_F64MM_SHIFT		56

commit 2c9d45b43c39e26fd2a73f2203321cdaee98b58b
Author: Ionela Voinescu <ionela.voinescu@arm.com>
Date:   Thu Mar 5 09:06:21 2020 +0000

    arm64: add support for the AMU extension v1
    
    The activity monitors extension is an optional extension introduced
    by the ARMv8.4 CPU architecture. This implements basic support for
    version 1 of the activity monitors architecture, AMUv1.
    
    This support includes:
    - Extension detection on each CPU (boot, secondary, hotplugged)
    - Register interface for AMU aarch64 registers
    
    Signed-off-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b91570ff9db1..085d248f824e 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -386,6 +386,42 @@
 #define SYS_TPIDR_EL0			sys_reg(3, 3, 13, 0, 2)
 #define SYS_TPIDRRO_EL0			sys_reg(3, 3, 13, 0, 3)
 
+/* Definitions for system register interface to AMU for ARMv8.4 onwards */
+#define SYS_AM_EL0(crm, op2)		sys_reg(3, 3, 13, (crm), (op2))
+#define SYS_AMCR_EL0			SYS_AM_EL0(2, 0)
+#define SYS_AMCFGR_EL0			SYS_AM_EL0(2, 1)
+#define SYS_AMCGCR_EL0			SYS_AM_EL0(2, 2)
+#define SYS_AMUSERENR_EL0		SYS_AM_EL0(2, 3)
+#define SYS_AMCNTENCLR0_EL0		SYS_AM_EL0(2, 4)
+#define SYS_AMCNTENSET0_EL0		SYS_AM_EL0(2, 5)
+#define SYS_AMCNTENCLR1_EL0		SYS_AM_EL0(3, 0)
+#define SYS_AMCNTENSET1_EL0		SYS_AM_EL0(3, 1)
+
+/*
+ * Group 0 of activity monitors (architected):
+ *                op0  op1  CRn   CRm       op2
+ * Counter:       11   011  1101  010:n<3>  n<2:0>
+ * Type:          11   011  1101  011:n<3>  n<2:0>
+ * n: 0-15
+ *
+ * Group 1 of activity monitors (auxiliary):
+ *                op0  op1  CRn   CRm       op2
+ * Counter:       11   011  1101  110:n<3>  n<2:0>
+ * Type:          11   011  1101  111:n<3>  n<2:0>
+ * n: 0-15
+ */
+
+#define SYS_AMEVCNTR0_EL0(n)		SYS_AM_EL0(4 + ((n) >> 3), (n) & 7)
+#define SYS_AMEVTYPE0_EL0(n)		SYS_AM_EL0(6 + ((n) >> 3), (n) & 7)
+#define SYS_AMEVCNTR1_EL0(n)		SYS_AM_EL0(12 + ((n) >> 3), (n) & 7)
+#define SYS_AMEVTYPE1_EL0(n)		SYS_AM_EL0(14 + ((n) >> 3), (n) & 7)
+
+/* AMU v1: Fixed (architecturally defined) activity monitors */
+#define SYS_AMEVCNTR0_CORE_EL0		SYS_AMEVCNTR0_EL0(0)
+#define SYS_AMEVCNTR0_CONST_EL0		SYS_AMEVCNTR0_EL0(1)
+#define SYS_AMEVCNTR0_INST_RET_EL0	SYS_AMEVCNTR0_EL0(2)
+#define SYS_AMEVCNTR0_MEM_STALL		SYS_AMEVCNTR0_EL0(3)
+
 #define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 
 #define SYS_CNTP_TVAL_EL0		sys_reg(3, 3, 14, 2, 0)
@@ -598,6 +634,7 @@
 #define ID_AA64PFR0_CSV3_SHIFT		60
 #define ID_AA64PFR0_CSV2_SHIFT		56
 #define ID_AA64PFR0_DIT_SHIFT		48
+#define ID_AA64PFR0_AMU_SHIFT		44
 #define ID_AA64PFR0_SVE_SHIFT		32
 #define ID_AA64PFR0_RAS_SHIFT		28
 #define ID_AA64PFR0_GIC_SHIFT		24
@@ -608,6 +645,7 @@
 #define ID_AA64PFR0_EL1_SHIFT		4
 #define ID_AA64PFR0_EL0_SHIFT		0
 
+#define ID_AA64PFR0_AMU			0x1
 #define ID_AA64PFR0_SVE			0x1
 #define ID_AA64PFR0_RAS_V1		0x1
 #define ID_AA64PFR0_FP_NI		0xf

commit bc206065944e2d6bd917e719dc897ffff8dbbca8
Merge: ab3906c53144 2e8e1ea88cbc
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:38:53 2020 +0000

    Merge branch 'for-next/rng' into for-next/core
    
    * for-next/rng: (2 commits)
      arm64: Use v8.5-RNG entropy for KASLR seed
      ...

commit 4f6cdf296cc4b5b25c85bd04a84b59e562ce234b
Merge: 3c23b83a88d0 8e3747beff8c c2c24edb1d9c e717d93b1c3f 3e3934176a4f d7bbd6c1b01c 1595fe299eb5 3751e728cef2 108eae2d4d10 52f73c383b24 9ee68b314e9a 500d14affdf7
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:32:31 2020 +0000

    Merge branches 'for-next/acpi', 'for-next/cpufeatures', 'for-next/csum', 'for-next/e0pd', 'for-next/entry', 'for-next/kbuild', 'for-next/kexec/cleanup', 'for-next/kexec/file-kdump', 'for-next/misc', 'for-next/nofpsimd', 'for-next/perf' and 'for-next/scs' into for-next/core
    
    * for-next/acpi:
      ACPI/IORT: Fix 'Number of IDs' handling in iort_id_map()
    
    * for-next/cpufeatures: (2 commits)
      arm64: Introduce ID_ISAR6 CPU register
      ...
    
    * for-next/csum: (2 commits)
      arm64: csum: Fix pathological zero-length calls
      ...
    
    * for-next/e0pd: (7 commits)
      arm64: kconfig: Fix alignment of E0PD help text
      ...
    
    * for-next/entry: (5 commits)
      arm64: entry: cleanup sp_el0 manipulation
      ...
    
    * for-next/kbuild: (4 commits)
      arm64: kbuild: remove compressed images on 'make ARCH=arm64 (dist)clean'
      ...
    
    * for-next/kexec/cleanup: (11 commits)
      Revert "arm64: kexec: make dtb_mem always enabled"
      ...
    
    * for-next/kexec/file-kdump: (2 commits)
      arm64: kexec_file: add crash dump support
      ...
    
    * for-next/misc: (12 commits)
      arm64: entry: Avoid empty alternatives entries
      ...
    
    * for-next/nofpsimd: (7 commits)
      arm64: nofpsmid: Handle TIF_FOREIGN_FPSTATE flag cleanly
      ...
    
    * for-next/perf: (2 commits)
      perf/imx_ddr: Fix cpu hotplug state cleanup
      ...
    
    * for-next/scs: (6 commits)
      arm64: kernel: avoid x18 in __cpu_soft_restart
      ...

commit 1a50ec0b3b2e9a83f1b1245ea37a853aac2f741c
Author: Richard Henderson <richard.henderson@linaro.org>
Date:   Tue Jan 21 12:58:52 2020 +0000

    arm64: Implement archrandom.h for ARMv8.5-RNG
    
    Expose the ID_AA64ISAR0.RNDR field to userspace, as the RNG system
    registers are always available at EL0.
    
    Implement arch_get_random_seed_long using RNDR.  Given that the
    TRNG is likely to be a shared resource between cores, and VMs,
    do not explicitly force re-seeding with RNDRRS.  In order to avoid
    code complexity and potential issues with hetrogenous systems only
    provide values after cpufeature has finalized the system capabilities.
    
    Signed-off-by: Richard Henderson <richard.henderson@linaro.org>
    [Modified to only function after cpufeature has finalized the system
    capabilities and move all the code into the header -- broonie]
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    [will: Advertise HWCAP via /proc/cpuinfo]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6e919fafb43d..5e718f279469 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -365,6 +365,9 @@
 #define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
 #define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
 
+#define SYS_RNDR_EL0			sys_reg(3, 3, 2, 4, 0)
+#define SYS_RNDRRS_EL0			sys_reg(3, 3, 2, 4, 1)
+
 #define SYS_PMCR_EL0			sys_reg(3, 3, 9, 12, 0)
 #define SYS_PMCNTENSET_EL0		sys_reg(3, 3, 9, 12, 1)
 #define SYS_PMCNTENCLR_EL0		sys_reg(3, 3, 9, 12, 2)
@@ -539,6 +542,7 @@
 			 ENDIAN_SET_EL1 | SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
 
 /* id_aa64isar0 */
+#define ID_AA64ISAR0_RNDR_SHIFT		60
 #define ID_AA64ISAR0_TS_SHIFT		52
 #define ID_AA64ISAR0_FHM_SHIFT		48
 #define ID_AA64ISAR0_DP_SHIFT		44

commit 95b3f74bec203804658e17f86fe20755bb8abcb9
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Dec 11 18:40:09 2019 +0000

    arm64: Use macros instead of hard-coded constants for MAIR_EL1
    
    Currently, the arm64 __cpu_setup has hard-coded constants for the memory
    attributes that go into the MAIR_EL1 register. Define proper macros in
    asm/sysreg.h and make use of them in proc.S.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6e919fafb43d..e21470337c5e 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -538,6 +538,18 @@
 			 SCTLR_EL1_NTWE | SCTLR_ELx_IESB | SCTLR_EL1_SPAN |\
 			 ENDIAN_SET_EL1 | SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
 
+/* MAIR_ELx memory attributes (used by Linux) */
+#define MAIR_ATTR_DEVICE_nGnRnE		UL(0x00)
+#define MAIR_ATTR_DEVICE_nGnRE		UL(0x04)
+#define MAIR_ATTR_DEVICE_GRE		UL(0x0c)
+#define MAIR_ATTR_NORMAL_NC		UL(0x44)
+#define MAIR_ATTR_NORMAL_WT		UL(0xbb)
+#define MAIR_ATTR_NORMAL		UL(0xff)
+#define MAIR_ATTR_MASK			UL(0xff)
+
+/* Position the attr at the correct index */
+#define MAIR_ATTRIDX(attr, idx)		((attr) << ((idx) * 8))
+
 /* id_aa64isar0 */
 #define ID_AA64ISAR0_TS_SHIFT		52
 #define ID_AA64ISAR0_FHM_SHIFT		48

commit 3e6c69a058deaa50d33c3dac36cde80b4ce590e8
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Dec 9 18:12:14 2019 +0000

    arm64: Add initial support for E0PD
    
    Kernel Page Table Isolation (KPTI) is used to mitigate some speculation
    based security issues by ensuring that the kernel is not mapped when
    userspace is running but this approach is expensive and is incompatible
    with SPE.  E0PD, introduced in the ARMv8.5 extensions, provides an
    alternative to this which ensures that accesses from userspace to the
    kernel's half of the memory map to always fault with constant time,
    preventing timing attacks without requiring constant unmapping and
    remapping or preventing legitimate accesses.
    
    Currently this feature will only be enabled if all CPUs in the system
    support E0PD, if some CPUs do not support the feature at boot time then
    the feature will not be enabled and in the unlikely event that a late
    CPU is the first CPU to lack the feature then we will reject that CPU.
    
    This initial patch does not yet integrate with KPTI, this will be dealt
    with in followup patches.  Ideally we could ensure that by default we
    don't use KPTI on CPUs where E0PD is present.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: Fixed typo in Kconfig text]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6e919fafb43d..b085258cfe4e 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -655,6 +655,7 @@
 #define ID_AA64MMFR1_VMIDBITS_16	2
 
 /* id_aa64mmfr2 */
+#define ID_AA64MMFR2_E0PD_SHIFT		60
 #define ID_AA64MMFR2_FWB_SHIFT		40
 #define ID_AA64MMFR2_AT_SHIFT		32
 #define ID_AA64MMFR2_LVA_SHIFT		16

commit 8e3747beff8c4533bf4f1a61b53e061266ef57db
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Dec 17 20:17:32 2019 +0530

    arm64: Introduce ID_ISAR6 CPU register
    
    This adds basic building blocks required for ID_ISAR6 CPU register which
    identifies support for various instruction implementation on AArch32 state.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: linux-kernel@vger.kernel.org
    Cc: kvmarm@lists.cs.columbia.edu
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    [will: Ensure SPECRES is treated the same as on A64]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index f56c4a02a127..6ad2db18d110 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -146,6 +146,7 @@
 #define SYS_ID_ISAR4_EL1		sys_reg(3, 0, 0, 2, 4)
 #define SYS_ID_ISAR5_EL1		sys_reg(3, 0, 0, 2, 5)
 #define SYS_ID_MMFR4_EL1		sys_reg(3, 0, 0, 2, 6)
+#define SYS_ID_ISAR6_EL1		sys_reg(3, 0, 0, 2, 7)
 
 #define SYS_MVFR0_EL1			sys_reg(3, 0, 0, 3, 0)
 #define SYS_MVFR1_EL1			sys_reg(3, 0, 0, 3, 1)
@@ -691,6 +692,14 @@
 #define ID_ISAR5_AES_SHIFT		4
 #define ID_ISAR5_SEVL_SHIFT		0
 
+#define ID_ISAR6_I8MM_SHIFT		24
+#define ID_ISAR6_BF16_SHIFT		20
+#define ID_ISAR6_SPECRES_SHIFT		16
+#define ID_ISAR6_SB_SHIFT		12
+#define ID_ISAR6_FHM_SHIFT		8
+#define ID_ISAR6_DP_SHIFT		4
+#define ID_ISAR6_JSCVT_SHIFT		0
+
 #define MVFR0_FPROUND_SHIFT		28
 #define MVFR0_FPSHVEC_SHIFT		24
 #define MVFR0_FPSQRT_SHIFT		20

commit d4209d8b717311d114b5d47ba7f8249fd44e97c2
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:33:37 2019 +0000

    arm64: cpufeature: Export matrix and other features to userspace
    
    Export the features introduced as part of ARMv8.6 exposed in the
    ID_AA64ISAR1_EL1 and ID_AA64ZFR0_EL1 registers. This introduces the
    Matrix features (ARMv8.2-I8MM, ARMv8.2-F64MM and ARMv8.2-F32MM) along
    with BFloat16 (Armv8.2-BF16), speculation invalidation (SPECRES) and
    Data Gathering Hint (ARMv8.0-DGH).
    
    Signed-off-by: Julien Grall <julien.grall@arm.com>
    [Added other features in those registers]
    Signed-off-by: Steven Price <steven.price@arm.com>
    [will: Don't advertise SPECRES to userspace]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6e919fafb43d..f56c4a02a127 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -553,6 +553,10 @@
 #define ID_AA64ISAR0_AES_SHIFT		4
 
 /* id_aa64isar1 */
+#define ID_AA64ISAR1_I8MM_SHIFT		52
+#define ID_AA64ISAR1_DGH_SHIFT		48
+#define ID_AA64ISAR1_BF16_SHIFT		44
+#define ID_AA64ISAR1_SPECRES_SHIFT	40
 #define ID_AA64ISAR1_SB_SHIFT		36
 #define ID_AA64ISAR1_FRINTTS_SHIFT	32
 #define ID_AA64ISAR1_GPI_SHIFT		28
@@ -605,12 +609,20 @@
 #define ID_AA64PFR1_SSBS_PSTATE_INSNS	2
 
 /* id_aa64zfr0 */
+#define ID_AA64ZFR0_F64MM_SHIFT		56
+#define ID_AA64ZFR0_F32MM_SHIFT		52
+#define ID_AA64ZFR0_I8MM_SHIFT		44
 #define ID_AA64ZFR0_SM4_SHIFT		40
 #define ID_AA64ZFR0_SHA3_SHIFT		32
+#define ID_AA64ZFR0_BF16_SHIFT		20
 #define ID_AA64ZFR0_BITPERM_SHIFT	16
 #define ID_AA64ZFR0_AES_SHIFT		4
 #define ID_AA64ZFR0_SVEVER_SHIFT	0
 
+#define ID_AA64ZFR0_F64MM		0x1
+#define ID_AA64ZFR0_F32MM		0x1
+#define ID_AA64ZFR0_I8MM		0x1
+#define ID_AA64ZFR0_BF16		0x1
 #define ID_AA64ZFR0_SM4			0x1
 #define ID_AA64ZFR0_SHA3		0x1
 #define ID_AA64ZFR0_BITPERM		0x1

commit 29a0f5ad87e6f45c984ffffa57b7142d178ff422
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Wed Oct 16 11:42:57 2019 +0800

    arm64: sysreg: fix incorrect definition of SYS_PAR_EL1_F
    
    The 'F' field of the PAR_EL1 register lives in bit 0, not bit 1.
    Fix the broken definition in 'sysreg.h'.
    
    Fixes: e8620cff9994 ("arm64: sysreg: Add some field definitions for PAR_EL1")
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 972d196c7714..6e919fafb43d 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -212,7 +212,7 @@
 #define SYS_FAR_EL1			sys_reg(3, 0, 6, 0, 0)
 #define SYS_PAR_EL1			sys_reg(3, 0, 7, 4, 0)
 
-#define SYS_PAR_EL1_F			BIT(1)
+#define SYS_PAR_EL1_F			BIT(0)
 #define SYS_PAR_EL1_FST			GENMASK(6, 1)
 
 /*** Statistical Profiling Extension ***/

commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

commit e8620cff99946ea1f7891d7bec071a23a1fdaef3
Author: Will Deacon <will@kernel.org>
Date:   Thu Aug 22 17:19:17 2019 +0100

    arm64: sysreg: Add some field definitions for PAR_EL1
    
    PAR_EL1 is a mysterious creature, but sometimes it's necessary to read
    it when translating addresses in situations where we cannot walk the
    page table directly.
    
    Add a couple of system register definitions for the fault indication
    field ('F') and the fault status code ('FST').
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 06ebcfef73df..2b229c23f3c1 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -212,6 +212,9 @@
 #define SYS_FAR_EL1			sys_reg(3, 0, 6, 0, 0)
 #define SYS_PAR_EL1			sys_reg(3, 0, 7, 4, 0)
 
+#define SYS_PAR_EL1_F			BIT(1)
+#define SYS_PAR_EL1_FST			GENMASK(6, 1)
+
 /*** Statistical Profiling Extension ***/
 /* ID registers */
 #define SYS_PMSIDR_EL1			sys_reg(3, 0, 9, 9, 7)

commit 73961dc1182ed7e1eb80a90ee28871ab3547af7e
Author: Will Deacon <will@kernel.org>
Date:   Mon Jul 15 14:28:17 2019 +0100

    arm64: sysreg: Remove unused and rotting SCTLR_ELx field definitions
    
    Our SCTLR_ELx field definitions are somewhat over-engineered in that
    they carefully define masks describing the RES0/RES1 bits and then use
    these to construct further masks representing bits to be set/cleared for
    the _EL1 and _EL2 registers.
    
    However, most of the resulting definitions aren't actually used by
    anybody and have subsequently started to bit-rot when new fields have
    been added by the architecture, resulting in fields being part of the
    RES0 mask despite being defined and used elsewhere.
    
    Rather than fix up these masks, simply remove the unused parts entirely
    so that we can drop the maintenance burden. We can always add things
    back if we need them in the future.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 06ebcfef73df..1df45c7ffcf7 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -499,28 +499,11 @@
 #define SCTLR_EL2_RES1	((BIT(4))  | (BIT(5))  | (BIT(11)) | (BIT(16)) | \
 			 (BIT(18)) | (BIT(22)) | (BIT(23)) | (BIT(28)) | \
 			 (BIT(29)))
-#define SCTLR_EL2_RES0	((BIT(6))  | (BIT(7))  | (BIT(8))  | (BIT(9))  | \
-			 (BIT(10)) | (BIT(13)) | (BIT(14)) | (BIT(15)) | \
-			 (BIT(17)) | (BIT(20)) | (BIT(24)) | (BIT(26)) | \
-			 (BIT(27)) | (BIT(30)) | (BIT(31)) | \
-			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL2		SCTLR_ELx_EE
-#define ENDIAN_CLEAR_EL2	0
 #else
 #define ENDIAN_SET_EL2		0
-#define ENDIAN_CLEAR_EL2	SCTLR_ELx_EE
-#endif
-
-/* SCTLR_EL2 value used for the hyp-stub */
-#define SCTLR_EL2_SET	(SCTLR_ELx_IESB   | ENDIAN_SET_EL2   | SCTLR_EL2_RES1)
-#define SCTLR_EL2_CLEAR	(SCTLR_ELx_M      | SCTLR_ELx_A    | SCTLR_ELx_C   | \
-			 SCTLR_ELx_SA     | SCTLR_ELx_I    | SCTLR_ELx_WXN | \
-			 SCTLR_ELx_DSSBS | ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
-
-#if (SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != 0xffffffffffffffffUL
-#error "Inconsistent SCTLR_EL2 set/clear bits"
 #endif
 
 /* SCTLR_EL1 specific flags. */
@@ -539,16 +522,11 @@
 
 #define SCTLR_EL1_RES1	((BIT(11)) | (BIT(20)) | (BIT(22)) | (BIT(28)) | \
 			 (BIT(29)))
-#define SCTLR_EL1_RES0  ((BIT(6))  | (BIT(10)) | (BIT(13)) | (BIT(17)) | \
-			 (BIT(27)) | (BIT(30)) | (BIT(31)) | \
-			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL1		(SCTLR_EL1_E0E | SCTLR_ELx_EE)
-#define ENDIAN_CLEAR_EL1	0
 #else
 #define ENDIAN_SET_EL1		0
-#define ENDIAN_CLEAR_EL1	(SCTLR_EL1_E0E | SCTLR_ELx_EE)
 #endif
 
 #define SCTLR_EL1_SET	(SCTLR_ELx_M    | SCTLR_ELx_C    | SCTLR_ELx_SA   |\
@@ -556,13 +534,6 @@
 			 SCTLR_EL1_DZE  | SCTLR_EL1_UCT                   |\
 			 SCTLR_EL1_NTWE | SCTLR_ELx_IESB | SCTLR_EL1_SPAN |\
 			 ENDIAN_SET_EL1 | SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
-#define SCTLR_EL1_CLEAR	(SCTLR_ELx_A   | SCTLR_EL1_CP15BEN | SCTLR_EL1_ITD    |\
-			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
-			 SCTLR_ELx_DSSBS | SCTLR_EL1_NTWI  | SCTLR_EL1_RES0)
-
-#if (SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != 0xffffffffffffffffUL
-#error "Inconsistent SCTLR_EL1 set/clear bits"
-#endif
 
 /* id_aa64isar0 */
 #define ID_AA64ISAR0_TS_SHIFT		52

commit fe6ba88b251aa76a94be2cb441d2e6b7c623b989
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Jul 16 16:27:01 2019 -0700

    arch: replace _BITUL() in kernel-space headers with BIT()
    
    Now that BIT() can be used from assembly code, we can safely replace
    _BITUL() with equivalent BIT().
    
    UAPI headers are still required to use _BITUL(), but there is no more
    reason to use it in kernel headers.  BIT() is shorter.
    
    Link: http://lkml.kernel.org/r/20190609153941.17249-2-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index a7522fca1105..06ebcfef73df 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -9,7 +9,7 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
-#include <linux/const.h>
+#include <linux/bits.h>
 #include <linux/stringify.h>
 
 /*
@@ -478,31 +478,31 @@
 #define SYS_CNTV_CVAL_EL02		sys_reg(3, 5, 14, 3, 2)
 
 /* Common SCTLR_ELx flags. */
-#define SCTLR_ELx_DSSBS	(_BITUL(44))
-#define SCTLR_ELx_ENIA	(_BITUL(31))
-#define SCTLR_ELx_ENIB	(_BITUL(30))
-#define SCTLR_ELx_ENDA	(_BITUL(27))
-#define SCTLR_ELx_EE    (_BITUL(25))
-#define SCTLR_ELx_IESB	(_BITUL(21))
-#define SCTLR_ELx_WXN	(_BITUL(19))
-#define SCTLR_ELx_ENDB	(_BITUL(13))
-#define SCTLR_ELx_I	(_BITUL(12))
-#define SCTLR_ELx_SA	(_BITUL(3))
-#define SCTLR_ELx_C	(_BITUL(2))
-#define SCTLR_ELx_A	(_BITUL(1))
-#define SCTLR_ELx_M	(_BITUL(0))
+#define SCTLR_ELx_DSSBS	(BIT(44))
+#define SCTLR_ELx_ENIA	(BIT(31))
+#define SCTLR_ELx_ENIB	(BIT(30))
+#define SCTLR_ELx_ENDA	(BIT(27))
+#define SCTLR_ELx_EE    (BIT(25))
+#define SCTLR_ELx_IESB	(BIT(21))
+#define SCTLR_ELx_WXN	(BIT(19))
+#define SCTLR_ELx_ENDB	(BIT(13))
+#define SCTLR_ELx_I	(BIT(12))
+#define SCTLR_ELx_SA	(BIT(3))
+#define SCTLR_ELx_C	(BIT(2))
+#define SCTLR_ELx_A	(BIT(1))
+#define SCTLR_ELx_M	(BIT(0))
 
 #define SCTLR_ELx_FLAGS	(SCTLR_ELx_M  | SCTLR_ELx_A | SCTLR_ELx_C | \
 			 SCTLR_ELx_SA | SCTLR_ELx_I | SCTLR_ELx_IESB)
 
 /* SCTLR_EL2 specific flags. */
-#define SCTLR_EL2_RES1	((_BITUL(4))  | (_BITUL(5))  | (_BITUL(11)) | (_BITUL(16)) | \
-			 (_BITUL(18)) | (_BITUL(22)) | (_BITUL(23)) | (_BITUL(28)) | \
-			 (_BITUL(29)))
-#define SCTLR_EL2_RES0	((_BITUL(6))  | (_BITUL(7))  | (_BITUL(8))  | (_BITUL(9))  | \
-			 (_BITUL(10)) | (_BITUL(13)) | (_BITUL(14)) | (_BITUL(15)) | \
-			 (_BITUL(17)) | (_BITUL(20)) | (_BITUL(24)) | (_BITUL(26)) | \
-			 (_BITUL(27)) | (_BITUL(30)) | (_BITUL(31)) | \
+#define SCTLR_EL2_RES1	((BIT(4))  | (BIT(5))  | (BIT(11)) | (BIT(16)) | \
+			 (BIT(18)) | (BIT(22)) | (BIT(23)) | (BIT(28)) | \
+			 (BIT(29)))
+#define SCTLR_EL2_RES0	((BIT(6))  | (BIT(7))  | (BIT(8))  | (BIT(9))  | \
+			 (BIT(10)) | (BIT(13)) | (BIT(14)) | (BIT(15)) | \
+			 (BIT(17)) | (BIT(20)) | (BIT(24)) | (BIT(26)) | \
+			 (BIT(27)) | (BIT(30)) | (BIT(31)) | \
 			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
@@ -524,23 +524,23 @@
 #endif
 
 /* SCTLR_EL1 specific flags. */
-#define SCTLR_EL1_UCI		(_BITUL(26))
-#define SCTLR_EL1_E0E		(_BITUL(24))
-#define SCTLR_EL1_SPAN		(_BITUL(23))
-#define SCTLR_EL1_NTWE		(_BITUL(18))
-#define SCTLR_EL1_NTWI		(_BITUL(16))
-#define SCTLR_EL1_UCT		(_BITUL(15))
-#define SCTLR_EL1_DZE		(_BITUL(14))
-#define SCTLR_EL1_UMA		(_BITUL(9))
-#define SCTLR_EL1_SED		(_BITUL(8))
-#define SCTLR_EL1_ITD		(_BITUL(7))
-#define SCTLR_EL1_CP15BEN	(_BITUL(5))
-#define SCTLR_EL1_SA0		(_BITUL(4))
-
-#define SCTLR_EL1_RES1	((_BITUL(11)) | (_BITUL(20)) | (_BITUL(22)) | (_BITUL(28)) | \
-			 (_BITUL(29)))
-#define SCTLR_EL1_RES0  ((_BITUL(6))  | (_BITUL(10)) | (_BITUL(13)) | (_BITUL(17)) | \
-			 (_BITUL(27)) | (_BITUL(30)) | (_BITUL(31)) | \
+#define SCTLR_EL1_UCI		(BIT(26))
+#define SCTLR_EL1_E0E		(BIT(24))
+#define SCTLR_EL1_SPAN		(BIT(23))
+#define SCTLR_EL1_NTWE		(BIT(18))
+#define SCTLR_EL1_NTWI		(BIT(16))
+#define SCTLR_EL1_UCT		(BIT(15))
+#define SCTLR_EL1_DZE		(BIT(14))
+#define SCTLR_EL1_UMA		(BIT(9))
+#define SCTLR_EL1_SED		(BIT(8))
+#define SCTLR_EL1_ITD		(BIT(7))
+#define SCTLR_EL1_CP15BEN	(BIT(5))
+#define SCTLR_EL1_SA0		(BIT(4))
+
+#define SCTLR_EL1_RES1	((BIT(11)) | (BIT(20)) | (BIT(22)) | (BIT(28)) | \
+			 (BIT(29)))
+#define SCTLR_EL1_RES0  ((BIT(6))  | (BIT(10)) | (BIT(13)) | (BIT(17)) | \
+			 (BIT(27)) | (BIT(30)) | (BIT(31)) | \
 			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
@@ -756,13 +756,13 @@
 #define ZCR_ELx_LEN_SIZE	9
 #define ZCR_ELx_LEN_MASK	0x1ff
 
-#define CPACR_EL1_ZEN_EL1EN	(_BITUL(16)) /* enable EL1 access */
-#define CPACR_EL1_ZEN_EL0EN	(_BITUL(17)) /* enable EL0 access, if EL1EN set */
+#define CPACR_EL1_ZEN_EL1EN	(BIT(16)) /* enable EL1 access */
+#define CPACR_EL1_ZEN_EL0EN	(BIT(17)) /* enable EL0 access, if EL1EN set */
 #define CPACR_EL1_ZEN		(CPACR_EL1_ZEN_EL1EN | CPACR_EL1_ZEN_EL0EN)
 
 
 /* Safe value for MPIDR_EL1: Bit31:RES1, Bit30:U:0, Bit24:MT:0 */
-#define SYS_MPIDR_SAFE_VAL	(_BITUL(31))
+#define SYS_MPIDR_SAFE_VAL	(BIT(31))
 
 #ifdef __ASSEMBLY__
 

commit 39d7530d7494b4e47ba1856e741f513dafd17e3d
Merge: 16c97650a56a a45ff5994c9c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 15:35:14 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for chained PMU counters in guests
       - improved SError handling
       - handle Neoverse N1 erratum #1349291
       - allow side-channel mitigation status to be migrated
       - standardise most AArch64 system register accesses to msr_s/mrs_s
       - fix host MPIDR corruption on 32bit
       - selftests ckleanups
    
      x86:
       - PMU event {white,black}listing
       - ability for the guest to disable host-side interrupt polling
       - fixes for enlightened VMCS (Hyper-V pv nested virtualization),
       - new hypercall to yield to IPI target
       - support for passing cstate MSRs through to the guest
       - lots of cleanups and optimizations
    
      Generic:
       - Some txt->rST conversions for the documentation"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (128 commits)
      Documentation: virtual: Add toctree hooks
      Documentation: kvm: Convert cpuid.txt to .rst
      Documentation: virtual: Convert paravirt_ops.txt to .rst
      KVM: x86: Unconditionally enable irqs in guest context
      KVM: x86: PMU Event Filter
      kvm: x86: Fix -Wmissing-prototypes warnings
      KVM: Properly check if "page" is valid in kvm_vcpu_unmap
      KVM: arm/arm64: Initialise host's MPIDRs by reading the actual register
      KVM: LAPIC: Retry tune per-vCPU timer_advance_ns if adaptive tuning goes insane
      kvm: LAPIC: write down valid APIC registers
      KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
      KVM: doc: Add API documentation on the KVM_REG_ARM_WORKAROUNDS register
      KVM: arm/arm64: Add save/restore support for firmware workaround state
      arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
      KVM: arm/arm64: Support chained PMU counters
      KVM: arm/arm64: Remove pmc->bitmask
      KVM: arm/arm64: Re-create event when setting counter value
      KVM: arm/arm64: Extract duplicated code to own function
      KVM: arm/arm64: Rename kvm_pmu_{enable/disable}_counter functions
      KVM: LAPIC: ARBPRI is a reserved register for x2APIC
      ...

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit fdec2a9ef853172529baaa192673b4cdb9a44fac
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Sat Apr 6 11:29:40 2019 +0100

    KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
    
    Currently, the {read,write}_sysreg_el*() accessors for accessing
    particular ELs' sysregs in the presence of VHE rely on some local
    hacks and define their system register encodings in a way that is
    inconsistent with the core definitions in <asm/sysreg.h>.
    
    As a result, it is necessary to add duplicate definitions for any
    system register that already needs a definition in sysreg.h for
    other reasons.
    
    This is a bit of a maintenance headache, and the reasons for the
    _el*() accessors working the way they do is a bit historical.
    
    This patch gets rid of the shadow sysreg definitions in
    <asm/kvm_hyp.h>, converts the _el*() accessors to use the core
    __msr_s/__mrs_s interface, and converts all call sites to use the
    standard sysreg #define names (i.e., upper case, with SYS_ prefix).
    
    This patch will conflict heavily anyway, so the opportunity
    to clean up some bad whitespace in the context of the changes is
    taken.
    
    The change exposes a few system registers that have no sysreg.h
    definition, due to msr_s/mrs_s being used in place of msr/mrs:
    additions are made in order to fill in the gaps.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://www.spinics.net/lists/kvm-arm/msg31717.html
    [Rebased to v4.21-rc1]
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    [Rebased to v5.2-rc5, changelog updates]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index cd7f7ce1a56a..852cc113de7c 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -191,6 +191,9 @@
 #define SYS_APGAKEYLO_EL1		sys_reg(3, 0, 2, 3, 0)
 #define SYS_APGAKEYHI_EL1		sys_reg(3, 0, 2, 3, 1)
 
+#define SYS_SPSR_EL1			sys_reg(3, 0, 4, 0, 0)
+#define SYS_ELR_EL1			sys_reg(3, 0, 4, 0, 1)
+
 #define SYS_ICC_PMR_EL1			sys_reg(3, 0, 4, 6, 0)
 
 #define SYS_AFSR0_EL1			sys_reg(3, 0, 5, 1, 0)
@@ -382,6 +385,9 @@
 #define SYS_CNTP_CTL_EL0		sys_reg(3, 3, 14, 2, 1)
 #define SYS_CNTP_CVAL_EL0		sys_reg(3, 3, 14, 2, 2)
 
+#define SYS_CNTV_CTL_EL0		sys_reg(3, 3, 14, 3, 1)
+#define SYS_CNTV_CVAL_EL0		sys_reg(3, 3, 14, 3, 2)
+
 #define SYS_AARCH32_CNTP_TVAL		sys_reg(0, 0, 14, 2, 0)
 #define SYS_AARCH32_CNTP_CTL		sys_reg(0, 0, 14, 2, 1)
 #define SYS_AARCH32_CNTP_CVAL		sys_reg(0, 2, 0, 14, 0)
@@ -392,14 +398,17 @@
 #define __TYPER_CRm(n)			(0xc | (((n) >> 3) & 0x3))
 #define SYS_PMEVTYPERn_EL0(n)		sys_reg(3, 3, 14, __TYPER_CRm(n), __PMEV_op2(n))
 
-#define SYS_PMCCFILTR_EL0		sys_reg (3, 3, 14, 15, 7)
+#define SYS_PMCCFILTR_EL0		sys_reg(3, 3, 14, 15, 7)
 
 #define SYS_ZCR_EL2			sys_reg(3, 4, 1, 2, 0)
-
 #define SYS_DACR32_EL2			sys_reg(3, 4, 3, 0, 0)
+#define SYS_SPSR_EL2			sys_reg(3, 4, 4, 0, 0)
+#define SYS_ELR_EL2			sys_reg(3, 4, 4, 0, 1)
 #define SYS_IFSR32_EL2			sys_reg(3, 4, 5, 0, 1)
+#define SYS_ESR_EL2			sys_reg(3, 4, 5, 2, 0)
 #define SYS_VSESR_EL2			sys_reg(3, 4, 5, 2, 3)
 #define SYS_FPEXC32_EL2			sys_reg(3, 4, 5, 3, 0)
+#define SYS_FAR_EL2			sys_reg(3, 4, 6, 0, 0)
 
 #define SYS_VDISR_EL2			sys_reg(3, 4, 12, 1,  1)
 #define __SYS__AP0Rx_EL2(x)		sys_reg(3, 4, 12, 8, x)
@@ -444,7 +453,29 @@
 #define SYS_ICH_LR15_EL2		__SYS__LR8_EL2(7)
 
 /* VHE encodings for architectural EL0/1 system registers */
+#define SYS_SCTLR_EL12			sys_reg(3, 5, 1, 0, 0)
+#define SYS_CPACR_EL12			sys_reg(3, 5, 1, 0, 2)
 #define SYS_ZCR_EL12			sys_reg(3, 5, 1, 2, 0)
+#define SYS_TTBR0_EL12			sys_reg(3, 5, 2, 0, 0)
+#define SYS_TTBR1_EL12			sys_reg(3, 5, 2, 0, 1)
+#define SYS_TCR_EL12			sys_reg(3, 5, 2, 0, 2)
+#define SYS_SPSR_EL12			sys_reg(3, 5, 4, 0, 0)
+#define SYS_ELR_EL12			sys_reg(3, 5, 4, 0, 1)
+#define SYS_AFSR0_EL12			sys_reg(3, 5, 5, 1, 0)
+#define SYS_AFSR1_EL12			sys_reg(3, 5, 5, 1, 1)
+#define SYS_ESR_EL12			sys_reg(3, 5, 5, 2, 0)
+#define SYS_FAR_EL12			sys_reg(3, 5, 6, 0, 0)
+#define SYS_MAIR_EL12			sys_reg(3, 5, 10, 2, 0)
+#define SYS_AMAIR_EL12			sys_reg(3, 5, 10, 3, 0)
+#define SYS_VBAR_EL12			sys_reg(3, 5, 12, 0, 0)
+#define SYS_CONTEXTIDR_EL12		sys_reg(3, 5, 13, 0, 1)
+#define SYS_CNTKCTL_EL12		sys_reg(3, 5, 14, 1, 0)
+#define SYS_CNTP_TVAL_EL02		sys_reg(3, 5, 14, 2, 0)
+#define SYS_CNTP_CTL_EL02		sys_reg(3, 5, 14, 2, 1)
+#define SYS_CNTP_CVAL_EL02		sys_reg(3, 5, 14, 2, 2)
+#define SYS_CNTV_TVAL_EL02		sys_reg(3, 5, 14, 3, 0)
+#define SYS_CNTV_CTL_EL02		sys_reg(3, 5, 14, 3, 1)
+#define SYS_CNTV_CVAL_EL02		sys_reg(3, 5, 14, 3, 2)
 
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_DSSBS	(_BITUL(44))

commit ca9503fc9e9812aa6258e55d44edb03eb30fc46f
Author: Mark Brown <broonie@kernel.org>
Date:   Tue Jun 18 19:10:55 2019 +0100

    arm64: Expose FRINT capabilities to userspace
    
    ARMv8.5 introduces the FRINT series of instructions for rounding floating
    point numbers to integers. Provide a capability to userspace in order to
    allow applications to determine if the system supports these instructions.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 902d75b60914..601972771807 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -560,6 +560,7 @@
 
 /* id_aa64isar1 */
 #define ID_AA64ISAR1_SB_SHIFT		36
+#define ID_AA64ISAR1_FRINTTS_SHIFT	32
 #define ID_AA64ISAR1_GPI_SHIFT		28
 #define ID_AA64ISAR1_GPA_SHIFT		24
 #define ID_AA64ISAR1_LRCPC_SHIFT	20

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 902d75b60914..cd7f7ce1a56a 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -1,20 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Macros for accessing system registers with older binutils.
  *
  * Copyright (C) 2014 ARM Ltd.
  * Author: Catalin Marinas <catalin.marinas@arm.com>
- *
- * This program is free software: you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #ifndef __ASM_SYSREG_H

commit 0ef0fd351550130129bbdb77362488befd7b69d2
Merge: 4489da718309 c011d23ba046
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 17 10:33:30 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for SVE and Pointer Authentication in guests
       - PMU improvements
    
      POWER:
       - support for direct access to the POWER9 XIVE interrupt controller
       - memory and performance optimizations
    
      x86:
       - support for accessing memory not backed by struct page
       - fixes and refactoring
    
      Generic:
       - dirty page tracking improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (155 commits)
      kvm: fix compilation on aarch64
      Revert "KVM: nVMX: Expose RDPMC-exiting only when guest supports PMU"
      kvm: x86: Fix L1TF mitigation for shadow MMU
      KVM: nVMX: Disable intercept for FS/GS base MSRs in vmcs02 when possible
      KVM: PPC: Book3S: Remove useless checks in 'release' method of KVM device
      KVM: PPC: Book3S HV: XIVE: Fix spelling mistake "acessing" -> "accessing"
      KVM: PPC: Book3S HV: Make sure to load LPID for radix VCPUs
      kvm: nVMX: Set nested_run_pending in vmx_set_nested_state after checks complete
      tests: kvm: Add tests for KVM_SET_NESTED_STATE
      KVM: nVMX: KVM_SET_NESTED_STATE - Tear down old EVMCS state before setting new state
      tests: kvm: Add tests for KVM_CAP_MAX_VCPUS and KVM_CAP_MAX_CPU_ID
      tests: kvm: Add tests to .gitignore
      KVM: Introduce KVM_CAP_MANUAL_DIRTY_LOG_PROTECT2
      KVM: Fix kvm_clear_dirty_log_protect off-by-(minus-)one
      KVM: Fix the bitmap range to copy during clear dirty
      KVM: arm64: Fix ptrauth ID register masking logic
      KVM: x86: use direct accessors for RIP and RSP
      KVM: VMX: Use accessors for GPRs outside of dedicated caching logic
      KVM: x86: Omit caching logic for always-available GPRs
      kvm, x86: Properly check whether a pfn is an MMIO or not
      ...

commit be604c616ca71cbf5c860d0cfa4595128ab74189
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Apr 24 09:55:37 2019 -0700

    arm64: sysreg: Make mrs_s and msr_s macros work with Clang and LTO
    
    Clang's integrated assembler does not allow assembly macros defined
    in one inline asm block using the .macro directive to be used across
    separate asm blocks. LLVM developers consider this a feature and not a
    bug, recommending code refactoring:
    
      https://bugs.llvm.org/show_bug.cgi?id=19749
    
    As binutils doesn't allow macros to be redefined, this change uses
    UNDEFINE_MRS_S and UNDEFINE_MSR_S to define corresponding macros
    in-place and workaround gcc and clang limitations on redefining macros
    across different assembler blocks.
    
    Specifically, the current state after preprocessing looks like this:
    
    asm volatile(".macro mXX_s ... .endm");
    void f()
    {
            asm volatile("mXX_s a, b");
    }
    
    With GCC, it gives macro redefinition error because sysreg.h is included
    in multiple source files, and assembler code for all of them is later
    combined for LTO (I've seen an intermediate file with hundreds of
    identical definitions).
    
    With clang, it gives macro undefined error because clang doesn't allow
    sharing macros between inline asm statements.
    
    I also seem to remember catching another sort of undefined error with
    GCC due to reordering of macro definition asm statement and generated
    asm code for function that uses the macro.
    
    The solution with defining and undefining for each use, while certainly
    not elegant, satisfies both GCC and clang, LTO and non-LTO.
    
    Co-developed-by: Alex Matveev <alxmtvv@gmail.com>
    Co-developed-by: Yury Norov <ynorov@caviumnetworks.com>
    Co-developed-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 29c655969320..3f7b917e8f3a 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -760,20 +760,39 @@
 #include <linux/build_bug.h>
 #include <linux/types.h>
 
-asm(
-"	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n"
-"	.equ	.L__reg_num_x\\num, \\num\n"
-"	.endr\n"
+#define __DEFINE_MRS_MSR_S_REGNUM				\
+"	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n" \
+"	.equ	.L__reg_num_x\\num, \\num\n"			\
+"	.endr\n"						\
 "	.equ	.L__reg_num_xzr, 31\n"
-"\n"
-"	.macro	mrs_s, rt, sreg\n"
-	__emit_inst(0xd5200000|(\\sreg)|(.L__reg_num_\\rt))
+
+#define DEFINE_MRS_S						\
+	__DEFINE_MRS_MSR_S_REGNUM				\
+"	.macro	mrs_s, rt, sreg\n"				\
+	__emit_inst(0xd5200000|(\\sreg)|(.L__reg_num_\\rt))	\
 "	.endm\n"
-"\n"
-"	.macro	msr_s, sreg, rt\n"
-	__emit_inst(0xd5000000|(\\sreg)|(.L__reg_num_\\rt))
+
+#define DEFINE_MSR_S						\
+	__DEFINE_MRS_MSR_S_REGNUM				\
+"	.macro	msr_s, sreg, rt\n"				\
+	__emit_inst(0xd5000000|(\\sreg)|(.L__reg_num_\\rt))	\
 "	.endm\n"
-);
+
+#define UNDEFINE_MRS_S						\
+"	.purgem	mrs_s\n"
+
+#define UNDEFINE_MSR_S						\
+"	.purgem	msr_s\n"
+
+#define __mrs_s(v, r)						\
+	DEFINE_MRS_S						\
+"	mrs_s " v ", " __stringify(r) "\n"			\
+	UNDEFINE_MRS_S
+
+#define __msr_s(r, v)						\
+	DEFINE_MSR_S						\
+"	msr_s " __stringify(r) ", " v "\n"			\
+	UNDEFINE_MSR_S
 
 /*
  * Unlike read_cpuid, calls to read_sysreg are never expected to be
@@ -801,13 +820,13 @@ asm(
  */
 #define read_sysreg_s(r) ({						\
 	u64 __val;							\
-	asm volatile("mrs_s %0, " __stringify(r) : "=r" (__val));	\
+	asm volatile(__mrs_s("%0", r) : "=r" (__val));			\
 	__val;								\
 })
 
 #define write_sysreg_s(v, r) do {					\
 	u64 __val = (u64)(v);						\
-	asm volatile("msr_s " __stringify(r) ", %x0" : : "rZ" (__val));	\
+	asm volatile(__msr_s(r, "%x0") : : "rZ" (__val));		\
 } while (0)
 
 /*

commit 06a916feca2b262ab0c1a2aeb68882f4b1108a07
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Apr 18 18:41:38 2019 +0100

    arm64: Expose SVE2 features for userspace
    
    This patch provides support for reporting the presence of SVE2 and
    its optional features to userspace.
    
    This will also enable visibility of SVE2 for guests, when KVM
    support for SVE-enabled guests is available.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 5b267dec6194..29c655969320 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -606,6 +606,20 @@
 #define ID_AA64PFR1_SSBS_PSTATE_ONLY	1
 #define ID_AA64PFR1_SSBS_PSTATE_INSNS	2
 
+/* id_aa64zfr0 */
+#define ID_AA64ZFR0_SM4_SHIFT		40
+#define ID_AA64ZFR0_SHA3_SHIFT		32
+#define ID_AA64ZFR0_BITPERM_SHIFT	16
+#define ID_AA64ZFR0_AES_SHIFT		4
+#define ID_AA64ZFR0_SVEVER_SHIFT	0
+
+#define ID_AA64ZFR0_SM4			0x1
+#define ID_AA64ZFR0_SHA3		0x1
+#define ID_AA64ZFR0_BITPERM		0x1
+#define ID_AA64ZFR0_AES			0x1
+#define ID_AA64ZFR0_AES_PMULL		0x2
+#define ID_AA64ZFR0_SVEVER_SVE2		0x1
+
 /* id_aa64mmfr0 */
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28
 #define ID_AA64MMFR0_TGRAN64_SHIFT	24

commit 73433762fcaeb9d59e84d299021c6b15466c96dd
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:16 2018 +0100

    KVM: arm64/sve: System register context switch and access support
    
    This patch adds the necessary support for context switching ZCR_EL1
    for each vcpu.
    
    ZCR_EL1 is trapped alongside the FPSIMD/SVE registers, so it makes
    sense for it to be handled as part of the guest FPSIMD/SVE context
    for context switch purposes instead of handling it as a general
    system register.  This means that it can be switched in lazily at
    the appropriate time.  No effort is made to track host context for
    this register, since SVE requires VHE: thus the hosts's value for
    this register lives permanently in ZCR_EL2 and does not alias the
    guest's value at any time.
    
    The Hyp switch and fpsimd context handling code is extended
    appropriately.
    
    Accessors are added in sys_regs.c to expose the SVE system
    registers and ID register fields.  Because these need to be
    conditionally visible based on the guest configuration, they are
    implemented separately for now rather than by use of the generic
    system register helpers.  This may be abstracted better later on
    when/if there are more features requiring this model.
    
    ID_AA64ZFR0_EL1 is RO-RAZ for MRS/MSR when SVE is disabled for the
    guest, but for compatibility with non-SVE aware KVM implementations
    the register should not be enumerated at all for KVM_GET_REG_LIST
    in this case.  For consistency we also reject ioctl access to the
    register.  This ensures that a non-SVE-enabled guest looks the same
    to userspace, irrespective of whether the kernel KVM implementation
    supports SVE.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 5b267dec6194..4d6262df79bb 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -454,6 +454,9 @@
 #define SYS_ICH_LR14_EL2		__SYS__LR8_EL2(6)
 #define SYS_ICH_LR15_EL2		__SYS__LR8_EL2(7)
 
+/* VHE encodings for architectural EL0/1 system registers */
+#define SYS_ZCR_EL12			sys_reg(3, 5, 1, 2, 0)
+
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_DSSBS	(_BITUL(44))
 #define SCTLR_ELx_ENIA	(_BITUL(31))

commit f7f2b15c3d42fa5754131b34a0f7ad5a5c3f777f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Jan 31 14:17:17 2019 +0100

    arm64: KVM: Expose sanitised cache type register to guest
    
    We currently permit CPUs in the same system to deviate in the exact
    topology of the caches, and we subsequently hide this fact from user
    space by exposing a sanitised value of the cache type register CTR_EL0.
    
    However, guests running under KVM see the bare value of CTR_EL0, which
    could potentially result in issues with, e.g., JITs or other pieces of
    code that are sensitive to misreported cache line sizes.
    
    So let's start trapping cache ID instructions if there is a mismatch,
    and expose the sanitised version of CTR_EL0 to guests. Note that CTR_EL0
    is treated as an invariant to KVM user space, so update that part as well.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6482e8bcf1b8..5b267dec6194 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -361,6 +361,7 @@
 
 #define SYS_CNTKCTL_EL1			sys_reg(3, 0, 14, 1, 0)
 
+#define SYS_CCSIDR_EL1			sys_reg(3, 1, 0, 0, 0)
 #define SYS_CLIDR_EL1			sys_reg(3, 1, 0, 0, 1)
 #define SYS_AIDR_EL1			sys_reg(3, 1, 0, 0, 7)
 

commit 84135d3d18da2ff17d3ad1a609b2818cc3049552
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Thu Jul 5 16:48:23 2018 +0100

    KVM: arm/arm64: consolidate arch timer trap handlers
    
    At the moment we have separate system register emulation handlers for
    each timer register. Actually they are quite similar, and we rely on
    kvm_arm_timer_[gs]et_reg() for the actual emulation anyways, so let's
    just merge all of those handlers into one function, which just marshalls
    the arguments and then hands off to a set of common accessors.
    This makes extending the emulation to include EL2 timers much easier.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [Fixed 32-bit VM breakage and reduced to reworking existing code]
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    [Fixed 32bit host, general cleanup]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 3e5650903d6d..6482e8bcf1b8 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -392,6 +392,10 @@
 #define SYS_CNTP_CTL_EL0		sys_reg(3, 3, 14, 2, 1)
 #define SYS_CNTP_CVAL_EL0		sys_reg(3, 3, 14, 2, 2)
 
+#define SYS_AARCH32_CNTP_TVAL		sys_reg(0, 0, 14, 2, 0)
+#define SYS_AARCH32_CNTP_CTL		sys_reg(0, 0, 14, 2, 1)
+#define SYS_AARCH32_CNTP_CVAL		sys_reg(0, 2, 0, 14, 0)
+
 #define __PMEV_op2(n)			((n) & 0x7)
 #define __CNTR_CRm(n)			(0x8 | (((n) >> 3) & 0x3))
 #define SYS_PMEVCNTRn_EL0(n)		sys_reg(3, 3, 14, __CNTR_CRm(n), __PMEV_op2(n))

commit b98c079ba480c606b13f6abf844187af09baeaab
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jan 4 11:33:42 2019 +0100

    KVM: arm64: Fix ICH_ELRSR_EL2 sysreg naming
    
    We previously incorrectly named the define for this system register.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 72dc4c011014..3e5650903d6d 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -426,7 +426,7 @@
 #define SYS_ICH_VTR_EL2			sys_reg(3, 4, 12, 11, 1)
 #define SYS_ICH_MISR_EL2		sys_reg(3, 4, 12, 11, 2)
 #define SYS_ICH_EISR_EL2		sys_reg(3, 4, 12, 11, 3)
-#define SYS_ICH_ELSR_EL2		sys_reg(3, 4, 12, 11, 5)
+#define SYS_ICH_ELRSR_EL2		sys_reg(3, 4, 12, 11, 5)
 #define SYS_ICH_VMCR_EL2		sys_reg(3, 4, 12, 11, 7)
 
 #define __SYS__LR0_EL2(x)		sys_reg(3, 4, 12, 12, x)

commit 5694cecdb092656a822287a6691aa7ce668c8160
Merge: 13e1ad2be3a8 12f799c8c739
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 25 17:41:56 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 festive updates from Will Deacon:
     "In the end, we ended up with quite a lot more than I expected:
    
       - Support for ARMv8.3 Pointer Authentication in userspace (CRIU and
         kernel-side support to come later)
    
       - Support for per-thread stack canaries, pending an update to GCC
         that is currently undergoing review
    
       - Support for kexec_file_load(), which permits secure boot of a kexec
         payload but also happens to improve the performance of kexec
         dramatically because we can avoid the sucky purgatory code from
         userspace. Kdump will come later (requires updates to libfdt).
    
       - Optimisation of our dynamic CPU feature framework, so that all
         detected features are enabled via a single stop_machine()
         invocation
    
       - KPTI whitelisting of Cortex-A CPUs unaffected by Meltdown, so that
         they can benefit from global TLB entries when KASLR is not in use
    
       - 52-bit virtual addressing for userspace (kernel remains 48-bit)
    
       - Patch in LSE atomics for per-cpu atomic operations
    
       - Custom preempt.h implementation to avoid unconditional calls to
         preempt_schedule() from preempt_enable()
    
       - Support for the new 'SB' Speculation Barrier instruction
    
       - Vectorised implementation of XOR checksumming and CRC32
         optimisations
    
       - Workaround for Cortex-A76 erratum #1165522
    
       - Improved compatibility with Clang/LLD
    
       - Support for TX2 system PMUS for profiling the L3 cache and DMC
    
       - Reflect read-only permissions in the linear map by default
    
       - Ensure MMIO reads are ordered with subsequent calls to Xdelay()
    
       - Initial support for memory hotplug
    
       - Tweak the threshold when we invalidate the TLB by-ASID, so that
         mremap() performance is improved for ranges spanning multiple PMDs.
    
       - Minor refactoring and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (125 commits)
      arm64: kaslr: print PHYS_OFFSET in dump_kernel_offset()
      arm64: sysreg: Use _BITUL() when defining register bits
      arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
      arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
      arm64: docs: document pointer authentication
      arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
      arm64: enable pointer authentication
      arm64: add prctl control for resetting ptrauth keys
      arm64: perf: strip PAC when unwinding userspace
      arm64: expose user PAC bit positions via ptrace
      arm64: add basic pointer authentication support
      arm64/cpufeature: detect pointer authentication
      arm64: Don't trap host pointer auth use to EL2
      arm64/kvm: hide ptrauth from guests
      arm64/kvm: consistently handle host HCR_EL2 flags
      arm64: add pointer authentication register bits
      arm64: add comments about EC exception levels
      arm64: perf: Treat EXCLUDE_EL* bit definitions as unsigned
      arm64: kpti: Whitelist Cortex-A CPUs that don't implement the CSV3 field
      arm64: enable per-task stack canaries
      ...

commit 97bebc5facf7d495e72df0b7c50be3699c7bdd73
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 11 16:42:31 2018 +0000

    arm64: sysreg: Use _BITUL() when defining register bits
    
    Using shifts directly is error-prone and can cause inadvertent sign
    extensions or build problems with older versions of binutils.
    
    Consistent use of the _BITUL() macro makes these problems disappear.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index bb15d09daf56..0e8104f6c660 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -20,6 +20,7 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
+#include <linux/const.h>
 #include <linux/stringify.h>
 
 /*
@@ -449,31 +450,31 @@
 #define SYS_ICH_LR15_EL2		__SYS__LR8_EL2(7)
 
 /* Common SCTLR_ELx flags. */
-#define SCTLR_ELx_DSSBS	(1UL << 44)
-#define SCTLR_ELx_ENIA	(1U << 31)
-#define SCTLR_ELx_ENIB	(1 << 30)
-#define SCTLR_ELx_ENDA	(1 << 27)
-#define SCTLR_ELx_EE    (1 << 25)
-#define SCTLR_ELx_IESB	(1 << 21)
-#define SCTLR_ELx_WXN	(1 << 19)
-#define SCTLR_ELx_ENDB	(1 << 13)
-#define SCTLR_ELx_I	(1 << 12)
-#define SCTLR_ELx_SA	(1 << 3)
-#define SCTLR_ELx_C	(1 << 2)
-#define SCTLR_ELx_A	(1 << 1)
-#define SCTLR_ELx_M	1
+#define SCTLR_ELx_DSSBS	(_BITUL(44))
+#define SCTLR_ELx_ENIA	(_BITUL(31))
+#define SCTLR_ELx_ENIB	(_BITUL(30))
+#define SCTLR_ELx_ENDA	(_BITUL(27))
+#define SCTLR_ELx_EE    (_BITUL(25))
+#define SCTLR_ELx_IESB	(_BITUL(21))
+#define SCTLR_ELx_WXN	(_BITUL(19))
+#define SCTLR_ELx_ENDB	(_BITUL(13))
+#define SCTLR_ELx_I	(_BITUL(12))
+#define SCTLR_ELx_SA	(_BITUL(3))
+#define SCTLR_ELx_C	(_BITUL(2))
+#define SCTLR_ELx_A	(_BITUL(1))
+#define SCTLR_ELx_M	(_BITUL(0))
 
 #define SCTLR_ELx_FLAGS	(SCTLR_ELx_M  | SCTLR_ELx_A | SCTLR_ELx_C | \
 			 SCTLR_ELx_SA | SCTLR_ELx_I | SCTLR_ELx_IESB)
 
 /* SCTLR_EL2 specific flags. */
-#define SCTLR_EL2_RES1	((1 << 4)  | (1 << 5)  | (1 << 11) | (1 << 16) | \
-			 (1 << 18) | (1 << 22) | (1 << 23) | (1 << 28) | \
-			 (1 << 29))
-#define SCTLR_EL2_RES0	((1 << 6)  | (1 << 7)  | (1 << 8)  | (1 << 9)  | \
-			 (1 << 10) | (1 << 13) | (1 << 14) | (1 << 15) | \
-			 (1 << 17) | (1 << 20) | (1 << 24) | (1 << 26) | \
-			 (1 << 27) | (1 << 30) | (1 << 31) | \
+#define SCTLR_EL2_RES1	((_BITUL(4))  | (_BITUL(5))  | (_BITUL(11)) | (_BITUL(16)) | \
+			 (_BITUL(18)) | (_BITUL(22)) | (_BITUL(23)) | (_BITUL(28)) | \
+			 (_BITUL(29)))
+#define SCTLR_EL2_RES0	((_BITUL(6))  | (_BITUL(7))  | (_BITUL(8))  | (_BITUL(9))  | \
+			 (_BITUL(10)) | (_BITUL(13)) | (_BITUL(14)) | (_BITUL(15)) | \
+			 (_BITUL(17)) | (_BITUL(20)) | (_BITUL(24)) | (_BITUL(26)) | \
+			 (_BITUL(27)) | (_BITUL(30)) | (_BITUL(31)) | \
 			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
@@ -495,23 +496,23 @@
 #endif
 
 /* SCTLR_EL1 specific flags. */
-#define SCTLR_EL1_UCI		(1 << 26)
-#define SCTLR_EL1_E0E		(1 << 24)
-#define SCTLR_EL1_SPAN		(1 << 23)
-#define SCTLR_EL1_NTWE		(1 << 18)
-#define SCTLR_EL1_NTWI		(1 << 16)
-#define SCTLR_EL1_UCT		(1 << 15)
-#define SCTLR_EL1_DZE		(1 << 14)
-#define SCTLR_EL1_UMA		(1 << 9)
-#define SCTLR_EL1_SED		(1 << 8)
-#define SCTLR_EL1_ITD		(1 << 7)
-#define SCTLR_EL1_CP15BEN	(1 << 5)
-#define SCTLR_EL1_SA0		(1 << 4)
-
-#define SCTLR_EL1_RES1	((1 << 11) | (1 << 20) | (1 << 22) | (1 << 28) | \
-			 (1 << 29))
-#define SCTLR_EL1_RES0  ((1 << 6)  | (1 << 10) | (1 << 13) | (1 << 17) | \
-			 (1 << 27) | (1 << 30) | (1 << 31) | \
+#define SCTLR_EL1_UCI		(_BITUL(26))
+#define SCTLR_EL1_E0E		(_BITUL(24))
+#define SCTLR_EL1_SPAN		(_BITUL(23))
+#define SCTLR_EL1_NTWE		(_BITUL(18))
+#define SCTLR_EL1_NTWI		(_BITUL(16))
+#define SCTLR_EL1_UCT		(_BITUL(15))
+#define SCTLR_EL1_DZE		(_BITUL(14))
+#define SCTLR_EL1_UMA		(_BITUL(9))
+#define SCTLR_EL1_SED		(_BITUL(8))
+#define SCTLR_EL1_ITD		(_BITUL(7))
+#define SCTLR_EL1_CP15BEN	(_BITUL(5))
+#define SCTLR_EL1_SA0		(_BITUL(4))
+
+#define SCTLR_EL1_RES1	((_BITUL(11)) | (_BITUL(20)) | (_BITUL(22)) | (_BITUL(28)) | \
+			 (_BITUL(29)))
+#define SCTLR_EL1_RES0  ((_BITUL(6))  | (_BITUL(10)) | (_BITUL(13)) | (_BITUL(17)) | \
+			 (_BITUL(27)) | (_BITUL(30)) | (_BITUL(31)) | \
 			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
@@ -712,13 +713,13 @@
 #define ZCR_ELx_LEN_SIZE	9
 #define ZCR_ELx_LEN_MASK	0x1ff
 
-#define CPACR_EL1_ZEN_EL1EN	(1 << 16) /* enable EL1 access */
-#define CPACR_EL1_ZEN_EL0EN	(1 << 17) /* enable EL0 access, if EL1EN set */
+#define CPACR_EL1_ZEN_EL1EN	(_BITUL(16)) /* enable EL1 access */
+#define CPACR_EL1_ZEN_EL0EN	(_BITUL(17)) /* enable EL0 access, if EL1EN set */
 #define CPACR_EL1_ZEN		(CPACR_EL1_ZEN_EL1EN | CPACR_EL1_ZEN_EL0EN)
 
 
 /* Safe value for MPIDR_EL1: Bit31:RES1, Bit30:U:0, Bit24:MT:0 */
-#define SYS_MPIDR_SAFE_VAL		(1UL << 31)
+#define SYS_MPIDR_SAFE_VAL	(_BITUL(31))
 
 #ifdef __ASSEMBLY__
 

commit aa6eece8ec5095e479a354365f5e5102bfab4f5a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:20 2018 +0000

    arm64: add pointer authentication register bits
    
    The ARMv8.3 pointer authentication extension adds:
    
    * New fields in ID_AA64ISAR1 to report the presence of pointer
      authentication functionality.
    
    * New control bits in SCTLR_ELx to enable this functionality.
    
    * New system registers to hold the keys necessary for this
      functionality.
    
    * A new ESR_ELx.EC code used when the new instructions are affected by
      configurable traps
    
    This patch adds the relevant definitions to <asm/sysreg.h> and
    <asm/esr.h> for these, to be used by subsequent patches.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 78f15858535e..bb15d09daf56 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -188,6 +188,19 @@
 #define SYS_TTBR1_EL1			sys_reg(3, 0, 2, 0, 1)
 #define SYS_TCR_EL1			sys_reg(3, 0, 2, 0, 2)
 
+#define SYS_APIAKEYLO_EL1		sys_reg(3, 0, 2, 1, 0)
+#define SYS_APIAKEYHI_EL1		sys_reg(3, 0, 2, 1, 1)
+#define SYS_APIBKEYLO_EL1		sys_reg(3, 0, 2, 1, 2)
+#define SYS_APIBKEYHI_EL1		sys_reg(3, 0, 2, 1, 3)
+
+#define SYS_APDAKEYLO_EL1		sys_reg(3, 0, 2, 2, 0)
+#define SYS_APDAKEYHI_EL1		sys_reg(3, 0, 2, 2, 1)
+#define SYS_APDBKEYLO_EL1		sys_reg(3, 0, 2, 2, 2)
+#define SYS_APDBKEYHI_EL1		sys_reg(3, 0, 2, 2, 3)
+
+#define SYS_APGAKEYLO_EL1		sys_reg(3, 0, 2, 3, 0)
+#define SYS_APGAKEYHI_EL1		sys_reg(3, 0, 2, 3, 1)
+
 #define SYS_ICC_PMR_EL1			sys_reg(3, 0, 4, 6, 0)
 
 #define SYS_AFSR0_EL1			sys_reg(3, 0, 5, 1, 0)
@@ -437,9 +450,13 @@
 
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_DSSBS	(1UL << 44)
+#define SCTLR_ELx_ENIA	(1U << 31)
+#define SCTLR_ELx_ENIB	(1 << 30)
+#define SCTLR_ELx_ENDA	(1 << 27)
 #define SCTLR_ELx_EE    (1 << 25)
 #define SCTLR_ELx_IESB	(1 << 21)
 #define SCTLR_ELx_WXN	(1 << 19)
+#define SCTLR_ELx_ENDB	(1 << 13)
 #define SCTLR_ELx_I	(1 << 12)
 #define SCTLR_ELx_SA	(1 << 3)
 #define SCTLR_ELx_C	(1 << 2)
@@ -534,11 +551,24 @@
 
 /* id_aa64isar1 */
 #define ID_AA64ISAR1_SB_SHIFT		36
+#define ID_AA64ISAR1_GPI_SHIFT		28
+#define ID_AA64ISAR1_GPA_SHIFT		24
 #define ID_AA64ISAR1_LRCPC_SHIFT	20
 #define ID_AA64ISAR1_FCMA_SHIFT		16
 #define ID_AA64ISAR1_JSCVT_SHIFT	12
+#define ID_AA64ISAR1_API_SHIFT		8
+#define ID_AA64ISAR1_APA_SHIFT		4
 #define ID_AA64ISAR1_DPB_SHIFT		0
 
+#define ID_AA64ISAR1_APA_NI		0x0
+#define ID_AA64ISAR1_APA_ARCHITECTED	0x1
+#define ID_AA64ISAR1_API_NI		0x0
+#define ID_AA64ISAR1_API_IMP_DEF	0x1
+#define ID_AA64ISAR1_GPA_NI		0x0
+#define ID_AA64ISAR1_GPA_ARCHITECTED	0x1
+#define ID_AA64ISAR1_GPI_NI		0x0
+#define ID_AA64ISAR1_GPI_IMP_DEF	0x1
+
 /* id_aa64pfr0 */
 #define ID_AA64PFR0_CSV3_SHIFT		60
 #define ID_AA64PFR0_CSV2_SHIFT		56

commit bd4fb6d270bc423a9a4098108784f7f9254c4e6d
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jun 14 11:21:34 2018 +0100

    arm64: Add support for SB barrier and patch in over DSB; ISB sequences
    
    We currently use a DSB; ISB sequence to inhibit speculation in set_fs().
    Whilst this works for current CPUs, future CPUs may implement a new SB
    barrier instruction which acts as an architected speculation barrier.
    
    On CPUs that support it, patch in an SB; NOP sequence over the DSB; ISB
    sequence and advertise the presence of the new instruction to userspace.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 0c909c4a932f..78f15858535e 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -104,6 +104,11 @@
 #define SET_PSTATE_UAO(x)		__emit_inst(0xd500401f | PSTATE_UAO | ((!!x) << PSTATE_Imm_shift))
 #define SET_PSTATE_SSBS(x)		__emit_inst(0xd500401f | PSTATE_SSBS | ((!!x) << PSTATE_Imm_shift))
 
+#define __SYS_BARRIER_INSN(CRm, op2, Rt) \
+	__emit_inst(0xd5000000 | sys_insn(0, 3, 3, (CRm), (op2)) | ((Rt) & 0x1f))
+
+#define SB_BARRIER_INSN			__SYS_BARRIER_INSN(0, 7, 31)
+
 #define SYS_DC_ISW			sys_insn(1, 0, 7, 6, 2)
 #define SYS_DC_CSW			sys_insn(1, 0, 7, 10, 2)
 #define SYS_DC_CISW			sys_insn(1, 0, 7, 14, 2)
@@ -528,6 +533,7 @@
 #define ID_AA64ISAR0_AES_SHIFT		4
 
 /* id_aa64isar1 */
+#define ID_AA64ISAR1_SB_SHIFT		36
 #define ID_AA64ISAR1_LRCPC_SHIFT	20
 #define ID_AA64ISAR1_FCMA_SHIFT		16
 #define ID_AA64ISAR1_JSCVT_SHIFT	12

commit b5d9a07ef7736b2456b9d3c90568de25e43d8ec3
Author: Sergey Matyukevich <geomatsi@gmail.com>
Date:   Fri Nov 16 21:21:30 2018 +0300

    arm64: sysreg: fix sparse warnings
    
    Specify correct type for the constants to avoid
    the following sparse complaints:
    
    ./arch/arm64/include/asm/sysreg.h:471:42: warning: constant 0xffffffffffffffff is so big it is unsigned long
    ./arch/arm64/include/asm/sysreg.h:512:42: warning: constant 0xffffffffffffffff is so big it is unsigned long
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Sergey Matyukevich <geomatsi@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 0c909c4a932f..842fb9572661 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -468,7 +468,7 @@
 			 SCTLR_ELx_SA     | SCTLR_ELx_I    | SCTLR_ELx_WXN | \
 			 SCTLR_ELx_DSSBS | ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
 
-#if (SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != 0xffffffffffffffff
+#if (SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != 0xffffffffffffffffUL
 #error "Inconsistent SCTLR_EL2 set/clear bits"
 #endif
 
@@ -509,7 +509,7 @@
 			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
 			 SCTLR_ELx_DSSBS | SCTLR_EL1_NTWI  | SCTLR_EL1_RES0)
 
-#if (SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != 0xffffffffffffffff
+#if (SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != 0xffffffffffffffffUL
 #error "Inconsistent SCTLR_EL1 set/clear bits"
 #endif
 

commit c219bc4e920518feb025749bdf9623aa57e94b64
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 1 12:19:43 2018 +0100

    arm64: Trap WFI executed in userspace
    
    It recently came to light that userspace can execute WFI, and that
    the arm64 kernel doesn't trap this event. This sounds rather benign,
    but the kernel should decide when it wants to wait for an interrupt,
    and not userspace.
    
    Let's trap WFI and immediately return after having skipped the
    instruction. This effectively makes WFI a rather expensive NOP.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 98e6f44cb021..0c909c4a932f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -502,12 +502,12 @@
 
 #define SCTLR_EL1_SET	(SCTLR_ELx_M    | SCTLR_ELx_C    | SCTLR_ELx_SA   |\
 			 SCTLR_EL1_SA0  | SCTLR_EL1_SED  | SCTLR_ELx_I    |\
-			 SCTLR_EL1_DZE  | SCTLR_EL1_UCT  | SCTLR_EL1_NTWI |\
+			 SCTLR_EL1_DZE  | SCTLR_EL1_UCT                   |\
 			 SCTLR_EL1_NTWE | SCTLR_ELx_IESB | SCTLR_EL1_SPAN |\
 			 ENDIAN_SET_EL1 | SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
 #define SCTLR_EL1_CLEAR	(SCTLR_ELx_A   | SCTLR_EL1_CP15BEN | SCTLR_EL1_ITD    |\
 			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
-			 SCTLR_ELx_DSSBS | SCTLR_EL1_RES0)
+			 SCTLR_ELx_DSSBS | SCTLR_EL1_NTWI  | SCTLR_EL1_RES0)
 
 #if (SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != 0xffffffffffffffff
 #error "Inconsistent SCTLR_EL1 set/clear bits"

commit 9376b1e7b62523c84fde34908af01a85b8d0468a
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Sep 27 15:07:37 2018 +0200

    arm64: remove unused asm/compiler.h header file
    
    arm64 does not define CONFIG_HAVE_ARCH_COMPILER_H, nor does it keep
    anything useful in its copy of asm/compiler.h, so let's remove it
    before anybody starts using it.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 7e9ab1fa090c..98e6f44cb021 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -20,7 +20,6 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
-#include <asm/compiler.h>
 #include <linux/stringify.h>
 
 /*

commit 74e248286e1d04b0d9bfdd002450ef0211f6f29f
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Sun Sep 16 23:17:23 2018 +0100

    arm64: sysreg: Clean up instructions for modifying PSTATE fields
    
    Instructions for modifying the PSTATE fields which were not supported
    in the older toolchains (e.g, PAN, UAO) are generated using macros.
    We have so far used the normal sys_reg() helper for defining the PSTATE
    fields. While this works fine, it is really difficult to correlate the
    code with the Arm ARM definition.
    
    As per Arm ARM, the PSTATE fields are defined only using Op1, Op2 fields,
    with fixed values for Op0, CRn. Also the CRm field has been reserved
    for the Immediate value for the instruction. So using the sys_reg()
    looks quite confusing.
    
    This patch cleans up the instruction helpers by bringing them
    in line with the Arm ARM definitions to make it easier to correlate
    code with the document. No functional changes.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 3091ae5975a3..7e9ab1fa090c 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -84,16 +84,26 @@
 
 #endif	/* CONFIG_BROKEN_GAS_INST */
 
-#define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
-#define REG_PSTATE_UAO_IMM		sys_reg(0, 0, 4, 0, 3)
-#define REG_PSTATE_SSBS_IMM		sys_reg(0, 3, 4, 0, 1)
-
-#define SET_PSTATE_PAN(x) __emit_inst(0xd5000000 | REG_PSTATE_PAN_IMM |	\
-				      (!!x)<<8 | 0x1f)
-#define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
-				      (!!x)<<8 | 0x1f)
-#define SET_PSTATE_SSBS(x) __emit_inst(0xd5000000 | REG_PSTATE_SSBS_IMM | \
-				       (!!x)<<8 | 0x1f)
+/*
+ * Instructions for modifying PSTATE fields.
+ * As per Arm ARM for v8-A, Section "C.5.1.3 op0 == 0b00, architectural hints,
+ * barriers and CLREX, and PSTATE access", ARM DDI 0487 C.a, system instructions
+ * for accessing PSTATE fields have the following encoding:
+ *	Op0 = 0, CRn = 4
+ *	Op1, Op2 encodes the PSTATE field modified and defines the constraints.
+ *	CRm = Imm4 for the instruction.
+ *	Rt = 0x1f
+ */
+#define pstate_field(op1, op2)		((op1) << Op1_shift | (op2) << Op2_shift)
+#define PSTATE_Imm_shift		CRm_shift
+
+#define PSTATE_PAN			pstate_field(0, 4)
+#define PSTATE_UAO			pstate_field(0, 3)
+#define PSTATE_SSBS			pstate_field(3, 1)
+
+#define SET_PSTATE_PAN(x)		__emit_inst(0xd500401f | PSTATE_PAN | ((!!x) << PSTATE_Imm_shift))
+#define SET_PSTATE_UAO(x)		__emit_inst(0xd500401f | PSTATE_UAO | ((!!x) << PSTATE_Imm_shift))
+#define SET_PSTATE_SSBS(x)		__emit_inst(0xd500401f | PSTATE_SSBS | ((!!x) << PSTATE_Imm_shift))
 
 #define SYS_DC_ISW			sys_insn(1, 0, 7, 6, 2)
 #define SYS_DC_CSW			sys_insn(1, 0, 7, 10, 2)

commit 8f04e8e6e29c93421a95b61cad62e3918425eac7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:47:06 2018 +0100

    arm64: ssbd: Add support for PSTATE.SSBS rather than trapping to EL3
    
    On CPUs with support for PSTATE.SSBS, the kernel can toggle the SSBD
    state without needing to call into firmware.
    
    This patch hooks into the existing SSBD infrastructure so that SSBS is
    used on CPUs that support it, but it's all made horribly complicated by
    the very real possibility of big/little systems that don't uniformly
    provide the new capability.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 2fc6242baf11..3091ae5975a3 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -86,11 +86,14 @@
 
 #define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
 #define REG_PSTATE_UAO_IMM		sys_reg(0, 0, 4, 0, 3)
+#define REG_PSTATE_SSBS_IMM		sys_reg(0, 3, 4, 0, 1)
 
 #define SET_PSTATE_PAN(x) __emit_inst(0xd5000000 | REG_PSTATE_PAN_IMM |	\
 				      (!!x)<<8 | 0x1f)
 #define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
 				      (!!x)<<8 | 0x1f)
+#define SET_PSTATE_SSBS(x) __emit_inst(0xd5000000 | REG_PSTATE_SSBS_IMM | \
+				       (!!x)<<8 | 0x1f)
 
 #define SYS_DC_ISW			sys_insn(1, 0, 7, 6, 2)
 #define SYS_DC_CSW			sys_insn(1, 0, 7, 10, 2)

commit d71be2b6c0e19180b5f80a6d42039cc074a693a2
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jun 15 11:37:34 2018 +0100

    arm64: cpufeature: Detect SSBS and advertise to userspace
    
    Armv8.5 introduces a new PSTATE bit known as Speculative Store Bypass
    Safe (SSBS) which can be used as a mitigation against Spectre variant 4.
    
    Additionally, a CPU may provide instructions to manipulate PSTATE.SSBS
    directly, so that userspace can toggle the SSBS control without trapping
    to the kernel.
    
    This patch probes for the existence of SSBS and advertise the new instructions
    to userspace if they exist.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index c1470931b897..2fc6242baf11 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -419,6 +419,7 @@
 #define SYS_ICH_LR15_EL2		__SYS__LR8_EL2(7)
 
 /* Common SCTLR_ELx flags. */
+#define SCTLR_ELx_DSSBS	(1UL << 44)
 #define SCTLR_ELx_EE    (1 << 25)
 #define SCTLR_ELx_IESB	(1 << 21)
 #define SCTLR_ELx_WXN	(1 << 19)
@@ -439,7 +440,7 @@
 			 (1 << 10) | (1 << 13) | (1 << 14) | (1 << 15) | \
 			 (1 << 17) | (1 << 20) | (1 << 24) | (1 << 26) | \
 			 (1 << 27) | (1 << 30) | (1 << 31) | \
-			 (0xffffffffUL << 32))
+			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL2		SCTLR_ELx_EE
@@ -453,7 +454,7 @@
 #define SCTLR_EL2_SET	(SCTLR_ELx_IESB   | ENDIAN_SET_EL2   | SCTLR_EL2_RES1)
 #define SCTLR_EL2_CLEAR	(SCTLR_ELx_M      | SCTLR_ELx_A    | SCTLR_ELx_C   | \
 			 SCTLR_ELx_SA     | SCTLR_ELx_I    | SCTLR_ELx_WXN | \
-			 ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
+			 SCTLR_ELx_DSSBS | ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
 
 #if (SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != 0xffffffffffffffff
 #error "Inconsistent SCTLR_EL2 set/clear bits"
@@ -477,7 +478,7 @@
 			 (1 << 29))
 #define SCTLR_EL1_RES0  ((1 << 6)  | (1 << 10) | (1 << 13) | (1 << 17) | \
 			 (1 << 27) | (1 << 30) | (1 << 31) | \
-			 (0xffffffffUL << 32))
+			 (0xffffefffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL1		(SCTLR_EL1_E0E | SCTLR_ELx_EE)
@@ -494,7 +495,7 @@
 			 ENDIAN_SET_EL1 | SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
 #define SCTLR_EL1_CLEAR	(SCTLR_ELx_A   | SCTLR_EL1_CP15BEN | SCTLR_EL1_ITD    |\
 			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
-			 SCTLR_EL1_RES0)
+			 SCTLR_ELx_DSSBS | SCTLR_EL1_RES0)
 
 #if (SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != 0xffffffffffffffff
 #error "Inconsistent SCTLR_EL1 set/clear bits"
@@ -544,6 +545,13 @@
 #define ID_AA64PFR0_EL0_64BIT_ONLY	0x1
 #define ID_AA64PFR0_EL0_32BIT_64BIT	0x2
 
+/* id_aa64pfr1 */
+#define ID_AA64PFR1_SSBS_SHIFT		4
+
+#define ID_AA64PFR1_SSBS_PSTATE_NI	0
+#define ID_AA64PFR1_SSBS_PSTATE_ONLY	1
+#define ID_AA64PFR1_SSBS_PSTATE_INSNS	2
+
 /* id_aa64mmfr0 */
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28
 #define ID_AA64MMFR0_TGRAN64_SHIFT	24

commit 631989303b06b8fdb15ec3b88aee2d25e80d4cec
Merge: ad1d69735878 976d34e2dab1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 22 14:07:56 2018 +0200

    Merge tag 'kvmarm-for-v4.19' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 4.19
    
    - Support for Group0 interrupts in guests
    - Cache management optimizations for ARMv8.4 systems
    - Userspace interface for RAS, allowing error retrival and injection
    - Fault path optimization
    - Emulated physical timer fixes
    - Random cleanups

commit 03bd646d863d1e4399d9dec658e1e5735f24cd2c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Aug 6 13:03:36 2018 +0100

    KVM: arm64: vgic-v3: Add support for ICC_SGI0R_EL1 and ICC_ASGI1R_EL1 accesses
    
    In order to generate Group0 SGIs, let's add some decoding logic to
    access_gic_sgi(), and pass the generating group accordingly.
    
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 98af0b37fb31..b0d2a52a71a3 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -314,6 +314,8 @@
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_RPR_EL1			sys_reg(3, 0, 12, 11, 3)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
+#define SYS_ICC_ASGI1R_EL1		sys_reg(3, 0, 12, 11, 6)
+#define SYS_ICC_SGI0R_EL1		sys_reg(3, 0, 12, 11, 7)
 #define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)
 #define SYS_ICC_EOIR1_EL1		sys_reg(3, 0, 12, 12, 1)
 #define SYS_ICC_HPPIR1_EL1		sys_reg(3, 0, 12, 12, 2)

commit 25be597ada0b49d2748ab520a78a28c1764d69e4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:38 2018 +0100

    arm64: kill config_sctlr_el1()
    
    Now that we have sysreg_clear_set(), we can consistently use this
    instead of config_sctlr_el1().
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index fefc17dae8ee..e205ec8489e9 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -742,16 +742,6 @@ asm(
 		write_sysreg(__scs_new, sysreg);			\
 } while (0)
 
-static inline void config_sctlr_el1(u32 clear, u32 set)
-{
-	u32 val;
-
-	val = read_sysreg(sctlr_el1);
-	val &= ~clear;
-	val |= set;
-	write_sysreg(val, sctlr_el1);
-}
-
 #endif
 
 #endif	/* __ASM_SYSREG_H */

commit 1c312e84c2d71da4101754fa6118f703f7473e01
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:37 2018 +0100

    arm64: move SCTLR_EL{1,2} assertions to <asm/sysreg.h>
    
    Currently we assert that the SCTLR_EL{1,2}_{SET,CLEAR} bits are
    self-consistent with an assertion in config_sctlr_el1(). This is a bit
    unusual, since config_sctlr_el1() doesn't make use of these definitions,
    and is far away from the definitions themselves.
    
    We can use the CPP #error directive to have equivalent assertions in
    <asm/sysreg.h>, next to the definitions of the set/clear bits, which is
    a bit clearer and simpler.
    
    At the same time, lets fill in the upper 32 bits for both registers in
    their respective RES0 definitions. This could be a little nicer with
    GENMASK_ULL(63, 32), but this currently lives in <linux/bitops.h>, which
    cannot safely be included from assembly, as <asm/sysreg.h> can.
    
    Note the when the preprocessor evaluates an expression for an #if
    directive, all signed or unsigned values are treated as intmax_t or
    uintmax_t respectively. To avoid ambiguity, we define explicitly define
    the mask of all 64 bits.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index a8f84812c6e8..fefc17dae8ee 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -436,7 +436,8 @@
 #define SCTLR_EL2_RES0	((1 << 6)  | (1 << 7)  | (1 << 8)  | (1 << 9)  | \
 			 (1 << 10) | (1 << 13) | (1 << 14) | (1 << 15) | \
 			 (1 << 17) | (1 << 20) | (1 << 24) | (1 << 26) | \
-			 (1 << 27) | (1 << 30) | (1 << 31))
+			 (1 << 27) | (1 << 30) | (1 << 31) | \
+			 (0xffffffffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL2		SCTLR_ELx_EE
@@ -452,9 +453,9 @@
 			 SCTLR_ELx_SA     | SCTLR_ELx_I    | SCTLR_ELx_WXN | \
 			 ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
 
-/* Check all the bits are accounted for */
-#define SCTLR_EL2_BUILD_BUG_ON_MISSING_BITS	BUILD_BUG_ON((SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != ~0)
-
+#if (SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != 0xffffffffffffffff
+#error "Inconsistent SCTLR_EL2 set/clear bits"
+#endif
 
 /* SCTLR_EL1 specific flags. */
 #define SCTLR_EL1_UCI		(1 << 26)
@@ -473,7 +474,8 @@
 #define SCTLR_EL1_RES1	((1 << 11) | (1 << 20) | (1 << 22) | (1 << 28) | \
 			 (1 << 29))
 #define SCTLR_EL1_RES0  ((1 << 6)  | (1 << 10) | (1 << 13) | (1 << 17) | \
-			 (1 << 27) | (1 << 30) | (1 << 31))
+			 (1 << 27) | (1 << 30) | (1 << 31) | \
+			 (0xffffffffUL << 32))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL1		(SCTLR_EL1_E0E | SCTLR_ELx_EE)
@@ -492,8 +494,9 @@
 			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
 			 SCTLR_EL1_RES0)
 
-/* Check all the bits are accounted for */
-#define SCTLR_EL1_BUILD_BUG_ON_MISSING_BITS	BUILD_BUG_ON((SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != ~0)
+#if (SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != 0xffffffffffffffff
+#error "Inconsistent SCTLR_EL1 set/clear bits"
+#endif
 
 /* id_aa64isar0 */
 #define ID_AA64ISAR0_TS_SHIFT		52
@@ -743,9 +746,6 @@ static inline void config_sctlr_el1(u32 clear, u32 set)
 {
 	u32 val;
 
-	SCTLR_EL2_BUILD_BUG_ON_MISSING_BITS;
-	SCTLR_EL1_BUILD_BUG_ON_MISSING_BITS;
-
 	val = read_sysreg(sctlr_el1);
 	val &= ~clear;
 	val |= set;

commit e48d53a91f6e90873e21a5ca5e8c0d7a9f8936a4
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 6 12:27:28 2018 +0100

    arm64: KVM: Add support for Stage-2 control of memory types and cacheability
    
    Up to ARMv8.3, the combinaison of Stage-1 and Stage-2 attributes
    results in the strongest attribute of the two stages.  This means
    that the hypervisor has to perform quite a lot of cache maintenance
    just in case the guest has some non-cacheable mappings around.
    
    ARMv8.4 solves this problem by offering a different mode (FWB) where
    Stage-2 has total control over the memory attribute (this is limited
    to systems where both I/O and instruction fetches are coherent with
    the dcache). This is achieved by having a different set of memory
    attributes in the page tables, and a new bit set in HCR_EL2.
    
    On such a system, we can then safely sidestep any form of dcache
    management.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index a8f84812c6e8..98af0b37fb31 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -576,6 +576,7 @@
 #define ID_AA64MMFR1_VMIDBITS_16	2
 
 /* id_aa64mmfr2 */
+#define ID_AA64MMFR2_FWB_SHIFT		40
 #define ID_AA64MMFR2_AT_SHIFT		32
 #define ID_AA64MMFR2_LVA_SHIFT		16
 #define ID_AA64MMFR2_IESB_SHIFT		12

commit 6ebdf4db8fa564a150f46d32178af0873eb5abbb
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jun 15 16:47:23 2018 +0100

    arm64: Introduce sysreg_clear_set()
    
    Currently we have a couple of helpers to manipulate bits in particular
    sysregs:
    
     * config_sctlr_el1(u32 clear, u32 set)
    
     * change_cpacr(u64 val, u64 mask)
    
    The parameters of these differ in naming convention, order, and size,
    which is unfortunate. They also differ slightly in behaviour, as
    change_cpacr() skips the sysreg write if the bits are unchanged, which
    is a useful optimization when sysreg writes are expensive.
    
    Before we gain yet another sysreg manipulation function, let's
    unify these with a common helper, providing a consistent order for
    clear/set operands, and the write skipping behaviour from
    change_cpacr(). Code will be migrated to the new helper in subsequent
    patches.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6171178075dc..a8f84812c6e8 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -728,6 +728,17 @@ asm(
 	asm volatile("msr_s " __stringify(r) ", %x0" : : "rZ" (__val));	\
 } while (0)
 
+/*
+ * Modify bits in a sysreg. Bits in the clear mask are zeroed, then bits in the
+ * set mask are set. Other bits are left as-is.
+ */
+#define sysreg_clear_set(sysreg, clear, set) do {			\
+	u64 __scs_val = read_sysreg(sysreg);				\
+	u64 __scs_new = (__scs_val & ~(u64)(clear)) | (set);		\
+	if (__scs_new != __scs_val)					\
+		write_sysreg(__scs_new, sysreg);			\
+} while (0)
+
 static inline void config_sctlr_el1(u32 clear, u32 set)
 {
 	u32 val;

commit d8312a3f61024352f1c7cb967571fd53631b0d6c
Merge: e9092d0d9796 e01bca2fc698
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 9 11:42:31 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - VHE optimizations
    
       - EL2 address space randomization
    
       - speculative execution mitigations ("variant 3a", aka execution past
         invalid privilege register access)
    
       - bugfixes and cleanups
    
      PPC:
       - improvements for the radix page fault handler for HV KVM on POWER9
    
      s390:
       - more kvm stat counters
    
       - virtio gpu plumbing
    
       - documentation
    
       - facilities improvements
    
      x86:
       - support for VMware magic I/O port and pseudo-PMCs
    
       - AMD pause loop exiting
    
       - support for AMD core performance extensions
    
       - support for synchronous register access
    
       - expose nVMX capabilities to userspace
    
       - support for Hyper-V signaling via eventfd
    
       - use Enlightened VMCS when running on Hyper-V
    
       - allow userspace to disable MWAIT/HLT/PAUSE vmexits
    
       - usual roundup of optimizations and nested virtualization bugfixes
    
      Generic:
       - API selftest infrastructure (though the only tests are for x86 as
         of now)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (174 commits)
      kvm: x86: fix a prototype warning
      kvm: selftests: add sync_regs_test
      kvm: selftests: add API testing infrastructure
      kvm: x86: fix a compile warning
      KVM: X86: Add Force Emulation Prefix for "emulate the next instruction"
      KVM: X86: Introduce handle_ud()
      KVM: vmx: unify adjacent #ifdefs
      x86: kvm: hide the unused 'cpu' variable
      KVM: VMX: remove bogus WARN_ON in handle_ept_misconfig
      Revert "KVM: X86: Fix SMRAM accessing even if VM is shutdown"
      kvm: Add emulation for movups/movupd
      KVM: VMX: raise internal error for exception during invalid protected mode state
      KVM: nVMX: Optimization: Dont set KVM_REQ_EVENT when VMExit with nested_run_pending
      KVM: nVMX: Require immediate-exit when event reinjected to L2 and L1 event pending
      KVM: x86: Fix misleading comments on handling pending exceptions
      KVM: x86: Rename interrupt.pending to interrupt.injected
      KVM: VMX: No need to clear pending NMI/interrupt on inject realmode interrupt
      x86/kvm: use Enlightened VMCS when running on Hyper-V
      x86/hyper-v: detect nested features
      x86/hyper-v: define struct hv_enlightened_vmcs and clean field bits
      ...

commit 7206dc93a58fb76421c4411eefa3c003337bcb2d
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 12 10:04:14 2018 +0000

    arm64: Expose Arm v8.4 features
    
    Expose the new features introduced by Arm v8.4 extensions to
    Arm v8-A profile.
    
    These include :
    
     1) Data indpendent timing of instructions. (DIT, exposed as HWCAP_DIT)
     2) Unaligned atomic instructions and Single-copy atomicity of loads
        and stores. (AT, expose as HWCAP_USCAT)
     3) LDAPR and STLR instructions with immediate offsets (extension to
        LRCPC, exposed as HWCAP_ILRCPC)
     4) Flag manipulation instructions (TS, exposed as HWCAP_FLAGM).
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 0e1960c59197..e7b9f154e476 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -490,6 +490,7 @@
 #define SCTLR_EL1_BUILD_BUG_ON_MISSING_BITS	BUILD_BUG_ON((SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != ~0)
 
 /* id_aa64isar0 */
+#define ID_AA64ISAR0_TS_SHIFT		52
 #define ID_AA64ISAR0_FHM_SHIFT		48
 #define ID_AA64ISAR0_DP_SHIFT		44
 #define ID_AA64ISAR0_SM4_SHIFT		40
@@ -511,6 +512,7 @@
 /* id_aa64pfr0 */
 #define ID_AA64PFR0_CSV3_SHIFT		60
 #define ID_AA64PFR0_CSV2_SHIFT		56
+#define ID_AA64PFR0_DIT_SHIFT		48
 #define ID_AA64PFR0_SVE_SHIFT		32
 #define ID_AA64PFR0_RAS_SHIFT		28
 #define ID_AA64PFR0_GIC_SHIFT		24
@@ -568,6 +570,7 @@
 #define ID_AA64MMFR1_VMIDBITS_16	2
 
 /* id_aa64mmfr2 */
+#define ID_AA64MMFR2_AT_SHIFT		32
 #define ID_AA64MMFR2_LVA_SHIFT		16
 #define ID_AA64MMFR2_IESB_SHIFT		12
 #define ID_AA64MMFR2_LSM_SHIFT		8

commit cc33c4e20185a391766ed5e78e2acc97e17ba511
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Feb 13 13:39:23 2018 +0000

    arm64/kvm: Prohibit guest LOR accesses
    
    We don't currently limit guest accesses to the LOR registers, which we
    neither virtualize nor context-switch. As such, guests are provided with
    unusable information/controls, and are not isolated from each other (or
    the host).
    
    To prevent these issues, we can trap register accesses and present the
    illusion LORegions are unssupported by the CPU. To do this, we mask
    ID_AA64MMFR1.LO, and set HCR_EL2.TLOR to trap accesses to the following
    registers:
    
    * LORC_EL1
    * LOREA_EL1
    * LORID_EL1
    * LORN_EL1
    * LORSA_EL1
    
    ... when trapped, we inject an UNDEFINED exception to EL1, simulating
    their non-existence.
    
    As noted in D7.2.67, when no LORegions are implemented, LoadLOAcquire
    and StoreLORelease must behave as LoadAcquire and StoreRelease
    respectively. We can ensure this by clearing LORC_EL1.EN when a CPU's
    EL2 is first initialized, as the host kernel will not modify this.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Vladimir Murzin <vladimir.murzin@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 0e1960c59197..69a99856461c 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -288,6 +288,12 @@
 #define SYS_MAIR_EL1			sys_reg(3, 0, 10, 2, 0)
 #define SYS_AMAIR_EL1			sys_reg(3, 0, 10, 3, 0)
 
+#define SYS_LORSA_EL1			sys_reg(3, 0, 10, 4, 0)
+#define SYS_LOREA_EL1			sys_reg(3, 0, 10, 4, 1)
+#define SYS_LORN_EL1			sys_reg(3, 0, 10, 4, 2)
+#define SYS_LORC_EL1			sys_reg(3, 0, 10, 4, 3)
+#define SYS_LORID_EL1			sys_reg(3, 0, 10, 4, 7)
+
 #define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
 #define SYS_DISR_EL1			sys_reg(3, 0, 12, 1, 1)
 

commit 558daf693e0c7ea118dbfb9539aa5a72f34bad2e
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Mon Jan 15 19:39:06 2018 +0000

    KVM: arm64: Emulate RAS error registers and set HCR_EL2's TERR & TEA
    
    ARMv8.2 adds a new bit HCR_EL2.TEA which routes synchronous external
    aborts to EL2, and adds a trap control bit HCR_EL2.TERR which traps
    all Non-secure EL1&0 error record accesses to EL2.
    
    This patch enables the two bits for the guest OS, guaranteeing that
    KVM takes external aborts and traps attempts to access the physical
    error registers.
    
    ERRIDR_EL1 advertises the number of error records, we return
    zero meaning we can treat all the other registers as RAZ/WI too.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    [removed specific emulation, use trap_raz_wi() directly for everything,
     rephrased parts of the commit message]
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 53d99c0944b1..0e1960c59197 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -176,6 +176,16 @@
 #define SYS_AFSR0_EL1			sys_reg(3, 0, 5, 1, 0)
 #define SYS_AFSR1_EL1			sys_reg(3, 0, 5, 1, 1)
 #define SYS_ESR_EL1			sys_reg(3, 0, 5, 2, 0)
+
+#define SYS_ERRIDR_EL1			sys_reg(3, 0, 5, 3, 0)
+#define SYS_ERRSELR_EL1			sys_reg(3, 0, 5, 3, 1)
+#define SYS_ERXFR_EL1			sys_reg(3, 0, 5, 4, 0)
+#define SYS_ERXCTLR_EL1			sys_reg(3, 0, 5, 4, 1)
+#define SYS_ERXSTATUS_EL1		sys_reg(3, 0, 5, 4, 2)
+#define SYS_ERXADDR_EL1			sys_reg(3, 0, 5, 4, 3)
+#define SYS_ERXMISC0_EL1		sys_reg(3, 0, 5, 5, 0)
+#define SYS_ERXMISC1_EL1		sys_reg(3, 0, 5, 5, 1)
+
 #define SYS_FAR_EL1			sys_reg(3, 0, 6, 0, 0)
 #define SYS_PAR_EL1			sys_reg(3, 0, 7, 4, 0)
 

commit c773ae2b34760a1ae409614aa31cdded81a645a5
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:02 2018 +0000

    KVM: arm64: Save/Restore guest DISR_EL1
    
    If we deliver a virtual SError to the guest, the guest may defer it
    with an ESB instruction. The guest reads the deferred value via DISR_EL1,
    but the guests view of DISR_EL1 is re-mapped to VDISR_EL2 when HCR_EL2.AMO
    is set.
    
    Add the KVM code to save/restore VDISR_EL2, and make it accessible to
    userspace as DISR_EL1.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 52cfdc216bcf..53d99c0944b1 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -358,6 +358,7 @@
 #define SYS_VSESR_EL2			sys_reg(3, 4, 5, 2, 3)
 #define SYS_FPEXC32_EL2			sys_reg(3, 4, 5, 3, 0)
 
+#define SYS_VDISR_EL2			sys_reg(3, 4, 12, 1,  1)
 #define __SYS__AP0Rx_EL2(x)		sys_reg(3, 4, 12, 8, x)
 #define SYS_ICH_AP0R0_EL2		__SYS__AP0Rx_EL2(0)
 #define SYS_ICH_AP0R1_EL2		__SYS__AP0Rx_EL2(1)

commit 4715c14bc136687bb79d12e24aafdc0f38786eb7
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:01 2018 +0000

    KVM: arm64: Set an impdef ESR for Virtual-SError using VSESR_EL2.
    
    Prior to v8.2's RAS Extensions, the HCR_EL2.VSE 'virtual SError' feature
    generated an SError with an implementation defined ESR_EL1.ISS, because we
    had no mechanism to specify the ESR value.
    
    On Juno this generates an all-zero ESR, the most significant bit 'ISV'
    is clear indicating the remainder of the ISS field is invalid.
    
    With the RAS Extensions we have a mechanism to specify this value, and the
    most significant bit has a new meaning: 'IDS - Implementation Defined
    Syndrome'. An all-zero SError ESR now means: 'RAS error: Uncategorized'
    instead of 'no valid ISS'.
    
    Add KVM support for the VSESR_EL2 register to specify an ESR value when
    HCR_EL2.VSE generates a virtual SError. Change kvm_inject_vabt() to
    specify an implementation-defined value.
    
    We only need to restore the VSESR_EL2 value when HCR_EL2.VSE is set, KVM
    save/restores this bit during __{,de}activate_traps() and hardware clears the
    bit once the guest has consumed the virtual-SError.
    
    Future patches may add an API (or KVM CAP) to pend a virtual SError with
    a specified ESR.
    
    Cc: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b5d543fc677d..52cfdc216bcf 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -355,6 +355,7 @@
 
 #define SYS_DACR32_EL2			sys_reg(3, 4, 3, 0, 0)
 #define SYS_IFSR32_EL2			sys_reg(3, 4, 5, 0, 1)
+#define SYS_VSESR_EL2			sys_reg(3, 4, 5, 2, 3)
 #define SYS_FPEXC32_EL2			sys_reg(3, 4, 5, 3, 0)
 
 #define __SYS__AP0Rx_EL2(x)		sys_reg(3, 4, 12, 8, x)

commit 68ddbf09ec5a888ec850edd7e7438d2daf069c56
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:38:59 2018 +0000

    arm64: kernel: Prepare for a DISR user
    
    KVM would like to consume any pending SError (or RAS error) after guest
    exit. Today it has to unmask SError and use dsb+isb to synchronise the
    CPU. With the RAS extensions we can use ESB to synchronise any pending
    SError.
    
    Add the necessary macros to allow DISR to be read and converted to an
    ESR.
    
    We clear the DISR register when we enable the RAS cpufeature, and the
    kernel has not executed any ESB instructions. Any value we find in DISR
    must have belonged to firmware. Executing an ESB instruction is the
    only way to update DISR, so we can expect firmware to have handled
    any deferred SError. By the same logic we clear DISR in the idle path.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 1281bc8263c2..b5d543fc677d 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -279,6 +279,7 @@
 #define SYS_AMAIR_EL1			sys_reg(3, 0, 10, 3, 0)
 
 #define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
+#define SYS_DISR_EL1			sys_reg(3, 0, 12, 1, 1)
 
 #define SYS_ICC_IAR0_EL1		sys_reg(3, 0, 12, 8, 0)
 #define SYS_ICC_EOIR0_EL1		sys_reg(3, 0, 12, 8, 1)

commit f751daa4f9d3da07e2777ea0c1ba2d58ff2c860f
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:38:58 2018 +0000

    arm64: Unconditionally enable IESB on exception entry/return for firmware-first
    
    ARM v8.2 has a feature to add implicit error synchronization barriers
    whenever the CPU enters or returns from an exception level. Add this to the
    features we always enable. CPUs that don't support this feature will treat
    the bit as RES0.
    
    This feature causes RAS errors that are not yet visible to software to
    become pending SErrors. We expect to have firmware-first RAS support
    so synchronised RAS errors will be take immediately to EL3.
    Any system without firmware-first handling of errors will take the SError
    either immediatly after exception return, or when we unmask SError after
    entry.S's work.
    
    Adding IESB to the ELx flags causes it to be enabled by KVM and kexec
    too.
    
    Platform level RAS support may require additional firmware support.
    
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Link: https://www.spinics.net/lists/kvm-arm/msg28192.html
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 321622e9f9c3..1281bc8263c2 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -399,6 +399,7 @@
 
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_EE    (1 << 25)
+#define SCTLR_ELx_IESB	(1 << 21)
 #define SCTLR_ELx_WXN	(1 << 19)
 #define SCTLR_ELx_I	(1 << 12)
 #define SCTLR_ELx_SA	(1 << 3)
@@ -406,8 +407,8 @@
 #define SCTLR_ELx_A	(1 << 1)
 #define SCTLR_ELx_M	1
 
-#define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
-			 SCTLR_ELx_SA | SCTLR_ELx_I)
+#define SCTLR_ELx_FLAGS	(SCTLR_ELx_M  | SCTLR_ELx_A | SCTLR_ELx_C | \
+			 SCTLR_ELx_SA | SCTLR_ELx_I | SCTLR_ELx_IESB)
 
 /* SCTLR_EL2 specific flags. */
 #define SCTLR_EL2_RES1	((1 << 4)  | (1 << 5)  | (1 << 11) | (1 << 16) | \
@@ -415,8 +416,8 @@
 			 (1 << 29))
 #define SCTLR_EL2_RES0	((1 << 6)  | (1 << 7)  | (1 << 8)  | (1 << 9)  | \
 			 (1 << 10) | (1 << 13) | (1 << 14) | (1 << 15) | \
-			 (1 << 17) | (1 << 20) | (1 << 21) | (1 << 24) | \
-			 (1 << 26) | (1 << 27) | (1 << 30) | (1 << 31))
+			 (1 << 17) | (1 << 20) | (1 << 24) | (1 << 26) | \
+			 (1 << 27) | (1 << 30) | (1 << 31))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL2		SCTLR_ELx_EE
@@ -427,7 +428,7 @@
 #endif
 
 /* SCTLR_EL2 value used for the hyp-stub */
-#define SCTLR_EL2_SET	(ENDIAN_SET_EL2   | SCTLR_EL2_RES1)
+#define SCTLR_EL2_SET	(SCTLR_ELx_IESB   | ENDIAN_SET_EL2   | SCTLR_EL2_RES1)
 #define SCTLR_EL2_CLEAR	(SCTLR_ELx_M      | SCTLR_ELx_A    | SCTLR_ELx_C   | \
 			 SCTLR_ELx_SA     | SCTLR_ELx_I    | SCTLR_ELx_WXN | \
 			 ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
@@ -453,7 +454,7 @@
 #define SCTLR_EL1_RES1	((1 << 11) | (1 << 20) | (1 << 22) | (1 << 28) | \
 			 (1 << 29))
 #define SCTLR_EL1_RES0  ((1 << 6)  | (1 << 10) | (1 << 13) | (1 << 17) | \
-			 (1 << 21) | (1 << 27) | (1 << 30) | (1 << 31))
+			 (1 << 27) | (1 << 30) | (1 << 31))
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define ENDIAN_SET_EL1		(SCTLR_EL1_E0E | SCTLR_ELx_EE)
@@ -466,8 +467,8 @@
 #define SCTLR_EL1_SET	(SCTLR_ELx_M    | SCTLR_ELx_C    | SCTLR_ELx_SA   |\
 			 SCTLR_EL1_SA0  | SCTLR_EL1_SED  | SCTLR_ELx_I    |\
 			 SCTLR_EL1_DZE  | SCTLR_EL1_UCT  | SCTLR_EL1_NTWI |\
-			 SCTLR_EL1_NTWE | SCTLR_EL1_SPAN | ENDIAN_SET_EL1 |\
-			 SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
+			 SCTLR_EL1_NTWE | SCTLR_ELx_IESB | SCTLR_EL1_SPAN |\
+			 ENDIAN_SET_EL1 | SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
 #define SCTLR_EL1_CLEAR	(SCTLR_ELx_A   | SCTLR_EL1_CP15BEN | SCTLR_EL1_ITD    |\
 			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
 			 SCTLR_EL1_RES0)

commit 64c02720ea3598bf5143b672274d923a941b8053
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Mon Jan 15 19:38:56 2018 +0000

    arm64: cpufeature: Detect CPU RAS Extentions
    
    ARM's v8.2 Extentions add support for Reliability, Availability and
    Serviceability (RAS). On CPUs with these extensions system software
    can use additional barriers to isolate errors and determine if faults
    are pending. Add cpufeature detection.
    
    Platform level RAS support may require additional firmware support.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    [Rebased added config option, reworded commit message]
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 1a8108f84932..321622e9f9c3 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -498,6 +498,7 @@
 #define ID_AA64PFR0_CSV3_SHIFT		60
 #define ID_AA64PFR0_CSV2_SHIFT		56
 #define ID_AA64PFR0_SVE_SHIFT		32
+#define ID_AA64PFR0_RAS_SHIFT		28
 #define ID_AA64PFR0_GIC_SHIFT		24
 #define ID_AA64PFR0_ASIMD_SHIFT		20
 #define ID_AA64PFR0_FP_SHIFT		16
@@ -507,6 +508,7 @@
 #define ID_AA64PFR0_EL0_SHIFT		0
 
 #define ID_AA64PFR0_SVE			0x1
+#define ID_AA64PFR0_RAS_V1		0x1
 #define ID_AA64PFR0_FP_NI		0xf
 #define ID_AA64PFR0_FP_SUPPORTED	0x0
 #define ID_AA64PFR0_ASIMD_NI		0xf

commit 7a00d68ebe5f07cb1db17e7fedfd031f0d87e8bb
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:38:55 2018 +0000

    arm64: sysreg: Move to use definitions for all the SCTLR bits
    
    __cpu_setup() configures SCTLR_EL1 using some hard coded hex masks,
    and el2_setup() duplicates some this when setting RES1 bits.
    
    Lets make this the same as KVM's hyp_init, which uses named bits.
    
    First, we add definitions for all the SCTLR_EL{1,2} bits, the RES{1,0}
    bits, and those we want to set or clear.
    
    Add a build_bug checks to ensures all bits are either set or clear.
    This means we don't need to preserve endian-ness configuration
    generated elsewhere.
    
    Finally, move the head.S and proc.S users of these hard-coded masks
    over to the macro versions.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 54e99af043c6..1a8108f84932 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -20,6 +20,7 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
+#include <asm/compiler.h>
 #include <linux/stringify.h>
 
 /*
@@ -398,25 +399,81 @@
 
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_EE    (1 << 25)
+#define SCTLR_ELx_WXN	(1 << 19)
 #define SCTLR_ELx_I	(1 << 12)
 #define SCTLR_ELx_SA	(1 << 3)
 #define SCTLR_ELx_C	(1 << 2)
 #define SCTLR_ELx_A	(1 << 1)
 #define SCTLR_ELx_M	1
 
+#define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
+			 SCTLR_ELx_SA | SCTLR_ELx_I)
+
+/* SCTLR_EL2 specific flags. */
 #define SCTLR_EL2_RES1	((1 << 4)  | (1 << 5)  | (1 << 11) | (1 << 16) | \
 			 (1 << 18) | (1 << 22) | (1 << 23) | (1 << 28) | \
 			 (1 << 29))
+#define SCTLR_EL2_RES0	((1 << 6)  | (1 << 7)  | (1 << 8)  | (1 << 9)  | \
+			 (1 << 10) | (1 << 13) | (1 << 14) | (1 << 15) | \
+			 (1 << 17) | (1 << 20) | (1 << 21) | (1 << 24) | \
+			 (1 << 26) | (1 << 27) | (1 << 30) | (1 << 31))
+
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define ENDIAN_SET_EL2		SCTLR_ELx_EE
+#define ENDIAN_CLEAR_EL2	0
+#else
+#define ENDIAN_SET_EL2		0
+#define ENDIAN_CLEAR_EL2	SCTLR_ELx_EE
+#endif
+
+/* SCTLR_EL2 value used for the hyp-stub */
+#define SCTLR_EL2_SET	(ENDIAN_SET_EL2   | SCTLR_EL2_RES1)
+#define SCTLR_EL2_CLEAR	(SCTLR_ELx_M      | SCTLR_ELx_A    | SCTLR_ELx_C   | \
+			 SCTLR_ELx_SA     | SCTLR_ELx_I    | SCTLR_ELx_WXN | \
+			 ENDIAN_CLEAR_EL2 | SCTLR_EL2_RES0)
+
+/* Check all the bits are accounted for */
+#define SCTLR_EL2_BUILD_BUG_ON_MISSING_BITS	BUILD_BUG_ON((SCTLR_EL2_SET ^ SCTLR_EL2_CLEAR) != ~0)
 
-#define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
-			 SCTLR_ELx_SA | SCTLR_ELx_I)
 
 /* SCTLR_EL1 specific flags. */
 #define SCTLR_EL1_UCI		(1 << 26)
+#define SCTLR_EL1_E0E		(1 << 24)
 #define SCTLR_EL1_SPAN		(1 << 23)
+#define SCTLR_EL1_NTWE		(1 << 18)
+#define SCTLR_EL1_NTWI		(1 << 16)
 #define SCTLR_EL1_UCT		(1 << 15)
+#define SCTLR_EL1_DZE		(1 << 14)
+#define SCTLR_EL1_UMA		(1 << 9)
 #define SCTLR_EL1_SED		(1 << 8)
+#define SCTLR_EL1_ITD		(1 << 7)
 #define SCTLR_EL1_CP15BEN	(1 << 5)
+#define SCTLR_EL1_SA0		(1 << 4)
+
+#define SCTLR_EL1_RES1	((1 << 11) | (1 << 20) | (1 << 22) | (1 << 28) | \
+			 (1 << 29))
+#define SCTLR_EL1_RES0  ((1 << 6)  | (1 << 10) | (1 << 13) | (1 << 17) | \
+			 (1 << 21) | (1 << 27) | (1 << 30) | (1 << 31))
+
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define ENDIAN_SET_EL1		(SCTLR_EL1_E0E | SCTLR_ELx_EE)
+#define ENDIAN_CLEAR_EL1	0
+#else
+#define ENDIAN_SET_EL1		0
+#define ENDIAN_CLEAR_EL1	(SCTLR_EL1_E0E | SCTLR_ELx_EE)
+#endif
+
+#define SCTLR_EL1_SET	(SCTLR_ELx_M    | SCTLR_ELx_C    | SCTLR_ELx_SA   |\
+			 SCTLR_EL1_SA0  | SCTLR_EL1_SED  | SCTLR_ELx_I    |\
+			 SCTLR_EL1_DZE  | SCTLR_EL1_UCT  | SCTLR_EL1_NTWI |\
+			 SCTLR_EL1_NTWE | SCTLR_EL1_SPAN | ENDIAN_SET_EL1 |\
+			 SCTLR_EL1_UCI  | SCTLR_EL1_RES1)
+#define SCTLR_EL1_CLEAR	(SCTLR_ELx_A   | SCTLR_EL1_CP15BEN | SCTLR_EL1_ITD    |\
+			 SCTLR_EL1_UMA | SCTLR_ELx_WXN     | ENDIAN_CLEAR_EL1 |\
+			 SCTLR_EL1_RES0)
+
+/* Check all the bits are accounted for */
+#define SCTLR_EL1_BUILD_BUG_ON_MISSING_BITS	BUILD_BUG_ON((SCTLR_EL1_SET ^ SCTLR_EL1_CLEAR) != ~0)
 
 /* id_aa64isar0 */
 #define ID_AA64ISAR0_FHM_SHIFT		48
@@ -593,6 +650,7 @@
 
 #else
 
+#include <linux/build_bug.h>
 #include <linux/types.h>
 
 asm(
@@ -649,6 +707,9 @@ static inline void config_sctlr_el1(u32 clear, u32 set)
 {
 	u32 val;
 
+	SCTLR_EL2_BUILD_BUG_ON_MISSING_BITS;
+	SCTLR_EL1_BUILD_BUG_ON_MISSING_BITS;
+
 	val = read_sysreg(sctlr_el1);
 	val &= ~clear;
 	val |= set;

commit 0f15adbb2861ce6f75ccfc5a92b19eae0ef327d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 3 11:17:58 2018 +0000

    arm64: Add skeleton to harden the branch predictor against aliasing attacks
    
    Aliasing attacks against CPU branch predictors can allow an attacker to
    redirect speculative control flow on some CPUs and potentially divulge
    information from one context to another.
    
    This patch adds initial skeleton code behind a new Kconfig option to
    enable implementation-specific mitigations against these attacks for
    CPUs that are affected.
    
    Co-developed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 262ae18f0e05..54e99af043c6 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -439,6 +439,7 @@
 
 /* id_aa64pfr0 */
 #define ID_AA64PFR0_CSV3_SHIFT		60
+#define ID_AA64PFR0_CSV2_SHIFT		56
 #define ID_AA64PFR0_SVE_SHIFT		32
 #define ID_AA64PFR0_GIC_SHIFT		24
 #define ID_AA64PFR0_ASIMD_SHIFT		20

commit 179a56f6f9fbda28f6ca07db1fc3dfad6bc7343c
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Nov 27 18:29:30 2017 +0000

    arm64: Take into account ID_AA64PFR0_EL1.CSV3
    
    For non-KASLR kernels where the KPTI behaviour has not been overridden
    on the command line we can use ID_AA64PFR0_EL1.CSV3 to determine whether
    or not we should unmap the kernel whilst running at EL0.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ab637886a6f9..262ae18f0e05 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -438,6 +438,7 @@
 #define ID_AA64ISAR1_DPB_SHIFT		0
 
 /* id_aa64pfr0 */
+#define ID_AA64PFR0_CSV3_SHIFT		60
 #define ID_AA64PFR0_SVE_SHIFT		32
 #define ID_AA64PFR0_GIC_SHIFT		24
 #define ID_AA64PFR0_ASIMD_SHIFT		20

commit 3b3b681097fae73b7f5dcdd42db6cfdf32943d4c
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Wed Dec 13 18:13:56 2017 +0800

    arm64: v8.4: Support for new floating point multiplication instructions
    
    ARM v8.4 extensions add new neon instructions for performing a
    multiplication of each FP16 element of one vector with the corresponding
    FP16 element of a second vector, and to add or subtract this without an
    intermediate rounding to the corresponding FP32 element in a third vector.
    
    This patch detects this feature and let the userspace know about it via a
    HWCAP bit and MRS emulation.
    
    Cc: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ec144f480b39..ab637886a6f9 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -419,6 +419,7 @@
 #define SCTLR_EL1_CP15BEN	(1 << 5)
 
 /* id_aa64isar0 */
+#define ID_AA64ISAR0_FHM_SHIFT		48
 #define ID_AA64ISAR0_DP_SHIFT		44
 #define ID_AA64ISAR0_SM4_SHIFT		40
 #define ID_AA64ISAR0_SM3_SHIFT		36

commit 787fd1d019b269af7912249231dfe34a5fe3e7c8
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Dec 13 17:07:17 2017 +0000

    arm64: limit PA size to supported range
    
    We currently copy the physical address size from
    ID_AA64MMFR0_EL1.PARange directly into TCR.(I)PS. This will not work for
    4k and 16k granule kernels on systems that support 52-bit physical
    addresses, since 52-bit addresses are only permitted with the 64k
    granule.
    
    To fix this, fall back to 48 bits when configuring the PA size when the
    kernel does not support 52-bit PAs. When it does, fall back to 52, to
    avoid similar problems in the future if the PA size is ever increased
    above 52.
    
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Bob Picco <bob.picco@oracle.com>
    Reviewed-by: Bob Picco <bob.picco@oracle.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [catalin.marinas@arm.com: tcr_set_pa_size macro renamed to tcr_compute_pa_size]
    [catalin.marinas@arm.com: comments added to tcr_compute_pa_size]
    [catalin.marinas@arm.com: definitions added for TCR_*PS_SHIFT]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 08cc88574659..ec144f480b39 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -471,6 +471,14 @@
 #define ID_AA64MMFR0_TGRAN64_SUPPORTED	0x0
 #define ID_AA64MMFR0_TGRAN16_NI		0x0
 #define ID_AA64MMFR0_TGRAN16_SUPPORTED	0x1
+#define ID_AA64MMFR0_PARANGE_48		0x5
+#define ID_AA64MMFR0_PARANGE_52		0x6
+
+#ifdef CONFIG_ARM64_PA_BITS_52
+#define ID_AA64MMFR0_PARANGE_MAX	ID_AA64MMFR0_PARANGE_52
+#else
+#define ID_AA64MMFR0_PARANGE_MAX	ID_AA64MMFR0_PARANGE_48
+#endif
 
 /* id_aa64mmfr1 */
 #define ID_AA64MMFR1_PAN_SHIFT		20

commit 672365649ccac68cf6fafecad1a7913951e7493b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:00 2017 +0000

    arm64/sve: System register and exception syndrome definitions
    
    The SVE architecture adds some system registers, ID register fields
    and a dedicated ESR exception class.
    
    This patch adds the appropriate definitions that will be needed by
    the kernel.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 609d59afe389..08cc88574659 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -145,6 +145,7 @@
 
 #define SYS_ID_AA64PFR0_EL1		sys_reg(3, 0, 0, 4, 0)
 #define SYS_ID_AA64PFR1_EL1		sys_reg(3, 0, 0, 4, 1)
+#define SYS_ID_AA64ZFR0_EL1		sys_reg(3, 0, 0, 4, 4)
 
 #define SYS_ID_AA64DFR0_EL1		sys_reg(3, 0, 0, 5, 0)
 #define SYS_ID_AA64DFR1_EL1		sys_reg(3, 0, 0, 5, 1)
@@ -163,6 +164,8 @@
 #define SYS_ACTLR_EL1			sys_reg(3, 0, 1, 0, 1)
 #define SYS_CPACR_EL1			sys_reg(3, 0, 1, 0, 2)
 
+#define SYS_ZCR_EL1			sys_reg(3, 0, 1, 2, 0)
+
 #define SYS_TTBR0_EL1			sys_reg(3, 0, 2, 0, 0)
 #define SYS_TTBR1_EL1			sys_reg(3, 0, 2, 0, 1)
 #define SYS_TCR_EL1			sys_reg(3, 0, 2, 0, 2)
@@ -346,6 +349,8 @@
 
 #define SYS_PMCCFILTR_EL0		sys_reg (3, 3, 14, 15, 7)
 
+#define SYS_ZCR_EL2			sys_reg(3, 4, 1, 2, 0)
+
 #define SYS_DACR32_EL2			sys_reg(3, 4, 3, 0, 0)
 #define SYS_IFSR32_EL2			sys_reg(3, 4, 5, 0, 1)
 #define SYS_FPEXC32_EL2			sys_reg(3, 4, 5, 3, 0)
@@ -432,6 +437,7 @@
 #define ID_AA64ISAR1_DPB_SHIFT		0
 
 /* id_aa64pfr0 */
+#define ID_AA64PFR0_SVE_SHIFT		32
 #define ID_AA64PFR0_GIC_SHIFT		24
 #define ID_AA64PFR0_ASIMD_SHIFT		20
 #define ID_AA64PFR0_FP_SHIFT		16
@@ -440,6 +446,7 @@
 #define ID_AA64PFR0_EL1_SHIFT		4
 #define ID_AA64PFR0_EL0_SHIFT		0
 
+#define ID_AA64PFR0_SVE			0x1
 #define ID_AA64PFR0_FP_NI		0xf
 #define ID_AA64PFR0_FP_SUPPORTED	0x0
 #define ID_AA64PFR0_ASIMD_NI		0xf
@@ -541,6 +548,20 @@
 #endif
 
 
+/*
+ * The ZCR_ELx_LEN_* definitions intentionally include bits [8:4] which
+ * are reserved by the SVE architecture for future expansion of the LEN
+ * field, with compatible semantics.
+ */
+#define ZCR_ELx_LEN_SHIFT	0
+#define ZCR_ELx_LEN_SIZE	9
+#define ZCR_ELx_LEN_MASK	0x1ff
+
+#define CPACR_EL1_ZEN_EL1EN	(1 << 16) /* enable EL1 access */
+#define CPACR_EL1_ZEN_EL0EN	(1 << 17) /* enable EL0 access, if EL1EN set */
+#define CPACR_EL1_ZEN		(CPACR_EL1_ZEN_EL1EN | CPACR_EL1_ZEN_EL0EN)
+
+
 /* Safe value for MPIDR_EL1: Bit31:RES1, Bit30:U:0, Bit24:MT:0 */
 #define SYS_MPIDR_SAFE_VAL		(1UL << 31)
 

commit 93390c0a1b20b98a59ee0ef1a850687b3fbc304e
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:50:56 2017 +0000

    arm64: KVM: Hide unsupported AArch64 CPU features from guests
    
    Currently, a guest kernel sees the true CPU feature registers
    (ID_*_EL1) when it reads them using MRS instructions.  This means
    that the guest may observe features that are present in the
    hardware but the host doesn't understand or doesn't provide support
    for.  A guest may legimitately try to use such a feature as per the
    architecture, but use of the feature may trap instead of working
    normally, triggering undef injection into the guest.
    
    This is not a problem for the host, but the guest may go wrong when
    running on newer hardware than the host knows about.
    
    This patch hides from guest VMs any AArch64-specific CPU features
    that the host doesn't support, by exposing to the guest the
    sanitised versions of the registers computed by the cpufeatures
    framework, instead of the true hardware registers.  To achieve
    this, HCR_EL2.TID3 is now set for AArch64 guests, and emulation
    code is added to KVM to report the sanitised versions of the
    affected registers in response to MRS and register reads from
    userspace.
    
    The affected registers are removed from invariant_sys_regs[] (since
    the invariant_sys_regs handling is no longer quite correct for
    them) and added to sys_reg_desgs[], with appropriate access(),
    get_user() and set_user() methods.  No runtime vcpu storage is
    allocated for the registers: instead, they are read on demand from
    the cpufeatures framework.  This may need modification in the
    future if there is a need for userspace to customise the features
    visible to the guest.
    
    Attempts by userspace to write the registers are handled similarly
    to the current invariant_sys_regs handling: writes are permitted,
    but only if they don't attempt to change the value.  This is
    sufficient to support VM snapshot/restore from userspace.
    
    Because of the additional registers, restoring a VM on an older
    kernel may not work unless userspace knows how to handle the extra
    VM registers exposed to the KVM user ABI by this patch.
    
    Under the principle of least damage, this patch makes no attempt to
    handle any of the other registers currently in
    invariant_sys_regs[], or to emulate registers for AArch32: however,
    these could be handled in a similar way in future, as necessary.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 4dceb12f4596..609d59afe389 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -149,6 +149,9 @@
 #define SYS_ID_AA64DFR0_EL1		sys_reg(3, 0, 0, 5, 0)
 #define SYS_ID_AA64DFR1_EL1		sys_reg(3, 0, 0, 5, 1)
 
+#define SYS_ID_AA64AFR0_EL1		sys_reg(3, 0, 0, 5, 4)
+#define SYS_ID_AA64AFR1_EL1		sys_reg(3, 0, 0, 5, 5)
+
 #define SYS_ID_AA64ISAR0_EL1		sys_reg(3, 0, 0, 6, 0)
 #define SYS_ID_AA64ISAR1_EL1		sys_reg(3, 0, 0, 6, 1)
 

commit 1e0c661f05ba0a3db52ed9debbe2b2e1d74aa0a4
Merge: 611479c79a94 07141342149a
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 24 16:05:15 2017 +0100

    Merge branch 'for-next/perf' into aarch64/for-next/core
    
    Merge in ARM PMU and perf updates for 4.15:
    
      - Support for the Statistical Profiling Extension
      - Support for Hisilicon's SoC PMU
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

commit a173c390d9c130d1dd254a9246fc7ffe4a2869f7
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Sep 20 16:48:33 2017 +0100

    arm64: sysreg: Move SPE registers and PSB into common header files
    
    SPE is part of the v8.2 architecture, so move its system register and
    field definitions into sysreg.h and the new PSB barrier into barrier.h
    
    Finally, move KVM over to using the generic definitions so that it
    doesn't have to open-code its own versions.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index f707fed5886f..94b7a0d42aad 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -172,6 +172,99 @@
 #define SYS_FAR_EL1			sys_reg(3, 0, 6, 0, 0)
 #define SYS_PAR_EL1			sys_reg(3, 0, 7, 4, 0)
 
+/*** Statistical Profiling Extension ***/
+/* ID registers */
+#define SYS_PMSIDR_EL1			sys_reg(3, 0, 9, 9, 7)
+#define SYS_PMSIDR_EL1_FE_SHIFT		0
+#define SYS_PMSIDR_EL1_FT_SHIFT		1
+#define SYS_PMSIDR_EL1_FL_SHIFT		2
+#define SYS_PMSIDR_EL1_ARCHINST_SHIFT	3
+#define SYS_PMSIDR_EL1_LDS_SHIFT	4
+#define SYS_PMSIDR_EL1_ERND_SHIFT	5
+#define SYS_PMSIDR_EL1_INTERVAL_SHIFT	8
+#define SYS_PMSIDR_EL1_INTERVAL_MASK	0xfUL
+#define SYS_PMSIDR_EL1_MAXSIZE_SHIFT	12
+#define SYS_PMSIDR_EL1_MAXSIZE_MASK	0xfUL
+#define SYS_PMSIDR_EL1_COUNTSIZE_SHIFT	16
+#define SYS_PMSIDR_EL1_COUNTSIZE_MASK	0xfUL
+
+#define SYS_PMBIDR_EL1			sys_reg(3, 0, 9, 10, 7)
+#define SYS_PMBIDR_EL1_ALIGN_SHIFT	0
+#define SYS_PMBIDR_EL1_ALIGN_MASK	0xfU
+#define SYS_PMBIDR_EL1_P_SHIFT		4
+#define SYS_PMBIDR_EL1_F_SHIFT		5
+
+/* Sampling controls */
+#define SYS_PMSCR_EL1			sys_reg(3, 0, 9, 9, 0)
+#define SYS_PMSCR_EL1_E0SPE_SHIFT	0
+#define SYS_PMSCR_EL1_E1SPE_SHIFT	1
+#define SYS_PMSCR_EL1_CX_SHIFT		3
+#define SYS_PMSCR_EL1_PA_SHIFT		4
+#define SYS_PMSCR_EL1_TS_SHIFT		5
+#define SYS_PMSCR_EL1_PCT_SHIFT		6
+
+#define SYS_PMSCR_EL2			sys_reg(3, 4, 9, 9, 0)
+#define SYS_PMSCR_EL2_E0HSPE_SHIFT	0
+#define SYS_PMSCR_EL2_E2SPE_SHIFT	1
+#define SYS_PMSCR_EL2_CX_SHIFT		3
+#define SYS_PMSCR_EL2_PA_SHIFT		4
+#define SYS_PMSCR_EL2_TS_SHIFT		5
+#define SYS_PMSCR_EL2_PCT_SHIFT		6
+
+#define SYS_PMSICR_EL1			sys_reg(3, 0, 9, 9, 2)
+
+#define SYS_PMSIRR_EL1			sys_reg(3, 0, 9, 9, 3)
+#define SYS_PMSIRR_EL1_RND_SHIFT	0
+#define SYS_PMSIRR_EL1_INTERVAL_SHIFT	8
+#define SYS_PMSIRR_EL1_INTERVAL_MASK	0xffffffUL
+
+/* Filtering controls */
+#define SYS_PMSFCR_EL1			sys_reg(3, 0, 9, 9, 4)
+#define SYS_PMSFCR_EL1_FE_SHIFT		0
+#define SYS_PMSFCR_EL1_FT_SHIFT		1
+#define SYS_PMSFCR_EL1_FL_SHIFT		2
+#define SYS_PMSFCR_EL1_B_SHIFT		16
+#define SYS_PMSFCR_EL1_LD_SHIFT		17
+#define SYS_PMSFCR_EL1_ST_SHIFT		18
+
+#define SYS_PMSEVFR_EL1			sys_reg(3, 0, 9, 9, 5)
+#define SYS_PMSEVFR_EL1_RES0		0x0000ffff00ff0f55UL
+
+#define SYS_PMSLATFR_EL1		sys_reg(3, 0, 9, 9, 6)
+#define SYS_PMSLATFR_EL1_MINLAT_SHIFT	0
+
+/* Buffer controls */
+#define SYS_PMBLIMITR_EL1		sys_reg(3, 0, 9, 10, 0)
+#define SYS_PMBLIMITR_EL1_E_SHIFT	0
+#define SYS_PMBLIMITR_EL1_FM_SHIFT	1
+#define SYS_PMBLIMITR_EL1_FM_MASK	0x3UL
+#define SYS_PMBLIMITR_EL1_FM_STOP_IRQ	(0 << SYS_PMBLIMITR_EL1_FM_SHIFT)
+
+#define SYS_PMBPTR_EL1			sys_reg(3, 0, 9, 10, 1)
+
+/* Buffer error reporting */
+#define SYS_PMBSR_EL1			sys_reg(3, 0, 9, 10, 3)
+#define SYS_PMBSR_EL1_COLL_SHIFT	16
+#define SYS_PMBSR_EL1_S_SHIFT		17
+#define SYS_PMBSR_EL1_EA_SHIFT		18
+#define SYS_PMBSR_EL1_DL_SHIFT		19
+#define SYS_PMBSR_EL1_EC_SHIFT		26
+#define SYS_PMBSR_EL1_EC_MASK		0x3fUL
+
+#define SYS_PMBSR_EL1_EC_BUF		(0x0UL << SYS_PMBSR_EL1_EC_SHIFT)
+#define SYS_PMBSR_EL1_EC_FAULT_S1	(0x24UL << SYS_PMBSR_EL1_EC_SHIFT)
+#define SYS_PMBSR_EL1_EC_FAULT_S2	(0x25UL << SYS_PMBSR_EL1_EC_SHIFT)
+
+#define SYS_PMBSR_EL1_FAULT_FSC_SHIFT	0
+#define SYS_PMBSR_EL1_FAULT_FSC_MASK	0x3fUL
+
+#define SYS_PMBSR_EL1_BUF_BSC_SHIFT	0
+#define SYS_PMBSR_EL1_BUF_BSC_MASK	0x3fUL
+
+#define SYS_PMBSR_EL1_BUF_BSC_FULL	(0x1UL << SYS_PMBSR_EL1_BUF_BSC_SHIFT)
+
+/*** End of Statistical Profiling Extension ***/
+
 #define SYS_PMINTENSET_EL1		sys_reg(3, 0, 9, 14, 1)
 #define SYS_PMINTENCLR_EL1		sys_reg(3, 0, 9, 14, 2)
 

commit f5e035f8694c3bdddc66ea46ecda965ee6853718
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Oct 11 14:01:02 2017 +0100

    arm64: Expose support for optional ARMv8-A features
    
    ARMv8-A adds a few optional features for ARMv8.2 and ARMv8.3.
    Expose them to the userspace via HWCAPs and mrs emulation.
    
    SHA2-512  - Instruction support for SHA512 Hash algorithm (e.g SHA512H,
                SHA512H2, SHA512U0, SHA512SU1)
    SHA3      - SHA3 crypto instructions (EOR3, RAX1, XAR, BCAX).
    SM3       - Instruction support for Chinese cryptography algorithm SM3
    SM4       - Instruction support for Chinese cryptography algorithm SM4
    DP        - Dot Product instructions (UDOT, SDOT).
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index f707fed5886f..32f489ba952f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -318,6 +318,10 @@
 #define SCTLR_EL1_CP15BEN	(1 << 5)
 
 /* id_aa64isar0 */
+#define ID_AA64ISAR0_DP_SHIFT		44
+#define ID_AA64ISAR0_SM4_SHIFT		40
+#define ID_AA64ISAR0_SM3_SHIFT		36
+#define ID_AA64ISAR0_SHA3_SHIFT		32
 #define ID_AA64ISAR0_RDM_SHIFT		28
 #define ID_AA64ISAR0_ATOMICS_SHIFT	20
 #define ID_AA64ISAR0_CRC32_SHIFT	16

commit 7aac405ebb3224037efd56b73d82d181111cdac3
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:40 2017 +0100

    arm64: Expose DC CVAP to userspace
    
    The ARMv8.2-DCPoP feature introduces persistent memory support to the
    architecture, by defining a point of persistence in the memory
    hierarchy, and a corresponding cache maintenance operation, DC CVAP.
    Expose the support via HWCAP and MRS emulation.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 248339e4aaf5..f707fed5886f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -329,6 +329,7 @@
 #define ID_AA64ISAR1_LRCPC_SHIFT	20
 #define ID_AA64ISAR1_FCMA_SHIFT		16
 #define ID_AA64ISAR1_JSCVT_SHIFT	12
+#define ID_AA64ISAR1_DPB_SHIFT		0
 
 /* id_aa64pfr0 */
 #define ID_AA64PFR0_GIC_SHIFT		24

commit d0153c7ff9226535a51e6a81f61656c9500957f4
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jul 25 12:52:41 2017 +0100

    arm64: sysreg: Fix unprotected macro argmuent in write_sysreg
    
    write_sysreg() may misparse the value argument because it is used
    without parentheses to protect it.
    
    This patch adds the ( ) in order to avoid any surprises.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    [will: same change to write_sysreg_s]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 16e44fa9b3b6..248339e4aaf5 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -492,7 +492,7 @@ asm(
  * the "%x0" template means XZR.
  */
 #define write_sysreg(v, r) do {					\
-	u64 __val = (u64)v;					\
+	u64 __val = (u64)(v);					\
 	asm volatile("msr " __stringify(r) ", %x0"		\
 		     : : "rZ" (__val));				\
 } while (0)
@@ -508,7 +508,7 @@ asm(
 })
 
 #define write_sysreg_s(v, r) do {					\
-	u64 __val = (u64)v;						\
+	u64 __val = (u64)(v);						\
 	asm volatile("msr_s " __stringify(r) ", %x0" : : "rZ" (__val));	\
 } while (0)
 

commit d38338e396ee0571b3502962fd2fbaec4d2d9a8f
Author: Stefan Traby <stefan@hello-penguin.com>
Date:   Tue Jun 20 15:30:42 2017 +0200

    arm64: Remove a redundancy in sysreg.h
    
    This is really trivial; there is a dup (1 << 16) in the code
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Stefan Traby <stefan@hello-penguin.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 040b607cb682..16e44fa9b3b6 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -304,8 +304,8 @@
 #define SCTLR_ELx_M	1
 
 #define SCTLR_EL2_RES1	((1 << 4)  | (1 << 5)  | (1 << 11) | (1 << 16) | \
-			 (1 << 16) | (1 << 18) | (1 << 22) | (1 << 23) | \
-			 (1 << 28) | (1 << 29))
+			 (1 << 18) | (1 << 22) | (1 << 23) | (1 << 28) | \
+			 (1 << 29))
 
 #define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
 			 SCTLR_ELx_SA | SCTLR_ELx_I)

commit 0959db6c0b069739ffedf3f6b97644213df586d4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jun 5 14:20:01 2017 +0100

    arm64/kvm: vgic: use SYS_DESC()
    
    Almost all of the arm64 KVM code uses the sysreg mnemonics for AArch64
    register descriptions. Move the last straggler over.
    
    To match what we do for SYS_ICH_AP*R*_EL2, the SYS_ICC_AP*R*_EL1
    mnemonics are expanded in <asm/sysreg.h>.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 00d493ba8dbd..040b607cb682 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -185,7 +185,15 @@
 #define SYS_ICC_HPPIR0_EL1		sys_reg(3, 0, 12, 8, 2)
 #define SYS_ICC_BPR0_EL1		sys_reg(3, 0, 12, 8, 3)
 #define SYS_ICC_AP0Rn_EL1(n)		sys_reg(3, 0, 12, 8, 4 | n)
+#define SYS_ICC_AP0R0_EL1		SYS_ICC_AP0Rn_EL1(0)
+#define SYS_ICC_AP0R1_EL1		SYS_ICC_AP0Rn_EL1(1)
+#define SYS_ICC_AP0R2_EL1		SYS_ICC_AP0Rn_EL1(2)
+#define SYS_ICC_AP0R3_EL1		SYS_ICC_AP0Rn_EL1(3)
 #define SYS_ICC_AP1Rn_EL1(n)		sys_reg(3, 0, 12, 9, n)
+#define SYS_ICC_AP1R0_EL1		SYS_ICC_AP1Rn_EL1(0)
+#define SYS_ICC_AP1R1_EL1		SYS_ICC_AP1Rn_EL1(1)
+#define SYS_ICC_AP1R2_EL1		SYS_ICC_AP1Rn_EL1(2)
+#define SYS_ICC_AP1R3_EL1		SYS_ICC_AP1Rn_EL1(3)
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_RPR_EL1			sys_reg(3, 0, 12, 11, 3)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)

commit 21bc52817772a5af6a8a5a750c676ea4a02d4d3b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jun 5 14:20:00 2017 +0100

    arm64/kvm: sysreg: fix typo'd SYS_ICC_IGRPEN*_EL1
    
    Per ARM DDI 0487B.a, the registers are named ICC_IGRPEN*_EL1 rather than
    ICC_GRPEN*_EL1. Correct our mnemonics and comments to match, before we
    add more GICv3 register definitions.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 56a3247e928c..00d493ba8dbd 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -195,8 +195,8 @@
 #define SYS_ICC_BPR1_EL1		sys_reg(3, 0, 12, 12, 3)
 #define SYS_ICC_CTLR_EL1		sys_reg(3, 0, 12, 12, 4)
 #define SYS_ICC_SRE_EL1			sys_reg(3, 0, 12, 12, 5)
-#define SYS_ICC_GRPEN0_EL1		sys_reg(3, 0, 12, 12, 6)
-#define SYS_ICC_GRPEN1_EL1		sys_reg(3, 0, 12, 12, 7)
+#define SYS_ICC_IGRPEN0_EL1		sys_reg(3, 0, 12, 12, 6)
+#define SYS_ICC_IGRPEN1_EL1		sys_reg(3, 0, 12, 12, 7)
 
 #define SYS_CONTEXTIDR_EL1		sys_reg(3, 0, 13, 0, 1)
 #define SYS_TPIDR_EL1			sys_reg(3, 0, 13, 0, 4)

commit 43515894c06f856b7743145e002591309f60b247
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:50 2017 +0100

    KVM: arm64: vgic-v3: Add ICV_RPR_EL1 handler
    
    Add a handler for reading the guest's view of the ICV_RPR_EL1
    register, returning the highest active priority.
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 670bf51d55e3..56a3247e928c 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -187,6 +187,7 @@
 #define SYS_ICC_AP0Rn_EL1(n)		sys_reg(3, 0, 12, 8, 4 | n)
 #define SYS_ICC_AP1Rn_EL1(n)		sys_reg(3, 0, 12, 9, n)
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
+#define SYS_ICC_RPR_EL1			sys_reg(3, 0, 12, 11, 3)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
 #define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)
 #define SYS_ICC_EOIR1_EL1		sys_reg(3, 0, 12, 12, 1)

commit eab0b2dc4f6f34147e3d10da49ab8032e15dbea0
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:44 2017 +0100

    KVM: arm64: vgic-v3: Add misc Group-0 handlers
    
    A number of Group-0 registers can be handled by the same accessors
    as that of Group-1, so let's add the required system register encodings
    and catch them in the dispatching function.
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 80b4e0a93574..670bf51d55e3 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -180,7 +180,11 @@
 
 #define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
 
+#define SYS_ICC_IAR0_EL1		sys_reg(3, 0, 12, 8, 0)
+#define SYS_ICC_EOIR0_EL1		sys_reg(3, 0, 12, 8, 1)
+#define SYS_ICC_HPPIR0_EL1		sys_reg(3, 0, 12, 8, 2)
 #define SYS_ICC_BPR0_EL1		sys_reg(3, 0, 12, 8, 3)
+#define SYS_ICC_AP0Rn_EL1(n)		sys_reg(3, 0, 12, 8, 4 | n)
 #define SYS_ICC_AP1Rn_EL1(n)		sys_reg(3, 0, 12, 9, n)
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)

commit fbc48a0011deb3d51cb657ca9c0f9083f41c0665
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:43 2017 +0100

    KVM: arm64: vgic-v3: Add ICV_IGNREN0_EL1 handler
    
    Add a handler for reading/writing the guest's view of the ICC_IGRPEN0_EL1
    register, which is located in the ICH_VMCR_EL2.VENG0 field.
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6b80211f9837..80b4e0a93574 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -190,6 +190,7 @@
 #define SYS_ICC_BPR1_EL1		sys_reg(3, 0, 12, 12, 3)
 #define SYS_ICC_CTLR_EL1		sys_reg(3, 0, 12, 12, 4)
 #define SYS_ICC_SRE_EL1			sys_reg(3, 0, 12, 12, 5)
+#define SYS_ICC_GRPEN0_EL1		sys_reg(3, 0, 12, 12, 6)
 #define SYS_ICC_GRPEN1_EL1		sys_reg(3, 0, 12, 12, 7)
 
 #define SYS_CONTEXTIDR_EL1		sys_reg(3, 0, 13, 0, 1)

commit 423de85a98c2b50715a0784a74f6124fbc0b1548
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:42 2017 +0100

    KVM: arm64: vgic-v3: Add ICV_BPR0_EL1 handler
    
    Add a handler for reading/writing the guest's view of the ICC_BPR0_EL1
    register, which is located in the ICH_VMCR_EL2.BPR0 field.
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 0ce7f81dd47e..6b80211f9837 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -180,6 +180,7 @@
 
 #define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
 
+#define SYS_ICC_BPR0_EL1		sys_reg(3, 0, 12, 8, 3)
 #define SYS_ICC_AP1Rn_EL1(n)		sys_reg(3, 0, 12, 9, n)
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)

commit 2724c11a1df4b22ee966c04809ea0e808f66b04e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:39 2017 +0100

    KVM: arm64: vgic-v3: Add ICV_HPPIR1_EL1 handler
    
    Add a handler for reading the guest's view of the ICV_HPPIR1_EL1
    register. This is a simple parsing of the available LRs, extracting the
    highest available interrupt.
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 563bba108442..0ce7f81dd47e 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -185,6 +185,7 @@
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
 #define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)
 #define SYS_ICC_EOIR1_EL1		sys_reg(3, 0, 12, 12, 1)
+#define SYS_ICC_HPPIR1_EL1		sys_reg(3, 0, 12, 12, 2)
 #define SYS_ICC_BPR1_EL1		sys_reg(3, 0, 12, 12, 3)
 #define SYS_ICC_CTLR_EL1		sys_reg(3, 0, 12, 12, 4)
 #define SYS_ICC_SRE_EL1			sys_reg(3, 0, 12, 12, 5)

commit f9e7449c780f688bf61a13dfa8c344afeb4ad6e0
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 9 12:49:38 2017 +0100

    KVM: arm64: vgic-v3: Add ICV_AP1Rn_EL1 handler
    
    Add a handler for reading/writing the guest's view of the ICV_AP1Rn_EL1
    registers. We just map them to the corresponding ICH_AP1Rn_EL2 registers.
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b4d13d9267ff..563bba108442 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -180,6 +180,7 @@
 
 #define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
 
+#define SYS_ICC_AP1Rn_EL1(n)		sys_reg(3, 0, 12, 9, n)
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
 #define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)

commit d68c1f7fd1b7148dab5fe658321d511998969f2d
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jun 6 19:08:33 2017 +0100

    arm64: KVM: Preserve RES1 bits in SCTLR_EL2
    
    __do_hyp_init has the rather bad habit of ignoring RES1 bits and
    writing them back as zero. On a v8.0-8.2 CPU, this doesn't do anything
    bad, but may end-up being pretty nasty on future revisions of the
    architecture.
    
    Let's preserve those bits so that we don't have to fix this later on.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 15c142ce991c..b4d13d9267ff 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -286,6 +286,10 @@
 #define SCTLR_ELx_A	(1 << 1)
 #define SCTLR_ELx_M	1
 
+#define SCTLR_EL2_RES1	((1 << 4)  | (1 << 5)  | (1 << 11) | (1 << 16) | \
+			 (1 << 16) | (1 << 18) | (1 << 22) | (1 << 23) | \
+			 (1 << 28) | (1 << 29))
+
 #define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
 			 SCTLR_ELx_SA | SCTLR_ELx_I)
 

commit dffb0113d522a1410dbc136281caf7b6751ec6f4
Merge: 9349e81e3837 4dc529257d9d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Apr 4 18:08:47 2017 +0100

    Merge branch 'arm64/common-sysreg' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/common-sysreg' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: sysreg: add Set/Way sys encodings
      arm64: sysreg: add register encodings used by KVM
      arm64: sysreg: add physical timer registers
      arm64: sysreg: subsume GICv3 sysreg definitions
      arm64: sysreg: add performance monitor registers
      arm64: sysreg: add debug system registers
      arm64: sysreg: sort by encoding

commit c651aae5a7732287c1c9bc974ece4ed798780544
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Mar 14 18:13:27 2017 +0000

    arm64: v8.3: Support for weaker release consistency
    
    ARMv8.3 adds new instructions to support Release Consistent
    processor consistent (RCpc) model, which is weaker than the
    RCsc model.
    
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 69af995c9b76..c776bde940bd 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -157,6 +157,7 @@
 #define ID_AA64ISAR0_AES_SHIFT		4
 
 /* id_aa64isar1 */
+#define ID_AA64ISAR1_LRCPC_SHIFT	20
 #define ID_AA64ISAR1_FCMA_SHIFT		16
 #define ID_AA64ISAR1_JSCVT_SHIFT	12
 

commit cb567e79fa504575cb97fb2f866d2040ed1c92e7
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Mar 14 18:13:26 2017 +0000

    arm64: v8.3: Support for complex number instructions
    
    ARM v8.3 adds support for new instructions to aid floating-point
    multiplication and addition of complex numbers. Expose the support
    via HWCAP and MRS emulation
    
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index cbb314389a5b..69af995c9b76 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -157,6 +157,7 @@
 #define ID_AA64ISAR0_AES_SHIFT		4
 
 /* id_aa64isar1 */
+#define ID_AA64ISAR1_FCMA_SHIFT		16
 #define ID_AA64ISAR1_JSCVT_SHIFT	12
 
 /* id_aa64pfr0 */

commit c8c3798d2369e4285da44b244638eafe446a8f8a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Mar 14 18:13:25 2017 +0000

    arm64: v8.3: Support for Javascript conversion instruction
    
    ARMv8.3 adds support for a new instruction to perform conversion
    from double precision floating point to integer  to match the
    architected behaviour of the equivalent Javascript conversion.
    Expose the availability via HWCAP and MRS emulation.
    
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ac24b6e798b1..cbb314389a5b 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -156,6 +156,9 @@
 #define ID_AA64ISAR0_SHA1_SHIFT		8
 #define ID_AA64ISAR0_AES_SHIFT		4
 
+/* id_aa64isar1 */
+#define ID_AA64ISAR1_JSCVT_SHIFT	12
+
 /* id_aa64pfr0 */
 #define ID_AA64PFR0_GIC_SHIFT		24
 #define ID_AA64PFR0_ASIMD_SHIFT		20

commit 4dc529257d9dd6a2c930c794ed8077cdd6e843ce
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jan 13 17:47:46 2017 +0000

    arm64: sysreg: add Set/Way sys encodings
    
    Cache maintenance ops fall in the SYS instruction class, and KVM needs
    to handle them. So as to keep all SYS encodings in one place, this
    patch adds them to sysreg.h.
    
    The encodings were taken from ARM DDI 0487A.k_iss10775, Table C5-2.
    
    To make it clear that these are instructions rather than registers, and
    to allow us to change the way these are handled in future, a new
    sys_insn() alias for sys_reg() is added and used for these new
    definitions.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index f6233200c793..128eae8cc97e 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -48,6 +48,8 @@
 	 ((crn) << CRn_shift) | ((crm) << CRm_shift) | \
 	 ((op2) << Op2_shift))
 
+#define sys_insn	sys_reg
+
 #define sys_reg_Op0(id)	(((id) >> Op0_shift) & Op0_mask)
 #define sys_reg_Op1(id)	(((id) >> Op1_shift) & Op1_mask)
 #define sys_reg_CRn(id)	(((id) >> CRn_shift) & CRn_mask)
@@ -89,6 +91,10 @@
 #define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
 				      (!!x)<<8 | 0x1f)
 
+#define SYS_DC_ISW			sys_insn(1, 0, 7, 6, 2)
+#define SYS_DC_CSW			sys_insn(1, 0, 7, 10, 2)
+#define SYS_DC_CISW			sys_insn(1, 0, 7, 14, 2)
+
 #define SYS_OSDTRRX_EL1			sys_reg(2, 0, 0, 0, 2)
 #define SYS_MDCCINT_EL1			sys_reg(2, 0, 0, 2, 0)
 #define SYS_MDSCR_EL1			sys_reg(2, 0, 0, 2, 2)

commit 14ae7518dd5553ddff7750895ee86539ff56c407
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jan 13 18:36:51 2017 +0000

    arm64: sysreg: add register encodings used by KVM
    
    This patch adds sysreg definitions for registers which KVM needs the
    encodings for, which are not currently describe in <asm/sysregs.h>.
    Subsequent patches will make use of these definitions.
    
    The encodings were taken from ARM DDI 0487A.k_iss10775, Table C5-6, but
    this is not an exhaustive addition. Additions are only made for
    registers used today by KVM.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 3e281b12ec3b..f6233200c793 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -119,6 +119,7 @@
 #define SYS_ID_PFR0_EL1			sys_reg(3, 0, 0, 1, 0)
 #define SYS_ID_PFR1_EL1			sys_reg(3, 0, 0, 1, 1)
 #define SYS_ID_DFR0_EL1			sys_reg(3, 0, 0, 1, 2)
+#define SYS_ID_AFR0_EL1			sys_reg(3, 0, 0, 1, 3)
 #define SYS_ID_MMFR0_EL1		sys_reg(3, 0, 0, 1, 4)
 #define SYS_ID_MMFR1_EL1		sys_reg(3, 0, 0, 1, 5)
 #define SYS_ID_MMFR2_EL1		sys_reg(3, 0, 0, 1, 6)
@@ -149,11 +150,30 @@
 #define SYS_ID_AA64MMFR1_EL1		sys_reg(3, 0, 0, 7, 1)
 #define SYS_ID_AA64MMFR2_EL1		sys_reg(3, 0, 0, 7, 2)
 
+#define SYS_SCTLR_EL1			sys_reg(3, 0, 1, 0, 0)
+#define SYS_ACTLR_EL1			sys_reg(3, 0, 1, 0, 1)
+#define SYS_CPACR_EL1			sys_reg(3, 0, 1, 0, 2)
+
+#define SYS_TTBR0_EL1			sys_reg(3, 0, 2, 0, 0)
+#define SYS_TTBR1_EL1			sys_reg(3, 0, 2, 0, 1)
+#define SYS_TCR_EL1			sys_reg(3, 0, 2, 0, 2)
+
 #define SYS_ICC_PMR_EL1			sys_reg(3, 0, 4, 6, 0)
 
+#define SYS_AFSR0_EL1			sys_reg(3, 0, 5, 1, 0)
+#define SYS_AFSR1_EL1			sys_reg(3, 0, 5, 1, 1)
+#define SYS_ESR_EL1			sys_reg(3, 0, 5, 2, 0)
+#define SYS_FAR_EL1			sys_reg(3, 0, 6, 0, 0)
+#define SYS_PAR_EL1			sys_reg(3, 0, 7, 4, 0)
+
 #define SYS_PMINTENSET_EL1		sys_reg(3, 0, 9, 14, 1)
 #define SYS_PMINTENCLR_EL1		sys_reg(3, 0, 9, 14, 2)
 
+#define SYS_MAIR_EL1			sys_reg(3, 0, 10, 2, 0)
+#define SYS_AMAIR_EL1			sys_reg(3, 0, 10, 3, 0)
+
+#define SYS_VBAR_EL1			sys_reg(3, 0, 12, 0, 0)
+
 #define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
 #define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
 #define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)
@@ -163,6 +183,16 @@
 #define SYS_ICC_SRE_EL1			sys_reg(3, 0, 12, 12, 5)
 #define SYS_ICC_GRPEN1_EL1		sys_reg(3, 0, 12, 12, 7)
 
+#define SYS_CONTEXTIDR_EL1		sys_reg(3, 0, 13, 0, 1)
+#define SYS_TPIDR_EL1			sys_reg(3, 0, 13, 0, 4)
+
+#define SYS_CNTKCTL_EL1			sys_reg(3, 0, 14, 1, 0)
+
+#define SYS_CLIDR_EL1			sys_reg(3, 1, 0, 0, 1)
+#define SYS_AIDR_EL1			sys_reg(3, 1, 0, 0, 7)
+
+#define SYS_CSSELR_EL1			sys_reg(3, 2, 0, 0, 0)
+
 #define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
 #define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
 
@@ -180,6 +210,9 @@
 #define SYS_PMUSERENR_EL0		sys_reg(3, 3, 9, 14, 0)
 #define SYS_PMOVSSET_EL0		sys_reg(3, 3, 9, 14, 3)
 
+#define SYS_TPIDR_EL0			sys_reg(3, 3, 13, 0, 2)
+#define SYS_TPIDRRO_EL0			sys_reg(3, 3, 13, 0, 3)
+
 #define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 
 #define SYS_CNTP_TVAL_EL0		sys_reg(3, 3, 14, 2, 0)
@@ -194,6 +227,10 @@
 
 #define SYS_PMCCFILTR_EL0		sys_reg (3, 3, 14, 15, 7)
 
+#define SYS_DACR32_EL2			sys_reg(3, 4, 3, 0, 0)
+#define SYS_IFSR32_EL2			sys_reg(3, 4, 5, 0, 1)
+#define SYS_FPEXC32_EL2			sys_reg(3, 4, 5, 3, 0)
+
 #define __SYS__AP0Rx_EL2(x)		sys_reg(3, 4, 12, 8, x)
 #define SYS_ICH_AP0R0_EL2		__SYS__AP0Rx_EL2(0)
 #define SYS_ICH_AP0R1_EL2		__SYS__AP0Rx_EL2(1)

commit 147a70ceda533f4b4c8fab17efa97efe11018f09
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Mar 9 16:47:06 2017 +0000

    arm64: sysreg: add physical timer registers
    
    This patch adds sysreg definitions for system registers used to control
    the architected physical timer. Subsequent patches will make use of
    these definitions.
    
    The encodings were taken from ARM DDI 0487A.k_iss10775, Table C5-6.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 9dc30bc06aa1..3e281b12ec3b 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -182,6 +182,10 @@
 
 #define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 
+#define SYS_CNTP_TVAL_EL0		sys_reg(3, 3, 14, 2, 0)
+#define SYS_CNTP_CTL_EL0		sys_reg(3, 3, 14, 2, 1)
+#define SYS_CNTP_CVAL_EL0		sys_reg(3, 3, 14, 2, 2)
+
 #define __PMEV_op2(n)			((n) & 0x7)
 #define __CNTR_CRm(n)			(0x8 | (((n) >> 3) & 0x3))
 #define SYS_PMEVCNTRn_EL0(n)		sys_reg(3, 3, 14, __CNTR_CRm(n), __PMEV_op2(n))

commit 0e9884fe63c68d7c9043410fd3ee009c1fc78985
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 19 17:57:43 2017 +0000

    arm64: sysreg: subsume GICv3 sysreg definitions
    
    Unlike most sysreg defintiions, the GICv3 definitions don't have a SYS_
    prefix, and they don't live in <asm/sysreg.h>. Additionally, some
    definitions are duplicated elsewhere (e.g. in the KVM save/restore
    code).
    
    For consistency, and to make it possible to share a common definition
    for these sysregs, this patch moves the definitions to <asm/sysreg.h>,
    adding a SYS_ prefix, and sorting the registers per their encoding.
    Existing users of the definitions are fixed up so that this change is
    not problematic.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 3498d02b29d9..9dc30bc06aa1 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -149,9 +149,20 @@
 #define SYS_ID_AA64MMFR1_EL1		sys_reg(3, 0, 0, 7, 1)
 #define SYS_ID_AA64MMFR2_EL1		sys_reg(3, 0, 0, 7, 2)
 
+#define SYS_ICC_PMR_EL1			sys_reg(3, 0, 4, 6, 0)
+
 #define SYS_PMINTENSET_EL1		sys_reg(3, 0, 9, 14, 1)
 #define SYS_PMINTENCLR_EL1		sys_reg(3, 0, 9, 14, 2)
 
+#define SYS_ICC_DIR_EL1			sys_reg(3, 0, 12, 11, 1)
+#define SYS_ICC_SGI1R_EL1		sys_reg(3, 0, 12, 11, 5)
+#define SYS_ICC_IAR1_EL1		sys_reg(3, 0, 12, 12, 0)
+#define SYS_ICC_EOIR1_EL1		sys_reg(3, 0, 12, 12, 1)
+#define SYS_ICC_BPR1_EL1		sys_reg(3, 0, 12, 12, 3)
+#define SYS_ICC_CTLR_EL1		sys_reg(3, 0, 12, 12, 4)
+#define SYS_ICC_SRE_EL1			sys_reg(3, 0, 12, 12, 5)
+#define SYS_ICC_GRPEN1_EL1		sys_reg(3, 0, 12, 12, 7)
+
 #define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
 #define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
 
@@ -179,6 +190,47 @@
 
 #define SYS_PMCCFILTR_EL0		sys_reg (3, 3, 14, 15, 7)
 
+#define __SYS__AP0Rx_EL2(x)		sys_reg(3, 4, 12, 8, x)
+#define SYS_ICH_AP0R0_EL2		__SYS__AP0Rx_EL2(0)
+#define SYS_ICH_AP0R1_EL2		__SYS__AP0Rx_EL2(1)
+#define SYS_ICH_AP0R2_EL2		__SYS__AP0Rx_EL2(2)
+#define SYS_ICH_AP0R3_EL2		__SYS__AP0Rx_EL2(3)
+
+#define __SYS__AP1Rx_EL2(x)		sys_reg(3, 4, 12, 9, x)
+#define SYS_ICH_AP1R0_EL2		__SYS__AP1Rx_EL2(0)
+#define SYS_ICH_AP1R1_EL2		__SYS__AP1Rx_EL2(1)
+#define SYS_ICH_AP1R2_EL2		__SYS__AP1Rx_EL2(2)
+#define SYS_ICH_AP1R3_EL2		__SYS__AP1Rx_EL2(3)
+
+#define SYS_ICH_VSEIR_EL2		sys_reg(3, 4, 12, 9, 4)
+#define SYS_ICC_SRE_EL2			sys_reg(3, 4, 12, 9, 5)
+#define SYS_ICH_HCR_EL2			sys_reg(3, 4, 12, 11, 0)
+#define SYS_ICH_VTR_EL2			sys_reg(3, 4, 12, 11, 1)
+#define SYS_ICH_MISR_EL2		sys_reg(3, 4, 12, 11, 2)
+#define SYS_ICH_EISR_EL2		sys_reg(3, 4, 12, 11, 3)
+#define SYS_ICH_ELSR_EL2		sys_reg(3, 4, 12, 11, 5)
+#define SYS_ICH_VMCR_EL2		sys_reg(3, 4, 12, 11, 7)
+
+#define __SYS__LR0_EL2(x)		sys_reg(3, 4, 12, 12, x)
+#define SYS_ICH_LR0_EL2			__SYS__LR0_EL2(0)
+#define SYS_ICH_LR1_EL2			__SYS__LR0_EL2(1)
+#define SYS_ICH_LR2_EL2			__SYS__LR0_EL2(2)
+#define SYS_ICH_LR3_EL2			__SYS__LR0_EL2(3)
+#define SYS_ICH_LR4_EL2			__SYS__LR0_EL2(4)
+#define SYS_ICH_LR5_EL2			__SYS__LR0_EL2(5)
+#define SYS_ICH_LR6_EL2			__SYS__LR0_EL2(6)
+#define SYS_ICH_LR7_EL2			__SYS__LR0_EL2(7)
+
+#define __SYS__LR8_EL2(x)		sys_reg(3, 4, 12, 13, x)
+#define SYS_ICH_LR8_EL2			__SYS__LR8_EL2(0)
+#define SYS_ICH_LR9_EL2			__SYS__LR8_EL2(1)
+#define SYS_ICH_LR10_EL2		__SYS__LR8_EL2(2)
+#define SYS_ICH_LR11_EL2		__SYS__LR8_EL2(3)
+#define SYS_ICH_LR12_EL2		__SYS__LR8_EL2(4)
+#define SYS_ICH_LR13_EL2		__SYS__LR8_EL2(5)
+#define SYS_ICH_LR14_EL2		__SYS__LR8_EL2(6)
+#define SYS_ICH_LR15_EL2		__SYS__LR8_EL2(7)
+
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_EE    (1 << 25)
 #define SCTLR_ELx_I	(1 << 12)

commit c7a3c61fc60642b9b1462c6deb9fd531d30e333d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jan 20 16:25:51 2017 +0000

    arm64: sysreg: add performance monitor registers
    
    This patch adds sysreg definitions for system registers which are part
    of the performance monitors extension. Subsequent patches will make use
    of these definitions.
    
    The set of registers is described in ARM DDI 0487A.k_iss10775, Table
    D5-9. The encodings were taken from Table C5-6 in the same document.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index b54f8a401664..3498d02b29d9 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -149,11 +149,36 @@
 #define SYS_ID_AA64MMFR1_EL1		sys_reg(3, 0, 0, 7, 1)
 #define SYS_ID_AA64MMFR2_EL1		sys_reg(3, 0, 0, 7, 2)
 
+#define SYS_PMINTENSET_EL1		sys_reg(3, 0, 9, 14, 1)
+#define SYS_PMINTENCLR_EL1		sys_reg(3, 0, 9, 14, 2)
+
 #define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
 #define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
 
+#define SYS_PMCR_EL0			sys_reg(3, 3, 9, 12, 0)
+#define SYS_PMCNTENSET_EL0		sys_reg(3, 3, 9, 12, 1)
+#define SYS_PMCNTENCLR_EL0		sys_reg(3, 3, 9, 12, 2)
+#define SYS_PMOVSCLR_EL0		sys_reg(3, 3, 9, 12, 3)
+#define SYS_PMSWINC_EL0			sys_reg(3, 3, 9, 12, 4)
+#define SYS_PMSELR_EL0			sys_reg(3, 3, 9, 12, 5)
+#define SYS_PMCEID0_EL0			sys_reg(3, 3, 9, 12, 6)
+#define SYS_PMCEID1_EL0			sys_reg(3, 3, 9, 12, 7)
+#define SYS_PMCCNTR_EL0			sys_reg(3, 3, 9, 13, 0)
+#define SYS_PMXEVTYPER_EL0		sys_reg(3, 3, 9, 13, 1)
+#define SYS_PMXEVCNTR_EL0		sys_reg(3, 3, 9, 13, 2)
+#define SYS_PMUSERENR_EL0		sys_reg(3, 3, 9, 14, 0)
+#define SYS_PMOVSSET_EL0		sys_reg(3, 3, 9, 14, 3)
+
 #define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 
+#define __PMEV_op2(n)			((n) & 0x7)
+#define __CNTR_CRm(n)			(0x8 | (((n) >> 3) & 0x3))
+#define SYS_PMEVCNTRn_EL0(n)		sys_reg(3, 3, 14, __CNTR_CRm(n), __PMEV_op2(n))
+#define __TYPER_CRm(n)			(0xc | (((n) >> 3) & 0x3))
+#define SYS_PMEVTYPERn_EL0(n)		sys_reg(3, 3, 14, __TYPER_CRm(n), __PMEV_op2(n))
+
+#define SYS_PMCCFILTR_EL0		sys_reg (3, 3, 14, 15, 7)
+
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_EE    (1 << 25)
 #define SCTLR_ELx_I	(1 << 12)

commit d9801207cc633760f6f92cae5b64e3c0f65b7c64
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jan 13 16:55:01 2017 +0000

    arm64: sysreg: add debug system registers
    
    This patch adds sysreg definitions for system registers in the debug and
    trace system register encoding space. Subsequent patches will make use
    of these definitions.
    
    The encodings were taken from ARM DDI 0487A.k_iss10775, Table C5-5.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index e6498ac5bd8f..b54f8a401664 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -89,6 +89,29 @@
 #define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
 				      (!!x)<<8 | 0x1f)
 
+#define SYS_OSDTRRX_EL1			sys_reg(2, 0, 0, 0, 2)
+#define SYS_MDCCINT_EL1			sys_reg(2, 0, 0, 2, 0)
+#define SYS_MDSCR_EL1			sys_reg(2, 0, 0, 2, 2)
+#define SYS_OSDTRTX_EL1			sys_reg(2, 0, 0, 3, 2)
+#define SYS_OSECCR_EL1			sys_reg(2, 0, 0, 6, 2)
+#define SYS_DBGBVRn_EL1(n)		sys_reg(2, 0, 0, n, 4)
+#define SYS_DBGBCRn_EL1(n)		sys_reg(2, 0, 0, n, 5)
+#define SYS_DBGWVRn_EL1(n)		sys_reg(2, 0, 0, n, 6)
+#define SYS_DBGWCRn_EL1(n)		sys_reg(2, 0, 0, n, 7)
+#define SYS_MDRAR_EL1			sys_reg(2, 0, 1, 0, 0)
+#define SYS_OSLAR_EL1			sys_reg(2, 0, 1, 0, 4)
+#define SYS_OSLSR_EL1			sys_reg(2, 0, 1, 1, 4)
+#define SYS_OSDLR_EL1			sys_reg(2, 0, 1, 3, 4)
+#define SYS_DBGPRCR_EL1			sys_reg(2, 0, 1, 4, 4)
+#define SYS_DBGCLAIMSET_EL1		sys_reg(2, 0, 7, 8, 6)
+#define SYS_DBGCLAIMCLR_EL1		sys_reg(2, 0, 7, 9, 6)
+#define SYS_DBGAUTHSTATUS_EL1		sys_reg(2, 0, 7, 14, 6)
+#define SYS_MDCCSR_EL0			sys_reg(2, 3, 0, 1, 0)
+#define SYS_DBGDTR_EL0			sys_reg(2, 3, 0, 4, 0)
+#define SYS_DBGDTRRX_EL0		sys_reg(2, 3, 0, 5, 0)
+#define SYS_DBGDTRTX_EL0		sys_reg(2, 3, 0, 5, 0)
+#define SYS_DBGVCR32_EL2		sys_reg(2, 4, 0, 7, 0)
+
 #define SYS_MIDR_EL1			sys_reg(3, 0, 0, 0, 0)
 #define SYS_MPIDR_EL1			sys_reg(3, 0, 0, 0, 5)
 #define SYS_REVIDR_EL1			sys_reg(3, 0, 0, 0, 6)

commit 47863d41ecf84541ad1392515b68e327125c908c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 19 17:18:30 2017 +0000

    arm64: sysreg: sort by encoding
    
    Out sysreg definitions are largely (but not entirely) in ascending order
    of op0:op1:CRn:CRm:op2.
    
    It would be preferable to enforce this sort, as this makes it easier to
    verify the set of encodings against documentation, and provides an
    obvious location for each addition in future, minimising conflicts.
    
    This patch enforces this order, by moving the few items that break it.
    There should be no functional change.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index ac24b6e798b1..e6498ac5bd8f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -81,6 +81,14 @@
 
 #endif	/* CONFIG_BROKEN_GAS_INST */
 
+#define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
+#define REG_PSTATE_UAO_IMM		sys_reg(0, 0, 4, 0, 3)
+
+#define SET_PSTATE_PAN(x) __emit_inst(0xd5000000 | REG_PSTATE_PAN_IMM |	\
+				      (!!x)<<8 | 0x1f)
+#define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
+				      (!!x)<<8 | 0x1f)
+
 #define SYS_MIDR_EL1			sys_reg(3, 0, 0, 0, 0)
 #define SYS_MPIDR_EL1			sys_reg(3, 0, 0, 0, 5)
 #define SYS_REVIDR_EL1			sys_reg(3, 0, 0, 0, 6)
@@ -118,17 +126,10 @@
 #define SYS_ID_AA64MMFR1_EL1		sys_reg(3, 0, 0, 7, 1)
 #define SYS_ID_AA64MMFR2_EL1		sys_reg(3, 0, 0, 7, 2)
 
-#define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 #define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
 #define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
 
-#define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
-#define REG_PSTATE_UAO_IMM		sys_reg(0, 0, 4, 0, 3)
-
-#define SET_PSTATE_PAN(x) __emit_inst(0xd5000000 | REG_PSTATE_PAN_IMM |	\
-				      (!!x)<<8 | 0x1f)
-#define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
-				      (!!x)<<8 | 0x1f)
+#define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_EE    (1 << 25)

commit 77c97b4ee21290f5f083173d957843b615abbff2
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 9 17:28:31 2017 +0000

    arm64: cpufeature: Expose CPUID registers by emulation
    
    This patch adds the hook for emulating MRS instruction to
    export the 'user visible' value of supported system registers.
    We emulate only the following id space for system registers:
    
     Op0=3, Op1=0, CRn=0, CRm=[0, 4-7]
    
    The rest will fall back to SIGILL. This capability is also
    advertised via a new HWCAP_CPUID.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: add missing static keyword to enable_mrs_emulation]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 36ae882e2dcc..ac24b6e798b1 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -265,6 +265,10 @@
 #define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN64_SUPPORTED
 #endif
 
+
+/* Safe value for MPIDR_EL1: Bit31:RES1, Bit30:U:0, Bit24:MT:0 */
+#define SYS_MPIDR_SAFE_VAL		(1UL << 31)
+
 #ifdef __ASSEMBLY__
 
 	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30

commit c9ee0f98662a6e358b5f2969755c938ce9c3a63a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 9 17:28:28 2017 +0000

    arm64: cpufeature: Define helpers for sys_reg id
    
    Define helper macros to extract op0, op1, CRn, CRm & op2
    for a given sys_reg id. While at it remove the explicit
    masking only used for Op0.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index e156e7793a65..36ae882e2dcc 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -32,8 +32,27 @@
  *	[11-8]  : CRm
  *	[7-5]   : Op2
  */
+#define Op0_shift	19
+#define Op0_mask	0x3
+#define Op1_shift	16
+#define Op1_mask	0x7
+#define CRn_shift	12
+#define CRn_mask	0xf
+#define CRm_shift	8
+#define CRm_mask	0xf
+#define Op2_shift	5
+#define Op2_mask	0x7
+
 #define sys_reg(op0, op1, crn, crm, op2) \
-	((((op0)&3)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
+	(((op0) << Op0_shift) | ((op1) << Op1_shift) | \
+	 ((crn) << CRn_shift) | ((crm) << CRm_shift) | \
+	 ((op2) << Op2_shift))
+
+#define sys_reg_Op0(id)	(((id) >> Op0_shift) & Op0_mask)
+#define sys_reg_Op1(id)	(((id) >> Op1_shift) & Op1_mask)
+#define sys_reg_CRn(id)	(((id) >> CRn_shift) & CRn_mask)
+#define sys_reg_CRm(id)	(((id) >> CRm_shift) & CRm_mask)
+#define sys_reg_Op2(id)	(((id) >> Op2_shift) & Op2_mask)
 
 #ifndef CONFIG_BROKEN_GAS_INST
 

commit f31deaadff0d81a4963b5d816c1ae4a67296aa0c
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 22 11:23:07 2016 +0100

    arm64: cpufeature: Don't enforce system-wide SPE capability
    
    The statistical profiling extension (SPE) is an optional feature of
    ARMv8.1 and is unlikely to be supported by all of the CPUs in a
    heterogeneous system.
    
    This patch updates the cpufeature checks so that such systems are not
    tainted as unsupported.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 98ae03f8eedd..e156e7793a65 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -190,6 +190,7 @@
 #define ID_AA64MMFR2_CNP_SHIFT		0
 
 /* id_aa64dfr0 */
+#define ID_AA64DFR0_PMSVER_SHIFT	32
 #define ID_AA64DFR0_CTX_CMPS_SHIFT	28
 #define ID_AA64DFR0_WRPS_SHIFT		20
 #define ID_AA64DFR0_BRPS_SHIFT		12

commit cd9e1927a525f6ce7c0d99c6e038f0a0b9e85176
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Dec 6 15:27:45 2016 +0000

    arm64: Work around broken .inst when defective gas is detected
    
    .inst being largely broken with older binutils, it'd be better not
    to emit it altogether when detecting such configuration (as it
    leads to all kind of horrors when using alternatives).
    
    Generalize the __emit_inst macro and use it extensively in
    asm/sysreg.h, and make it generate a .long when a broken gas is
    detected. The disassembly will be crap, but at least we can write
    semi-sane code.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 9e16a185badb..98ae03f8eedd 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -35,12 +35,33 @@
 #define sys_reg(op0, op1, crn, crm, op2) \
 	((((op0)&3)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
 
+#ifndef CONFIG_BROKEN_GAS_INST
+
 #ifdef __ASSEMBLY__
 #define __emit_inst(x)			.inst (x)
 #else
 #define __emit_inst(x)			".inst " __stringify((x)) "\n\t"
 #endif
 
+#else  /* CONFIG_BROKEN_GAS_INST */
+
+#ifndef CONFIG_CPU_BIG_ENDIAN
+#define __INSTR_BSWAP(x)		(x)
+#else  /* CONFIG_CPU_BIG_ENDIAN */
+#define __INSTR_BSWAP(x)		((((x) << 24) & 0xff000000)	| \
+					 (((x) <<  8) & 0x00ff0000)	| \
+					 (((x) >>  8) & 0x0000ff00)	| \
+					 (((x) >> 24) & 0x000000ff))
+#endif	/* CONFIG_CPU_BIG_ENDIAN */
+
+#ifdef __ASSEMBLY__
+#define __emit_inst(x)			.long __INSTR_BSWAP(x)
+#else  /* __ASSEMBLY__ */
+#define __emit_inst(x)			".long " __stringify(__INSTR_BSWAP(x)) "\n\t"
+#endif	/* __ASSEMBLY__ */
+
+#endif	/* CONFIG_BROKEN_GAS_INST */
+
 #define SYS_MIDR_EL1			sys_reg(3, 0, 0, 0, 0)
 #define SYS_MPIDR_EL1			sys_reg(3, 0, 0, 0, 5)
 #define SYS_REVIDR_EL1			sys_reg(3, 0, 0, 0, 6)
@@ -232,11 +253,11 @@
 	.equ	.L__reg_num_xzr, 31
 
 	.macro	mrs_s, rt, sreg
-	.inst	0xd5200000|(\sreg)|(.L__reg_num_\rt)
+	 __emit_inst(0xd5200000|(\sreg)|(.L__reg_num_\rt))
 	.endm
 
 	.macro	msr_s, sreg, rt
-	.inst	0xd5000000|(\sreg)|(.L__reg_num_\rt)
+	__emit_inst(0xd5000000|(\sreg)|(.L__reg_num_\rt))
 	.endm
 
 #else
@@ -250,11 +271,11 @@ asm(
 "	.equ	.L__reg_num_xzr, 31\n"
 "\n"
 "	.macro	mrs_s, rt, sreg\n"
-"	.inst	0xd5200000|(\\sreg)|(.L__reg_num_\\rt)\n"
+	__emit_inst(0xd5200000|(\\sreg)|(.L__reg_num_\\rt))
 "	.endm\n"
 "\n"
 "	.macro	msr_s, sreg, rt\n"
-"	.inst	0xd5000000|(\\sreg)|(.L__reg_num_\\rt)\n"
+	__emit_inst(0xd5000000|(\\sreg)|(.L__reg_num_\\rt))
 "	.endm\n"
 );
 

commit bca8f17f57bd76ddf2bbd2527eb890d6f588853e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 1 10:44:33 2016 +0000

    arm64: Get rid of asm/opcodes.h
    
    The opcodes.h drags in a lot of definition from the 32bit port, most
    of which is not required at all. Clean things up a bit by moving
    the bare minimum of what is required next to the actual users,
    and drop the include file.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 6c80b3699cb8..9e16a185badb 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -22,8 +22,6 @@
 
 #include <linux/stringify.h>
 
-#include <asm/opcodes.h>
-
 /*
  * ARMv8 ARM reserves the following encoding for system registers:
  * (Ref: ARMv8 ARM, Section: "System instruction class encoding overview",
@@ -37,6 +35,12 @@
 #define sys_reg(op0, op1, crn, crm, op2) \
 	((((op0)&3)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
 
+#ifdef __ASSEMBLY__
+#define __emit_inst(x)			.inst (x)
+#else
+#define __emit_inst(x)			".inst " __stringify((x)) "\n\t"
+#endif
+
 #define SYS_MIDR_EL1			sys_reg(3, 0, 0, 0, 0)
 #define SYS_MPIDR_EL1			sys_reg(3, 0, 0, 0, 5)
 #define SYS_REVIDR_EL1			sys_reg(3, 0, 0, 0, 6)
@@ -81,10 +85,10 @@
 #define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
 #define REG_PSTATE_UAO_IMM		sys_reg(0, 0, 4, 0, 3)
 
-#define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
-				     (!!x)<<8 | 0x1f)
-#define SET_PSTATE_UAO(x) __inst_arm(0xd5000000 | REG_PSTATE_UAO_IMM |\
-				     (!!x)<<8 | 0x1f)
+#define SET_PSTATE_PAN(x) __emit_inst(0xd5000000 | REG_PSTATE_PAN_IMM |	\
+				      (!!x)<<8 | 0x1f)
+#define SET_PSTATE_UAO(x) __emit_inst(0xd5000000 | REG_PSTATE_UAO_IMM |	\
+				      (!!x)<<8 | 0x1f)
 
 /* Common SCTLR_ELx flags. */
 #define SCTLR_ELx_EE    (1 << 25)

commit 91cb163e4d141c74e99639fbee7c2a6332c92901
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Oct 17 13:38:14 2016 +0100

    arm64: sysreg: Fix use of XZR in write_sysreg_s
    
    Commit 8a71f0c656e0 ("arm64: sysreg: replace open-coded mrs_s/msr_s with
    {read,write}_sysreg_s") introduced a write_sysreg_s macro for writing
    to system registers that are not supported by binutils.
    
    Unfortunately, this was implemented with the wrong template (%0 vs %x0),
    so in the case that we are writing a constant 0, we will generate
    invalid instruction syntax and bail with a cryptic assembler error:
    
      | Error: constant expression required
    
    This patch fixes the template.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index e8d46e8e6079..6c80b3699cb8 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -286,7 +286,7 @@ asm(
 
 #define write_sysreg_s(v, r) do {					\
 	u64 __val = (u64)v;						\
-	asm volatile("msr_s " __stringify(r) ", %0" : : "rZ" (__val));	\
+	asm volatile("msr_s " __stringify(r) ", %x0" : : "rZ" (__val));	\
 } while (0)
 
 static inline void config_sctlr_el1(u32 clear, u32 set)

commit 8a71f0c656e0521867931eecff54eb3a35ca65a7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 6 14:04:45 2016 +0100

    arm64: sysreg: replace open-coded mrs_s/msr_s with {read,write}_sysreg_s
    
    Similar to our {read,write}_sysreg accessors for architected, named
    system registers, this patch introduces {read,write}_sysreg_s variants
    that can take arbitrary sys_reg output and therefore access IMPDEF
    registers or registers that unsupported by binutils.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 7e4ecd1d2ac9..e8d46e8e6079 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -274,6 +274,21 @@ asm(
 		     : : "rZ" (__val));				\
 } while (0)
 
+/*
+ * For registers without architectural names, or simply unsupported by
+ * GAS.
+ */
+#define read_sysreg_s(r) ({						\
+	u64 __val;							\
+	asm volatile("mrs_s %0, " __stringify(r) : "=r" (__val));	\
+	__val;								\
+})
+
+#define write_sysreg_s(v, r) do {					\
+	u64 __val = (u64)v;						\
+	asm volatile("msr_s " __stringify(r) ", %0" : : "rZ" (__val));	\
+} while (0)
+
 static inline void config_sctlr_el1(u32 clear, u32 set)
 {
 	u32 val;

commit 116c81f427ff6c5380850963e3fb8798cc821d2b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:16 2016 +0100

    arm64: Work around systems with mismatched cache line sizes
    
    Systems with differing CPU i-cache/d-cache line sizes can cause
    problems with the cache management by software when the execution
    is migrated from one to another. Usually, the application reads
    the cache size on a CPU and then uses that length to perform cache
    operations. However, if it gets migrated to another CPU with a smaller
    cache line size, things could go completely wrong. To prevent such
    cases, always use the smallest cache line size among the CPUs. The
    kernel CPU feature infrastructure already keeps track of the safe
    value for all CPUID registers including CTR. This patch works around
    the problem by :
    
    For kernel, dynamically patch the kernel to read the cache size
    from the system wide copy of CTR_EL0.
    
    For applications, trap read accesses to CTR_EL0 (by clearing the SCTLR.UCT)
    and emulate the mrs instruction to return the system wide safe value
    of CTR_EL0.
    
    For faster access (i.e, avoiding to lookup the system wide value of CTR_EL0
    via read_system_reg), we keep track of the pointer to table entry for
    CTR_EL0 in the CPU feature infrastructure.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index e91aef2bb33d..7e4ecd1d2ac9 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -100,6 +100,7 @@
 /* SCTLR_EL1 specific flags. */
 #define SCTLR_EL1_UCI		(1 << 26)
 #define SCTLR_EL1_SPAN		(1 << 23)
+#define SCTLR_EL1_UCT		(1 << 15)
 #define SCTLR_EL1_SED		(1 << 8)
 #define SCTLR_EL1_CP15BEN	(1 << 5)
 

commit adf7589997927b1d84a5d003027b866bbef61ef2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Sep 8 13:55:38 2016 +0100

    arm64: simplify sysreg manipulation
    
    A while back we added {read,write}_sysreg accessors to handle accesses
    to system registers, without the usual boilerplate asm volatile,
    temporary variable, etc.
    
    This patch makes use of these across arm64 to make code shorter and
    clearer. For sequences with a trailing ISB, the existing isb() macro is
    also used so that asm blocks can be removed entirely.
    
    A few uses of inline assembly for msr/mrs are left as-is. Those
    manipulating sp_el0 for the current thread_info value have special
    clobber requiremends.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 39fed2e56e98..e91aef2bb33d 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -253,16 +253,6 @@ asm(
 "	.endm\n"
 );
 
-static inline void config_sctlr_el1(u32 clear, u32 set)
-{
-	u32 val;
-
-	asm volatile("mrs %0, sctlr_el1" : "=r" (val));
-	val &= ~clear;
-	val |= set;
-	asm volatile("msr sctlr_el1, %0" : : "r" (val));
-}
-
 /*
  * Unlike read_cpuid, calls to read_sysreg are never expected to be
  * optimized away or replaced with synthetic values.
@@ -283,6 +273,16 @@ static inline void config_sctlr_el1(u32 clear, u32 set)
 		     : : "rZ" (__val));				\
 } while (0)
 
+static inline void config_sctlr_el1(u32 clear, u32 set)
+{
+	u32 val;
+
+	val = read_sysreg(sctlr_el1);
+	val &= ~clear;
+	val |= set;
+	write_sysreg(val, sctlr_el1);
+}
+
 #endif
 
 #endif	/* __ASM_SYSREG_H */

commit 7aff4a2dd3db4e519c636f43aa863078e3b30d0f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Sep 8 13:55:34 2016 +0100

    arm64: sysreg: allow write_sysreg to use XZR
    
    Currently write_sysreg has to allocate a temporary register to write
    zero to a system register, which is unfortunate given that the MSR
    instruction accepts XZR as an operand.
    
    Allow XZR to be used when appropriate by fiddling with the assembly
    constraints.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index cc06794b7346..39fed2e56e98 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -273,10 +273,14 @@ static inline void config_sctlr_el1(u32 clear, u32 set)
 	__val;							\
 })
 
+/*
+ * The "Z" constraint normally means a zero immediate, but when combined with
+ * the "%x0" template means XZR.
+ */
 #define write_sysreg(v, r) do {					\
 	u64 __val = (u64)v;					\
-	asm volatile("msr " __stringify(r) ", %0"		\
-		     : : "r" (__val));				\
+	asm volatile("msr " __stringify(r) ", %x0"		\
+		     : : "rZ" (__val));				\
 } while (0)
 
 #endif

commit 7dd01aef055792260287c6708daf75aac3918f66
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:32 2016 +0100

    arm64: trap userspace "dc cvau" cache operation on errata-affected core
    
    The ARM errata 819472, 826319, 827319 and 824069 for affected
    Cortex-A53 cores demand to promote "dc cvau" instructions to
    "dc civac". Since we allow userspace to also emit those instructions,
    we should make sure that "dc cvau" gets promoted there too.
    So lets grasp the nettle here and actually trap every userland cache
    maintenance instruction once we detect at least one affected core in
    the system.
    We then emulate the instruction by executing it on behalf of userland,
    promoting "dc cvau" to "dc civac" on the way and injecting access
    fault back into userspace.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [catalin.marinas@arm.com: s/set_segfault/arm64_notify_segfault/]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 751e901c8d37..cc06794b7346 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -98,11 +98,11 @@
 			 SCTLR_ELx_SA | SCTLR_ELx_I)
 
 /* SCTLR_EL1 specific flags. */
+#define SCTLR_EL1_UCI		(1 << 26)
 #define SCTLR_EL1_SPAN		(1 << 23)
 #define SCTLR_EL1_SED		(1 << 8)
 #define SCTLR_EL1_CP15BEN	(1 << 5)
 
-
 /* id_aa64isar0 */
 #define ID_AA64ISAR0_RDM_SHIFT		28
 #define ID_AA64ISAR0_ATOMICS_SHIFT	20

commit e7227d0e528f9a96d4a866f43e20dd9b33f0e782
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Apr 27 17:47:01 2016 +0100

    arm64: Cleanup SCTLR flags
    
    We currently have macros defining flags for the arm64 sctlr registers in
    both kvm_arm.h and sysreg.h.  To clean things up and simplify move the
    definitions of the SCTLR_EL2 flags from kvm_arm.h to sysreg.h, rename any
    SCTLR_EL1 or SCTLR_EL2 flags that are common to both registers to be
    SCTLR_ELx, with 'x' indicating a common flag, and fixup all files to
    include the proper header or to use the new macro names.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    [Restored pgtable-hwdef.h include]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index d39d4343318a..751e901c8d37 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -86,10 +86,21 @@
 #define SET_PSTATE_UAO(x) __inst_arm(0xd5000000 | REG_PSTATE_UAO_IMM |\
 				     (!!x)<<8 | 0x1f)
 
-/* SCTLR_EL1 */
-#define SCTLR_EL1_CP15BEN	(0x1 << 5)
-#define SCTLR_EL1_SED		(0x1 << 8)
-#define SCTLR_EL1_SPAN		(0x1 << 23)
+/* Common SCTLR_ELx flags. */
+#define SCTLR_ELx_EE    (1 << 25)
+#define SCTLR_ELx_I	(1 << 12)
+#define SCTLR_ELx_SA	(1 << 3)
+#define SCTLR_ELx_C	(1 << 2)
+#define SCTLR_ELx_A	(1 << 1)
+#define SCTLR_ELx_M	1
+
+#define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
+			 SCTLR_ELx_SA | SCTLR_ELx_I)
+
+/* SCTLR_EL1 specific flags. */
+#define SCTLR_EL1_SPAN		(1 << 23)
+#define SCTLR_EL1_SED		(1 << 8)
+#define SCTLR_EL1_CP15BEN	(1 << 5)
 
 
 /* id_aa64isar0 */

commit c80aba803a9aa131f997f62a71ab453e456d08a8
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Apr 18 10:28:34 2016 +0100

    arm64: Add helpers for detecting AArch32 support at EL0
    
    Adds a helper to extract the support for AArch32 at EL0
    
    Tested-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 19182ef18f8f..d39d4343318a 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -115,6 +115,7 @@
 #define ID_AA64PFR0_ASIMD_SUPPORTED	0x0
 #define ID_AA64PFR0_EL1_64BIT_ONLY	0x1
 #define ID_AA64PFR0_EL0_64BIT_ONLY	0x1
+#define ID_AA64PFR0_EL0_32BIT_64BIT	0x2
 
 /* id_aa64mmfr0 */
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28

commit 7d7b4ae418728916456c6fd03a97ef35345ff30d
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Fri Mar 25 17:30:07 2016 +0800

    arm64: cpufeature: append additional id_aa64mmfr2 fields to cpufeature
    
    There are some new cpu features which can be identified by id_aa64mmfr2,
    this patch appends all fields of it.
    
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 12874164b0ae..19182ef18f8f 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -145,7 +145,11 @@
 #define ID_AA64MMFR1_VMIDBITS_16	2
 
 /* id_aa64mmfr2 */
+#define ID_AA64MMFR2_LVA_SHIFT		16
+#define ID_AA64MMFR2_IESB_SHIFT		12
+#define ID_AA64MMFR2_LSM_SHIFT		8
 #define ID_AA64MMFR2_UAO_SHIFT		4
+#define ID_AA64MMFR2_CNP_SHIFT		0
 
 /* id_aa64dfr0 */
 #define ID_AA64DFR0_CTX_CMPS_SHIFT	28

commit cb678d6016510cc8c6572dd9f426fe74dd4eda84
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Mar 30 14:33:59 2016 +0100

    arm64: kvm: 4.6-rc1: Fix VTCR_EL2 VS setting
    
    When we detect support for 16bit VMID in ID_AA64MMFR1, we set the
    VTCR_EL2_VS field to 1 to make use of 16bit vmids. But, with
    commit 3a3604bc5eb4 ("arm64: KVM: Switch to C-based stage2 init")
    this is broken and we corrupt VTCR_EL2:T0SZ instead of updating the VS
    field. VTCR_EL2_VS was actually defined to the field shift (19) and
    not the real value for VS. This patch fixes the issue.
    
    Fixes: commit 3a3604bc5eb4 ("arm64: KVM: Switch to C-based stage2 init")
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 1a78d6e2a78b..12874164b0ae 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -141,6 +141,9 @@
 #define ID_AA64MMFR1_VMIDBITS_SHIFT	4
 #define ID_AA64MMFR1_HADBS_SHIFT	0
 
+#define ID_AA64MMFR1_VMIDBITS_8		0
+#define ID_AA64MMFR1_VMIDBITS_16	2
+
 /* id_aa64mmfr2 */
 #define ID_AA64MMFR2_UAO_SHIFT		4
 

commit 57f4959bad0a154aeca125b7d38d1d9471a12422
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:48 2016 +0000

    arm64: kernel: Add support for User Access Override
    
    'User Access Override' is a new ARMv8.2 feature which allows the
    unprivileged load and store instructions to be overridden to behave in
    the normal way.
    
    This patch converts {get,put}_user() and friends to use ldtr*/sttr*
    instructions - so that they can only access EL0 memory, then enables
    UAO when fs==KERNEL_DS so that these functions can access kernel memory.
    
    This allows user space's read/write permissions to be checked against the
    page tables, instead of testing addr<USER_DS, then using the kernel's
    read/write permissions.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: move uao_thread_switch() above dsb()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index e9c68e636576..1a78d6e2a78b 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -79,9 +79,12 @@
 #define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
 
 #define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
+#define REG_PSTATE_UAO_IMM		sys_reg(0, 0, 4, 0, 3)
 
 #define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
 				     (!!x)<<8 | 0x1f)
+#define SET_PSTATE_UAO(x) __inst_arm(0xd5000000 | REG_PSTATE_UAO_IMM |\
+				     (!!x)<<8 | 0x1f)
 
 /* SCTLR_EL1 */
 #define SCTLR_EL1_CP15BEN	(0x1 << 5)

commit 406e308770a92bd33995b2e5b681e86358328bb0
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:47 2016 +0000

    arm64: add ARMv8.2 id_aa64mmfr2 boiler plate
    
    ARMv8.2 adds a new feature register id_aa64mmfr2. This patch adds the
    cpu feature boiler plate used by the actual features in later patches.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 7d771ac01289..e9c68e636576 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -72,6 +72,7 @@
 
 #define SYS_ID_AA64MMFR0_EL1		sys_reg(3, 0, 0, 7, 0)
 #define SYS_ID_AA64MMFR1_EL1		sys_reg(3, 0, 0, 7, 1)
+#define SYS_ID_AA64MMFR2_EL1		sys_reg(3, 0, 0, 7, 2)
 
 #define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
 #define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
@@ -137,6 +138,9 @@
 #define ID_AA64MMFR1_VMIDBITS_SHIFT	4
 #define ID_AA64MMFR1_HADBS_SHIFT	0
 
+/* id_aa64mmfr2 */
+#define ID_AA64MMFR2_UAO_SHIFT		4
+
 /* id_aa64dfr0 */
 #define ID_AA64DFR0_CTX_CMPS_SHIFT	28
 #define ID_AA64DFR0_WRPS_SHIFT		20

commit 7abc7d833c9eb16efc8a59239d3771a6e30be367
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Feb 15 09:51:49 2016 +0100

    arm64: use local label prefixes for __reg_num symbols
    
    The __reg_num_xNN symbols that are used to implement the msr_s and
    mrs_s macros are recorded in the ELF metadata of each object file.
    This does not affect the size of the final binary, but it does clutter
    the output of tools like readelf, i.e.,
    
      $ readelf -a vmlinux |grep -c __reg_num_x
      50976
    
    So let's use symbols with the .L prefix, these are strictly local,
    and don't end up in the object files.
    
      $ readelf -a vmlinux |grep -c __reg_num_x
      0
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 4aeebec3d882..7d771ac01289 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -196,16 +196,16 @@
 #ifdef __ASSEMBLY__
 
 	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
-	.equ	__reg_num_x\num, \num
+	.equ	.L__reg_num_x\num, \num
 	.endr
-	.equ	__reg_num_xzr, 31
+	.equ	.L__reg_num_xzr, 31
 
 	.macro	mrs_s, rt, sreg
-	.inst	0xd5200000|(\sreg)|(__reg_num_\rt)
+	.inst	0xd5200000|(\sreg)|(.L__reg_num_\rt)
 	.endm
 
 	.macro	msr_s, sreg, rt
-	.inst	0xd5000000|(\sreg)|(__reg_num_\rt)
+	.inst	0xd5000000|(\sreg)|(.L__reg_num_\rt)
 	.endm
 
 #else
@@ -214,16 +214,16 @@
 
 asm(
 "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n"
-"	.equ	__reg_num_x\\num, \\num\n"
+"	.equ	.L__reg_num_x\\num, \\num\n"
 "	.endr\n"
-"	.equ	__reg_num_xzr, 31\n"
+"	.equ	.L__reg_num_xzr, 31\n"
 "\n"
 "	.macro	mrs_s, rt, sreg\n"
-"	.inst	0xd5200000|(\\sreg)|(__reg_num_\\rt)\n"
+"	.inst	0xd5200000|(\\sreg)|(.L__reg_num_\\rt)\n"
 "	.endm\n"
 "\n"
 "	.macro	msr_s, sreg, rt\n"
-"	.inst	0xd5000000|(\\sreg)|(__reg_num_\\rt)\n"
+"	.inst	0xd5000000|(\\sreg)|(.L__reg_num_\\rt)\n"
 "	.endm\n"
 );
 

commit 3600c2fdc09a43a30909743569e35a29121602ed
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 5 15:09:17 2015 +0000

    arm64: Add macros to read/write system registers
    
    Rather than crafting custom macros for reading/writing each system
    register provide generics accessors, read_sysreg and write_sysreg, for
    this purpose.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index d48ab5b41f52..4aeebec3d882 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -20,6 +20,8 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
+#include <linux/stringify.h>
+
 #include <asm/opcodes.h>
 
 /*
@@ -208,6 +210,8 @@
 
 #else
 
+#include <linux/types.h>
+
 asm(
 "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n"
 "	.equ	__reg_num_x\\num, \\num\n"
@@ -232,6 +236,23 @@ static inline void config_sctlr_el1(u32 clear, u32 set)
 	val |= set;
 	asm volatile("msr sctlr_el1, %0" : : "r" (val));
 }
+
+/*
+ * Unlike read_cpuid, calls to read_sysreg are never expected to be
+ * optimized away or replaced with synthetic values.
+ */
+#define read_sysreg(r) ({					\
+	u64 __val;						\
+	asm volatile("mrs %0, " __stringify(r) : "=r" (__val));	\
+	__val;							\
+})
+
+#define write_sysreg(v, r) do {					\
+	u64 __val = (u64)v;					\
+	asm volatile("msr " __stringify(r) ", %0"		\
+		     : : "r" (__val));				\
+} while (0)
+
 #endif
 
 #endif	/* __ASM_SYSREG_H */

commit 3c739b57108436211c7f798ba3de0bb0cd8ef469
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:45 2015 +0100

    arm64: Keep track of CPU feature registers
    
    This patch adds an infrastructure to keep track of the CPU feature
    registers on the system. For each register, the infrastructure keeps
    track of the system wide safe value of the feature bits. Also, tracks
    the which fields of a register should be matched strictly across all
    the CPUs on the system for the SANITY check infrastructure.
    
    The feature bits are classified into following 3 types depending on
    the implication of the possible values. This information is used to
    decide the safe value for a feature.
    
    LOWER_SAFE  - The smaller value is safer
    HIGHER_SAFE - The bigger value is safer
    EXACT       - We can't decide between the two, so a predefined safe_value is used.
    
    This infrastructure will be later used to make better decisions for:
    
     - Kernel features (e.g, KVM, Debug)
     - SANITY Check
     - CPU capability
     - ELF HWCAP
     - Exposing CPU Feature register to userspace.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    [catalin.marinas@arm.com: whitespace fix]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 061a2792179a..d48ab5b41f52 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -22,9 +22,6 @@
 
 #include <asm/opcodes.h>
 
-#define SCTLR_EL1_CP15BEN	(0x1 << 5)
-#define SCTLR_EL1_SED		(0x1 << 8)
-
 /*
  * ARMv8 ARM reserves the following encoding for system registers:
  * (Ref: ARMv8 ARM, Section: "System instruction class encoding overview",
@@ -38,14 +35,139 @@
 #define sys_reg(op0, op1, crn, crm, op2) \
 	((((op0)&3)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
 
-#define REG_PSTATE_PAN_IMM                     sys_reg(0, 0, 4, 0, 4)
-#define SCTLR_EL1_SPAN                         (1 << 23)
+#define SYS_MIDR_EL1			sys_reg(3, 0, 0, 0, 0)
+#define SYS_MPIDR_EL1			sys_reg(3, 0, 0, 0, 5)
+#define SYS_REVIDR_EL1			sys_reg(3, 0, 0, 0, 6)
+
+#define SYS_ID_PFR0_EL1			sys_reg(3, 0, 0, 1, 0)
+#define SYS_ID_PFR1_EL1			sys_reg(3, 0, 0, 1, 1)
+#define SYS_ID_DFR0_EL1			sys_reg(3, 0, 0, 1, 2)
+#define SYS_ID_MMFR0_EL1		sys_reg(3, 0, 0, 1, 4)
+#define SYS_ID_MMFR1_EL1		sys_reg(3, 0, 0, 1, 5)
+#define SYS_ID_MMFR2_EL1		sys_reg(3, 0, 0, 1, 6)
+#define SYS_ID_MMFR3_EL1		sys_reg(3, 0, 0, 1, 7)
+
+#define SYS_ID_ISAR0_EL1		sys_reg(3, 0, 0, 2, 0)
+#define SYS_ID_ISAR1_EL1		sys_reg(3, 0, 0, 2, 1)
+#define SYS_ID_ISAR2_EL1		sys_reg(3, 0, 0, 2, 2)
+#define SYS_ID_ISAR3_EL1		sys_reg(3, 0, 0, 2, 3)
+#define SYS_ID_ISAR4_EL1		sys_reg(3, 0, 0, 2, 4)
+#define SYS_ID_ISAR5_EL1		sys_reg(3, 0, 0, 2, 5)
+#define SYS_ID_MMFR4_EL1		sys_reg(3, 0, 0, 2, 6)
+
+#define SYS_MVFR0_EL1			sys_reg(3, 0, 0, 3, 0)
+#define SYS_MVFR1_EL1			sys_reg(3, 0, 0, 3, 1)
+#define SYS_MVFR2_EL1			sys_reg(3, 0, 0, 3, 2)
+
+#define SYS_ID_AA64PFR0_EL1		sys_reg(3, 0, 0, 4, 0)
+#define SYS_ID_AA64PFR1_EL1		sys_reg(3, 0, 0, 4, 1)
+
+#define SYS_ID_AA64DFR0_EL1		sys_reg(3, 0, 0, 5, 0)
+#define SYS_ID_AA64DFR1_EL1		sys_reg(3, 0, 0, 5, 1)
+
+#define SYS_ID_AA64ISAR0_EL1		sys_reg(3, 0, 0, 6, 0)
+#define SYS_ID_AA64ISAR1_EL1		sys_reg(3, 0, 0, 6, 1)
+
+#define SYS_ID_AA64MMFR0_EL1		sys_reg(3, 0, 0, 7, 0)
+#define SYS_ID_AA64MMFR1_EL1		sys_reg(3, 0, 0, 7, 1)
+
+#define SYS_CNTFRQ_EL0			sys_reg(3, 3, 14, 0, 0)
+#define SYS_CTR_EL0			sys_reg(3, 3, 0, 0, 1)
+#define SYS_DCZID_EL0			sys_reg(3, 3, 0, 0, 7)
+
+#define REG_PSTATE_PAN_IMM		sys_reg(0, 0, 4, 0, 4)
 
 #define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
 				     (!!x)<<8 | 0x1f)
 
+/* SCTLR_EL1 */
+#define SCTLR_EL1_CP15BEN	(0x1 << 5)
+#define SCTLR_EL1_SED		(0x1 << 8)
+#define SCTLR_EL1_SPAN		(0x1 << 23)
+
+
+/* id_aa64isar0 */
+#define ID_AA64ISAR0_RDM_SHIFT		28
+#define ID_AA64ISAR0_ATOMICS_SHIFT	20
+#define ID_AA64ISAR0_CRC32_SHIFT	16
+#define ID_AA64ISAR0_SHA2_SHIFT		12
+#define ID_AA64ISAR0_SHA1_SHIFT		8
+#define ID_AA64ISAR0_AES_SHIFT		4
+
+/* id_aa64pfr0 */
+#define ID_AA64PFR0_GIC_SHIFT		24
+#define ID_AA64PFR0_ASIMD_SHIFT		20
+#define ID_AA64PFR0_FP_SHIFT		16
+#define ID_AA64PFR0_EL3_SHIFT		12
+#define ID_AA64PFR0_EL2_SHIFT		8
+#define ID_AA64PFR0_EL1_SHIFT		4
+#define ID_AA64PFR0_EL0_SHIFT		0
+
+#define ID_AA64PFR0_FP_NI		0xf
+#define ID_AA64PFR0_FP_SUPPORTED	0x0
+#define ID_AA64PFR0_ASIMD_NI		0xf
+#define ID_AA64PFR0_ASIMD_SUPPORTED	0x0
+#define ID_AA64PFR0_EL1_64BIT_ONLY	0x1
+#define ID_AA64PFR0_EL0_64BIT_ONLY	0x1
+
+/* id_aa64mmfr0 */
+#define ID_AA64MMFR0_TGRAN4_SHIFT	28
+#define ID_AA64MMFR0_TGRAN64_SHIFT	24
+#define ID_AA64MMFR0_TGRAN16_SHIFT	20
 #define ID_AA64MMFR0_BIGENDEL0_SHIFT	16
+#define ID_AA64MMFR0_SNSMEM_SHIFT	12
 #define ID_AA64MMFR0_BIGENDEL_SHIFT	8
+#define ID_AA64MMFR0_ASID_SHIFT		4
+#define ID_AA64MMFR0_PARANGE_SHIFT	0
+
+#define ID_AA64MMFR0_TGRAN4_NI		0xf
+#define ID_AA64MMFR0_TGRAN4_SUPPORTED	0x0
+#define ID_AA64MMFR0_TGRAN64_NI		0xf
+#define ID_AA64MMFR0_TGRAN64_SUPPORTED	0x0
+#define ID_AA64MMFR0_TGRAN16_NI		0x0
+#define ID_AA64MMFR0_TGRAN16_SUPPORTED	0x1
+
+/* id_aa64mmfr1 */
+#define ID_AA64MMFR1_PAN_SHIFT		20
+#define ID_AA64MMFR1_LOR_SHIFT		16
+#define ID_AA64MMFR1_HPD_SHIFT		12
+#define ID_AA64MMFR1_VHE_SHIFT		8
+#define ID_AA64MMFR1_VMIDBITS_SHIFT	4
+#define ID_AA64MMFR1_HADBS_SHIFT	0
+
+/* id_aa64dfr0 */
+#define ID_AA64DFR0_CTX_CMPS_SHIFT	28
+#define ID_AA64DFR0_WRPS_SHIFT		20
+#define ID_AA64DFR0_BRPS_SHIFT		12
+#define ID_AA64DFR0_PMUVER_SHIFT	8
+#define ID_AA64DFR0_TRACEVER_SHIFT	4
+#define ID_AA64DFR0_DEBUGVER_SHIFT	0
+
+#define ID_ISAR5_RDM_SHIFT		24
+#define ID_ISAR5_CRC32_SHIFT		16
+#define ID_ISAR5_SHA2_SHIFT		12
+#define ID_ISAR5_SHA1_SHIFT		8
+#define ID_ISAR5_AES_SHIFT		4
+#define ID_ISAR5_SEVL_SHIFT		0
+
+#define MVFR0_FPROUND_SHIFT		28
+#define MVFR0_FPSHVEC_SHIFT		24
+#define MVFR0_FPSQRT_SHIFT		20
+#define MVFR0_FPDIVIDE_SHIFT		16
+#define MVFR0_FPTRAP_SHIFT		12
+#define MVFR0_FPDP_SHIFT		8
+#define MVFR0_FPSP_SHIFT		4
+#define MVFR0_SIMD_SHIFT		0
+
+#define MVFR1_SIMDFMAC_SHIFT		28
+#define MVFR1_FPHP_SHIFT		24
+#define MVFR1_SIMDHP_SHIFT		20
+#define MVFR1_SIMDSP_SHIFT		16
+#define MVFR1_SIMDINT_SHIFT		12
+#define MVFR1_SIMDLS_SHIFT		8
+#define MVFR1_FPDNAN_SHIFT		4
+#define MVFR1_FPFTZ_SHIFT		0
+
 
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28
 #define ID_AA64MMFR0_TGRAN64_SHIFT	24

commit cdcf817b7e4b62b935d8797f7d07ea0b97760884
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:42 2015 +0100

    arm64: Move mixed endian support detection
    
    Move the mixed endian support detection code to cpufeature.c
    from cpuinfo.c. This also moves the update_cpu_features()
    used by mixed endian detection code, which will get more
    functionality.
    
    Also moves the ID register field shifts to asm/sysreg.h,
    where all the useful definitions will end up in later patches.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 4b57e1080538..061a2792179a 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -44,6 +44,8 @@
 #define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
 				     (!!x)<<8 | 0x1f)
 
+#define ID_AA64MMFR0_BIGENDEL0_SHIFT	16
+#define ID_AA64MMFR0_BIGENDEL_SHIFT	8
 
 #define ID_AA64MMFR0_TGRAN4_SHIFT	28
 #define ID_AA64MMFR0_TGRAN64_SHIFT	24

commit 44eaacf1b8999b15cec89bd9d9cd989da4798d53
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:19:37 2015 +0100

    arm64: Add 16K page size support
    
    This patch turns on the 16K page support in the kernel. We
    support 48bit VA (4 level page tables) and 47bit VA (3 level
    page tables).
    
    With 16K we can map 128 entries using contiguous bit hint
    at level 3 to map 2M using single TLB entry.
    
    TODO: 16K supports 32 contiguous entries at level 2 to get us
    1G(which is not yet supported by the infrastructure). That should
    be a separate patch altogether.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Steve Capper <steve.capper@linaro.org>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index d59cb231a673..4b57e1080538 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -59,6 +59,9 @@
 #if defined(CONFIG_ARM64_4K_PAGES)
 #define ID_AA64MMFR0_TGRAN_SHIFT	ID_AA64MMFR0_TGRAN4_SHIFT
 #define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN4_SUPPORTED
+#elif defined(CONFIG_ARM64_16K_PAGES)
+#define ID_AA64MMFR0_TGRAN_SHIFT	ID_AA64MMFR0_TGRAN16_SHIFT
+#define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN16_SUPPORTED
 #elif defined(CONFIG_ARM64_64K_PAGES)
 #define ID_AA64MMFR0_TGRAN_SHIFT	ID_AA64MMFR0_TGRAN64_SHIFT
 #define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN64_SUPPORTED

commit 4bf8b96ed3f7e11422d8b4f58cf43896ed02d1f6
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:19:35 2015 +0100

    arm64: Check for selected granule support
    
    Ensure that the selected page size is supported by the CPU(s). If it doesn't
    park it.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index a7f3d4b2514d..d59cb231a673 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -44,6 +44,26 @@
 #define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
 				     (!!x)<<8 | 0x1f)
 
+
+#define ID_AA64MMFR0_TGRAN4_SHIFT	28
+#define ID_AA64MMFR0_TGRAN64_SHIFT	24
+#define ID_AA64MMFR0_TGRAN16_SHIFT	20
+
+#define ID_AA64MMFR0_TGRAN4_NI		0xf
+#define ID_AA64MMFR0_TGRAN4_SUPPORTED	0x0
+#define ID_AA64MMFR0_TGRAN64_NI		0xf
+#define ID_AA64MMFR0_TGRAN64_SUPPORTED	0x0
+#define ID_AA64MMFR0_TGRAN16_NI		0x0
+#define ID_AA64MMFR0_TGRAN16_SUPPORTED	0x1
+
+#if defined(CONFIG_ARM64_4K_PAGES)
+#define ID_AA64MMFR0_TGRAN_SHIFT	ID_AA64MMFR0_TGRAN4_SHIFT
+#define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN4_SUPPORTED
+#elif defined(CONFIG_ARM64_64K_PAGES)
+#define ID_AA64MMFR0_TGRAN_SHIFT	ID_AA64MMFR0_TGRAN64_SHIFT
+#define ID_AA64MMFR0_TGRAN_SUPPORTED	ID_AA64MMFR0_TGRAN64_SUPPORTED
+#endif
+
 #ifdef __ASSEMBLY__
 
 	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30

commit 338d4f49d6f7114a017d294ccf7374df4f998edc
Author: James Morse <james.morse@arm.com>
Date:   Wed Jul 22 19:05:54 2015 +0100

    arm64: kernel: Add support for Privileged Access Never
    
    'Privileged Access Never' is a new arm8.1 feature which prevents
    privileged code from accessing any virtual address where read or write
    access is also permitted at EL0.
    
    This patch enables the PAN feature on all CPUs, and modifies {get,put}_user
    helpers temporarily to permit access.
    
    This will catch kernel bugs where user memory is accessed directly.
    'Unprivileged loads and stores' using ldtrb et al are unaffected by PAN.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [will: use ALTERNATIVE in asm and tidy up pan_enable check]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 5295bcbcb374..a7f3d4b2514d 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -20,6 +20,8 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
+#include <asm/opcodes.h>
+
 #define SCTLR_EL1_CP15BEN	(0x1 << 5)
 #define SCTLR_EL1_SED		(0x1 << 8)
 
@@ -36,6 +38,12 @@
 #define sys_reg(op0, op1, crn, crm, op2) \
 	((((op0)&3)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
 
+#define REG_PSTATE_PAN_IMM                     sys_reg(0, 0, 4, 0, 4)
+#define SCTLR_EL1_SPAN                         (1 << 23)
+
+#define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
+				     (!!x)<<8 | 0x1f)
+
 #ifdef __ASSEMBLY__
 
 	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30

commit 9ded63aaf83eba76e1a54ac02581c2badc497f1a
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Wed Jul 22 11:38:14 2015 +0100

    arm64: Generalise msr_s/mrs_s operations
    
    The system register encoding generated by sys_reg() works only
    for MRS/MSR(Register) operations, as we hardcode Bit20 to 1 in
    mrs_s/msr_s mask. This makes it unusable for generating instructions
    accessing registers with Op0 < 2(e.g, PSTATE.x with Op0=0).
    
    As per ARMv8 ARM, (Ref: ARMv8 ARM, Section: "System instruction class
    encoding overview", C5.2, version:ARM DDI 0487A.f), the instruction
    encoding reserves bits [20-19] for Op0.
    
    This patch generalises the sys_reg, mrs_s and msr_s macros, so that
    we could use them to access any of the supported system register.
    
    Cc: James Morse <james.morse@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 56391fbae1e1..5295bcbcb374 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -23,8 +23,18 @@
 #define SCTLR_EL1_CP15BEN	(0x1 << 5)
 #define SCTLR_EL1_SED		(0x1 << 8)
 
+/*
+ * ARMv8 ARM reserves the following encoding for system registers:
+ * (Ref: ARMv8 ARM, Section: "System instruction class encoding overview",
+ *  C5.2, version:ARM DDI 0487A.f)
+ *	[20-19] : Op0
+ *	[18-16] : Op1
+ *	[15-12] : CRn
+ *	[11-8]  : CRm
+ *	[7-5]   : Op2
+ */
 #define sys_reg(op0, op1, crn, crm, op2) \
-	((((op0)-2)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
+	((((op0)&3)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
 
 #ifdef __ASSEMBLY__
 
@@ -34,11 +44,11 @@
 	.equ	__reg_num_xzr, 31
 
 	.macro	mrs_s, rt, sreg
-	.inst	0xd5300000|(\sreg)|(__reg_num_\rt)
+	.inst	0xd5200000|(\sreg)|(__reg_num_\rt)
 	.endm
 
 	.macro	msr_s, sreg, rt
-	.inst	0xd5100000|(\sreg)|(__reg_num_\rt)
+	.inst	0xd5000000|(\sreg)|(__reg_num_\rt)
 	.endm
 
 #else
@@ -50,11 +60,11 @@ asm(
 "	.equ	__reg_num_xzr, 31\n"
 "\n"
 "	.macro	mrs_s, rt, sreg\n"
-"	.inst	0xd5300000|(\\sreg)|(__reg_num_\\rt)\n"
+"	.inst	0xd5200000|(\\sreg)|(__reg_num_\\rt)\n"
 "	.endm\n"
 "\n"
 "	.macro	msr_s, sreg, rt\n"
-"	.inst	0xd5100000|(\\sreg)|(__reg_num_\\rt)\n"
+"	.inst	0xd5000000|(\\sreg)|(__reg_num_\\rt)\n"
 "	.endm\n"
 );
 

commit 870828e57b141eff76a5325f20e4691dd2a599b1
Author: James Morse <james.morse@arm.com>
Date:   Tue Jul 21 13:23:27 2015 +0100

    arm64: kernel: Move config_sctlr_el1
    
    Later patches need config_sctlr_el1 to set/clear bits in the sctlr_el1
    register.
    
    This patch moves this function into header a file.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
index 5c89df0acbcb..56391fbae1e1 100644
--- a/arch/arm64/include/asm/sysreg.h
+++ b/arch/arm64/include/asm/sysreg.h
@@ -20,6 +20,9 @@
 #ifndef __ASM_SYSREG_H
 #define __ASM_SYSREG_H
 
+#define SCTLR_EL1_CP15BEN	(0x1 << 5)
+#define SCTLR_EL1_SED		(0x1 << 8)
+
 #define sys_reg(op0, op1, crn, crm, op2) \
 	((((op0)-2)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
 
@@ -55,6 +58,15 @@ asm(
 "	.endm\n"
 );
 
+static inline void config_sctlr_el1(u32 clear, u32 set)
+{
+	u32 val;
+
+	asm volatile("mrs %0, sctlr_el1" : "=r" (val));
+	val &= ~clear;
+	val |= set;
+	asm volatile("msr sctlr_el1, %0" : : "r" (val));
+}
 #endif
 
 #endif	/* __ASM_SYSREG_H */

commit 72c5839515260dce966cd24f54436e6583288e6c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jul 24 14:14:42 2014 +0100

    arm64: gicv3: Allow GICv3 compilation with older binutils
    
    GICv3 introduces new system registers accessible with the full msr/mrs
    syntax (e.g. mrs x0, Sop0_op1_CRm_CRn_op2). However, only recent
    binutils understand the new syntax. This patch introduces msr_s/mrs_s
    assembly macros which generate the equivalent instructions above and
    converts the existing GICv3 code (both drivers/irqchip/ and
    arch/arm64/kernel/).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Olof Johansson <olof@lixom.net>
    Tested-by: Olof Johansson <olof@lixom.net>
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Jason Cooper <jason@lakedaemon.net>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
new file mode 100644
index 000000000000..5c89df0acbcb
--- /dev/null
+++ b/arch/arm64/include/asm/sysreg.h
@@ -0,0 +1,60 @@
+/*
+ * Macros for accessing system registers with older binutils.
+ *
+ * Copyright (C) 2014 ARM Ltd.
+ * Author: Catalin Marinas <catalin.marinas@arm.com>
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __ASM_SYSREG_H
+#define __ASM_SYSREG_H
+
+#define sys_reg(op0, op1, crn, crm, op2) \
+	((((op0)-2)<<19)|((op1)<<16)|((crn)<<12)|((crm)<<8)|((op2)<<5))
+
+#ifdef __ASSEMBLY__
+
+	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30
+	.equ	__reg_num_x\num, \num
+	.endr
+	.equ	__reg_num_xzr, 31
+
+	.macro	mrs_s, rt, sreg
+	.inst	0xd5300000|(\sreg)|(__reg_num_\rt)
+	.endm
+
+	.macro	msr_s, sreg, rt
+	.inst	0xd5100000|(\sreg)|(__reg_num_\rt)
+	.endm
+
+#else
+
+asm(
+"	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n"
+"	.equ	__reg_num_x\\num, \\num\n"
+"	.endr\n"
+"	.equ	__reg_num_xzr, 31\n"
+"\n"
+"	.macro	mrs_s, rt, sreg\n"
+"	.inst	0xd5300000|(\\sreg)|(__reg_num_\\rt)\n"
+"	.endm\n"
+"\n"
+"	.macro	msr_s, sreg, rt\n"
+"	.inst	0xd5100000|(\\sreg)|(__reg_num_\\rt)\n"
+"	.endm\n"
+);
+
+#endif
+
+#endif	/* __ASM_SYSREG_H */
