commit 28f6bf9e247fe23d177cfdbf7e709270e8cc7fa6
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Thu Feb 27 09:51:40 2020 +0100

    arm64: Prepare arch_nmi_enter() for recursion
    
    When using nmi_enter() recursively, arch_nmi_enter() must also be recursion
    safe. In particular, it must be ensured that HCR_TGE is always set while in
    NMI context when in HYP mode, and be restored to it's former state when
    done.
    
    The current code fails this when interleaved wrong. Notably it overwrites
    the original hcr state on nesting.
    
    Introduce a nesting counter to make sure to store the original value.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Link: https://lkml.kernel.org/r/20200505134100.771491291@linutronix.de

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 87ad961f3c97..985493af704b 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -32,30 +32,70 @@ u64 smp_irq_stat_cpu(unsigned int cpu);
 
 struct nmi_ctx {
 	u64 hcr;
+	unsigned int cnt;
 };
 
 DECLARE_PER_CPU(struct nmi_ctx, nmi_contexts);
 
-#define arch_nmi_enter()							\
-	do {									\
-		if (is_kernel_in_hyp_mode()) {					\
-			struct nmi_ctx *nmi_ctx = this_cpu_ptr(&nmi_contexts);	\
-			nmi_ctx->hcr = read_sysreg(hcr_el2);			\
-			if (!(nmi_ctx->hcr & HCR_TGE)) {			\
-				write_sysreg(nmi_ctx->hcr | HCR_TGE, hcr_el2);	\
-				isb();						\
-			}							\
-		}								\
-	} while (0)
+#define arch_nmi_enter()						\
+do {									\
+	struct nmi_ctx *___ctx;						\
+	u64 ___hcr;							\
+									\
+	if (!is_kernel_in_hyp_mode())					\
+		break;							\
+									\
+	___ctx = this_cpu_ptr(&nmi_contexts);				\
+	if (___ctx->cnt) {						\
+		___ctx->cnt++;						\
+		break;							\
+	}								\
+									\
+	___hcr = read_sysreg(hcr_el2);					\
+	if (!(___hcr & HCR_TGE)) {					\
+		write_sysreg(___hcr | HCR_TGE, hcr_el2);		\
+		isb();							\
+	}								\
+	/*								\
+	 * Make sure the sysreg write is performed before ___ctx->cnt	\
+	 * is set to 1. NMIs that see cnt == 1 will rely on us.		\
+	 */								\
+	barrier();							\
+	___ctx->cnt = 1;                                                \
+	/*								\
+	 * Make sure ___ctx->cnt is set before we save ___hcr. We	\
+	 * don't want ___ctx->hcr to be overwritten.			\
+	 */								\
+	barrier();							\
+	___ctx->hcr = ___hcr;						\
+} while (0)
 
-#define arch_nmi_exit()								\
-	do {									\
-		if (is_kernel_in_hyp_mode()) {					\
-			struct nmi_ctx *nmi_ctx = this_cpu_ptr(&nmi_contexts);	\
-			if (!(nmi_ctx->hcr & HCR_TGE))				\
-				write_sysreg(nmi_ctx->hcr, hcr_el2);		\
-		}								\
-	} while (0)
+#define arch_nmi_exit()							\
+do {									\
+	struct nmi_ctx *___ctx;						\
+	u64 ___hcr;							\
+									\
+	if (!is_kernel_in_hyp_mode())					\
+		break;							\
+									\
+	___ctx = this_cpu_ptr(&nmi_contexts);				\
+	___hcr = ___ctx->hcr;						\
+	/*								\
+	 * Make sure we read ___ctx->hcr before we release		\
+	 * ___ctx->cnt as it makes ___ctx->hcr updatable again.		\
+	 */								\
+	barrier();							\
+	___ctx->cnt--;							\
+	/*								\
+	 * Make sure ___ctx->cnt release is visible before we		\
+	 * restore the sysreg. Otherwise a new NMI occurring		\
+	 * right after write_sysreg() can be fooled and think		\
+	 * we secured things for it.					\
+	 */								\
+	barrier();							\
+	if (!___ctx->cnt && !(___hcr & HCR_TGE))			\
+		write_sysreg(___hcr, hcr_el2);				\
+} while (0)
 
 static inline void ack_bad_irq(unsigned int irq)
 {

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 89691c86640a..87ad961f3c97 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_HARDIRQ_H
 #define __ASM_HARDIRQ_H

commit 5870970b9a828d8693aa6d15742573289d7dbcd0
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:39 2019 +0000

    arm64: Fix HCR.TGE status for NMI contexts
    
    When using VHE, the host needs to clear HCR_EL2.TGE bit in order
    to interact with guest TLBs, switching from EL2&0 translation regime
    to EL1&0.
    
    However, some non-maskable asynchronous event could happen while TGE is
    cleared like SDEI. Because of this address translation operations
    relying on EL2&0 translation regime could fail (tlb invalidation,
    userspace access, ...).
    
    Fix this by properly setting HCR_EL2.TGE when entering NMI context and
    clear it if necessary when returning to the interrupted context.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 1473fc2f7ab7..89691c86640a 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -17,8 +17,12 @@
 #define __ASM_HARDIRQ_H
 
 #include <linux/cache.h>
+#include <linux/percpu.h>
 #include <linux/threads.h>
+#include <asm/barrier.h>
 #include <asm/irq.h>
+#include <asm/kvm_arm.h>
+#include <asm/sysreg.h>
 
 #define NR_IPI	7
 
@@ -37,6 +41,33 @@ u64 smp_irq_stat_cpu(unsigned int cpu);
 
 #define __ARCH_IRQ_EXIT_IRQS_DISABLED	1
 
+struct nmi_ctx {
+	u64 hcr;
+};
+
+DECLARE_PER_CPU(struct nmi_ctx, nmi_contexts);
+
+#define arch_nmi_enter()							\
+	do {									\
+		if (is_kernel_in_hyp_mode()) {					\
+			struct nmi_ctx *nmi_ctx = this_cpu_ptr(&nmi_contexts);	\
+			nmi_ctx->hcr = read_sysreg(hcr_el2);			\
+			if (!(nmi_ctx->hcr & HCR_TGE)) {			\
+				write_sysreg(nmi_ctx->hcr | HCR_TGE, hcr_el2);	\
+				isb();						\
+			}							\
+		}								\
+	} while (0)
+
+#define arch_nmi_exit()								\
+	do {									\
+		if (is_kernel_in_hyp_mode()) {					\
+			struct nmi_ctx *nmi_ctx = this_cpu_ptr(&nmi_contexts);	\
+			if (!(nmi_ctx->hcr & HCR_TGE))				\
+				write_sysreg(nmi_ctx->hcr, hcr_el2);		\
+		}								\
+	} while (0)
+
 static inline void ack_bad_irq(unsigned int irq)
 {
 	extern unsigned long irq_err_count;

commit 78fd584cdec0518075cf3aa75e5ec491cc8f3ff3
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:36 2017 +0900

    arm64: kdump: implement machine_crash_shutdown()
    
    Primary kernel calls machine_crash_shutdown() to shut down non-boot cpus
    and save registers' status in per-cpu ELF notes before starting crash
    dump kernel. See kernel_kexec().
    Even if not all secondary cpus have shut down, we do kdump anyway.
    
    As we don't have to make non-boot(crashed) cpus offline (to preserve
    correct status of cpus at crash dump) before shutting down, this patch
    also adds a variant of smp_send_stop().
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 8740297dac77..1473fc2f7ab7 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -20,7 +20,7 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	6
+#define NR_IPI	7
 
 typedef struct {
 	unsigned int __softirq_pending;

commit 5e89c55e4ed81d7abb1ce8828db35fa389dc0e90
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Tue Jan 26 11:10:38 2016 +0000

    arm64: kernel: implement ACPI parking protocol
    
    The SBBR and ACPI specifications allow ACPI based systems that do not
    implement PSCI (eg systems with no EL3) to boot through the ACPI parking
    protocol specification[1].
    
    This patch implements the ACPI parking protocol CPU operations, and adds
    code that eases parsing the parking protocol data structures to the
    ARM64 SMP initializion carried out at the same time as cpus enumeration.
    
    To wake-up the CPUs from the parked state, this patch implements a
    wakeup IPI for ARM64 (ie arch_send_wakeup_ipi_mask()) that mirrors the
    ARM one, so that a specific IPI is sent for wake-up purpose in order
    to distinguish it from other IPI sources.
    
    Given the current ACPI MADT parsing API, the patch implements a glue
    layer that helps passing MADT GICC data structure from SMP initialization
    code to the parking protocol implementation somewhat overriding the CPU
    operations interfaces. This to avoid creating a completely trasparent
    DT/ACPI CPU operations layer that would require creating opaque
    structure handling for CPUs data (DT represents CPU through DT nodes, ACPI
    through static MADT table entries), which seems overkill given that ACPI
    on ARM64 mandates only two booting protocols (PSCI and parking protocol),
    so there is no need for further protocol additions.
    
    Based on the original work by Mark Salter <msalter@redhat.com>
    
    [1] https://acpica.org/sites/acpica/files/MP%20Startup%20for%20ARM%20platforms.docx
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Loc Ho <lho@apm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Al Stone <ahs3@redhat.com>
    [catalin.marinas@arm.com: Added WARN_ONCE(!acpi_parking_protocol_valid() on the IPI]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index a57601f9d17c..8740297dac77 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -20,7 +20,7 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	5
+#define NR_IPI	6
 
 typedef struct {
 	unsigned int __softirq_pending;

commit ae80a2f2d142b97ec68434313f0263256916683d
Author: Rob Herring <robh@kernel.org>
Date:   Sat Aug 29 18:01:25 2015 -0500

    arm64: Remove ununsed set_irq_flags
    
    Now that all users of set_irq_flags and custom flags are converted to
    genirq functions, the ARM specific set_irq_flags can be removed.
    
    Signed-off-by: Rob Herring <robh@kernel.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 2bb7009bdac7..a57601f9d17c 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -43,9 +43,4 @@ static inline void ack_bad_irq(unsigned int irq)
 	irq_err_count++;
 }
 
-/*
- * No arch-specific IRQ flags.
- */
-#define set_irq_flags(irq, flags)
-
 #endif /* __ASM_HARDIRQ_H */

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 6aae421f4d73..2bb7009bdac7 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -24,9 +24,7 @@
 
 typedef struct {
 	unsigned int __softirq_pending;
-#ifdef CONFIG_SMP
 	unsigned int ipi_irqs[NR_IPI];
-#endif
 } ____cacheline_aligned irq_cpustat_t;
 
 #include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
@@ -34,10 +32,8 @@ typedef struct {
 #define __inc_irq_stat(cpu, member)	__IRQ_STAT(cpu, member)++
 #define __get_irq_stat(cpu, member)	__IRQ_STAT(cpu, member)
 
-#ifdef CONFIG_SMP
 u64 smp_irq_stat_cpu(unsigned int cpu);
 #define arch_irq_stat_cpu	smp_irq_stat_cpu
-#endif
 
 #define __ARCH_IRQ_EXIT_IRQS_DISABLED	1
 

commit 0aaf0dae81b586134faeb52e28b7ad567629dd68
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Fri Jan 23 05:36:42 2015 +0000

    smp, ARM64: Kill SMP single function call interrupt
    
    Commit 9a46ad6d6df3b54 "smp: make smp_call_function_many() use logic
    similar to smp_call_function_single()" has unified the way to handle
    single and multiple cross-CPU function calls. Now only one interrupt
    is needed for architecture specific code to support generic SMP function
    call interfaces, so kill the redundant single function call interrupt.
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index e8a3268a891c..6aae421f4d73 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -20,7 +20,7 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	6
+#define NR_IPI	5
 
 typedef struct {
 	unsigned int __softirq_pending;

commit c59e1ef874e699bb37c8ed20b70113e1e8f45f52
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 26 11:03:40 2014 +0100

    arm64: Get rid of handle_IRQ
    
    All the arm64 irqchip drivers have been converted to handle_domain_irq,
    making it possible to remove the handle_IRQ stub entierely.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: https://lkml.kernel.org/r/1409047421-27649-26-git-send-email-marc.zyngier@arm.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 0be67821f9ce..e8a3268a891c 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -47,8 +47,6 @@ static inline void ack_bad_irq(unsigned int irq)
 	irq_err_count++;
 }
 
-extern void handle_IRQ(unsigned int, struct pt_regs *);
-
 /*
  * No arch-specific IRQ flags.
  */

commit eb631bb5bf5b042202aaaee4a8dd8f863ba2a900
Author: Larry Bassel <larry.bassel@linaro.org>
Date:   Mon May 12 16:48:51 2014 +0100

    arm64: Support arch_irq_work_raise() via self IPIs
    
    Support for arch_irq_work_raise() was missing from
    arm64 (a prerequisite for FULL_NOHZ).
    
    This patch is based on the arm32 patch ARM 7872/1.
    
    commit bf18525fd793101df42a1344ecc48b49b62e48c9
    Author: Stephen Boyd <sboyd@codeaurora.org>
    Date:   Tue Oct 29 20:32:56 2013 +0100
    
        ARM: 7872/1: Support arch_irq_work_raise() via self IPIs
    
        By default, IRQ work is run from the tick interrupt (see
        irq_work_run() in update_process_times()). When we're in full
        NOHZ mode, restarting the tick requires the use of IRQ work and
        if the only place we run IRQ work is in the tick interrupt we
        have an unbreakable cycle. Implement arch_irq_work_raise() via
        self IPIs to break this cycle and get the tick started again.
        Note that we implement this via IPIs which are only available on
        SMP builds. This shouldn't be a problem because full NOHZ is only
        supported on SMP builds anyway.
    
        Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
        Reviewed-by: Kevin Hilman <khilman@linaro.org>
        Cc: Frederic Weisbecker <fweisbec@gmail.com>
        Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Signed-off-by: Larry Bassel <larry.bassel@linaro.org>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index ae4801d77514..0be67821f9ce 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -20,7 +20,7 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	5
+#define NR_IPI	6
 
 typedef struct {
 	unsigned int __softirq_pending;

commit 1f85008e74768a88e1ddb96cc1fe45bb2378166c
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed Sep 4 10:55:17 2013 +0100

    arm64: enable generic clockevent broadcast
    
    On platforms with power management capabilities, timers that are shut
    down when a CPU enters deep C-states must be emulated using an always-on
    timer and a timer IPI to relay the timer IRQ to target CPUs on an SMP
    system.
    
    This patch enables the generic clockevents broadcast infrastructure for
    arm64, by providing the required Kconfig entries and adding the timer
    IPI infrastructure.
    
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 990c051e7829..ae4801d77514 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -20,7 +20,7 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	4
+#define NR_IPI	5
 
 typedef struct {
 	unsigned int __softirq_pending;

commit e851b58cb77b47a5c14267723bd6b76655d21840
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jan 14 12:39:31 2013 +0000

    arm64: Use irqchip_init() for interrupt controller initialisation
    
    This patch uses the generic irqchip_init() function for initialising the
    interrupt controller on arm64. It also adds several definitions required
    by the ARM GIC irqchip driver but does not enable ARM_GIC yet.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index 507546353d62..990c051e7829 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -49,4 +49,9 @@ static inline void ack_bad_irq(unsigned int irq)
 
 extern void handle_IRQ(unsigned int, struct pt_regs *);
 
+/*
+ * No arch-specific IRQ flags.
+ */
+#define set_irq_flags(irq, flags)
+
 #endif /* __ASM_HARDIRQ_H */

commit 08e875c16a16c950e1e6d85755df5f3440844675
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:30 2012 +0000

    arm64: SMP support
    
    This patch adds SMP initialisation and spinlocks implementation for
    AArch64. The spinlock support uses the new load-acquire/store-release
    instructions to avoid explicit barriers. The architecture also specifies
    that an event is automatically generated when clearing the exclusive
    monitor state to wake up processors in WFE, so there is no need for an
    explicit DSB/SEV instruction sequence. The SEVL instruction is used to
    set the exclusive monitor locally as there is no conditional WFE and a
    branch is more expensive.
    
    For the SMP booting protocol, see Documentation/arm64/booting.txt.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
index c6c95145c17c..507546353d62 100644
--- a/arch/arm64/include/asm/hardirq.h
+++ b/arch/arm64/include/asm/hardirq.h
@@ -20,8 +20,13 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
+#define NR_IPI	4
+
 typedef struct {
 	unsigned int __softirq_pending;
+#ifdef CONFIG_SMP
+	unsigned int ipi_irqs[NR_IPI];
+#endif
 } ____cacheline_aligned irq_cpustat_t;
 
 #include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */

commit fb9bd7d6df81ddf1e7ab6648ac89ddbe0625b26b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Mar 5 11:49:29 2012 +0000

    arm64: IRQ handling
    
    This patch adds the support for IRQ handling. The actual interrupt
    controller will be part of a separate patch (going into
    drivers/irqchip/).
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
new file mode 100644
index 000000000000..c6c95145c17c
--- /dev/null
+++ b/arch/arm64/include/asm/hardirq.h
@@ -0,0 +1,47 @@
+/*
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_HARDIRQ_H
+#define __ASM_HARDIRQ_H
+
+#include <linux/cache.h>
+#include <linux/threads.h>
+#include <asm/irq.h>
+
+typedef struct {
+	unsigned int __softirq_pending;
+} ____cacheline_aligned irq_cpustat_t;
+
+#include <linux/irq_cpustat.h>	/* Standard mappings for irq_cpustat_t above */
+
+#define __inc_irq_stat(cpu, member)	__IRQ_STAT(cpu, member)++
+#define __get_irq_stat(cpu, member)	__IRQ_STAT(cpu, member)
+
+#ifdef CONFIG_SMP
+u64 smp_irq_stat_cpu(unsigned int cpu);
+#define arch_irq_stat_cpu	smp_irq_stat_cpu
+#endif
+
+#define __ARCH_IRQ_EXIT_IRQS_DISABLED	1
+
+static inline void ack_bad_irq(unsigned int irq)
+{
+	extern unsigned long irq_err_count;
+	irq_err_count++;
+}
+
+extern void handle_IRQ(unsigned int, struct pt_regs *);
+
+#endif /* __ASM_HARDIRQ_H */
