commit 4fcdf106a4330bb5c2306a1efbb3af3b7c0db537
Author: Ionela Voinescu <ionela.voinescu@arm.com>
Date:   Thu Mar 5 09:06:23 2020 +0000

    arm64/kvm: disable access to AMU registers from kvm guests
    
    Access to the AMU counters should be disabled by default in kvm guests,
    as information from the counters might reveal activity in other guests
    or activity on the host.
    
    Therefore, disable access to AMU registers from EL0 and EL1 in kvm
    guests by:
     - Hiding the presence of the extension in the feature register
       (SYS_ID_AA64PFR0_EL1) on the VCPU.
     - Disabling access to the AMU registers before switching to the guest.
     - Trapping accesses and injecting an undefined instruction into the
       guest.
    
    Signed-off-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 6e5d839f42b5..51c1d9918999 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -267,6 +267,7 @@
 
 /* Hyp Coprocessor Trap Register */
 #define CPTR_EL2_TCPAC	(1 << 31)
+#define CPTR_EL2_TAM	(1 << 30)
 #define CPTR_EL2_TTA	(1 << 20)
 #define CPTR_EL2_TFP	(1 << CPTR_EL2_TFP_SHIFT)
 #define CPTR_EL2_TZ	(1 << 8)

commit 5c401308017f256ae9de804b4a1c65be1d390571
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Mon Oct 28 14:05:41 2019 +0100

    KVM: arm64: Don't set HCR_EL2.TVM when S2FWB is supported
    
    On CPUs that support S2FWB (Armv8.4+), KVM configures the stage 2 page
    tables to override the memory attributes of memory accesses, regardless
    of the stage 1 page table configurations, and also when the stage 1 MMU
    is turned off.  This results in all memory accesses to RAM being
    cacheable, including during early boot of the guest.
    
    On CPUs without this feature, memory accesses were non-cacheable during
    boot until the guest turned on the stage 1 MMU, and we had to detect
    when the guest turned on the MMU, such that we could invalidate all cache
    entries and ensure a consistent view of memory with the MMU turned on.
    When the guest turned on the caches, we would call stage2_flush_vm()
    from kvm_toggle_cache().
    
    However, stage2_flush_vm() walks all the stage 2 tables, and calls
    __kvm_flush-dcache_pte, which on a system with S2FWB does ... absolutely
    nothing.
    
    We can avoid that whole song and dance, and simply not set TVM when
    creating a VM on a system that has S2FWB.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Link: https://lore.kernel.org/r/20191028130541.30536-1-christoffer.dall@arm.com

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index ddf9d762ac62..6e5d839f42b5 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -61,7 +61,6 @@
  * RW:		64bit by default, can be overridden for 32bit VMs
  * TAC:		Trap ACTLR
  * TSC:		Trap SMC
- * TVM:		Trap VM ops (until M+C set in SCTLR_EL1)
  * TSW:		Trap cache operations by set/way
  * TWE:		Trap WFE
  * TWI:		Trap WFI
@@ -74,7 +73,7 @@
  * SWIO:	Turn set/way invalidates into set/way clean+invalidate
  */
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
-			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
+			 HCR_BSU_IS | HCR_FB | HCR_TAC | \
 			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR | \
 			 HCR_FMO | HCR_IMO)
 #define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)

commit 6701c619fa082e6660ecd7573fbad2177380c7cc
Author: Zenghui Yu <yuzenghui@huawei.com>
Date:   Sat Jul 13 04:40:54 2019 +0000

    KVM: arm64: Update kvm_arm_exception_class and esr_class_str for new EC
    
    We've added two ESR exception classes for new ARM hardware extensions:
    ESR_ELx_EC_PAC and ESR_ELx_EC_SVE, but failed to update the strings
    used in tracing and other debug.
    
    Let's update "kvm_arm_exception_class" for these two EC, which the
    new EC will be visible to user-space via kvm_exit trace events
    Also update to "esr_class_str" for ESR_ELx_EC_PAC, by which we can
    get more readable debug info.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index a8b205e5c4a8..ddf9d762ac62 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -316,9 +316,10 @@
 
 #define kvm_arm_exception_class \
 	ECN(UNKNOWN), ECN(WFx), ECN(CP15_32), ECN(CP15_64), ECN(CP14_MR), \
-	ECN(CP14_LS), ECN(FP_ASIMD), ECN(CP10_ID), ECN(CP14_64), ECN(SVC64), \
-	ECN(HVC64), ECN(SMC64), ECN(SYS64), ECN(IMP_DEF), ECN(IABT_LOW), \
-	ECN(IABT_CUR), ECN(PC_ALIGN), ECN(DABT_LOW), ECN(DABT_CUR), \
+	ECN(CP14_LS), ECN(FP_ASIMD), ECN(CP10_ID), ECN(PAC), ECN(CP14_64), \
+	ECN(SVC64), ECN(HVC64), ECN(SMC64), ECN(SYS64), ECN(SVE), \
+	ECN(IMP_DEF), ECN(IABT_LOW), ECN(IABT_CUR), \
+	ECN(PC_ALIGN), ECN(DABT_LOW), ECN(DABT_CUR), \
 	ECN(SP_ALIGN), ECN(FP_EXC32), ECN(FP_EXC64), ECN(SERROR), \
 	ECN(BREAKPT_LOW), ECN(BREAKPT_CUR), ECN(SOFTSTP_LOW), \
 	ECN(SOFTSTP_CUR), ECN(WATCHPT_LOW), ECN(WATCHPT_CUR), \

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 7f9d2bfcf82e..a8b205e5c4a8 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -1,18 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2012,2013 - ARM Ltd
  * Author: Marc Zyngier <marc.zyngier@arm.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #ifndef __ARM64_KVM_ARM_H__

commit 42b00f122cfbfed79fc29b0b3610f3abbb1e3864
Merge: 460023a5d1d2 a0aea130afeb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 11:46:28 2018 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - selftests improvements
       - large PUD support for HugeTLB
       - single-stepping fixes
       - improved tracing
       - various timer and vGIC fixes
    
      x86:
       - Processor Tracing virtualization
       - STIBP support
       - some correctness fixes
       - refactorings and splitting of vmx.c
       - use the Hyper-V range TLB flush hypercall
       - reduce order of vcpu struct
       - WBNOINVD support
       - do not use -ftrace for __noclone functions
       - nested guest support for PAUSE filtering on AMD
       - more Hyper-V enlightenments (direct mode for synthetic timers)
    
      PPC:
       -  nested VFIO
    
      s390:
       - bugfixes only this time"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (171 commits)
      KVM: x86: Add CPUID support for new instruction WBNOINVD
      kvm: selftests: ucall: fix exit mmio address guessing
      Revert "compiler-gcc: disable -ftracer for __noclone functions"
      KVM: VMX: Move VM-Enter + VM-Exit handling to non-inline sub-routines
      KVM: VMX: Explicitly reference RCX as the vmx_vcpu pointer in asm blobs
      KVM: x86: Use jmp to invoke kvm_spurious_fault() from .fixup
      MAINTAINERS: Add arch/x86/kvm sub-directories to existing KVM/x86 entry
      KVM/x86: Use SVM assembly instruction mnemonics instead of .byte streams
      KVM/MMU: Flush tlb directly in the kvm_zap_gfn_range()
      KVM/MMU: Flush tlb directly in kvm_set_pte_rmapp()
      KVM/MMU: Move tlb flush in kvm_set_pte_rmapp() to kvm_mmu_notifier_change_pte()
      KVM: Make kvm_set_spte_hva() return int
      KVM: Replace old tlb flush function with new one to flush a specified range.
      KVM/MMU: Add tlb flush with range helper function
      KVM/VMX: Add hv tlb range flush support
      x86/hyper-v: Add HvFlushGuestAddressList hypercall support
      KVM: Add tlb_remote_flush_with_range callback in kvm_x86_ops
      KVM: x86: Disable Intel PT when VMXON in L1 guest
      KVM: x86: Set intercept for Intel PT MSRs read/write
      KVM: x86: Implement Intel PT MSRs read/write emulation
      ...

commit df655b75c43fba0f2621680ab261083297fd6d16
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Dec 13 16:06:14 2018 +0000

    arm64: KVM: Avoid setting the upper 32 bits of VTCR_EL2 to 1
    
    Although bit 31 of VTCR_EL2 is RES1, we inadvertently end up setting all
    of the upper 32 bits to 1 as well because we define VTCR_EL2_RES1 as
    signed, which is sign-extended when assigning to kvm->arch.vtcr.
    
    Lucky for us, the architecture currently treats these upper bits as RES0
    so, whilst we've been naughty, we haven't set fire to anything yet.
    
    Cc: <stable@vger.kernel.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 9921bb7ab6d8..9c1a065b78ea 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -104,7 +104,7 @@
 			 TCR_EL2_ORGN0_MASK | TCR_EL2_IRGN0_MASK | TCR_EL2_T0SZ_MASK)
 
 /* VTCR_EL2 Registers bits */
-#define VTCR_EL2_RES1		(1 << 31)
+#define VTCR_EL2_RES1		(1U << 31)
 #define VTCR_EL2_HD		(1 << 22)
 #define VTCR_EL2_HA		(1 << 21)
 #define VTCR_EL2_PS_SHIFT	TCR_EL2_PS_SHIFT

commit 71a7e47f39a2fb971ef9934e98ba089581eff641
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Mon Dec 3 21:31:24 2018 +0100

    KVM: arm/arm64: Fixup the kvm_exit tracepoint
    
    The kvm_exit tracepoint strangely always reported exits as being IRQs.
    This seems to be because either the __print_symbolic or the tracepoint
    macros use a variable named idx.
    
    Take this chance to update the fields in the tracepoint to reflect the
    concepts in the arm64 architecture that we pass to the tracepoint and
    move the exception type table to the same location and header files as
    the exits code.
    
    We also clear out the exception code to 0 for IRQ exits (which
    translates to UNKNOWN in text) to make it slighyly less confusing to
    parse the trace output.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 6f602af5263c..9921bb7ab6d8 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -320,10 +320,6 @@
 #define PAR_TO_HPFAR(par)		\
 	(((par) & GENMASK_ULL(PHYS_MASK_SHIFT - 1, 12)) >> 8)
 
-#define kvm_arm_exception_type	\
-	{0, "IRQ" }, 		\
-	{1, "TRAP" }
-
 #define ECN(x) { ESR_ELx_EC_##x, #x }
 
 #define kvm_arm_exception_class \

commit b3669b1e1c09890d61109a1a8ece2c5b66804714
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:23 2018 +0000

    arm64: Don't trap host pointer auth use to EL2
    
    To allow EL0 (and/or EL1) to use pointer authentication functionality,
    we must ensure that pointer authentication instructions and accesses to
    pointer authentication keys are not trapped to EL2.
    
    This patch ensures that HCR_EL2 is configured appropriately when the
    kernel is booted at EL2. For non-VHE kernels we set HCR_EL2.{API,APK},
    ensuring that EL1 can access keys and permit EL0 use of instructions.
    For VHE kernels host EL0 (TGE && E2H) is unaffected by these settings,
    and it doesn't matter how we configure HCR_EL2.{API,APK}, so we don't
    bother setting them.
    
    This does not enable support for KVM guests, since KVM manages HCR_EL2
    itself when running VMs.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index c8825c5a8dd0..f9123fe8fcf3 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -24,6 +24,8 @@
 
 /* Hyp Configuration Register (HCR) bits */
 #define HCR_FWB		(UL(1) << 46)
+#define HCR_API		(UL(1) << 41)
+#define HCR_APK		(UL(1) << 40)
 #define HCR_TEA		(UL(1) << 37)
 #define HCR_TERR	(UL(1) << 36)
 #define HCR_TLOR	(UL(1) << 35)
@@ -87,7 +89,7 @@
 			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR | \
 			 HCR_FMO | HCR_IMO)
 #define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)
-#define HCR_HOST_NVHE_FLAGS (HCR_RW)
+#define HCR_HOST_NVHE_FLAGS (HCR_RW | HCR_API | HCR_APK)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 
 /* TCR_EL2 Registers bits */

commit 4eaed6aa2c628101246bcabc91b203bfac1193f8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:21 2018 +0000

    arm64/kvm: consistently handle host HCR_EL2 flags
    
    In KVM we define the configuration of HCR_EL2 for a VHE HOST in
    HCR_HOST_VHE_FLAGS, but we don't have a similar definition for the
    non-VHE host flags, and open-code HCR_RW. Further, in head.S we
    open-code the flags for VHE and non-VHE configurations.
    
    In future, we're going to want to configure more flags for the host, so
    lets add a HCR_HOST_NVHE_FLAGS defintion, and consistently use both
    HCR_HOST_VHE_FLAGS and HCR_HOST_NVHE_FLAGS in the kvm code and head.S.
    
    We now use mov_q to generate the HCR_EL2 value, as we use when
    configuring other registers in head.S.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 6f602af5263c..c8825c5a8dd0 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -87,6 +87,7 @@
 			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR | \
 			 HCR_FMO | HCR_IMO)
 #define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)
+#define HCR_HOST_NVHE_FLAGS (HCR_RW)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 
 /* TCR_EL2 Registers bits */

commit 0d1e8b8d2bcd3150d51754d8d0fdbf44dc88b0d3
Merge: 83c4087ce468 22a7cdcae6a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 17:57:35 2018 -0700

    Merge tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
       - Improved guest IPA space support (32 to 52 bits)
    
       - RAS event delivery for 32bit
    
       - PMU fixes
    
       - Guest entry hardening
    
       - Various cleanups
    
       - Port of dirty_log_test selftest
    
      PPC:
       - Nested HV KVM support for radix guests on POWER9. The performance
         is much better than with PR KVM. Migration and arbitrary level of
         nesting is supported.
    
       - Disable nested HV-KVM on early POWER9 chips that need a particular
         hardware bug workaround
    
       - One VM per core mode to prevent potential data leaks
    
       - PCI pass-through optimization
    
       - merge ppc-kvm topic branch and kvm-ppc-fixes to get a better base
    
      s390:
       - Initial version of AP crypto virtualization via vfio-mdev
    
       - Improvement for vfio-ap
    
       - Set the host program identifier
    
       - Optimize page table locking
    
      x86:
       - Enable nested virtualization by default
    
       - Implement Hyper-V IPI hypercalls
    
       - Improve #PF and #DB handling
    
       - Allow guests to use Enlightened VMCS
    
       - Add migration selftests for VMCS and Enlightened VMCS
    
       - Allow coalesced PIO accesses
    
       - Add an option to perform nested VMCS host state consistency check
         through hardware
    
       - Automatic tuning of lapic_timer_advance_ns
    
       - Many fixes, minor improvements, and cleanups"
    
    * tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      KVM/nVMX: Do not validate that posted_intr_desc_addr is page aligned
      Revert "kvm: x86: optimize dr6 restore"
      KVM: PPC: Optimize clearing TCEs for sparse tables
      x86/kvm/nVMX: tweak shadow fields
      selftests/kvm: add missing executables to .gitignore
      KVM: arm64: Safety check PSTATE when entering guest and handle IL
      KVM: PPC: Book3S HV: Don't use streamlined entry path on early POWER9 chips
      arm/arm64: KVM: Enable 32 bits kvm vcpu events support
      arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
      KVM: arm64: Fix caching of host MDCR_EL2 value
      KVM: VMX: enable nested virtualization by default
      KVM/x86: Use 32bit xor to clear registers in svm.c
      kvm: x86: Introduce KVM_CAP_EXCEPTION_PAYLOAD
      kvm: vmx: Defer setting of DR6 until #DB delivery
      kvm: x86: Defer setting of CR2 until #PF delivery
      kvm: x86: Add payload operands to kvm_multiple_exception
      kvm: x86: Add exception payload fields to kvm_vcpu_events
      kvm: x86: Add has_payload and payload to kvm_queued_exception
      KVM: Documentation: Fix omission in struct kvm_vcpu_events
      KVM: selftests: add Enlightened VMCS test
      ...

commit bca607ebc76af9540e4aad5b2241a7323354be43
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 1 13:40:36 2018 +0100

    KVM: arm/arm64: Rename kvm_arm_config_vm to kvm_arm_setup_stage2
    
    VM tends to be a very overloaded term in KVM, so let's keep it
    to describe the virtual machine. For the virtual memory setup,
    let's use the "stage2" suffix.
    
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index f1330284720d..6e324d1f1231 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -133,7 +133,7 @@
  * 40 bits wide (T0SZ = 24).  Systems with a PARange smaller than 40 bits are
  * not known to exist and will break with this configuration.
  *
- * The VTCR_EL2 is configured per VM and is initialised in kvm_arm_config_vm().
+ * The VTCR_EL2 is configured per VM and is initialised in kvm_arm_setup_stage2().
  *
  * Note that when using 4K pages, we concatenate two first level page tables
  * together. With 16K pages, we concatenate 16 first level page tables.

commit bc1d7de8c550db2f8fd59458a07fefa863358b8d
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:51 2018 +0100

    kvm: arm64: Add 52bit support for PAR to HPFAR conversoin
    
    Add support for handling 52bit addresses in PAR to HPFAR
    conversion. Instead of hardcoding the address limits, we
    now use PHYS_MASK_SHIFT.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index e4240568cc18..f1330284720d 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -311,6 +311,13 @@
 
 /* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */
 #define HPFAR_MASK	(~UL(0xf))
+/*
+ * We have
+ *	PAR	[PA_Shift - 1	: 12] = PA	[PA_Shift - 1 : 12]
+ *	HPFAR	[PA_Shift - 9	: 4]  = FIPA	[PA_Shift - 1 : 12]
+ */
+#define PAR_TO_HPFAR(par)		\
+	(((par) & GENMASK_ULL(PHYS_MASK_SHIFT - 1, 12)) >> 8)
 
 #define kvm_arm_exception_type	\
 	{0, "IRQ" }, 		\

commit 13ac4bbcc457d3925b4031cc70e3031fd8b9c3b7
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:49 2018 +0100

    kvm: arm64: Switch to per VM IPA limit
    
    Now that we can manage the stage2 page table per VM, switch the
    configuration details to per VM instance. The VTCR is updated
    with the values specific to the VM based on the configuration.
    We store the IPA size and the number of stage2 page table levels
    for the guest already in VTCR. Decode it back from the vtcr
    field wherever we need it.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index f913adb44f93..e4240568cc18 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -197,6 +197,8 @@
 	VTCR_EL2_SL0_TO_LVLS(((vtcr) & VTCR_EL2_SL0_MASK) >> VTCR_EL2_SL0_SHIFT)
 
 #define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN)
+#define VTCR_EL2_IPA(vtcr)		(64 - ((vtcr) & VTCR_EL2_T0SZ_MASK))
+
 /*
  * ARM VMSAv8-64 defines an algorithm for finding the translation table
  * descriptors in section D4.2.8 in ARM DDI 0487C.a.

commit 7e8130456e067f49693fdcc25b5ba242a9c5568b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:48 2018 +0100

    kvm: arm64: Configure VTCR_EL2.SL0 per VM
    
    VTCR_EL2 holds the following key stage2 translation table
    parameters:
      SL0  - Entry level in the page table lookup.
      T0SZ - Denotes the size of the memory addressed by the table.
    
    We have been using fixed values for the SL0 depending on the
    page size as we have a fixed IPA size. But since we are about
    to make it dynamic, we need to calculate the SL0 at runtime
    per VM. This patch adds a helper to compute the value of SL0
    for a VM based on the IPA size.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index b236d90ca056..f913adb44f93 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -121,7 +121,6 @@
 #define VTCR_EL2_IRGN0_WBWA	TCR_IRGN0_WBWA
 #define VTCR_EL2_SL0_SHIFT	6
 #define VTCR_EL2_SL0_MASK	(3 << VTCR_EL2_SL0_SHIFT)
-#define VTCR_EL2_SL0_LVL1	(1 << VTCR_EL2_SL0_SHIFT)
 #define VTCR_EL2_T0SZ_MASK	0x3f
 #define VTCR_EL2_VS_SHIFT	19
 #define VTCR_EL2_VS_8BIT	(0 << VTCR_EL2_VS_SHIFT)
@@ -144,30 +143,60 @@
 #define VTCR_EL2_COMMON_BITS	(VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
 				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_RES1)
 
-#ifdef CONFIG_ARM64_64K_PAGES
 /*
- * Stage2 translation configuration:
- * 64kB pages (TG0 = 1)
- * 2 level page tables (SL = 1)
+ * VTCR_EL2:SL0 indicates the entry level for Stage2 translation.
+ * Interestingly, it depends on the page size.
+ * See D.10.2.121, VTCR_EL2, in ARM DDI 0487C.a
+ *
+ *	-----------------------------------------
+ *	| Entry level		|  4K  | 16K/64K |
+ *	------------------------------------------
+ *	| Level: 0		|  2   |   -     |
+ *	------------------------------------------
+ *	| Level: 1		|  1   |   2     |
+ *	------------------------------------------
+ *	| Level: 2		|  0   |   1     |
+ *	------------------------------------------
+ *	| Level: 3		|  -   |   0     |
+ *	------------------------------------------
+ *
+ * The table roughly translates to :
+ *
+ *	SL0(PAGE_SIZE, Entry_level) = TGRAN_SL0_BASE - Entry_Level
+ *
+ * Where TGRAN_SL0_BASE is a magic number depending on the page size:
+ * 	TGRAN_SL0_BASE(4K) = 2
+ *	TGRAN_SL0_BASE(16K) = 3
+ *	TGRAN_SL0_BASE(64K) = 3
+ * provided we take care of ruling out the unsupported cases and
+ * Entry_Level = 4 - Number_of_levels.
+ *
  */
-#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SL0_LVL1)
+#ifdef CONFIG_ARM64_64K_PAGES
+
+#define VTCR_EL2_TGRAN			VTCR_EL2_TG0_64K
+#define VTCR_EL2_TGRAN_SL0_BASE		3UL
+
 #elif defined(CONFIG_ARM64_16K_PAGES)
-/*
- * Stage2 translation configuration:
- * 16kB pages (TG0 = 2)
- * 2 level page tables (SL = 1)
- */
-#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_16K | VTCR_EL2_SL0_LVL1)
+
+#define VTCR_EL2_TGRAN			VTCR_EL2_TG0_16K
+#define VTCR_EL2_TGRAN_SL0_BASE		3UL
+
 #else	/* 4K */
-/*
- * Stage2 translation configuration:
- * 4kB pages (TG0 = 0)
- * 3 level page tables (SL = 1)
- */
-#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SL0_LVL1)
+
+#define VTCR_EL2_TGRAN			VTCR_EL2_TG0_4K
+#define VTCR_EL2_TGRAN_SL0_BASE		2UL
+
 #endif
 
-#define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN_FLAGS)
+#define VTCR_EL2_LVLS_TO_SL0(levels)	\
+	((VTCR_EL2_TGRAN_SL0_BASE - (4 - (levels))) << VTCR_EL2_SL0_SHIFT)
+#define VTCR_EL2_SL0_TO_LVLS(sl0)	\
+	((sl0) + 4 - VTCR_EL2_TGRAN_SL0_BASE)
+#define VTCR_EL2_LVLS(vtcr)		\
+	VTCR_EL2_SL0_TO_LVLS(((vtcr) & VTCR_EL2_SL0_MASK) >> VTCR_EL2_SL0_SHIFT)
+
+#define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN)
 /*
  * ARM VMSAv8-64 defines an algorithm for finding the translation table
  * descriptors in section D4.2.8 in ARM DDI 0487C.a.

commit 595583306434caafac4a71102dca5a8d32d1a769
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:47 2018 +0100

    kvm: arm64: Dynamic configuration of VTTBR mask
    
    On arm64 VTTBR_EL2:BADDR holds the base address for the stage2
    translation table. The Arm ARM mandates that the bits BADDR[x-1:0]
    should be 0, where 'x' is defined for a given IPA Size and the
    number of levels for a translation granule size. It is defined
    using some magical constants. This patch is a reverse engineered
    implementation to calculate the 'x' at runtime for a given ipa and
    number of page table levels. See patch for more details.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 14317b3a1820..b236d90ca056 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -123,7 +123,6 @@
 #define VTCR_EL2_SL0_MASK	(3 << VTCR_EL2_SL0_SHIFT)
 #define VTCR_EL2_SL0_LVL1	(1 << VTCR_EL2_SL0_SHIFT)
 #define VTCR_EL2_T0SZ_MASK	0x3f
-#define VTCR_EL2_T0SZ_40B	24
 #define VTCR_EL2_VS_SHIFT	19
 #define VTCR_EL2_VS_8BIT	(0 << VTCR_EL2_VS_SHIFT)
 #define VTCR_EL2_VS_16BIT	(1 << VTCR_EL2_VS_SHIFT)
@@ -140,11 +139,8 @@
  * Note that when using 4K pages, we concatenate two first level page tables
  * together. With 16K pages, we concatenate 16 first level page tables.
  *
- * The magic numbers used for VTTBR_X in this patch can be found in Tables
- * D4-23 and D4-25 in ARM DDI 0487A.b.
  */
 
-#define VTCR_EL2_T0SZ_IPA	VTCR_EL2_T0SZ_40B
 #define VTCR_EL2_COMMON_BITS	(VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
 				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_RES1)
 
@@ -155,7 +151,6 @@
  * 2 level page tables (SL = 1)
  */
 #define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SL0_LVL1)
-#define VTTBR_X_TGRAN_MAGIC		38
 #elif defined(CONFIG_ARM64_16K_PAGES)
 /*
  * Stage2 translation configuration:
@@ -163,7 +158,6 @@
  * 2 level page tables (SL = 1)
  */
 #define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_16K | VTCR_EL2_SL0_LVL1)
-#define VTTBR_X_TGRAN_MAGIC		42
 #else	/* 4K */
 /*
  * Stage2 translation configuration:
@@ -171,13 +165,74 @@
  * 3 level page tables (SL = 1)
  */
 #define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SL0_LVL1)
-#define VTTBR_X_TGRAN_MAGIC		37
 #endif
 
 #define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN_FLAGS)
-#define VTTBR_X				(VTTBR_X_TGRAN_MAGIC - VTCR_EL2_T0SZ_IPA)
+/*
+ * ARM VMSAv8-64 defines an algorithm for finding the translation table
+ * descriptors in section D4.2.8 in ARM DDI 0487C.a.
+ *
+ * The algorithm defines the expectations on the translation table
+ * addresses for each level, based on PAGE_SIZE, entry level
+ * and the translation table size (T0SZ). The variable "x" in the
+ * algorithm determines the alignment of a table base address at a given
+ * level and thus determines the alignment of VTTBR:BADDR for stage2
+ * page table entry level.
+ * Since the number of bits resolved at the entry level could vary
+ * depending on the T0SZ, the value of "x" is defined based on a
+ * Magic constant for a given PAGE_SIZE and Entry Level. The
+ * intermediate levels must be always aligned to the PAGE_SIZE (i.e,
+ * x = PAGE_SHIFT).
+ *
+ * The value of "x" for entry level is calculated as :
+ *    x = Magic_N - T0SZ
+ *
+ * where Magic_N is an integer depending on the page size and the entry
+ * level of the page table as below:
+ *
+ *	--------------------------------------------
+ *	| Entry level		|  4K    16K   64K |
+ *	--------------------------------------------
+ *	| Level: 0 (4 levels)	| 28   |  -  |  -  |
+ *	--------------------------------------------
+ *	| Level: 1 (3 levels)	| 37   | 31  | 25  |
+ *	--------------------------------------------
+ *	| Level: 2 (2 levels)	| 46   | 42  | 38  |
+ *	--------------------------------------------
+ *	| Level: 3 (1 level)	| -    | 53  | 51  |
+ *	--------------------------------------------
+ *
+ * We have a magic formula for the Magic_N below:
+ *
+ *  Magic_N(PAGE_SIZE, Level) = 64 - ((PAGE_SHIFT - 3) * Number_of_levels)
+ *
+ * where Number_of_levels = (4 - Level). We are only interested in the
+ * value for Entry_Level for the stage2 page table.
+ *
+ * So, given that T0SZ = (64 - IPA_SHIFT), we can compute 'x' as follows:
+ *
+ *	x = (64 - ((PAGE_SHIFT - 3) * Number_of_levels)) - (64 - IPA_SHIFT)
+ *	  = IPA_SHIFT - ((PAGE_SHIFT - 3) * Number of levels)
+ *
+ * Here is one way to explain the Magic Formula:
+ *
+ *  x = log2(Size_of_Entry_Level_Table)
+ *
+ * Since, we can resolve (PAGE_SHIFT - 3) bits at each level, and another
+ * PAGE_SHIFT bits in the PTE, we have :
+ *
+ *  Bits_Entry_level = IPA_SHIFT - ((PAGE_SHIFT - 3) * (n - 1) + PAGE_SHIFT)
+ *		     = IPA_SHIFT - (PAGE_SHIFT - 3) * n - 3
+ *  where n = number of levels, and since each pointer is 8bytes, we have:
+ *
+ *  x = Bits_Entry_Level + 3
+ *    = IPA_SHIFT - (PAGE_SHIFT - 3) * n
+ *
+ * The only constraint here is that, we have to find the number of page table
+ * levels for a given IPA size (which we do, see stage2_pt_levels())
+ */
+#define ARM64_VTTBR_X(ipa, levels)	((ipa) - ((levels) * (PAGE_SHIFT - 3)))
 
-#define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_X)
 #define VTTBR_VMID_SHIFT  (UL(48))
 #define VTTBR_VMID_MASK(size) (_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
 

commit 7665f3a8491b0ed3c6f65c0bc3a5424ea8f87731
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:43 2018 +0100

    kvm: arm64: Configure VTCR_EL2 per VM
    
    Add support for setting the VTCR_EL2 per VM, rather than hard
    coding a value at boot time per CPU. This would allow us to tune
    the stage2 page table parameters per VM in the later changes.
    
    We compute the VTCR fields based on the system wide sanitised
    feature registers, except for the hardware management of Access
    Flags (VTCR_EL2.HA). It is fine to run a system with a mix of
    CPUs that may or may not update the page table Access Flags.
    Since the bit is RES0 on CPUs that don't support it, the bit
    should be ignored on them.
    
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 5f807b680a5f..14317b3a1820 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -135,8 +135,7 @@
  * 40 bits wide (T0SZ = 24).  Systems with a PARange smaller than 40 bits are
  * not known to exist and will break with this configuration.
  *
- * VTCR_EL2.PS is extracted from ID_AA64MMFR0_EL1.PARange at boot time
- * (see hyp-init.S).
+ * The VTCR_EL2 is configured per VM and is initialised in kvm_arm_config_vm().
  *
  * Note that when using 4K pages, we concatenate two first level page tables
  * together. With 16K pages, we concatenate 16 first level page tables.

commit b2df44ffba363bd90274340e0adfd8137f5cd878
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:41 2018 +0100

    kvm: arm64: Clean up VTCR_EL2 initialisation
    
    Use the new helper for converting the parange to the physical shift.
    Also, add the missing definitions for the VTCR_EL2 register fields
    and use them instead of hard coding numbers.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index aa45df752a16..5f807b680a5f 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -107,6 +107,7 @@
 #define VTCR_EL2_RES1		(1 << 31)
 #define VTCR_EL2_HD		(1 << 22)
 #define VTCR_EL2_HA		(1 << 21)
+#define VTCR_EL2_PS_SHIFT	TCR_EL2_PS_SHIFT
 #define VTCR_EL2_PS_MASK	TCR_EL2_PS_MASK
 #define VTCR_EL2_TG0_MASK	TCR_TG0_MASK
 #define VTCR_EL2_TG0_4K		TCR_TG0_4K
@@ -127,6 +128,8 @@
 #define VTCR_EL2_VS_8BIT	(0 << VTCR_EL2_VS_SHIFT)
 #define VTCR_EL2_VS_16BIT	(1 << VTCR_EL2_VS_SHIFT)
 
+#define VTCR_EL2_T0SZ(x)	TCR_T0SZ(x)
+
 /*
  * We configure the Stage-2 page tables to always restrict the IPA space to be
  * 40 bits wide (T0SZ = 24).  Systems with a PARange smaller than 40 bits are

commit ab510027dc4dbd1eeb611a34b0cda8b21fcde492
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Tue Jul 31 14:08:57 2018 +0100

    arm64: KVM: Enable Common Not Private translations
    
    We rely on cpufeature framework to detect and enable CNP so for KVM we
    need to patch hyp to set CNP bit just before TTBR0_EL2 gets written.
    
    For the guest we encode CNP bit while building vttbr, so we don't need
    to bother with that in a world switch.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index aa45df752a16..b476bc46f0ab 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -175,6 +175,7 @@
 #define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN_FLAGS)
 #define VTTBR_X				(VTTBR_X_TGRAN_MAGIC - VTCR_EL2_T0SZ_IPA)
 
+#define VTTBR_CNP_BIT     (UL(1))
 #define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_X)
 #define VTTBR_VMID_SHIFT  (UL(48))
 #define VTTBR_VMID_MASK(size) (_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)

commit e48d53a91f6e90873e21a5ca5e8c0d7a9f8936a4
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 6 12:27:28 2018 +0100

    arm64: KVM: Add support for Stage-2 control of memory types and cacheability
    
    Up to ARMv8.3, the combinaison of Stage-1 and Stage-2 attributes
    results in the strongest attribute of the two stages.  This means
    that the hypervisor has to perform quite a lot of cache maintenance
    just in case the guest has some non-cacheable mappings around.
    
    ARMv8.4 solves this problem by offering a different mode (FWB) where
    Stage-2 has total control over the memory attribute (this is limited
    to systems where both I/O and instruction fetches are coherent with
    the dcache). This is achieved by having a different set of memory
    attributes in the page tables, and a new bit set in HCR_EL2.
    
    On such a system, we can then safely sidestep any form of dcache
    management.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 6dd285e979c9..aa45df752a16 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -23,6 +23,7 @@
 #include <asm/types.h>
 
 /* Hyp Configuration Register (HCR) bits */
+#define HCR_FWB		(UL(1) << 46)
 #define HCR_TEA		(UL(1) << 37)
 #define HCR_TERR	(UL(1) << 36)
 #define HCR_TLOR	(UL(1) << 35)

commit 35a84dec00a707aed97c1ff9ebb1cd1eb67c7052
Author: Shih-Wei Li <shihwei@cs.columbia.edu>
Date:   Thu Aug 3 11:45:21 2017 +0200

    KVM: arm64: Move HCR_INT_OVERRIDE to default HCR_EL2 guest flag
    
    We always set the IMO and FMO bits in the HCR_EL2 when running the
    guest, regardless if we use the vgic or not.  By moving these flags to
    HCR_GUEST_FLAGS we can avoid one of the extra save/restore operations of
    HCR_EL2 in the world switch code, and we can also soon get rid of the
    other one.
    
    This is safe, because even though the IMO and FMO bits control both
    taking the interrupts to EL2 and remapping ICC_*_EL1 to ICV_*_EL1 when
    executed at EL1, as long as we ensure that these bits are clear when
    running the EL1 host, we're OK, because we reset the HCR_EL2 to only
    have the HCR_RW bit set when returning to EL1 on non-VHE systems.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Shih-Wei Li <shihwei@cs.columbia.edu>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 1b438c334463..6dd285e979c9 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -83,9 +83,9 @@
  */
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
 			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
-			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR)
+			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR | \
+			 HCR_FMO | HCR_IMO)
 #define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)
-#define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 
 /* TCR_EL2 Registers bits */

commit cc33c4e20185a391766ed5e78e2acc97e17ba511
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Feb 13 13:39:23 2018 +0000

    arm64/kvm: Prohibit guest LOR accesses
    
    We don't currently limit guest accesses to the LOR registers, which we
    neither virtualize nor context-switch. As such, guests are provided with
    unusable information/controls, and are not isolated from each other (or
    the host).
    
    To prevent these issues, we can trap register accesses and present the
    illusion LORegions are unssupported by the CPU. To do this, we mask
    ID_AA64MMFR1.LO, and set HCR_EL2.TLOR to trap accesses to the following
    registers:
    
    * LORC_EL1
    * LOREA_EL1
    * LORID_EL1
    * LORN_EL1
    * LORSA_EL1
    
    ... when trapped, we inject an UNDEFINED exception to EL1, simulating
    their non-existence.
    
    As noted in D7.2.67, when no LORegions are implemented, LoadLOAcquire
    and StoreLORelease must behave as LoadAcquire and StoreRelease
    respectively. We can ensure this by clearing LORC_EL1.EN when a CPU's
    EL2 is first initialized, as the host kernel will not modify this.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Vladimir Murzin <vladimir.murzin@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index b0c84171e6a3..1b438c334463 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -25,6 +25,7 @@
 /* Hyp Configuration Register (HCR) bits */
 #define HCR_TEA		(UL(1) << 37)
 #define HCR_TERR	(UL(1) << 36)
+#define HCR_TLOR	(UL(1) << 35)
 #define HCR_E2H		(UL(1) << 34)
 #define HCR_ID		(UL(1) << 33)
 #define HCR_CD		(UL(1) << 32)
@@ -64,6 +65,7 @@
 
 /*
  * The bits we set in HCR:
+ * TLOR:	Trap LORegion register accesses
  * RW:		64bit by default, can be overridden for 32bit VMs
  * TAC:		Trap ACTLR
  * TSC:		Trap SMC
@@ -81,7 +83,7 @@
  */
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
 			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
-			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW)
+			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR)
 #define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)
 #define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)

commit 558daf693e0c7ea118dbfb9539aa5a72f34bad2e
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Mon Jan 15 19:39:06 2018 +0000

    KVM: arm64: Emulate RAS error registers and set HCR_EL2's TERR & TEA
    
    ARMv8.2 adds a new bit HCR_EL2.TEA which routes synchronous external
    aborts to EL2, and adds a trap control bit HCR_EL2.TERR which traps
    all Non-secure EL1&0 error record accesses to EL2.
    
    This patch enables the two bits for the guest OS, guaranteeing that
    KVM takes external aborts and traps attempts to access the physical
    error registers.
    
    ERRIDR_EL1 advertises the number of error records, we return
    zero meaning we can treat all the other registers as RAZ/WI too.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    [removed specific emulation, use trap_raz_wi() directly for everything,
     rephrased parts of the commit message]
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 715d395ef45b..b0c84171e6a3 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -23,6 +23,8 @@
 #include <asm/types.h>
 
 /* Hyp Configuration Register (HCR) bits */
+#define HCR_TEA		(UL(1) << 37)
+#define HCR_TERR	(UL(1) << 36)
 #define HCR_E2H		(UL(1) << 34)
 #define HCR_ID		(UL(1) << 33)
 #define HCR_CD		(UL(1) << 32)

commit 26aa7b3b1c0fb3f1a6176a0c1847204ef4355693
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Thu Nov 16 17:58:20 2017 +0000

    arm64: KVM: fix VTTBR_BADDR_MASK BUG_ON off-by-one
    
    VTTBR_BADDR_MASK is used to sanity check the size and alignment of the
    VTTBR address. It seems to currently be off by one, thereby only
    allowing up to 47-bit addresses (instead of 48-bit) and also
    insufficiently checking the alignment. This patch fixes it.
    
    As an example, with 4k pages, before this patch we have:
    
      PHYS_MASK_SHIFT = 48
      VTTBR_X = 37 - 24 = 13
      VTTBR_BADDR_SHIFT = 13 - 1 = 12
      VTTBR_BADDR_MASK = ((1 << 35) - 1) << 12 = 0x00007ffffffff000
    
    Which is wrong, because the mask doesn't allow bit 47 of the VTTBR
    address to be set, and only requires the address to be 12-bit (4k)
    aligned, while it actually needs to be 13-bit (8k) aligned because we
    concatenate two 4k tables.
    
    With this patch, the mask becomes 0x0000ffffffffe000, which is what we
    want.
    
    Fixes: 0369f6a34b9f ("arm64: KVM: EL2 register definitions")
    Cc: <stable@vger.kernel.org> # 3.11.x
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 7f069ff37f06..715d395ef45b 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -170,8 +170,7 @@
 #define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN_FLAGS)
 #define VTTBR_X				(VTTBR_X_TGRAN_MAGIC - VTCR_EL2_T0SZ_IPA)
 
-#define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
-#define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+#define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_X)
 #define VTTBR_VMID_SHIFT  (UL(48))
 #define VTTBR_VMID_MASK(size) (_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
 

commit 17eed27b02da88560b4592390952b9a71042ab8b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:16 2017 +0000

    arm64/sve: KVM: Prevent guests from using SVE
    
    Until KVM has full SVE support, guests must not be allowed to
    execute SVE instructions.
    
    This patch enables the necessary traps, and also ensures that the
    traps are disabled again on exit from the guest so that the host
    can still use SVE if it wants to.
    
    On guest exit, high bits of the SVE Zn registers may have been
    clobbered as a side-effect the execution of FPSIMD instructions in
    the guest.  The existing KVM host FPSIMD restore code is not
    sufficient to restore these bits, so this patch explicitly marks
    the CPU as not containing cached vector state for any task, thus
    forcing a reload on the next return to userspace.  This is an
    interim measure, in advance of adding full SVE awareness to KVM.
    
    This marking of cached vector state in the CPU as invalid is done
    using __this_cpu_write(fpsimd_last_state, NULL) in fpsimd.c.  Due
    to the repeated use of this rather obscure operation, it makes
    sense to factor it out as a separate helper with a clearer name.
    This patch factors it out as fpsimd_flush_cpu_state(), and ports
    all callers to use it.
    
    As a side effect of this refactoring, a this_cpu_write() in
    fpsimd_cpu_pm_notifier() is changed to __this_cpu_write().  This
    should be fine, since cpu_pm_enter() is supposed to be called only
    with interrupts disabled.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index dbf05370169a..7f069ff37f06 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -186,7 +186,8 @@
 #define CPTR_EL2_TTA	(1 << 20)
 #define CPTR_EL2_TFP	(1 << CPTR_EL2_TFP_SHIFT)
 #define CPTR_EL2_TZ	(1 << 8)
-#define CPTR_EL2_DEFAULT	0x000033ff
+#define CPTR_EL2_RES1	0x000032ff /* known RES1 bits in CPTR_EL2 */
+#define CPTR_EL2_DEFAULT	CPTR_EL2_RES1
 
 /* Hyp Debug Configuration Register bits */
 #define MDCR_EL2_TPMS		(1 << 14)
@@ -237,5 +238,6 @@
 
 #define CPACR_EL1_FPEN		(3 << 20)
 #define CPACR_EL1_TTA		(1 << 28)
+#define CPACR_EL1_DEFAULT	(CPACR_EL1_FPEN | CPACR_EL1_ZEN_EL1EN)
 
 #endif /* __ARM64_KVM_ARM_H__ */

commit 672365649ccac68cf6fafecad1a7913951e7493b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:00 2017 +0000

    arm64/sve: System register and exception syndrome definitions
    
    The SVE architecture adds some system registers, ID register fields
    and a dedicated ESR exception class.
    
    This patch adds the appropriate definitions that will be needed by
    the kernel.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 61d694c2eae5..dbf05370169a 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -185,6 +185,7 @@
 #define CPTR_EL2_TCPAC	(1 << 31)
 #define CPTR_EL2_TTA	(1 << 20)
 #define CPTR_EL2_TFP	(1 << CPTR_EL2_TFP_SHIFT)
+#define CPTR_EL2_TZ	(1 << 8)
 #define CPTR_EL2_DEFAULT	0x000033ff
 
 /* Hyp Debug Configuration Register bits */

commit 621f48e40ee9b0100a802531069166d7d94796e0
Author: Tyler Baicar <tbaicar@codeaurora.org>
Date:   Wed Jun 21 12:17:14 2017 -0600

    arm/arm64: KVM: add guest SEA support
    
    Currently external aborts are unsupported by the guest abort
    handling. Add handling for SEAs so that the host kernel reports
    SEAs which occur in the guest kernel.
    
    When an SEA occurs in the guest kernel, the guest exits and is
    routed to kvm_handle_guest_abort(). Prior to this patch, a print
    message of an unsupported FSC would be printed and nothing else
    would happen. With this patch, the code gets routed to the APEI
    handling of SEAs in the host kernel to report the SEA information.
    
    Signed-off-by: Tyler Baicar <tbaicar@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 6e99978e83bd..61d694c2eae5 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -204,6 +204,16 @@
 #define FSC_FAULT	ESR_ELx_FSC_FAULT
 #define FSC_ACCESS	ESR_ELx_FSC_ACCESS
 #define FSC_PERM	ESR_ELx_FSC_PERM
+#define FSC_SEA		ESR_ELx_FSC_EXTABT
+#define FSC_SEA_TTW0	(0x14)
+#define FSC_SEA_TTW1	(0x15)
+#define FSC_SEA_TTW2	(0x16)
+#define FSC_SEA_TTW3	(0x17)
+#define FSC_SECC	(0x18)
+#define FSC_SECC_TTW0	(0x1c)
+#define FSC_SECC_TTW1	(0x1d)
+#define FSC_SECC_TTW2	(0x1e)
+#define FSC_SECC_TTW3	(0x1f)
 
 /* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */
 #define HPFAR_MASK	(~UL(0xf))

commit f85279b4bd481a1a0697c1d2a8a5f15de216b120
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 22 11:35:43 2016 +0100

    arm64: KVM: Save/restore the host SPE state when entering/leaving a VM
    
    The SPE buffer is virtually addressed, using the page tables of the CPU
    MMU. Unusually, this means that the EL0/1 page table may be live whilst
    we're executing at EL2 on non-VHE configurations. When VHE is in use,
    we can use the same property to profile the guest behind its back.
    
    This patch adds the relevant disabling and flushing code to KVM so that
    the host can make use of SPE without corrupting guest memory, and any
    attempts by a guest to use SPE will result in a trap.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 2a2752b5b6aa..6e99978e83bd 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -188,6 +188,9 @@
 #define CPTR_EL2_DEFAULT	0x000033ff
 
 /* Hyp Debug Configuration Register bits */
+#define MDCR_EL2_TPMS		(1 << 14)
+#define MDCR_EL2_E2PB_MASK	(UL(0x3))
+#define MDCR_EL2_E2PB_SHIFT	(UL(12))
 #define MDCR_EL2_TDRA		(1 << 11)
 #define MDCR_EL2_TDOSA		(1 << 10)
 #define MDCR_EL2_TDA		(1 << 9)

commit 7b17145ec7de9267e24038c0621c9ed47853ae4f
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 14:01:59 2016 +0100

    arm64: KVM: Rename HCR_VA to HCR_VSE
    
    HCR_VA is a leftover from ARMv7, On ARMv8, this is HCR_VSE
    (which stands for Virtual System Error), and has better
    defined semantics.
    
    Let's rename the constant.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 4b5c977af465..2a2752b5b6aa 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -50,7 +50,7 @@
 #define HCR_BSU		(3 << 10)
 #define HCR_BSU_IS	(UL(1) << 10)
 #define HCR_FB		(UL(1) << 9)
-#define HCR_VA		(UL(1) << 8)
+#define HCR_VSE		(UL(1) << 8)
 #define HCR_VI		(UL(1) << 7)
 #define HCR_VF		(UL(1) << 6)
 #define HCR_AMO		(UL(1) << 5)
@@ -80,7 +80,7 @@
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
 			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
 			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW)
-#define HCR_VIRT_EXCP_MASK (HCR_VA | HCR_VI | HCR_VF)
+#define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)
 #define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 

commit edce2292c1e026c6a2da6899c114d930bc1f518b
Author: Andrea Gelmini <andrea.gelmini@gelma.net>
Date:   Sat May 21 13:53:14 2016 +0200

    KVM: ARM64: Fix typos
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 2cdb6b551ac6..4b5c977af465 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -178,7 +178,7 @@
 /* Hyp System Trap Register */
 #define HSTR_EL2_T(x)	(1 << x)
 
-/* Hyp Coproccessor Trap Register Shifts */
+/* Hyp Coprocessor Trap Register Shifts */
 #define CPTR_EL2_TFP_SHIFT 10
 
 /* Hyp Coprocessor Trap Register */

commit 7beaa24ba49717419e24d1f6321e8b3c265a719c
Merge: 07b75260ebc2 9842df62004f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 19 11:27:09 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small release overall.
    
      x86:
       - miscellaneous fixes
       - AVIC support (local APIC virtualization, AMD version)
    
      s390:
       - polling for interrupts after a VCPU goes to halted state is now
         enabled for s390
       - use hardware provided information about facility bits that do not
         need any hypervisor activity, and other fixes for cpu models and
         facilities
       - improve perf output
       - floating interrupt controller improvements.
    
      MIPS:
       - miscellaneous fixes
    
      PPC:
       - bugfixes only
    
      ARM:
       - 16K page size support
       - generic firmware probing layer for timer and GIC
    
      Christoffer Dall (KVM-ARM maintainer) says:
        "There are a few changes in this pull request touching things
         outside KVM, but they should all carry the necessary acks and it
         made the merge process much easier to do it this way."
    
      though actually the irqchip maintainers' acks didn't make it into the
      patches.  Marc Zyngier, who is both irqchip and KVM-ARM maintainer,
      later acked at http://mid.gmane.org/573351D1.4060303@arm.com ('more
      formally and for documentation purposes')"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (82 commits)
      KVM: MTRR: remove MSR 0x2f8
      KVM: x86: make hwapic_isr_update and hwapic_irr_update look the same
      svm: Manage vcpu load/unload when enable AVIC
      svm: Do not intercept CR8 when enable AVIC
      svm: Do not expose x2APIC when enable AVIC
      KVM: x86: Introducing kvm_x86_ops.apicv_post_state_restore
      svm: Add VMEXIT handlers for AVIC
      svm: Add interrupt injection via AVIC
      KVM: x86: Detect and Initialize AVIC support
      svm: Introduce new AVIC VMCB registers
      KVM: split kvm_vcpu_wake_up from kvm_vcpu_kick
      KVM: x86: Introducing kvm_x86_ops VCPU blocking/unblocking hooks
      KVM: x86: Introducing kvm_x86_ops VM init/destroy hooks
      KVM: x86: Rename kvm_apic_get_reg to kvm_lapic_get_reg
      KVM: x86: Misc LAPIC changes to expose helper functions
      KVM: shrink halt polling even more for invalid wakeups
      KVM: s390: set halt polling to 80 microseconds
      KVM: halt_polling: provide a way to qualify wakeups during poll
      KVM: PPC: Book3S HV: Re-enable XICS fast path for irqfd-generated interrupts
      kvm: Conditionally register IRQ bypass consumer
      ...

commit be092017b6ffbd013f481f915632db6aa9fc3ca3
Merge: fb6363e9f4ee e6d9a5254333
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 17:17:24 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
    
     - virt_to_page/page_address optimisations
    
     - support for NUMA systems described using device-tree
    
     - support for hibernate/suspend-to-disk
    
     - proper support for maxcpus= command line parameter
    
     - detection and graceful handling of AArch64-only CPUs
    
     - miscellaneous cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      arm64: do not enforce strict 16 byte alignment to stack pointer
      arm64: kernel: Fix incorrect brk randomization
      arm64: cpuinfo: Missing NULL terminator in compat_hwcap_str
      arm64: secondary_start_kernel: Remove unnecessary barrier
      arm64: Ensure pmd_present() returns false after pmd_mknotpresent()
      arm64: Replace hard-coded values in the pmd/pud_bad() macros
      arm64: Implement pmdp_set_access_flags() for hardware AF/DBM
      arm64: Fix typo in the pmdp_huge_get_and_clear() definition
      arm64: mm: remove unnecessary EXPORT_SYMBOL_GPL
      arm64: always use STRICT_MM_TYPECHECKS
      arm64: kvm: Fix kvm teardown for systems using the extended idmap
      arm64: kaslr: increase randomization granularity
      arm64: kconfig: drop CONFIG_RTC_LIB dependency
      arm64: make ARCH_SUPPORTS_DEBUG_PAGEALLOC depend on !HIBERNATION
      arm64: hibernate: Refuse to hibernate if the boot cpu is offline
      arm64: kernel: Add support for hibernate/suspend-to-disk
      PM / Hibernate: Call flush_icache_range() on pages restored in-place
      arm64: Add new asm macro copy_page
      arm64: Promote KERNEL_START/KERNEL_END definitions to a header file
      arm64: kernel: Include _AC definition in page.h
      ...

commit 06485053244480f5f403d8f89b8617bd7d549113
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 13 17:57:37 2016 +0100

    kvm: arm64: Enable hardware updates of the Access Flag for Stage 2 page tables
    
    The ARMv8.1 architecture extensions introduce support for hardware
    updates of the access and dirty information in page table entries. With
    VTCR_EL2.HA enabled (bit 21), when the CPU accesses an IPA with the
    PTE_AF bit cleared in the stage 2 page table, instead of raising an
    Access Flag fault to EL2 the CPU sets the actual page table entry bit
    (10). To ensure that kernel modifications to the page table do not
    inadvertently revert a bit set by hardware updates, certain Stage 2
    software pte/pmd operations must be performed atomically.
    
    The main user of the AF bit is the kvm_age_hva() mechanism. The
    kvm_age_hva_handler() function performs a "test and clear young" action
    on the pte/pmd. This needs to be atomic in respect of automatic hardware
    updates of the AF bit. Since the AF bit is in the same position for both
    Stage 1 and Stage 2, the patch reuses the existing
    ptep_test_and_clear_young() functionality if
    __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG is defined. Otherwise, the
    existing pte_young/pte_mkold mechanism is preserved.
    
    The kvm_set_s2pte_readonly() (and the corresponding pmd equivalent) have
    to perform atomic modifications in order to avoid a race with updates of
    the AF bit. The arm64 implementation has been re-written using
    exclusives.
    
    Currently, kvm_set_s2pte_writable() (and pmd equivalent) take a pointer
    argument and modify the pte/pmd in place. However, these functions are
    only used on local variables rather than actual page table entries, so
    it makes more sense to follow the pte_mkwrite() approach for stage 1
    attributes. The change to kvm_s2pte_mkwrite() makes it clear that these
    functions do not modify the actual page table entries.
    
    The (pte|pmd)_mkyoung() uses on Stage 2 entries (setting the AF bit
    explicitly) do not need to be modified since hardware updates of the
    dirty status are not supported by KVM, so there is no possibility of
    losing such information.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index c6cbb361bbcf..ffde15fed3e1 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -111,6 +111,8 @@
 
 /* VTCR_EL2 Registers bits */
 #define VTCR_EL2_RES1		(1 << 31)
+#define VTCR_EL2_HD		(1 << 22)
+#define VTCR_EL2_HA		(1 << 21)
 #define VTCR_EL2_PS_MASK	TCR_EL2_PS_MASK
 #define VTCR_EL2_TG0_MASK	TCR_TG0_MASK
 #define VTCR_EL2_TG0_4K		TCR_TG0_4K

commit e7227d0e528f9a96d4a866f43e20dd9b33f0e782
Author: Geoff Levand <geoff@infradead.org>
Date:   Wed Apr 27 17:47:01 2016 +0100

    arm64: Cleanup SCTLR flags
    
    We currently have macros defining flags for the arm64 sctlr registers in
    both kvm_arm.h and sysreg.h.  To clean things up and simplify move the
    definitions of the SCTLR_EL2 flags from kvm_arm.h to sysreg.h, rename any
    SCTLR_EL1 or SCTLR_EL2 flags that are common to both registers to be
    SCTLR_ELx, with 'x' indicating a common flag, and fixup all files to
    include the proper header or to use the new macro names.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    [Restored pgtable-hwdef.h include]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 4150fd8bae01..763762e0868a 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -84,17 +84,6 @@
 #define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 
-/* Hyp System Control Register (SCTLR_EL2) bits */
-#define SCTLR_EL2_EE	(1 << 25)
-#define SCTLR_EL2_WXN	(1 << 19)
-#define SCTLR_EL2_I	(1 << 12)
-#define SCTLR_EL2_SA	(1 << 3)
-#define SCTLR_EL2_C	(1 << 2)
-#define SCTLR_EL2_A	(1 << 1)
-#define SCTLR_EL2_M	1
-#define SCTLR_EL2_FLAGS	(SCTLR_EL2_M | SCTLR_EL2_A | SCTLR_EL2_C |	\
-			 SCTLR_EL2_SA | SCTLR_EL2_I)
-
 /* TCR_EL2 Registers bits */
 #define TCR_EL2_RES1	((1 << 31) | (1 << 23))
 #define TCR_EL2_TBI	(1 << 20)

commit 02e0b7600f8350078f01328095c20dd715700921
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Thu Mar 17 14:29:24 2016 +0000

    arm64: kvm: Add support for 16K pages
    
    Now that we can handle stage-2 page tables independent
    of the host page table levels, wire up the 16K page
    support.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 1281d98392a0..c6cbb361bbcf 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -114,6 +114,7 @@
 #define VTCR_EL2_PS_MASK	TCR_EL2_PS_MASK
 #define VTCR_EL2_TG0_MASK	TCR_TG0_MASK
 #define VTCR_EL2_TG0_4K		TCR_TG0_4K
+#define VTCR_EL2_TG0_16K	TCR_TG0_16K
 #define VTCR_EL2_TG0_64K	TCR_TG0_64K
 #define VTCR_EL2_SH0_MASK	TCR_SH0_MASK
 #define VTCR_EL2_SH0_INNER	TCR_SH0_INNER
@@ -139,7 +140,7 @@
  * (see hyp-init.S).
  *
  * Note that when using 4K pages, we concatenate two first level page tables
- * together.
+ * together. With 16K pages, we concatenate 16 first level page tables.
  *
  * The magic numbers used for VTTBR_X in this patch can be found in Tables
  * D4-23 and D4-25 in ARM DDI 0487A.b.
@@ -157,7 +158,15 @@
  */
 #define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SL0_LVL1)
 #define VTTBR_X_TGRAN_MAGIC		38
-#else
+#elif defined(CONFIG_ARM64_16K_PAGES)
+/*
+ * Stage2 translation configuration:
+ * 16kB pages (TG0 = 2)
+ * 2 level page tables (SL = 1)
+ */
+#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_16K | VTCR_EL2_SL0_LVL1)
+#define VTTBR_X_TGRAN_MAGIC		42
+#else	/* 4K */
 /*
  * Stage2 translation configuration:
  * 4kB pages (TG0 = 0)

commit acd05010400215b281a9197a889bec3e67998654
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Apr 4 11:53:52 2016 +0100

    arm64: Cleanup VTCR_EL2 and VTTBR field values
    
    We share most of the bits for VTCR_EL2 for different page sizes,
    except for the TG0 value and the entry level value. This patch
    makes the definitions a bit more cleaner to reflect this fact.
    
    Also cleans up the VTTBR_X calculation. No functional changes.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index a46b39f62426..1281d98392a0 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -144,30 +144,32 @@
  * The magic numbers used for VTTBR_X in this patch can be found in Tables
  * D4-23 and D4-25 in ARM DDI 0487A.b.
  */
+
+#define VTCR_EL2_T0SZ_IPA	VTCR_EL2_T0SZ_40B
+#define VTCR_EL2_COMMON_BITS	(VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
+				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_RES1)
+
 #ifdef CONFIG_ARM64_64K_PAGES
 /*
  * Stage2 translation configuration:
- * 40bits input  (T0SZ = 24)
  * 64kB pages (TG0 = 1)
  * 2 level page tables (SL = 1)
  */
-#define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SH0_INNER | \
-				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
-				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_RES1)
-#define VTTBR_X		(38 - VTCR_EL2_T0SZ_40B)
+#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SL0_LVL1)
+#define VTTBR_X_TGRAN_MAGIC		38
 #else
 /*
  * Stage2 translation configuration:
- * 40bits input  (T0SZ = 24)
  * 4kB pages (TG0 = 0)
  * 3 level page tables (SL = 1)
  */
-#define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SH0_INNER | \
-				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
-				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_RES1)
-#define VTTBR_X		(37 - VTCR_EL2_T0SZ_40B)
+#define VTCR_EL2_TGRAN_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SL0_LVL1)
+#define VTTBR_X_TGRAN_MAGIC		37
 #endif
 
+#define VTCR_EL2_FLAGS			(VTCR_EL2_COMMON_BITS | VTCR_EL2_TGRAN_FLAGS)
+#define VTTBR_X				(VTTBR_X_TGRAN_MAGIC - VTCR_EL2_T0SZ_IPA)
+
 #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
 #define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
 #define VTTBR_VMID_SHIFT  (UL(48))

commit a563f7598198b8389e00451ef6f3f1c12efbfb99
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Apr 4 11:43:15 2016 +0100

    arm64: Reuse TCR field definitions for EL1 and EL2
    
    TCR_EL1, TCR_EL2 and VTCR_EL2, all share some field positions
    (TG0, ORGN0, IRGN0 and SH0) and their corresponding value definitions.
    
    This patch makes the TCR_EL1 definitions reusable and uses them for TCR_EL2
    and VTCR_EL2 fields.
    
    This also fixes a bug where we assume TG0 in {V}TCR_EL2 is 1bit field.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 3f29887995bc..a46b39f62426 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -96,32 +96,34 @@
 			 SCTLR_EL2_SA | SCTLR_EL2_I)
 
 /* TCR_EL2 Registers bits */
-#define TCR_EL2_RES1	((1 << 31) | (1 << 23))
-#define TCR_EL2_TBI	(1 << 20)
-#define TCR_EL2_PS	(7 << 16)
-#define TCR_EL2_PS_40B	(2 << 16)
-#define TCR_EL2_TG0	(1 << 14)
-#define TCR_EL2_SH0	(3 << 12)
-#define TCR_EL2_ORGN0	(3 << 10)
-#define TCR_EL2_IRGN0	(3 << 8)
-#define TCR_EL2_T0SZ	0x3f
-#define TCR_EL2_MASK	(TCR_EL2_TG0 | TCR_EL2_SH0 | \
-			 TCR_EL2_ORGN0 | TCR_EL2_IRGN0 | TCR_EL2_T0SZ)
+#define TCR_EL2_RES1		((1 << 31) | (1 << 23))
+#define TCR_EL2_TBI		(1 << 20)
+#define TCR_EL2_PS_SHIFT	16
+#define TCR_EL2_PS_MASK		(7 << TCR_EL2_PS_SHIFT)
+#define TCR_EL2_PS_40B		(2 << TCR_EL2_PS_SHIFT)
+#define TCR_EL2_TG0_MASK	TCR_TG0_MASK
+#define TCR_EL2_SH0_MASK	TCR_SH0_MASK
+#define TCR_EL2_ORGN0_MASK	TCR_ORGN0_MASK
+#define TCR_EL2_IRGN0_MASK	TCR_IRGN0_MASK
+#define TCR_EL2_T0SZ_MASK	0x3f
+#define TCR_EL2_MASK	(TCR_EL2_TG0_MASK | TCR_EL2_SH0_MASK | \
+			 TCR_EL2_ORGN0_MASK | TCR_EL2_IRGN0_MASK | TCR_EL2_T0SZ_MASK)
 
 /* VTCR_EL2 Registers bits */
 #define VTCR_EL2_RES1		(1 << 31)
-#define VTCR_EL2_PS_MASK	(7 << 16)
-#define VTCR_EL2_TG0_MASK	(1 << 14)
-#define VTCR_EL2_TG0_4K		(0 << 14)
-#define VTCR_EL2_TG0_64K	(1 << 14)
-#define VTCR_EL2_SH0_MASK	(3 << 12)
-#define VTCR_EL2_SH0_INNER	(3 << 12)
-#define VTCR_EL2_ORGN0_MASK	(3 << 10)
-#define VTCR_EL2_ORGN0_WBWA	(1 << 10)
-#define VTCR_EL2_IRGN0_MASK	(3 << 8)
-#define VTCR_EL2_IRGN0_WBWA	(1 << 8)
-#define VTCR_EL2_SL0_MASK	(3 << 6)
-#define VTCR_EL2_SL0_LVL1	(1 << 6)
+#define VTCR_EL2_PS_MASK	TCR_EL2_PS_MASK
+#define VTCR_EL2_TG0_MASK	TCR_TG0_MASK
+#define VTCR_EL2_TG0_4K		TCR_TG0_4K
+#define VTCR_EL2_TG0_64K	TCR_TG0_64K
+#define VTCR_EL2_SH0_MASK	TCR_SH0_MASK
+#define VTCR_EL2_SH0_INNER	TCR_SH0_INNER
+#define VTCR_EL2_ORGN0_MASK	TCR_ORGN0_MASK
+#define VTCR_EL2_ORGN0_WBWA	TCR_ORGN0_WBWA
+#define VTCR_EL2_IRGN0_MASK	TCR_IRGN0_MASK
+#define VTCR_EL2_IRGN0_WBWA	TCR_IRGN0_WBWA
+#define VTCR_EL2_SL0_SHIFT	6
+#define VTCR_EL2_SL0_MASK	(3 << VTCR_EL2_SL0_SHIFT)
+#define VTCR_EL2_SL0_LVL1	(1 << VTCR_EL2_SL0_SHIFT)
 #define VTCR_EL2_T0SZ_MASK	0x3f
 #define VTCR_EL2_T0SZ_40B	24
 #define VTCR_EL2_VS_SHIFT	19

commit 6141570c36f0c937d5deff20d9cf08cbd8d8ed48
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 5 16:11:47 2016 +0100

    arm64: KVM: Warn when PARange is less than 40 bits
    
    We always thought that 40bits of PA range would be the minimum people
    would actually build. Anything less is terrifyingly small.
    
    Turns out that we were both right and wrong. Nobody has ever built
    such a system, but the ARM Foundation Model has a PARange set to 36bits.
    Just because we can. Oh well. Now, the KVM API explicitely says that
    we offer a 40bit PA space to the VM, so we shouldn't run KVM on
    the Foundation Model at all.
    
    That being said, this patch offers a less agressive alternative, and
    loudly warns about the configuration being unsupported. You'll still
    be able to run VMs (at your own risks, though).
    
    This is just a workaround until we have a proper userspace API where
    we report the PARange to userspace.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 4150fd8bae01..3f29887995bc 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -151,8 +151,7 @@
  */
 #define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SH0_INNER | \
 				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
-				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B | \
-				 VTCR_EL2_RES1)
+				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_RES1)
 #define VTTBR_X		(38 - VTCR_EL2_T0SZ_40B)
 #else
 /*
@@ -163,8 +162,7 @@
  */
 #define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SH0_INNER | \
 				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
-				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B | \
-				 VTCR_EL2_RES1)
+				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_RES1)
 #define VTTBR_X		(37 - VTCR_EL2_T0SZ_40B)
 #endif
 

commit cb678d6016510cc8c6572dd9f426fe74dd4eda84
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Mar 30 14:33:59 2016 +0100

    arm64: kvm: 4.6-rc1: Fix VTCR_EL2 VS setting
    
    When we detect support for 16bit VMID in ID_AA64MMFR1, we set the
    VTCR_EL2_VS field to 1 to make use of 16bit vmids. But, with
    commit 3a3604bc5eb4 ("arm64: KVM: Switch to C-based stage2 init")
    this is broken and we corrupt VTCR_EL2:T0SZ instead of updating the VS
    field. VTCR_EL2_VS was actually defined to the field shift (19) and
    not the real value for VS. This patch fixes the issue.
    
    Fixes: commit 3a3604bc5eb4 ("arm64: KVM: Switch to C-based stage2 init")
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 0e391dbfc420..4150fd8bae01 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -124,7 +124,9 @@
 #define VTCR_EL2_SL0_LVL1	(1 << 6)
 #define VTCR_EL2_T0SZ_MASK	0x3f
 #define VTCR_EL2_T0SZ_40B	24
-#define VTCR_EL2_VS		19
+#define VTCR_EL2_VS_SHIFT	19
+#define VTCR_EL2_VS_8BIT	(0 << VTCR_EL2_VS_SHIFT)
+#define VTCR_EL2_VS_16BIT	(1 << VTCR_EL2_VS_SHIFT)
 
 /*
  * We configure the Stage-2 page tables to always restrict the IPA space to be

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit ef769e320863a186e489e3f66ed8df60487fe9bf
Author: Adam Buchbinder <adam.buchbinder@gmail.com>
Date:   Wed Feb 24 09:52:41 2016 -0800

    arm64: Fix misspellings in comments.
    
    Signed-off-by: Adam Buchbinder <adam.buchbinder@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index bef6e9243c63..3d30c1532578 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -61,7 +61,7 @@
 
 /*
  * The bits we set in HCR:
- * RW:		64bit by default, can be overriden for 32bit VMs
+ * RW:		64bit by default, can be overridden for 32bit VMs
  * TAC:		Trap ACTLR
  * TSC:		Trap SMC
  * TVM:		Trap VM ops (until M+C set in SCTLR_EL1)

commit 68908bf789b7fd376538a4bad8367d5dcb9ec983
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jan 29 15:47:55 2015 +0000

    arm64: KVM: VHE: Implement VHE activate/deactivate_traps
    
    Running the kernel in HYP mode requires the HCR_E2H bit to be set
    at all times, and the HCR_TGE bit to be set when running as a host
    (and cleared when running as a guest). At the same time, the vector
     must be set to the current role of the kernel (either host or
    hypervisor), and a couple of system registers differ between VHE
    and non-VHE.
    
    We implement these by using another set of alternate functions
    that get dynamically patched.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index afa2f4a96210..b56a0a81e4cb 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -23,6 +23,7 @@
 #include <asm/types.h>
 
 /* Hyp Configuration Register (HCR) bits */
+#define HCR_E2H		(UL(1) << 34)
 #define HCR_ID		(UL(1) << 33)
 #define HCR_CD		(UL(1) << 32)
 #define HCR_RW_SHIFT	31
@@ -81,7 +82,7 @@
 			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW)
 #define HCR_VIRT_EXCP_MASK (HCR_VA | HCR_VI | HCR_VF)
 #define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
-
+#define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 
 /* Hyp System Control Register (SCTLR_EL2) bits */
 #define SCTLR_EL2_EE	(1 << 25)

commit 328762247cd33b4533f9dd89a4faf40288f359b7
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Oct 28 14:15:45 2015 +0000

    arm64: KVM: VHE: Make __fpsimd_enabled VHE aware
    
    As non-VHE and VHE have different ways to express the trapping of
    FPSIMD registers to EL2, make __fpsimd_enabled a patchable predicate
    and provide a VHE implementation.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index d201d4b396d1..afa2f4a96210 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -216,4 +216,7 @@
 	ECN(SOFTSTP_CUR), ECN(WATCHPT_LOW), ECN(WATCHPT_CUR), \
 	ECN(BKPT32), ECN(VECTOR32), ECN(BRK64)
 
+#define CPACR_EL1_FPEN		(3 << 20)
+#define CPACR_EL1_TTA		(1 << 28)
+
 #endif /* __ARM64_KVM_ARM_H__ */

commit 3c5b1d92b3b02be07873d611a27950addff544d3
Author: Tirumalesh Chalamarla <tchalamarla@caviumnetworks.com>
Date:   Wed Feb 10 10:46:53 2016 -0800

    arm64: KVM: Configure TCR_EL2.PS at runtime
    
    Setting TCR_EL2.PS to 40 bits is wrong on systems with less that
    less than 40 bits of physical addresses. and breaks KVM on systems
    where the RAM is above 40 bits.
    
    This patch uses ID_AA64MMFR0_EL1.PARange to set TCR_EL2.PS dynamically,
    just like we already do for VTCR_EL2.PS.
    
    [Marc: rewrote commit message, patch tidy up]
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Tirumalesh Chalamarla <tchalamarla@caviumnetworks.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index bef6e9243c63..d201d4b396d1 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -107,8 +107,6 @@
 #define TCR_EL2_MASK	(TCR_EL2_TG0 | TCR_EL2_SH0 | \
 			 TCR_EL2_ORGN0 | TCR_EL2_IRGN0 | TCR_EL2_T0SZ)
 
-#define TCR_EL2_FLAGS	(TCR_EL2_RES1 | TCR_EL2_PS_40B)
-
 /* VTCR_EL2 Registers bits */
 #define VTCR_EL2_RES1		(1 << 31)
 #define VTCR_EL2_PS_MASK	(7 << 16)

commit a7e0ac295d964086af3bf98352614f33c381213e
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jan 19 16:20:18 2016 +0000

    arm64: KVM: Obey RES0/1 reserved bits when setting CPTR_EL2
    
    Some bits in CPTR are defined as RES1 in the architecture.  Setting
    these bits to zero may unintentionally enable future architecture
    extensions, allowing guests to use them without supervision by the host.
    
    This would be bad: for forwards compatibility, this patch makes
    sure the affected bits are always written with 1, not 0.
    
    This patch only addresses CPTR_EL2.  Initialisation of other system
    registers may still need review.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 738a95f93e49..bef6e9243c63 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -182,6 +182,7 @@
 #define CPTR_EL2_TCPAC	(1 << 31)
 #define CPTR_EL2_TTA	(1 << 20)
 #define CPTR_EL2_TFP	(1 << CPTR_EL2_TFP_SHIFT)
+#define CPTR_EL2_DEFAULT	0x000033ff
 
 /* Hyp Debug Configuration Register bits */
 #define MDCR_EL2_TDRA		(1 << 11)

commit 20475f784d29991b3b843c80c38a36f2ebb35ac4
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Nov 16 11:28:18 2015 +0000

    arm64: KVM: Add support for 16-bit VMID
    
    The ARMv8.1 architecture extension allows to choose between 8-bit and
    16-bit of VMID, so use this capability for KVM.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 5e6857b6bdc4..738a95f93e49 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -125,6 +125,7 @@
 #define VTCR_EL2_SL0_LVL1	(1 << 6)
 #define VTCR_EL2_T0SZ_MASK	0x3f
 #define VTCR_EL2_T0SZ_40B	24
+#define VTCR_EL2_VS		19
 
 /*
  * We configure the Stage-2 page tables to always restrict the IPA space to be
@@ -169,7 +170,7 @@
 #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
 #define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
 #define VTTBR_VMID_SHIFT  (UL(48))
-#define VTTBR_VMID_MASK	  (UL(0xFF) << VTTBR_VMID_SHIFT)
+#define VTTBR_VMID_MASK(size) (_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
 
 /* Hyp System Trap Register */
 #define HSTR_EL2_T(x)	(1 << x)

commit b5905dc12ed4254f7e0aac62bab48f002181f639
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Sun Aug 30 15:55:22 2015 +0200

    arm/arm64: KVM: Improve kvm_exit tracepoint
    
    The ARM architecture only saves the exit class to the HSR (ESR_EL2 for
    arm64) on synchronous exceptions, not on asynchronous exceptions like an
    IRQ.  However, we only report the exception class on kvm_exit, which is
    confusing because an IRQ looks like it exited at some PC with the same
    reason as the previous exit.  Add a lookup table for the exception index
    and prepend the kvm_exit tracepoint text with the exception type to
    clarify this situation.
    
    Also resolve the exception class (EC) to a human-friendly text version
    so the trace output becomes immediately usable for debugging this code.
    
    Cc: Wei Huang <wei@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 9694f2654593..5e6857b6bdc4 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -200,4 +200,20 @@
 /* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */
 #define HPFAR_MASK	(~UL(0xf))
 
+#define kvm_arm_exception_type	\
+	{0, "IRQ" }, 		\
+	{1, "TRAP" }
+
+#define ECN(x) { ESR_ELx_EC_##x, #x }
+
+#define kvm_arm_exception_class \
+	ECN(UNKNOWN), ECN(WFx), ECN(CP15_32), ECN(CP15_64), ECN(CP14_MR), \
+	ECN(CP14_LS), ECN(FP_ASIMD), ECN(CP10_ID), ECN(CP14_64), ECN(SVC64), \
+	ECN(HVC64), ECN(SMC64), ECN(SYS64), ECN(IMP_DEF), ECN(IABT_LOW), \
+	ECN(IABT_CUR), ECN(PC_ALIGN), ECN(DABT_LOW), ECN(DABT_CUR), \
+	ECN(SP_ALIGN), ECN(FP_EXC32), ECN(FP_EXC64), ECN(SERROR), \
+	ECN(BREAKPT_LOW), ECN(BREAKPT_CUR), ECN(SOFTSTP_LOW), \
+	ECN(SOFTSTP_CUR), ECN(WATCHPT_LOW), ECN(WATCHPT_CUR), \
+	ECN(BKPT32), ECN(VECTOR32), ECN(BRK64)
+
 #endif /* __ARM64_KVM_ARM_H__ */

commit 34c3faa353db8f5d3ce9966cf854d5643c64c4db
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 15 17:15:33 2015 +0100

    arm64: KVM: Remove all traces of the ThumbEE registers
    
    Although the ThumbEE registers and traps were present in earlier
    versions of the v8 architecture, it was retrospectively removed and so
    we can do the same.
    
    Whilst this breaks migrating a guest started on a previous version of
    the kernel, it is much better to kill these (non existent) registers
    as soon as possible.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [maz: added commend about migration]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index cbc5e1a0ea8f..9694f2654593 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -172,7 +172,6 @@
 #define VTTBR_VMID_MASK	  (UL(0xFF) << VTTBR_VMID_SHIFT)
 
 /* Hyp System Trap Register */
-#define HSTR_EL2_TTEE	(1 << 16)
 #define HSTR_EL2_T(x)	(1 << x)
 
 /* Hyp Coproccessor Trap Register Shifts */

commit 857d1a973077245f03b351e2539529c86267bfe4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Aug 24 14:42:05 2015 +0100

    arm64: KVM: set {v,}TCR_EL2 RES1 bits
    
    Currently we don't set the RES1 bits of TCR_EL2 and VTCR_EL2 when
    configuring them, which could lead to unexpected behaviour when an
    architectural meaning is defined for those bits.
    
    Set the RES1 bits to avoid issues.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 7605e095217f..cbc5e1a0ea8f 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -95,6 +95,7 @@
 			 SCTLR_EL2_SA | SCTLR_EL2_I)
 
 /* TCR_EL2 Registers bits */
+#define TCR_EL2_RES1	((1 << 31) | (1 << 23))
 #define TCR_EL2_TBI	(1 << 20)
 #define TCR_EL2_PS	(7 << 16)
 #define TCR_EL2_PS_40B	(2 << 16)
@@ -106,9 +107,10 @@
 #define TCR_EL2_MASK	(TCR_EL2_TG0 | TCR_EL2_SH0 | \
 			 TCR_EL2_ORGN0 | TCR_EL2_IRGN0 | TCR_EL2_T0SZ)
 
-#define TCR_EL2_FLAGS	(TCR_EL2_PS_40B)
+#define TCR_EL2_FLAGS	(TCR_EL2_RES1 | TCR_EL2_PS_40B)
 
 /* VTCR_EL2 Registers bits */
+#define VTCR_EL2_RES1		(1 << 31)
 #define VTCR_EL2_PS_MASK	(7 << 16)
 #define VTCR_EL2_TG0_MASK	(1 << 14)
 #define VTCR_EL2_TG0_4K		(0 << 14)
@@ -147,7 +149,8 @@
  */
 #define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SH0_INNER | \
 				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
-				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B)
+				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B | \
+				 VTCR_EL2_RES1)
 #define VTTBR_X		(38 - VTCR_EL2_T0SZ_40B)
 #else
 /*
@@ -158,7 +161,8 @@
  */
 #define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SH0_INNER | \
 				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
-				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B)
+				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B | \
+				 VTCR_EL2_RES1)
 #define VTTBR_X		(37 - VTCR_EL2_T0SZ_40B)
 #endif
 

commit 33c76a0b8126946026fa1122cb774553afae0956
Author: Mario Smarduch <m.smarduch@samsung.com>
Date:   Thu Jul 16 22:29:37 2015 +0100

    arm64: KVM: Optimize arm64 skip 30-50% vfp/simd save/restore on exits
    
    This patch only saves and restores FP/SIMD registers on Guest access. To do
    this cptr_el2 FP/SIMD trap is set on Guest entry and later checked on exit.
    lmbench, hackbench show significant improvements, for 30-50% exits FP/SIMD
    context is not saved/restored
    
    [chazy/maz: fixed save/restore logic for 32bit guests]
    
    Signed-off-by: Mario Smarduch <m.smarduch@samsung.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index ac6fafb95fe7..7605e095217f 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -171,10 +171,13 @@
 #define HSTR_EL2_TTEE	(1 << 16)
 #define HSTR_EL2_T(x)	(1 << x)
 
+/* Hyp Coproccessor Trap Register Shifts */
+#define CPTR_EL2_TFP_SHIFT 10
+
 /* Hyp Coprocessor Trap Register */
 #define CPTR_EL2_TCPAC	(1 << 31)
 #define CPTR_EL2_TTA	(1 << 20)
-#define CPTR_EL2_TFP	(1 << 10)
+#define CPTR_EL2_TFP	(1 << CPTR_EL2_TFP_SHIFT)
 
 /* Hyp Debug Configuration Register bits */
 #define MDCR_EL2_TDRA		(1 << 11)

commit bf0fb67cf957fc8ecfaaa2819b7d6a0f795e2ef2
Merge: 8999602d08a8 d44758c0dfc5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Apr 7 18:09:20 2015 +0200

    Merge tag 'kvm-arm-for-4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into 'kvm-next'
    
    KVM/ARM changes for v4.1:
    
    - fixes for live migration
    - irqfd support
    - kvm-io-bus & vgic rework to enable ioeventfd
    - page ageing for stage-2 translation
    - various cleanups

commit 35307b9a5f7ebcc8d8db41c73b69c131b48ace2b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Mar 12 18:16:51 2015 +0000

    arm/arm64: KVM: Implement Stage-2 page aging
    
    Until now, KVM/arm didn't care much for page aging (who was swapping
    anyway?), and simply provided empty hooks to the core KVM code. With
    server-type systems now being available, things are quite different.
    
    This patch implements very simple support for page aging, by clearing
    the Access flag in the Stage-2 page tables. On access fault, the current
    fault handling will write the PTE or PMD again, putting the Access flag
    back on.
    
    It should be possible to implement a much faster handling for Access
    faults, but that's left for a later patch.
    
    With this in place, performance in VMs is degraded much more gracefully.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 94674eb7e7bb..9e5543e08955 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -187,6 +187,7 @@
 
 /* For compatibility with fault code shared with 32-bit */
 #define FSC_FAULT	ESR_ELx_FSC_FAULT
+#define FSC_ACCESS	ESR_ELx_FSC_ACCESS
 #define FSC_PERM	ESR_ELx_FSC_PERM
 
 /* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */

commit 84ed7412b5eee1011579b3db7454b9cb6d26fa65
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Mar 10 19:07:01 2015 +0000

    arm64: KVM: Fix outdated comment about VTCR_EL2.PS
    
    Commit 87366d8cf7b3 ("arm64: Add boot time configuration of
    Intermediate Physical Address size") removed the hardcoded setting
    of VTCR_EL2.PS to use ID_AA64MMFR0_EL1.PARange instead, but didn't
    remove the (now rather misleading) comment.
    
    Fix the comments to match reality (at least for the next few minutes).
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 94674eb7e7bb..54bb4ba97441 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -129,6 +129,9 @@
  * 40 bits wide (T0SZ = 24).  Systems with a PARange smaller than 40 bits are
  * not known to exist and will break with this configuration.
  *
+ * VTCR_EL2.PS is extracted from ID_AA64MMFR0_EL1.PARange at boot time
+ * (see hyp-init.S).
+ *
  * Note that when using 4K pages, we concatenate two first level page tables
  * together.
  *
@@ -138,7 +141,6 @@
 #ifdef CONFIG_ARM64_64K_PAGES
 /*
  * Stage2 translation configuration:
- * 40bits output (PS = 2)
  * 40bits input  (T0SZ = 24)
  * 64kB pages (TG0 = 1)
  * 2 level page tables (SL = 1)
@@ -150,7 +152,6 @@
 #else
 /*
  * Stage2 translation configuration:
- * 40bits output (PS = 2)
  * 40bits input  (T0SZ = 24)
  * 4kB pages (TG0 = 0)
  * 3 level page tables (SL = 1)

commit 6e53031ed840fc6d6ad4d4e5778eed5a50183f23
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Nov 24 14:05:44 2014 +0000

    arm64: kvm: remove ESR_EL2_* macros
    
    Now that all users have been moved over to the common ESR_ELx_* macros,
    remove the redundant ESR_EL2 macros. To maintain compatibility with the
    fault handling code shared with 32-bit, the FSC_{FAULT,PERM} macros are
    retained as aliases for the common ESR_ELx_FSC_{FAULT,PERM} definitions.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 8afb863f5a9e..94674eb7e7bb 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -18,6 +18,7 @@
 #ifndef __ARM64_KVM_ARM_H__
 #define __ARM64_KVM_ARM_H__
 
+#include <asm/esr.h>
 #include <asm/memory.h>
 #include <asm/types.h>
 
@@ -184,77 +185,11 @@
 #define MDCR_EL2_TPMCR		(1 << 5)
 #define MDCR_EL2_HPMN_MASK	(0x1F)
 
-/* Exception Syndrome Register (ESR) bits */
-#define ESR_EL2_EC_SHIFT	(26)
-#define ESR_EL2_EC		(UL(0x3f) << ESR_EL2_EC_SHIFT)
-#define ESR_EL2_IL		(UL(1) << 25)
-#define ESR_EL2_ISS		(ESR_EL2_IL - 1)
-#define ESR_EL2_ISV_SHIFT	(24)
-#define ESR_EL2_ISV		(UL(1) << ESR_EL2_ISV_SHIFT)
-#define ESR_EL2_SAS_SHIFT	(22)
-#define ESR_EL2_SAS		(UL(3) << ESR_EL2_SAS_SHIFT)
-#define ESR_EL2_SSE		(1 << 21)
-#define ESR_EL2_SRT_SHIFT	(16)
-#define ESR_EL2_SRT_MASK	(0x1f << ESR_EL2_SRT_SHIFT)
-#define ESR_EL2_SF 		(1 << 15)
-#define ESR_EL2_AR 		(1 << 14)
-#define ESR_EL2_EA 		(1 << 9)
-#define ESR_EL2_CM 		(1 << 8)
-#define ESR_EL2_S1PTW 		(1 << 7)
-#define ESR_EL2_WNR		(1 << 6)
-#define ESR_EL2_FSC		(0x3f)
-#define ESR_EL2_FSC_TYPE	(0x3c)
-
-#define ESR_EL2_CV_SHIFT	(24)
-#define ESR_EL2_CV		(UL(1) << ESR_EL2_CV_SHIFT)
-#define ESR_EL2_COND_SHIFT	(20)
-#define ESR_EL2_COND		(UL(0xf) << ESR_EL2_COND_SHIFT)
-
-
-#define FSC_FAULT	(0x04)
-#define FSC_PERM	(0x0c)
+/* For compatibility with fault code shared with 32-bit */
+#define FSC_FAULT	ESR_ELx_FSC_FAULT
+#define FSC_PERM	ESR_ELx_FSC_PERM
 
 /* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */
 #define HPFAR_MASK	(~UL(0xf))
 
-#define ESR_EL2_EC_UNKNOWN	(0x00)
-#define ESR_EL2_EC_WFI		(0x01)
-#define ESR_EL2_EC_CP15_32	(0x03)
-#define ESR_EL2_EC_CP15_64	(0x04)
-#define ESR_EL2_EC_CP14_MR	(0x05)
-#define ESR_EL2_EC_CP14_LS	(0x06)
-#define ESR_EL2_EC_FP_ASIMD	(0x07)
-#define ESR_EL2_EC_CP10_ID	(0x08)
-#define ESR_EL2_EC_CP14_64	(0x0C)
-#define ESR_EL2_EC_ILL_ISS	(0x0E)
-#define ESR_EL2_EC_SVC32	(0x11)
-#define ESR_EL2_EC_HVC32	(0x12)
-#define ESR_EL2_EC_SMC32	(0x13)
-#define ESR_EL2_EC_SVC64	(0x15)
-#define ESR_EL2_EC_HVC64	(0x16)
-#define ESR_EL2_EC_SMC64	(0x17)
-#define ESR_EL2_EC_SYS64	(0x18)
-#define ESR_EL2_EC_IABT		(0x20)
-#define ESR_EL2_EC_IABT_HYP	(0x21)
-#define ESR_EL2_EC_PC_ALIGN	(0x22)
-#define ESR_EL2_EC_DABT		(0x24)
-#define ESR_EL2_EC_DABT_HYP	(0x25)
-#define ESR_EL2_EC_SP_ALIGN	(0x26)
-#define ESR_EL2_EC_FP_EXC32	(0x28)
-#define ESR_EL2_EC_FP_EXC64	(0x2C)
-#define ESR_EL2_EC_SERROR	(0x2F)
-#define ESR_EL2_EC_BREAKPT	(0x30)
-#define ESR_EL2_EC_BREAKPT_HYP	(0x31)
-#define ESR_EL2_EC_SOFTSTP	(0x32)
-#define ESR_EL2_EC_SOFTSTP_HYP	(0x33)
-#define ESR_EL2_EC_WATCHPT	(0x34)
-#define ESR_EL2_EC_WATCHPT_HYP	(0x35)
-#define ESR_EL2_EC_BKPT32	(0x38)
-#define ESR_EL2_EC_VECTOR32	(0x3A)
-#define ESR_EL2_EC_BRK64	(0x3C)
-
-#define ESR_EL2_EC_xABT_xFSR_EXTABT	0x10
-
-#define ESR_EL2_EC_WFI_ISS_WFE	(1 << 0)
-
 #endif /* __ARM64_KVM_ARM_H__ */

commit 286fb1cc32b11c18da3573a8c8c37a4f9da16e30
Author: Geoff Levand <geoff@infradead.org>
Date:   Fri Oct 31 23:06:47 2014 +0000

    arm64/kvm: Fix assembler compatibility of macros
    
    Some of the macros defined in kvm_arm.h are useful in assembly files, but are
    not compatible with the assembler.  Change any C language integer constant
    definitions using appended U, UL, or ULL to the UL() preprocessor macro.  Also,
    add a preprocessor include of the asm/memory.h file which defines the UL()
    macro.
    
    Fixes build errors like these when using kvm_arm.h in assembly
    source files:
    
      Error: unexpected characters following instruction at operand 3 -- `and x0,x1,#((1U<<25)-1)'
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 7fd3e27e3ccc..8afb863f5a9e 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -18,6 +18,7 @@
 #ifndef __ARM64_KVM_ARM_H__
 #define __ARM64_KVM_ARM_H__
 
+#include <asm/memory.h>
 #include <asm/types.h>
 
 /* Hyp Configuration Register (HCR) bits */
@@ -160,9 +161,9 @@
 #endif
 
 #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
-#define VTTBR_BADDR_MASK  (((1LLU << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
-#define VTTBR_VMID_SHIFT  (48LLU)
-#define VTTBR_VMID_MASK	  (0xffLLU << VTTBR_VMID_SHIFT)
+#define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+#define VTTBR_VMID_SHIFT  (UL(48))
+#define VTTBR_VMID_MASK	  (UL(0xFF) << VTTBR_VMID_SHIFT)
 
 /* Hyp System Trap Register */
 #define HSTR_EL2_TTEE	(1 << 16)
@@ -185,13 +186,13 @@
 
 /* Exception Syndrome Register (ESR) bits */
 #define ESR_EL2_EC_SHIFT	(26)
-#define ESR_EL2_EC		(0x3fU << ESR_EL2_EC_SHIFT)
-#define ESR_EL2_IL		(1U << 25)
+#define ESR_EL2_EC		(UL(0x3f) << ESR_EL2_EC_SHIFT)
+#define ESR_EL2_IL		(UL(1) << 25)
 #define ESR_EL2_ISS		(ESR_EL2_IL - 1)
 #define ESR_EL2_ISV_SHIFT	(24)
-#define ESR_EL2_ISV		(1U << ESR_EL2_ISV_SHIFT)
+#define ESR_EL2_ISV		(UL(1) << ESR_EL2_ISV_SHIFT)
 #define ESR_EL2_SAS_SHIFT	(22)
-#define ESR_EL2_SAS		(3U << ESR_EL2_SAS_SHIFT)
+#define ESR_EL2_SAS		(UL(3) << ESR_EL2_SAS_SHIFT)
 #define ESR_EL2_SSE		(1 << 21)
 #define ESR_EL2_SRT_SHIFT	(16)
 #define ESR_EL2_SRT_MASK	(0x1f << ESR_EL2_SRT_SHIFT)
@@ -205,16 +206,16 @@
 #define ESR_EL2_FSC_TYPE	(0x3c)
 
 #define ESR_EL2_CV_SHIFT	(24)
-#define ESR_EL2_CV		(1U << ESR_EL2_CV_SHIFT)
+#define ESR_EL2_CV		(UL(1) << ESR_EL2_CV_SHIFT)
 #define ESR_EL2_COND_SHIFT	(20)
-#define ESR_EL2_COND		(0xfU << ESR_EL2_COND_SHIFT)
+#define ESR_EL2_COND		(UL(0xf) << ESR_EL2_COND_SHIFT)
 
 
 #define FSC_FAULT	(0x04)
 #define FSC_PERM	(0x0c)
 
 /* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */
-#define HPFAR_MASK	(~0xFUL)
+#define HPFAR_MASK	(~UL(0xf))
 
 #define ESR_EL2_EC_UNKNOWN	(0x00)
 #define ESR_EL2_EC_WFI		(0x01)

commit dbff124e29fa24aff9705b354b5f4648cd96e0bb
Author: Joel Schopp <joel.schopp@amd.com>
Date:   Wed Jul 9 11:17:04 2014 -0500

    arm/arm64: KVM: Fix VTTBR_BADDR_MASK and pgd alloc
    
    The current aarch64 calculation for VTTBR_BADDR_MASK masks only 39 bits
    and not all the bits in the PA range. This is clearly a bug that
    manifests itself on systems that allocate memory in the higher address
    space range.
    
     [ Modified from Joel's original patch to be based on PHYS_MASK_SHIFT
       instead of a hard-coded value and to move the alignment check of the
       allocation to mmu.c.  Also added a comment explaining why we hardcode
       the IPA range and changed the stage-2 pgd allocation to be based on
       the 40 bit IPA range instead of the maximum possible 48 bit PA range.
       - Christoffer ]
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Joel Schopp <joel.schopp@amd.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index cc83520459ed..7fd3e27e3ccc 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -122,6 +122,17 @@
 #define VTCR_EL2_T0SZ_MASK	0x3f
 #define VTCR_EL2_T0SZ_40B	24
 
+/*
+ * We configure the Stage-2 page tables to always restrict the IPA space to be
+ * 40 bits wide (T0SZ = 24).  Systems with a PARange smaller than 40 bits are
+ * not known to exist and will break with this configuration.
+ *
+ * Note that when using 4K pages, we concatenate two first level page tables
+ * together.
+ *
+ * The magic numbers used for VTTBR_X in this patch can be found in Tables
+ * D4-23 and D4-25 in ARM DDI 0487A.b.
+ */
 #ifdef CONFIG_ARM64_64K_PAGES
 /*
  * Stage2 translation configuration:
@@ -149,7 +160,7 @@
 #endif
 
 #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
-#define VTTBR_BADDR_MASK  (((1LLU << (40 - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+#define VTTBR_BADDR_MASK  (((1LLU << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
 #define VTTBR_VMID_SHIFT  (48LLU)
 #define VTTBR_VMID_MASK	  (0xffLLU << VTTBR_VMID_SHIFT)
 

commit ac3c3747e2db2f326ffc601651de544cdd33a8e9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Aug 9 18:19:11 2013 +0100

    arm64: KVM: move HCR_EL2.{IMO,FMO} manipulation into the vgic switch code
    
    GICv3 requires the IMO and FMO bits to be tightly coupled with some
    of the interrupt controller's register switch.
    
    In order to have similar code paths, move the manipulation of these
    bits to the GICv2 switch code.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 3d6903006a8a..cc83520459ed 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -76,9 +76,10 @@
  */
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
 			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
-			 HCR_AMO | HCR_IMO | HCR_FMO | \
-			 HCR_SWIO | HCR_TIDCP | HCR_RW)
+			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW)
 #define HCR_VIRT_EXCP_MASK (HCR_VA | HCR_VI | HCR_VF)
+#define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
+
 
 /* Hyp System Control Register (SCTLR_EL2) bits */
 #define SCTLR_EL2_EE	(1 << 25)

commit 7cbb39d4d4d530dff12f2ff06ed6c85c504ba91a
Merge: 64056a94256e 7227fc066660
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 2 14:50:10 2014 -0700

    Merge tag 'kvm-3.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "PPC and ARM do not have much going on this time.  Most of the cool
      stuff, instead, is in s390 and (after a few releases) x86.
    
      ARM has some caching fixes and PPC has transactional memory support in
      guests.  MIPS has some fixes, with more probably coming in 3.16 as
      QEMU will soon get support for MIPS KVM.
    
      For x86 there are optimizations for debug registers, which trigger on
      some Windows games, and other important fixes for Windows guests.  We
      now expose to the guest Broadwell instruction set extensions and also
      Intel MPX.  There's also a fix/workaround for OS X guests, nested
      virtualization features (preemption timer), and a couple kvmclock
      refinements.
    
      For s390, the main news is asynchronous page faults, together with
      improvements to IRQs (floating irqs and adapter irqs) that speed up
      virtio devices"
    
    * tag 'kvm-3.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (96 commits)
      KVM: PPC: Book3S HV: Save/restore host PMU registers that are new in POWER8
      KVM: PPC: Book3S HV: Fix decrementer timeouts with non-zero TB offset
      KVM: PPC: Book3S HV: Don't use kvm_memslots() in real mode
      KVM: PPC: Book3S HV: Return ENODEV error rather than EIO
      KVM: PPC: Book3S: Trim top 4 bits of physical address in RTAS code
      KVM: PPC: Book3S HV: Add get/set_one_reg for new TM state
      KVM: PPC: Book3S HV: Add transactional memory support
      KVM: Specify byte order for KVM_EXIT_MMIO
      KVM: vmx: fix MPX detection
      KVM: PPC: Book3S HV: Fix KVM hang with CONFIG_KVM_XICS=n
      KVM: PPC: Book3S: Introduce hypervisor call H_GET_TCE
      KVM: PPC: Book3S HV: Fix incorrect userspace exit on ioeventfd write
      KVM: s390: clear local interrupts at cpu initial reset
      KVM: s390: Fix possible memory leak in SIGP functions
      KVM: s390: fix calculation of idle_mask array size
      KVM: s390: randomize sca address
      KVM: ioapic: reinject pending interrupts on KVM_SET_IRQCHIP
      KVM: Bump KVM_MAX_IRQ_ROUTES for s390
      KVM: s390: irq routing for adapter interrupts.
      KVM: s390: adapter interrupt sources
      ...

commit 87366d8cf7b3f6dc34633938aa8766e5a390ce33
Author: Radha Mohan Chintakuntla <rchintakuntla@cavium.com>
Date:   Fri Mar 7 08:49:25 2014 +0000

    arm64: Add boot time configuration of Intermediate Physical Address size
    
    ARMv8 supports a range of physical address bit sizes. The PARange bits
    from ID_AA64MMFR0_EL1 register are read during boot-time and the
    intermediate physical address size bits are written in the translation
    control registers (TCR_EL1 and VTCR_EL2).
    
    There is no change in the VA bits and levels of translation.
    
    Signed-off-by: Radha Mohan Chintakuntla <rchintakuntla@cavium.com>
    Reviewed-by: Will Deacon <Will.deacon@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 0eb398655378..21ef48d32ff2 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -106,7 +106,6 @@
 
 /* VTCR_EL2 Registers bits */
 #define VTCR_EL2_PS_MASK	(7 << 16)
-#define VTCR_EL2_PS_40B		(2 << 16)
 #define VTCR_EL2_TG0_MASK	(1 << 14)
 #define VTCR_EL2_TG0_4K		(0 << 14)
 #define VTCR_EL2_TG0_64K	(1 << 14)
@@ -129,10 +128,9 @@
  * 64kB pages (TG0 = 1)
  * 2 level page tables (SL = 1)
  */
-#define VTCR_EL2_FLAGS		(VTCR_EL2_PS_40B | VTCR_EL2_TG0_64K | \
-				 VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
-				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_SL0_LVL1 | \
-				 VTCR_EL2_T0SZ_40B)
+#define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_64K | VTCR_EL2_SH0_INNER | \
+				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
+				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B)
 #define VTTBR_X		(38 - VTCR_EL2_T0SZ_40B)
 #else
 /*
@@ -142,10 +140,9 @@
  * 4kB pages (TG0 = 0)
  * 3 level page tables (SL = 1)
  */
-#define VTCR_EL2_FLAGS		(VTCR_EL2_PS_40B | VTCR_EL2_TG0_4K | \
-				 VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
-				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_SL0_LVL1 | \
-				 VTCR_EL2_T0SZ_40B)
+#define VTCR_EL2_FLAGS		(VTCR_EL2_TG0_4K | VTCR_EL2_SH0_INNER | \
+				 VTCR_EL2_ORGN0_WBWA | VTCR_EL2_IRGN0_WBWA | \
+				 VTCR_EL2_SL0_LVL1 | VTCR_EL2_T0SZ_40B)
 #define VTTBR_X		(37 - VTCR_EL2_T0SZ_40B)
 #endif
 

commit 4d44923b17bff283c002ed961373848284aaff1b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Jan 14 18:00:55 2014 +0000

    arm64: KVM: trap VM system registers until MMU and caches are ON
    
    In order to be able to detect the point where the guest enables
    its MMU and caches, trap all the VM related system registers.
    
    Once we see the guest enabling both the MMU and the caches, we
    can go back to a saner mode of operation, which is to leave these
    registers in complete control of the guest.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 0eb398655378..00fbaa75dc7b 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -62,6 +62,7 @@
  * RW:		64bit by default, can be overriden for 32bit VMs
  * TAC:		Trap ACTLR
  * TSC:		Trap SMC
+ * TVM:		Trap VM ops (until M+C set in SCTLR_EL1)
  * TSW:		Trap cache operations by set/way
  * TWE:		Trap WFE
  * TWI:		Trap WFI
@@ -74,7 +75,7 @@
  * SWIO:	Turn set/way invalidates into set/way clean+invalidate
  */
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
-			 HCR_BSU_IS | HCR_FB | HCR_TAC | \
+			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
 			 HCR_AMO | HCR_IMO | HCR_FMO | \
 			 HCR_SWIO | HCR_TIDCP | HCR_RW)
 #define HCR_VIRT_EXCP_MASK (HCR_VA | HCR_VI | HCR_VF)

commit bfb67a5606376bb32cb6f93dc05cda2e8c2038a5
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Feb 5 10:24:12 2014 +0000

    arm64: fix typo: s/SERRROR/SERROR/
    
    Somehow SERROR has acquired an additional 'R' in a couple of headers.
    This patch removes them before they spread further. As neither instance
    is in use yet, no other sites need to be fixed up.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index c98ef4771c73..0eb398655378 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -231,7 +231,7 @@
 #define ESR_EL2_EC_SP_ALIGN	(0x26)
 #define ESR_EL2_EC_FP_EXC32	(0x28)
 #define ESR_EL2_EC_FP_EXC64	(0x2C)
-#define ESR_EL2_EC_SERRROR	(0x2F)
+#define ESR_EL2_EC_SERROR	(0x2F)
 #define ESR_EL2_EC_BREAKPT	(0x30)
 #define ESR_EL2_EC_BREAKPT_HYP	(0x31)
 #define ESR_EL2_EC_SOFTSTP	(0x32)

commit d241aac798eb042e605f78c31a4122e583b2cd13
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Aug 2 11:41:13 2013 +0100

    arm64: KVM: Yield CPU when vcpu executes a WFE
    
    On an (even slightly) oversubscribed system, spinlocks are quickly
    becoming a bottleneck, as some vcpus are spinning, waiting for a
    lock to be released, while the vcpu holding the lock may not be
    running at all.
    
    The solution is to trap blocking WFEs and tell KVM that we're
    now spinning. This ensures that other vpus will get a scheduling
    boost, allowing the lock to be released more quickly. Also, using
    CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT slightly improves the performance
    when the VM is severely overcommited.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index a5f28e2720c7..c98ef4771c73 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -63,6 +63,7 @@
  * TAC:		Trap ACTLR
  * TSC:		Trap SMC
  * TSW:		Trap cache operations by set/way
+ * TWE:		Trap WFE
  * TWI:		Trap WFI
  * TIDCP:	Trap L2CTLR/L2ECTLR
  * BSU_IS:	Upgrade barriers to the inner shareable domain
@@ -72,8 +73,9 @@
  * FMO:		Override CPSR.F and enable signaling with VF
  * SWIO:	Turn set/way invalidates into set/way clean+invalidate
  */
-#define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWI | HCR_VM | HCR_BSU_IS | \
-			 HCR_FB | HCR_TAC | HCR_AMO | HCR_IMO | HCR_FMO | \
+#define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
+			 HCR_BSU_IS | HCR_FB | HCR_TAC | \
+			 HCR_AMO | HCR_IMO | HCR_FMO | \
 			 HCR_SWIO | HCR_TIDCP | HCR_RW)
 #define HCR_VIRT_EXCP_MASK (HCR_VA | HCR_VI | HCR_VF)
 
@@ -242,4 +244,6 @@
 
 #define ESR_EL2_EC_xABT_xFSR_EXTABT	0x10
 
+#define ESR_EL2_EC_WFI_ISS_WFE	(1 << 0)
+
 #endif /* __ARM64_KVM_ARM_H__ */

commit 0369f6a34b9facd16eea4236518ca6f9cbc9e5ef
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Dec 10 10:46:47 2012 +0000

    arm64: KVM: EL2 register definitions
    
    Define all the useful bitfields for EL2 registers.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
new file mode 100644
index 000000000000..a5f28e2720c7
--- /dev/null
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -0,0 +1,245 @@
+/*
+ * Copyright (C) 2012,2013 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __ARM64_KVM_ARM_H__
+#define __ARM64_KVM_ARM_H__
+
+#include <asm/types.h>
+
+/* Hyp Configuration Register (HCR) bits */
+#define HCR_ID		(UL(1) << 33)
+#define HCR_CD		(UL(1) << 32)
+#define HCR_RW_SHIFT	31
+#define HCR_RW		(UL(1) << HCR_RW_SHIFT)
+#define HCR_TRVM	(UL(1) << 30)
+#define HCR_HCD		(UL(1) << 29)
+#define HCR_TDZ		(UL(1) << 28)
+#define HCR_TGE		(UL(1) << 27)
+#define HCR_TVM		(UL(1) << 26)
+#define HCR_TTLB	(UL(1) << 25)
+#define HCR_TPU		(UL(1) << 24)
+#define HCR_TPC		(UL(1) << 23)
+#define HCR_TSW		(UL(1) << 22)
+#define HCR_TAC		(UL(1) << 21)
+#define HCR_TIDCP	(UL(1) << 20)
+#define HCR_TSC		(UL(1) << 19)
+#define HCR_TID3	(UL(1) << 18)
+#define HCR_TID2	(UL(1) << 17)
+#define HCR_TID1	(UL(1) << 16)
+#define HCR_TID0	(UL(1) << 15)
+#define HCR_TWE		(UL(1) << 14)
+#define HCR_TWI		(UL(1) << 13)
+#define HCR_DC		(UL(1) << 12)
+#define HCR_BSU		(3 << 10)
+#define HCR_BSU_IS	(UL(1) << 10)
+#define HCR_FB		(UL(1) << 9)
+#define HCR_VA		(UL(1) << 8)
+#define HCR_VI		(UL(1) << 7)
+#define HCR_VF		(UL(1) << 6)
+#define HCR_AMO		(UL(1) << 5)
+#define HCR_IMO		(UL(1) << 4)
+#define HCR_FMO		(UL(1) << 3)
+#define HCR_PTW		(UL(1) << 2)
+#define HCR_SWIO	(UL(1) << 1)
+#define HCR_VM		(UL(1) << 0)
+
+/*
+ * The bits we set in HCR:
+ * RW:		64bit by default, can be overriden for 32bit VMs
+ * TAC:		Trap ACTLR
+ * TSC:		Trap SMC
+ * TSW:		Trap cache operations by set/way
+ * TWI:		Trap WFI
+ * TIDCP:	Trap L2CTLR/L2ECTLR
+ * BSU_IS:	Upgrade barriers to the inner shareable domain
+ * FB:		Force broadcast of all maintainance operations
+ * AMO:		Override CPSR.A and enable signaling with VA
+ * IMO:		Override CPSR.I and enable signaling with VI
+ * FMO:		Override CPSR.F and enable signaling with VF
+ * SWIO:	Turn set/way invalidates into set/way clean+invalidate
+ */
+#define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWI | HCR_VM | HCR_BSU_IS | \
+			 HCR_FB | HCR_TAC | HCR_AMO | HCR_IMO | HCR_FMO | \
+			 HCR_SWIO | HCR_TIDCP | HCR_RW)
+#define HCR_VIRT_EXCP_MASK (HCR_VA | HCR_VI | HCR_VF)
+
+/* Hyp System Control Register (SCTLR_EL2) bits */
+#define SCTLR_EL2_EE	(1 << 25)
+#define SCTLR_EL2_WXN	(1 << 19)
+#define SCTLR_EL2_I	(1 << 12)
+#define SCTLR_EL2_SA	(1 << 3)
+#define SCTLR_EL2_C	(1 << 2)
+#define SCTLR_EL2_A	(1 << 1)
+#define SCTLR_EL2_M	1
+#define SCTLR_EL2_FLAGS	(SCTLR_EL2_M | SCTLR_EL2_A | SCTLR_EL2_C |	\
+			 SCTLR_EL2_SA | SCTLR_EL2_I)
+
+/* TCR_EL2 Registers bits */
+#define TCR_EL2_TBI	(1 << 20)
+#define TCR_EL2_PS	(7 << 16)
+#define TCR_EL2_PS_40B	(2 << 16)
+#define TCR_EL2_TG0	(1 << 14)
+#define TCR_EL2_SH0	(3 << 12)
+#define TCR_EL2_ORGN0	(3 << 10)
+#define TCR_EL2_IRGN0	(3 << 8)
+#define TCR_EL2_T0SZ	0x3f
+#define TCR_EL2_MASK	(TCR_EL2_TG0 | TCR_EL2_SH0 | \
+			 TCR_EL2_ORGN0 | TCR_EL2_IRGN0 | TCR_EL2_T0SZ)
+
+#define TCR_EL2_FLAGS	(TCR_EL2_PS_40B)
+
+/* VTCR_EL2 Registers bits */
+#define VTCR_EL2_PS_MASK	(7 << 16)
+#define VTCR_EL2_PS_40B		(2 << 16)
+#define VTCR_EL2_TG0_MASK	(1 << 14)
+#define VTCR_EL2_TG0_4K		(0 << 14)
+#define VTCR_EL2_TG0_64K	(1 << 14)
+#define VTCR_EL2_SH0_MASK	(3 << 12)
+#define VTCR_EL2_SH0_INNER	(3 << 12)
+#define VTCR_EL2_ORGN0_MASK	(3 << 10)
+#define VTCR_EL2_ORGN0_WBWA	(1 << 10)
+#define VTCR_EL2_IRGN0_MASK	(3 << 8)
+#define VTCR_EL2_IRGN0_WBWA	(1 << 8)
+#define VTCR_EL2_SL0_MASK	(3 << 6)
+#define VTCR_EL2_SL0_LVL1	(1 << 6)
+#define VTCR_EL2_T0SZ_MASK	0x3f
+#define VTCR_EL2_T0SZ_40B	24
+
+#ifdef CONFIG_ARM64_64K_PAGES
+/*
+ * Stage2 translation configuration:
+ * 40bits output (PS = 2)
+ * 40bits input  (T0SZ = 24)
+ * 64kB pages (TG0 = 1)
+ * 2 level page tables (SL = 1)
+ */
+#define VTCR_EL2_FLAGS		(VTCR_EL2_PS_40B | VTCR_EL2_TG0_64K | \
+				 VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
+				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_SL0_LVL1 | \
+				 VTCR_EL2_T0SZ_40B)
+#define VTTBR_X		(38 - VTCR_EL2_T0SZ_40B)
+#else
+/*
+ * Stage2 translation configuration:
+ * 40bits output (PS = 2)
+ * 40bits input  (T0SZ = 24)
+ * 4kB pages (TG0 = 0)
+ * 3 level page tables (SL = 1)
+ */
+#define VTCR_EL2_FLAGS		(VTCR_EL2_PS_40B | VTCR_EL2_TG0_4K | \
+				 VTCR_EL2_SH0_INNER | VTCR_EL2_ORGN0_WBWA | \
+				 VTCR_EL2_IRGN0_WBWA | VTCR_EL2_SL0_LVL1 | \
+				 VTCR_EL2_T0SZ_40B)
+#define VTTBR_X		(37 - VTCR_EL2_T0SZ_40B)
+#endif
+
+#define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
+#define VTTBR_BADDR_MASK  (((1LLU << (40 - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+#define VTTBR_VMID_SHIFT  (48LLU)
+#define VTTBR_VMID_MASK	  (0xffLLU << VTTBR_VMID_SHIFT)
+
+/* Hyp System Trap Register */
+#define HSTR_EL2_TTEE	(1 << 16)
+#define HSTR_EL2_T(x)	(1 << x)
+
+/* Hyp Coprocessor Trap Register */
+#define CPTR_EL2_TCPAC	(1 << 31)
+#define CPTR_EL2_TTA	(1 << 20)
+#define CPTR_EL2_TFP	(1 << 10)
+
+/* Hyp Debug Configuration Register bits */
+#define MDCR_EL2_TDRA		(1 << 11)
+#define MDCR_EL2_TDOSA		(1 << 10)
+#define MDCR_EL2_TDA		(1 << 9)
+#define MDCR_EL2_TDE		(1 << 8)
+#define MDCR_EL2_HPME		(1 << 7)
+#define MDCR_EL2_TPM		(1 << 6)
+#define MDCR_EL2_TPMCR		(1 << 5)
+#define MDCR_EL2_HPMN_MASK	(0x1F)
+
+/* Exception Syndrome Register (ESR) bits */
+#define ESR_EL2_EC_SHIFT	(26)
+#define ESR_EL2_EC		(0x3fU << ESR_EL2_EC_SHIFT)
+#define ESR_EL2_IL		(1U << 25)
+#define ESR_EL2_ISS		(ESR_EL2_IL - 1)
+#define ESR_EL2_ISV_SHIFT	(24)
+#define ESR_EL2_ISV		(1U << ESR_EL2_ISV_SHIFT)
+#define ESR_EL2_SAS_SHIFT	(22)
+#define ESR_EL2_SAS		(3U << ESR_EL2_SAS_SHIFT)
+#define ESR_EL2_SSE		(1 << 21)
+#define ESR_EL2_SRT_SHIFT	(16)
+#define ESR_EL2_SRT_MASK	(0x1f << ESR_EL2_SRT_SHIFT)
+#define ESR_EL2_SF 		(1 << 15)
+#define ESR_EL2_AR 		(1 << 14)
+#define ESR_EL2_EA 		(1 << 9)
+#define ESR_EL2_CM 		(1 << 8)
+#define ESR_EL2_S1PTW 		(1 << 7)
+#define ESR_EL2_WNR		(1 << 6)
+#define ESR_EL2_FSC		(0x3f)
+#define ESR_EL2_FSC_TYPE	(0x3c)
+
+#define ESR_EL2_CV_SHIFT	(24)
+#define ESR_EL2_CV		(1U << ESR_EL2_CV_SHIFT)
+#define ESR_EL2_COND_SHIFT	(20)
+#define ESR_EL2_COND		(0xfU << ESR_EL2_COND_SHIFT)
+
+
+#define FSC_FAULT	(0x04)
+#define FSC_PERM	(0x0c)
+
+/* Hyp Prefetch Fault Address Register (HPFAR/HDFAR) */
+#define HPFAR_MASK	(~0xFUL)
+
+#define ESR_EL2_EC_UNKNOWN	(0x00)
+#define ESR_EL2_EC_WFI		(0x01)
+#define ESR_EL2_EC_CP15_32	(0x03)
+#define ESR_EL2_EC_CP15_64	(0x04)
+#define ESR_EL2_EC_CP14_MR	(0x05)
+#define ESR_EL2_EC_CP14_LS	(0x06)
+#define ESR_EL2_EC_FP_ASIMD	(0x07)
+#define ESR_EL2_EC_CP10_ID	(0x08)
+#define ESR_EL2_EC_CP14_64	(0x0C)
+#define ESR_EL2_EC_ILL_ISS	(0x0E)
+#define ESR_EL2_EC_SVC32	(0x11)
+#define ESR_EL2_EC_HVC32	(0x12)
+#define ESR_EL2_EC_SMC32	(0x13)
+#define ESR_EL2_EC_SVC64	(0x15)
+#define ESR_EL2_EC_HVC64	(0x16)
+#define ESR_EL2_EC_SMC64	(0x17)
+#define ESR_EL2_EC_SYS64	(0x18)
+#define ESR_EL2_EC_IABT		(0x20)
+#define ESR_EL2_EC_IABT_HYP	(0x21)
+#define ESR_EL2_EC_PC_ALIGN	(0x22)
+#define ESR_EL2_EC_DABT		(0x24)
+#define ESR_EL2_EC_DABT_HYP	(0x25)
+#define ESR_EL2_EC_SP_ALIGN	(0x26)
+#define ESR_EL2_EC_FP_EXC32	(0x28)
+#define ESR_EL2_EC_FP_EXC64	(0x2C)
+#define ESR_EL2_EC_SERRROR	(0x2F)
+#define ESR_EL2_EC_BREAKPT	(0x30)
+#define ESR_EL2_EC_BREAKPT_HYP	(0x31)
+#define ESR_EL2_EC_SOFTSTP	(0x32)
+#define ESR_EL2_EC_SOFTSTP_HYP	(0x33)
+#define ESR_EL2_EC_WATCHPT	(0x34)
+#define ESR_EL2_EC_WATCHPT_HYP	(0x35)
+#define ESR_EL2_EC_BKPT32	(0x38)
+#define ESR_EL2_EC_VECTOR32	(0x3A)
+#define ESR_EL2_EC_BRK64	(0x3C)
+
+#define ESR_EL2_EC_xABT_xFSR_EXTABT	0x10
+
+#endif /* __ARM64_KVM_ARM_H__ */
