commit 7733306bd593c737c63110175da6c35b4b8bb32c
Author: Alexandru Elisei <alexandru.elisei@arm.com>
Date:   Thu Jun 18 18:12:54 2020 +0100

    KVM: arm64: Annotate hyp NMI-related functions as __always_inline
    
    The "inline" keyword is a hint for the compiler to inline a function.  The
    functions system_uses_irq_prio_masking() and gic_write_pmr() are used by
    the code running at EL2 on a non-VHE system, so mark them as
    __always_inline to make sure they'll always be part of the .hyp.text
    section.
    
    This fixes the following splat when trying to run a VM:
    
    [   47.625273] Kernel panic - not syncing: HYP panic:
    [   47.625273] PS:a00003c9 PC:0000ca0b42049fc4 ESR:86000006
    [   47.625273] FAR:0000ca0b42049fc4 HPFAR:0000000010001000 PAR:0000000000000000
    [   47.625273] VCPU:0000000000000000
    [   47.647261] CPU: 1 PID: 217 Comm: kvm-vcpu-0 Not tainted 5.8.0-rc1-ARCH+ #61
    [   47.654508] Hardware name: Globalscale Marvell ESPRESSOBin Board (DT)
    [   47.661139] Call trace:
    [   47.663659]  dump_backtrace+0x0/0x1cc
    [   47.667413]  show_stack+0x18/0x24
    [   47.670822]  dump_stack+0xb8/0x108
    [   47.674312]  panic+0x124/0x2f4
    [   47.677446]  panic+0x0/0x2f4
    [   47.680407] SMP: stopping secondary CPUs
    [   47.684439] Kernel Offset: disabled
    [   47.688018] CPU features: 0x240402,20002008
    [   47.692318] Memory Limit: none
    [   47.695465] ---[ end Kernel panic - not syncing: HYP panic:
    [   47.695465] PS:a00003c9 PC:0000ca0b42049fc4 ESR:86000006
    [   47.695465] FAR:0000ca0b42049fc4 HPFAR:0000000010001000 PAR:0000000000000000
    [   47.695465] VCPU:0000000000000000 ]---
    
    The instruction abort was caused by the code running at EL2 trying to fetch
    an instruction which wasn't mapped in the EL2 translation tables. Using
    objdump showed the two functions as separate symbols in the .text section.
    
    Fixes: 85738e05dc38 ("arm64: kvm: Unmask PMR before entering guest")
    Cc: stable@vger.kernel.org
    Signed-off-by: Alexandru Elisei <alexandru.elisei@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: James Morse <james.morse@arm.com>
    Link: https://lore.kernel.org/r/20200618171254.1596055-1-alexandru.elisei@arm.com

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 5d1f4ae42799..f7c3d1ff091d 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -675,7 +675,7 @@ static inline bool system_supports_generic_auth(void)
 		cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH);
 }
 
-static inline bool system_uses_irq_prio_masking(void)
+static __always_inline bool system_uses_irq_prio_masking(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_PSEUDO_NMI) &&
 	       cpus_have_const_cap(ARM64_HAS_IRQ_PRIO_MASKING);

commit d27865279f12035c730818aa1a0280fada866a37
Merge: 342403bcb4df a4eb355a3fda
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:00:51 2020 +0100

    Merge branch 'for-next/bti' into for-next/core
    
    Support for Branch Target Identification (BTI) in user and kernel
    (Mark Brown and others)
    * for-next/bti: (39 commits)
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      arm64: bti: Fix support for userspace only BTI
      arm64: kconfig: Update and comment GCC version check for kernel BTI
      arm64: vdso: Map the vDSO text with guarded pages when built for BTI
      arm64: vdso: Force the vDSO to be linked as BTI when built for BTI
      arm64: vdso: Annotate for BTI
      arm64: asm: Provide a mechanism for generating ELF note for BTI
      arm64: bti: Provide Kconfig for kernel mode BTI
      arm64: mm: Mark executable text as guarded pages
      arm64: bpf: Annotate JITed code for BTI
      arm64: Set GP bit in kernel page tables to enable BTI for the kernel
      arm64: asm: Override SYM_FUNC_START when building the kernel with BTI
      arm64: bti: Support building kernel C code using BTI
      arm64: Document why we enable PAC support for leaf functions
      arm64: insn: Report PAC and BTI instructions as skippable
      arm64: insn: Don't assume unrecognized HINTs are skippable
      arm64: insn: Provide a better name for aarch64_insn_is_nop()
      arm64: insn: Add constants for new HINT instruction decode
      arm64: Disable old style assembly annotations
      ...

commit c73433fc630cda102f6527d4e5dfd289a9baec08
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 12 07:27:27 2020 +0530

    arm64/cpufeature: Validate hypervisor capabilities during CPU hotplug
    
    This validates hypervisor capabilities like VMID width, IPA range for any
    hot plug CPU against system finalized values. KVM's view of the IPA space
    is used while allowing a given CPU to come up. While here, it factors out
    get_vmid_bits() for general use.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/1589248647-22925-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index f5c4672e498b..928814d35669 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -752,6 +752,24 @@ static inline bool cpu_has_hw_af(void)
 extern bool cpu_has_amu_feat(int cpu);
 #endif
 
+static inline unsigned int get_vmid_bits(u64 mmfr1)
+{
+	int vmid_bits;
+
+	vmid_bits = cpuid_feature_extract_unsigned_field(mmfr1,
+						ID_AA64MMFR1_VMIDBITS_SHIFT);
+	if (vmid_bits == ID_AA64MMFR1_VMIDBITS_16)
+		return 16;
+
+	/*
+	 * Return the default here even if any reserved
+	 * value is fetched from the system register.
+	 */
+	return 8;
+}
+
+u32 get_kvm_ipa_limit(void);
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 80e4e561321595d2e5f4a173e8cf8d8432078995
Merge: 6a8b55ed4056 5d1b631c773f
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 15:15:58 2020 +0100

    Merge branch 'for-next/bti-user' into for-next/bti
    
    Merge in user support for Branch Target Identification, which narrowly
    missed the cut for 5.7 after a late ABI concern.
    
    * for-next/bti-user:
      arm64: bti: Document behaviour for dynamically linked binaries
      arm64: elf: Fix allnoconfig kernel build with !ARCH_USE_GNU_PROPERTY
      arm64: BTI: Add Kconfig entry for userspace BTI
      mm: smaps: Report arm64 guarded pages in smaps
      arm64: mm: Display guarded pages in ptdump
      KVM: arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: traps: Shuffle code to eliminate forward declarations
      arm64: unify native/compat instruction skipping
      arm64: BTI: Decode BYTPE bits when printing PSTATE
      arm64: elf: Enable BTI at exec based on ELF program properties
      elf: Allow arch to tweak initial mmap prot flags
      arm64: Basic Branch Target Identification support
      ELF: Add ELF program property parsing support
      ELF: UAPI and Kconfig additions for ELF program properties

commit eab2f92607461fc7fa9dba599772a4b214fd9d1a
Author: Will Deacon <will@kernel.org>
Date:   Tue Apr 21 15:29:20 2020 +0100

    arm64: cpufeature: Relax AArch32 system checks if EL1 is 64-bit only
    
    If AArch32 is not supported at EL1, the AArch32 feature register fields
    no longer advertise support for some system features:
    
      * ISAR4.SMC
      * PFR1.{Virt_frac, Sec_frac, Virtualization, Security, ProgMod}
    
    In which case, we don't need to emit "SANITY CHECK" failures for all of
    them.
    
    Add logic to relax the strictness of individual feature register fields
    at runtime and use this for the fields above if 32-bit EL1 is not
    supported.
    
    Tested-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Link: https://lore.kernel.org/r/20200421142922.18950-7-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index afe08251ff95..f5c4672e498b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -551,6 +551,13 @@ static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
 		cpuid_feature_extract_unsigned_field(mmfr0, ID_AA64MMFR0_BIGENDEL0_SHIFT) == 0x1;
 }
 
+static inline bool id_aa64pfr0_32bit_el1(u64 pfr0)
+{
+	u32 val = cpuid_feature_extract_unsigned_field(pfr0, ID_AA64PFR0_EL1_SHIFT);
+
+	return val == ID_AA64PFR0_EL1_32BIT_64BIT;
+}
+
 static inline bool id_aa64pfr0_32bit_el0(u64 pfr0)
 {
 	u32 val = cpuid_feature_extract_unsigned_field(pfr0, ID_AA64PFR0_EL0_SHIFT);

commit 3cd86a58f7734bf9cef38f6f899608ebcaa3da13
Merge: a8222fd5b80c b2a84de2a2de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 10:05:01 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The bulk is in-kernel pointer authentication, activity monitors and
      lots of asm symbol annotations. I also queued the sys_mremap() patch
      commenting the asymmetry in the address untagging.
    
      Summary:
    
       - In-kernel Pointer Authentication support (previously only offered
         to user space).
    
       - ARM Activity Monitors (AMU) extension support allowing better CPU
         utilisation numbers for the scheduler (frequency invariance).
    
       - Memory hot-remove support for arm64.
    
       - Lots of asm annotations (SYM_*) in preparation for the in-kernel
         Branch Target Identification (BTI) support.
    
       - arm64 perf updates: ARMv8.5-PMU 64-bit counters, refactoring the
         PMU init callbacks, support for new DT compatibles.
    
       - IPv6 header checksum optimisation.
    
       - Fixes: SDEI (software delegated exception interface) double-lock on
         hibernate with shared events.
    
       - Minor clean-ups and refactoring: cpu_ops accessor,
         cpu_do_switch_mm() converted to C, cpufeature finalisation helper.
    
       - sys_mremap() comment explaining the asymmetric address untagging
         behaviour"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (81 commits)
      mm/mremap: Add comment explaining the untagging behaviour of mremap()
      arm64: head: Convert install_el2_stub to SYM_INNER_LABEL
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
      arm64: move kimage_vaddr to .rodata
      arm64: use mov_q instead of literal ldr
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      ...

commit 44ca0e00b6a05ea9cf89d8a5290a225de19f4a2a
Merge: 806dc825f01f 3b446c7d27dd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:11:08 2020 +0000

    Merge branch 'for-next/kernel-ptrauth' into for-next/core
    
    * for-next/kernel-ptrauth:
      : Return address signing - in-kernel support
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      arm64: ptrauth: Add bootup/runtime flags for __cpu_setup
      arm64: install user ptrauth keys at kernel exit time
      arm64: rename ptrauth key structures to be user-specific
      arm64: cpufeature: add pointer auth meta-capabilities
      arm64: cpufeature: Fix meta-capability cpufeature check

commit da12d2739fb69531bf6bb6eb7e46d73d1dabc814
Merge: bbd6ec605c0f f7d5ef0c654e c265861af2af b5475d8caedb de58ed5e16e6 c17a290f7e7e 8673e02e5841
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:10:32 2020 +0000

    Merge branches 'for-next/memory-hotremove', 'for-next/arm_sdei', 'for-next/amu', 'for-next/final-cap-helper', 'for-next/cpu_ops-cleanup', 'for-next/misc' and 'for-next/perf' into for-next/core
    
    * for-next/memory-hotremove:
      : Memory hot-remove support for arm64
      arm64/mm: Enable memory hot remove
      arm64/mm: Hold memory hotplug lock while walking for kernel page table dump
    
    * for-next/arm_sdei:
      : SDEI: fix double locking on return from hibernate and clean-up
      firmware: arm_sdei: clean up sdei_event_create()
      firmware: arm_sdei: Use cpus_read_lock() to avoid races with cpuhp
      firmware: arm_sdei: fix possible double-lock on hibernate error path
      firmware: arm_sdei: fix double-lock on hibernate with shared events
    
    * for-next/amu:
      : ARMv8.4 Activity Monitors support
      clocksource/drivers/arm_arch_timer: validate arch_timer_rate
      arm64: use activity monitors for frequency invariance
      cpufreq: add function to get the hardware max frequency
      Documentation: arm64: document support for the AMU extension
      arm64/kvm: disable access to AMU registers from kvm guests
      arm64: trap to EL1 accesses to AMU counters from EL0
      arm64: add support for the AMU extension v1
    
    * for-next/final-cap-helper:
      : Introduce cpus_have_final_cap_helper(), migrate arm64 KVM to it
      arm64: kvm: hyp: use cpus_have_final_cap()
      arm64: cpufeature: add cpus_have_final_cap()
    
    * for-next/cpu_ops-cleanup:
      : cpu_ops[] access code clean-up
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: define __alloc_zeroed_user_highpage
      arm64/kernel: Simplify __cpu_up() by bailing out early
      arm64: remove redundant blank for '=' operator
      arm64: kexec_file: Fixed code style.
      arm64: add blank after 'if'
      arm64: fix spelling mistake "ca not" -> "cannot"
      arm64: entry: unmask IRQ in el0_sp()
      arm64: efi: add efi-entry.o to targets instead of extra-$(CONFIG_EFI)
      arm64: csum: Optimise IPv6 header checksum
      arch/arm64: fix typo in a comment
      arm64: remove gratuitious/stray .ltorg stanzas
      arm64: Update comment for ASID() macro
      arm64: mm: convert cpu_do_switch_mm() to C
      arm64: fix NUMA Kconfig typos
    
    * for-next/perf:
      : arm64 perf updates
      arm64: perf: Add support for ARMv8.5-PMU 64-bit counters
      KVM: arm64: limit PMU version to PMUv3 for ARMv8.1
      arm64: cpufeature: Extract capped perfmon fields
      arm64: perf: Clean up enable/disable calls
      perf: arm-ccn: Use scnprintf() for robustness
      arm64: perf: Support new DT compatibles
      arm64: perf: Refactor PMU init callbacks
      perf: arm_spe: Remove unnecessary zero check on 'nr_pages'

commit 6982934e19f8ebb4152ba77308facdb1a38533f9
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:55 2020 +0530

    arm64: enable ptrauth earlier
    
    When the kernel is compiled with pointer auth instructions, the boot CPU
    needs to start using address auth very early, so change the cpucap to
    account for this.
    
    Pointer auth must be enabled before we call C functions, because it is
    not possible to enter a function with pointer auth disabled and exit it
    with pointer auth enabled. Note, mismatches between architected and
    IMPDEF algorithms will still be caught by the cpufeature framework (the
    separate *_ARCH and *_IMP_DEF cpucaps).
    
    Note the change in behavior: if the boot CPU has address auth and a
    late CPU does not, then the late CPU is parked by the cpufeature
    framework. This is possible as kernel will only have NOP space intructions
    for PAC so such mismatched late cpu will silently ignore those
    instructions in C functions. Also, if the boot CPU does not have address
    auth and the late CPU has then the late cpu will still boot but with
    ptrauth feature disabled.
    
    Leave generic authentication as a "system scope" cpucap for now, since
    initially the kernel will only use address authentication.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Re-worked ptrauth setup logic, comments]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 46388e65bbcd..2b5a088053e4 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -291,6 +291,15 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 #define ARM64_CPUCAP_STRICT_BOOT_CPU_FEATURE		\
 	(ARM64_CPUCAP_SCOPE_BOOT_CPU | ARM64_CPUCAP_PANIC_ON_CONFLICT)
 
+/*
+ * CPU feature used early in the boot based on the boot CPU. It is safe for a
+ * late CPU to have this feature even though the boot CPU hasn't enabled it,
+ * although the feature will not be used by Linux in this case. If the boot CPU
+ * has enabled this feature already, then every late CPU must have it.
+ */
+#define ARM64_CPUCAP_BOOT_CPU_FEATURE                  \
+	(ARM64_CPUCAP_SCOPE_BOOT_CPU | ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU)
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;

commit deeaac5175a577cbbe1a2319903781d0a7ef7720
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:54 2020 +0530

    arm64: cpufeature: handle conflicts based on capability
    
    Each system capability can be of either boot, local, or system scope,
    depending on when the state of the capability is finalized. When we
    detect a conflict on a late CPU, we either offline the CPU or panic the
    system. We currently always panic if the conflict is caused by a boot
    scope capability, and offline the CPU if the conflict is caused by a
    local or system scope capability.
    
    We're going to want to add a new capability (for pointer authentication)
    which needs to be boot scope but doesn't need to panic the system when a
    conflict is detected. So add a new flag to specify whether the
    capability requires the system to panic or not. Current boot scope
    capabilities are updated to set the flag, so there should be no
    functional change as a result of this patch.
    
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Reviewed-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index cea3c1cdf252..46388e65bbcd 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -208,6 +208,10 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *     In some non-typical cases either both (a) and (b), or neither,
  *     should be permitted. This can be described by including neither
  *     or both flags in the capability's type field.
+ *
+ *     In case of a conflict, the CPU is prevented from booting. If the
+ *     ARM64_CPUCAP_PANIC_ON_CONFLICT flag is specified for the capability,
+ *     then a kernel panic is triggered.
  */
 
 
@@ -240,6 +244,8 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 #define ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU	((u16)BIT(4))
 /* Is it safe for a late CPU to miss this capability when system has it */
 #define ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU	((u16)BIT(5))
+/* Panic when a conflict is detected */
+#define ARM64_CPUCAP_PANIC_ON_CONFLICT		((u16)BIT(6))
 
 /*
  * CPU errata workarounds that need to be enabled at boot time if one or
@@ -279,9 +285,11 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 
 /*
  * CPU feature used early in the boot based on the boot CPU. All secondary
- * CPUs must match the state of the capability as detected by the boot CPU.
+ * CPUs must match the state of the capability as detected by the boot CPU. In
+ * case of a conflict, a kernel panic is triggered.
  */
-#define ARM64_CPUCAP_STRICT_BOOT_CPU_FEATURE ARM64_CPUCAP_SCOPE_BOOT_CPU
+#define ARM64_CPUCAP_STRICT_BOOT_CPU_FEATURE		\
+	(ARM64_CPUCAP_SCOPE_BOOT_CPU | ARM64_CPUCAP_PANIC_ON_CONFLICT)
 
 struct arm64_cpu_capabilities {
 	const char *desc;

commit 8c176e1625a66d35362d4eac7ceab55c1229b481
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Fri Mar 13 14:34:53 2020 +0530

    arm64: cpufeature: Move cpu capability helpers inside C file
    
    These helpers are used only by functions inside cpufeature.c and
    hence makes sense to be moved from cpufeature.h to cpufeature.c as
    they are not expected to be used globally.
    
    This change helps in reducing the header file size as well as to add
    future cpu capability types without confusion. Only a cpu capability
    type macro is sufficient to expose those capabilities globally.
    
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8c8048372c8e..cea3c1cdf252 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -340,18 +340,6 @@ static inline int cpucap_default_scope(const struct arm64_cpu_capabilities *cap)
 	return cap->type & ARM64_CPUCAP_SCOPE_MASK;
 }
 
-static inline bool
-cpucap_late_cpu_optional(const struct arm64_cpu_capabilities *cap)
-{
-	return !!(cap->type & ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU);
-}
-
-static inline bool
-cpucap_late_cpu_permitted(const struct arm64_cpu_capabilities *cap)
-{
-	return !!(cap->type & ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU);
-}
-
 /*
  * Generic helper for handling capabilties with multiple (match,enable) pairs
  * of call backs, sharing the same capability bit.

commit cfef06bd0686a578aa53e039c9aec0b1a5581d3b
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:49 2020 +0530

    arm64: cpufeature: add pointer auth meta-capabilities
    
    To enable pointer auth for the kernel, we're going to need to check for
    the presence of address auth and generic auth using alternative_if. We
    currently have two cpucaps for each, but alternative_if needs to check a
    single cpucap. So define meta-capabilities that are present when either
    of the current two capabilities is present.
    
    Leave the existing four cpucaps in place, as they are still needed to
    check for mismatched systems where one CPU has the architected algorithm
    but another has the IMP DEF algorithm.
    
    Note, the meta-capabilities were present before but were removed in
    commit a56005d32105 ("arm64: cpufeature: Reduce number of pointer auth
    CPU caps from 6 to 4") and commit 1e013d06120c ("arm64: cpufeature: Rework
    ptr auth hwcaps using multi_entry_cap_matches"), as they were not needed
    then. Note, unlike before, the current patch checks the cpucap values
    directly, instead of reading the CPU ID register value.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: commit message and macro rebase, use __system_matches_cap]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 92ef9539874a..8c8048372c8e 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -590,15 +590,13 @@ static inline bool system_supports_cnp(void)
 static inline bool system_supports_address_auth(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_PTR_AUTH) &&
-		(cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH_ARCH) ||
-		 cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH_IMP_DEF));
+		cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH);
 }
 
 static inline bool system_supports_generic_auth(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_PTR_AUTH) &&
-		(cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH_ARCH) ||
-		 cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH_IMP_DEF));
+		cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH);
 }
 
 static inline bool system_uses_irq_prio_masking(void)

commit 8e35aa642ee4dab01b16cc4b2df59d1936f3b3c2
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Mon Mar 2 18:17:50 2020 +0000

    arm64: cpufeature: Extract capped perfmon fields
    
    When emulating ID registers there is often a need to cap the version
    bits of a feature such that the guest will not use features that the
    host is not aware of. For example, when KVM mediates access to the PMU
    by emulating register accesses.
    
    Let's add a helper that extracts a performance monitors ID field and
    caps the version to a given value.
    
    Fields that identify the version of the Performance Monitors Extension
    do not follow the standard ID scheme, and instead follow the scheme
    described in ARM DDI 0487E.a page D13-2825 "Alternative ID scheme used
    for the Performance Monitors Extension version". The value 0xF means an
    IMPLEMENTATION DEFINED PMU is present, and values 0x0-OxE can be treated
    the same as an unsigned field with 0x0 meaning no PMU is present.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [Mark: rework to handle perfmon fields]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 92ef9539874a..186f4e19207e 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -447,6 +447,29 @@ cpuid_feature_extract_unsigned_field(u64 features, int field)
 	return cpuid_feature_extract_unsigned_field_width(features, field, 4);
 }
 
+/*
+ * Fields that identify the version of the Performance Monitors Extension do
+ * not follow the standard ID scheme. See ARM DDI 0487E.a page D13-2825,
+ * "Alternative ID scheme used for the Performance Monitors Extension version".
+ */
+static inline u64 __attribute_const__
+cpuid_feature_cap_perfmon_field(u64 features, int field, u64 cap)
+{
+	u64 val = cpuid_feature_extract_unsigned_field(features, field);
+	u64 mask = GENMASK_ULL(field + 3, field);
+
+	/* Treat IMPLEMENTATION DEFINED functionality as unimplemented */
+	if (val == 0xf)
+		val = 0;
+
+	if (val > cap) {
+		features &= ~mask;
+		features |= (cap << field) & mask;
+	}
+
+	return features;
+}
+
 static inline u64 arm64_ftr_mask(const struct arm64_ftr_bits *ftrp)
 {
 	return (u64)GENMASK(ftrp->shift + ftrp->width - 1, ftrp->shift);

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 92ef9539874a..e3ebcc59e83b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -613,6 +613,12 @@ static inline bool system_has_prio_mask_debugging(void)
 	       system_uses_irq_prio_masking();
 }
 
+static inline bool system_supports_bti(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_BTI) &&
+		cpus_have_const_cap(ARM64_BTI);
+}
+
 static inline bool system_capabilities_finalized(void)
 {
 	return static_branch_likely(&arm64_const_caps_ready);

commit 1db5cdeccd813330aaab19b3fccab15e1d07fe12
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Feb 21 14:50:21 2020 +0000

    arm64: cpufeature: add cpus_have_final_cap()
    
    When cpus_have_const_cap() was originally introduced it was intended to
    be safe in hyp context, where it is not safe to access the cpu_hwcaps
    array as cpus_have_cap() did. For more details see commit:
    
      a4023f682739439b ("arm64: Add hypervisor safe helper for checking constant capabilities")
    
    We then made use of cpus_have_const_cap() throughout the kernel.
    
    Subsequently, we had to defer updating the static_key associated with
    each capability in order to avoid lockdep complaints. To avoid breaking
    kernel-wide usage of cpus_have_const_cap(), this was updated to fall
    back to the cpu_hwcaps array if called before the static_keys were
    updated. As the kvm hyp code was only called later than this, the
    fallback is redundant but not functionally harmful. For more details,
    see commit:
    
      63a1e1c95e60e798 ("arm64/cpufeature: don't use mutex in bringup path")
    
    Today we have more users of cpus_have_const_cap() which are only called
    once the relevant static keys are initialized, and it would be
    beneficial to avoid the redundant code.
    
    To that end, this patch adds a new cpus_have_final_cap(), helper which
    is intend to be used in code which is only run once capabilities have
    been finalized, and will never check the cpus_hwcap array. This helps
    the compiler to generate better code as it no longer needs to generate
    code to address and test the cpus_hwcap array. To help catch misuse,
    cpus_have_final_cap() will BUG() if called before capabilities are
    finalized.
    
    In hyp context, BUG() will result in a hyp panic, but the specific BUG()
    instance will not be identified in the usual way.
    
    Comments are added to the various cpus_have_*_cap() helpers to describe
    the constraints on when they can be used. For clarity cpus_have_cap() is
    moved above the other helpers. Similarly the helpers are updated to use
    system_capabilities_finalized() consistently, and this is made
    __always_inline as required by its new callers.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 92ef9539874a..940b2b67b428 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -390,14 +390,16 @@ unsigned long cpu_get_elf_hwcap2(void);
 #define cpu_set_named_feature(name) cpu_set_feature(cpu_feature(name))
 #define cpu_have_named_feature(name) cpu_have_feature(cpu_feature(name))
 
-/* System capability check for constant caps */
-static __always_inline bool __cpus_have_const_cap(int num)
+static __always_inline bool system_capabilities_finalized(void)
 {
-	if (num >= ARM64_NCAPS)
-		return false;
-	return static_branch_unlikely(&cpu_hwcap_keys[num]);
+	return static_branch_likely(&arm64_const_caps_ready);
 }
 
+/*
+ * Test for a capability with a runtime check.
+ *
+ * Before the capability is detected, this returns false.
+ */
 static inline bool cpus_have_cap(unsigned int num)
 {
 	if (num >= ARM64_NCAPS)
@@ -405,14 +407,53 @@ static inline bool cpus_have_cap(unsigned int num)
 	return test_bit(num, cpu_hwcaps);
 }
 
+/*
+ * Test for a capability without a runtime check.
+ *
+ * Before capabilities are finalized, this returns false.
+ * After capabilities are finalized, this is patched to avoid a runtime check.
+ *
+ * @num must be a compile-time constant.
+ */
+static __always_inline bool __cpus_have_const_cap(int num)
+{
+	if (num >= ARM64_NCAPS)
+		return false;
+	return static_branch_unlikely(&cpu_hwcap_keys[num]);
+}
+
+/*
+ * Test for a capability, possibly with a runtime check.
+ *
+ * Before capabilities are finalized, this behaves as cpus_have_cap().
+ * After capabilities are finalized, this is patched to avoid a runtime check.
+ *
+ * @num must be a compile-time constant.
+ */
 static __always_inline bool cpus_have_const_cap(int num)
 {
-	if (static_branch_likely(&arm64_const_caps_ready))
+	if (system_capabilities_finalized())
 		return __cpus_have_const_cap(num);
 	else
 		return cpus_have_cap(num);
 }
 
+/*
+ * Test for a capability without a runtime check.
+ *
+ * Before capabilities are finalized, this will BUG().
+ * After capabilities are finalized, this is patched to avoid a runtime check.
+ *
+ * @num must be a compile-time constant.
+ */
+static __always_inline bool cpus_have_final_cap(int num)
+{
+	if (system_capabilities_finalized())
+		return __cpus_have_const_cap(num);
+	else
+		BUG();
+}
+
 static inline void cpus_set_cap(unsigned int num)
 {
 	if (num >= ARM64_NCAPS) {
@@ -613,11 +654,6 @@ static inline bool system_has_prio_mask_debugging(void)
 	       system_uses_irq_prio_masking();
 }
 
-static inline bool system_capabilities_finalized(void)
-{
-	return static_branch_likely(&arm64_const_caps_ready);
-}
-
 #define ARM64_BP_HARDEN_UNKNOWN		-1
 #define ARM64_BP_HARDEN_WA_NEEDED	0
 #define ARM64_BP_HARDEN_NOT_REQUIRED	1

commit 2c9d45b43c39e26fd2a73f2203321cdaee98b58b
Author: Ionela Voinescu <ionela.voinescu@arm.com>
Date:   Thu Mar 5 09:06:21 2020 +0000

    arm64: add support for the AMU extension v1
    
    The activity monitors extension is an optional extension introduced
    by the ARMv8.4 CPU architecture. This implements basic support for
    version 1 of the activity monitors architecture, AMUv1.
    
    This support includes:
    - Extension detection on each CPU (boot, secondary, hotplugged)
    - Register interface for AMU aarch64 registers
    
    Signed-off-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 92ef9539874a..485e069d8768 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -678,6 +678,11 @@ static inline bool cpu_has_hw_af(void)
 						ID_AA64MMFR1_HADBS_SHIFT);
 }
 
+#ifdef CONFIG_ARM64_AMU_EXTN
+/* Check whether the cpu supports the Activity Monitors Unit (AMU) */
+extern bool cpu_has_amu_feat(int cpu);
+#endif
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit e951445f4d3b5d0df69c0c5d18ab1e9058c20e52
Merge: ef935c25fd64 e43f1331e2ef
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Feb 28 11:46:59 2020 +0100

    Merge tag 'kvmarm-fixes-5.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm fixes for 5.6, take #1
    
    - Fix compilation on 32bit
    - Move  VHE guest entry/exit into the VHE-specific entry code
    - Make sure all functions called by the non-VHE HYP code is tagged as __always_inline

commit e43f1331e2ef913b8c566920c9af75e0ccdd1d3f
Author: James Morse <james.morse@arm.com>
Date:   Thu Feb 20 16:58:39 2020 +0000

    arm64: Ask the compiler to __always_inline functions used by KVM at HYP
    
    KVM uses some of the static-inline helpers like icache_is_vipt() from
    its HYP code. This assumes the function is inlined so that the code is
    mapped to EL2. The compiler may decide not to inline these, and the
    out-of-line version may not be in the __hyp_text section.
    
    Add the additional __always_ hint to these static-inlines that are used
    by KVM.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200220165839.256881-4-james.morse@arm.com

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 0e6d03c7e368..be078699ac4b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -435,13 +435,13 @@ cpuid_feature_extract_signed_field(u64 features, int field)
 	return cpuid_feature_extract_signed_field_width(features, field, 4);
 }
 
-static inline unsigned int __attribute_const__
+static __always_inline unsigned int __attribute_const__
 cpuid_feature_extract_unsigned_field_width(u64 features, int field, int width)
 {
 	return (u64)(features << (64 - width - field)) >> (64 - width);
 }
 
-static inline unsigned int __attribute_const__
+static __always_inline unsigned int __attribute_const__
 cpuid_feature_extract_unsigned_field(u64 features, int field)
 {
 	return cpuid_feature_extract_unsigned_field_width(features, field, 4);
@@ -564,7 +564,7 @@ static inline bool system_supports_mixed_endian(void)
 	return val == 0x1;
 }
 
-static inline bool system_supports_fpsimd(void)
+static __always_inline bool system_supports_fpsimd(void)
 {
 	return !cpus_have_const_cap(ARM64_HAS_NO_FPSIMD);
 }
@@ -575,7 +575,7 @@ static inline bool system_uses_ttbr0_pan(void)
 		!cpus_have_const_cap(ARM64_HAS_PAN);
 }
 
-static inline bool system_supports_sve(void)
+static __always_inline bool system_supports_sve(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_SVE) &&
 		cpus_have_const_cap(ARM64_SVE);

commit 5c37f1ae1c335800d16b207cb578009c695dcd39
Author: James Morse <james.morse@arm.com>
Date:   Thu Feb 20 16:58:37 2020 +0000

    KVM: arm64: Ask the compiler to __always_inline functions used at HYP
    
    On non VHE CPUs, KVM's __hyp_text contains code run at EL2 while the rest
    of the kernel runs at EL1. This code lives in its own section with start
    and end markers so we can map it to EL2.
    
    The compiler may decide not to inline static-inline functions from the
    header file. It may also decide not to put these out-of-line functions
    in the same section, meaning they aren't mapped when called at EL2.
    
    Clang-9 does exactly this with __kern_hyp_va() and a few others when
    x18 is reserved for the shadow call stack. Add the additional __always_
    hint to all the static-inlines that are called from a hyp file.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200220165839.256881-2-james.morse@arm.com
    
    ----
    kvm_get_hyp_vector() pulls in all the regular per-cpu accessors
    and this_cpu_has_cap(), fortunately its only called for VHE.

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 4261d55e8506..0e6d03c7e368 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -581,7 +581,7 @@ static inline bool system_supports_sve(void)
 		cpus_have_const_cap(ARM64_SVE);
 }
 
-static inline bool system_supports_cnp(void)
+static __always_inline bool system_supports_cnp(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_CNP) &&
 		cpus_have_const_cap(ARM64_HAS_CNP);

commit b51c6ac220f77eb246e940442d970b4065c197b0
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 13 23:30:17 2020 +0000

    arm64: Introduce system_capabilities_finalized() marker
    
    We finalize the system wide capabilities after the SMP CPUs
    are booted by the kernel. This is used as a marker for deciding
    various checks in the kernel. e.g, sanity check the hotplugged
    CPUs for missing mandatory features.
    
    However there is no explicit helper available for this in the
    kernel. There is sys_caps_initialised, which is not exposed.
    The other closest one we have is the jump_label arm64_const_caps_ready
    which denotes that the capabilities are set and the capability checks
    could use the individual jump_labels for fast path. This is
    performed before setting the ELF Hwcaps, which must be checked
    against the new CPUs. We also perform some of the other initialization
    e.g, SVE setup, which is important for the use of FP/SIMD
    where SVE is supported. Normally userspace doesn't get to run
    before we finish this. However the in-kernel users may
    potentially start using the neon mode. So, we need to
    reject uses of neon mode before we are set. Instead of defining
    a new marker for the completion of SVE setup, we could simply
    reuse the arm64_const_caps_ready and enable it once we have
    finished all the setup. Also we could expose this to the
    various users as "system_capabilities_finalized()" to make
    it more meaningful than "const_caps_ready".
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 4261d55e8506..92ef9539874a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -613,6 +613,11 @@ static inline bool system_has_prio_mask_debugging(void)
 	       system_uses_irq_prio_masking();
 }
 
+static inline bool system_capabilities_finalized(void)
+{
+	return static_branch_likely(&arm64_const_caps_ready);
+}
+
 #define ARM64_BP_HARDEN_UNKNOWN		-1
 #define ARM64_BP_HARDEN_WA_NEEDED	0
 #define ARM64_BP_HARDEN_NOT_REQUIRED	1

commit 47d7b15b88f96a90694cfc607d0717d62dff6c45
Author: Jia He <justin.he@arm.com>
Date:   Fri Oct 11 22:09:36 2019 +0800

    arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    We unconditionally set the HW_AFDBM capability and only enable it on
    CPUs which really have the feature. But sometimes we need to know
    whether this cpu has the capability of HW AF. So decouple AF from
    DBM by a new helper cpu_has_hw_af().
    
    If later we noticed a potential performance issue on this path, we can
    turn it into a static label as with other CPU features.
    
    Signed-off-by: Jia He <justin.he@arm.com>
    Suggested-by: Suzuki Poulose <Suzuki.Poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 9cde5d2e768f..4261d55e8506 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -659,6 +659,20 @@ static inline u32 id_aa64mmfr0_parange_to_phys_shift(int parange)
 	default: return CONFIG_ARM64_PA_BITS;
 	}
 }
+
+/* Check whether hardware update of the Access flag is supported */
+static inline bool cpu_has_hw_af(void)
+{
+	u64 mmfr1;
+
+	if (!IS_ENABLED(CONFIG_ARM64_HW_AFDBM))
+		return false;
+
+	mmfr1 = read_cpuid(ID_AA64MMFR1_EL1);
+	return cpuid_feature_extract_unsigned_field(mmfr1,
+						ID_AA64MMFR1_HADBS_SHIFT);
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 38d16667604e31f30a715baefec2fe3aa88024f0
Author: Mark Brown <broonie@kernel.org>
Date:   Thu Aug 8 15:05:54 2019 +0100

    arm64: Clarify when cpu_enable() is called
    
    Strengthen the wording in the documentation for cpu_enable() to make it
    more obvious to readers not already familiar with the code when the core
    will call this callback and that this is intentional.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: minor tweak to emphasis in the comment]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index cf65a47ee6b4..9cde5d2e768f 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -289,9 +289,16 @@ struct arm64_cpu_capabilities {
 	u16 type;
 	bool (*matches)(const struct arm64_cpu_capabilities *caps, int scope);
 	/*
-	 * Take the appropriate actions to enable this capability for this CPU.
-	 * For each successfully booted CPU, this method is called for each
-	 * globally detected capability.
+	 * Take the appropriate actions to configure this capability
+	 * for this CPU. If the capability is detected by the kernel
+	 * this will be called on all the CPUs in the system,
+	 * including the hotplugged CPUs, regardless of whether the
+	 * capability is available on that specific CPU. This is
+	 * useful for some capabilities (e.g, working around CPU
+	 * errata), where all the CPUs must take some action (e.g,
+	 * changing system control/configuration). Thus, if an action
+	 * is required only if the CPU has the capability, then the
+	 * routine must check it before taking any action.
 	 */
 	void (*cpu_enable)(const struct arm64_cpu_capabilities *cap);
 	union {

commit 2f8f180b3ceed7a16a92cc3c164368c26e1f9320
Author: Mark Brown <broonie@kernel.org>
Date:   Tue Jul 30 11:26:31 2019 +0100

    arm64: Remove unused cpucap_multi_entry_cap_cpu_enable()
    
    The function cpucap_multi_entry_cap_cpu_enable() is unused, remove it to
    avoid any confusion reading the code and potential for bit rot.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index c96ffa4722d3..cf65a47ee6b4 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -363,21 +363,6 @@ cpucap_multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
 	return false;
 }
 
-/*
- * Take appropriate action for all matching entries in the shared capability
- * entry.
- */
-static inline void
-cpucap_multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
-{
-	const struct arm64_cpu_capabilities *caps;
-
-	for (caps = entry->match_list; caps->matches; caps++)
-		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
-		    caps->cpu_enable)
-			caps->cpu_enable(caps);
-}
-
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;

commit 147b9635e6347104b91f48ca9dca61eb0fbf2a54
Author: Will Deacon <will@kernel.org>
Date:   Tue Jul 30 15:40:20 2019 +0100

    arm64: cpufeature: Fix feature comparison for CTR_EL0.{CWG,ERG}
    
    If CTR_EL0.{CWG,ERG} are 0b0000 then they must be interpreted to have
    their architecturally maximum values, which defeats the use of
    FTR_HIGHER_SAFE when sanitising CPU ID registers on heterogeneous
    machines.
    
    Introduce FTR_HIGHER_OR_ZERO_SAFE so that these fields effectively
    saturate at zero.
    
    Fixes: 3c739b571084 ("arm64: Keep track of CPU feature registers")
    Cc: <stable@vger.kernel.org> # 4.4.x-
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 407e2bf23676..c96ffa4722d3 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -35,9 +35,10 @@
  */
 
 enum ftr_type {
-	FTR_EXACT,	/* Use a predefined safe value */
-	FTR_LOWER_SAFE,	/* Smaller value is safe */
-	FTR_HIGHER_SAFE,/* Bigger value is safe */
+	FTR_EXACT,			/* Use a predefined safe value */
+	FTR_LOWER_SAFE,			/* Smaller value is safe */
+	FTR_HIGHER_SAFE,		/* Bigger value is safe */
+	FTR_HIGHER_OR_ZERO_SAFE,	/* Bigger value is safe, but 0 is biggest */
 };
 
 #define FTR_STRICT	true	/* SANITY check strict matching required */

commit 39d7530d7494b4e47ba1856e741f513dafd17e3d
Merge: 16c97650a56a a45ff5994c9c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 15:35:14 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for chained PMU counters in guests
       - improved SError handling
       - handle Neoverse N1 erratum #1349291
       - allow side-channel mitigation status to be migrated
       - standardise most AArch64 system register accesses to msr_s/mrs_s
       - fix host MPIDR corruption on 32bit
       - selftests ckleanups
    
      x86:
       - PMU event {white,black}listing
       - ability for the guest to disable host-side interrupt polling
       - fixes for enlightened VMCS (Hyper-V pv nested virtualization),
       - new hypercall to yield to IPI target
       - support for passing cstate MSRs through to the guest
       - lots of cleanups and optimizations
    
      Generic:
       - Some txt->rST conversions for the documentation"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (128 commits)
      Documentation: virtual: Add toctree hooks
      Documentation: kvm: Convert cpuid.txt to .rst
      Documentation: virtual: Convert paravirt_ops.txt to .rst
      KVM: x86: Unconditionally enable irqs in guest context
      KVM: x86: PMU Event Filter
      kvm: x86: Fix -Wmissing-prototypes warnings
      KVM: Properly check if "page" is valid in kvm_vcpu_unmap
      KVM: arm/arm64: Initialise host's MPIDRs by reading the actual register
      KVM: LAPIC: Retry tune per-vCPU timer_advance_ns if adaptive tuning goes insane
      kvm: LAPIC: write down valid APIC registers
      KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
      KVM: doc: Add API documentation on the KVM_REG_ARM_WORKAROUNDS register
      KVM: arm/arm64: Add save/restore support for firmware workaround state
      arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
      KVM: arm/arm64: Support chained PMU counters
      KVM: arm/arm64: Remove pmc->bitmask
      KVM: arm/arm64: Re-create event when setting counter value
      KVM: arm/arm64: Extract duplicated code to own function
      KVM: arm/arm64: Rename kvm_pmu_{enable/disable}_counter functions
      KVM: LAPIC: ARBPRI is a reserved register for x2APIC
      ...

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit c118bbb52743df70e6297671606c1c08edc659fe
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri May 3 15:27:48 2019 +0100

    arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
    
    Recent commits added the explicit notion of "workaround not required" to
    the state of the Spectre v2 (aka. BP_HARDENING) workaround, where we
    just had "needed" and "unknown" before.
    
    Export this knowledge to the rest of the kernel and enhance the existing
    kvm_arm_harden_branch_predictor() to report this new state as well.
    Export this new state to guests when they use KVM's firmware interface
    emulation.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 373799b7982f..948427f6b3d9 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -614,6 +614,12 @@ static inline bool system_uses_irq_prio_masking(void)
 	       cpus_have_const_cap(ARM64_HAS_IRQ_PRIO_MASKING);
 }
 
+#define ARM64_BP_HARDEN_UNKNOWN		-1
+#define ARM64_BP_HARDEN_WA_NEEDED	0
+#define ARM64_BP_HARDEN_NOT_REQUIRED	1
+
+int get_spectre_v2_workaround_state(void);
+
 #define ARM64_SSBD_UNKNOWN		-1
 #define ARM64_SSBD_FORCE_DISABLE	0
 #define ARM64_SSBD_KERNEL		1

commit 48ce8f80f5901f1f031b00be66d659d39f33b0a1
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:11 2019 +0100

    arm64: irqflags: Introduce explicit debugging for IRQ priorities
    
    Using IRQ priority masking to enable/disable interrupts is a bit
    sensitive as it requires to deal with both ICC_PMR_EL1 and PSR.I.
    
    Introduce some validity checks to both highlight the states in which
    functions dealing with IRQ enabling/disabling can (not) be called, and
    bark a warning when called in an unexpected state.
    
    Since these checks are done on hotpaths, introduce a build option to
    choose whether to do the checking.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index bc895c869892..693a086e2148 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -617,6 +617,12 @@ static inline bool system_uses_irq_prio_masking(void)
 	       cpus_have_const_cap(ARM64_HAS_IRQ_PRIO_MASKING);
 }
 
+static inline bool system_has_prio_mask_debugging(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_DEBUG_PRIORITY_MASKING) &&
+	       system_uses_irq_prio_masking();
+}
+
 #define ARM64_SSBD_UNKNOWN		-1
 #define ARM64_SSBD_FORCE_DISABLE	0
 #define ARM64_SSBD_KERNEL		1

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index bc895c869892..373799b7982f 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -1,9 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2014 Linaro Ltd. <ard.biesheuvel@linaro.org>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #ifndef __ASM_CPUFEATURE_H

commit 02166b88d376f21ea5caac26e7e8f74ab5578986
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue May 14 15:41:52 2019 -0700

    arm64: mark (__)cpus_have_const_cap as __always_inline
    
    This prepares to move CONFIG_OPTIMIZE_INLINING from x86 to a common
    place.  We need to eliminate potential issues beforehand.
    
    If it is enabled for arm64, the following errors are reported:
    
      In file included from include/linux/compiler_types.h:68,
                       from <command-line>:
      arch/arm64/include/asm/jump_label.h: In function 'cpus_have_const_cap':
      include/linux/compiler-gcc.h:120:38: warning: asm operand 0 probably doesn't match constraints
       #define asm_volatile_goto(x...) do { asm goto(x); asm (""); } while (0)
                                            ^~~
      arch/arm64/include/asm/jump_label.h:32:2: note: in expansion of macro 'asm_volatile_goto'
        asm_volatile_goto(
        ^~~~~~~~~~~~~~~~~
      include/linux/compiler-gcc.h:120:38: error: impossible constraint in 'asm'
       #define asm_volatile_goto(x...) do { asm goto(x); asm (""); } while (0)
                                            ^~~
      arch/arm64/include/asm/jump_label.h:32:2: note: in expansion of macro 'asm_volatile_goto'
        asm_volatile_goto(
        ^~~~~~~~~~~~~~~~~
    
    Link: http://lkml.kernel.org/r/20190423034959.13525-3-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Brezillon <bbrezillon@kernel.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Norris <computersforpeace@gmail.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Marek Vasut <marek.vasut@gmail.com>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Miquel Raynal <miquel.raynal@bootlin.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Stefan Agner <stefan@agner.ch>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index f210bcf096f7..bc895c869892 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -401,7 +401,7 @@ unsigned long cpu_get_elf_hwcap2(void);
 #define cpu_have_named_feature(name) cpu_have_feature(cpu_feature(name))
 
 /* System capability check for constant caps */
-static inline bool __cpus_have_const_cap(int num)
+static __always_inline bool __cpus_have_const_cap(int num)
 {
 	if (num >= ARM64_NCAPS)
 		return false;
@@ -415,7 +415,7 @@ static inline bool cpus_have_cap(unsigned int num)
 	return test_bit(num, cpu_hwcaps);
 }
 
-static inline bool cpus_have_const_cap(int num)
+static __always_inline bool cpus_have_const_cap(int num)
 {
 	if (static_branch_likely(&arm64_const_caps_ready))
 		return __cpus_have_const_cap(num);

commit 50abbe19623e08e2aa34e0e526bd6115569f3dc3
Merge: 9431ac2bf6b7 4ad499c94264
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed May 1 15:34:56 2019 +0100

    Merge branch 'for-next/mitigations' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into for-next/core

commit d42281b6e49510f078ace15a8ea10f71e6262581
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Mon Apr 15 16:21:27 2019 -0500

    arm64: Always enable ssb vulnerability detection
    
    Ensure we are always able to detect whether or not the CPU is affected
    by SSB, so that we can later advertise this to userspace.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    [will: Use IS_ENABLED instead of #ifdef]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index e505e1fbd2b9..6ccdc97e5d6a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -638,11 +638,7 @@ static inline int arm64_get_ssbd_state(void)
 #endif
 }
 
-#ifdef CONFIG_ARM64_SSBD
 void arm64_set_ssbd_mitigation(bool state);
-#else
-static inline void arm64_set_ssbd_mitigation(bool state) {}
-#endif
 
 extern int do_emulate_mrs(struct pt_regs *regs, u32 sys_reg, u32 rt);
 

commit aec0bff757c937489d003ab7b36c52e77e4b096a
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 10:52:41 2019 +0100

    arm64: HWCAP: encapsulate elf_hwcap
    
    The introduction of AT_HWCAP2 introduced accessors which ensure that
    hwcap features are set and tested appropriately.
    
    Let's now mandate access to elf_hwcap via these accessors by making
    elf_hwcap static within cpufeature.c.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 347c17046668..a3f028f82def 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -392,19 +392,12 @@ extern DECLARE_BITMAP(boot_capabilities, ARM64_NPATCHABLE);
 	for_each_set_bit(cap, cpu_hwcaps, ARM64_NCAPS)
 
 bool this_cpu_has_cap(unsigned int cap);
+void cpu_set_feature(unsigned int num);
+bool cpu_have_feature(unsigned int num);
+unsigned long cpu_get_elf_hwcap(void);
+unsigned long cpu_get_elf_hwcap2(void);
 
-static inline void cpu_set_feature(unsigned int num)
-{
-	WARN_ON(num >= MAX_CPU_FEATURES);
-	elf_hwcap |= BIT(num);
-}
 #define cpu_set_named_feature(name) cpu_set_feature(cpu_feature(name))
-
-static inline bool cpu_have_feature(unsigned int num)
-{
-	WARN_ON(num >= MAX_CPU_FEATURES);
-	return elf_hwcap & BIT(num);
-}
 #define cpu_have_named_feature(name) cpu_have_feature(cpu_feature(name))
 
 /* System capability check for constant caps */

commit aaba098fe6ce594ae6f963dc041be6307e499f19
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 10:52:40 2019 +0100

    arm64: HWCAP: add support for AT_HWCAP2
    
    As we will exhaust the first 32 bits of AT_HWCAP let's start
    exposing AT_HWCAP2 to userspace to give us up to 64 caps.
    
    Whilst it's possible to use the remaining 32 bits of AT_HWCAP, we
    prefer to expand into AT_HWCAP2 in order to provide a consistent
    view to userspace between ILP32 and LP64. However internal to the
    kernel we prefer to continue to use the full space of elf_hwcap.
    
    To reduce complexity and allow for future expansion, we now
    represent hwcaps in the kernel as ordinals and use a
    KERNEL_HWCAP_ prefix. This allows us to support automatic feature
    based module loading for all our hwcaps.
    
    We introduce cpu_set_feature to set hwcaps which complements the
    existing cpu_have_feature helper. These helpers allow us to clean
    up existing direct uses of elf_hwcap and reduce any future effort
    required to move beyond 64 caps.
    
    For convenience we also introduce cpu_{have,set}_named_feature which
    makes use of the cpu_feature macro to allow providing a hwcap name
    without a {KERNEL_}HWCAP_ prefix.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    [will: use const_ilog2() and tweak documentation]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index e505e1fbd2b9..347c17046668 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -14,15 +14,8 @@
 #include <asm/hwcap.h>
 #include <asm/sysreg.h>
 
-/*
- * In the arm64 world (as in the ARM world), elf_hwcap is used both internally
- * in the kernel and for user space to keep track of which optional features
- * are supported by the current system. So let's map feature 'x' to HWCAP_x.
- * Note that HWCAP_x constants are bit fields so we need to take the log.
- */
-
-#define MAX_CPU_FEATURES	(8 * sizeof(elf_hwcap))
-#define cpu_feature(x)		ilog2(HWCAP_ ## x)
+#define MAX_CPU_FEATURES	64
+#define cpu_feature(x)		KERNEL_HWCAP_ ## x
 
 #ifndef __ASSEMBLY__
 
@@ -400,10 +393,19 @@ extern DECLARE_BITMAP(boot_capabilities, ARM64_NPATCHABLE);
 
 bool this_cpu_has_cap(unsigned int cap);
 
+static inline void cpu_set_feature(unsigned int num)
+{
+	WARN_ON(num >= MAX_CPU_FEATURES);
+	elf_hwcap |= BIT(num);
+}
+#define cpu_set_named_feature(name) cpu_set_feature(cpu_feature(name))
+
 static inline bool cpu_have_feature(unsigned int num)
 {
-	return elf_hwcap & (1UL << num);
+	WARN_ON(num >= MAX_CPU_FEATURES);
+	return elf_hwcap & BIT(num);
 }
+#define cpu_have_named_feature(name) cpu_have_feature(cpu_feature(name))
 
 /* System capability check for constant caps */
 static inline bool __cpus_have_const_cap(int num)

commit 0ceb0d56905e3d141fae77e5936d00eee9233473
Author: Daniel Thompson <daniel.thompson@linaro.org>
Date:   Thu Jan 31 14:58:53 2019 +0000

    arm64: alternative: Apply alternatives early in boot process
    
    Currently alternatives are applied very late in the boot process (and
    a long time after we enable scheduling). Some alternative sequences,
    such as those that alter the way CPU context is stored, must be applied
    much earlier in the boot sequence.
    
    Introduce apply_boot_alternatives() to allow some alternatives to be
    applied immediately after we detect the CPU features of the boot CPU.
    
    Signed-off-by: Daniel Thompson <daniel.thompson@linaro.org>
    [julien.thierry@arm.com: rename to fit new cpufeature framework better,
                             apply BOOT_SCOPE feature early in boot]
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 89c3f318f6be..e505e1fbd2b9 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -391,6 +391,10 @@ extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;
 
+/* ARM64 CAPS + alternative_cb */
+#define ARM64_NPATCHABLE (ARM64_NCAPS + 1)
+extern DECLARE_BITMAP(boot_capabilities, ARM64_NPATCHABLE);
+
 #define for_each_available_cap(cap)		\
 	for_each_set_bit(cap, cpu_hwcaps, ARM64_NCAPS)
 

commit b90d2b22afdc7ce150a9ee7a8d82378bcfc395a5
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:42 2019 +0000

    arm64: cpufeature: Add cpufeature for IRQ priority masking
    
    Add a cpufeature indicating whether a cpu supports masking interrupts
    by priority.
    
    The feature will be properly enabled in a later patch.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index dfcfba725d72..89c3f318f6be 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -612,6 +612,12 @@ static inline bool system_supports_generic_auth(void)
 		 cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH_IMP_DEF));
 }
 
+static inline bool system_uses_irq_prio_masking(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_PSEUDO_NMI) &&
+	       cpus_have_const_cap(ARM64_HAS_IRQ_PRIO_MASKING);
+}
+
 #define ARM64_SSBD_UNKNOWN		-1
 #define ARM64_SSBD_FORCE_DISABLE	0
 #define ARM64_SSBD_KERNEL		1

commit 1e013d06120cbf67e771848fc5d98174c33b078a
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 15:53:54 2018 +0000

    arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
    
    Open-coding the pointer-auth HWCAPs is a mess and can be avoided by
    reusing the multi-cap logic from the CPU errata framework.
    
    Move the multi_entry_cap_matches code to cpufeature.h and reuse it for
    the pointer auth HWCAPs.
    
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 1e7fcd12b1c1..dfcfba725d72 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -321,19 +321,20 @@ struct arm64_cpu_capabilities {
 			bool sign;
 			unsigned long hwcap;
 		};
-		/*
-		 * A list of "matches/cpu_enable" pair for the same
-		 * "capability" of the same "type" as described by the parent.
-		 * Only matches(), cpu_enable() and fields relevant to these
-		 * methods are significant in the list. The cpu_enable is
-		 * invoked only if the corresponding entry "matches()".
-		 * However, if a cpu_enable() method is associated
-		 * with multiple matches(), care should be taken that either
-		 * the match criteria are mutually exclusive, or that the
-		 * method is robust against being called multiple times.
-		 */
-		const struct arm64_cpu_capabilities *match_list;
 	};
+
+	/*
+	 * An optional list of "matches/cpu_enable" pair for the same
+	 * "capability" of the same "type" as described by the parent.
+	 * Only matches(), cpu_enable() and fields relevant to these
+	 * methods are significant in the list. The cpu_enable is
+	 * invoked only if the corresponding entry "matches()".
+	 * However, if a cpu_enable() method is associated
+	 * with multiple matches(), care should be taken that either
+	 * the match criteria are mutually exclusive, or that the
+	 * method is robust against being called multiple times.
+	 */
+	const struct arm64_cpu_capabilities *match_list;
 };
 
 static inline int cpucap_default_scope(const struct arm64_cpu_capabilities *cap)
@@ -353,6 +354,39 @@ cpucap_late_cpu_permitted(const struct arm64_cpu_capabilities *cap)
 	return !!(cap->type & ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU);
 }
 
+/*
+ * Generic helper for handling capabilties with multiple (match,enable) pairs
+ * of call backs, sharing the same capability bit.
+ * Iterate over each entry to see if at least one matches.
+ */
+static inline bool
+cpucap_multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
+			       int scope)
+{
+	const struct arm64_cpu_capabilities *caps;
+
+	for (caps = entry->match_list; caps->matches; caps++)
+		if (caps->matches(caps, scope))
+			return true;
+
+	return false;
+}
+
+/*
+ * Take appropriate action for all matching entries in the shared capability
+ * entry.
+ */
+static inline void
+cpucap_multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
+{
+	const struct arm64_cpu_capabilities *caps;
+
+	for (caps = entry->match_list; caps->matches; caps++)
+		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
+		    caps->cpu_enable)
+			caps->cpu_enable(caps);
+}
+
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;
@@ -476,7 +510,6 @@ static inline bool id_aa64pfr0_sve(u64 pfr0)
 void __init setup_cpu_features(void);
 void check_local_cpu_capabilities(void);
 
-
 u64 read_sanitised_ftr_reg(u32 id);
 
 static inline bool cpu_supports_mixed_endian_el0(void)

commit a56005d3210500f8a166fcb83cbb5ac5d0f909e4
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 15:52:02 2018 +0000

    arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
    
    We can easily avoid defining the two meta-capabilities for the address
    and generic keys, so remove them and instead just check both of the
    architected and impdef capabilities when determining the level of system
    support.
    
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index daec4b50d7fb..1e7fcd12b1c1 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -568,13 +568,15 @@ static inline bool system_supports_cnp(void)
 static inline bool system_supports_address_auth(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_PTR_AUTH) &&
-		cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH);
+		(cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH_ARCH) ||
+		 cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH_IMP_DEF));
 }
 
 static inline bool system_supports_generic_auth(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_PTR_AUTH) &&
-		cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH);
+		(cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH_ARCH) ||
+		 cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH_IMP_DEF));
 }
 
 #define ARM64_SSBD_UNKNOWN		-1

commit 6984eb47d5c1a74bb44467ee4eee22d680f10785
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:24 2018 +0000

    arm64/cpufeature: detect pointer authentication
    
    So that we can dynamically handle the presence of pointer authentication
    functionality, wire up probing code in cpufeature.c.
    
    From ARMv8.3 onwards, ID_AA64ISAR1 is no longer entirely RES0, and now
    has four fields describing the presence of pointer authentication
    functionality:
    
    * APA - address authentication present, using an architected algorithm
    * API - address authentication present, using an IMP DEF algorithm
    * GPA - generic authentication present, using an architected algorithm
    * GPI - generic authentication present, using an IMP DEF algorithm
    
    This patch checks for both address and generic authentication,
    separately. It is assumed that if all CPUs support an IMP DEF algorithm,
    the same algorithm is used across all CPUs.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 38e674f6e18b..daec4b50d7fb 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -565,6 +565,18 @@ static inline bool system_supports_cnp(void)
 		cpus_have_const_cap(ARM64_HAS_CNP);
 }
 
+static inline bool system_supports_address_auth(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_PTR_AUTH) &&
+		cpus_have_const_cap(ARM64_HAS_ADDRESS_AUTH);
+}
+
+static inline bool system_supports_generic_auth(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_PTR_AUTH) &&
+		cpus_have_const_cap(ARM64_HAS_GENERIC_AUTH);
+}
+
 #define ARM64_SSBD_UNKNOWN		-1
 #define ARM64_SSBD_FORCE_DISABLE	0
 #define ARM64_SSBD_KERNEL		1

commit d34664f63bba9c884920d86ab67379a08a4ee8e9
Merge: bc84a2d106be 394135c1ff13
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 18:57:17 2018 +0000

    Merge branch 'for-next/kexec' into aarch64/for-next/core
    
    Merge in kexec_file_load() support from Akashi Takahiro.

commit 0b587c84e42151fc5a636c7cebf7b03b281dc672
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Nov 30 17:18:06 2018 +0000

    arm64: capabilities: Batch cpu_enable callbacks
    
    We use a stop_machine call for each available capability to
    enable it on all the CPUs available at boot time. Instead
    we could batch the cpu_enable callbacks to a single stop_machine()
    call to save us some time.
    
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Tested-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7e2ec64aa414..0a15e2c55f1b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -357,6 +357,9 @@ extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;
 
+#define for_each_available_cap(cap)		\
+	for_each_set_bit(cap, cpu_hwcaps, ARM64_NCAPS)
+
 bool this_cpu_has_cap(unsigned int cap);
 
 static inline bool cpu_have_feature(unsigned int num)

commit bdd2c9d1c33357462d1d38eb04bbb5dc452eed40
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Thu Nov 15 14:52:47 2018 +0900

    arm64: cpufeature: add MMFR0 helper functions
    
    Those helper functions for MMFR0 register will be used later by kexec_file
    loader.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7e2ec64aa414..ef118d819fe8 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -486,11 +486,59 @@ static inline bool system_supports_32bit_el0(void)
 	return cpus_have_const_cap(ARM64_HAS_32BIT_EL0);
 }
 
+static inline bool system_supports_4kb_granule(void)
+{
+	u64 mmfr0;
+	u32 val;
+
+	mmfr0 =	read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
+	val = cpuid_feature_extract_unsigned_field(mmfr0,
+						ID_AA64MMFR0_TGRAN4_SHIFT);
+
+	return val == ID_AA64MMFR0_TGRAN4_SUPPORTED;
+}
+
+static inline bool system_supports_64kb_granule(void)
+{
+	u64 mmfr0;
+	u32 val;
+
+	mmfr0 =	read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
+	val = cpuid_feature_extract_unsigned_field(mmfr0,
+						ID_AA64MMFR0_TGRAN64_SHIFT);
+
+	return val == ID_AA64MMFR0_TGRAN64_SUPPORTED;
+}
+
+static inline bool system_supports_16kb_granule(void)
+{
+	u64 mmfr0;
+	u32 val;
+
+	mmfr0 =	read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
+	val = cpuid_feature_extract_unsigned_field(mmfr0,
+						ID_AA64MMFR0_TGRAN16_SHIFT);
+
+	return val == ID_AA64MMFR0_TGRAN16_SUPPORTED;
+}
+
 static inline bool system_supports_mixed_endian_el0(void)
 {
 	return id_aa64mmfr0_mixed_endian_el0(read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1));
 }
 
+static inline bool system_supports_mixed_endian(void)
+{
+	u64 mmfr0;
+	u32 val;
+
+	mmfr0 =	read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
+	val = cpuid_feature_extract_unsigned_field(mmfr0,
+						ID_AA64MMFR0_BIGENDEL_SHIFT);
+
+	return val == 0x1;
+}
+
 static inline bool system_supports_fpsimd(void)
 {
 	return !cpus_have_const_cap(ARM64_HAS_NO_FPSIMD);

commit 0d1e8b8d2bcd3150d51754d8d0fdbf44dc88b0d3
Merge: 83c4087ce468 22a7cdcae6a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 17:57:35 2018 -0700

    Merge tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
       - Improved guest IPA space support (32 to 52 bits)
    
       - RAS event delivery for 32bit
    
       - PMU fixes
    
       - Guest entry hardening
    
       - Various cleanups
    
       - Port of dirty_log_test selftest
    
      PPC:
       - Nested HV KVM support for radix guests on POWER9. The performance
         is much better than with PR KVM. Migration and arbitrary level of
         nesting is supported.
    
       - Disable nested HV-KVM on early POWER9 chips that need a particular
         hardware bug workaround
    
       - One VM per core mode to prevent potential data leaks
    
       - PCI pass-through optimization
    
       - merge ppc-kvm topic branch and kvm-ppc-fixes to get a better base
    
      s390:
       - Initial version of AP crypto virtualization via vfio-mdev
    
       - Improvement for vfio-ap
    
       - Set the host program identifier
    
       - Optimize page table locking
    
      x86:
       - Enable nested virtualization by default
    
       - Implement Hyper-V IPI hypercalls
    
       - Improve #PF and #DB handling
    
       - Allow guests to use Enlightened VMCS
    
       - Add migration selftests for VMCS and Enlightened VMCS
    
       - Allow coalesced PIO accesses
    
       - Add an option to perform nested VMCS host state consistency check
         through hardware
    
       - Automatic tuning of lapic_timer_advance_ns
    
       - Many fixes, minor improvements, and cleanups"
    
    * tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      KVM/nVMX: Do not validate that posted_intr_desc_addr is page aligned
      Revert "kvm: x86: optimize dr6 restore"
      KVM: PPC: Optimize clearing TCEs for sparse tables
      x86/kvm/nVMX: tweak shadow fields
      selftests/kvm: add missing executables to .gitignore
      KVM: arm64: Safety check PSTATE when entering guest and handle IL
      KVM: PPC: Book3S HV: Don't use streamlined entry path on early POWER9 chips
      arm/arm64: KVM: Enable 32 bits kvm vcpu events support
      arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
      KVM: arm64: Fix caching of host MDCR_EL2 value
      KVM: VMX: enable nested virtualization by default
      KVM/x86: Use 32bit xor to clear registers in svm.c
      kvm: x86: Introduce KVM_CAP_EXCEPTION_PAYLOAD
      kvm: vmx: Defer setting of DR6 until #DB delivery
      kvm: x86: Defer setting of CR2 until #PF delivery
      kvm: x86: Add payload operands to kvm_multiple_exception
      kvm: x86: Add exception payload fields to kvm_vcpu_events
      kvm: x86: Add has_payload and payload to kvm_queued_exception
      KVM: Documentation: Fix omission in struct kvm_vcpu_events
      KVM: selftests: add Enlightened VMCS test
      ...

commit ce00e3cb4fb496683708db6bfce470e5c7710ddc
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:40 2018 +0100

    arm64: Add a helper for PARange to physical shift conversion
    
    On arm64, ID_AA64MMFR0_EL1.PARange encodes the maximum Physical
    Address range supported by the CPU. Add a helper to decode this
    to actual physical shift. If we hit an unallocated value, return
    the maximum range supported by the kernel.
    This will be used by KVM to set the VTCR_EL2.T0SZ, as it
    is about to move its place. Having this helper keeps the code
    movement cleaner.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 1717ba1db35d..072cc1c970c2 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -530,6 +530,26 @@ void arm64_set_ssbd_mitigation(bool state);
 static inline void arm64_set_ssbd_mitigation(bool state) {}
 #endif
 
+static inline u32 id_aa64mmfr0_parange_to_phys_shift(int parange)
+{
+	switch (parange) {
+	case 0: return 32;
+	case 1: return 36;
+	case 2: return 40;
+	case 3: return 42;
+	case 4: return 44;
+	case 5: return 48;
+	case 6: return 52;
+	/*
+	 * A future PE could use a value unknown to the kernel.
+	 * However, by the "D10.1.4 Principles of the ID scheme
+	 * for fields in ID registers", ARM DDI 0487C.a, any new
+	 * value is guaranteed to be higher than what we know already.
+	 * As a safe limit, we return the limit supported by the kernel.
+	 */
+	default: return CONFIG_ARM64_PA_BITS;
+	}
+}
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 520ad98871a072471d2583a9386b9d7243fa584d
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Thu Sep 20 09:36:20 2018 +0530

    arm64/cpufeatures: Factorize emulate_mrs()
    
    MRS emulation gets triggered with exception class (0x00 or 0x18) eventually
    calling the function emulate_mrs() which fetches the user space instruction
    and analyses it's encodings (OP0, OP1, OP2, CRN, CRM, RT). The kernel tries
    to emulate the given instruction looking into the encoding details. Going
    forward these encodings can also be parsed from ESR_ELx.ISS fields without
    requiring to fetch/decode faulting userspace instruction which can improve
    performance. This factorizes emulate_mrs() function in a way that it can be
    called directly with MRS encodings (OP0, OP1, OP2, CRN, CRM) for any given
    target register which can then be used directly from 0x18 exception class.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index bb9fbf6f910a..6db48d90ad63 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -536,6 +536,7 @@ void arm64_set_ssbd_mitigation(bool state);
 static inline void arm64_set_ssbd_mitigation(bool state) {}
 #endif
 
+extern int do_emulate_mrs(struct pt_regs *regs, u32 sys_reg, u32 rt);
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 5ffdfaedfa0aba3f5db0fbb8ed4f3192be2b39b8
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Tue Jul 31 14:08:56 2018 +0100

    arm64: mm: Support Common Not Private translations
    
    Common Not Private (CNP) is a feature of ARMv8.2 extension which
    allows translation table entries to be shared between different PEs in
    the same inner shareable domain, so the hardware can use this fact to
    optimise the caching of such entries in the TLB.
    
    CNP occupies one bit in TTBRx_ELy and VTTBR_EL2, which advertises to
    the hardware that the translation table entries pointed to by this
    TTBR are the same as every PE in the same inner shareable domain for
    which the equivalent TTBR also has CNP bit set. In case CNP bit is set
    but TTBR does not point at the same translation table entries for a
    given ASID and VMID, then the system is mis-configured, so the results
    of translations are UNPREDICTABLE.
    
    For kernel we postpone setting CNP till all cpus are up and rely on
    cpufeature framework to 1) patch the code which is sensitive to CNP
    and 2) update TTBR1_EL1 with CNP bit set. TTBR1_EL1 can be
    reprogrammed as result of hibernation or cpuidle (via __enable_mmu).
    For these two cases we restore CnP bit via __cpu_suspend_exit().
    
    There are a few cases we need to care of changes in TTBR0_EL1:
      - a switch to idmap
      - software emulated PAN
    
    we rule out latter via Kconfig options and for the former we make
    sure that CNP is set for non-zero ASIDs only.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    [catalin.marinas@arm.com: default y for CONFIG_ARM64_CNP]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 9079715794af..bb9fbf6f910a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -508,6 +508,12 @@ static inline bool system_supports_sve(void)
 		cpus_have_const_cap(ARM64_SVE);
 }
 
+static inline bool system_supports_cnp(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_CNP) &&
+		cpus_have_const_cap(ARM64_HAS_CNP);
+}
+
 #define ARM64_SSBD_UNKNOWN		-1
 #define ARM64_SSBD_FORCE_DISABLE	0
 #define ARM64_SSBD_KERNEL		1

commit ca7f686ac9fe87a9175696a8744e095ab9749c49
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jun 15 11:36:43 2018 +0100

    arm64: Fix silly typo in comment
    
    I was passing through and figuered I'd fix this up:
    
            featuer -> feature
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 1717ba1db35d..9079715794af 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -262,7 +262,7 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 /*
  * CPU feature detected at boot time based on system-wide value of a
  * feature. It is safe for a late CPU to have this feature even though
- * the system hasn't enabled it, although the featuer will not be used
+ * the system hasn't enabled it, although the feature will not be used
  * by Linux in this case. If the system has enabled this feature already,
  * then every late CPU must have it.
  */

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit 647d0519b53f440a55df163de21c52a8205431cc
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:12 2018 +0100

    arm64: ssbd: Restore mitigation status on CPU resume
    
    On a system where firmware can dynamically change the state of the
    mitigation, the CPU will always come up with the mitigation enabled,
    including when coming back from suspend.
    
    If the user has requested "no mitigation" via a command line option,
    let's enforce it by calling into the firmware again to disable it.
    
    Similarily, for a resume from hibernate, the mitigation could have
    been disabled by the boot kernel. Let's ensure that it is set
    back on in that case.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b0fc3224ce8a..55bc1f073bfb 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -553,6 +553,12 @@ static inline int arm64_get_ssbd_state(void)
 #endif
 }
 
+#ifdef CONFIG_ARM64_SSBD
+void arm64_set_ssbd_mitigation(bool state);
+#else
+static inline void arm64_set_ssbd_mitigation(bool state) {}
+#endif
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit c32e1736ca03904c03de0e4459a673be194f56fd
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:10 2018 +0100

    arm64: ssbd: Add global mitigation state accessor
    
    We're about to need the mitigation state in various parts of the
    kernel in order to do the right thing for userspace and guests.
    
    Let's expose an accessor that will let other subsystems know
    about the state.
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b50650f3e496..b0fc3224ce8a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -543,6 +543,16 @@ static inline u64 read_zcr_features(void)
 #define ARM64_SSBD_FORCE_ENABLE		2
 #define ARM64_SSBD_MITIGATED		3
 
+static inline int arm64_get_ssbd_state(void)
+{
+#ifdef CONFIG_ARM64_SSBD
+	extern int ssbd_state;
+	return ssbd_state;
+#else
+	return ARM64_SSBD_UNKNOWN;
+#endif
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit a43ae4dfe56a01f5b98ba0cb2f784b6a43bafcc6
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:09 2018 +0100

    arm64: Add 'ssbd' command-line option
    
    On a system where the firmware implements ARCH_WORKAROUND_2,
    it may be useful to either permanently enable or disable the
    workaround for cases where the user decides that they'd rather
    not get a trap overhead, and keep the mitigation permanently
    on or off instead of switching it on exception entry/exit.
    
    In any case, default to the mitigation being enabled.
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 09b0f2a80c8f..b50650f3e496 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -537,6 +537,12 @@ static inline u64 read_zcr_features(void)
 	return zcr;
 }
 
+#define ARM64_SSBD_UNKNOWN		-1
+#define ARM64_SSBD_FORCE_DISABLE	0
+#define ARM64_SSBD_KERNEL		1
+#define ARM64_SSBD_FORCE_ENABLE		2
+#define ARM64_SSBD_MITIGATED		3
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 31dc52b3c8faf47bf3ff5ced661488a20e5d1811
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Apr 12 16:47:20 2018 +0100

    arm64/sve: Move read_zcr_features() out of cpufeature.h
    
    Having read_zcr_features() inline in cpufeature.h results in that
    header requiring #includes which make it hard to include
    <asm/fpsimd.h> elsewhere without triggering header inclusion
    cycles.
    
    This is not a hot-path function and arguably should not be in
    cpufeature.h in the first place, so this patch moves it to
    fpsimd.c, compiled conditionally if CONFIG_ARM64_SVE=y.
    
    This allows some SVE-related #includes to be dropped from
    cpufeature.h, which will ease future maintenance.
    
    A couple of missing #includes of <asm/fpsimd.h> are exposed by this
    change under arch/arm64/.  This patch adds the missing #includes as
    necessary.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 09b0f2a80c8f..0a6b7133195e 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -11,9 +11,7 @@
 
 #include <asm/cpucaps.h>
 #include <asm/cputype.h>
-#include <asm/fpsimd.h>
 #include <asm/hwcap.h>
-#include <asm/sigcontext.h>
 #include <asm/sysreg.h>
 
 /*
@@ -510,33 +508,6 @@ static inline bool system_supports_sve(void)
 		cpus_have_const_cap(ARM64_SVE);
 }
 
-/*
- * Read the pseudo-ZCR used by cpufeatures to identify the supported SVE
- * vector length.
- *
- * Use only if SVE is present.
- * This function clobbers the SVE vector length.
- */
-static inline u64 read_zcr_features(void)
-{
-	u64 zcr;
-	unsigned int vq_max;
-
-	/*
-	 * Set the maximum possible VL, and write zeroes to all other
-	 * bits to see if they stick.
-	 */
-	sve_kernel_enable(NULL);
-	write_sysreg_s(ZCR_ELx_LEN_MASK, SYS_ZCR_EL1);
-
-	zcr = read_sysreg_s(SYS_ZCR_EL1);
-	zcr &= ~(u64)ZCR_ELx_LEN_MASK; /* find sticky 1s outside LEN field */
-	vq_max = sve_vq_from_vl(sve_get_vl());
-	zcr |= vq_max - 1; /* set LEN field to maximum effective value */
-
-	return zcr;
-}
-
 #endif /* __ASSEMBLY__ */
 
 #endif

commit ba7d9233c21997eb7eb8514cfb21ff46247dc162
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:46 2018 +0100

    arm64: capabilities: Handle shared entries
    
    Some capabilities have different criteria for detection and associated
    actions based on the matching criteria, even though they all share the
    same capability bit. So far we have used multiple entries with the same
    capability bit to handle this. This is prone to errors, as the
    cpu_enable is invoked for each entry, irrespective of whether the
    detection rule applies to the CPU or not. And also this complicates
    other helpers, e.g, __this_cpu_has_cap.
    
    This patch adds a wrapper entry to cover all the possible variations
    of a capability by maintaining list of matches + cpu_enable callbacks.
    To avoid complicating the prototypes for the "matches()", we use
    arm64_cpu_capabilities maintain the list and we ignore all the other
    fields except the matches & cpu_enable.
    
    This ensures :
    
     1) The capabilitiy is set when at least one of the entry detects
     2) Action is only taken for the entries that "matches".
    
    This avoids explicit checks in the cpu_enable() take some action.
    The only constraint here is that, all the entries should have the
    same "type" (i.e, scope and conflict rules).
    
    If a cpu_enable() method is associated with multiple matches for a
    single capability, care should be taken that either the match criteria
    are mutually exclusive, or that the method is robust against being
    called multiple times.
    
    This also reverts the changes introduced by commit 67948af41f2e6818ed
    ("arm64: capabilities: Handle duplicate entries for a capability").
    
    Cc: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index a16eb0731290..09b0f2a80c8f 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -323,6 +323,18 @@ struct arm64_cpu_capabilities {
 			bool sign;
 			unsigned long hwcap;
 		};
+		/*
+		 * A list of "matches/cpu_enable" pair for the same
+		 * "capability" of the same "type" as described by the parent.
+		 * Only matches(), cpu_enable() and fields relevant to these
+		 * methods are significant in the list. The cpu_enable is
+		 * invoked only if the corresponding entry "matches()".
+		 * However, if a cpu_enable() method is associated
+		 * with multiple matches(), care should be taken that either
+		 * the match criteria are mutually exclusive, or that the
+		 * method is robust against being called multiple times.
+		 */
+		const struct arm64_cpu_capabilities *match_list;
 	};
 };
 

commit be5b299830c63ed76e0357473c4218c85fb388b3
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:45 2018 +0100

    arm64: capabilities: Add support for checks based on a list of MIDRs
    
    Add helpers for detecting an errata on list of midr ranges
    of affected CPUs, with the same work around.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index cd245871b578..a16eb0731290 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -314,6 +314,7 @@ struct arm64_cpu_capabilities {
 			} * const fixed_revs;
 		};
 
+		const struct midr_range *midr_range_list;
 		struct {	/* Feature register checking */
 			u32 sys_reg;
 			u8 field_pos;

commit 1df310505d6d544802016f6bae49aab836ae8510
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:44 2018 +0100

    arm64: Add helpers for checking CPU MIDR against a range
    
    Add helpers for checking if the given CPU midr falls in a range
    of variants/revisions for a given model.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 6a1280493f57..cd245871b578 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -10,6 +10,7 @@
 #define __ASM_CPUFEATURE_H
 
 #include <asm/cpucaps.h>
+#include <asm/cputype.h>
 #include <asm/fpsimd.h>
 #include <asm/hwcap.h>
 #include <asm/sigcontext.h>
@@ -306,8 +307,7 @@ struct arm64_cpu_capabilities {
 	void (*cpu_enable)(const struct arm64_cpu_capabilities *cap);
 	union {
 		struct {	/* To be used for erratum handling only */
-			u32 midr_model;
-			u32 midr_range_min, midr_range_max;
+			struct midr_range midr_range;
 			const struct arm64_midr_revidr {
 				u32 midr_rv;		/* revision/variant */
 				u32 revidr_mask;

commit 830dcc9f9a7cd26a812522a26efaacf7df6fc365
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:42 2018 +0100

    arm64: capabilities: Change scope of VHE to Boot CPU feature
    
    We expect all CPUs to be running at the same EL inside the kernel
    with or without VHE enabled and we have strict checks to ensure
    that any mismatch triggers a kernel panic. If VHE is enabled,
    we use the feature based on the boot CPU and all other CPUs
    should follow. This makes it a perfect candidate for a capability
    based on the boot CPU,  which should be matched by all the CPUs
    (both when is ON and OFF). This saves us some not-so-pretty
    hooks and special code, just for verifying the conflict.
    
    The patch also makes the VHE capability entry depend on
    CONFIG_ARM64_VHE.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 2f5edefdff99..6a1280493f57 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -287,6 +287,12 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 	(ARM64_CPUCAP_SCOPE_LOCAL_CPU		|	\
 	 ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU)
 
+/*
+ * CPU feature used early in the boot based on the boot CPU. All secondary
+ * CPUs must match the state of the capability as detected by the boot CPU.
+ */
+#define ARM64_CPUCAP_STRICT_BOOT_CPU_FEATURE ARM64_CPUCAP_SCOPE_BOOT_CPU
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;

commit fd9d63da17daf09c0099e3d5e3f0c0f03d9b251b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:41 2018 +0100

    arm64: capabilities: Add support for features enabled early
    
    The kernel detects and uses some of the features based on the boot
    CPU and expects that all the following CPUs conform to it. e.g,
    with VHE and the boot CPU running at EL2, the kernel decides to
    keep the kernel running at EL2. If another CPU is brought up without
    this capability, we use custom hooks (via check_early_cpu_features())
    to handle it. To handle such capabilities add support for detecting
    and enabling capabilities based on the boot CPU.
    
    A bit is added to indicate if the capability should be detected
    early on the boot CPU. The infrastructure then ensures that such
    capabilities are probed and "enabled" early on in the boot CPU
    and, enabled on the subsequent CPUs.
    
    Cc: Julien Thierry <julien.thierry@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 4f050259dff3..2f5edefdff99 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -108,7 +108,7 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *    value of a field in CPU ID feature register or checking the cpu
  *    model. The capability provides a call back ( @matches() ) to
  *    perform the check. Scope defines how the checks should be performed.
- *    There are two cases:
+ *    There are three cases:
  *
  *     a) SCOPE_LOCAL_CPU: check all the CPUs and "detect" if at least one
  *        matches. This implies, we have to run the check on all the
@@ -121,6 +121,11 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *        capability relies on a field in one of the CPU ID feature
  *        registers, we use the sanitised value of the register from the
  *        CPU feature infrastructure to make the decision.
+ *		Or
+ *     c) SCOPE_BOOT_CPU: Check only on the primary boot CPU to detect the
+ *        feature. This category is for features that are "finalised"
+ *        (or used) by the kernel very early even before the SMP cpus
+ *        are brought up.
  *
  *    The process of detection is usually denoted by "update" capability
  *    state in the code.
@@ -140,6 +145,11 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *    CPUs are treated "late CPUs" for capabilities determined by the boot
  *    CPU.
  *
+ *    At the moment there are two passes of finalising the capabilities.
+ *      a) Boot CPU scope capabilities - Finalised by primary boot CPU via
+ *         setup_boot_cpu_capabilities().
+ *      b) Everything except (a) - Run via setup_system_capabilities().
+ *
  * 3) Verification: When a CPU is brought online (e.g, by user or by the
  *    kernel), the kernel should make sure that it is safe to use the CPU,
  *    by verifying that the CPU is compliant with the state of the
@@ -148,12 +158,21 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *	secondary_start_kernel()-> check_local_cpu_capabilities()
  *
  *    As explained in (2) above, capabilities could be finalised at
- *    different points in the execution. Each CPU is verified against the
- *    "finalised" capabilities and if there is a conflict, the kernel takes
- *    an action, based on the severity (e.g, a CPU could be prevented from
- *    booting or cause a kernel panic). The CPU is allowed to "affect" the
- *    state of the capability, if it has not been finalised already.
- *    See section 5 for more details on conflicts.
+ *    different points in the execution. Each newly booted CPU is verified
+ *    against the capabilities that have been finalised by the time it
+ *    boots.
+ *
+ *	a) SCOPE_BOOT_CPU : All CPUs are verified against the capability
+ *	except for the primary boot CPU.
+ *
+ *	b) SCOPE_LOCAL_CPU, SCOPE_SYSTEM: All CPUs hotplugged on by the
+ *	user after the kernel boot are verified against the capability.
+ *
+ *    If there is a conflict, the kernel takes an action, based on the
+ *    severity (e.g, a CPU could be prevented from booting or cause a
+ *    kernel panic). The CPU is allowed to "affect" the state of the
+ *    capability, if it has not been finalised already. See section 5
+ *    for more details on conflicts.
  *
  * 4) Action: As mentioned in (2), the kernel can take an action for each
  *    detected capability, on all CPUs on the system. Appropriate actions
@@ -202,15 +221,26 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  */
 
 
-/* Decide how the capability is detected. On a local CPU vs System wide */
+/*
+ * Decide how the capability is detected.
+ * On any local CPU vs System wide vs the primary boot CPU
+ */
 #define ARM64_CPUCAP_SCOPE_LOCAL_CPU		((u16)BIT(0))
 #define ARM64_CPUCAP_SCOPE_SYSTEM		((u16)BIT(1))
+/*
+ * The capabilitiy is detected on the Boot CPU and is used by kernel
+ * during early boot. i.e, the capability should be "detected" and
+ * "enabled" as early as possibly on all booting CPUs.
+ */
+#define ARM64_CPUCAP_SCOPE_BOOT_CPU		((u16)BIT(2))
 #define ARM64_CPUCAP_SCOPE_MASK			\
 	(ARM64_CPUCAP_SCOPE_SYSTEM	|	\
-	 ARM64_CPUCAP_SCOPE_LOCAL_CPU)
+	 ARM64_CPUCAP_SCOPE_LOCAL_CPU	|	\
+	 ARM64_CPUCAP_SCOPE_BOOT_CPU)
 
 #define SCOPE_SYSTEM				ARM64_CPUCAP_SCOPE_SYSTEM
 #define SCOPE_LOCAL_CPU				ARM64_CPUCAP_SCOPE_LOCAL_CPU
+#define SCOPE_BOOT_CPU				ARM64_CPUCAP_SCOPE_BOOT_CPU
 #define SCOPE_ALL				ARM64_CPUCAP_SCOPE_MASK
 
 /*

commit d3aec8a28be3b88bf75442e7c24fd9da8d69a6df
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:40 2018 +0100

    arm64: capabilities: Restrict KPTI detection to boot-time CPUs
    
    KPTI is treated as a system wide feature and is only detected if all
    the CPUs in the sysetm needs the defense, unless it is forced via kernel
    command line. This leaves a system with a mix of CPUs with and without
    the defense vulnerable. Also, if a late CPU needs KPTI but KPTI was not
    activated at boot time, the CPU is currently allowed to boot, which is a
    potential security vulnerability.
    This patch ensures that the KPTI is turned on if at least one CPU detects
    the capability (i.e, change scope to SCOPE_LOCAL_CPU). Also rejetcs a late
    CPU, if it requires the defense, when the system hasn't enabled it,
    
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index effb1c038221..4f050259dff3 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -248,6 +248,15 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 	 ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU	|	\
 	 ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU)
 
+/*
+ * CPU feature detected at boot time, on one or more CPUs. A late CPU
+ * is not allowed to have the capability when the system doesn't have it.
+ * It is Ok for a late CPU to miss the feature.
+ */
+#define ARM64_CPUCAP_BOOT_RESTRICTED_CPU_LOCAL_FEATURE	\
+	(ARM64_CPUCAP_SCOPE_LOCAL_CPU		|	\
+	 ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU)
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;

commit 5c137714dd8cae464dbd5f028c07af149e6d09fc
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:39 2018 +0100

    arm64: capabilities: Introduce weak features based on local CPU
    
    Now that we have the flexibility of defining system features based
    on individual CPUs, introduce CPU feature type that can be detected
    on a local SCOPE and ignores the conflict on late CPUs. This is
    applicable for ARM64_HAS_NO_HW_PREFETCH, where it is fine for
    the system to have CPUs without hardware prefetch turning up
    later. We only suffer a performance penalty, nothing fatal.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index c187b926daf9..effb1c038221 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -239,6 +239,14 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  */
 #define ARM64_CPUCAP_SYSTEM_FEATURE	\
 	(ARM64_CPUCAP_SCOPE_SYSTEM | ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU)
+/*
+ * CPU feature detected at boot time based on feature of one or more CPUs.
+ * All possible conflicts for a late CPU are ignored.
+ */
+#define ARM64_CPUCAP_WEAK_LOCAL_CPU_FEATURE		\
+	(ARM64_CPUCAP_SCOPE_LOCAL_CPU		|	\
+	 ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU	|	\
+	 ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU)
 
 struct arm64_cpu_capabilities {
 	const char *desc;

commit cce360b54ce6ca1bcf4b0a870ec076d83606775e
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:34 2018 +0100

    arm64: capabilities: Filter the entries based on a given mask
    
    While processing the list of capabilities, it is useful to
    filter out some of the entries based on the given mask for the
    scope of the capabilities to allow better control. This can be
    used later for handling LOCAL vs SYSTEM wide capabilities and more.
    All capabilities should have their scope set to either LOCAL_CPU or
    SYSTEM. No functional/flow change.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b8d8e6012385..c187b926daf9 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -211,6 +211,7 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 
 #define SCOPE_SYSTEM				ARM64_CPUCAP_SCOPE_SYSTEM
 #define SCOPE_LOCAL_CPU				ARM64_CPUCAP_SCOPE_LOCAL_CPU
+#define SCOPE_ALL				ARM64_CPUCAP_SCOPE_MASK
 
 /*
  * Is it permitted for a late CPU to have this capability when system

commit 5b4747c5dce7a873e1e7fe1608835825f714267a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:32 2018 +0100

    arm64: capabilities: Add flags to handle the conflicts on late CPU
    
    When a CPU is brought up, it is checked against the caps that are
    known to be enabled on the system (via verify_local_cpu_capabilities()).
    Based on the state of the capability on the CPU vs. that of System we
    could have the following combinations of conflict.
    
            x-----------------------------x
            | Type  | System   | Late CPU |
            |-----------------------------|
            |  a    |   y      |    n     |
            |-----------------------------|
            |  b    |   n      |    y     |
            x-----------------------------x
    
    Case (a) is not permitted for caps which are system features, which the
    system expects all the CPUs to have (e.g VHE). While (a) is ignored for
    all errata work arounds. However, there could be exceptions to the plain
    filtering approach. e.g, KPTI is an optional feature for a late CPU as
    long as the system already enables it.
    
    Case (b) is not permitted for errata work arounds that cannot be activated
    after the kernel has finished booting.And we ignore (b) for features. Here,
    yet again, KPTI is an exception, where if a late CPU needs KPTI we are too
    late to enable it (because we change the allocation of ASIDs etc).
    
    Add two different flags to indicate how the conflict should be handled.
    
     ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU - CPUs may have the capability
     ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU - CPUs may not have the cappability.
    
    Now that we have the flags to describe the behavior of the errata and
    the features, as we treat them, define types for ERRATUM and FEATURE.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 13fde0952c31..b8d8e6012385 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -153,6 +153,7 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *    an action, based on the severity (e.g, a CPU could be prevented from
  *    booting or cause a kernel panic). The CPU is allowed to "affect" the
  *    state of the capability, if it has not been finalised already.
+ *    See section 5 for more details on conflicts.
  *
  * 4) Action: As mentioned in (2), the kernel can take an action for each
  *    detected capability, on all CPUs on the system. Appropriate actions
@@ -170,6 +171,34 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
  *
  *	  check_local_cpu_capabilities() -> verify_local_cpu_capabilities()
  *
+ * 5) Conflicts: Based on the state of the capability on a late CPU vs.
+ *    the system state, we could have the following combinations :
+ *
+ *		x-----------------------------x
+ *		| Type  | System   | Late CPU |
+ *		|-----------------------------|
+ *		|  a    |   y      |    n     |
+ *		|-----------------------------|
+ *		|  b    |   n      |    y     |
+ *		x-----------------------------x
+ *
+ *     Two separate flag bits are defined to indicate whether each kind of
+ *     conflict can be allowed:
+ *		ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU - Case(a) is allowed
+ *		ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU - Case(b) is allowed
+ *
+ *     Case (a) is not permitted for a capability that the system requires
+ *     all CPUs to have in order for the capability to be enabled. This is
+ *     typical for capabilities that represent enhanced functionality.
+ *
+ *     Case (b) is not permitted for a capability that must be enabled
+ *     during boot if any CPU in the system requires it in order to run
+ *     safely. This is typical for erratum work arounds that cannot be
+ *     enabled after the corresponding capability is finalised.
+ *
+ *     In some non-typical cases either both (a) and (b), or neither,
+ *     should be permitted. This can be described by including neither
+ *     or both flags in the capability's type field.
  */
 
 
@@ -183,6 +212,33 @@ extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 #define SCOPE_SYSTEM				ARM64_CPUCAP_SCOPE_SYSTEM
 #define SCOPE_LOCAL_CPU				ARM64_CPUCAP_SCOPE_LOCAL_CPU
 
+/*
+ * Is it permitted for a late CPU to have this capability when system
+ * hasn't already enabled it ?
+ */
+#define ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU	((u16)BIT(4))
+/* Is it safe for a late CPU to miss this capability when system has it */
+#define ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU	((u16)BIT(5))
+
+/*
+ * CPU errata workarounds that need to be enabled at boot time if one or
+ * more CPUs in the system requires it. When one of these capabilities
+ * has been enabled, it is safe to allow any CPU to boot that doesn't
+ * require the workaround. However, it is not safe if a "late" CPU
+ * requires a workaround and the system hasn't enabled it already.
+ */
+#define ARM64_CPUCAP_LOCAL_CPU_ERRATUM		\
+	(ARM64_CPUCAP_SCOPE_LOCAL_CPU | ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU)
+/*
+ * CPU feature detected at boot time based on system-wide value of a
+ * feature. It is safe for a late CPU to have this feature even though
+ * the system hasn't enabled it, although the featuer will not be used
+ * by Linux in this case. If the system has enabled this feature already,
+ * then every late CPU must have it.
+ */
+#define ARM64_CPUCAP_SYSTEM_FEATURE	\
+	(ARM64_CPUCAP_SCOPE_SYSTEM | ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU)
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;
@@ -220,6 +276,18 @@ static inline int cpucap_default_scope(const struct arm64_cpu_capabilities *cap)
 	return cap->type & ARM64_CPUCAP_SCOPE_MASK;
 }
 
+static inline bool
+cpucap_late_cpu_optional(const struct arm64_cpu_capabilities *cap)
+{
+	return !!(cap->type & ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU);
+}
+
+static inline bool
+cpucap_late_cpu_permitted(const struct arm64_cpu_capabilities *cap)
+{
+	return !!(cap->type & ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU);
+}
+
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;

commit 143ba05d867af34827faf99e0eed4de27106c7cb
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:31 2018 +0100

    arm64: capabilities: Prepare for fine grained capabilities
    
    We use arm64_cpu_capabilities to represent CPU ELF HWCAPs exposed
    to the userspace and the CPU hwcaps used by the kernel, which
    include cpu features and CPU errata work arounds. Capabilities
    have some properties that decide how they should be treated :
    
     1) Detection, i.e scope : A cap could be "detected" either :
        - if it is present on at least one CPU (SCOPE_LOCAL_CPU)
            Or
        - if it is present on all the CPUs (SCOPE_SYSTEM)
    
     2) When is it enabled ? - A cap is treated as "enabled" when the
      system takes some action based on whether the capability is detected or
      not. e.g, setting some control register, patching the kernel code.
      Right now, we treat all caps are enabled at boot-time, after all
      the CPUs are brought up by the kernel. But there are certain caps,
      which are enabled early during the boot (e.g, VHE, GIC_CPUIF for NMI)
      and kernel starts using them, even before the secondary CPUs are brought
      up. We would need a way to describe this for each capability.
    
     3) Conflict on a late CPU - When a CPU is brought up, it is checked
      against the caps that are known to be enabled on the system (via
      verify_local_cpu_capabilities()). Based on the state of the capability
      on the CPU vs. that of System we could have the following combinations
      of conflict.
    
            x-----------------------------x
            | Type  | System   | Late CPU |
            ------------------------------|
            |  a    |   y      |    n     |
            ------------------------------|
            |  b    |   n      |    y     |
            x-----------------------------x
    
      Case (a) is not permitted for caps which are system features, which the
      system expects all the CPUs to have (e.g VHE). While (a) is ignored for
      all errata work arounds. However, there could be exceptions to the plain
      filtering approach. e.g, KPTI is an optional feature for a late CPU as
      long as the system already enables it.
    
      Case (b) is not permitted for errata work arounds which requires some
      work around, which cannot be delayed. And we ignore (b) for features.
      Here, yet again, KPTI is an exception, where if a late CPU needs KPTI we
      are too late to enable it (because we change the allocation of ASIDs
      etc).
    
    So this calls for a lot more fine grained behavior for each capability.
    And if we define all the attributes to control their behavior properly,
    we may be able to use a single table for the CPU hwcaps (which cover
    errata and features, not the ELF HWCAPs). This is a prepartory step
    to get there. More bits would be added for the properties listed above.
    
    We are going to use a bit-mask to encode all the properties of a
    capabilities. This patch encodes the "SCOPE" of the capability.
    
    As such there is no change in how the capabilities are treated.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8efbda2858a8..13fde0952c31 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -89,16 +89,104 @@ struct arm64_ftr_reg {
 
 extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
 
-/* scope of capability check */
-enum {
-	SCOPE_SYSTEM,
-	SCOPE_LOCAL_CPU,
-};
+/*
+ * CPU capabilities:
+ *
+ * We use arm64_cpu_capabilities to represent system features, errata work
+ * arounds (both used internally by kernel and tracked in cpu_hwcaps) and
+ * ELF HWCAPs (which are exposed to user).
+ *
+ * To support systems with heterogeneous CPUs, we need to make sure that we
+ * detect the capabilities correctly on the system and take appropriate
+ * measures to ensure there are no incompatibilities.
+ *
+ * This comment tries to explain how we treat the capabilities.
+ * Each capability has the following list of attributes :
+ *
+ * 1) Scope of Detection : The system detects a given capability by
+ *    performing some checks at runtime. This could be, e.g, checking the
+ *    value of a field in CPU ID feature register or checking the cpu
+ *    model. The capability provides a call back ( @matches() ) to
+ *    perform the check. Scope defines how the checks should be performed.
+ *    There are two cases:
+ *
+ *     a) SCOPE_LOCAL_CPU: check all the CPUs and "detect" if at least one
+ *        matches. This implies, we have to run the check on all the
+ *        booting CPUs, until the system decides that state of the
+ *        capability is finalised. (See section 2 below)
+ *		Or
+ *     b) SCOPE_SYSTEM: check all the CPUs and "detect" if all the CPUs
+ *        matches. This implies, we run the check only once, when the
+ *        system decides to finalise the state of the capability. If the
+ *        capability relies on a field in one of the CPU ID feature
+ *        registers, we use the sanitised value of the register from the
+ *        CPU feature infrastructure to make the decision.
+ *
+ *    The process of detection is usually denoted by "update" capability
+ *    state in the code.
+ *
+ * 2) Finalise the state : The kernel should finalise the state of a
+ *    capability at some point during its execution and take necessary
+ *    actions if any. Usually, this is done, after all the boot-time
+ *    enabled CPUs are brought up by the kernel, so that it can make
+ *    better decision based on the available set of CPUs. However, there
+ *    are some special cases, where the action is taken during the early
+ *    boot by the primary boot CPU. (e.g, running the kernel at EL2 with
+ *    Virtualisation Host Extensions). The kernel usually disallows any
+ *    changes to the state of a capability once it finalises the capability
+ *    and takes any action, as it may be impossible to execute the actions
+ *    safely. A CPU brought up after a capability is "finalised" is
+ *    referred to as "Late CPU" w.r.t the capability. e.g, all secondary
+ *    CPUs are treated "late CPUs" for capabilities determined by the boot
+ *    CPU.
+ *
+ * 3) Verification: When a CPU is brought online (e.g, by user or by the
+ *    kernel), the kernel should make sure that it is safe to use the CPU,
+ *    by verifying that the CPU is compliant with the state of the
+ *    capabilities finalised already. This happens via :
+ *
+ *	secondary_start_kernel()-> check_local_cpu_capabilities()
+ *
+ *    As explained in (2) above, capabilities could be finalised at
+ *    different points in the execution. Each CPU is verified against the
+ *    "finalised" capabilities and if there is a conflict, the kernel takes
+ *    an action, based on the severity (e.g, a CPU could be prevented from
+ *    booting or cause a kernel panic). The CPU is allowed to "affect" the
+ *    state of the capability, if it has not been finalised already.
+ *
+ * 4) Action: As mentioned in (2), the kernel can take an action for each
+ *    detected capability, on all CPUs on the system. Appropriate actions
+ *    include, turning on an architectural feature, modifying the control
+ *    registers (e.g, SCTLR, TCR etc.) or patching the kernel via
+ *    alternatives. The kernel patching is batched and performed at later
+ *    point. The actions are always initiated only after the capability
+ *    is finalised. This is usally denoted by "enabling" the capability.
+ *    The actions are initiated as follows :
+ *	a) Action is triggered on all online CPUs, after the capability is
+ *	finalised, invoked within the stop_machine() context from
+ *	enable_cpu_capabilitie().
+ *
+ *	b) Any late CPU, brought up after (1), the action is triggered via:
+ *
+ *	  check_local_cpu_capabilities() -> verify_local_cpu_capabilities()
+ *
+ */
+
+
+/* Decide how the capability is detected. On a local CPU vs System wide */
+#define ARM64_CPUCAP_SCOPE_LOCAL_CPU		((u16)BIT(0))
+#define ARM64_CPUCAP_SCOPE_SYSTEM		((u16)BIT(1))
+#define ARM64_CPUCAP_SCOPE_MASK			\
+	(ARM64_CPUCAP_SCOPE_SYSTEM	|	\
+	 ARM64_CPUCAP_SCOPE_LOCAL_CPU)
+
+#define SCOPE_SYSTEM				ARM64_CPUCAP_SCOPE_SYSTEM
+#define SCOPE_LOCAL_CPU				ARM64_CPUCAP_SCOPE_LOCAL_CPU
 
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;
-	int def_scope;			/* default scope */
+	u16 type;
 	bool (*matches)(const struct arm64_cpu_capabilities *caps, int scope);
 	/*
 	 * Take the appropriate actions to enable this capability for this CPU.
@@ -127,6 +215,11 @@ struct arm64_cpu_capabilities {
 	};
 };
 
+static inline int cpucap_default_scope(const struct arm64_cpu_capabilities *cap)
+{
+	return cap->type & ARM64_CPUCAP_SCOPE_MASK;
+}
+
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 extern struct static_key_false arm64_const_caps_ready;

commit 1e89baed5d50d2b8d9fd420830902570270703f1
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:30 2018 +0100

    arm64: capabilities: Move errata processing code
    
    We have errata work around processing code in cpu_errata.c,
    which calls back into helpers defined in cpufeature.c. Now
    that we are going to make the handling of capabilities
    generic, by adding the information to each capability,
    move the errata work around specific processing code.
    No functional changes.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index cba67cb24b22..8efbda2858a8 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -245,15 +245,8 @@ static inline bool id_aa64pfr0_sve(u64 pfr0)
 }
 
 void __init setup_cpu_features(void);
-
-void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
-			    const char *info);
-void enable_cpu_capabilities(const struct arm64_cpu_capabilities *caps);
 void check_local_cpu_capabilities(void);
 
-void update_cpu_errata_workarounds(void);
-void __init enable_errata_workarounds(void);
-void verify_local_cpu_errata_workarounds(void);
 
 u64 read_sanitised_ftr_reg(u32 id);
 

commit c0cda3b8ee6b4b6851b2fd8b6db91fd7b0e2524a
Author: Dave Martin <dave.martin@arm.com>
Date:   Mon Mar 26 15:12:28 2018 +0100

    arm64: capabilities: Update prototype for enable call back
    
    We issue the enable() call back for all CPU hwcaps capabilities
    available on the system, on all the CPUs. So far we have ignored
    the argument passed to the call back, which had a prototype to
    accept a "void *" for use with on_each_cpu() and later with
    stop_machine(). However, with commit 0a0d111d40fd1
    ("arm64: cpufeature: Pass capability structure to ->enable callback"),
    there are some users of the argument who wants the matching capability
    struct pointer where there are multiple matching criteria for a single
    capability. Clean up the declaration of the call back to make it clear.
    
     1) Renamed to cpu_enable(), to imply taking necessary actions on the
        called CPU for the entry.
     2) Pass const pointer to the capability, to allow the call back to
        check the entry. (e.,g to check if any action is needed on the CPU)
     3) We don't care about the result of the call back, turning this to
        a void.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [suzuki: convert more users, rename call back and drop results]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index fbf0aab94d67..cba67cb24b22 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -100,7 +100,12 @@ struct arm64_cpu_capabilities {
 	u16 capability;
 	int def_scope;			/* default scope */
 	bool (*matches)(const struct arm64_cpu_capabilities *caps, int scope);
-	int (*enable)(void *);		/* Called on all active CPUs */
+	/*
+	 * Take the appropriate actions to enable this capability for this CPU.
+	 * For each successfully booted CPU, this method is called for each
+	 * globally detected capability.
+	 */
+	void (*cpu_enable)(const struct arm64_cpu_capabilities *cap);
 	union {
 		struct {	/* To be used for erratum handling only */
 			u32 midr_model;

commit e8002e02abf052c07bb87b867789034bc79aac10
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 6 17:15:34 2018 +0000

    arm64/errata: add REVIDR handling to framework
    
    In some cases, core variants that are affected by a certain erratum
    also exist in versions that have the erratum fixed, and this fact is
    recorded in a dedicated bit in system register REVIDR_EL1.
    
    Since the architecture does not require that a certain bit retains
    its meaning across different variants of the same model, each such
    REVIDR bit is tightly coupled to a certain revision/variant value,
    and so we need a list of revidr_mask/midr pairs to carry this
    information.
    
    So add the struct member and the associated macros and handling to
    allow REVIDR fixes to be taken into account.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 060e3a4008ab..fbf0aab94d67 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -105,6 +105,10 @@ struct arm64_cpu_capabilities {
 		struct {	/* To be used for erratum handling only */
 			u32 midr_model;
 			u32 midr_range_min, midr_range_max;
+			const struct arm64_midr_revidr {
+				u32 midr_rv;		/* revision/variant */
+				u32 revidr_mask;
+			} * const fixed_revs;
 		};
 
 		struct {	/* Feature register checking */

commit 3fab39997a98b97138c886978af660c4f6c7e9e6
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Dec 14 14:03:44 2017 +0000

    arm64/sve: Report SVE to userspace via CPUID only if supported
    
    Currently, the SVE field in ID_AA64PFR0_EL1 is visible
    unconditionally to userspace via the CPU ID register emulation,
    irrespective of the kernel config.  This means that if a kernel
    configured with CONFIG_ARM64_SVE=n is run on SVE-capable hardware,
    userspace will see SVE reported as present in the ID regs even
    though the kernel forbids execution of SVE instructions.
    
    This patch makes the exposure of the SVE field in ID_AA64PFR0_EL1
    conditional on CONFIG_ARM64_SVE=y.
    
    Since future architecture features are likely to encounter a
    similar requirement, this patch adds a suitable helper macros for
    use when declaring config-conditional ID register fields.
    
    Fixes: 43994d824e84 ("arm64/sve: Detect SVE and activate runtime support")
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Cc: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index ac67cfc2585a..060e3a4008ab 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -60,6 +60,9 @@ enum ftr_type {
 #define FTR_VISIBLE	true	/* Feature visible to the user space */
 #define FTR_HIDDEN	false	/* Feature is hidden from the user */
 
+#define FTR_VISIBLE_IF_IS_ENABLED(config)		\
+	(IS_ENABLED(config) ? FTR_VISIBLE : FTR_HIDDEN)
+
 struct arm64_ftr_bits {
 	bool		sign;	/* Value is signed ? */
 	bool		visible;

commit 43994d824e8443263dc98b151e6326bf677be52e
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:19 2017 +0000

    arm64/sve: Detect SVE and activate runtime support
    
    This patch enables detection of hardware SVE support via the
    cpufeatures framework, and reports its presence to the kernel and
    userspace via the new ARM64_SVE cpucap and HWCAP_SVE hwcap
    respectively.
    
    Userspace can also detect SVE using ID_AA64PFR0_EL1, using the
    cpufeatures MRS emulation.
    
    When running on hardware that supports SVE, this enables runtime
    kernel support for SVE, and allows user tasks to execute SVE
    instructions and make of the of the SVE-specific user/kernel
    interface extensions implemented by this series.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 9b27e8c10086..ac67cfc2585a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -273,7 +273,8 @@ static inline bool system_uses_ttbr0_pan(void)
 
 static inline bool system_supports_sve(void)
 {
-	return false;
+	return IS_ENABLED(CONFIG_ARM64_SVE) &&
+		cpus_have_const_cap(ARM64_SVE);
 }
 
 /*

commit 2e0f2478ea37eba945bee007884a2988b8f7d332
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:10 2017 +0000

    arm64/sve: Probe SVE capabilities and usable vector lengths
    
    This patch uses the cpufeatures framework to determine common SVE
    capabilities and vector lengths, and configures the runtime SVE
    support code appropriately.
    
    ZCR_ELx is not really a feature register, but it is convenient to
    use it as a template for recording the maximum vector length
    supported by a CPU, using the LEN field.  This field is similar to
    a feature field in that it is a contiguous bitfield for which we
    want to determine the minimum system-wide value.  This patch adds
    ZCR as a pseudo-register in cpuinfo/cpufeatures, with appropriate
    custom code to populate it.  Finding the minimum supported value of
    the LEN field is left to the cpufeatures framework in the usual
    way.
    
    The meaning of ID_AA64ZFR0_EL1 is not architecturally defined yet,
    so for now we just require it to be zero.
    
    Note that much of this code is dormant and SVE still won't be used
    yet, since system_supports_sve() remains hardwired to false.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 4ea3441a8fa7..9b27e8c10086 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -10,7 +10,9 @@
 #define __ASM_CPUFEATURE_H
 
 #include <asm/cpucaps.h>
+#include <asm/fpsimd.h>
 #include <asm/hwcap.h>
+#include <asm/sigcontext.h>
 #include <asm/sysreg.h>
 
 /*
@@ -223,6 +225,13 @@ static inline bool id_aa64pfr0_32bit_el0(u64 pfr0)
 	return val == ID_AA64PFR0_EL0_32BIT_64BIT;
 }
 
+static inline bool id_aa64pfr0_sve(u64 pfr0)
+{
+	u32 val = cpuid_feature_extract_unsigned_field(pfr0, ID_AA64PFR0_SVE_SHIFT);
+
+	return val > 0;
+}
+
 void __init setup_cpu_features(void);
 
 void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
@@ -267,6 +276,33 @@ static inline bool system_supports_sve(void)
 	return false;
 }
 
+/*
+ * Read the pseudo-ZCR used by cpufeatures to identify the supported SVE
+ * vector length.
+ *
+ * Use only if SVE is present.
+ * This function clobbers the SVE vector length.
+ */
+static inline u64 read_zcr_features(void)
+{
+	u64 zcr;
+	unsigned int vq_max;
+
+	/*
+	 * Set the maximum possible VL, and write zeroes to all other
+	 * bits to see if they stick.
+	 */
+	sve_kernel_enable(NULL);
+	write_sysreg_s(ZCR_ELx_LEN_MASK, SYS_ZCR_EL1);
+
+	zcr = read_sysreg_s(SYS_ZCR_EL1);
+	zcr &= ~(u64)ZCR_ELx_LEN_MASK; /* find sticky 1s outside LEN field */
+	vq_max = sve_vq_from_vl(sve_get_vl());
+	zcr |= vq_max - 1; /* set LEN field to maximum effective value */
+
+	return zcr;
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit ddd25ad1fde8456810dee1b26a870395bcd6339d
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:02 2017 +0000

    arm64/sve: Kconfig update and conditional compilation support
    
    This patch adds CONFIG_ARM64_SVE to control building of SVE support
    into the kernel, and adds a stub predicate system_supports_sve() to
    control conditional compilation and runtime SVE support.
    
    system_supports_sve() just returns false for now: it will be
    replaced with a non-trivial implementation in a later patch, once
    SVE support is complete enough to be enabled safely.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 428ee1f2468c..4ea3441a8fa7 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -262,6 +262,11 @@ static inline bool system_uses_ttbr0_pan(void)
 		!cpus_have_const_cap(ARM64_HAS_PAN);
 }
 
+static inline bool system_supports_sve(void)
+{
+	return false;
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 63a1e1c95e60e798fa09ab3c536fb555aa5bbf2b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 16 15:18:05 2017 +0100

    arm64/cpufeature: don't use mutex in bringup path
    
    Currently, cpus_set_cap() calls static_branch_enable_cpuslocked(), which
    must take the jump_label mutex.
    
    We call cpus_set_cap() in the secondary bringup path, from the idle
    thread where interrupts are disabled. Taking a mutex in this path "is a
    NONO" regardless of whether it's contended, and something we must avoid.
    We didn't spot this until recently, as ___might_sleep() won't warn for
    this case until all CPUs have been brought up.
    
    This patch avoids taking the mutex in the secondary bringup path. The
    poking of static keys is deferred until enable_cpu_capabilities(), which
    runs in a suitable context on the boot CPU. To account for the static
    keys being set later, cpus_have_const_cap() is updated to use another
    static key to check whether the const cap keys have been initialised,
    falling back to the caps bitmap until this is the case.
    
    This means that users of cpus_have_const_cap() gain should only gain a
    single additional NOP in the fast path once the const caps are
    initialised, but should always see the current cap value.
    
    The hyp code should never dereference the caps array, since the caps are
    initialized before we run the module initcall to initialise hyp. A check
    is added to the hyp init code to document this requirement.
    
    This change will sidestep a number of issues when the upcoming hotplug
    locking rework is merged.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Marc Zyniger <marc.zyngier@arm.com>
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Sewior <bigeasy@linutronix.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index e7f84a7b4465..428ee1f2468c 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -115,6 +115,7 @@ struct arm64_cpu_capabilities {
 
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
+extern struct static_key_false arm64_const_caps_ready;
 
 bool this_cpu_has_cap(unsigned int cap);
 
@@ -124,7 +125,7 @@ static inline bool cpu_have_feature(unsigned int num)
 }
 
 /* System capability check for constant caps */
-static inline bool cpus_have_const_cap(int num)
+static inline bool __cpus_have_const_cap(int num)
 {
 	if (num >= ARM64_NCAPS)
 		return false;
@@ -138,6 +139,14 @@ static inline bool cpus_have_cap(unsigned int num)
 	return test_bit(num, cpu_hwcaps);
 }
 
+static inline bool cpus_have_const_cap(int num)
+{
+	if (static_branch_likely(&arm64_const_caps_ready))
+		return __cpus_have_const_cap(num);
+	else
+		return cpus_have_cap(num);
+}
+
 static inline void cpus_set_cap(unsigned int num)
 {
 	if (num >= ARM64_NCAPS) {
@@ -145,7 +154,6 @@ static inline void cpus_set_cap(unsigned int num)
 			num, ARM64_NCAPS);
 	} else {
 		__set_bit(num, cpu_hwcaps);
-		static_branch_enable(&cpu_hwcap_keys[num]);
 	}
 }
 

commit 46823dd17c676d1e1830774e93be813dc3638d6c
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Mar 23 15:14:39 2017 +0000

    arm64: cpufeature: Make ID reg accessor naming less counterintuitive
    
    read_system_reg() can readily be confused with read_sysreg(),
    whereas these are really quite different in their meaning.
    
    This patches attempts to reduce the ambiguity be reserving "sysreg"
    for the actual system register accessors.
    
    read_system_reg() is instead renamed to read_sanitised_ftr_reg(),
    to make it more obvious that the Linux-defined sanitised feature
    register cache is being accessed here, not the underlying
    architectural system registers.
    
    cpufeature.c's internal __raw_read_system_reg() function is renamed
    in line with its actual purpose: a form of read_sysreg() that
    indexes on (non-compiletime-constant) encoding rather than symbolic
    register name.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index f31c48d0cd68..e7f84a7b4465 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -226,7 +226,7 @@ void update_cpu_errata_workarounds(void);
 void __init enable_errata_workarounds(void);
 void verify_local_cpu_errata_workarounds(void);
 
-u64 read_system_reg(u32 id);
+u64 read_sanitised_ftr_reg(u32 id);
 
 static inline bool cpu_supports_mixed_endian_el0(void)
 {
@@ -240,7 +240,7 @@ static inline bool system_supports_32bit_el0(void)
 
 static inline bool system_supports_mixed_endian_el0(void)
 {
-	return id_aa64mmfr0_mixed_endian_el0(read_system_reg(SYS_ID_AA64MMFR0_EL1));
+	return id_aa64mmfr0_mixed_endian_el0(read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1));
 }
 
 static inline bool system_supports_fpsimd(void)

commit 14088540ad63c648e5cdf490412033f792d16b6b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Mar 10 17:44:18 2017 +0000

    arm64: use const cap for system_uses_ttbr0_pan()
    
    Since commit 4b65a5db362783ab ("arm64: Introduce
    uaccess_{disable,enable} functionality based on TTBR0_EL1"),
    system_uses_ttbr0_pan() has used cpus_have_cap() to determine whether
    PAN is present.
    
    Since commit a4023f682739439b ("arm64: Add hypervisor safe helper for
    checking constant capabilities"), which was introduced around the same
    time, cpus_have_cap() doesn't try to use a static key, and must always
    perform a load, test, and consitional branch (likely a tbnz for the
    latter two).
    
    Elsewhere, we moved to using cpus_have_const_cap(), which can use a
    static key (i.e. a non-conditional branch), which is patched at runtime
    when the feature is detected.
    
    This patch makes system_uses_ttbr0_pan() use cpus_have_const_cap(). The
    static key is likely a win for hot-paths like the uacccess primitives,
    and this makes our usage consistent regardless.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 05310ad8c5ab..f31c48d0cd68 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -251,7 +251,7 @@ static inline bool system_supports_fpsimd(void)
 static inline bool system_uses_ttbr0_pan(void)
 {
 	return IS_ENABLED(CONFIG_ARM64_SW_TTBR0_PAN) &&
-		!cpus_have_cap(ARM64_HAS_PAN);
+		!cpus_have_const_cap(ARM64_HAS_PAN);
 }
 
 #endif /* __ASSEMBLY__ */

commit 638f863dbbc8da16834ee0acc6ac10754f79c486
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Feb 23 16:03:17 2017 +0000

    arm64/cpufeature: check correct field width when updating sys_val
    
    When we're updating a register's sys_val, we use arm64_ftr_value() to
    find the new field value. We use cpuid_feature_extract_field() to find
    the new value, but this implicitly assumes a 4-bit field, so we may
    extract more bits than we mean to for fields like CTR_EL0.L1ip.
    
    This affects update_cpu_ftr_reg(), where we may extract erroneous values
    for ftr_cur and ftr_new. Depending on the additional bits extracted in
    either case, we may erroneously detect that the value is mismatched, and
    we'll try to compute a new safe value.
    
    Dependent on these extra bits and feature type, arm64_ftr_safe_value()
    may pessimistically select the always-safe value, or may erroneously
    choose either the extracted cur or new value as the safe option. The
    extra bits will subsequently be masked out in arm64_ftr_set_value(), so
    we may choose a higher value, yet write back a lower one.
    
    Fix this by passing the width down explicitly in arm64_ftr_value(), so
    we always extract the correct amount.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 4ce82ed3e7c3..05310ad8c5ab 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -184,16 +184,22 @@ static inline u64 arm64_ftr_reg_user_value(const struct arm64_ftr_reg *reg)
 }
 
 static inline int __attribute_const__
-cpuid_feature_extract_field(u64 features, int field, bool sign)
+cpuid_feature_extract_field_width(u64 features, int field, int width, bool sign)
 {
 	return (sign) ?
-		cpuid_feature_extract_signed_field(features, field) :
-		cpuid_feature_extract_unsigned_field(features, field);
+		cpuid_feature_extract_signed_field_width(features, field, width) :
+		cpuid_feature_extract_unsigned_field_width(features, field, width);
+}
+
+static inline int __attribute_const__
+cpuid_feature_extract_field(u64 features, int field, bool sign)
+{
+	return cpuid_feature_extract_field_width(features, field, 4, sign);
 }
 
 static inline s64 arm64_ftr_value(const struct arm64_ftr_bits *ftrp, u64 val)
 {
-	return (s64)cpuid_feature_extract_field(val, ftrp->shift, ftrp->sign);
+	return (s64)cpuid_feature_extract_field_width(val, ftrp->shift, ftrp->width, ftrp->sign);
 }
 
 static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)

commit fe4fbdbcddeaab58a4f9b5297f28b8a4babf6f1f
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 9 17:28:30 2017 +0000

    arm64: cpufeature: Track user visible fields
    
    Track the user visible fields of a CPU feature register. This will be
    used for exposing the value to the userspace. All the user visible
    fields of a feature register will be passed on as it is, while the
    others would be filled with their respective safe value.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 10d56242e649..4ce82ed3e7c3 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -55,8 +55,12 @@ enum ftr_type {
 #define FTR_SIGNED	true	/* Value should be treated as signed */
 #define FTR_UNSIGNED	false	/* Value should be treated as unsigned */
 
+#define FTR_VISIBLE	true	/* Feature visible to the user space */
+#define FTR_HIDDEN	false	/* Feature is hidden from the user */
+
 struct arm64_ftr_bits {
 	bool		sign;	/* Value is signed ? */
+	bool		visible;
 	bool		strict;	/* CPU Sanity check: strict matching required ? */
 	enum ftr_type	type;
 	u8		shift;
@@ -72,7 +76,9 @@ struct arm64_ftr_bits {
 struct arm64_ftr_reg {
 	const char			*name;
 	u64				strict_mask;
+	u64				user_mask;
 	u64				sys_val;
+	u64				user_val;
 	const struct arm64_ftr_bits	*ftr_bits;
 };
 
@@ -172,6 +178,11 @@ static inline u64 arm64_ftr_mask(const struct arm64_ftr_bits *ftrp)
 	return (u64)GENMASK(ftrp->shift + ftrp->width - 1, ftrp->shift);
 }
 
+static inline u64 arm64_ftr_reg_user_value(const struct arm64_ftr_reg *reg)
+{
+	return (reg->user_val | (reg->sys_val & reg->user_mask));
+}
+
 static inline int __attribute_const__
 cpuid_feature_extract_field(u64 features, int field, bool sign)
 {

commit 156e0d57f8bc9a4c0f09f4f79af7b51c6d7d0600
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 9 17:28:27 2017 +0000

    arm64: cpufeature: Document the rules of safe value for features
    
    Document the rules for choosing the safe value for different types
    of features.
    
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b4989df48670..10d56242e649 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -29,7 +29,20 @@
 #include <linux/jump_label.h>
 #include <linux/kernel.h>
 
-/* CPU feature register tracking */
+/*
+ * CPU feature register tracking
+ *
+ * The safe value of a CPUID feature field is dependent on the implications
+ * of the values assigned to it by the architecture. Based on the relationship
+ * between the values, the features are classified into 3 types - LOWER_SAFE,
+ * HIGHER_SAFE and EXACT.
+ *
+ * The lowest value of all the CPUs is chosen for LOWER_SAFE and highest
+ * for HIGHER_SAFE. It is expected that all CPUs have the same value for
+ * a field when EXACT is specified, failing which, the safe value specified
+ * in the table is chosen.
+ */
+
 enum ftr_type {
 	FTR_EXACT,	/* Use a predefined safe value */
 	FTR_LOWER_SAFE,	/* Smaller value is safe */

commit f4000cd99750065d5177555c0a805c97174d1b9f
Merge: 2ec4584eb89b 75037120e62b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 16:39:21 2016 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - struct thread_info moved off-stack (also touching
       include/linux/thread_info.h and include/linux/restart_block.h)
    
     - cpus_have_cap() reworked to avoid __builtin_constant_p() for static
       key use (also touching drivers/irqchip/irq-gic-v3.c)
    
     - uprobes support (currently only for native 64-bit tasks)
    
     - Emulation of kernel Privileged Access Never (PAN) using TTBR0_EL1
       switching to a reserved page table
    
     - CPU capacity information passing via DT or sysfs (used by the
       scheduler)
    
     - support for systems without FP/SIMD (IOW, kernel avoids touching
       these registers; there is no soft-float ABI, nor kernel emulation for
       AArch64 FP/SIMD)
    
     - handling of hardware watchpoint with unaligned addresses, varied
       lengths and offsets from base
    
     - use of the page table contiguous hint for kernel mappings
    
     - hugetlb fixes for sizes involving the contiguous hint
    
     - remove unnecessary I-cache invalidation in flush_cache_range()
    
     - CNTHCTL_EL2 access fix for CPUs with VHE support (ARMv8.1)
    
     - boot-time checks for writable+executable kernel mappings
    
     - simplify asm/opcodes.h and avoid including the 32-bit ARM counterpart
       and make the arm64 kernel headers self-consistent (Xen headers patch
       merged separately)
    
     - Workaround for broken .inst support in certain binutils versions
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (60 commits)
      arm64: Disable PAN on uaccess_enable()
      arm64: Work around broken .inst when defective gas is detected
      arm64: Add detection code for broken .inst support in binutils
      arm64: Remove reference to asm/opcodes.h
      arm64: Get rid of asm/opcodes.h
      arm64: smp: Prevent raw_smp_processor_id() recursion
      arm64: head.S: Fix CNTHCTL_EL2 access on VHE system
      arm64: Remove I-cache invalidation from flush_cache_range()
      arm64: Enable HIBERNATION in defconfig
      arm64: Enable CONFIG_ARM64_SW_TTBR0_PAN
      arm64: xen: Enable user access before a privcmd hvc call
      arm64: Handle faults caused by inadvertent user access with PAN enabled
      arm64: Disable TTBR0_EL1 during normal kernel execution
      arm64: Introduce uaccess_{disable,enable} functionality based on TTBR0_EL1
      arm64: Factor out TTBR0_EL1 post-update workaround into a specific asm macro
      arm64: Factor out PAN enabling/disabling into separate uaccess_* macros
      arm64: Update the synchronous external abort fault description
      selftests: arm64: add test for unaligned/inexact watchpoint handling
      arm64: Allow hw watchpoint of length 3,5,6 and 7
      arm64: hw_breakpoint: Handle inexact watchpoint addresses
      ...

commit 4b65a5db362783ab4b04ca1c1d2ad70ed9b0ba2a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 1 16:53:00 2016 +0100

    arm64: Introduce uaccess_{disable,enable} functionality based on TTBR0_EL1
    
    This patch adds the uaccess macros/functions to disable access to user
    space by setting TTBR0_EL1 to a reserved zeroed page. Since the value
    written to TTBR0_EL1 must be a physical address, for simplicity this
    patch introduces a reserved_ttbr0 page at a constant offset from
    swapper_pg_dir. The uaccess_disable code uses the ttbr1_el1 value
    adjusted by the reserved_ttbr0 offset.
    
    Enabling access to user is done by restoring TTBR0_EL1 with the value
    from the struct thread_info ttbr0 variable. Interrupts must be disabled
    during the uaccess_ttbr0_enable code to ensure the atomicity of the
    thread_info.ttbr0 read and TTBR0_EL1 write. This patch also moves the
    get_thread_info asm macro from entry.S to assembler.h for reuse in the
    uaccess_ttbr0_* macros.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 0ef718b67c54..a081531f9ff4 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -241,6 +241,12 @@ static inline bool system_supports_fpsimd(void)
 	return !cpus_have_const_cap(ARM64_HAS_NO_FPSIMD);
 }
 
+static inline bool system_uses_ttbr0_pan(void)
+{
+	return IS_ENABLED(CONFIG_ARM64_SW_TTBR0_PAN) &&
+		!cpus_have_cap(ARM64_HAS_PAN);
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 82e0191a1aa11abfddb22c8944989b7735560efc
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Nov 8 13:56:21 2016 +0000

    arm64: Support systems without FP/ASIMD
    
    The arm64 kernel assumes that FP/ASIMD units are always present
    and accesses the FP/ASIMD specific registers unconditionally. This
    could cause problems when they are absent. This patch adds the
    support for kernel handling systems without FP/ASIMD by skipping the
    register access within the kernel. For kvm, we trap the accesses
    to FP/ASIMD and inject an undefined instruction exception to the VM.
    
    The callers of the exported kernel_neon_begin_partial() should
    make sure that the FP/ASIMD is supported.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    [catalin.marinas@arm.com: add comment on the ARM64_HAS_NO_FPSIMD conflict and the new location]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8b63adb148e7..0ef718b67c54 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -38,8 +38,13 @@
 #define ARM64_HAS_32BIT_EL0			13
 #define ARM64_HYP_OFFSET_LOW			14
 #define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
+/*
+ * The macro below will be moved to asm/cpucaps.h together with the
+ * ARM64_NCAPS update.
+ */
+#define ARM64_HAS_NO_FPSIMD			16
 
-#define ARM64_NCAPS				16
+#define ARM64_NCAPS				17
 
 #ifndef __ASSEMBLY__
 
@@ -231,6 +236,11 @@ static inline bool system_supports_mixed_endian_el0(void)
 	return id_aa64mmfr0_mixed_endian_el0(read_system_reg(SYS_ID_AA64MMFR0_EL1));
 }
 
+static inline bool system_supports_fpsimd(void)
+{
+	return !cpus_have_const_cap(ARM64_HAS_NO_FPSIMD);
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit a4023f682739439b434165b54af7cb3676a4766e
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Nov 8 13:56:20 2016 +0000

    arm64: Add hypervisor safe helper for checking constant capabilities
    
    The hypervisor may not have full access to the kernel data structures
    and hence cannot safely use cpus_have_cap() helper for checking the
    system capability. Add a safe helper for hypervisors to check a constant
    system capability, which *doesn't* fall back to checking the bitmap
    maintained by the kernel. With this, make the cpus_have_cap() only
    check the bitmask and force constant cap checks to use the new API
    for quicker checks.
    
    Cc: Robert Ritcher <rritcher@cavium.com>
    Cc: Tirumalesh Chalamarla <tchalamarla@cavium.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index a27c3245ba21..8b63adb148e7 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -9,8 +9,6 @@
 #ifndef __ASM_CPUFEATURE_H
 #define __ASM_CPUFEATURE_H
 
-#include <linux/jump_label.h>
-
 #include <asm/hwcap.h>
 #include <asm/sysreg.h>
 
@@ -45,6 +43,8 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/bug.h>
+#include <linux/jump_label.h>
 #include <linux/kernel.h>
 
 /* CPU feature register tracking */
@@ -122,14 +122,19 @@ static inline bool cpu_have_feature(unsigned int num)
 	return elf_hwcap & (1UL << num);
 }
 
+/* System capability check for constant caps */
+static inline bool cpus_have_const_cap(int num)
+{
+	if (num >= ARM64_NCAPS)
+		return false;
+	return static_branch_unlikely(&cpu_hwcap_keys[num]);
+}
+
 static inline bool cpus_have_cap(unsigned int num)
 {
 	if (num >= ARM64_NCAPS)
 		return false;
-	if (__builtin_constant_p(num))
-		return static_branch_unlikely(&cpu_hwcap_keys[num]);
-	else
-		return test_bit(num, cpu_hwcaps);
+	return test_bit(num, cpu_hwcaps);
 }
 
 static inline void cpus_set_cap(unsigned int num)
@@ -218,7 +223,7 @@ static inline bool cpu_supports_mixed_endian_el0(void)
 
 static inline bool system_supports_32bit_el0(void)
 {
-	return cpus_have_cap(ARM64_HAS_32BIT_EL0);
+	return cpus_have_const_cap(ARM64_HAS_32BIT_EL0);
 }
 
 static inline bool system_supports_mixed_endian_el0(void)

commit 272d01bd790fdf3f1b16372fe28136e27756756f
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 3 18:34:34 2016 +0000

    arm64: Fix circular include of asm/lse.h through linux/jump_label.h
    
    Commit efd9e03facd0 ("arm64: Use static keys for CPU features")
    introduced support for static keys in asm/cpufeature.h, including
    linux/jump_label.h. When CC_HAVE_ASM_GOTO is not defined, this causes a
    circular dependency via linux/atomic.h, asm/lse.h and asm/cpufeature.h.
    
    This patch moves the capability macros out out of asm/cpufeature.h into
    a separate asm/cpucaps.h and modifies some of the #includes accordingly.
    
    Fixes: efd9e03facd0 ("arm64: Use static keys for CPU features")
    Reported-by: Artem Savkov <asavkov@redhat.com>
    Tested-by: Artem Savkov <asavkov@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index a27c3245ba21..0bc0b1de90c4 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -11,6 +11,7 @@
 
 #include <linux/jump_label.h>
 
+#include <asm/cpucaps.h>
 #include <asm/hwcap.h>
 #include <asm/sysreg.h>
 
@@ -24,25 +25,6 @@
 #define MAX_CPU_FEATURES	(8 * sizeof(elf_hwcap))
 #define cpu_feature(x)		ilog2(HWCAP_ ## x)
 
-#define ARM64_WORKAROUND_CLEAN_CACHE		0
-#define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
-#define ARM64_WORKAROUND_845719			2
-#define ARM64_HAS_SYSREG_GIC_CPUIF		3
-#define ARM64_HAS_PAN				4
-#define ARM64_HAS_LSE_ATOMICS			5
-#define ARM64_WORKAROUND_CAVIUM_23154		6
-#define ARM64_WORKAROUND_834220			7
-#define ARM64_HAS_NO_HW_PREFETCH		8
-#define ARM64_HAS_UAO				9
-#define ARM64_ALT_PAN_NOT_UAO			10
-#define ARM64_HAS_VIRT_HOST_EXTN		11
-#define ARM64_WORKAROUND_CAVIUM_27456		12
-#define ARM64_HAS_32BIT_EL0			13
-#define ARM64_HYP_OFFSET_LOW			14
-#define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
-
-#define ARM64_NCAPS				16
-
 #ifndef __ASSEMBLY__
 
 #include <linux/kernel.h>

commit 2a6dcb2b5f3e21592ca8dfa198dcce7bec09b020
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:46 2016 +0100

    arm64: cpufeature: Schedule enable() calls instead of calling them via IPI
    
    The enable() call for a cpufeature/errata is called using on_each_cpu().
    This issues a cross-call IPI to get the work done. Implicitly, this
    stashes the running PSTATE in SPSR when the CPU receives the IPI, and
    restores it when we return. This means an enable() call can never modify
    PSTATE.
    
    To allow PAN to do this, change the on_each_cpu() call to use
    stop_machine(). This schedules the work on each CPU which allows
    us to modify PSTATE.
    
    This involves changing the protype of all the enable() functions.
    
    enable_cpu_capabilities() is called during boot and enables the feature
    on all online CPUs. This path now uses stop_machine(). CPU features for
    hotplug'd CPUs are enabled by verify_local_cpu_features() which only
    acts on the local CPU, and can already modify the running PSTATE as it
    is called from secondary_start_kernel().
    
    Reported-by: Tony Thompson <anthony.thompson@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 758d74fedfad..a27c3245ba21 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -94,7 +94,7 @@ struct arm64_cpu_capabilities {
 	u16 capability;
 	int def_scope;			/* default scope */
 	bool (*matches)(const struct arm64_cpu_capabilities *caps, int scope);
-	void (*enable)(void *);		/* Called on all active CPUs */
+	int (*enable)(void *);		/* Called on all active CPUs */
 	union {
 		struct {	/* To be used for erratum handling only */
 			u32 midr_model;

commit 116c81f427ff6c5380850963e3fb8798cc821d2b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:16 2016 +0100

    arm64: Work around systems with mismatched cache line sizes
    
    Systems with differing CPU i-cache/d-cache line sizes can cause
    problems with the cache management by software when the execution
    is migrated from one to another. Usually, the application reads
    the cache size on a CPU and then uses that length to perform cache
    operations. However, if it gets migrated to another CPU with a smaller
    cache line size, things could go completely wrong. To prevent such
    cases, always use the smallest cache line size among the CPUs. The
    kernel CPU feature infrastructure already keeps track of the safe
    value for all CPUID registers including CTR. This patch works around
    the problem by :
    
    For kernel, dynamically patch the kernel to read the cache size
    from the system wide copy of CTR_EL0.
    
    For applications, trap read accesses to CTR_EL0 (by clearing the SCTLR.UCT)
    and emulate the mrs instruction to return the system wide safe value
    of CTR_EL0.
    
    For faster access (i.e, avoiding to lookup the system wide value of CTR_EL0
    via read_system_reg), we keep track of the pointer to table entry for
    CTR_EL0 in the CPU feature infrastructure.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 6806b86ab791..758d74fedfad 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -39,8 +39,9 @@
 #define ARM64_WORKAROUND_CAVIUM_27456		12
 #define ARM64_HAS_32BIT_EL0			13
 #define ARM64_HYP_OFFSET_LOW			14
+#define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
 
-#define ARM64_NCAPS				15
+#define ARM64_NCAPS				16
 
 #ifndef __ASSEMBLY__
 

commit c47a1900ad710fd2c97127e2ba19da1df79cf733
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:10 2016 +0100

    arm64: Rearrange CPU errata workaround checks
    
    Right now we run through the work around checks on a CPU
    from __cpuinfo_store_cpu. There are some problems with that:
    
    1) We initialise the system wide CPU feature registers only after the
    Boot CPU updates its cpuinfo. Now, if a work around depends on the
    variance of a CPU ID feature (e.g, check for Cache Line size mismatch),
    we have no way of performing it cleanly for the boot CPU.
    
    2) It is out of place, invoked from __cpuinfo_store_cpu() in cpuinfo.c. It
    is not an obvious place for that.
    
    This patch rearranges the CPU specific capability(aka work around) checks.
    
    1) At the moment we use verify_local_cpu_capabilities() to check if a new
    CPU has all the system advertised features. Use this for the secondary CPUs
    to perform the work around check. For that we rename
      verify_local_cpu_capabilities() => check_local_cpu_capabilities()
    which:
    
       If the system wide capabilities haven't been initialised (i.e, the CPU
       is activated at the boot), update the system wide detected work arounds.
    
       Otherwise (i.e a CPU hotplugged in later) verify that this CPU conforms to the
       system wide capabilities.
    
    2) Boot CPU updates the work arounds from smp_prepare_boot_cpu() after we have
    initialised the system wide CPU feature values.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index ddea66642ce1..6806b86ab791 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -202,11 +202,11 @@ void __init setup_cpu_features(void);
 void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void enable_cpu_capabilities(const struct arm64_cpu_capabilities *caps);
+void check_local_cpu_capabilities(void);
+
 void update_cpu_errata_workarounds(void);
 void __init enable_errata_workarounds(void);
-
 void verify_local_cpu_errata_workarounds(void);
-void verify_local_cpu_capabilities(void);
 
 u64 read_system_reg(u32 id);
 

commit 89ba26458b72422e0a1d85eb729a15220b204458
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:09 2016 +0100

    arm64: Use consistent naming for errata handling
    
    This is a cosmetic change to rename the functions dealing with
    the errata work arounds to be more consistent with their naming.
    
    1) check_local_cpu_errata() => update_cpu_errata_workarounds()
    check_local_cpu_errata() actually updates the system's errata work
    arounds. So rename it to reflect the same.
    
    2) verify_local_cpu_errata() => verify_local_cpu_errata_workarounds()
    Use errata_workarounds instead of _errata.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index bd950b00a575..ddea66642ce1 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -202,10 +202,10 @@ void __init setup_cpu_features(void);
 void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void enable_cpu_capabilities(const struct arm64_cpu_capabilities *caps);
-void check_local_cpu_errata(void);
+void update_cpu_errata_workarounds(void);
 void __init enable_errata_workarounds(void);
 
-void verify_local_cpu_errata(void);
+void verify_local_cpu_errata_workarounds(void);
 void verify_local_cpu_capabilities(void);
 
 u64 read_system_reg(u32 id);

commit ee7bc638f140e0586941002ffb82765743dabb97
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:08 2016 +0100

    arm64: Set the safe value for L1 icache policy
    
    Right now we use 0 as the safe value for CTR_EL0:L1Ip, which is
    not defined at the moment. The safer value for the L1Ip should be
    the weakest of the policies, which happens to be AIVIVT. While at it,
    fix the comment about safe_val.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 23a76dc5a6cf..bd950b00a575 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -65,7 +65,7 @@ struct arm64_ftr_bits {
 	enum ftr_type	type;
 	u8		shift;
 	u8		width;
-	s64		safe_val; /* safe value for discrete features */
+	s64		safe_val; /* safe value for FTR_EXACT features */
 };
 
 /*

commit efd9e03facd075f5b76bf82e6c785bd45d5cbf4f
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 5 18:25:48 2016 +0100

    arm64: Use static keys for CPU features
    
    This patch adds static keys transparently for all the cpu_hwcaps
    features by implementing an array of default-false static keys and
    enabling them when detected. The cpus_have_cap() check uses the static
    keys if the feature being checked is a constant, otherwise the compiler
    generates the bitmap test.
    
    Because of the early call to static_branch_enable() via
    check_local_cpu_errata() -> update_cpu_capabilities(), the jump labels
    are initialised in cpuinfo_store_boot_cpu().
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Suzuki K. Poulose <Suzuki.Poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index c07c5d1cd04a..23a76dc5a6cf 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -9,6 +9,8 @@
 #ifndef __ASM_CPUFEATURE_H
 #define __ASM_CPUFEATURE_H
 
+#include <linux/jump_label.h>
+
 #include <asm/hwcap.h>
 #include <asm/sysreg.h>
 
@@ -110,6 +112,7 @@ struct arm64_cpu_capabilities {
 };
 
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
+extern struct static_key_false cpu_hwcap_keys[ARM64_NCAPS];
 
 bool this_cpu_has_cap(unsigned int cap);
 
@@ -122,16 +125,21 @@ static inline bool cpus_have_cap(unsigned int num)
 {
 	if (num >= ARM64_NCAPS)
 		return false;
-	return test_bit(num, cpu_hwcaps);
+	if (__builtin_constant_p(num))
+		return static_branch_unlikely(&cpu_hwcap_keys[num]);
+	else
+		return test_bit(num, cpu_hwcaps);
 }
 
 static inline void cpus_set_cap(unsigned int num)
 {
-	if (num >= ARM64_NCAPS)
+	if (num >= ARM64_NCAPS) {
 		pr_warn("Attempt to set an illegal CPU capability (%d >= %d)\n",
 			num, ARM64_NCAPS);
-	else
+	} else {
 		__set_bit(num, cpu_hwcaps);
+		static_branch_enable(&cpu_hwcap_keys[num]);
+	}
 }
 
 static inline int __attribute_const__

commit 675b0563d6b26aa97bb8fe5bbde0ab9dc358433b
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Aug 31 11:31:10 2016 +0100

    arm64: cpufeature: expose arm64_ftr_reg struct for CTR_EL0
    
    Expose the arm64_ftr_reg struct covering CTR_EL0 outside of cpufeature.o
    so that other code can refer to it directly (i.e., without performing the
    binary search)
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8bb4f1527b26..c07c5d1cd04a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -78,6 +78,8 @@ struct arm64_ftr_reg {
 	const struct arm64_ftr_bits	*ftr_bits;
 };
 
+extern struct arm64_ftr_reg arm64_ftr_reg_ctrel0;
+
 /* scope of capability check */
 enum {
 	SCOPE_SYSTEM,

commit 6f2b7eeff9dbadeb7366d44086aa34792a996fc9
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Aug 31 11:31:09 2016 +0100

    arm64: cpufeature: constify arm64_ftr_regs array
    
    Constify the arm64_ftr_regs array, by moving the mutable arm64_ftr_reg
    fields out of the array itself. This also streamlines the bsearch, since
    the entire array can be covered by fewer cachelines. Moving the payload
    out of the array also allows us to have special explicitly defined
    struct instance in case other code needs to refer to it directly.
    
    Note that this replaces the runtime sorting of the array with a runtime
    BUG() check whether the array is sorted correctly in the code.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7c0b7cff17df..8bb4f1527b26 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -72,7 +72,6 @@ struct arm64_ftr_bits {
  * @sys_val		Safe value across the CPUs (system view)
  */
 struct arm64_ftr_reg {
-	u32				sys_id;
 	const char			*name;
 	u64				strict_mask;
 	u64				sys_val;

commit 5e49d73c1d87de50353844d263c1c7664aefeec8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Aug 31 11:31:08 2016 +0100

    arm64: cpufeature: constify arm64_ftr_bits structures
    
    The arm64_ftr_bits structures are never modified, so make them read-only.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7099f26e3702..7c0b7cff17df 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -72,11 +72,11 @@ struct arm64_ftr_bits {
  * @sys_val		Safe value across the CPUs (system view)
  */
 struct arm64_ftr_reg {
-	u32			sys_id;
-	const char		*name;
-	u64			strict_mask;
-	u64			sys_val;
-	struct arm64_ftr_bits	*ftr_bits;
+	u32				sys_id;
+	const char			*name;
+	u64				strict_mask;
+	u64				sys_val;
+	const struct arm64_ftr_bits	*ftr_bits;
 };
 
 /* scope of capability check */
@@ -157,7 +157,7 @@ cpuid_feature_extract_unsigned_field(u64 features, int field)
 	return cpuid_feature_extract_unsigned_field_width(features, field, 4);
 }
 
-static inline u64 arm64_ftr_mask(struct arm64_ftr_bits *ftrp)
+static inline u64 arm64_ftr_mask(const struct arm64_ftr_bits *ftrp)
 {
 	return (u64)GENMASK(ftrp->shift + ftrp->width - 1, ftrp->shift);
 }
@@ -170,7 +170,7 @@ cpuid_feature_extract_field(u64 features, int field, bool sign)
 		cpuid_feature_extract_unsigned_field(features, field);
 }
 
-static inline s64 arm64_ftr_value(struct arm64_ftr_bits *ftrp, u64 val)
+static inline s64 arm64_ftr_value(const struct arm64_ftr_bits *ftrp, u64 val)
 {
 	return (s64)cpuid_feature_extract_field(val, ftrp->shift, ftrp->sign);
 }

commit 221bb8a46e230b9824204ae86537183d9991ff2a
Merge: f7b32e4c021f 23528bb21ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:11:27 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
    
     - ARM: GICv3 ITS emulation and various fixes.  Removal of the
       old VGIC implementation.
    
     - s390: support for trapping software breakpoints, nested
       virtualization (vSIE), the STHYI opcode, initial extensions
       for CPU model support.
    
     - MIPS: support for MIPS64 hosts (32-bit guests only) and lots
       of cleanups, preliminary to this and the upcoming support for
       hardware virtualization extensions.
    
     - x86: support for execute-only mappings in nested EPT; reduced
       vmexit latency for TSC deadline timer (by about 30%) on Intel
       hosts; support for more than 255 vCPUs.
    
     - PPC: bugfixes.
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (302 commits)
      KVM: PPC: Introduce KVM_CAP_PPC_HTM
      MIPS: Select HAVE_KVM for MIPS64_R{2,6}
      MIPS: KVM: Reset CP0_PageMask during host TLB flush
      MIPS: KVM: Fix ptr->int cast via KVM_GUEST_KSEGX()
      MIPS: KVM: Sign extend MFC0/RDHWR results
      MIPS: KVM: Fix 64-bit big endian dynamic translation
      MIPS: KVM: Fail if ebase doesn't fit in CP0_EBase
      MIPS: KVM: Use 64-bit CP0_EBase when appropriate
      MIPS: KVM: Set CP0_Status.KX on MIPS64
      MIPS: KVM: Make entry code MIPS64 friendly
      MIPS: KVM: Use kmap instead of CKSEG0ADDR()
      MIPS: KVM: Use virt_to_phys() to get commpage PFN
      MIPS: Fix definition of KSEGX() for 64-bit
      KVM: VMX: Add VMCS to CPU's loaded VMCSs before VMPTRLD
      kvm: x86: nVMX: maintain internal copy of current VMCS
      KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE
      KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures
      KVM: arm64: vgic-its: Simplify MAPI error handling
      KVM: arm64: vgic-its: Make vgic_its_cmd_handle_mapi similar to other handlers
      KVM: arm64: vgic-its: Turn device_id validation into generic ID validation
      ...

commit 853c3b21ff35816a2ae351fd7c2adb101c1f4503
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:38 2016 +0100

    arm64: Add ARM64_HYP_OFFSET_LOW capability
    
    As we need to indicate to the rest of the kernel which region of
    the HYP VA space is safe to use, add a capability that will
    indicate that KVM should use the [VA_BITS-2:0] range.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 224efe730e46..d40edbb6ef23 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -36,8 +36,9 @@
 #define ARM64_HAS_VIRT_HOST_EXTN		11
 #define ARM64_WORKAROUND_CAVIUM_27456		12
 #define ARM64_HAS_32BIT_EL0			13
+#define ARM64_HYP_OFFSET_LOW			14
 
-#define ARM64_NCAPS				14
+#define ARM64_NCAPS				15
 
 #ifndef __ASSEMBLY__
 

commit 8e2318521bf5837dae093413f81292b59d49d030
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:30 2016 +0100

    arm64: errata: Calling enable functions for CPU errata too
    
    Currently we call the (optional) enable function for CPU _features_
    only. As CPU _errata_ descriptions share the same data structure and
    having an enable function is useful for errata as well (for instance
    to set bits in SCTLR), lets call it when enumerating erratas too.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 224efe730e46..49dd1bd3ea50 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -191,7 +191,9 @@ void __init setup_cpu_features(void);
 
 void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
+void enable_cpu_capabilities(const struct arm64_cpu_capabilities *caps);
 void check_local_cpu_errata(void);
+void __init enable_errata_workarounds(void);
 
 void verify_local_cpu_errata(void);
 void verify_local_cpu_capabilities(void);

commit 6a6efbb45b7d95c84840010095367eb06a64f342
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Apr 22 12:25:34 2016 +0100

    arm64: Verify CPU errata work arounds on hotplugged CPU
    
    CPU Errata work arounds are detected and applied to the
    kernel code at boot time and the data is then freed up.
    If a new hotplugged CPU requires a work around which
    was not applied at boot time, there is nothing we can
    do but simply fail the booting.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index d39db6387746..224efe730e46 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -193,6 +193,7 @@ void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void check_local_cpu_errata(void);
 
+void verify_local_cpu_errata(void);
 void verify_local_cpu_capabilities(void);
 
 u64 read_system_reg(u32 id);

commit e3661b128e53ee281e1e7c589a5b647890bd6d7c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 22 12:25:32 2016 +0100

    arm64: Allow a capability to be checked on a single CPU
    
    Now that the capabilities are only available once all the CPUs
    have booted, we're unable to check for a particular feature
    in any subsystem that gets initialized before then.
    
    In order to support this, introduce a local_cpu_has_cap() function
    that tests for the presence of a given capability independently
    of the whole framework.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    [ Added preemptible() check ]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: remove duplicate initialisation of caps in this_cpu_has_cap]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index e501e4af2ebd..d39db6387746 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -109,6 +109,8 @@ struct arm64_cpu_capabilities {
 
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 
+bool this_cpu_has_cap(unsigned int cap);
+
 static inline bool cpu_have_feature(unsigned int num)
 {
 	return elf_hwcap & (1UL << num);

commit 92406f0cc9e3d5cc77bf3de6d68c9c2373dcd701
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Apr 22 12:25:31 2016 +0100

    arm64: cpufeature: Add scope for capability check
    
    Add scope parameter to the arm64_cpu_capabilities::matches(), so that
    this can be reused for checking the capability on a given CPU vs the
    system wide. The system uses the default scope associated with the
    capability for initialising the CPU_HWCAPs and ELF_HWCAPs.
    
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index ca8fb4bcfb1a..e501e4af2ebd 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -78,10 +78,17 @@ struct arm64_ftr_reg {
 	struct arm64_ftr_bits	*ftr_bits;
 };
 
+/* scope of capability check */
+enum {
+	SCOPE_SYSTEM,
+	SCOPE_LOCAL_CPU,
+};
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;
-	bool (*matches)(const struct arm64_cpu_capabilities *);
+	int def_scope;			/* default scope */
+	bool (*matches)(const struct arm64_cpu_capabilities *caps, int scope);
 	void (*enable)(void *);		/* Called on all active CPUs */
 	union {
 		struct {	/* To be used for erratum handling only */

commit 042446a31e3803d81c7e618dd80928dc3dce70c5
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Apr 18 10:28:36 2016 +0100

    arm64: cpufeature: Track 32bit EL0 support
    
    Add cpu_hwcap bit for keeping track of the support for 32bit EL0.
    
    Tested-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7f64285e4ef7..ca8fb4bcfb1a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -35,8 +35,9 @@
 #define ARM64_ALT_PAN_NOT_UAO			10
 #define ARM64_HAS_VIRT_HOST_EXTN		11
 #define ARM64_WORKAROUND_CAVIUM_27456		12
+#define ARM64_HAS_32BIT_EL0			13
 
-#define ARM64_NCAPS				13
+#define ARM64_NCAPS				14
 
 #ifndef __ASSEMBLY__
 
@@ -192,6 +193,11 @@ static inline bool cpu_supports_mixed_endian_el0(void)
 	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(ID_AA64MMFR0_EL1));
 }
 
+static inline bool system_supports_32bit_el0(void)
+{
+	return cpus_have_cap(ARM64_HAS_32BIT_EL0);
+}
+
 static inline bool system_supports_mixed_endian_el0(void)
 {
 	return id_aa64mmfr0_mixed_endian_el0(read_system_reg(SYS_ID_AA64MMFR0_EL1));

commit c80aba803a9aa131f997f62a71ab453e456d08a8
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Apr 18 10:28:34 2016 +0100

    arm64: Add helpers for detecting AArch32 support at EL0
    
    Adds a helper to extract the support for AArch32 at EL0
    
    Tested-by: Yury Norov <ynorov@caviumnetworks.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b9b649422fca..7f64285e4ef7 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -170,6 +170,13 @@ static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
 		cpuid_feature_extract_unsigned_field(mmfr0, ID_AA64MMFR0_BIGENDEL0_SHIFT) == 0x1;
 }
 
+static inline bool id_aa64pfr0_32bit_el0(u64 pfr0)
+{
+	u32 val = cpuid_feature_extract_unsigned_field(pfr0, ID_AA64PFR0_EL0_SHIFT);
+
+	return val == ID_AA64PFR0_EL0_32BIT_64BIT;
+}
+
 void __init setup_cpu_features(void);
 
 void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit 1cc6ed90dd7313055dec0174e2cda745ebadd6b0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Mar 4 12:54:05 2016 +0000

    arm64: make mrs_s prefixing implicit in read_cpuid
    
    Commit 0f54b14e76f5302a ("arm64: cpufeature: Change read_cpuid() to use
    sysreg's mrs_s macro") changed read_cpuid to require a SYS_ prefix on
    register names, to allow manual assembly of registers unknown by the
    toolchain, using tables in sysreg.h.
    
    This interacts poorly with commit 42b55734030c1f72 ("efi/arm64: Check
    for h/w support before booting a >4 KB granular kernel"), which is
    curretly queued via the tip tree, and uses read_cpuid without a SYS_
    prefix. Due to this, a build of next-20160304 fails if EFI and 64K pages
    are selected.
    
    To avoid this issue when trees are merged, move the required SYS_
    prefixing into read_cpuid, and revert all of the updated callsites to
    pass plain register names. This effectively reverts the bulk of commit
    0f54b14e76f5302a.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 1497163213ed..f6f7423e51d0 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -181,7 +181,7 @@ u64 read_system_reg(u32 id);
 
 static inline bool cpu_supports_mixed_endian_el0(void)
 {
-	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(SYS_ID_AA64MMFR0_EL1));
+	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(ID_AA64MMFR0_EL1));
 }
 
 static inline bool system_supports_mixed_endian_el0(void)

commit d88701bea3664cea99b8b7380f63a3bd0ec3ead3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jan 29 11:24:05 2015 +0000

    arm64: Add ARM64_HAS_VIRT_HOST_EXTN feature
    
    Add a new ARM64_HAS_VIRT_HOST_EXTN features to indicate that the
    CPU has the ARMv8.1 VHE capability.
    
    This will be used to trigger kernel patching in KVM.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8f271b83f910..a5c769b1c65b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -30,8 +30,12 @@
 #define ARM64_HAS_LSE_ATOMICS			5
 #define ARM64_WORKAROUND_CAVIUM_23154		6
 #define ARM64_WORKAROUND_834220			7
+/* #define ARM64_HAS_NO_HW_PREFETCH		8 */
+/* #define ARM64_HAS_UAO			9 */
+/* #define ARM64_ALT_PAN_NOT_UAO		10 */
+#define ARM64_HAS_VIRT_HOST_EXTN		11
 
-#define ARM64_NCAPS				8
+#define ARM64_NCAPS				12
 
 #ifndef __ASSEMBLY__
 

commit 104a0c02e8b1936c049e18a6d4e4ab040fb61213
Author: Andrew Pinski <apinski@cavium.com>
Date:   Wed Feb 24 17:44:57 2016 -0800

    arm64: Add workaround for Cavium erratum 27456
    
    On ThunderX T88 pass 1.x through 2.1 parts, broadcast TLBI
    instructions may cause the icache to become corrupted if it contains
    data for a non-current ASID.
    
    This patch implements the workaround (which invalidates the local
    icache when switching the mm) by using code patching.
    
    Signed-off-by: Andrew Pinski <apinski@cavium.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index be88aef01f3d..1497163213ed 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -33,8 +33,9 @@
 #define ARM64_HAS_NO_HW_PREFETCH		8
 #define ARM64_HAS_UAO				9
 #define ARM64_ALT_PAN_NOT_UAO			10
+#define ARM64_WORKAROUND_CAVIUM_27456		12
 
-#define ARM64_NCAPS				11
+#define ARM64_NCAPS				13
 
 #ifndef __ASSEMBLY__
 

commit 28c5dcb22f90113dea101b0421bc6971bccb7a74
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Jan 26 10:58:16 2016 +0000

    arm64: Rename cpuid_feature field extract routines
    
    Now that we have a clear understanding of the sign of a feature,
    rename the routines to reflect the sign, so that it is not misused.
    The cpuid_feature_extract_field() now accepts a 'sign' parameter.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 42e492a9e0fd..be88aef01f3d 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -121,15 +121,15 @@ static inline void cpus_set_cap(unsigned int num)
 }
 
 static inline int __attribute_const__
-cpuid_feature_extract_field_width(u64 features, int field, int width)
+cpuid_feature_extract_signed_field_width(u64 features, int field, int width)
 {
 	return (s64)(features << (64 - width - field)) >> (64 - width);
 }
 
 static inline int __attribute_const__
-cpuid_feature_extract_field(u64 features, int field)
+cpuid_feature_extract_signed_field(u64 features, int field)
 {
-	return cpuid_feature_extract_field_width(features, field, 4);
+	return cpuid_feature_extract_signed_field_width(features, field, 4);
 }
 
 static inline unsigned int __attribute_const__
@@ -149,17 +149,23 @@ static inline u64 arm64_ftr_mask(struct arm64_ftr_bits *ftrp)
 	return (u64)GENMASK(ftrp->shift + ftrp->width - 1, ftrp->shift);
 }
 
+static inline int __attribute_const__
+cpuid_feature_extract_field(u64 features, int field, bool sign)
+{
+	return (sign) ?
+		cpuid_feature_extract_signed_field(features, field) :
+		cpuid_feature_extract_unsigned_field(features, field);
+}
+
 static inline s64 arm64_ftr_value(struct arm64_ftr_bits *ftrp, u64 val)
 {
-	return ftrp->sign ?
-		cpuid_feature_extract_field_width(val, ftrp->shift, ftrp->width) :
-		cpuid_feature_extract_unsigned_field_width(val, ftrp->shift, ftrp->width);
+	return (s64)cpuid_feature_extract_field(val, ftrp->shift, ftrp->sign);
 }
 
 static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
 {
-	return cpuid_feature_extract_field(mmfr0, ID_AA64MMFR0_BIGENDEL_SHIFT) == 0x1 ||
-		cpuid_feature_extract_field(mmfr0, ID_AA64MMFR0_BIGENDEL0_SHIFT) == 0x1;
+	return cpuid_feature_extract_unsigned_field(mmfr0, ID_AA64MMFR0_BIGENDEL_SHIFT) == 0x1 ||
+		cpuid_feature_extract_unsigned_field(mmfr0, ID_AA64MMFR0_BIGENDEL0_SHIFT) == 0x1;
 }
 
 void __init setup_cpu_features(void);

commit ff96f7bc7bf6393eef8ff2bde1279715ce13343a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Jan 26 10:58:15 2016 +0000

    arm64: capabilities: Handle sign of the feature bit
    
    Use the appropriate accessor for the feature bit by keeping
    track of the sign of the feature
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 14007b126277..42e492a9e0fd 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -88,9 +88,10 @@ struct arm64_cpu_capabilities {
 
 		struct {	/* Feature register checking */
 			u32 sys_reg;
-			int field_pos;
-			int min_field_value;
-			int hwcap_type;
+			u8 field_pos;
+			u8 min_field_value;
+			u8 hwcap_type;
+			bool sign;
 			unsigned long hwcap;
 		};
 	};

commit fd9c2790cb2825207e636ba3093110f705ed1b57
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Feb 23 10:31:43 2016 +0000

    arm64: Enable CPU capability verification unconditionally
    
    We verify the capabilities of the secondary CPUs only when
    hotplug is enabled. The boot time activated CPUs do not
    go through the verification by checking whether the system
    wide capabilities were initialised or not.
    
    This patch removes the capability check dependency on CONFIG_HOTPLUG_CPU,
    to make sure that all the secondary CPUs go through the check.
    The boot time activated CPUs will still skip the system wide
    capability check. The plan is to hook in a check for CPU features
    used by the kernel at early boot up, based on the Boot CPU values.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 37a53fc6b384..14007b126277 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -167,13 +167,7 @@ void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void check_local_cpu_errata(void);
 
-#ifdef CONFIG_HOTPLUG_CPU
 void verify_local_cpu_capabilities(void);
-#else
-static inline void verify_local_cpu_capabilities(void)
-{
-}
-#endif
 
 u64 read_system_reg(u32 id);
 

commit 705441960033e66b63524521f153fbb28c99ddbd
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:50 2016 +0000

    arm64: kernel: Don't toggle PAN on systems with UAO
    
    If a CPU supports both Privileged Access Never (PAN) and User Access
    Override (UAO), we don't need to disable/re-enable PAN round all
    copy_to_user() like calls.
    
    UAO alternatives cause these calls to use the 'unprivileged' load/store
    instructions, which are overridden to be the privileged kind when
    fs==KERNEL_DS.
    
    This patch changes the copy_to_user() calls to have their PAN toggling
    depend on a new composite 'feature' ARM64_ALT_PAN_NOT_UAO.
    
    If both features are detected, PAN will be enabled, but the copy_to_user()
    alternatives will not be applied. This means PAN will be enabled all the
    time for these functions. If only PAN is detected, the toggling will be
    enabled as normal.
    
    This will save the time taken to disable/re-enable PAN, and allow us to
    catch copy_to_user() accesses that occur with fs==KERNEL_DS.
    
    Futex and swp-emulation code continue to hang their PAN toggling code on
    ARM64_HAS_PAN.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index a5df7cde616b..37a53fc6b384 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -32,8 +32,9 @@
 #define ARM64_WORKAROUND_834220			7
 #define ARM64_HAS_NO_HW_PREFETCH		8
 #define ARM64_HAS_UAO				9
+#define ARM64_ALT_PAN_NOT_UAO			10
 
-#define ARM64_NCAPS				10
+#define ARM64_NCAPS				11
 
 #ifndef __ASSEMBLY__
 

commit 57f4959bad0a154aeca125b7d38d1d9471a12422
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:48 2016 +0000

    arm64: kernel: Add support for User Access Override
    
    'User Access Override' is a new ARMv8.2 feature which allows the
    unprivileged load and store instructions to be overridden to behave in
    the normal way.
    
    This patch converts {get,put}_user() and friends to use ldtr*/sttr*
    instructions - so that they can only access EL0 memory, then enables
    UAO when fs==KERNEL_DS so that these functions can access kernel memory.
    
    This allows user space's read/write permissions to be checked against the
    page tables, instead of testing addr<USER_DS, then using the kernel's
    read/write permissions.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: move uao_thread_switch() above dsb()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8131abfabb0a..a5df7cde616b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -31,8 +31,9 @@
 #define ARM64_WORKAROUND_CAVIUM_23154		6
 #define ARM64_WORKAROUND_834220			7
 #define ARM64_HAS_NO_HW_PREFETCH		8
+#define ARM64_HAS_UAO				9
 
-#define ARM64_NCAPS				9
+#define ARM64_NCAPS				10
 
 #ifndef __ASSEMBLY__
 

commit 0f54b14e76f5302afe164dc911b049b5df836ff5
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:46 2016 +0000

    arm64: cpufeature: Change read_cpuid() to use sysreg's mrs_s macro
    
    Older assemblers may not have support for newer feature registers. To get
    round this, sysreg.h provides a 'mrs_s' macro that takes a register
    encoding and generates the raw instruction.
    
    Change read_cpuid() to use mrs_s in all cases so that new registers
    don't have to be a special case. Including sysreg.h means we need to move
    the include and definition of read_cpuid() after the #ifndef __ASSEMBLY__
    to avoid syntax errors in vmlinux.lds.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8d56bd8550dc..8131abfabb0a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -177,7 +177,7 @@ u64 read_system_reg(u32 id);
 
 static inline bool cpu_supports_mixed_endian_el0(void)
 {
-	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(ID_AA64MMFR0_EL1));
+	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(SYS_ID_AA64MMFR0_EL1));
 }
 
 static inline bool system_supports_mixed_endian_el0(void)

commit d5370f754875460662abe8561388e019d90dd0c4
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 2 12:46:24 2016 +0000

    arm64: prefetch: add alternative pattern for CPUs without a prefetcher
    
    Most CPUs have a hardware prefetcher which generally performs better
    without explicit prefetch instructions issued by software, however
    some CPUs (e.g. Cavium ThunderX) rely solely on explicit prefetch
    instructions.
    
    This patch adds an alternative pattern (ARM64_HAS_NO_HW_PREFETCH) to
    allow our library code to make use of explicit prefetch instructions
    during things like copy routines only when the CPU does not have the
    capability to perform the prefetching itself.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Andrew Pinski <apinski@cavium.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 8f271b83f910..8d56bd8550dc 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -30,8 +30,9 @@
 #define ARM64_HAS_LSE_ATOMICS			5
 #define ARM64_WORKAROUND_CAVIUM_23154		6
 #define ARM64_WORKAROUND_834220			7
+#define ARM64_HAS_NO_HW_PREFETCH		8
 
-#define ARM64_NCAPS				8
+#define ARM64_NCAPS				9
 
 #ifndef __ASSEMBLY__
 

commit 5d8686276a5acc0a3d8055028a6e9d990c9c4fbd
Merge: 5a44ed0d30dd 66362c9afc1d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 27 11:09:59 2015 -0800

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Catalin Marinas:
    
     - Build fix when !CONFIG_UID16 (the patch is touching generic files but
       it only affects arm64 builds; submitted by Arnd Bergmann)
    
     - EFI fixes to deal with early_memremap() returning NULL and correctly
       mapping run-time regions
    
     - Fix CPUID register extraction of unsigned fields (not to be
       sign-extended)
    
     - ASID allocator fix to deal with long-running tasks over multiple
       generation roll-overs
    
     - Revert support for marking page ranges as contiguous PTEs (it leads
       to TLB conflicts and requires additional non-trivial kernel changes)
    
     - Proper early_alloc() failure check
    
     - Disable KASan for 48-bit VA and 16KB page configuration (the pgd is
       larger than the KASan shadow memory)
    
     - Update the fault_info table (original descriptions based on early
       engineering spec)
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: efi: fix initcall return values
      arm64: efi: deal with NULL return value of early_memremap()
      arm64: debug: Treat the BRPs/WRPs as unsigned
      arm64: cpufeature: Track unsigned fields
      arm64: cpufeature: Add helpers for extracting unsigned values
      Revert "arm64: Mark kernel page ranges contiguous"
      arm64: mm: keep reserved ASIDs in sync with mm after multiple rollovers
      arm64: KASAN depends on !(ARM64_16K_PAGES && ARM64_VA_BITS_48)
      arm64: efi: correctly map runtime regions
      arm64: mm: fix fault_info table xFSC decoding
      arm64: fix building without CONFIG_UID16
      arm64: early_alloc: Fix check for allocation failure

commit 4f0a606bce5ecd3a8210e896c33f41d1bf7f752f
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Wed Nov 18 17:08:57 2015 +0000

    arm64: cpufeature: Track unsigned fields
    
    Some of the feature bits have unsigned values and need
    to be treated accordingly to avoid errors. Adds the property
    to the feature bits and use the appropriate field extract helpers.
    
    Reported-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 7a161027ee12..29c3f5d5c750 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -46,8 +46,12 @@ enum ftr_type {
 #define FTR_STRICT	true	/* SANITY check strict matching required */
 #define FTR_NONSTRICT	false	/* SANITY check ignored */
 
+#define FTR_SIGNED	true	/* Value should be treated as signed */
+#define FTR_UNSIGNED	false	/* Value should be treated as unsigned */
+
 struct arm64_ftr_bits {
-	bool		strict;	  /* CPU Sanity check: strict matching required ? */
+	bool		sign;	/* Value is signed ? */
+	bool		strict;	/* CPU Sanity check: strict matching required ? */
 	enum ftr_type	type;
 	u8		shift;
 	u8		width;
@@ -142,7 +146,9 @@ static inline u64 arm64_ftr_mask(struct arm64_ftr_bits *ftrp)
 
 static inline s64 arm64_ftr_value(struct arm64_ftr_bits *ftrp, u64 val)
 {
-	return cpuid_feature_extract_field_width(val, ftrp->shift, ftrp->width);
+	return ftrp->sign ?
+		cpuid_feature_extract_field_width(val, ftrp->shift, ftrp->width) :
+		cpuid_feature_extract_unsigned_field_width(val, ftrp->shift, ftrp->width);
 }
 
 static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)

commit d21182718400f51f0729ae8268d94bd820ec8b71
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Wed Nov 18 17:08:56 2015 +0000

    arm64: cpufeature: Add helpers for extracting unsigned values
    
    The cpuid_feature_extract_field() extracts the feature value
    as a signed integer. This could be problematic for features
    whose values are unsigned. e.g, ID_AA64DFR0_EL1:BRPs. Add
    an unsigned variant for the unsigned fields.
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reported-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 11d5bb0fdd54..7a161027ee12 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -123,6 +123,18 @@ cpuid_feature_extract_field(u64 features, int field)
 	return cpuid_feature_extract_field_width(features, field, 4);
 }
 
+static inline unsigned int __attribute_const__
+cpuid_feature_extract_unsigned_field_width(u64 features, int field, int width)
+{
+	return (u64)(features << (64 - width - field)) >> (64 - width);
+}
+
+static inline unsigned int __attribute_const__
+cpuid_feature_extract_unsigned_field(u64 features, int field)
+{
+	return cpuid_feature_extract_unsigned_field_width(features, field, 4);
+}
+
 static inline u64 arm64_ftr_mask(struct arm64_ftr_bits *ftrp)
 {
 	return (u64)GENMASK(ftrp->shift + ftrp->width - 1, ftrp->shift);

commit 498cd5c32be6e32bc0f8efcad48ab094bb2bfdf3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Nov 16 10:28:18 2015 +0000

    arm64: KVM: Add workaround for Cortex-A57 erratum 834220
    
    Cortex-A57 parts up to r1p2 can misreport Stage 2 translation faults
    when a Stage 1 permission fault or device alignment fault should
    have been reported.
    
    This patch implements the workaround (which is to validate that the
    Stage-1 translation actually succeeds) by using code patching.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 11d5bb0fdd54..52722ee73dba 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -29,8 +29,9 @@
 #define ARM64_HAS_PAN				4
 #define ARM64_HAS_LSE_ATOMICS			5
 #define ARM64_WORKAROUND_CAVIUM_23154		6
+#define ARM64_WORKAROUND_834220			7
 
-#define ARM64_NCAPS				7
+#define ARM64_NCAPS				8
 
 #ifndef __ASSEMBLY__
 

commit 2dc10ad81fc017837037e60439662e1b16bdffb9
Merge: e627078a0cbd f8f8bdc48851
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 14:47:13 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - "genirq: Introduce generic irq migration for cpu hotunplugged" patch
       merged from tip/irq/for-arm to allow the arm64-specific part to be
       upstreamed via the arm64 tree
    
     - CPU feature detection reworked to cope with heterogeneous systems
       where CPUs may not have exactly the same features.  The features
       reported by the kernel via internal data structures or ELF_HWCAP are
       delayed until all the CPUs are up (and before user space starts)
    
     - Support for 16KB pages, with the additional bonus of a 36-bit VA
       space, though the latter only depending on EXPERT
    
     - Implement native {relaxed, acquire, release} atomics for arm64
    
     - New ASID allocation algorithm which avoids IPI on roll-over, together
       with TLB invalidation optimisations (using local vs global where
       feasible)
    
     - KASan support for arm64
    
     - EFI_STUB clean-up and isolation for the kernel proper (required by
       KASan)
    
     - copy_{to,from,in}_user optimisations (sharing the memcpy template)
    
     - perf: moving arm64 to the arm32/64 shared PMU framework
    
     - L1_CACHE_BYTES increased to 128 to accommodate Cavium hardware
    
     - Support for the contiguous PTE hint on kernel mapping (16 consecutive
       entries may be able to use a single TLB entry)
    
     - Generic CONFIG_HZ now used on arm64
    
     - defconfig updates
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (91 commits)
      arm64/efi: fix libstub build under CONFIG_MODVERSIONS
      ARM64: Enable multi-core scheduler support by default
      arm64/efi: move arm64 specific stub C code to libstub
      arm64: page-align sections for DEBUG_RODATA
      arm64: Fix build with CONFIG_ZONE_DMA=n
      arm64: Fix compat register mappings
      arm64: Increase the max granular size
      arm64: remove bogus TASK_SIZE_64 check
      arm64: make Timer Interrupt Frequency selectable
      arm64/mm: use PAGE_ALIGNED instead of IS_ALIGNED
      arm64: cachetype: fix definitions of ICACHEF_* flags
      arm64: cpufeature: declare enable_cpu_capabilities as static
      genirq: Make the cpuhotplug migration code less noisy
      arm64: Constify hwcap name string arrays
      arm64/kvm: Make use of the system wide safe values
      arm64/debug: Make use of the system wide safe value
      arm64: Move FP/ASIMD hwcap handling to common code
      arm64/HWCAP: Use system wide safe values
      arm64/capabilities: Make use of system wide safe value
      arm64: Delay cpu feature capability checks
      ...

commit 37b01d53ceefa390d6eee7a82f3c156b64951bf3
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:52 2015 +0100

    arm64/HWCAP: Use system wide safe values
    
    Extend struct arm64_cpu_capabilities to handle the HWCAP detection
    and make use of the system wide value of the feature registers for
    a reliable set of HWCAPs.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index eec584e7c701..ca4621033ca0 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -81,6 +81,8 @@ struct arm64_cpu_capabilities {
 			u32 sys_reg;
 			int field_pos;
 			int min_field_value;
+			int hwcap_type;
+			unsigned long hwcap;
 		};
 	};
 };

commit da8d02d19ffdd201af632c755a473b6df4b3e4cc
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:51 2015 +0100

    arm64/capabilities: Make use of system wide safe value
    
    Now that we can reliably read the system wide safe value for a
    feature register, use that to compute the system capability.
    This patch also replaces the 'feature-register-specific'
    methods with a generic routine to check the capability.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 1603ae84ea45..eec584e7c701 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -78,6 +78,7 @@ struct arm64_cpu_capabilities {
 		};
 
 		struct {	/* Feature register checking */
+			u32 sys_reg;
 			int field_pos;
 			int min_field_value;
 		};

commit dbb4e152b8da1f977d9d8cd7e494ab4ee3622f72
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:50 2015 +0100

    arm64: Delay cpu feature capability checks
    
    At the moment we run through the arm64_features capability list for
    each CPU and set the capability if one of the CPU supports it. This
    could be problematic in a heterogeneous system with differing capabilities.
    Delay the CPU feature checks until all the enabled CPUs are up(i.e,
    smp_cpus_done(), so that we can make better decisions based on the
    overall system capability. Once we decide and advertise the capabilities
    the alternatives can be applied. From this state, we cannot roll back
    a feature to disabled based on the values from a new hotplugged CPU,
    due to the runtime patching and other reasons. So, for all new CPUs,
    we need to make sure that they have the established system capabilities.
    Failing which, we bring the CPU down, preventing it from turning online.
    Once the capabilities are decided, any new CPU booting up goes through
    verification to ensure that it has all the enabled capabilities and also
    invokes the respective enable() method on the CPU.
    
    The CPU errata checks are not delayed and is still executed per-CPU
    to detect the respective capabilities. If we ever come across a non-errata
    capability that needs to be checked on each-CPU, we could introduce them via
    a new capability table(or introduce a flag), which can be processed per CPU.
    
    The next patch will make the feature checks use the system wide
    safe value of a feature register.
    
    NOTE: The enable() methods associated with the capability is scheduled
    on all the CPUs (which is the only use case at the moment). If we need
    a different type of 'enable()' which only needs to be run once on any CPU,
    we should be able to handle that when needed.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    [catalin.marinas@arm.com: static variable and coding style fixes]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 12d883a067c4..1603ae84ea45 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -70,7 +70,7 @@ struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;
 	bool (*matches)(const struct arm64_cpu_capabilities *);
-	void (*enable)(void);
+	void (*enable)(void *);		/* Called on all active CPUs */
 	union {
 		struct {	/* To be used for erratum handling only */
 			u32 midr_model;
@@ -140,7 +140,14 @@ void __init setup_cpu_features(void);
 void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void check_local_cpu_errata(void);
-void check_local_cpu_features(void);
+
+#ifdef CONFIG_HOTPLUG_CPU
+void verify_local_cpu_capabilities(void);
+#else
+static inline void verify_local_cpu_capabilities(void)
+{
+}
+#endif
 
 u64 read_system_reg(u32 id);
 

commit ce8b602c694c9482e0ffb7432cd59fa2276673fe
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:49 2015 +0100

    arm64: Refactor check_cpu_capabilities
    
    check_cpu_capabilities runs through a given list of caps and
    checks if the system has the cap, updates the system capability
    bitmap and also runs any enable() methods associated with them.
    All of this is not quite obvious from the name 'check'. This
    patch splits the check_cpu_capabilities into two parts :
    
    1) update_cpu_capabilities
     => Runs through the given list and updates the system
        wide capability map.
    2) enable_cpu_capabilities
     => Runs through the given list and invokes enable() (if any)
        for the caps enabled on the system.
    
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Suggested-by: Catalin Marinas <catalin.marinsa@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 1e281b27732b..12d883a067c4 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -137,7 +137,7 @@ static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
 
 void __init setup_cpu_features(void);
 
-void check_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
+void update_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void check_local_cpu_errata(void);
 void check_local_cpu_features(void);

commit c1e8656cbae139c8aaf34d7b802edecbc8a1cf58
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:48 2015 +0100

    arm64: Cleanup mixed endian support detection
    
    Make use of the system wide safe register to decide the support
    for mixed endian.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 81217220eb92..1e281b27732b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -141,11 +141,19 @@ void check_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void check_local_cpu_errata(void);
 void check_local_cpu_features(void);
-bool cpu_supports_mixed_endian_el0(void);
-bool system_supports_mixed_endian_el0(void);
 
 u64 read_system_reg(u32 id);
 
+static inline bool cpu_supports_mixed_endian_el0(void)
+{
+	return id_aa64mmfr0_mixed_endian_el0(read_cpuid(ID_AA64MMFR0_EL1));
+}
+
+static inline bool system_supports_mixed_endian_el0(void)
+{
+	return id_aa64mmfr0_mixed_endian_el0(read_system_reg(SYS_ID_AA64MMFR0_EL1));
+}
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit b3f1537893b54d0f42f52e0f4cde5e17e21f564c
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:47 2015 +0100

    arm64: Read system wide CPUID value
    
    Add an API for reading the safe CPUID value across the
    system from the new infrastructure.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 01bb5cf995af..81217220eb92 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -144,6 +144,8 @@ void check_local_cpu_features(void);
 bool cpu_supports_mixed_endian_el0(void);
 bool system_supports_mixed_endian_el0(void);
 
+u64 read_system_reg(u32 id);
+
 #endif /* __ASSEMBLY__ */
 
 #endif

commit 3c739b57108436211c7f798ba3de0bb0cd8ef469
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:45 2015 +0100

    arm64: Keep track of CPU feature registers
    
    This patch adds an infrastructure to keep track of the CPU feature
    registers on the system. For each register, the infrastructure keeps
    track of the system wide safe value of the feature bits. Also, tracks
    the which fields of a register should be matched strictly across all
    the CPUs on the system for the SANITY check infrastructure.
    
    The feature bits are classified into following 3 types depending on
    the implication of the possible values. This information is used to
    decide the safe value for a feature.
    
    LOWER_SAFE  - The smaller value is safer
    HIGHER_SAFE - The bigger value is safer
    EXACT       - We can't decide between the two, so a predefined safe_value is used.
    
    This infrastructure will be later used to make better decisions for:
    
     - Kernel features (e.g, KVM, Debug)
     - SANITY Check
     - CPU capability
     - ELF HWCAP
     - Exposing CPU Feature register to userspace.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    [catalin.marinas@arm.com: whitespace fix]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 85507fecf287..01bb5cf995af 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -35,6 +35,37 @@
 
 #include <linux/kernel.h>
 
+/* CPU feature register tracking */
+enum ftr_type {
+	FTR_EXACT,	/* Use a predefined safe value */
+	FTR_LOWER_SAFE,	/* Smaller value is safe */
+	FTR_HIGHER_SAFE,/* Bigger value is safe */
+};
+
+#define FTR_STRICT	true	/* SANITY check strict matching required */
+#define FTR_NONSTRICT	false	/* SANITY check ignored */
+
+struct arm64_ftr_bits {
+	bool		strict;	  /* CPU Sanity check: strict matching required ? */
+	enum ftr_type	type;
+	u8		shift;
+	u8		width;
+	s64		safe_val; /* safe value for discrete features */
+};
+
+/*
+ * @arm64_ftr_reg - Feature register
+ * @strict_mask		Bits which should match across all CPUs for sanity.
+ * @sys_val		Safe value across the CPUs (system view)
+ */
+struct arm64_ftr_reg {
+	u32			sys_id;
+	const char		*name;
+	u64			strict_mask;
+	u64			sys_val;
+	struct arm64_ftr_bits	*ftr_bits;
+};
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;
@@ -88,6 +119,16 @@ cpuid_feature_extract_field(u64 features, int field)
 	return cpuid_feature_extract_field_width(features, field, 4);
 }
 
+static inline u64 arm64_ftr_mask(struct arm64_ftr_bits *ftrp)
+{
+	return (u64)GENMASK(ftrp->shift + ftrp->width - 1, ftrp->shift);
+}
+
+static inline s64 arm64_ftr_value(struct arm64_ftr_bits *ftrp, u64 val)
+{
+	return cpuid_feature_extract_field_width(val, ftrp->shift, ftrp->width);
+}
+
 static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
 {
 	return cpuid_feature_extract_field(mmfr0, ID_AA64MMFR0_BIGENDEL_SHIFT) == 0x1 ||

commit ce98a677d897dbaac86905652292fab1eeeb2b93
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:44 2015 +0100

    arm64: Handle width of a cpuid feature
    
    Introduce a helper to extract cpuid feature for any given
    width.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b5f313d42d79..85507fecf287 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -76,10 +76,16 @@ static inline void cpus_set_cap(unsigned int num)
 		__set_bit(num, cpu_hwcaps);
 }
 
-static inline int __attribute_const__ cpuid_feature_extract_field(u64 features,
-								  int field)
+static inline int __attribute_const__
+cpuid_feature_extract_field_width(u64 features, int field, int width)
 {
-	return (s64)(features << (64 - 4 - field)) >> (64 - 4);
+	return (s64)(features << (64 - width - field)) >> (64 - width);
+}
+
+static inline int __attribute_const__
+cpuid_feature_extract_field(u64 features, int field)
+{
+	return cpuid_feature_extract_field_width(features, field, 4);
 }
 
 static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)

commit cdcf817b7e4b62b935d8797f7d07ea0b97760884
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:42 2015 +0100

    arm64: Move mixed endian support detection
    
    Move the mixed endian support detection code to cpufeature.c
    from cpuinfo.c. This also moves the update_cpu_features()
    used by mixed endian detection code, which will get more
    functionality.
    
    Also moves the ID register field shifts to asm/sysreg.h,
    where all the useful definitions will end up in later patches.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b7769f698a75..b5f313d42d79 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -10,6 +10,7 @@
 #define __ASM_CPUFEATURE_H
 
 #include <asm/hwcap.h>
+#include <asm/sysreg.h>
 
 /*
  * In the arm64 world (as in the ARM world), elf_hwcap is used both internally
@@ -81,6 +82,12 @@ static inline int __attribute_const__ cpuid_feature_extract_field(u64 features,
 	return (s64)(features << (64 - 4 - field)) >> (64 - 4);
 }
 
+static inline bool id_aa64mmfr0_mixed_endian_el0(u64 mmfr0)
+{
+	return cpuid_feature_extract_field(mmfr0, ID_AA64MMFR0_BIGENDEL_SHIFT) == 0x1 ||
+		cpuid_feature_extract_field(mmfr0, ID_AA64MMFR0_BIGENDEL0_SHIFT) == 0x1;
+}
+
 void __init setup_cpu_features(void);
 
 void check_cpu_capabilities(const struct arm64_cpu_capabilities *caps,

commit 3a75578efae64b94d76eacbf8adf2a3ab13c6aa1
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:39 2015 +0100

    arm64: Delay ELF HWCAP initialisation until all CPUs are up
    
    Delay the ELF HWCAP initialisation until all the (enabled) CPUs are
    up, i.e, smp_cpus_done(). This is in preparation for detecting the
    common features across the CPUS and creating a consistent ELF HWCAP
    for the system.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 171570702bb8..b7769f698a75 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -81,6 +81,7 @@ static inline int __attribute_const__ cpuid_feature_extract_field(u64 features,
 	return (s64)(features << (64 - 4 - field)) >> (64 - 4);
 }
 
+void __init setup_cpu_features(void);
 
 void check_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);

commit 6d4e11c5e2e8cd54a035ba395bf8ccfa7e22cfd8
Author: Robert Richter <rrichter@cavium.com>
Date:   Mon Sep 21 22:58:35 2015 +0200

    irqchip/gicv3: Workaround for Cavium ThunderX erratum 23154
    
    This patch implements Cavium ThunderX erratum 23154.
    
    The gicv3 of ThunderX requires a modified version for reading the IAR
    status to ensure data synchronization. Since this is in the fast-path
    and called with each interrupt, runtime patching is used using jump
    label patching for smallest overhead (no-op). This is the same
    technique as used for tracepoints.
    
    Signed-off-by: Robert Richter <rrichter@cavium.com>
    Reviewed-by: Marc Zygnier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Tirumalesh Chalamarla <tchalamarla@cavium.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1442869119-1814-3-git-send-email-rric@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 171570702bb8..dbc78d2b8cc6 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -27,8 +27,9 @@
 #define ARM64_HAS_SYSREG_GIC_CPUIF		3
 #define ARM64_HAS_PAN				4
 #define ARM64_HAS_LSE_ATOMICS			5
+#define ARM64_WORKAROUND_CAVIUM_23154		6
 
-#define ARM64_NCAPS				6
+#define ARM64_NCAPS				7
 
 #ifndef __ASSEMBLY__
 

commit c739dc83a0b6db01abfbcc5246a30c7a575e4272
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jul 27 14:11:55 2015 +0100

    arm64: lse: rename ARM64_CPU_FEAT_LSE_ATOMICS for consistency
    
    Other CPU features follow an 'ARM64_HAS_*' naming scheme, so do the same
    for the LSE atomics.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index d9262d4b4dcd..171570702bb8 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -26,7 +26,7 @@
 #define ARM64_WORKAROUND_845719			2
 #define ARM64_HAS_SYSREG_GIC_CPUIF		3
 #define ARM64_HAS_PAN				4
-#define ARM64_CPU_FEAT_LSE_ATOMICS		5
+#define ARM64_HAS_LSE_ATOMICS			5
 
 #define ARM64_NCAPS				6
 

commit d964b7229e7f94428a1e8d26999adffbe8a69db2
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Feb 4 12:17:55 2015 +0000

    arm64: alternatives: add cpu feature for lse atomics
    
    Add a CPU feature for the LSE atomic instructions, so that they can be
    patched in at runtime when we detect that they are supported.
    
    Reviewed-by: Steve Capper <steve.capper@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 40e106f81f27..d9262d4b4dcd 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -26,8 +26,9 @@
 #define ARM64_WORKAROUND_845719			2
 #define ARM64_HAS_SYSREG_GIC_CPUIF		3
 #define ARM64_HAS_PAN				4
+#define ARM64_CPU_FEAT_LSE_ATOMICS		5
 
-#define ARM64_NCAPS				5
+#define ARM64_NCAPS				6
 
 #ifndef __ASSEMBLY__
 

commit 144e9697a9e70b5549fd52df90111f1410bcbfeb
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Apr 30 18:55:50 2015 +0100

    arm64: cpufeature.h: add missing #include of kernel.h
    
    cpufeature.h makes use of DECLARE_BITMAP, which in turn relies on the
    BITS_TO_LONGS and DIV_ROUND_UP macros.
    
    This patch includes kernel.h in cpufeature.h to prevent all users having
    to do the same thing.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index d71140b76773..40e106f81f27 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -31,6 +31,8 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/kernel.h>
+
 struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;

commit 338d4f49d6f7114a017d294ccf7374df4f998edc
Author: James Morse <james.morse@arm.com>
Date:   Wed Jul 22 19:05:54 2015 +0100

    arm64: kernel: Add support for Privileged Access Never
    
    'Privileged Access Never' is a new arm8.1 feature which prevents
    privileged code from accessing any virtual address where read or write
    access is also permitted at EL0.
    
    This patch enables the PAN feature on all CPUs, and modifies {get,put}_user
    helpers temporarily to permit access.
    
    This will catch kernel bugs where user memory is accessed directly.
    'Unprivileged loads and stores' using ldtrb et al are unaffected by PAN.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [will: use ALTERNATIVE in asm and tidy up pan_enable check]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index f595f7ddd43b..d71140b76773 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -25,8 +25,9 @@
 #define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
 #define ARM64_WORKAROUND_845719			2
 #define ARM64_HAS_SYSREG_GIC_CPUIF		3
+#define ARM64_HAS_PAN				4
 
-#define ARM64_NCAPS				4
+#define ARM64_NCAPS				5
 
 #ifndef __ASSEMBLY__
 

commit 18ffa046c509d0cd011eeea2c0418f2d014771fc
Author: James Morse <james.morse@arm.com>
Date:   Tue Jul 21 13:23:29 2015 +0100

    arm64: kernel: Add min_field_value and use '>=' for feature detection
    
    When a new cpu feature is available, the cpu feature bits will have some
    initial value, which is incremented when the feature is updated.
    This patch changes 'register_value' to be 'min_field_value', and checks
    the feature bits value (interpreted as a signed int) is greater than this
    minimum.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 484fa9425314..f595f7ddd43b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -42,8 +42,8 @@ struct arm64_cpu_capabilities {
 		};
 
 		struct {	/* Feature register checking */
-			u64 register_mask;
-			u64 register_value;
+			int field_pos;
+			int min_field_value;
 		};
 	};
 };

commit 1c0763037f1e1caef739e36e09c6d41ed7b61b2d
Author: James Morse <james.morse@arm.com>
Date:   Tue Jul 21 13:23:28 2015 +0100

    arm64: kernel: Add cpufeature 'enable' callback
    
    This patch adds an 'enable()' callback to cpu capability/feature
    detection, allowing features that require some setup or configuration
    to get this opportunity once the feature has been detected.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 9fafa7537997..484fa9425314 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -34,6 +34,7 @@ struct arm64_cpu_capabilities {
 	const char *desc;
 	u16 capability;
 	bool (*matches)(const struct arm64_cpu_capabilities *);
+	void (*enable)(void);
 	union {
 		struct {	/* To be used for erratum handling only */
 			u32 midr_model;

commit 79b0e09a3c9bd74ee54582efdb351179d7c00351
Author: James Morse <james.morse@arm.com>
Date:   Tue Jul 21 13:23:26 2015 +0100

    arm64: kernel: Add cpuid_feature_extract_field() for 4bit sign extension
    
    Based on arch/arm/include/asm/cputype.h, this function does the
    shifting and sign extension necessary when accessing cpu feature fields.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Suggested-by: Russell King <linux@arm.linux.org.uk>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index c1044218a63a..9fafa7537997 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -70,6 +70,13 @@ static inline void cpus_set_cap(unsigned int num)
 		__set_bit(num, cpu_hwcaps);
 }
 
+static inline int __attribute_const__ cpuid_feature_extract_field(u64 features,
+								  int field)
+{
+	return (s64)(features << (64 - 4 - field)) >> (64 - 4);
+}
+
+
 void check_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
 			    const char *info);
 void check_local_cpu_errata(void);

commit 94a9e04aa16abd1194d9b4158c618ba87f5d01e6
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jun 12 12:06:36 2015 +0100

    arm64: alternative: Introduce feature for GICv3 CPU interface
    
    Add a new item to the feature set (ARM64_HAS_SYSREG_GIC_CPUIF)
    to indicate that we have a system register GIC CPU interface
    
    This will help KVM switching to alternative instruction patching.
    
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 82cb9f98ba1a..c1044218a63a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -24,8 +24,9 @@
 #define ARM64_WORKAROUND_CLEAN_CACHE		0
 #define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
 #define ARM64_WORKAROUND_845719			2
+#define ARM64_HAS_SYSREG_GIC_CPUIF		3
 
-#define ARM64_NCAPS				3
+#define ARM64_NCAPS				4
 
 #ifndef __ASSEMBLY__
 
@@ -38,6 +39,11 @@ struct arm64_cpu_capabilities {
 			u32 midr_model;
 			u32 midr_range_min, midr_range_max;
 		};
+
+		struct {	/* Feature register checking */
+			u64 register_mask;
+			u64 register_value;
+		};
 	};
 };
 

commit 905e8c5dcaa147163672b06fe9dcb5abaacbc711
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Mar 23 19:07:02 2015 +0000

    arm64: errata: add workaround for cortex-a53 erratum #845719
    
    When running a compat (AArch32) userspace on Cortex-A53, a load at EL0
    from a virtual address that matches the bottom 32 bits of the virtual
    address used by a recent load at (AArch64) EL1 might return incorrect
    data.
    
    This patch works around the issue by writing to the contextidr_el1
    register on the exception return path when returning to a 32-bit task.
    This workaround is patched in at runtime based on the MIDR value of the
    processor.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 6ae35d160464..82cb9f98ba1a 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -23,8 +23,9 @@
 
 #define ARM64_WORKAROUND_CLEAN_CACHE		0
 #define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
+#define ARM64_WORKAROUND_845719			2
 
-#define ARM64_NCAPS				2
+#define ARM64_NCAPS				3
 
 #ifndef __ASSEMBLY__
 

commit 359b706473b47da3c93bd99fd10d798fe411ab67
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Mar 27 13:09:23 2015 +0000

    arm64: Extract feature parsing code from cpu_errata.c
    
    As we detect more architectural features at runtime, it makes
    sense to reuse the existing framework whilst avoiding to call
    a feature an erratum...
    
    This patch extract the core capability parsing, moves it into
    a new file (cpufeature.c), and let the CPU errata detection code
    use it.
    
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index b6c16d5f622f..6ae35d160464 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -28,6 +28,18 @@
 
 #ifndef __ASSEMBLY__
 
+struct arm64_cpu_capabilities {
+	const char *desc;
+	u16 capability;
+	bool (*matches)(const struct arm64_cpu_capabilities *);
+	union {
+		struct {	/* To be used for erratum handling only */
+			u32 midr_model;
+			u32 midr_range_min, midr_range_max;
+		};
+	};
+};
+
 extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 
 static inline bool cpu_have_feature(unsigned int num)
@@ -51,7 +63,10 @@ static inline void cpus_set_cap(unsigned int num)
 		__set_bit(num, cpu_hwcaps);
 }
 
+void check_cpu_capabilities(const struct arm64_cpu_capabilities *caps,
+			    const char *info);
 void check_local_cpu_errata(void);
+void check_local_cpu_features(void);
 bool cpu_supports_mixed_endian_el0(void);
 bool system_supports_mixed_endian_el0(void);
 

commit 04597a65c5efc207257a736d339c6f2f5b00250f
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Wed Jan 21 12:43:09 2015 +0000

    arm64: Track system support for mixed endian EL0
    
    This patch keeps track of the mixed endian EL0 support across
    the system and provides helper functions to export it. The status
    is a boolean indicating whether all the CPUs on the system supports
    mixed endian at EL0.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 07547ccc1f2b..b6c16d5f622f 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -52,6 +52,8 @@ static inline void cpus_set_cap(unsigned int num)
 }
 
 void check_local_cpu_errata(void);
+bool cpu_supports_mixed_endian_el0(void);
+bool system_supports_mixed_endian_el0(void);
 
 #endif /* __ASSEMBLY__ */
 

commit 06f9eb884be81431d54d7d37390043e3b5b7f14a
Author: Fabio Estevam <fabio.estevam@freescale.com>
Date:   Thu Dec 4 01:17:01 2014 +0000

    arm64: Provide a namespace to NCAPS
    
    Building arm64.allmodconfig leads to the following warning:
    
    usb/gadget/function/f_ncm.c:203:0: warning: "NCAPS" redefined
     #define NCAPS (USB_CDC_NCM_NCAP_ETH_FILTER | USB_CDC_NCM_NCAP_CRC_MODE)
     ^
    In file included from /home/build/work/batch/arch/arm64/include/asm/io.h:32:0,
                     from /home/build/work/batch/include/linux/clocksource.h:19,
                     from /home/build/work/batch/include/clocksource/arm_arch_timer.h:19,
                     from /home/build/work/batch/arch/arm64/include/asm/arch_timer.h:27,
                     from /home/build/work/batch/arch/arm64/include/asm/timex.h:19,
                     from /home/build/work/batch/include/linux/timex.h:65,
                     from /home/build/work/batch/include/linux/sched.h:19,
                     from /home/build/work/batch/arch/arm64/include/asm/compat.h:25,
                     from /home/build/work/batch/arch/arm64/include/asm/stat.h:23,
                     from /home/build/work/batch/include/linux/stat.h:5,
                     from /home/build/work/batch/include/linux/module.h:10,
                     from /home/build/work/batch/drivers/usb/gadget/function/f_ncm.c:19:
    arch/arm64/include/asm/cpufeature.h:27:0: note: this is the location of the previous definition
     #define NCAPS     2
    
    So add a ARM64 prefix to avoid such problem.
    
    Reported-by: Olof's autobuilder <build@lixom.net>
    Signed-off-by: Fabio Estevam <fabio.estevam@freescale.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 0362f8020d46..07547ccc1f2b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -24,11 +24,11 @@
 #define ARM64_WORKAROUND_CLEAN_CACHE		0
 #define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
 
-#define NCAPS					2
+#define ARM64_NCAPS				2
 
 #ifndef __ASSEMBLY__
 
-extern DECLARE_BITMAP(cpu_hwcaps, NCAPS);
+extern DECLARE_BITMAP(cpu_hwcaps, ARM64_NCAPS);
 
 static inline bool cpu_have_feature(unsigned int num)
 {
@@ -37,16 +37,16 @@ static inline bool cpu_have_feature(unsigned int num)
 
 static inline bool cpus_have_cap(unsigned int num)
 {
-	if (num >= NCAPS)
+	if (num >= ARM64_NCAPS)
 		return false;
 	return test_bit(num, cpu_hwcaps);
 }
 
 static inline void cpus_set_cap(unsigned int num)
 {
-	if (num >= NCAPS)
+	if (num >= ARM64_NCAPS)
 		pr_warn("Attempt to set an illegal CPU capability (%d >= %d)\n",
-			num, NCAPS);
+			num, ARM64_NCAPS);
 	else
 		__set_bit(num, cpu_hwcaps);
 }

commit 5afaa1fc1b320cec48affa7e6949f2493f875c12
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:11 2014 +0000

    arm64: add Cortex-A57 erratum 832075 workaround
    
    The ARM erratum 832075 applies to certain revisions of Cortex-A57,
    one of the workarounds is to change device loads into using
    load-aquire semantics.
    This is achieved using the alternatives framework.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 92b6ee44669b..0362f8020d46 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -21,9 +21,10 @@
 #define MAX_CPU_FEATURES	(8 * sizeof(elf_hwcap))
 #define cpu_feature(x)		ilog2(HWCAP_ ## x)
 
-#define ARM64_WORKAROUND_CLEAN_CACHE	0
+#define ARM64_WORKAROUND_CLEAN_CACHE		0
+#define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
 
-#define NCAPS				1
+#define NCAPS					2
 
 #ifndef __ASSEMBLY__
 

commit 301bcfac42897dbd1b0b3c1be49f24654a1bc49e
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:10 2014 +0000

    arm64: add Cortex-A53 cache errata workaround
    
    The ARM errata 819472, 826319, 827319 and 824069 define the same
    workaround for these hardware issues in certain Cortex-A53 parts.
    Use the new alternatives framework and the CPU MIDR detection to
    patch "cache clean" into "cache clean and invalidate" instructions if
    an affected CPU is detected at runtime.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [will: add __maybe_unused to squash gcc warning]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 744eaf7fab0f..92b6ee44669b 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -21,7 +21,11 @@
 #define MAX_CPU_FEATURES	(8 * sizeof(elf_hwcap))
 #define cpu_feature(x)		ilog2(HWCAP_ ## x)
 
-#define NCAPS			0
+#define ARM64_WORKAROUND_CLEAN_CACHE	0
+
+#define NCAPS				1
+
+#ifndef __ASSEMBLY__
 
 extern DECLARE_BITMAP(cpu_hwcaps, NCAPS);
 
@@ -48,4 +52,6 @@ static inline void cpus_set_cap(unsigned int num)
 
 void check_local_cpu_errata(void);
 
+#endif /* __ASSEMBLY__ */
+
 #endif

commit e116a375423393cdb94714e90a96857005d58428
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:09 2014 +0000

    arm64: detect silicon revisions and set cap bits accordingly
    
    After each CPU has been started, we iterate through a list of
    CPU features or bugs to detect CPUs which need (or could benefit
    from) kernel code patches.
    For each feature/bug there is a function which checks if that
    particular CPU is affected. We will later provide some more generic
    functions for common things like testing for certain MIDR ranges.
    We do this for every CPU to cover big.LITTLE systems properly as
    well.
    If a certain feature/bug has been detected, the capability bit will
    be set, so that later the call to apply_alternatives() will trigger
    the actual code patching.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index 20b2b3d6b702..744eaf7fab0f 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -46,4 +46,6 @@ static inline void cpus_set_cap(unsigned int num)
 		__set_bit(num, cpu_hwcaps);
 }
 
+void check_local_cpu_errata(void);
+
 #endif

commit 930da09f5e50dd22fb0a8600388da8677d62d671
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:07 2014 +0000

    arm64: add cpu_capabilities bitmap
    
    For taking note if at least one CPU in the system needs a bug
    workaround or would benefit from a code optimization, we create a new
    bitmap to hold (artificial) feature bits.
    Since elf_hwcap is part of the userland ABI, we keep it alone and
    introduce a new data structure for that (along with some accessors).
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
index cd4ac0516488..20b2b3d6b702 100644
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -21,9 +21,29 @@
 #define MAX_CPU_FEATURES	(8 * sizeof(elf_hwcap))
 #define cpu_feature(x)		ilog2(HWCAP_ ## x)
 
+#define NCAPS			0
+
+extern DECLARE_BITMAP(cpu_hwcaps, NCAPS);
+
 static inline bool cpu_have_feature(unsigned int num)
 {
 	return elf_hwcap & (1UL << num);
 }
 
+static inline bool cpus_have_cap(unsigned int num)
+{
+	if (num >= NCAPS)
+		return false;
+	return test_bit(num, cpu_hwcaps);
+}
+
+static inline void cpus_set_cap(unsigned int num)
+{
+	if (num >= NCAPS)
+		pr_warn("Attempt to set an illegal CPU capability (%d >= %d)\n",
+			num, NCAPS);
+	else
+		__set_bit(num, cpu_hwcaps);
+}
+
 #endif

commit 3be1a5c4f75989cf457f13f38ff0913dff6d4996
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 4 01:10:04 2014 +0000

    arm64: enable generic CPU feature modalias matching for this architecture
    
    This enables support for the generic CPU feature modalias implementation that
    wires up optional CPU features to udev based module autoprobing.
    
    A file <asm/cpufeature.h> is provided that maps CPU feature numbers to
    elf_hwcap bits, which is the standard way on arm64 to advertise optional CPU
    features both internally and to user space.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [catalin.marinas@arm.com: removed unnecessary "!!"]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
new file mode 100644
index 000000000000..cd4ac0516488
--- /dev/null
+++ b/arch/arm64/include/asm/cpufeature.h
@@ -0,0 +1,29 @@
+/*
+ * Copyright (C) 2014 Linaro Ltd. <ard.biesheuvel@linaro.org>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __ASM_CPUFEATURE_H
+#define __ASM_CPUFEATURE_H
+
+#include <asm/hwcap.h>
+
+/*
+ * In the arm64 world (as in the ARM world), elf_hwcap is used both internally
+ * in the kernel and for user space to keep track of which optional features
+ * are supported by the current system. So let's map feature 'x' to HWCAP_x.
+ * Note that HWCAP_x constants are bit fields so we need to take the log.
+ */
+
+#define MAX_CPU_FEATURES	(8 * sizeof(elf_hwcap))
+#define cpu_feature(x)		ilog2(HWCAP_ ## x)
+
+static inline bool cpu_have_feature(unsigned int num)
+{
+	return elf_hwcap & (1UL << num);
+}
+
+#endif
