commit e9c7ddbf8b4b6a291bf3b5bfa7c883235164d9be
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Mon Jan 20 18:52:29 2020 +0000

    arm64: csum: Optimise IPv6 header checksum
    
    Throwing our __uint128_t idioms at csum_ipv6_magic() makes it
    about 1.3x-2x faster across a range of microarchitecture/compiler
    combinations. Not much in absolute terms, but every little helps.
    
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/checksum.h b/arch/arm64/include/asm/checksum.h
index 8d2a7de39744..b6f7bc6da5fb 100644
--- a/arch/arm64/include/asm/checksum.h
+++ b/arch/arm64/include/asm/checksum.h
@@ -5,7 +5,12 @@
 #ifndef __ASM_CHECKSUM_H
 #define __ASM_CHECKSUM_H
 
-#include <linux/types.h>
+#include <linux/in6.h>
+
+#define _HAVE_ARCH_IPV6_CSUM
+__sum16 csum_ipv6_magic(const struct in6_addr *saddr,
+			const struct in6_addr *daddr,
+			__u32 len, __u8 proto, __wsum sum);
 
 static inline __sum16 csum_fold(__wsum csum)
 {

commit 5777eaed566a1d63e344d3dd8f2b5e33be20643e
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Wed Jan 15 16:42:39 2020 +0000

    arm64: Implement optimised checksum routine
    
    Apparently there exist certain workloads which rely heavily on software
    checksumming, for which the generic do_csum() implementation becomes a
    significant bottleneck. Therefore let's give arm64 its own optimised
    version - for ease of maintenance this foregoes assembly or intrisics,
    and is thus not actually arm64-specific, but does rely heavily on C
    idioms that translate well to the A64 ISA and the typical load/store
    capabilities of most ARMv8 CPU cores.
    
    The resulting increase in checksum throughput scales nicely with buffer
    size, tending towards 4x for a small in-order core (Cortex-A53), and up
    to 6x or more for an aggressive big core (Ampere eMAG).
    
    Reported-by: Lingyan Huang <huanglingyan2@huawei.com>
    Tested-by: Lingyan Huang <huanglingyan2@huawei.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/checksum.h b/arch/arm64/include/asm/checksum.h
index d064a50deb5f..8d2a7de39744 100644
--- a/arch/arm64/include/asm/checksum.h
+++ b/arch/arm64/include/asm/checksum.h
@@ -35,6 +35,9 @@ static inline __sum16 ip_fast_csum(const void *iph, unsigned int ihl)
 }
 #define ip_fast_csum ip_fast_csum
 
+extern unsigned int do_csum(const unsigned char *buff, int len);
+#define do_csum do_csum
+
 #include <asm-generic/checksum.h>
 
 #endif	/* __ASM_CHECKSUM_H */

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/checksum.h b/arch/arm64/include/asm/checksum.h
index 0b6f5a7d4027..d064a50deb5f 100644
--- a/arch/arm64/include/asm/checksum.h
+++ b/arch/arm64/include/asm/checksum.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2016 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_CHECKSUM_H
 #define __ASM_CHECKSUM_H

commit 50a4b05609929003ce98987bb901ee10fe21fb20
Author: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Date:   Thu Jun 29 16:31:40 2017 +0200

    arm64: add missing conversion to __wsum in ip_fast_csum()
    
    ARM64 implementation of ip_fast_csum() do most of the work
    in 128 or 64 bit and call csum_fold() to finalize. csum_fold()
    itself take a __wsum argument, to insure that this value is
    always a 32bit native-order value.
    
    Fix this by adding the sadly needed '__force' to cast the native
    'sum' to the type '__wsum'.
    
    Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/checksum.h b/arch/arm64/include/asm/checksum.h
index 09f65339d66d..0b6f5a7d4027 100644
--- a/arch/arm64/include/asm/checksum.h
+++ b/arch/arm64/include/asm/checksum.h
@@ -42,7 +42,7 @@ static inline __sum16 ip_fast_csum(const void *iph, unsigned int ihl)
 	} while (--ihl);
 
 	sum += ((sum >> 32) | (sum << 32));
-	return csum_fold(sum >> 32);
+	return csum_fold((__force u32)(sum >> 32));
 }
 #define ip_fast_csum ip_fast_csum
 

commit 0e455d8e80aa22d7ed42c9f5e4e3a75c558aa543
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue May 31 18:04:40 2016 +0100

    arm64: Implement optimised IP checksum helpers
    
    AArch64 is capable of 128-bit memory accesses without alignment
    restrictions, which makes it both possible and highly practical to slurp
    up a typical 20-byte IP header in just 2 loads. Implement our own
    version of ip_fast_checksum() to take advantage of that, resulting in
    considerably fewer instructions and memory accesses than the generic
    version. We can also get more optimal code generation for csum_fold() by
    defining it a slightly different way round from the generic version, so
    throw that into the mix too.
    
    Suggested-by: Luke Starrett <luke.starrett@broadcom.com>
    Acked-by: Luke Starrett <luke.starrett@broadcom.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/checksum.h b/arch/arm64/include/asm/checksum.h
new file mode 100644
index 000000000000..09f65339d66d
--- /dev/null
+++ b/arch/arm64/include/asm/checksum.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright (C) 2016 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_CHECKSUM_H
+#define __ASM_CHECKSUM_H
+
+#include <linux/types.h>
+
+static inline __sum16 csum_fold(__wsum csum)
+{
+	u32 sum = (__force u32)csum;
+	sum += (sum >> 16) | (sum << 16);
+	return ~(__force __sum16)(sum >> 16);
+}
+#define csum_fold csum_fold
+
+static inline __sum16 ip_fast_csum(const void *iph, unsigned int ihl)
+{
+	__uint128_t tmp;
+	u64 sum;
+
+	tmp = *(const __uint128_t *)iph;
+	iph += 16;
+	ihl -= 4;
+	tmp += ((tmp >> 64) | (tmp << 64));
+	sum = tmp >> 64;
+	do {
+		sum += *(const u32 *)iph;
+		iph += 4;
+	} while (--ihl);
+
+	sum += ((sum >> 32) | (sum << 32));
+	return csum_fold(sum >> 32);
+}
+#define ip_fast_csum ip_fast_csum
+
+#include <asm-generic/checksum.h>
+
+#endif	/* __ASM_CHECKSUM_H */
