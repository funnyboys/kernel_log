commit c350717ec7de67daad26d996efe7f3d97d95aa9c
Merge: d27865279f12 fe677be98914
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:02:51 2020 +0100

    Merge branch 'for-next/kvm/errata' into for-next/core
    
    KVM CPU errata rework
    (Andrew Scull and Marc Zyngier)
    * for-next/kvm/errata:
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}

commit d27865279f12035c730818aa1a0280fada866a37
Merge: 342403bcb4df a4eb355a3fda
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:00:51 2020 +0100

    Merge branch 'for-next/bti' into for-next/core
    
    Support for Branch Target Identification (BTI) in user and kernel
    (Mark Brown and others)
    * for-next/bti: (39 commits)
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      arm64: bti: Fix support for userspace only BTI
      arm64: kconfig: Update and comment GCC version check for kernel BTI
      arm64: vdso: Map the vDSO text with guarded pages when built for BTI
      arm64: vdso: Force the vDSO to be linked as BTI when built for BTI
      arm64: vdso: Annotate for BTI
      arm64: asm: Provide a mechanism for generating ELF note for BTI
      arm64: bti: Provide Kconfig for kernel mode BTI
      arm64: mm: Mark executable text as guarded pages
      arm64: bpf: Annotate JITed code for BTI
      arm64: Set GP bit in kernel page tables to enable BTI for the kernel
      arm64: asm: Override SYM_FUNC_START when building the kernel with BTI
      arm64: bti: Support building kernel C code using BTI
      arm64: Document why we enable PAC support for leaf functions
      arm64: insn: Report PAC and BTI instructions as skippable
      arm64: insn: Don't assume unrecognized HINTs are skippable
      arm64: insn: Provide a better name for aarch64_insn_is_nop()
      arm64: insn: Add constants for new HINT instruction decode
      arm64: Disable old style assembly annotations
      ...

commit 80e4e561321595d2e5f4a173e8cf8d8432078995
Merge: 6a8b55ed4056 5d1b631c773f
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 15:15:58 2020 +0100

    Merge branch 'for-next/bti-user' into for-next/bti
    
    Merge in user support for Branch Target Identification, which narrowly
    missed the cut for 5.7 after a late ABI concern.
    
    * for-next/bti-user:
      arm64: bti: Document behaviour for dynamically linked binaries
      arm64: elf: Fix allnoconfig kernel build with !ARCH_USE_GNU_PROPERTY
      arm64: BTI: Add Kconfig entry for userspace BTI
      mm: smaps: Report arm64 guarded pages in smaps
      arm64: mm: Display guarded pages in ptdump
      KVM: arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: traps: Shuffle code to eliminate forward declarations
      arm64: unify native/compat instruction skipping
      arm64: BTI: Decode BYTPE bits when printing PSTATE
      arm64: elf: Enable BTI at exec based on ELF program properties
      elf: Allow arch to tweak initial mmap prot flags
      arm64: Basic Branch Target Identification support
      ELF: Add ELF program property parsing support
      ELF: UAPI and Kconfig additions for ELF program properties

commit 02ab1f5018c3ad0b8677e797b5d3333d2e3b7f20
Author: Andrew Scull <ascull@google.com>
Date:   Mon May 4 10:48:58 2020 +0100

    arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}
    
    Errata 1165522, 1319367 and 1530923 each allow TLB entries to be
    allocated as a result of a speculative AT instruction. In order to
    avoid mandating VHE on certain affected CPUs, apply the workaround to
    both the nVHE and the VHE case for all affected CPUs.
    
    Signed-off-by: Andrew Scull <ascull@google.com>
    Acked-by: Will Deacon <will@kernel.org>
    CC: Marc Zyngier <maz@kernel.org>
    CC: James Morse <james.morse@arm.com>
    CC: Suzuki K Poulose <suzuki.poulose@arm.com>
    CC: Will Deacon <will@kernel.org>
    CC: Steven Price <steven.price@arm.com>
    Link: https://lore.kernel.org/r/20200504094858.108917-1-ascull@google.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 8eb5a088ae65..dc70883062ba 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -44,7 +44,7 @@
 #define ARM64_SSBS				34
 #define ARM64_WORKAROUND_1418040		35
 #define ARM64_HAS_SB				36
-#define ARM64_WORKAROUND_SPECULATIVE_AT_VHE	37
+#define ARM64_WORKAROUND_SPECULATIVE_AT	37
 #define ARM64_HAS_ADDRESS_AUTH_ARCH		38
 #define ARM64_HAS_ADDRESS_AUTH_IMP_DEF		39
 #define ARM64_HAS_GENERIC_AUTH_ARCH		40
@@ -55,13 +55,12 @@
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	45
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
 #define ARM64_WORKAROUND_1542419		47
-#define ARM64_WORKAROUND_SPECULATIVE_AT_NVHE	48
-#define ARM64_HAS_E0PD				49
-#define ARM64_HAS_RNG				50
-#define ARM64_HAS_AMU_EXTN			51
-#define ARM64_HAS_ADDRESS_AUTH			52
-#define ARM64_HAS_GENERIC_AUTH			53
+#define ARM64_HAS_E0PD				48
+#define ARM64_HAS_RNG				49
+#define ARM64_HAS_AMU_EXTN			50
+#define ARM64_HAS_ADDRESS_AUTH			51
+#define ARM64_HAS_GENERIC_AUTH			52
 
-#define ARM64_NCAPS				54
+#define ARM64_NCAPS				53
 
 #endif /* __ASM_CPUCAPS_H */

commit 540f76d12c662d3da2ebdf0086ee289123fcd120
Author: Will Deacon <will@kernel.org>
Date:   Tue Apr 21 15:29:17 2020 +0100

    arm64: cpufeature: Add CPU capability for AArch32 EL1 support
    
    Although we emit a "SANITY CHECK" warning and taint the kernel if we
    detect a CPU mismatch for AArch32 support at EL1, we still online the
    CPU with disastrous consequences for any running 32-bit VMs.
    
    Introduce a capability for AArch32 support at EL1 so that late onlining
    of incompatible CPUs is forbidden.
    
    Tested-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200421142922.18950-4-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 8eb5a088ae65..c54c674e6c21 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -61,7 +61,8 @@
 #define ARM64_HAS_AMU_EXTN			51
 #define ARM64_HAS_ADDRESS_AUTH			52
 #define ARM64_HAS_GENERIC_AUTH			53
+#define ARM64_HAS_32BIT_EL1			54
 
-#define ARM64_NCAPS				54
+#define ARM64_NCAPS				55
 
 #endif /* __ASM_CPUCAPS_H */

commit 44ca0e00b6a05ea9cf89d8a5290a225de19f4a2a
Merge: 806dc825f01f 3b446c7d27dd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:11:08 2020 +0000

    Merge branch 'for-next/kernel-ptrauth' into for-next/core
    
    * for-next/kernel-ptrauth:
      : Return address signing - in-kernel support
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      arm64: ptrauth: Add bootup/runtime flags for __cpu_setup
      arm64: install user ptrauth keys at kernel exit time
      arm64: rename ptrauth key structures to be user-specific
      arm64: cpufeature: add pointer auth meta-capabilities
      arm64: cpufeature: Fix meta-capability cpufeature check

commit cfef06bd0686a578aa53e039c9aec0b1a5581d3b
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:49 2020 +0530

    arm64: cpufeature: add pointer auth meta-capabilities
    
    To enable pointer auth for the kernel, we're going to need to check for
    the presence of address auth and generic auth using alternative_if. We
    currently have two cpucaps for each, but alternative_if needs to check a
    single cpucap. So define meta-capabilities that are present when either
    of the current two capabilities is present.
    
    Leave the existing four cpucaps in place, as they are still needed to
    check for mismatched systems where one CPU has the architected algorithm
    but another has the IMP DEF algorithm.
    
    Note, the meta-capabilities were present before but were removed in
    commit a56005d32105 ("arm64: cpufeature: Reduce number of pointer auth
    CPU caps from 6 to 4") and commit 1e013d06120c ("arm64: cpufeature: Rework
    ptr auth hwcaps using multi_entry_cap_matches"), as they were not needed
    then. Note, unlike before, the current patch checks the cpucap values
    directly, instead of reading the CPU ID register value.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: commit message and macro rebase, use __system_matches_cap]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 865e0253fc1e..72e4e0580ddb 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -58,7 +58,9 @@
 #define ARM64_WORKAROUND_SPECULATIVE_AT_NVHE	48
 #define ARM64_HAS_E0PD				49
 #define ARM64_HAS_RNG				50
+#define ARM64_HAS_ADDRESS_AUTH			51
+#define ARM64_HAS_GENERIC_AUTH			52
 
-#define ARM64_NCAPS				51
+#define ARM64_NCAPS				53
 
 #endif /* __ASM_CPUCAPS_H */

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 865e0253fc1e..58e776c22aab 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -58,7 +58,8 @@
 #define ARM64_WORKAROUND_SPECULATIVE_AT_NVHE	48
 #define ARM64_HAS_E0PD				49
 #define ARM64_HAS_RNG				50
+#define ARM64_BTI				51
 
-#define ARM64_NCAPS				51
+#define ARM64_NCAPS				52
 
 #endif /* __ASM_CPUCAPS_H */

commit 2c9d45b43c39e26fd2a73f2203321cdaee98b58b
Author: Ionela Voinescu <ionela.voinescu@arm.com>
Date:   Thu Mar 5 09:06:21 2020 +0000

    arm64: add support for the AMU extension v1
    
    The activity monitors extension is an optional extension introduced
    by the ARMv8.4 CPU architecture. This implements basic support for
    version 1 of the activity monitors architecture, AMUv1.
    
    This support includes:
    - Extension detection on each CPU (boot, secondary, hotplugged)
    - Register interface for AMU aarch64 registers
    
    Signed-off-by: Ionela Voinescu <ionela.voinescu@arm.com>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 865e0253fc1e..185e44aa2713 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -58,7 +58,8 @@
 #define ARM64_WORKAROUND_SPECULATIVE_AT_NVHE	48
 #define ARM64_HAS_E0PD				49
 #define ARM64_HAS_RNG				50
+#define ARM64_HAS_AMU_EXTN			51
 
-#define ARM64_NCAPS				51
+#define ARM64_NCAPS				52
 
 #endif /* __ASM_CPUCAPS_H */

commit bc206065944e2d6bd917e719dc897ffff8dbbca8
Merge: ab3906c53144 2e8e1ea88cbc
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:38:53 2020 +0000

    Merge branch 'for-next/rng' into for-next/core
    
    * for-next/rng: (2 commits)
      arm64: Use v8.5-RNG entropy for KASLR seed
      ...

commit ab3906c53144837f1a192b5c3ba71ec2f938c187
Merge: aa246c056c43 275fa0ea2cf7
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:35:05 2020 +0000

    Merge branch 'for-next/errata' into for-next/core
    
    * for-next/errata: (3 commits)
      arm64: Workaround for Cortex-A55 erratum 1530923
      ...

commit 1a50ec0b3b2e9a83f1b1245ea37a853aac2f741c
Author: Richard Henderson <richard.henderson@linaro.org>
Date:   Tue Jan 21 12:58:52 2020 +0000

    arm64: Implement archrandom.h for ARMv8.5-RNG
    
    Expose the ID_AA64ISAR0.RNDR field to userspace, as the RNG system
    registers are always available at EL0.
    
    Implement arch_get_random_seed_long using RNDR.  Given that the
    TRNG is likely to be a shared resource between cores, and VMs,
    do not explicitly force re-seeding with RNDRRS.  In order to avoid
    code complexity and potential issues with hetrogenous systems only
    provide values after cpufeature has finalized the system capabilities.
    
    Signed-off-by: Richard Henderson <richard.henderson@linaro.org>
    [Modified to only function after cpufeature has finalized the system
    capabilities and move all the code into the header -- broonie]
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    [will: Advertise HWCAP via /proc/cpuinfo]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index b92683871119..515f4fbcbf91 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -56,7 +56,8 @@
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
 #define ARM64_WORKAROUND_1542419		47
 #define ARM64_WORKAROUND_1319367		48
+#define ARM64_HAS_RNG				49
 
-#define ARM64_NCAPS				49
+#define ARM64_NCAPS				50
 
 #endif /* __ASM_CPUCAPS_H */

commit db0d46a58d34c7cd9d5ece98daf4b8afe3d770f8
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:30 2019 +0000

    arm64: Rename WORKAROUND_1319367 to SPECULATIVE_AT_NVHE
    
    To match SPECULATIVE_AT_VHE let's also have a generic name for the NVHE
    variant.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 327a38a5162f..3d1aa1b02093 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -55,7 +55,7 @@
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	45
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
 #define ARM64_WORKAROUND_1542419		47
-#define ARM64_WORKAROUND_1319367		48
+#define ARM64_WORKAROUND_SPECULATIVE_AT_NVHE	48
 
 #define ARM64_NCAPS				49
 

commit e85d68faed4e79fd0b481c72de8245d4290369db
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:29 2019 +0000

    arm64: Rename WORKAROUND_1165522 to SPECULATIVE_AT_VHE
    
    Cortex-A55 is affected by a similar erratum, so rename the existing
    workaround for errarum 1165522 so it can be used for both errata.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index b92683871119..327a38a5162f 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -44,7 +44,7 @@
 #define ARM64_SSBS				34
 #define ARM64_WORKAROUND_1418040		35
 #define ARM64_HAS_SB				36
-#define ARM64_WORKAROUND_1165522		37
+#define ARM64_WORKAROUND_SPECULATIVE_AT_VHE	37
 #define ARM64_HAS_ADDRESS_AUTH_ARCH		38
 #define ARM64_HAS_ADDRESS_AUTH_IMP_DEF		39
 #define ARM64_HAS_GENERIC_AUTH_ARCH		40

commit 3e6c69a058deaa50d33c3dac36cde80b4ce590e8
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Dec 9 18:12:14 2019 +0000

    arm64: Add initial support for E0PD
    
    Kernel Page Table Isolation (KPTI) is used to mitigate some speculation
    based security issues by ensuring that the kernel is not mapped when
    userspace is running but this approach is expensive and is incompatible
    with SPE.  E0PD, introduced in the ARMv8.5 extensions, provides an
    alternative to this which ensures that accesses from userspace to the
    kernel's half of the memory map to always fault with constant time,
    preventing timing attacks without requiring constant unmapping and
    remapping or preventing legitimate accesses.
    
    Currently this feature will only be enabled if all CPUs in the system
    support E0PD, if some CPUs do not support the feature at boot time then
    the feature will not be enabled and in the unlikely event that a late
    CPU is the first CPU to lack the feature then we will reject that CPU.
    
    This initial patch does not yet integrate with KPTI, this will be dealt
    with in followup patches.  Ideally we could ensure that by default we
    don't use KPTI on CPUs where E0PD is present.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: Fixed typo in Kconfig text]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index b92683871119..33ff25c1ab1b 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -56,7 +56,8 @@
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
 #define ARM64_WORKAROUND_1542419		47
 #define ARM64_WORKAROUND_1319367		48
+#define ARM64_HAS_E0PD				49
 
-#define ARM64_NCAPS				49
+#define ARM64_NCAPS				50
 
 #endif /* __ASM_CPUCAPS_H */

commit 346f6a4636f64c19a27722cf6ec93b38bb4251d4
Merge: 6a036afb5511 c2cc62d83186
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 16:22:49 2019 +0000

    Merge branch 'kvm-arm64/erratum-1319367' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms into for-next/core
    
    Similarly to erratum 1165522 that affects Cortex-A76, A57 and A72
    respectively suffer from errata 1319537 and 1319367, potentially
    resulting in TLB corruption if the CPU speculates an AT instruction
    while switching guests.
    
    The fix is slightly more involved since we don't have VHE to help us
    here, but the idea is the same: when switching a guest in, we must
    prevent any speculated AT from being able to parse the page tables
    until S2 is up and running. Only at this stage can we allow AT to take
    place.
    
    For this, we always restore the guest sysregs first, except for its
    SCTLR and TCR registers, which must be set with SCTLR.M=1 and
    TCR.EPD{0,1} = {1, 1}, effectively disabling the PTW and TLB
    allocation. Once S2 is setup, we restore the guest's SCTLR and
    TCR. Similar things must be done on TLB invalidation...
    
    * 'kvm-arm64/erratum-1319367' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms:
      arm64: Enable and document ARM errata 1319367 and 1319537
      arm64: KVM: Prevent speculative S1 PTW when restoring vcpu context
      arm64: KVM: Disable EL1 PTW when invalidating S2 TLBs
      arm64: KVM: Reorder system register restoration and stage-2 activation
      arm64: Add ARM64_WORKAROUND_1319367 for all A57 and A72 versions

commit 6a036afb5511418995359a9131dbea276758bd10
Merge: ba95e9bd9637 27a22fbdeedd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 16:12:40 2019 +0000

    Merge branch 'for-next/neoverse-n1-stale-instr' into for-next/core
    
    Neoverse-N1 cores with the 'COHERENT_ICACHE' feature may fetch stale
    instructions when software depends on prefetch-speculation-protection
    instead of explicit synchronization. [0]
    
    The workaround is to trap I-Cache maintenance and issue an
    inner-shareable TLBI. The affected cores have a Coherent I-Cache, so the
    I-Cache maintenance isn't necessary. The core tells user-space it can
    skip it with CTR_EL0.DIC. We also have to trap this register to hide the
    bit forcing DIC-aware user-space to perform the maintenance.
    
    To avoid trapping all cache-maintenance, this workaround depends on
    a firmware component that only traps I-cache maintenance from EL0 and
    performs the workaround.
    
    For user-space, the kernel's work is to trap CTR_EL0 to hide DIC, and
    produce a fake IminLine. EL3 traps the now-necessary I-Cache maintenance
    and performs the inner-shareable-TLBI that makes everything better.
    
    [0] https://developer.arm.com/docs/sden885747/latest/arm-neoverse-n1-mp050-software-developer-errata-notice
    
    * for-next/neoverse-n1-stale-instr:
      arm64: Silence clang warning on mismatched value/register sizes
      arm64: compat: Workaround Neoverse-N1 #1542419 for compat user-space
      arm64: Fake the IminLine size on systems affected by Neoverse-N1 #1542419
      arm64: errata: Hide CTR_EL0.DIC on systems affected by Neoverse-N1 #1542419

commit 05460849c3b51180d5ada3373d0449aea19075e4
Author: James Morse <james.morse@arm.com>
Date:   Thu Oct 17 18:42:58 2019 +0100

    arm64: errata: Hide CTR_EL0.DIC on systems affected by Neoverse-N1 #1542419
    
    Cores affected by Neoverse-N1 #1542419 could execute a stale instruction
    when a branch is updated to point to freshly generated instructions.
    
    To workaround this issue we need user-space to issue unnecessary
    icache maintenance that we can trap. Start by hiding CTR_EL0.DIC.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index f19fe4b9acc4..f05afaec18cd 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -52,7 +52,8 @@
 #define ARM64_HAS_IRQ_PRIO_MASKING		42
 #define ARM64_HAS_DCPODP			43
 #define ARM64_WORKAROUND_1463225		44
+#define ARM64_WORKAROUND_1542419		45
 
-#define ARM64_NCAPS				45
+#define ARM64_NCAPS				46
 
 #endif /* __ASM_CPUCAPS_H */

commit f75e2294a4415621b223150065c8d1e823896da5
Author: Marc Zyngier <maz@kernel.org>
Date:   Fri Nov 23 17:25:52 2018 +0000

    arm64: Add ARM64_WORKAROUND_1319367 for all A57 and A72 versions
    
    Rework the EL2 vector hardening that is only selected for A57 and A72
    so that the table can also be used for ARM64_WORKAROUND_1319367.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index f19fe4b9acc4..277e37b2a513 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -52,7 +52,8 @@
 #define ARM64_HAS_IRQ_PRIO_MASKING		42
 #define ARM64_HAS_DCPODP			43
 #define ARM64_WORKAROUND_1463225		44
+#define ARM64_WORKAROUND_1319367		45
 
-#define ARM64_NCAPS				45
+#define ARM64_NCAPS				46
 
 #endif /* __ASM_CPUCAPS_H */

commit 9405447ef79bc93101373e130f72e9e6cbf17dbb
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 9 16:22:24 2019 +0100

    arm64: Avoid Cavium TX2 erratum 219 when switching TTBR
    
    As a PRFM instruction racing against a TTBR update can have undesirable
    effects on TX2, NOP-out such PRFM on cores that are affected by
    the TX2-219 erratum.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index e81e0cbd728f..ac1dbca3d0cd 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -53,7 +53,8 @@
 #define ARM64_HAS_DCPODP			43
 #define ARM64_WORKAROUND_1463225		44
 #define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	45
+#define ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM	46
 
-#define ARM64_NCAPS				46
+#define ARM64_NCAPS				47
 
 #endif /* __ASM_CPUCAPS_H */

commit d3ec3a08fa700c8b46abb137dce4e2514a6f9668
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Feb 7 16:01:21 2019 +0000

    arm64: KVM: Trap VM ops when ARM64_WORKAROUND_CAVIUM_TX2_219_TVM is set
    
    In order to workaround the TX2-219 erratum, it is necessary to trap
    TTBRx_EL1 accesses to EL2. This is done by setting HCR_EL2.TVM on
    guest entry, which has the side effect of trapping all the other
    VM-related sysregs as well.
    
    To minimize the overhead, a fast path is used so that we don't
    have to go all the way back to the main sysreg handling code,
    unless the rest of the hypervisor expects to see these accesses.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index f19fe4b9acc4..e81e0cbd728f 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -52,7 +52,8 @@
 #define ARM64_HAS_IRQ_PRIO_MASKING		42
 #define ARM64_HAS_DCPODP			43
 #define ARM64_WORKAROUND_1463225		44
+#define ARM64_WORKAROUND_CAVIUM_TX2_219_TVM	45
 
-#define ARM64_NCAPS				45
+#define ARM64_NCAPS				46
 
 #endif /* __ASM_CPUCAPS_H */

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 33401ebc187c..f19fe4b9acc4 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -1,19 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * arch/arm64/include/asm/cpucaps.h
  *
  * Copyright (C) 2016 ARM Ltd.
- *
- * This program is free software: you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ASM_CPUCAPS_H
 #define __ASM_CPUCAPS_H

commit a5325089bd05a7b0259cc4038479d36308edbda2
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu May 23 11:24:50 2019 +0100

    arm64: Handle erratum 1418040 as a superset of erratum 1188873
    
    We already mitigate erratum 1188873 affecting Cortex-A76 and
    Neoverse-N1 r0p0 to r2p0. It turns out that revisions r0p0 to
    r3p1 of the same cores are affected by erratum 1418040, which
    has the same workaround as 1188873.
    
    Let's expand the range of affected revisions to match 1418040,
    and repaint all occurences of 1188873 to 1418040. Whilst we're
    there, do a bit of reformating in silicon-errata.txt and drop
    a now unnecessary dependency on ARM_ARCH_TIMER_OOL_WORKAROUND.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 73faee64e498..33401ebc187c 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -53,7 +53,7 @@
 #define ARM64_HAS_STAGE2_FWB			32
 #define ARM64_HAS_CRC32				33
 #define ARM64_SSBS				34
-#define ARM64_WORKAROUND_1188873		35
+#define ARM64_WORKAROUND_1418040		35
 #define ARM64_HAS_SB				36
 #define ARM64_WORKAROUND_1165522		37
 #define ARM64_HAS_ADDRESS_AUTH_ARCH		38

commit 969f5ea627570e91c9d54403287ee3ed657f58fe
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 29 13:03:57 2019 +0100

    arm64: errata: Add workaround for Cortex-A76 erratum #1463225
    
    Revisions of the Cortex-A76 CPU prior to r4p0 are affected by an erratum
    that can prevent interrupts from being taken when single-stepping.
    
    This patch implements a software workaround to prevent userspace from
    effectively being able to disable interrupts.
    
    Cc: <stable@vger.kernel.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index defdc67d9ab4..73faee64e498 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -62,7 +62,8 @@
 #define ARM64_HAS_GENERIC_AUTH_IMP_DEF		41
 #define ARM64_HAS_IRQ_PRIO_MASKING		42
 #define ARM64_HAS_DCPODP			43
+#define ARM64_WORKAROUND_1463225		44
 
-#define ARM64_NCAPS				44
+#define ARM64_NCAPS				45
 
 #endif /* __ASM_CPUCAPS_H */

commit b9585f53bcf1ada1fbac22c15fd723c65ef67ff1
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 10:52:45 2019 +0100

    arm64: Advertise ARM64_HAS_DCPODP cpu feature
    
    Advertise ARM64_HAS_DCPODP when both DC CVAP and DC CVADP are supported.
    
    Even though we don't use this feature now, we provide it for consistency
    with DCPOP and anticipate it being used in the future.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index f6a76e43f39e..defdc67d9ab4 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -61,7 +61,8 @@
 #define ARM64_HAS_GENERIC_AUTH_ARCH		40
 #define ARM64_HAS_GENERIC_AUTH_IMP_DEF		41
 #define ARM64_HAS_IRQ_PRIO_MASKING		42
+#define ARM64_HAS_DCPODP			43
 
-#define ARM64_NCAPS				43
+#define ARM64_NCAPS				44
 
 #endif /* __ASM_CPUCAPS_H */

commit b90d2b22afdc7ce150a9ee7a8d82378bcfc395a5
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:42 2019 +0000

    arm64: cpufeature: Add cpufeature for IRQ priority masking
    
    Add a cpufeature indicating whether a cpu supports masking interrupts
    by priority.
    
    The feature will be properly enabled in a later patch.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 82e9099834ae..f6a76e43f39e 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -60,7 +60,8 @@
 #define ARM64_HAS_ADDRESS_AUTH_IMP_DEF		39
 #define ARM64_HAS_GENERIC_AUTH_ARCH		40
 #define ARM64_HAS_GENERIC_AUTH_IMP_DEF		41
+#define ARM64_HAS_IRQ_PRIO_MASKING		42
 
-#define ARM64_NCAPS				42
+#define ARM64_NCAPS				43
 
 #endif /* __ASM_CPUCAPS_H */

commit a56005d3210500f8a166fcb83cbb5ac5d0f909e4
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 15:52:02 2018 +0000

    arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
    
    We can easily avoid defining the two meta-capabilities for the address
    and generic keys, so remove them and instead just check both of the
    architected and impdef capabilities when determining the level of system
    support.
    
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 803f388e81d4..82e9099834ae 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -58,11 +58,9 @@
 #define ARM64_WORKAROUND_1165522		37
 #define ARM64_HAS_ADDRESS_AUTH_ARCH		38
 #define ARM64_HAS_ADDRESS_AUTH_IMP_DEF		39
-#define ARM64_HAS_ADDRESS_AUTH			40
-#define ARM64_HAS_GENERIC_AUTH_ARCH		41
-#define ARM64_HAS_GENERIC_AUTH_IMP_DEF		42
-#define ARM64_HAS_GENERIC_AUTH			43
+#define ARM64_HAS_GENERIC_AUTH_ARCH		40
+#define ARM64_HAS_GENERIC_AUTH_IMP_DEF		41
 
-#define ARM64_NCAPS				44
+#define ARM64_NCAPS				42
 
 #endif /* __ASM_CPUCAPS_H */

commit 6984eb47d5c1a74bb44467ee4eee22d680f10785
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:24 2018 +0000

    arm64/cpufeature: detect pointer authentication
    
    So that we can dynamically handle the presence of pointer authentication
    functionality, wire up probing code in cpufeature.c.
    
    From ARMv8.3 onwards, ID_AA64ISAR1 is no longer entirely RES0, and now
    has four fields describing the presence of pointer authentication
    functionality:
    
    * APA - address authentication present, using an architected algorithm
    * API - address authentication present, using an IMP DEF algorithm
    * GPA - generic authentication present, using an architected algorithm
    * GPI - generic authentication present, using an IMP DEF algorithm
    
    This patch checks for both address and generic authentication,
    separately. It is assumed that if all CPUs support an IMP DEF algorithm,
    the same algorithm is used across all CPUs.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index a89f587d4842..803f388e81d4 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -56,7 +56,13 @@
 #define ARM64_WORKAROUND_1188873		35
 #define ARM64_HAS_SB				36
 #define ARM64_WORKAROUND_1165522		37
+#define ARM64_HAS_ADDRESS_AUTH_ARCH		38
+#define ARM64_HAS_ADDRESS_AUTH_IMP_DEF		39
+#define ARM64_HAS_ADDRESS_AUTH			40
+#define ARM64_HAS_GENERIC_AUTH_ARCH		41
+#define ARM64_HAS_GENERIC_AUTH_IMP_DEF		42
+#define ARM64_HAS_GENERIC_AUTH			43
 
-#define ARM64_NCAPS				38
+#define ARM64_NCAPS				44
 
 #endif /* __ASM_CPUCAPS_H */

commit bc84a2d106beab6000223b569c3bcb9ebf49d9ec
Merge: f357b3a7e17a a457b0f7f50d
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 18:53:03 2018 +0000

    Merge branch 'kvm/cortex-a76-erratum-1165522' into aarch64/for-next/core
    
    Pull in KVM workaround for A76 erratum #116522.
    
    Conflicts:
            arch/arm64/include/asm/cpucaps.h

commit 8b2cca9ade2c0f1d2ba94e39781e7306c918e544
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 6 17:31:23 2018 +0000

    arm64: KVM: Force VHE for systems affected by erratum 1165522
    
    In order to easily mitigate ARM erratum 1165522, we need to force
    affected CPUs to run in VHE mode if using KVM.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 6e2d254c09eb..62d8cd15fdf2 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -54,7 +54,8 @@
 #define ARM64_HAS_CRC32				33
 #define ARM64_SSBS				34
 #define ARM64_WORKAROUND_1188873		35
+#define ARM64_WORKAROUND_1165522		36
 
-#define ARM64_NCAPS				36
+#define ARM64_NCAPS				37
 
 #endif /* __ASM_CPUCAPS_H */

commit bd4fb6d270bc423a9a4098108784f7f9254c4e6d
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jun 14 11:21:34 2018 +0100

    arm64: Add support for SB barrier and patch in over DSB; ISB sequences
    
    We currently use a DSB; ISB sequence to inhibit speculation in set_fs().
    Whilst this works for current CPUs, future CPUs may implement a new SB
    barrier instruction which acts as an architected speculation barrier.
    
    On CPUs that support it, patch in an SB; NOP sequence over the DSB; ISB
    sequence and advertise the presence of the new instruction to userspace.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 6e2d254c09eb..b7f0709a21af 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -54,7 +54,8 @@
 #define ARM64_HAS_CRC32				33
 #define ARM64_SSBS				34
 #define ARM64_WORKAROUND_1188873		35
+#define ARM64_HAS_SB				36
 
-#define ARM64_NCAPS				36
+#define ARM64_NCAPS				37
 
 #endif /* __ASM_CPUCAPS_H */

commit 95b861a4a6d94f64d5242605569218160ebacdbe
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:34 2018 +0100

    arm64: arch_timer: Add workaround for ARM erratum 1188873
    
    When running on Cortex-A76, a timer access from an AArch32 EL0
    task may end up with a corrupted value or register. The workaround for
    this is to trap these accesses at EL1/EL2 and execute them there.
    
    This only affects versions r0p0, r1p0 and r2p0 of the CPU.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 6eb1b3fd0493..6e2d254c09eb 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -53,7 +53,8 @@
 #define ARM64_HAS_STAGE2_FWB			32
 #define ARM64_HAS_CRC32				33
 #define ARM64_SSBS				34
+#define ARM64_WORKAROUND_1188873		35
 
-#define ARM64_NCAPS				35
+#define ARM64_NCAPS				36
 
 #endif /* __ASM_CPUCAPS_H */

commit 880f7cc47265e7b195781dfa9a0cd62ef78304e3
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Sep 19 11:41:21 2018 +0100

    arm64: cpu_errata: Remove ARM64_MISMATCHED_CACHE_LINE_SIZE
    
    There's no need to treat mismatched cache-line sizes reported by CTR_EL0
    differently to any other mismatched fields that we treat as "STRICT" in
    the cpufeature code. In both cases we need to trap and emulate EL0
    accesses to the register, so drop ARM64_MISMATCHED_CACHE_LINE_SIZE and
    rely on ARM64_MISMATCHED_CACHE_TYPE instead.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: move ARM64_HAS_CNP in the empty cpucaps.h slot]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index c51d7e868f3f..6eb1b3fd0493 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -33,7 +33,7 @@
 #define ARM64_WORKAROUND_CAVIUM_27456		12
 #define ARM64_HAS_32BIT_EL0			13
 #define ARM64_HARDEN_EL2_VECTORS		14
-#define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
+#define ARM64_HAS_CNP				15
 #define ARM64_HAS_NO_FPSIMD			16
 #define ARM64_WORKAROUND_REPEAT_TLBI		17
 #define ARM64_WORKAROUND_QCOM_FALKOR_E1003	18
@@ -53,8 +53,7 @@
 #define ARM64_HAS_STAGE2_FWB			32
 #define ARM64_HAS_CRC32				33
 #define ARM64_SSBS				34
-#define ARM64_HAS_CNP				35
 
-#define ARM64_NCAPS				36
+#define ARM64_NCAPS				35
 
 #endif /* __ASM_CPUCAPS_H */

commit 5ffdfaedfa0aba3f5db0fbb8ed4f3192be2b39b8
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Tue Jul 31 14:08:56 2018 +0100

    arm64: mm: Support Common Not Private translations
    
    Common Not Private (CNP) is a feature of ARMv8.2 extension which
    allows translation table entries to be shared between different PEs in
    the same inner shareable domain, so the hardware can use this fact to
    optimise the caching of such entries in the TLB.
    
    CNP occupies one bit in TTBRx_ELy and VTTBR_EL2, which advertises to
    the hardware that the translation table entries pointed to by this
    TTBR are the same as every PE in the same inner shareable domain for
    which the equivalent TTBR also has CNP bit set. In case CNP bit is set
    but TTBR does not point at the same translation table entries for a
    given ASID and VMID, then the system is mis-configured, so the results
    of translations are UNPREDICTABLE.
    
    For kernel we postpone setting CNP till all cpus are up and rely on
    cpufeature framework to 1) patch the code which is sensitive to CNP
    and 2) update TTBR1_EL1 with CNP bit set. TTBR1_EL1 can be
    reprogrammed as result of hibernation or cpuidle (via __enable_mmu).
    For these two cases we restore CnP bit via __cpu_suspend_exit().
    
    There are a few cases we need to care of changes in TTBR0_EL1:
      - a switch to idmap
      - software emulated PAN
    
    we rule out latter via Kconfig options and for the former we make
    sure that CNP is set for non-zero ASIDs only.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    [catalin.marinas@arm.com: default y for CONFIG_ARM64_CNP]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 38eec9cf30f2..c51d7e868f3f 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -53,7 +53,8 @@
 #define ARM64_HAS_STAGE2_FWB			32
 #define ARM64_HAS_CRC32				33
 #define ARM64_SSBS				34
+#define ARM64_HAS_CNP				35
 
-#define ARM64_NCAPS				35
+#define ARM64_NCAPS				36
 
 #endif /* __ASM_CPUCAPS_H */

commit d71be2b6c0e19180b5f80a6d42039cc074a693a2
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jun 15 11:37:34 2018 +0100

    arm64: cpufeature: Detect SSBS and advertise to userspace
    
    Armv8.5 introduces a new PSTATE bit known as Speculative Store Bypass
    Safe (SSBS) which can be used as a mitigation against Spectre variant 4.
    
    Additionally, a CPU may provide instructions to manipulate PSTATE.SSBS
    directly, so that userspace can toggle the SSBS control without trapping
    to the kernel.
    
    This patch probes for the existence of SSBS and advertise the new instructions
    to userspace if they exist.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 9932aca9704b..38eec9cf30f2 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -52,7 +52,8 @@
 #define ARM64_MISMATCHED_CACHE_TYPE		31
 #define ARM64_HAS_STAGE2_FWB			32
 #define ARM64_HAS_CRC32				33
+#define ARM64_SSBS				34
 
-#define ARM64_NCAPS				34
+#define ARM64_NCAPS				35
 
 #endif /* __ASM_CPUCAPS_H */

commit 86d0dd34eafffbc76a81aba6ae2d71927d3835a8
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Aug 27 13:02:43 2018 +0200

    arm64: cpufeature: add feature for CRC32 instructions
    
    Add a CRC32 feature bit and wire it up to the CPU id register so we
    will be able to use alternatives patching for CRC32 operations.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index ae1f70450fb2..9932aca9704b 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -51,7 +51,8 @@
 #define ARM64_SSBD				30
 #define ARM64_MISMATCHED_CACHE_TYPE		31
 #define ARM64_HAS_STAGE2_FWB			32
+#define ARM64_HAS_CRC32				33
 
-#define ARM64_NCAPS				33
+#define ARM64_NCAPS				34
 
 #endif /* __ASM_CPUCAPS_H */

commit 631989303b06b8fdb15ec3b88aee2d25e80d4cec
Merge: ad1d69735878 976d34e2dab1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 22 14:07:56 2018 +0200

    Merge tag 'kvmarm-for-v4.19' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 4.19
    
    - Support for Group0 interrupts in guests
    - Cache management optimizations for ARMv8.4 systems
    - Userspace interface for RAS, allowing error retrival and injection
    - Fault path optimization
    - Emulated physical timer fixes
    - Random cleanups

commit e48d53a91f6e90873e21a5ca5e8c0d7a9f8936a4
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Apr 6 12:27:28 2018 +0100

    arm64: KVM: Add support for Stage-2 control of memory types and cacheability
    
    Up to ARMv8.3, the combinaison of Stage-1 and Stage-2 attributes
    results in the strongest attribute of the two stages.  This means
    that the hypervisor has to perform quite a lot of cache maintenance
    just in case the guest has some non-cacheable mappings around.
    
    ARMv8.4 solves this problem by offering a different mode (FWB) where
    Stage-2 has total control over the memory attribute (this is limited
    to systems where both I/O and instruction fetches are coherent with
    the dcache). This is achieved by having a different set of memory
    attributes in the page tables, and a new bit set in HCR_EL2.
    
    On such a system, we can then safely sidestep any form of dcache
    management.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 8a699c708fc9..ed84d6536830 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -49,7 +49,8 @@
 #define ARM64_HAS_CACHE_DIC			28
 #define ARM64_HW_DBM				29
 #define ARM64_SSBD				30
+#define ARM64_HAS_STAGE2_FWB			31
 
-#define ARM64_NCAPS				31
+#define ARM64_NCAPS				32
 
 #endif /* __ASM_CPUCAPS_H */

commit 314d53d297980676011e6fd83dac60db4a01dc70
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Jul 4 23:07:46 2018 +0100

    arm64: Handle mismatched cache type
    
    Track mismatches in the cache type register (CTR_EL0), other
    than the D/I min line sizes and trap user accesses if there are any.
    
    Fixes: be68a8aaf925 ("arm64: cpufeature: Fix CTR_EL0 field definitions")
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 8a699c708fc9..be3bf3d08916 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -49,7 +49,8 @@
 #define ARM64_HAS_CACHE_DIC			28
 #define ARM64_HW_DBM				29
 #define ARM64_SSBD				30
+#define ARM64_MISMATCHED_CACHE_TYPE		31
 
-#define ARM64_NCAPS				31
+#define ARM64_NCAPS				32
 
 #endif /* __ASM_CPUCAPS_H */

commit a725e3dda1813ed306734823ac4c65ca04e38500
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:08 2018 +0100

    arm64: Add ARCH_WORKAROUND_2 probing
    
    As for Spectre variant-2, we rely on SMCCC 1.1 to provide the
    discovery mechanism for detecting the SSBD mitigation.
    
    A new capability is also allocated for that purpose, and a
    config option.
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index bc51b72fafd4..8a699c708fc9 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -48,7 +48,8 @@
 #define ARM64_HAS_CACHE_IDC			27
 #define ARM64_HAS_CACHE_DIC			28
 #define ARM64_HW_DBM				29
+#define ARM64_SSBD				30
 
-#define ARM64_NCAPS				30
+#define ARM64_NCAPS				31
 
 #endif /* __ASM_CPUCAPS_H */

commit 4bc352ffb39e4eec253e70f8c076f2f48a6c1926
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Tue Apr 10 11:36:42 2018 +0100

    arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening
    
    The function SMCCC_ARCH_WORKAROUND_1 was introduced as part of SMC
    V1.1 Calling Convention to mitigate CVE-2017-5715. This patch uses
    the standard call SMCCC_ARCH_WORKAROUND_1 for Falkor chips instead
    of Silicon provider service ID 0xC2001700.
    
    Cc: <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    [maz: reworked errata framework integration]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index a311880feb0f..bc51b72fafd4 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -43,13 +43,12 @@
 #define ARM64_SVE				22
 #define ARM64_UNMAP_KERNEL_AT_EL0		23
 #define ARM64_HARDEN_BRANCH_PREDICTOR		24
-#define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
-#define ARM64_HAS_RAS_EXTN			26
-#define ARM64_WORKAROUND_843419			27
-#define ARM64_HAS_CACHE_IDC			28
-#define ARM64_HAS_CACHE_DIC			29
-#define ARM64_HW_DBM				30
+#define ARM64_HAS_RAS_EXTN			25
+#define ARM64_WORKAROUND_843419			26
+#define ARM64_HAS_CACHE_IDC			27
+#define ARM64_HAS_CACHE_DIC			28
+#define ARM64_HW_DBM				29
 
-#define ARM64_NCAPS				31
+#define ARM64_NCAPS				30
 
 #endif /* __ASM_CPUCAPS_H */

commit d8312a3f61024352f1c7cb967571fd53631b0d6c
Merge: e9092d0d9796 e01bca2fc698
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 9 11:42:31 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - VHE optimizations
    
       - EL2 address space randomization
    
       - speculative execution mitigations ("variant 3a", aka execution past
         invalid privilege register access)
    
       - bugfixes and cleanups
    
      PPC:
       - improvements for the radix page fault handler for HV KVM on POWER9
    
      s390:
       - more kvm stat counters
    
       - virtio gpu plumbing
    
       - documentation
    
       - facilities improvements
    
      x86:
       - support for VMware magic I/O port and pseudo-PMCs
    
       - AMD pause loop exiting
    
       - support for AMD core performance extensions
    
       - support for synchronous register access
    
       - expose nVMX capabilities to userspace
    
       - support for Hyper-V signaling via eventfd
    
       - use Enlightened VMCS when running on Hyper-V
    
       - allow userspace to disable MWAIT/HLT/PAUSE vmexits
    
       - usual roundup of optimizations and nested virtualization bugfixes
    
      Generic:
       - API selftest infrastructure (though the only tests are for x86 as
         of now)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (174 commits)
      kvm: x86: fix a prototype warning
      kvm: selftests: add sync_regs_test
      kvm: selftests: add API testing infrastructure
      kvm: x86: fix a compile warning
      KVM: X86: Add Force Emulation Prefix for "emulate the next instruction"
      KVM: X86: Introduce handle_ud()
      KVM: vmx: unify adjacent #ifdefs
      x86: kvm: hide the unused 'cpu' variable
      KVM: VMX: remove bogus WARN_ON in handle_ept_misconfig
      Revert "KVM: X86: Fix SMRAM accessing even if VM is shutdown"
      kvm: Add emulation for movups/movupd
      KVM: VMX: raise internal error for exception during invalid protected mode state
      KVM: nVMX: Optimization: Dont set KVM_REQ_EVENT when VMExit with nested_run_pending
      KVM: nVMX: Require immediate-exit when event reinjected to L2 and L1 event pending
      KVM: x86: Fix misleading comments on handling pending exceptions
      KVM: x86: Rename interrupt.pending to interrupt.injected
      KVM: VMX: No need to clear pending NMI/interrupt on inject realmode interrupt
      x86/kvm: use Enlightened VMCS when running on Hyper-V
      x86/hyper-v: detect nested features
      x86/hyper-v: define struct hv_enlightened_vmcs and clean field bits
      ...

commit adc91ab7854195f107c137aa197ddfe8b82a2331
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Mar 28 11:59:13 2018 +0100

    Revert "arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening"
    
    Creates far too many conflicts with arm64/for-next/core, to be
    resent post -rc1.
    
    This reverts commit f9f5dc19509bbef6f5e675346f1a7d7b846bdb12.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index c7dfe68fbcc7..d4cc54ed0656 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -43,7 +43,7 @@
 #define ARM64_SVE				22
 #define ARM64_UNMAP_KERNEL_AT_EL0		23
 #define ARM64_HARDEN_BRANCH_PREDICTOR		24
-/* #define ARM64_UNUSED_CAP_TO_BE_REMOVED	25 */
+#define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
 #define ARM64_HAS_RAS_EXTN			26
 
 #define ARM64_NCAPS				27

commit 05abb595bbaccc9c4290bee62086d0eeea9f0f32
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:48 2018 +0100

    arm64: Delay enabling hardware DBM feature
    
    We enable hardware DBM bit in a capable CPU, very early in the
    boot via __cpu_setup. This doesn't give us a flexibility of
    optionally disable the feature, as the clearing the bit
    is a bit costly as the TLB can cache the settings. Instead,
    we delay enabling the feature until the CPU is brought up
    into the kernel. We use the feature capability mechanism
    to handle it.
    
    The hardware DBM is a non-conflicting feature. i.e, the kernel
    can safely run with a mix of CPUs with some using the feature
    and the others don't. So, it is safe for a late CPU to have
    this capability and enable it, even if the active CPUs don't.
    
    To get this handled properly by the infrastructure, we
    unconditionally set the capability and only enable it
    on CPUs which really have the feature. Also, we print the
    feature detection from the "matches" call back to make sure
    we don't mislead the user when none of the CPUs could use the
    feature.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index ff9fb3aba17b..21bb624e0a7a 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -48,7 +48,8 @@
 #define ARM64_WORKAROUND_843419			27
 #define ARM64_HAS_CACHE_IDC			28
 #define ARM64_HAS_CACHE_DIC			29
+#define ARM64_HW_DBM				30
 
-#define ARM64_NCAPS				30
+#define ARM64_NCAPS				31
 
 #endif /* __ASM_CPUCAPS_H */

commit f9f5dc19509bbef6f5e675346f1a7d7b846bdb12
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Mon Mar 5 11:06:43 2018 -0600

    arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening
    
    The function SMCCC_ARCH_WORKAROUND_1 was introduced as part of SMC
    V1.1 Calling Convention to mitigate CVE-2017-5715. This patch uses
    the standard call SMCCC_ARCH_WORKAROUND_1 for Falkor chips instead
    of Silicon provider service ID 0xC2001700.
    
    Cc: <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index d4cc54ed0656..c7dfe68fbcc7 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -43,7 +43,7 @@
 #define ARM64_SVE				22
 #define ARM64_UNMAP_KERNEL_AT_EL0		23
 #define ARM64_HARDEN_BRANCH_PREDICTOR		24
-#define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
+/* #define ARM64_UNUSED_CAP_TO_BE_REMOVED	25 */
 #define ARM64_HAS_RAS_EXTN			26
 
 #define ARM64_NCAPS				27

commit 71dcb8be6d29cffff3f4a4463232f38786e97797
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 27 17:38:08 2018 +0000

    arm64: KVM: Allow far branches from vector slots to the main vectors
    
    So far, the branch from the vector slots to the main vectors can at
    most be 4GB from the main vectors (the reach of ADRP), and this
    distance is known at compile time. If we were to remap the slots
    to an unrelated VA, things would break badly.
    
    A way to achieve VA independence would be to load the absolute
    address of the vectors (__kvm_hyp_vector), either using a constant
    pool or a series of movs, followed by an indirect branch.
    
    This patches implements the latter solution, using another instance
    of a patching callback. Note that since we have to save a register
    pair on the stack, we branch to the *second* instruction in the
    vectors in order to compensate for it. This also results in having
    to adjust this balance in the invalid vector entry point.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 76a43a17449a..d4cc54ed0656 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -32,7 +32,7 @@
 #define ARM64_HAS_VIRT_HOST_EXTN		11
 #define ARM64_WORKAROUND_CAVIUM_27456		12
 #define ARM64_HAS_32BIT_EL0			13
-/* #define ARM64_UNALLOCATED_ENTRY			14 */
+#define ARM64_HARDEN_EL2_VECTORS		14
 #define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
 #define ARM64_HAS_NO_FPSIMD			16
 #define ARM64_WORKAROUND_REPEAT_TLBI		17

commit a1efdff442ec7bd1ad2dfa1f96c4c373f019ad46
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sun Dec 3 17:42:37 2017 +0000

    arm64: cpufeatures: Drop the ARM64_HYP_OFFSET_LOW feature flag
    
    Now that we can dynamically compute the kernek/hyp VA mask, there
    is no need for a feature flag to trigger the alternative patching.
    Let's drop the flag and everything that depends on it.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index bb263820de13..76a43a17449a 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -32,7 +32,7 @@
 #define ARM64_HAS_VIRT_HOST_EXTN		11
 #define ARM64_WORKAROUND_CAVIUM_27456		12
 #define ARM64_HAS_32BIT_EL0			13
-#define ARM64_HYP_OFFSET_LOW			14
+/* #define ARM64_UNALLOCATED_ENTRY			14 */
 #define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
 #define ARM64_HAS_NO_FPSIMD			16
 #define ARM64_WORKAROUND_REPEAT_TLBI		17

commit 6ae4b6e0578886eb36cedbf99f04031d93f9e315
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Wed Mar 7 09:00:08 2018 -0600

    arm64: Add support for new control bits CTR_EL0.DIC and CTR_EL0.IDC
    
    The DCache clean & ICache invalidation requirements for instructions
    to be data coherence are discoverable through new fields in CTR_EL0.
    The following two control bits DIC and IDC were defined for this
    purpose. No need to perform point of unification cache maintenance
    operations from software on systems where CPU caches are transparent.
    
    This patch optimize the three functions __flush_cache_user_range(),
    clean_dcache_area_pou() and invalidate_icache_range() if the hardware
    reports CTR_EL0.IDC and/or CTR_EL0.IDC. Basically it skips the two
    instructions 'DC CVAU' and 'IC IVAU', and the associated loop logic
    in order to avoid the unnecessary overhead.
    
    CTR_EL0.DIC: Instruction cache invalidation requirements for
     instruction to data coherence. The meaning of this bit[29].
      0: Instruction cache invalidation to the point of unification
         is required for instruction to data coherence.
      1: Instruction cache cleaning to the point of unification is
          not required for instruction to data coherence.
    
    CTR_EL0.IDC: Data cache clean requirements for instruction to data
     coherence. The meaning of this bit[28].
      0: Data cache clean to the point of unification is required for
         instruction to data coherence, unless CLIDR_EL1.LoC == 0b000
         or (CLIDR_EL1.LoUIS == 0b000 && CLIDR_EL1.LoUU == 0b000).
      1: Data cache clean to the point of unification is not required
         for instruction to data coherence.
    
    Co-authored-by: Philip Elcan <pelcan@codeaurora.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 39134c46bb13..ff9fb3aba17b 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -46,7 +46,9 @@
 #define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
 #define ARM64_HAS_RAS_EXTN			26
 #define ARM64_WORKAROUND_843419			27
+#define ARM64_HAS_CACHE_IDC			28
+#define ARM64_HAS_CACHE_DIC			29
 
-#define ARM64_NCAPS				28
+#define ARM64_NCAPS				30
 
 #endif /* __ASM_CPUCAPS_H */

commit ca79acca273630935f2cfdfdf3fc7425ff51ce1c
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 6 17:15:35 2018 +0000

    arm64/kernel: enable A53 erratum #8434319 handling at runtime
    
    Omit patching of ADRP instruction at module load time if the current
    CPUs are not susceptible to the erratum.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [will: Drop duplicate initialisation of .def_scope field]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index bb263820de13..39134c46bb13 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -45,7 +45,8 @@
 #define ARM64_HARDEN_BRANCH_PREDICTOR		24
 #define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
 #define ARM64_HAS_RAS_EXTN			26
+#define ARM64_WORKAROUND_843419			27
 
-#define ARM64_NCAPS				27
+#define ARM64_NCAPS				28
 
 #endif /* __ASM_CPUCAPS_H */

commit 64c02720ea3598bf5143b672274d923a941b8053
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Mon Jan 15 19:38:56 2018 +0000

    arm64: cpufeature: Detect CPU RAS Extentions
    
    ARM's v8.2 Extentions add support for Reliability, Availability and
    Serviceability (RAS). On CPUs with these extensions system software
    can use additional barriers to isolate errors and determine if faults
    are pending. Add cpufeature detection.
    
    Platform level RAS support may require additional firmware support.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    [Rebased added config option, reworded commit message]
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 7049b4802587..bb263820de13 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -44,7 +44,8 @@
 #define ARM64_UNMAP_KERNEL_AT_EL0		23
 #define ARM64_HARDEN_BRANCH_PREDICTOR		24
 #define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
+#define ARM64_HAS_RAS_EXTN			26
 
-#define ARM64_NCAPS				26
+#define ARM64_NCAPS				27
 
 #endif /* __ASM_CPUCAPS_H */

commit ec82b567a74fbdffdf418d4bb381d55f6a9096af
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Fri Jan 5 14:28:59 2018 -0600

    arm64: Implement branch predictor hardening for Falkor
    
    Falkor is susceptible to branch predictor aliasing and can
    theoretically be attacked by malicious code. This patch
    implements a mitigation for these attacks, preventing any
    malicious entries from affecting other victim contexts.
    
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    [will: fix label name when !CONFIG_KVM and remove references to MIDR_FALKOR]
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 51616e77fe6b..7049b4802587 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -43,7 +43,8 @@
 #define ARM64_SVE				22
 #define ARM64_UNMAP_KERNEL_AT_EL0		23
 #define ARM64_HARDEN_BRANCH_PREDICTOR		24
+#define ARM64_HARDEN_BP_POST_GUEST_EXIT		25
 
-#define ARM64_NCAPS				25
+#define ARM64_NCAPS				26
 
 #endif /* __ASM_CPUCAPS_H */

commit 0f15adbb2861ce6f75ccfc5a92b19eae0ef327d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 3 11:17:58 2018 +0000

    arm64: Add skeleton to harden the branch predictor against aliasing attacks
    
    Aliasing attacks against CPU branch predictors can allow an attacker to
    redirect speculative control flow on some CPUs and potentially divulge
    information from one context to another.
    
    This patch adds initial skeleton code behind a new Kconfig option to
    enable implementation-specific mitigations against these attacks for
    CPUs that are affected.
    
    Co-developed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index b4537ffd1018..51616e77fe6b 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -42,7 +42,8 @@
 #define ARM64_HAS_DCPOP				21
 #define ARM64_SVE				22
 #define ARM64_UNMAP_KERNEL_AT_EL0		23
+#define ARM64_HARDEN_BRANCH_PREDICTOR		24
 
-#define ARM64_NCAPS				24
+#define ARM64_NCAPS				25
 
 #endif /* __ASM_CPUCAPS_H */

commit ea1e3de85e94d711f63437c04624aa0e8de5c8b3
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 14 14:38:19 2017 +0000

    arm64: entry: Add fake CPU feature for unmapping the kernel at EL0
    
    Allow explicit disabling of the entry trampoline on the kernel command
    line (kpti=off) by adding a fake CPU feature (ARM64_UNMAP_KERNEL_AT_EL0)
    that can be used to toggle the alternative sequences in our entry code and
    avoid use of the trampoline altogether if desired. This also allows us to
    make use of a static key in arm64_kernel_unmapped_at_el0().
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 2ff7c5e8efab..b4537ffd1018 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -41,7 +41,8 @@
 #define ARM64_WORKAROUND_CAVIUM_30115		20
 #define ARM64_HAS_DCPOP				21
 #define ARM64_SVE				22
+#define ARM64_UNMAP_KERNEL_AT_EL0		23
 
-#define ARM64_NCAPS				23
+#define ARM64_NCAPS				24
 
 #endif /* __ASM_CPUCAPS_H */

commit 43994d824e8443263dc98b151e6326bf677be52e
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:19 2017 +0000

    arm64/sve: Detect SVE and activate runtime support
    
    This patch enables detection of hardware SVE support via the
    cpufeatures framework, and reports its presence to the kernel and
    userspace via the new ARM64_SVE cpucap and HWCAP_SVE hwcap
    respectively.
    
    Userspace can also detect SVE using ID_AA64PFR0_EL1, using the
    cpufeatures MRS emulation.
    
    When running on hardware that supports SVE, this enables runtime
    kernel support for SVE, and allows user tasks to execute SVE
    instructions and make of the of the SVE-specific user/kernel
    interface extensions implemented by this series.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 8da621627d7c..2ff7c5e8efab 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -40,7 +40,8 @@
 #define ARM64_WORKAROUND_858921			19
 #define ARM64_WORKAROUND_CAVIUM_30115		20
 #define ARM64_HAS_DCPOP				21
+#define ARM64_SVE				22
 
-#define ARM64_NCAPS				22
+#define ARM64_NCAPS				23
 
 #endif /* __ASM_CPUCAPS_H */

commit d50e071fdaa33c1b399c764c44fa1ce879881185
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:42 2017 +0100

    arm64: Implement pmem API support
    
    Add a clean-to-point-of-persistence cache maintenance helper, and wire
    up the basic architectural support for the pmem driver based on it.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    [catalin.marinas@arm.com: move arch_*_pmem() functions to arch/arm64/mm/flush.c]
    [catalin.marinas@arm.com: change dmb(sy) to dmb(osh)]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 8d2272c6822c..8da621627d7c 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -39,7 +39,8 @@
 #define ARM64_WORKAROUND_QCOM_FALKOR_E1003	18
 #define ARM64_WORKAROUND_858921			19
 #define ARM64_WORKAROUND_CAVIUM_30115		20
+#define ARM64_HAS_DCPOP				21
 
-#define ARM64_NCAPS				21
+#define ARM64_NCAPS				22
 
 #endif /* __ASM_CPUCAPS_H */

commit 690a341577f9adf2c275ababe0dcefe91898bbf0
Author: David Daney <david.daney@cavium.com>
Date:   Fri Jun 9 12:49:48 2017 +0100

    arm64: Add workaround for Cavium Thunder erratum 30115
    
    Some Cavium Thunder CPUs suffer a problem where a KVM guest may
    inadvertently cause the host kernel to quit receiving interrupts.
    
    Use the Group-0/1 trapping in order to deal with it.
    
    [maz]: Adapted patch to the Group-0/1 trapping, reworked commit log
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index b3aab8a17868..8d2272c6822c 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -38,7 +38,8 @@
 #define ARM64_WORKAROUND_REPEAT_TLBI		17
 #define ARM64_WORKAROUND_QCOM_FALKOR_E1003	18
 #define ARM64_WORKAROUND_858921			19
+#define ARM64_WORKAROUND_CAVIUM_30115		20
 
-#define ARM64_NCAPS				20
+#define ARM64_NCAPS				21
 
 #endif /* __ASM_CPUCAPS_H */

commit eeb1efbcb83c0cfe6d567abbacd675bbddf3d658
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Mar 20 17:18:06 2017 +0000

    arm64: cpu_errata: Add capability to advertise Cortex-A73 erratum 858921
    
    In order to work around Cortex-A73 erratum 858921 in a subsequent
    patch, add the required capability that advertise the erratum.
    
    As the configuration option it depends on is not present yet,
    this has no immediate effect.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index fb78a5d3b60b..b3aab8a17868 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -37,7 +37,8 @@
 #define ARM64_HAS_NO_FPSIMD			16
 #define ARM64_WORKAROUND_REPEAT_TLBI		17
 #define ARM64_WORKAROUND_QCOM_FALKOR_E1003	18
+#define ARM64_WORKAROUND_858921			19
 
-#define ARM64_NCAPS				19
+#define ARM64_NCAPS				20
 
 #endif /* __ASM_CPUCAPS_H */

commit 38fd94b0275c91071157a03cc27676909b23dcde
Author: Christopher Covington <cov@codeaurora.org>
Date:   Wed Feb 8 15:08:37 2017 -0500

    arm64: Work around Falkor erratum 1003
    
    The Qualcomm Datacenter Technologies Falkor v1 CPU may allocate TLB entries
    using an incorrect ASID when TTBRx_EL1 is being updated. When the erratum
    is triggered, page table entries using the new translation table base
    address (BADDR) will be allocated into the TLB using the old ASID. All
    circumstances leading to the incorrect ASID being cached in the TLB arise
    when software writes TTBRx_EL1[ASID] and TTBRx_EL1[BADDR], a memory
    operation is in the process of performing a translation using the specific
    TTBRx_EL1 being written, and the memory operation uses a translation table
    descriptor designated as non-global. EL2 and EL3 code changing the EL1&0
    ASID is not subject to this erratum because hardware is prohibited from
    performing translations from an out-of-context translation regime.
    
    Consider the following pseudo code.
    
      write new BADDR and ASID values to TTBRx_EL1
    
    Replacing the above sequence with the one below will ensure that no TLB
    entries with an incorrect ASID are used by software.
    
      write reserved value to TTBRx_EL1[ASID]
      ISB
      write new value to TTBRx_EL1[BADDR]
      ISB
      write new value to TTBRx_EL1[ASID]
      ISB
    
    When the above sequence is used, page table entries using the new BADDR
    value may still be incorrectly allocated into the TLB using the reserved
    ASID. Yet this will not reduce functionality, since TLB entries incorrectly
    tagged with the reserved ASID will never be hit by a later instruction.
    
    Based on work by Shanker Donthineni <shankerd@codeaurora.org>
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Christopher Covington <cov@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index d1207ac696ac..fb78a5d3b60b 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -36,7 +36,8 @@
 #define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
 #define ARM64_HAS_NO_FPSIMD			16
 #define ARM64_WORKAROUND_REPEAT_TLBI		17
+#define ARM64_WORKAROUND_QCOM_FALKOR_E1003	18
 
-#define ARM64_NCAPS				18
+#define ARM64_NCAPS				19
 
 #endif /* __ASM_CPUCAPS_H */

commit d9ff80f83ecbf4cbdf56d32d01c312498e4fb1cd
Author: Christopher Covington <cov@codeaurora.org>
Date:   Tue Jan 31 12:50:19 2017 -0500

    arm64: Work around Falkor erratum 1009
    
    During a TLB invalidate sequence targeting the inner shareable domain,
    Falkor may prematurely complete the DSB before all loads and stores using
    the old translation are observed. Instruction fetches are not subject to
    the conditions of this erratum. If the original code sequence includes
    multiple TLB invalidate instructions followed by a single DSB, onle one of
    the TLB instructions needs to be repeated to work around this erratum.
    While the erratum only applies to cases in which the TLBI specifies the
    inner-shareable domain (*IS form of TLBI) and the DSB is ISH form or
    stronger (OSH, SYS), this changes applies the workaround overabundantly--
    to local TLBI, DSB NSH sequences as well--for simplicity.
    
    Based on work by Shanker Donthineni <shankerd@codeaurora.org>
    
    Signed-off-by: Christopher Covington <cov@codeaurora.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
index 4174f09678c4..d1207ac696ac 100644
--- a/arch/arm64/include/asm/cpucaps.h
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -35,7 +35,8 @@
 #define ARM64_HYP_OFFSET_LOW			14
 #define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
 #define ARM64_HAS_NO_FPSIMD			16
+#define ARM64_WORKAROUND_REPEAT_TLBI		17
 
-#define ARM64_NCAPS				17
+#define ARM64_NCAPS				18
 
 #endif /* __ASM_CPUCAPS_H */

commit f4000cd99750065d5177555c0a805c97174d1b9f
Merge: 2ec4584eb89b 75037120e62b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 13 16:39:21 2016 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - struct thread_info moved off-stack (also touching
       include/linux/thread_info.h and include/linux/restart_block.h)
    
     - cpus_have_cap() reworked to avoid __builtin_constant_p() for static
       key use (also touching drivers/irqchip/irq-gic-v3.c)
    
     - uprobes support (currently only for native 64-bit tasks)
    
     - Emulation of kernel Privileged Access Never (PAN) using TTBR0_EL1
       switching to a reserved page table
    
     - CPU capacity information passing via DT or sysfs (used by the
       scheduler)
    
     - support for systems without FP/SIMD (IOW, kernel avoids touching
       these registers; there is no soft-float ABI, nor kernel emulation for
       AArch64 FP/SIMD)
    
     - handling of hardware watchpoint with unaligned addresses, varied
       lengths and offsets from base
    
     - use of the page table contiguous hint for kernel mappings
    
     - hugetlb fixes for sizes involving the contiguous hint
    
     - remove unnecessary I-cache invalidation in flush_cache_range()
    
     - CNTHCTL_EL2 access fix for CPUs with VHE support (ARMv8.1)
    
     - boot-time checks for writable+executable kernel mappings
    
     - simplify asm/opcodes.h and avoid including the 32-bit ARM counterpart
       and make the arm64 kernel headers self-consistent (Xen headers patch
       merged separately)
    
     - Workaround for broken .inst support in certain binutils versions
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (60 commits)
      arm64: Disable PAN on uaccess_enable()
      arm64: Work around broken .inst when defective gas is detected
      arm64: Add detection code for broken .inst support in binutils
      arm64: Remove reference to asm/opcodes.h
      arm64: Get rid of asm/opcodes.h
      arm64: smp: Prevent raw_smp_processor_id() recursion
      arm64: head.S: Fix CNTHCTL_EL2 access on VHE system
      arm64: Remove I-cache invalidation from flush_cache_range()
      arm64: Enable HIBERNATION in defconfig
      arm64: Enable CONFIG_ARM64_SW_TTBR0_PAN
      arm64: xen: Enable user access before a privcmd hvc call
      arm64: Handle faults caused by inadvertent user access with PAN enabled
      arm64: Disable TTBR0_EL1 during normal kernel execution
      arm64: Introduce uaccess_{disable,enable} functionality based on TTBR0_EL1
      arm64: Factor out TTBR0_EL1 post-update workaround into a specific asm macro
      arm64: Factor out PAN enabling/disabling into separate uaccess_* macros
      arm64: Update the synchronous external abort fault description
      selftests: arm64: add test for unaligned/inexact watchpoint handling
      arm64: Allow hw watchpoint of length 3,5,6 and 7
      arm64: hw_breakpoint: Handle inexact watchpoint addresses
      ...

commit 272d01bd790fdf3f1b16372fe28136e27756756f
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 3 18:34:34 2016 +0000

    arm64: Fix circular include of asm/lse.h through linux/jump_label.h
    
    Commit efd9e03facd0 ("arm64: Use static keys for CPU features")
    introduced support for static keys in asm/cpufeature.h, including
    linux/jump_label.h. When CC_HAVE_ASM_GOTO is not defined, this causes a
    circular dependency via linux/atomic.h, asm/lse.h and asm/cpufeature.h.
    
    This patch moves the capability macros out out of asm/cpufeature.h into
    a separate asm/cpucaps.h and modifies some of the #includes accordingly.
    
    Fixes: efd9e03facd0 ("arm64: Use static keys for CPU features")
    Reported-by: Artem Savkov <asavkov@redhat.com>
    Tested-by: Artem Savkov <asavkov@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/include/asm/cpucaps.h b/arch/arm64/include/asm/cpucaps.h
new file mode 100644
index 000000000000..87b446535185
--- /dev/null
+++ b/arch/arm64/include/asm/cpucaps.h
@@ -0,0 +1,40 @@
+/*
+ * arch/arm64/include/asm/cpucaps.h
+ *
+ * Copyright (C) 2016 ARM Ltd.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_CPUCAPS_H
+#define __ASM_CPUCAPS_H
+
+#define ARM64_WORKAROUND_CLEAN_CACHE		0
+#define ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE	1
+#define ARM64_WORKAROUND_845719			2
+#define ARM64_HAS_SYSREG_GIC_CPUIF		3
+#define ARM64_HAS_PAN				4
+#define ARM64_HAS_LSE_ATOMICS			5
+#define ARM64_WORKAROUND_CAVIUM_23154		6
+#define ARM64_WORKAROUND_834220			7
+#define ARM64_HAS_NO_HW_PREFETCH		8
+#define ARM64_HAS_UAO				9
+#define ARM64_ALT_PAN_NOT_UAO			10
+#define ARM64_HAS_VIRT_HOST_EXTN		11
+#define ARM64_WORKAROUND_CAVIUM_27456		12
+#define ARM64_HAS_32BIT_EL0			13
+#define ARM64_HYP_OFFSET_LOW			14
+#define ARM64_MISMATCHED_CACHE_LINE_SIZE	15
+
+#define ARM64_NCAPS				16
+
+#endif /* __ASM_CPUCAPS_H */
