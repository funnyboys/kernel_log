commit 2d21889f8b5c50f65f5162bc972b0b1626b97be2
Author: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date:   Wed Jun 24 13:22:54 2020 +0200

    arm64: Don't insert a BTI instruction at inner labels
    
    Some ftrace features are broken since commit 714a8d02ca4d ("arm64: asm:
    Override SYM_FUNC_START when building the kernel with BTI"). For example
    the function_graph tracer:
    
    $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
    [   36.107016] WARNING: CPU: 0 PID: 115 at kernel/trace/ftrace.c:2691 ftrace_modify_all_code+0xc8/0x14c
    
    When ftrace_modify_graph_caller() attempts to write a branch at
    ftrace_graph_call, it finds the "BTI J" instruction inserted by
    SYM_INNER_LABEL() instead of a NOP, and aborts.
    
    It turns out we don't currently need the BTI landing pads inserted by
    SYM_INNER_LABEL:
    
    * ftrace_call and ftrace_graph_call are only used for runtime patching
      of the active tracer. The patched code is not reached from a branch.
    * install_el2_stub is reached from a CBZ instruction, which doesn't
      change PSTATE.BTYPE.
    * __guest_exit is reached from B instructions in the hyp-entry vectors,
      which aren't subject to BTI checks either.
    
    Remove the BTI annotation from SYM_INNER_LABEL.
    
    Fixes: 714a8d02ca4d ("arm64: asm: Override SYM_FUNC_START when building the kernel with BTI")
    Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
    Reviewed-by: Mark Brown <broonie@kernel.org>
    Link: https://lore.kernel.org/r/20200624112253.1602786-1-jean-philippe@linaro.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/linkage.h b/arch/arm64/include/asm/linkage.h
index 81fefd2a1d02..ba89a9af820a 100644
--- a/arch/arm64/include/asm/linkage.h
+++ b/arch/arm64/include/asm/linkage.h
@@ -12,7 +12,6 @@
  * instead.
  */
 #define BTI_C hint 34 ;
-#define BTI_J hint 36 ;
 
 /*
  * When using in-kernel BTI we need to ensure that PCS-conformant assembly
@@ -43,11 +42,6 @@
 	SYM_START(name, SYM_L_WEAK, SYM_A_NONE)		\
 	BTI_C
 
-#define SYM_INNER_LABEL(name, linkage)			\
-	.type name SYM_T_NONE ASM_NL			\
-	SYM_ENTRY(name, linkage, SYM_A_NONE)		\
-	BTI_J
-
 #endif
 
 /*

commit 9a964285572b5a3ea268bd744bb6837aecf09640
Author: Will Deacon <will@kernel.org>
Date:   Tue May 19 12:38:33 2020 +0100

    arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
    
    For better or worse, GDB relies on the exact instruction sequence in the
    VDSO sigreturn trampoline in order to unwind from signals correctly.
    Commit c91db232da48 ("arm64: vdso: Convert to modern assembler annotations")
    unfortunately added a BTI C instruction to the start of __kernel_rt_sigreturn,
    which breaks this check. Thankfully, it's also not required, since the
    trampoline is called from a RET instruction when returning from the signal
    handler
    
    Remove the unnecessary BTI C instruction from __kernel_rt_sigreturn,
    and do the same for the 32-bit VDSO as well for good measure.
    
    Cc: Daniel Kiss <daniel.kiss@arm.com>
    Cc: Tamas Zsoldos <tamas.zsoldos@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Reviewed-by: Mark Brown <broonie@kernel.org>
    Fixes: c91db232da48 ("arm64: vdso: Convert to modern assembler annotations")
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/linkage.h b/arch/arm64/include/asm/linkage.h
index b5a7998a6b2a..81fefd2a1d02 100644
--- a/arch/arm64/include/asm/linkage.h
+++ b/arch/arm64/include/asm/linkage.h
@@ -15,9 +15,9 @@
 #define BTI_J hint 36 ;
 
 /*
- * When using in-kernel BTI we need to ensure that assembly functions
- * have suitable annotations.  Override SYM_FUNC_START to insert a BTI
- * landing pad at the start of everything.
+ * When using in-kernel BTI we need to ensure that PCS-conformant assembly
+ * functions have suitable annotations.  Override SYM_FUNC_START to insert
+ * a BTI landing pad at the start of everything.
  */
 #define SYM_FUNC_START(name)				\
 	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)	\

commit 714a8d02ca4da1479bf0b778fc1951dc88515c3d
Author: Mark Brown <broonie@kernel.org>
Date:   Wed May 6 20:51:30 2020 +0100

    arm64: asm: Override SYM_FUNC_START when building the kernel with BTI
    
    When the kernel is built for BTI override SYM_FUNC_START and related macros
    to add a BTI landing pad to the start of all global functions, ensuring that
    they are BTI safe. The ; at the end of the BTI_x macros is for the
    benefit of the macro-generated functions in xen-hypercall.S.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: https://lore.kernel.org/r/20200506195138.22086-4-broonie@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/linkage.h b/arch/arm64/include/asm/linkage.h
index ebee3113a62f..b5a7998a6b2a 100644
--- a/arch/arm64/include/asm/linkage.h
+++ b/arch/arm64/include/asm/linkage.h
@@ -4,6 +4,52 @@
 #define __ALIGN		.align 2
 #define __ALIGN_STR	".align 2"
 
+#if defined(CONFIG_ARM64_BTI_KERNEL) && defined(__aarch64__)
+
+/*
+ * Since current versions of gas reject the BTI instruction unless we
+ * set the architecture version to v8.5 we use the hint instruction
+ * instead.
+ */
+#define BTI_C hint 34 ;
+#define BTI_J hint 36 ;
+
+/*
+ * When using in-kernel BTI we need to ensure that assembly functions
+ * have suitable annotations.  Override SYM_FUNC_START to insert a BTI
+ * landing pad at the start of everything.
+ */
+#define SYM_FUNC_START(name)				\
+	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)	\
+	BTI_C
+
+#define SYM_FUNC_START_NOALIGN(name)			\
+	SYM_START(name, SYM_L_GLOBAL, SYM_A_NONE)	\
+	BTI_C
+
+#define SYM_FUNC_START_LOCAL(name)			\
+	SYM_START(name, SYM_L_LOCAL, SYM_A_ALIGN)	\
+	BTI_C
+
+#define SYM_FUNC_START_LOCAL_NOALIGN(name)		\
+	SYM_START(name, SYM_L_LOCAL, SYM_A_NONE)	\
+	BTI_C
+
+#define SYM_FUNC_START_WEAK(name)			\
+	SYM_START(name, SYM_L_WEAK, SYM_A_ALIGN)	\
+	BTI_C
+
+#define SYM_FUNC_START_WEAK_NOALIGN(name)		\
+	SYM_START(name, SYM_L_WEAK, SYM_A_NONE)		\
+	BTI_C
+
+#define SYM_INNER_LABEL(name, linkage)			\
+	.type name SYM_T_NONE ASM_NL			\
+	SYM_ENTRY(name, linkage, SYM_A_NONE)		\
+	BTI_J
+
+#endif
+
 /*
  * Annotate a function as position independent, i.e., safe to be called before
  * the kernel virtual mapping is activated.

commit 35e61c77ef386555f3df1bc2057098c6997ca10b
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Jan 6 19:58:16 2020 +0000

    arm64: asm: Add new-style position independent function annotations
    
    As part of an effort to make the annotations in assembly code clearer and
    more consistent new macros have been introduced, including replacements
    for ENTRY() and ENDPROC().
    
    On arm64 we have ENDPIPROC(), a custom version of ENDPROC() which is
    used for code that will need to run in position independent environments
    like EFI, it creates an alias for the function with the prefix __pi_ and
    then emits the standard ENDPROC. Add new-style macros to replace this
    which expand to the standard SYM_FUNC_*() and SYM_FUNC_ALIAS_*(),
    resulting in the same object code. These are added in linkage.h for
    consistency with where the generic assembler code has its macros.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    [will: Rename 'WEAK' macro, use ';' instead of ASM_NL, deprecate ENDPIPROC]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/linkage.h b/arch/arm64/include/asm/linkage.h
index 1b266292f0be..ebee3113a62f 100644
--- a/arch/arm64/include/asm/linkage.h
+++ b/arch/arm64/include/asm/linkage.h
@@ -4,4 +4,20 @@
 #define __ALIGN		.align 2
 #define __ALIGN_STR	".align 2"
 
+/*
+ * Annotate a function as position independent, i.e., safe to be called before
+ * the kernel virtual mapping is activated.
+ */
+#define SYM_FUNC_START_PI(x)			\
+		SYM_FUNC_START_ALIAS(__pi_##x);	\
+		SYM_FUNC_START(x)
+
+#define SYM_FUNC_START_WEAK_PI(x)		\
+		SYM_FUNC_START_ALIAS(__pi_##x);	\
+		SYM_FUNC_START_WEAK(x)
+
+#define SYM_FUNC_END_PI(x)			\
+		SYM_FUNC_END(x);		\
+		SYM_FUNC_END_ALIAS(__pi_##x)
+
 #endif

commit c73cc120a33e12e4e254b4b42bc613204ccb923b
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Sep 18 11:20:19 2017 +0100

    arm64: relax assembly code alignment from 16 byte to 4 byte
    
    Aarch64 instructions must be word aligned.  The current 16 byte
    alignment is more than enough.  Relax it into 4 byte alignment.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/linkage.h b/arch/arm64/include/asm/linkage.h
index 636c1bced7d4..1b266292f0be 100644
--- a/arch/arm64/include/asm/linkage.h
+++ b/arch/arm64/include/asm/linkage.h
@@ -1,7 +1,7 @@
 #ifndef __ASM_LINKAGE_H
 #define __ASM_LINKAGE_H
 
-#define __ALIGN		.align 4
-#define __ALIGN_STR	".align 4"
+#define __ALIGN		.align 2
+#define __ALIGN_STR	".align 2"
 
 #endif

commit aeed41a9371ee02257b608eb06a9058507a7d0f4
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Fri Oct 19 17:33:27 2012 +0100

    arm64: fix alignment padding in assembly code
    
    An interesting effect of using the generic version of linkage.h
    is that the padding is defined in terms of x86 NOPs, which can have
    even more interesting effects when the assembly code looks like this:
    
    ENTRY(func1)
            mov     x0, xzr
    ENDPROC(func1)
            // fall through
    ENTRY(func2)
            mov     x0, #1
            ret
    ENDPROC(func2)
    
    Admittedly, the code is not very nice. But having code from another
    architecture doesn't look completely sane either.
    
    The fix is to add arm64's version of linkage.h, which causes the insertion
    of proper AArch64 NOPs.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/include/asm/linkage.h b/arch/arm64/include/asm/linkage.h
new file mode 100644
index 000000000000..636c1bced7d4
--- /dev/null
+++ b/arch/arm64/include/asm/linkage.h
@@ -0,0 +1,7 @@
+#ifndef __ASM_LINKAGE_H
+#define __ASM_LINKAGE_H
+
+#define __ALIGN		.align 4
+#define __ALIGN_STR	".align 4"
+
+#endif
