commit 4d395762599dbab1eb29d9011d5b75ca3cc4f70a
Author: Peter Xu <peterx@redhat.com>
Date:   Fri Feb 28 13:30:20 2020 -0500

    KVM: Remove unnecessary asm/kvm_host.h includes
    
    Remove includes of asm/kvm_host.h from files that already include
    linux/kvm_host.h to make it more obvious that there is no ordering issue
    between the two headers.  linux/kvm_host.h includes asm/kvm_host.h to
    pick up architecture specific settings, and this will never change, i.e.
    including asm/kvm_host.h after linux/kvm_host.h may seem problematic,
    but in practice is simply redundant.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 525010504f9d..e329a36b2bee 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -11,7 +11,6 @@
 #include <linux/kvm_host.h>
 #include <asm/fpsimd.h>
 #include <asm/kvm_asm.h>
-#include <asm/kvm_host.h>
 #include <asm/kvm_mmu.h>
 #include <asm/sysreg.h>
 

commit 54b8c7cbc57c1ce21f4e35101f2609092c4af49a
Author: Julien Grall <julien.grall@arm.com>
Date:   Tue May 21 18:21:38 2019 +0100

    arm64/fpsimd: Introduce fpsimd_save_and_flush_cpu_state() and use it
    
    The only external user of fpsimd_save() and fpsimd_flush_cpu_state() is
    the KVM FPSIMD code.
    
    A following patch will introduce a mechanism to acquire owernship of the
    FPSIMD/SVE context for performing context management operations. Rather
    than having to export the new helpers to get/put the context, we can just
    introduce a new function to combine fpsimd_save() and
    fpsimd_flush_cpu_state().
    
    This has also the advantage to remove any external call of fpsimd_save()
    and fpsimd_flush_cpu_state(), so they can be turned static.
    
    Lastly, the new function can also be used in the PM notifier.
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Grall <julien.grall@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 6e3c9c8b2df9..525010504f9d 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -112,9 +112,7 @@ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
 		u64 *guest_zcr = &vcpu->arch.ctxt.sys_regs[ZCR_EL1];
 
-		/* Clean guest FP state to memory and invalidate cpu view */
-		fpsimd_save();
-		fpsimd_flush_cpu_state();
+		fpsimd_save_and_flush_cpu_state();
 
 		if (guest_has_sve)
 			*guest_zcr = read_sysreg_s(SYS_ZCR_EL12);

commit b43b5dd990eb28047dafe639ce44db347496cb56
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:17 2018 +0100

    KVM: arm64/sve: Context switch the SVE registers
    
    In order to give each vcpu its own view of the SVE registers, this
    patch adds context storage via a new sve_state pointer in struct
    vcpu_arch.  An additional member sve_max_vl is also added for each
    vcpu, to determine the maximum vector length visible to the guest
    and thus the value to be configured in ZCR_EL2.LEN while the vcpu
    is active.  This also determines the layout and size of the storage
    in sve_state, which is read and written by the same backend
    functions that are used for context-switching the SVE state for
    host tasks.
    
    On SVE-enabled vcpus, SVE access traps are now handled by switching
    in the vcpu's SVE context and disabling the trap before returning
    to the guest.  On other vcpus, the trap is not handled and an exit
    back to the host occurs, where the handle_sve() fallback path
    reflects an undefined instruction exception back to the guest,
    consistently with the behaviour of non-SVE-capable hardware (as was
    done unconditionally prior to this patch).
    
    No SVE handling is added on non-VHE-only paths, since VHE is an
    architectural and Kconfig prerequisite of SVE.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 7053bf402131..6e3c9c8b2df9 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -87,10 +87,11 @@ void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu)
 
 	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
 		fpsimd_bind_state_to_cpu(&vcpu->arch.ctxt.gp_regs.fp_regs,
-					 NULL, SVE_VL_MIN);
+					 vcpu->arch.sve_state,
+					 vcpu->arch.sve_max_vl);
 
 		clear_thread_flag(TIF_FOREIGN_FPSTATE);
-		clear_thread_flag(TIF_SVE);
+		update_thread_flag(TIF_SVE, vcpu_has_sve(vcpu));
 	}
 }
 

commit 73433762fcaeb9d59e84d299021c6b15466c96dd
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:16 2018 +0100

    KVM: arm64/sve: System register context switch and access support
    
    This patch adds the necessary support for context switching ZCR_EL1
    for each vcpu.
    
    ZCR_EL1 is trapped alongside the FPSIMD/SVE registers, so it makes
    sense for it to be handled as part of the guest FPSIMD/SVE context
    for context switch purposes instead of handling it as a general
    system register.  This means that it can be switched in lazily at
    the appropriate time.  No effort is made to track host context for
    this register, since SVE requires VHE: thus the hosts's value for
    this register lives permanently in ZCR_EL2 and does not alias the
    guest's value at any time.
    
    The Hyp switch and fpsimd context handling code is extended
    appropriately.
    
    Accessors are added in sys_regs.c to expose the SVE system
    registers and ID register fields.  Because these need to be
    conditionally visible based on the guest configuration, they are
    implemented separately for now rather than by use of the generic
    system register helpers.  This may be abstracted better later on
    when/if there are more features requiring this model.
    
    ID_AA64ZFR0_EL1 is RO-RAZ for MRS/MSR when SVE is disabled for the
    guest, but for compatibility with non-SVE aware KVM implementations
    the register should not be enumerated at all for KVM_GET_REG_LIST
    in this case.  For consistency we also reject ioctl access to the
    register.  This ensures that a non-SVE-enabled guest looks the same
    to userspace, irrespective of whether the kernel KVM implementation
    supports SVE.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 1cf4f0269471..7053bf402131 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -103,14 +103,21 @@ void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu)
 void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 {
 	unsigned long flags;
+	bool host_has_sve = system_supports_sve();
+	bool guest_has_sve = vcpu_has_sve(vcpu);
 
 	local_irq_save(flags);
 
 	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
+		u64 *guest_zcr = &vcpu->arch.ctxt.sys_regs[ZCR_EL1];
+
 		/* Clean guest FP state to memory and invalidate cpu view */
 		fpsimd_save();
 		fpsimd_flush_cpu_state();
-	} else if (system_supports_sve()) {
+
+		if (guest_has_sve)
+			*guest_zcr = read_sysreg_s(SYS_ZCR_EL12);
+	} else if (host_has_sve) {
 		/*
 		 * The FPSIMD/SVE state in the CPU has not been touched, and we
 		 * have SVE (and VHE): CPACR_EL1 (alias CPTR_EL2) has been

commit 0495067420f352a0b8ed37ee412d7dd8e7b95c61
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Sep 28 14:39:11 2018 +0100

    arm64/sve: Enable SVE state tracking for non-task contexts
    
    The current FPSIMD/SVE context handling support for non-task (i.e.,
    KVM vcpu) contexts does not take SVE into account.  This means that
    only task contexts can safely use SVE at present.
    
    In preparation for enabling KVM guests to use SVE, it is necessary
    to keep track of SVE state for non-task contexts too.
    
    This patch adds the necessary support, removing assumptions from
    the context switch code about the location of the SVE context
    storage.
    
    When binding a vcpu context, its vector length is arbitrarily
    specified as SVE_VL_MIN for now.  In any case, because TIF_SVE is
    presently cleared at vcpu context bind time, the specified vector
    length will not be used for anything yet.  In later patches TIF_SVE
    will be set here as appropriate, and the appropriate maximum vector
    length for the vcpu will be passed when binding.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benn√©e <alex.bennee@linaro.org>
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index aac7808ce216..1cf4f0269471 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -9,6 +9,7 @@
 #include <linux/sched.h>
 #include <linux/thread_info.h>
 #include <linux/kvm_host.h>
+#include <asm/fpsimd.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_host.h>
 #include <asm/kvm_mmu.h>
@@ -85,7 +86,9 @@ void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu)
 	WARN_ON_ONCE(!irqs_disabled());
 
 	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
-		fpsimd_bind_state_to_cpu(&vcpu->arch.ctxt.gp_regs.fp_regs);
+		fpsimd_bind_state_to_cpu(&vcpu->arch.ctxt.gp_regs.fp_regs,
+					 NULL, SVE_VL_MIN);
+
 		clear_thread_flag(TIF_FOREIGN_FPSTATE);
 		clear_thread_flag(TIF_SVE);
 	}

commit 2955bcc8c309bb8f2c773db4798649aa802a491f
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Jun 15 16:47:26 2018 +0100

    KVM: arm64: Avoid mistaken attempts to save SVE state for vcpus
    
    Commit e6b673b ("KVM: arm64: Optimise FPSIMD handling to reduce
    guest/host thrashing") uses fpsimd_save() to save the FPSIMD state
    for a vcpu when scheduling the vcpu out.  However, currently
    current's value of TIF_SVE is restored before calling fpsimd_save()
    which means that fpsimd_save() may erroneously attempt to save SVE
    state from the vcpu.  This enables current's vector state to be
    polluted with guest data.  current->thread.sve_state may be
    unallocated or not large enough, so this can also trigger a NULL
    dereference or buffer overrun.
    
    Instead of this, TIF_SVE should be configured properly for the
    guest when calling fpsimd_save() with the vcpu context loaded.
    
    This patch ensures this by delaying restoration of current's
    TIF_SVE until after the call to fpsimd_save().
    
    Fixes: e6b673b741ea ("KVM: arm64: Optimise FPSIMD handling to reduce guest/host thrashing")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 98d19d1afa50..aac7808ce216 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -103,9 +103,6 @@ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 
 	local_irq_save(flags);
 
-	update_thread_flag(TIF_SVE,
-			   vcpu->arch.flags & KVM_ARM64_HOST_SVE_IN_USE);
-
 	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
 		/* Clean guest FP state to memory and invalidate cpu view */
 		fpsimd_save();
@@ -124,5 +121,8 @@ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 			sysreg_clear_set(CPACR_EL1, CPACR_EL1_ZEN_EL0EN, 0);
 	}
 
+	update_thread_flag(TIF_SVE,
+			   vcpu->arch.flags & KVM_ARM64_HOST_SVE_IN_USE);
+
 	local_irq_restore(flags);
 }

commit b3eb56b629d1095dde56fa37f4d7bcd5f783c8b2
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Jun 15 16:47:25 2018 +0100

    KVM: arm64/sve: Fix SVE trap restoration for non-current tasks
    
    Commit e6b673b ("KVM: arm64: Optimise FPSIMD handling to reduce
    guest/host thrashing") attempts to restore the configuration of
    userspace SVE trapping via a call to fpsimd_bind_task_to_cpu(), but
    the logic for determining when to do this is not correct.
    
    The patch makes the errnoenous assumption that the only task that
    may try to enter userspace with the currently loaded FPSIMD/SVE
    register content is current.  This may not be the case however:  if
    some other user task T is scheduled on the CPU during the execution
    of the KVM run loop, and the vcpu does not try to use the registers
    in the meantime, then T's state may be left there intact.  If T
    happens to be the next task to enter userspace on this CPU then the
    hooks for reloading the register state and configuring traps will
    be skipped.
    
    (Also, current never has SVE state at this point anyway and should
    always have the trap enabled, as a side-effect of the ioctl()
    syscall needed to reach the KVM run loop in the first place.)
    
    This patch instead restores the state of the EL0 trap from the
    state observed at the most recent vcpu_load(), ensuring that the
    trap is set correctly for the loaded context (if any).
    
    Fixes: e6b673b741ea ("KVM: arm64: Optimise FPSIMD handling to reduce guest/host thrashing")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index f9d09318b8db..98d19d1afa50 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -12,6 +12,7 @@
 #include <asm/kvm_asm.h>
 #include <asm/kvm_host.h>
 #include <asm/kvm_mmu.h>
+#include <asm/sysreg.h>
 
 /*
  * Called on entry to KVM_RUN unless this vcpu previously ran at least
@@ -61,10 +62,16 @@ void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu)
 {
 	BUG_ON(!current->mm);
 
-	vcpu->arch.flags &= ~(KVM_ARM64_FP_ENABLED | KVM_ARM64_HOST_SVE_IN_USE);
+	vcpu->arch.flags &= ~(KVM_ARM64_FP_ENABLED |
+			      KVM_ARM64_HOST_SVE_IN_USE |
+			      KVM_ARM64_HOST_SVE_ENABLED);
 	vcpu->arch.flags |= KVM_ARM64_FP_HOST;
+
 	if (test_thread_flag(TIF_SVE))
 		vcpu->arch.flags |= KVM_ARM64_HOST_SVE_IN_USE;
+
+	if (read_sysreg(cpacr_el1) & CPACR_EL1_ZEN_EL0EN)
+		vcpu->arch.flags |= KVM_ARM64_HOST_SVE_ENABLED;
 }
 
 /*
@@ -103,9 +110,18 @@ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 		/* Clean guest FP state to memory and invalidate cpu view */
 		fpsimd_save();
 		fpsimd_flush_cpu_state();
-	} else if (!test_thread_flag(TIF_FOREIGN_FPSTATE)) {
-		/* Ensure user trap controls are correctly restored */
-		fpsimd_bind_task_to_cpu();
+	} else if (system_supports_sve()) {
+		/*
+		 * The FPSIMD/SVE state in the CPU has not been touched, and we
+		 * have SVE (and VHE): CPACR_EL1 (alias CPTR_EL2) has been
+		 * reset to CPACR_EL1_DEFAULT by the Hyp code, disabling SVE
+		 * for EL0.  To avoid spurious traps, restore the trap state
+		 * seen by kvm_arch_vcpu_load_fp():
+		 */
+		if (vcpu->arch.flags & KVM_ARM64_HOST_SVE_ENABLED)
+			sysreg_clear_set(CPACR_EL1, 0, CPACR_EL1_ZEN_EL0EN);
+		else
+			sysreg_clear_set(CPACR_EL1, CPACR_EL1_ZEN_EL0EN, 0);
 	}
 
 	local_irq_restore(flags);

commit b045e4d0f392cbdab2674b0aa78c8d2b187e4e27
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Jun 15 16:47:24 2018 +0100

    KVM: arm64: Don't mask softirq with IRQs disabled in vcpu_put()
    
    Commit e6b673b ("KVM: arm64: Optimise FPSIMD handling to reduce
    guest/host thrashing") introduces a specific helper
    kvm_arch_vcpu_put_fp() for saving the vcpu FPSIMD state during
    vcpu_put().
    
    This function uses local_bh_disable()/_enable() to protect the
    FPSIMD context manipulation from interruption by softirqs.
    
    This approach is not correct, because vcpu_put() can be invoked
    either from the KVM host vcpu thread (when exiting the vcpu run
    loop), or via a preempt notifier.  In the former case, only
    preemption is disabled.  In the latter case, the function is called
    from inside __schedule(), which means that IRQs are disabled.
    
    Use of local_bh_disable()/_enable() with IRQs disabled is considerd
    an error, resulting in lockdep splats while running VMs if lockdep
    is enabled.
    
    This patch disables IRQs instead of attempting to disable softirqs,
    avoiding the problem of calling local_bh_enable() with IRQs
    disabled in the __schedule() path.  This creates an additional
    interrupt blackout during vcpu run loop exit, but this is the rare
    case and the blackout latency is still less than that of
    __schedule().
    
    Fixes: e6b673b741ea ("KVM: arm64: Optimise FPSIMD handling to reduce guest/host thrashing")
    Reported-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index dc6ecfa5a2d2..f9d09318b8db 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -5,7 +5,7 @@
  * Copyright 2018 Arm Limited
  * Author: Dave Martin <Dave.Martin@arm.com>
  */
-#include <linux/bottom_half.h>
+#include <linux/irqflags.h>
 #include <linux/sched.h>
 #include <linux/thread_info.h>
 #include <linux/kvm_host.h>
@@ -92,7 +92,9 @@ void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu)
  */
 void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 {
-	local_bh_disable();
+	unsigned long flags;
+
+	local_irq_save(flags);
 
 	update_thread_flag(TIF_SVE,
 			   vcpu->arch.flags & KVM_ARM64_HOST_SVE_IN_USE);
@@ -106,5 +108,5 @@ void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
 		fpsimd_bind_task_to_cpu();
 	}
 
-	local_bh_enable();
+	local_irq_restore(flags);
 }

commit 85acda3b4a27ee3e20c54783a44f307b51912c2b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 20 16:20:43 2018 +0100

    KVM: arm64: Save host SVE context as appropriate
    
    This patch adds SVE context saving to the hyp FPSIMD context switch
    path.  This means that it is no longer necessary to save the host
    SVE state in advance of entering the guest, when in use.
    
    In order to avoid adding pointless complexity to the code, VHE is
    assumed if SVE is in use.  VHE is an architectural prerequisite for
    SVE, so there is no good reason to turn CONFIG_ARM64_VHE off in
    kernels that support both SVE and KVM.
    
    Historically, software models exist that can expose the
    architecturally invalid configuration of SVE without VHE, so if
    this situation is detected at kvm_init() time then KVM will be
    disabled.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Benn√©e <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
index 365933a98a7c..dc6ecfa5a2d2 100644
--- a/arch/arm64/kvm/fpsimd.c
+++ b/arch/arm64/kvm/fpsimd.c
@@ -59,7 +59,6 @@ int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu)
  */
 void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu)
 {
-	BUG_ON(system_supports_sve());
 	BUG_ON(!current->mm);
 
 	vcpu->arch.flags &= ~(KVM_ARM64_FP_ENABLED | KVM_ARM64_HOST_SVE_IN_USE);

commit e6b673b741ea0d7cd275ad40748bfc225accc423
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 6 14:55:59 2018 +0100

    KVM: arm64: Optimise FPSIMD handling to reduce guest/host thrashing
    
    This patch refactors KVM to align the host and guest FPSIMD
    save/restore logic with each other for arm64.  This reduces the
    number of redundant save/restore operations that must occur, and
    reduces the common-case IRQ blackout time during guest exit storms
    by saving the host state lazily and optimising away the need to
    restore the host state before returning to the run loop.
    
    Four hooks are defined in order to enable this:
    
     * kvm_arch_vcpu_run_map_fp():
       Called on PID change to map necessary bits of current to Hyp.
    
     * kvm_arch_vcpu_load_fp():
       Set up FP/SIMD for entering the KVM run loop (parse as
       "vcpu_load fp").
    
     * kvm_arch_vcpu_ctxsync_fp():
       Get FP/SIMD into a safe state for re-enabling interrupts after a
       guest exit back to the run loop.
    
       For arm64 specifically, this involves updating the host kernel's
       FPSIMD context tracking metadata so that kernel-mode NEON use
       will cause the vcpu's FPSIMD state to be saved back correctly
       into the vcpu struct.  This must be done before re-enabling
       interrupts because kernel-mode NEON may be used by softirqs.
    
     * kvm_arch_vcpu_put_fp():
       Save guest FP/SIMD state back to memory and dissociate from the
       CPU ("vcpu_put fp").
    
    Also, the arm64 FPSIMD context switch code is updated to enable it
    to save back FPSIMD state for a vcpu, not just current.  A few
    helpers drive this:
    
     * fpsimd_bind_state_to_cpu(struct user_fpsimd_state *fp):
       mark this CPU as having context fp (which may belong to a vcpu)
       currently loaded in its registers.  This is the non-task
       equivalent of the static function fpsimd_bind_to_cpu() in
       fpsimd.c.
    
     * task_fpsimd_save():
       exported to allow KVM to save the guest's FPSIMD state back to
       memory on exit from the run loop.
    
     * fpsimd_flush_state():
       invalidate any context's FPSIMD state that is currently loaded.
       Used to disassociate the vcpu from the CPU regs on run loop exit.
    
    These changes allow the run loop to enable interrupts (and thus
    softirqs that may use kernel-mode NEON) without having to save the
    guest's FPSIMD state eagerly.
    
    Some new vcpu_arch fields are added to make all this work.  Because
    host FPSIMD state can now be saved back directly into current's
    thread_struct as appropriate, host_cpu_context is no longer used
    for preserving the FPSIMD state.  However, it is still needed for
    preserving other things such as the host's system registers.  To
    avoid ABI churn, the redundant storage space in host_cpu_context is
    not removed for now.
    
    arch/arm is not addressed by this patch and continues to use its
    current save/restore logic.  It could provide implementations of
    the helpers later if desired.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Alex Benn√©e <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/fpsimd.c b/arch/arm64/kvm/fpsimd.c
new file mode 100644
index 000000000000..365933a98a7c
--- /dev/null
+++ b/arch/arm64/kvm/fpsimd.c
@@ -0,0 +1,111 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * arch/arm64/kvm/fpsimd.c: Guest/host FPSIMD context coordination helpers
+ *
+ * Copyright 2018 Arm Limited
+ * Author: Dave Martin <Dave.Martin@arm.com>
+ */
+#include <linux/bottom_half.h>
+#include <linux/sched.h>
+#include <linux/thread_info.h>
+#include <linux/kvm_host.h>
+#include <asm/kvm_asm.h>
+#include <asm/kvm_host.h>
+#include <asm/kvm_mmu.h>
+
+/*
+ * Called on entry to KVM_RUN unless this vcpu previously ran at least
+ * once and the most recent prior KVM_RUN for this vcpu was called from
+ * the same task as current (highly likely).
+ *
+ * This is guaranteed to execute before kvm_arch_vcpu_load_fp(vcpu),
+ * such that on entering hyp the relevant parts of current are already
+ * mapped.
+ */
+int kvm_arch_vcpu_run_map_fp(struct kvm_vcpu *vcpu)
+{
+	int ret;
+
+	struct thread_info *ti = &current->thread_info;
+	struct user_fpsimd_state *fpsimd = &current->thread.uw.fpsimd_state;
+
+	/*
+	 * Make sure the host task thread flags and fpsimd state are
+	 * visible to hyp:
+	 */
+	ret = create_hyp_mappings(ti, ti + 1, PAGE_HYP);
+	if (ret)
+		goto error;
+
+	ret = create_hyp_mappings(fpsimd, fpsimd + 1, PAGE_HYP);
+	if (ret)
+		goto error;
+
+	vcpu->arch.host_thread_info = kern_hyp_va(ti);
+	vcpu->arch.host_fpsimd_state = kern_hyp_va(fpsimd);
+error:
+	return ret;
+}
+
+/*
+ * Prepare vcpu for saving the host's FPSIMD state and loading the guest's.
+ * The actual loading is done by the FPSIMD access trap taken to hyp.
+ *
+ * Here, we just set the correct metadata to indicate that the FPSIMD
+ * state in the cpu regs (if any) belongs to current on the host.
+ *
+ * TIF_SVE is backed up here, since it may get clobbered with guest state.
+ * This flag is restored by kvm_arch_vcpu_put_fp(vcpu).
+ */
+void kvm_arch_vcpu_load_fp(struct kvm_vcpu *vcpu)
+{
+	BUG_ON(system_supports_sve());
+	BUG_ON(!current->mm);
+
+	vcpu->arch.flags &= ~(KVM_ARM64_FP_ENABLED | KVM_ARM64_HOST_SVE_IN_USE);
+	vcpu->arch.flags |= KVM_ARM64_FP_HOST;
+	if (test_thread_flag(TIF_SVE))
+		vcpu->arch.flags |= KVM_ARM64_HOST_SVE_IN_USE;
+}
+
+/*
+ * If the guest FPSIMD state was loaded, update the host's context
+ * tracking data mark the CPU FPSIMD regs as dirty and belonging to vcpu
+ * so that they will be written back if the kernel clobbers them due to
+ * kernel-mode NEON before re-entry into the guest.
+ */
+void kvm_arch_vcpu_ctxsync_fp(struct kvm_vcpu *vcpu)
+{
+	WARN_ON_ONCE(!irqs_disabled());
+
+	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
+		fpsimd_bind_state_to_cpu(&vcpu->arch.ctxt.gp_regs.fp_regs);
+		clear_thread_flag(TIF_FOREIGN_FPSTATE);
+		clear_thread_flag(TIF_SVE);
+	}
+}
+
+/*
+ * Write back the vcpu FPSIMD regs if they are dirty, and invalidate the
+ * cpu FPSIMD regs so that they can't be spuriously reused if this vcpu
+ * disappears and another task or vcpu appears that recycles the same
+ * struct fpsimd_state.
+ */
+void kvm_arch_vcpu_put_fp(struct kvm_vcpu *vcpu)
+{
+	local_bh_disable();
+
+	update_thread_flag(TIF_SVE,
+			   vcpu->arch.flags & KVM_ARM64_HOST_SVE_IN_USE);
+
+	if (vcpu->arch.flags & KVM_ARM64_FP_ENABLED) {
+		/* Clean guest FP state to memory and invalidate cpu view */
+		fpsimd_save();
+		fpsimd_flush_cpu_state();
+	} else if (!test_thread_flag(TIF_FOREIGN_FPSTATE)) {
+		/* Ensure user trap controls are correctly restored */
+		fpsimd_bind_task_to_cpu();
+	}
+
+	local_bh_enable();
+}
