commit 29eb5a3c57f7e06d803bb44a0ce2f9ed79f39cd9
Author: Marc Zyngier <maz@kernel.org>
Date:   Thu Jun 4 11:14:00 2020 +0100

    KVM: arm64: Handle PtrAuth traps early
    
    The current way we deal with PtrAuth is a bit heavy handed:
    
    - We forcefully save the host's keys on each vcpu_load()
    - Handling the PtrAuth trap forces us to go all the way back
      to the exit handling code to just set the HCR bits
    
    Overall, this is pretty cumbersome. A better approach would be
    to handle it the same way we deal with the FPSIMD registers:
    
    - On vcpu_load() disable PtrAuth for the guest
    - On first use, save the host's keys, enable PtrAuth in the
      guest
    
    Crucially, this can happen as a fixup, which is done very early
    on exit. We can then reenter the guest immediately without
    leaving the hypervisor role.
    
    Another thing is that it simplify the rest of the host handling:
    exiting all the way to the host means that the only possible
    outcome for this trap is to inject an UNDEF.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 065251efa2e6..5a02d4c90559 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -162,25 +162,14 @@ static int handle_sve(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
-/*
- * Handle the guest trying to use a ptrauth instruction, or trying to access a
- * ptrauth register.
- */
-void kvm_arm_vcpu_ptrauth_trap(struct kvm_vcpu *vcpu)
-{
-	if (vcpu_has_ptrauth(vcpu))
-		vcpu_ptrauth_enable(vcpu);
-	else
-		kvm_inject_undefined(vcpu);
-}
-
 /*
  * Guest usage of a ptrauth instruction (which the guest EL1 did not turn into
- * a NOP).
+ * a NOP). If we get here, it is that we didn't fixup ptrauth on exit, and all
+ * that we can do is give the guest an UNDEF.
  */
 static int kvm_handle_ptrauth(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	kvm_arm_vcpu_ptrauth_trap(vcpu);
+	kvm_inject_undefined(vcpu);
 	return 1;
 }
 

commit ef3e40a7ea8dbe2abd0a345032cd7d5023b9684f
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed Jun 3 18:24:01 2020 +0100

    KVM: arm64: Save the host's PtrAuth keys in non-preemptible context
    
    When using the PtrAuth feature in a guest, we need to save the host's
    keys before allowing the guest to program them. For that, we dump
    them in a per-CPU data structure (the so called host context).
    
    But both call sites that do this are in preemptible context,
    which may end up in disaster should the vcpu thread get preempted
    before reentering the guest.
    
    Instead, save the keys eagerly on each vcpu_load(). This has an
    increased overhead, but is at least safe.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index eb194696ef62..065251efa2e6 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -162,31 +162,16 @@ static int handle_sve(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
-#define __ptrauth_save_key(regs, key)						\
-({										\
-	regs[key ## KEYLO_EL1] = read_sysreg_s(SYS_ ## key ## KEYLO_EL1);	\
-	regs[key ## KEYHI_EL1] = read_sysreg_s(SYS_ ## key ## KEYHI_EL1);	\
-})
-
 /*
  * Handle the guest trying to use a ptrauth instruction, or trying to access a
  * ptrauth register.
  */
 void kvm_arm_vcpu_ptrauth_trap(struct kvm_vcpu *vcpu)
 {
-	struct kvm_cpu_context *ctxt;
-
-	if (vcpu_has_ptrauth(vcpu)) {
+	if (vcpu_has_ptrauth(vcpu))
 		vcpu_ptrauth_enable(vcpu);
-		ctxt = vcpu->arch.host_cpu_context;
-		__ptrauth_save_key(ctxt->sys_regs, APIA);
-		__ptrauth_save_key(ctxt->sys_regs, APIB);
-		__ptrauth_save_key(ctxt->sys_regs, APDA);
-		__ptrauth_save_key(ctxt->sys_regs, APDB);
-		__ptrauth_save_key(ctxt->sys_regs, APGA);
-	} else {
+	else
 		kvm_inject_undefined(vcpu);
-	}
 }
 
 /*

commit 9ed24f4b712b855dcf7be3025b75b051cb73a2b7
Author: Marc Zyngier <maz@kernel.org>
Date:   Wed May 13 11:40:34 2020 +0100

    KVM: arm64: Move virt/kvm/arm to arch/arm64
    
    Now that the 32bit KVM/arm host is a distant memory, let's move the
    whole of the KVM/arm64 code into the arm64 tree.
    
    As they said in the song: Welcome Home (Sanitarium).
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200513104034.74741-1-maz@kernel.org

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index aacfc55de44c..eb194696ef62 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -23,7 +23,7 @@
 #include <kvm/arm_hypercalls.h>
 
 #define CREATE_TRACE_POINTS
-#include "trace.h"
+#include "trace_handle_exit.h"
 
 typedef int (*exit_handle_fn)(struct kvm_vcpu *, struct kvm_run *);
 

commit 55009c6ed2d24fc0f5521ab2482f145d269389ea
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Mon Oct 21 16:28:15 2019 +0100

    KVM: arm/arm64: Factor out hypercall handling from PSCI code
    
    We currently intertwine the KVM PSCI implementation with the general
    dispatch of hypercall handling, which makes perfect sense because PSCI
    is the only category of hypercalls we support.
    
    However, as we are about to support additional hypercalls, factor out
    this functionality into a separate hypercall handler file.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    [steven.price@arm.com: rebased]
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 706cca23f0d2..aacfc55de44c 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -11,8 +11,6 @@
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
 
-#include <kvm/arm_psci.h>
-
 #include <asm/esr.h>
 #include <asm/exception.h>
 #include <asm/kvm_asm.h>
@@ -22,6 +20,8 @@
 #include <asm/debug-monitors.h>
 #include <asm/traps.h>
 
+#include <kvm/arm_hypercalls.h>
+
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 516aead3c2a9..706cca23f0d2 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2012,2013 - ARM Ltd
  * Author: Marc Zyngier <marc.zyngier@arm.com>
@@ -5,18 +6,6 @@
  * Derived from arch/arm/kvm/handle_exit.c:
  * Copyright (C) 2012 - Virtual Open Systems and Columbia University
  * Author: Christoffer Dall <c.dall@virtualopensystems.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/kvm.h>

commit 384b40caa8afae44a54e8f69bd37097c0279fdce
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 23 10:12:35 2019 +0530

    KVM: arm/arm64: Context-switch ptrauth registers
    
    When pointer authentication is supported, a guest may wish to use it.
    This patch adds the necessary KVM infrastructure for this to work, with
    a semi-lazy context switch of the pointer auth state.
    
    Pointer authentication feature is only enabled when VHE is built
    in the kernel and present in the CPU implementation so only VHE code
    paths are modified.
    
    When we schedule a vcpu, we disable guest usage of pointer
    authentication instructions and accesses to the keys. While these are
    disabled, we avoid context-switching the keys. When we trap the guest
    trying to use pointer authentication functionality, we change to eagerly
    context-switching the keys, and enable the feature. The next time the
    vcpu is scheduled out/in, we start again. However the host key save is
    optimized and implemented inside ptrauth instruction/register access
    trap.
    
    Pointer authentication consists of address authentication and generic
    authentication, and CPUs in a system might have varied support for
    either. Where support for either feature is not uniform, it is hidden
    from guests via ID register emulation, as a result of the cpufeature
    framework in the host.
    
    Unfortunately, address authentication and generic authentication cannot
    be trapped separately, as the architecture provides a single EL2 trap
    covering both. If we wish to expose one without the other, we cannot
    prevent a (badly-written) guest from intermittently using a feature
    which is not uniformly supported (when scheduled on a physical CPU which
    supports the relevant feature). Hence, this patch expects both type of
    authentication to be present in a cpu.
    
    This switch of key is done from guest enter/exit assembly as preparation
    for the upcoming in-kernel pointer authentication support. Hence, these
    key switching routines are not implemented in C code as they may cause
    pointer authentication key signing error in some situations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    [Only VHE, key switch in full assembly, vcpu_has_ptrauth checks
    , save host key in ptrauth exception trap]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    [maz: various fixups]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 0b7983442071..516aead3c2a9 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -173,20 +173,40 @@ static int handle_sve(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
+#define __ptrauth_save_key(regs, key)						\
+({										\
+	regs[key ## KEYLO_EL1] = read_sysreg_s(SYS_ ## key ## KEYLO_EL1);	\
+	regs[key ## KEYHI_EL1] = read_sysreg_s(SYS_ ## key ## KEYHI_EL1);	\
+})
+
+/*
+ * Handle the guest trying to use a ptrauth instruction, or trying to access a
+ * ptrauth register.
+ */
+void kvm_arm_vcpu_ptrauth_trap(struct kvm_vcpu *vcpu)
+{
+	struct kvm_cpu_context *ctxt;
+
+	if (vcpu_has_ptrauth(vcpu)) {
+		vcpu_ptrauth_enable(vcpu);
+		ctxt = vcpu->arch.host_cpu_context;
+		__ptrauth_save_key(ctxt->sys_regs, APIA);
+		__ptrauth_save_key(ctxt->sys_regs, APIB);
+		__ptrauth_save_key(ctxt->sys_regs, APDA);
+		__ptrauth_save_key(ctxt->sys_regs, APDB);
+		__ptrauth_save_key(ctxt->sys_regs, APGA);
+	} else {
+		kvm_inject_undefined(vcpu);
+	}
+}
+
 /*
  * Guest usage of a ptrauth instruction (which the guest EL1 did not turn into
  * a NOP).
  */
 static int kvm_handle_ptrauth(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	/*
-	 * We don't currently support ptrauth in a guest, and we mask the ID
-	 * registers to prevent well-behaved guests from trying to make use of
-	 * it.
-	 *
-	 * Inject an UNDEF, as if the feature really isn't present.
-	 */
-	kvm_inject_undefined(vcpu);
+	kvm_arm_vcpu_ptrauth_trap(vcpu);
 	return 1;
 }
 

commit 42b00f122cfbfed79fc29b0b3610f3abbb1e3864
Merge: 460023a5d1d2 a0aea130afeb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 26 11:46:28 2018 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - selftests improvements
       - large PUD support for HugeTLB
       - single-stepping fixes
       - improved tracing
       - various timer and vGIC fixes
    
      x86:
       - Processor Tracing virtualization
       - STIBP support
       - some correctness fixes
       - refactorings and splitting of vmx.c
       - use the Hyper-V range TLB flush hypercall
       - reduce order of vcpu struct
       - WBNOINVD support
       - do not use -ftrace for __noclone functions
       - nested guest support for PAUSE filtering on AMD
       - more Hyper-V enlightenments (direct mode for synthetic timers)
    
      PPC:
       -  nested VFIO
    
      s390:
       - bugfixes only this time"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (171 commits)
      KVM: x86: Add CPUID support for new instruction WBNOINVD
      kvm: selftests: ucall: fix exit mmio address guessing
      Revert "compiler-gcc: disable -ftracer for __noclone functions"
      KVM: VMX: Move VM-Enter + VM-Exit handling to non-inline sub-routines
      KVM: VMX: Explicitly reference RCX as the vmx_vcpu pointer in asm blobs
      KVM: x86: Use jmp to invoke kvm_spurious_fault() from .fixup
      MAINTAINERS: Add arch/x86/kvm sub-directories to existing KVM/x86 entry
      KVM/x86: Use SVM assembly instruction mnemonics instead of .byte streams
      KVM/MMU: Flush tlb directly in the kvm_zap_gfn_range()
      KVM/MMU: Flush tlb directly in kvm_set_pte_rmapp()
      KVM/MMU: Move tlb flush in kvm_set_pte_rmapp() to kvm_mmu_notifier_change_pte()
      KVM: Make kvm_set_spte_hva() return int
      KVM: Replace old tlb flush function with new one to flush a specified range.
      KVM/MMU: Add tlb flush with range helper function
      KVM/VMX: Add hv tlb range flush support
      x86/hyper-v: Add HvFlushGuestAddressList hypercall support
      KVM: Add tlb_remote_flush_with_range callback in kvm_x86_ops
      KVM: x86: Disable Intel PT when VMXON in L1 guest
      KVM: x86: Set intercept for Intel PT MSRs read/write
      KVM: x86: Implement Intel PT MSRs read/write emulation
      ...

commit bd7d95cafb499e24903b7d21f9eeb2c5208160c2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Nov 9 15:07:11 2018 +0000

    arm64: KVM: Consistently advance singlestep when emulating instructions
    
    When we emulate a guest instruction, we don't advance the hardware
    singlestep state machine, and thus the guest will receive a software
    step exception after a next instruction which is not emulated by the
    host.
    
    We bodge around this in an ad-hoc fashion. Sometimes we explicitly check
    whether userspace requested a single step, and fake a debug exception
    from within the kernel. Other times, we advance the HW singlestep state
    rely on the HW to generate the exception for us. Thus, the observed step
    behaviour differs for host and guest.
    
    Let's make this simpler and consistent by always advancing the HW
    singlestep state machine when we skip an instruction. Thus we can rely
    on the hardware to generate the singlestep exception for us, and never
    need to explicitly check for an active-pending step, nor do we need to
    fake a debug exception from the guest.
    
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 35a81bebd02b..b0643f9c4873 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -229,13 +229,6 @@ static int handle_trap_exceptions(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		handled = exit_handler(vcpu, run);
 	}
 
-	/*
-	 * kvm_arm_handle_step_debug() sets the exit_reason on the kvm_run
-	 * structure if we need to return to userspace.
-	 */
-	if (handled > 0 && kvm_arm_handle_step_debug(vcpu, run))
-		handled = 0;
-
 	return handled;
 }
 
@@ -269,12 +262,7 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	case ARM_EXCEPTION_IRQ:
 		return 1;
 	case ARM_EXCEPTION_EL1_SERROR:
-		/* We may still need to return for single-step */
-		if (!(*vcpu_cpsr(vcpu) & DBG_SPSR_SS)
-			&& kvm_arm_handle_step_debug(vcpu, run))
-			return 0;
-		else
-			return 1;
+		return 1;
 	case ARM_EXCEPTION_TRAP:
 		return handle_trap_exceptions(vcpu, run);
 	case ARM_EXCEPTION_HYP_GONE:

commit a1ee8abb95c55948dc187d0cccc88814841c85d2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:22 2018 +0000

    arm64/kvm: hide ptrauth from guests
    
    In subsequent patches we're going to expose ptrauth to the host kernel
    and userspace, but things are a bit trickier for guest kernels. For the
    time being, let's hide ptrauth from KVM guests.
    
    Regardless of how well-behaved the guest kernel is, guest userspace
    could attempt to use ptrauth instructions, triggering a trap to EL2,
    resulting in noise from kvm_handle_unknown_ec(). So let's write up a
    handler for the PAC trap, which silently injects an UNDEF into the
    guest, as if the feature were really missing.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 35a81bebd02b..ab35929dcb3c 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -173,6 +173,23 @@ static int handle_sve(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
+/*
+ * Guest usage of a ptrauth instruction (which the guest EL1 did not turn into
+ * a NOP).
+ */
+static int kvm_handle_ptrauth(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	/*
+	 * We don't currently support ptrauth in a guest, and we mask the ID
+	 * registers to prevent well-behaved guests from trying to make use of
+	 * it.
+	 *
+	 * Inject an UNDEF, as if the feature really isn't present.
+	 */
+	kvm_inject_undefined(vcpu);
+	return 1;
+}
+
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -195,6 +212,7 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_BKPT32]	= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BRK64]	= kvm_handle_guest_debug,
 	[ESR_ELx_EC_FP_ASIMD]	= handle_no_fpsimd,
+	[ESR_ELx_EC_PAC]	= kvm_handle_ptrauth,
 };
 
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)

commit e4e11cc0f81ee7be17d6f6fb96128a6d51c0e838
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Wed Oct 17 20:21:16 2018 +0200

    KVM: arm64: Safety check PSTATE when entering guest and handle IL
    
    This commit adds a paranoid check when entering the guest to make sure
    we don't attempt running guest code in an equally or more privilged mode
    than the hypervisor.  We also catch other accidental programming of the
    SPSR_EL2 which results in an illegal exception return and report this
    safely back to the user.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index e5e741bfffe1..35a81bebd02b 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -284,6 +284,13 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		 */
 		run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 		return 0;
+	case ARM_EXCEPTION_IL:
+		/*
+		 * We attempted an illegal exception return.  Guest state must
+		 * have been corrupted somehow.  Give up.
+		 */
+		run->exit_reason = KVM_EXIT_FAIL_ENTRY;
+		return -EINVAL;
 	default:
 		kvm_pr_unimpl("Unsupported exception type: %d",
 			      exception_index);

commit 09e6be12effdb33bf7210c8867bbd213b66a499e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:12 2018 +0000

    arm/arm64: KVM: Advertise SMCCC v1.1
    
    The new SMC Calling Convention (v1.1) allows for a reduced overhead
    when calling into the firmware, and provides a new feature discovery
    mechanism.
    
    Make it visible to KVM guests.
    
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 588f910632a7..e5e741bfffe1 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -52,7 +52,7 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 			    kvm_vcpu_hvc_get_imm(vcpu));
 	vcpu->stat.hvc_exit_stat++;
 
-	ret = kvm_psci_call(vcpu);
+	ret = kvm_hvc_call_handler(vcpu);
 	if (ret < 0) {
 		vcpu_set_reg(vcpu, 0, ~0UL);
 		return 1;

commit 1a2fb94e6a771ff94f4afa22497a4695187b820c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:08 2018 +0000

    arm/arm64: KVM: Consolidate the PSCI include files
    
    As we're about to update the PSCI support, and because I'm lazy,
    let's move the PSCI include file to include/kvm so that both
    ARM architectures can find it.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 5493bbefbd0d..588f910632a7 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -22,13 +22,14 @@
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
 
+#include <kvm/arm_psci.h>
+
 #include <asm/esr.h>
 #include <asm/exception.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_coproc.h>
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>
-#include <asm/kvm_psci.h>
 #include <asm/debug-monitors.h>
 #include <asm/traps.h>
 

commit f5115e8869e1dfafac0e414b4f1664f3a84a4683
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:07 2018 +0000

    arm64: KVM: Increment PC after handling an SMC trap
    
    When handling an SMC trap, the "preferred return address" is set
    to that of the SMC, and not the next PC (which is a departure from
    the behaviour of an SMC that isn't trapped).
    
    Increment PC in the handler, as the guest is otherwise forever
    stuck...
    
    Cc: stable@vger.kernel.org
    Fixes: acfb3b883f6d ("arm64: KVM: Fix SMCCC handling of unimplemented SMC/HVC calls")
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 520b0dad3c62..5493bbefbd0d 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -62,7 +62,16 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
+	/*
+	 * "If an SMC instruction executed at Non-secure EL1 is
+	 * trapped to EL2 because HCR_EL2.TSC is 1, the exception is a
+	 * Trap exception, not a Secure Monitor Call exception [...]"
+	 *
+	 * We need to advance the PC after the trap, as it would
+	 * otherwise return to the same address...
+	 */
 	vcpu_set_reg(vcpu, 0, ~0UL);
+	kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
 	return 1;
 }
 

commit c0938c72f8070aabb766b06edba85941ea7911da
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:05 2018 +0000

    arm64: KVM: Fix SMCCC handling of unimplemented SMC/HVC calls
    
    KVM doesn't follow the SMCCC when it comes to unimplemented calls,
    and inject an UNDEF instead of returning an error. Since firmware
    calls are now used for security mitigation, they are becoming more
    common, and the undef is counter productive.
    
    Instead, let's follow the SMCCC which states that -1 must be returned
    to the caller when getting an unknown function number.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index c09fc5a576c7..520b0dad3c62 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -53,7 +53,7 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	ret = kvm_psci_call(vcpu);
 	if (ret < 0) {
-		kvm_inject_undefined(vcpu);
+		vcpu_set_reg(vcpu, 0, ~0UL);
 		return 1;
 	}
 
@@ -62,7 +62,7 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	kvm_inject_undefined(vcpu);
+	vcpu_set_reg(vcpu, 0, ~0UL);
 	return 1;
 }
 

commit 0067df413bd9d7e9ee3a78ece1e1a93535378862
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:05 2018 +0000

    KVM: arm64: Handle RAS SErrors from EL2 on guest exit
    
    We expect to have firmware-first handling of RAS SErrors, with errors
    notified via an APEI method. For systems without firmware-first, add
    some minimal handling to KVM.
    
    There are two ways KVM can take an SError due to a guest, either may be a
    RAS error: we exit the guest due to an SError routed to EL2 by HCR_EL2.AMO,
    or we take an SError from EL2 when we unmask PSTATE.A from __guest_exit.
    
    The current SError from EL2 code unmasks SError and tries to fence any
    pending SError into a single instruction window. It then leaves SError
    unmasked.
    
    With the v8.2 RAS Extensions we may take an SError for a 'corrected'
    error, but KVM is only able to handle SError from EL2 if they occur
    during this single instruction window...
    
    The RAS Extensions give us a new instruction to synchronise and
    consume SErrors. The RAS Extensions document (ARM DDI0587),
    '2.4.1 ESB and Unrecoverable errors' describes ESB as synchronising
    SError interrupts generated by 'instructions, translation table walks,
    hardware updates to the translation tables, and instruction fetches on
    the same PE'. This makes ESB equivalent to KVMs existing
    'dsb, mrs-daifclr, isb' sequence.
    
    Use the alternatives to synchronise and consume any SError using ESB
    instead of unmasking and taking the SError. Set ARM_EXIT_WITH_SERROR_BIT
    in the exit_code so that we can restart the vcpu if it turns out this
    SError has no impact on the vcpu.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 6a5a5db4292f..c09fc5a576c7 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -23,6 +23,7 @@
 #include <linux/kvm_host.h>
 
 #include <asm/esr.h>
+#include <asm/exception.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_coproc.h>
 #include <asm/kvm_emulate.h>
@@ -249,7 +250,6 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 			*vcpu_pc(vcpu) -= adj;
 		}
 
-		kvm_inject_vabt(vcpu);
 		return 1;
 	}
 
@@ -286,6 +286,18 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 void handle_exit_early(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		       int exception_index)
 {
+	if (ARM_SERROR_PENDING(exception_index)) {
+		if (this_cpu_has_cap(ARM64_HAS_RAS_EXTN)) {
+			u64 disr = kvm_vcpu_get_disr(vcpu);
+
+			kvm_handle_guest_serror(vcpu, disr_to_esr(disr));
+		} else {
+			kvm_inject_vabt(vcpu);
+		}
+
+		return;
+	}
+
 	exception_index = ARM_EXCEPTION_CODE(exception_index);
 
 	if (exception_index == ARM_EXCEPTION_EL1_SERROR)

commit 3368bd809764d3ef0810e16c1e1531fec32e8d8e
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:39:04 2018 +0000

    KVM: arm64: Handle RAS SErrors from EL1 on guest exit
    
    We expect to have firmware-first handling of RAS SErrors, with errors
    notified via an APEI method. For systems without firmware-first, add
    some minimal handling to KVM.
    
    There are two ways KVM can take an SError due to a guest, either may be a
    RAS error: we exit the guest due to an SError routed to EL2 by HCR_EL2.AMO,
    or we take an SError from EL2 when we unmask PSTATE.A from __guest_exit.
    
    For SError that interrupt a guest and are routed to EL2 the existing
    behaviour is to inject an impdef SError into the guest.
    
    Add code to handle RAS SError based on the ESR. For uncontained and
    uncategorized errors arm64_is_fatal_ras_serror() will panic(), these
    errors compromise the host too. All other error types are contained:
    For the fatal errors the vCPU can't make progress, so we inject a virtual
    SError. We ignore contained errors where we can make progress as if
    we're lucky, we may not hit them again.
    
    If only some of the CPUs support RAS the guest will see the cpufeature
    sanitised version of the id registers, but we may still take RAS SError
    on this CPU. Move the SError handling out of handle_exit() into a new
    handler that runs before we can be preempted. This allows us to use
    this_cpu_has_cap(), via arm64_is_ras_serror().
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 304203fa9e33..6a5a5db4292f 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -29,12 +29,19 @@
 #include <asm/kvm_mmu.h>
 #include <asm/kvm_psci.h>
 #include <asm/debug-monitors.h>
+#include <asm/traps.h>
 
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
 typedef int (*exit_handle_fn)(struct kvm_vcpu *, struct kvm_run *);
 
+static void kvm_handle_guest_serror(struct kvm_vcpu *vcpu, u32 esr)
+{
+	if (!arm64_is_ras_serror(esr) || arm64_is_fatal_ras_serror(NULL, esr))
+		kvm_inject_vabt(vcpu);
+}
+
 static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	int ret;
@@ -252,7 +259,6 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	case ARM_EXCEPTION_IRQ:
 		return 1;
 	case ARM_EXCEPTION_EL1_SERROR:
-		kvm_inject_vabt(vcpu);
 		/* We may still need to return for single-step */
 		if (!(*vcpu_cpsr(vcpu) & DBG_SPSR_SS)
 			&& kvm_arm_handle_step_debug(vcpu, run))
@@ -275,3 +281,13 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		return 0;
 	}
 }
+
+/* For exit types that need handling before we can be preempted */
+void handle_exit_early(struct kvm_vcpu *vcpu, struct kvm_run *run,
+		       int exception_index)
+{
+	exception_index = ARM_EXCEPTION_CODE(exception_index);
+
+	if (exception_index == ARM_EXCEPTION_EL1_SERROR)
+		kvm_handle_guest_serror(vcpu, kvm_vcpu_get_hsr(vcpu));
+}

commit e70dce73befcf96607bc6e24c2c8f84229d6721e
Author: Alex Bennée <alex.bennee@linaro.org>
Date:   Thu Nov 23 12:11:33 2017 +0000

    kvm: arm64: handle single-step during SError exceptions
    
    When an SError arrives during single-step both the SError and debug
    exceptions may be pending when the step is completed, and the
    architecture doesn't define the ordering of the two.  This means that we
    can observe en SError even though we've just completed a step, without
    receiving a debug exception.  In that case the DBG_SPSR_SS bit will have
    flipped as the instruction executed. After handling the abort in
    handle_exit() we test to see if the bit is clear and we were
    single-stepping before deciding if we need to exit to user space.
    
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Alex Bennée <alex.bennee@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 029c28dd25e9..304203fa9e33 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -28,6 +28,7 @@
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>
 #include <asm/kvm_psci.h>
+#include <asm/debug-monitors.h>
 
 #define CREATE_TRACE_POINTS
 #include "trace.h"
@@ -252,7 +253,12 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		return 1;
 	case ARM_EXCEPTION_EL1_SERROR:
 		kvm_inject_vabt(vcpu);
-		return 1;
+		/* We may still need to return for single-step */
+		if (!(*vcpu_cpsr(vcpu) & DBG_SPSR_SS)
+			&& kvm_arm_handle_step_debug(vcpu, run))
+			return 0;
+		else
+			return 1;
 	case ARM_EXCEPTION_TRAP:
 		return handle_trap_exceptions(vcpu, run);
 	case ARM_EXCEPTION_HYP_GONE:

commit 7226bc2e126dfb6d81e85a75f373ea4cc3619406
Author: Alex Bennée <alex.bennee@linaro.org>
Date:   Thu Nov 16 15:39:20 2017 +0000

    kvm: arm64: handle single-stepping trapped instructions
    
    If we are using guest debug to single-step the guest, we need to ensure
    that we exit after emulating the instruction. This only affects
    instructions completely emulated by the kernel. For instructions
    emulated in userspace, we need to exit and return to complete the
    emulation.
    
    The kvm_arm_handle_step_debug() helper sets up the necessary exit
    state if needed.
    
    Signed-off-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index b71247995469..029c28dd25e9 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -186,6 +186,40 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 	return arm_exit_handlers[hsr_ec];
 }
 
+/*
+ * We may be single-stepping an emulated instruction. If the emulation
+ * has been completed in the kernel, we can return to userspace with a
+ * KVM_EXIT_DEBUG, otherwise userspace needs to complete its
+ * emulation first.
+ */
+static int handle_trap_exceptions(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	int handled;
+
+	/*
+	 * See ARM ARM B1.14.1: "Hyp traps on instructions
+	 * that fail their condition code check"
+	 */
+	if (!kvm_condition_valid(vcpu)) {
+		kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
+		handled = 1;
+	} else {
+		exit_handle_fn exit_handler;
+
+		exit_handler = kvm_get_exit_handler(vcpu);
+		handled = exit_handler(vcpu, run);
+	}
+
+	/*
+	 * kvm_arm_handle_step_debug() sets the exit_reason on the kvm_run
+	 * structure if we need to return to userspace.
+	 */
+	if (handled > 0 && kvm_arm_handle_step_debug(vcpu, run))
+		handled = 0;
+
+	return handled;
+}
+
 /*
  * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
  * proper exit to userspace.
@@ -193,8 +227,6 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		       int exception_index)
 {
-	exit_handle_fn exit_handler;
-
 	if (ARM_SERROR_PENDING(exception_index)) {
 		u8 hsr_ec = ESR_ELx_EC(kvm_vcpu_get_hsr(vcpu));
 
@@ -222,18 +254,7 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		kvm_inject_vabt(vcpu);
 		return 1;
 	case ARM_EXCEPTION_TRAP:
-		/*
-		 * See ARM ARM B1.14.1: "Hyp traps on instructions
-		 * that fail their condition code check"
-		 */
-		if (!kvm_condition_valid(vcpu)) {
-			kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
-			return 1;
-		}
-
-		exit_handler = kvm_get_exit_handler(vcpu);
-
-		return exit_handler(vcpu, run);
+		return handle_trap_exceptions(vcpu, run);
 	case ARM_EXCEPTION_HYP_GONE:
 		/*
 		 * EL2 has been reset to the hyp-stub. This happens when a guest

commit aac45ffd1f8eff7b919df3deaa06d872b665eb09
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:17 2017 +0000

    arm64/sve: KVM: Treat guest SVE use as undefined instruction execution
    
    When trapping forbidden attempts by a guest to use SVE, we want the
    guest to see a trap consistent with SVE not being implemented.
    
    This patch injects an undefined instruction exception into the
    guest in response to such an exception.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 7debb74843a0..b71247995469 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -147,6 +147,13 @@ static int kvm_handle_unknown_ec(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
+static int handle_sve(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	/* Until SVE is supported for guests: */
+	kvm_inject_undefined(vcpu);
+	return 1;
+}
+
 static exit_handle_fn arm_exit_handlers[] = {
 	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
@@ -160,6 +167,7 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_HVC64]	= handle_hvc,
 	[ESR_ELx_EC_SMC64]	= handle_smc,
 	[ESR_ELx_EC_SYS64]	= kvm_handle_sys_reg,
+	[ESR_ELx_EC_SVE]	= handle_sve,
 	[ESR_ELx_EC_IABT_LOW]	= kvm_handle_guest_abort,
 	[ESR_ELx_EC_DABT_LOW]	= kvm_handle_guest_abort,
 	[ESR_ELx_EC_SOFTSTP_LOW]= kvm_handle_guest_debug,

commit f01fbd2fad73434f62cbd0aada8f9403fc0fa527
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:35 2017 +0800

    KVM: arm: implements the kvm_arch_vcpu_in_kernel()
    
    This implements the kvm_arch_vcpu_in_kernel() for ARM, and adjusts
    the calls to kvm_vcpu_on_spin().
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index da57622cacca..7debb74843a0 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -84,7 +84,7 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	if (kvm_vcpu_get_hsr(vcpu) & ESR_ELx_WFx_ISS_WFE) {
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), true);
 		vcpu->stat.wfe_exit_stat++;
-		kvm_vcpu_on_spin(vcpu, false);
+		kvm_vcpu_on_spin(vcpu, vcpu_mode_priv(vcpu));
 	} else {
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), false);
 		vcpu->stat.wfi_exit_stat++;

commit 199b5763d329b43c88f6ad539db8a6c6b42f8edb
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:32 2017 +0800

    KVM: add spinlock optimization framework
    
    If a vcpu exits due to request a user mode spinlock, then
    the spinlock-holder may be preempted in user mode or kernel mode.
    (Note that not all architectures trap spin loops in user mode,
    only AMD x86 and ARM/ARM64 currently do).
    
    But if a vcpu exits in kernel mode, then the holder must be
    preempted in kernel mode, so we should choose a vcpu in kernel mode
    as a more likely candidate for the lock holder.
    
    This introduces kvm_arch_vcpu_in_kernel() to decide whether the
    vcpu is in kernel-mode when it's preempted.  kvm_vcpu_on_spin's
    new argument says the same of the spinning VCPU.
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 17d8a1677a0b..da57622cacca 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -84,7 +84,7 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	if (kvm_vcpu_get_hsr(vcpu) & ESR_ELx_WFx_ISS_WFE) {
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), true);
 		vcpu->stat.wfe_exit_stat++;
-		kvm_vcpu_on_spin(vcpu);
+		kvm_vcpu_on_spin(vcpu, false);
 	} else {
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), false);
 		vcpu->stat.wfi_exit_stat++;

commit 6a6d73be12fbe492d0678cd84d3b35e2bc9698e4
Author: Andrew Jones <drjones@redhat.com>
Date:   Sun Jun 4 14:43:54 2017 +0200

    KVM: arm/arm64: properly use vcpu requests
    
    arm/arm64 already has one VCPU request used when setting pause,
    but it doesn't properly check requests in VCPU RUN. Check it
    and also make sure we set vcpu->mode at the appropriate time
    (before the check) and with the appropriate barriers. See
    Documentation/virtual/kvm/vcpu-requests.rst. Also make sure we
    don't leave any vcpu requests we don't intend to handle later
    set in the request bitmap. If we don't clear them, then
    kvm_request_pending() may return true when it shouldn't.
    
    Using VCPU requests properly fixes a small race where pause
    could get set just as a VCPU was entering guest mode.
    
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index fa1b18e364fc..17d8a1677a0b 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -89,6 +89,7 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), false);
 		vcpu->stat.wfi_exit_stat++;
 		kvm_vcpu_block(vcpu);
+		kvm_clear_request(KVM_REQ_UNHALT, vcpu);
 	}
 
 	kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));

commit ba4dd156eabdca93501d92a980ba27fa5f4bbd27
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Feb 20 12:30:12 2017 +0000

    arm64: KVM: Survive unknown traps from guests
    
    Currently we BUG() if we see an ESR_EL2.EC value we don't recognise. As
    configurable disables/enables are added to the architecture (controlled
    by RES1/RES0 bits respectively), with associated synchronous exceptions,
    it may be possible for a guest to trigger exceptions with classes that
    we don't recognise.
    
    While we can't service these exceptions in a manner useful to the guest,
    we can avoid bringing down the host. Per ARM DDI 0487A.k_iss10775, page
    D7-1937, EC values within the range 0x00 - 0x2c are reserved for future
    use with synchronous exceptions, and EC values within the range 0x2d -
    0x3f may be used for either synchronous or asynchronous exceptions.
    
    The patch makes KVM handle any unknown EC by injecting an UNDEFINED
    exception into the guest, with a corresponding (ratelimited) warning in
    the host dmesg. We could later improve on this with with a new (opt-in)
    exit to the host userspace.
    
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 1bfe30dfbfe7..fa1b18e364fc 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -135,7 +135,19 @@ static int kvm_handle_guest_debug(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return ret;
 }
 
+static int kvm_handle_unknown_ec(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	u32 hsr = kvm_vcpu_get_hsr(vcpu);
+
+	kvm_pr_unimpl("Unknown exception class: hsr: %#08x -- %s\n",
+		      hsr, esr_get_class_string(hsr));
+
+	kvm_inject_undefined(vcpu);
+	return 1;
+}
+
 static exit_handle_fn arm_exit_handlers[] = {
+	[0 ... ESR_ELx_EC_MAX]	= kvm_handle_unknown_ec,
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
 	[ESR_ELx_EC_CP15_32]	= kvm_handle_cp15_32,
 	[ESR_ELx_EC_CP15_64]	= kvm_handle_cp15_64,
@@ -162,13 +174,6 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 	u32 hsr = kvm_vcpu_get_hsr(vcpu);
 	u8 hsr_ec = ESR_ELx_EC(hsr);
 
-	if (hsr_ec >= ARRAY_SIZE(arm_exit_handlers) ||
-	    !arm_exit_handlers[hsr_ec]) {
-		kvm_err("Unknown exception class: hsr: %#08x -- %s\n",
-			hsr, esr_get_class_string(hsr));
-		BUG();
-	}
-
 	return arm_exit_handlers[hsr_ec];
 }
 

commit 82e0191a1aa11abfddb22c8944989b7735560efc
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Nov 8 13:56:21 2016 +0000

    arm64: Support systems without FP/ASIMD
    
    The arm64 kernel assumes that FP/ASIMD units are always present
    and accesses the FP/ASIMD specific registers unconditionally. This
    could cause problems when they are absent. This patch adds the
    support for kernel handling systems without FP/ASIMD by skipping the
    register access within the kernel. For kvm, we trap the accesses
    to FP/ASIMD and inject an undefined instruction exception to the VM.
    
    The callers of the exported kernel_neon_begin_partial() should
    make sure that the FP/ASIMD is supported.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    [catalin.marinas@arm.com: add comment on the ARM64_HAS_NO_FPSIMD conflict and the new location]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index a204adf29f0a..1bfe30dfbfe7 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -57,6 +57,16 @@ static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
+/*
+ * Guest access to FP/ASIMD registers are routed to this handler only
+ * when the system doesn't support FP/ASIMD.
+ */
+static int handle_no_fpsimd(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	kvm_inject_undefined(vcpu);
+	return 1;
+}
+
 /**
  * kvm_handle_wfx - handle a wait-for-interrupts or wait-for-event
  *		    instruction executed by a guest
@@ -144,6 +154,7 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_BREAKPT_LOW]= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BKPT32]	= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BRK64]	= kvm_handle_guest_debug,
+	[ESR_ELx_EC_FP_ASIMD]	= handle_no_fpsimd,
 };
 
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)

commit ddb3d07cfe90ce58c342cf97ce6ce53ba7d10973
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 14:02:06 2016 +0100

    arm64: KVM: Inject a Virtual SError if it was pending
    
    If we have caught an SError whilst exiting, we've tagged the
    exit code with the pending information. In that case, let's
    re-inject the error into the guest, after having adjusted
    the PC if required.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 08afc69ab157..a204adf29f0a 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -170,6 +170,26 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 {
 	exit_handle_fn exit_handler;
 
+	if (ARM_SERROR_PENDING(exception_index)) {
+		u8 hsr_ec = ESR_ELx_EC(kvm_vcpu_get_hsr(vcpu));
+
+		/*
+		 * HVC/SMC already have an adjusted PC, which we need
+		 * to correct in order to return to after having
+		 * injected the SError.
+		 */
+		if (hsr_ec == ESR_ELx_EC_HVC32 || hsr_ec == ESR_ELx_EC_HVC64 ||
+		    hsr_ec == ESR_ELx_EC_SMC32 || hsr_ec == ESR_ELx_EC_SMC64) {
+			u32 adj =  kvm_vcpu_trap_il_is32bit(vcpu) ? 4 : 2;
+			*vcpu_pc(vcpu) -= adj;
+		}
+
+		kvm_inject_vabt(vcpu);
+		return 1;
+	}
+
+	exception_index = ARM_EXCEPTION_CODE(exception_index);
+
 	switch (exception_index) {
 	case ARM_EXCEPTION_IRQ:
 		return 1;

commit 0215a6e6dd4fdbf4f7dd6e9db116aec7818ff388
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Sep 6 14:02:03 2016 +0100

    arm64: KVM: Add EL1 async abort handler
    
    If we've exited the guest because it has triggered an asynchronous
    abort from EL1, a possible course of action is to let it know it
    screwed up by giving it a Virtual Abort to chew on.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index fa96fe2bd469..08afc69ab157 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -173,6 +173,9 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 	switch (exception_index) {
 	case ARM_EXCEPTION_IRQ:
 		return 1;
+	case ARM_EXCEPTION_EL1_SERROR:
+		kvm_inject_vabt(vcpu);
+		return 1;
 	case ARM_EXCEPTION_TRAP:
 		/*
 		 * See ARM ARM B1.14.1: "Hyp traps on instructions

commit 561454e25dfa27aeac9e9d05f88ce7cb43d02d71
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 31 12:33:02 2016 +0100

    arm64/kvm: use ESR_ELx_EC to extract EC
    
    Now that we have a helper to extract the EC from an ESR_ELx value, make
    use of this in the arm64 KVM code for simplicity and consistency. There
    should be no functional changes as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave P Martin <dave.martin@arm.com>
    Cc: Huang Shijie <shijie.huang@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 3246c4aba5b1..fa96fe2bd469 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -106,7 +106,7 @@ static int kvm_handle_guest_debug(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	run->exit_reason = KVM_EXIT_DEBUG;
 	run->debug.arch.hsr = hsr;
 
-	switch (hsr >> ESR_ELx_EC_SHIFT) {
+	switch (ESR_ELx_EC(hsr)) {
 	case ESR_ELx_EC_WATCHPT_LOW:
 		run->debug.arch.far = vcpu->arch.fault.far_el2;
 		/* fall through */
@@ -149,7 +149,7 @@ static exit_handle_fn arm_exit_handlers[] = {
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
 	u32 hsr = kvm_vcpu_get_hsr(vcpu);
-	u8 hsr_ec = hsr >> ESR_ELx_EC_SHIFT;
+	u8 hsr_ec = ESR_ELx_EC(hsr);
 
 	if (hsr_ec >= ARRAY_SIZE(arm_exit_handlers) ||
 	    !arm_exit_handlers[hsr_ec]) {

commit c94b0cf28281d483c8b43b4874fcb7ab14ade1b1
Author: James Morse <james.morse@arm.com>
Date:   Wed Apr 27 17:47:04 2016 +0100

    arm64: hyp/kvm: Make hyp-stub reject kvm_call_hyp()
    
    A later patch implements kvm_arch_hardware_disable(), to remove kvm
    from el2, and re-instate the hyp-stub.
    
    This can happen while guests are running, particularly when kvm_reboot()
    calls kvm_arch_hardware_disable() on each cpu. This can interrupt a guest,
    remove kvm, then allow the guest to be scheduled again. This causes
    kvm_call_hyp() to be run against the hyp-stub.
    
    Change the hyp-stub to return a new exception type when this happens,
    and add code to kvm's handle_exit() to tell userspace we failed to
    enter the guest.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index eba89e42f0ed..3246c4aba5b1 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -186,6 +186,13 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
 		exit_handler = kvm_get_exit_handler(vcpu);
 
 		return exit_handler(vcpu, run);
+	case ARM_EXCEPTION_HYP_GONE:
+		/*
+		 * EL2 has been reset to the hyp-stub. This happens when a guest
+		 * is pre-empted by kvm_reboot()'s shutdown call.
+		 */
+		run->exit_reason = KVM_EXIT_FAIL_ENTRY;
+		return 0;
 	default:
 		kvm_pr_unimpl("Unsupported exception type: %d",
 			      exception_index);

commit 9d8415d6c148a16b6d906a96f0596851d7e4d607
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sun Oct 25 19:57:11 2015 +0000

    arm64: KVM: Turn system register numbers to an enum
    
    Having the system register numbers as #defines has been a pain
    since day one, as the ordering is pretty fragile, and moving
    things around leads to renumbering and epic conflict resolutions.
    
    Now that we're mostly acessing the sysreg file in C, an enum is
    a much better type to use, and we can clean things up a bit.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 8bddae140461..eba89e42f0ed 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -23,6 +23,7 @@
 #include <linux/kvm_host.h>
 
 #include <asm/esr.h>
+#include <asm/kvm_asm.h>
 #include <asm/kvm_coproc.h>
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>

commit b19e6892a90e7c9d15fde0a08516ec891a4e7d54
Author: Amit Tomar <amittomer25@gmail.com>
Date:   Thu Nov 26 10:09:43 2015 +0000

    KVM: arm/arm64: Count guest exit due to various reasons
    
    It would add guest exit statistics to debugfs, this can be helpful
    while measuring KVM performance.
    
      [ Renamed some of the field names - Christoffer ]
    
    Signed-off-by: Amit Singh Tomar <amittomer25@gmail.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 15f0477b0d2a..8bddae140461 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -39,6 +39,7 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	trace_kvm_hvc_arm64(*vcpu_pc(vcpu), vcpu_get_reg(vcpu, 0),
 			    kvm_vcpu_hvc_get_imm(vcpu));
+	vcpu->stat.hvc_exit_stat++;
 
 	ret = kvm_psci_call(vcpu);
 	if (ret < 0) {
@@ -71,9 +72,11 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	if (kvm_vcpu_get_hsr(vcpu) & ESR_ELx_WFx_ISS_WFE) {
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), true);
+		vcpu->stat.wfe_exit_stat++;
 		kvm_vcpu_on_spin(vcpu);
 	} else {
 		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), false);
+		vcpu->stat.wfi_exit_stat++;
 		kvm_vcpu_block(vcpu);
 	}
 

commit f6be563abb60f0fa6978dec46da01164df89a635
Author: Pavel Fedin <p.fedin@samsung.com>
Date:   Fri Dec 4 15:03:14 2015 +0300

    arm64: KVM: Get rid of old vcpu_reg()
    
    Using oldstyle vcpu_reg() accessor is proven to be inappropriate and
    unsafe on ARM64. This patch converts the rest of use cases to new
    accessors and completely removes vcpu_reg() on ARM64.
    
    Signed-off-by: Pavel Fedin <p.fedin@samsung.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 68a0759b1375..15f0477b0d2a 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -37,7 +37,7 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	int ret;
 
-	trace_kvm_hvc_arm64(*vcpu_pc(vcpu), *vcpu_reg(vcpu, 0),
+	trace_kvm_hvc_arm64(*vcpu_pc(vcpu), vcpu_get_reg(vcpu, 0),
 			    kvm_vcpu_hvc_get_imm(vcpu));
 
 	ret = kvm_psci_call(vcpu);

commit 834bf88726f0f11ddc7ff9679fc9458654c01a12
Author: Alex Bennée <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:30:02 2015 +0100

    KVM: arm64: enable KVM_CAP_SET_GUEST_DEBUG
    
    Finally advertise the KVM capability for SET_GUEST_DEBUG. Once arm
    support is added this check can be moved to the common
    kvm_vm_ioctl_check_extension() code.
    
    Signed-off-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index e9de13ed477e..68a0759b1375 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -103,7 +103,11 @@ static int kvm_handle_guest_debug(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	run->debug.arch.hsr = hsr;
 
 	switch (hsr >> ESR_ELx_EC_SHIFT) {
+	case ESR_ELx_EC_WATCHPT_LOW:
+		run->debug.arch.far = vcpu->arch.fault.far_el2;
+		/* fall through */
 	case ESR_ELx_EC_SOFTSTP_LOW:
+	case ESR_ELx_EC_BREAKPT_LOW:
 	case ESR_ELx_EC_BKPT32:
 	case ESR_ELx_EC_BRK64:
 		break;
@@ -132,6 +136,8 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_IABT_LOW]	= kvm_handle_guest_abort,
 	[ESR_ELx_EC_DABT_LOW]	= kvm_handle_guest_abort,
 	[ESR_ELx_EC_SOFTSTP_LOW]= kvm_handle_guest_debug,
+	[ESR_ELx_EC_WATCHPT_LOW]= kvm_handle_guest_debug,
+	[ESR_ELx_EC_BREAKPT_LOW]= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BKPT32]	= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BRK64]	= kvm_handle_guest_debug,
 };

commit 337b99bf7edfb2044781447e7ca386edb1fdba9d
Author: Alex Bennée <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:29:58 2015 +0100

    KVM: arm64: guest debug, add support for single-step
    
    This adds support for single-stepping the guest. To do this we need to
    manipulate the guests PSTATE.SS and MDSCR_EL1.SS bits to trigger
    stepping. We take care to preserve MDSCR_EL1 and trap access to it to
    ensure we don't affect the apparent state of the guest.
    
    As we have to enable trapping of all software debug exceptions we
    suppress the ability of the guest to single-step itself. If we didn't we
    would have to deal with the exception arriving while the guest was in
    kernelspace when the guest is expecting to single-step userspace. This
    is something we don't want to unwind in the kernel. Once the host is no
    longer debugging the guest its ability to single-step userspace is
    restored.
    
    Signed-off-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 27f38a9f9ea2..e9de13ed477e 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -103,6 +103,7 @@ static int kvm_handle_guest_debug(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	run->debug.arch.hsr = hsr;
 
 	switch (hsr >> ESR_ELx_EC_SHIFT) {
+	case ESR_ELx_EC_SOFTSTP_LOW:
 	case ESR_ELx_EC_BKPT32:
 	case ESR_ELx_EC_BRK64:
 		break;
@@ -130,6 +131,7 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_SYS64]	= kvm_handle_sys_reg,
 	[ESR_ELx_EC_IABT_LOW]	= kvm_handle_guest_abort,
 	[ESR_ELx_EC_DABT_LOW]	= kvm_handle_guest_abort,
+	[ESR_ELx_EC_SOFTSTP_LOW]= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BKPT32]	= kvm_handle_guest_debug,
 	[ESR_ELx_EC_BRK64]	= kvm_handle_guest_debug,
 };

commit 4bd611ca60afa155bca25b40312ed61c4d46237f
Author: Alex Bennée <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:29:57 2015 +0100

    KVM: arm64: guest debug, add SW break point support
    
    This adds support for SW breakpoints inserted by userspace.
    
    We do this by trapping all guest software debug exceptions to the
    hypervisor (MDCR_EL2.TDE). The exit handler sets an exit reason of
    KVM_EXIT_DEBUG with the kvm_debug_exit_arch structure holding the
    exception syndrome information.
    
    It will be up to userspace to extract the PC (via GET_ONE_REG) and
    determine if the debug event was for a breakpoint it inserted. If not
    userspace will need to re-inject the correct exception restart the
    hypervisor to deliver the debug exception to the guest.
    
    Any other guest software debug exception (e.g. single step or HW
    assisted breakpoints) will cause an error and the VM to be killed. This
    is addressed by later patches which add support for the other debug
    types.
    
    Signed-off-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 524fa25671fc..27f38a9f9ea2 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -82,6 +82,40 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return 1;
 }
 
+/**
+ * kvm_handle_guest_debug - handle a debug exception instruction
+ *
+ * @vcpu:	the vcpu pointer
+ * @run:	access to the kvm_run structure for results
+ *
+ * We route all debug exceptions through the same handler. If both the
+ * guest and host are using the same debug facilities it will be up to
+ * userspace to re-inject the correct exception for guest delivery.
+ *
+ * @return: 0 (while setting run->exit_reason), -1 for error
+ */
+static int kvm_handle_guest_debug(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	u32 hsr = kvm_vcpu_get_hsr(vcpu);
+	int ret = 0;
+
+	run->exit_reason = KVM_EXIT_DEBUG;
+	run->debug.arch.hsr = hsr;
+
+	switch (hsr >> ESR_ELx_EC_SHIFT) {
+	case ESR_ELx_EC_BKPT32:
+	case ESR_ELx_EC_BRK64:
+		break;
+	default:
+		kvm_err("%s: un-handled case hsr: %#08x\n",
+			__func__, (unsigned int) hsr);
+		ret = -1;
+		break;
+	}
+
+	return ret;
+}
+
 static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
 	[ESR_ELx_EC_CP15_32]	= kvm_handle_cp15_32,
@@ -96,6 +130,8 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_ELx_EC_SYS64]	= kvm_handle_sys_reg,
 	[ESR_ELx_EC_IABT_LOW]	= kvm_handle_guest_abort,
 	[ESR_ELx_EC_DABT_LOW]	= kvm_handle_guest_abort,
+	[ESR_ELx_EC_BKPT32]	= kvm_handle_guest_debug,
+	[ESR_ELx_EC_BRK64]	= kvm_handle_guest_debug,
 };
 
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)

commit 1c6007d59a20762052cc92c0a2889ff11030d23a
Merge: c6156df9d321 4b990589952f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jan 23 13:39:51 2015 +0100

    Merge tag 'kvm-arm-for-3.20' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into kvm-next
    
    KVM/ARM changes for v3.20 including GICv3 emulation, dirty page logging, added
    trace symbols, and adding an explicit VGIC init device control IOCTL.
    
    Conflicts:
            arch/arm64/include/asm/kvm_arm.h
            arch/arm64/kvm/handle_exit.c

commit 056bb5f51c357ee00046fde4929a03468ff45e7a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jan 7 11:26:18 2015 +0000

    arm64: kvm: decode ESR_ELx.EC when reporting exceptions
    
    To aid the developer when something triggers an unexpected exception,
    decode the ESR_ELx.EC field when logging an ESR_ELx value using the
    newly introduced esr_get_class_string. This doesn't tell the developer
    the specifics of the exception encoded in the remaining IL and ISS bits,
    but it can be helpful to distinguish between exception classes (e.g.
    SError and a data abort) without having to manually decode the field,
    which can be tiresome.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index bcbc923d3060..29b184a8f3f8 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -91,12 +91,13 @@ static exit_handle_fn arm_exit_handlers[] = {
 
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 {
-	u8 hsr_ec = kvm_vcpu_trap_get_class(vcpu);
+	u32 hsr = kvm_vcpu_get_hsr(vcpu);
+	u8 hsr_ec = hsr >> ESR_ELx_EC_SHIFT;
 
 	if (hsr_ec >= ARRAY_SIZE(arm_exit_handlers) ||
 	    !arm_exit_handlers[hsr_ec]) {
-		kvm_err("Unknown exception class: hsr: %#08x\n",
-			(unsigned int)kvm_vcpu_get_hsr(vcpu));
+		kvm_err("Unknown exception class: hsr: %#08x -- %s\n",
+			hsr, esr_get_class_string(hsr));
 		BUG();
 	}
 

commit c6d01a947a51193e839516165286bc8d14a0e409
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Nov 24 13:59:30 2014 +0000

    arm64: kvm: move to ESR_ELx macros
    
    Now that we have common ESR_ELx macros, make use of them in the arm64
    KVM code. The addition of <asm/esr.h> to the include path highlighted
    badly ordered (i.e. not alphabetical) include lists; these are changed
    to alphabetical order.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 34b8bd0711e9..bcbc923d3060 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -21,8 +21,10 @@
 
 #include <linux/kvm.h>
 #include <linux/kvm_host.h>
-#include <asm/kvm_emulate.h>
+
+#include <asm/esr.h>
 #include <asm/kvm_coproc.h>
+#include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>
 #include <asm/kvm_psci.h>
 
@@ -61,7 +63,7 @@ static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
  */
 static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	if (kvm_vcpu_get_hsr(vcpu) & ESR_EL2_EC_WFI_ISS_WFE)
+	if (kvm_vcpu_get_hsr(vcpu) & ESR_ELx_WFx_ISS_WFE)
 		kvm_vcpu_on_spin(vcpu);
 	else
 		kvm_vcpu_block(vcpu);
@@ -72,19 +74,19 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 }
 
 static exit_handle_fn arm_exit_handlers[] = {
-	[ESR_EL2_EC_WFI]	= kvm_handle_wfx,
-	[ESR_EL2_EC_CP15_32]	= kvm_handle_cp15_32,
-	[ESR_EL2_EC_CP15_64]	= kvm_handle_cp15_64,
-	[ESR_EL2_EC_CP14_MR]	= kvm_handle_cp14_32,
-	[ESR_EL2_EC_CP14_LS]	= kvm_handle_cp14_load_store,
-	[ESR_EL2_EC_CP14_64]	= kvm_handle_cp14_64,
-	[ESR_EL2_EC_HVC32]	= handle_hvc,
-	[ESR_EL2_EC_SMC32]	= handle_smc,
-	[ESR_EL2_EC_HVC64]	= handle_hvc,
-	[ESR_EL2_EC_SMC64]	= handle_smc,
-	[ESR_EL2_EC_SYS64]	= kvm_handle_sys_reg,
-	[ESR_EL2_EC_IABT]	= kvm_handle_guest_abort,
-	[ESR_EL2_EC_DABT]	= kvm_handle_guest_abort,
+	[ESR_ELx_EC_WFx]	= kvm_handle_wfx,
+	[ESR_ELx_EC_CP15_32]	= kvm_handle_cp15_32,
+	[ESR_ELx_EC_CP15_64]	= kvm_handle_cp15_64,
+	[ESR_ELx_EC_CP14_MR]	= kvm_handle_cp14_32,
+	[ESR_ELx_EC_CP14_LS]	= kvm_handle_cp14_load_store,
+	[ESR_ELx_EC_CP14_64]	= kvm_handle_cp14_64,
+	[ESR_ELx_EC_HVC32]	= handle_hvc,
+	[ESR_ELx_EC_SMC32]	= handle_smc,
+	[ESR_ELx_EC_HVC64]	= handle_hvc,
+	[ESR_ELx_EC_SMC64]	= handle_smc,
+	[ESR_ELx_EC_SYS64]	= kvm_handle_sys_reg,
+	[ESR_ELx_EC_IABT_LOW]	= kvm_handle_guest_abort,
+	[ESR_ELx_EC_DABT_LOW]	= kvm_handle_guest_abort,
 };
 
 static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)

commit 0d97f884810410b2e0515e29424fe1ec5357176f
Author: Wei Huang <wei@redhat.com>
Date:   Mon Jan 12 11:53:36 2015 -0500

    arm/arm64: KVM: add tracing support for arm64 exit handler
    
    arm64 uses its own copy of exit handler (arm64/kvm/handle_exit.c).
    Currently this file doesn't hook up with any trace points. As a result
    users might not see certain events (e.g. HVC & WFI) while using ftrace
    with arm64 KVM. This patch fixes this issue by adding a new trace file
    and defining two trace events (one of which is shared by wfi and wfe)
    for arm64. The new trace points are then linked with related functions
    in handle_exit.c.
    
    Signed-off-by: Wei Huang <wei@redhat.com>
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 34b8bd0711e9..6a7eb3ce7027 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -26,12 +26,18 @@
 #include <asm/kvm_mmu.h>
 #include <asm/kvm_psci.h>
 
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
 typedef int (*exit_handle_fn)(struct kvm_vcpu *, struct kvm_run *);
 
 static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	int ret;
 
+	trace_kvm_hvc_arm64(*vcpu_pc(vcpu), *vcpu_reg(vcpu, 0),
+			    kvm_vcpu_hvc_get_imm(vcpu));
+
 	ret = kvm_psci_call(vcpu);
 	if (ret < 0) {
 		kvm_inject_undefined(vcpu);
@@ -61,10 +67,13 @@ static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
  */
 static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	if (kvm_vcpu_get_hsr(vcpu) & ESR_EL2_EC_WFI_ISS_WFE)
+	if (kvm_vcpu_get_hsr(vcpu) & ESR_EL2_EC_WFI_ISS_WFE) {
+		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), true);
 		kvm_vcpu_on_spin(vcpu);
-	else
+	} else {
+		trace_kvm_wfx_arm64(*vcpu_pc(vcpu), false);
 		kvm_vcpu_block(vcpu);
+	}
 
 	kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
 

commit 05e0127f9e362b36aa35f17b1a3d52bca9322a3a
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Tue Aug 26 14:33:02 2014 +0200

    arm/arm64: KVM: Complete WFI/WFE instructions
    
    The architecture specifies that when the processor wakes up from a WFE
    or WFI instruction, the instruction is considered complete, however we
    currrently return to EL1 (or EL0) at the WFI/WFE instruction itself.
    
    While most guests may not be affected by this because their local
    exception handler performs an exception returning setting the event bit
    or with an interrupt pending, some guests like UEFI will get wedged due
    this little mishap.
    
    Simply skip the instruction when we have completed the emulation.
    
    Cc: <stable@vger.kernel.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index e28be510380c..34b8bd0711e9 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -66,6 +66,8 @@ static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	else
 		kvm_vcpu_block(vcpu);
 
+	kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
+
 	return 1;
 }
 

commit 72564016aae45f42e488f926bc803f9a2e1c771c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Apr 24 10:27:13 2014 +0100

    arm64: KVM: common infrastructure for handling AArch32 CP14/CP15
    
    As we're about to trap a bunch of CP14 registers, let's rework
    the CP15 handling so it can be generalized and work with multiple
    tables.
    
    Reviewed-by: Anup Patel <anup.patel@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 182415e1a952..e28be510380c 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -73,9 +73,9 @@ static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_EL2_EC_WFI]	= kvm_handle_wfx,
 	[ESR_EL2_EC_CP15_32]	= kvm_handle_cp15_32,
 	[ESR_EL2_EC_CP15_64]	= kvm_handle_cp15_64,
-	[ESR_EL2_EC_CP14_MR]	= kvm_handle_cp14_access,
+	[ESR_EL2_EC_CP14_MR]	= kvm_handle_cp14_32,
 	[ESR_EL2_EC_CP14_LS]	= kvm_handle_cp14_load_store,
-	[ESR_EL2_EC_CP14_64]	= kvm_handle_cp14_access,
+	[ESR_EL2_EC_CP14_64]	= kvm_handle_cp14_64,
 	[ESR_EL2_EC_HVC32]	= handle_hvc,
 	[ESR_EL2_EC_SMC32]	= handle_smc,
 	[ESR_EL2_EC_HVC64]	= handle_hvc,

commit e8e7fcc5e2710b31ef842ee799db99c07986c364
Author: Anup Patel <anup.patel@linaro.org>
Date:   Tue Apr 29 11:24:18 2014 +0530

    ARM/ARM64: KVM: Make kvm_psci_call() return convention more flexible
    
    Currently, the kvm_psci_call() returns 'true' or 'false' based on whether
    the PSCI function call was handled successfully or not. This does not help
    us emulate system-level PSCI functions where the actual emulation work will
    be done by user space (QEMU or KVMTOOL). Examples of such system-level PSCI
    functions are: PSCI v0.2 SYSTEM_OFF and SYSTEM_RESET.
    
    This patch updates kvm_psci_call() to return three types of values:
    1) > 0 (success)
    2) = 0 (success but exit to user space)
    3) < 0 (errors)
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Pranavkumar Sawargaonkar <pranavkumar@linaro.org>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 7bc41eab4c64..182415e1a952 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -30,11 +30,15 @@ typedef int (*exit_handle_fn)(struct kvm_vcpu *, struct kvm_run *);
 
 static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	if (kvm_psci_call(vcpu))
+	int ret;
+
+	ret = kvm_psci_call(vcpu);
+	if (ret < 0) {
+		kvm_inject_undefined(vcpu);
 		return 1;
+	}
 
-	kvm_inject_undefined(vcpu);
-	return 1;
+	return ret;
 }
 
 static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)

commit 7ebd3faa9b5b42caf2d5aa1352a93dcfa0098011
Merge: bb1281f2aae0 7650b6870930
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 22 21:40:43 2014 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "First round of KVM updates for 3.14; PPC parts will come next week.
    
      Nothing major here, just bugfixes all over the place.  The most
      interesting part is the ARM guys' virtualized interrupt controller
      overhaul, which lets userspace get/set the state and thus enables
      migration of ARM VMs"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (67 commits)
      kvm: make KVM_MMU_AUDIT help text more readable
      KVM: s390: Fix memory access error detection
      KVM: nVMX: Update guest activity state field on L2 exits
      KVM: nVMX: Fix nested_run_pending on activity state HLT
      KVM: nVMX: Clean up handling of VMX-related MSRs
      KVM: nVMX: Add tracepoints for nested_vmexit and nested_vmexit_inject
      KVM: nVMX: Pass vmexit parameters to nested_vmx_vmexit
      KVM: nVMX: Leave VMX mode on clearing of feature control MSR
      KVM: VMX: Fix DR6 update on #DB exception
      KVM: SVM: Fix reading of DR6
      KVM: x86: Sync DR7 on KVM_SET_DEBUGREGS
      add support for Hyper-V reference time counter
      KVM: remove useless write to vcpu->hv_clock.tsc_timestamp
      KVM: x86: fix tsc catchup issue with tsc scaling
      KVM: x86: limit PIT timer frequency
      KVM: x86: handle invalid root_hpa everywhere
      kvm: Provide kvm_vcpu_eligible_for_directed_yield() stub
      kvm: vfio: silence GCC warning
      KVM: ARM: Remove duplicate include
      arm/arm64: KVM: relax the requirements of VMA alignment for THP
      ...

commit e5cf9dcdbfd26cd4e1991db08755da900454efeb
Author: Anup Patel <anup.patel@linaro.org>
Date:   Thu Dec 12 16:12:23 2013 +0000

    arm64: KVM: Force undefined exception for Guest SMC intructions
    
    The SMC-based PSCI emulation for Guest is going to be very different
    from the in-kernel HVC-based PSCI emulation hence for now just inject
    undefined exception when Guest executes SMC instruction.
    
    Signed-off-by: Anup Patel <anup.patel@linaro.org>
    Signed-off-by: Pranavkumar Sawargaonkar <pranavkumar@linaro.org>
    Signed-off-by: marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 8da56067c304..df84d7bcc7df 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -39,9 +39,6 @@ static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	if (kvm_psci_call(vcpu))
-		return 1;
-
 	kvm_inject_undefined(vcpu);
 	return 1;
 }

commit 77d84ff87e9d38072abcca665ca22cb1da41cb86
Author: Masanari Iida <standby24x7@gmail.com>
Date:   Mon Dec 9 00:22:53 2013 +0900

    treewide: Fix typos in printk
    
    Correct spelling typo in various part of kernel
    
    Signed-off-by: Masanari Iida <standby24x7@gmail.com>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 8da56067c304..42a0f1bddfe7 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -90,7 +90,7 @@ static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
 
 	if (hsr_ec >= ARRAY_SIZE(arm_exit_handlers) ||
 	    !arm_exit_handlers[hsr_ec]) {
-		kvm_err("Unkown exception class: hsr: %#08x\n",
+		kvm_err("Unknown exception class: hsr: %#08x\n",
 			(unsigned int)kvm_vcpu_get_hsr(vcpu));
 		BUG();
 	}

commit d241aac798eb042e605f78c31a4122e583b2cd13
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Aug 2 11:41:13 2013 +0100

    arm64: KVM: Yield CPU when vcpu executes a WFE
    
    On an (even slightly) oversubscribed system, spinlocks are quickly
    becoming a bottleneck, as some vcpus are spinning, waiting for a
    lock to be released, while the vcpu holding the lock may not be
    running at all.
    
    The solution is to trap blocking WFEs and tell KVM that we're
    now spinning. This ensures that other vpus will get a scheduling
    boost, allowing the lock to be released more quickly. Also, using
    CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT slightly improves the performance
    when the VM is severely overcommited.
    
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 9beaca033437..8da56067c304 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -47,21 +47,29 @@ static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 }
 
 /**
- * kvm_handle_wfi - handle a wait-for-interrupts instruction executed by a guest
+ * kvm_handle_wfx - handle a wait-for-interrupts or wait-for-event
+ *		    instruction executed by a guest
+ *
  * @vcpu:	the vcpu pointer
  *
- * Simply call kvm_vcpu_block(), which will halt execution of
+ * WFE: Yield the CPU and come back to this vcpu when the scheduler
+ * decides to.
+ * WFI: Simply call kvm_vcpu_block(), which will halt execution of
  * world-switches and schedule other host processes until there is an
  * incoming IRQ or FIQ to the VM.
  */
-static int kvm_handle_wfi(struct kvm_vcpu *vcpu, struct kvm_run *run)
+static int kvm_handle_wfx(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	kvm_vcpu_block(vcpu);
+	if (kvm_vcpu_get_hsr(vcpu) & ESR_EL2_EC_WFI_ISS_WFE)
+		kvm_vcpu_on_spin(vcpu);
+	else
+		kvm_vcpu_block(vcpu);
+
 	return 1;
 }
 
 static exit_handle_fn arm_exit_handlers[] = {
-	[ESR_EL2_EC_WFI]	= kvm_handle_wfi,
+	[ESR_EL2_EC_WFI]	= kvm_handle_wfx,
 	[ESR_EL2_EC_CP15_32]	= kvm_handle_cp15_32,
 	[ESR_EL2_EC_CP15_64]	= kvm_handle_cp15_64,
 	[ESR_EL2_EC_CP14_MR]	= kvm_handle_cp14_access,

commit 62a89c44954f09072bf07a714c8f68bda14ab87e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Feb 7 10:32:33 2013 +0000

    arm64: KVM: 32bit handling of coprocessor traps
    
    Provide the necessary infrastructure to trap coprocessor accesses that
    occur when running 32bit guests.
    
    Also wire SMC and HVC trapped in 32bit mode while were at it.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index 4766b7f3515e..9beaca033437 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -62,6 +62,13 @@ static int kvm_handle_wfi(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 static exit_handle_fn arm_exit_handlers[] = {
 	[ESR_EL2_EC_WFI]	= kvm_handle_wfi,
+	[ESR_EL2_EC_CP15_32]	= kvm_handle_cp15_32,
+	[ESR_EL2_EC_CP15_64]	= kvm_handle_cp15_64,
+	[ESR_EL2_EC_CP14_MR]	= kvm_handle_cp14_access,
+	[ESR_EL2_EC_CP14_LS]	= kvm_handle_cp14_load_store,
+	[ESR_EL2_EC_CP14_64]	= kvm_handle_cp14_access,
+	[ESR_EL2_EC_HVC32]	= handle_hvc,
+	[ESR_EL2_EC_SMC32]	= handle_smc,
 	[ESR_EL2_EC_HVC64]	= handle_hvc,
 	[ESR_EL2_EC_SMC64]	= handle_smc,
 	[ESR_EL2_EC_SYS64]	= kvm_handle_sys_reg,

commit dcd2e40c1e1cce302498d16d095b0f8a30326f74
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Dec 12 18:52:05 2012 +0000

    arm64: KVM: PSCI implementation
    
    Wire the PSCI backend into the exit handling code.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
index c65d1154f969..4766b7f3515e 100644
--- a/arch/arm64/kvm/handle_exit.c
+++ b/arch/arm64/kvm/handle_exit.c
@@ -24,26 +24,24 @@
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_coproc.h>
 #include <asm/kvm_mmu.h>
+#include <asm/kvm_psci.h>
 
 typedef int (*exit_handle_fn)(struct kvm_vcpu *, struct kvm_run *);
 
 static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	/*
-	 * Guest called HVC instruction:
-	 * Let it know we don't want that by injecting an undefined exception.
-	 */
-	kvm_debug("hvc: %x (at %08lx)", kvm_vcpu_get_hsr(vcpu) & ((1 << 16) - 1),
-		  *vcpu_pc(vcpu));
-	kvm_debug("         HSR: %8x", kvm_vcpu_get_hsr(vcpu));
+	if (kvm_psci_call(vcpu))
+		return 1;
+
 	kvm_inject_undefined(vcpu);
 	return 1;
 }
 
 static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	/* We don't support SMC; don't do that. */
-	kvm_debug("smc: at %08lx", *vcpu_pc(vcpu));
+	if (kvm_psci_call(vcpu))
+		return 1;
+
 	kvm_inject_undefined(vcpu);
 	return 1;
 }

commit c4b1afd022e93eada6ee4b209be37101cd4b3494
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Dec 10 16:40:41 2012 +0000

    arm64: KVM: Exit handling
    
    Handle the exit of a VM, decoding the exit reason from HYP mode
    and calling the corresponding handler.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
new file mode 100644
index 000000000000..c65d1154f969
--- /dev/null
+++ b/arch/arm64/kvm/handle_exit.c
@@ -0,0 +1,119 @@
+/*
+ * Copyright (C) 2012,2013 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * Derived from arch/arm/kvm/handle_exit.c:
+ * Copyright (C) 2012 - Virtual Open Systems and Columbia University
+ * Author: Christoffer Dall <c.dall@virtualopensystems.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <asm/kvm_emulate.h>
+#include <asm/kvm_coproc.h>
+#include <asm/kvm_mmu.h>
+
+typedef int (*exit_handle_fn)(struct kvm_vcpu *, struct kvm_run *);
+
+static int handle_hvc(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	/*
+	 * Guest called HVC instruction:
+	 * Let it know we don't want that by injecting an undefined exception.
+	 */
+	kvm_debug("hvc: %x (at %08lx)", kvm_vcpu_get_hsr(vcpu) & ((1 << 16) - 1),
+		  *vcpu_pc(vcpu));
+	kvm_debug("         HSR: %8x", kvm_vcpu_get_hsr(vcpu));
+	kvm_inject_undefined(vcpu);
+	return 1;
+}
+
+static int handle_smc(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	/* We don't support SMC; don't do that. */
+	kvm_debug("smc: at %08lx", *vcpu_pc(vcpu));
+	kvm_inject_undefined(vcpu);
+	return 1;
+}
+
+/**
+ * kvm_handle_wfi - handle a wait-for-interrupts instruction executed by a guest
+ * @vcpu:	the vcpu pointer
+ *
+ * Simply call kvm_vcpu_block(), which will halt execution of
+ * world-switches and schedule other host processes until there is an
+ * incoming IRQ or FIQ to the VM.
+ */
+static int kvm_handle_wfi(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	kvm_vcpu_block(vcpu);
+	return 1;
+}
+
+static exit_handle_fn arm_exit_handlers[] = {
+	[ESR_EL2_EC_WFI]	= kvm_handle_wfi,
+	[ESR_EL2_EC_HVC64]	= handle_hvc,
+	[ESR_EL2_EC_SMC64]	= handle_smc,
+	[ESR_EL2_EC_SYS64]	= kvm_handle_sys_reg,
+	[ESR_EL2_EC_IABT]	= kvm_handle_guest_abort,
+	[ESR_EL2_EC_DABT]	= kvm_handle_guest_abort,
+};
+
+static exit_handle_fn kvm_get_exit_handler(struct kvm_vcpu *vcpu)
+{
+	u8 hsr_ec = kvm_vcpu_trap_get_class(vcpu);
+
+	if (hsr_ec >= ARRAY_SIZE(arm_exit_handlers) ||
+	    !arm_exit_handlers[hsr_ec]) {
+		kvm_err("Unkown exception class: hsr: %#08x\n",
+			(unsigned int)kvm_vcpu_get_hsr(vcpu));
+		BUG();
+	}
+
+	return arm_exit_handlers[hsr_ec];
+}
+
+/*
+ * Return > 0 to return to guest, < 0 on error, 0 (and set exit_reason) on
+ * proper exit to userspace.
+ */
+int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
+		       int exception_index)
+{
+	exit_handle_fn exit_handler;
+
+	switch (exception_index) {
+	case ARM_EXCEPTION_IRQ:
+		return 1;
+	case ARM_EXCEPTION_TRAP:
+		/*
+		 * See ARM ARM B1.14.1: "Hyp traps on instructions
+		 * that fail their condition code check"
+		 */
+		if (!kvm_condition_valid(vcpu)) {
+			kvm_skip_instr(vcpu, kvm_vcpu_trap_il_is32bit(vcpu));
+			return 1;
+		}
+
+		exit_handler = kvm_get_exit_handler(vcpu);
+
+		return exit_handler(vcpu, run);
+	default:
+		kvm_pr_unimpl("Unsupported exception type: %d",
+			      exception_index);
+		run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+		return 0;
+	}
+}
