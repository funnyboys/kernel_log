commit 66b7e05dc0239c5817859f261098ba9cc2efbd2b
Author: Steven Price <steven.price@arm.com>
Date:   Wed Jun 17 11:54:56 2020 +0100

    KVM: arm64: Fix kvm_reset_vcpu() return code being incorrect with SVE
    
    If SVE is enabled then 'ret' can be assigned the return value of
    kvm_vcpu_enable_sve() which may be 0 causing future "goto out" sites to
    erroneously return 0 on failure rather than -EINVAL as expected.
    
    Remove the initialisation of 'ret' and make setting the return value
    explicit to avoid this situation in the future.
    
    Fixes: 9a3cdf26e336 ("KVM: arm64/sve: Allow userspace to enable SVE for vcpus")
    Cc: stable@vger.kernel.org
    Reported-by: James Morse <james.morse@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200617105456.28245-1-steven.price@arm.com

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index d3b209023727..6ed36be51b4b 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -245,7 +245,7 @@ static int kvm_vcpu_enable_ptrauth(struct kvm_vcpu *vcpu)
  */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
-	int ret = -EINVAL;
+	int ret;
 	bool loaded;
 	u32 pstate;
 
@@ -269,15 +269,19 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 
 	if (test_bit(KVM_ARM_VCPU_PTRAUTH_ADDRESS, vcpu->arch.features) ||
 	    test_bit(KVM_ARM_VCPU_PTRAUTH_GENERIC, vcpu->arch.features)) {
-		if (kvm_vcpu_enable_ptrauth(vcpu))
+		if (kvm_vcpu_enable_ptrauth(vcpu)) {
+			ret = -EINVAL;
 			goto out;
+		}
 	}
 
 	switch (vcpu->arch.target) {
 	default:
 		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {
-			if (!cpus_have_const_cap(ARM64_HAS_32BIT_EL1))
+			if (!cpus_have_const_cap(ARM64_HAS_32BIT_EL1)) {
+				ret = -EINVAL;
 				goto out;
+			}
 			pstate = VCPU_RESET_PSTATE_SVC;
 		} else {
 			pstate = VCPU_RESET_PSTATE_EL1;

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit b130a8f70cbbf907c399799bd1073a78763ca0e7
Author: Marc Zyngier <maz@kernel.org>
Date:   Thu May 28 14:12:58 2020 +0100

    KVM: arm64: Check advertised Stage-2 page size capability
    
    With ARMv8.5-GTG, the hardware (or more likely a hypervisor) can
    advertise the supported Stage-2 page sizes.
    
    Let's check this at boot time.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Alexandru Elisei <alexandru.elisei@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index d8800ef4f42d..70cd7bcca433 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -337,14 +337,45 @@ u32 get_kvm_ipa_limit(void)
 	return kvm_ipa_limit;
 }
 
-void kvm_set_ipa_limit(void)
+int kvm_set_ipa_limit(void)
 {
-	unsigned int ipa_max, pa_max, va_max, parange;
+	unsigned int ipa_max, pa_max, va_max, parange, tgran_2;
 	u64 mmfr0;
 
 	mmfr0 = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
 	parange = cpuid_feature_extract_unsigned_field(mmfr0,
 				ID_AA64MMFR0_PARANGE_SHIFT);
+
+	/*
+	 * Check with ARMv8.5-GTG that our PAGE_SIZE is supported at
+	 * Stage-2. If not, things will stop very quickly.
+	 */
+	switch (PAGE_SIZE) {
+	default:
+	case SZ_4K:
+		tgran_2 = ID_AA64MMFR0_TGRAN4_2_SHIFT;
+		break;
+	case SZ_16K:
+		tgran_2 = ID_AA64MMFR0_TGRAN16_2_SHIFT;
+		break;
+	case SZ_64K:
+		tgran_2 = ID_AA64MMFR0_TGRAN64_2_SHIFT;
+		break;
+	}
+
+	switch (cpuid_feature_extract_unsigned_field(mmfr0, tgran_2)) {
+	default:
+	case 1:
+		kvm_err("PAGE_SIZE not supported at Stage-2, giving up\n");
+		return -EINVAL;
+	case 0:
+		kvm_debug("PAGE_SIZE supported at Stage-2 (default)\n");
+		break;
+	case 2:
+		kvm_debug("PAGE_SIZE supported at Stage-2 (advertised)\n");
+		break;
+	}
+
 	pa_max = id_aa64mmfr0_parange_to_phys_shift(parange);
 
 	/* Clamp the IPA limit to the PA size supported by the kernel */
@@ -378,6 +409,8 @@ void kvm_set_ipa_limit(void)
 	     "KVM IPA limit (%d bit) is smaller than default size\n", ipa_max);
 	kvm_ipa_limit = ipa_max;
 	kvm_info("IPA Size Limit: %dbits\n", kvm_ipa_limit);
+
+	return 0;
 }
 
 /*

commit 349c330ced9764667678f4d2804fd4ebc16110c9
Author: Marc Zyngier <maz@kernel.org>
Date:   Sun Apr 12 18:49:31 2020 +0100

    KVM: arm64: Don't use empty structures as CPU reset state
    
    Keeping empty structure as the vcpu state initializer is slightly
    wasteful: we only want to set pstate, and zero everything else.
    Just do that.
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 658f3a79617b..865c8aa670bc 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -36,15 +36,11 @@ static u32 kvm_ipa_limit;
 /*
  * ARMv8 Reset Values
  */
-static const struct kvm_regs default_regs_reset = {
-	.regs.pstate = (PSR_MODE_EL1h | PSR_A_BIT | PSR_I_BIT |
-			PSR_F_BIT | PSR_D_BIT),
-};
+#define VCPU_RESET_PSTATE_EL1	(PSR_MODE_EL1h | PSR_A_BIT | PSR_I_BIT | \
+				 PSR_F_BIT | PSR_D_BIT)
 
-static const struct kvm_regs default_regs_reset32 = {
-	.regs.pstate = (PSR_AA32_MODE_SVC | PSR_AA32_A_BIT |
-			PSR_AA32_I_BIT | PSR_AA32_F_BIT),
-};
+#define VCPU_RESET_PSTATE_SVC	(PSR_AA32_MODE_SVC | PSR_AA32_A_BIT | \
+				 PSR_AA32_I_BIT | PSR_AA32_F_BIT)
 
 static bool cpu_has_32bit_el1(void)
 {
@@ -257,9 +253,9 @@ static int kvm_vcpu_enable_ptrauth(struct kvm_vcpu *vcpu)
  */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
-	const struct kvm_regs *cpu_reset;
 	int ret = -EINVAL;
 	bool loaded;
+	u32 pstate;
 
 	/* Reset PMU outside of the non-preemptible section */
 	kvm_pmu_vcpu_reset(vcpu);
@@ -290,16 +286,17 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {
 			if (!cpu_has_32bit_el1())
 				goto out;
-			cpu_reset = &default_regs_reset32;
+			pstate = VCPU_RESET_PSTATE_SVC;
 		} else {
-			cpu_reset = &default_regs_reset;
+			pstate = VCPU_RESET_PSTATE_EL1;
 		}
 
 		break;
 	}
 
 	/* Reset core registers */
-	memcpy(vcpu_gp_regs(vcpu), cpu_reset, sizeof(*cpu_reset));
+	memset(vcpu_gp_regs(vcpu), 0, sizeof(*vcpu_gp_regs(vcpu)));
+	vcpu_gp_regs(vcpu)->regs.pstate = pstate;
 
 	/* Reset system registers */
 	kvm_reset_sys_regs(vcpu);

commit f73531f0257f6bac44a8c9d5c2f3a3ccaea3d1e9
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Wed May 13 14:33:34 2020 +0530

    arm64/cpufeature: Drop open encodings while extracting parange
    
    Currently there are multiple instances of parange feature width mask open
    encodings while fetching it's value. Even the width mask value (0x7) itself
    is not accurate. It should be (0xf) per ID_AA64MMFR0_EL1.PARange[3:0] as in
    ARM ARM (0487F.a). Replace them with cpuid_feature_extract_unsigned_field()
    which can extract given standard feature (4 bits width i.e 0xf mask) field.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/1589360614-1164-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index d4eb6612bb3c..d8800ef4f42d 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -340,8 +340,11 @@ u32 get_kvm_ipa_limit(void)
 void kvm_set_ipa_limit(void)
 {
 	unsigned int ipa_max, pa_max, va_max, parange;
+	u64 mmfr0;
 
-	parange = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1) & 0x7;
+	mmfr0 = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
+	parange = cpuid_feature_extract_unsigned_field(mmfr0,
+				ID_AA64MMFR0_PARANGE_SHIFT);
 	pa_max = id_aa64mmfr0_parange_to_phys_shift(parange);
 
 	/* Clamp the IPA limit to the PA size supported by the kernel */
@@ -387,7 +390,7 @@ void kvm_set_ipa_limit(void)
  */
 int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type)
 {
-	u64 vtcr = VTCR_EL2_FLAGS;
+	u64 vtcr = VTCR_EL2_FLAGS, mmfr0;
 	u32 parange, phys_shift;
 	u8 lvls;
 
@@ -403,7 +406,9 @@ int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type)
 		phys_shift = KVM_PHYS_SHIFT;
 	}
 
-	parange = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1) & 7;
+	mmfr0 = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1);
+	parange = cpuid_feature_extract_unsigned_field(mmfr0,
+				ID_AA64MMFR0_PARANGE_SHIFT);
 	if (parange > ID_AA64MMFR0_PARANGE_MAX)
 		parange = ID_AA64MMFR0_PARANGE_MAX;
 	vtcr |= parange << VTCR_EL2_PS_SHIFT;

commit c73433fc630cda102f6527d4e5dfd289a9baec08
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue May 12 07:27:27 2020 +0530

    arm64/cpufeature: Validate hypervisor capabilities during CPU hotplug
    
    This validates hypervisor capabilities like VMID width, IPA range for any
    hot plug CPU against system finalized values. KVM's view of the IPA space
    is used while allowing a given CPU to come up. While here, it factors out
    get_vmid_bits() for general use.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-kernel@vger.kernel.org
    
    Suggested-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/1589248647-22925-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 102e5c4e01a0..d4eb6612bb3c 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -332,6 +332,11 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	return ret;
 }
 
+u32 get_kvm_ipa_limit(void)
+{
+	return kvm_ipa_limit;
+}
+
 void kvm_set_ipa_limit(void)
 {
 	unsigned int ipa_max, pa_max, va_max, parange;

commit 656012c731fcfd0f770007366e2b952a613745f2
Author: Fuad Tabba <tabba@google.com>
Date:   Wed Apr 1 15:03:10 2020 +0100

    KVM: Fix spelling in code comments
    
    Fix spelling and typos (e.g., repeated words) in comments.
    
    Signed-off-by: Fuad Tabba <tabba@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200401140310.29701-1-tabba@google.com

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 30b7ea680f66..658f3a79617b 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -163,7 +163,7 @@ static int kvm_vcpu_finalize_sve(struct kvm_vcpu *vcpu)
 	vl = vcpu->arch.sve_max_vl;
 
 	/*
-	 * Resposibility for these properties is shared between
+	 * Responsibility for these properties is shared between
 	 * kvm_arm_init_arch_resources(), kvm_vcpu_enable_sve() and
 	 * set_sve_vls().  Double-check here just to be sure:
 	 */
@@ -249,7 +249,7 @@ static int kvm_vcpu_enable_ptrauth(struct kvm_vcpu *vcpu)
  * ioctl or as part of handling a request issued by another VCPU in the PSCI
  * handling code.  In the first case, the VCPU will not be loaded, and in the
  * second case the VCPU will be loaded.  Because this function operates purely
- * on the memory-backed valus of system registers, we want to do a full put if
+ * on the memory-backed values of system registers, we want to do a full put if
  * we were loaded (handling a request) and load the values back at the end of
  * the function.  Otherwise we leave the state alone.  In both cases, we
  * disable preemption around the vcpu reset as we would otherwise race with
@@ -357,7 +357,7 @@ void kvm_set_ipa_limit(void)
 	 *
 	 * So clamp the ipa limit further down to limit the number of levels.
 	 * Since we can concatenate upto 16 tables at entry level, we could
-	 * go upto 4bits above the maximum VA addressible with the current
+	 * go upto 4bits above the maximum VA addressable with the current
 	 * number of levels.
 	 */
 	va_max = PGDIR_SHIFT + PAGE_SHIFT - 3;

commit 540f76d12c662d3da2ebdf0086ee289123fcd120
Author: Will Deacon <will@kernel.org>
Date:   Tue Apr 21 15:29:17 2020 +0100

    arm64: cpufeature: Add CPU capability for AArch32 EL1 support
    
    Although we emit a "SANITY CHECK" warning and taint the kernel if we
    detect a CPU mismatch for AArch32 support at EL1, we still online the
    CPU with disastrous consequences for any running 32-bit VMs.
    
    Introduce a capability for AArch32 support at EL1 so that late onlining
    of incompatible CPUs is forbidden.
    
    Tested-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200421142922.18950-4-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 30b7ea680f66..102e5c4e01a0 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -46,14 +46,6 @@ static const struct kvm_regs default_regs_reset32 = {
 			PSR_AA32_I_BIT | PSR_AA32_F_BIT),
 };
 
-static bool cpu_has_32bit_el1(void)
-{
-	u64 pfr0;
-
-	pfr0 = read_sanitised_ftr_reg(SYS_ID_AA64PFR0_EL1);
-	return !!(pfr0 & 0x20);
-}
-
 /**
  * kvm_arch_vm_ioctl_check_extension
  *
@@ -66,7 +58,7 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 
 	switch (ext) {
 	case KVM_CAP_ARM_EL1_32BIT:
-		r = cpu_has_32bit_el1();
+		r = cpus_have_const_cap(ARM64_HAS_32BIT_EL1);
 		break;
 	case KVM_CAP_GUEST_DEBUG_HW_BPS:
 		r = get_num_brps();
@@ -288,7 +280,7 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	switch (vcpu->arch.target) {
 	default:
 		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {
-			if (!cpu_has_32bit_el1())
+			if (!cpus_have_const_cap(ARM64_HAS_32BIT_EL1))
 				goto out;
 			cpu_reset = &default_regs_reset32;
 		} else {

commit ddd259c9aaba08244dba8877687ee856f79c4f45
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:28 2019 -0800

    KVM: Drop kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit()
    
    Remove kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit() now that all
    arch specific implementations are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index ff3512a0ca97..30b7ea680f66 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -204,11 +204,6 @@ bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu)
 	return true;
 }
 
-void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
-{
-
-}
-
 void kvm_arm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kfree(vcpu->arch.sve_state);

commit 19bcc89eb8a9fa1d4be4bff5b5e7917cb8bbc1f7
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:27 2019 -0800

    KVM: arm64: Free sve_state via arm specific hook
    
    Add an arm specific hook to free the arm64-only sve_state.  Doing so
    eliminates the last functional code from kvm_arch_vcpu_uninit() across
    all architectures and paves the way for removing kvm_arch_vcpu_init()
    and kvm_arch_vcpu_uninit() entirely.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f4a8ae918827..ff3512a0ca97 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -205,6 +205,11 @@ bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu)
 }
 
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
+{
+
+}
+
+void kvm_arm_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	kfree(vcpu->arch.sve_state);
 }

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 1140b4485575..f4a8ae918827 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2012,2013 - ARM Ltd
  * Author: Marc Zyngier <marc.zyngier@arm.com>
@@ -5,18 +6,6 @@
  * Derived from arch/arm/kvm/reset.c
  * Copyright (C) 2012 - Virtual Open Systems and Columbia University
  * Author: Christoffer Dall <c.dall@virtualopensystems.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, version 2, as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/errno.h>

commit dd53f6102c30a774e0db8e55d49017a38060f6f6
Merge: 59c5c58c5b93 9eecfc22e0bf
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 15 23:41:43 2019 +0200

    Merge tag 'kvmarm-for-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 5.2
    
    - guest SVE support
    - guest Pointer Authentication support
    - Better discrimination of perf counters between host and guests
    
    Conflicts:
            include/uapi/linux/kvm.h

commit a243c16d18be130b17cf1064e9115de73bfdff5a
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Tue Apr 23 10:12:37 2019 +0530

    KVM: arm64: Add capability to advertise ptrauth for guest
    
    This patch advertises the capability of two cpu feature called address
    pointer authentication and generic pointer authentication. These
    capabilities depend upon system support for pointer authentication and
    VHE mode.
    
    The current arm64 KVM partially implements pointer authentication and
    support of address/generic authentication are tied together. However,
    separate ABI requirements for both of them is added so that any future
    isolated implementation will not require any ABI changes.
    
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 028d0c604652..f0faf54f5857 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -101,6 +101,11 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_ARM_SVE:
 		r = system_supports_sve();
 		break;
+	case KVM_CAP_ARM_PTRAUTH_ADDRESS:
+	case KVM_CAP_ARM_PTRAUTH_GENERIC:
+		r = has_vhe() && system_supports_address_auth() &&
+				 system_supports_generic_auth();
+		break;
 	default:
 		r = 0;
 	}

commit a22fa321d13b0264976cbbc1d22f4c27c41d3642
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Tue Apr 23 10:12:36 2019 +0530

    KVM: arm64: Add userspace flag to enable pointer authentication
    
    Now that the building blocks of pointer authentication are present, lets
    add userspace flags KVM_ARM_VCPU_PTRAUTH_ADDRESS and
    KVM_ARM_VCPU_PTRAUTH_GENERIC. These flags will enable pointer
    authentication for the KVM guest on a per-vcpu basis through the ioctl
    KVM_ARM_VCPU_INIT.
    
    This features will allow the KVM guest to allow the handling of
    pointer authentication instructions or to treat them as undefined
    if not set.
    
    Necessary documentations are added to reflect the changes done.
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 3402543fdcd3..028d0c604652 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -221,6 +221,27 @@ static void kvm_vcpu_reset_sve(struct kvm_vcpu *vcpu)
 		memset(vcpu->arch.sve_state, 0, vcpu_sve_state_size(vcpu));
 }
 
+static int kvm_vcpu_enable_ptrauth(struct kvm_vcpu *vcpu)
+{
+	/* Support ptrauth only if the system supports these capabilities. */
+	if (!has_vhe())
+		return -EINVAL;
+
+	if (!system_supports_address_auth() ||
+	    !system_supports_generic_auth())
+		return -EINVAL;
+	/*
+	 * For now make sure that both address/generic pointer authentication
+	 * features are requested by the userspace together.
+	 */
+	if (!test_bit(KVM_ARM_VCPU_PTRAUTH_ADDRESS, vcpu->arch.features) ||
+	    !test_bit(KVM_ARM_VCPU_PTRAUTH_GENERIC, vcpu->arch.features))
+		return -EINVAL;
+
+	vcpu->arch.flags |= KVM_ARM64_GUEST_HAS_PTRAUTH;
+	return 0;
+}
+
 /**
  * kvm_reset_vcpu - sets core registers and sys_regs to reset value
  * @vcpu: The VCPU pointer
@@ -261,6 +282,12 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 		kvm_vcpu_reset_sve(vcpu);
 	}
 
+	if (test_bit(KVM_ARM_VCPU_PTRAUTH_ADDRESS, vcpu->arch.features) ||
+	    test_bit(KVM_ARM_VCPU_PTRAUTH_GENERIC, vcpu->arch.features)) {
+		if (kvm_vcpu_enable_ptrauth(vcpu))
+			goto out;
+	}
+
 	switch (vcpu->arch.target) {
 	default:
 		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {

commit 92e68b2b1ba004be55b2094fb8b5c11d0b24d11d
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Apr 10 17:17:37 2019 +0100

    KVM: arm/arm64: Clean up vcpu finalization function parameter naming
    
    Currently, the internal vcpu finalization functions use a different
    name ("what") for the feature parameter than the name ("feature")
    used in the documentation.
    
    To avoid future confusion, this patch converts everything to use
    the name "feature" consistently.
    
    No functional change.
    
    Suggested-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 8847f389f56d..3402543fdcd3 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -186,9 +186,9 @@ static int kvm_vcpu_finalize_sve(struct kvm_vcpu *vcpu)
 	return 0;
 }
 
-int kvm_arm_vcpu_finalize(struct kvm_vcpu *vcpu, int what)
+int kvm_arm_vcpu_finalize(struct kvm_vcpu *vcpu, int feature)
 {
-	switch (what) {
+	switch (feature) {
 	case KVM_ARM_VCPU_SVE:
 		if (!vcpu_has_sve(vcpu))
 			return -EINVAL;

commit a3be836df7cb777fa8ecbfd662224bfe0394f771
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Apr 12 15:30:58 2019 +0100

    KVM: arm/arm64: Demote kvm_arm_init_arch_resources() to just set up SVE
    
    The introduction of kvm_arm_init_arch_resources() looks like
    premature factoring, since nothing else uses this hook yet and it
    is not clear what will use it in the future.
    
    For now, let's not pretend that this is a general thing:
    
    This patch simply renames the function to kvm_arm_init_sve(),
    retaining the arm stub version under the new name.
    
    Suggested-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f13378d0a0ad..8847f389f56d 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -110,7 +110,7 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 
 unsigned int kvm_sve_max_vl;
 
-int kvm_arm_init_arch_resources(void)
+int kvm_arm_init_sve(void)
 {
 	if (system_supports_sve()) {
 		kvm_sve_max_vl = sve_max_virtualisable_vl;

commit 555f3d03e7fb43a539050aee91337978da14a25e
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jan 15 12:21:22 2019 +0000

    KVM: arm64: Add a capability to advertise SVE support
    
    To provide a uniform way to check for KVM SVE support amongst other
    features, this patch adds a suitable capability KVM_CAP_ARM_SVE,
    and reports it as present when SVE is available.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 32c5ac0a3872..f13378d0a0ad 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -98,6 +98,9 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_ARM_VM_IPA_SIZE:
 		r = kvm_ipa_limit;
 		break;
+	case KVM_CAP_ARM_SVE:
+		r = system_supports_sve();
+		break;
 	default:
 		r = 0;
 	}

commit 9a3cdf26e3363ec5460ebe20c508114fa63bcf26
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Feb 28 18:56:50 2019 +0000

    KVM: arm64/sve: Allow userspace to enable SVE for vcpus
    
    Now that all the pieces are in place, this patch offers a new flag
    KVM_ARM_VCPU_SVE that userspace can pass to KVM_ARM_VCPU_INIT to
    turn on SVE for the guest, on a per-vcpu basis.
    
    As part of this, support for initialisation and reset of the SVE
    vector length set and registers is added in the appropriate places,
    as well as finally setting the KVM_ARM64_GUEST_HAS_SVE vcpu flag,
    to turn on the SVE support code.
    
    Allocation of the SVE register storage in vcpu->arch.sve_state is
    deferred until the SVE configuration is finalized, by which time
    the size of the registers is known.
    
    Setting the vector lengths supported by the vcpu is considered
    configuration of the emulated hardware rather than runtime
    configuration, so no support is offered for changing the vector
    lengths available to an existing vcpu across reset.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index e7f9c06fdbbb..32c5ac0a3872 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -20,10 +20,12 @@
  */
 
 #include <linux/errno.h>
+#include <linux/kernel.h>
 #include <linux/kvm_host.h>
 #include <linux/kvm.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/slab.h>
+#include <linux/string.h>
 #include <linux/types.h>
 
 #include <kvm/arm_arch_timer.h>
@@ -37,6 +39,7 @@
 #include <asm/kvm_coproc.h>
 #include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>
+#include <asm/virt.h>
 
 /* Maximum phys_shift supported for any VM on this host */
 static u32 kvm_ipa_limit;
@@ -130,6 +133,27 @@ int kvm_arm_init_arch_resources(void)
 	return 0;
 }
 
+static int kvm_vcpu_enable_sve(struct kvm_vcpu *vcpu)
+{
+	if (!system_supports_sve())
+		return -EINVAL;
+
+	/* Verify that KVM startup enforced this when SVE was detected: */
+	if (WARN_ON(!has_vhe()))
+		return -EINVAL;
+
+	vcpu->arch.sve_max_vl = kvm_sve_max_vl;
+
+	/*
+	 * Userspace can still customize the vector lengths by writing
+	 * KVM_REG_ARM64_SVE_VLS.  Allocation is deferred until
+	 * kvm_arm_vcpu_finalize(), which freezes the configuration.
+	 */
+	vcpu->arch.flags |= KVM_ARM64_GUEST_HAS_SVE;
+
+	return 0;
+}
+
 /*
  * Finalize vcpu's maximum SVE vector length, allocating
  * vcpu->arch.sve_state as necessary.
@@ -188,13 +212,20 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 	kfree(vcpu->arch.sve_state);
 }
 
+static void kvm_vcpu_reset_sve(struct kvm_vcpu *vcpu)
+{
+	if (vcpu_has_sve(vcpu))
+		memset(vcpu->arch.sve_state, 0, vcpu_sve_state_size(vcpu));
+}
+
 /**
  * kvm_reset_vcpu - sets core registers and sys_regs to reset value
  * @vcpu: The VCPU pointer
  *
  * This function finds the right table above and sets the registers on
  * the virtual CPU struct to their architecturally defined reset
- * values.
+ * values, except for registers whose reset is deferred until
+ * kvm_arm_vcpu_finalize().
  *
  * Note: This function can be called from two paths: The KVM_ARM_VCPU_INIT
  * ioctl or as part of handling a request issued by another VCPU in the PSCI
@@ -217,6 +248,16 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	if (loaded)
 		kvm_arch_vcpu_put(vcpu);
 
+	if (!kvm_arm_vcpu_sve_finalized(vcpu)) {
+		if (test_bit(KVM_ARM_VCPU_SVE, vcpu->arch.features)) {
+			ret = kvm_vcpu_enable_sve(vcpu);
+			if (ret)
+				goto out;
+		}
+	} else {
+		kvm_vcpu_reset_sve(vcpu);
+	}
+
 	switch (vcpu->arch.target) {
 	default:
 		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {

commit 9033bba4b53527b57bec217509a967a25cb19357
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Feb 28 18:46:44 2019 +0000

    KVM: arm64/sve: Add pseudo-register for the guest's vector lengths
    
    This patch adds a new pseudo-register KVM_REG_ARM64_SVE_VLS to
    allow userspace to set and query the set of vector lengths visible
    to the guest.
    
    In the future, multiple register slices per SVE register may be
    visible through the ioctl interface.  Once the set of slices has
    been determined we would not be able to allow the vector length set
    to be changed any more, in order to avoid userspace seeing
    inconsistent sets of registers.  For this reason, this patch adds
    support for explicit finalization of the SVE configuration via the
    KVM_ARM_VCPU_FINALIZE ioctl.
    
    Finalization is the proper place to allocate the SVE register state
    storage in vcpu->arch.sve_state, so this patch adds that as
    appropriate.  The data is freed via kvm_arch_vcpu_uninit(), which
    was previously a no-op on arm64.
    
    To simplify the logic for determining what vector lengths can be
    supported, some code is added to KVM init to work this out, in the
    kvm_arm_init_arch_resources() hook.
    
    The KVM_REG_ARM64_SVE_VLS pseudo-register is not exposed yet.
    Subsequent patches will allow SVE to be turned on for guest vcpus,
    making it visible.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Tested-by: zhang.lei <zhang.lei@jp.fujitsu.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f16a5f8ff2b4..e7f9c06fdbbb 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -23,11 +23,14 @@
 #include <linux/kvm_host.h>
 #include <linux/kvm.h>
 #include <linux/hw_breakpoint.h>
+#include <linux/slab.h>
+#include <linux/types.h>
 
 #include <kvm/arm_arch_timer.h>
 
 #include <asm/cpufeature.h>
 #include <asm/cputype.h>
+#include <asm/fpsimd.h>
 #include <asm/ptrace.h>
 #include <asm/kvm_arm.h>
 #include <asm/kvm_asm.h>
@@ -99,6 +102,92 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	return r;
 }
 
+unsigned int kvm_sve_max_vl;
+
+int kvm_arm_init_arch_resources(void)
+{
+	if (system_supports_sve()) {
+		kvm_sve_max_vl = sve_max_virtualisable_vl;
+
+		/*
+		 * The get_sve_reg()/set_sve_reg() ioctl interface will need
+		 * to be extended with multiple register slice support in
+		 * order to support vector lengths greater than
+		 * SVE_VL_ARCH_MAX:
+		 */
+		if (WARN_ON(kvm_sve_max_vl > SVE_VL_ARCH_MAX))
+			kvm_sve_max_vl = SVE_VL_ARCH_MAX;
+
+		/*
+		 * Don't even try to make use of vector lengths that
+		 * aren't available on all CPUs, for now:
+		 */
+		if (kvm_sve_max_vl < sve_max_vl)
+			pr_warn("KVM: SVE vector length for guests limited to %u bytes\n",
+				kvm_sve_max_vl);
+	}
+
+	return 0;
+}
+
+/*
+ * Finalize vcpu's maximum SVE vector length, allocating
+ * vcpu->arch.sve_state as necessary.
+ */
+static int kvm_vcpu_finalize_sve(struct kvm_vcpu *vcpu)
+{
+	void *buf;
+	unsigned int vl;
+
+	vl = vcpu->arch.sve_max_vl;
+
+	/*
+	 * Resposibility for these properties is shared between
+	 * kvm_arm_init_arch_resources(), kvm_vcpu_enable_sve() and
+	 * set_sve_vls().  Double-check here just to be sure:
+	 */
+	if (WARN_ON(!sve_vl_valid(vl) || vl > sve_max_virtualisable_vl ||
+		    vl > SVE_VL_ARCH_MAX))
+		return -EIO;
+
+	buf = kzalloc(SVE_SIG_REGS_SIZE(sve_vq_from_vl(vl)), GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	vcpu->arch.sve_state = buf;
+	vcpu->arch.flags |= KVM_ARM64_VCPU_SVE_FINALIZED;
+	return 0;
+}
+
+int kvm_arm_vcpu_finalize(struct kvm_vcpu *vcpu, int what)
+{
+	switch (what) {
+	case KVM_ARM_VCPU_SVE:
+		if (!vcpu_has_sve(vcpu))
+			return -EINVAL;
+
+		if (kvm_arm_vcpu_sve_finalized(vcpu))
+			return -EPERM;
+
+		return kvm_vcpu_finalize_sve(vcpu);
+	}
+
+	return -EINVAL;
+}
+
+bool kvm_arm_vcpu_is_finalized(struct kvm_vcpu *vcpu)
+{
+	if (vcpu_has_sve(vcpu) && !kvm_arm_vcpu_sve_finalized(vcpu))
+		return false;
+
+	return true;
+}
+
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
+{
+	kfree(vcpu->arch.sve_state);
+}
+
 /**
  * kvm_reset_vcpu - sets core registers and sys_regs to reset value
  * @vcpu: The VCPU pointer

commit ebff0b0e3d3c862c16c487959db5e0d879632559
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Mar 4 17:37:44 2019 +0000

    KVM: arm64: Reset the PMU in preemptible context
    
    We've become very cautious to now always reset the vcpu when nothing
    is loaded on the physical CPU. To do so, we now disable preemption
    and do a kvm_arch_vcpu_put() to make sure we have all the state
    in memory (and that it won't be loaded behind out back).
    
    This now causes issues with resetting the PMU, which calls into perf.
    Perf itself uses mutexes, which clashes with the lack of preemption.
    It is worth realizing that the PMU is fully emulated, and that
    no PMU state is ever loaded on the physical CPU. This means we can
    perfectly reset the PMU outside of the non-preemptible section.
    
    Fixes: e761a927bc9a ("KVM: arm/arm64: Reset the VCPU without preemption and vcpu state loaded")
    Reported-by: Julien Grall <julien.grall@arm.com>
    Tested-by: Julien Grall <julien.grall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f16a5f8ff2b4..e2a0500cd7a2 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -123,6 +123,9 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	int ret = -EINVAL;
 	bool loaded;
 
+	/* Reset PMU outside of the non-preemptible section */
+	kvm_pmu_vcpu_reset(vcpu);
+
 	preempt_disable();
 	loaded = (vcpu->cpu != -1);
 	if (loaded)
@@ -170,9 +173,6 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 		vcpu->arch.reset_state.reset = false;
 	}
 
-	/* Reset PMU */
-	kvm_pmu_vcpu_reset(vcpu);
-
 	/* Default workaround setup is enabled (if supported) */
 	if (kvm_arm_have_ssbd() == KVM_SSBD_KERNEL)
 		vcpu->arch.workaround_flags |= VCPU_WORKAROUND_2_FLAG;

commit 358b28f09f0ab074d781df72b8a671edb1547789
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 20 11:36:07 2018 +0000

    arm/arm64: KVM: Allow a VCPU to fully reset itself
    
    The current kvm_psci_vcpu_on implementation will directly try to
    manipulate the state of the VCPU to reset it.  However, since this is
    not done on the thread that runs the VCPU, we can end up in a strangely
    corrupted state when the source and target VCPUs are running at the same
    time.
    
    Fix this by factoring out all reset logic from the PSCI implementation
    and forwarding the required information along with a request to the
    target VCPU.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f21a2a575939..f16a5f8ff2b4 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -32,6 +32,7 @@
 #include <asm/kvm_arm.h>
 #include <asm/kvm_asm.h>
 #include <asm/kvm_coproc.h>
+#include <asm/kvm_emulate.h>
 #include <asm/kvm_mmu.h>
 
 /* Maximum phys_shift supported for any VM on this host */
@@ -146,6 +147,29 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset system registers */
 	kvm_reset_sys_regs(vcpu);
 
+	/*
+	 * Additional reset state handling that PSCI may have imposed on us.
+	 * Must be done after all the sys_reg reset.
+	 */
+	if (vcpu->arch.reset_state.reset) {
+		unsigned long target_pc = vcpu->arch.reset_state.pc;
+
+		/* Gracefully handle Thumb2 entry point */
+		if (vcpu_mode_is_32bit(vcpu) && (target_pc & 1)) {
+			target_pc &= ~1UL;
+			vcpu_set_thumb(vcpu);
+		}
+
+		/* Propagate caller endianness */
+		if (vcpu->arch.reset_state.be)
+			kvm_vcpu_set_be(vcpu);
+
+		*vcpu_pc(vcpu) = target_pc;
+		vcpu_set_reg(vcpu, 0, vcpu->arch.reset_state.r0);
+
+		vcpu->arch.reset_state.reset = false;
+	}
+
 	/* Reset PMU */
 	kvm_pmu_vcpu_reset(vcpu);
 

commit e761a927bc9a7ee6ceb7c4f63d5922dbced87f0d
Author: Christoffer Dall <christoffer.dall@arm.com>
Date:   Thu Dec 20 12:44:05 2018 +0100

    KVM: arm/arm64: Reset the VCPU without preemption and vcpu state loaded
    
    We have two ways to reset a vcpu:
    - either through VCPU_INIT
    - or through a PSCI_ON call
    
    The first one is easy to reason about. The second one is implemented
    in a more bizarre way, as it is the vcpu that handles PSCI_ON that
    resets the vcpu that is being powered-on. As we need to turn the logic
    around and have the target vcpu to reset itself, we must take some
    preliminary steps.
    
    Resetting the VCPU state modifies the system register state in memory,
    but this may interact with vcpu_load/vcpu_put if running with preemption
    disabled, which in turn may lead to corrupted system register state.
    
    Address this by disabling preemption and doing put/load if required
    around the reset logic.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index b72a3dd56204..f21a2a575939 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -105,16 +105,33 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
  * This function finds the right table above and sets the registers on
  * the virtual CPU struct to their architecturally defined reset
  * values.
+ *
+ * Note: This function can be called from two paths: The KVM_ARM_VCPU_INIT
+ * ioctl or as part of handling a request issued by another VCPU in the PSCI
+ * handling code.  In the first case, the VCPU will not be loaded, and in the
+ * second case the VCPU will be loaded.  Because this function operates purely
+ * on the memory-backed valus of system registers, we want to do a full put if
+ * we were loaded (handling a request) and load the values back at the end of
+ * the function.  Otherwise we leave the state alone.  In both cases, we
+ * disable preemption around the vcpu reset as we would otherwise race with
+ * preempt notifiers which also call put/load.
  */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
 	const struct kvm_regs *cpu_reset;
+	int ret = -EINVAL;
+	bool loaded;
+
+	preempt_disable();
+	loaded = (vcpu->cpu != -1);
+	if (loaded)
+		kvm_arch_vcpu_put(vcpu);
 
 	switch (vcpu->arch.target) {
 	default:
 		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {
 			if (!cpu_has_32bit_el1())
-				return -EINVAL;
+				goto out;
 			cpu_reset = &default_regs_reset32;
 		} else {
 			cpu_reset = &default_regs_reset;
@@ -137,7 +154,12 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 		vcpu->arch.workaround_flags |= VCPU_WORKAROUND_2_FLAG;
 
 	/* Reset timer */
-	return kvm_timer_vcpu_reset(vcpu);
+	ret = kvm_timer_vcpu_reset(vcpu);
+out:
+	if (loaded)
+		kvm_arch_vcpu_load(vcpu, smp_processor_id());
+	preempt_enable();
+	return ret;
 }
 
 void kvm_set_ipa_limit(void)

commit 58bf437ff64eac8aca606e42d7e4623e40b61fa1
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Sat Oct 13 00:12:49 2018 +0800

    arm/arm64: KVM: Enable 32 bits kvm vcpu events support
    
    The commit 539aee0edb9f ("KVM: arm64: Share the parts of
    get/set events useful to 32bit") shares the get/set events
    helper for arm64 and arm32, but forgot to share the cap
    extension code.
    
    User space will check whether KVM supports vcpu events by
    checking the KVM_CAP_VCPU_EVENTS extension
    
    Acked-by: James Morse <james.morse@arm.com>
    Reviewed-by : Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 337d2fbc2f06..b72a3dd56204 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -86,7 +86,6 @@ int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	case KVM_CAP_SET_GUEST_DEBUG:
 	case KVM_CAP_VCPU_ATTRIBUTES:
-	case KVM_CAP_VCPU_EVENTS:
 		r = 1;
 		break;
 	case KVM_CAP_ARM_VM_IPA_SIZE:

commit 375bdd3b5d4f7cf146f0df1488b4671b141dd799
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Sat Oct 13 00:12:48 2018 +0800

    arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
    
    Rename kvm_arch_dev_ioctl_check_extension() to
    kvm_arch_vm_ioctl_check_extension(), because it does
    not have any relationship with device.
    
    Renaming this function can make code readable.
    
    Cc: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index aa806d582552..337d2fbc2f06 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -59,12 +59,12 @@ static bool cpu_has_32bit_el1(void)
 }
 
 /**
- * kvm_arch_dev_ioctl_check_extension
+ * kvm_arch_vm_ioctl_check_extension
  *
  * We currently assume that the number of HW registers is uniform
  * across all CPUs (see cpuinfo_sanity_check).
  */
-int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
+int kvm_arch_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r;
 

commit bca607ebc76af9540e4aad5b2241a7323354be43
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 1 13:40:36 2018 +0100

    KVM: arm/arm64: Rename kvm_arm_config_vm to kvm_arm_setup_stage2
    
    VM tends to be a very overloaded term in KVM, so let's keep it
    to describe the virtual machine. For the virtual memory setup,
    let's use the "stage2" suffix.
    
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 95f28d5950e0..aa806d582552 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -189,7 +189,7 @@ void kvm_set_ipa_limit(void)
  * all CPUs, as it is safe to run with or without the feature and
  * the bit is RES0 on CPUs that don't support it.
  */
-int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
+int kvm_arm_setup_stage2(struct kvm *kvm, unsigned long type)
 {
 	u64 vtcr = VTCR_EL2_FLAGS;
 	u32 parange, phys_shift;

commit 233a7cb235318223df8133235383f4c595c654c1
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:54 2018 +0100

    kvm: arm64: Allow tuning the physical address size for VM
    
    Allow specifying the physical address size limit for a new
    VM via the kvm_type argument for the KVM_CREATE_VM ioctl. This
    allows us to finalise the stage2 page table as early as possible
    and hence perform the right checks on the memory slots
    without complication. The size is encoded as Log2(PA_Size) in
    bits[7:0] of the type field. For backward compatibility the
    value 0 is reserved and implies 40bits. Also, lift the limit
    of the IPA to host limit and allow lower IPA sizes (e.g, 32).
    
    The userspace could check the extension KVM_CAP_ARM_VM_IPA_SIZE
    for the availability of this feature. The cap check returns the
    maximum limit for the physical address shift supported by the host.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f156e45760bc..95f28d5950e0 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -89,6 +89,9 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_VCPU_EVENTS:
 		r = 1;
 		break;
+	case KVM_CAP_ARM_VM_IPA_SIZE:
+		r = kvm_ipa_limit;
+		break;
 	default:
 		r = 0;
 	}
@@ -192,17 +195,23 @@ int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
 	u32 parange, phys_shift;
 	u8 lvls;
 
-	if (type)
+	if (type & ~KVM_VM_TYPE_ARM_IPA_SIZE_MASK)
 		return -EINVAL;
 
+	phys_shift = KVM_VM_TYPE_ARM_IPA_SIZE(type);
+	if (phys_shift) {
+		if (phys_shift > kvm_ipa_limit ||
+		    phys_shift < 32)
+			return -EINVAL;
+	} else {
+		phys_shift = KVM_PHYS_SHIFT;
+	}
+
 	parange = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1) & 7;
 	if (parange > ID_AA64MMFR0_PARANGE_MAX)
 		parange = ID_AA64MMFR0_PARANGE_MAX;
 	vtcr |= parange << VTCR_EL2_PS_SHIFT;
 
-	phys_shift = id_aa64mmfr0_parange_to_phys_shift(parange);
-	if (phys_shift > KVM_PHYS_SHIFT)
-		phys_shift = KVM_PHYS_SHIFT;
 	vtcr |= VTCR_EL2_T0SZ(phys_shift);
 	/*
 	 * Use a minimum 2 level page table to prevent splitting

commit 58b3efc820acd3219e89ff014e93346a734229b8
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:53 2018 +0100

    kvm: arm64: Limit the minimum number of page table levels
    
    Since we are about to remove the lower limit on the IPA size,
    make sure that we do not go to 1 level page table (e.g, with
    32bit IPA on 64K host with concatenation) to avoid splitting
    the host PMD huge pages at stage2.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 96b3f50101bc..f156e45760bc 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -190,6 +190,7 @@ int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
 {
 	u64 vtcr = VTCR_EL2_FLAGS;
 	u32 parange, phys_shift;
+	u8 lvls;
 
 	if (type)
 		return -EINVAL;
@@ -203,7 +204,14 @@ int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
 	if (phys_shift > KVM_PHYS_SHIFT)
 		phys_shift = KVM_PHYS_SHIFT;
 	vtcr |= VTCR_EL2_T0SZ(phys_shift);
-	vtcr |= VTCR_EL2_LVLS_TO_SL0(stage2_pgtable_levels(phys_shift));
+	/*
+	 * Use a minimum 2 level page table to prevent splitting
+	 * host PMD huge pages at stage2.
+	 */
+	lvls = stage2_pgtable_levels(phys_shift);
+	if (lvls < 2)
+		lvls = 2;
+	vtcr |= VTCR_EL2_LVLS_TO_SL0(lvls);
 
 	/*
 	 * Enable the Hardware Access Flag management, unconditionally

commit 0f62f0e95be29200ab2ab98ca870e22c9b148dfa
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:52 2018 +0100

    kvm: arm64: Set a limit on the IPA size
    
    So far we have restricted the IPA size of the VM to the default
    value (40bits). Now that we can manage the IPA size per VM and
    support dynamic stage2 page tables, we can allow VMs to have
    larger IPA. This patch introduces a the maximum IPA size
    supported on the host. This is decided by the following factors :
    
     1) Maximum PARange supported by the CPUs - This can be inferred
        from the system wide safe value.
     2) Maximum PA size supported by the host kernel (48 vs 52)
     3) Number of levels in the host page table (as we base our
        stage2 tables on the host table helpers).
    
    Since the stage2 page table code is dependent on the stage1
    page table, we always ensure that :
    
      Number of Levels at Stage1 >= Number of Levels at Stage2
    
    So we limit the IPA to make sure that the above condition
    is satisfied. This will affect the following combinations
    of VA_BITS and IPA for different page sizes.
    
      Host configuration | Unsupported IPA ranges
      39bit VA, 4K       | [44, 48]
      36bit VA, 16K      | [41, 48]
      42bit VA, 64K      | [47, 52]
    
    Supporting the above combinations need independent stage2
    page table manipulation code, which would need substantial
    changes. We could purse the solution independently and
    switch the page table code once we have it ready.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 2bf41e007390..96b3f50101bc 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -34,6 +34,9 @@
 #include <asm/kvm_coproc.h>
 #include <asm/kvm_mmu.h>
 
+/* Maximum phys_shift supported for any VM on this host */
+static u32 kvm_ipa_limit;
+
 /*
  * ARMv8 Reset Values
  */
@@ -135,6 +138,46 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	return kvm_timer_vcpu_reset(vcpu);
 }
 
+void kvm_set_ipa_limit(void)
+{
+	unsigned int ipa_max, pa_max, va_max, parange;
+
+	parange = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1) & 0x7;
+	pa_max = id_aa64mmfr0_parange_to_phys_shift(parange);
+
+	/* Clamp the IPA limit to the PA size supported by the kernel */
+	ipa_max = (pa_max > PHYS_MASK_SHIFT) ? PHYS_MASK_SHIFT : pa_max;
+	/*
+	 * Since our stage2 table is dependent on the stage1 page table code,
+	 * we must always honor the following condition:
+	 *
+	 *  Number of levels in Stage1 >= Number of levels in Stage2.
+	 *
+	 * So clamp the ipa limit further down to limit the number of levels.
+	 * Since we can concatenate upto 16 tables at entry level, we could
+	 * go upto 4bits above the maximum VA addressible with the current
+	 * number of levels.
+	 */
+	va_max = PGDIR_SHIFT + PAGE_SHIFT - 3;
+	va_max += 4;
+
+	if (va_max < ipa_max)
+		ipa_max = va_max;
+
+	/*
+	 * If the final limit is lower than the real physical address
+	 * limit of the CPUs, report the reason.
+	 */
+	if (ipa_max < pa_max)
+		pr_info("kvm: Limiting the IPA size due to kernel %s Address limit\n",
+			(va_max < pa_max) ? "Virtual" : "Physical");
+
+	WARN(ipa_max < KVM_PHYS_SHIFT,
+	     "KVM IPA limit (%d bit) is smaller than default size\n", ipa_max);
+	kvm_ipa_limit = ipa_max;
+	kvm_info("IPA Size Limit: %dbits\n", kvm_ipa_limit);
+}
+
 /*
  * Configure the VTCR_EL2 for this VM. The VTCR value is common
  * across all the physical CPUs on the system. We use system wide

commit 13ac4bbcc457d3925b4031cc70e3031fd8b9c3b7
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:49 2018 +0100

    kvm: arm64: Switch to per VM IPA limit
    
    Now that we can manage the stage2 page table per VM, switch the
    configuration details to per VM instance. The VTCR is updated
    with the values specific to the VM based on the configuration.
    We store the IPA size and the number of stage2 page table levels
    for the guest already in VTCR. Decode it back from the vtcr
    field wherever we need it.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 1ced1e37374e..2bf41e007390 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -160,7 +160,7 @@ int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
 	if (phys_shift > KVM_PHYS_SHIFT)
 		phys_shift = KVM_PHYS_SHIFT;
 	vtcr |= VTCR_EL2_T0SZ(phys_shift);
-	vtcr |= VTCR_EL2_LVLS_TO_SL0(kvm_stage2_levels(kvm));
+	vtcr |= VTCR_EL2_LVLS_TO_SL0(stage2_pgtable_levels(phys_shift));
 
 	/*
 	 * Enable the Hardware Access Flag management, unconditionally

commit 7e8130456e067f49693fdcc25b5ba242a9c5568b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:48 2018 +0100

    kvm: arm64: Configure VTCR_EL2.SL0 per VM
    
    VTCR_EL2 holds the following key stage2 translation table
    parameters:
      SL0  - Entry level in the page table lookup.
      T0SZ - Denotes the size of the memory addressed by the table.
    
    We have been using fixed values for the SL0 depending on the
    page size as we have a fixed IPA size. But since we are about
    to make it dynamic, we need to calculate the SL0 at runtime
    per VM. This patch adds a helper to compute the value of SL0
    for a VM based on the IPA size.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 616120c4176b..1ced1e37374e 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -160,6 +160,7 @@ int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
 	if (phys_shift > KVM_PHYS_SHIFT)
 		phys_shift = KVM_PHYS_SHIFT;
 	vtcr |= VTCR_EL2_T0SZ(phys_shift);
+	vtcr |= VTCR_EL2_LVLS_TO_SL0(kvm_stage2_levels(kvm));
 
 	/*
 	 * Enable the Hardware Access Flag management, unconditionally

commit 7665f3a8491b0ed3c6f65c0bc3a5424ea8f87731
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:43 2018 +0100

    kvm: arm64: Configure VTCR_EL2 per VM
    
    Add support for setting the VTCR_EL2 per VM, rather than hard
    coding a value at boot time per CPU. This would allow us to tune
    the stage2 page table parameters per VM in the later changes.
    
    We compute the VTCR fields based on the system wide sanitised
    feature registers, except for the hardware management of Access
    Flags (VTCR_EL2.HA). It is fine to run a system with a mix of
    CPUs that may or may not update the page table Access Flags.
    Since the bit is RES0 on CPUs that don't support it, the bit
    should be ignored on them.
    
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index b0c07dab5cb3..616120c4176b 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -26,6 +26,7 @@
 
 #include <kvm/arm_arch_timer.h>
 
+#include <asm/cpufeature.h>
 #include <asm/cputype.h>
 #include <asm/ptrace.h>
 #include <asm/kvm_arm.h>
@@ -134,9 +135,43 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	return kvm_timer_vcpu_reset(vcpu);
 }
 
+/*
+ * Configure the VTCR_EL2 for this VM. The VTCR value is common
+ * across all the physical CPUs on the system. We use system wide
+ * sanitised values to fill in different fields, except for Hardware
+ * Management of Access Flags. HA Flag is set unconditionally on
+ * all CPUs, as it is safe to run with or without the feature and
+ * the bit is RES0 on CPUs that don't support it.
+ */
 int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
 {
+	u64 vtcr = VTCR_EL2_FLAGS;
+	u32 parange, phys_shift;
+
 	if (type)
 		return -EINVAL;
+
+	parange = read_sanitised_ftr_reg(SYS_ID_AA64MMFR0_EL1) & 7;
+	if (parange > ID_AA64MMFR0_PARANGE_MAX)
+		parange = ID_AA64MMFR0_PARANGE_MAX;
+	vtcr |= parange << VTCR_EL2_PS_SHIFT;
+
+	phys_shift = id_aa64mmfr0_parange_to_phys_shift(parange);
+	if (phys_shift > KVM_PHYS_SHIFT)
+		phys_shift = KVM_PHYS_SHIFT;
+	vtcr |= VTCR_EL2_T0SZ(phys_shift);
+
+	/*
+	 * Enable the Hardware Access Flag management, unconditionally
+	 * on all CPUs. The features is RES0 on CPUs without the support
+	 * and must be ignored by the CPUs.
+	 */
+	vtcr |= VTCR_EL2_HA;
+
+	/* Set the vmid bits */
+	vtcr |= (kvm_get_vmid_bits() == 16) ?
+		VTCR_EL2_VS_16BIT :
+		VTCR_EL2_VS_8BIT;
+	kvm->arch.vtcr = vtcr;
 	return 0;
 }

commit 5b6c6742b5350a6fb5c631fb99a6bc046a62739c
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Sep 26 17:32:42 2018 +0100

    kvm: arm/arm64: Allow arch specific configurations for VM
    
    Allow the arch backends to perform VM specific initialisation.
    This will be later used to handle IPA size configuration and per-VM
    VTCR configuration on arm64.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <cdall@kernel.org>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index e37c78bbe1ca..b0c07dab5cb3 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -133,3 +133,10 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset timer */
 	return kvm_timer_vcpu_reset(vcpu);
 }
+
+int kvm_arm_config_vm(struct kvm *kvm, unsigned long type)
+{
+	if (type)
+		return -EINVAL;
+	return 0;
+}

commit 631989303b06b8fdb15ec3b88aee2d25e80d4cec
Merge: ad1d69735878 976d34e2dab1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Aug 22 14:07:56 2018 +0200

    Merge tag 'kvmarm-for-v4.19' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 4.19
    
    - Support for Group0 interrupts in guests
    - Cache management optimizations for ARMv8.4 systems
    - Userspace interface for RAS, allowing error retrival and injection
    - Fault path optimization
    - Emulated physical timer fixes
    - Random cleanups

commit be26b3a73413c2ebf14d5e76a66ad964e6458080
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Thu Jul 19 16:24:23 2018 +0100

    arm64: KVM: export the capability to set guest SError syndrome
    
    For the arm64 RAS Extension, user space can inject a virtual-SError
    with specified ESR. So user space needs to know whether KVM support
    to inject such SError, this interface adds this query for this capability.
    
    KVM will check whether system support RAS Extension, if supported, KVM
    returns true to user space, otherwise returns false.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    [expanded documentation wording]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index a3db01a28062..067c6ba969bd 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -77,6 +77,9 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_ARM_PMU_V3:
 		r = kvm_arm_support_pmu_v3();
 		break;
+	case KVM_CAP_ARM_INJECT_SERROR_ESR:
+		r = cpus_have_const_cap(ARM64_HAS_RAS_EXTN);
+		break;
 	case KVM_CAP_SET_GUEST_DEBUG:
 	case KVM_CAP_VCPU_ATTRIBUTES:
 	case KVM_CAP_VCPU_EVENTS:

commit b7b27facc7b50a5fce0afaa3df56157136ce181a
Author: Dongjiu Geng <gengdongjiu@huawei.com>
Date:   Thu Jul 19 16:24:22 2018 +0100

    arm/arm64: KVM: Add KVM_GET/SET_VCPU_EVENTS
    
    For the migrating VMs, user space may need to know the exception
    state. For example, in the machine A, KVM make an SError pending,
    when migrate to B, KVM also needs to pend an SError.
    
    This new IOCTL exports user-invisible states related to SError.
    Together with appropriate user space changes, user space can get/set
    the SError exception state to do migrate/snapshot/suspend.
    
    Signed-off-by: Dongjiu Geng <gengdongjiu@huawei.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    [expanded documentation wording]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index a74311beda35..a3db01a28062 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -79,6 +79,7 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 	case KVM_CAP_SET_GUEST_DEBUG:
 	case KVM_CAP_VCPU_ATTRIBUTES:
+	case KVM_CAP_VCPU_EVENTS:
 		r = 1;
 		break;
 	default:

commit 256c0960b7b6453dc90a4e879da52ab76b4037f9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 5 15:16:53 2018 +0100

    kvm/arm: use PSR_AA32 definitions
    
    Some code cares about the SPSR_ELx format for exceptions taken from
    AArch32 to inspect or manipulate the SPSR_ELx value, which is already in
    the SPSR_ELx format, and not in the AArch32 PSR format.
    
    To separate these from cases where we care about the AArch32 PSR format,
    migrate these cases to use the PSR_AA32_* definitions rather than
    COMPAT_PSR_*.
    
    There should be no functional change as a result of this patch.
    
    Note that arm64 KVM does not support a compat KVM API, and always uses
    the SPSR_ELx format, even for AArch32 guests.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index a74311beda35..4e4aedaf7ab7 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -42,8 +42,8 @@ static const struct kvm_regs default_regs_reset = {
 };
 
 static const struct kvm_regs default_regs_reset32 = {
-	.regs.pstate = (COMPAT_PSR_MODE_SVC | COMPAT_PSR_A_BIT |
-			COMPAT_PSR_I_BIT | COMPAT_PSR_F_BIT),
+	.regs.pstate = (PSR_AA32_MODE_SVC | PSR_AA32_A_BIT |
+			PSR_AA32_I_BIT | PSR_AA32_F_BIT),
 };
 
 static bool cpu_has_32bit_el1(void)

commit 5d81f7dc9bca4f4963092433e27b508cbe524a32
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:18 2018 +0100

    arm64: KVM: Add ARCH_WORKAROUND_2 discovery through ARCH_FEATURES_FUNC_ID
    
    Now that all our infrastructure is in place, let's expose the
    availability of ARCH_WORKAROUND_2 to guests. We take this opportunity
    to tidy up a couple of SMCCC constants.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 3256b9228e75..a74311beda35 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -122,6 +122,10 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset PMU */
 	kvm_pmu_vcpu_reset(vcpu);
 
+	/* Default workaround setup is enabled (if supported) */
+	if (kvm_arm_have_ssbd() == KVM_SSBD_KERNEL)
+		vcpu->arch.workaround_flags |= VCPU_WORKAROUND_2_FLAG;
+
 	/* Reset timer */
 	return kvm_timer_vcpu_reset(vcpu);
 }

commit 85e69ad7f2cc6dd829987a70cf32785b1d8c8b27
Author: Christoffer Dall <cdall@linaro.org>
Date:   Tue May 2 20:14:06 2017 +0200

    KVM: arm/arm64: Move timer IRQ default init to arch_timer.c
    
    We currently initialize the arch timer IRQ numbers from the reset code,
    presumably because we once intended to model multiple CPU or SoC types
    from within the kernel and have hard-coded reset values in the reset
    code.
    
    As we are moving towards userspace being in charge of more fine-grained
    CPU emulation and stitching together the pieces needed to emulate a
    particular type of CPU, we should no longer have a tight coupling
    between resetting a VCPU and setting IRQ numbers.
    
    Therefore, move the logic to define and use the default IRQ numbers to
    the timer code and set the IRQ number immediately when creating the
    VCPU.
    
    Signed-off-by: Christoffer Dall <cdall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 561badf93de8..3256b9228e75 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -46,16 +46,6 @@ static const struct kvm_regs default_regs_reset32 = {
 			COMPAT_PSR_I_BIT | COMPAT_PSR_F_BIT),
 };
 
-static const struct kvm_irq_level default_ptimer_irq = {
-	.irq	= 30,
-	.level	= 1,
-};
-
-static const struct kvm_irq_level default_vtimer_irq = {
-	.irq	= 27,
-	.level	= 1,
-};
-
 static bool cpu_has_32bit_el1(void)
 {
 	u64 pfr0;
@@ -108,8 +98,6 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
  */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
-	const struct kvm_irq_level *cpu_vtimer_irq;
-	const struct kvm_irq_level *cpu_ptimer_irq;
 	const struct kvm_regs *cpu_reset;
 
 	switch (vcpu->arch.target) {
@@ -122,8 +110,6 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 			cpu_reset = &default_regs_reset;
 		}
 
-		cpu_vtimer_irq = &default_vtimer_irq;
-		cpu_ptimer_irq = &default_ptimer_irq;
 		break;
 	}
 
@@ -137,5 +123,5 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	kvm_pmu_vcpu_reset(vcpu);
 
 	/* Reset timer */
-	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq, cpu_ptimer_irq);
+	return kvm_timer_vcpu_reset(vcpu);
 }

commit 46823dd17c676d1e1830774e93be813dc3638d6c
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Mar 23 15:14:39 2017 +0000

    arm64: cpufeature: Make ID reg accessor naming less counterintuitive
    
    read_system_reg() can readily be confused with read_sysreg(),
    whereas these are really quite different in their meaning.
    
    This patches attempts to reduce the ambiguity be reserving "sysreg"
    for the actual system register accessors.
    
    read_system_reg() is instead renamed to read_sanitised_ftr_reg(),
    to make it more obvious that the Linux-defined sanitised feature
    register cache is being accessed here, not the underlying
    architectural system registers.
    
    cpufeature.c's internal __raw_read_system_reg() function is renamed
    in line with its actual purpose: a form of read_sysreg() that
    indexes on (non-compiletime-constant) encoding rather than symbolic
    register name.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index d9e9697de1b2..561badf93de8 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -60,7 +60,7 @@ static bool cpu_has_32bit_el1(void)
 {
 	u64 pfr0;
 
-	pfr0 = read_system_reg(SYS_ID_AA64PFR0_EL1);
+	pfr0 = read_sanitised_ftr_reg(SYS_ID_AA64PFR0_EL1);
 	return !!(pfr0 & 0x20);
 }
 

commit a91d18551e7b35e34a04b6fd199ca8568e7e9315
Author: Jintack Lim <jintack@cs.columbia.edu>
Date:   Fri Feb 3 10:20:03 2017 -0500

    KVM: arm/arm64: Initialize the emulated EL1 physical timer
    
    Initialize the emulated EL1 physical timer with the default irq number.
    
    Signed-off-by: Jintack Lim <jintack@cs.columbia.edu>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index e95d4f68bf54..d9e9697de1b2 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -46,6 +46,11 @@ static const struct kvm_regs default_regs_reset32 = {
 			COMPAT_PSR_I_BIT | COMPAT_PSR_F_BIT),
 };
 
+static const struct kvm_irq_level default_ptimer_irq = {
+	.irq	= 30,
+	.level	= 1,
+};
+
 static const struct kvm_irq_level default_vtimer_irq = {
 	.irq	= 27,
 	.level	= 1,
@@ -104,6 +109,7 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
 	const struct kvm_irq_level *cpu_vtimer_irq;
+	const struct kvm_irq_level *cpu_ptimer_irq;
 	const struct kvm_regs *cpu_reset;
 
 	switch (vcpu->arch.target) {
@@ -117,6 +123,7 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 		}
 
 		cpu_vtimer_irq = &default_vtimer_irq;
+		cpu_ptimer_irq = &default_ptimer_irq;
 		break;
 	}
 
@@ -130,5 +137,5 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	kvm_pmu_vcpu_reset(vcpu);
 
 	/* Reset timer */
-	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
+	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq, cpu_ptimer_irq);
 }

commit 2988509dd8a0e9c2b64192a46ec2fe8211af6d3c
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Wed Nov 2 11:55:34 2016 +0000

    ARM: KVM: Support vGICv3 ITS
    
    This patch allows to build and use vGICv3 ITS in 32-bit mode.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 5bc460884639..e95d4f68bf54 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -86,12 +86,6 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_VCPU_ATTRIBUTES:
 		r = 1;
 		break;
-	case KVM_CAP_MSI_DEVID:
-		if (!kvm)
-			r = -EINVAL;
-		else
-			r = kvm->arch.vgic.msis_require_devid;
-		break;
 	default:
 		r = 0;
 	}

commit 0e4e82f154e387969ea7ecd2c8876689fb68f710
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Jul 15 12:43:38 2016 +0100

    KVM: arm64: vgic-its: Enable ITS emulation as a virtual MSI controller
    
    Now that all ITS emulation functionality is in place, we advertise
    MSI functionality to userland and also the ITS device to the guest - if
    userland has configured that.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index e95d4f68bf54..5bc460884639 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -86,6 +86,12 @@ int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_VCPU_ATTRIBUTES:
 		r = 1;
 		break;
+	case KVM_CAP_MSI_DEVID:
+		if (!kvm)
+			r = -EINVAL;
+		else
+			r = kvm->arch.vgic.msis_require_devid;
+		break;
 	default:
 		r = 0;
 	}

commit b46f01ce4dcfdce636588fe2ef5035724c77f266
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Jul 15 12:43:25 2016 +0100

    KVM: arm/arm64: Extend arch CAP checks to allow per-VM capabilities
    
    KVM capabilities can be a per-VM property, though ARM/ARM64 currently
    does not pass on the VM pointer to the architecture specific
    capability handlers.
    Add a "struct kvm*" parameter to those function to later allow proper
    per-VM capability reporting.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Eric Auger <eric.auger@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Tested-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 79f324823340..e95d4f68bf54 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -65,7 +65,7 @@ static bool cpu_has_32bit_el1(void)
  * We currently assume that the number of HW registers is uniform
  * across all CPUs (see cpuinfo_sanity_check).
  */
-int kvm_arch_dev_ioctl_check_extension(long ext)
+int kvm_arch_dev_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r;
 

commit 3421e9d88d7ae70fbc8c903e44a5acace8ae2d29
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:44 2016 +0100

    arm64: KVM: Simplify HYP init/teardown
    
    Now that we only have the "merged page tables" case to deal with,
    there is a bunch of things we can simplify in the HYP code (both
    at init and teardown time).
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 8ed7e4a92e95..79f324823340 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -132,14 +132,3 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset timer */
 	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
 }
-
-unsigned long kvm_hyp_reset_entry(void)
-{
-	/*
-	 * KVM is running with merged page tables, which don't have the
-	 * trampoline page mapped. We know the idmap is still mapped,
-	 * but can't be called into directly. Use
-	 * __extended_idmap_trampoline to do the call.
-	 */
-	return (unsigned long)kvm_ksym_ref(__extended_idmap_trampoline);
-}

commit 0535a3e2b2d518a21d93e7cfe07821f1b24ccd0c
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jun 30 18:40:43 2016 +0100

    arm/arm64: KVM: Always have merged page tables
    
    We're in a position where we can now always have "merged" page
    tables, where both the runtime mapping and the idmap coexist.
    
    This results in some code being removed, but there is more to come.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 7be24f2b18db..8ed7e4a92e95 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -133,30 +133,13 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
 }
 
-extern char __hyp_idmap_text_start[];
-
 unsigned long kvm_hyp_reset_entry(void)
 {
-	if (!__kvm_cpu_uses_extended_idmap()) {
-		unsigned long offset;
-
-		/*
-		 * Find the address of __kvm_hyp_reset() in the trampoline page.
-		 * This is present in the running page tables, and the boot page
-		 * tables, so we call the code here to start the trampoline
-		 * dance in reverse.
-		 */
-		offset = (unsigned long)__kvm_hyp_reset
-			 - ((unsigned long)__hyp_idmap_text_start & PAGE_MASK);
-
-		return TRAMPOLINE_VA + offset;
-	} else {
-		/*
-		 * KVM is running with merged page tables, which don't have the
-		 * trampoline page mapped. We know the idmap is still mapped,
-		 * but can't be called into directly. Use
-		 * __extended_idmap_trampoline to do the call.
-		 */
-		return (unsigned long)kvm_ksym_ref(__extended_idmap_trampoline);
-	}
+	/*
+	 * KVM is running with merged page tables, which don't have the
+	 * trampoline page mapped. We know the idmap is still mapped,
+	 * but can't be called into directly. Use
+	 * __extended_idmap_trampoline to do the call.
+	 */
+	return (unsigned long)kvm_ksym_ref(__extended_idmap_trampoline);
 }

commit edce2292c1e026c6a2da6899c114d930bc1f518b
Author: Andrea Gelmini <andrea.gelmini@gelma.net>
Date:   Sat May 21 13:53:14 2016 +0200

    KVM: ARM64: Fix typos
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index b1ad730e1567..7be24f2b18db 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -98,7 +98,7 @@ int kvm_arch_dev_ioctl_check_extension(long ext)
  * @vcpu: The VCPU pointer
  *
  * This function finds the right table above and sets the registers on
- * the virtual CPU struct to their architectually defined reset
+ * the virtual CPU struct to their architecturally defined reset
  * values.
  */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)

commit c612505f860c6d4fac03924879982adcd042e239
Author: James Morse <james.morse@arm.com>
Date:   Fri Apr 29 18:27:03 2016 +0100

    arm64: kvm: Fix kvm teardown for systems using the extended idmap
    
    If memory is located above 1<<VA_BITS, kvm adds an extra level to its page
    tables, merging the runtime tables and boot tables that contain the idmap.
    This lets us avoid the trampoline dance during initialisation.
    
    This also means there is no trampoline page mapped, so
    __cpu_reset_hyp_mode() can't call __kvm_hyp_reset() in this page. The good
    news is the idmap is still mapped, so we don't need the trampoline page.
    The bad news is we can't call it directly as the idmap is above
    HYP_PAGE_OFFSET, so its address is masked by kvm_call_hyp.
    
    Add a function __extended_idmap_trampoline which will branch into
    __kvm_hyp_reset in the idmap, change kvm_hyp_reset_entry() to return
    this address if __kvm_cpu_uses_extended_idmap(). In this case
    __kvm_hyp_reset() will still switch to the boot tables (which are the
    merged tables that were already in use), and branch into the idmap (where
    it already was).
    
    This fixes boot failures on these systems, where we fail to execute the
    missing trampoline page when tearing down kvm in init_subsystems():
    [    2.508922] kvm [1]: 8-bit VMID
    [    2.512057] kvm [1]: Hyp mode initialized successfully
    [    2.517242] kvm [1]: interrupt-controller@e1140000 IRQ13
    [    2.522622] kvm [1]: timer IRQ3
    [    2.525783] Kernel panic - not syncing: HYP panic:
    [    2.525783] PS:200003c9 PC:0000007ffffff820 ESR:86000005
    [    2.525783] FAR:0000007ffffff820 HPFAR:00000000003ffff0 PAR:0000000000000000
    [    2.525783] VCPU:          (null)
    [    2.525783]
    [    2.547667] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G        W       4.6.0-rc5+ #1
    [    2.555137] Hardware name: Default string Default string/Default string, BIOS ROD0084E 09/03/2015
    [    2.563994] Call trace:
    [    2.566432] [<ffffff80080888d0>] dump_backtrace+0x0/0x240
    [    2.571818] [<ffffff8008088b24>] show_stack+0x14/0x20
    [    2.576858] [<ffffff80083423ac>] dump_stack+0x94/0xb8
    [    2.581899] [<ffffff8008152130>] panic+0x10c/0x250
    [    2.586677] [<ffffff8008152024>] panic+0x0/0x250
    [    2.591281] SMP: stopping secondary CPUs
    [    3.649692] SMP: failed to stop secondary CPUs 0-2,4-7
    [    3.654818] Kernel Offset: disabled
    [    3.658293] Memory Limit: none
    [    3.661337] ---[ end Kernel panic - not syncing: HYP panic:
    [    3.661337] PS:200003c9 PC:0000007ffffff820 ESR:86000005
    [    3.661337] FAR:0000007ffffff820 HPFAR:00000000003ffff0 PAR:0000000000000000
    [    3.661337] VCPU:          (null)
    [    3.661337]
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 4062e6dd4cc1..b1ad730e1567 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -135,12 +135,28 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 
 extern char __hyp_idmap_text_start[];
 
-phys_addr_t kvm_hyp_reset_entry(void)
+unsigned long kvm_hyp_reset_entry(void)
 {
-	unsigned long offset;
-
-	offset = (unsigned long)__kvm_hyp_reset
-		 - ((unsigned long)__hyp_idmap_text_start & PAGE_MASK);
-
-	return TRAMPOLINE_VA + offset;
+	if (!__kvm_cpu_uses_extended_idmap()) {
+		unsigned long offset;
+
+		/*
+		 * Find the address of __kvm_hyp_reset() in the trampoline page.
+		 * This is present in the running page tables, and the boot page
+		 * tables, so we call the code here to start the trampoline
+		 * dance in reverse.
+		 */
+		offset = (unsigned long)__kvm_hyp_reset
+			 - ((unsigned long)__hyp_idmap_text_start & PAGE_MASK);
+
+		return TRAMPOLINE_VA + offset;
+	} else {
+		/*
+		 * KVM is running with merged page tables, which don't have the
+		 * trampoline page mapped. We know the idmap is still mapped,
+		 * but can't be called into directly. Use
+		 * __extended_idmap_trampoline to do the call.
+		 */
+		return (unsigned long)kvm_ksym_ref(__extended_idmap_trampoline);
+	}
 }

commit 67f6919766620e7ea7aab11a6a3470dc7b451359
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 27 17:47:05 2016 +0100

    arm64: kvm: allows kvm cpu hotplug
    
    The current kvm implementation on arm64 does cpu-specific initialization
    at system boot, and has no way to gracefully shutdown a core in terms of
    kvm. This prevents kexec from rebooting the system at EL2.
    
    This patch adds a cpu tear-down function and also puts an existing cpu-init
    code into a separate function, kvm_arch_hardware_disable() and
    kvm_arch_hardware_enable() respectively.
    We don't need the arm64 specific cpu hotplug hook any more.
    
    Since this patch modifies common code between arm and arm64, one stub
    definition, __cpu_reset_hyp_mode(), is added on arm side to avoid
    compilation errors.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [Rebase, added separate VHE init/exit path, changed resets use of
     kvm_call_hyp() to the __version, en/disabled hardware in init_subsystems(),
     added icache maintenance to __kvm_hyp_reset() and removed lr restore, removed
     guest-enter after teardown handling]
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 9677bf069bcc..4062e6dd4cc1 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -29,7 +29,9 @@
 #include <asm/cputype.h>
 #include <asm/ptrace.h>
 #include <asm/kvm_arm.h>
+#include <asm/kvm_asm.h>
 #include <asm/kvm_coproc.h>
+#include <asm/kvm_mmu.h>
 
 /*
  * ARMv8 Reset Values
@@ -130,3 +132,15 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset timer */
 	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
 }
+
+extern char __hyp_idmap_text_start[];
+
+phys_addr_t kvm_hyp_reset_entry(void)
+{
+	unsigned long offset;
+
+	offset = (unsigned long)__kvm_hyp_reset
+		 - ((unsigned long)__hyp_idmap_text_start & PAGE_MASK);
+
+	return TRAMPOLINE_VA + offset;
+}

commit f577f6c2a6a5ccabe98061f256a1e2ff468d5e93
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Mon Jan 11 20:56:17 2016 +0800

    arm64: KVM: Introduce per-vcpu kvm device controls
    
    In some cases it needs to get/set attributes specific to a vcpu and so
    needs something else than ONE_REG.
    
    Let's copy the KVM_DEVICE approach, and define the respective ioctls
    for the vcpu file descriptor.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Peter Maydell <peter.maydell@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index cf4f28a7a514..9677bf069bcc 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -81,6 +81,7 @@ int kvm_arch_dev_ioctl_check_extension(long ext)
 		r = kvm_arm_support_pmu_v3();
 		break;
 	case KVM_CAP_SET_GUEST_DEBUG:
+	case KVM_CAP_VCPU_ATTRIBUTES:
 		r = 1;
 		break;
 	default:

commit 808e738142e7086ef793ebf9797099c392894e65
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Mon Jan 11 22:46:15 2016 +0800

    arm64: KVM: Add a new feature bit for PMUv3
    
    To support guest PMUv3, use one bit of the VCPU INIT feature array.
    Initialize the PMU when initialzing the vcpu with that bit and PMU
    overflow interrupt set.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Acked-by: Peter Maydell <peter.maydell@linaro.org>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index dfbce781d284..cf4f28a7a514 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -77,6 +77,9 @@ int kvm_arch_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_GUEST_DEBUG_HW_WPS:
 		r = get_num_wrps();
 		break;
+	case KVM_CAP_ARM_PMU_V3:
+		r = kvm_arm_support_pmu_v3();
+		break;
 	case KVM_CAP_SET_GUEST_DEBUG:
 		r = 1;
 		break;

commit 2aa36e9840d71710f06b3c29634f044fde8bcbe5
Author: Shannon Zhao <shannon.zhao@linaro.org>
Date:   Fri Sep 11 11:30:22 2015 +0800

    arm64: KVM: Reset PMU state when resetting vcpu
    
    When resetting vcpu, it needs to reset the PMU state to initial status.
    
    Signed-off-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f34745cb3d23..dfbce781d284 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -120,6 +120,9 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset system registers */
 	kvm_reset_sys_regs(vcpu);
 
+	/* Reset PMU */
+	kvm_pmu_vcpu_reset(vcpu);
+
 	/* Reset timer */
 	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
 }

commit 4db8e5ea6b07ae83e4361aee3f967f7126e01fa4
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:55 2015 +0100

    arm64/kvm: Make use of the system wide safe values
    
    Use the system wide safe value from the new API for safer
    decisions
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: kvmarm@lists.cs.columbia.edu
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 91cf5350b328..f34745cb3d23 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -53,7 +53,7 @@ static bool cpu_has_32bit_el1(void)
 {
 	u64 pfr0;
 
-	pfr0 = read_cpuid(ID_AA64PFR0_EL1);
+	pfr0 = read_system_reg(SYS_ID_AA64PFR0_EL1);
 	return !!(pfr0 & 0x20);
 }
 

commit f120cd6533d21075ab103ae6c225b1697853660d
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Jun 23 13:59:13 2014 +0100

    KVM: arm/arm64: timer: Allow the timer to control the active state
    
    In order to remove the crude hack where we sneak the masked bit
    into the timer's control register, make use of the phys_irq_map
    API control the active state of the interrupt.
    
    This causes some limited changes to allow for potential error
    propagation.
    
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index b4af6185713f..91cf5350b328 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -121,7 +121,5 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	kvm_reset_sys_regs(vcpu);
 
 	/* Reset timer */
-	kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
-
-	return 0;
+	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
 }

commit 834bf88726f0f11ddc7ff9679fc9458654c01a12
Author: Alex Bennée <alex.bennee@linaro.org>
Date:   Tue Jul 7 17:30:02 2015 +0100

    KVM: arm64: enable KVM_CAP_SET_GUEST_DEBUG
    
    Finally advertise the KVM capability for SET_GUEST_DEBUG. Once arm
    support is added this check can be moved to the common
    kvm_vm_ioctl_check_extension() code.
    
    Signed-off-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 0b4326578985..b4af6185713f 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -22,6 +22,7 @@
 #include <linux/errno.h>
 #include <linux/kvm_host.h>
 #include <linux/kvm.h>
+#include <linux/hw_breakpoint.h>
 
 #include <kvm/arm_arch_timer.h>
 
@@ -56,6 +57,12 @@ static bool cpu_has_32bit_el1(void)
 	return !!(pfr0 & 0x20);
 }
 
+/**
+ * kvm_arch_dev_ioctl_check_extension
+ *
+ * We currently assume that the number of HW registers is uniform
+ * across all CPUs (see cpuinfo_sanity_check).
+ */
 int kvm_arch_dev_ioctl_check_extension(long ext)
 {
 	int r;
@@ -64,6 +71,15 @@ int kvm_arch_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_ARM_EL1_32BIT:
 		r = cpu_has_32bit_el1();
 		break;
+	case KVM_CAP_GUEST_DEBUG_HW_BPS:
+		r = get_num_brps();
+		break;
+	case KVM_CAP_GUEST_DEBUG_HW_WPS:
+		r = get_num_wrps();
+		break;
+	case KVM_CAP_SET_GUEST_DEBUG:
+		r = 1;
+		break;
 	default:
 		r = 0;
 	}

commit 801f6772cecea6cfc7da61aa197716ab64db5f9e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Sun Jan 11 14:10:11 2015 +0100

    arm64: KVM: Fix HCR setting for 32bit guests
    
    Commit b856a59141b1 (arm/arm64: KVM: Reset the HCR on each vcpu
    when resetting the vcpu) moved the init of the HCR register to
    happen later in the init of a vcpu, but left out the fixup
    done in kvm_reset_vcpu when preparing for a 32bit guest.
    
    As a result, the 32bit guest is run as a 64bit guest, but the
    rest of the kernel still manages it as a 32bit. Fun follows.
    
    Moving the fixup to vcpu_reset_hcr solves the problem for good.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 70a7816535cd..0b4326578985 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -90,7 +90,6 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 			if (!cpu_has_32bit_el1())
 				return -EINVAL;
 			cpu_reset = &default_regs_reset32;
-			vcpu->arch.hcr_el2 &= ~HCR_RW;
 		} else {
 			cpu_reset = &default_regs_reset;
 		}

commit 0d854a60b1d7d39a37b25dd28f63cfa0df637b91
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Feb 7 10:46:46 2013 +0000

    arm64: KVM: enable initialization of a 32bit vcpu
    
    Wire the init of a 32bit vcpu by allowing 32bit modes in pstate,
    and providing sensible defaults out of reset state.
    
    This feature is of course conditioned by the presence of 32bit
    capability on the physical CPU, and is checked by the KVM_CAP_ARM_EL1_32BIT
    capability.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index 766150ac76ed..70a7816535cd 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -38,16 +38,32 @@ static const struct kvm_regs default_regs_reset = {
 			PSR_F_BIT | PSR_D_BIT),
 };
 
+static const struct kvm_regs default_regs_reset32 = {
+	.regs.pstate = (COMPAT_PSR_MODE_SVC | COMPAT_PSR_A_BIT |
+			COMPAT_PSR_I_BIT | COMPAT_PSR_F_BIT),
+};
+
 static const struct kvm_irq_level default_vtimer_irq = {
 	.irq	= 27,
 	.level	= 1,
 };
 
+static bool cpu_has_32bit_el1(void)
+{
+	u64 pfr0;
+
+	pfr0 = read_cpuid(ID_AA64PFR0_EL1);
+	return !!(pfr0 & 0x20);
+}
+
 int kvm_arch_dev_ioctl_check_extension(long ext)
 {
 	int r;
 
 	switch (ext) {
+	case KVM_CAP_ARM_EL1_32BIT:
+		r = cpu_has_32bit_el1();
+		break;
 	default:
 		r = 0;
 	}
@@ -70,7 +86,15 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 
 	switch (vcpu->arch.target) {
 	default:
-		cpu_reset = &default_regs_reset;
+		if (test_bit(KVM_ARM_VCPU_EL1_32BIT, vcpu->arch.features)) {
+			if (!cpu_has_32bit_el1())
+				return -EINVAL;
+			cpu_reset = &default_regs_reset32;
+			vcpu->arch.hcr_el2 &= ~HCR_RW;
+		} else {
+			cpu_reset = &default_regs_reset;
+		}
+
 		cpu_vtimer_irq = &default_vtimer_irq;
 		break;
 	}

commit 003300de6c3e51934fb52eb2677f6f4fb4996cbd
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Dec 7 17:52:03 2012 +0000

    arm64: KVM: Plug the arch timer
    
    Add support for the in-kernel timer emulation.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
index f6536a06231a..766150ac76ed 100644
--- a/arch/arm64/kvm/reset.c
+++ b/arch/arm64/kvm/reset.c
@@ -23,6 +23,8 @@
 #include <linux/kvm_host.h>
 #include <linux/kvm.h>
 
+#include <kvm/arm_arch_timer.h>
+
 #include <asm/cputype.h>
 #include <asm/ptrace.h>
 #include <asm/kvm_arm.h>
@@ -36,6 +38,11 @@ static const struct kvm_regs default_regs_reset = {
 			PSR_F_BIT | PSR_D_BIT),
 };
 
+static const struct kvm_irq_level default_vtimer_irq = {
+	.irq	= 27,
+	.level	= 1,
+};
+
 int kvm_arch_dev_ioctl_check_extension(long ext)
 {
 	int r;
@@ -58,11 +65,13 @@ int kvm_arch_dev_ioctl_check_extension(long ext)
  */
 int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 {
+	const struct kvm_irq_level *cpu_vtimer_irq;
 	const struct kvm_regs *cpu_reset;
 
 	switch (vcpu->arch.target) {
 	default:
 		cpu_reset = &default_regs_reset;
+		cpu_vtimer_irq = &default_vtimer_irq;
 		break;
 	}
 
@@ -72,5 +81,8 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
 	/* Reset system registers */
 	kvm_reset_sys_regs(vcpu);
 
+	/* Reset timer */
+	kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
+
 	return 0;
 }

commit f4672752c321ea36ce099cebdd7a082a8f327505
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Dec 10 16:23:59 2012 +0000

    arm64: KVM: virtual CPU reset
    
    Provide the reset code for a virtual CPU booted in 64bit mode.
    
    Reviewed-by: Christopher Covington <cov@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
new file mode 100644
index 000000000000..f6536a06231a
--- /dev/null
+++ b/arch/arm64/kvm/reset.c
@@ -0,0 +1,76 @@
+/*
+ * Copyright (C) 2012,2013 - ARM Ltd
+ * Author: Marc Zyngier <marc.zyngier@arm.com>
+ *
+ * Derived from arch/arm/kvm/reset.c
+ * Copyright (C) 2012 - Virtual Open Systems and Columbia University
+ * Author: Christoffer Dall <c.dall@virtualopensystems.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/errno.h>
+#include <linux/kvm_host.h>
+#include <linux/kvm.h>
+
+#include <asm/cputype.h>
+#include <asm/ptrace.h>
+#include <asm/kvm_arm.h>
+#include <asm/kvm_coproc.h>
+
+/*
+ * ARMv8 Reset Values
+ */
+static const struct kvm_regs default_regs_reset = {
+	.regs.pstate = (PSR_MODE_EL1h | PSR_A_BIT | PSR_I_BIT |
+			PSR_F_BIT | PSR_D_BIT),
+};
+
+int kvm_arch_dev_ioctl_check_extension(long ext)
+{
+	int r;
+
+	switch (ext) {
+	default:
+		r = 0;
+	}
+
+	return r;
+}
+
+/**
+ * kvm_reset_vcpu - sets core registers and sys_regs to reset value
+ * @vcpu: The VCPU pointer
+ *
+ * This function finds the right table above and sets the registers on
+ * the virtual CPU struct to their architectually defined reset
+ * values.
+ */
+int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
+{
+	const struct kvm_regs *cpu_reset;
+
+	switch (vcpu->arch.target) {
+	default:
+		cpu_reset = &default_regs_reset;
+		break;
+	}
+
+	/* Reset core registers */
+	memcpy(vcpu_gp_regs(vcpu), cpu_reset, sizeof(*cpu_reset));
+
+	/* Reset system registers */
+	kvm_reset_sys_regs(vcpu);
+
+	return 0;
+}
