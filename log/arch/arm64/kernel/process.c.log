commit c76898373f9b71586edaf150190c493ae9ed3e77
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:30:23 2020 -0700

    arm64: add loglvl to dump_backtrace()
    
    Currently, the log-level of show_stack() depends on a platform
    realization.  It creates situations where the headers are printed with
    lower log level or higher than the stacktrace (depending on a platform or
    user).
    
    Furthermore, it forces the logic decision from user to an architecture
    side.  In result, some users as sysrq/kdb/etc are doing tricks with
    temporary rising console_loglevel while printing their messages.  And in
    result it not only may print unwanted messages from other CPUs, but also
    omit printing at all in the unlucky case where the printk() was deferred.
    
    Introducing log-level parameter and KERN_UNSUPPRESSED [1] seems an easier
    approach than introducing more printk buffers.  Also, it will consolidate
    printings with headers.
    
    Add log level argument to dump_backtrace() as a preparation for
    introducing show_stack_loglvl().
    
    [1]: https://lore.kernel.org/lkml/20190528002412.1625-1-dima@arista.com/T/#u
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-10-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index eade7807e819..6089638c7d43 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -306,7 +306,7 @@ void __show_regs(struct pt_regs *regs)
 void show_regs(struct pt_regs * regs)
 {
 	__show_regs(regs);
-	dump_backtrace(regs, NULL);
+	dump_backtrace(regs, NULL, KERN_DEFAULT);
 }
 
 static void tls_thread_flush(void)

commit 80e4e561321595d2e5f4a173e8cf8d8432078995
Merge: 6a8b55ed4056 5d1b631c773f
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 15:15:58 2020 +0100

    Merge branch 'for-next/bti-user' into for-next/bti
    
    Merge in user support for Branch Target Identification, which narrowly
    missed the cut for 5.7 after a late ABI concern.
    
    * for-next/bti-user:
      arm64: bti: Document behaviour for dynamically linked binaries
      arm64: elf: Fix allnoconfig kernel build with !ARCH_USE_GNU_PROPERTY
      arm64: BTI: Add Kconfig entry for userspace BTI
      mm: smaps: Report arm64 guarded pages in smaps
      arm64: mm: Display guarded pages in ptdump
      KVM: arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: traps: Shuffle code to eliminate forward declarations
      arm64: unify native/compat instruction skipping
      arm64: BTI: Decode BYTPE bits when printing PSTATE
      arm64: elf: Enable BTI at exec based on ELF program properties
      elf: Allow arch to tweak initial mmap prot flags
      arm64: Basic Branch Target Identification support
      ELF: Add ELF program property parsing support
      ELF: UAPI and Kconfig additions for ELF program properties

commit 3cd86a58f7734bf9cef38f6f899608ebcaa3da13
Merge: a8222fd5b80c b2a84de2a2de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 10:05:01 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The bulk is in-kernel pointer authentication, activity monitors and
      lots of asm symbol annotations. I also queued the sys_mremap() patch
      commenting the asymmetry in the address untagging.
    
      Summary:
    
       - In-kernel Pointer Authentication support (previously only offered
         to user space).
    
       - ARM Activity Monitors (AMU) extension support allowing better CPU
         utilisation numbers for the scheduler (frequency invariance).
    
       - Memory hot-remove support for arm64.
    
       - Lots of asm annotations (SYM_*) in preparation for the in-kernel
         Branch Target Identification (BTI) support.
    
       - arm64 perf updates: ARMv8.5-PMU 64-bit counters, refactoring the
         PMU init callbacks, support for new DT compatibles.
    
       - IPv6 header checksum optimisation.
    
       - Fixes: SDEI (software delegated exception interface) double-lock on
         hibernate with shared events.
    
       - Minor clean-ups and refactoring: cpu_ops accessor,
         cpu_do_switch_mm() converted to C, cpufeature finalisation helper.
    
       - sys_mremap() comment explaining the asymmetric address untagging
         behaviour"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (81 commits)
      mm/mremap: Add comment explaining the untagging behaviour of mremap()
      arm64: head: Convert install_el2_stub to SYM_INNER_LABEL
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
      arm64: move kimage_vaddr to .rodata
      arm64: use mov_q instead of literal ldr
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      ...

commit 5efbe6a6e1c077b4022d9e89d79543c6106c6e25
Author: Qais Yousef <qais.yousef@arm.com>
Date:   Mon Mar 23 13:51:00 2020 +0000

    arm64: Use reboot_cpu instead of hardconding it to 0
    
    Use `reboot_cpu` variable instead of hardcoding 0 as the reboot cpu in
    machine_shutdown().
    
    Signed-off-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200323135110.30522-8-qais.yousef@arm.com

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1b9f7b749d75..3e5a6ad66cbe 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -145,7 +145,7 @@ void arch_cpu_idle_dead(void)
  */
 void machine_shutdown(void)
 {
-	smp_shutdown_nonboot_cpus(0);
+	smp_shutdown_nonboot_cpus(reboot_cpu);
 }
 
 /*

commit d66b16f5df4b41c719b98f7b5f61f0161e9e9246
Author: Qais Yousef <qais.yousef@arm.com>
Date:   Mon Mar 23 13:50:59 2020 +0000

    arm64: Don't use disable_nonboot_cpus()
    
    disable_nonboot_cpus() is not safe to use when doing machine_down(),
    because it relies on freeze_secondary_cpus() which in turn is
    a suspend/resume related freeze and could abort if the logic detects any
    pending activities that can prevent finishing the offlining process.
    
    Beside disable_nonboot_cpus() is dependent on CONFIG_PM_SLEEP_SMP which
    is an othogonal config to rely on to ensure this function works
    correctly.
    
    Signed-off-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lkml.kernel.org/r/20200323135110.30522-7-qais.yousef@arm.com

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 00626057a384..1b9f7b749d75 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -141,11 +141,11 @@ void arch_cpu_idle_dead(void)
  * to execute e.g. a RAM-based pin loop is not sufficient. This allows the
  * kexec'd kernel to use any and all RAM as it sees fit, without having to
  * avoid any code or data used by any SW CPU pin loop. The CPU hotplug
- * functionality embodied in disable_nonboot_cpus() to achieve this.
+ * functionality embodied in smpt_shutdown_nonboot_cpus() to achieve this.
  */
 void machine_shutdown(void)
 {
-	disable_nonboot_cpus();
+	smp_shutdown_nonboot_cpus(0);
 }
 
 /*

commit 5d1b631c773ffbbadcbb3176a2ae0ea9d1c114c7
Author: Mark Brown <broonie@kernel.org>
Date:   Mon Mar 23 17:01:19 2020 +0000

    arm64: bti: Document behaviour for dynamically linked binaries
    
    For dynamically linked binaries the interpreter is responsible for setting
    PROT_BTI on everything except itself. The dynamic linker needs to be aware
    of PROT_BTI, for example in order to avoid dropping that when marking
    executable pages read only after doing relocations, and doing everything
    in userspace ensures that we don't get any issues due to divergences in
    behaviour between the kernel and dynamic linker within a single executable.
    Add a comment indicating that this is intentional to the code to help
    people trying to understand what's going on.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 24af13d7bde6..127aee478433 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -674,6 +674,11 @@ asmlinkage void __sched arm64_preempt_schedule_irq(void)
 int arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
 			 bool has_interp, bool is_interp)
 {
+	/*
+	 * For dynamically linked executables the interpreter is
+	 * responsible for setting PROT_BTI on everything except
+	 * itself.
+	 */
 	if (is_interp != has_interp)
 		return prot;
 

commit cdcb61ae4c56f9edcd1eca4c2df444f3f5e96e1d
Author: Amit Daniel Kachhap <amit.kachhap@arm.com>
Date:   Fri Mar 13 14:35:00 2020 +0530

    arm64: __show_regs: strip PAC from lr in printk
    
    lr is printed with %pS which will try to find an entry in kallsyms.
    After enabling pointer authentication, this match will fail due to
    PAC present in the lr.
    
    Strip PAC from the lr to display the correct symbol name.
    
    Suggested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 7db0302bec00..cacae291ba27 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -262,7 +262,7 @@ void __show_regs(struct pt_regs *regs)
 
 	if (!user_mode(regs)) {
 		printk("pc : %pS\n", (void *)regs->pc);
-		printk("lr : %pS\n", (void *)lr);
+		printk("lr : %pS\n", (void *)ptrauth_strip_insn_pac(lr));
 	} else {
 		printk("pc : %016llx\n", regs->pc);
 		printk("lr : %016llx\n", lr);

commit 33e45234987ea3ed4b05fc512f4441696478f12d
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:56 2020 +0530

    arm64: initialize and switch ptrauth kernel keys
    
    Set up keys to use pointer authentication within the kernel. The kernel
    will be compiled with APIAKey instructions, the other keys are currently
    unused. Each task is given its own APIAKey, which is initialized during
    fork. The key is changed during context switch and on kernel entry from
    EL0.
    
    The keys for idle threads need to be set before calling any C functions,
    because it is not possible to enter and exit a function with different
    keys.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Modified secondary cores key structure, comments]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6140e791bf92..7db0302bec00 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -376,6 +376,8 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long stack_start,
 	 */
 	fpsimd_flush_task_state(p);
 
+	ptrauth_thread_init_kernel(p);
+
 	if (likely(!(p->flags & PF_KTHREAD))) {
 		*childregs = *current_pt_regs();
 		childregs->regs[0] = 0;

commit be129842566599f2c6f8fbba277c098802cd4b3d
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:51 2020 +0530

    arm64: install user ptrauth keys at kernel exit time
    
    As we're going to enable pointer auth within the kernel and use a
    different APIAKey for the kernel itself, so move the user APIAKey
    switch to EL0 exception return.
    
    The other 4 keys could remain switched during task switch, but are also
    moved to keep things consistent.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: commit msg, re-positioned the patch, comments]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 00626057a384..6140e791bf92 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -512,7 +512,6 @@ __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev,
 	contextidr_thread_switch(next);
 	entry_task_switch(next);
 	uao_thread_switch(next);
-	ptrauth_thread_switch(next);
 	ssbs_thread_switch(next);
 
 	/*

commit ec94a46ee7ac999b0f10f7772c40aed3f604831b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:48 2020 +0000

    arm64: BTI: Decode BYTPE bits when printing PSTATE
    
    The current code to print PSTATE symbolically when generating
    backtraces etc., does not include the BYTPE field used by Branch
    Target Identification.
    
    So, decode BYTPE and print it too.
    
    In the interests of human-readability, print the classes of BTI
    matched.  The symbolic notation, BYTPE (PSTATE[11:10]) and
    permitted classes of subsequent instruction are:
    
        -- (BTYPE=0b00): any insn
        jc (BTYPE=0b01): BTI jc, BTI j, BTI c, PACIxSP
        -c (BYTPE=0b10): BTI jc, BTI c, PACIxSP
        j- (BTYPE=0b11): BTI jc, BTI j
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index b8e3faa8d406..24af13d7bde6 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -211,6 +211,15 @@ void machine_restart(char *cmd)
 	while (1);
 }
 
+#define bstr(suffix, str) [PSR_BTYPE_ ## suffix >> PSR_BTYPE_SHIFT] = str
+static const char *const btypes[] = {
+	bstr(NONE, "--"),
+	bstr(  JC, "jc"),
+	bstr(   C, "-c"),
+	bstr(  J , "j-")
+};
+#undef bstr
+
 static void print_pstate(struct pt_regs *regs)
 {
 	u64 pstate = regs->pstate;
@@ -229,7 +238,10 @@ static void print_pstate(struct pt_regs *regs)
 			pstate & PSR_AA32_I_BIT ? 'I' : 'i',
 			pstate & PSR_AA32_F_BIT ? 'F' : 'f');
 	} else {
-		printk("pstate: %08llx (%c%c%c%c %c%c%c%c %cPAN %cUAO)\n",
+		const char *btype_str = btypes[(pstate & PSR_BTYPE_MASK) >>
+					       PSR_BTYPE_SHIFT];
+
+		printk("pstate: %08llx (%c%c%c%c %c%c%c%c %cPAN %cUAO BTYPE=%s)\n",
 			pstate,
 			pstate & PSR_N_BIT ? 'N' : 'n',
 			pstate & PSR_Z_BIT ? 'Z' : 'z',
@@ -240,7 +252,8 @@ static void print_pstate(struct pt_regs *regs)
 			pstate & PSR_I_BIT ? 'I' : 'i',
 			pstate & PSR_F_BIT ? 'F' : 'f',
 			pstate & PSR_PAN_BIT ? '+' : '-',
-			pstate & PSR_UAO_BIT ? '+' : '-');
+			pstate & PSR_UAO_BIT ? '+' : '-',
+			btype_str);
 	}
 }
 

commit ab7876a98a2160092133de4c648e94b18bc3f139
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:47 2020 +0000

    arm64: elf: Enable BTI at exec based on ELF program properties
    
    For BTI protection to be as comprehensive as possible, it is
    desirable to have BTI enabled from process startup.  If this is not
    done, the process must use mprotect() to enable BTI for each of its
    executable mappings, but this is painful to do in the libc startup
    code.  It's simpler and more sound to have the kernel do it
    instead.
    
    To this end, detect BTI support in the executable (or ELF
    interpreter, as appropriate), via the
    NT_GNU_PROGRAM_PROPERTY_TYPE_0 note, and tweak the initial prot
    flags for the process' executable pages to include PROT_BTI as
    appropriate.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 00626057a384..b8e3faa8d406 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -11,6 +11,7 @@
 
 #include <linux/compat.h>
 #include <linux/efi.h>
+#include <linux/elf.h>
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
@@ -18,6 +19,7 @@
 #include <linux/sched/task_stack.h>
 #include <linux/kernel.h>
 #include <linux/lockdep.h>
+#include <linux/mman.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>
 #include <linux/sysctl.h>
@@ -654,3 +656,20 @@ asmlinkage void __sched arm64_preempt_schedule_irq(void)
 	if (system_capabilities_finalized())
 		preempt_schedule_irq();
 }
+
+#ifdef CONFIG_BINFMT_ELF
+int arch_elf_adjust_prot(int prot, const struct arch_elf_state *state,
+			 bool has_interp, bool is_interp)
+{
+	if (is_interp != has_interp)
+		return prot;
+
+	if (!(state->flags & ARM64_ELF_BTI))
+		return prot;
+
+	if (prot & PROT_EXEC)
+		prot |= PROT_BTI;
+
+	return prot;
+}
+#endif

commit fca3d33d8ad61eb53eca3ee4cac476d1e31b9008
Author: Will Deacon <will@kernel.org>
Date:   Thu Feb 6 10:42:58 2020 +0000

    arm64: ssbs: Fix context-switch when SSBS is present on all CPUs
    
    When all CPUs in the system implement the SSBS extension, the SSBS field
    in PSTATE is the definitive indication of the mitigation state. Further,
    when the CPUs implement the SSBS manipulation instructions (advertised
    to userspace via an HWCAP), EL0 can toggle the SSBS field directly and
    so we cannot rely on any shadow state such as TIF_SSBD at all.
    
    Avoid forcing the SSBS field in context-switch on such a system, and
    simply rely on the PSTATE register instead.
    
    Cc: <stable@vger.kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Srinivas Ramana <sramana@codeaurora.org>
    Fixes: cbdf8a189a66 ("arm64: Force SSBS on context switch")
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index a480b6760808..00626057a384 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -466,6 +466,13 @@ static void ssbs_thread_switch(struct task_struct *next)
 	if (unlikely(next->flags & PF_KTHREAD))
 		return;
 
+	/*
+	 * If all CPUs implement the SSBS extension, then we just need to
+	 * context-switch the PSTATE field.
+	 */
+	if (cpu_have_feature(cpu_feature(SSBS)))
+		return;
+
 	/* If the mitigation is enabled, then we leave SSBS clear. */
 	if ((arm64_get_ssbd_state() == ARM64_SSBD_FORCE_ENABLE) ||
 	    test_tsk_thread_flag(next, TIF_SSBD))

commit 2c614c1194f2803750c14b751871bd168dcc8054
Author: Matteo Croce <mcroce@redhat.com>
Date:   Fri Jan 24 16:51:27 2020 +0100

    arm64: use shared sysctl constants
    
    Use shared sysctl variables for zero and one constants, as in
    commit eec4844fae7c ("proc/sysctl: add shared variables for range check")
    
    Fixes: 63f0c6037965 ("arm64: Introduce prctl() options to control the tagged user addresses ABI")
    Signed-off-by: Matteo Croce <mcroce@redhat.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index bbb0f0c145f6..a480b6760808 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -608,8 +608,6 @@ long get_tagged_addr_ctrl(void)
  * only prevents the tagged address ABI enabling via prctl() and does not
  * disable it for tasks that already opted in to the relaxed ABI.
  */
-static int zero;
-static int one = 1;
 
 static struct ctl_table tagged_addr_sysctl_table[] = {
 	{
@@ -618,8 +616,8 @@ static struct ctl_table tagged_addr_sysctl_table[] = {
 		.data		= &tagged_addr_disabled,
 		.maxlen		= sizeof(int),
 		.proc_handler	= proc_dointvec_minmax,
-		.extra1		= &zero,
-		.extra2		= &one,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
 	},
 	{ }
 };

commit 0238d3c75303d63839ca20e71e4993fdab3fec7b
Merge: d5226fa6dbae e533dbe9dcb1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 08:58:19 2020 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "The changes are a real mixed bag this time around.
    
      The only scary looking one from the diffstat is the uapi change to
      asm-generic/mman-common.h, but this has been acked by Arnd and is
      actually just adding a pair of comments in an attempt to prevent
      allocation of some PROT values which tend to get used for
      arch-specific purposes. We'll be using them for Branch Target
      Identification (a CFI-like hardening feature), which is currently
      under review on the mailing list.
    
      New architecture features:
    
       - Support for Armv8.5 E0PD, which benefits KASLR in the same way as
         KPTI but without the overhead. This allows KPTI to be disabled on
         CPUs that are not affected by Meltdown, even is KASLR is enabled.
    
       - Initial support for the Armv8.5 RNG instructions, which claim to
         provide access to a high bandwidth, cryptographically secure
         hardware random number generator. As well as exposing these to
         userspace, we also use them as part of the KASLR seed and to seed
         the crng once all CPUs have come online.
    
       - Advertise a bunch of new instructions to userspace, including
         support for Data Gathering Hint, Matrix Multiply and 16-bit
         floating point.
    
      Kexec:
    
       - Cleanups in preparation for relocating with the MMU enabled
    
       - Support for loading crash dump kernels with kexec_file_load()
    
      Perf and PMU drivers:
    
       - Cleanups and non-critical fixes for a couple of system PMU drivers
    
      FPU-less (aka broken) CPU support:
    
       - Considerable fixes to support CPUs without the FP/SIMD extensions,
         including their presence in heterogeneous systems. Good luck
         finding a 64-bit userspace that handles this.
    
      Modern assembly function annotations:
    
       - Start migrating our use of ENTRY() and ENDPROC() over to the
         new-fangled SYM_{CODE,FUNC}_{START,END} macros, which are intended
         to aid debuggers
    
      Kbuild:
    
       - Cleanup detection of LSE support in the assembler by introducing
         'as-instr'
    
       - Remove compressed Image files when building clean targets
    
      IP checksumming:
    
       - Implement optimised IPv4 checksumming routine when hardware offload
         is not in use. An IPv6 version is in the works, pending testing.
    
      Hardware errata:
    
       - Work around Cortex-A55 erratum #1530923
    
      Shadow call stack:
    
       - Work around some issues with Clang's integrated assembler not
         liking our perfectly reasonable assembly code
    
       - Avoid allocating the X18 register, so that it can be used to hold
         the shadow call stack pointer in future
    
      ACPI:
    
       - Fix ID count checking in IORT code. This may regress broken
         firmware that happened to work with the old implementation, in
         which case we'll have to revert it and try something else
    
       - Fix DAIF corruption on return from GHES handler with pseudo-NMIs
    
      Miscellaneous:
    
       - Whitelist some CPUs that are unaffected by Spectre-v2
    
       - Reduce frequency of ASID rollover when KPTI is compiled in but
         inactive
    
       - Reserve a couple of arch-specific PROT flags that are already used
         by Sparc and PowerPC and are planned for later use with BTI on
         arm64
    
       - Preparatory cleanup of our entry assembly code in preparation for
         moving more of it into C later on
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (73 commits)
      arm64: acpi: fix DAIF manipulation with pNMI
      arm64: kconfig: Fix alignment of E0PD help text
      arm64: Use v8.5-RNG entropy for KASLR seed
      arm64: Implement archrandom.h for ARMv8.5-RNG
      arm64: kbuild: remove compressed images on 'make ARCH=arm64 (dist)clean'
      arm64: entry: Avoid empty alternatives entries
      arm64: Kconfig: select HAVE_FUTEX_CMPXCHG
      arm64: csum: Fix pathological zero-length calls
      arm64: entry: cleanup sp_el0 manipulation
      arm64: entry: cleanup el0 svc handler naming
      arm64: entry: mark all entry code as notrace
      arm64: assembler: remove smp_dmb macro
      arm64: assembler: remove inherit_daif macro
      ACPI/IORT: Fix 'Number of IDs' handling in iort_id_map()
      mm: Reserve asm-generic prot flags 0x10 and 0x20 for arch use
      arm64: Use macros instead of hard-coded constants for MAIR_EL1
      arm64: Add KRYO{3,4}XX CPU cores to spectre-v2 safe list
      arm64: kernel: avoid x18 in __cpu_soft_restart
      arm64: kvm: stop treating register x18 as caller save
      arm64/lib: copy_page: avoid x18 register in assembler code
      ...

commit b51c6ac220f77eb246e940442d970b4065c197b0
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 13 23:30:17 2020 +0000

    arm64: Introduce system_capabilities_finalized() marker
    
    We finalize the system wide capabilities after the SMP CPUs
    are booted by the kernel. This is used as a marker for deciding
    various checks in the kernel. e.g, sanity check the hotplugged
    CPUs for missing mandatory features.
    
    However there is no explicit helper available for this in the
    kernel. There is sys_caps_initialised, which is not exposed.
    The other closest one we have is the jump_label arm64_const_caps_ready
    which denotes that the capabilities are set and the capability checks
    could use the individual jump_labels for fast path. This is
    performed before setting the ELF Hwcaps, which must be checked
    against the new CPUs. We also perform some of the other initialization
    e.g, SVE setup, which is important for the use of FP/SIMD
    where SVE is supported. Normally userspace doesn't get to run
    before we finish this. However the in-kernel users may
    potentially start using the neon mode. So, we need to
    reject uses of neon mode before we are set. Instead of defining
    a new marker for the completion of SVE setup, we could simply
    reuse the arm64_const_caps_ready and enable it once we have
    finished all the setup. Also we could expose this to the
    various users as "system_capabilities_finalized()" to make
    it more meaningful than "const_caps_ready".
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 71f788cd2b18..48a38144ea7b 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -646,6 +646,6 @@ asmlinkage void __sched arm64_preempt_schedule_irq(void)
 	 * Only allow a task to be preempted once cpufeatures have been
 	 * enabled.
 	 */
-	if (static_branch_likely(&arm64_const_caps_ready))
+	if (system_capabilities_finalized())
 		preempt_schedule_irq();
 }

commit a4376f2fbcc8084832f2f114577c8d68234c7903
Author: Amanieu d'Antras <amanieu@gmail.com>
Date:   Thu Jan 2 18:24:08 2020 +0100

    arm64: Implement copy_thread_tls
    
    This is required for clone3 which passes the TLS value through a
    struct rather than a register.
    
    Signed-off-by: Amanieu d'Antras <amanieu@gmail.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: <stable@vger.kernel.org> # 5.3.x
    Acked-by: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200102172413.654385-3-amanieu@gmail.com
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 71f788cd2b18..d54586d5b031 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -360,8 +360,8 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 
 asmlinkage void ret_from_fork(void) asm("ret_from_fork");
 
-int copy_thread(unsigned long clone_flags, unsigned long stack_start,
-		unsigned long stk_sz, struct task_struct *p)
+int copy_thread_tls(unsigned long clone_flags, unsigned long stack_start,
+		unsigned long stk_sz, struct task_struct *p, unsigned long tls)
 {
 	struct pt_regs *childregs = task_pt_regs(p);
 
@@ -394,11 +394,11 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		}
 
 		/*
-		 * If a TLS pointer was passed to clone (4th argument), use it
-		 * for the new thread.
+		 * If a TLS pointer was passed to clone, use it for the new
+		 * thread.
 		 */
 		if (clone_flags & CLONE_SETTLS)
-			p->thread.uw.tp_value = childregs->regs[3];
+			p->thread.uw.tp_value = tls;
 	} else {
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->pstate = PSR_MODE_EL1h;

commit 19c95f261c6558d4c2cbbfacd2d8bb6501384601
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Oct 15 18:25:44 2019 +0100

    arm64: entry.S: Do not preempt from IRQ before all cpufeatures are enabled
    
    Preempting from IRQ-return means that the task has its PSTATE saved
    on the stack, which will get restored when the task is resumed and does
    the actual IRQ return.
    
    However, enabling some CPU features requires modifying the PSTATE. This
    means that, if a task was scheduled out during an IRQ-return before all
    CPU features are enabled, the task might restore a PSTATE that does not
    include the feature enablement changes once scheduled back in.
    
    * Task 1:
    
    PAN == 0 ---|                          |---------------
                |                          |<- return from IRQ, PSTATE.PAN = 0
                | <- IRQ                   |
                +--------+ <- preempt()  +--
                                         ^
                                         |
                                         reschedule Task 1, PSTATE.PAN == 1
    * Init:
            --------------------+------------------------
                                ^
                                |
                                enable_cpu_features
                                set PSTATE.PAN on all CPUs
    
    Worse than this, since PSTATE is untouched when task switching is done,
    a task missing the new bits in PSTATE might affect another task, if both
    do direct calls to schedule() (outside of IRQ/exception contexts).
    
    Fix this by preventing preemption on IRQ-return until features are
    enabled on all CPUs.
    
    This way the only PSTATE values that are saved on the stack are from
    synchronous exceptions. These are expected to be fatal this early, the
    exception is BRK for WARN_ON(), but as this uses do_debug_exception()
    which keeps IRQs masked, it shouldn't call schedule().
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    [james: Replaced a really cool hack, with an even simpler static key in C.
     expanded commit message with Julien's cover-letter ascii art]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1fb2819fc048..71f788cd2b18 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -17,6 +17,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/kernel.h>
+#include <linux/lockdep.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>
 #include <linux/sysctl.h>
@@ -44,6 +45,7 @@
 #include <asm/alternative.h>
 #include <asm/arch_gicv3.h>
 #include <asm/compat.h>
+#include <asm/cpufeature.h>
 #include <asm/cacheflush.h>
 #include <asm/exec.h>
 #include <asm/fpsimd.h>
@@ -631,3 +633,19 @@ static int __init tagged_addr_init(void)
 
 core_initcall(tagged_addr_init);
 #endif	/* CONFIG_ARM64_TAGGED_ADDR_ABI */
+
+asmlinkage void __sched arm64_preempt_schedule_irq(void)
+{
+	lockdep_assert_irqs_disabled();
+
+	/*
+	 * Preempting a task from an IRQ means we leave copies of PSTATE
+	 * on the stack. cpufeature's enable calls may modify PSTATE, but
+	 * resuming one of these preempted tasks would undo those changes.
+	 *
+	 * Only allow a task to be preempted once cpufeatures have been
+	 * enabled.
+	 */
+	if (static_branch_likely(&arm64_const_caps_ready))
+		preempt_schedule_irq();
+}

commit 4585fc59c0e813188d6a4c5de1f6976fce461fc2
Author: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
Date:   Mon Sep 30 16:56:00 2019 -0400

    arm64/sve: Fix wrong free for task->thread.sve_state
    
    The system which has SVE feature crashed because of
    the memory pointed by task->thread.sve_state was destroyed
    by someone.
    
    That is because sve_state is freed while the forking the
    child process. The child process has the pointer of sve_state
    which is same as the parent's because the child's task_struct
    is copied from the parent's one. If the copy_process()
    fails as an error on somewhere, for example, copy_creds(),
    then the sve_state is freed even if the parent is alive.
    The flow is as follows.
    
    copy_process
            p = dup_task_struct
                => arch_dup_task_struct
                    *dst = *src;  // copy the entire region.
    :
            retval = copy_creds
            if (retval < 0)
                    goto bad_fork_free;
    :
    bad_fork_free:
    ...
            delayed_free_task(p);
              => free_task
                 => arch_release_task_struct
                    => fpsimd_release_task
                       => __sve_free
                          => kfree(task->thread.sve_state);
                             // free the parent's sve_state
    
    Move child's sve_state = NULL and clearing TIF_SVE flag
    to arch_dup_task_struct() so that the child doesn't free the
    parent's one.
    There is no need to wait until copy_process() to clear TIF_SVE for
    dst, because the thread flags for dst are initialized already by
    copying the src task_struct.
    This change simplifies the code, so get rid of comments that are no
    longer needed.
    
    As a note, arm64 used to have thread_info on the stack. So it
    would not be possible to clear TIF_SVE until the stack is initialized.
    From commit c02433dd6de3 ("arm64: split thread_info from task stack"),
    the thread_info is part of the task, so it should be valid to modify
    the flag from arch_dup_task_struct().
    
    Cc: stable@vger.kernel.org # 4.15.x-
    Fixes: bc0ee4760364 ("arm64/sve: Core task context handling")
    Signed-off-by: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
    Reported-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Suggested-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Tested-by: Julien Grall <julien.grall@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index a47462def04b..1fb2819fc048 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -332,22 +332,27 @@ void arch_release_task_struct(struct task_struct *tsk)
 	fpsimd_release_task(tsk);
 }
 
-/*
- * src and dst may temporarily have aliased sve_state after task_struct
- * is copied.  We cannot fix this properly here, because src may have
- * live SVE state and dst's thread_info may not exist yet, so tweaking
- * either src's or dst's TIF_SVE is not safe.
- *
- * The unaliasing is done in copy_thread() instead.  This works because
- * dst is not schedulable or traceable until both of these functions
- * have been called.
- */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	if (current->mm)
 		fpsimd_preserve_current_state();
 	*dst = *src;
 
+	/* We rely on the above assignment to initialize dst's thread_flags: */
+	BUILD_BUG_ON(!IS_ENABLED(CONFIG_THREAD_INFO_IN_TASK));
+
+	/*
+	 * Detach src's sve_state (if any) from dst so that it does not
+	 * get erroneously used or freed prematurely.  dst's sve_state
+	 * will be allocated on demand later on if dst uses SVE.
+	 * For consistency, also clear TIF_SVE here: this could be done
+	 * later in copy_process(), but to avoid tripping up future
+	 * maintainers it is best not to leave TIF_SVE and sve_state in
+	 * an inconsistent state, even temporarily.
+	 */
+	dst->thread.sve_state = NULL;
+	clear_tsk_thread_flag(dst, TIF_SVE);
+
 	return 0;
 }
 
@@ -360,13 +365,6 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 
 	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
 
-	/*
-	 * Unalias p->thread.sve_state (if any) from the parent task
-	 * and disable discard SVE state for p:
-	 */
-	clear_tsk_thread_flag(p, TIF_SVE);
-	p->thread.sve_state = NULL;
-
 	/*
 	 * In case p was allocated the same task_struct pointer as some
 	 * other recently-exited task, make sure p is disassociated from

commit e7142bf5d231f3ccdf6ea6764d5080999b8e299d
Author: Alexandre Ghiti <alex@ghiti.fr>
Date:   Mon Sep 23 15:38:50 2019 -0700

    arm64, mm: make randomization selected by generic topdown mmap layout
    
    This commits selects ARCH_HAS_ELF_RANDOMIZE when an arch uses the generic
    topdown mmap layout functions so that this security feature is on by
    default.
    
    Note that this commit also removes the possibility for arm64 to have elf
    randomization and no MMU: without MMU, the security added by randomization
    is worth nothing.
    
    Link: http://lkml.kernel.org/r/20190730055113.23635-6-alex@ghiti.fr
    Signed-off-by: Alexandre Ghiti <alex@ghiti.fr>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Luis Chamberlain <mcgrof@kernel.org>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 03689c0beb34..a47462def04b 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -557,14 +557,6 @@ unsigned long arch_align_stack(unsigned long sp)
 	return sp & ~0xf;
 }
 
-unsigned long arch_randomize_brk(struct mm_struct *mm)
-{
-	if (is_compat_task())
-		return randomize_page(mm->brk, SZ_32M);
-	else
-		return randomize_page(mm->brk, SZ_1G);
-}
-
 /*
  * Called from setup_new_exec() after (COMPAT_)SET_PERSONALITY.
  */

commit 413235fcedc7f61e925fe9818bc3f5eff8ad2494
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Aug 15 16:44:01 2019 +0100

    arm64: Change the tagged_addr sysctl control semantics to only prevent the opt-in
    
    First rename the sysctl control to abi.tagged_addr_disabled and make it
    default off (zero). When abi.tagged_addr_disabled == 1, only block the
    enabling of the TBI ABI via prctl(PR_SET_TAGGED_ADDR_CTRL, PR_TAGGED_ADDR_ENABLE).
    Getting the status of the ABI or disabling it is still allowed.
    
    Acked-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 76b7c55026aa..03689c0beb34 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -579,17 +579,22 @@ void arch_setup_new_exec(void)
 /*
  * Control the relaxed ABI allowing tagged user addresses into the kernel.
  */
-static unsigned int tagged_addr_prctl_allowed = 1;
+static unsigned int tagged_addr_disabled;
 
 long set_tagged_addr_ctrl(unsigned long arg)
 {
-	if (!tagged_addr_prctl_allowed)
-		return -EINVAL;
 	if (is_compat_task())
 		return -EINVAL;
 	if (arg & ~PR_TAGGED_ADDR_ENABLE)
 		return -EINVAL;
 
+	/*
+	 * Do not allow the enabling of the tagged address ABI if globally
+	 * disabled via sysctl abi.tagged_addr_disabled.
+	 */
+	if (arg & PR_TAGGED_ADDR_ENABLE && tagged_addr_disabled)
+		return -EINVAL;
+
 	update_thread_flag(TIF_TAGGED_ADDR, arg & PR_TAGGED_ADDR_ENABLE);
 
 	return 0;
@@ -597,8 +602,6 @@ long set_tagged_addr_ctrl(unsigned long arg)
 
 long get_tagged_addr_ctrl(void)
 {
-	if (!tagged_addr_prctl_allowed)
-		return -EINVAL;
 	if (is_compat_task())
 		return -EINVAL;
 
@@ -618,9 +621,9 @@ static int one = 1;
 
 static struct ctl_table tagged_addr_sysctl_table[] = {
 	{
-		.procname	= "tagged_addr",
+		.procname	= "tagged_addr_disabled",
 		.mode		= 0644,
-		.data		= &tagged_addr_prctl_allowed,
+		.data		= &tagged_addr_disabled,
 		.maxlen		= sizeof(int),
 		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &zero,

commit 63f0c60379650d82250f22e4cf4137ef3dc4f43d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 23 19:58:39 2019 +0200

    arm64: Introduce prctl() options to control the tagged user addresses ABI
    
    It is not desirable to relax the ABI to allow tagged user addresses into
    the kernel indiscriminately. This patch introduces a prctl() interface
    for enabling or disabling the tagged ABI with a global sysctl control
    for preventing applications from enabling the relaxed ABI (meant for
    testing user-space prctl() return error checking without reconfiguring
    the kernel). The ABI properties are inherited by threads of the same
    application and fork()'ed children but cleared on execve(). A Kconfig
    option allows the overall disabling of the relaxed ABI.
    
    The PR_SET_TAGGED_ADDR_CTRL will be expanded in the future to handle
    MTE-specific settings like imprecise vs precise exceptions.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f674f28df663..76b7c55026aa 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -19,6 +19,7 @@
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>
+#include <linux/sysctl.h>
 #include <linux/unistd.h>
 #include <linux/user.h>
 #include <linux/delay.h>
@@ -38,6 +39,7 @@
 #include <trace/events/power.h>
 #include <linux/percpu.h>
 #include <linux/thread_info.h>
+#include <linux/prctl.h>
 
 #include <asm/alternative.h>
 #include <asm/arch_gicv3.h>
@@ -307,11 +309,18 @@ static void tls_thread_flush(void)
 	}
 }
 
+static void flush_tagged_addr_state(void)
+{
+	if (IS_ENABLED(CONFIG_ARM64_TAGGED_ADDR_ABI))
+		clear_thread_flag(TIF_TAGGED_ADDR);
+}
+
 void flush_thread(void)
 {
 	fpsimd_flush_thread();
 	tls_thread_flush();
 	flush_ptrace_hw_breakpoint(current);
+	flush_tagged_addr_state();
 }
 
 void release_thread(struct task_struct *dead_task)
@@ -565,3 +574,67 @@ void arch_setup_new_exec(void)
 
 	ptrauth_thread_init_user(current);
 }
+
+#ifdef CONFIG_ARM64_TAGGED_ADDR_ABI
+/*
+ * Control the relaxed ABI allowing tagged user addresses into the kernel.
+ */
+static unsigned int tagged_addr_prctl_allowed = 1;
+
+long set_tagged_addr_ctrl(unsigned long arg)
+{
+	if (!tagged_addr_prctl_allowed)
+		return -EINVAL;
+	if (is_compat_task())
+		return -EINVAL;
+	if (arg & ~PR_TAGGED_ADDR_ENABLE)
+		return -EINVAL;
+
+	update_thread_flag(TIF_TAGGED_ADDR, arg & PR_TAGGED_ADDR_ENABLE);
+
+	return 0;
+}
+
+long get_tagged_addr_ctrl(void)
+{
+	if (!tagged_addr_prctl_allowed)
+		return -EINVAL;
+	if (is_compat_task())
+		return -EINVAL;
+
+	if (test_thread_flag(TIF_TAGGED_ADDR))
+		return PR_TAGGED_ADDR_ENABLE;
+
+	return 0;
+}
+
+/*
+ * Global sysctl to disable the tagged user addresses support. This control
+ * only prevents the tagged address ABI enabling via prctl() and does not
+ * disable it for tasks that already opted in to the relaxed ABI.
+ */
+static int zero;
+static int one = 1;
+
+static struct ctl_table tagged_addr_sysctl_table[] = {
+	{
+		.procname	= "tagged_addr",
+		.mode		= 0644,
+		.data		= &tagged_addr_prctl_allowed,
+		.maxlen		= sizeof(int),
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= &zero,
+		.extra2		= &one,
+	},
+	{ }
+};
+
+static int __init tagged_addr_init(void)
+{
+	if (!register_sysctl("abi", tagged_addr_sysctl_table))
+		return -EINVAL;
+	return 0;
+}
+
+core_initcall(tagged_addr_init);
+#endif	/* CONFIG_ARM64_TAGGED_ADDR_ABI */

commit cbdf8a189a66001c36007bf0f5c975d0376c5c3a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Jul 22 14:53:09 2019 +0100

    arm64: Force SSBS on context switch
    
    On a CPU that doesn't support SSBS, PSTATE[12] is RES0.  In a system
    where only some of the CPUs implement SSBS, we end-up losing track of
    the SSBS bit across task migration.
    
    To address this issue, let's force the SSBS bit on context switch.
    
    Fixes: 8f04e8e6e29c ("arm64: ssbd: Add support for PSTATE.SSBS rather than trapping to EL3")
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    [will: inverted logic and added comments]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 8d836d0abc96..f674f28df663 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -398,7 +398,7 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 			childregs->pstate |= PSR_UAO_BIT;
 
 		if (arm64_get_ssbd_state() == ARM64_SSBD_FORCE_DISABLE)
-			childregs->pstate |= PSR_SSBS_BIT;
+			set_ssbs_bit(childregs);
 
 		if (system_uses_irq_prio_masking())
 			childregs->pmr_save = GIC_PRIO_IRQON;
@@ -442,6 +442,32 @@ void uao_thread_switch(struct task_struct *next)
 	}
 }
 
+/*
+ * Force SSBS state on context-switch, since it may be lost after migrating
+ * from a CPU which treats the bit as RES0 in a heterogeneous system.
+ */
+static void ssbs_thread_switch(struct task_struct *next)
+{
+	struct pt_regs *regs = task_pt_regs(next);
+
+	/*
+	 * Nothing to do for kernel threads, but 'regs' may be junk
+	 * (e.g. idle task) so check the flags and bail early.
+	 */
+	if (unlikely(next->flags & PF_KTHREAD))
+		return;
+
+	/* If the mitigation is enabled, then we leave SSBS clear. */
+	if ((arm64_get_ssbd_state() == ARM64_SSBD_FORCE_ENABLE) ||
+	    test_tsk_thread_flag(next, TIF_SSBD))
+		return;
+
+	if (compat_user_mode(regs))
+		set_compat_ssbs_bit(regs);
+	else if (user_mode(regs))
+		set_ssbs_bit(regs);
+}
+
 /*
  * We store our current task in sp_el0, which is clobbered by userspace. Keep a
  * shadow copy so that we can restore this upon entry from userspace.
@@ -471,6 +497,7 @@ __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev,
 	entry_task_switch(next);
 	uao_thread_switch(next);
 	ptrauth_thread_switch(next);
+	ssbs_thread_switch(next);
 
 	/*
 	 * Complete any pending TLB or cache maintenance on this CPU in case

commit f3dcbe67ed424f1cf92065f9ad0cc647f2b44eac
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jul 2 14:07:28 2019 +0100

    arm64: stacktrace: Factor out backtrace initialisation
    
    Some common code is required by each stacktrace user to initialise
    struct stackframe before the first call to unwind_frame().
    
    In preparation for adding to the common code, this patch factors it
    out into a separate function start_backtrace(), and modifies the
    stacktrace callers appropriately.
    
    No functional change.
    
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [Mark: drop tsk argument, update more callsites]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6a869d9f304f..8d836d0abc96 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -498,11 +498,8 @@ unsigned long get_wchan(struct task_struct *p)
 	if (!stack_page)
 		return 0;
 
-	frame.fp = thread_saved_fp(p);
-	frame.pc = thread_saved_pc(p);
-#ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	frame.graph = 0;
-#endif
+	start_backtrace(&frame, thread_saved_fp(p), thread_saved_pc(p));
+
 	do {
 		if (unwind_frame(p, &frame))
 			goto out;

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit bd82d4bd21880b7c4d5f5756be435095d6ae07b5
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:10 2019 +0100

    arm64: Fix incorrect irqflag restore for priority masking
    
    When using IRQ priority masking to disable interrupts, in order to deal
    with the PSR.I state, local_irq_save() would convert the I bit into a
    PMR value (GIC_PRIO_IRQOFF). This resulted in local_irq_restore()
    potentially modifying the value of PMR in undesired location due to the
    state of PSR.I upon flag saving [1].
    
    In an attempt to solve this issue in a less hackish manner, introduce
    a bit (GIC_PRIO_IGNORE_PMR) for the PMR values that can represent
    whether PSR.I is being used to disable interrupts, in which case it
    takes precedence of the status of interrupt masking via PMR.
    
    GIC_PRIO_PSR_I_SET is chosen such that (<pmr_value> |
    GIC_PRIO_PSR_I_SET) does not mask more interrupts than <pmr_value> as
    some sections (e.g. arch_cpu_idle(), interrupt acknowledge path)
    requires PMR not to mask interrupts that could be signaled to the
    CPU when using only PSR.I.
    
    [1] https://www.spinics.net/lists/arm-kernel/msg716956.html
    
    Fixes: 4a503217ce37 ("arm64: irqflags: Use ICC_PMR_EL1 for interrupt masking")
    Cc: <stable@vger.kernel.org> # 5.1.x-
    Reported-by: Zenghui Yu <yuzenghui@huawei.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Wei Li <liwei391@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Pouloze <suzuki.poulose@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 3767fb21a5b8..58efc3727778 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -94,7 +94,7 @@ static void __cpu_do_idle_irqprio(void)
 	 * be raised.
 	 */
 	pmr = gic_read_pmr();
-	gic_write_pmr(GIC_PRIO_IRQON);
+	gic_write_pmr(GIC_PRIO_IRQON | GIC_PRIO_PSR_I_SET);
 
 	__cpu_do_idle();
 

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 3767fb21a5b8..9856395ccdb7 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -1,21 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/kernel/process.c
  *
  * Original Copyright (C) 1995  Linus Torvalds
  * Copyright (C) 1996-2000 Russell King - Converted to ARM.
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <stdarg.h>

commit a9806aa259feb2f6fd582b6342c835a3482fccc6
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:47 2019 +0000

    arm64: Unmask PMR before going idle
    
    CPU does not received signals for interrupts with a priority masked by
    ICC_PMR_EL1. This means the CPU might not come back from a WFI
    instruction.
    
    Make sure ICC_PMR_EL1 does not mask interrupts when doing a WFI.
    
    Since the logic of cpu_do_idle is becoming a bit more complex than just
    two instructions, lets turn it from ASM to C.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Suggested-by: Daniel Thompson <daniel.thompson@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6d410fc2849b..3767fb21a5b8 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -51,6 +51,7 @@
 #include <linux/thread_info.h>
 
 #include <asm/alternative.h>
+#include <asm/arch_gicv3.h>
 #include <asm/compat.h>
 #include <asm/cacheflush.h>
 #include <asm/exec.h>
@@ -74,6 +75,50 @@ EXPORT_SYMBOL_GPL(pm_power_off);
 
 void (*arm_pm_restart)(enum reboot_mode reboot_mode, const char *cmd);
 
+static void __cpu_do_idle(void)
+{
+	dsb(sy);
+	wfi();
+}
+
+static void __cpu_do_idle_irqprio(void)
+{
+	unsigned long pmr;
+	unsigned long daif_bits;
+
+	daif_bits = read_sysreg(daif);
+	write_sysreg(daif_bits | PSR_I_BIT, daif);
+
+	/*
+	 * Unmask PMR before going idle to make sure interrupts can
+	 * be raised.
+	 */
+	pmr = gic_read_pmr();
+	gic_write_pmr(GIC_PRIO_IRQON);
+
+	__cpu_do_idle();
+
+	gic_write_pmr(pmr);
+	write_sysreg(daif_bits, daif);
+}
+
+/*
+ *	cpu_do_idle()
+ *
+ *	Idle the processor (wait for interrupt).
+ *
+ *	If the CPU supports priority masking we must do additional work to
+ *	ensure that interrupts are not masked at the PMR (because the core will
+ *	not wake up if we block the wake up signal in the interrupt controller).
+ */
+void cpu_do_idle(void)
+{
+	if (system_uses_irq_prio_masking())
+		__cpu_do_idle_irqprio();
+	else
+		__cpu_do_idle();
+}
+
 /*
  * This is our default idle handler.
  */

commit 133d05186325ce04494ea6488a6b86e50a446c12
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:46 2019 +0000

    arm64: Make PMR part of task context
    
    In order to replace PSR.I interrupt disabling/enabling with ICC_PMR_EL1
    interrupt masking, ICC_PMR_EL1 needs to be saved/restored when
    taking/returning from an exception. This mimics the way hardware saves
    and restores PSR.I bit in spsr_el1 for exceptions and ERET.
    
    Add PMR to the registers to save in the pt_regs struct upon kernel entry,
    and restore it before ERET. Also, initialize it to a sane value when
    creating new tasks.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index a0f985a6ac50..6d410fc2849b 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -232,6 +232,9 @@ void __show_regs(struct pt_regs *regs)
 
 	printk("sp : %016llx\n", sp);
 
+	if (system_uses_irq_prio_masking())
+		printk("pmr_save: %08llx\n", regs->pmr_save);
+
 	i = top_reg;
 
 	while (i >= 0) {
@@ -363,6 +366,9 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		if (arm64_get_ssbd_state() == ARM64_SSBD_FORCE_DISABLE)
 			childregs->pstate |= PSR_SSBS_BIT;
 
+		if (system_uses_irq_prio_masking())
+			childregs->pmr_save = GIC_PRIO_IRQON;
+
 		p->thread.cpu_context.x19 = stack_start;
 		p->thread.cpu_context.x20 = stk_sz;
 	}

commit 495d714ad140e1732e66c45d0409054b24c1a0d6
Merge: f12e840c819b 3d739c1f6156
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 31 11:46:59 2018 -0800

    Merge tag 'trace-v4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
    
     - Rework of the kprobe/uprobe and synthetic events to consolidate all
       the dynamic event code. This will make changes in the future easier.
    
     - Partial rewrite of the function graph tracing infrastructure. This
       will allow for multiple users of hooking onto functions to get the
       callback (return) of the function. This is the ground work for having
       kprobes and function graph tracer using one code base.
    
     - Clean up of the histogram code that will facilitate adding more
       features to the histograms in the future.
    
     - Addition of str_has_prefix() and a few use cases. There currently is
       a similar function strstart() that is used in a few places, but only
       returns a bool and not a length. These instances will be removed in
       the future to use str_has_prefix() instead.
    
     - A few other various clean ups as well.
    
    * tag 'trace-v4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (57 commits)
      tracing: Use the return of str_has_prefix() to remove open coded numbers
      tracing: Have the historgram use the result of str_has_prefix() for len of prefix
      tracing: Use str_has_prefix() instead of using fixed sizes
      tracing: Use str_has_prefix() helper for histogram code
      string.h: Add str_has_prefix() helper function
      tracing: Make function ftrace_exports static
      tracing: Simplify printf'ing in seq_print_sym
      tracing: Avoid -Wformat-nonliteral warning
      tracing: Merge seq_print_sym_short() and seq_print_sym_offset()
      tracing: Add hist trigger comments for variable-related fields
      tracing: Remove hist trigger synth_var_refs
      tracing: Use hist trigger's var_ref array to destroy var_refs
      tracing: Remove open-coding of hist trigger var_ref management
      tracing: Use var_refs[] for hist trigger reference checking
      tracing: Change strlen to sizeof for hist trigger static strings
      tracing: Remove unnecessary hist trigger struct field
      tracing: Fix ftrace_graph_get_ret_stack() to use task and not current
      seq_buf: Use size_t for len in seq_buf_puts()
      seq_buf: Make seq_buf_puts() null-terminate the buffer
      arm64: Use ftrace_graph_get_ret_stack() instead of curr_ret_stack
      ...

commit a448276ce515c91cde4675be497364b91c764d95
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 7 13:13:28 2018 -0500

    arm64: Use ftrace_graph_get_ret_stack() instead of curr_ret_stack
    
    The structure of the ret_stack array on the task struct is going to
    change, and accessing it directly via the curr_ret_stack index will no
    longer give the ret_stack entry that holds the return address. To access
    that, architectures must now use ftrace_graph_get_ret_stack() to get the
    associated ret_stack that matches the saved return address.
    
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index d9a4c2d6dd8b..37a66394b07d 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -459,7 +459,7 @@ unsigned long get_wchan(struct task_struct *p)
 	frame.fp = thread_saved_fp(p);
 	frame.pc = thread_saved_pc(p);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	frame.graph = p->curr_ret_stack;
+	frame.graph = 0;
 #endif
 	do {
 		if (unwind_frame(p, &frame))

commit 7503197562567b57ec14feb3a9d5400ebc56812f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:25 2018 +0000

    arm64: add basic pointer authentication support
    
    This patch adds basic support for pointer authentication, allowing
    userspace to make use of APIAKey, APIBKey, APDAKey, APDBKey, and
    APGAKey. The kernel maintains key values for each process (shared by all
    threads within), which are initialised to random values at exec() time.
    
    The ID_AA64ISAR1_EL1.{APA,API,GPA,GPI} fields are exposed to userspace,
    to describe that pointer authentication instructions are available and
    that the kernel is managing the keys. Two new hwcaps are added for the
    same reason: PACA (for address authentication) and PACG (for generic
    authentication).
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Tested-by: Adam Wallis <awallis@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [will: Fix sizeof() usage and unroll address key initialisation]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 8a2d68f04e0d..e0a443730e04 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -57,6 +57,7 @@
 #include <asm/fpsimd.h>
 #include <asm/mmu_context.h>
 #include <asm/processor.h>
+#include <asm/pointer_auth.h>
 #include <asm/stacktrace.h>
 
 #if defined(CONFIG_STACKPROTECTOR) && !defined(CONFIG_STACKPROTECTOR_PER_TASK)
@@ -429,6 +430,7 @@ __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev,
 	contextidr_thread_switch(next);
 	entry_task_switch(next);
 	uao_thread_switch(next);
+	ptrauth_thread_switch(next);
 
 	/*
 	 * Complete any pending TLB or cache maintenance on this CPU in case
@@ -496,4 +498,6 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 void arch_setup_new_exec(void)
 {
 	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
+
+	ptrauth_thread_init_user(current);
 }

commit 0a1213fa7432778b71a1c0166bf56660a3aab030
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Dec 12 13:08:44 2018 +0100

    arm64: enable per-task stack canaries
    
    This enables the use of per-task stack canary values if GCC has
    support for emitting the stack canary reference relative to the
    value of sp_el0, which holds the task struct pointer in the arm64
    kernel.
    
    The $(eval) extends KBUILD_CFLAGS at the moment the make rule is
    applied, which means asm-offsets.o (which we rely on for the offset
    value) is built without the arguments, and everything built afterwards
    has the options set.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index d9a4c2d6dd8b..8a2d68f04e0d 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -59,7 +59,7 @@
 #include <asm/processor.h>
 #include <asm/stacktrace.h>
 
-#ifdef CONFIG_STACKPROTECTOR
+#if defined(CONFIG_STACKPROTECTOR) && !defined(CONFIG_STACKPROTECTOR_PER_TASK)
 #include <linux/stackprotector.h>
 unsigned long __stack_chk_guard __read_mostly;
 EXPORT_SYMBOL(__stack_chk_guard);

commit 2d6bb6adb714b133db92ccd4bfc9c20f75f71f3f
Merge: 7c6c54b505b8 6fcde9046673
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 1 11:46:27 2018 -0700

    Merge tag 'stackleak-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux
    
    Pull stackleak gcc plugin from Kees Cook:
     "Please pull this new GCC plugin, stackleak, for v4.20-rc1. This plugin
      was ported from grsecurity by Alexander Popov. It provides efficient
      stack content poisoning at syscall exit. This creates a defense
      against at least two classes of flaws:
    
       - Uninitialized stack usage. (We continue to work on improving the
         compiler to do this in other ways: e.g. unconditional zero init was
         proposed to GCC and Clang, and more plugin work has started too).
    
       - Stack content exposure. By greatly reducing the lifetime of valid
         stack contents, exposures via either direct read bugs or unknown
         cache side-channels become much more difficult to exploit. This
         complements the existing buddy and heap poisoning options, but
         provides the coverage for stacks.
    
      The x86 hooks are included in this series (which have been reviewed by
      Ingo, Dave Hansen, and Thomas Gleixner). The arm64 hooks have already
      been merged through the arm64 tree (written by Laura Abbott and
      reviewed by Mark Rutland and Will Deacon).
    
      With VLAs having been removed this release, there is no need for
      alloca() protection, so it has been removed from the plugin"
    
    * tag 'stackleak-v4.20-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux:
      arm64: Drop unneeded stackleak_check_alloca()
      stackleak: Allow runtime disabling of kernel stack erasing
      doc: self-protection: Add information about STACKLEAK feature
      fs/proc: Show STACKLEAK metrics in the /proc file system
      lkdtm: Add a test for STACKLEAK
      gcc-plugins: Add STACKLEAK plugin for tracking the kernel stack
      x86/entry: Add STACKLEAK erasing the kernel stack at the end of syscalls

commit 8f04e8e6e29c93421a95b61cad62e3918425eac7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:47:06 2018 +0100

    arm64: ssbd: Add support for PSTATE.SSBS rather than trapping to EL3
    
    On CPUs with support for PSTATE.SSBS, the kernel can toggle the SSBD
    state without needing to call into firmware.
    
    This patch hooks into the existing SSBD infrastructure so that SSBS is
    used on CPUs that support it, but it's all made horribly complicated by
    the very real possibility of big/little systems that don't uniformly
    provide the new capability.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 7f1628effe6d..ce99c58cd1f1 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -358,6 +358,10 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		if (IS_ENABLED(CONFIG_ARM64_UAO) &&
 		    cpus_have_const_cap(ARM64_HAS_UAO))
 			childregs->pstate |= PSR_UAO_BIT;
+
+		if (arm64_get_ssbd_state() == ARM64_SSBD_FORCE_DISABLE)
+			childregs->pstate |= PSR_SSBS_BIT;
+
 		p->thread.cpu_context.x19 = stack_start;
 		p->thread.cpu_context.x20 = stk_sz;
 	}

commit 6fcde90466738b84a073e4f4d18c50015ee29fb2
Author: Alexander Popov <alex.popov@linux.com>
Date:   Fri Aug 17 01:17:04 2018 +0300

    arm64: Drop unneeded stackleak_check_alloca()
    
    Drop stackleak_check_alloca() for arm64 since the STACKLEAK gcc plugin now
    doesn't track stack depth overflow caused by alloca().
    
    Signed-off-by: Alexander Popov <alex.popov@linux.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 7f1628effe6d..740b31f77ade 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -493,25 +493,3 @@ void arch_setup_new_exec(void)
 {
 	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
 }
-
-#ifdef CONFIG_GCC_PLUGIN_STACKLEAK
-void __used stackleak_check_alloca(unsigned long size)
-{
-	unsigned long stack_left;
-	unsigned long current_sp = current_stack_pointer;
-	struct stack_info info;
-
-	BUG_ON(!on_accessible_stack(current, current_sp, &info));
-
-	stack_left = current_sp - info.low;
-
-	/*
-	 * There's a good chance we're almost out of stack space if this
-	 * is true. Using panic() over BUG() is more likely to give
-	 * reliable debugging output.
-	 */
-	if (size >= stack_left)
-		panic("alloca() over the kernel stack boundary\n");
-}
-EXPORT_SYMBOL(stackleak_check_alloca);
-#endif

commit 0b3e336601b82c6afa0e9cf21db9cb8793e25399
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jul 20 14:41:54 2018 -0700

    arm64: Add support for STACKLEAK gcc plugin
    
    This adds support for the STACKLEAK gcc plugin to arm64 by implementing
    stackleak_check_alloca(), based heavily on the x86 version, and adding the
    two helpers used by the stackleak common code: current_top_of_stack() and
    on_thread_stack(). The stack erasure calls are made at syscall returns.
    Additionally, this disables the plugin in hypervisor and EFI stub code,
    which are out of scope for the protection.
    
    Acked-by: Alexander Popov <alex.popov@linux.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 740b31f77ade..7f1628effe6d 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -493,3 +493,25 @@ void arch_setup_new_exec(void)
 {
 	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
 }
+
+#ifdef CONFIG_GCC_PLUGIN_STACKLEAK
+void __used stackleak_check_alloca(unsigned long size)
+{
+	unsigned long stack_left;
+	unsigned long current_sp = current_stack_pointer;
+	struct stack_info info;
+
+	BUG_ON(!on_accessible_stack(current, current_sp, &info));
+
+	stack_left = current_sp - info.low;
+
+	/*
+	 * There's a good chance we're almost out of stack space if this
+	 * is true. Using panic() over BUG() is more likely to give
+	 * reliable debugging output.
+	 */
+	if (size >= stack_left)
+		panic("alloca() over the kernel stack boundary\n");
+}
+EXPORT_SYMBOL(stackleak_check_alloca);
+#endif

commit d64567f67835736d65086e9bfc41a19b2863c32e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 5 15:16:52 2018 +0100

    arm64: use PSR_AA32 definitions
    
    Some code cares about the SPSR_ELx format for exceptions taken from
    AArch32 to inspect or manipulate the SPSR_ELx value, which is already in
    the SPSR_ELx format, and not in the AArch32 PSR format.
    
    To separate these from cases where we care about the AArch32 PSR format,
    migrate these cases to use the PSR_AA32_* definitions rather than
    COMPAT_PSR_*.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index e10bc363f533..740b31f77ade 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -177,16 +177,16 @@ static void print_pstate(struct pt_regs *regs)
 	if (compat_user_mode(regs)) {
 		printk("pstate: %08llx (%c%c%c%c %c %s %s %c%c%c)\n",
 			pstate,
-			pstate & COMPAT_PSR_N_BIT ? 'N' : 'n',
-			pstate & COMPAT_PSR_Z_BIT ? 'Z' : 'z',
-			pstate & COMPAT_PSR_C_BIT ? 'C' : 'c',
-			pstate & COMPAT_PSR_V_BIT ? 'V' : 'v',
-			pstate & COMPAT_PSR_Q_BIT ? 'Q' : 'q',
-			pstate & COMPAT_PSR_T_BIT ? "T32" : "A32",
-			pstate & COMPAT_PSR_E_BIT ? "BE" : "LE",
-			pstate & COMPAT_PSR_A_BIT ? 'A' : 'a',
-			pstate & COMPAT_PSR_I_BIT ? 'I' : 'i',
-			pstate & COMPAT_PSR_F_BIT ? 'F' : 'f');
+			pstate & PSR_AA32_N_BIT ? 'N' : 'n',
+			pstate & PSR_AA32_Z_BIT ? 'Z' : 'z',
+			pstate & PSR_AA32_C_BIT ? 'C' : 'c',
+			pstate & PSR_AA32_V_BIT ? 'V' : 'v',
+			pstate & PSR_AA32_Q_BIT ? 'Q' : 'q',
+			pstate & PSR_AA32_T_BIT ? "T32" : "A32",
+			pstate & PSR_AA32_E_BIT ? "BE" : "LE",
+			pstate & PSR_AA32_A_BIT ? 'A' : 'a',
+			pstate & PSR_AA32_I_BIT ? 'I' : 'i',
+			pstate & PSR_AA32_F_BIT ? 'F' : 'f');
 	} else {
 		printk("pstate: %08llx (%c%c%c%c %c%c%c%c %cPAN %cUAO)\n",
 			pstate,

commit 050e9baa9dc9fbd9ce2b27f0056990fc9e0a08a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 14 12:21:18 2018 +0900

    Kbuild: rename CC_STACKPROTECTOR[_STRONG] config variables
    
    The changes to automatically test for working stack protector compiler
    support in the Kconfig files removed the special STACKPROTECTOR_AUTO
    option that picked the strongest stack protector that the compiler
    supported.
    
    That was all a nice cleanup - it makes no sense to have the AUTO case
    now that the Kconfig phase can just determine the compiler support
    directly.
    
    HOWEVER.
    
    It also meant that doing "make oldconfig" would now _disable_ the strong
    stackprotector if you had AUTO enabled, because in a legacy config file,
    the sane stack protector configuration would look like
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_NONE is not set
      # CONFIG_CC_STACKPROTECTOR_REGULAR is not set
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_STACKPROTECTOR_AUTO=y
    
    and when you ran this through "make oldconfig" with the Kbuild changes,
    it would ask you about the regular CONFIG_CC_STACKPROTECTOR (that had
    been renamed from CONFIG_CC_STACKPROTECTOR_REGULAR to just
    CONFIG_CC_STACKPROTECTOR), but it would think that the STRONG version
    used to be disabled (because it was really enabled by AUTO), and would
    disable it in the new config, resulting in:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_CC_STACKPROTECTOR=y
      # CONFIG_CC_STACKPROTECTOR_STRONG is not set
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    That's dangerously subtle - people could suddenly find themselves with
    the weaker stack protector setup without even realizing.
    
    The solution here is to just rename not just the old RECULAR stack
    protector option, but also the strong one.  This does that by just
    removing the CC_ prefix entirely for the user choices, because it really
    is not about the compiler support (the compiler support now instead
    automatially impacts _visibility_ of the options to users).
    
    This results in "make oldconfig" actually asking the user for their
    choice, so that we don't have any silent subtle security model changes.
    The end result would generally look like this:
    
      CONFIG_HAVE_CC_STACKPROTECTOR=y
      CONFIG_CC_HAS_STACKPROTECTOR_NONE=y
      CONFIG_STACKPROTECTOR=y
      CONFIG_STACKPROTECTOR_STRONG=y
      CONFIG_CC_HAS_SANE_STACKPROTECTOR=y
    
    where the "CC_" versions really are about internal compiler
    infrastructure, not the user selections.
    
    Acked-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f08a2ed9db0d..e10bc363f533 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -59,7 +59,7 @@
 #include <asm/processor.h>
 #include <asm/stacktrace.h>
 
-#ifdef CONFIG_CC_STACKPROTECTOR
+#ifdef CONFIG_STACKPROTECTOR
 #include <linux/stackprotector.h>
 unsigned long __stack_chk_guard __read_mostly;
 EXPORT_SYMBOL(__stack_chk_guard);

commit 65896545b69ffaac947c12e11d3dcc57fd1fb772
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Mar 28 10:50:49 2018 +0100

    arm64: uaccess: Fix omissions from usercopy whitelist
    
    When the hardend usercopy support was added for arm64, it was
    concluded that all cases of usercopy into and out of thread_struct
    were statically sized and so didn't require explicit whitelisting
    of the appropriate fields in thread_struct.
    
    Testing with usercopy hardening enabled has revealed that this is
    not the case for certain ptrace regset manipulation calls on arm64.
    This occurs because the sizes of usercopies associated with the
    regset API are dynamic by construction, and because arm64 does not
    always stage such copies via the stack: indeed the regset API is
    designed to avoid the need for that by adding some bounds checking.
    
    This is currently believed to affect only the fpsimd and TLS
    registers.
    
    Because the whitelisted fields in thread_struct must be contiguous,
    this patch groups them together in a nested struct.  It is also
    necessary to be able to determine the location and size of that
    struct, so rather than making the struct anonymous (which would
    save on edits elsewhere) or adding an anonymous union containing
    named and unnamed instances of the same struct (gross), this patch
    gives the struct a name and makes the necessary edits to code that
    references it (noisy but simple).
    
    Care is needed to ensure that the new struct does not contain
    padding (which the usercopy hardening would fail to protect).
    
    For this reason, the presence of tp2_value is made unconditional,
    since a padding field would be needed there in any case.  This pads
    up to the 16-byte alignment required by struct user_fpsimd_state.
    
    Acked-by: Kees Cook <keescook@chromium.org>
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 9e8084d3f761 ("arm64: Implement thread_struct whitelist for hardened usercopy")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c0da6efe5465..f08a2ed9db0d 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -257,7 +257,7 @@ static void tls_thread_flush(void)
 	write_sysreg(0, tpidr_el0);
 
 	if (is_compat_task()) {
-		current->thread.tp_value = 0;
+		current->thread.uw.tp_value = 0;
 
 		/*
 		 * We need to ensure ordering between the shadow state and the
@@ -351,7 +351,7 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		 * for the new thread.
 		 */
 		if (clone_flags & CLONE_SETTLS)
-			p->thread.tp_value = childregs->regs[3];
+			p->thread.uw.tp_value = childregs->regs[3];
 	} else {
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->pstate = PSR_MODE_EL1h;
@@ -379,7 +379,7 @@ static void tls_thread_switch(struct task_struct *next)
 	tls_preserve_current_state();
 
 	if (is_compat_thread(task_thread_info(next)))
-		write_sysreg(next->thread.tp_value, tpidrro_el0);
+		write_sysreg(next->thread.uw.tp_value, tpidrro_el0);
 	else if (!arm64_kernel_unmapped_at_el0())
 		write_sysreg(0, tpidrro_el0);
 

commit a06f818a70de21b4b3b4186816094208fc7accf9
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 19 16:46:57 2018 +0000

    arm64: __show_regs: Only resolve kernel symbols when running at EL1
    
    __show_regs pretty prints PC and LR by attempting to map them to kernel
    function names to improve the utility of crash reports. Unfortunately,
    this mapping is applied even when the pt_regs corresponds to user mode,
    resulting in a KASLR oracle.
    
    Avoid this issue by only looking up the function symbols when the register
    state indicates that we're actually running at EL1.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: NCSC Security <security@ncsc.gov.uk>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index ad8aeb098b31..c0da6efe5465 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -220,8 +220,15 @@ void __show_regs(struct pt_regs *regs)
 
 	show_regs_print_info(KERN_DEFAULT);
 	print_pstate(regs);
-	printk("pc : %pS\n", (void *)regs->pc);
-	printk("lr : %pS\n", (void *)lr);
+
+	if (!user_mode(regs)) {
+		printk("pc : %pS\n", (void *)regs->pc);
+		printk("lr : %pS\n", (void *)lr);
+	} else {
+		printk("pc : %016llx\n", regs->pc);
+		printk("lr : %016llx\n", lr);
+	}
+
 	printk("sp : %016llx\n", sp);
 
 	i = top_reg;

commit ab486bc9a591689f3ac2b6ebc072309371f8f451
Merge: 34b1cf60abb0 bb4f552a598d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 1 13:36:15 2018 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/pmladek/printk
    
    Pull printk updates from Petr Mladek:
    
     - Add a console_msg_format command line option:
    
         The value "default" keeps the old "[time stamp] text\n" format. The
         value "syslog" allows to see the syslog-like "<log
         level>[timestamp] text" format.
    
         This feature was requested by people doing regression tests, for
         example, 0day robot. They want to have both filtered and full logs
         at hands.
    
     - Reduce the risk of softlockup:
    
         Pass the console owner in a busy loop.
    
         This is a new approach to the old problem. It was first proposed by
         Steven Rostedt on Kernel Summit 2017. It marks a context in which
         the console_lock owner calls console drivers and could not sleep.
         On the other side, printk() callers could detect this state and use
         a busy wait instead of a simple console_trylock(). Finally, the
         console_lock owner checks if there is a busy waiter at the end of
         the special context and eventually passes the console_lock to the
         waiter.
    
         The hand-off works surprisingly well and helps in many situations.
         Well, there is still a possibility of the softlockup, for example,
         when the flood of messages stops and the last owner still has too
         much to flush.
    
         There is increasing number of people having problems with
         printk-related softlockups. We might eventually need to get better
         solution. Anyway, this looks like a good start and promising
         direction.
    
     - Do not allow to schedule in console_unlock() called from printk():
    
         This reverts an older controversial commit. The reschedule helped
         to avoid softlockups. But it also slowed down the console output.
         This patch is obsoleted by the new console waiter logic described
         above. In fact, the reschedule made the hand-off less effective.
    
     - Deprecate "%pf" and "%pF" format specifier:
    
         It was needed on ia64, ppc64 and parisc64 to dereference function
         descriptors and show the real function address. It is done
         transparently by "%ps" and "pS" format specifier now.
    
         Sergey Senozhatsky found that all the function descriptors were in
         a special elf section and could be easily detected.
    
     - Remove printk_symbol() API:
    
         It has been obsoleted by "%pS" format specifier, and this change
         helped to remove few continuous lines and a less intuitive old API.
    
     - Remove redundant memsets:
    
         Sergey removed unnecessary memset when processing printk.devkmsg
         command line option.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/pmladek/printk: (27 commits)
      printk: drop redundant devkmsg_log_str memsets
      printk: Never set console_may_schedule in console_trylock()
      printk: Hide console waiter logic into helpers
      printk: Add console owner and waiter logic to load balance console writes
      kallsyms: remove print_symbol() function
      checkpatch: add pF/pf deprecation warning
      symbol lookup: introduce dereference_symbol_descriptor()
      parisc64: Add .opd based function descriptor dereference
      powerpc64: Add .opd based function descriptor dereference
      ia64: Add .opd based function descriptor dereference
      sections: split dereference_function_descriptor()
      openrisc: Fix conflicting types for _exext and _stext
      lib: do not use print_symbol()
      irq debug: do not use print_symbol()
      sysfs: do not use print_symbol()
      drivers: do not use print_symbol()
      x86: do not use print_symbol()
      unicore32: do not use print_symbol()
      sh: do not use print_symbol()
      mn10300: do not use print_symbol()
      ...

commit 4ef7963843d3243260aa335dfb9cb2fede06aacf
Author: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
Date:   Mon Dec 11 21:50:14 2017 +0900

    arm64: do not use print_symbol()
    
    print_symbol() is a very old API that has been obsoleted by %pS format
    specifier in a normal printk() call.
    
    Replace print_symbol() with a direct printk("%pS") call.
    
    Link: http://lkml.kernel.org/r/20171211125025.2270-3-sergey.senozhatsky@gmail.com
    To: Andrew Morton <akpm@linux-foundation.org>
    To: Russell King <linux@armlinux.org.uk>
    To: Catalin Marinas <catalin.marinas@arm.com>
    To: Mark Salter <msalter@redhat.com>
    To: Tony Luck <tony.luck@intel.com>
    To: David Howells <dhowells@redhat.com>
    To: Yoshinori Sato <ysato@users.sourceforge.jp>
    To: Guan Xuetao <gxt@mprc.pku.edu.cn>
    To: Borislav Petkov <bp@alien8.de>
    To: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    To: Thomas Gleixner <tglx@linutronix.de>
    To: Peter Zijlstra <peterz@infradead.org>
    To: Vineet Gupta <vgupta@synopsys.com>
    To: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: LKML <linux-kernel@vger.kernel.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-c6x-dev@linux-c6x.org
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-am33-list@redhat.com
    Cc: linux-sh@vger.kernel.org
    Cc: linux-edac@vger.kernel.org
    Cc: x86@kernel.org
    Cc: linux-snps-arc@lists.infradead.org
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    [pmladek@suse.com: updated commit message]
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index b2adcce7bc18..37c9470cbdaa 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -35,7 +35,6 @@
 #include <linux/delay.h>
 #include <linux/reboot.h>
 #include <linux/interrupt.h>
-#include <linux/kallsyms.h>
 #include <linux/init.h>
 #include <linux/cpu.h>
 #include <linux/elfcore.h>
@@ -221,8 +220,8 @@ void __show_regs(struct pt_regs *regs)
 
 	show_regs_print_info(KERN_DEFAULT);
 	print_pstate(regs);
-	print_symbol("pc : %s\n", regs->pc);
-	print_symbol("lr : %s\n", lr);
+	printk("pc : %pS\n", (void *)regs->pc);
+	printk("lr : %pS\n", (void *)lr);
 	printk("sp : %016llx\n", sp);
 
 	i = top_reg;

commit 18011eac28c7cb31c87b86b7d0e5b01894405c7f
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 14 14:33:28 2017 +0000

    arm64: tls: Avoid unconditional zeroing of tpidrro_el0 for native tasks
    
    When unmapping the kernel at EL0, we use tpidrro_el0 as a scratch register
    during exception entry from native tasks and subsequently zero it in
    the kernel_ventry macro. We can therefore avoid zeroing tpidrro_el0
    in the context-switch path for native tasks using the entry trampoline.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6b7dcf4310ac..583fd8154695 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -370,16 +370,14 @@ void tls_preserve_current_state(void)
 
 static void tls_thread_switch(struct task_struct *next)
 {
-	unsigned long tpidr, tpidrro;
-
 	tls_preserve_current_state();
 
-	tpidr = *task_user_tls(next);
-	tpidrro = is_compat_thread(task_thread_info(next)) ?
-		  next->thread.tp_value : 0;
+	if (is_compat_thread(task_thread_info(next)))
+		write_sysreg(next->thread.tp_value, tpidrro_el0);
+	else if (!arm64_kernel_unmapped_at_el0())
+		write_sysreg(0, tpidrro_el0);
 
-	write_sysreg(tpidr, tpidr_el0);
-	write_sysreg(tpidrro, tpidrro_el0);
+	write_sysreg(*task_user_tls(next), tpidr_el0);
 }
 
 /* Restore the UAO state depending on next's addr_limit */

commit 071b6d4a5d343046f253a5a8835d477d93992002
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Dec 5 14:56:42 2017 +0000

    arm64: fpsimd: Prevent registers leaking from dead tasks
    
    Currently, loading of a task's fpsimd state into the CPU registers
    is skipped if that task's state is already present in the registers
    of that CPU.
    
    However, the code relies on the struct fpsimd_state * (and by
    extension struct task_struct *) to unambiguously identify a task.
    
    There is a particular case in which this doesn't work reliably:
    when a task exits, its task_struct may be recycled to describe a
    new task.
    
    Consider the following scenario:
    
     1) Task P loads its fpsimd state onto cpu C.
            per_cpu(fpsimd_last_state, C) := P;
            P->thread.fpsimd_state.cpu := C;
    
     2) Task X is scheduled onto C and loads its fpsimd state on C.
            per_cpu(fpsimd_last_state, C) := X;
            X->thread.fpsimd_state.cpu := C;
    
     3) X exits, causing X's task_struct to be freed.
    
     4) P forks a new child T, which obtains X's recycled task_struct.
            T == X.
            T->thread.fpsimd_state.cpu == C (inherited from P).
    
     5) T is scheduled on C.
            T's fpsimd state is not loaded, because
            per_cpu(fpsimd_last_state, C) == T (== X) &&
            T->thread.fpsimd_state.cpu == C.
    
            (This is the check performed by fpsimd_thread_switch().)
    
    So, T gets X's registers because the last registers loaded onto C
    were those of X, in (2).
    
    This patch fixes the problem by ensuring that the sched-in check
    fails in (5): fpsimd_flush_task_state(T) is called when T is
    forked, so that T->thread.fpsimd_state.cpu == C cannot be true.
    This relies on the fact that T is not schedulable until after
    copy_thread() completes.
    
    Once T's fpsimd state has been loaded on some CPU C there may still
    be other cpus D for which per_cpu(fpsimd_last_state, D) ==
    &X->thread.fpsimd_state.  But D is necessarily != C in this case,
    and the check in (5) must fail.
    
    An alternative fix would be to do refcounting on task_struct.  This
    would result in each CPU holding a reference to the last task whose
    fpsimd state was loaded there.  It's not clear whether this is
    preferable, and it involves higher overhead than the fix proposed
    in this patch.  It would also move all the task_struct freeing
    work into the context switch critical section, or otherwise some
    deferred cleanup mechanism would need to be introduced, neither of
    which seems obviously justified.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 005f78cd8849 ("arm64: defer reloading a task's FPSIMD state to userland resume")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [will: word-smithed the comment so it makes more sense]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index b2adcce7bc18..6b7dcf4310ac 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -314,6 +314,15 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	clear_tsk_thread_flag(p, TIF_SVE);
 	p->thread.sve_state = NULL;
 
+	/*
+	 * In case p was allocated the same task_struct pointer as some
+	 * other recently-exited task, make sure p is disassociated from
+	 * any cpu that may have run that now-exited task recently.
+	 * Otherwise we could erroneously skip reloading the FPSIMD
+	 * registers for p.
+	 */
+	fpsimd_flush_task_state(p);
+
 	if (likely(!(p->flags & PF_KTHREAD))) {
 		*childregs = *current_pt_regs();
 		childregs->regs[0] = 0;

commit bc0ee476036478a85beeed51f0d94c8729fd0544
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:05 2017 +0000

    arm64/sve: Core task context handling
    
    This patch adds the core support for switching and managing the SVE
    architectural state of user tasks.
    
    Calls to the existing FPSIMD low-level save/restore functions are
    factored out as new functions task_fpsimd_{save,load}(), since SVE
    now dynamically may or may not need to be handled at these points
    depending on the kernel configuration, hardware features discovered
    at boot, and the runtime state of the task.  To make these
    decisions as fast as possible, const cpucaps are used where
    feasible, via the system_supports_sve() helper.
    
    The SVE registers are only tracked for threads that have explicitly
    used SVE, indicated by the new thread flag TIF_SVE.  Otherwise, the
    FPSIMD view of the architectural state is stored in
    thread.fpsimd_state as usual.
    
    When in use, the SVE registers are not stored directly in
    thread_struct due to their potentially large and variable size.
    Because the task_struct slab allocator must be configured very
    early during kernel boot, it is also tricky to configure it
    correctly to match the maximum vector length provided by the
    hardware, since this depends on examining secondary CPUs as well as
    the primary.  Instead, a pointer sve_state in thread_struct points
    to a dynamically allocated buffer containing the SVE register data,
    and code is added to allocate and free this buffer at appropriate
    times.
    
    TIF_SVE is set when taking an SVE access trap from userspace, if
    suitable hardware support has been detected.  This enables SVE for
    the thread: a subsequent return to userspace will disable the trap
    accordingly.  If such a trap is taken without sufficient system-
    wide hardware support, SIGILL is sent to the thread instead as if
    an undefined instruction had been executed: this may happen if
    userspace tries to use SVE in a system where not all CPUs support
    it for example.
    
    The kernel will clear TIF_SVE and disable SVE for the thread
    whenever an explicit syscall is made by userspace.  For backwards
    compatibility reasons and conformance with the spirit of the base
    AArch64 procedure call standard, the subset of the SVE register
    state that aliases the FPSIMD registers is still preserved across a
    syscall even if this happens.  The remainder of the SVE register
    state logically becomes zero at syscall entry, though the actual
    zeroing work is currently deferred until the thread next tries to
    use SVE, causing another trap to the kernel.  This implementation
    is suboptimal: in the future, the fastpath case may be optimised
    to zero the registers in-place and leave SVE enabled for the task,
    where beneficial.
    
    TIF_SVE is also cleared in the following slowpath cases, which are
    taken as reasonable hints that the task may no longer use SVE:
     * exec
     * fork and clone
    
    Code is added to sync data between thread.fpsimd_state and
    thread.sve_state whenever enabling/disabling SVE, in a manner
    consistent with the SVE architectural programmer's model.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Alex Benne <alex.bennee@linaro.org>
    [will: added #include to fix allnoconfig build]
    [will: use enable_daif in do_sve_acc]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c15ec41e9f84..b2adcce7bc18 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -49,6 +49,7 @@
 #include <linux/notifier.h>
 #include <trace/events/power.h>
 #include <linux/percpu.h>
+#include <linux/thread_info.h>
 
 #include <asm/alternative.h>
 #include <asm/compat.h>
@@ -273,11 +274,27 @@ void release_thread(struct task_struct *dead_task)
 {
 }
 
+void arch_release_task_struct(struct task_struct *tsk)
+{
+	fpsimd_release_task(tsk);
+}
+
+/*
+ * src and dst may temporarily have aliased sve_state after task_struct
+ * is copied.  We cannot fix this properly here, because src may have
+ * live SVE state and dst's thread_info may not exist yet, so tweaking
+ * either src's or dst's TIF_SVE is not safe.
+ *
+ * The unaliasing is done in copy_thread() instead.  This works because
+ * dst is not schedulable or traceable until both of these functions
+ * have been called.
+ */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
 	if (current->mm)
 		fpsimd_preserve_current_state();
 	*dst = *src;
+
 	return 0;
 }
 
@@ -290,6 +307,13 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 
 	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
 
+	/*
+	 * Unalias p->thread.sve_state (if any) from the parent task
+	 * and disable discard SVE state for p:
+	 */
+	clear_tsk_thread_flag(p, TIF_SVE);
+	p->thread.sve_state = NULL;
+
 	if (likely(!(p->flags & PF_KTHREAD))) {
 		*childregs = *current_pt_regs();
 		childregs->regs[0] = 0;

commit b7300d4c035cc98c7f5c40e2c4e53f58c16617ea
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Oct 19 13:26:26 2017 +0100

    arm64: traps: Pretty-print pstate in register dumps
    
    We can decode the PSTATE easily enough, so pretty-print it in register
    dumps.
    
    Tested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c20896b8fb2d..c15ec41e9f84 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -170,6 +170,39 @@ void machine_restart(char *cmd)
 	while (1);
 }
 
+static void print_pstate(struct pt_regs *regs)
+{
+	u64 pstate = regs->pstate;
+
+	if (compat_user_mode(regs)) {
+		printk("pstate: %08llx (%c%c%c%c %c %s %s %c%c%c)\n",
+			pstate,
+			pstate & COMPAT_PSR_N_BIT ? 'N' : 'n',
+			pstate & COMPAT_PSR_Z_BIT ? 'Z' : 'z',
+			pstate & COMPAT_PSR_C_BIT ? 'C' : 'c',
+			pstate & COMPAT_PSR_V_BIT ? 'V' : 'v',
+			pstate & COMPAT_PSR_Q_BIT ? 'Q' : 'q',
+			pstate & COMPAT_PSR_T_BIT ? "T32" : "A32",
+			pstate & COMPAT_PSR_E_BIT ? "BE" : "LE",
+			pstate & COMPAT_PSR_A_BIT ? 'A' : 'a',
+			pstate & COMPAT_PSR_I_BIT ? 'I' : 'i',
+			pstate & COMPAT_PSR_F_BIT ? 'F' : 'f');
+	} else {
+		printk("pstate: %08llx (%c%c%c%c %c%c%c%c %cPAN %cUAO)\n",
+			pstate,
+			pstate & PSR_N_BIT ? 'N' : 'n',
+			pstate & PSR_Z_BIT ? 'Z' : 'z',
+			pstate & PSR_C_BIT ? 'C' : 'c',
+			pstate & PSR_V_BIT ? 'V' : 'v',
+			pstate & PSR_D_BIT ? 'D' : 'd',
+			pstate & PSR_A_BIT ? 'A' : 'a',
+			pstate & PSR_I_BIT ? 'I' : 'i',
+			pstate & PSR_F_BIT ? 'F' : 'f',
+			pstate & PSR_PAN_BIT ? '+' : '-',
+			pstate & PSR_UAO_BIT ? '+' : '-');
+	}
+}
+
 void __show_regs(struct pt_regs *regs)
 {
 	int i, top_reg;
@@ -186,9 +219,10 @@ void __show_regs(struct pt_regs *regs)
 	}
 
 	show_regs_print_info(KERN_DEFAULT);
+	print_pstate(regs);
 	print_symbol("pc : %s\n", regs->pc);
 	print_symbol("lr : %s\n", lr);
-	printk("sp : %016llx pstate : %08llx\n", sp, regs->pstate);
+	printk("sp : %016llx\n", sp);
 
 	i = top_reg;
 

commit a25ffd3a6302a67814280274d8f1aa4ae2ea4b59
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Oct 19 13:19:20 2017 +0100

    arm64: traps: Don't print stack or raw PC/LR values in backtraces
    
    Printing raw pointer values in backtraces has potential security
    implications and are of questionable value anyway.
    
    This patch follows x86's lead and removes the "Exception stack:" dump
    from kernel backtraces, as well as converting PC/LR values to symbols
    such as "sysrq_handle_crash+0x20/0x30".
    
    Tested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 2dc0f8482210..c20896b8fb2d 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -186,11 +186,9 @@ void __show_regs(struct pt_regs *regs)
 	}
 
 	show_regs_print_info(KERN_DEFAULT);
-	print_symbol("PC is at %s\n", instruction_pointer(regs));
-	print_symbol("LR is at %s\n", lr);
-	printk("pc : [<%016llx>] lr : [<%016llx>] pstate: %08llx\n",
-	       regs->pc, lr, regs->pstate);
-	printk("sp : %016llx\n", sp);
+	print_symbol("pc : %s\n", regs->pc);
+	print_symbol("lr : %s\n", lr);
+	printk("sp : %016llx pstate : %08llx\n", sp, regs->pstate);
 
 	i = top_reg;
 

commit 04759194dc447ff0b9ef35bc641ce3bb076c2930
Merge: 9e85ae6af6e9 d1be5c99a034
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 5 09:53:37 2017 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - VMAP_STACK support, allowing the kernel stacks to be allocated in the
       vmalloc space with a guard page for trapping stack overflows. One of
       the patches introduces THREAD_ALIGN and changes the generic
       alloc_thread_stack_node() to use this instead of THREAD_SIZE (no
       functional change for other architectures)
    
     - Contiguous PTE hugetlb support re-enabled (after being reverted a
       couple of times). We now have the semantics agreed in the generic mm
       layer together with API improvements so that the architecture code
       can detect between contiguous and non-contiguous huge PTEs
    
     - Initial support for persistent memory on ARM: DC CVAP instruction
       exposed to user space (HWCAP) and the in-kernel pmem API implemented
    
     - raid6 improvements for arm64: faster algorithm for the delta syndrome
       and implementation of the recovery routines using Neon
    
     - FP/SIMD refactoring and removal of support for Neon in interrupt
       context. This is in preparation for full SVE support
    
     - PTE accessors converted from inline asm to cmpxchg so that we can use
       LSE atomics if available (ARMv8.1)
    
     - Perf support for Cortex-A35 and A73
    
     - Non-urgent fixes and cleanups
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (75 commits)
      arm64: cleanup {COMPAT_,}SET_PERSONALITY() macro
      arm64: introduce separated bits for mm_context_t flags
      arm64: hugetlb: Cleanup setup_hugepagesz
      arm64: Re-enable support for contiguous hugepages
      arm64: hugetlb: Override set_huge_swap_pte_at() to support contiguous hugepages
      arm64: hugetlb: Override huge_pte_clear() to support contiguous hugepages
      arm64: hugetlb: Handle swap entries in huge_pte_offset() for contiguous hugepages
      arm64: hugetlb: Add break-before-make logic for contiguous entries
      arm64: hugetlb: Spring clean huge pte accessors
      arm64: hugetlb: Introduce pte_pgprot helper
      arm64: hugetlb: set_huge_pte_at Add WARN_ON on !pte_present
      arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
      arm64: dma-mapping: Mark atomic_pool as __ro_after_init
      arm64: dma-mapping: Do not pass data to gen_pool_set_algo()
      arm64: Remove the !CONFIG_ARM64_HW_AFDBM alternative code paths
      arm64: Ignore hardware dirty bit updates in ptep_set_wrprotect()
      arm64: Move PTE_RDONLY bit handling out of set_pte_at()
      kvm: arm64: Convert kvm_set_s2pte_readonly() from inline asm to cmpxchg()
      arm64: Convert pte handling from inline asm to using (cmp)xchg
      arm64: neon/efi: Make EFI fpsimd save/restore variables static
      ...

commit d1be5c99a0341249bf6f74eb1cbc3d5fc4ef2be7
Author: Yury Norov <ynorov@caviumnetworks.com>
Date:   Sun Aug 20 13:20:48 2017 +0300

    arm64: cleanup {COMPAT_,}SET_PERSONALITY() macro
    
    There is some work that should be done after setting the personality.
    Currently it's done in the macro, which is not the best idea.
    
    In this patch new arch_setup_new_exec() routine is introduced, and all
    setup code is moved there, as suggested by Catalin:
    https://lkml.org/lkml/2017/8/4/494
    
    Cc: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Yury Norov <ynorov@caviumnetworks.com>
    [catalin.marinas@arm.com: comments changed or removed]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 85b953dd023a..e6bf19c1dddb 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -414,3 +414,11 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	else
 		return randomize_page(mm->brk, SZ_1G);
 }
+
+/*
+ * Called from setup_new_exec() after (COMPAT_)SET_PERSONALITY.
+ */
+void arch_setup_new_exec(void)
+{
+	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
+}

commit 22e4ebb975822833b083533035233d128b30e98f
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Fri Jul 28 16:40:40 2017 -0400

    membarrier: Provide expedited private command
    
    Implement MEMBARRIER_CMD_PRIVATE_EXPEDITED with IPIs using cpumask built
    from all runqueues for which current thread's mm is the same as the
    thread calling sys_membarrier. It executes faster than the non-expedited
    variant (no blocking). It also works on NOHZ_FULL configurations.
    
    Scheduler-wise, it requires a memory barrier before and after context
    switching between processes (which have different mm). The memory
    barrier before context switch is already present. For the barrier after
    context switch:
    
    * Our TSO archs can do RELEASE without being a full barrier. Look at
      x86 spin_unlock() being a regular STORE for example.  But for those
      archs, all atomics imply smp_mb and all of them have atomic ops in
      switch_mm() for mm_cpumask(), and on x86 the CR3 load acts as a full
      barrier.
    
    * From all weakly ordered machines, only ARM64 and PPC can do RELEASE,
      the rest does indeed do smp_mb(), so there the spin_unlock() is a full
      barrier and we're good.
    
    * ARM64 has a very heavy barrier in switch_to(), which suffices.
    
    * PPC just removed its barrier from switch_to(), but appears to be
      talking about adding something to switch_mm(). So add a
      smp_mb__after_unlock_lock() for now, until this is settled on the PPC
      side.
    
    Changes since v3:
    - Properly document the memory barriers provided by each architecture.
    
    Changes since v2:
    - Address comments from Peter Zijlstra,
    - Add smp_mb__after_unlock_lock() after finish_lock_switch() in
      finish_task_switch() to add the memory barrier we need after storing
      to rq->curr. This is much simpler than the previous approach relying
      on atomic_dec_and_test() in mmdrop(), which actually added a memory
      barrier in the common case of switching between userspace processes.
    - Return -EINVAL when MEMBARRIER_CMD_SHARED is used on a nohz_full
      kernel, rather than having the whole membarrier system call returning
      -ENOSYS. Indeed, CMD_PRIVATE_EXPEDITED is compatible with nohz_full.
      Adapt the CMD_QUERY mask accordingly.
    
    Changes since v1:
    - move membarrier code under kernel/sched/ because it uses the
      scheduler runqueue,
    - only add the barrier when we switch from a kernel thread. The case
      where we switch from a user-space thread is already handled by
      the atomic_dec_and_test() in mmdrop().
    - add a comment to mmdrop() documenting the requirement on the implicit
      memory barrier.
    
    CC: Peter Zijlstra <peterz@infradead.org>
    CC: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    CC: Boqun Feng <boqun.feng@gmail.com>
    CC: Andrew Hunter <ahh@google.com>
    CC: Maged Michael <maged.michael@gmail.com>
    CC: gromer@google.com
    CC: Avi Kivity <avi@scylladb.com>
    CC: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    CC: Paul Mackerras <paulus@samba.org>
    CC: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Dave Watson <davejwatson@fb.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 659ae8094ed5..c8f7d98d8cb9 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -360,6 +360,8 @@ __notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev,
 	/*
 	 * Complete any pending TLB or cache maintenance on this CPU in case
 	 * the thread migrates to a different CPU.
+	 * This full barrier is also required by the membarrier system
+	 * call.
 	 */
 	dsb(ish);
 

commit 31e43ad3b74a5d7b282023b72f25fc677c14c727
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sun Jul 23 09:05:38 2017 +0100

    arm64: unwind: remove sp from struct stackframe
    
    The unwind code sets the sp member of struct stackframe to
    'frame pointer + 0x10' unconditionally, without regard for whether
    doing so produces a legal value. So let's simply remove it now that
    we have stopped using it anyway.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 659ae8094ed5..85b953dd023a 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -382,15 +382,12 @@ unsigned long get_wchan(struct task_struct *p)
 		return 0;
 
 	frame.fp = thread_saved_fp(p);
-	frame.sp = thread_saved_sp(p);
 	frame.pc = thread_saved_pc(p);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	frame.graph = p->curr_ret_stack;
 #endif
 	do {
-		if (frame.sp < stack_page ||
-		    frame.sp >= stack_page + THREAD_SIZE ||
-		    unwind_frame(p, &frame))
+		if (unwind_frame(p, &frame))
 			goto out;
 		if (!in_sched_functions(frame.pc)) {
 			ret = frame.pc;

commit 936eb65ca22ad856cb3a995e8cd742e982dc2dd0
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jun 21 16:00:44 2017 +0100

    arm64: ptrace: Flush user-RW TLS reg to thread_struct before reading
    
    When reading current's user-writable TLS register (which occurs
    when dumping core for native tasks), it is possible that userspace
    has modified it since the time the task was last scheduled out.
    The new TLS register value is not guaranteed to have been written
    immediately back to thread_struct in this case.
    
    As a result, a coredump can capture stale data for this register.
    Reading the register for a stopped task via ptrace is unaffected.
    
    For native tasks, this patch explicitly flushes the TPIDR_EL0
    register back to thread_struct before dumping when operating on
    current, thus ensuring that coredump contents are up to date.  For
    compat tasks, the TLS register is not user-writable and so cannot
    be out of sync, so no flush is required in compat_tls_get().
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index af1ea258c212..659ae8094ed5 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -298,12 +298,16 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	return 0;
 }
 
+void tls_preserve_current_state(void)
+{
+	*task_user_tls(current) = read_sysreg(tpidr_el0);
+}
+
 static void tls_thread_switch(struct task_struct *next)
 {
 	unsigned long tpidr, tpidrro;
 
-	tpidr = read_sysreg(tpidr_el0);
-	*task_user_tls(current) = tpidr;
+	tls_preserve_current_state();
 
 	tpidr = *task_user_tls(next);
 	tpidrro = is_compat_thread(task_thread_info(next)) ?

commit 1149aad10b1e2f2cf1ea023889ac8604ae869c5a
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Tue May 9 09:53:37 2017 +0800

    arm64: Add dump_backtrace() in show_regs
    
    Generic code expects show_regs() to dump the stack, but arm64's
    show_regs() does not. This makes it hard to debug softlockups and
    other issues that result in show_regs() being called.
    
    This patch updates arm64's show_regs() to dump the stack, as common
    code expects.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    [will: folded in bug_handler fix from mrutland]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index ae2a835898d7..af1ea258c212 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -210,6 +210,7 @@ void __show_regs(struct pt_regs *regs)
 void show_regs(struct pt_regs * regs)
 {
 	__show_regs(regs);
+	dump_backtrace(regs, NULL);
 }
 
 static void tls_thread_flush(void)

commit 29d981217a5d091fb423763d0a6c8e390b20237b
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Mon Mar 20 20:42:55 2017 +0800

    arm64: drop unnecessary newlines in show_regs()
    
    There are two unnecessary newlines, one is in show_regs, another
    is in __show_regs(), drop them.
    
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 043d373b8369..ae2a835898d7 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -205,12 +205,10 @@ void __show_regs(struct pt_regs *regs)
 
 		pr_cont("\n");
 	}
-	printk("\n");
 }
 
 void show_regs(struct pt_regs * regs)
 {
-	printk("\n");
 	__show_regs(regs);
 }
 

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f9b8e74ff8f2..043d373b8369 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -26,6 +26,7 @@
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 40a16cdd9873..f9b8e74ff8f2 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -25,6 +25,7 @@
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/task.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1ad48f93abdd..40a16cdd9873 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -24,6 +24,7 @@
 #include <linux/efi.h>
 #include <linux/export.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>

commit ffe3d1e43cc01457d513bf6e13605ebe8ae0fd9a
Author: Miles Chen <miles.chen@mediatek.com>
Date:   Thu Feb 9 09:52:03 2017 +0800

    arm64: use linux/sizes.h for constants
    
    Use linux/size.h to improve code readability.
    
    Signed-off-by: Miles Chen <miles.chen@mediatek.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 0b07d42819da..1ad48f93abdd 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -407,7 +407,7 @@ unsigned long arch_align_stack(unsigned long sp)
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
 	if (is_compat_task())
-		return randomize_page(mm->brk, 0x02000000);
+		return randomize_page(mm->brk, SZ_32M);
 	else
-		return randomize_page(mm->brk, 0x40000000);
+		return randomize_page(mm->brk, SZ_1G);
 }

commit 8f4b326d663b92e1c2a53a6857bef42e91aea5a6
Author: Joel Fernandes <joelaf@google.com>
Date:   Wed Dec 21 14:44:46 2016 -0800

    arm64: Don't trace __switch_to if function graph tracer is enabled
    
    Function graph tracer shows negative time (wrap around) when tracing
    __switch_to if the nosleep-time trace option is enabled.
    
    Time compensation for nosleep-time is done by an ftrace probe on
    sched_switch. This doesn't work well for the following events (with
    letters representing timestamps):
    A - sched switch probe called for task T switch out
    B - __switch_to calltime is recorded
    C - sched_switch probe called for task T switch in
    D - __switch_to rettime is recorded
    
    If C - A > D - B, then we end up over compensating for the time spent in
    __switch_to giving rise to negative times in the trace output.
    
    On x86, __switch_to is not traced if function graph tracer is enabled.
    Do the same for arm64 as well.
    
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index a3a2816ba73a..0b07d42819da 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -339,7 +339,7 @@ static void entry_task_switch(struct task_struct *next)
 /*
  * Thread switching.
  */
-struct task_struct *__switch_to(struct task_struct *prev,
+__notrace_funcgraph struct task_struct *__switch_to(struct task_struct *prev,
 				struct task_struct *next)
 {
 	struct task_struct *last;

commit a4023f682739439b434165b54af7cb3676a4766e
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Nov 8 13:56:20 2016 +0000

    arm64: Add hypervisor safe helper for checking constant capabilities
    
    The hypervisor may not have full access to the kernel data structures
    and hence cannot safely use cpus_have_cap() helper for checking the
    system capability. Add a safe helper for hypervisors to check a constant
    system capability, which *doesn't* fall back to checking the bitmap
    maintained by the kernel. With this, make the cpus_have_cap() only
    check the bitmask and force constant cap checks to use the new API
    for quicker checks.
    
    Cc: Robert Ritcher <rritcher@cavium.com>
    Cc: Tirumalesh Chalamarla <tchalamarla@cavium.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index a98b743631c5..a3a2816ba73a 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -283,7 +283,7 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->pstate = PSR_MODE_EL1h;
 		if (IS_ENABLED(CONFIG_ARM64_UAO) &&
-		    cpus_have_cap(ARM64_HAS_UAO))
+		    cpus_have_const_cap(ARM64_HAS_UAO))
 			childregs->pstate |= PSR_UAO_BIT;
 		p->thread.cpu_context.x19 = stack_start;
 		p->thread.cpu_context.x20 = stk_sz;

commit c02433dd6de32f042cf3ffe476746b1115b8c096
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:13 2016 +0000

    arm64: split thread_info from task stack
    
    This patch moves arm64's struct thread_info from the task stack into
    task_struct. This protects thread_info from corruption in the case of
    stack overflows, and makes its address harder to determine if stack
    addresses are leaked, making a number of attacks more difficult. Precise
    detection and handling of overflow is left for subsequent patches.
    
    Largely, this involves changing code to store the task_struct in sp_el0,
    and acquire the thread_info from the task struct. Core code now
    implements current_thread_info(), and as noted in <linux/sched.h> this
    relies on offsetof(task_struct, thread_info) == 0, enforced by core
    code.
    
    This change means that the 'tsk' register used in entry.S now points to
    a task_struct, rather than a thread_info as it used to. To make this
    clear, the TI_* field offsets are renamed to TSK_TI_*, with asm-offsets
    appropriately updated to account for the structural change.
    
    Userspace clobbers sp_el0, and we can no longer restore this from the
    stack. Instead, the current task is cached in a per-cpu variable that we
    can safely access from early assembly as interrupts are disabled (and we
    are thus not preemptible).
    
    Both secondary entry and idle are updated to stash the sp and task
    pointer separately.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index ec7b9c00effe..a98b743631c5 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -45,6 +45,7 @@
 #include <linux/personality.h>
 #include <linux/notifier.h>
 #include <trace/events/power.h>
+#include <linux/percpu.h>
 
 #include <asm/alternative.h>
 #include <asm/compat.h>
@@ -321,6 +322,20 @@ void uao_thread_switch(struct task_struct *next)
 	}
 }
 
+/*
+ * We store our current task in sp_el0, which is clobbered by userspace. Keep a
+ * shadow copy so that we can restore this upon entry from userspace.
+ *
+ * This is *only* for exception entry from EL0, and is not valid until we
+ * __switch_to() a user task.
+ */
+DEFINE_PER_CPU(struct task_struct *, __entry_task);
+
+static void entry_task_switch(struct task_struct *next)
+{
+	__this_cpu_write(__entry_task, next);
+}
+
 /*
  * Thread switching.
  */
@@ -333,6 +348,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	tls_thread_switch(next);
 	hw_breakpoint_thread_switch(next);
 	contextidr_thread_switch(next);
+	entry_task_switch(next);
 	uao_thread_switch(next);
 
 	/*

commit 9bbd4c56b0b642f04396da378296e68096d5afca
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:08 2016 +0000

    arm64: prep stack walkers for THREAD_INFO_IN_TASK
    
    When CONFIG_THREAD_INFO_IN_TASK is selected, task stacks may be freed
    before a task is destroyed. To account for this, the stacks are
    refcounted, and when manipulating the stack of another task, it is
    necessary to get/put the stack to ensure it isn't freed and/or re-used
    while we do so.
    
    This patch reworks the arm64 stack walking code to account for this.
    When CONFIG_THREAD_INFO_IN_TASK is not selected these perform no
    refcounting, and this should only be a structural change that does not
    affect behaviour.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 01753cd7d3f0..ec7b9c00effe 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -350,27 +350,35 @@ struct task_struct *__switch_to(struct task_struct *prev,
 unsigned long get_wchan(struct task_struct *p)
 {
 	struct stackframe frame;
-	unsigned long stack_page;
+	unsigned long stack_page, ret = 0;
 	int count = 0;
 	if (!p || p == current || p->state == TASK_RUNNING)
 		return 0;
 
+	stack_page = (unsigned long)try_get_task_stack(p);
+	if (!stack_page)
+		return 0;
+
 	frame.fp = thread_saved_fp(p);
 	frame.sp = thread_saved_sp(p);
 	frame.pc = thread_saved_pc(p);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	frame.graph = p->curr_ret_stack;
 #endif
-	stack_page = (unsigned long)task_stack_page(p);
 	do {
 		if (frame.sp < stack_page ||
 		    frame.sp >= stack_page + THREAD_SIZE ||
 		    unwind_frame(p, &frame))
-			return 0;
-		if (!in_sched_functions(frame.pc))
-			return frame.pc;
+			goto out;
+		if (!in_sched_functions(frame.pc)) {
+			ret = frame.pc;
+			goto out;
+		}
 	} while (count ++ < 16);
-	return 0;
+
+out:
+	put_task_stack(p);
+	return ret;
 }
 
 unsigned long arch_align_stack(unsigned long sp)

commit db4b0710fae90a4407bfa77b23db396e580b9e23
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 20 12:23:16 2016 +0100

    arm64: fix show_regs fallout from KERN_CONT changes
    
    Recently in commit 4bcc595ccd80decb ("printk: reinstate KERN_CONT for
    printing continuation lines"), the behaviour of printk changed w.r.t.
    KERN_CONT. Now, KERN_CONT is mandatory to continue existing lines.
    Without this, prefixes are inserted, making output illegible, e.g.
    
    [ 1007.069010] pc : [<ffff00000871898c>] lr : [<ffff000008718948>] pstate: 40000145
    [ 1007.076329] sp : ffff000008d53ec0
    [ 1007.079606] x29: ffff000008d53ec0 [ 1007.082797] x28: 0000000080c50018
    [ 1007.086160]
    [ 1007.087630] x27: ffff000008e0c7f8 [ 1007.090820] x26: ffff80097631ca00
    [ 1007.094183]
    [ 1007.095653] x25: 0000000000000001 [ 1007.098843] x24: 000000ea68b61cac
    [ 1007.102206]
    
    ... or when dumped with the userpace dmesg tool, which has slightly
    different implicit newline behaviour. e.g.
    
    [ 1007.069010] pc : [<ffff00000871898c>] lr : [<ffff000008718948>] pstate: 40000145
    [ 1007.076329] sp : ffff000008d53ec0
    [ 1007.079606] x29: ffff000008d53ec0
    [ 1007.082797] x28: 0000000080c50018
    [ 1007.086160]
    [ 1007.087630] x27: ffff000008e0c7f8
    [ 1007.090820] x26: ffff80097631ca00
    [ 1007.094183]
    [ 1007.095653] x25: 0000000000000001
    [ 1007.098843] x24: 000000ea68b61cac
    [ 1007.102206]
    
    We can't simply always use KERN_CONT for lines which may or may not be
    continuations. That causes line prefixes (e.g. timestamps) to be
    supressed, and the alignment of all but the first line will be broken.
    
    For even more fun, we can't simply insert some dummy empty-string printk
    calls, as GCC warns for an empty printk string, and even if we pass
    KERN_DEFAULT explcitly to silence the warning, the prefix gets swallowed
    unless there is an additional part to the string.
    
    Instead, we must manually iterate over pairs of registers, which gives
    us the legible output we want in either case, e.g.
    
    [  169.771790] pc : [<ffff00000871898c>] lr : [<ffff000008718948>] pstate: 40000145
    [  169.779109] sp : ffff000008d53ec0
    [  169.782386] x29: ffff000008d53ec0 x28: 0000000080c50018
    [  169.787650] x27: ffff000008e0c7f8 x26: ffff80097631de00
    [  169.792913] x25: 0000000000000001 x24: 00000027827b2cf4
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 4f186c56c5eb..01753cd7d3f0 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -187,10 +187,19 @@ void __show_regs(struct pt_regs *regs)
 	printk("pc : [<%016llx>] lr : [<%016llx>] pstate: %08llx\n",
 	       regs->pc, lr, regs->pstate);
 	printk("sp : %016llx\n", sp);
-	for (i = top_reg; i >= 0; i--) {
+
+	i = top_reg;
+
+	while (i >= 0) {
 		printk("x%-2d: %016llx ", i, regs->regs[i]);
-		if (i % 2 == 0)
-			printk("\n");
+		i--;
+
+		if (i % 2 == 0) {
+			pr_cont("x%-2d: %016llx ", i, regs->regs[i]);
+			i--;
+		}
+
+		pr_cont("\n");
 	}
 	printk("\n");
 }

commit d08544127d9fb4505635e3cb6871fd50a42947bd
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:48 2016 +0100

    arm64: suspend: Reconfigure PSTATE after resume from idle
    
    The suspend/resume path in kernel/sleep.S, as used by cpu-idle, does not
    save/restore PSTATE. As a result of this cpufeatures that were detected
    and have bits in PSTATE get lost when we resume from idle.
    
    UAO gets set appropriately on the next context switch. PAN will be
    re-enabled next time we return from user-space, but on a preemptible
    kernel we may run work accessing user space before this point.
    
    Add code to re-enable theses two features in __cpu_suspend_exit().
    We re-use uao_thread_switch() passing current.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 27b2f1387df4..4f186c56c5eb 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -49,6 +49,7 @@
 #include <asm/alternative.h>
 #include <asm/compat.h>
 #include <asm/cacheflush.h>
+#include <asm/exec.h>
 #include <asm/fpsimd.h>
 #include <asm/mmu_context.h>
 #include <asm/processor.h>
@@ -301,7 +302,7 @@ static void tls_thread_switch(struct task_struct *next)
 }
 
 /* Restore the UAO state depending on next's addr_limit */
-static void uao_thread_switch(struct task_struct *next)
+void uao_thread_switch(struct task_struct *next)
 {
 	if (IS_ENABLED(CONFIG_ARM64_UAO)) {
 		if (task_thread_info(next)->addr_limit == KERNEL_DS)

commit fa5114c78c596e977af04865b697b7fcb092a0fb
Author: Jason Cooper <jason@lakedaemon.net>
Date:   Tue Oct 11 13:54:02 2016 -0700

    arm64: use simpler API for random address requests
    
    Currently, all callers to randomize_range() set the length to 0 and
    calculate end by adding a constant to the start address.  We can simplify
    the API to remove a bunch of needless checks and variables.
    
    Use the new randomize_addr(start, range) call to set the requested
    address.
    
    Link: http://lkml.kernel.org/r/20160803233913.32511-5-jason@lakedaemon.net
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: "Russell King - ARM Linux" <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index a4f5f766af08..27b2f1387df4 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -372,12 +372,8 @@ unsigned long arch_align_stack(unsigned long sp)
 
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	unsigned long range_end = mm->brk;
-
 	if (is_compat_task())
-		range_end += 0x02000000;
+		return randomize_page(mm->brk, 0x02000000);
 	else
-		range_end += 0x40000000;
-
-	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
+		return randomize_page(mm->brk, 0x40000000);
 }

commit adf7589997927b1d84a5d003027b866bbef61ef2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Sep 8 13:55:38 2016 +0100

    arm64: simplify sysreg manipulation
    
    A while back we added {read,write}_sysreg accessors to handle accesses
    to system registers, without the usual boilerplate asm volatile,
    temporary variable, etc.
    
    This patch makes use of these across arm64 to make code shorter and
    clearer. For sequences with a trailing ISB, the existing isb() macro is
    also used so that asm blocks can be removed entirely.
    
    A few uses of inline assembly for msr/mrs are left as-is. Those
    manipulating sp_el0 for the current thread_info value have special
    clobber requiremends.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6cd2612236dc..a4f5f766af08 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -202,7 +202,7 @@ void show_regs(struct pt_regs * regs)
 
 static void tls_thread_flush(void)
 {
-	asm ("msr tpidr_el0, xzr");
+	write_sysreg(0, tpidr_el0);
 
 	if (is_compat_task()) {
 		current->thread.tp_value = 0;
@@ -213,7 +213,7 @@ static void tls_thread_flush(void)
 		 * with a stale shadow state during context switch.
 		 */
 		barrier();
-		asm ("msr tpidrro_el0, xzr");
+		write_sysreg(0, tpidrro_el0);
 	}
 }
 
@@ -253,7 +253,7 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		 * Read the current TLS pointer from tpidr_el0 as it may be
 		 * out-of-sync with the saved value.
 		 */
-		asm("mrs %0, tpidr_el0" : "=r" (*task_user_tls(p)));
+		*task_user_tls(p) = read_sysreg(tpidr_el0);
 
 		if (stack_start) {
 			if (is_compat_thread(task_thread_info(p)))
@@ -289,17 +289,15 @@ static void tls_thread_switch(struct task_struct *next)
 {
 	unsigned long tpidr, tpidrro;
 
-	asm("mrs %0, tpidr_el0" : "=r" (tpidr));
+	tpidr = read_sysreg(tpidr_el0);
 	*task_user_tls(current) = tpidr;
 
 	tpidr = *task_user_tls(next);
 	tpidrro = is_compat_thread(task_thread_info(next)) ?
 		  next->thread.tp_value : 0;
 
-	asm(
-	"	msr	tpidr_el0, %0\n"
-	"	msr	tpidrro_el0, %1"
-	: : "r" (tpidr), "r" (tpidrro));
+	write_sysreg(tpidr, tpidr_el0);
+	write_sysreg(tpidrro, tpidrro_el0);
 }
 
 /* Restore the UAO state depending on next's addr_limit */

commit 5f56a5dfdb9bcb3bca03df59980d4d2f012cbb53
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:16 2016 -0700

    exit_thread: remove empty bodies
    
    Define HAVE_EXIT_THREAD for archs which want to do something in
    exit_thread. For others, let's define exit_thread as an empty inline.
    
    This is a cleanup before we change the prototype of exit_thread to
    accept a task parameter.
    
    [akpm@linux-foundation.org: fix mips]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 48eea6866c67..6cd2612236dc 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -200,13 +200,6 @@ void show_regs(struct pt_regs * regs)
 	__show_regs(regs);
 }
 
-/*
- * Free current thread data structures etc..
- */
-void exit_thread(void)
-{
-}
-
 static void tls_thread_flush(void)
 {
 	asm ("msr tpidr_el0, xzr");

commit e6d9a52543338603e25e71e0e4942f05dae0dd8a
Author: Colin Ian King <colin.king@canonical.com>
Date:   Wed May 11 17:56:54 2016 +0100

    arm64: do not enforce strict 16 byte alignment to stack pointer
    
    copy_thread should not be enforcing 16 byte aligment and returning
    -EINVAL. Other architectures trap misaligned stack access with SIGBUS
    so arm64 should follow this convention, so remove the strict enforcement
    check.
    
    For example, currently clone(2) fails with -EINVAL when passing
    a misaligned stack and this gives little clue to what is wrong. Instead,
    it is arguable that a SIGBUS on the fist access to a misaligned stack
    allows one to figure out that it is a misaligned stack issue rather
    than trying to figure out why an unconventional (and undocumented)
    -EINVAL is being returned.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index ad4a7e132ead..48eea6866c67 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -265,9 +265,6 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		if (stack_start) {
 			if (is_compat_thread(task_thread_info(p)))
 				childregs->compat_sp = stack_start;
-			/* 16-byte aligned stack mandatory on AArch64 */
-			else if (stack_start & 15)
-				return -EINVAL;
 			else
 				childregs->sp = stack_start;
 		}

commit 61462c8a6b140fe2f93cb911684837e05950e680
Author: Kees Cook <keescook@chromium.org>
Date:   Tue May 10 10:55:49 2016 -0700

    arm64: kernel: Fix incorrect brk randomization
    
    This fixes two issues with the arm64 brk randomziation. First, the
    STACK_RND_MASK was being used incorrectly. The original code was:
    
            unsigned long range_end = base + (STACK_RND_MASK << PAGE_SHIFT) + 1;
    
    STACK_RND_MASK is 0x7ff (32-bit) or 0x3ffff (64-bit), with 4K pages where
    PAGE_SHIFT is 12:
    
            #define STACK_RND_MASK  (test_thread_flag(TIF_32BIT) ? \
                                                    0x7ff >> (PAGE_SHIFT - 12) : \
                                                    0x3ffff >> (PAGE_SHIFT - 12))
    
    This means the resulting offset from base would be 0x7ff0001 or 0x3ffff0001,
    which is wrong since it creates an unaligned end address. It was likely
    intended to be:
    
            unsigned long range_end = base + ((STACK_RND_MASK + 1) << PAGE_SHIFT)
    
    Which would result in offsets of 0x800000 (32-bit) and 0x40000000 (64-bit).
    
    However, even this corrected 32-bit compat offset (0x00800000) is much
    smaller than native ARM's brk randomization value (0x02000000):
    
            unsigned long arch_randomize_brk(struct mm_struct *mm)
            {
                    unsigned long range_end = mm->brk + 0x02000000;
                    return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
            }
    
    So, instead of basing arm64's brk randomization on mistaken STACK_RND_MASK
    calculations, just use specific corrected values for compat (0x2000000)
    and native arm64 (0x40000000).
    
    Reviewed-by: Jon Medhurst <tixy@linaro.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    [will: use is_compat_task() as suggested by tixy]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 80624829db61..ad4a7e132ead 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -382,13 +382,14 @@ unsigned long arch_align_stack(unsigned long sp)
 	return sp & ~0xf;
 }
 
-static unsigned long randomize_base(unsigned long base)
-{
-	unsigned long range_end = base + (STACK_RND_MASK << PAGE_SHIFT) + 1;
-	return randomize_range(base, range_end, 0) ? : base;
-}
-
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	return randomize_base(mm->brk);
+	unsigned long range_end = mm->brk;
+
+	if (is_compat_task())
+		range_end += 0x02000000;
+	else
+		range_end += 0x40000000;
+
+	return randomize_range(mm->brk, range_end, 0) ? : mm->brk;
 }

commit e950631e84e7e38892ffbeee5e1816b270026b0e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 18 15:50:04 2016 +0000

    arm64: Remove the get_thread_info() function
    
    This function was introduced by previous commits implementing UAO.
    However, it can be replaced with task_thread_info() in
    uao_thread_switch() or get_fs() in do_page_fault() (the latter being
    called only on the current context, so no need for using the saved
    pt_regs).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c1ca4ea065d4..80624829db61 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -315,15 +315,12 @@ static void tls_thread_switch(struct task_struct *next)
 /* Restore the UAO state depending on next's addr_limit */
 static void uao_thread_switch(struct task_struct *next)
 {
-	unsigned long next_sp = next->thread.cpu_context.sp;
-
-	if (IS_ENABLED(CONFIG_ARM64_UAO) &&
-	    get_thread_info(next_sp)->addr_limit == KERNEL_DS)
-		asm(ALTERNATIVE("nop", SET_PSTATE_UAO(1), ARM64_HAS_UAO,
-			        CONFIG_ARM64_UAO));
-	else
-		asm(ALTERNATIVE("nop", SET_PSTATE_UAO(0), ARM64_HAS_UAO,
-				CONFIG_ARM64_UAO));
+	if (IS_ENABLED(CONFIG_ARM64_UAO)) {
+		if (task_thread_info(next)->addr_limit == KERNEL_DS)
+			asm(ALTERNATIVE("nop", SET_PSTATE_UAO(1), ARM64_HAS_UAO));
+		else
+			asm(ALTERNATIVE("nop", SET_PSTATE_UAO(0), ARM64_HAS_UAO));
+	}
 }
 
 /*

commit 57f4959bad0a154aeca125b7d38d1d9471a12422
Author: James Morse <james.morse@arm.com>
Date:   Fri Feb 5 14:58:48 2016 +0000

    arm64: kernel: Add support for User Access Override
    
    'User Access Override' is a new ARMv8.2 feature which allows the
    unprivileged load and store instructions to be overridden to behave in
    the normal way.
    
    This patch converts {get,put}_user() and friends to use ldtr*/sttr*
    instructions - so that they can only access EL0 memory, then enables
    UAO when fs==KERNEL_DS so that these functions can access kernel memory.
    
    This allows user space's read/write permissions to be checked against the
    page tables, instead of testing addr<USER_DS, then using the kernel's
    read/write permissions.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: move uao_thread_switch() above dsb()]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 88d742ba19d5..c1ca4ea065d4 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -46,6 +46,7 @@
 #include <linux/notifier.h>
 #include <trace/events/power.h>
 
+#include <asm/alternative.h>
 #include <asm/compat.h>
 #include <asm/cacheflush.h>
 #include <asm/fpsimd.h>
@@ -280,6 +281,9 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	} else {
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->pstate = PSR_MODE_EL1h;
+		if (IS_ENABLED(CONFIG_ARM64_UAO) &&
+		    cpus_have_cap(ARM64_HAS_UAO))
+			childregs->pstate |= PSR_UAO_BIT;
 		p->thread.cpu_context.x19 = stack_start;
 		p->thread.cpu_context.x20 = stk_sz;
 	}
@@ -308,6 +312,20 @@ static void tls_thread_switch(struct task_struct *next)
 	: : "r" (tpidr), "r" (tpidrro));
 }
 
+/* Restore the UAO state depending on next's addr_limit */
+static void uao_thread_switch(struct task_struct *next)
+{
+	unsigned long next_sp = next->thread.cpu_context.sp;
+
+	if (IS_ENABLED(CONFIG_ARM64_UAO) &&
+	    get_thread_info(next_sp)->addr_limit == KERNEL_DS)
+		asm(ALTERNATIVE("nop", SET_PSTATE_UAO(1), ARM64_HAS_UAO,
+			        CONFIG_ARM64_UAO));
+	else
+		asm(ALTERNATIVE("nop", SET_PSTATE_UAO(0), ARM64_HAS_UAO,
+				CONFIG_ARM64_UAO));
+}
+
 /*
  * Thread switching.
  */
@@ -320,6 +338,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	tls_thread_switch(next);
 	hw_breakpoint_thread_switch(next);
 	contextidr_thread_switch(next);
+	uao_thread_switch(next);
 
 	/*
 	 * Complete any pending TLB or cache maintenance on this CPU in case

commit 20380bb390a443b2c5c8800cec59743faf8151b4
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:41 2015 +0900

    arm64: ftrace: fix a stack tracer's output under function graph tracer
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function return. This will result in many useless entries
    (return_to_handler) showing up in
     a) a stack tracer's output
     b) perf call graph (with perf record -g)
     c) dump_backtrace (at panic et al.)
    
    For example, in case of a),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo 1 > /proc/sys/kernel/stack_trace_enabled
      $ cat /sys/kernel/debug/tracing/stack_trace
            Depth    Size   Location    (54 entries)
            -----    ----   --------
      0)     4504      16   gic_raise_softirq+0x28/0x150
      1)     4488      80   smp_cross_call+0x38/0xb8
      2)     4408      48   return_to_handler+0x0/0x40
      3)     4360      32   return_to_handler+0x0/0x40
      ...
    
    In case of b),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ perf record -e mem:XXX:x -ag -- sleep 10
      $ perf report
                      ...
                      |          |          |--0.22%-- 0x550f8
                      |          |          |          0x10888
                      |          |          |          el0_svc_naked
                      |          |          |          sys_openat
                      |          |          |          return_to_handler
                      |          |          |          return_to_handler
                      ...
    
    In case of c),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo c > /proc/sysrq-trigger
      ...
      Call trace:
      [<ffffffc00044d3ac>] sysrq_handle_crash+0x24/0x30
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      ...
    
    This patch replaces such entries with real addresses preserved in
    current->ret_stack[] at unwind_frame(). This way, we can cover all
    the cases.
    
    Reviewed-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [will: fixed minor context changes conflicting with irq stack bits]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 98bf5461d4b6..88d742ba19d5 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -344,6 +344,9 @@ unsigned long get_wchan(struct task_struct *p)
 	frame.fp = thread_saved_fp(p);
 	frame.sp = thread_saved_sp(p);
 	frame.pc = thread_saved_pc(p);
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	frame.graph = p->curr_ret_stack;
+#endif
 	stack_page = (unsigned long)task_stack_page(p);
 	do {
 		if (frame.sp < stack_page ||

commit fe13f95b720075327a761fe6ddb45b0c90cab504
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:40 2015 +0900

    arm64: pass a task parameter to unwind_frame()
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function's return. This will result in many useless entries
    (return_to_handler) showing up in a call stack list.
    We will fix this problem in a later patch ("arm64: ftrace: fix a stack
    tracer's output under function graph tracer"). But since real return
    addresses are saved in ret_stack[] array in struct task_struct,
    unwind functions need to be notified of, in addition to a stack pointer
    address, which task is being traced in order to find out real return
    addresses.
    
    This patch extends unwind functions' interfaces by adding an extra
    argument of a pointer to task_struct.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f75b540bc3b4..98bf5461d4b6 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -348,7 +348,7 @@ unsigned long get_wchan(struct task_struct *p)
 	do {
 		if (frame.sp < stack_page ||
 		    frame.sp >= stack_page + THREAD_SIZE ||
-		    unwind_frame(&frame))
+		    unwind_frame(p, &frame))
 			return 0;
 		if (!in_sched_functions(frame.pc))
 			return frame.pc;

commit 096b3224d5e7239ec3e5033bbc7612ac2d5dec3a
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Wed Sep 16 22:23:21 2015 +0800

    arm64: add cpu_idle tracepoints to arch_cpu_idle
    
    Currently, if cpuidle is disabled or not supported, powertop reports
    zero wakeups and zero events. This is due to the cpu_idle tracepoints
    are missing.
    
    This patch is to make cpu_idle tracepoints always available even if
    cpuidle is disabled or not supported.
    
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 223b093c9440..f75b540bc3b4 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -44,6 +44,7 @@
 #include <linux/hw_breakpoint.h>
 #include <linux/personality.h>
 #include <linux/notifier.h>
+#include <trace/events/power.h>
 
 #include <asm/compat.h>
 #include <asm/cacheflush.h>
@@ -75,8 +76,10 @@ void arch_cpu_idle(void)
 	 * This should do all the clock switching and wait for interrupt
 	 * tricks
 	 */
+	trace_cpu_idle_rcuidle(1, smp_processor_id());
 	cpu_do_idle();
 	local_irq_enable();
+	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 6eb6c80187c55b7f95683bc6502dccac54b95b92
Author: Janet Liu <janet.liu@spreadtrum.com>
Date:   Thu Jun 11 12:04:32 2015 +0800

    arm64: kernel thread don't need to save fpsimd context.
    
    kernel thread's default fpsimd state is zero. When fork a thread, if parent is kernel thread,
     and save hardware context to parent's fpsimd state, but this hardware context is user
    process's context, because kernel thread don't use fpsimd, it will not introduce issue,
    it add a little cost.
    
    Signed-off-by: Janet Liu <janet.liu@spreadtrum.com>
    Signed-off-by: Chunyan Zhang <chunyan.zhang@spreadtrum.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 369f485f4b71..223b093c9440 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -233,7 +233,8 @@ void release_thread(struct task_struct *dead_task)
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	fpsimd_preserve_current_state();
+	if (current->mm)
+		fpsimd_preserve_current_state();
 	*dst = *src;
 	return 0;
 }

commit d00a3810c16207d2541b7796a73cca5a24ea3742
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed May 27 15:39:40 2015 +0100

    arm64: context-switch user tls register tpidr_el0 for compat tasks
    
    Since commit a4780adeefd0 ("ARM: 7735/2: Preserve the user r/w register
    TPIDRURW on context switch and fork"), arch/arm/ has context switched
    the user-writable TLS register, so do the same for compat tasks running
    under the arm64 kernel.
    
    Reported-by: Andr Hentschel <nerv@dawncrow.de>
    Tested-by: Andr Hentschel <nerv@dawncrow.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c506bee6b613..369f485f4b71 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -244,35 +244,35 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		unsigned long stk_sz, struct task_struct *p)
 {
 	struct pt_regs *childregs = task_pt_regs(p);
-	unsigned long tls = p->thread.tp_value;
 
 	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
 
 	if (likely(!(p->flags & PF_KTHREAD))) {
 		*childregs = *current_pt_regs();
 		childregs->regs[0] = 0;
-		if (is_compat_thread(task_thread_info(p))) {
-			if (stack_start)
+
+		/*
+		 * Read the current TLS pointer from tpidr_el0 as it may be
+		 * out-of-sync with the saved value.
+		 */
+		asm("mrs %0, tpidr_el0" : "=r" (*task_user_tls(p)));
+
+		if (stack_start) {
+			if (is_compat_thread(task_thread_info(p)))
 				childregs->compat_sp = stack_start;
-		} else {
-			/*
-			 * Read the current TLS pointer from tpidr_el0 as it may be
-			 * out-of-sync with the saved value.
-			 */
-			asm("mrs %0, tpidr_el0" : "=r" (tls));
-			if (stack_start) {
-				/* 16-byte aligned stack mandatory on AArch64 */
-				if (stack_start & 15)
-					return -EINVAL;
+			/* 16-byte aligned stack mandatory on AArch64 */
+			else if (stack_start & 15)
+				return -EINVAL;
+			else
 				childregs->sp = stack_start;
-			}
 		}
+
 		/*
 		 * If a TLS pointer was passed to clone (4th argument), use it
 		 * for the new thread.
 		 */
 		if (clone_flags & CLONE_SETTLS)
-			tls = childregs->regs[3];
+			p->thread.tp_value = childregs->regs[3];
 	} else {
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->pstate = PSR_MODE_EL1h;
@@ -281,7 +281,6 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	}
 	p->thread.cpu_context.pc = (unsigned long)ret_from_fork;
 	p->thread.cpu_context.sp = (unsigned long)childregs;
-	p->thread.tp_value = tls;
 
 	ptrace_hw_copy_thread(p);
 
@@ -292,18 +291,12 @@ static void tls_thread_switch(struct task_struct *next)
 {
 	unsigned long tpidr, tpidrro;
 
-	if (!is_compat_task()) {
-		asm("mrs %0, tpidr_el0" : "=r" (tpidr));
-		current->thread.tp_value = tpidr;
-	}
+	asm("mrs %0, tpidr_el0" : "=r" (tpidr));
+	*task_user_tls(current) = tpidr;
 
-	if (is_compat_thread(task_thread_info(next))) {
-		tpidr = 0;
-		tpidrro = next->thread.tp_value;
-	} else {
-		tpidr = next->thread.tp_value;
-		tpidrro = 0;
-	}
+	tpidr = *task_user_tls(next);
+	tpidrro = is_compat_thread(task_thread_info(next)) ?
+		  next->thread.tp_value : 0;
 
 	asm(
 	"	msr	tpidr_el0, %0\n"

commit 68234df4ea7939f98431aa81113fbdce10c4a84b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Apr 20 10:24:35 2015 +0100

    arm64: kill flush_cache_all()
    
    The documented semantics of flush_cache_all are not possible to provide
    for arm64 (short of flushing the entire physical address space by VA),
    and there are currently no users; KVM uses VA maintenance exclusively,
    cpu_reset is never called, and the only two users outside of arch code
    cannot be built for arm64.
    
    While cpu_soft_reset and related functions (which call flush_cache_all)
    were thought to be useful for kexec, their current implementations only
    serve to mask bugs. For correctness kexec will need to perform
    maintenance by VA anyway to account for system caches, line migration,
    and other subtleties of the cache architecture. As the extent of this
    cache maintenance will be kexec-specific, it should probably live in the
    kexec code.
    
    This patch removes flush_cache_all, and related unused components,
    preventing further abuse.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Geoff Levand <geoff@infradead.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c6b1f3b96f45..c506bee6b613 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -58,14 +58,6 @@ unsigned long __stack_chk_guard __read_mostly;
 EXPORT_SYMBOL(__stack_chk_guard);
 #endif
 
-void soft_restart(unsigned long addr)
-{
-	setup_mm_for_reboot();
-	cpu_soft_restart(virt_to_phys(cpu_reset), addr);
-	/* Should never get here */
-	BUG();
-}
-
 /*
  * Function pointers to optional machine specific functions
  */
@@ -136,9 +128,7 @@ void machine_power_off(void)
 
 /*
  * Restart requires that the secondary CPUs stop performing any activity
- * while the primary CPU resets the system. Systems with a single CPU can
- * use soft_restart() as their machine descriptor's .restart hook, since that
- * will cause the only available CPU to reset. Systems with multiple CPUs must
+ * while the primary CPU resets the system. Systems with multiple CPUs must
  * provide a HW restart implementation, to ensure that all CPUs reset at once.
  * This is required so that any code running after reset on the primary CPU
  * doesn't have to co-ordinate with other CPUs to ensure they aren't still

commit 60c0d45a7f7ab4e30452fa14deb23a33e29adbc2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Mar 6 15:49:24 2015 +0100

    efi/arm64: use UEFI for system reset and poweroff
    
    If UEFI Runtime Services are available, they are preferred over direct
    PSCI calls or other methods to reset the system.
    
    For the reset case, we need to hook into machine_restart(), as the
    arm_pm_restart function pointer may be overwritten by modules.
    
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index fde9923af859..c6b1f3b96f45 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -21,6 +21,7 @@
 #include <stdarg.h>
 
 #include <linux/compat.h>
+#include <linux/efi.h>
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
@@ -150,6 +151,13 @@ void machine_restart(char *cmd)
 	local_irq_disable();
 	smp_send_stop();
 
+	/*
+	 * UpdateCapsule() depends on the system being reset via
+	 * ResetSystem().
+	 */
+	if (efi_enabled(EFI_RUNTIME_SERVICES))
+		efi_reboot(reboot_mode, NULL);
+
 	/* Now call the architecture specific reboot code. */
 	if (arm_pm_restart)
 		arm_pm_restart(reboot_mode, cmd);

commit 92980405f3537136b8e81007a9df576762f49bbb
Author: Arun Chandran <achandran@mvista.com>
Date:   Fri Oct 10 12:31:24 2014 +0100

    arm64: ASLR: Don't randomise text when randomise_va_space == 0
    
    When user asks to turn off ASLR by writing "0" to
    /proc/sys/kernel/randomize_va_space there should not be
    any randomization to mmap base, stack, VDSO, libs, text and heap
    
    Currently arm64 violates this behavior by randomising text.
    Fix this by defining a constant ELF_ET_DYN_BASE. The randomisation of
    mm->mmap_base is done by setup_new_exec -> arch_pick_mmap_layout ->
    mmap_base -> mmap_rnd.
    
    Signed-off-by: Arun Chandran <achandran@mvista.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c3065dbc4fa2..fde9923af859 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -378,8 +378,3 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
 	return randomize_base(mm->brk);
 }
-
-unsigned long randomize_et_dyn(unsigned long base)
-{
-	return randomize_base(base);
-}

commit 93834c6419bccf102a17971c6b114826597a61c5
Merge: c798360cd143 6cd6d94d96d9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 10 16:38:02 2014 -0400

    Merge tag 'restart-handler-for-v3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/groeck/linux-staging
    
    Pull restart handler infrastructure from Guenter Roeck:
     "This series was supposed to be pulled through various trees using it,
      and I did not plan to send a separate pull request.  As it turns out,
      the pinctrl tree did not merge with it, is now upstream, and uses it,
      meaning there are now build failures.
    
      Please pull this series directly to fix those build failures"
    
    * tag 'restart-handler-for-v3.18' of git://git.kernel.org/pub/scm/linux/kernel/git/groeck/linux-staging:
      arm/arm64: unexport restart handlers
      watchdog: sunxi: register restart handler with kernel restart handler
      watchdog: alim7101: register restart handler with kernel restart handler
      watchdog: moxart: register restart handler with kernel restart handler
      arm: support restart through restart handler call chain
      arm64: support restart through restart handler call chain
      power/restart: call machine_restart instead of arm_pm_restart
      kernel: add support for kernel restart handler call chain

commit 6325e940e7e0c690c6bdfaf5d54309e71845d3d9
Merge: 536fd93d4328 0a6479b0ffad
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 8 05:34:24 2014 -0400

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     - eBPF JIT compiler for arm64
     - CPU suspend backend for PSCI (firmware interface) with standard idle
       states defined in DT (generic idle driver to be merged via a
       different tree)
     - Support for CONFIG_DEBUG_SET_MODULE_RONX
     - Support for unmapped cpu-release-addr (outside kernel linear mapping)
     - set_arch_dma_coherent_ops() implemented and bus notifiers removed
     - EFI_STUB improvements when base of DRAM is occupied
     - Typos in KGDB macros
     - Clean-up to (partially) allow kernel building with LLVM
     - Other clean-ups (extern keyword, phys_addr_t usage)
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (51 commits)
      arm64: Remove unneeded extern keyword
      ARM64: make of_device_ids const
      arm64: Use phys_addr_t type for physical address
      aarch64: filter $x from kallsyms
      arm64: Use DMA_ERROR_CODE to denote failed allocation
      arm64: Fix typos in KGDB macros
      arm64: insn: Add return statements after BUG_ON()
      arm64: debug: don't re-enable debug exceptions on return from el1_dbg
      Revert "arm64: dmi: Add SMBIOS/DMI support"
      arm64: Implement set_arch_dma_coherent_ops() to replace bus notifiers
      of: amba: use of_dma_configure for AMBA devices
      arm64: dmi: Add SMBIOS/DMI support
      arm64: Correct ftrace calls to aarch64_insn_gen_branch_imm()
      arm64:mm: initialize max_mapnr using function set_max_mapnr
      setup: Move unmask of async interrupts after possible earlycon setup
      arm64: LLVMLinux: Fix inline arm64 assembly for use with clang
      arm64: pageattr: Correctly adjust unaligned start addresses
      net: bpf: arm64: fix module memory leak when JIT image build fails
      arm64: add PSCI CPU_SUSPEND based cpu_suspend support
      arm64: kernel: introduce cpu_init_idle CPU operation
      ...

commit 6cd6d94d96d9b1cd8a62da91aac44cf56e301e75
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Fri Sep 26 00:03:17 2014 +0000

    arm/arm64: unexport restart handlers
    
    Implementing a restart handler in a module don't make sense as there would
    be no guarantee that the module is loaded when a restart is needed.
    Unexport arm_pm_restart to ensure that no one gets the idea to do it
    anyway.
    
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Heiko Stuebner <heiko@sntech.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dmitry Eremin-Solenikov <dbaryshkov@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jonas Jensen <jonas.jensen@gmail.com>
    Cc: Maxime Ripard <maxime.ripard@free-electrons.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wim Van Sebroeck <wim@iguana.be>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 0d3fb9fd0dca..398ab05081b4 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -98,7 +98,6 @@ void (*pm_power_off)(void);
 EXPORT_SYMBOL_GPL(pm_power_off);
 
 void (*arm_pm_restart)(enum reboot_mode reboot_mode, const char *cmd);
-EXPORT_SYMBOL_GPL(arm_pm_restart);
 
 /*
  * This is our default idle handler.

commit 1c7ffc32eaadfae3e7ab106359af0cf21b7e94c1
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Fri Sep 26 00:03:16 2014 +0000

    arm64: support restart through restart handler call chain
    
    The kernel core now supports a restart handler call chain to restart the
    system.  Call it if arm_pm_restart is not set.
    
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Heiko Stuebner <heiko@sntech.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Dmitry Eremin-Solenikov <dbaryshkov@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jonas Jensen <jonas.jensen@gmail.com>
    Cc: Maxime Ripard <maxime.ripard@free-electrons.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tomasz Figa <t.figa@samsung.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wim Van Sebroeck <wim@iguana.be>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1309d64aa926..0d3fb9fd0dca 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -180,6 +180,8 @@ void machine_restart(char *cmd)
 	/* Now call the architecture specific reboot code. */
 	if (arm_pm_restart)
 		arm_pm_restart(reboot_mode, cmd);
+	else
+		do_kernel_restart(cmd);
 
 	/*
 	 * Whoops - the architecture was unable to reboot.

commit eb35bdd7bca29a13c8ecd44e6fd747a84ce675db
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 11 14:38:16 2014 +0100

    arm64: flush TLS registers during exec
    
    Nathan reports that we leak TLS information from the parent context
    during an exec, as we don't clear the TLS registers when flushing the
    thread state.
    
    This patch updates the flushing code so that we:
    
      (1) Unconditionally zero the tpidr_el0 register (since this is fully
          context switched for native tasks and zeroed for compat tasks)
    
      (2) Zero the tp_value state in thread_info before clearing the
          tpidrr0_el0 register for compat tasks (since this is only writable
          by the set_tls compat syscall and therefore not fully switched).
    
    A missing compiler barrier is also added to the compat set_tls syscall.
    
    Cc: <stable@vger.kernel.org>
    Acked-by: Nathan Lynch <Nathan_Lynch@mentor.com>
    Reported-by: Nathan Lynch <Nathan_Lynch@mentor.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1309d64aa926..29d48690f2ac 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -230,9 +230,27 @@ void exit_thread(void)
 {
 }
 
+static void tls_thread_flush(void)
+{
+	asm ("msr tpidr_el0, xzr");
+
+	if (is_compat_task()) {
+		current->thread.tp_value = 0;
+
+		/*
+		 * We need to ensure ordering between the shadow state and the
+		 * hardware state, so that we don't corrupt the hardware state
+		 * with a stale shadow state during context switch.
+		 */
+		barrier();
+		asm ("msr tpidrro_el0, xzr");
+	}
+}
+
 void flush_thread(void)
 {
 	fpsimd_flush_thread();
+	tls_thread_flush();
 	flush_ptrace_hw_breakpoint(current);
 }
 

commit 5e051531447259e5df95c44bccb69979537c19e4
Author: Arun Chandran <achandran@mvista.com>
Date:   Mon Aug 18 10:06:58 2014 +0100

    arm64: convert part of soft_restart() to assembly
    
    The current soft_restart() and setup_restart implementations incorrectly
    assume that compiler will not spill/fill values to/from stack. However
    this assumption seems to be wrong, revealed by the disassembly of the
    currently existing code (v3.16) built with Linaro GCC 4.9-2014.05.
    
    ffffffc000085224 <soft_restart>:
    ffffffc000085224:  a9be7bfd  stp    x29, x30, [sp,#-32]!
    ffffffc000085228:  910003fd  mov    x29, sp
    ffffffc00008522c:  f9000fa0  str    x0, [x29,#24]
    ffffffc000085230:  94003d21  bl     ffffffc0000946b4 <setup_mm_for_reboot>
    ffffffc000085234:  94003b33  bl     ffffffc000093f00 <flush_cache_all>
    ffffffc000085238:  94003dfa  bl     ffffffc000094a20 <cpu_cache_off>
    ffffffc00008523c:  94003b31  bl     ffffffc000093f00 <flush_cache_all>
    ffffffc000085240:  b0003321  adrp   x1, ffffffc0006ea000 <reset_devices>
    
    ffffffc000085244:  f9400fa0  ldr    x0, [x29,#24] ----> spilled addr
    ffffffc000085248:  f942fc22  ldr    x2, [x1,#1528] ----> global memstart_addr
    
    ffffffc00008524c:  f0000061  adrp   x1, ffffffc000094000 <__inval_cache_range+0x40>
    ffffffc000085250:  91290021  add    x1, x1, #0xa40
    ffffffc000085254:  8b010041  add    x1, x2, x1
    ffffffc000085258:  d2c00802  mov    x2, #0x4000000000           // #274877906944
    ffffffc00008525c:  8b020021  add    x1, x1, x2
    ffffffc000085260:  d63f0020  blr    x1
    ...
    
    Here the compiler generates memory accesses after the cache is disabled,
    loading stale values for the spilled value and global variable. As we cannot
    control when the compiler will access memory we must rewrite the
    functions in assembly to stash values we need in registers prior to
    disabling the cache, avoiding the use of memory.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Arun Chandran <achandran@mvista.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1309d64aa926..bf669228a59f 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -57,36 +57,10 @@ unsigned long __stack_chk_guard __read_mostly;
 EXPORT_SYMBOL(__stack_chk_guard);
 #endif
 
-static void setup_restart(void)
-{
-	/*
-	 * Tell the mm system that we are going to reboot -
-	 * we may need it to insert some 1:1 mappings so that
-	 * soft boot works.
-	 */
-	setup_mm_for_reboot();
-
-	/* Clean and invalidate caches */
-	flush_cache_all();
-
-	/* Turn D-cache off */
-	cpu_cache_off();
-
-	/* Push out any further dirty data, and ensure cache is empty */
-	flush_cache_all();
-}
-
 void soft_restart(unsigned long addr)
 {
-	typedef void (*phys_reset_t)(unsigned long);
-	phys_reset_t phys_reset;
-
-	setup_restart();
-
-	/* Switch to the identity mapping */
-	phys_reset = (phys_reset_t)virt_to_phys(cpu_reset);
-	phys_reset(addr);
-
+	setup_mm_for_reboot();
+	cpu_soft_restart(virt_to_phys(cpu_reset), addr);
 	/* Should never get here */
 	BUG();
 }

commit c0c264ae5112d1cdb7d37d4e208b7a7e766a7418
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Jun 25 23:55:03 2014 +0100

    arm64: Add CONFIG_CC_STACKPROTECTOR
    
    arm64 currently lacks support for -fstack-protector. Add
    similar functionality to arm to detect stack corruption.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 43b7c34f92cb..1309d64aa926 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -51,6 +51,12 @@
 #include <asm/processor.h>
 #include <asm/stacktrace.h>
 
+#ifdef CONFIG_CC_STACKPROTECTOR
+#include <linux/stackprotector.h>
+unsigned long __stack_chk_guard __read_mostly;
+EXPORT_SYMBOL(__stack_chk_guard);
+#endif
+
 static void setup_restart(void)
 {
 	/*

commit b9acc49ee9464f8f8232a790045d057eb158e869
Author: Arun KS <arunks.linux@gmail.com>
Date:   Wed May 7 02:41:23 2014 +0100

    arm64: Fix deadlock scenario with smp_send_stop()
    
    If one process calls sys_reboot and that process then stops other
    CPUs while those CPUs are within a spin_lock() region we can
    potentially encounter a deadlock scenario like below.
    
    CPU 0                   CPU 1
    -----                   -----
                            spin_lock(my_lock)
    smp_send_stop()
     <send IPI>             handle_IPI()
                             disable_preemption/irqs
                              while(1);
     <PREEMPT>
    spin_lock(my_lock) <--- Waits forever
    
    We shouldn't attempt to run any other tasks after we send a stop
    IPI to a CPU so disable preemption so that this task runs to
    completion. We use local_irq_disable() here for cross-arch
    consistency with x86.
    
    Based-on-work-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Arun KS <getarunks@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index ed37c3694cb8..43b7c34f92cb 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -135,6 +135,7 @@ void machine_shutdown(void)
  */
 void machine_halt(void)
 {
+	local_irq_disable();
 	smp_send_stop();
 	while (1);
 }
@@ -147,6 +148,7 @@ void machine_halt(void)
  */
 void machine_power_off(void)
 {
+	local_irq_disable();
 	smp_send_stop();
 	if (pm_power_off)
 		pm_power_off();
@@ -165,10 +167,9 @@ void machine_power_off(void)
  */
 void machine_restart(char *cmd)
 {
-	smp_send_stop();
-
 	/* Disable interrupts first */
 	local_irq_disable();
+	smp_send_stop();
 
 	/* Now call the architecture specific reboot code. */
 	if (arm_pm_restart)

commit 90f51a09ef83703b6c2e0434e0384b61b59699d3
Author: Arun KS <arun.linux@gmail.com>
Date:   Wed May 7 02:41:22 2014 +0100

    arm64: Fix machine_shutdown() definition
    
    This patch ports most of commit 19ab428f4b79 "ARM: 7759/1: decouple CPU
    offlining from reboot/shutdown" by Stephen Warren from arch/arm to
    arch/arm64.
    
    machine_shutdown() is a hook for kexec. Add a comment saying so, since
    it isn't obvious from the function name.
    
    Halt, power-off, and restart have different requirements re: stopping
    secondary CPUs than kexec has. The former simply require the secondary
    CPUs to be quiesced somehow, whereas kexec requires them to be
    completely non-operational, so that no matter where the kexec target
    images are written in RAM, they won't influence operation of the
    secondary CPUS,which could happen if the CPUs were still executing some
    kind of pin loop. To this end, modify machine_halt, power_off, and
    restart to call smp_send_stop() directly, rather than calling
    machine_shutdown().
    
    In machine_shutdown(), replace the call to smp_send_stop() with a call
    to disable_nonboot_cpus(). This completely disables all but one CPU,
    thus satisfying the kexec requirements a couple paragraphs above.
    
    Signed-off-by: Arun KS <getarunks@gmail.com>
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 9f2d6020b6c2..ed37c3694cb8 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -114,29 +114,58 @@ void arch_cpu_idle_dead(void)
 }
 #endif
 
+/*
+ * Called by kexec, immediately prior to machine_kexec().
+ *
+ * This must completely disable all secondary CPUs; simply causing those CPUs
+ * to execute e.g. a RAM-based pin loop is not sufficient. This allows the
+ * kexec'd kernel to use any and all RAM as it sees fit, without having to
+ * avoid any code or data used by any SW CPU pin loop. The CPU hotplug
+ * functionality embodied in disable_nonboot_cpus() to achieve this.
+ */
 void machine_shutdown(void)
 {
-#ifdef CONFIG_SMP
-	smp_send_stop();
-#endif
+	disable_nonboot_cpus();
 }
 
+/*
+ * Halting simply requires that the secondary CPUs stop performing any
+ * activity (executing tasks, handling interrupts). smp_send_stop()
+ * achieves this.
+ */
 void machine_halt(void)
 {
-	machine_shutdown();
+	smp_send_stop();
 	while (1);
 }
 
+/*
+ * Power-off simply requires that the secondary CPUs stop performing any
+ * activity (executing tasks, handling interrupts). smp_send_stop()
+ * achieves this. When the system power is turned off, it will take all CPUs
+ * with it.
+ */
 void machine_power_off(void)
 {
-	machine_shutdown();
+	smp_send_stop();
 	if (pm_power_off)
 		pm_power_off();
 }
 
+/*
+ * Restart requires that the secondary CPUs stop performing any activity
+ * while the primary CPU resets the system. Systems with a single CPU can
+ * use soft_restart() as their machine descriptor's .restart hook, since that
+ * will cause the only available CPU to reset. Systems with multiple CPUs must
+ * provide a HW restart implementation, to ensure that all CPUs reset at once.
+ * This is required so that any code running after reset on the primary CPU
+ * doesn't have to co-ordinate with other CPUs to ensure they aren't still
+ * executing pre-reset code, and using RAM that the primary CPU's code wishes
+ * to use. Implementing such co-ordination would be essentially impossible.
+ */
 void machine_restart(char *cmd)
 {
-	machine_shutdown();
+	smp_send_stop();
 
 	/* Disable interrupts first */
 	local_irq_disable();

commit cf5c95db57ffa02e430c3840c08d1ee0403849d4
Merge: fd92d4a54a06 49788fe2a128
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 16 10:05:11 2014 +0100

    Merge tag 'for-3.16' of git://git.linaro.org/people/ard.biesheuvel/linux-arm into upstream
    
    FPSIMD register bank context switching and crypto algorithms
    optimisations for arm64 from Ard Biesheuvel.
    
    * tag 'for-3.16' of git://git.linaro.org/people/ard.biesheuvel/linux-arm:
      arm64/crypto: AES-ECB/CBC/CTR/XTS using ARMv8 NEON and Crypto Extensions
      arm64: pull in <asm/simd.h> from asm-generic
      arm64/crypto: AES in CCM mode using ARMv8 Crypto Extensions
      arm64/crypto: AES using ARMv8 Crypto Extensions
      arm64/crypto: GHASH secure hash using ARMv8 Crypto Extensions
      arm64/crypto: SHA-224/SHA-256 using ARMv8 Crypto Extensions
      arm64/crypto: SHA-1 using ARMv8 Crypto Extensions
      arm64: add support for kernel mode NEON in interrupt context
      arm64: defer reloading a task's FPSIMD state to userland resume
      arm64: add abstractions for FPSIMD state manipulation
      asm-generic: allow generic unaligned access if the arch supports it
    
    Conflicts:
            arch/arm64/include/asm/thread_info.h

commit fd92d4a54a069953b4679958121317f2a25389cd
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:51:32 2014 +0100

    arm64: is_compat_task is defined both in asm/compat.h and linux/compat.h
    
    Some kernel files may include both linux/compat.h and asm/compat.h directly
    or indirectly. Since both header files contain is_compat_task() under
    !CONFIG_COMPAT, compiling them with !CONFIG_COMPAT will eventually fail.
    Such files include kernel/auditsc.c, kernel/seccomp.c and init/do_mountfs.c
    (do_mountfs.c may read asm/compat.h via asm/ftrace.h once ftrace is
    implemented).
    
    So this patch proactively
    1) removes is_compat_task() under !CONFIG_COMPAT from asm/compat.h
    2) replaces asm/compat.h to linux/compat.h in kernel/*.c,
       but asm/compat.h is still necessary in ptrace.c and process.c because
       they use is_compat_thread().
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f7c446a5e97b..d04eb871cb0e 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -20,6 +20,7 @@
 
 #include <stdarg.h>
 
+#include <linux/compat.h>
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>

commit 98f7685ee69f871ba991089cb9685f0da07517ea
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 2 16:24:10 2014 +0100

    arm64: barriers: make use of barrier options with explicit barriers
    
    When calling our low-level barrier macros directly, we can often suffice
    with more relaxed behaviour than the default "all accesses, full system"
    option.
    
    This patch updates the users of dsb() to specify the option which they
    actually require.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6391485f342d..f7c446a5e97b 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -300,7 +300,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 * Complete any pending TLB or cache maintenance on this CPU in case
 	 * the thread migrates to a different CPU.
 	 */
-	dsb();
+	dsb(ish);
 
 	/* the actual thread switch */
 	last = cpu_switch_to(prev, next);

commit c51f92693c35c141cf7d9b7e2fcbb81128324eb4
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Feb 24 15:26:27 2014 +0100

    arm64: add abstractions for FPSIMD state manipulation
    
    There are two tacit assumptions in the FPSIMD handling code that will no longer
    hold after the next patch that optimizes away some FPSIMD state restores:
    . the FPSIMD registers of this CPU contain the userland FPSIMD state of
      task 'current';
    . when switching to a task, its FPSIMD state will always be restored from
      memory.
    
    This patch adds the following functions to abstract away from straight FPSIMD
    register file saves and restores:
    - fpsimd_preserve_current_state -> ensure current's FPSIMD state is saved
    - fpsimd_update_current_state -> replace current's FPSIMD state
    
    Where necessary, the signal handling and fork code are updated to use the above
    wrappers instead of poking into the FPSIMD registers directly.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6391485f342d..c5693163408c 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -205,7 +205,7 @@ void release_thread(struct task_struct *dead_task)
 
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	fpsimd_save_state(&current->thread.fpsimd_state);
+	fpsimd_preserve_current_state();
 	*dst = *src;
 	return 0;
 }

commit 1ce235faa8fefa4eb7199cad890944c1d2ba1b3e
Merge: e38be1b10666 196adf2f3015
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 31 15:01:45 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull ARM64 updates from Catalin Marinas:
     - KGDB support for arm64
     - PCI I/O space extended to 16M (in preparation of PCIe support
       patches)
     - Dropping ZONE_DMA32 in favour of ZONE_DMA (we only need one for the
       time being), together with swiotlb late initialisation to correctly
       setup the bounce buffer
     - DMA API cache maintenance support (not all ARMv8 platforms have
       hardware cache coherency)
     - Crypto extensions advertising via ELF_HWCAP2 for compat user space
     - Perf support for dwarf unwinding in compat mode
     - asm/tlb.h converted to the generic mmu_gather code
     - asm-generic rwsem implementation
     - Code clean-up
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (42 commits)
      arm64: Remove pgprot_dmacoherent()
      arm64: Support DMA_ATTR_WRITE_COMBINE
      arm64: Implement custom mmap functions for dma mapping
      arm64: Fix __range_ok macro
      arm64: Fix duplicated Kconfig entries
      arm64: mm: Route pmd thp functions through pte equivalents
      arm64: rwsem: use asm-generic rwsem implementation
      asm-generic: rwsem: de-PPCify rwsem.h
      arm64: enable generic CPU feature modalias matching for this architecture
      arm64: smp: make local symbol static
      arm64: debug: make local symbols static
      ARM64: perf: support dwarf unwinding in compat mode
      ARM64: perf: add support for frame pointer unwinding in compat mode
      ARM64: perf: add support for perf registers API
      arm64: Add boot time configuration of Intermediate Physical Address size
      arm64: Do not synchronise I and D caches for special ptes
      arm64: Make DMA coherent and strongly ordered mappings not executable
      arm64: barriers: add dmb barrier
      arm64: topology: Implement basic CPU topology support
      arm64: advertise ARMv8 extensions to 32-bit compat ELF binaries
      ...

commit 09024aa61e1bc994404683e2e5b363484a15dd12
Author: Geoff Levand <geoff@infradead.org>
Date:   Tue Dec 17 00:19:29 2013 +0000

    arm64: Fix the soft_restart routine
    
    Change the soft_restart() routine to call cpu_reset() at its identity mapped
    physical address.
    
    The cpu_reset() routine must be called at its identity mapped physical address
    so that when the MMU is turned off the instruction pointer will be at the correct
    location in physical memory.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org> for Huawei, Linaro
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1c0a9be2ffa8..fa6b5bba15f6 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -72,8 +72,17 @@ static void setup_restart(void)
 
 void soft_restart(unsigned long addr)
 {
+	typedef void (*phys_reset_t)(unsigned long);
+	phys_reset_t phys_reset;
+
 	setup_restart();
-	cpu_reset(addr);
+
+	/* Switch to the identity mapping */
+	phys_reset = (phys_reset_t)virt_to_phys(cpu_reset);
+	phys_reset(addr);
+
+	/* Should never get here */
+	BUG();
 }
 
 /*

commit 6990566b535908905b4eccda7cc9e09c2db52187
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon Feb 17 10:59:30 2014 -0500

    cpuidle/arm64: Remove redundant cpuidle_idle_call()
    
    The core idle loop now takes care of it.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Deepthi Dharwar <deepthi@linux.vnet.ibm.com>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: linux-pm@vger.kernel.org
    Cc: linaro-kernel@lists.linaro.org
    Link: http://lkml.kernel.org/n/tip-wk9vpc8dsn46s12pl602ljpo@git.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1c0a9be2ffa8..9cce0098f4cd 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -33,7 +33,6 @@
 #include <linux/kallsyms.h>
 #include <linux/init.h>
 #include <linux/cpu.h>
-#include <linux/cpuidle.h>
 #include <linux/elfcore.h>
 #include <linux/pm.h>
 #include <linux/tick.h>
@@ -94,10 +93,8 @@ void arch_cpu_idle(void)
 	 * This should do all the clock switching and wait for interrupt
 	 * tricks
 	 */
-	if (cpuidle_idle_call()) {
-		cpu_do_idle();
-		local_irq_enable();
-	}
+	cpu_do_idle();
+	local_irq_enable();
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit f864b61ee49bbf3faf9a10b9770c719536328d01
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jan 29 18:00:45 2014 +0000

    arm64: FIQs are unused
    
    So any FIQ handling is superfluous at the moment.  The functions to
    disable/enable FIQs is kept around if ever someone needs them in the
    future, but existing calling sites including arch_cpu_idle_prepare()
    may go for now.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 248a15db37f2..1c0a9be2ffa8 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -85,11 +85,6 @@ EXPORT_SYMBOL_GPL(pm_power_off);
 void (*arm_pm_restart)(enum reboot_mode reboot_mode, const char *cmd);
 EXPORT_SYMBOL_GPL(arm_pm_restart);
 
-void arch_cpu_idle_prepare(void)
-{
-	local_fiq_enable();
-}
-
 /*
  * This is our default idle handler.
  */
@@ -138,7 +133,6 @@ void machine_restart(char *cmd)
 
 	/* Disable interrupts first */
 	local_irq_disable();
-	local_fiq_disable();
 
 	/* Now call the architecture specific reboot code. */
 	if (arm_pm_restart)

commit 0a5be743e8c3c3230600fbc0cf923fb5dbefd579
Merge: 6ac2104debc2 1307220d7bb7
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Dec 19 17:57:51 2013 +0000

    Merge tag 'arm64-suspend' of git://linux-arm.org/linux-2.6-lp into upstream
    
    * tag 'arm64-suspend' of git://linux-arm.org/linux-2.6-lp:
      arm64: add CPU power management menu/entries
      arm64: kernel: add PM build infrastructure
      arm64: kernel: add CPU idle call
      arm64: enable generic clockevent broadcast
      arm64: kernel: implement HW breakpoints CPU PM notifier
      arm64: kernel: refactor code to install/uninstall breakpoints
      arm: kvm: implement CPU PM notifier
      arm64: kernel: implement fpsimd CPU PM notifier
      arm64: kernel: cpu_{suspend/resume} implementation
      arm64: kernel: suspend/resume registers save/restore
      arm64: kernel: build MPIDR_EL1 hash function data structure
      arm64: kernel: add MPIDR_EL1 accessors macros
    
    Conflicts:
            arch/arm64/Kconfig

commit 408c3658b0d49315974ce8b5aed385c8e1527595
Author: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
Date:   Thu Dec 5 13:30:10 2013 +0000

    ARM64: check stack pointer in get_wchan
    
    get_wchan() is lockless. Task may wakeup at any time and change its own stack,
    thus each next stack frame may be overwritten and filled with random stuff.
    
    Signed-off-by: Konstantin Khlebnikov <k.khlebnikov@samsung.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index de17c89985db..0adb8f0f4549 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -308,6 +308,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 unsigned long get_wchan(struct task_struct *p)
 {
 	struct stackframe frame;
+	unsigned long stack_page;
 	int count = 0;
 	if (!p || p == current || p->state == TASK_RUNNING)
 		return 0;
@@ -315,9 +316,11 @@ unsigned long get_wchan(struct task_struct *p)
 	frame.fp = thread_saved_fp(p);
 	frame.sp = thread_saved_sp(p);
 	frame.pc = thread_saved_pc(p);
+	stack_page = (unsigned long)task_stack_page(p);
 	do {
-		int ret = unwind_frame(&frame);
-		if (ret < 0)
+		if (frame.sp < stack_page ||
+		    frame.sp >= stack_page + THREAD_SIZE ||
+		    unwind_frame(&frame))
 			return 0;
 		if (!in_sched_functions(frame.pc))
 			return frame.pc;

commit b8824dfe1b3fe1d4ef3d7ee78a071b8290d3b153
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed Jul 17 10:12:24 2013 +0100

    arm64: kernel: add CPU idle call
    
    When CPU idle is enabled, the architectural idle call should go through
    the idle subsystem to allow CPUs to enter idle states defined
    by the platform CPU idle back-end operations.
    
    This patch, mirroring other archs behaviour, adds the CPU idle call to the
    architectural arch_cpu_idle implementation for arm64.
    
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index de17c89985db..50491ec4de34 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -33,6 +33,7 @@
 #include <linux/kallsyms.h>
 #include <linux/init.h>
 #include <linux/cpu.h>
+#include <linux/cpuidle.h>
 #include <linux/elfcore.h>
 #include <linux/pm.h>
 #include <linux/tick.h>
@@ -98,8 +99,10 @@ void arch_cpu_idle(void)
 	 * This should do all the clock switching and wait for interrupt
 	 * tricks
 	 */
-	cpu_do_idle();
-	local_irq_enable();
+	if (cpuidle_idle_call()) {
+		cpu_do_idle();
+		local_irq_enable();
+	}
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit 9327e2c6bb8cb0131b38a07847cd58c78dc095e9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:18 2013 +0100

    arm64: add CPU_HOTPLUG infrastructure
    
    This patch adds the basic infrastructure necessary to support
    CPU_HOTPLUG on arm64, based on the arm implementation. Actual hotplug
    support will depend on an implementation's cpu_operations (e.g. PSCI).
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 7ae8a1f00c3c..de17c89985db 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -102,6 +102,13 @@ void arch_cpu_idle(void)
 	local_irq_enable();
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+void arch_cpu_idle_dead(void)
+{
+       cpu_die();
+}
+#endif
+
 void machine_shutdown(void)
 {
 #ifdef CONFIG_SMP

commit 6ca68e802612c87c31aa83d50c37ed0d88774a46
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Sep 17 18:49:46 2013 +0100

    arm64: Correctly report LR and SP for compat tasks
    
    When a task crashes and we print debugging information, ensure that
    compat tasks show the actual AArch32 LR and SP registers rather than the
    AArch64 ones.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 57fb55c44c90..7ae8a1f00c3c 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -143,15 +143,26 @@ void machine_restart(char *cmd)
 
 void __show_regs(struct pt_regs *regs)
 {
-	int i;
+	int i, top_reg;
+	u64 lr, sp;
+
+	if (compat_user_mode(regs)) {
+		lr = regs->compat_lr;
+		sp = regs->compat_sp;
+		top_reg = 12;
+	} else {
+		lr = regs->regs[30];
+		sp = regs->sp;
+		top_reg = 29;
+	}
 
 	show_regs_print_info(KERN_DEFAULT);
 	print_symbol("PC is at %s\n", instruction_pointer(regs));
-	print_symbol("LR is at %s\n", regs->regs[30]);
+	print_symbol("LR is at %s\n", lr);
 	printk("pc : [<%016llx>] lr : [<%016llx>] pstate: %08llx\n",
-	       regs->pc, regs->regs[30], regs->pstate);
-	printk("sp : %016llx\n", regs->sp);
-	for (i = 29; i >= 0; i--) {
+	       regs->pc, lr, regs->pstate);
+	printk("sp : %016llx\n", sp);
+	for (i = top_reg; i >= 0; i--) {
 		printk("x%-2d: %016llx ", i, regs->regs[i]);
 		if (i % 2 == 0)
 			printk("\n");

commit b0946fc84628b8d60e7a2034b48d1aff7da9d1df
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 23 11:05:10 2013 +0100

    arm64: Fix definition of arm_pm_restart to match the declaration
    
    Commit ff70130 (arm64: use common reboot infrastructure) converted the
    arm_pm_restart declaration to the new reboot infrastructure but missed
    the actual definition.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 1788bf6b471f..57fb55c44c90 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -81,7 +81,7 @@ void soft_restart(unsigned long addr)
 void (*pm_power_off)(void);
 EXPORT_SYMBOL_GPL(pm_power_off);
 
-void (*arm_pm_restart)(char str, const char *cmd);
+void (*arm_pm_restart)(enum reboot_mode reboot_mode, const char *cmd);
 EXPORT_SYMBOL_GPL(arm_pm_restart);
 
 void arch_cpu_idle_prepare(void)

commit ff701306cd49aaff80fb852323b387812bc76491
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Jul 11 12:13:00 2013 +0100

    arm64: use common reboot infrastructure
    
    Commit 7b6d864b48d9 (reboot: arm: change reboot_mode to use enum
    reboot_mode) changed the way reboot is handled on arm, which has a
    direct impact on arm64 as we share the reset driver on the VE platform.
    
    The obvious fix is to move arm64 to use the same infrastructure.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    [catalin.marinas@arm.com: removed reboot_mode = REBOOT_HARD default setting]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 46f02c3b5015..1788bf6b471f 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -132,7 +132,7 @@ void machine_restart(char *cmd)
 
 	/* Now call the architecture specific reboot code. */
 	if (arm_pm_restart)
-		arm_pm_restart('h', cmd);
+		arm_pm_restart(reboot_mode, cmd);
 
 	/*
 	 * Whoops - the architecture was unable to reboot.

commit 3d15b798eafd3b6b3cc25f20747008ab9401a57f
Merge: 942d33da999b 420c158dcf96
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 8 15:15:27 2013 -0700

    Merge tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull arm64 update from Catalin Marinas:
    
     - Since drivers/irqchip/irq-gic.c no longer has dependencies on arm32
       specifics (the 'gic' branch merged), it can be enabled on arm64.
    
     - Enable arm64 support for poweroff/restart (for code under
       drivers/power/reset/).
    
     - Fixes (dts file, exception handling, bitops)
    
    * tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64:
      arm64: Treat the bitops index argument as an 'int'
      arm64: Ignore the 'write' ESR flag on cache maintenance faults
      arm64: dts: fix #address-cells for foundation-v8
      arm64: vexpress: Add support for poweroff/restart
      arm64: Enable support for the ARM GIC interrupt controller

commit aa1e8ec1d2a997b39aebab13c32b77da2ac0f287
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Feb 28 18:14:37 2013 +0000

    arm64: vexpress: Add support for poweroff/restart
    
    This patch adds the arm_pm_poweroff definition expected by the
    vexpress-poweroff.c driver and enables the latter for arm64.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Pawel Moll <pawel.moll@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 116a60abe86f..bbefb6fdfee2 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -81,8 +81,8 @@ void soft_restart(unsigned long addr)
 void (*pm_power_off)(void);
 EXPORT_SYMBOL_GPL(pm_power_off);
 
-void (*pm_restart)(const char *cmd);
-EXPORT_SYMBOL_GPL(pm_restart);
+void (*arm_pm_restart)(char str, const char *cmd);
+EXPORT_SYMBOL_GPL(arm_pm_restart);
 
 
 /*
@@ -164,8 +164,8 @@ void machine_restart(char *cmd)
 	local_fiq_disable();
 
 	/* Now call the architecture specific reboot code. */
-	if (pm_restart)
-		pm_restart(cmd);
+	if (arm_pm_restart)
+		arm_pm_restart('h', cmd);
 
 	/*
 	 * Whoops - the architecture was unable to reboot.

commit a43cb95d547a061ed5bf1acb28e0f5fd575e26c1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:17 2013 -0700

    dump_stack: unify debug information printed by show_regs()
    
    show_regs() is inherently arch-dependent but it does make sense to print
    generic debug information and some archs already do albeit in slightly
    different forms.  This patch introduces a generic function to print debug
    information from show_regs() so that different archs print out the same
    information and it's much easier to modify what's printed.
    
    show_regs_print_info() prints out the same debug info as dump_stack()
    does plus task and thread_info pointers.
    
    * Archs which didn't print debug info now do.
    
      alpha, arc, blackfin, c6x, cris, frv, h8300, hexagon, ia64, m32r,
      metag, microblaze, mn10300, openrisc, parisc, score, sh64, sparc,
      um, xtensa
    
    * Already prints debug info.  Replaced with show_regs_print_info().
      The printed information is superset of what used to be there.
    
      arm, arm64, avr32, mips, powerpc, sh32, tile, unicore32, x86
    
    * s390 is special in that it used to print arch-specific information
      along with generic debug info.  Heiko and Martin think that the
      arch-specific extra isn't worth keeping s390 specfic implementation.
      Converted to use the generic version.
    
    Note that now all archs print the debug info before actual register
    dumps.
    
    An example BUG() dump follows.
    
     kernel BUG at /work/os/work/kernel/workqueue.c:4841!
     invalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.9.0-rc1-work+ #7
     Hardware name: empty empty/S3992, BIOS 080011  10/26/2007
     task: ffff88007c85e040 ti: ffff88007c860000 task.ti: ffff88007c860000
     RIP: 0010:[<ffffffff8234a07e>]  [<ffffffff8234a07e>] init_workqueues+0x4/0x6
     RSP: 0000:ffff88007c861ec8  EFLAGS: 00010246
     RAX: ffff88007c861fd8 RBX: ffffffff824466a8 RCX: 0000000000000001
     RDX: 0000000000000046 RSI: 0000000000000001 RDI: ffffffff8234a07a
     RBP: ffff88007c861ec8 R08: 0000000000000000 R09: 0000000000000000
     R10: 0000000000000001 R11: 0000000000000000 R12: ffffffff8234a07a
     R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
     FS:  0000000000000000(0000) GS:ffff88007dc00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffff88015f7ff000 CR3: 00000000021f1000 CR4: 00000000000007f0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
     Stack:
      ffff88007c861ef8 ffffffff81000312 ffffffff824466a8 ffff88007c85e650
      0000000000000003 0000000000000000 ffff88007c861f38 ffffffff82335e5d
      ffff88007c862080 ffffffff8223d8c0 ffff88007c862080 ffffffff81c47760
     Call Trace:
      [<ffffffff81000312>] do_one_initcall+0x122/0x170
      [<ffffffff82335e5d>] kernel_init_freeable+0x9b/0x1c8
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      [<ffffffff81c4776e>] kernel_init+0xe/0xf0
      [<ffffffff81c6be9c>] ret_from_fork+0x7c/0xb0
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      ...
    
    v2: Typo fix in x86-32.
    
    v3: CPU number dropped from show_regs_print_info() as
        dump_stack_print_info() has been updated to print it.  s390
        specific implementation dropped as requested by s390 maintainers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>           [tile bits]
    Acked-by: Richard Kuo <rkuo@codeaurora.org>             [hexagon bits]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 6f3822f98dcd..f4919721f7dd 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -145,11 +145,7 @@ void __show_regs(struct pt_regs *regs)
 {
 	int i;
 
-	printk("CPU: %d    %s  (%s %.*s)\n",
-		raw_smp_processor_id(), print_tainted(),
-		init_utsname()->release,
-		(int)strcspn(init_utsname()->version, " "),
-		init_utsname()->version);
+	show_regs_print_info(KERN_DEFAULT);
 	print_symbol("PC is at %s\n", instruction_pointer(regs));
 	print_symbol("LR is at %s\n", regs->regs[30]);
 	printk("pc : [<%016llx>] lr : [<%016llx>] pstate: %08llx\n",
@@ -166,7 +162,6 @@ void __show_regs(struct pt_regs *regs)
 void show_regs(struct pt_regs * regs)
 {
 	printk("\n");
-	printk("Pid: %d, comm: %20s\n", task_pid_nr(current), current->comm);
 	__show_regs(regs);
 }
 

commit c9ef713993ba168b38d1a97ea0ab00874f1da022
Merge: 87c1f0f8c944 16c85a1fd73e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 10:10:48 2013 -0700

    Merge tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull arm64 update from Catalin Marinas:
     "Main features:
    
       - Versatile Express SoC (model) support - DT files and Kconfig
         entries (there are no arch/arm64/mach-* directories).  The bulk of
         the code has already been moved to drivers/ as part of the ARM SoC
         clean-up.
    
       - Basic multi-cluster support (CPU logical map initialised from the
         DT)
    
       - Simple earlyprintk support for UART 8250/16550 and FastModel
         console output
    
       - Optimised kernel library bitops and string functions.
    
       - Automatic initialisation of the irqchip and clocks via DT"
    
    * tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (26 commits)
      arm64: Use acquire/release semantics instead of explicit DMB
      arm64: klib: bitops: fix unpredictable stxr usage
      arm64: vexpress: Enable ARMv8 RTSM model (SoC) support
      arm64: vexpress: Add dts files for the ARMv8 RTSM models
      arm64: Survive invalid cpu enable-methods
      arm64: mm: Correct show_pte behaviour
      arm64: Fix compat types affecting struct compat_stat
      arm64: Execute DSB during thread switching for TLB/cache maintenance
      arm64: compiling issue, need add include/asm/vga.h file
      arm64: smp: honour #address-size when parsing CPU reg property
      arm64: Define cmpxchg64 and cmpxchg64_local for outside use
      arm64: Define readq and writeq for driver module using
      arm64: Fix task tracing
      arm64: add explicit symbols to ESR_EL1 decoding
      arm64: Use irqchip_init() for interrupt controller initialisation
      arm64: psci: Use the MPIDR values from cpu_logical_map for cpu ids.
      arm64: klib: Optimised atomic bitops
      arm64: klib: Optimised string functions
      arm64: klib: Optimised memory functions
      arm64: head: match all affinity levels in the pen of the secondaries
      ...

commit 5108c67c376b3ee59cc7fbe46eaba481eb3419aa
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 24 14:47:02 2013 +0100

    arm64: Execute DSB during thread switching for TLB/cache maintenance
    
    The DSB following TLB or cache maintenance ops must be run on the same
    CPU. With kernel preemption enabled or for user-space cache maintenance
    this may not be the case. This patch adds an explicit DSB in the
    __switch_to() function.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index c2cc2493481b..116a60abe86f 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -313,6 +313,12 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	hw_breakpoint_thread_switch(next);
 	contextidr_thread_switch(next);
 
+	/*
+	 * Complete any pending TLB or cache maintenance on this CPU in case
+	 * the thread migrates to a different CPU.
+	 */
+	dsb();
+
 	/* the actual thread switch */
 	last = cpu_switch_to(prev, next);
 

commit 3325732f3b223812e54ac67dbb0a8ea7c99cf519
Author: Christopher Covington <cov@codeaurora.org>
Date:   Wed Apr 3 19:01:01 2013 +0100

    arm64: Fix task tracing
    
    For accurate accounting call contextidr_thread_switch before a
    task is scheduled, rather than after, when the 'next' variable has a
    different meaning since we switched the stacks.
    
    Signed-off-by: Christopher Covington <cov@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 0337cdb0667b..c2cc2493481b 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -311,11 +311,11 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	fpsimd_thread_switch(next);
 	tls_thread_switch(next);
 	hw_breakpoint_thread_switch(next);
+	contextidr_thread_switch(next);
 
 	/* the actual thread switch */
 	last = cpu_switch_to(prev, next);
 
-	contextidr_thread_switch(next);
 	return last;
 }
 

commit 0087298f68a726493a637c4f68d148b31102b0d9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:39 2013 +0100

    arm64: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/r/20130321215233.887563095@linutronix.de

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 0337cdb0667b..83a0ad5936a5 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -84,11 +84,15 @@ EXPORT_SYMBOL_GPL(pm_power_off);
 void (*pm_restart)(const char *cmd);
 EXPORT_SYMBOL_GPL(pm_restart);
 
+void arch_cpu_idle_prepare(void)
+{
+	local_fiq_enable();
+}
 
 /*
  * This is our default idle handler.
  */
-static void default_idle(void)
+void arch_cpu_idle(void)
 {
 	/*
 	 * This should do all the clock switching and wait for interrupt
@@ -98,43 +102,6 @@ static void default_idle(void)
 	local_irq_enable();
 }
 
-/*
- * The idle thread.
- * We always respect 'hlt_counter' to prevent low power idle.
- */
-void cpu_idle(void)
-{
-	local_fiq_enable();
-
-	/* endless idle loop with no priority at all */
-	while (1) {
-		tick_nohz_idle_enter();
-		rcu_idle_enter();
-		while (!need_resched()) {
-			/*
-			 * We need to disable interrupts here to ensure
-			 * we don't miss a wakeup call.
-			 */
-			local_irq_disable();
-			if (!need_resched()) {
-				stop_critical_timings();
-				default_idle();
-				start_critical_timings();
-				/*
-				 * default_idle functions should always return
-				 * with IRQs enabled.
-				 */
-				WARN_ON(irqs_disabled());
-			} else {
-				local_irq_enable();
-			}
-		}
-		rcu_idle_exit();
-		tick_nohz_idle_exit();
-		schedule_preempt_disabled();
-	}
-}
-
 void machine_shutdown(void)
 {
 #ifdef CONFIG_SMP

commit 79a69d342d71b2b4eafdf51e2451606cfe380a44
Merge: 6db167dfc013 ec45d1cfd3cb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 20 14:30:33 2013 -0800

    Merge tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull arm64 patches from Catalin Marinas:
    
     - SMP support for the PSCI booting protocol (power state coordination
       interface).
    
     - Simple earlyprintk support.
    
     - Platform devices populated by default from the DT (SoC-agnostic).
    
     - CONTEXTIDR support (used by external trace tools).
    
    * tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64:
      arm64: mm: update CONTEXTIDR register to contain PID of current process
      arm64: atomics: fix grossly inconsistent asm constraints for exclusives
      arm64: compat: use compat_uptr_t type for compat_ucontext.uc_link
      arm64: Select ARCH_WANT_FRAME_POINTERS
      arm64: Add kvm_para.h and xor.h generic headers
      arm64: SMP: enable PSCI boot method
      arm64: psci: add support for PSCI invocations from the kernel
      arm64: SMP: rework the SMP code to be enabling method agnostic
      arm64: perf: add guest vs host discrimination
      arm64: add COMPAT_PSR_*_BIT flags
      arm64: Add simple earlyprintk support
      arm64: Populate the platform devices

commit dc883ca34a4e8a309d652c86daab6f1b4edd9d4b
Author: Len Brown <len.brown@intel.com>
Date:   Sat Feb 9 23:15:13 2013 -0500

    ARM64 idle: delete pm_idle
    
    pm_idle() on arm64 was a synonym for default_idle(),
    so remove it and invoke default_idle() directly.
    
    Signed-off-by: Len Brown <len.brown@intel.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index cb0956bc96ed..c7002d40a9b0 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -97,14 +97,9 @@ static void default_idle(void)
 	local_irq_enable();
 }
 
-void (*pm_idle)(void) = default_idle;
-EXPORT_SYMBOL_GPL(pm_idle);
-
 /*
- * The idle thread, has rather strange semantics for calling pm_idle,
- * but this is what x86 does and we need to do the same, so that
- * things like cpuidle get called in the same way.  The only difference
- * is that we always respect 'hlt_counter' to prevent low power idle.
+ * The idle thread.
+ * We always respect 'hlt_counter' to prevent low power idle.
  */
 void cpu_idle(void)
 {
@@ -122,10 +117,10 @@ void cpu_idle(void)
 			local_irq_disable();
 			if (!need_resched()) {
 				stop_critical_timings();
-				pm_idle();
+				default_idle();
 				start_critical_timings();
 				/*
-				 * pm_idle functions should always return
+				 * default_idle functions should always return
 				 * with IRQs enabled.
 				 */
 				WARN_ON(irqs_disabled());

commit ec45d1cfd3cb65121fc52f39efc17d832f4f7b91
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jan 17 12:31:45 2013 +0000

    arm64: mm: update CONTEXTIDR register to contain PID of current process
    
    This patch is a port of 575320d62 ("ARM: 7445/1: mm: update CONTEXTIDR
    register to contain PID of current process") from ARM that introduces a
    new Kconfig option which, when enabled, causes the kernel to write the
    PID of the current task into the CONTEXTIDR register on context switch.
    This is useful when analysing hardware trace, since writes to this
    register can be configured to emit an event into the trace stream.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: contextidr_thread_switch() moved to mmu_context.h]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index cb0956bc96ed..a8fbd7eaa2ed 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -45,9 +45,10 @@
 
 #include <asm/compat.h>
 #include <asm/cacheflush.h>
+#include <asm/fpsimd.h>
+#include <asm/mmu_context.h>
 #include <asm/processor.h>
 #include <asm/stacktrace.h>
-#include <asm/fpsimd.h>
 
 static void setup_restart(void)
 {
@@ -319,6 +320,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	/* the actual thread switch */
 	last = cpu_switch_to(prev, next);
 
+	contextidr_thread_switch(next);
 	return last;
 }
 

commit afa86fc426ff7e7f5477f15da9c405d08d5cf790
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:51:14 2012 -0400

    flagday: don't pass regs to copy_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 5a1335caf6f1..cb0956bc96ed 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -234,8 +234,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 asmlinkage void ret_from_fork(void) asm("ret_from_fork");
 
 int copy_thread(unsigned long clone_flags, unsigned long stack_start,
-		unsigned long stk_sz, struct task_struct *p,
-		struct pt_regs *unused)
+		unsigned long stk_sz, struct task_struct *p)
 {
 	struct pt_regs *childregs = task_pt_regs(p);
 	unsigned long tls = p->thread.tp_value;

commit 9ac08002130b591d0f2ee035aa9062f84f2f15cb
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 21 15:56:52 2012 -0400

    arm64: sanitize copy_thread(), switch to generic fork/vfork/clone
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index 8a5f3341861e..5a1335caf6f1 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -235,15 +235,15 @@ asmlinkage void ret_from_fork(void) asm("ret_from_fork");
 
 int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		unsigned long stk_sz, struct task_struct *p,
-		struct pt_regs *regs)
+		struct pt_regs *unused)
 {
 	struct pt_regs *childregs = task_pt_regs(p);
 	unsigned long tls = p->thread.tp_value;
 
 	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
 
-	if (likely(regs)) {
-		*childregs = *regs;
+	if (likely(!(p->flags & PF_KTHREAD))) {
+		*childregs = *current_pt_regs();
 		childregs->regs[0] = 0;
 		if (is_compat_thread(task_thread_info(p))) {
 			if (stack_start)
@@ -266,7 +266,7 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		 * for the new thread.
 		 */
 		if (clone_flags & CLONE_SETTLS)
-			tls = regs->regs[3];
+			tls = childregs->regs[3];
 	} else {
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->pstate = PSR_MODE_EL1h;

commit 6929039761a3414e5c71448eb3dcc1d82fc1891d
Merge: e0fd18ce1169 6ba1bc826d16
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Nov 16 20:53:36 2012 -0500

    Merge commit '6ba1bc826d160fe4f32bcb188687dcca4bdfaf3d' into arch-arm64
    
    Backmerge from mainline commit that introduced a trivial conflict in
    arch/arm64/kernel/process.c - a bunch of functions removed next to the
    place where kernel_thread() used to be.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

commit 6ba1bc826d160fe4f32bcb188687dcca4bdfaf3d
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 6 19:28:48 2012 +0000

    arm64: elf: fix core dumping definitions for GP and FP registers
    
    struct user_fp does not exist for arm64, so use struct user_fpsimd_state
    instead for the ELF core dumping definitions. Furthermore, since we use
    regset-based core dumping, we do not need definitions for dump_task_regs
    and dump_fpu.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f22965ea1cfc..e04cebdbb47f 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -309,24 +309,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	return last;
 }
 
-/*
- * Fill in the task's elfregs structure for a core dump.
- */
-int dump_task_regs(struct task_struct *t, elf_gregset_t *elfregs)
-{
-	elf_core_copy_regs(elfregs, task_pt_regs(t));
-	return 1;
-}
-
-/*
- * fill in the fpe structure for a core dump...
- */
-int dump_fpu (struct pt_regs *regs, struct user_fp *fp)
-{
-	return 0;
-}
-EXPORT_SYMBOL(dump_fpu);
-
 /*
  * Shuffle the argument into the correct register before calling the
  * thread function.  x1 is the thread argument, x2 is the pointer to

commit e0fd18ce1169595df929373cad2ae9b00b2289c2
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Oct 18 00:55:54 2012 -0400

    arm64: get rid of fork/vfork/clone wrappers
    
    [fixes from Catalin Marinas folded]
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index bf615e212c6c..f82987a784af 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -246,14 +246,20 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 		*childregs = *regs;
 		childregs->regs[0] = 0;
 		if (is_compat_thread(task_thread_info(p))) {
-			childregs->compat_sp = stack_start;
+			if (stack_start)
+				childregs->compat_sp = stack_start;
 		} else {
 			/*
 			 * Read the current TLS pointer from tpidr_el0 as it may be
 			 * out-of-sync with the saved value.
 			 */
 			asm("mrs %0, tpidr_el0" : "=r" (tls));
-			childregs->sp = stack_start;
+			if (stack_start) {
+				/* 16-byte aligned stack mandatory on AArch64 */
+				if (stack_start & 15)
+					return -EINVAL;
+				childregs->sp = stack_start;
+			}
 		}
 		/*
 		 * If a TLS pointer was passed to clone (4th argument), use it

commit c34501d21b005a6e363386a19519bd11cf92a67c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Oct 5 12:31:20 2012 +0100

    arm64: Use generic kernel_thread() implementation
    
    This patch enables CONFIG_GENERIC_KERNEL_THREAD on arm64, changes
    copy_threads to cope with kernel threads creation and adapts
    ret_from_fork accordingly. The arm64-specific kernel_thread
    implementation is no longer needed.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
index f22965ea1cfc..bf615e212c6c 100644
--- a/arch/arm64/kernel/process.c
+++ b/arch/arm64/kernel/process.c
@@ -240,27 +240,35 @@ int copy_thread(unsigned long clone_flags, unsigned long stack_start,
 	struct pt_regs *childregs = task_pt_regs(p);
 	unsigned long tls = p->thread.tp_value;
 
-	*childregs = *regs;
-	childregs->regs[0] = 0;
+	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
 
-	if (is_compat_thread(task_thread_info(p)))
-		childregs->compat_sp = stack_start;
-	else {
+	if (likely(regs)) {
+		*childregs = *regs;
+		childregs->regs[0] = 0;
+		if (is_compat_thread(task_thread_info(p))) {
+			childregs->compat_sp = stack_start;
+		} else {
+			/*
+			 * Read the current TLS pointer from tpidr_el0 as it may be
+			 * out-of-sync with the saved value.
+			 */
+			asm("mrs %0, tpidr_el0" : "=r" (tls));
+			childregs->sp = stack_start;
+		}
 		/*
-		 * Read the current TLS pointer from tpidr_el0 as it may be
-		 * out-of-sync with the saved value.
+		 * If a TLS pointer was passed to clone (4th argument), use it
+		 * for the new thread.
 		 */
-		asm("mrs %0, tpidr_el0" : "=r" (tls));
-		childregs->sp = stack_start;
+		if (clone_flags & CLONE_SETTLS)
+			tls = regs->regs[3];
+	} else {
+		memset(childregs, 0, sizeof(struct pt_regs));
+		childregs->pstate = PSR_MODE_EL1h;
+		p->thread.cpu_context.x19 = stack_start;
+		p->thread.cpu_context.x20 = stk_sz;
 	}
-
-	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
-	p->thread.cpu_context.sp = (unsigned long)childregs;
 	p->thread.cpu_context.pc = (unsigned long)ret_from_fork;
-
-	/* If a TLS pointer was passed to clone, use that for the new thread. */
-	if (clone_flags & CLONE_SETTLS)
-		tls = regs->regs[3];
+	p->thread.cpu_context.sp = (unsigned long)childregs;
 	p->thread.tp_value = tls;
 
 	ptrace_hw_copy_thread(p);
@@ -327,43 +335,6 @@ int dump_fpu (struct pt_regs *regs, struct user_fp *fp)
 }
 EXPORT_SYMBOL(dump_fpu);
 
-/*
- * Shuffle the argument into the correct register before calling the
- * thread function.  x1 is the thread argument, x2 is the pointer to
- * the thread function, and x3 points to the exit function.
- */
-extern void kernel_thread_helper(void);
-asm(	".section .text\n"
-"	.align\n"
-"	.type	kernel_thread_helper, #function\n"
-"kernel_thread_helper:\n"
-"	mov	x0, x1\n"
-"	mov	x30, x3\n"
-"	br	x2\n"
-"	.size	kernel_thread_helper, . - kernel_thread_helper\n"
-"	.previous");
-
-#define kernel_thread_exit	do_exit
-
-/*
- * Create a kernel thread.
- */
-pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
-{
-	struct pt_regs regs;
-
-	memset(&regs, 0, sizeof(regs));
-
-	regs.regs[1] = (unsigned long)arg;
-	regs.regs[2] = (unsigned long)fn;
-	regs.regs[3] = (unsigned long)kernel_thread_exit;
-	regs.pc = (unsigned long)kernel_thread_helper;
-	regs.pstate = PSR_MODE_EL1h;
-
-	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
-}
-EXPORT_SYMBOL(kernel_thread);
-
 unsigned long get_wchan(struct task_struct *p)
 {
 	struct stackframe frame;

commit b3901d54dc4f73acdc6b7c6e5a7a496d3afeae61
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:28 2012 +0000

    arm64: Process management
    
    The patch adds support for thread creation and context switching. The
    context switching CPU specific code is introduced with the CPU support
    patch (part of the arch/arm64/mm/proc.S file). AArch64 supports
    ASID-tagged TLBs and the ASID can be either 8 or 16-bit wide (detectable
    via the ID_AA64AFR0_EL1 register).
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
new file mode 100644
index 000000000000..f22965ea1cfc
--- /dev/null
+++ b/arch/arm64/kernel/process.c
@@ -0,0 +1,408 @@
+/*
+ * Based on arch/arm/kernel/process.c
+ *
+ * Original Copyright (C) 1995  Linus Torvalds
+ * Copyright (C) 1996-2000 Russell King - Converted to ARM.
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <stdarg.h>
+
+#include <linux/export.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/stddef.h>
+#include <linux/unistd.h>
+#include <linux/user.h>
+#include <linux/delay.h>
+#include <linux/reboot.h>
+#include <linux/interrupt.h>
+#include <linux/kallsyms.h>
+#include <linux/init.h>
+#include <linux/cpu.h>
+#include <linux/elfcore.h>
+#include <linux/pm.h>
+#include <linux/tick.h>
+#include <linux/utsname.h>
+#include <linux/uaccess.h>
+#include <linux/random.h>
+#include <linux/hw_breakpoint.h>
+#include <linux/personality.h>
+#include <linux/notifier.h>
+
+#include <asm/compat.h>
+#include <asm/cacheflush.h>
+#include <asm/processor.h>
+#include <asm/stacktrace.h>
+#include <asm/fpsimd.h>
+
+static void setup_restart(void)
+{
+	/*
+	 * Tell the mm system that we are going to reboot -
+	 * we may need it to insert some 1:1 mappings so that
+	 * soft boot works.
+	 */
+	setup_mm_for_reboot();
+
+	/* Clean and invalidate caches */
+	flush_cache_all();
+
+	/* Turn D-cache off */
+	cpu_cache_off();
+
+	/* Push out any further dirty data, and ensure cache is empty */
+	flush_cache_all();
+}
+
+void soft_restart(unsigned long addr)
+{
+	setup_restart();
+	cpu_reset(addr);
+}
+
+/*
+ * Function pointers to optional machine specific functions
+ */
+void (*pm_power_off)(void);
+EXPORT_SYMBOL_GPL(pm_power_off);
+
+void (*pm_restart)(const char *cmd);
+EXPORT_SYMBOL_GPL(pm_restart);
+
+
+/*
+ * This is our default idle handler.
+ */
+static void default_idle(void)
+{
+	/*
+	 * This should do all the clock switching and wait for interrupt
+	 * tricks
+	 */
+	cpu_do_idle();
+	local_irq_enable();
+}
+
+void (*pm_idle)(void) = default_idle;
+EXPORT_SYMBOL_GPL(pm_idle);
+
+/*
+ * The idle thread, has rather strange semantics for calling pm_idle,
+ * but this is what x86 does and we need to do the same, so that
+ * things like cpuidle get called in the same way.  The only difference
+ * is that we always respect 'hlt_counter' to prevent low power idle.
+ */
+void cpu_idle(void)
+{
+	local_fiq_enable();
+
+	/* endless idle loop with no priority at all */
+	while (1) {
+		tick_nohz_idle_enter();
+		rcu_idle_enter();
+		while (!need_resched()) {
+			/*
+			 * We need to disable interrupts here to ensure
+			 * we don't miss a wakeup call.
+			 */
+			local_irq_disable();
+			if (!need_resched()) {
+				stop_critical_timings();
+				pm_idle();
+				start_critical_timings();
+				/*
+				 * pm_idle functions should always return
+				 * with IRQs enabled.
+				 */
+				WARN_ON(irqs_disabled());
+			} else {
+				local_irq_enable();
+			}
+		}
+		rcu_idle_exit();
+		tick_nohz_idle_exit();
+		schedule_preempt_disabled();
+	}
+}
+
+void machine_shutdown(void)
+{
+#ifdef CONFIG_SMP
+	smp_send_stop();
+#endif
+}
+
+void machine_halt(void)
+{
+	machine_shutdown();
+	while (1);
+}
+
+void machine_power_off(void)
+{
+	machine_shutdown();
+	if (pm_power_off)
+		pm_power_off();
+}
+
+void machine_restart(char *cmd)
+{
+	machine_shutdown();
+
+	/* Disable interrupts first */
+	local_irq_disable();
+	local_fiq_disable();
+
+	/* Now call the architecture specific reboot code. */
+	if (pm_restart)
+		pm_restart(cmd);
+
+	/*
+	 * Whoops - the architecture was unable to reboot.
+	 */
+	printk("Reboot failed -- System halted\n");
+	while (1);
+}
+
+void __show_regs(struct pt_regs *regs)
+{
+	int i;
+
+	printk("CPU: %d    %s  (%s %.*s)\n",
+		raw_smp_processor_id(), print_tainted(),
+		init_utsname()->release,
+		(int)strcspn(init_utsname()->version, " "),
+		init_utsname()->version);
+	print_symbol("PC is at %s\n", instruction_pointer(regs));
+	print_symbol("LR is at %s\n", regs->regs[30]);
+	printk("pc : [<%016llx>] lr : [<%016llx>] pstate: %08llx\n",
+	       regs->pc, regs->regs[30], regs->pstate);
+	printk("sp : %016llx\n", regs->sp);
+	for (i = 29; i >= 0; i--) {
+		printk("x%-2d: %016llx ", i, regs->regs[i]);
+		if (i % 2 == 0)
+			printk("\n");
+	}
+	printk("\n");
+}
+
+void show_regs(struct pt_regs * regs)
+{
+	printk("\n");
+	printk("Pid: %d, comm: %20s\n", task_pid_nr(current), current->comm);
+	__show_regs(regs);
+}
+
+/*
+ * Free current thread data structures etc..
+ */
+void exit_thread(void)
+{
+}
+
+void flush_thread(void)
+{
+	fpsimd_flush_thread();
+	flush_ptrace_hw_breakpoint(current);
+}
+
+void release_thread(struct task_struct *dead_task)
+{
+}
+
+int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
+{
+	fpsimd_save_state(&current->thread.fpsimd_state);
+	*dst = *src;
+	return 0;
+}
+
+asmlinkage void ret_from_fork(void) asm("ret_from_fork");
+
+int copy_thread(unsigned long clone_flags, unsigned long stack_start,
+		unsigned long stk_sz, struct task_struct *p,
+		struct pt_regs *regs)
+{
+	struct pt_regs *childregs = task_pt_regs(p);
+	unsigned long tls = p->thread.tp_value;
+
+	*childregs = *regs;
+	childregs->regs[0] = 0;
+
+	if (is_compat_thread(task_thread_info(p)))
+		childregs->compat_sp = stack_start;
+	else {
+		/*
+		 * Read the current TLS pointer from tpidr_el0 as it may be
+		 * out-of-sync with the saved value.
+		 */
+		asm("mrs %0, tpidr_el0" : "=r" (tls));
+		childregs->sp = stack_start;
+	}
+
+	memset(&p->thread.cpu_context, 0, sizeof(struct cpu_context));
+	p->thread.cpu_context.sp = (unsigned long)childregs;
+	p->thread.cpu_context.pc = (unsigned long)ret_from_fork;
+
+	/* If a TLS pointer was passed to clone, use that for the new thread. */
+	if (clone_flags & CLONE_SETTLS)
+		tls = regs->regs[3];
+	p->thread.tp_value = tls;
+
+	ptrace_hw_copy_thread(p);
+
+	return 0;
+}
+
+static void tls_thread_switch(struct task_struct *next)
+{
+	unsigned long tpidr, tpidrro;
+
+	if (!is_compat_task()) {
+		asm("mrs %0, tpidr_el0" : "=r" (tpidr));
+		current->thread.tp_value = tpidr;
+	}
+
+	if (is_compat_thread(task_thread_info(next))) {
+		tpidr = 0;
+		tpidrro = next->thread.tp_value;
+	} else {
+		tpidr = next->thread.tp_value;
+		tpidrro = 0;
+	}
+
+	asm(
+	"	msr	tpidr_el0, %0\n"
+	"	msr	tpidrro_el0, %1"
+	: : "r" (tpidr), "r" (tpidrro));
+}
+
+/*
+ * Thread switching.
+ */
+struct task_struct *__switch_to(struct task_struct *prev,
+				struct task_struct *next)
+{
+	struct task_struct *last;
+
+	fpsimd_thread_switch(next);
+	tls_thread_switch(next);
+	hw_breakpoint_thread_switch(next);
+
+	/* the actual thread switch */
+	last = cpu_switch_to(prev, next);
+
+	return last;
+}
+
+/*
+ * Fill in the task's elfregs structure for a core dump.
+ */
+int dump_task_regs(struct task_struct *t, elf_gregset_t *elfregs)
+{
+	elf_core_copy_regs(elfregs, task_pt_regs(t));
+	return 1;
+}
+
+/*
+ * fill in the fpe structure for a core dump...
+ */
+int dump_fpu (struct pt_regs *regs, struct user_fp *fp)
+{
+	return 0;
+}
+EXPORT_SYMBOL(dump_fpu);
+
+/*
+ * Shuffle the argument into the correct register before calling the
+ * thread function.  x1 is the thread argument, x2 is the pointer to
+ * the thread function, and x3 points to the exit function.
+ */
+extern void kernel_thread_helper(void);
+asm(	".section .text\n"
+"	.align\n"
+"	.type	kernel_thread_helper, #function\n"
+"kernel_thread_helper:\n"
+"	mov	x0, x1\n"
+"	mov	x30, x3\n"
+"	br	x2\n"
+"	.size	kernel_thread_helper, . - kernel_thread_helper\n"
+"	.previous");
+
+#define kernel_thread_exit	do_exit
+
+/*
+ * Create a kernel thread.
+ */
+pid_t kernel_thread(int (*fn)(void *), void *arg, unsigned long flags)
+{
+	struct pt_regs regs;
+
+	memset(&regs, 0, sizeof(regs));
+
+	regs.regs[1] = (unsigned long)arg;
+	regs.regs[2] = (unsigned long)fn;
+	regs.regs[3] = (unsigned long)kernel_thread_exit;
+	regs.pc = (unsigned long)kernel_thread_helper;
+	regs.pstate = PSR_MODE_EL1h;
+
+	return do_fork(flags|CLONE_VM|CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
+}
+EXPORT_SYMBOL(kernel_thread);
+
+unsigned long get_wchan(struct task_struct *p)
+{
+	struct stackframe frame;
+	int count = 0;
+	if (!p || p == current || p->state == TASK_RUNNING)
+		return 0;
+
+	frame.fp = thread_saved_fp(p);
+	frame.sp = thread_saved_sp(p);
+	frame.pc = thread_saved_pc(p);
+	do {
+		int ret = unwind_frame(&frame);
+		if (ret < 0)
+			return 0;
+		if (!in_sched_functions(frame.pc))
+			return frame.pc;
+	} while (count ++ < 16);
+	return 0;
+}
+
+unsigned long arch_align_stack(unsigned long sp)
+{
+	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
+		sp -= get_random_int() & ~PAGE_MASK;
+	return sp & ~0xf;
+}
+
+static unsigned long randomize_base(unsigned long base)
+{
+	unsigned long range_end = base + (STACK_RND_MASK << PAGE_SHIFT) + 1;
+	return randomize_range(base, range_end, 0) ? : base;
+}
+
+unsigned long arch_randomize_brk(struct mm_struct *mm)
+{
+	return randomize_base(mm->brk);
+}
+
+unsigned long randomize_et_dyn(unsigned long base)
+{
+	return randomize_base(base);
+}
