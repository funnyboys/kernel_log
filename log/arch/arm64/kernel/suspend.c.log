commit 65fddcfca8ad14778f71a57672fd01e8112d30fa
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:42 2020 -0700

    mm: reorder includes after introduction of linux/pgtable.h
    
    The replacement of <asm/pgrable.h> with <linux/pgtable.h> made the include
    of the latter in the middle of asm includes.  Fix this up with the aid of
    the below script and manual adjustments here and there.
    
            import sys
            import re
    
            if len(sys.argv) is not 3:
                print "USAGE: %s <file> <header>" % (sys.argv[0])
                sys.exit(1)
    
            hdr_to_move="#include <linux/%s>" % sys.argv[2]
            moved = False
            in_hdrs = False
    
            with open(sys.argv[1], "r") as f:
                lines = f.readlines()
                for _line in lines:
                    line = _line.rstrip('
    ')
                    if line == hdr_to_move:
                        continue
                    if line.startswith("#include <linux/"):
                        in_hdrs = True
                    elif not moved and in_hdrs:
                        moved = True
                        print hdr_to_move
                    print line
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-4-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 10a1ee8a6eeb..c1dee9066ff9 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -3,13 +3,13 @@
 #include <linux/percpu.h>
 #include <linux/slab.h>
 #include <linux/uaccess.h>
+#include <linux/pgtable.h>
 #include <asm/alternative.h>
 #include <asm/cacheflush.h>
 #include <asm/cpufeature.h>
 #include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/exec.h>
-#include <linux/pgtable.h>
 #include <asm/memory.h>
 #include <asm/mmu_context.h>
 #include <asm/smp_plat.h>

commit ca5999fde0a1761665a38e4c9a72dbcd7d190a81
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:38 2020 -0700

    mm: introduce include/linux/pgtable.h
    
    The include/linux/pgtable.h is going to be the home of generic page table
    manipulation functions.
    
    Start with moving asm-generic/pgtable.h to include/linux/pgtable.h and
    make the latter include asm/pgtable.h.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 9405d1b7f4b0..10a1ee8a6eeb 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -9,7 +9,7 @@
 #include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/exec.h>
-#include <asm/pgtable.h>
+#include <linux/pgtable.h>
 #include <asm/memory.h>
 #include <asm/mmu_context.h>
 #include <asm/smp_plat.h>

commit 5ffdfaedfa0aba3f5db0fbb8ed4f3192be2b39b8
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Tue Jul 31 14:08:56 2018 +0100

    arm64: mm: Support Common Not Private translations
    
    Common Not Private (CNP) is a feature of ARMv8.2 extension which
    allows translation table entries to be shared between different PEs in
    the same inner shareable domain, so the hardware can use this fact to
    optimise the caching of such entries in the TLB.
    
    CNP occupies one bit in TTBRx_ELy and VTTBR_EL2, which advertises to
    the hardware that the translation table entries pointed to by this
    TTBR are the same as every PE in the same inner shareable domain for
    which the equivalent TTBR also has CNP bit set. In case CNP bit is set
    but TTBR does not point at the same translation table entries for a
    given ASID and VMID, then the system is mis-configured, so the results
    of translations are UNPREDICTABLE.
    
    For kernel we postpone setting CNP till all cpus are up and rely on
    cpufeature framework to 1) patch the code which is sensitive to CNP
    and 2) update TTBR1_EL1 with CNP bit set. TTBR1_EL1 can be
    reprogrammed as result of hibernation or cpuidle (via __enable_mmu).
    For these two cases we restore CnP bit via __cpu_suspend_exit().
    
    There are a few cases we need to care of changes in TTBR0_EL1:
      - a switch to idmap
      - software emulated PAN
    
    we rule out latter via Kconfig options and for the former we make
    sure that CNP is set for non-zero ASIDs only.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    [catalin.marinas@arm.com: default y for CONFIG_ARM64_CNP]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 70c283368b64..9405d1b7f4b0 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -48,6 +48,10 @@ void notrace __cpu_suspend_exit(void)
 	 */
 	cpu_uninstall_idmap();
 
+	/* Restore CnP bit in TTBR1_EL1 */
+	if (system_supports_cnp())
+		cpu_replace_ttbr1(lm_alias(swapper_pg_dir));
+
 	/*
 	 * PSTATE was not saved over suspend/resume, re-enable any detected
 	 * features that might not have been set correctly.

commit 647d0519b53f440a55df163de21c52a8205431cc
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:12 2018 +0100

    arm64: ssbd: Restore mitigation status on CPU resume
    
    On a system where firmware can dynamically change the state of the
    mitigation, the CPU will always come up with the mitigation enabled,
    including when coming back from suspend.
    
    If the user has requested "no mitigation" via a command line option,
    let's enforce it by calling into the firmware again to disable it.
    
    Similarily, for a resume from hibernate, the mitigation could have
    been disabled by the boot kernel. Let's ensure that it is set
    back on in that case.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index a307b9e13392..70c283368b64 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -62,6 +62,14 @@ void notrace __cpu_suspend_exit(void)
 	 */
 	if (hw_breakpoint_restore)
 		hw_breakpoint_restore(cpu);
+
+	/*
+	 * On resume, firmware implementing dynamic mitigation will
+	 * have turned the mitigation on. If the user has forcefully
+	 * disabled it, make sure their wishes are obeyed.
+	 */
+	if (arm64_get_ssbd_state() == ARM64_SSBD_FORCE_DISABLE)
+		arm64_set_ssbd_mitigation(false);
 }
 
 /*

commit e1281f56f114f3a945bf9ec30698bd3caa59d322
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:11 2018 +0000

    arm64: uaccess: Add PAN helper
    
    Add __uaccess_{en,dis}able_hw_pan() helpers to set/clear the PSTATE.PAN
    bit.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 3fe5ad884418..a307b9e13392 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -2,6 +2,7 @@
 #include <linux/ftrace.h>
 #include <linux/percpu.h>
 #include <linux/slab.h>
+#include <linux/uaccess.h>
 #include <asm/alternative.h>
 #include <asm/cacheflush.h>
 #include <asm/cpufeature.h>
@@ -51,8 +52,7 @@ void notrace __cpu_suspend_exit(void)
 	 * PSTATE was not saved over suspend/resume, re-enable any detected
 	 * features that might not have been set correctly.
 	 */
-	asm(ALTERNATIVE("nop", SET_PSTATE_PAN(1), ARM64_HAS_PAN,
-			CONFIG_ARM64_PAN));
+	__uaccess_enable_hw_pan();
 	uao_thread_switch(current);
 
 	/*

commit c9b012e5f4a1d01dfa8abc6318211a67ba7d5db2
Merge: b293fca43be5 6cfa7cc46b1a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 10:56:56 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "The big highlight is support for the Scalable Vector Extension (SVE)
      which required extensive ABI work to ensure we don't break existing
      applications by blowing away their signal stack with the rather large
      new vector context (<= 2 kbit per vector register). There's further
      work to be done optimising things like exception return, but the ABI
      is solid now.
    
      Much of the line count comes from some new PMU drivers we have, but
      they're pretty self-contained and I suspect we'll have more of them in
      future.
    
      Plenty of acronym soup here:
    
       - initial support for the Scalable Vector Extension (SVE)
    
       - improved handling for SError interrupts (required to handle RAS
         events)
    
       - enable GCC support for 128-bit integer types
    
       - remove kernel text addresses from backtraces and register dumps
    
       - use of WFE to implement long delay()s
    
       - ACPI IORT updates from Lorenzo Pieralisi
    
       - perf PMU driver for the Statistical Profiling Extension (SPE)
    
       - perf PMU driver for Hisilicon's system PMUs
    
       - misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (97 commits)
      arm64: Make ARMV8_DEPRECATED depend on SYSCTL
      arm64: Implement __lshrti3 library function
      arm64: support __int128 on gcc 5+
      arm64/sve: Add documentation
      arm64/sve: Detect SVE and activate runtime support
      arm64/sve: KVM: Hide SVE from CPU features exposed to guests
      arm64/sve: KVM: Treat guest SVE use as undefined instruction execution
      arm64/sve: KVM: Prevent guests from using SVE
      arm64/sve: Add sysctl to set the default vector length for new processes
      arm64/sve: Add prctl controls for userspace vector length management
      arm64/sve: ptrace and ELF coredump support
      arm64/sve: Preserve SVE registers around EFI runtime service calls
      arm64/sve: Preserve SVE registers around kernel-mode NEON use
      arm64/sve: Probe SVE capabilities and usable vector lengths
      arm64: cpufeature: Move sys_caps_initialised declarations
      arm64/sve: Backend logic for setting the vector length
      arm64/sve: Signal handling support
      arm64/sve: Support vector length resetting for new processes
      arm64/sve: Core task context handling
      arm64/sve: Low-level CPU setup
      ...

commit 0fbeb318754860b37150fd42c2058d636a431426
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:34 2017 +0000

    arm64: explicitly mask all exceptions
    
    There are a few places where we want to mask all exceptions. Today we
    do this in a piecemeal fashion, typically we expect the caller to
    have masked irqs and the arch code masks debug exceptions, ignoring
    serror which is probably masked.
    
    Make it clear that 'mask all exceptions' is the intention by adding
    helpers to do exactly that.
    
    This will let us unmask SError without having to add 'oh and SError'
    to these paths.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 5794326975f8..9079dfe885fa 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -4,6 +4,7 @@
 #include <asm/alternative.h>
 #include <asm/cacheflush.h>
 #include <asm/cpufeature.h>
+#include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/exec.h>
 #include <asm/pgtable.h>
@@ -56,7 +57,7 @@ void notrace __cpu_suspend_exit(void)
 	/*
 	 * Restore HW breakpoint registers to sane values
 	 * before debug exceptions are possibly reenabled
-	 * through local_dbg_restore.
+	 * by cpu_suspend()s local_daif_restore() call.
 	 */
 	if (hw_breakpoint_restore)
 		hw_breakpoint_restore(cpu);
@@ -80,7 +81,7 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	 * updates to mdscr register (saved and restored along with
 	 * general purpose registers) from kernel debuggers.
 	 */
-	local_dbg_save(flags);
+	flags = local_daif_save();
 
 	/*
 	 * Function graph tracer state gets incosistent when the kernel
@@ -113,7 +114,7 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	 * restored, so from this point onwards, debugging is fully
 	 * renabled if it was enabled when core started shutdown.
 	 */
-	local_dbg_restore(flags);
+	local_daif_restore(flags);
 
 	return ret;
 }

commit c10f0d06ad2652913acd21a630f4c2a97cebbfbd
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Wed Nov 1 17:46:19 2017 +0800

    arm64: suspend: remove useless included file
    
    After commit 9e8e865bbe29 ("arm64: unify idmap removal"), we no need to
    flush tlb in suspend.c, so the included file tlbflush.h can be removed.
    
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 1e3be9064cfa..5794326975f8 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -11,7 +11,6 @@
 #include <asm/mmu_context.h>
 #include <asm/smp_plat.h>
 #include <asm/suspend.h>
-#include <asm/tlbflush.h>
 
 /*
  * This is allocated by cpu_suspend_init(), and used to store a pointer to

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 1e3be9064cfa..77cd655e6eb7 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/ftrace.h>
 #include <linux/percpu.h>
 #include <linux/slab.h>

commit 623b476fc815464a0241ea7483da7b3580b7d8ac
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:09 2016 +0000

    arm64: move sp_el0 and tpidr_el1 into cpu_suspend_ctx
    
    When returning from idle, we rely on the fact that thread_info lives at
    the end of the kernel stack, and restore this by masking the saved stack
    pointer. Subsequent patches will sever the relationship between the
    stack and thread_info, and to cater for this we must save/restore sp_el0
    explicitly, storing it in cpu_suspend_ctx.
    
    As cpu_suspend_ctx must be doubleword aligned, this leaves us with an
    extra slot in cpu_suspend_ctx. We can use this to save/restore tpidr_el1
    in the same way, which simplifies the code, avoiding pointer chasing on
    the restore path (as we no longer need to load thread_info::cpu followed
    by the relevant slot in __per_cpu_offset based on this).
    
    This patch stashes both registers in cpu_suspend_ctx.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index bb0cd787a9d3..1e3be9064cfa 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -46,12 +46,6 @@ void notrace __cpu_suspend_exit(void)
 	 */
 	cpu_uninstall_idmap();
 
-	/*
-	 * Restore per-cpu offset before any kernel
-	 * subsystem relying on it has a chance to run.
-	 */
-	set_my_cpu_offset(per_cpu_offset(cpu));
-
 	/*
 	 * PSTATE was not saved over suspend/resume, re-enable any detected
 	 * features that might not have been set correctly.

commit d08544127d9fb4505635e3cb6871fd50a42947bd
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:48 2016 +0100

    arm64: suspend: Reconfigure PSTATE after resume from idle
    
    The suspend/resume path in kernel/sleep.S, as used by cpu-idle, does not
    save/restore PSTATE. As a result of this cpufeatures that were detected
    and have bits in PSTATE get lost when we resume from idle.
    
    UAO gets set appropriately on the next context switch. PAN will be
    re-enabled next time we return from user-space, but on a preemptible
    kernel we may run work accessing user space before this point.
    
    Add code to re-enable theses two features in __cpu_suspend_exit().
    We re-use uao_thread_switch() passing current.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index ad734142070d..bb0cd787a9d3 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -1,8 +1,11 @@
 #include <linux/ftrace.h>
 #include <linux/percpu.h>
 #include <linux/slab.h>
+#include <asm/alternative.h>
 #include <asm/cacheflush.h>
+#include <asm/cpufeature.h>
 #include <asm/debug-monitors.h>
+#include <asm/exec.h>
 #include <asm/pgtable.h>
 #include <asm/memory.h>
 #include <asm/mmu_context.h>
@@ -49,6 +52,14 @@ void notrace __cpu_suspend_exit(void)
 	 */
 	set_my_cpu_offset(per_cpu_offset(cpu));
 
+	/*
+	 * PSTATE was not saved over suspend/resume, re-enable any detected
+	 * features that might not have been set correctly.
+	 */
+	asm(ALTERNATIVE("nop", SET_PSTATE_PAN(1), ARM64_HAS_PAN,
+			CONFIG_ARM64_PAN));
+	uao_thread_switch(current);
+
 	/*
 	 * Restore HW breakpoint registers to sane values
 	 * before debug exceptions are possibly reenabled

commit d7a83d127a64fd91ef1ad39b7e2d78db36cf388b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Aug 15 18:55:11 2016 +0100

    arm64: hw_breakpoint: convert CPU hotplug notifier to new infrastructure
    
    The arm64 hw_breakpoint implementation uses a CPU hotplug notifier to
    reset the {break,watch}point registers when CPUs come online.
    
    This patch converts the code to the new hotplug mechanism, whilst moving
    the invocation earlier to remove the need to disable IRQs explicitly in
    the driver (which could cause havok if we trip a watchpoint in an IRQ
    handler whilst restoring the debug register state).
    
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index b616e365cee3..ad734142070d 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -23,8 +23,8 @@ unsigned long *sleep_save_stash;
  * time the notifier runs debug exceptions might have been enabled already,
  * with HW breakpoints registers content still in an unknown state.
  */
-static void (*hw_breakpoint_restore)(void *);
-void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
+static int (*hw_breakpoint_restore)(unsigned int);
+void __init cpu_suspend_set_dbg_restorer(int (*hw_bp_restore)(unsigned int))
 {
 	/* Prevent multiple restore hook initializations */
 	if (WARN_ON(hw_breakpoint_restore))
@@ -34,6 +34,8 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
 
 void notrace __cpu_suspend_exit(void)
 {
+	unsigned int cpu = smp_processor_id();
+
 	/*
 	 * We are resuming from reset with the idmap active in TTBR0_EL1.
 	 * We must uninstall the idmap and restore the expected MMU
@@ -45,7 +47,7 @@ void notrace __cpu_suspend_exit(void)
 	 * Restore per-cpu offset before any kernel
 	 * subsystem relying on it has a chance to run.
 	 */
-	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
+	set_my_cpu_offset(per_cpu_offset(cpu));
 
 	/*
 	 * Restore HW breakpoint registers to sane values
@@ -53,7 +55,7 @@ void notrace __cpu_suspend_exit(void)
 	 * through local_dbg_restore.
 	 */
 	if (hw_breakpoint_restore)
-		hw_breakpoint_restore(NULL);
+		hw_breakpoint_restore(cpu);
 }
 
 /*

commit cabe1c81ea5be983425d117912d7883e252a3b09
Author: James Morse <james.morse@arm.com>
Date:   Wed Apr 27 17:47:07 2016 +0100

    arm64: Change cpu_resume() to enable mmu early then access sleep_sp by va
    
    By enabling the MMU early in cpu_resume(), the sleep_save_sp and stack can
    be accessed by VA, which avoids the need to convert-addresses and clean to
    PoC on the suspend path.
    
    MMU setup is shared with the boot path, meaning the swapper_pg_dir is
    restored directly: ttbr1_el1 is no longer saved/restored.
    
    struct sleep_save_sp is removed, replacing it with a single array of
    pointers.
    
    cpu_do_{suspend,resume} could be further reduced to not restore: cpacr_el1,
    mdscr_el1, tcr_el1, vbar_el1 and sctlr_el1, all of which are set by
    __cpu_setup(). However these values all contain res0 bits that may be used
    to enable future features.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 0d52ec972a38..b616e365cee3 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -10,30 +10,11 @@
 #include <asm/suspend.h>
 #include <asm/tlbflush.h>
 
-
 /*
- * This is called by __cpu_suspend_enter() to save the state, and do whatever
- * flushing is required to ensure that when the CPU goes to sleep we have
- * the necessary data available when the caches are not searched.
- *
- * ptr: sleep_stack_data containing cpu state virtual address.
- * save_ptr: address of the location where the context physical address
- *           must be saved
+ * This is allocated by cpu_suspend_init(), and used to store a pointer to
+ * the 'struct sleep_stack_data' the contains a particular CPUs state.
  */
-void notrace __cpu_suspend_save(struct sleep_stack_data *ptr,
-				phys_addr_t *save_ptr)
-{
-	*save_ptr = virt_to_phys(ptr);
-
-	cpu_do_suspend(&ptr->system_regs);
-	/*
-	 * Only flush the context that must be retrieved with the MMU
-	 * off. VA primitives ensure the flush is applied to all
-	 * cache levels so context is pushed to DRAM.
-	 */
-	__flush_dcache_area(ptr, sizeof(*ptr));
-	__flush_dcache_area(save_ptr, sizeof(*save_ptr));
-}
+unsigned long *sleep_save_stash;
 
 /*
  * This hook is provided so that cpu_suspend code can restore HW
@@ -131,22 +112,15 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	return ret;
 }
 
-struct sleep_save_sp sleep_save_sp;
-
 static int __init cpu_suspend_init(void)
 {
-	void *ctx_ptr;
-
 	/* ctx_ptr is an array of physical addresses */
-	ctx_ptr = kcalloc(mpidr_hash_size(), sizeof(phys_addr_t), GFP_KERNEL);
+	sleep_save_stash = kcalloc(mpidr_hash_size(), sizeof(*sleep_save_stash),
+				   GFP_KERNEL);
 
-	if (WARN_ON(!ctx_ptr))
+	if (WARN_ON(!sleep_save_stash))
 		return -ENOMEM;
 
-	sleep_save_sp.save_ptr_stash = ctx_ptr;
-	sleep_save_sp.save_ptr_stash_phys = virt_to_phys(ctx_ptr);
-	__flush_dcache_area(&sleep_save_sp, sizeof(struct sleep_save_sp));
-
 	return 0;
 }
 early_initcall(cpu_suspend_init);

commit adc9b2dfd00924e9e9b98613f36a6cb8c51f0dc6
Author: James Morse <james.morse@arm.com>
Date:   Wed Apr 27 17:47:06 2016 +0100

    arm64: kernel: Rework finisher callback out of __cpu_suspend_enter()
    
    Hibernate could make use of the cpu_suspend() code to save/restore cpu
    state, however it needs to be able to return '0' from the 'finisher'.
    
    Rework cpu_suspend() so that the finisher is called from C code,
    independently from the save/restore of cpu state. Space to save the context
    in is allocated in the caller's stack frame, and passed into
    __cpu_suspend_enter().
    
    Hibernate's use of this API will look like a copy of the cpu_suspend()
    function.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 66055392f445..0d52ec972a38 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -10,22 +10,22 @@
 #include <asm/suspend.h>
 #include <asm/tlbflush.h>
 
-extern int __cpu_suspend_enter(unsigned long arg, int (*fn)(unsigned long));
+
 /*
  * This is called by __cpu_suspend_enter() to save the state, and do whatever
  * flushing is required to ensure that when the CPU goes to sleep we have
  * the necessary data available when the caches are not searched.
  *
- * ptr: CPU context virtual address
+ * ptr: sleep_stack_data containing cpu state virtual address.
  * save_ptr: address of the location where the context physical address
  *           must be saved
  */
-void notrace __cpu_suspend_save(struct cpu_suspend_ctx *ptr,
+void notrace __cpu_suspend_save(struct sleep_stack_data *ptr,
 				phys_addr_t *save_ptr)
 {
 	*save_ptr = virt_to_phys(ptr);
 
-	cpu_do_suspend(ptr);
+	cpu_do_suspend(&ptr->system_regs);
 	/*
 	 * Only flush the context that must be retrieved with the MMU
 	 * off. VA primitives ensure the flush is applied to all
@@ -51,6 +51,30 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
 	hw_breakpoint_restore = hw_bp_restore;
 }
 
+void notrace __cpu_suspend_exit(void)
+{
+	/*
+	 * We are resuming from reset with the idmap active in TTBR0_EL1.
+	 * We must uninstall the idmap and restore the expected MMU
+	 * state before we can possibly return to userspace.
+	 */
+	cpu_uninstall_idmap();
+
+	/*
+	 * Restore per-cpu offset before any kernel
+	 * subsystem relying on it has a chance to run.
+	 */
+	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
+
+	/*
+	 * Restore HW breakpoint registers to sane values
+	 * before debug exceptions are possibly reenabled
+	 * through local_dbg_restore.
+	 */
+	if (hw_breakpoint_restore)
+		hw_breakpoint_restore(NULL);
+}
+
 /*
  * cpu_suspend
  *
@@ -60,8 +84,9 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
  */
 int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 {
-	int ret;
+	int ret = 0;
 	unsigned long flags;
+	struct sleep_stack_data state;
 
 	/*
 	 * From this point debug exceptions are disabled to prevent
@@ -77,34 +102,21 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	 */
 	pause_graph_tracing();
 
-	/*
-	 * mm context saved on the stack, it will be restored when
-	 * the cpu comes out of reset through the identity mapped
-	 * page tables, so that the thread address space is properly
-	 * set-up on function return.
-	 */
-	ret = __cpu_suspend_enter(arg, fn);
-	if (ret == 0) {
-		/*
-		 * We are resuming from reset with the idmap active in TTBR0_EL1.
-		 * We must uninstall the idmap and restore the expected MMU
-		 * state before we can possibly return to userspace.
-		 */
-		cpu_uninstall_idmap();
-
-		/*
-		 * Restore per-cpu offset before any kernel
-		 * subsystem relying on it has a chance to run.
-		 */
-		set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
+	if (__cpu_suspend_enter(&state)) {
+		/* Call the suspend finisher */
+		ret = fn(arg);
 
 		/*
-		 * Restore HW breakpoint registers to sane values
-		 * before debug exceptions are possibly reenabled
-		 * through local_dbg_restore.
+		 * Never gets here, unless the suspend finisher fails.
+		 * Successful cpu_suspend() should return from cpu_resume(),
+		 * returning through this code path is considered an error
+		 * If the return value is set to 0 force ret = -EOPNOTSUPP
+		 * to make sure a proper error condition is propagated
 		 */
-		if (hw_breakpoint_restore)
-			hw_breakpoint_restore(NULL);
+		if (!ret)
+			ret = -EOPNOTSUPP;
+	} else {
+		__cpu_suspend_exit();
 	}
 
 	unpause_graph_tracing();

commit 9e8e865bbe294a69666a1996bda3e87825b258c0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 25 11:44:58 2016 +0000

    arm64: unify idmap removal
    
    We currently open-code the removal of the idmap and restoration of the
    current task's MMU state in a few places.
    
    Before introducing yet more copies of this sequence, unify these to call
    a new helper, cpu_uninstall_idmap.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 1095aa483a1c..66055392f445 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -60,7 +60,6 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
  */
 int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 {
-	struct mm_struct *mm = current->active_mm;
 	int ret;
 	unsigned long flags;
 
@@ -87,22 +86,11 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	ret = __cpu_suspend_enter(arg, fn);
 	if (ret == 0) {
 		/*
-		 * We are resuming from reset with TTBR0_EL1 set to the
-		 * idmap to enable the MMU; set the TTBR0 to the reserved
-		 * page tables to prevent speculative TLB allocations, flush
-		 * the local tlb and set the default tcr_el1.t0sz so that
-		 * the TTBR0 address space set-up is properly restored.
-		 * If the current active_mm != &init_mm we entered cpu_suspend
-		 * with mappings in TTBR0 that must be restored, so we switch
-		 * them back to complete the address space configuration
-		 * restoration before returning.
+		 * We are resuming from reset with the idmap active in TTBR0_EL1.
+		 * We must uninstall the idmap and restore the expected MMU
+		 * state before we can possibly return to userspace.
 		 */
-		cpu_set_reserved_ttbr0();
-		local_flush_tlb_all();
-		cpu_set_default_tcr_t0sz();
-
-		if (mm != &init_mm)
-			cpu_switch_mm(mm->pgd, mm);
+		cpu_uninstall_idmap();
 
 		/*
 		 * Restore per-cpu offset before any kernel

commit de818bd4522c40ea02a81b387d2fa86f989c9623
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Tue Nov 17 11:50:51 2015 +0000

    arm64: kernel: pause/unpause function graph tracer in cpu_suspend()
    
    The function graph tracer adds instrumentation that is required to trace
    both entry and exit of a function. In particular the function graph
    tracer updates the "return address" of a function in order to insert
    a trace callback on function exit.
    
    Kernel power management functions like cpu_suspend() are called
    upon power down entry with functions called "finishers" that are in turn
    called to trigger the power down sequence but they may not return to the
    kernel through the normal return path.
    
    When the core resumes from low-power it returns to the cpu_suspend()
    function through the cpu_resume path, which leaves the trace stack frame
    set-up by the function tracer in an incosistent state upon return to the
    kernel when tracing is enabled.
    
    This patch fixes the issue by pausing/resuming the function graph
    tracer on the thread executing cpu_suspend() (ie the function call that
    subsequently triggers the "suspend finishers"), so that the function graph
    tracer state is kept consistent across functions that enter power down
    states and never return by effectively disabling graph tracer while they
    are executing.
    
    Fixes: 819e50e25d0c ("arm64: Add ftrace support")
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reported-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org> # 3.16+
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index fce95e17cf7f..1095aa483a1c 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -1,3 +1,4 @@
+#include <linux/ftrace.h>
 #include <linux/percpu.h>
 #include <linux/slab.h>
 #include <asm/cacheflush.h>
@@ -70,6 +71,13 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	 */
 	local_dbg_save(flags);
 
+	/*
+	 * Function graph tracer state gets incosistent when the kernel
+	 * calls functions that never return (aka suspend finishers) hence
+	 * disable graph tracing during their execution.
+	 */
+	pause_graph_tracing();
+
 	/*
 	 * mm context saved on the stack, it will be restored when
 	 * the cpu comes out of reset through the identity mapped
@@ -111,6 +119,8 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 			hw_breakpoint_restore(NULL);
 	}
 
+	unpause_graph_tracing();
+
 	/*
 	 * Restore pstate flags. OS lock and mdscr have been already
 	 * restored, so from this point onwards, debugging is fully

commit a18e2fa5e670a1b84e66522b221c42875b02028a
Merge: 7dac7102afbe 01b305a23494
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 12 15:33:11 2015 -0800

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes and clean-ups from Catalin Marinas:
     "Here's a second pull request for this merging window with some
      fixes/clean-ups:
    
       - __cmpxchg_double*() return type fix to avoid truncation of a long
         to int and subsequent logical "not" in cmpxchg_double()
         misinterpreting the operation success/failure
    
       - BPF fixes for mod and div by zero
    
       - Fix compilation with STRICT_MM_TYPECHECKS enabled
    
       - VDSO build fix without libgcov
    
       - Some static and __maybe_unused annotations
    
       - Kconfig clean-up (FRAME_POINTER)
    
       - defconfig update for CRYPTO_CRC32_ARM64"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: suspend: make hw_breakpoint_restore static
      arm64: mmu: make split_pud and fixup_executable static
      arm64: smp: make of_parse_and_init_cpus static
      arm64: use linux/types.h in kvm.h
      arm64: build vdso without libgcov
      arm64: mark cpus_have_hwcap as __maybe_unused
      arm64: remove redundant FRAME_POINTER kconfig option and force to select it
      arm64: fix R/O permissions of FDT mapping
      arm64: fix STRICT_MM_TYPECHECKS issue in PTE_CONT manipulation
      arm64: bpf: fix mod-by-zero case
      arm64: bpf: fix div-by-zero case
      arm64: Enable CRYPTO_CRC32_ARM64 in defconfig
      arm64: cmpxchg_dbl: fix return value type

commit 01b305a234943c25c336a6f2f77932a4eaf125fa
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Thu Nov 12 20:04:44 2015 +0800

    arm64: suspend: make hw_breakpoint_restore static
    
    hw_breakpoint_restore is only used within suspend.c, so it can be
    declared static.
    
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 3c5e4e6dcf68..d87403e8ba1d 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -41,7 +41,7 @@ void notrace __cpu_suspend_save(struct cpu_suspend_ctx *ptr,
  * time the notifier runs debug exceptions might have been enabled already,
  * with HW breakpoints registers content still in an unknown state.
  */
-void (*hw_breakpoint_restore)(void *);
+static void (*hw_breakpoint_restore)(void *);
 void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
 {
 	/* Prevent multiple restore hook initializations */

commit 2dc10ad81fc017837037e60439662e1b16bdffb9
Merge: e627078a0cbd f8f8bdc48851
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 14:47:13 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - "genirq: Introduce generic irq migration for cpu hotunplugged" patch
       merged from tip/irq/for-arm to allow the arm64-specific part to be
       upstreamed via the arm64 tree
    
     - CPU feature detection reworked to cope with heterogeneous systems
       where CPUs may not have exactly the same features.  The features
       reported by the kernel via internal data structures or ELF_HWCAP are
       delayed until all the CPUs are up (and before user space starts)
    
     - Support for 16KB pages, with the additional bonus of a 36-bit VA
       space, though the latter only depending on EXPERT
    
     - Implement native {relaxed, acquire, release} atomics for arm64
    
     - New ASID allocation algorithm which avoids IPI on roll-over, together
       with TLB invalidation optimisations (using local vs global where
       feasible)
    
     - KASan support for arm64
    
     - EFI_STUB clean-up and isolation for the kernel proper (required by
       KASan)
    
     - copy_{to,from,in}_user optimisations (sharing the memcpy template)
    
     - perf: moving arm64 to the arm32/64 shared PMU framework
    
     - L1_CACHE_BYTES increased to 128 to accommodate Cavium hardware
    
     - Support for the contiguous PTE hint on kernel mapping (16 consecutive
       entries may be able to use a single TLB entry)
    
     - Generic CONFIG_HZ now used on arm64
    
     - defconfig updates
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (91 commits)
      arm64/efi: fix libstub build under CONFIG_MODVERSIONS
      ARM64: Enable multi-core scheduler support by default
      arm64/efi: move arm64 specific stub C code to libstub
      arm64: page-align sections for DEBUG_RODATA
      arm64: Fix build with CONFIG_ZONE_DMA=n
      arm64: Fix compat register mappings
      arm64: Increase the max granular size
      arm64: remove bogus TASK_SIZE_64 check
      arm64: make Timer Interrupt Frequency selectable
      arm64/mm: use PAGE_ALIGNED instead of IS_ALIGNED
      arm64: cachetype: fix definitions of ICACHEF_* flags
      arm64: cpufeature: declare enable_cpu_capabilities as static
      genirq: Make the cpuhotplug migration code less noisy
      arm64: Constify hwcap name string arrays
      arm64/kvm: Make use of the system wide safe values
      arm64/debug: Make use of the system wide safe value
      arm64: Move FP/ASIMD hwcap handling to common code
      arm64/HWCAP: Use system wide safe values
      arm64/capabilities: Make use of system wide safe value
      arm64: Delay cpu feature capability checks
      ...

commit e13d918a19a7b6cba62b32884f5e336e764c2cc6
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Tue Oct 27 17:29:10 2015 +0000

    arm64: kernel: fix tcr_el1.t0sz restore on systems with extended idmap
    
    Commit dd006da21646 ("arm64: mm: increase VA range of identity map")
    introduced a mechanism to extend the virtual memory map range
    to support arm64 systems with system RAM located at very high offset,
    where the identity mapping used to enable/disable the MMU requires
    additional translation levels to map the physical memory at an equal
    virtual offset.
    
    The kernel detects at boot time the tcr_el1.t0sz value required by the
    identity mapping and sets-up the tcr_el1.t0sz register field accordingly,
    any time the identity map is required in the kernel (ie when enabling the
    MMU).
    
    After enabling the MMU, in the cold boot path the kernel resets the
    tcr_el1.t0sz to its default value (ie the actual configuration value for
    the system virtual address space) so that after enabling the MMU the
    memory space translated by ttbr0_el1 is restored as expected.
    
    Commit dd006da21646 ("arm64: mm: increase VA range of identity map")
    also added code to set-up the tcr_el1.t0sz value when the kernel resumes
    from low-power states with the MMU off through cpu_resume() in order to
    effectively use the identity mapping to enable the MMU but failed to add
    the code required to restore the tcr_el1.t0sz to its default value, when
    the core returns to the kernel with the MMU enabled, so that the kernel
    might end up running with tcr_el1.t0sz value set-up for the identity
    mapping which can be lower than the value required by the actual virtual
    address space, resulting in an erroneous set-up.
    
    This patchs adds code in the resume path that restores the tcr_el1.t0sz
    default value upon core resume, mirroring this way the cold boot path
    behaviour therefore fixing the issue.
    
    Cc: <stable@vger.kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Fixes: dd006da21646 ("arm64: mm: increase VA range of identity map")
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 8297d502217e..44ca4143b013 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -80,17 +80,21 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	if (ret == 0) {
 		/*
 		 * We are resuming from reset with TTBR0_EL1 set to the
-		 * idmap to enable the MMU; restore the active_mm mappings in
-		 * TTBR0_EL1 unless the active_mm == &init_mm, in which case
-		 * the thread entered cpu_suspend with TTBR0_EL1 set to
-		 * reserved TTBR0 page tables and should be restored as such.
+		 * idmap to enable the MMU; set the TTBR0 to the reserved
+		 * page tables to prevent speculative TLB allocations, flush
+		 * the local tlb and set the default tcr_el1.t0sz so that
+		 * the TTBR0 address space set-up is properly restored.
+		 * If the current active_mm != &init_mm we entered cpu_suspend
+		 * with mappings in TTBR0 that must be restored, so we switch
+		 * them back to complete the address space configuration
+		 * restoration before returning.
 		 */
-		if (mm == &init_mm)
-			cpu_set_reserved_ttbr0();
-		else
-			cpu_switch_mm(mm->pgd, mm);
-
+		cpu_set_reserved_ttbr0();
 		flush_tlb_all();
+		cpu_set_default_tcr_t0sz();
+
+		if (mm != &init_mm)
+			cpu_switch_mm(mm->pgd, mm);
 
 		/*
 		 * Restore per-cpu offset before any kernel

commit 8e63d38876691756f9bc6930850f1fb77809be1b
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:23 2015 +0100

    arm64: flush: use local TLB and I-cache invalidation
    
    There are a number of places where a single CPU is running with a
    private page-table and we need to perform maintenance on the TLB and
    I-cache in order to ensure correctness, but do not require the operation
    to be broadcast to other CPUs.
    
    This patch adds local variants of tlb_flush_all and __flush_icache_all
    to support these use-cases and updates the callers respectively.
    __local_flush_icache_all also implies an isb, since it is intended to be
    used synchronously.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 8297d502217e..3c5e4e6dcf68 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -90,7 +90,7 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 		else
 			cpu_switch_mm(mm->pgd, mm);
 
-		flush_tlb_all();
+		local_flush_tlb_all();
 
 		/*
 		 * Restore per-cpu offset before any kernel

commit af391b15f7b56ce19f52862d36595637dd42b575
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Thu Jun 18 15:41:32 2015 +0100

    arm64: kernel: rename __cpu_suspend to keep it aligned with arm
    
    This patch renames __cpu_suspend to cpu_suspend so that it's aligned
    with ARM32. It also removes the redundant wrapper created.
    
    This is in preparation to implement generic PSCI system suspend using
    the cpu_{suspend,resume} which now has the same interface on both ARM
    and ARM64.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Ashwin Chaugule <ashwin.chaugule@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index f6073c27d65f..8297d502217e 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -51,13 +51,13 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
 }
 
 /*
- * __cpu_suspend
+ * cpu_suspend
  *
  * arg: argument to pass to the finisher function
  * fn: finisher function pointer
  *
  */
-int __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 {
 	struct mm_struct *mm = current->active_mm;
 	int ret;
@@ -82,7 +82,7 @@ int __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 		 * We are resuming from reset with TTBR0_EL1 set to the
 		 * idmap to enable the MMU; restore the active_mm mappings in
 		 * TTBR0_EL1 unless the active_mm == &init_mm, in which case
-		 * the thread entered __cpu_suspend with TTBR0_EL1 set to
+		 * the thread entered cpu_suspend with TTBR0_EL1 set to
 		 * reserved TTBR0 page tables and should be restored as such.
 		 */
 		if (mm == &init_mm)

commit 9acdc2af0c0b836183b7f31f630bbed341a7cf4d
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Jun 1 13:40:34 2015 +0200

    arm64: drop sleep_idmap_phys and clean up cpu_resume()
    
    Two cleanups of the asm function cpu_resume():
    - The global variable sleep_idmap_phys always points to idmap_pg_dir,
      so we can just use that value directly in the CPU resume path.
    - Unclutter the load of sleep_save_sp::save_ptr_stash_phys.
    
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index d7daf45ae7a2..f6073c27d65f 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -118,7 +118,6 @@ int __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 }
 
 struct sleep_save_sp sleep_save_sp;
-phys_addr_t sleep_idmap_phys;
 
 static int __init cpu_suspend_init(void)
 {
@@ -132,9 +131,7 @@ static int __init cpu_suspend_init(void)
 
 	sleep_save_sp.save_ptr_stash = ctx_ptr;
 	sleep_save_sp.save_ptr_stash_phys = virt_to_phys(ctx_ptr);
-	sleep_idmap_phys = virt_to_phys(idmap_pg_dir);
 	__flush_dcache_area(&sleep_save_sp, sizeof(struct sleep_save_sp));
-	__flush_dcache_area(&sleep_idmap_phys, sizeof(sleep_idmap_phys));
 
 	return 0;
 }

commit af3cfdbf56b91785650f54e7c9a899d814b4b9fb
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Mon Jan 26 18:33:44 2015 +0000

    arm64: kernel: remove ARM64_CPU_SUSPEND config option
    
    ARM64_CPU_SUSPEND config option was introduced to make code providing
    context save/restore selectable only on platforms requiring power
    management capabilities.
    
    Currently ARM64_CPU_SUSPEND depends on the PM_SLEEP config option which
    in turn is set by the SUSPEND config option.
    
    The introduction of CPU_IDLE for arm64 requires that code configured
    by ARM64_CPU_SUSPEND (context save/restore) should be compiled in
    in order to enable the CPU idle driver to rely on CPU operations
    carrying out context save/restore.
    
    The ARM64_CPUIDLE config option (ARM64 generic idle driver) is therefore
    forced to select ARM64_CPU_SUSPEND, even if there may be (ie PM_SLEEP)
    failed dependencies, which is not a clean way of handling the kernel
    configuration option.
    
    For these reasons, this patch removes the ARM64_CPU_SUSPEND config option
    and makes the context save/restore dependent on CPU_PM, which is selected
    whenever either SUSPEND or CPU_IDLE are configured, cleaning up dependencies
    in the process.
    
    This way, code previously configured through ARM64_CPU_SUSPEND is
    compiled in whenever a power management subsystem requires it to be
    present in the kernel (SUSPEND || CPU_IDLE), which is the behaviour
    expected on ARM64 kernels.
    
    The cpu_suspend and cpu_init_idle CPU operations are added only if
    CPU_IDLE is selected, since they are CPU_IDLE specific methods and
    should be grouped and defined accordingly.
    
    PSCI CPU operations are updated to reflect the introduced changes.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 2d6b6065fe7f..d7daf45ae7a2 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -1,7 +1,6 @@
 #include <linux/percpu.h>
 #include <linux/slab.h>
 #include <asm/cacheflush.h>
-#include <asm/cpu_ops.h>
 #include <asm/debug-monitors.h>
 #include <asm/pgtable.h>
 #include <asm/memory.h>
@@ -51,26 +50,6 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
 	hw_breakpoint_restore = hw_bp_restore;
 }
 
-/**
- * cpu_suspend() - function to enter a low-power state
- * @arg: argument to pass to CPU suspend operations
- *
- * Return: 0 on success, -EOPNOTSUPP if CPU suspend hook not initialized, CPU
- * operations back-end error code otherwise.
- */
-int cpu_suspend(unsigned long arg)
-{
-	int cpu = smp_processor_id();
-
-	/*
-	 * If cpu_ops have not been registered or suspend
-	 * has not been initialized, cpu_suspend call fails early.
-	 */
-	if (!cpu_ops[cpu] || !cpu_ops[cpu]->cpu_suspend)
-		return -EOPNOTSUPP;
-	return cpu_ops[cpu]->cpu_suspend(arg);
-}
-
 /*
  * __cpu_suspend
  *

commit f43c27188a49111b58e9611afa2f0365b0b55625
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Fri Dec 19 17:03:47 2014 +0000

    arm64: kernel: fix __cpu_suspend mm switch on warm-boot
    
    On arm64 the TTBR0_EL1 register is set to either the reserved TTBR0
    page tables on boot or to the active_mm mappings belonging to user space
    processes, it must never be set to swapper_pg_dir page tables mappings.
    
    When a CPU is booted its active_mm is set to init_mm even though its
    TTBR0_EL1 points at the reserved TTBR0 page mappings. This implies
    that when __cpu_suspend is triggered the active_mm can point at
    init_mm even if the current TTBR0_EL1 register contains the reserved
    TTBR0_EL1 mappings.
    
    Therefore, the mm save and restore executed in __cpu_suspend might
    turn out to be erroneous in that, if the current->active_mm corresponds
    to init_mm, on resume from low power it ends up restoring in the
    TTBR0_EL1 the init_mm mappings that are global and can cause speculation
    of TLB entries which end up being propagated to user space.
    
    This patch fixes the issue by checking the active_mm pointer before
    restoring the TTBR0 mappings. If the current active_mm == &init_mm,
    the code sets the TTBR0_EL1 to the reserved TTBR0 mapping instead of
    switching back to the active_mm, which is the expected behaviour
    corresponding to the TTBR0_EL1 settings when __cpu_suspend was entered.
    
    Fixes: 95322526ef62 ("arm64: kernel: cpu_{suspend/resume} implementation")
    Cc: <stable@vger.kernel.org> # 3.14+: 18ab7db
    Cc: <stable@vger.kernel.org> # 3.14+: 714f599
    Cc: <stable@vger.kernel.org> # 3.14+: c3684fb
    Cc: <stable@vger.kernel.org> # 3.14+
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 3771b72b6569..2d6b6065fe7f 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -5,6 +5,7 @@
 #include <asm/debug-monitors.h>
 #include <asm/pgtable.h>
 #include <asm/memory.h>
+#include <asm/mmu_context.h>
 #include <asm/smp_plat.h>
 #include <asm/suspend.h>
 #include <asm/tlbflush.h>
@@ -98,7 +99,18 @@ int __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	 */
 	ret = __cpu_suspend_enter(arg, fn);
 	if (ret == 0) {
-		cpu_switch_mm(mm->pgd, mm);
+		/*
+		 * We are resuming from reset with TTBR0_EL1 set to the
+		 * idmap to enable the MMU; restore the active_mm mappings in
+		 * TTBR0_EL1 unless the active_mm == &init_mm, in which case
+		 * the thread entered __cpu_suspend with TTBR0_EL1 set to
+		 * reserved TTBR0 page tables and should be restored as such.
+		 */
+		if (mm == &init_mm)
+			cpu_set_reserved_ttbr0();
+		else
+			cpu_switch_mm(mm->pgd, mm);
+
 		flush_tlb_all();
 
 		/*

commit c3684fbb446501b48dec6677a6a9f61c215053de
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Nov 21 21:50:40 2014 +0000

    arm64: Move cpu_resume into the text section
    
    The function cpu_resume currently lives in the .data section.
    There's no reason for it to be there since we can use relative
    instructions without a problem. Move a few cpu_resume data
    structures out of the assembly file so the .data annotation
    can be dropped completely and cpu_resume ends up in the read
    only text section.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 13ad4dbb1615..3771b72b6569 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -126,8 +126,8 @@ int __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	return ret;
 }
 
-extern struct sleep_save_sp sleep_save_sp;
-extern phys_addr_t sleep_idmap_phys;
+struct sleep_save_sp sleep_save_sp;
+phys_addr_t sleep_idmap_phys;
 
 static int __init cpu_suspend_init(void)
 {

commit 714f59925595b9c2ea9c22b107b340d38e3b3bc9
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Aug 7 14:54:50 2014 +0100

    arm64: kernel: refactor the CPU suspend API for retention states
    
    CPU suspend is the standard kernel interface to be used to enter
    low-power states on ARM64 systems. Current cpu_suspend implementation
    by default assumes that all low power states are losing the CPU context,
    so the CPU registers must be saved and cleaned to DRAM upon state
    entry. Furthermore, the current cpu_suspend() implementation assumes
    that if the CPU suspend back-end method returns when called, this has
    to be considered an error regardless of the return code (which can be
    successful) since the CPU was not expected to return from a code path that
    is different from cpu_resume code path - eg returning from the reset vector.
    
    All in all this means that the current API does not cope well with low-power
    states that preserve the CPU context when entered (ie retention states),
    since first of all the context is saved for nothing on state entry for
    those states and a successful state entry can return as a normal function
    return, which is considered an error by the current CPU suspend
    implementation.
    
    This patch refactors the cpu_suspend() API so that it can be split in
    two separate functionalities. The arm64 cpu_suspend API just provides
    a wrapper around CPU suspend operation hook. A new function is
    introduced (for architecture code use only) for states that require
    context saving upon entry:
    
    __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
    
    __cpu_suspend() saves the context on function entry and calls the
    so called suspend finisher (ie fn) to complete the suspend operation.
    The finisher is not expected to return, unless it fails in which case
    the error is propagated back to the __cpu_suspend caller.
    
    The API refactoring results in the following pseudo code call sequence for a
    suspending CPU, when triggered from a kernel subsystem:
    
    /*
     * int cpu_suspend(unsigned long idx)
     * @idx: idle state index
     */
    {
    -> cpu_suspend(idx)
            |---> CPU operations suspend hook called, if present
                    |--> if (retention_state)
                            |--> direct suspend back-end call (eg PSCI suspend)
                         else
                            |--> __cpu_suspend(idx, &back_end_finisher);
    }
    
    By refactoring the cpu_suspend API this way, the CPU operations back-end
    has a chance to detect whether idle states require state saving or not
    and can call the required suspend operations accordingly either through
    simple function call or indirectly through __cpu_suspend() which carries out
    state saving and suspend finisher dispatching to complete idle state entry.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 55a99b9a97e0..13ad4dbb1615 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -9,22 +9,19 @@
 #include <asm/suspend.h>
 #include <asm/tlbflush.h>
 
-extern int __cpu_suspend(unsigned long);
+extern int __cpu_suspend_enter(unsigned long arg, int (*fn)(unsigned long));
 /*
- * This is called by __cpu_suspend() to save the state, and do whatever
+ * This is called by __cpu_suspend_enter() to save the state, and do whatever
  * flushing is required to ensure that when the CPU goes to sleep we have
  * the necessary data available when the caches are not searched.
  *
- * @arg: Argument to pass to suspend operations
- * @ptr: CPU context virtual address
- * @save_ptr: address of the location where the context physical address
- *            must be saved
+ * ptr: CPU context virtual address
+ * save_ptr: address of the location where the context physical address
+ *           must be saved
  */
-int __cpu_suspend_finisher(unsigned long arg, struct cpu_suspend_ctx *ptr,
-			   phys_addr_t *save_ptr)
+void notrace __cpu_suspend_save(struct cpu_suspend_ctx *ptr,
+				phys_addr_t *save_ptr)
 {
-	int cpu = smp_processor_id();
-
 	*save_ptr = virt_to_phys(ptr);
 
 	cpu_do_suspend(ptr);
@@ -35,8 +32,6 @@ int __cpu_suspend_finisher(unsigned long arg, struct cpu_suspend_ctx *ptr,
 	 */
 	__flush_dcache_area(ptr, sizeof(*ptr));
 	__flush_dcache_area(save_ptr, sizeof(*save_ptr));
-
-	return cpu_ops[cpu]->cpu_suspend(arg);
 }
 
 /*
@@ -56,15 +51,15 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
 }
 
 /**
- * cpu_suspend
+ * cpu_suspend() - function to enter a low-power state
+ * @arg: argument to pass to CPU suspend operations
  *
- * @arg: argument to pass to the finisher function
+ * Return: 0 on success, -EOPNOTSUPP if CPU suspend hook not initialized, CPU
+ * operations back-end error code otherwise.
  */
 int cpu_suspend(unsigned long arg)
 {
-	struct mm_struct *mm = current->active_mm;
-	int ret, cpu = smp_processor_id();
-	unsigned long flags;
+	int cpu = smp_processor_id();
 
 	/*
 	 * If cpu_ops have not been registered or suspend
@@ -72,6 +67,21 @@ int cpu_suspend(unsigned long arg)
 	 */
 	if (!cpu_ops[cpu] || !cpu_ops[cpu]->cpu_suspend)
 		return -EOPNOTSUPP;
+	return cpu_ops[cpu]->cpu_suspend(arg);
+}
+
+/*
+ * __cpu_suspend
+ *
+ * arg: argument to pass to the finisher function
+ * fn: finisher function pointer
+ *
+ */
+int __cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+{
+	struct mm_struct *mm = current->active_mm;
+	int ret;
+	unsigned long flags;
 
 	/*
 	 * From this point debug exceptions are disabled to prevent
@@ -86,7 +96,7 @@ int cpu_suspend(unsigned long arg)
 	 * page tables, so that the thread address space is properly
 	 * set-up on function return.
 	 */
-	ret = __cpu_suspend(arg);
+	ret = __cpu_suspend_enter(arg, fn);
 	if (ret == 0) {
 		cpu_switch_mm(mm->pgd, mm);
 		flush_tlb_all();
@@ -95,7 +105,7 @@ int cpu_suspend(unsigned long arg)
 		 * Restore per-cpu offset before any kernel
 		 * subsystem relying on it has a chance to run.
 		 */
-		set_my_cpu_offset(per_cpu_offset(cpu));
+		set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 
 		/*
 		 * Restore HW breakpoint registers to sane values

commit 18ab7db6b749ac27aac08d572afbbd2f4d937934
Author: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
Date:   Thu Jul 17 18:19:20 2014 +0100

    arm64: kernel: add missing __init section marker to cpu_suspend_init
    
    Suspend init function must be marked as __init, since it is not needed
    after the kernel has booted. This patch moves the cpu_suspend_init()
    function to the __init section.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 1fa9ce4afd8f..55a99b9a97e0 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -119,7 +119,7 @@ int cpu_suspend(unsigned long arg)
 extern struct sleep_save_sp sleep_save_sp;
 extern phys_addr_t sleep_idmap_phys;
 
-static int cpu_suspend_init(void)
+static int __init cpu_suspend_init(void)
 {
 	void *ctx_ptr;
 

commit fb4a96029c8a091c4365f57307e098543b48a222
Author: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
Date:   Fri Jan 24 10:56:19 2014 +0000

    arm64: kernel: fix per-cpu offset restore on resume
    
    The introduction of percpu offset optimisation through tpidr_el1 in:
    
    Commit id :7158627686f02319c50c8d9d78f75d4c8
    "arm64: percpu: implement optimised pcpu access using tpidr_el1"
    
    requires cpu_{suspend/resume} to restore the tpidr_el1 register upon resume
    so that percpu variables can be addressed correctly when a CPU comes out
    of reset from warm-boot.
    
    This patch fixes cpu_{suspend}/{resume} tpidr_el1 restoration on resume, by
    calling the set_my_cpu_offset C API, as it is done on primary and secondary
    CPUs on cold boot, so that, even if the register used to store the percpu
    offset is changed, the save and restore of general purpose registers does not
    have to be updated.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index 430344e2c989..1fa9ce4afd8f 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -1,3 +1,4 @@
+#include <linux/percpu.h>
 #include <linux/slab.h>
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
@@ -89,6 +90,13 @@ int cpu_suspend(unsigned long arg)
 	if (ret == 0) {
 		cpu_switch_mm(mm->pgd, mm);
 		flush_tlb_all();
+
+		/*
+		 * Restore per-cpu offset before any kernel
+		 * subsystem relying on it has a chance to run.
+		 */
+		set_my_cpu_offset(per_cpu_offset(cpu));
+
 		/*
 		 * Restore HW breakpoint registers to sane values
 		 * before debug exceptions are possibly reenabled

commit 65c021bb496a46ec06264e9d5e836dffa70ef380
Author: Lorenzo Pieralisi <Lorenzo.Pieralisi@arm.com>
Date:   Fri Jan 10 13:15:05 2014 +0000

    arm64: kernel: restore HW breakpoint registers in cpu_suspend
    
    When a CPU resumes from low-power, it restores HW breakpoint and
    watchpoint slots through a CPU PM notifier. Since we want to enable
    debugging as early as possible in the resume path, the mdscr content
    is restored along the general purpose registers in the cpu_suspend API
    and debug exceptions are reenabled when cpu_suspend returns. Since the
    CPU PM notifier is run after a CPU has been resumed, we cannot expect
    HW breakpoint registers to contain sane values till the notifier is run,
    since the HW breakpoints registers content is unknown at reset; this means
    that the CPU might run with debug exceptions enabled, mdscr restored but HW
    breakpoint registers containing junk values that can trigger spurious
    debug exceptions.
    
    This patch fixes current HW breakpoints restore by moving the HW breakpoints
    registers restoration to the cpu_suspend API, before the debug exceptions are
    enabled. This way, as soon as the cpu_suspend function returns the
    kernel can resume debugging with sane values in HW breakpoint registers.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
index e074b1c32723..430344e2c989 100644
--- a/arch/arm64/kernel/suspend.c
+++ b/arch/arm64/kernel/suspend.c
@@ -38,6 +38,22 @@ int __cpu_suspend_finisher(unsigned long arg, struct cpu_suspend_ctx *ptr,
 	return cpu_ops[cpu]->cpu_suspend(arg);
 }
 
+/*
+ * This hook is provided so that cpu_suspend code can restore HW
+ * breakpoints as early as possible in the resume path, before reenabling
+ * debug exceptions. Code cannot be run from a CPU PM notifier since by the
+ * time the notifier runs debug exceptions might have been enabled already,
+ * with HW breakpoints registers content still in an unknown state.
+ */
+void (*hw_breakpoint_restore)(void *);
+void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
+{
+	/* Prevent multiple restore hook initializations */
+	if (WARN_ON(hw_breakpoint_restore))
+		return;
+	hw_breakpoint_restore = hw_bp_restore;
+}
+
 /**
  * cpu_suspend
  *
@@ -73,6 +89,13 @@ int cpu_suspend(unsigned long arg)
 	if (ret == 0) {
 		cpu_switch_mm(mm->pgd, mm);
 		flush_tlb_all();
+		/*
+		 * Restore HW breakpoint registers to sane values
+		 * before debug exceptions are possibly reenabled
+		 * through local_dbg_restore.
+		 */
+		if (hw_breakpoint_restore)
+			hw_breakpoint_restore(NULL);
 	}
 
 	/*

commit 95322526ef62b84adb469c27535ab0252a369a85
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Mon Jul 22 12:22:13 2013 +0100

    arm64: kernel: cpu_{suspend/resume} implementation
    
    Kernel subsystems like CPU idle and suspend to RAM require a generic
    mechanism to suspend a processor, save its context and put it into
    a quiescent state. The cpu_{suspend}/{resume} implementation provides
    such a framework through a kernel interface allowing to save/restore
    registers, flush the context to DRAM and suspend/resume to/from
    low-power states where processor context may be lost.
    
    The CPU suspend implementation relies on the suspend protocol registered
    in CPU operations to carry out a suspend request after context is
    saved and flushed to DRAM. The cpu_suspend interface:
    
    int cpu_suspend(unsigned long arg);
    
    allows to pass an opaque parameter that is handed over to the suspend CPU
    operations back-end so that it can take action according to the
    semantics attached to it. The arg parameter allows suspend to RAM and CPU
    idle drivers to communicate to suspend protocol back-ends; it requires
    standardization so that the interface can be reused seamlessly across
    systems, paving the way for generic drivers.
    
    Context memory is allocated on the stack, whose address is stashed in a
    per-cpu variable to keep track of it and passed to core functions that
    save/restore the registers required by the architecture.
    
    Even though, upon successful execution, the cpu_suspend function shuts
    down the suspending processor, the warm boot resume mechanism, based
    on the cpu_resume function, makes the resume path operate as a
    cpu_suspend function return, so that cpu_suspend can be treated as a C
    function by the caller, which simplifies coding the PM drivers that rely
    on the cpu_suspend API.
    
    Upon context save, the minimal amount of memory is flushed to DRAM so
    that it can be retrieved when the MMU is off and caches are not searched.
    
    The suspend CPU operation, depending on the required operations (eg CPU vs
    Cluster shutdown) is in charge of flushing the cache hierarchy either
    implicitly (by calling firmware implementations like PSCI) or explicitly
    by executing the required cache maintainance functions.
    
    Debug exceptions are disabled during cpu_{suspend}/{resume} operations
    so that debug registers can be saved and restored properly preventing
    preemption from debug agents enabled in the kernel.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
new file mode 100644
index 000000000000..e074b1c32723
--- /dev/null
+++ b/arch/arm64/kernel/suspend.c
@@ -0,0 +1,109 @@
+#include <linux/slab.h>
+#include <asm/cacheflush.h>
+#include <asm/cpu_ops.h>
+#include <asm/debug-monitors.h>
+#include <asm/pgtable.h>
+#include <asm/memory.h>
+#include <asm/smp_plat.h>
+#include <asm/suspend.h>
+#include <asm/tlbflush.h>
+
+extern int __cpu_suspend(unsigned long);
+/*
+ * This is called by __cpu_suspend() to save the state, and do whatever
+ * flushing is required to ensure that when the CPU goes to sleep we have
+ * the necessary data available when the caches are not searched.
+ *
+ * @arg: Argument to pass to suspend operations
+ * @ptr: CPU context virtual address
+ * @save_ptr: address of the location where the context physical address
+ *            must be saved
+ */
+int __cpu_suspend_finisher(unsigned long arg, struct cpu_suspend_ctx *ptr,
+			   phys_addr_t *save_ptr)
+{
+	int cpu = smp_processor_id();
+
+	*save_ptr = virt_to_phys(ptr);
+
+	cpu_do_suspend(ptr);
+	/*
+	 * Only flush the context that must be retrieved with the MMU
+	 * off. VA primitives ensure the flush is applied to all
+	 * cache levels so context is pushed to DRAM.
+	 */
+	__flush_dcache_area(ptr, sizeof(*ptr));
+	__flush_dcache_area(save_ptr, sizeof(*save_ptr));
+
+	return cpu_ops[cpu]->cpu_suspend(arg);
+}
+
+/**
+ * cpu_suspend
+ *
+ * @arg: argument to pass to the finisher function
+ */
+int cpu_suspend(unsigned long arg)
+{
+	struct mm_struct *mm = current->active_mm;
+	int ret, cpu = smp_processor_id();
+	unsigned long flags;
+
+	/*
+	 * If cpu_ops have not been registered or suspend
+	 * has not been initialized, cpu_suspend call fails early.
+	 */
+	if (!cpu_ops[cpu] || !cpu_ops[cpu]->cpu_suspend)
+		return -EOPNOTSUPP;
+
+	/*
+	 * From this point debug exceptions are disabled to prevent
+	 * updates to mdscr register (saved and restored along with
+	 * general purpose registers) from kernel debuggers.
+	 */
+	local_dbg_save(flags);
+
+	/*
+	 * mm context saved on the stack, it will be restored when
+	 * the cpu comes out of reset through the identity mapped
+	 * page tables, so that the thread address space is properly
+	 * set-up on function return.
+	 */
+	ret = __cpu_suspend(arg);
+	if (ret == 0) {
+		cpu_switch_mm(mm->pgd, mm);
+		flush_tlb_all();
+	}
+
+	/*
+	 * Restore pstate flags. OS lock and mdscr have been already
+	 * restored, so from this point onwards, debugging is fully
+	 * renabled if it was enabled when core started shutdown.
+	 */
+	local_dbg_restore(flags);
+
+	return ret;
+}
+
+extern struct sleep_save_sp sleep_save_sp;
+extern phys_addr_t sleep_idmap_phys;
+
+static int cpu_suspend_init(void)
+{
+	void *ctx_ptr;
+
+	/* ctx_ptr is an array of physical addresses */
+	ctx_ptr = kcalloc(mpidr_hash_size(), sizeof(phys_addr_t), GFP_KERNEL);
+
+	if (WARN_ON(!ctx_ptr))
+		return -ENOMEM;
+
+	sleep_save_sp.save_ptr_stash = ctx_ptr;
+	sleep_save_sp.save_ptr_stash_phys = virt_to_phys(ctx_ptr);
+	sleep_idmap_phys = virt_to_phys(idmap_pg_dir);
+	__flush_dcache_area(&sleep_save_sp, sizeof(struct sleep_save_sp));
+	__flush_dcache_area(&sleep_idmap_phys, sizeof(sleep_idmap_phys));
+
+	return 0;
+}
+early_initcall(cpu_suspend_init);
