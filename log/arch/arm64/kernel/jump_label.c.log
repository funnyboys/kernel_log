commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
index 1eff270e8861..9a8a0ae1e75f 100644
--- a/arch/arm64/kernel/jump_label.c
+++ b/arch/arm64/kernel/jump_label.c
@@ -1,20 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (C) 2013 Huawei Ltd.
  * Author: Jiang Liu <liuj97@gmail.com>
  *
  * Based on arch/arm/kernel/jump_label.c
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #include <linux/kernel.h>
 #include <linux/jump_label.h>

commit e9666d10a5677a494260d60d1fa0b73cc7646eb3
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Dec 31 00:14:15 2018 +0900

    jump_label: move 'asm goto' support test to Kconfig
    
    Currently, CONFIG_JUMP_LABEL just means "I _want_ to use jump label".
    
    The jump label is controlled by HAVE_JUMP_LABEL, which is defined
    like this:
    
      #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
      # define HAVE_JUMP_LABEL
      #endif
    
    We can improve this by testing 'asm goto' support in Kconfig, then
    make JUMP_LABEL depend on CC_HAS_ASM_GOTO.
    
    Ugly #ifdef HAVE_JUMP_LABEL will go away, and CONFIG_JUMP_LABEL will
    match to the real kernel capability.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
index 646b9562ee64..1eff270e8861 100644
--- a/arch/arm64/kernel/jump_label.c
+++ b/arch/arm64/kernel/jump_label.c
@@ -20,8 +20,6 @@
 #include <linux/jump_label.h>
 #include <asm/insn.h>
 
-#ifdef HAVE_JUMP_LABEL
-
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
@@ -49,5 +47,3 @@ void arch_jump_label_transform_static(struct jump_entry *entry,
 	 * NOP needs to be replaced by a branch.
 	 */
 }
-
-#endif	/* HAVE_JUMP_LABEL */

commit c296146c058c87e9ed53001b6a3988519dbbb6a5
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 18 23:51:38 2018 -0700

    arm64/kernel: jump_label: Switch to relative references
    
    On a randomly chosen distro kernel build for arm64, vmlinux.o shows the
    following sections, containing jump label entries, and the associated
    RELA relocation records, respectively:
    
      ...
      [38088] __jump_table      PROGBITS         0000000000000000  00e19f30
           000000000002ea10  0000000000000000  WA       0     0     8
      [38089] .rela__jump_table RELA             0000000000000000  01fd8bb0
           000000000008be30  0000000000000018   I      38178   38088     8
      ...
    
    In other words, we have 190 KB worth of 'struct jump_entry' instances,
    and 573 KB worth of RELA entries to relocate each entry's code, target
    and key members. This means the RELA section occupies 10% of the .init
    segment, and the two sections combined represent 5% of vmlinux's entire
    memory footprint.
    
    So let's switch from 64-bit absolute references to 32-bit relative
    references for the code and target field, and a 64-bit relative
    reference for the 'key' field (which may reside in another module or the
    core kernel, which may be more than 4 GB way on arm64 when running with
    KASLR enable): this reduces the size of the __jump_table by 33%, and
    gets rid of the RELA section entirely.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-s390@vger.kernel.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Jessica Yu <jeyu@kernel.org>
    Link: https://lkml.kernel.org/r/20180919065144.25010-4-ard.biesheuvel@linaro.org

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
index e0756416e567..646b9562ee64 100644
--- a/arch/arm64/kernel/jump_label.c
+++ b/arch/arm64/kernel/jump_label.c
@@ -25,12 +25,12 @@
 void arch_jump_label_transform(struct jump_entry *entry,
 			       enum jump_label_type type)
 {
-	void *addr = (void *)entry->code;
+	void *addr = (void *)jump_entry_code(entry);
 	u32 insn;
 
 	if (type == JUMP_LABEL_JMP) {
-		insn = aarch64_insn_gen_branch_imm(entry->code,
-						   entry->target,
+		insn = aarch64_insn_gen_branch_imm(jump_entry_code(entry),
+						   jump_entry_target(entry),
 						   AARCH64_INSN_BRANCH_NOLINK);
 	} else {
 		insn = aarch64_insn_gen_nop();

commit f6cc0c50164994afd538911603b4a8f8029aeba7
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Aug 16 11:45:50 2018 +0100

    arm64: Avoid calling stop_machine() when patching jump labels
    
    Patching a jump label involves patching a single instruction at a time,
    swizzling between a branch and a NOP. The architecture treats these
    instructions specially, so a concurrently executing CPU is guaranteed to
    see either the NOP or the branch, rather than an amalgamation of the two
    instruction encodings.
    
    However, in order to guarantee that the new instruction is visible, it
    is necessary to send an IPI to the concurrently executing CPU so that it
    discards any previously fetched instructions from its pipeline. This
    operation therefore cannot be completed from a context with IRQs
    disabled, but this is exactly what happens on the jump label path where
    the hotplug lock is held and irqs are subsequently disabled by
    stop_machine_cpuslocked(). This results in a deadlock during boot on
    Hikey-960.
    
    Due to the architectural guarantees around patching NOPs and branches,
    we don't actually need to stop_machine() at all on the jump label path,
    so we can avoid the deadlock by using the "nosync" variant of our
    instruction patching routine.
    
    Fixes: 693350a79980 ("arm64: insn: Don't fallback on nosync path for general insn patching")
    Reported-by: Tuomas Tynkkynen <tuomas.tynkkynen@iki.fi>
    Reported-by: John Stultz <john.stultz@linaro.org>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>
    Tested-by: Tuomas Tynkkynen <tuomas@tuxera.com>
    Tested-by: John Stultz <john.stultz@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
index c2dd1ad3e648..e0756416e567 100644
--- a/arch/arm64/kernel/jump_label.c
+++ b/arch/arm64/kernel/jump_label.c
@@ -36,7 +36,7 @@ void arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
-	aarch64_insn_patch_text(&addr, &insn, 1);
+	aarch64_insn_patch_text_nosync(addr, insn);
 }
 
 void arch_jump_label_transform_static(struct jump_entry *entry,

commit 76b235c6bcb16062d663e2ee96db0b69f2e6bc14
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 24 14:45:44 2015 +0200

    jump_label: Rename JUMP_LABEL_{EN,DIS}ABLE to JUMP_LABEL_{JMP,NOP}
    
    Since we've already stepped away from ENABLE is a JMP and DISABLE is a
    NOP with the branch_default bits, and are going to make it even worse,
    rename it to make it all clearer.
    
    This way we don't mix multiple levels of logic attributes, but have a
    plain 'physical' name for what the current instruction patching status
    of a jump label is.
    
    This is a first step in removing the naming confusion that has led to
    a stream of avoidable bugs such as:
    
      a833581e372a ("x86, perf: Fix static_key bug in load_mm_cr4()")
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    [ Beefed up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
index 4f1fec7a46db..c2dd1ad3e648 100644
--- a/arch/arm64/kernel/jump_label.c
+++ b/arch/arm64/kernel/jump_label.c
@@ -28,7 +28,7 @@ void arch_jump_label_transform(struct jump_entry *entry,
 	void *addr = (void *)entry->code;
 	u32 insn;
 
-	if (type == JUMP_LABEL_ENABLE) {
+	if (type == JUMP_LABEL_JMP) {
 		insn = aarch64_insn_gen_branch_imm(entry->code,
 						   entry->target,
 						   AARCH64_INSN_BRANCH_NOLINK);

commit 6ddae4186886a81e22ad78ad7c6936ed36bc8225
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Nov 25 15:44:18 2014 +0000

    arm64: jump labels: NOP out NOP -> NOP replacement
    
    In the arm64 arch_static_branch implementation we place an A64 NOP into
    the instruction stream and log relevant details to a jump_entry in a
    __jump_table section. Later this may be replaced with an immediate
    branch without link to the code for the unlikely case.
    
    At init time, the core calls arch_jump_label_transform_static to
    initialise the NOPs. On x86 this involves inserting the optimal NOP for
    a given microarchitecture, but on arm64 we only use the architectural
    NOP, and hence replace each NOP with the exact same NOP. This is
    somewhat pointless.
    
    Additionally, at module load time we don't call jump_label_apply_nops to
    patch the optimal NOPs in, unlike other architectures, but get away with
    this because we only use the architectural NOP anyway. A later notifier
    will patch NOPs with branches as required.
    
    Similarly to x86 commit 11570da1c5b1dee1 (x86/jump-label: Do not bother
    updating NOPs if they are correct), we can avoid patching NOPs with
    identical NOPs. Given that we only use a single NOP encoding, this means
    we can NOP-out the body of arch_jump_label_transform_static entirely. As
    the default __weak arch_jump_label_transform_static implementation
    performs a patch, we must use an empty function to achieve this.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
index 263a166291fb..4f1fec7a46db 100644
--- a/arch/arm64/kernel/jump_label.c
+++ b/arch/arm64/kernel/jump_label.c
@@ -22,9 +22,8 @@
 
 #ifdef HAVE_JUMP_LABEL
 
-static void __arch_jump_label_transform(struct jump_entry *entry,
-					enum jump_label_type type,
-					bool is_static)
+void arch_jump_label_transform(struct jump_entry *entry,
+			       enum jump_label_type type)
 {
 	void *addr = (void *)entry->code;
 	u32 insn;
@@ -37,22 +36,18 @@ static void __arch_jump_label_transform(struct jump_entry *entry,
 		insn = aarch64_insn_gen_nop();
 	}
 
-	if (is_static)
-		aarch64_insn_patch_text_nosync(addr, insn);
-	else
-		aarch64_insn_patch_text(&addr, &insn, 1);
-}
-
-void arch_jump_label_transform(struct jump_entry *entry,
-			       enum jump_label_type type)
-{
-	__arch_jump_label_transform(entry, type, false);
+	aarch64_insn_patch_text(&addr, &insn, 1);
 }
 
 void arch_jump_label_transform_static(struct jump_entry *entry,
 				      enum jump_label_type type)
 {
-	__arch_jump_label_transform(entry, type, true);
+	/*
+	 * We use the architected A64 NOP in arch_static_branch, so there's no
+	 * need to patch an identical A64 NOP over the top of it here. The core
+	 * will call arch_jump_label_transform from a module notifier if the
+	 * NOP needs to be replaced by a branch.
+	 */
 }
 
 #endif	/* HAVE_JUMP_LABEL */

commit 9732cafd9dc0206479be919baf0067239f0a63ca
Author: Jiang Liu <liuj97@gmail.com>
Date:   Tue Jan 7 22:17:13 2014 +0800

    arm64, jump label: optimize jump label implementation
    
    Optimize jump label implementation for ARM64 by dynamically patching
    kernel text.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Jiang Liu <liuj97@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/jump_label.c b/arch/arm64/kernel/jump_label.c
new file mode 100644
index 000000000000..263a166291fb
--- /dev/null
+++ b/arch/arm64/kernel/jump_label.c
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2013 Huawei Ltd.
+ * Author: Jiang Liu <liuj97@gmail.com>
+ *
+ * Based on arch/arm/kernel/jump_label.c
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/kernel.h>
+#include <linux/jump_label.h>
+#include <asm/insn.h>
+
+#ifdef HAVE_JUMP_LABEL
+
+static void __arch_jump_label_transform(struct jump_entry *entry,
+					enum jump_label_type type,
+					bool is_static)
+{
+	void *addr = (void *)entry->code;
+	u32 insn;
+
+	if (type == JUMP_LABEL_ENABLE) {
+		insn = aarch64_insn_gen_branch_imm(entry->code,
+						   entry->target,
+						   AARCH64_INSN_BRANCH_NOLINK);
+	} else {
+		insn = aarch64_insn_gen_nop();
+	}
+
+	if (is_static)
+		aarch64_insn_patch_text_nosync(addr, insn);
+	else
+		aarch64_insn_patch_text(&addr, &insn, 1);
+}
+
+void arch_jump_label_transform(struct jump_entry *entry,
+			       enum jump_label_type type)
+{
+	__arch_jump_label_transform(entry, type, false);
+}
+
+void arch_jump_label_transform_static(struct jump_entry *entry,
+				      enum jump_label_type type)
+{
+	__arch_jump_label_transform(entry, type, true);
+}
+
+#endif	/* HAVE_JUMP_LABEL */
