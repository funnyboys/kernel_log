commit 09c717c92b52df54918e12cbfe6a4658233fda69
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Wed Jul 8 22:13:40 2020 -0700

    arm64: Add missing sentinel to erratum_1463225
    
    When the erratum_1463225 array was introduced a sentinel at the end was
    missing thus causing a KASAN: global-out-of-bounds in
    is_affected_midr_range_list on arm64 error.
    
    Fixes: a9e821b89daa ("arm64: Add KRYO4XX gold CPU cores to erratum list 1463225 and 1418040")
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Reviewed-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Link: https://lore.kernel.org/linux-arm-kernel/CA+G9fYs3EavpU89-rTQfqQ9GgxAMgMAk7jiiVrfP0yxj5s+Q6g@mail.gmail.com/
    Link: https://lore.kernel.org/r/20200709051345.14544-1-f.fainelli@gmail.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 8e302dc093d0..79728bfb5351 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -782,6 +782,7 @@ static const struct midr_range erratum_1463225[] = {
 	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 1),
 	/* Kryo4xx Gold (rcpe to rfpf) => (r0p0 to r3p1) */
 	MIDR_RANGE(MIDR_QCOM_KRYO_4XX_GOLD, 0xc, 0xe, 0xf, 0xf),
+	{},
 };
 #endif
 

commit 9b23d95c539ebc5d6d6b5d6f20d2d7922384e76e
Author: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
Date:   Tue Jun 30 23:30:55 2020 +0530

    arm64: Add KRYO4XX silver CPU cores to erratum list 1530923 and 1024718
    
    KRYO4XX silver/LITTLE CPU cores with revision r1p0 are affected by
    erratum 1530923 and 1024718, so add them to the respective list.
    The variant and revision bits are implementation defined and are
    different from the their Cortex CPU counterparts on which they are
    based on, i.e., r1p0 is equivalent to rdpe.
    
    Signed-off-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Link: https://lore.kernel.org/r/7013e8a3f857ca7e82863cc9e34a614293d7f80c.1593539394.git.saiprakash.ranjan@codeaurora.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 044f1d7aebdf..8e302dc093d0 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -769,6 +769,8 @@ static const struct midr_range erratum_speculative_at_list[] = {
 #ifdef CONFIG_ARM64_ERRATUM_1530923
 	/* Cortex A55 r0p0 to r2p0 */
 	MIDR_RANGE(MIDR_CORTEX_A55, 0, 0, 2, 0),
+	/* Kryo4xx Silver (rdpe => r1p0) */
+	MIDR_REV(MIDR_QCOM_KRYO_4XX_SILVER, 0xd, 0xe),
 #endif
 	{},
 };

commit a9e821b89daa55cc940c546b124101939d3f0451
Author: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
Date:   Tue Jun 30 23:30:54 2020 +0530

    arm64: Add KRYO4XX gold CPU cores to erratum list 1463225 and 1418040
    
    KRYO4XX gold/big CPU core revisions r0p0 to r3p1 are affected by
    erratum 1463225 and 1418040, so add them to the respective list.
    The variant and revision bits are implementation defined and are
    different from the their Cortex CPU counterparts on which they are
    based on, i.e., (r0p0 to r3p1) is equivalent to (rcpe to rfpf).
    
    Signed-off-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Link: https://lore.kernel.org/r/83780e80c6377c12ca51b5d53186b61241685e49.1593539394.git.saiprakash.ranjan@codeaurora.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index cf50c53e9357..044f1d7aebdf 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -472,12 +472,7 @@ static bool
 has_cortex_a76_erratum_1463225(const struct arm64_cpu_capabilities *entry,
 			       int scope)
 {
-	u32 midr = read_cpuid_id();
-	/* Cortex-A76 r0p0 - r3p1 */
-	struct midr_range range = MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 1);
-
-	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
-	return is_midr_in_range(midr, &range) && is_kernel_in_hyp_mode();
+	return is_affected_midr_range_list(entry, scope) && is_kernel_in_hyp_mode();
 }
 #endif
 
@@ -728,6 +723,8 @@ static const struct midr_range erratum_1418040_list[] = {
 	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 1),
 	/* Neoverse-N1 r0p0 to r3p1 */
 	MIDR_RANGE(MIDR_NEOVERSE_N1, 0, 0, 3, 1),
+	/* Kryo4xx Gold (rcpe to rfpf) => (r0p0 to r3p1) */
+	MIDR_RANGE(MIDR_QCOM_KRYO_4XX_GOLD, 0xc, 0xe, 0xf, 0xf),
 	{},
 };
 #endif
@@ -777,6 +774,15 @@ static const struct midr_range erratum_speculative_at_list[] = {
 };
 #endif
 
+#ifdef CONFIG_ARM64_ERRATUM_1463225
+static const struct midr_range erratum_1463225[] = {
+	/* Cortex-A76 r0p0 - r3p1 */
+	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 1),
+	/* Kryo4xx Gold (rcpe to rfpf) => (r0p0 to r3p1) */
+	MIDR_RANGE(MIDR_QCOM_KRYO_4XX_GOLD, 0xc, 0xe, 0xf, 0xf),
+};
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
@@ -916,6 +922,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_1463225,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_cortex_a76_erratum_1463225,
+		.midr_range_list = erratum_1463225,
 	},
 #endif
 #ifdef CONFIG_CAVIUM_TX2_ERRATUM_219

commit 108447fd0d1a34b0929cd26dc637c917a734ebab
Author: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
Date:   Thu Jun 25 16:01:23 2020 +0530

    arm64: Add KRYO{3,4}XX silver CPU cores to SSB safelist
    
    QCOM KRYO{3,4}XX silver/LITTLE CPU cores are based on
    Cortex-A55 and are SSB safe, hence add them to SSB
    safelist -> arm64_ssb_cpus[].
    
    Reported-by: Stephen Boyd <swboyd@chromium.org>
    Signed-off-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    Reviewed-by: Douglas Anderson <dianders@chromium.org>
    Link: https://lore.kernel.org/r/20200625103123.7240-1-saiprakash.ranjan@codeaurora.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index ad06d6802d2e..cf50c53e9357 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -460,6 +460,8 @@ static const struct midr_range arm64_ssb_cpus[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
 	MIDR_ALL_VERSIONS(MIDR_BRAHMA_B53),
+	MIDR_ALL_VERSIONS(MIDR_QCOM_KRYO_3XX_SILVER),
+	MIDR_ALL_VERSIONS(MIDR_QCOM_KRYO_4XX_SILVER),
 	{},
 };
 

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit c350717ec7de67daad26d996efe7f3d97d95aa9c
Merge: d27865279f12 fe677be98914
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:02:51 2020 +0100

    Merge branch 'for-next/kvm/errata' into for-next/core
    
    KVM CPU errata rework
    (Andrew Scull and Marc Zyngier)
    * for-next/kvm/errata:
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}

commit d82755b2e781c8989614c82df7582f5649e265b8
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 16:45:17 2020 +0100

    KVM: arm64: Kill off CONFIG_KVM_ARM_HOST
    
    CONFIG_KVM_ARM_HOST is just a proxy for CONFIG_KVM, so remove it in favour
    of the latter.
    
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Fuad Tabba <tabba@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200505154520.194120-2-tabba@google.com

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index df56d2295d16..a102321fc8a2 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -234,7 +234,7 @@ static int detect_harden_bp_fw(void)
 		smccc_end = NULL;
 		break;
 
-#if IS_ENABLED(CONFIG_KVM_ARM_HOST)
+#if IS_ENABLED(CONFIG_KVM)
 	case SMCCC_CONDUIT_SMC:
 		cb = call_smc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_smc;

commit 357dd8a2aff25270971e11a37ddec807f37488e9
Author: Geert Uytterhoeven <geert+renesas@glider.be>
Date:   Tue May 12 16:52:55 2020 +0200

    arm64: cpufeature: Add "or" to mitigations for multiple errata
    
    Several actions are not mitigations for a single erratum, but for
    multiple errata.  However, printing a line like
    
        CPU features: detected: ARM errata 1165522, 1530923
    
    may give the false impression that all three listed errata have been
    detected.  This can confuse the user, who may think his Cortex-A55 is
    suddenly affected by a Cortex-A76 erratum.
    
    Add "or" to all descriptions for mitigations for multiple errata, to
    make it clear that only one or more of the errata printed are
    applicable, and not necessarily all of them.
    
    Signed-off-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Link: https://lore.kernel.org/r/20200512145255.5520-1-geert+renesas@glider.be
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index df56d2295d16..c8cb98a4fa6d 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -774,7 +774,7 @@ static const struct midr_range erratum_speculative_at_vhe_list[] = {
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
-		.desc = "ARM errata 826319, 827319, 824069, 819472",
+		.desc = "ARM errata 826319, 827319, 824069, or 819472",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
 		ERRATA_MIDR_RANGE_LIST(workaround_clean_cache),
 		.cpu_enable = cpu_enable_cache_maint_trap,
@@ -856,7 +856,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_ARM64_WORKAROUND_REPEAT_TLBI
 	{
-		.desc = "Qualcomm erratum 1009, ARM erratum 1286807",
+		.desc = "Qualcomm erratum 1009, or ARM erratum 1286807",
 		.capability = ARM64_WORKAROUND_REPEAT_TLBI,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = cpucap_multi_entry_cap_matches,
@@ -899,7 +899,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT_VHE
 	{
-		.desc = "ARM errata 1165522, 1530923",
+		.desc = "ARM errata 1165522 or 1530923",
 		.capability = ARM64_WORKAROUND_SPECULATIVE_AT_VHE,
 		ERRATA_MIDR_RANGE_LIST(erratum_speculative_at_vhe_list),
 	},

commit 02ab1f5018c3ad0b8677e797b5d3333d2e3b7f20
Author: Andrew Scull <ascull@google.com>
Date:   Mon May 4 10:48:58 2020 +0100

    arm64: Unify WORKAROUND_SPECULATIVE_AT_{NVHE,VHE}
    
    Errata 1165522, 1319367 and 1530923 each allow TLB entries to be
    allocated as a result of a speculative AT instruction. In order to
    avoid mandating VHE on certain affected CPUs, apply the workaround to
    both the nVHE and the VHE case for all affected CPUs.
    
    Signed-off-by: Andrew Scull <ascull@google.com>
    Acked-by: Will Deacon <will@kernel.org>
    CC: Marc Zyngier <maz@kernel.org>
    CC: James Morse <james.morse@arm.com>
    CC: Suzuki K Poulose <suzuki.poulose@arm.com>
    CC: Will Deacon <will@kernel.org>
    CC: Steven Price <steven.price@arm.com>
    Link: https://lore.kernel.org/r/20200504094858.108917-1-ascull@google.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index df56d2295d16..95006a791026 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -635,7 +635,7 @@ has_neoverse_n1_erratum_1542419(const struct arm64_cpu_capabilities *entry,
 	return is_midr_in_range(midr, &range) && has_dic;
 }
 
-#if defined(CONFIG_HARDEN_EL2_VECTORS) || defined(CONFIG_ARM64_ERRATUM_1319367)
+#if defined(CONFIG_HARDEN_EL2_VECTORS)
 
 static const struct midr_range ca57_a72[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
@@ -757,12 +757,16 @@ static const struct arm64_cpu_capabilities erratum_843419_list[] = {
 };
 #endif
 
-#ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT_VHE
-static const struct midr_range erratum_speculative_at_vhe_list[] = {
+#ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT
+static const struct midr_range erratum_speculative_at_list[] = {
 #ifdef CONFIG_ARM64_ERRATUM_1165522
 	/* Cortex A76 r0p0 to r2p0 */
 	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
 #endif
+#ifdef CONFIG_ARM64_ERRATUM_1319367
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+#endif
 #ifdef CONFIG_ARM64_ERRATUM_1530923
 	/* Cortex A55 r0p0 to r2p0 */
 	MIDR_RANGE(MIDR_CORTEX_A55, 0, 0, 2, 0),
@@ -897,11 +901,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		ERRATA_MIDR_RANGE_LIST(erratum_1418040_list),
 	},
 #endif
-#ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT_VHE
+#ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT
 	{
-		.desc = "ARM errata 1165522, 1530923",
-		.capability = ARM64_WORKAROUND_SPECULATIVE_AT_VHE,
-		ERRATA_MIDR_RANGE_LIST(erratum_speculative_at_vhe_list),
+		.desc = "ARM errata 1165522, 1319367, 1530923",
+		.capability = ARM64_WORKAROUND_SPECULATIVE_AT,
+		ERRATA_MIDR_RANGE_LIST(erratum_speculative_at_list),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_1463225
@@ -934,13 +938,6 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.matches = has_neoverse_n1_erratum_1542419,
 		.cpu_enable = cpu_enable_trap_ctr_access,
 	},
-#endif
-#ifdef CONFIG_ARM64_ERRATUM_1319367
-	{
-		.desc = "ARM erratum 1319367",
-		.capability = ARM64_WORKAROUND_SPECULATIVE_AT_NVHE,
-		ERRATA_MIDR_RANGE_LIST(ca57_a72),
-	},
 #endif
 	{
 	}

commit 9a25136a61179fe79689abf3c3fc67f89ce9ec13
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Tue Mar 10 16:25:44 2020 -0700

    arm64: Mark call_smc_arch_workaround_1 as __maybe_unused
    
    When building allnoconfig:
    
    arch/arm64/kernel/cpu_errata.c:174:13: warning: unused function
    'call_smc_arch_workaround_1' [-Wunused-function]
    static void call_smc_arch_workaround_1(void)
                ^
    1 warning generated.
    
    Follow arch/arm and mark this function as __maybe_unused.
    
    Fixes: 4db61fef16a1 ("arm64: kvm: Modernize __smccc_workaround_1_smc_start annotations")
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 6a2ca339741c..df56d2295d16 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -171,7 +171,7 @@ static void install_bp_hardening_cb(bp_hardening_cb_t fn,
 
 #include <linux/arm-smccc.h>
 
-static void call_smc_arch_workaround_1(void)
+static void __maybe_unused call_smc_arch_workaround_1(void)
 {
 	arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
 }

commit 4db61fef16a104f94bde24fe163064b98cee6b7c
Author: Mark Brown <broonie@kernel.org>
Date:   Tue Feb 18 19:58:39 2020 +0000

    arm64: kvm: Modernize __smccc_workaround_1_smc_start annotations
    
    In an effort to clarify and simplify the annotation of assembly functions
    in the kernel new macros have been introduced. These replace ENTRY and
    ENDPROC with separate annotations for standard C callable functions,
    data and code with different calling conventions.
    
    Using these for __smccc_workaround_1_smc is more involved than for most
    symbols as this symbol is annotated quite unusually, rather than just have
    the explicit symbol we define _start and _end symbols which we then use to
    compute the length. This does not play at all nicely with the new style
    macros. Instead define a constant for the size of the function and use that
    in both the C code and for .org based size checks in the assembly code.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 0af2201cefda..6a2ca339741c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -11,6 +11,7 @@
 #include <asm/cpu.h>
 #include <asm/cputype.h>
 #include <asm/cpufeature.h>
+#include <asm/kvm_asm.h>
 #include <asm/smp_plat.h>
 
 static bool __maybe_unused
@@ -113,9 +114,6 @@ atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM_INDIRECT_VECTORS
-extern char __smccc_workaround_1_smc_start[];
-extern char __smccc_workaround_1_smc_end[];
-
 static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 				const char *hyp_vecs_end)
 {
@@ -163,9 +161,6 @@ static void install_bp_hardening_cb(bp_hardening_cb_t fn,
 	raw_spin_unlock(&bp_lock);
 }
 #else
-#define __smccc_workaround_1_smc_start		NULL
-#define __smccc_workaround_1_smc_end		NULL
-
 static void install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
 				      const char *hyp_vecs_end)
@@ -239,11 +234,14 @@ static int detect_harden_bp_fw(void)
 		smccc_end = NULL;
 		break;
 
+#if IS_ENABLED(CONFIG_KVM_ARM_HOST)
 	case SMCCC_CONDUIT_SMC:
 		cb = call_smc_arch_workaround_1;
-		smccc_start = __smccc_workaround_1_smc_start;
-		smccc_end = __smccc_workaround_1_smc_end;
+		smccc_start = __smccc_workaround_1_smc;
+		smccc_end = __smccc_workaround_1_smc +
+			__SMCCC_WORKAROUND_1_SMC_SZ;
 		break;
+#endif
 
 	default:
 		return -1;

commit 6e52aab9015277c39c6f03a76a7e9487370013f8
Author: Mark Brown <broonie@kernel.org>
Date:   Tue Feb 18 19:58:38 2020 +0000

    arm64: kvm: Modernize annotation for __bp_harden_hyp_vecs
    
    We have recently introduced new macros for annotating assembly symbols
    for things that aren't C functions, SYM_CODE_START() and SYM_CODE_END(),
    in an effort to clarify and simplify our annotations of assembly files.
    
    Using these for __bp_harden_hyp_vecs is more involved than for most symbols
    as this symbol is annotated quite unusually as rather than just have the
    explicit symbol we define _start and _end symbols which we then use to
    compute the length. This does not play at all nicely with the new style
    macros. Since the size of the vectors is a known constant which won't vary
    the simplest thing to do is simply to drop the separate _start and _end
    symbols and just use a #define for the size.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 703ad0a84f99..0af2201cefda 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -119,7 +119,7 @@ extern char __smccc_workaround_1_smc_end[];
 static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 				const char *hyp_vecs_end)
 {
-	void *dst = lm_alias(__bp_harden_hyp_vecs_start + slot * SZ_2K);
+	void *dst = lm_alias(__bp_harden_hyp_vecs + slot * SZ_2K);
 	int i;
 
 	for (i = 0; i < SZ_2K; i += 0x80)

commit ab3906c53144837f1a192b5c3ba71ec2f938c187
Merge: aa246c056c43 275fa0ea2cf7
Author: Will Deacon <will@kernel.org>
Date:   Wed Jan 22 11:35:05 2020 +0000

    Merge branch 'for-next/errata' into for-next/core
    
    * for-next/errata: (3 commits)
      arm64: Workaround for Cortex-A55 erratum 1530923
      ...

commit 83b0c36b8a136042aa9209fc9ca2fab31d5fbe7c
Author: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
Date:   Fri Jan 17 01:33:53 2020 +0530

    arm64: Add KRYO{3,4}XX CPU cores to spectre-v2 safe list
    
    The "silver" KRYO3XX and KRYO4XX CPU cores are not affected by Spectre
    variant 2. Add them to spectre_v2 safe list to correct the spurious
    ARM_SMCCC_ARCH_WORKAROUND_1 warning and vulnerability status reported
    under sysfs.
    
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Tested-by: Stephen Boyd <swboyd@chromium.org>
    Signed-off-by: Sai Prakash Ranjan <saiprakash.ranjan@codeaurora.org>
    [will: tweaked commit message to remove stale mention of "gold" cores]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 85f4bec22f6d..d661c7d0e6e2 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -548,6 +548,8 @@ static const struct midr_range spectre_v2_safe_list[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
 	MIDR_ALL_VERSIONS(MIDR_BRAHMA_B53),
 	MIDR_ALL_VERSIONS(MIDR_HISI_TSV110),
+	MIDR_ALL_VERSIONS(MIDR_QCOM_KRYO_3XX_SILVER),
+	MIDR_ALL_VERSIONS(MIDR_QCOM_KRYO_4XX_SILVER),
 	{ /* sentinel */ }
 };
 

commit 275fa0ea2cf7a84450f9c0ec0d9e7ec168ed2e2d
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:31 2019 +0000

    arm64: Workaround for Cortex-A55 erratum 1530923
    
    Cortex-A55 erratum 1530923 allows TLB entries to be allocated as a
    result of a speculative AT instruction. This may happen in the middle of
    a guest world switch while the relevant VMSA configuration is in an
    inconsistent state, leading to erroneous content being allocated into
    TLBs.
    
    The same workaround as is used for Cortex-A76 erratum 1165522
    (WORKAROUND_SPECULATIVE_AT_VHE) can be used here. Note that this
    mandates the use of VHE on affected parts.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 0332fca5564a..0bd2867f3248 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -762,6 +762,10 @@ static const struct midr_range erratum_speculative_at_vhe_list[] = {
 #ifdef CONFIG_ARM64_ERRATUM_1165522
 	/* Cortex A76 r0p0 to r2p0 */
 	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1530923
+	/* Cortex A55 r0p0 to r2p0 */
+	MIDR_RANGE(MIDR_CORTEX_A55, 0, 0, 2, 0),
 #endif
 	{},
 };
@@ -895,7 +899,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT_VHE
 	{
-		.desc = "ARM erratum 1165522",
+		.desc = "ARM errata 1165522, 1530923",
 		.capability = ARM64_WORKAROUND_SPECULATIVE_AT_VHE,
 		ERRATA_MIDR_RANGE_LIST(erratum_speculative_at_vhe_list),
 	},

commit db0d46a58d34c7cd9d5ece98daf4b8afe3d770f8
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:30 2019 +0000

    arm64: Rename WORKAROUND_1319367 to SPECULATIVE_AT_NVHE
    
    To match SPECULATIVE_AT_VHE let's also have a generic name for the NVHE
    variant.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 7886ddbafded..0332fca5564a 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -934,7 +934,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_ERRATUM_1319367
 	{
 		.desc = "ARM erratum 1319367",
-		.capability = ARM64_WORKAROUND_1319367,
+		.capability = ARM64_WORKAROUND_SPECULATIVE_AT_NVHE,
 		ERRATA_MIDR_RANGE_LIST(ca57_a72),
 	},
 #endif

commit e85d68faed4e79fd0b481c72de8245d4290369db
Author: Steven Price <steven.price@arm.com>
Date:   Mon Dec 16 11:56:29 2019 +0000

    arm64: Rename WORKAROUND_1165522 to SPECULATIVE_AT_VHE
    
    Cortex-A55 is affected by a similar erratum, so rename the existing
    workaround for errarum 1165522 so it can be used for both errata.
    
    Acked-by: Marc Zyngier <maz@kernel.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 85f4bec22f6d..7886ddbafded 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -757,6 +757,16 @@ static const struct arm64_cpu_capabilities erratum_843419_list[] = {
 };
 #endif
 
+#ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT_VHE
+static const struct midr_range erratum_speculative_at_vhe_list[] = {
+#ifdef CONFIG_ARM64_ERRATUM_1165522
+	/* Cortex A76 r0p0 to r2p0 */
+	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+#endif
+	{},
+};
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
@@ -883,12 +893,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		ERRATA_MIDR_RANGE_LIST(erratum_1418040_list),
 	},
 #endif
-#ifdef CONFIG_ARM64_ERRATUM_1165522
+#ifdef CONFIG_ARM64_WORKAROUND_SPECULATIVE_AT_VHE
 	{
-		/* Cortex-A76 r0p0 to r2p0 */
 		.desc = "ARM erratum 1165522",
-		.capability = ARM64_WORKAROUND_1165522,
-		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+		.capability = ARM64_WORKAROUND_SPECULATIVE_AT_VHE,
+		ERRATA_MIDR_RANGE_LIST(erratum_speculative_at_vhe_list),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_1463225

commit aa638cfe3e7358122a15cb1d295b622aae69e006
Author: Wei Li <liwei391@huawei.com>
Date:   Fri Dec 20 17:17:10 2019 +0800

    arm64: cpu_errata: Add Hisilicon TSV110 to spectre-v2 safe list
    
    HiSilicon Taishan v110 CPUs didn't implement CSV2 field of the
    ID_AA64PFR0_EL1, but spectre-v2 is mitigated by hardware, so
    whitelist the MIDR in the safe list.
    
    Signed-off-by: Wei Li <liwei391@huawei.com>
    [hanjun: re-write the commit log]
    Signed-off-by: Hanjun Guo <guohanjun@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 6a09ca7644ea..85f4bec22f6d 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -547,6 +547,7 @@ static const struct midr_range spectre_v2_safe_list[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
 	MIDR_ALL_VERSIONS(MIDR_BRAHMA_B53),
+	MIDR_ALL_VERSIONS(MIDR_HISI_TSV110),
 	{ /* sentinel */ }
 };
 

commit 752272f16dd18f2cac58a583a8673c8e2fb93abb
Merge: 3f3c8be973af 96710247298d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 18:02:36 2019 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - data abort report and injection
       - steal time support
       - GICv4 performance improvements
       - vgic ITS emulation fixes
       - simplify FWB handling
       - enable halt polling counters
       - make the emulated timer PREEMPT_RT compliant
    
      s390:
       - small fixes and cleanups
       - selftest improvements
       - yield improvements
    
      PPC:
       - add capability to tell userspace whether we can single-step the
         guest
       - improve the allocation of XIVE virtual processor IDs
       - rewrite interrupt synthesis code to deliver interrupts in virtual
         mode when appropriate.
       - minor cleanups and improvements.
    
      x86:
       - XSAVES support for AMD
       - more accurate report of nested guest TSC to the nested hypervisor
       - retpoline optimizations
       - support for nested 5-level page tables
       - PMU virtualization optimizations, and improved support for nested
         PMU virtualization
       - correct latching of INITs for nested virtualization
       - IOAPIC optimization
       - TSX_CTRL virtualization for more TAA happiness
       - improved allocation and flushing of SEV ASIDs
       - many bugfixes and cleanups"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (127 commits)
      kvm: nVMX: Relax guest IA32_FEATURE_CONTROL constraints
      KVM: x86: Grab KVM's srcu lock when setting nested state
      KVM: x86: Open code shared_msr_update() in its only caller
      KVM: Fix jump label out_free_* in kvm_init()
      KVM: x86: Remove a spurious export of a static function
      KVM: x86: create mmu/ subdirectory
      KVM: nVMX: Remove unnecessary TLB flushes on L1<->L2 switches when L1 use apic-access-page
      KVM: x86: remove set but not used variable 'called'
      KVM: nVMX: Do not mark vmcs02->apic_access_page as dirty when unpinning
      KVM: vmx: use MSR_IA32_TSX_CTRL to hard-disable TSX on guest that lack it
      KVM: vmx: implement MSR_IA32_TSX_CTRL disable RTM functionality
      KVM: x86: implement MSR_IA32_TSX_CTRL effect on CPUID
      KVM: x86: do not modify masked bits of shared MSRs
      KVM: x86: fix presentation of TSX feature in ARCH_CAPABILITIES
      KVM: PPC: Book3S HV: XIVE: Fix potential page leak on error path
      KVM: PPC: Book3S HV: XIVE: Free previous EQ page when setting up a new one
      KVM: nVMX: Assume TLB entries of L1 and L2 are tagged differently if L0 use EPT
      KVM: x86: Unexport kvm_vcpu_reload_apic_access_page()
      KVM: nVMX: add CR4_LA57 bit to nested CR4_FIXED1
      KVM: nVMX: Use semi-colon instead of comma for exit-handlers initialization
      ...

commit 4ba380f61624113395bebdc2f9f6da990a0738f9
Merge: e25645b181ae d8e85e144bbe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 15:39:19 2019 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Apart from the arm64-specific bits (core arch and perf, new arm64
      selftests), it touches the generic cow_user_page() (reviewed by
      Kirill) together with a macro for x86 to preserve the existing
      behaviour on this architecture.
    
      Summary:
    
       - On ARMv8 CPUs without hardware updates of the access flag, avoid
         failing cow_user_page() on PFN mappings if the pte is old. The
         patches introduce an arch_faults_on_old_pte() macro, defined as
         false on x86. When true, cow_user_page() makes the pte young before
         attempting __copy_from_user_inatomic().
    
       - Covert the synchronous exception handling paths in
         arch/arm64/kernel/entry.S to C.
    
       - FTRACE_WITH_REGS support for arm64.
    
       - ZONE_DMA re-introduced on arm64 to support Raspberry Pi 4
    
       - Several kselftest cases specific to arm64, together with a
         MAINTAINERS update for these files (moved to the ARM64 PORT entry).
    
       - Workaround for a Neoverse-N1 erratum where the CPU may fetch stale
         instructions under certain conditions.
    
       - Workaround for Cortex-A57 and A72 errata where the CPU may
         speculatively execute an AT instruction and associate a VMID with
         the wrong guest page tables (corrupting the TLB).
    
       - Perf updates for arm64: additional PMU topologies on HiSilicon
         platforms, support for CCN-512 interconnect, AXI ID filtering in
         the IMX8 DDR PMU, support for the CCPI2 uncore PMU in ThunderX2.
    
       - GICv3 optimisation to avoid a heavy barrier when accessing the
         ICC_PMR_EL1 register.
    
       - ELF HWCAP documentation updates and clean-up.
    
       - SMC calling convention conduit code clean-up.
    
       - KASLR diagnostics printed during boot
    
       - NVIDIA Carmel CPU added to the KPTI whitelist
    
       - Some arm64 mm clean-ups: use generic free_initrd_mem(), remove
         stale macro, simplify calculation in __create_pgd_mapping(), typos.
    
       - Kconfig clean-ups: CMDLINE_FORCE to depend on CMDLINE, choice for
         endinanness to help with allmodconfig"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (93 commits)
      arm64: Kconfig: add a choice for endianness
      kselftest: arm64: fix spelling mistake "contiguos" -> "contiguous"
      arm64: Kconfig: make CMDLINE_FORCE depend on CMDLINE
      MAINTAINERS: Add arm64 selftests to the ARM64 PORT entry
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
      drivers/perf: hisi: update the sccl_id/ccl_id for certain HiSilicon platform
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      ...

commit 46f4f0aabc61bfd365e1eb3c8a6d766d1a49cf32
Merge: 14edff88315a b07a5c53d42a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 10:01:51 2019 +0100

    Merge branch 'kvm-tsx-ctrl' into HEAD
    
    Conflicts:
            arch/x86/kvm/vmx/vmx.c

commit 14edff88315add29099fd8eebb9ef989c2e47c18
Merge: 992edeaefed6 cd7056ae34af
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Nov 21 09:58:35 2019 +0100

    Merge tag 'kvmarm-5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for Linux 5.5:
    
    - Allow non-ISV data aborts to be reported to userspace
    - Allow injection of data aborts from userspace
    - Expose stolen time to guests
    - GICv4 performance improvements
    - vgic ITS emulation fixes
    - Simplify FWB handling
    - Enable halt pool counters
    - Make the emulated timer PREEMPT_RT compliant
    
    Conflicts:
            include/uapi/linux/kvm.h

commit 6be22809e5c8f286877127e8a24c13c959b9fb4e
Merge: 51effa6d1153 478016c3839d e6ea46511b1a bff3b04460a8 7e3a57fa6ca8 83d116c53058 918e1946c8ac 3f484ce3750f 2203e1adb936
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Nov 8 17:46:11 2019 +0000

    Merge branches 'for-next/elf-hwcap-docs', 'for-next/smccc-conduit-cleanup', 'for-next/zone-dma', 'for-next/relax-icc_pmr_el1-sync', 'for-next/double-page-fault', 'for-next/misc', 'for-next/kselftest-arm64-signal' and 'for-next/kaslr-diagnostics' into for-next/core
    
    * for-next/elf-hwcap-docs:
      : Update the arm64 ELF HWCAP documentation
      docs/arm64: cpu-feature-registers: Rewrite bitfields that don't follow [e, s]
      docs/arm64: cpu-feature-registers: Documents missing visible fields
      docs/arm64: elf_hwcaps: Document HWCAP_SB
      docs/arm64: elf_hwcaps: sort the HWCAP{, 2} documentation by ascending value
    
    * for-next/smccc-conduit-cleanup:
      : SMC calling convention conduit clean-up
      firmware: arm_sdei: use common SMCCC_CONDUIT_*
      firmware/psci: use common SMCCC_CONDUIT_*
      arm: spectre-v2: use arm_smccc_1_1_get_conduit()
      arm64: errata: use arm_smccc_1_1_get_conduit()
      arm/arm64: smccc/psci: add arm_smccc_1_1_get_conduit()
    
    * for-next/zone-dma:
      : Reintroduction of ZONE_DMA for Raspberry Pi 4 support
      arm64: mm: reserve CMA and crashkernel in ZONE_DMA32
      dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
      arm64: Make arm64_dma32_phys_limit static
      arm64: mm: Fix unused variable warning in zone_sizes_init
      mm: refresh ZONE_DMA and ZONE_DMA32 comments in 'enum zone_type'
      arm64: use both ZONE_DMA and ZONE_DMA32
      arm64: rename variables used to calculate ZONE_DMA32's size
      arm64: mm: use arm64_dma_phys_limit instead of calling max_zone_dma_phys()
    
    * for-next/relax-icc_pmr_el1-sync:
      : Relax ICC_PMR_EL1 (GICv3) accesses when ICC_CTLR_EL1.PMHE is clear
      arm64: Document ICC_CTLR_EL3.PMHE setting requirements
      arm64: Relax ICC_PMR_EL1 accesses when ICC_CTLR_EL1.PMHE is clear
    
    * for-next/double-page-fault:
      : Avoid a double page fault in __copy_from_user_inatomic() if hw does not support auto Access Flag
      mm: fix double page fault on arm64 if PTE_AF is cleared
      x86/mm: implement arch_faults_on_old_pte() stub on x86
      arm64: mm: implement arch_faults_on_old_pte() on arm64
      arm64: cpufeature: introduce helper cpu_has_hw_af()
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: kpti: Add NVIDIA's Carmel core to the KPTI whitelist
      arm64: mm: Remove MAX_USER_VA_BITS definition
      arm64: mm: simplify the page end calculation in __create_pgd_mapping()
      arm64: print additional fault message when executing non-exec memory
      arm64: psci: Reduce the waiting time for cpu_psci_cpu_kill()
      arm64: pgtable: Correct typo in comment
      arm64: docs: cpu-feature-registers: Document ID_AA64PFR1_EL1
      arm64: cpufeature: Fix typos in comment
      arm64/mm: Poison initmem while freeing with free_reserved_area()
      arm64: use generic free_initrd_mem()
      arm64: simplify syscall wrapper ifdeffery
    
    * for-next/kselftest-arm64-signal:
      : arm64-specific kselftest support with signal-related test-cases
      kselftest: arm64: fake_sigreturn_misaligned_sp
      kselftest: arm64: fake_sigreturn_bad_size
      kselftest: arm64: fake_sigreturn_duplicated_fpsimd
      kselftest: arm64: fake_sigreturn_missing_fpsimd
      kselftest: arm64: fake_sigreturn_bad_size_for_magic0
      kselftest: arm64: fake_sigreturn_bad_magic
      kselftest: arm64: add helper get_current_context
      kselftest: arm64: extend test_init functionalities
      kselftest: arm64: mangle_pstate_invalid_mode_el[123][ht]
      kselftest: arm64: mangle_pstate_invalid_daif_bits
      kselftest: arm64: mangle_pstate_invalid_compat_toggle and common utils
      kselftest: arm64: extend toplevel skeleton Makefile
    
    * for-next/kaslr-diagnostics:
      : Provide diagnostics on boot for KASLR
      arm64: kaslr: Check command line before looking for a seed
      arm64: kaslr: Announce KASLR status on boot

commit 1cf45b8fdbb87040e1d1bd793891089f4678aa41
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Thu Oct 31 14:47:25 2019 -0700

    arm64: apply ARM64_ERRATUM_843419 workaround for Brahma-B53 core
    
    The Broadcom Brahma-B53 core is susceptible to the issue described by
    ARM64_ERRATUM_843419 so this commit enables the workaround to be applied
    when executing on that core.
    
    Since there are now multiple entries to match, we must convert the
    existing ARM64_ERRATUM_843419 into an erratum list and use
    cpucap_multi_entry_cap_matches to match our entries.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a1983c0a872b..93f34b4eca25 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -755,6 +755,23 @@ static const struct midr_range erratum_845719_list[] = {
 };
 #endif
 
+#ifdef CONFIG_ARM64_ERRATUM_843419
+static const struct arm64_cpu_capabilities erratum_843419_list[] = {
+	{
+		/* Cortex-A53 r0p[01234] */
+		.matches = is_affected_midr_range,
+		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 4),
+		MIDR_FIXED(0x4, BIT(8)),
+	},
+	{
+		/* Brahma-B53 r0p[0] */
+		.matches = is_affected_midr_range,
+		ERRATA_MIDR_REV(MIDR_BRAHMA_B53, 0, 0),
+	},
+	{},
+};
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
@@ -786,11 +803,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_843419
 	{
-	/* Cortex-A53 r0p[01234] */
 		.desc = "ARM erratum 843419",
 		.capability = ARM64_WORKAROUND_843419,
-		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 4),
-		MIDR_FIXED(0x4, BIT(8)),
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = cpucap_multi_entry_cap_matches,
+		.match_list = erratum_843419_list,
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_845719

commit e059770cb1cdfbcbe3f1748f76005861cc79dd1a
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Thu Oct 31 14:47:24 2019 -0700

    arm64: Brahma-B53 is SSB and spectre v2 safe
    
    Add the Brahma-B53 CPU (all versions) to the whitelists of CPUs for the
    SSB and spectre v2 mitigations.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index b5eeba7f5d84..a1983c0a872b 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -489,6 +489,7 @@ static const struct midr_range arm64_ssb_cpus[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
+	MIDR_ALL_VERSIONS(MIDR_BRAHMA_B53),
 	{},
 };
 
@@ -573,6 +574,7 @@ static const struct midr_range spectre_v2_safe_list[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
+	MIDR_ALL_VERSIONS(MIDR_BRAHMA_B53),
 	{ /* sentinel */ }
 };
 

commit bfc97f9f199cb041cf897af3af096540948cc705
Author: Doug Berger <opendmb@gmail.com>
Date:   Thu Oct 31 14:47:23 2019 -0700

    arm64: apply ARM64_ERRATUM_845719 workaround for Brahma-B53 core
    
    The Broadcom Brahma-B53 core is susceptible to the issue described by
    ARM64_ERRATUM_845719 so this commit enables the workaround to be applied
    when executing on that core.
    
    Since there are now multiple entries to match, we must convert the
    existing ARM64_ERRATUM_845719 into an erratum list.
    
    Signed-off-by: Doug Berger <opendmb@gmail.com>
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 091e3ec0f420..b5eeba7f5d84 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -743,6 +743,16 @@ static const struct midr_range erratum_1418040_list[] = {
 };
 #endif
 
+#ifdef CONFIG_ARM64_ERRATUM_845719
+static const struct midr_range erratum_845719_list[] = {
+	/* Cortex-A53 r0p[01234] */
+	MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 4),
+	/* Brahma-B53 r0p[0] */
+	MIDR_REV(MIDR_BRAHMA_B53, 0, 0),
+	{},
+};
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
@@ -783,10 +793,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_845719
 	{
-	/* Cortex-A53 r0p[01234] */
 		.desc = "ARM erratum 845719",
 		.capability = ARM64_WORKAROUND_845719,
-		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 4),
+		ERRATA_MIDR_RANGE_LIST(erratum_845719_list),
 	},
 #endif
 #ifdef CONFIG_CAVIUM_ERRATUM_23154

commit 36c602dcdd872e9f9b91aae5266b6d7d72b69b96
Author: Bjorn Andersson <bjorn.andersson@linaro.org>
Date:   Tue Oct 29 16:27:38 2019 -0700

    arm64: cpufeature: Enable Qualcomm Falkor errata 1009 for Kryo
    
    The Kryo cores share errata 1009 with Falkor, so add their model
    definitions and enable it for them as well.
    
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    [will: Update entry in silicon-errata.rst]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 7f9b699969c7..091e3ec0f420 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -659,17 +659,23 @@ static const struct midr_range arm64_harden_el2_vectors[] = {
 #endif
 
 #ifdef CONFIG_ARM64_WORKAROUND_REPEAT_TLBI
-
-static const struct midr_range arm64_repeat_tlbi_cpus[] = {
+static const struct arm64_cpu_capabilities arm64_repeat_tlbi_list[] = {
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009
-	MIDR_RANGE(MIDR_QCOM_FALKOR_V1, 0, 0, 0, 0),
+	{
+		ERRATA_MIDR_REV(MIDR_QCOM_FALKOR_V1, 0, 0)
+	},
+	{
+		.midr_range.model = MIDR_QCOM_KRYO,
+		.matches = is_kryo_midr,
+	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_1286807
-	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 0),
+	{
+		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 0),
+	},
 #endif
 	{},
 };
-
 #endif
 
 #ifdef CONFIG_CAVIUM_ERRATUM_27456
@@ -825,7 +831,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm erratum 1009, ARM erratum 1286807",
 		.capability = ARM64_WORKAROUND_REPEAT_TLBI,
-		ERRATA_MIDR_RANGE_LIST(arm64_repeat_tlbi_cpus),
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = cpucap_multi_entry_cap_matches,
+		.match_list = arm64_repeat_tlbi_list,
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_858921

commit d4af3c4b81f4cd5662baa6f1492f998d89783318
Author: Bjorn Andersson <bjorn.andersson@linaro.org>
Date:   Tue Oct 29 10:15:39 2019 -0700

    arm64: cpufeature: Enable Qualcomm Falkor/Kryo errata 1003
    
    With the introduction of 'cce360b54ce6 ("arm64: capabilities: Filter the
    entries based on a given mask")' the Qualcomm Falkor/Kryo errata 1003 is
    no long applied.
    
    The result of not applying errata 1003 is that MSM8996 runs into various
    RCU stalls and fails to boot most of the times.
    
    Give 1003 a "type" to ensure they are not filtered out in
    update_cpu_capabilities().
    
    Fixes: cce360b54ce6 ("arm64: capabilities: Filter the entries based on a given mask")
    Cc: stable@vger.kernel.org
    Reported-by: Mark Brown <broonie@kernel.org>
    Suggested-by: Will Deacon <will@kernel.org>
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 6c3b10a41bd8..7f9b699969c7 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -816,6 +816,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm Technologies Falkor/Kryo erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = cpucap_multi_entry_cap_matches,
 		.match_list = qcom_erratum_1003_list,
 	},

commit 346f6a4636f64c19a27722cf6ec93b38bb4251d4
Merge: 6a036afb5511 c2cc62d83186
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 16:22:49 2019 +0000

    Merge branch 'kvm-arm64/erratum-1319367' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms into for-next/core
    
    Similarly to erratum 1165522 that affects Cortex-A76, A57 and A72
    respectively suffer from errata 1319537 and 1319367, potentially
    resulting in TLB corruption if the CPU speculates an AT instruction
    while switching guests.
    
    The fix is slightly more involved since we don't have VHE to help us
    here, but the idea is the same: when switching a guest in, we must
    prevent any speculated AT from being able to parse the page tables
    until S2 is up and running. Only at this stage can we allow AT to take
    place.
    
    For this, we always restore the guest sysregs first, except for its
    SCTLR and TCR registers, which must be set with SCTLR.M=1 and
    TCR.EPD{0,1} = {1, 1}, effectively disabling the PTW and TLB
    allocation. Once S2 is setup, we restore the guest's SCTLR and
    TCR. Similar things must be done on TLB invalidation...
    
    * 'kvm-arm64/erratum-1319367' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms:
      arm64: Enable and document ARM errata 1319367 and 1319537
      arm64: KVM: Prevent speculative S1 PTW when restoring vcpu context
      arm64: KVM: Disable EL1 PTW when invalidating S2 TLBs
      arm64: KVM: Reorder system register restoration and stage-2 activation
      arm64: Add ARM64_WORKAROUND_1319367 for all A57 and A72 versions

commit 6a036afb5511418995359a9131dbea276758bd10
Merge: ba95e9bd9637 27a22fbdeedd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 16:12:40 2019 +0000

    Merge branch 'for-next/neoverse-n1-stale-instr' into for-next/core
    
    Neoverse-N1 cores with the 'COHERENT_ICACHE' feature may fetch stale
    instructions when software depends on prefetch-speculation-protection
    instead of explicit synchronization. [0]
    
    The workaround is to trap I-Cache maintenance and issue an
    inner-shareable TLBI. The affected cores have a Coherent I-Cache, so the
    I-Cache maintenance isn't necessary. The core tells user-space it can
    skip it with CTR_EL0.DIC. We also have to trap this register to hide the
    bit forcing DIC-aware user-space to perform the maintenance.
    
    To avoid trapping all cache-maintenance, this workaround depends on
    a firmware component that only traps I-cache maintenance from EL0 and
    performs the workaround.
    
    For user-space, the kernel's work is to trap CTR_EL0 to hide DIC, and
    produce a fake IminLine. EL3 traps the now-necessary I-Cache maintenance
    and performs the inner-shareable-TLBI that makes everything better.
    
    [0] https://developer.arm.com/docs/sden885747/latest/arm-neoverse-n1-mp050-software-developer-errata-notice
    
    * for-next/neoverse-n1-stale-instr:
      arm64: Silence clang warning on mismatched value/register sizes
      arm64: compat: Workaround Neoverse-N1 #1542419 for compat user-space
      arm64: Fake the IminLine size on systems affected by Neoverse-N1 #1542419
      arm64: errata: Hide CTR_EL0.DIC on systems affected by Neoverse-N1 #1542419

commit 05460849c3b51180d5ada3373d0449aea19075e4
Author: James Morse <james.morse@arm.com>
Date:   Thu Oct 17 18:42:58 2019 +0100

    arm64: errata: Hide CTR_EL0.DIC on systems affected by Neoverse-N1 #1542419
    
    Cores affected by Neoverse-N1 #1542419 could execute a stale instruction
    when a branch is updated to point to freshly generated instructions.
    
    To workaround this issue we need user-space to issue unnecessary
    icache maintenance that we can trap. Start by hiding CTR_EL0.DIC.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index f593f4cffc0d..3ae9b78b6d94 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -87,13 +87,21 @@ has_mismatched_cache_type(const struct arm64_cpu_capabilities *entry,
 }
 
 static void
-cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *__unused)
+cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *cap)
 {
 	u64 mask = arm64_ftr_reg_ctrel0.strict_mask;
+	bool enable_uct_trap = false;
 
 	/* Trap CTR_EL0 access on this CPU, only if it has a mismatch */
 	if ((read_cpuid_cachetype() & mask) !=
 	    (arm64_ftr_reg_ctrel0.sys_val & mask))
+		enable_uct_trap = true;
+
+	/* ... or if the system is affected by an erratum */
+	if (cap->capability == ARM64_WORKAROUND_1542419)
+		enable_uct_trap = true;
+
+	if (enable_uct_trap)
 		sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
 }
 
@@ -623,6 +631,18 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	return (need_wa > 0);
 }
 
+static bool __maybe_unused
+has_neoverse_n1_erratum_1542419(const struct arm64_cpu_capabilities *entry,
+				int scope)
+{
+	u32 midr = read_cpuid_id();
+	bool has_dic = read_cpuid_cachetype() & BIT(CTR_DIC_SHIFT);
+	const struct midr_range range = MIDR_ALL_VERSIONS(MIDR_NEOVERSE_N1);
+
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+	return is_midr_in_range(midr, &range) && has_dic;
+}
+
 #ifdef CONFIG_HARDEN_EL2_VECTORS
 
 static const struct midr_range arm64_harden_el2_vectors[] = {
@@ -851,6 +871,16 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_cortex_a76_erratum_1463225,
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1542419
+	{
+		/* we depend on the firmware portion for correctness */
+		.desc = "ARM erratum 1542419 (kernel portion)",
+		.capability = ARM64_WORKAROUND_1542419,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = has_neoverse_n1_erratum_1542419,
+		.cpu_enable = cpu_enable_trap_ctr_access,
+	},
 #endif
 	{
 	}

commit ce4d5ca2b9dd5d85944eb93c1bbf9eb11b7a907d
Author: Steven Price <steven.price@arm.com>
Date:   Mon Oct 21 16:28:22 2019 +0100

    arm/arm64: Make use of the SMCCC 1.1 wrapper
    
    Rather than directly choosing which function to use based on
    psci_ops.conduit, use the new arm_smccc_1_1 wrapper instead.
    
    In some cases we still need to do some operations based on the
    conduit, but the code duplication is removed.
    
    No functional change.
    
    Signed-off-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9c0b011eee20..401246e095e7 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -209,40 +209,31 @@ static int detect_harden_bp_fw(void)
 	struct arm_smccc_res res;
 	u32 midr = read_cpuid_id();
 
+	arm_smccc_1_1_invoke(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
+			     ARM_SMCCC_ARCH_WORKAROUND_1, &res);
+
+	switch ((int)res.a0) {
+	case 1:
+		/* Firmware says we're just fine */
+		return 0;
+	case 0:
+		break;
+	default:
+		return -1;
+	}
+
 	switch (arm_smccc_1_1_get_conduit()) {
 	case SMCCC_CONDUIT_HVC:
-		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
-				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
-		switch ((int)res.a0) {
-		case 1:
-			/* Firmware says we're just fine */
-			return 0;
-		case 0:
-			cb = call_hvc_arch_workaround_1;
-			/* This is a guest, no need to patch KVM vectors */
-			smccc_start = NULL;
-			smccc_end = NULL;
-			break;
-		default:
-			return -1;
-		}
+		cb = call_hvc_arch_workaround_1;
+		/* This is a guest, no need to patch KVM vectors */
+		smccc_start = NULL;
+		smccc_end = NULL;
 		break;
 
 	case SMCCC_CONDUIT_SMC:
-		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
-				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
-		switch ((int)res.a0) {
-		case 1:
-			/* Firmware says we're just fine */
-			return 0;
-		case 0:
-			cb = call_smc_arch_workaround_1;
-			smccc_start = __smccc_workaround_1_smc_start;
-			smccc_end = __smccc_workaround_1_smc_end;
-			break;
-		default:
-			return -1;
-		}
+		cb = call_smc_arch_workaround_1;
+		smccc_start = __smccc_workaround_1_smc_start;
+		smccc_end = __smccc_workaround_1_smc_end;
 		break;
 
 	default:
@@ -332,6 +323,8 @@ void __init arm64_enable_wa2_handling(struct alt_instr *alt,
 
 void arm64_set_ssbd_mitigation(bool state)
 {
+	int conduit;
+
 	if (!IS_ENABLED(CONFIG_ARM64_SSBD)) {
 		pr_info_once("SSBD disabled by kernel configuration\n");
 		return;
@@ -345,19 +338,10 @@ void arm64_set_ssbd_mitigation(bool state)
 		return;
 	}
 
-	switch (arm_smccc_1_1_get_conduit()) {
-	case SMCCC_CONDUIT_HVC:
-		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
-		break;
-
-	case SMCCC_CONDUIT_SMC:
-		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
-		break;
+	conduit = arm_smccc_1_1_invoke(ARM_SMCCC_ARCH_WORKAROUND_2, state,
+				       NULL);
 
-	default:
-		WARN_ON_ONCE(1);
-		break;
-	}
+	WARN_ON_ONCE(conduit == SMCCC_CONDUIT_NONE);
 }
 
 static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
@@ -367,6 +351,7 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 	bool required = true;
 	s32 val;
 	bool this_cpu_safe = false;
+	int conduit;
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 
@@ -384,18 +369,10 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 		goto out_printmsg;
 	}
 
-	switch (arm_smccc_1_1_get_conduit()) {
-	case SMCCC_CONDUIT_HVC:
-		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
-				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
-		break;
+	conduit = arm_smccc_1_1_invoke(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
+				       ARM_SMCCC_ARCH_WORKAROUND_2, &res);
 
-	case SMCCC_CONDUIT_SMC:
-		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
-				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
-		break;
-
-	default:
+	if (conduit == SMCCC_CONDUIT_NONE) {
 		ssbd_state = ARM64_SSBD_UNKNOWN;
 		if (!this_cpu_safe)
 			__ssb_safe = false;

commit f75e2294a4415621b223150065c8d1e823896da5
Author: Marc Zyngier <maz@kernel.org>
Date:   Fri Nov 23 17:25:52 2018 +0000

    arm64: Add ARM64_WORKAROUND_1319367 for all A57 and A72 versions
    
    Rework the EL2 vector hardening that is only selected for A57 and A72
    so that the table can also be used for ARM64_WORKAROUND_1319367.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index f593f4cffc0d..a7afa6d4a58f 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -623,9 +623,9 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	return (need_wa > 0);
 }
 
-#ifdef CONFIG_HARDEN_EL2_VECTORS
+#if defined(CONFIG_HARDEN_EL2_VECTORS) || defined(CONFIG_ARM64_ERRATUM_1319367)
 
-static const struct midr_range arm64_harden_el2_vectors[] = {
+static const struct midr_range ca57_a72[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
 	{},
@@ -819,7 +819,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "EL2 vector hardening",
 		.capability = ARM64_HARDEN_EL2_VECTORS,
-		ERRATA_MIDR_RANGE_LIST(arm64_harden_el2_vectors),
+		ERRATA_MIDR_RANGE_LIST(ca57_a72),
 	},
 #endif
 	{
@@ -851,6 +851,13 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_cortex_a76_erratum_1463225,
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1319367
+	{
+		.desc = "ARM erratum 1319367",
+		.capability = ARM64_WORKAROUND_1319367,
+		ERRATA_MIDR_RANGE_LIST(ca57_a72),
+	},
 #endif
 	{
 	}

commit 777d062e5bee0e3c0751cdcbce116a76ee2310ec
Merge: 597399d0cb91 603afdc9438a
Author: Will Deacon <will@kernel.org>
Date:   Thu Oct 17 13:42:42 2019 -0700

    Merge branch 'errata/tx2-219' into for-next/fixes
    
    Workaround for Cavium/Marvell ThunderX2 erratum #219.
    
    * errata/tx2-219:
      arm64: Allow CAVIUM_TX2_ERRATUM_219 to be selected
      arm64: Avoid Cavium TX2 erratum 219 when switching TTBR
      arm64: Enable workaround for Cavium TX2 erratum 219 when running SMT
      arm64: KVM: Trap VM ops when ARM64_WORKAROUND_CAVIUM_TX2_219_TVM is set

commit c98bd29917281a5023f71a3148f538ad2709c0f0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Aug 9 14:22:41 2019 +0100

    arm64: errata: use arm_smccc_1_1_get_conduit()
    
    Now that we have arm_smccc_1_1_get_conduit(), we can hide the PSCI
    implementation details from the arm64 cpu errata code, so let's do so.
    
    As arm_smccc_1_1_get_conduit() implicitly checks that the SMCCC version
    is at least SMCCC_VERSION_1_1, we no longer need to check this
    explicitly where switch statements have a default case, e.g. in
    has_ssbd_mitigation().
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index f593f4cffc0d..9c0b011eee20 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -6,7 +6,6 @@
  */
 
 #include <linux/arm-smccc.h>
-#include <linux/psci.h>
 #include <linux/types.h>
 #include <linux/cpu.h>
 #include <asm/cpu.h>
@@ -166,9 +165,7 @@ static void install_bp_hardening_cb(bp_hardening_cb_t fn,
 }
 #endif	/* CONFIG_KVM_INDIRECT_VECTORS */
 
-#include <uapi/linux/psci.h>
 #include <linux/arm-smccc.h>
-#include <linux/psci.h>
 
 static void call_smc_arch_workaround_1(void)
 {
@@ -212,11 +209,8 @@ static int detect_harden_bp_fw(void)
 	struct arm_smccc_res res;
 	u32 midr = read_cpuid_id();
 
-	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
-		return -1;
-
-	switch (psci_ops.conduit) {
-	case PSCI_CONDUIT_HVC:
+	switch (arm_smccc_1_1_get_conduit()) {
+	case SMCCC_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		switch ((int)res.a0) {
@@ -234,7 +228,7 @@ static int detect_harden_bp_fw(void)
 		}
 		break;
 
-	case PSCI_CONDUIT_SMC:
+	case SMCCC_CONDUIT_SMC:
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		switch ((int)res.a0) {
@@ -308,11 +302,11 @@ void __init arm64_update_smccc_conduit(struct alt_instr *alt,
 
 	BUG_ON(nr_inst != 1);
 
-	switch (psci_ops.conduit) {
-	case PSCI_CONDUIT_HVC:
+	switch (arm_smccc_1_1_get_conduit()) {
+	case SMCCC_CONDUIT_HVC:
 		insn = aarch64_insn_get_hvc_value();
 		break;
-	case PSCI_CONDUIT_SMC:
+	case SMCCC_CONDUIT_SMC:
 		insn = aarch64_insn_get_smc_value();
 		break;
 	default:
@@ -351,12 +345,12 @@ void arm64_set_ssbd_mitigation(bool state)
 		return;
 	}
 
-	switch (psci_ops.conduit) {
-	case PSCI_CONDUIT_HVC:
+	switch (arm_smccc_1_1_get_conduit()) {
+	case SMCCC_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
 		break;
 
-	case PSCI_CONDUIT_SMC:
+	case SMCCC_CONDUIT_SMC:
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
 		break;
 
@@ -390,20 +384,13 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 		goto out_printmsg;
 	}
 
-	if (psci_ops.smccc_version == SMCCC_VERSION_1_0) {
-		ssbd_state = ARM64_SSBD_UNKNOWN;
-		if (!this_cpu_safe)
-			__ssb_safe = false;
-		return false;
-	}
-
-	switch (psci_ops.conduit) {
-	case PSCI_CONDUIT_HVC:
+	switch (arm_smccc_1_1_get_conduit()) {
+	case SMCCC_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
 		break;
 
-	case PSCI_CONDUIT_SMC:
+	case SMCCC_CONDUIT_SMC:
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
 		break;

commit 9405447ef79bc93101373e130f72e9e6cbf17dbb
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 9 16:22:24 2019 +0100

    arm64: Avoid Cavium TX2 erratum 219 when switching TTBR
    
    As a PRFM instruction racing against a TTBR update can have undesirable
    effects on TX2, NOP-out such PRFM on cores that are affected by
    the TX2-219 erratum.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index d999ca2dd760..a19bb3e4bcfb 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -884,6 +884,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		ERRATA_MIDR_RANGE_LIST(tx2_family_cpus),
 		.matches = needs_tx2_tvm_workaround,
 	},
+	{
+		.desc = "Cavium ThunderX2 erratum 219 (PRFM removal)",
+		.capability = ARM64_WORKAROUND_CAVIUM_TX2_219_PRFM,
+		ERRATA_MIDR_RANGE_LIST(tx2_family_cpus),
+	},
 #endif
 	{
 	}

commit 93916beb70143c46bf1d2bacf814be3a124b253b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 9 16:26:21 2019 +0100

    arm64: Enable workaround for Cavium TX2 erratum 219 when running SMT
    
    It appears that the only case where we need to apply the TX2_219_TVM
    mitigation is when the core is in SMT mode. So let's condition the
    enabling on detecting a CPU whose MPIDR_EL1.Aff0 is non-zero.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 1e43ba5c79b7..d999ca2dd760 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -12,6 +12,7 @@
 #include <asm/cpu.h>
 #include <asm/cputype.h>
 #include <asm/cpufeature.h>
+#include <asm/smp_plat.h>
 
 static bool __maybe_unused
 is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
@@ -623,6 +624,30 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	return (need_wa > 0);
 }
 
+static const __maybe_unused struct midr_range tx2_family_cpus[] = {
+	MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
+	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
+	{},
+};
+
+static bool __maybe_unused
+needs_tx2_tvm_workaround(const struct arm64_cpu_capabilities *entry,
+			 int scope)
+{
+	int i;
+
+	if (!is_affected_midr_range_list(entry, scope) ||
+	    !is_hyp_mode_available())
+		return false;
+
+	for_each_possible_cpu(i) {
+		if (MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 0) != 0)
+			return true;
+	}
+
+	return false;
+}
+
 #ifdef CONFIG_HARDEN_EL2_VECTORS
 
 static const struct midr_range arm64_harden_el2_vectors[] = {
@@ -851,6 +876,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_cortex_a76_erratum_1463225,
 	},
+#endif
+#ifdef CONFIG_CAVIUM_TX2_ERRATUM_219
+	{
+		.desc = "Cavium ThunderX2 erratum 219 (KVM guest sysreg trapping)",
+		.capability = ARM64_WORKAROUND_CAVIUM_TX2_219_TVM,
+		ERRATA_MIDR_RANGE_LIST(tx2_family_cpus),
+		.matches = needs_tx2_tvm_workaround,
+	},
 #endif
 	{
 	}

commit 7a292b6c7c9c35afee01ce3b2248f705869d0ff1
Author: Thierry Reding <treding@nvidia.com>
Date:   Mon Sep 23 11:12:29 2019 +0200

    arm64: errata: Update stale comment
    
    Commit 73f381660959 ("arm64: Advertise mitigation of Spectre-v2, or lack
    thereof") renamed the caller of the install_bp_hardening_cb() function
    but forgot to update a comment, which can be confusing when trying to
    follow the code flow.
    
    Fixes: 73f381660959 ("arm64: Advertise mitigation of Spectre-v2, or lack thereof")
    Signed-off-by: Thierry Reding <treding@nvidia.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 1e43ba5c79b7..f593f4cffc0d 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -128,8 +128,8 @@ static void install_bp_hardening_cb(bp_hardening_cb_t fn,
 	int cpu, slot = -1;
 
 	/*
-	 * enable_smccc_arch_workaround_1() passes NULL for the hyp_vecs
-	 * start/end if we're a guest. Skip the hyp-vectors work.
+	 * detect_harden_bp_fw() passes NULL for the hyp_vecs start/end if
+	 * we're a guest. Skip the hyp-vectors work.
 	 */
 	if (!hyp_vecs_start) {
 		__this_cpu_write(bp_hardening_data.fn, fn);

commit c118bbb52743df70e6297671606c1c08edc659fe
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri May 3 15:27:48 2019 +0100

    arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
    
    Recent commits added the explicit notion of "workaround not required" to
    the state of the Spectre v2 (aka. BP_HARDENING) workaround, where we
    just had "needed" and "unknown" before.
    
    Export this knowledge to the rest of the kernel and enhance the existing
    kvm_arm_harden_branch_predictor() to report this new state as well.
    Export this new state to guests when they use KVM's firmware interface
    emulation.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Steven Price <steven.price@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index ca11ff7bf55e..1e43ba5c79b7 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -554,6 +554,17 @@ cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 static bool __hardenbp_enab = true;
 static bool __spectrev2_safe = true;
 
+int get_spectre_v2_workaround_state(void)
+{
+	if (__spectrev2_safe)
+		return ARM64_BP_HARDEN_NOT_REQUIRED;
+
+	if (!__hardenbp_enab)
+		return ARM64_BP_HARDEN_UNKNOWN;
+
+	return ARM64_BP_HARDEN_WA_NEEDED;
+}
+
 /*
  * List of CPUs that do not need any Spectre-v2 mitigation at all.
  */
@@ -854,13 +865,15 @@ ssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr,
 ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr,
 		char *buf)
 {
-	if (__spectrev2_safe)
+	switch (get_spectre_v2_workaround_state()) {
+	case ARM64_BP_HARDEN_NOT_REQUIRED:
 		return sprintf(buf, "Not affected\n");
-
-	if (__hardenbp_enab)
+        case ARM64_BP_HARDEN_WA_NEEDED:
 		return sprintf(buf, "Mitigation: Branch predictor hardening\n");
-
-	return sprintf(buf, "Vulnerable\n");
+        case ARM64_BP_HARDEN_UNKNOWN:
+	default:
+		return sprintf(buf, "Vulnerable\n");
+	}
 }
 
 ssize_t cpu_show_spec_store_bypass(struct device *dev,

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index d61beedba101..ca11ff7bf55e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -1,19 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Contains CPU specific errata definitions
  *
  * Copyright (C) 2014 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/arm-smccc.h>

commit a5325089bd05a7b0259cc4038479d36308edbda2
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu May 23 11:24:50 2019 +0100

    arm64: Handle erratum 1418040 as a superset of erratum 1188873
    
    We already mitigate erratum 1188873 affecting Cortex-A76 and
    Neoverse-N1 r0p0 to r2p0. It turns out that revisions r0p0 to
    r3p1 of the same cores are affected by erratum 1418040, which
    has the same workaround as 1188873.
    
    Let's expand the range of affected revisions to match 1418040,
    and repaint all occurences of 1188873 to 1418040. Whilst we're
    there, do a bit of reformating in silicon-errata.txt and drop
    a now unnecessary dependency on ARM_ARCH_TIMER_OOL_WORKAROUND.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index ac6432bfc1e4..d61beedba101 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -698,12 +698,16 @@ static const struct midr_range workaround_clean_cache[] = {
 };
 #endif
 
-#ifdef CONFIG_ARM64_ERRATUM_1188873
-static const struct midr_range erratum_1188873_list[] = {
-	/* Cortex-A76 r0p0 to r2p0 */
-	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
-	/* Neoverse-N1 r0p0 to r2p0 */
-	MIDR_RANGE(MIDR_NEOVERSE_N1, 0, 0, 2, 0),
+#ifdef CONFIG_ARM64_ERRATUM_1418040
+/*
+ * - 1188873 affects r0p0 to r2p0
+ * - 1418040 affects r0p0 to r3p1
+ */
+static const struct midr_range erratum_1418040_list[] = {
+	/* Cortex-A76 r0p0 to r3p1 */
+	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 1),
+	/* Neoverse-N1 r0p0 to r3p1 */
+	MIDR_RANGE(MIDR_NEOVERSE_N1, 0, 0, 3, 1),
 	{},
 };
 #endif
@@ -825,11 +829,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.matches = has_ssbd_mitigation,
 		.midr_range_list = arm64_ssb_cpus,
 	},
-#ifdef CONFIG_ARM64_ERRATUM_1188873
+#ifdef CONFIG_ARM64_ERRATUM_1418040
 	{
-		.desc = "ARM erratum 1188873",
-		.capability = ARM64_WORKAROUND_1188873,
-		ERRATA_MIDR_RANGE_LIST(erratum_1188873_list),
+		.desc = "ARM erratum 1418040",
+		.capability = ARM64_WORKAROUND_1418040,
+		ERRATA_MIDR_RANGE_LIST(erratum_1418040_list),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_1165522

commit 969f5ea627570e91c9d54403287ee3ed657f58fe
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 29 13:03:57 2019 +0100

    arm64: errata: Add workaround for Cortex-A76 erratum #1463225
    
    Revisions of the Cortex-A76 CPU prior to r4p0 are affected by an erratum
    that can prevent interrupts from being taken when single-stepping.
    
    This patch implements a software workaround to prevent userspace from
    effectively being able to disable interrupts.
    
    Cc: <stable@vger.kernel.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index e88d4e7bdfc7..ac6432bfc1e4 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -502,6 +502,22 @@ static const struct midr_range arm64_ssb_cpus[] = {
 	{},
 };
 
+#ifdef CONFIG_ARM64_ERRATUM_1463225
+DEFINE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+
+static bool
+has_cortex_a76_erratum_1463225(const struct arm64_cpu_capabilities *entry,
+			       int scope)
+{
+	u32 midr = read_cpuid_id();
+	/* Cortex-A76 r0p0 - r3p1 */
+	struct midr_range range = MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 1);
+
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+	return is_midr_in_range(midr, &range) && is_kernel_in_hyp_mode();
+}
+#endif
+
 static void __maybe_unused
 cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 {
@@ -823,6 +839,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_1165522,
 		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1463225
+	{
+		.desc = "ARM erratum 1463225",
+		.capability = ARM64_WORKAROUND_1463225,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = has_cortex_a76_erratum_1463225,
+	},
 #endif
 	{
 	}

commit 24cf262da1ad303fc940c798aab0bd1bd50e3fc2
Merge: 50abbe19623e 0ea415390cd3
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed May 1 15:45:36 2019 +0100

    Merge branch 'for-next/timers' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into for-next/core
    
    Conflicts:
            arch/arm64/Kconfig
            arch/arm64/include/asm/arch_timer.h

commit a111b7c0f20e13b54df2fa959b3dc0bdf1925ae6
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Apr 12 15:39:32 2019 -0500

    arm64/speculation: Support 'mitigations=' cmdline option
    
    Configure arm64 runtime CPU speculation bug mitigations in accordance
    with the 'mitigations=' cmdline option.  This affects Meltdown, Spectre
    v2, and Speculative Store Bypass.
    
    The default behavior is unchanged.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    [will: reorder checks so KASLR implies KPTI and SSBS is affected by cmdline]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 44ef98be001e..1b9ce0fdd81d 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -19,6 +19,7 @@
 #include <linux/arm-smccc.h>
 #include <linux/psci.h>
 #include <linux/types.h>
+#include <linux/cpu.h>
 #include <asm/cpu.h>
 #include <asm/cputype.h>
 #include <asm/cpufeature.h>
@@ -386,6 +387,9 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 
+	if (cpu_mitigations_off())
+		ssbd_state = ARM64_SSBD_FORCE_DISABLE;
+
 	/* delay setting __ssb_safe until we get a firmware response */
 	if (is_midr_in_range_list(read_cpuid_id(), entry->midr_range_list))
 		this_cpu_safe = true;
@@ -589,7 +593,7 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	}
 
 	/* forced off */
-	if (__nospectre_v2) {
+	if (__nospectre_v2 || cpu_mitigations_off()) {
 		pr_info_once("spectrev2 mitigation disabled by command line option\n");
 		__hardenbp_enab = false;
 		return false;

commit eb337cdfcd5dd3b10522c2f34140a73a4c285c30
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Apr 30 16:58:56 2019 +0100

    arm64: ssbs: Don't treat CPUs with SSBS as unaffected by SSB
    
    SSBS provides a relatively cheap mitigation for SSB, but it is still a
    mitigation and its presence does not indicate that the CPU is unaffected
    by the vulnerability.
    
    Tweak the mitigation logic so that we report the correct string in sysfs.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 4bb0f7cad418..44ef98be001e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -386,15 +386,17 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 
+	/* delay setting __ssb_safe until we get a firmware response */
+	if (is_midr_in_range_list(read_cpuid_id(), entry->midr_range_list))
+		this_cpu_safe = true;
+
 	if (this_cpu_has_cap(ARM64_SSBS)) {
+		if (!this_cpu_safe)
+			__ssb_safe = false;
 		required = false;
 		goto out_printmsg;
 	}
 
-	/* delay setting __ssb_safe until we get a firmware response */
-	if (is_midr_in_range_list(read_cpuid_id(), entry->midr_range_list))
-		this_cpu_safe = true;
-
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0) {
 		ssbd_state = ARM64_SSBD_UNKNOWN;
 		if (!this_cpu_safe)

commit 526e065dbca6df0b5a130b84b836b8b3c9f54e21
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Mon Apr 15 16:21:28 2019 -0500

    arm64: add sysfs vulnerability show for speculative store bypass
    
    Return status based on ssbd_state and __ssb_safe. If the
    mitigation is disabled, or the firmware isn't responding then
    return the expected machine state based on a whitelist of known
    good cores.
    
    Given a heterogeneous machine, the overall machine vulnerability
    defaults to safe but is reset to unsafe when we miss the whitelist
    and the firmware doesn't explicitly tell us the core is safe.
    In order to make that work we delay transitioning to vulnerable
    until we know the firmware isn't responding to avoid a case
    where we miss the whitelist, but the firmware goes ahead and
    reports the core is not vulnerable. If all the cores in the
    machine have SSBS, then __ssb_safe will remain true.
    
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index b6132783e8cf..4bb0f7cad418 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -278,6 +278,7 @@ static int detect_harden_bp_fw(void)
 DEFINE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
 
 int ssbd_state __read_mostly = ARM64_SSBD_KERNEL;
+static bool __ssb_safe = true;
 
 static const struct ssbd_options {
 	const char	*str;
@@ -381,6 +382,7 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 	struct arm_smccc_res res;
 	bool required = true;
 	s32 val;
+	bool this_cpu_safe = false;
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 
@@ -389,8 +391,14 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 		goto out_printmsg;
 	}
 
+	/* delay setting __ssb_safe until we get a firmware response */
+	if (is_midr_in_range_list(read_cpuid_id(), entry->midr_range_list))
+		this_cpu_safe = true;
+
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0) {
 		ssbd_state = ARM64_SSBD_UNKNOWN;
+		if (!this_cpu_safe)
+			__ssb_safe = false;
 		return false;
 	}
 
@@ -407,6 +415,8 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	default:
 		ssbd_state = ARM64_SSBD_UNKNOWN;
+		if (!this_cpu_safe)
+			__ssb_safe = false;
 		return false;
 	}
 
@@ -415,14 +425,18 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 	switch (val) {
 	case SMCCC_RET_NOT_SUPPORTED:
 		ssbd_state = ARM64_SSBD_UNKNOWN;
+		if (!this_cpu_safe)
+			__ssb_safe = false;
 		return false;
 
+	/* machines with mixed mitigation requirements must not return this */
 	case SMCCC_RET_NOT_REQUIRED:
 		pr_info_once("%s mitigation not required\n", entry->desc);
 		ssbd_state = ARM64_SSBD_MITIGATED;
 		return false;
 
 	case SMCCC_RET_SUCCESS:
+		__ssb_safe = false;
 		required = true;
 		break;
 
@@ -432,6 +446,8 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	default:
 		WARN_ON(1);
+		if (!this_cpu_safe)
+			__ssb_safe = false;
 		return false;
 	}
 
@@ -472,6 +488,14 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 	return required;
 }
 
+/* known invulnerable cores */
+static const struct midr_range arm64_ssb_cpus[] = {
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
+	{},
+};
+
 static void __maybe_unused
 cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 {
@@ -767,6 +791,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_SSBD,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_ssbd_mitigation,
+		.midr_range_list = arm64_ssb_cpus,
 	},
 #ifdef CONFIG_ARM64_ERRATUM_1188873
 	{
@@ -805,3 +830,20 @@ ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr,
 
 	return sprintf(buf, "Vulnerable\n");
 }
+
+ssize_t cpu_show_spec_store_bypass(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	if (__ssb_safe)
+		return sprintf(buf, "Not affected\n");
+
+	switch (ssbd_state) {
+	case ARM64_SSBD_KERNEL:
+	case ARM64_SSBD_FORCE_ENABLE:
+		if (IS_ENABLED(CONFIG_ARM64_SSBD))
+			return sprintf(buf,
+			    "Mitigation: Speculative Store Bypass disabled via prctl\n");
+	}
+
+	return sprintf(buf, "Vulnerable\n");
+}

commit 6989303a3b2d864fd8e17d3fa3365d3e9649a598
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 15 13:03:54 2019 +0100

    arm64: Apply ARM64_ERRATUM_1188873 to Neoverse-N1
    
    Neoverse-N1 is also affected by ARM64_ERRATUM_1188873, so let's
    add it to the list of affected CPUs.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    [will: Update silicon-errata.txt]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9950bb0cbd52..06f1c8aae1dc 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -603,6 +603,16 @@ static const struct midr_range workaround_clean_cache[] = {
 };
 #endif
 
+#ifdef CONFIG_ARM64_ERRATUM_1188873
+static const struct midr_range erratum_1188873_list[] = {
+	/* Cortex-A76 r0p0 to r2p0 */
+	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+	/* Neoverse-N1 r0p0 to r2p0 */
+	MIDR_RANGE(MIDR_NEOVERSE_N1, 0, 0, 2, 0),
+	{},
+};
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
@@ -725,10 +735,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_1188873
 	{
-		/* Cortex-A76 r0p0 to r2p0 */
 		.desc = "ARM erratum 1188873",
 		.capability = ARM64_WORKAROUND_1188873,
-		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+		ERRATA_MIDR_RANGE_LIST(erratum_1188873_list),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_1165522

commit d42281b6e49510f078ace15a8ea10f71e6262581
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Mon Apr 15 16:21:27 2019 -0500

    arm64: Always enable ssb vulnerability detection
    
    Ensure we are always able to detect whether or not the CPU is affected
    by SSB, so that we can later advertise this to userspace.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    [will: Use IS_ENABLED instead of #ifdef]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index d2bbafa04b3c..b6132783e8cf 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -275,7 +275,6 @@ static int detect_harden_bp_fw(void)
 	return 1;
 }
 
-#ifdef CONFIG_ARM64_SSBD
 DEFINE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
 
 int ssbd_state __read_mostly = ARM64_SSBD_KERNEL;
@@ -348,6 +347,11 @@ void __init arm64_enable_wa2_handling(struct alt_instr *alt,
 
 void arm64_set_ssbd_mitigation(bool state)
 {
+	if (!IS_ENABLED(CONFIG_ARM64_SSBD)) {
+		pr_info_once("SSBD disabled by kernel configuration\n");
+		return;
+	}
+
 	if (this_cpu_has_cap(ARM64_SSBS)) {
 		if (state)
 			asm volatile(SET_PSTATE_SSBS(0));
@@ -467,7 +471,6 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	return required;
 }
-#endif	/* CONFIG_ARM64_SSBD */
 
 static void __maybe_unused
 cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
@@ -759,14 +762,12 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		ERRATA_MIDR_RANGE_LIST(arm64_harden_el2_vectors),
 	},
 #endif
-#ifdef CONFIG_ARM64_SSBD
 	{
 		.desc = "Speculative Store Bypass Disable",
 		.capability = ARM64_SSBD,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_ssbd_mitigation,
 	},
-#endif
 #ifdef CONFIG_ARM64_ERRATUM_1188873
 	{
 		/* Cortex-A76 r0p0 to r2p0 */

commit d2532e27b5638bb2e2dd52b80b7ea2ec65135377
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Mon Apr 15 16:21:26 2019 -0500

    arm64: add sysfs vulnerability show for spectre-v2
    
    Track whether all the cores in the machine are vulnerable to Spectre-v2,
    and whether all the vulnerable cores have been mitigated. We then expose
    this information to userspace via sysfs.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a9c3ad4f7948..d2bbafa04b3c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -512,6 +512,10 @@ cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
 	CAP_MIDR_RANGE_LIST(midr_list)
 
+/* Track overall mitigation state. We are only mitigated if all cores are ok */
+static bool __hardenbp_enab = true;
+static bool __spectrev2_safe = true;
+
 /*
  * List of CPUs that do not need any Spectre-v2 mitigation at all.
  */
@@ -522,6 +526,10 @@ static const struct midr_range spectre_v2_safe_list[] = {
 	{ /* sentinel */ }
 };
 
+/*
+ * Track overall bp hardening for all heterogeneous cores in the machine.
+ * We are only considered "safe" if all booted cores are known safe.
+ */
 static bool __maybe_unused
 check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 {
@@ -543,6 +551,8 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	if (!need_wa)
 		return false;
 
+	__spectrev2_safe = false;
+
 	if (!IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR)) {
 		pr_warn_once("spectrev2 mitigation disabled by kernel configuration\n");
 		__hardenbp_enab = false;
@@ -552,11 +562,14 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	/* forced off */
 	if (__nospectre_v2) {
 		pr_info_once("spectrev2 mitigation disabled by command line option\n");
+		__hardenbp_enab = false;
 		return false;
 	}
 
-	if (need_wa < 0)
+	if (need_wa < 0) {
 		pr_warn_once("ARM_SMCCC_ARCH_WORKAROUND_1 missing from firmware\n");
+		__hardenbp_enab = false;
+	}
 
 	return (need_wa > 0);
 }
@@ -779,3 +792,15 @@ ssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr,
 {
 	return sprintf(buf, "Mitigation: __user pointer sanitization\n");
 }
+
+ssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	if (__spectrev2_safe)
+		return sprintf(buf, "Not affected\n");
+
+	if (__hardenbp_enab)
+		return sprintf(buf, "Mitigation: Branch predictor hardening\n");
+
+	return sprintf(buf, "Vulnerable\n");
+}

commit 8c1e3d2bb44cbb998cb28ff9a18f105fee7f1eb3
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Mon Apr 15 16:21:25 2019 -0500

    arm64: Always enable spectre-v2 vulnerability detection
    
    Ensure we are always able to detect whether or not the CPU is affected
    by Spectre-v2, so that we can later advertise this to userspace.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 60cf87c4deb7..a9c3ad4f7948 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -109,7 +109,6 @@ cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *__unused)
 
 atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
 
-#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 
@@ -270,11 +269,11 @@ static int detect_harden_bp_fw(void)
 	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
 		cb = qcom_link_stack_sanitization;
 
-	install_bp_hardening_cb(cb, smccc_start, smccc_end);
+	if (IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR))
+		install_bp_hardening_cb(cb, smccc_start, smccc_end);
 
 	return 1;
 }
-#endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #ifdef CONFIG_ARM64_SSBD
 DEFINE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
@@ -513,7 +512,6 @@ cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
 	CAP_MIDR_RANGE_LIST(midr_list)
 
-#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 /*
  * List of CPUs that do not need any Spectre-v2 mitigation at all.
  */
@@ -545,6 +543,12 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 	if (!need_wa)
 		return false;
 
+	if (!IS_ENABLED(CONFIG_HARDEN_BRANCH_PREDICTOR)) {
+		pr_warn_once("spectrev2 mitigation disabled by kernel configuration\n");
+		__hardenbp_enab = false;
+		return false;
+	}
+
 	/* forced off */
 	if (__nospectre_v2) {
 		pr_info_once("spectrev2 mitigation disabled by command line option\n");
@@ -556,7 +560,6 @@ check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
 
 	return (need_wa > 0);
 }
-#endif
 
 #ifdef CONFIG_HARDEN_EL2_VECTORS
 
@@ -731,13 +734,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
 	},
 #endif
-#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = check_branch_predictor,
 	},
-#endif
 #ifdef CONFIG_HARDEN_EL2_VECTORS
 	{
 		.desc = "EL2 vector hardening",

commit 517953c2c47f9c00a002f588ac856a5bc70cede3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 15 16:21:24 2019 -0500

    arm64: Use firmware to detect CPUs that are not affected by Spectre-v2
    
    The SMCCC ARCH_WORKAROUND_1 service can indicate that although the
    firmware knows about the Spectre-v2 mitigation, this particular
    CPU is not vulnerable, and it is thus not necessary to call
    the firmware on this CPU.
    
    Let's use this information to our benefit.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 032f1a4dbea2..60cf87c4deb7 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -230,22 +230,36 @@ static int detect_harden_bp_fw(void)
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
-		if ((int)res.a0 < 0)
+		switch ((int)res.a0) {
+		case 1:
+			/* Firmware says we're just fine */
+			return 0;
+		case 0:
+			cb = call_hvc_arch_workaround_1;
+			/* This is a guest, no need to patch KVM vectors */
+			smccc_start = NULL;
+			smccc_end = NULL;
+			break;
+		default:
 			return -1;
-		cb = call_hvc_arch_workaround_1;
-		/* This is a guest, no need to patch KVM vectors */
-		smccc_start = NULL;
-		smccc_end = NULL;
+		}
 		break;
 
 	case PSCI_CONDUIT_SMC:
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
-		if ((int)res.a0 < 0)
+		switch ((int)res.a0) {
+		case 1:
+			/* Firmware says we're just fine */
+			return 0;
+		case 0:
+			cb = call_smc_arch_workaround_1;
+			smccc_start = __smccc_workaround_1_smc_start;
+			smccc_end = __smccc_workaround_1_smc_end;
+			break;
+		default:
 			return -1;
-		cb = call_smc_arch_workaround_1;
-		smccc_start = __smccc_workaround_1_smc_start;
-		smccc_end = __smccc_workaround_1_smc_end;
+		}
 		break;
 
 	default:

commit 73f38166095947f3b86b02fbed6bd592223a7ac8
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 15 16:21:23 2019 -0500

    arm64: Advertise mitigation of Spectre-v2, or lack thereof
    
    We currently have a list of CPUs affected by Spectre-v2, for which
    we check that the firmware implements ARCH_WORKAROUND_1. It turns
    out that not all firmwares do implement the required mitigation,
    and that we fail to let the user know about it.
    
    Instead, let's slightly revamp our checks, and rely on a whitelist
    of cores that are known to be non-vulnerable, and let the user know
    the status of the mitigation in the kernel log.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index cf623657cf3c..032f1a4dbea2 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -131,9 +131,9 @@ static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 	__flush_icache_range((uintptr_t)dst, (uintptr_t)dst + SZ_2K);
 }
 
-static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
-				      const char *hyp_vecs_start,
-				      const char *hyp_vecs_end)
+static void install_bp_hardening_cb(bp_hardening_cb_t fn,
+				    const char *hyp_vecs_start,
+				    const char *hyp_vecs_end)
 {
 	static DEFINE_RAW_SPINLOCK(bp_lock);
 	int cpu, slot = -1;
@@ -169,7 +169,7 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 #define __smccc_workaround_1_smc_start		NULL
 #define __smccc_workaround_1_smc_end		NULL
 
-static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
+static void install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
 				      const char *hyp_vecs_end)
 {
@@ -177,23 +177,6 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 }
 #endif	/* CONFIG_KVM_INDIRECT_VECTORS */
 
-static void  install_bp_hardening_cb(const struct arm64_cpu_capabilities *entry,
-				     bp_hardening_cb_t fn,
-				     const char *hyp_vecs_start,
-				     const char *hyp_vecs_end)
-{
-	u64 pfr0;
-
-	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
-		return;
-
-	pfr0 = read_cpuid(ID_AA64PFR0_EL1);
-	if (cpuid_feature_extract_unsigned_field(pfr0, ID_AA64PFR0_CSV2_SHIFT))
-		return;
-
-	__install_bp_hardening_cb(fn, hyp_vecs_start, hyp_vecs_end);
-}
-
 #include <uapi/linux/psci.h>
 #include <linux/arm-smccc.h>
 #include <linux/psci.h>
@@ -228,31 +211,27 @@ static int __init parse_nospectre_v2(char *str)
 }
 early_param("nospectre_v2", parse_nospectre_v2);
 
-static void
-enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
+/*
+ * -1: No workaround
+ *  0: No workaround required
+ *  1: Workaround installed
+ */
+static int detect_harden_bp_fw(void)
 {
 	bp_hardening_cb_t cb;
 	void *smccc_start, *smccc_end;
 	struct arm_smccc_res res;
 	u32 midr = read_cpuid_id();
 
-	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
-		return;
-
-	if (__nospectre_v2) {
-		pr_info_once("spectrev2 mitigation disabled by command line option\n");
-		return;
-	}
-
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
-		return;
+		return -1;
 
 	switch (psci_ops.conduit) {
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		if ((int)res.a0 < 0)
-			return;
+			return -1;
 		cb = call_hvc_arch_workaround_1;
 		/* This is a guest, no need to patch KVM vectors */
 		smccc_start = NULL;
@@ -263,23 +242,23 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		if ((int)res.a0 < 0)
-			return;
+			return -1;
 		cb = call_smc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_smc_start;
 		smccc_end = __smccc_workaround_1_smc_end;
 		break;
 
 	default:
-		return;
+		return -1;
 	}
 
 	if (((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR) ||
 	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
 		cb = qcom_link_stack_sanitization;
 
-	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
+	install_bp_hardening_cb(cb, smccc_start, smccc_end);
 
-	return;
+	return 1;
 }
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
@@ -521,24 +500,48 @@ cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 	CAP_MIDR_RANGE_LIST(midr_list)
 
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
-
 /*
- * List of CPUs where we need to issue a psci call to
- * harden the branch predictor.
+ * List of CPUs that do not need any Spectre-v2 mitigation at all.
  */
-static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
-	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
-	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
-	MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
-	MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
-	MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
-	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
-	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
-	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
-	MIDR_ALL_VERSIONS(MIDR_NVIDIA_DENVER),
-	{},
+static const struct midr_range spectre_v2_safe_list[] = {
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A35),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A53),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A55),
+	{ /* sentinel */ }
 };
 
+static bool __maybe_unused
+check_branch_predictor(const struct arm64_cpu_capabilities *entry, int scope)
+{
+	int need_wa;
+
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+
+	/* If the CPU has CSV2 set, we're safe */
+	if (cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64PFR0_EL1),
+						 ID_AA64PFR0_CSV2_SHIFT))
+		return false;
+
+	/* Alternatively, we have a list of unaffected CPUs */
+	if (is_midr_in_range_list(read_cpuid_id(), spectre_v2_safe_list))
+		return false;
+
+	/* Fallback to firmware detection */
+	need_wa = detect_harden_bp_fw();
+	if (!need_wa)
+		return false;
+
+	/* forced off */
+	if (__nospectre_v2) {
+		pr_info_once("spectrev2 mitigation disabled by command line option\n");
+		return false;
+	}
+
+	if (need_wa < 0)
+		pr_warn_once("ARM_SMCCC_ARCH_WORKAROUND_1 missing from firmware\n");
+
+	return (need_wa > 0);
+}
 #endif
 
 #ifdef CONFIG_HARDEN_EL2_VECTORS
@@ -717,8 +720,8 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		.cpu_enable = enable_smccc_arch_workaround_1,
-		ERRATA_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = check_branch_predictor,
 	},
 #endif
 #ifdef CONFIG_HARDEN_EL2_VECTORS

commit 3891ebccace188af075ce143d8b072b65e90f695
Author: Mian Yousaf Kaukab <ykaukab@suse.de>
Date:   Mon Apr 15 16:21:21 2019 -0500

    arm64: Add sysfs vulnerability show for spectre-v1
    
    spectre-v1 has been mitigated and the mitigation is always active.
    Report this to userspace via sysfs
    
    Signed-off-by: Mian Yousaf Kaukab <ykaukab@suse.de>
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Acked-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index d2b2c69d31bb..cf623657cf3c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -755,3 +755,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 	}
 };
+
+ssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr,
+			    char *buf)
+{
+	return sprintf(buf, "Mitigation: __user pointer sanitization\n");
+}

commit e5ce5e7267ddcbe13ab9ead2542524e1b7993e5a
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Mon Apr 15 16:21:20 2019 -0500

    arm64: Provide a command line to disable spectre_v2 mitigation
    
    There are various reasons, such as benchmarking, to disable spectrev2
    mitigation on a machine. Provide a command-line option to do so.
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: linux-doc@vger.kernel.org
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9950bb0cbd52..d2b2c69d31bb 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -220,6 +220,14 @@ static void qcom_link_stack_sanitization(void)
 		     : "=&r" (tmp));
 }
 
+static bool __nospectre_v2;
+static int __init parse_nospectre_v2(char *str)
+{
+	__nospectre_v2 = true;
+	return 0;
+}
+early_param("nospectre_v2", parse_nospectre_v2);
+
 static void
 enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 {
@@ -231,6 +239,11 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
 		return;
 
+	if (__nospectre_v2) {
+		pr_info_once("spectrev2 mitigation disabled by command line option\n");
+		return;
+	}
+
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
 		return;
 

commit b89d82ef01b33bc50cbaa8ff05607879b40d0704
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jan 8 16:19:01 2019 +0000

    arm64: kpti: Avoid rewriting early page tables when KASLR is enabled
    
    A side effect of commit c55191e96caa ("arm64: mm: apply r/o permissions
    of VM areas to its linear alias as well") is that the linear map is
    created with page granularity, which means that transitioning the early
    page table from global to non-global mappings when enabling kpti can
    take a significant amount of time during boot.
    
    Given that most CPU implementations do not require kpti, this mainly
    impacts KASLR builds where kpti is forcefully enabled. However, in these
    situations we know early on that non-global mappings are required and
    can avoid the use of global mappings from the beginning. The only gotcha
    is Cavium erratum #27456, which we must detect based on the MIDR value
    of the boot CPU.
    
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reported-by: John Garry <john.garry@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 09ac548c9d44..9950bb0cbd52 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -553,7 +553,7 @@ static const struct midr_range arm64_repeat_tlbi_cpus[] = {
 #endif
 
 #ifdef CONFIG_CAVIUM_ERRATUM_27456
-static const struct midr_range cavium_erratum_27456_cpus[] = {
+const struct midr_range cavium_erratum_27456_cpus[] = {
 	/* Cavium ThunderX, T88 pass 1.x - 2.1 */
 	MIDR_RANGE(MIDR_THUNDERX, 0, 0, 1, 1),
 	/* Cavium ThunderX, T81 pass 1.0 */

commit 5694cecdb092656a822287a6691aa7ce668c8160
Merge: 13e1ad2be3a8 12f799c8c739
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 25 17:41:56 2018 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 festive updates from Will Deacon:
     "In the end, we ended up with quite a lot more than I expected:
    
       - Support for ARMv8.3 Pointer Authentication in userspace (CRIU and
         kernel-side support to come later)
    
       - Support for per-thread stack canaries, pending an update to GCC
         that is currently undergoing review
    
       - Support for kexec_file_load(), which permits secure boot of a kexec
         payload but also happens to improve the performance of kexec
         dramatically because we can avoid the sucky purgatory code from
         userspace. Kdump will come later (requires updates to libfdt).
    
       - Optimisation of our dynamic CPU feature framework, so that all
         detected features are enabled via a single stop_machine()
         invocation
    
       - KPTI whitelisting of Cortex-A CPUs unaffected by Meltdown, so that
         they can benefit from global TLB entries when KASLR is not in use
    
       - 52-bit virtual addressing for userspace (kernel remains 48-bit)
    
       - Patch in LSE atomics for per-cpu atomic operations
    
       - Custom preempt.h implementation to avoid unconditional calls to
         preempt_schedule() from preempt_enable()
    
       - Support for the new 'SB' Speculation Barrier instruction
    
       - Vectorised implementation of XOR checksumming and CRC32
         optimisations
    
       - Workaround for Cortex-A76 erratum #1165522
    
       - Improved compatibility with Clang/LLD
    
       - Support for TX2 system PMUS for profiling the L3 cache and DMC
    
       - Reflect read-only permissions in the linear map by default
    
       - Ensure MMIO reads are ordered with subsequent calls to Xdelay()
    
       - Initial support for memory hotplug
    
       - Tweak the threshold when we invalidate the TLB by-ASID, so that
         mremap() performance is improved for ranges spanning multiple PMDs.
    
       - Minor refactoring and cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (125 commits)
      arm64: kaslr: print PHYS_OFFSET in dump_kernel_offset()
      arm64: sysreg: Use _BITUL() when defining register bits
      arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
      arm64: cpufeature: Reduce number of pointer auth CPU caps from 6 to 4
      arm64: docs: document pointer authentication
      arm64: ptr auth: Move per-thread keys from thread_info to thread_struct
      arm64: enable pointer authentication
      arm64: add prctl control for resetting ptrauth keys
      arm64: perf: strip PAC when unwinding userspace
      arm64: expose user PAC bit positions via ptrace
      arm64: add basic pointer authentication support
      arm64/cpufeature: detect pointer authentication
      arm64: Don't trap host pointer auth use to EL2
      arm64/kvm: hide ptrauth from guests
      arm64/kvm: consistently handle host HCR_EL2 flags
      arm64: add pointer authentication register bits
      arm64: add comments about EC exception levels
      arm64: perf: Treat EXCLUDE_EL* bit definitions as unsigned
      arm64: kpti: Whitelist Cortex-A CPUs that don't implement the CSV3 field
      arm64: enable per-task stack canaries
      ...

commit 1e013d06120cbf67e771848fc5d98174c33b078a
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Dec 12 15:53:54 2018 +0000

    arm64: cpufeature: Rework ptr auth hwcaps using multi_entry_cap_matches
    
    Open-coding the pointer-auth HWCAPs is a mess and can be avoided by
    reusing the multi-cap logic from the CPU errata framework.
    
    Move the multi_entry_cap_matches code to cpufeature.h and reuse it for
    the pointer auth HWCAPs.
    
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index e3123055e552..ff2fda3a98e1 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -507,38 +507,6 @@ cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
 	CAP_MIDR_RANGE_LIST(midr_list)
 
-/*
- * Generic helper for handling capabilties with multiple (match,enable) pairs
- * of call backs, sharing the same capability bit.
- * Iterate over each entry to see if at least one matches.
- */
-static bool __maybe_unused
-multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry, int scope)
-{
-	const struct arm64_cpu_capabilities *caps;
-
-	for (caps = entry->match_list; caps->matches; caps++)
-		if (caps->matches(caps, scope))
-			return true;
-
-	return false;
-}
-
-/*
- * Take appropriate action for all matching entries in the shared capability
- * entry.
- */
-static void __maybe_unused
-multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
-{
-	const struct arm64_cpu_capabilities *caps;
-
-	for (caps = entry->match_list; caps->matches; caps++)
-		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
-		    caps->cpu_enable)
-			caps->cpu_enable(caps);
-}
-
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 
 /*
@@ -700,7 +668,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm Technologies Falkor/Kryo erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
-		.matches = multi_entry_cap_matches,
+		.matches = cpucap_multi_entry_cap_matches,
 		.match_list = qcom_erratum_1003_list,
 	},
 #endif

commit bc84a2d106beab6000223b569c3bcb9ebf49d9ec
Merge: f357b3a7e17a a457b0f7f50d
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 18:53:03 2018 +0000

    Merge branch 'kvm/cortex-a76-erratum-1165522' into aarch64/for-next/core
    
    Pull in KVM workaround for A76 erratum #116522.
    
    Conflicts:
            arch/arm64/include/asm/cpucaps.h

commit 8b2cca9ade2c0f1d2ba94e39781e7306c918e544
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Dec 6 17:31:23 2018 +0000

    arm64: KVM: Force VHE for systems affected by erratum 1165522
    
    In order to easily mitigate ARM erratum 1165522, we need to force
    affected CPUs to run in VHE mode if using KVM.
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a509e35132d2..476e738e6c46 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -739,6 +739,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_1188873,
 		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1165522
+	{
+		/* Cortex-A76 r0p0 to r2p0 */
+		.desc = "ARM erratum 1165522",
+		.capability = ARM64_WORKAROUND_1165522,
+		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+	},
 #endif
 	{
 	}

commit a3dcea2c85129716f323d504b087a04200687242
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Nov 30 17:18:02 2018 +0000

    arm64: capabilities: Merge duplicate entries for Qualcomm erratum 1003
    
    Remove duplicate entries for Qualcomm erratum 1003. Since the entries
    are not purely based on generic MIDR checks, use the multi_cap_entry
    type to merge the entries.
    
    Cc: Christopher Covington <cov@codeaurora.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Tested-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 51f70f60fe3b..bb44635026f8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -592,6 +592,19 @@ static const struct midr_range cavium_erratum_30115_cpus[] = {
 };
 #endif
 
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
+static const struct arm64_cpu_capabilities qcom_erratum_1003_list[] = {
+	{
+		ERRATA_MIDR_REV(MIDR_QCOM_FALKOR_V1, 0, 0),
+	},
+	{
+		.midr_range.model = MIDR_QCOM_KRYO,
+		.matches = is_kryo_midr,
+	},
+	{},
+};
+#endif
+
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 static const struct midr_range workaround_clean_cache[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
@@ -685,16 +698,10 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	},
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
 	{
-		.desc = "Qualcomm Technologies Falkor erratum 1003",
+		.desc = "Qualcomm Technologies Falkor/Kryo erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
-		ERRATA_MIDR_REV(MIDR_QCOM_FALKOR_V1, 0, 0),
-	},
-	{
-		.desc = "Qualcomm Technologies Kryo erratum 1003",
-		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
-		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
-		.midr_range.model = MIDR_QCOM_KRYO,
-		.matches = is_kryo_midr,
+		.matches = multi_entry_cap_matches,
+		.match_list = qcom_erratum_1003_list,
 	},
 #endif
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009

commit f58cdf7e3cab33306efd999c23b4fb606184abf3
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Nov 30 17:18:01 2018 +0000

    arm64: capabilities: Merge duplicate Cavium erratum entries
    
    Merge duplicate entries for a single capability using the midr
    range list for Cavium errata 30115 and 27456.
    
    Cc: Andrew Pinski <apinski@cavium.com>
    Cc: David Daney <david.daney@cavium.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Tested-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index dc7ab05b9400..51f70f60fe3b 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -570,6 +570,28 @@ static const struct midr_range arm64_harden_el2_vectors[] = {
 
 #endif
 
+#ifdef CONFIG_CAVIUM_ERRATUM_27456
+static const struct midr_range cavium_erratum_27456_cpus[] = {
+	/* Cavium ThunderX, T88 pass 1.x - 2.1 */
+	MIDR_RANGE(MIDR_THUNDERX, 0, 0, 1, 1),
+	/* Cavium ThunderX, T81 pass 1.0 */
+	MIDR_REV(MIDR_THUNDERX_81XX, 0, 0),
+	{},
+};
+#endif
+
+#ifdef CONFIG_CAVIUM_ERRATUM_30115
+static const struct midr_range cavium_erratum_30115_cpus[] = {
+	/* Cavium ThunderX, T88 pass 1.x - 2.2 */
+	MIDR_RANGE(MIDR_THUNDERX, 0, 0, 1, 2),
+	/* Cavium ThunderX, T81 pass 1.0 - 1.2 */
+	MIDR_REV_RANGE(MIDR_THUNDERX_81XX, 0, 0, 2),
+	/* Cavium ThunderX, T83 pass 1.0 */
+	MIDR_REV(MIDR_THUNDERX_83XX, 0, 0),
+	{},
+};
+#endif
+
 #ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 static const struct midr_range workaround_clean_cache[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
@@ -642,40 +664,16 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_CAVIUM_ERRATUM_27456
 	{
-	/* Cavium ThunderX, T88 pass 1.x - 2.1 */
 		.desc = "Cavium erratum 27456",
 		.capability = ARM64_WORKAROUND_CAVIUM_27456,
-		ERRATA_MIDR_RANGE(MIDR_THUNDERX,
-				  0, 0,
-				  1, 1),
-	},
-	{
-	/* Cavium ThunderX, T81 pass 1.0 */
-		.desc = "Cavium erratum 27456",
-		.capability = ARM64_WORKAROUND_CAVIUM_27456,
-		ERRATA_MIDR_REV(MIDR_THUNDERX_81XX, 0, 0),
+		ERRATA_MIDR_RANGE_LIST(cavium_erratum_27456_cpus),
 	},
 #endif
 #ifdef CONFIG_CAVIUM_ERRATUM_30115
 	{
-	/* Cavium ThunderX, T88 pass 1.x - 2.2 */
-		.desc = "Cavium erratum 30115",
-		.capability = ARM64_WORKAROUND_CAVIUM_30115,
-		ERRATA_MIDR_RANGE(MIDR_THUNDERX,
-				      0, 0,
-				      1, 2),
-	},
-	{
-	/* Cavium ThunderX, T81 pass 1.0 - 1.2 */
-		.desc = "Cavium erratum 30115",
-		.capability = ARM64_WORKAROUND_CAVIUM_30115,
-		ERRATA_MIDR_REV_RANGE(MIDR_THUNDERX_81XX, 0, 0, 2),
-	},
-	{
-	/* Cavium ThunderX, T83 pass 1.0 */
 		.desc = "Cavium erratum 30115",
 		.capability = ARM64_WORKAROUND_CAVIUM_30115,
-		ERRATA_MIDR_REV(MIDR_THUNDERX_83XX, 0, 0),
+		ERRATA_MIDR_RANGE_LIST(cavium_erratum_30115_cpus),
 	},
 #endif
 	{

commit c9460dcb06ee68af1c75f9232603ece071901abe
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Nov 30 17:18:00 2018 +0000

    arm64: capabilities: Merge entries for ARM64_WORKAROUND_CLEAN_CACHE
    
    We have two entries for ARM64_WORKAROUND_CLEAN_CACHE capability :
    
    1) ARM Errata 826319, 827319, 824069, 819472 on A53 r0p[012]
    2) ARM Errata 819472 on A53 r0p[01]
    
    Both have the same work around. Merge these entries to avoid
    duplicate entries for a single capability. Add a new Kconfig
    entry to control the "capability" entry to make it easier
    to handle combinations of the CONFIGs.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 00d93a91c4d5..dc7ab05b9400 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -570,24 +570,28 @@ static const struct midr_range arm64_harden_el2_vectors[] = {
 
 #endif
 
-const struct arm64_cpu_capabilities arm64_errata[] = {
+#ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
+static const struct midr_range workaround_clean_cache[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \
 	defined(CONFIG_ARM64_ERRATUM_824069)
-	{
-	/* Cortex-A53 r0p[012] */
-		.desc = "ARM errata 826319, 827319, 824069",
-		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
-		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 2),
-		.cpu_enable = cpu_enable_cache_maint_trap,
-	},
+	/* Cortex-A53 r0p[012]: ARM errata 826319, 827319, 824069 */
+	MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 2),
+#endif
+#ifdef	CONFIG_ARM64_ERRATUM_819472
+	/* Cortex-A53 r0p[01] : ARM errata 819472 */
+	MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 1),
 #endif
-#ifdef CONFIG_ARM64_ERRATUM_819472
+	{},
+};
+#endif
+
+const struct arm64_cpu_capabilities arm64_errata[] = {
+#ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
 	{
-	/* Cortex-A53 r0p[01] */
-		.desc = "ARM errata 819472",
+		.desc = "ARM errata 826319, 827319, 824069, 819472",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
-		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 1),
+		ERRATA_MIDR_RANGE_LIST(workaround_clean_cache),
 		.cpu_enable = cpu_enable_cache_maint_trap,
 	},
 #endif

commit ce8c80c536dac9f325a051b30bf7730ee505eddc
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Nov 19 11:27:28 2018 +0000

    arm64: Add workaround for Cortex-A76 erratum 1286807
    
    On the affected Cortex-A76 cores (r0p0 to r3p0), if a virtual address
    for a cacheable mapping of a location is being accessed by a core while
    another core is remapping the virtual address to a new physical page
    using the recommended break-before-make sequence, then under very rare
    circumstances TLBI+DSB completes before a read using the translation
    being invalidated has been observed by other observers. The workaround
    repeats the TLBI+DSB operation and is shared with the Qualcomm Falkor
    erratum 1009
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a509e35132d2..6ad715d67df8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -570,6 +570,20 @@ static const struct midr_range arm64_harden_el2_vectors[] = {
 
 #endif
 
+#ifdef CONFIG_ARM64_WORKAROUND_REPEAT_TLBI
+
+static const struct midr_range arm64_repeat_tlbi_cpus[] = {
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009
+	MIDR_RANGE(MIDR_QCOM_FALKOR_V1, 0, 0, 0, 0),
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1286807
+	MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 3, 0),
+#endif
+	{},
+};
+
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \
@@ -695,11 +709,11 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.matches = is_kryo_midr,
 	},
 #endif
-#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009
+#ifdef CONFIG_ARM64_WORKAROUND_REPEAT_TLBI
 	{
-		.desc = "Qualcomm Technologies Falkor erratum 1009",
+		.desc = "Qualcomm erratum 1009, ARM erratum 1286807",
 		.capability = ARM64_WORKAROUND_REPEAT_TLBI,
-		ERRATA_MIDR_REV(MIDR_QCOM_FALKOR_V1, 0, 0),
+		ERRATA_MIDR_RANGE_LIST(arm64_repeat_tlbi_cpus),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_858921

commit d8797b125711f23d83f5a71e908d34dfcd1fc3e9
Author: James Morse <james.morse@arm.com>
Date:   Tue Nov 27 15:35:21 2018 +0000

    arm64: Use a raw spinlock in __install_bp_hardening_cb()
    
    __install_bp_hardening_cb() is called via stop_machine() as part
    of the cpu_enable callback. To force each CPU to take its turn
    when allocating slots, they take a spinlock.
    
    With the RT patches applied, the spinlock becomes a mutex,
    and we get warnings about sleeping while in stop_machine():
    | [    0.319176] CPU features: detected: RAS Extension Support
    | [    0.319950] BUG: scheduling while atomic: migration/3/36/0x00000002
    | [    0.319955] Modules linked in:
    | [    0.319958] Preemption disabled at:
    | [    0.319969] [<ffff000008181ae4>] cpu_stopper_thread+0x7c/0x108
    | [    0.319973] CPU: 3 PID: 36 Comm: migration/3 Not tainted 4.19.1-rt3-00250-g330fc2c2a880 #2
    | [    0.319975] Hardware name: linux,dummy-virt (DT)
    | [    0.319976] Call trace:
    | [    0.319981]  dump_backtrace+0x0/0x148
    | [    0.319983]  show_stack+0x14/0x20
    | [    0.319987]  dump_stack+0x80/0xa4
    | [    0.319989]  __schedule_bug+0x94/0xb0
    | [    0.319991]  __schedule+0x510/0x560
    | [    0.319992]  schedule+0x38/0xe8
    | [    0.319994]  rt_spin_lock_slowlock_locked+0xf0/0x278
    | [    0.319996]  rt_spin_lock_slowlock+0x5c/0x90
    | [    0.319998]  rt_spin_lock+0x54/0x58
    | [    0.320000]  enable_smccc_arch_workaround_1+0xdc/0x260
    | [    0.320001]  __enable_cpu_capability+0x10/0x20
    | [    0.320003]  multi_cpu_stop+0x84/0x108
    | [    0.320004]  cpu_stopper_thread+0x84/0x108
    | [    0.320008]  smpboot_thread_fn+0x1e8/0x2b0
    | [    0.320009]  kthread+0x124/0x128
    | [    0.320010]  ret_from_fork+0x10/0x18
    
    Switch this to a raw spinlock, as we know this is only called with
    IRQs masked.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a509e35132d2..00d93a91c4d5 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -135,7 +135,7 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
 				      const char *hyp_vecs_end)
 {
-	static DEFINE_SPINLOCK(bp_lock);
+	static DEFINE_RAW_SPINLOCK(bp_lock);
 	int cpu, slot = -1;
 
 	/*
@@ -147,7 +147,7 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 		return;
 	}
 
-	spin_lock(&bp_lock);
+	raw_spin_lock(&bp_lock);
 	for_each_possible_cpu(cpu) {
 		if (per_cpu(bp_hardening_data.fn, cpu) == fn) {
 			slot = per_cpu(bp_hardening_data.hyp_vectors_slot, cpu);
@@ -163,7 +163,7 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 
 	__this_cpu_write(bp_hardening_data.hyp_vectors_slot, slot);
 	__this_cpu_write(bp_hardening_data.fn, fn);
-	spin_unlock(&bp_lock);
+	raw_spin_unlock(&bp_lock);
 }
 #else
 #define __smccc_workaround_1_smc_start		NULL

commit 4debef5510071032c6d5dace31ca1cc42a120073
Author: James Morse <james.morse@arm.com>
Date:   Fri Sep 21 21:49:19 2018 +0100

    arm64: KVM: Guests can skip __install_bp_hardening_cb()s HYP work
    
    enable_smccc_arch_workaround_1() passes NULL as the hyp_vecs start and
    end if the HVC conduit is in use, and ARM_SMCCC_ARCH_WORKAROUND_1 is
    detected.
    
    If the guest kernel happened to be built with KVM_INDIRECT_VECTORS,
    we go on to allocate a slot, memcpy() the empty workaround in and
    do the appropriate cache maintenance.
    
    This works as we always tell memcpy() the range is 0, so it never
    accesses the NULL src pointer, but we still do the cache maintenance.
    
    If hyp_vecs_start is NULL we know we're a guest, just update the fn
    like the !KVM_INDIRECT_VECTORS version.
    
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9af8df96ec49..a509e35132d2 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -138,6 +138,15 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	static DEFINE_SPINLOCK(bp_lock);
 	int cpu, slot = -1;
 
+	/*
+	 * enable_smccc_arch_workaround_1() passes NULL for the hyp_vecs
+	 * start/end if we're a guest. Skip the hyp-vectors work.
+	 */
+	if (!hyp_vecs_start) {
+		__this_cpu_write(bp_hardening_data.fn, fn);
+		return;
+	}
+
 	spin_lock(&bp_lock);
 	for_each_possible_cpu(cpu) {
 		if (per_cpu(bp_hardening_data.fn, cpu) == fn) {

commit 4afe8e79da920bdf6698b01bc668fffc6758f37b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Oct 9 14:47:07 2018 +0100

    arm64: cpufeature: Trap CTR_EL0 access only where it is necessary
    
    When there is a mismatch in the CTR_EL0 field, we trap
    access to CTR from EL0 on all CPUs to expose the safe
    value. However, we could skip trapping on a CPU which
    matches the safe value.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 3deb01c6ed49..9af8df96ec49 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -99,7 +99,12 @@ has_mismatched_cache_type(const struct arm64_cpu_capabilities *entry,
 static void
 cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *__unused)
 {
-	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
+	u64 mask = arm64_ftr_reg_ctrel0.strict_mask;
+
+	/* Trap CTR_EL0 access on this CPU, only if it has a mismatch */
+	if ((read_cpuid_cachetype() & mask) !=
+	    (arm64_ftr_reg_ctrel0.sys_val & mask))
+		sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
 }
 
 atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);

commit 1602df02f33f61fe0de1bbfeba0d1c97c14bff19
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Oct 9 14:47:06 2018 +0100

    arm64: cpufeature: Fix handling of CTR_EL0.IDC field
    
    CTR_EL0.IDC reports the data cache clean requirements for instruction
    to data coherence. However, if the field is 0, we need to check the
    CLIDR_EL1 fields to detect the status of the feature. Currently we
    don't do this and generate a warning with tainting the kernel, when
    there is a mismatch in the field among the CPUs. Also the userspace
    doesn't have a reliable way to check the CLIDR_EL1 register to check
    the status.
    
    This patch fixes the problem by checking the CLIDR_EL1 fields, when
    (CTR_EL0.IDC == 0) and updates the kernel's copy of the CTR_EL0 for
    the CPU with the actual status of the feature. This would allow the
    sanity check infrastructure to do the proper checking of the fields
    and also allow the CTR_EL0 emulation code to supply the real status
    of the feature.
    
    Now, if a CPU has raw CTR_EL0.IDC == 0 and effective IDC == 1 (with
    overall system wide IDC == 1), we need to expose the real value to
    the user. So, we trap CTR_EL0 access on the CPU which reports incorrect
    CTR_EL0.IDC.
    
    Fixes: commit 6ae4b6e057888 ("arm64: Add support for new control bits CTR_EL0.DIC and CTR_EL0.IDC")
    Cc: Shanker Donthineni <shankerd@codeaurora.org>
    Cc: Philip Elcan <pelcan@codeaurora.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index cde948991d68..3deb01c6ed49 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -68,11 +68,32 @@ static bool
 has_mismatched_cache_type(const struct arm64_cpu_capabilities *entry,
 			  int scope)
 {
-	u64 mask = arm64_ftr_reg_ctrel0.strict_mask;;
+	u64 mask = arm64_ftr_reg_ctrel0.strict_mask;
+	u64 sys = arm64_ftr_reg_ctrel0.sys_val & mask;
+	u64 ctr_raw, ctr_real;
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
-	return (read_cpuid_cachetype() & mask) !=
-	       (arm64_ftr_reg_ctrel0.sys_val & mask);
+
+	/*
+	 * We want to make sure that all the CPUs in the system expose
+	 * a consistent CTR_EL0 to make sure that applications behaves
+	 * correctly with migration.
+	 *
+	 * If a CPU has CTR_EL0.IDC but does not advertise it via CTR_EL0 :
+	 *
+	 * 1) It is safe if the system doesn't support IDC, as CPU anyway
+	 *    reports IDC = 0, consistent with the rest.
+	 *
+	 * 2) If the system has IDC, it is still safe as we trap CTR_EL0
+	 *    access on this CPU via the ARM64_HAS_CACHE_IDC capability.
+	 *
+	 * So, we need to make sure either the raw CTR_EL0 or the effective
+	 * CTR_EL0 matches the system's copy to allow a secondary CPU to boot.
+	 */
+	ctr_raw = read_cpuid_cachetype() & mask;
+	ctr_real = read_cpuid_effective_cachetype() & mask;
+
+	return (ctr_real != sys) && (ctr_raw != sys);
 }
 
 static void

commit 95b861a4a6d94f64d5242605569218160ebacdbe
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:34 2018 +0100

    arm64: arch_timer: Add workaround for ARM erratum 1188873
    
    When running on Cortex-A76, a timer access from an AArch32 EL0
    task may end up with a corrupted value or register. The workaround for
    this is to trap these accesses at EL1/EL2 and execute them there.
    
    This only affects versions r0p0, r1p0 and r2p0 of the CPU.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 20be4c578e0a..cde948991d68 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -696,6 +696,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.matches = has_ssbd_mitigation,
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_1188873
+	{
+		/* Cortex-A76 r0p0 to r2p0 */
+		.desc = "ARM erratum 1188873",
+		.capability = ARM64_WORKAROUND_1188873,
+		ERRATA_MIDR_RANGE(MIDR_CORTEX_A76, 0, 0, 2, 0),
+	},
 #endif
 	{
 	}

commit 880f7cc47265e7b195781dfa9a0cd62ef78304e3
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Sep 19 11:41:21 2018 +0100

    arm64: cpu_errata: Remove ARM64_MISMATCHED_CACHE_LINE_SIZE
    
    There's no need to treat mismatched cache-line sizes reported by CTR_EL0
    differently to any other mismatched fields that we treat as "STRICT" in
    the cpufeature code. In both cases we need to trap and emulate EL0
    accesses to the register, so drop ARM64_MISMATCHED_CACHE_LINE_SIZE and
    rely on ARM64_MISMATCHED_CACHE_TYPE instead.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: move ARM64_HAS_CNP in the empty cpucaps.h slot]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 8900cb0615f8..20be4c578e0a 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -68,11 +68,7 @@ static bool
 has_mismatched_cache_type(const struct arm64_cpu_capabilities *entry,
 			  int scope)
 {
-	u64 mask = CTR_CACHE_MINLINE_MASK;
-
-	/* Skip matching the min line sizes for cache type check */
-	if (entry->capability == ARM64_MISMATCHED_CACHE_TYPE)
-		mask ^= arm64_ftr_reg_ctrel0.strict_mask;
+	u64 mask = arm64_ftr_reg_ctrel0.strict_mask;;
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 	return (read_cpuid_cachetype() & mask) !=
@@ -644,14 +640,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	},
 #endif
 	{
-		.desc = "Mismatched cache line size",
-		.capability = ARM64_MISMATCHED_CACHE_LINE_SIZE,
-		.matches = has_mismatched_cache_type,
-		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
-		.cpu_enable = cpu_enable_trap_ctr_access,
-	},
-	{
-		.desc = "Mismatched cache type",
+		.desc = "Mismatched cache type (CTR_EL0)",
 		.capability = ARM64_MISMATCHED_CACHE_TYPE,
 		.matches = has_mismatched_cache_type,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,

commit b8925ee2e12d1cb9a11d6f28b5814f2bfa59dce1
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:53:41 2018 +0100

    arm64: cpu: Move errata and feature enable callbacks closer to callers
    
    The cpu errata and feature enable callbacks are only called via their
    respective arm64_cpu_capabilities structure and therefore shouldn't
    exist in the global namespace.
    
    Move the PAN, RAS and cache maintenance emulation enable callbacks into
    the same files as their corresponding arm64_cpu_capabilities structures,
    making them static in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index c063490d7b51..8900cb0615f8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -433,6 +433,12 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 }
 #endif	/* CONFIG_ARM64_SSBD */
 
+static void __maybe_unused
+cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
+{
+	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCI, 0);
+}
+
 #define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
 	.matches = is_affected_midr_range,			\
 	.midr_range = MIDR_RANGE(model, v_min, r_min, v_max, r_max)

commit 8f04e8e6e29c93421a95b61cad62e3918425eac7
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:47:06 2018 +0100

    arm64: ssbd: Add support for PSTATE.SSBS rather than trapping to EL3
    
    On CPUs with support for PSTATE.SSBS, the kernel can toggle the SSBD
    state without needing to call into firmware.
    
    This patch hooks into the existing SSBD infrastructure so that SSBS is
    used on CPUs that support it, but it's all made horribly complicated by
    the very real possibility of big/little systems that don't uniformly
    provide the new capability.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index dec10898d688..c063490d7b51 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -312,6 +312,14 @@ void __init arm64_enable_wa2_handling(struct alt_instr *alt,
 
 void arm64_set_ssbd_mitigation(bool state)
 {
+	if (this_cpu_has_cap(ARM64_SSBS)) {
+		if (state)
+			asm volatile(SET_PSTATE_SSBS(0));
+		else
+			asm volatile(SET_PSTATE_SSBS(1));
+		return;
+	}
+
 	switch (psci_ops.conduit) {
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
@@ -336,6 +344,11 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 
+	if (this_cpu_has_cap(ARM64_SSBS)) {
+		required = false;
+		goto out_printmsg;
+	}
+
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0) {
 		ssbd_state = ARM64_SSBD_UNKNOWN;
 		return false;
@@ -384,7 +397,6 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 
 	switch (ssbd_state) {
 	case ARM64_SSBD_FORCE_DISABLE:
-		pr_info_once("%s disabled from command-line\n", entry->desc);
 		arm64_set_ssbd_mitigation(false);
 		required = false;
 		break;
@@ -397,7 +409,6 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 		break;
 
 	case ARM64_SSBD_FORCE_ENABLE:
-		pr_info_once("%s forced from command-line\n", entry->desc);
 		arm64_set_ssbd_mitigation(true);
 		required = true;
 		break;
@@ -407,6 +418,17 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 		break;
 	}
 
+out_printmsg:
+	switch (ssbd_state) {
+	case ARM64_SSBD_FORCE_DISABLE:
+		pr_info_once("%s disabled from command-line\n", entry->desc);
+		break;
+
+	case ARM64_SSBD_FORCE_ENABLE:
+		pr_info_once("%s forced from command-line\n", entry->desc);
+		break;
+	}
+
 	return required;
 }
 #endif	/* CONFIG_ARM64_SSBD */

commit 25be597ada0b49d2748ab520a78a28c1764d69e4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:38 2018 +0100

    arm64: kill config_sctlr_el1()
    
    Now that we have sysreg_clear_set(), we can consistently use this
    instead of config_sctlr_el1().
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 94876ab071c7..dec10898d688 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -82,8 +82,7 @@ has_mismatched_cache_type(const struct arm64_cpu_capabilities *entry,
 static void
 cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *__unused)
 {
-	/* Clear SCTLR_EL1.UCT */
-	config_sctlr_el1(SCTLR_EL1_UCT, 0);
+	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCT, 0);
 }
 
 atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);

commit 178909a669584b97e9154375df2126fd93b6306c
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jul 6 09:57:45 2018 +0100

    arm64: errata: Don't define type field twice for arm64_errata[] entries
    
    The ERRATA_MIDR_REV_RANGE macro assigns ARM64_CPUCAP_LOCAL_CPU_ERRATUM
    to the '.type' field of the 'struct arm64_cpu_capabilities', so there's
    no need to assign it explicitly as well.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 459129712dfa..94876ab071c7 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -662,7 +662,6 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.cpu_enable = enable_smccc_arch_workaround_1,
 		ERRATA_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
 	},
@@ -671,7 +670,6 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "EL2 vector hardening",
 		.capability = ARM64_HARDEN_EL2_VECTORS,
-		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		ERRATA_MIDR_RANGE_LIST(arm64_harden_el2_vectors),
 	},
 #endif

commit 3b8c9f1cdfc506e94e992ae66b68bbe416f89610
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 11 14:22:09 2018 +0100

    arm64: IPI each CPU after invalidating the I-cache for kernel mappings
    
    When invalidating the instruction cache for a kernel mapping via
    flush_icache_range(), it is also necessary to flush the pipeline for
    other CPUs so that instructions fetched into the pipeline before the
    I-cache invalidation are discarded. For example, if module 'foo' is
    unloaded and then module 'bar' is loaded into the same area of memory,
    a CPU could end up executing instructions from 'foo' when branching into
    'bar' if these instructions were fetched into the pipeline before 'foo'
    was unloaded.
    
    Whilst this is highly unlikely to occur in practice, particularly as
    any exception acts as a context-synchronizing operation, following the
    letter of the architecture requires us to execute an ISB on each CPU
    in order for the new instruction stream to be visible.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 5d59ff9a8da9..459129712dfa 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -107,7 +107,7 @@ static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 	for (i = 0; i < SZ_2K; i += 0x80)
 		memcpy(dst + i, hyp_vecs_start, hyp_vecs_end - hyp_vecs_start);
 
-	flush_icache_range((uintptr_t)dst, (uintptr_t)dst + SZ_2K);
+	__flush_icache_range((uintptr_t)dst, (uintptr_t)dst + SZ_2K);
 }
 
 static void __install_bp_hardening_cb(bp_hardening_cb_t fn,

commit 314d53d297980676011e6fd83dac60db4a01dc70
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Jul 4 23:07:46 2018 +0100

    arm64: Handle mismatched cache type
    
    Track mismatches in the cache type register (CTR_EL0), other
    than the D/I min line sizes and trap user accesses if there are any.
    
    Fixes: be68a8aaf925 ("arm64: cpufeature: Fix CTR_EL0 field definitions")
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 5d1fa928ea4b..5d59ff9a8da9 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -65,11 +65,15 @@ is_kryo_midr(const struct arm64_cpu_capabilities *entry, int scope)
 }
 
 static bool
-has_mismatched_cache_line_size(const struct arm64_cpu_capabilities *entry,
-				int scope)
+has_mismatched_cache_type(const struct arm64_cpu_capabilities *entry,
+			  int scope)
 {
 	u64 mask = CTR_CACHE_MINLINE_MASK;
 
+	/* Skip matching the min line sizes for cache type check */
+	if (entry->capability == ARM64_MISMATCHED_CACHE_TYPE)
+		mask ^= arm64_ftr_reg_ctrel0.strict_mask;
+
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 	return (read_cpuid_cachetype() & mask) !=
 	       (arm64_ftr_reg_ctrel0.sys_val & mask);
@@ -615,7 +619,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Mismatched cache line size",
 		.capability = ARM64_MISMATCHED_CACHE_LINE_SIZE,
-		.matches = has_mismatched_cache_line_size,
+		.matches = has_mismatched_cache_type,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.cpu_enable = cpu_enable_trap_ctr_access,
+	},
+	{
+		.desc = "Mismatched cache type",
+		.capability = ARM64_MISMATCHED_CACHE_TYPE,
+		.matches = has_mismatched_cache_type,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.cpu_enable = cpu_enable_trap_ctr_access,
 	},

commit 4c4a39dd5fe2d13e2d2fa5fceb8ef95d19fc389a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Jul 4 23:07:45 2018 +0100

    arm64: Fix mismatched cache line size detection
    
    If there is a mismatch in the I/D min line size, we must
    always use the system wide safe value both in applications
    and in the kernel, while performing cache operations. However,
    we have been checking more bits than just the min line sizes,
    which triggers false negatives. We may need to trap the user
    accesses in such cases, but not necessarily patch the kernel.
    
    This patch fixes the check to do the right thing as advertised.
    A new capability will be added to check mismatches in other
    fields and ensure we trap the CTR accesses.
    
    Fixes: be68a8aaf925 ("arm64: cpufeature: Fix CTR_EL0 field definitions")
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 1d2b6d768efe..5d1fa928ea4b 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -68,9 +68,11 @@ static bool
 has_mismatched_cache_line_size(const struct arm64_cpu_capabilities *entry,
 				int scope)
 {
+	u64 mask = CTR_CACHE_MINLINE_MASK;
+
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
-	return (read_cpuid_cachetype() & arm64_ftr_reg_ctrel0.strict_mask) !=
-		(arm64_ftr_reg_ctrel0.sys_val & arm64_ftr_reg_ctrel0.strict_mask);
+	return (read_cpuid_cachetype() & mask) !=
+	       (arm64_ftr_reg_ctrel0.sys_val & mask);
 }
 
 static void

commit 410feb75de245664d66bc05ab2e2412751d10acf
Merge: 2996148a9d41 0fe42512b2f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 8 11:10:58 2018 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Apart from the core arm64 and perf changes, the Spectre v4 mitigation
      touches the arm KVM code and the ACPI PPTT support touches drivers/
      (acpi and cacheinfo). I should have the maintainers' acks in place.
    
      Summary:
    
       - Spectre v4 mitigation (Speculative Store Bypass Disable) support
         for arm64 using SMC firmware call to set a hardware chicken bit
    
       - ACPI PPTT (Processor Properties Topology Table) parsing support and
         enable the feature for arm64
    
       - Report signal frame size to user via auxv (AT_MINSIGSTKSZ). The
         primary motivation is Scalable Vector Extensions which requires
         more space on the signal frame than the currently defined
         MINSIGSTKSZ
    
       - ARM perf patches: allow building arm-cci as module, demote
         dev_warn() to dev_dbg() in arm-ccn event_init(), miscellaneous
         cleanups
    
       - cmpwait() WFE optimisation to avoid some spurious wakeups
    
       - L1_CACHE_BYTES reverted back to 64 (for performance reasons that
         have to do with some network allocations) while keeping
         ARCH_DMA_MINALIGN to 128. cache_line_size() returns the actual
         hardware Cache Writeback Granule
    
       - Turn LSE atomics on by default in Kconfig
    
       - Kernel fault reporting tidying
    
       - Some #include and miscellaneous cleanups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (53 commits)
      arm64: Fix syscall restarting around signal suppressed by tracer
      arm64: topology: Avoid checking numa mask for scheduler MC selection
      ACPI / PPTT: fix build when CONFIG_ACPI_PPTT is not enabled
      arm64: cpu_errata: include required headers
      arm64: KVM: Move VCPU_WORKAROUND_2_FLAG macros to the top of the file
      arm64: signal: Report signal frame size to userspace via auxv
      arm64/sve: Thin out initialisation sanity-checks for sve_max_vl
      arm64: KVM: Add ARCH_WORKAROUND_2 discovery through ARCH_FEATURES_FUNC_ID
      arm64: KVM: Handle guest's ARCH_WORKAROUND_2 requests
      arm64: KVM: Add ARCH_WORKAROUND_2 support for guests
      arm64: KVM: Add HYP per-cpu accessors
      arm64: ssbd: Add prctl interface for per-thread mitigation
      arm64: ssbd: Introduce thread flag to control userspace mitigation
      arm64: ssbd: Restore mitigation status on CPU resume
      arm64: ssbd: Skip apply_ssbd if not using dynamic mitigation
      arm64: ssbd: Add global mitigation state accessor
      arm64: Add 'ssbd' command-line option
      arm64: Add ARCH_WORKAROUND_2 probing
      arm64: Add per-cpu infrastructure to call ARCH_WORKAROUND_2
      arm64: Call ARCH_WORKAROUND_2 on transitions between EL0 and EL1
      ...

commit 94a5d8790e79ab78f499d2d9f1ff2cab63849d9f
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Jun 5 13:50:07 2018 +0200

    arm64: cpu_errata: include required headers
    
    Without including psci.h and arm-smccc.h, we now get a build failure in
    some configurations:
    
    arch/arm64/kernel/cpu_errata.c: In function 'arm64_update_smccc_conduit':
    arch/arm64/kernel/cpu_errata.c:278:10: error: 'psci_ops' undeclared (first use in this function); did you mean 'sysfs_ops'?
    
    arch/arm64/kernel/cpu_errata.c: In function 'arm64_set_ssbd_mitigation':
    arch/arm64/kernel/cpu_errata.c:311:3: error: implicit declaration of function 'arm_smccc_1_1_hvc' [-Werror=implicit-function-declaration]
       arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index cf37ca6fa5f2..97681274da6a 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -16,6 +16,8 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/arm-smccc.h>
+#include <linux/psci.h>
 #include <linux/types.h>
 #include <asm/cpu.h>
 #include <asm/cputype.h>

commit 647d0519b53f440a55df163de21c52a8205431cc
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:12 2018 +0100

    arm64: ssbd: Restore mitigation status on CPU resume
    
    On a system where firmware can dynamically change the state of the
    mitigation, the CPU will always come up with the mitigation enabled,
    including when coming back from suspend.
    
    If the user has requested "no mitigation" via a command line option,
    let's enforce it by calling into the firmware again to disable it.
    
    Similarily, for a resume from hibernate, the mitigation could have
    been disabled by the boot kernel. Let's ensure that it is set
    back on in that case.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 2797bc2c8c6a..cf37ca6fa5f2 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -303,7 +303,7 @@ void __init arm64_enable_wa2_handling(struct alt_instr *alt,
 		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
 }
 
-static void arm64_set_ssbd_mitigation(bool state)
+void arm64_set_ssbd_mitigation(bool state)
 {
 	switch (psci_ops.conduit) {
 	case PSCI_CONDUIT_HVC:

commit 986372c4367f46b34a3c0f6918d7fb95cbdf39d6
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:11 2018 +0100

    arm64: ssbd: Skip apply_ssbd if not using dynamic mitigation
    
    In order to avoid checking arm64_ssbd_callback_required on each
    kernel entry/exit even if no mitigation is required, let's
    add yet another alternative that by default jumps over the mitigation,
    and that gets nop'ed out if we're doing dynamic mitigation.
    
    Think of it as a poor man's static key...
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 1075f90fdd8c..2797bc2c8c6a 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -289,6 +289,20 @@ void __init arm64_update_smccc_conduit(struct alt_instr *alt,
 	*updptr = cpu_to_le32(insn);
 }
 
+void __init arm64_enable_wa2_handling(struct alt_instr *alt,
+				      __le32 *origptr, __le32 *updptr,
+				      int nr_inst)
+{
+	BUG_ON(nr_inst != 1);
+	/*
+	 * Only allow mitigation on EL1 entry/exit and guest
+	 * ARCH_WORKAROUND_2 handling if the SSBD state allows it to
+	 * be flipped.
+	 */
+	if (arm64_get_ssbd_state() == ARM64_SSBD_KERNEL)
+		*updptr = cpu_to_le32(aarch64_insn_gen_nop());
+}
+
 static void arm64_set_ssbd_mitigation(bool state)
 {
 	switch (psci_ops.conduit) {

commit a43ae4dfe56a01f5b98ba0cb2f784b6a43bafcc6
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:09 2018 +0100

    arm64: Add 'ssbd' command-line option
    
    On a system where the firmware implements ARCH_WORKAROUND_2,
    it may be useful to either permanently enable or disable the
    workaround for cases where the user decides that they'd rather
    not get a trap overhead, and keep the mitigation permanently
    on or off instead of switching it on exception entry/exit.
    
    In any case, default to the mitigation being enabled.
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 7e8f12d85d99..1075f90fdd8c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -235,6 +235,38 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 #ifdef CONFIG_ARM64_SSBD
 DEFINE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
 
+int ssbd_state __read_mostly = ARM64_SSBD_KERNEL;
+
+static const struct ssbd_options {
+	const char	*str;
+	int		state;
+} ssbd_options[] = {
+	{ "force-on",	ARM64_SSBD_FORCE_ENABLE, },
+	{ "force-off",	ARM64_SSBD_FORCE_DISABLE, },
+	{ "kernel",	ARM64_SSBD_KERNEL, },
+};
+
+static int __init ssbd_cfg(char *buf)
+{
+	int i;
+
+	if (!buf || !buf[0])
+		return -EINVAL;
+
+	for (i = 0; i < ARRAY_SIZE(ssbd_options); i++) {
+		int len = strlen(ssbd_options[i].str);
+
+		if (strncmp(buf, ssbd_options[i].str, len))
+			continue;
+
+		ssbd_state = ssbd_options[i].state;
+		return 0;
+	}
+
+	return -EINVAL;
+}
+early_param("ssbd", ssbd_cfg);
+
 void __init arm64_update_smccc_conduit(struct alt_instr *alt,
 				       __le32 *origptr, __le32 *updptr,
 				       int nr_inst)
@@ -278,44 +310,83 @@ static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
 				    int scope)
 {
 	struct arm_smccc_res res;
-	bool supported = true;
+	bool required = true;
+	s32 val;
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 
-	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
+	if (psci_ops.smccc_version == SMCCC_VERSION_1_0) {
+		ssbd_state = ARM64_SSBD_UNKNOWN;
 		return false;
+	}
 
-	/*
-	 * The probe function return value is either negative
-	 * (unsupported or mitigated), positive (unaffected), or zero
-	 * (requires mitigation). We only need to do anything in the
-	 * last case.
-	 */
 	switch (psci_ops.conduit) {
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
-		if ((int)res.a0 != 0)
-			supported = false;
 		break;
 
 	case PSCI_CONDUIT_SMC:
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
-		if ((int)res.a0 != 0)
-			supported = false;
 		break;
 
 	default:
-		supported = false;
+		ssbd_state = ARM64_SSBD_UNKNOWN;
+		return false;
 	}
 
-	if (supported) {
-		__this_cpu_write(arm64_ssbd_callback_required, 1);
+	val = (s32)res.a0;
+
+	switch (val) {
+	case SMCCC_RET_NOT_SUPPORTED:
+		ssbd_state = ARM64_SSBD_UNKNOWN;
+		return false;
+
+	case SMCCC_RET_NOT_REQUIRED:
+		pr_info_once("%s mitigation not required\n", entry->desc);
+		ssbd_state = ARM64_SSBD_MITIGATED;
+		return false;
+
+	case SMCCC_RET_SUCCESS:
+		required = true;
+		break;
+
+	case 1:	/* Mitigation not required on this CPU */
+		required = false;
+		break;
+
+	default:
+		WARN_ON(1);
+		return false;
+	}
+
+	switch (ssbd_state) {
+	case ARM64_SSBD_FORCE_DISABLE:
+		pr_info_once("%s disabled from command-line\n", entry->desc);
+		arm64_set_ssbd_mitigation(false);
+		required = false;
+		break;
+
+	case ARM64_SSBD_KERNEL:
+		if (required) {
+			__this_cpu_write(arm64_ssbd_callback_required, 1);
+			arm64_set_ssbd_mitigation(true);
+		}
+		break;
+
+	case ARM64_SSBD_FORCE_ENABLE:
+		pr_info_once("%s forced from command-line\n", entry->desc);
 		arm64_set_ssbd_mitigation(true);
+		required = true;
+		break;
+
+	default:
+		WARN_ON(1);
+		break;
 	}
 
-	return supported;
+	return required;
 }
 #endif	/* CONFIG_ARM64_SSBD */
 

commit a725e3dda1813ed306734823ac4c65ca04e38500
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:08 2018 +0100

    arm64: Add ARCH_WORKAROUND_2 probing
    
    As for Spectre variant-2, we rely on SMCCC 1.1 to provide the
    discovery mechanism for detecting the SSBD mitigation.
    
    A new capability is also allocated for that purpose, and a
    config option.
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index cd91ca0250f1..7e8f12d85d99 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -256,6 +256,67 @@ void __init arm64_update_smccc_conduit(struct alt_instr *alt,
 
 	*updptr = cpu_to_le32(insn);
 }
+
+static void arm64_set_ssbd_mitigation(bool state)
+{
+	switch (psci_ops.conduit) {
+	case PSCI_CONDUIT_HVC:
+		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
+		break;
+
+	case PSCI_CONDUIT_SMC:
+		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_2, state, NULL);
+		break;
+
+	default:
+		WARN_ON_ONCE(1);
+		break;
+	}
+}
+
+static bool has_ssbd_mitigation(const struct arm64_cpu_capabilities *entry,
+				    int scope)
+{
+	struct arm_smccc_res res;
+	bool supported = true;
+
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+
+	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
+		return false;
+
+	/*
+	 * The probe function return value is either negative
+	 * (unsupported or mitigated), positive (unaffected), or zero
+	 * (requires mitigation). We only need to do anything in the
+	 * last case.
+	 */
+	switch (psci_ops.conduit) {
+	case PSCI_CONDUIT_HVC:
+		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
+				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
+		if ((int)res.a0 != 0)
+			supported = false;
+		break;
+
+	case PSCI_CONDUIT_SMC:
+		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
+				  ARM_SMCCC_ARCH_WORKAROUND_2, &res);
+		if ((int)res.a0 != 0)
+			supported = false;
+		break;
+
+	default:
+		supported = false;
+	}
+
+	if (supported) {
+		__this_cpu_write(arm64_ssbd_callback_required, 1);
+		arm64_set_ssbd_mitigation(true);
+	}
+
+	return supported;
+}
 #endif	/* CONFIG_ARM64_SSBD */
 
 #define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
@@ -512,6 +573,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		ERRATA_MIDR_RANGE_LIST(arm64_harden_el2_vectors),
 	},
+#endif
+#ifdef CONFIG_ARM64_SSBD
+	{
+		.desc = "Speculative Store Bypass Disable",
+		.capability = ARM64_SSBD,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = has_ssbd_mitigation,
+	},
 #endif
 	{
 	}

commit 5cf9ce6e5ea50f805c6188c04ed0daaec7b6887d
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:07 2018 +0100

    arm64: Add per-cpu infrastructure to call ARCH_WORKAROUND_2
    
    In a heterogeneous system, we can end up with both affected and
    unaffected CPUs. Let's check their status before calling into the
    firmware.
    
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index c1eda6be7758..cd91ca0250f1 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -233,6 +233,8 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #ifdef CONFIG_ARM64_SSBD
+DEFINE_PER_CPU_READ_MOSTLY(u64, arm64_ssbd_callback_required);
+
 void __init arm64_update_smccc_conduit(struct alt_instr *alt,
 				       __le32 *origptr, __le32 *updptr,
 				       int nr_inst)

commit 8e2906245f1e3b0d027169d9f2e55ce0548cb96e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue May 29 13:11:06 2018 +0100

    arm64: Call ARCH_WORKAROUND_2 on transitions between EL0 and EL1
    
    In order for the kernel to protect itself, let's call the SSBD mitigation
    implemented by the higher exception level (either hypervisor or firmware)
    on each transition between userspace and kernel.
    
    We must take the PSCI conduit into account in order to target the
    right exception level, hence the introduction of a runtime patching
    callback.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a900befadfe8..c1eda6be7758 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -232,6 +232,30 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 }
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
+#ifdef CONFIG_ARM64_SSBD
+void __init arm64_update_smccc_conduit(struct alt_instr *alt,
+				       __le32 *origptr, __le32 *updptr,
+				       int nr_inst)
+{
+	u32 insn;
+
+	BUG_ON(nr_inst != 1);
+
+	switch (psci_ops.conduit) {
+	case PSCI_CONDUIT_HVC:
+		insn = aarch64_insn_get_hvc_value();
+		break;
+	case PSCI_CONDUIT_SMC:
+		insn = aarch64_insn_get_smc_value();
+		break;
+	default:
+		return;
+	}
+
+	*updptr = cpu_to_le32(insn);
+}
+#endif	/* CONFIG_ARM64_SSBD */
+
 #define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
 	.matches = is_affected_midr_range,			\
 	.midr_range = MIDR_RANGE(model, v_min, r_min, v_max, r_max)

commit 0583a4ef05987f7e0f3a7bdd3365e5dc36ca306d
Author: David Gilhooley <dgilhooley@nvidia.com>
Date:   Tue May 8 15:49:43 2018 -0700

    arm64: capabilities: Add NVIDIA Denver CPU to bp_harden list
    
    The NVIDIA Denver CPU also needs a PSCI call to harden the branch
    predictor.
    
    Signed-off-by: David Gilhooley <dgilhooley@nvidia.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a900befadfe8..e4a1182deff7 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -316,6 +316,7 @@ static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
 	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
 	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+	MIDR_ALL_VERSIONS(MIDR_NVIDIA_DENVER),
 	{},
 };
 

commit e8b22d0f4500c7bb6aab879d4e32b2a00c89d5f8
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 10 11:36:45 2018 +0100

    arm64: Move the content of bpi.S to hyp-entry.S
    
    bpi.S was introduced as we were starting to build the Spectre v2
    mitigation framework, and it was rather unclear that it would
    become strictly KVM specific.
    
    Now that the picture is a lot clearer, let's move the content
    of that file to hyp-entry.S, where it actually belong.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index fc9d017f2d33..a900befadfe8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -86,7 +86,7 @@ atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
 
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
-#ifdef CONFIG_KVM
+#ifdef CONFIG_KVM_INDIRECT_VECTORS
 extern char __smccc_workaround_1_smc_start[];
 extern char __smccc_workaround_1_smc_end[];
 
@@ -137,7 +137,7 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 {
 	__this_cpu_write(bp_hardening_data.fn, fn);
 }
-#endif	/* CONFIG_KVM */
+#endif	/* CONFIG_KVM_INDIRECT_VECTORS */
 
 static void  install_bp_hardening_cb(const struct arm64_cpu_capabilities *entry,
 				     bp_hardening_cb_t fn,

commit 22765f30dbaf1118c6ff0fcb8b99c9f2b4d396d5
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 10 11:36:44 2018 +0100

    arm64: Get rid of __smccc_workaround_1_hvc_*
    
    The very existence of __smccc_workaround_1_hvc_* is a thinko, as
    KVM will never use a HVC call to perform the branch prediction
    invalidation. Even as a nested hypervisor, it would use an SMC
    instruction.
    
    Let's get rid of it.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 4e9f6a36d5e6..fc9d017f2d33 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -89,8 +89,6 @@ DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 #ifdef CONFIG_KVM
 extern char __smccc_workaround_1_smc_start[];
 extern char __smccc_workaround_1_smc_end[];
-extern char __smccc_workaround_1_hvc_start[];
-extern char __smccc_workaround_1_hvc_end[];
 
 static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 				const char *hyp_vecs_end)
@@ -132,8 +130,6 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 #else
 #define __smccc_workaround_1_smc_start		NULL
 #define __smccc_workaround_1_smc_end		NULL
-#define __smccc_workaround_1_hvc_start		NULL
-#define __smccc_workaround_1_hvc_end		NULL
 
 static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
@@ -207,8 +203,9 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 		if ((int)res.a0 < 0)
 			return;
 		cb = call_hvc_arch_workaround_1;
-		smccc_start = __smccc_workaround_1_hvc_start;
-		smccc_end = __smccc_workaround_1_hvc_end;
+		/* This is a guest, no need to patch KVM vectors */
+		smccc_start = NULL;
+		smccc_end = NULL;
 		break;
 
 	case PSCI_CONDUIT_SMC:

commit 8892b71885df7d6d7b0f491f9a8e2bb12fd4afdd
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Apr 10 11:36:43 2018 +0100

    arm64: capabilities: Rework EL2 vector hardening entry
    
    Since 5e7951ce19ab ("arm64: capabilities: Clean up midr range helpers"),
    capabilities must be represented with a single entry. If multiple
    CPU types can use the same capability, then they need to be enumerated
    in a list.
    
    The EL2 hardening stuff (which affects both A57 and A72) managed to
    escape the conversion in the above patch thanks to the 4.17 merge
    window. Let's fix it now.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 103c53fb0b4d..4e9f6a36d5e6 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -324,8 +324,14 @@ static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
 
 #endif
 
-#ifndef ERRATA_MIDR_ALL_VERSIONS
-#define	ERRATA_MIDR_ALL_VERSIONS(x)	MIDR_ALL_VERSIONS(x)
+#ifdef CONFIG_HARDEN_EL2_VECTORS
+
+static const struct midr_range arm64_harden_el2_vectors[] = {
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+	{},
+};
+
 #endif
 
 const struct arm64_cpu_capabilities arm64_errata[] = {
@@ -478,14 +484,10 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #endif
 #ifdef CONFIG_HARDEN_EL2_VECTORS
 	{
-		.desc = "Cortex-A57 EL2 vector hardening",
+		.desc = "EL2 vector hardening",
 		.capability = ARM64_HARDEN_EL2_VECTORS,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
-	},
-	{
-		.desc = "Cortex-A72 EL2 vector hardening",
-		.capability = ARM64_HARDEN_EL2_VECTORS,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		ERRATA_MIDR_RANGE_LIST(arm64_harden_el2_vectors),
 	},
 #endif
 	{

commit 4bc352ffb39e4eec253e70f8c076f2f48a6c1926
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Tue Apr 10 11:36:42 2018 +0100

    arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening
    
    The function SMCCC_ARCH_WORKAROUND_1 was introduced as part of SMC
    V1.1 Calling Convention to mitigate CVE-2017-5715. This patch uses
    the standard call SMCCC_ARCH_WORKAROUND_1 for Falkor chips instead
    of Silicon provider service ID 0xC2001700.
    
    Cc: <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    [maz: reworked errata framework integration]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9262ec57f5ab..103c53fb0b4d 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -87,8 +87,6 @@ atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM
-extern char __qcom_hyp_sanitize_link_stack_start[];
-extern char __qcom_hyp_sanitize_link_stack_end[];
 extern char __smccc_workaround_1_smc_start[];
 extern char __smccc_workaround_1_smc_end[];
 extern char __smccc_workaround_1_hvc_start[];
@@ -132,8 +130,6 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	spin_unlock(&bp_lock);
 }
 #else
-#define __qcom_hyp_sanitize_link_stack_start	NULL
-#define __qcom_hyp_sanitize_link_stack_end	NULL
 #define __smccc_workaround_1_smc_start		NULL
 #define __smccc_workaround_1_smc_end		NULL
 #define __smccc_workaround_1_hvc_start		NULL
@@ -178,12 +174,25 @@ static void call_hvc_arch_workaround_1(void)
 	arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
 }
 
+static void qcom_link_stack_sanitization(void)
+{
+	u64 tmp;
+
+	asm volatile("mov	%0, x30		\n"
+		     ".rept	16		\n"
+		     "bl	. + 4		\n"
+		     ".endr			\n"
+		     "mov	x30, %0		\n"
+		     : "=&r" (tmp));
+}
+
 static void
 enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 {
 	bp_hardening_cb_t cb;
 	void *smccc_start, *smccc_end;
 	struct arm_smccc_res res;
+	u32 midr = read_cpuid_id();
 
 	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
 		return;
@@ -216,30 +225,14 @@ enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 		return;
 	}
 
+	if (((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR) ||
+	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
+		cb = qcom_link_stack_sanitization;
+
 	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
 
 	return;
 }
-
-static void qcom_link_stack_sanitization(void)
-{
-	u64 tmp;
-
-	asm volatile("mov	%0, x30		\n"
-		     ".rept	16		\n"
-		     "bl	. + 4		\n"
-		     ".endr			\n"
-		     "mov	x30, %0		\n"
-		     : "=&r" (tmp));
-}
-
-static void
-qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
-{
-	install_bp_hardening_cb(entry, qcom_link_stack_sanitization,
-				__qcom_hyp_sanitize_link_stack_start,
-				__qcom_hyp_sanitize_link_stack_end);
-}
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
@@ -324,27 +317,11 @@ static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
 	MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
 	MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
 	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
-	{},
-};
-
-static const struct midr_range qcom_bp_harden_cpus[] = {
 	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
 	{},
 };
 
-static const struct arm64_cpu_capabilities arm64_bp_harden_list[] = {
-	{
-		CAP_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
-		.cpu_enable = enable_smccc_arch_workaround_1,
-	},
-	{
-		CAP_MIDR_RANGE_LIST(qcom_bp_harden_cpus),
-		.cpu_enable = qcom_enable_link_stack_sanitization,
-	},
-	{},
-};
-
 #endif
 
 #ifndef ERRATA_MIDR_ALL_VERSIONS
@@ -495,13 +472,8 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
-		.matches = multi_entry_cap_matches,
-		.cpu_enable = multi_entry_cap_cpu_enable,
-		.match_list = arm64_bp_harden_list,
-	},
-	{
-		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		ERRATA_MIDR_RANGE_LIST(qcom_bp_harden_cpus),
+		.cpu_enable = enable_smccc_arch_workaround_1,
+		ERRATA_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
 	},
 #endif
 #ifdef CONFIG_HARDEN_EL2_VECTORS

commit d8312a3f61024352f1c7cb967571fd53631b0d6c
Merge: e9092d0d9796 e01bca2fc698
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 9 11:42:31 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - VHE optimizations
    
       - EL2 address space randomization
    
       - speculative execution mitigations ("variant 3a", aka execution past
         invalid privilege register access)
    
       - bugfixes and cleanups
    
      PPC:
       - improvements for the radix page fault handler for HV KVM on POWER9
    
      s390:
       - more kvm stat counters
    
       - virtio gpu plumbing
    
       - documentation
    
       - facilities improvements
    
      x86:
       - support for VMware magic I/O port and pseudo-PMCs
    
       - AMD pause loop exiting
    
       - support for AMD core performance extensions
    
       - support for synchronous register access
    
       - expose nVMX capabilities to userspace
    
       - support for Hyper-V signaling via eventfd
    
       - use Enlightened VMCS when running on Hyper-V
    
       - allow userspace to disable MWAIT/HLT/PAUSE vmexits
    
       - usual roundup of optimizations and nested virtualization bugfixes
    
      Generic:
       - API selftest infrastructure (though the only tests are for x86 as
         of now)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (174 commits)
      kvm: x86: fix a prototype warning
      kvm: selftests: add sync_regs_test
      kvm: selftests: add API testing infrastructure
      kvm: x86: fix a compile warning
      KVM: X86: Add Force Emulation Prefix for "emulate the next instruction"
      KVM: X86: Introduce handle_ud()
      KVM: vmx: unify adjacent #ifdefs
      x86: kvm: hide the unused 'cpu' variable
      KVM: VMX: remove bogus WARN_ON in handle_ept_misconfig
      Revert "KVM: X86: Fix SMRAM accessing even if VM is shutdown"
      kvm: Add emulation for movups/movupd
      KVM: VMX: raise internal error for exception during invalid protected mode state
      KVM: nVMX: Optimization: Dont set KVM_REQ_EVENT when VMExit with nested_run_pending
      KVM: nVMX: Require immediate-exit when event reinjected to L2 and L1 event pending
      KVM: x86: Fix misleading comments on handling pending exceptions
      KVM: x86: Rename interrupt.pending to interrupt.injected
      KVM: VMX: No need to clear pending NMI/interrupt on inject realmode interrupt
      x86/kvm: use Enlightened VMCS when running on Hyper-V
      x86/hyper-v: detect nested features
      x86/hyper-v: define struct hv_enlightened_vmcs and clean field bits
      ...

commit 23221d997b3d28cb80c4d4d1b4bd36610f8e12fc
Merge: 5b1f3dc927a2 65896545b69f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 4 16:01:43 2018 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Nothing particularly stands out here, probably because people were
      tied up with spectre/meltdown stuff last time around. Still, the main
      pieces are:
    
       - Rework of our CPU features framework so that we can whitelist CPUs
         that don't require kpti even in a heterogeneous system
    
       - Support for the IDC/DIC architecture extensions, which allow us to
         elide instruction and data cache maintenance when writing out
         instructions
    
       - Removal of the large memory model which resulted in suboptimal
         codegen by the compiler and increased the use of literal pools,
         which could potentially be used as ROP gadgets since they are
         mapped as executable
    
       - Rework of forced signal delivery so that the siginfo_t is
         well-formed and handling of show_unhandled_signals is consolidated
         and made consistent between different fault types
    
       - More siginfo cleanup based on the initial patches from Eric
         Biederman
    
       - Workaround for Cortex-A55 erratum #1024718
    
       - Some small ACPI IORT updates and cleanups from Lorenzo Pieralisi
    
       - Misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (70 commits)
      arm64: uaccess: Fix omissions from usercopy whitelist
      arm64: fpsimd: Split cpu field out from struct fpsimd_state
      arm64: tlbflush: avoid writing RES0 bits
      arm64: cmpxchg: Include linux/compiler.h in asm/cmpxchg.h
      arm64: move percpu cmpxchg implementation from cmpxchg.h to percpu.h
      arm64: cmpxchg: Include build_bug.h instead of bug.h for BUILD_BUG
      arm64: lse: Include compiler_types.h and export.h for out-of-line LL/SC
      arm64: fpsimd: include <linux/init.h> in fpsimd.h
      drivers/perf: arm_pmu_platform: do not warn about affinity on uniprocessor
      perf: arm_spe: include linux/vmalloc.h for vmap()
      Revert "arm64: Revert L1_CACHE_SHIFT back to 6 (64-byte cache line size)"
      arm64: cpufeature: Avoid warnings due to unused symbols
      arm64: Add work around for Arm Cortex-A55 Erratum 1024718
      arm64: Delay enabling hardware DBM feature
      arm64: Add MIDR encoding for Arm Cortex-A55 and Cortex-A35
      arm64: capabilities: Handle shared entries
      arm64: capabilities: Add support for checks based on a list of MIDRs
      arm64: Add helpers for checking CPU MIDR against a range
      arm64: capabilities: Clean up midr range helpers
      arm64: capabilities: Change scope of VHE to Boot CPU feature
      ...

commit dc6ed61d2f824a595033744fc1f3bf4cb98768b5
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Mar 28 12:46:07 2018 +0100

    arm64: Add temporary ERRATA_MIDR_ALL_VERSIONS compatibility macro
    
    MIDR_ALL_VERSIONS is changing, and won't have the same meaning
    in 4.17, and the right thing to use will be ERRATA_MIDR_ALL_VERSIONS.
    
    In order to cope with the merge window, let's add a compatibility
    macro that will allow a relatively smooth transition, and that
    can be removed post 4.17-rc1.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index aacdc118c4c9..7369ad552700 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -241,6 +241,10 @@ static int qcom_enable_link_stack_sanitization(void *data)
 	.midr_range_min = 0, \
 	.midr_range_max = (MIDR_VARIANT_MASK | MIDR_REVISION_MASK)
 
+#ifndef ERRATA_MIDR_ALL_VERSIONS
+#define	ERRATA_MIDR_ALL_VERSIONS(x)	MIDR_ALL_VERSIONS(x)
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \
@@ -429,12 +433,12 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Cortex-A57 EL2 vector hardening",
 		.capability = ARM64_HARDEN_EL2_VECTORS,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
 	},
 	{
 		.desc = "Cortex-A72 EL2 vector hardening",
 		.capability = ARM64_HARDEN_EL2_VECTORS,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
 	},
 #endif
 	{

commit adc91ab7854195f107c137aa197ddfe8b82a2331
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Mar 28 11:59:13 2018 +0100

    Revert "arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening"
    
    Creates far too many conflicts with arm64/for-next/core, to be
    resent post -rc1.
    
    This reverts commit f9f5dc19509bbef6f5e675346f1a7d7b846bdb12.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index caa73af7d26e..aacdc118c4c9 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -69,6 +69,8 @@ atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM
+extern char __qcom_hyp_sanitize_link_stack_start[];
+extern char __qcom_hyp_sanitize_link_stack_end[];
 extern char __smccc_workaround_1_smc_start[];
 extern char __smccc_workaround_1_smc_end[];
 extern char __smccc_workaround_1_hvc_start[];
@@ -112,6 +114,8 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	spin_unlock(&bp_lock);
 }
 #else
+#define __qcom_hyp_sanitize_link_stack_start	NULL
+#define __qcom_hyp_sanitize_link_stack_end	NULL
 #define __smccc_workaround_1_smc_start		NULL
 #define __smccc_workaround_1_smc_end		NULL
 #define __smccc_workaround_1_hvc_start		NULL
@@ -156,25 +160,12 @@ static void call_hvc_arch_workaround_1(void)
 	arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
 }
 
-static void qcom_link_stack_sanitization(void)
-{
-	u64 tmp;
-
-	asm volatile("mov	%0, x30		\n"
-		     ".rept	16		\n"
-		     "bl	. + 4		\n"
-		     ".endr			\n"
-		     "mov	x30, %0		\n"
-		     : "=&r" (tmp));
-}
-
 static int enable_smccc_arch_workaround_1(void *data)
 {
 	const struct arm64_cpu_capabilities *entry = data;
 	bp_hardening_cb_t cb;
 	void *smccc_start, *smccc_end;
 	struct arm_smccc_res res;
-	u32 midr = read_cpuid_id();
 
 	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
 		return 0;
@@ -207,15 +198,33 @@ static int enable_smccc_arch_workaround_1(void *data)
 		return 0;
 	}
 
-	if (((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR) ||
-	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
-		cb = qcom_link_stack_sanitization;
-
 	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
 
 	return 0;
 }
 
+static void qcom_link_stack_sanitization(void)
+{
+	u64 tmp;
+
+	asm volatile("mov	%0, x30		\n"
+		     ".rept	16		\n"
+		     "bl	. + 4		\n"
+		     ".endr			\n"
+		     "mov	x30, %0		\n"
+		     : "=&r" (tmp));
+}
+
+static int qcom_enable_link_stack_sanitization(void *data)
+{
+	const struct arm64_cpu_capabilities *entry = data;
+
+	install_bp_hardening_cb(entry, qcom_link_stack_sanitization,
+				__qcom_hyp_sanitize_link_stack_start,
+				__qcom_hyp_sanitize_link_stack_end);
+
+	return 0;
+}
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define MIDR_RANGE(model, min, max) \
@@ -390,12 +399,20 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
-		.enable = enable_smccc_arch_workaround_1,
+		.enable = qcom_enable_link_stack_sanitization,
+	},
+	{
+		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
+		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
-		.enable = enable_smccc_arch_workaround_1,
+		.enable = qcom_enable_link_stack_sanitization,
+	},
+	{
+		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
+		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,

commit 12eb369125abe92bfc55e9ce198200f5807b63ff
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Mar 27 11:51:12 2018 +0100

    arm64: cpufeature: Avoid warnings due to unused symbols
    
    An allnoconfig build complains about unused symbols due to functions
    that are called via conditional cpufeature and cpu_errata table entries.
    
    Annotate these as __maybe_unused if they are likely to be generic, or
    predicate their compilation on the same option as the table entry if
    they are specific to a given alternative.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 6de823a1be10..4613e4d75c73 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -285,8 +285,8 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
  * of call backs, sharing the same capability bit.
  * Iterate over each entry to see if at least one matches.
  */
-static bool multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
-				    int scope)
+static bool __maybe_unused
+multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry, int scope)
 {
 	const struct arm64_cpu_capabilities *caps;
 
@@ -301,7 +301,7 @@ static bool multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
  * Take appropriate action for all matching entries in the shared capability
  * entry.
  */
-static void
+static void __maybe_unused
 multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
 {
 	const struct arm64_cpu_capabilities *caps;

commit ba7d9233c21997eb7eb8514cfb21ff46247dc162
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:46 2018 +0100

    arm64: capabilities: Handle shared entries
    
    Some capabilities have different criteria for detection and associated
    actions based on the matching criteria, even though they all share the
    same capability bit. So far we have used multiple entries with the same
    capability bit to handle this. This is prone to errors, as the
    cpu_enable is invoked for each entry, irrespective of whether the
    detection rule applies to the CPU or not. And also this complicates
    other helpers, e.g, __this_cpu_has_cap.
    
    This patch adds a wrapper entry to cover all the possible variations
    of a capability by maintaining list of matches + cpu_enable callbacks.
    To avoid complicating the prototypes for the "matches()", we use
    arm64_cpu_capabilities maintain the list and we ignore all the other
    fields except the matches & cpu_enable.
    
    This ensures :
    
     1) The capabilitiy is set when at least one of the entry detects
     2) Action is only taken for the entries that "matches".
    
    This avoids explicit checks in the cpu_enable() take some action.
    The only constraint here is that, all the entries should have the
    same "type" (i.e, scope and conflict rules).
    
    If a cpu_enable() method is associated with multiple matches for a
    single capability, care should be taken that either the match criteria
    are mutually exclusive, or that the method is robust against being
    called multiple times.
    
    This also reverts the changes introduced by commit 67948af41f2e6818ed
    ("arm64: capabilities: Handle duplicate entries for a capability").
    
    Cc: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9490b560d3fe..6de823a1be10 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -280,6 +280,38 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
 	CAP_MIDR_RANGE_LIST(midr_list)
 
+/*
+ * Generic helper for handling capabilties with multiple (match,enable) pairs
+ * of call backs, sharing the same capability bit.
+ * Iterate over each entry to see if at least one matches.
+ */
+static bool multi_entry_cap_matches(const struct arm64_cpu_capabilities *entry,
+				    int scope)
+{
+	const struct arm64_cpu_capabilities *caps;
+
+	for (caps = entry->match_list; caps->matches; caps++)
+		if (caps->matches(caps, scope))
+			return true;
+
+	return false;
+}
+
+/*
+ * Take appropriate action for all matching entries in the shared capability
+ * entry.
+ */
+static void
+multi_entry_cap_cpu_enable(const struct arm64_cpu_capabilities *entry)
+{
+	const struct arm64_cpu_capabilities *caps;
+
+	for (caps = entry->match_list; caps->matches; caps++)
+		if (caps->matches(caps, SCOPE_LOCAL_CPU) &&
+		    caps->cpu_enable)
+			caps->cpu_enable(caps);
+}
+
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 
 /*
@@ -302,6 +334,18 @@ static const struct midr_range qcom_bp_harden_cpus[] = {
 	{},
 };
 
+static const struct arm64_cpu_capabilities arm64_bp_harden_list[] = {
+	{
+		CAP_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
+		.cpu_enable = enable_smccc_arch_workaround_1,
+	},
+	{
+		CAP_MIDR_RANGE_LIST(qcom_bp_harden_cpus),
+		.cpu_enable = qcom_enable_link_stack_sanitization,
+	},
+	{},
+};
+
 #endif
 
 const struct arm64_cpu_capabilities arm64_errata[] = {
@@ -447,13 +491,10 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
-		.cpu_enable = enable_smccc_arch_workaround_1,
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_RANGE_LIST(qcom_bp_harden_cpus),
-		.cpu_enable = qcom_enable_link_stack_sanitization,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
+		.matches = multi_entry_cap_matches,
+		.cpu_enable = multi_entry_cap_cpu_enable,
+		.match_list = arm64_bp_harden_list,
 	},
 	{
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,

commit be5b299830c63ed76e0357473c4218c85fb388b3
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:45 2018 +0100

    arm64: capabilities: Add support for checks based on a list of MIDRs
    
    Add helpers for detecting an errata on list of midr ranges
    of affected CPUs, with the same work around.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 3c0bb6c4ed02..9490b560d3fe 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -40,6 +40,14 @@ is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
 	return true;
 }
 
+static bool __maybe_unused
+is_affected_midr_range_list(const struct arm64_cpu_capabilities *entry,
+			    int scope)
+{
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+	return is_midr_in_range_list(read_cpuid_id(), entry->midr_range_list);
+}
+
 static bool __maybe_unused
 is_kryo_midr(const struct arm64_cpu_capabilities *entry, int scope)
 {
@@ -250,6 +258,10 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,				\
 	CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)
 
+#define CAP_MIDR_RANGE_LIST(list)				\
+	.matches = is_affected_midr_range_list,			\
+	.midr_range_list = list
+
 /* Errata affecting a range of revisions of  given model variant */
 #define ERRATA_MIDR_REV_RANGE(m, var, r_min, r_max)	 \
 	ERRATA_MIDR_RANGE(m, var, r_min, var, r_max)
@@ -263,6 +275,35 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
 	CAP_MIDR_ALL_VERSIONS(model)
 
+/* Errata affecting a list of midr ranges, with same work around */
+#define ERRATA_MIDR_RANGE_LIST(midr_list)			\
+	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
+	CAP_MIDR_RANGE_LIST(midr_list)
+
+#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
+
+/*
+ * List of CPUs where we need to issue a psci call to
+ * harden the branch predictor.
+ */
+static const struct midr_range arm64_bp_harden_smccc_cpus[] = {
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
+	MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
+	MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
+	MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
+	{},
+};
+
+static const struct midr_range qcom_bp_harden_cpus[] = {
+	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+	MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+	{},
+};
+
+#endif
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \
@@ -406,51 +447,17 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
-		.cpu_enable = enable_smccc_arch_workaround_1,
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
-		.cpu_enable = enable_smccc_arch_workaround_1,
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
-		.cpu_enable = enable_smccc_arch_workaround_1,
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
+		ERRATA_MIDR_RANGE_LIST(arm64_bp_harden_smccc_cpus),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+		ERRATA_MIDR_RANGE_LIST(qcom_bp_harden_cpus),
 		.cpu_enable = qcom_enable_link_stack_sanitization,
 	},
 	{
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
-		.cpu_enable = qcom_enable_link_stack_sanitization,
-	},
-	{
-		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
-		.cpu_enable = enable_smccc_arch_workaround_1,
-	},
-	{
-		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		ERRATA_MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
-		.cpu_enable = enable_smccc_arch_workaround_1,
+		ERRATA_MIDR_RANGE_LIST(qcom_bp_harden_cpus),
 	},
 #endif
 	{

commit 1df310505d6d544802016f6bae49aab836ae8510
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:44 2018 +0100

    arm64: Add helpers for checking CPU MIDR against a range
    
    Add helpers for checking if the given CPU midr falls in a range
    of variants/revisions for a given model.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9ea14954972c..3c0bb6c4ed02 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -28,9 +28,7 @@ is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
 	u32 midr = read_cpuid_id(), revidr;
 
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
-	if (!MIDR_IS_CPU_MODEL_RANGE(midr, entry->midr_model,
-				     entry->midr_range_min,
-				     entry->midr_range_max))
+	if (!is_midr_in_range(midr, &entry->midr_range))
 		return false;
 
 	midr &= MIDR_REVISION_MASK | MIDR_VARIANT_MASK;
@@ -53,7 +51,7 @@ is_kryo_midr(const struct arm64_cpu_capabilities *entry, int scope)
 	model &= MIDR_IMPLEMENTOR_MASK | (0xf00 << MIDR_PARTNUM_SHIFT) |
 		 MIDR_ARCHITECTURE_MASK;
 
-	return model == entry->midr_model;
+	return model == entry->midr_range.model;
 }
 
 static bool
@@ -239,15 +237,11 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 
 #define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
 	.matches = is_affected_midr_range,			\
-	.midr_model = model,					\
-	.midr_range_min = MIDR_CPU_VAR_REV(v_min, r_min),	\
-	.midr_range_max = MIDR_CPU_VAR_REV(v_max, r_max)
+	.midr_range = MIDR_RANGE(model, v_min, r_min, v_max, r_max)
 
 #define CAP_MIDR_ALL_VERSIONS(model)					\
 	.matches = is_affected_midr_range,				\
-	.midr_model = model,						\
-	.midr_range_min = MIDR_CPU_VAR_REV(0, 0),			\
-	.midr_range_max = (MIDR_VARIANT_MASK | MIDR_REVISION_MASK)
+	.midr_range = MIDR_ALL_VERSIONS(model)
 
 #define MIDR_FIXED(rev, revidr_mask) \
 	.fixed_revs = (struct arm64_midr_revidr[]){{ (rev), (revidr_mask) }, {}}
@@ -390,7 +384,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "Qualcomm Technologies Kryo erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
 		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
-		.midr_model = MIDR_QCOM_KRYO,
+		.midr_range.model = MIDR_QCOM_KRYO,
 		.matches = is_kryo_midr,
 	},
 #endif

commit 5e7951ce19abf4113645ae789c033917356ee96f
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:43 2018 +0100

    arm64: capabilities: Clean up midr range helpers
    
    We are about to introduce generic MIDR range helpers. Clean
    up the existing helpers in erratum handling, preparing them
    to use generic version.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index f5ab9545c5ea..9ea14954972c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -237,23 +237,38 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 }
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
-#define MIDR_RANGE(model, min, max) \
-	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM, \
-	.matches = is_affected_midr_range, \
-	.midr_model = model, \
-	.midr_range_min = min, \
-	.midr_range_max = max
-
-#define MIDR_ALL_VERSIONS(model) \
-	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM, \
-	.matches = is_affected_midr_range, \
-	.midr_model = model, \
-	.midr_range_min = 0, \
+#define CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)	\
+	.matches = is_affected_midr_range,			\
+	.midr_model = model,					\
+	.midr_range_min = MIDR_CPU_VAR_REV(v_min, r_min),	\
+	.midr_range_max = MIDR_CPU_VAR_REV(v_max, r_max)
+
+#define CAP_MIDR_ALL_VERSIONS(model)					\
+	.matches = is_affected_midr_range,				\
+	.midr_model = model,						\
+	.midr_range_min = MIDR_CPU_VAR_REV(0, 0),			\
 	.midr_range_max = (MIDR_VARIANT_MASK | MIDR_REVISION_MASK)
 
 #define MIDR_FIXED(rev, revidr_mask) \
 	.fixed_revs = (struct arm64_midr_revidr[]){{ (rev), (revidr_mask) }, {}}
 
+#define ERRATA_MIDR_RANGE(model, v_min, r_min, v_max, r_max)		\
+	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,				\
+	CAP_MIDR_RANGE(model, v_min, r_min, v_max, r_max)
+
+/* Errata affecting a range of revisions of  given model variant */
+#define ERRATA_MIDR_REV_RANGE(m, var, r_min, r_max)	 \
+	ERRATA_MIDR_RANGE(m, var, r_min, var, r_max)
+
+/* Errata affecting a single variant/revision of a model */
+#define ERRATA_MIDR_REV(model, var, rev)	\
+	ERRATA_MIDR_RANGE(model, var, rev, var, rev)
+
+/* Errata affecting all variants/revisions of a given a model */
+#define ERRATA_MIDR_ALL_VERSIONS(model)				\
+	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,			\
+	CAP_MIDR_ALL_VERSIONS(model)
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \
@@ -262,7 +277,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A53 r0p[012] */
 		.desc = "ARM errata 826319, 827319, 824069",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
-		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x02),
+		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 2),
 		.cpu_enable = cpu_enable_cache_maint_trap,
 	},
 #endif
@@ -271,7 +286,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A53 r0p[01] */
 		.desc = "ARM errata 819472",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
-		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x01),
+		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 1),
 		.cpu_enable = cpu_enable_cache_maint_trap,
 	},
 #endif
@@ -280,9 +295,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A57 r0p0 - r1p2 */
 		.desc = "ARM erratum 832075",
 		.capability = ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE,
-		MIDR_RANGE(MIDR_CORTEX_A57,
-			   MIDR_CPU_VAR_REV(0, 0),
-			   MIDR_CPU_VAR_REV(1, 2)),
+		ERRATA_MIDR_RANGE(MIDR_CORTEX_A57,
+				  0, 0,
+				  1, 2),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_834220
@@ -290,9 +305,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A57 r0p0 - r1p2 */
 		.desc = "ARM erratum 834220",
 		.capability = ARM64_WORKAROUND_834220,
-		MIDR_RANGE(MIDR_CORTEX_A57,
-			   MIDR_CPU_VAR_REV(0, 0),
-			   MIDR_CPU_VAR_REV(1, 2)),
+		ERRATA_MIDR_RANGE(MIDR_CORTEX_A57,
+				  0, 0,
+				  1, 2),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_843419
@@ -300,7 +315,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A53 r0p[01234] */
 		.desc = "ARM erratum 843419",
 		.capability = ARM64_WORKAROUND_843419,
-		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x04),
+		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 4),
 		MIDR_FIXED(0x4, BIT(8)),
 	},
 #endif
@@ -309,7 +324,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A53 r0p[01234] */
 		.desc = "ARM erratum 845719",
 		.capability = ARM64_WORKAROUND_845719,
-		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x04),
+		ERRATA_MIDR_REV_RANGE(MIDR_CORTEX_A53, 0, 0, 4),
 	},
 #endif
 #ifdef CONFIG_CAVIUM_ERRATUM_23154
@@ -317,7 +332,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cavium ThunderX, pass 1.x */
 		.desc = "Cavium erratum 23154",
 		.capability = ARM64_WORKAROUND_CAVIUM_23154,
-		MIDR_RANGE(MIDR_THUNDERX, 0x00, 0x01),
+		ERRATA_MIDR_REV_RANGE(MIDR_THUNDERX, 0, 0, 1),
 	},
 #endif
 #ifdef CONFIG_CAVIUM_ERRATUM_27456
@@ -325,15 +340,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cavium ThunderX, T88 pass 1.x - 2.1 */
 		.desc = "Cavium erratum 27456",
 		.capability = ARM64_WORKAROUND_CAVIUM_27456,
-		MIDR_RANGE(MIDR_THUNDERX,
-			   MIDR_CPU_VAR_REV(0, 0),
-			   MIDR_CPU_VAR_REV(1, 1)),
+		ERRATA_MIDR_RANGE(MIDR_THUNDERX,
+				  0, 0,
+				  1, 1),
 	},
 	{
 	/* Cavium ThunderX, T81 pass 1.0 */
 		.desc = "Cavium erratum 27456",
 		.capability = ARM64_WORKAROUND_CAVIUM_27456,
-		MIDR_RANGE(MIDR_THUNDERX_81XX, 0x00, 0x00),
+		ERRATA_MIDR_REV(MIDR_THUNDERX_81XX, 0, 0),
 	},
 #endif
 #ifdef CONFIG_CAVIUM_ERRATUM_30115
@@ -341,20 +356,21 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cavium ThunderX, T88 pass 1.x - 2.2 */
 		.desc = "Cavium erratum 30115",
 		.capability = ARM64_WORKAROUND_CAVIUM_30115,
-		MIDR_RANGE(MIDR_THUNDERX, 0x00,
-			   (1 << MIDR_VARIANT_SHIFT) | 2),
+		ERRATA_MIDR_RANGE(MIDR_THUNDERX,
+				      0, 0,
+				      1, 2),
 	},
 	{
 	/* Cavium ThunderX, T81 pass 1.0 - 1.2 */
 		.desc = "Cavium erratum 30115",
 		.capability = ARM64_WORKAROUND_CAVIUM_30115,
-		MIDR_RANGE(MIDR_THUNDERX_81XX, 0x00, 0x02),
+		ERRATA_MIDR_REV_RANGE(MIDR_THUNDERX_81XX, 0, 0, 2),
 	},
 	{
 	/* Cavium ThunderX, T83 pass 1.0 */
 		.desc = "Cavium erratum 30115",
 		.capability = ARM64_WORKAROUND_CAVIUM_30115,
-		MIDR_RANGE(MIDR_THUNDERX_83XX, 0x00, 0x00),
+		ERRATA_MIDR_REV(MIDR_THUNDERX_83XX, 0, 0),
 	},
 #endif
 	{
@@ -368,9 +384,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm Technologies Falkor erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
-		MIDR_RANGE(MIDR_QCOM_FALKOR_V1,
-			   MIDR_CPU_VAR_REV(0, 0),
-			   MIDR_CPU_VAR_REV(0, 0)),
+		ERRATA_MIDR_REV(MIDR_QCOM_FALKOR_V1, 0, 0),
 	},
 	{
 		.desc = "Qualcomm Technologies Kryo erratum 1003",
@@ -384,9 +398,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm Technologies Falkor erratum 1009",
 		.capability = ARM64_WORKAROUND_REPEAT_TLBI,
-		MIDR_RANGE(MIDR_QCOM_FALKOR_V1,
-			   MIDR_CPU_VAR_REV(0, 0),
-			   MIDR_CPU_VAR_REV(0, 0)),
+		ERRATA_MIDR_REV(MIDR_QCOM_FALKOR_V1, 0, 0),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_858921
@@ -394,56 +406,56 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A73 all versions */
 		.desc = "ARM erratum 858921",
 		.capability = ARM64_WORKAROUND_858921,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
 	},
 #endif
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 		.cpu_enable = qcom_enable_link_stack_sanitization,
 	},
 	{
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
 		.cpu_enable = qcom_enable_link_stack_sanitization,
 	},
 	{
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
-		MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
+		ERRATA_MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
 		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 #endif

commit 5b4747c5dce7a873e1e7fe1608835825f714267a
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:32 2018 +0100

    arm64: capabilities: Add flags to handle the conflicts on late CPU
    
    When a CPU is brought up, it is checked against the caps that are
    known to be enabled on the system (via verify_local_cpu_capabilities()).
    Based on the state of the capability on the CPU vs. that of System we
    could have the following combinations of conflict.
    
            x-----------------------------x
            | Type  | System   | Late CPU |
            |-----------------------------|
            |  a    |   y      |    n     |
            |-----------------------------|
            |  b    |   n      |    y     |
            x-----------------------------x
    
    Case (a) is not permitted for caps which are system features, which the
    system expects all the CPUs to have (e.g VHE). While (a) is ignored for
    all errata work arounds. However, there could be exceptions to the plain
    filtering approach. e.g, KPTI is an optional feature for a late CPU as
    long as the system already enables it.
    
    Case (b) is not permitted for errata work arounds that cannot be activated
    after the kernel has finished booting.And we ignore (b) for features. Here,
    yet again, KPTI is an exception, where if a late CPU needs KPTI we are too
    late to enable it (because we change the allocation of ASIDs etc).
    
    Add two different flags to indicate how the conflict should be handled.
    
     ARM64_CPUCAP_PERMITTED_FOR_LATE_CPU - CPUs may have the capability
     ARM64_CPUCAP_OPTIONAL_FOR_LATE_CPU - CPUs may not have the cappability.
    
    Now that we have the flags to describe the behavior of the errata and
    the features, as we treat them, define types for ERRATUM and FEATURE.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 2b5233157af1..f5ab9545c5ea 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -238,14 +238,14 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define MIDR_RANGE(model, min, max) \
-	.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU, \
+	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM, \
 	.matches = is_affected_midr_range, \
 	.midr_model = model, \
 	.midr_range_min = min, \
 	.midr_range_max = max
 
 #define MIDR_ALL_VERSIONS(model) \
-	.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU, \
+	.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM, \
 	.matches = is_affected_midr_range, \
 	.midr_model = model, \
 	.midr_range_min = 0, \
@@ -361,7 +361,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "Mismatched cache line size",
 		.capability = ARM64_MISMATCHED_CACHE_LINE_SIZE,
 		.matches = has_mismatched_cache_line_size,
-		.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.cpu_enable = cpu_enable_trap_ctr_access,
 	},
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
@@ -375,7 +375,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm Technologies Kryo erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
-		.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU,
+		.type = ARM64_CPUCAP_LOCAL_CPU_ERRATUM,
 		.midr_model = MIDR_QCOM_KRYO,
 		.matches = is_kryo_midr,
 	},

commit 143ba05d867af34827faf99e0eed4de27106c7cb
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:31 2018 +0100

    arm64: capabilities: Prepare for fine grained capabilities
    
    We use arm64_cpu_capabilities to represent CPU ELF HWCAPs exposed
    to the userspace and the CPU hwcaps used by the kernel, which
    include cpu features and CPU errata work arounds. Capabilities
    have some properties that decide how they should be treated :
    
     1) Detection, i.e scope : A cap could be "detected" either :
        - if it is present on at least one CPU (SCOPE_LOCAL_CPU)
            Or
        - if it is present on all the CPUs (SCOPE_SYSTEM)
    
     2) When is it enabled ? - A cap is treated as "enabled" when the
      system takes some action based on whether the capability is detected or
      not. e.g, setting some control register, patching the kernel code.
      Right now, we treat all caps are enabled at boot-time, after all
      the CPUs are brought up by the kernel. But there are certain caps,
      which are enabled early during the boot (e.g, VHE, GIC_CPUIF for NMI)
      and kernel starts using them, even before the secondary CPUs are brought
      up. We would need a way to describe this for each capability.
    
     3) Conflict on a late CPU - When a CPU is brought up, it is checked
      against the caps that are known to be enabled on the system (via
      verify_local_cpu_capabilities()). Based on the state of the capability
      on the CPU vs. that of System we could have the following combinations
      of conflict.
    
            x-----------------------------x
            | Type  | System   | Late CPU |
            ------------------------------|
            |  a    |   y      |    n     |
            ------------------------------|
            |  b    |   n      |    y     |
            x-----------------------------x
    
      Case (a) is not permitted for caps which are system features, which the
      system expects all the CPUs to have (e.g VHE). While (a) is ignored for
      all errata work arounds. However, there could be exceptions to the plain
      filtering approach. e.g, KPTI is an optional feature for a late CPU as
      long as the system already enables it.
    
      Case (b) is not permitted for errata work arounds which requires some
      work around, which cannot be delayed. And we ignore (b) for features.
      Here, yet again, KPTI is an exception, where if a late CPU needs KPTI we
      are too late to enable it (because we change the allocation of ASIDs
      etc).
    
    So this calls for a lot more fine grained behavior for each capability.
    And if we define all the attributes to control their behavior properly,
    we may be able to use a single table for the CPU hwcaps (which cover
    errata and features, not the ELF HWCAPs). This is a prepartory step
    to get there. More bits would be added for the properties listed above.
    
    We are going to use a bit-mask to encode all the properties of a
    capabilities. This patch encodes the "SCOPE" of the capability.
    
    As such there is no change in how the capabilities are treated.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 0de0ab6a874c..2b5233157af1 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -238,14 +238,14 @@ qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define MIDR_RANGE(model, min, max) \
-	.def_scope = SCOPE_LOCAL_CPU, \
+	.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU, \
 	.matches = is_affected_midr_range, \
 	.midr_model = model, \
 	.midr_range_min = min, \
 	.midr_range_max = max
 
 #define MIDR_ALL_VERSIONS(model) \
-	.def_scope = SCOPE_LOCAL_CPU, \
+	.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU, \
 	.matches = is_affected_midr_range, \
 	.midr_model = model, \
 	.midr_range_min = 0, \
@@ -361,7 +361,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "Mismatched cache line size",
 		.capability = ARM64_MISMATCHED_CACHE_LINE_SIZE,
 		.matches = has_mismatched_cache_line_size,
-		.def_scope = SCOPE_LOCAL_CPU,
+		.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU,
 		.cpu_enable = cpu_enable_trap_ctr_access,
 	},
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
@@ -375,7 +375,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.desc = "Qualcomm Technologies Kryo erratum 1003",
 		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
-		.def_scope = SCOPE_LOCAL_CPU,
+		.type = ARM64_CPUCAP_SCOPE_LOCAL_CPU,
 		.midr_model = MIDR_QCOM_KRYO,
 		.matches = is_kryo_midr,
 	},

commit 1e89baed5d50d2b8d9fd420830902570270703f1
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:30 2018 +0100

    arm64: capabilities: Move errata processing code
    
    We have errata work around processing code in cpu_errata.c,
    which calls back into helpers defined in cpufeature.c. Now
    that we are going to make the handling of capabilities
    generic, by adding the information to each capability,
    move the errata work around specific processing code.
    No functional changes.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 74f058421dd8..0de0ab6a874c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -450,36 +450,3 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 	}
 };
-
-/*
- * The CPU Errata work arounds are detected and applied at boot time
- * and the related information is freed soon after. If the new CPU requires
- * an errata not detected at boot, fail this CPU.
- */
-void verify_local_cpu_errata_workarounds(void)
-{
-	const struct arm64_cpu_capabilities *caps = arm64_errata;
-
-	for (; caps->matches; caps++) {
-		if (cpus_have_cap(caps->capability)) {
-			if (caps->cpu_enable)
-				caps->cpu_enable(caps);
-		} else if (caps->matches(caps, SCOPE_LOCAL_CPU)) {
-			pr_crit("CPU%d: Requires work around for %s, not detected"
-					" at boot time\n",
-				smp_processor_id(),
-				caps->desc ? : "an erratum");
-			cpu_die_early();
-		}
-	}
-}
-
-void update_cpu_errata_workarounds(void)
-{
-	update_cpu_capabilities(arm64_errata, "enabling workaround for");
-}
-
-void __init enable_errata_workarounds(void)
-{
-	enable_cpu_capabilities(arm64_errata);
-}

commit c0cda3b8ee6b4b6851b2fd8b6db91fd7b0e2524a
Author: Dave Martin <dave.martin@arm.com>
Date:   Mon Mar 26 15:12:28 2018 +0100

    arm64: capabilities: Update prototype for enable call back
    
    We issue the enable() call back for all CPU hwcaps capabilities
    available on the system, on all the CPUs. So far we have ignored
    the argument passed to the call back, which had a prototype to
    accept a "void *" for use with on_each_cpu() and later with
    stop_machine(). However, with commit 0a0d111d40fd1
    ("arm64: cpufeature: Pass capability structure to ->enable callback"),
    there are some users of the argument who wants the matching capability
    struct pointer where there are multiple matching criteria for a single
    capability. Clean up the declaration of the call back to make it clear.
    
     1) Renamed to cpu_enable(), to imply taking necessary actions on the
        called CPU for the entry.
     2) Pass const pointer to the capability, to allow the call back to
        check the entry. (e.,g to check if any action is needed on the CPU)
     3) We don't care about the result of the call back, turning this to
        a void.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [suzuki: convert more users, rename call back and drop results]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 186c0fc61dcd..74f058421dd8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -65,11 +65,11 @@ has_mismatched_cache_line_size(const struct arm64_cpu_capabilities *entry,
 		(arm64_ftr_reg_ctrel0.sys_val & arm64_ftr_reg_ctrel0.strict_mask);
 }
 
-static int cpu_enable_trap_ctr_access(void *__unused)
+static void
+cpu_enable_trap_ctr_access(const struct arm64_cpu_capabilities *__unused)
 {
 	/* Clear SCTLR_EL1.UCT */
 	config_sctlr_el1(SCTLR_EL1_UCT, 0);
-	return 0;
 }
 
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
@@ -173,25 +173,25 @@ static void call_hvc_arch_workaround_1(void)
 	arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
 }
 
-static int enable_smccc_arch_workaround_1(void *data)
+static void
+enable_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
 {
-	const struct arm64_cpu_capabilities *entry = data;
 	bp_hardening_cb_t cb;
 	void *smccc_start, *smccc_end;
 	struct arm_smccc_res res;
 
 	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
-		return 0;
+		return;
 
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
-		return 0;
+		return;
 
 	switch (psci_ops.conduit) {
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		if (res.a0)
-			return 0;
+			return;
 		cb = call_hvc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_hvc_start;
 		smccc_end = __smccc_workaround_1_hvc_end;
@@ -201,19 +201,19 @@ static int enable_smccc_arch_workaround_1(void *data)
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		if (res.a0)
-			return 0;
+			return;
 		cb = call_smc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_smc_start;
 		smccc_end = __smccc_workaround_1_smc_end;
 		break;
 
 	default:
-		return 0;
+		return;
 	}
 
 	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
 
-	return 0;
+	return;
 }
 
 static void qcom_link_stack_sanitization(void)
@@ -228,15 +228,12 @@ static void qcom_link_stack_sanitization(void)
 		     : "=&r" (tmp));
 }
 
-static int qcom_enable_link_stack_sanitization(void *data)
+static void
+qcom_enable_link_stack_sanitization(const struct arm64_cpu_capabilities *entry)
 {
-	const struct arm64_cpu_capabilities *entry = data;
-
 	install_bp_hardening_cb(entry, qcom_link_stack_sanitization,
 				__qcom_hyp_sanitize_link_stack_start,
 				__qcom_hyp_sanitize_link_stack_end);
-
-	return 0;
 }
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
@@ -266,7 +263,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "ARM errata 826319, 827319, 824069",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x02),
-		.enable = cpu_enable_cache_maint_trap,
+		.cpu_enable = cpu_enable_cache_maint_trap,
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_819472
@@ -275,7 +272,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "ARM errata 819472",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x01),
-		.enable = cpu_enable_cache_maint_trap,
+		.cpu_enable = cpu_enable_cache_maint_trap,
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_832075
@@ -365,7 +362,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_MISMATCHED_CACHE_LINE_SIZE,
 		.matches = has_mismatched_cache_line_size,
 		.def_scope = SCOPE_LOCAL_CPU,
-		.enable = cpu_enable_trap_ctr_access,
+		.cpu_enable = cpu_enable_trap_ctr_access,
 	},
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
 	{
@@ -404,27 +401,27 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
-		.enable = enable_smccc_arch_workaround_1,
+		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
-		.enable = enable_smccc_arch_workaround_1,
+		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
-		.enable = enable_smccc_arch_workaround_1,
+		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
-		.enable = enable_smccc_arch_workaround_1,
+		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
-		.enable = qcom_enable_link_stack_sanitization,
+		.cpu_enable = qcom_enable_link_stack_sanitization,
 	},
 	{
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
@@ -433,7 +430,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
-		.enable = qcom_enable_link_stack_sanitization,
+		.cpu_enable = qcom_enable_link_stack_sanitization,
 	},
 	{
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
@@ -442,12 +439,12 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
-		.enable = enable_smccc_arch_workaround_1,
+		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
-		.enable = enable_smccc_arch_workaround_1,
+		.cpu_enable = enable_smccc_arch_workaround_1,
 	},
 #endif
 	{
@@ -465,8 +462,8 @@ void verify_local_cpu_errata_workarounds(void)
 
 	for (; caps->matches; caps++) {
 		if (cpus_have_cap(caps->capability)) {
-			if (caps->enable)
-				caps->enable((void *)caps);
+			if (caps->cpu_enable)
+				caps->cpu_enable(caps);
 		} else if (caps->matches(caps, SCOPE_LOCAL_CPU)) {
 			pr_crit("CPU%d: Requires work around for %s, not detected"
 					" at boot time\n",

commit f9f5dc19509bbef6f5e675346f1a7d7b846bdb12
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Mon Mar 5 11:06:43 2018 -0600

    arm64: KVM: Use SMCCC_ARCH_WORKAROUND_1 for Falkor BP hardening
    
    The function SMCCC_ARCH_WORKAROUND_1 was introduced as part of SMC
    V1.1 Calling Convention to mitigate CVE-2017-5715. This patch uses
    the standard call SMCCC_ARCH_WORKAROUND_1 for Falkor chips instead
    of Silicon provider service ID 0xC2001700.
    
    Cc: <stable@vger.kernel.org> # 4.14+
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index aacdc118c4c9..caa73af7d26e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -69,8 +69,6 @@ atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM
-extern char __qcom_hyp_sanitize_link_stack_start[];
-extern char __qcom_hyp_sanitize_link_stack_end[];
 extern char __smccc_workaround_1_smc_start[];
 extern char __smccc_workaround_1_smc_end[];
 extern char __smccc_workaround_1_hvc_start[];
@@ -114,8 +112,6 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	spin_unlock(&bp_lock);
 }
 #else
-#define __qcom_hyp_sanitize_link_stack_start	NULL
-#define __qcom_hyp_sanitize_link_stack_end	NULL
 #define __smccc_workaround_1_smc_start		NULL
 #define __smccc_workaround_1_smc_end		NULL
 #define __smccc_workaround_1_hvc_start		NULL
@@ -160,12 +156,25 @@ static void call_hvc_arch_workaround_1(void)
 	arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
 }
 
+static void qcom_link_stack_sanitization(void)
+{
+	u64 tmp;
+
+	asm volatile("mov	%0, x30		\n"
+		     ".rept	16		\n"
+		     "bl	. + 4		\n"
+		     ".endr			\n"
+		     "mov	x30, %0		\n"
+		     : "=&r" (tmp));
+}
+
 static int enable_smccc_arch_workaround_1(void *data)
 {
 	const struct arm64_cpu_capabilities *entry = data;
 	bp_hardening_cb_t cb;
 	void *smccc_start, *smccc_end;
 	struct arm_smccc_res res;
+	u32 midr = read_cpuid_id();
 
 	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
 		return 0;
@@ -198,33 +207,15 @@ static int enable_smccc_arch_workaround_1(void *data)
 		return 0;
 	}
 
+	if (((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR) ||
+	    ((midr & MIDR_CPU_MODEL_MASK) == MIDR_QCOM_FALKOR_V1))
+		cb = qcom_link_stack_sanitization;
+
 	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
 
 	return 0;
 }
 
-static void qcom_link_stack_sanitization(void)
-{
-	u64 tmp;
-
-	asm volatile("mov	%0, x30		\n"
-		     ".rept	16		\n"
-		     "bl	. + 4		\n"
-		     ".endr			\n"
-		     "mov	x30, %0		\n"
-		     : "=&r" (tmp));
-}
-
-static int qcom_enable_link_stack_sanitization(void *data)
-{
-	const struct arm64_cpu_capabilities *entry = data;
-
-	install_bp_hardening_cb(entry, qcom_link_stack_sanitization,
-				__qcom_hyp_sanitize_link_stack_start,
-				__qcom_hyp_sanitize_link_stack_end);
-
-	return 0;
-}
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define MIDR_RANGE(model, min, max) \
@@ -399,20 +390,12 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
-		.enable = qcom_enable_link_stack_sanitization,
-	},
-	{
-		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
-		.enable = qcom_enable_link_stack_sanitization,
-	},
-	{
-		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
-		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,

commit 4b472ffd1513407ee3607b8e638e8695e9b487ca
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Feb 15 11:49:20 2018 +0000

    arm64: Enable ARM64_HARDEN_EL2_VECTORS on Cortex-A57 and A72
    
    Cortex-A57 and A72 are vulnerable to the so-called "variant 3a" of
    Meltdown, where an attacker can speculatively obtain the value
    of a privileged system register.
    
    By enabling ARM64_HARDEN_EL2_VECTORS on these CPUs, obtaining
    VBAR_EL2 is not disclosing the hypervisor mappings anymore.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 8cf6b60a085a..aacdc118c4c9 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -424,6 +424,18 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
 		.enable = enable_smccc_arch_workaround_1,
 	},
+#endif
+#ifdef CONFIG_HARDEN_EL2_VECTORS
+	{
+		.desc = "Cortex-A57 EL2 vector hardening",
+		.capability = ARM64_HARDEN_EL2_VECTORS,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+	},
+	{
+		.desc = "Cortex-A72 EL2 vector hardening",
+		.capability = ARM64_HARDEN_EL2_VECTORS,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+	},
 #endif
 	{
 	}

commit 4205a89b8060141ac0216a507b9f70728f056a10
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Mar 13 12:40:39 2018 +0000

    arm64: Make BP hardening slot counter available
    
    We're about to need to allocate hardening slots from other parts
    of the kernel (in order to support ARM64_HARDEN_EL2_VECTORS).
    
    Turn the counter into an atomic_t and make it available to the
    rest of the kernel. Also add BP_HARDEN_EL2_SLOTS as the number of
    slots instead of the hardcoded 4...
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 52f15cd896e1..8cf6b60a085a 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -60,6 +60,8 @@ static int cpu_enable_trap_ctr_access(void *__unused)
 	return 0;
 }
 
+atomic_t arm64_el2_vector_last_slot = ATOMIC_INIT(-1);
+
 #ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
@@ -90,7 +92,6 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
 				      const char *hyp_vecs_end)
 {
-	static int last_slot = -1;
 	static DEFINE_SPINLOCK(bp_lock);
 	int cpu, slot = -1;
 
@@ -103,10 +104,8 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	}
 
 	if (slot == -1) {
-		last_slot++;
-		BUG_ON(((__bp_harden_hyp_vecs_end - __bp_harden_hyp_vecs_start)
-			/ SZ_2K) <= last_slot);
-		slot = last_slot;
+		slot = atomic_inc_return(&arm64_el2_vector_last_slot);
+		BUG_ON(slot >= BP_HARDEN_EL2_SLOTS);
 		__copy_hyp_vect_bpi(slot, hyp_vecs_start, hyp_vecs_end);
 	}
 

commit e21da1c992007594d391e7b301779cf30f438691
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Mar 9 15:40:50 2018 +0000

    arm64: Relax ARM_SMCCC_ARCH_WORKAROUND_1 discovery
    
    A recent update to the ARM SMCCC ARCH_WORKAROUND_1 specification
    allows firmware to return a non zero, positive value to describe
    that although the mitigation is implemented at the higher exception
    level, the CPU on which the call is made is not affected.
    
    Let's relax the check on the return value from ARCH_WORKAROUND_1
    so that we only error out if the returned value is negative.
    
    Fixes: b092201e0020 ("arm64: Add ARM_SMCCC_ARCH_WORKAROUND_1 BP hardening support")
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 52f15cd896e1..b5a28336c077 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -178,7 +178,7 @@ static int enable_smccc_arch_workaround_1(void *data)
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
-		if (res.a0)
+		if ((int)res.a0 < 0)
 			return 0;
 		cb = call_hvc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_hvc_start;
@@ -188,7 +188,7 @@ static int enable_smccc_arch_workaround_1(void *data)
 	case PSCI_CONDUIT_SMC:
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
-		if (res.a0)
+		if ((int)res.a0 < 0)
 			return 0;
 		cb = call_smc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_smc_start;

commit ca79acca273630935f2cfdfdf3fc7425ff51ce1c
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 6 17:15:35 2018 +0000

    arm64/kernel: enable A53 erratum #8434319 handling at runtime
    
    Omit patching of ADRP instruction at module load time if the current
    CPUs are not susceptible to the erratum.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [will: Drop duplicate initialisation of .def_scope field]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index b161abdd6e27..186c0fc61dcd 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -298,6 +298,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 			   MIDR_CPU_VAR_REV(1, 2)),
 	},
 #endif
+#ifdef CONFIG_ARM64_ERRATUM_843419
+	{
+	/* Cortex-A53 r0p[01234] */
+		.desc = "ARM erratum 843419",
+		.capability = ARM64_WORKAROUND_843419,
+		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x04),
+		MIDR_FIXED(0x4, BIT(8)),
+	},
+#endif
 #ifdef CONFIG_ARM64_ERRATUM_845719
 	{
 	/* Cortex-A53 r0p[01234] */

commit e8002e02abf052c07bb87b867789034bc79aac10
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 6 17:15:34 2018 +0000

    arm64/errata: add REVIDR handling to framework
    
    In some cases, core variants that are affected by a certain erratum
    also exist in versions that have the erratum fixed, and this fact is
    recorded in a dedicated bit in system register REVIDR_EL1.
    
    Since the architecture does not require that a certain bit retains
    its meaning across different variants of the same model, each such
    REVIDR bit is tightly coupled to a certain revision/variant value,
    and so we need a list of revidr_mask/midr pairs to carry this
    information.
    
    So add the struct member and the associated macros and handling to
    allow REVIDR fixes to be taken into account.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 52f15cd896e1..b161abdd6e27 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -24,10 +24,22 @@
 static bool __maybe_unused
 is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
 {
+	const struct arm64_midr_revidr *fix;
+	u32 midr = read_cpuid_id(), revidr;
+
 	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
-	return MIDR_IS_CPU_MODEL_RANGE(read_cpuid_id(), entry->midr_model,
-				       entry->midr_range_min,
-				       entry->midr_range_max);
+	if (!MIDR_IS_CPU_MODEL_RANGE(midr, entry->midr_model,
+				     entry->midr_range_min,
+				     entry->midr_range_max))
+		return false;
+
+	midr &= MIDR_REVISION_MASK | MIDR_VARIANT_MASK;
+	revidr = read_cpuid(REVIDR_EL1);
+	for (fix = entry->fixed_revs; fix && fix->revidr_mask; fix++)
+		if (midr == fix->midr_rv && (revidr & fix->revidr_mask))
+			return false;
+
+	return true;
 }
 
 static bool __maybe_unused
@@ -242,6 +254,9 @@ static int qcom_enable_link_stack_sanitization(void *data)
 	.midr_range_min = 0, \
 	.midr_range_max = (MIDR_VARIANT_MASK | MIDR_REVISION_MASK)
 
+#define MIDR_FIXED(rev, revidr_mask) \
+	.fixed_revs = (struct arm64_midr_revidr[]){{ (rev), (revidr_mask) }, {}}
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \

commit 16e574d762ac5512eb922ac0ac5eed360b7db9d8
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Sun Feb 11 19:16:15 2018 -0600

    arm64: Add missing Falkor part number for branch predictor hardening
    
    References to CPU part number MIDR_QCOM_FALKOR were dropped from the
    mailing list patch due to mainline/arm64 branch dependency. So this
    patch adds the missing part number.
    
    Fixes: ec82b567a74f ("arm64: Implement branch predictor hardening for Falkor")
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 07823595b7f0..52f15cd896e1 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -406,6 +406,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+		.enable = qcom_enable_link_stack_sanitization,
+	},
+	{
+		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
+		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR),
+	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),

commit 3a0a397ff5ff8b56ca9f7908b75dee6bf0b5fabb
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:21 2018 +0000

    arm64: Kill PSCI_GET_VERSION as a variant-2 workaround
    
    Now that we've standardised on SMCCC v1.1 to perform the branch
    prediction invalidation, let's drop the previous band-aid.
    If vendors haven't updated their firmware to do SMCCC 1.1, they
    haven't updated PSCI either, so we don't loose anything.
    
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9e77809a3b23..07823595b7f0 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -67,7 +67,6 @@ static int cpu_enable_trap_ctr_access(void *__unused)
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM
-extern char __psci_hyp_bp_inval_start[], __psci_hyp_bp_inval_end[];
 extern char __qcom_hyp_sanitize_link_stack_start[];
 extern char __qcom_hyp_sanitize_link_stack_end[];
 extern char __smccc_workaround_1_smc_start[];
@@ -116,8 +115,6 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	spin_unlock(&bp_lock);
 }
 #else
-#define __psci_hyp_bp_inval_start		NULL
-#define __psci_hyp_bp_inval_end			NULL
 #define __qcom_hyp_sanitize_link_stack_start	NULL
 #define __qcom_hyp_sanitize_link_stack_end	NULL
 #define __smccc_workaround_1_smc_start		NULL
@@ -164,24 +161,25 @@ static void call_hvc_arch_workaround_1(void)
 	arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
 }
 
-static bool check_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
+static int enable_smccc_arch_workaround_1(void *data)
 {
+	const struct arm64_cpu_capabilities *entry = data;
 	bp_hardening_cb_t cb;
 	void *smccc_start, *smccc_end;
 	struct arm_smccc_res res;
 
 	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
-		return false;
+		return 0;
 
 	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
-		return false;
+		return 0;
 
 	switch (psci_ops.conduit) {
 	case PSCI_CONDUIT_HVC:
 		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		if (res.a0)
-			return false;
+			return 0;
 		cb = call_hvc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_hvc_start;
 		smccc_end = __smccc_workaround_1_hvc_end;
@@ -191,35 +189,18 @@ static bool check_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *e
 		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
 				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
 		if (res.a0)
-			return false;
+			return 0;
 		cb = call_smc_arch_workaround_1;
 		smccc_start = __smccc_workaround_1_smc_start;
 		smccc_end = __smccc_workaround_1_smc_end;
 		break;
 
 	default:
-		return false;
+		return 0;
 	}
 
 	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
 
-	return true;
-}
-
-static int enable_psci_bp_hardening(void *data)
-{
-	const struct arm64_cpu_capabilities *entry = data;
-
-	if (psci_ops.get_version) {
-		if (check_smccc_arch_workaround_1(entry))
-			return 0;
-
-		install_bp_hardening_cb(entry,
-				       (bp_hardening_cb_t)psci_ops.get_version,
-				       __psci_hyp_bp_inval_start,
-				       __psci_hyp_bp_inval_end);
-	}
-
 	return 0;
 }
 
@@ -399,22 +380,22 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
-		.enable = enable_psci_bp_hardening,
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
-		.enable = enable_psci_bp_hardening,
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
-		.enable = enable_psci_bp_hardening,
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
-		.enable = enable_psci_bp_hardening,
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
@@ -428,12 +409,12 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
-		.enable = enable_psci_bp_hardening,
+		.enable = enable_smccc_arch_workaround_1,
 	},
 	{
 		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
 		MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
-		.enable = enable_psci_bp_hardening,
+		.enable = enable_smccc_arch_workaround_1,
 	},
 #endif
 	{

commit b092201e0020614127f495c092e0a12d26a2116e
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Feb 6 17:56:20 2018 +0000

    arm64: Add ARM_SMCCC_ARCH_WORKAROUND_1 BP hardening support
    
    Add the detection and runtime code for ARM_SMCCC_ARCH_WORKAROUND_1.
    It is lovely. Really.
    
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index ed6881882231..9e77809a3b23 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -70,6 +70,10 @@ DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 extern char __psci_hyp_bp_inval_start[], __psci_hyp_bp_inval_end[];
 extern char __qcom_hyp_sanitize_link_stack_start[];
 extern char __qcom_hyp_sanitize_link_stack_end[];
+extern char __smccc_workaround_1_smc_start[];
+extern char __smccc_workaround_1_smc_end[];
+extern char __smccc_workaround_1_hvc_start[];
+extern char __smccc_workaround_1_hvc_end[];
 
 static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 				const char *hyp_vecs_end)
@@ -116,6 +120,10 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 #define __psci_hyp_bp_inval_end			NULL
 #define __qcom_hyp_sanitize_link_stack_start	NULL
 #define __qcom_hyp_sanitize_link_stack_end	NULL
+#define __smccc_workaround_1_smc_start		NULL
+#define __smccc_workaround_1_smc_end		NULL
+#define __smccc_workaround_1_hvc_start		NULL
+#define __smccc_workaround_1_hvc_end		NULL
 
 static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
@@ -142,17 +150,75 @@ static void  install_bp_hardening_cb(const struct arm64_cpu_capabilities *entry,
 	__install_bp_hardening_cb(fn, hyp_vecs_start, hyp_vecs_end);
 }
 
+#include <uapi/linux/psci.h>
+#include <linux/arm-smccc.h>
 #include <linux/psci.h>
 
+static void call_smc_arch_workaround_1(void)
+{
+	arm_smccc_1_1_smc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
+}
+
+static void call_hvc_arch_workaround_1(void)
+{
+	arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_WORKAROUND_1, NULL);
+}
+
+static bool check_smccc_arch_workaround_1(const struct arm64_cpu_capabilities *entry)
+{
+	bp_hardening_cb_t cb;
+	void *smccc_start, *smccc_end;
+	struct arm_smccc_res res;
+
+	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
+		return false;
+
+	if (psci_ops.smccc_version == SMCCC_VERSION_1_0)
+		return false;
+
+	switch (psci_ops.conduit) {
+	case PSCI_CONDUIT_HVC:
+		arm_smccc_1_1_hvc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
+				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
+		if (res.a0)
+			return false;
+		cb = call_hvc_arch_workaround_1;
+		smccc_start = __smccc_workaround_1_hvc_start;
+		smccc_end = __smccc_workaround_1_hvc_end;
+		break;
+
+	case PSCI_CONDUIT_SMC:
+		arm_smccc_1_1_smc(ARM_SMCCC_ARCH_FEATURES_FUNC_ID,
+				  ARM_SMCCC_ARCH_WORKAROUND_1, &res);
+		if (res.a0)
+			return false;
+		cb = call_smc_arch_workaround_1;
+		smccc_start = __smccc_workaround_1_smc_start;
+		smccc_end = __smccc_workaround_1_smc_end;
+		break;
+
+	default:
+		return false;
+	}
+
+	install_bp_hardening_cb(entry, cb, smccc_start, smccc_end);
+
+	return true;
+}
+
 static int enable_psci_bp_hardening(void *data)
 {
 	const struct arm64_cpu_capabilities *entry = data;
 
-	if (psci_ops.get_version)
+	if (psci_ops.get_version) {
+		if (check_smccc_arch_workaround_1(entry))
+			return 0;
+
 		install_bp_hardening_cb(entry,
 				       (bp_hardening_cb_t)psci_ops.get_version,
 				       __psci_hyp_bp_inval_start,
 				       __psci_hyp_bp_inval_end);
+	}
 
 	return 0;
 }

commit f3d795d9b360523beca6d13ba64c2c532f601149
Author: Jayachandran C <jnair@caviumnetworks.com>
Date:   Fri Jan 19 04:22:47 2018 -0800

    arm64: Branch predictor hardening for Cavium ThunderX2
    
    Use PSCI based mitigation for speculative execution attacks targeting
    the branch predictor. We use the same mechanism as the one used for
    Cortex-A CPUs, we expect the PSCI version call to have a side effect
    of clearing the BTBs.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Jayachandran C <jnair@caviumnetworks.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 54e41dfe41f6..ed6881882231 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -359,6 +359,16 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
 		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
 	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_BRCM_VULCAN),
+		.enable = enable_psci_bp_hardening,
+	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_CAVIUM_THUNDERX2),
+		.enable = enable_psci_bp_hardening,
+	},
 #endif
 	{
 	}

commit 55b35d070c2534dfb714b883f3c3ae05d02032da
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Jan 17 17:42:20 2018 +0000

    arm64: Run enable method for errata work arounds on late CPUs
    
    When a CPU is brought up after we have finalised the system
    wide capabilities (i.e, features and errata), we make sure the
    new CPU doesn't need a new errata work around which has not been
    detected already. However we don't run enable() method on the new
    CPU for the errata work arounds already detected. This could
    cause the new CPU running without potential work arounds.
    It is upto the "enable()" method to decide if this CPU should
    do something about the errata.
    
    Fixes: commit 6a6efbb45b7d95c84 ("arm64: Verify CPU errata work arounds on hotplugged CPU")
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 90a9e465339c..54e41dfe41f6 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -373,15 +373,18 @@ void verify_local_cpu_errata_workarounds(void)
 {
 	const struct arm64_cpu_capabilities *caps = arm64_errata;
 
-	for (; caps->matches; caps++)
-		if (!cpus_have_cap(caps->capability) &&
-			caps->matches(caps, SCOPE_LOCAL_CPU)) {
+	for (; caps->matches; caps++) {
+		if (cpus_have_cap(caps->capability)) {
+			if (caps->enable)
+				caps->enable((void *)caps);
+		} else if (caps->matches(caps, SCOPE_LOCAL_CPU)) {
 			pr_crit("CPU%d: Requires work around for %s, not detected"
 					" at boot time\n",
 				smp_processor_id(),
 				caps->desc ? : "an erratum");
 			cpu_die_early();
 		}
+	}
 }
 
 void update_cpu_errata_workarounds(void)

commit bb48711800e6d7aedbf4dfd3367e0ab1270a6bea
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Dec 13 14:19:37 2017 -0800

    arm64: cpu_errata: Add Kryo to Falkor 1003 errata
    
    The Kryo CPUs are also affected by the Falkor 1003 errata, so
    we need to do the same workaround on Kryo CPUs. The MIDR is
    slightly more complicated here, where the PART number is not
    always the same when looking at all the bits from 15 to 4. Drop
    the lower 8 bits and just look at the top 4 to see if it's '2'
    and then consider those as Kryo CPUs. This covers all the
    combinations without having to list them all out.
    
    Fixes: 38fd94b0275c ("arm64: Work around Falkor erratum 1003")
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 70e5f1826fd9..90a9e465339c 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -30,6 +30,20 @@ is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
 				       entry->midr_range_max);
 }
 
+static bool __maybe_unused
+is_kryo_midr(const struct arm64_cpu_capabilities *entry, int scope)
+{
+	u32 model;
+
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+
+	model = read_cpuid_id();
+	model &= MIDR_IMPLEMENTOR_MASK | (0xf00 << MIDR_PARTNUM_SHIFT) |
+		 MIDR_ARCHITECTURE_MASK;
+
+	return model == entry->midr_model;
+}
+
 static bool
 has_mismatched_cache_line_size(const struct arm64_cpu_capabilities *entry,
 				int scope)
@@ -290,6 +304,13 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 			   MIDR_CPU_VAR_REV(0, 0),
 			   MIDR_CPU_VAR_REV(0, 0)),
 	},
+	{
+		.desc = "Qualcomm Technologies Kryo erratum 1003",
+		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
+		.def_scope = SCOPE_LOCAL_CPU,
+		.midr_model = MIDR_QCOM_KRYO,
+		.matches = is_kryo_midr,
+	},
 #endif
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009
 	{

commit ec82b567a74fbdffdf418d4bb381d55f6a9096af
Author: Shanker Donthineni <shankerd@codeaurora.org>
Date:   Fri Jan 5 14:28:59 2018 -0600

    arm64: Implement branch predictor hardening for Falkor
    
    Falkor is susceptible to branch predictor aliasing and can
    theoretically be attacked by malicious code. This patch
    implements a mitigation for these attacks, preventing any
    malicious entries from affecting other victim contexts.
    
    Signed-off-by: Shanker Donthineni <shankerd@codeaurora.org>
    [will: fix label name when !CONFIG_KVM and remove references to MIDR_FALKOR]
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index cb0fb3796bb8..70e5f1826fd9 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -54,6 +54,8 @@ DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM
 extern char __psci_hyp_bp_inval_start[], __psci_hyp_bp_inval_end[];
+extern char __qcom_hyp_sanitize_link_stack_start[];
+extern char __qcom_hyp_sanitize_link_stack_end[];
 
 static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 				const char *hyp_vecs_end)
@@ -96,8 +98,10 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	spin_unlock(&bp_lock);
 }
 #else
-#define __psci_hyp_bp_inval_start	NULL
-#define __psci_hyp_bp_inval_end		NULL
+#define __psci_hyp_bp_inval_start		NULL
+#define __psci_hyp_bp_inval_end			NULL
+#define __qcom_hyp_sanitize_link_stack_start	NULL
+#define __qcom_hyp_sanitize_link_stack_end	NULL
 
 static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
@@ -138,6 +142,29 @@ static int enable_psci_bp_hardening(void *data)
 
 	return 0;
 }
+
+static void qcom_link_stack_sanitization(void)
+{
+	u64 tmp;
+
+	asm volatile("mov	%0, x30		\n"
+		     ".rept	16		\n"
+		     "bl	. + 4		\n"
+		     ".endr			\n"
+		     "mov	x30, %0		\n"
+		     : "=&r" (tmp));
+}
+
+static int qcom_enable_link_stack_sanitization(void *data)
+{
+	const struct arm64_cpu_capabilities *entry = data;
+
+	install_bp_hardening_cb(entry, qcom_link_stack_sanitization,
+				__qcom_hyp_sanitize_link_stack_start,
+				__qcom_hyp_sanitize_link_stack_end);
+
+	return 0;
+}
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define MIDR_RANGE(model, min, max) \
@@ -302,6 +329,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
 		.enable = enable_psci_bp_hardening,
 	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+		.enable = qcom_enable_link_stack_sanitization,
+	},
+	{
+		.capability = ARM64_HARDEN_BP_POST_GUEST_EXIT,
+		MIDR_ALL_VERSIONS(MIDR_QCOM_FALKOR_V1),
+	},
 #endif
 	{
 	}

commit aa6acde65e03186b5add8151e1ffe36c3c62639b
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 3 12:46:21 2018 +0000

    arm64: Implement branch predictor hardening for affected Cortex-A CPUs
    
    Cortex-A57, A72, A73 and A75 are susceptible to branch predictor aliasing
    and can theoretically be attacked by malicious code.
    
    This patch implements a PSCI-based mitigation for these CPUs when available.
    The call into firmware will invalidate the branch predictor state, preventing
    any malicious entries from affecting other victim contexts.
    
    Co-developed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 16ea5c6f314e..cb0fb3796bb8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -53,6 +53,8 @@ static int cpu_enable_trap_ctr_access(void *__unused)
 DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
 
 #ifdef CONFIG_KVM
+extern char __psci_hyp_bp_inval_start[], __psci_hyp_bp_inval_end[];
+
 static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
 				const char *hyp_vecs_end)
 {
@@ -94,6 +96,9 @@ static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 	spin_unlock(&bp_lock);
 }
 #else
+#define __psci_hyp_bp_inval_start	NULL
+#define __psci_hyp_bp_inval_end		NULL
+
 static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
 				      const char *hyp_vecs_start,
 				      const char *hyp_vecs_end)
@@ -118,6 +123,21 @@ static void  install_bp_hardening_cb(const struct arm64_cpu_capabilities *entry,
 
 	__install_bp_hardening_cb(fn, hyp_vecs_start, hyp_vecs_end);
 }
+
+#include <linux/psci.h>
+
+static int enable_psci_bp_hardening(void *data)
+{
+	const struct arm64_cpu_capabilities *entry = data;
+
+	if (psci_ops.get_version)
+		install_bp_hardening_cb(entry,
+				       (bp_hardening_cb_t)psci_ops.get_version,
+				       __psci_hyp_bp_inval_start,
+				       __psci_hyp_bp_inval_end);
+
+	return 0;
+}
 #endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
 
 #define MIDR_RANGE(model, min, max) \
@@ -260,6 +280,28 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_858921,
 		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
 	},
+#endif
+#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A57),
+		.enable = enable_psci_bp_hardening,
+	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A72),
+		.enable = enable_psci_bp_hardening,
+	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
+		.enable = enable_psci_bp_hardening,
+	},
+	{
+		.capability = ARM64_HARDEN_BRANCH_PREDICTOR,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A75),
+		.enable = enable_psci_bp_hardening,
+	},
 #endif
 	{
 	}

commit 0f15adbb2861ce6f75ccfc5a92b19eae0ef327d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jan 3 11:17:58 2018 +0000

    arm64: Add skeleton to harden the branch predictor against aliasing attacks
    
    Aliasing attacks against CPU branch predictors can allow an attacker to
    redirect speculative control flow on some CPUs and potentially divulge
    information from one context to another.
    
    This patch adds initial skeleton code behind a new Kconfig option to
    enable implementation-specific mitigations against these attacks for
    CPUs that are affected.
    
    Co-developed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 0e27f86ee709..16ea5c6f314e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -46,6 +46,80 @@ static int cpu_enable_trap_ctr_access(void *__unused)
 	return 0;
 }
 
+#ifdef CONFIG_HARDEN_BRANCH_PREDICTOR
+#include <asm/mmu_context.h>
+#include <asm/cacheflush.h>
+
+DEFINE_PER_CPU_READ_MOSTLY(struct bp_hardening_data, bp_hardening_data);
+
+#ifdef CONFIG_KVM
+static void __copy_hyp_vect_bpi(int slot, const char *hyp_vecs_start,
+				const char *hyp_vecs_end)
+{
+	void *dst = lm_alias(__bp_harden_hyp_vecs_start + slot * SZ_2K);
+	int i;
+
+	for (i = 0; i < SZ_2K; i += 0x80)
+		memcpy(dst + i, hyp_vecs_start, hyp_vecs_end - hyp_vecs_start);
+
+	flush_icache_range((uintptr_t)dst, (uintptr_t)dst + SZ_2K);
+}
+
+static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
+				      const char *hyp_vecs_start,
+				      const char *hyp_vecs_end)
+{
+	static int last_slot = -1;
+	static DEFINE_SPINLOCK(bp_lock);
+	int cpu, slot = -1;
+
+	spin_lock(&bp_lock);
+	for_each_possible_cpu(cpu) {
+		if (per_cpu(bp_hardening_data.fn, cpu) == fn) {
+			slot = per_cpu(bp_hardening_data.hyp_vectors_slot, cpu);
+			break;
+		}
+	}
+
+	if (slot == -1) {
+		last_slot++;
+		BUG_ON(((__bp_harden_hyp_vecs_end - __bp_harden_hyp_vecs_start)
+			/ SZ_2K) <= last_slot);
+		slot = last_slot;
+		__copy_hyp_vect_bpi(slot, hyp_vecs_start, hyp_vecs_end);
+	}
+
+	__this_cpu_write(bp_hardening_data.hyp_vectors_slot, slot);
+	__this_cpu_write(bp_hardening_data.fn, fn);
+	spin_unlock(&bp_lock);
+}
+#else
+static void __install_bp_hardening_cb(bp_hardening_cb_t fn,
+				      const char *hyp_vecs_start,
+				      const char *hyp_vecs_end)
+{
+	__this_cpu_write(bp_hardening_data.fn, fn);
+}
+#endif	/* CONFIG_KVM */
+
+static void  install_bp_hardening_cb(const struct arm64_cpu_capabilities *entry,
+				     bp_hardening_cb_t fn,
+				     const char *hyp_vecs_start,
+				     const char *hyp_vecs_end)
+{
+	u64 pfr0;
+
+	if (!entry->matches(entry, SCOPE_LOCAL_CPU))
+		return;
+
+	pfr0 = read_cpuid(ID_AA64PFR0_EL1);
+	if (cpuid_feature_extract_unsigned_field(pfr0, ID_AA64PFR0_CSV2_SHIFT))
+		return;
+
+	__install_bp_hardening_cb(fn, hyp_vecs_start, hyp_vecs_end);
+}
+#endif	/* CONFIG_HARDEN_BRANCH_PREDICTOR */
+
 #define MIDR_RANGE(model, min, max) \
 	.def_scope = SCOPE_LOCAL_CPU, \
 	.matches = is_affected_midr_range, \

commit 690a341577f9adf2c275ababe0dcefe91898bbf0
Author: David Daney <david.daney@cavium.com>
Date:   Fri Jun 9 12:49:48 2017 +0100

    arm64: Add workaround for Cavium Thunder erratum 30115
    
    Some Cavium Thunder CPUs suffer a problem where a KVM guest may
    inadvertently cause the host kernel to quit receiving interrupts.
    
    Use the Group-0/1 trapping in order to deal with it.
    
    [maz]: Adapted patch to the Group-0/1 trapping, reworked commit log
    
    Tested-by: Alexander Graf <agraf@suse.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Eric Auger <eric.auger@redhat.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 2ed2a7657711..0e27f86ee709 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -132,6 +132,27 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_CAVIUM_27456,
 		MIDR_RANGE(MIDR_THUNDERX_81XX, 0x00, 0x00),
 	},
+#endif
+#ifdef CONFIG_CAVIUM_ERRATUM_30115
+	{
+	/* Cavium ThunderX, T88 pass 1.x - 2.2 */
+		.desc = "Cavium erratum 30115",
+		.capability = ARM64_WORKAROUND_CAVIUM_30115,
+		MIDR_RANGE(MIDR_THUNDERX, 0x00,
+			   (1 << MIDR_VARIANT_SHIFT) | 2),
+	},
+	{
+	/* Cavium ThunderX, T81 pass 1.0 - 1.2 */
+		.desc = "Cavium erratum 30115",
+		.capability = ARM64_WORKAROUND_CAVIUM_30115,
+		MIDR_RANGE(MIDR_THUNDERX_81XX, 0x00, 0x02),
+	},
+	{
+	/* Cavium ThunderX, T83 pass 1.0 */
+		.desc = "Cavium erratum 30115",
+		.capability = ARM64_WORKAROUND_CAVIUM_30115,
+		MIDR_RANGE(MIDR_THUNDERX_83XX, 0x00, 0x00),
+	},
 #endif
 	{
 		.desc = "Mismatched cache line size",

commit eeb1efbcb83c0cfe6d567abbacd675bbddf3d658
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Mar 20 17:18:06 2017 +0000

    arm64: cpu_errata: Add capability to advertise Cortex-A73 erratum 858921
    
    In order to work around Cortex-A73 erratum 858921 in a subsequent
    patch, add the required capability that advertise the erratum.
    
    As the configuration option it depends on is not present yet,
    this has no immediate effect.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 2be1d1c05303..2ed2a7657711 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -157,6 +157,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 			   MIDR_CPU_VAR_REV(0, 0),
 			   MIDR_CPU_VAR_REV(0, 0)),
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_858921
+	{
+	/* Cortex-A73 all versions */
+		.desc = "ARM erratum 858921",
+		.capability = ARM64_WORKAROUND_858921,
+		MIDR_ALL_VERSIONS(MIDR_CORTEX_A73),
+	},
 #endif
 	{
 	}

commit 06f1494f837da8997d670a1ba87add7963b08922
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Feb 1 14:38:46 2017 +0000

    arm64: cpu_errata: Allow an erratum to be match for all revisions of a core
    
    Some minor erratum may not be fixed in further revisions of a core,
    leading to a situation where the workaround needs to be updated each
    time an updated core is released.
    
    Introduce a MIDR_ALL_VERSIONS match helper that will work for all
    versions of that MIDR, once and for all.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index f6cc67e7626e..2be1d1c05303 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -53,6 +53,13 @@ static int cpu_enable_trap_ctr_access(void *__unused)
 	.midr_range_min = min, \
 	.midr_range_max = max
 
+#define MIDR_ALL_VERSIONS(model) \
+	.def_scope = SCOPE_LOCAL_CPU, \
+	.matches = is_affected_midr_range, \
+	.midr_model = model, \
+	.midr_range_min = 0, \
+	.midr_range_max = (MIDR_VARIANT_MASK | MIDR_REVISION_MASK)
+
 const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \

commit 38fd94b0275c91071157a03cc27676909b23dcde
Author: Christopher Covington <cov@codeaurora.org>
Date:   Wed Feb 8 15:08:37 2017 -0500

    arm64: Work around Falkor erratum 1003
    
    The Qualcomm Datacenter Technologies Falkor v1 CPU may allocate TLB entries
    using an incorrect ASID when TTBRx_EL1 is being updated. When the erratum
    is triggered, page table entries using the new translation table base
    address (BADDR) will be allocated into the TLB using the old ASID. All
    circumstances leading to the incorrect ASID being cached in the TLB arise
    when software writes TTBRx_EL1[ASID] and TTBRx_EL1[BADDR], a memory
    operation is in the process of performing a translation using the specific
    TTBRx_EL1 being written, and the memory operation uses a translation table
    descriptor designated as non-global. EL2 and EL3 code changing the EL1&0
    ASID is not subject to this erratum because hardware is prohibited from
    performing translations from an out-of-context translation regime.
    
    Consider the following pseudo code.
    
      write new BADDR and ASID values to TTBRx_EL1
    
    Replacing the above sequence with the one below will ensure that no TLB
    entries with an incorrect ASID are used by software.
    
      write reserved value to TTBRx_EL1[ASID]
      ISB
      write new value to TTBRx_EL1[BADDR]
      ISB
      write new value to TTBRx_EL1[ASID]
      ISB
    
    When the above sequence is used, page table entries using the new BADDR
    value may still be incorrectly allocated into the TLB using the reserved
    ASID. Yet this will not reduce functionality, since TLB entries incorrectly
    tagged with the reserved ASID will never be hit by a later instruction.
    
    Based on work by Shanker Donthineni <shankerd@codeaurora.org>
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Christopher Covington <cov@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 32b9beda2ac8..f6cc67e7626e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -133,6 +133,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.def_scope = SCOPE_LOCAL_CPU,
 		.enable = cpu_enable_trap_ctr_access,
 	},
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1003
+	{
+		.desc = "Qualcomm Technologies Falkor erratum 1003",
+		.capability = ARM64_WORKAROUND_QCOM_FALKOR_E1003,
+		MIDR_RANGE(MIDR_QCOM_FALKOR_V1,
+			   MIDR_CPU_VAR_REV(0, 0),
+			   MIDR_CPU_VAR_REV(0, 0)),
+	},
+#endif
 #ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009
 	{
 		.desc = "Qualcomm Technologies Falkor erratum 1009",

commit d9ff80f83ecbf4cbdf56d32d01c312498e4fb1cd
Author: Christopher Covington <cov@codeaurora.org>
Date:   Tue Jan 31 12:50:19 2017 -0500

    arm64: Work around Falkor erratum 1009
    
    During a TLB invalidate sequence targeting the inner shareable domain,
    Falkor may prematurely complete the DSB before all loads and stores using
    the old translation are observed. Instruction fetches are not subject to
    the conditions of this erratum. If the original code sequence includes
    multiple TLB invalidate instructions followed by a single DSB, onle one of
    the TLB instructions needs to be repeated to work around this erratum.
    While the erratum only applies to cases in which the TLBI specifies the
    inner-shareable domain (*IS form of TLBI) and the DSB is ISH form or
    stronger (OSH, SYS), this changes applies the workaround overabundantly--
    to local TLBI, DSB NSH sequences as well--for simplicity.
    
    Based on work by Shanker Donthineni <shankerd@codeaurora.org>
    
    Signed-off-by: Christopher Covington <cov@codeaurora.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 722284eaf51e..32b9beda2ac8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -133,6 +133,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.def_scope = SCOPE_LOCAL_CPU,
 		.enable = cpu_enable_trap_ctr_access,
 	},
+#ifdef CONFIG_QCOM_FALKOR_ERRATUM_1009
+	{
+		.desc = "Qualcomm Technologies Falkor erratum 1009",
+		.capability = ARM64_WORKAROUND_REPEAT_TLBI,
+		MIDR_RANGE(MIDR_QCOM_FALKOR_V1,
+			   MIDR_CPU_VAR_REV(0, 0),
+			   MIDR_CPU_VAR_REV(0, 0)),
+	},
+#endif
 	{
 	}
 };

commit fa5ce3d1928c441c3d241c34a00c07c8f5880b1a
Author: Robert Richter <rrichter@cavium.com>
Date:   Fri Jan 13 14:12:09 2017 +0100

    arm64: errata: Provide macro for major and minor cpu revisions
    
    Definition of cpu ranges are hard to read if the cpu variant is not
    zero. Provide MIDR_CPU_VAR_REV() macro to describe the full hardware
    revision of a cpu including variant and (minor) revision.
    
    Signed-off-by: Robert Richter <rrichter@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index b75e917aac46..722284eaf51e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -79,8 +79,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A57 r0p0 - r1p2 */
 		.desc = "ARM erratum 832075",
 		.capability = ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE,
-		MIDR_RANGE(MIDR_CORTEX_A57, 0x00,
-			   (1 << MIDR_VARIANT_SHIFT) | 2),
+		MIDR_RANGE(MIDR_CORTEX_A57,
+			   MIDR_CPU_VAR_REV(0, 0),
+			   MIDR_CPU_VAR_REV(1, 2)),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_834220
@@ -88,8 +89,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A57 r0p0 - r1p2 */
 		.desc = "ARM erratum 834220",
 		.capability = ARM64_WORKAROUND_834220,
-		MIDR_RANGE(MIDR_CORTEX_A57, 0x00,
-			   (1 << MIDR_VARIANT_SHIFT) | 2),
+		MIDR_RANGE(MIDR_CORTEX_A57,
+			   MIDR_CPU_VAR_REV(0, 0),
+			   MIDR_CPU_VAR_REV(1, 2)),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_845719
@@ -113,8 +115,9 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cavium ThunderX, T88 pass 1.x - 2.1 */
 		.desc = "Cavium erratum 27456",
 		.capability = ARM64_WORKAROUND_CAVIUM_27456,
-		MIDR_RANGE(MIDR_THUNDERX, 0x00,
-			   (1 << MIDR_VARIANT_SHIFT) | 1),
+		MIDR_RANGE(MIDR_THUNDERX,
+			   MIDR_CPU_VAR_REV(0, 0),
+			   MIDR_CPU_VAR_REV(1, 1)),
 	},
 	{
 	/* Cavium ThunderX, T81 pass 1.0 */

commit 2a6dcb2b5f3e21592ca8dfa198dcce7bec09b020
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:46 2016 +0100

    arm64: cpufeature: Schedule enable() calls instead of calling them via IPI
    
    The enable() call for a cpufeature/errata is called using on_each_cpu().
    This issues a cross-call IPI to get the work done. Implicitly, this
    stashes the running PSTATE in SPSR when the CPU receives the IPI, and
    restores it when we return. This means an enable() call can never modify
    PSTATE.
    
    To allow PAN to do this, change the on_each_cpu() call to use
    stop_machine(). This schedules the work on each CPU which allows
    us to modify PSTATE.
    
    This involves changing the protype of all the enable() functions.
    
    enable_cpu_capabilities() is called during boot and enables the feature
    on all online CPUs. This path now uses stop_machine(). CPU features for
    hotplug'd CPUs are enabled by verify_local_cpu_features() which only
    acts on the local CPU, and can already modify the running PSTATE as it
    is called from secondary_start_kernel().
    
    Reported-by: Tony Thompson <anthony.thompson@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 0150394f4cab..b75e917aac46 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -39,10 +39,11 @@ has_mismatched_cache_line_size(const struct arm64_cpu_capabilities *entry,
 		(arm64_ftr_reg_ctrel0.sys_val & arm64_ftr_reg_ctrel0.strict_mask);
 }
 
-static void cpu_enable_trap_ctr_access(void *__unused)
+static int cpu_enable_trap_ctr_access(void *__unused)
 {
 	/* Clear SCTLR_EL1.UCT */
 	config_sctlr_el1(SCTLR_EL1_UCT, 0);
+	return 0;
 }
 
 #define MIDR_RANGE(model, min, max) \

commit 116c81f427ff6c5380850963e3fb8798cc821d2b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:16 2016 +0100

    arm64: Work around systems with mismatched cache line sizes
    
    Systems with differing CPU i-cache/d-cache line sizes can cause
    problems with the cache management by software when the execution
    is migrated from one to another. Usually, the application reads
    the cache size on a CPU and then uses that length to perform cache
    operations. However, if it gets migrated to another CPU with a smaller
    cache line size, things could go completely wrong. To prevent such
    cases, always use the smallest cache line size among the CPUs. The
    kernel CPU feature infrastructure already keeps track of the safe
    value for all CPUID registers including CTR. This patch works around
    the problem by :
    
    For kernel, dynamically patch the kernel to read the cache size
    from the system wide copy of CTR_EL0.
    
    For applications, trap read accesses to CTR_EL0 (by clearing the SCTLR.UCT)
    and emulate the mrs instruction to return the system wide safe value
    of CTR_EL0.
    
    For faster access (i.e, avoiding to lookup the system wide value of CTR_EL0
    via read_system_reg), we keep track of the pointer to table entry for
    CTR_EL0 in the CPU feature infrastructure.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 5836b3df0094..0150394f4cab 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -30,6 +30,21 @@ is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
 				       entry->midr_range_max);
 }
 
+static bool
+has_mismatched_cache_line_size(const struct arm64_cpu_capabilities *entry,
+				int scope)
+{
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
+	return (read_cpuid_cachetype() & arm64_ftr_reg_ctrel0.strict_mask) !=
+		(arm64_ftr_reg_ctrel0.sys_val & arm64_ftr_reg_ctrel0.strict_mask);
+}
+
+static void cpu_enable_trap_ctr_access(void *__unused)
+{
+	/* Clear SCTLR_EL1.UCT */
+	config_sctlr_el1(SCTLR_EL1_UCT, 0);
+}
+
 #define MIDR_RANGE(model, min, max) \
 	.def_scope = SCOPE_LOCAL_CPU, \
 	.matches = is_affected_midr_range, \
@@ -107,6 +122,13 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		MIDR_RANGE(MIDR_THUNDERX_81XX, 0x00, 0x00),
 	},
 #endif
+	{
+		.desc = "Mismatched cache line size",
+		.capability = ARM64_MISMATCHED_CACHE_LINE_SIZE,
+		.matches = has_mismatched_cache_line_size,
+		.def_scope = SCOPE_LOCAL_CPU,
+		.enable = cpu_enable_trap_ctr_access,
+	},
 	{
 	}
 };

commit 89ba26458b72422e0a1d85eb729a15220b204458
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:09 2016 +0100

    arm64: Use consistent naming for errata handling
    
    This is a cosmetic change to rename the functions dealing with
    the errata work arounds to be more consistent with their naming.
    
    1) check_local_cpu_errata() => update_cpu_errata_workarounds()
    check_local_cpu_errata() actually updates the system's errata work
    arounds. So rename it to reflect the same.
    
    2) verify_local_cpu_errata() => verify_local_cpu_errata_workarounds()
    Use errata_workarounds instead of _errata.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 82b0fc2e637b..5836b3df0094 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -116,7 +116,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
  * and the related information is freed soon after. If the new CPU requires
  * an errata not detected at boot, fail this CPU.
  */
-void verify_local_cpu_errata(void)
+void verify_local_cpu_errata_workarounds(void)
 {
 	const struct arm64_cpu_capabilities *caps = arm64_errata;
 
@@ -131,7 +131,7 @@ void verify_local_cpu_errata(void)
 		}
 }
 
-void check_local_cpu_errata(void)
+void update_cpu_errata_workarounds(void)
 {
 	update_cpu_capabilities(arm64_errata, "enabling workaround for");
 }

commit e831101a73fbc8339ef1d1909dad3ef64f089e70
Merge: f9abf53af4c7 fd6380b75065
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 11:16:05 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Kexec support for arm64
    
     - Kprobes support
    
     - Expose MIDR_EL1 and REVIDR_EL1 CPU identification registers to sysfs
    
     - Trapping of user space cache maintenance operations and emulation in
       the kernel (CPU errata workaround)
    
     - Clean-up of the early page tables creation (kernel linear mapping,
       EFI run-time maps) to avoid splitting larger blocks (e.g.  pmds) into
       smaller ones (e.g.  ptes)
    
     - VDSO support for CLOCK_MONOTONIC_RAW in clock_gettime()
    
     - ARCH_HAS_KCOV enabled for arm64
    
     - Optimise IP checksum helpers
    
     - SWIOTLB optimisation to only allocate/initialise the buffer if the
       available RAM is beyond the 32-bit mask
    
     - Properly handle the "nosmp" command line argument
    
     - Fix for the initialisation of the CPU debug state during early boot
    
     - vdso-offsets.h build dependency workaround
    
     - Build fix when RANDOMIZE_BASE is enabled with MODULES off
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (64 commits)
      arm64: arm: Fix-up the removal of the arm64 regs_query_register_name() prototype
      arm64: Only select ARM64_MODULE_PLTS if MODULES=y
      arm64: mm: run pgtable_page_ctor() on non-swapper translation table pages
      arm64: mm: make create_mapping_late() non-allocating
      arm64: Honor nosmp kernel command line option
      arm64: Fix incorrect per-cpu usage for boot CPU
      arm64: kprobes: Add KASAN instrumentation around stack accesses
      arm64: kprobes: Cleanup jprobe_return
      arm64: kprobes: Fix overflow when saving stack
      arm64: kprobes: WARN if attempting to step with PSTATE.D=1
      arm64: debug: remove unused local_dbg_{enable, disable} macros
      arm64: debug: remove redundant spsr manipulation
      arm64: debug: unmask PSTATE.D earlier
      arm64: localise Image objcopy flags
      arm64: ptrace: remove extra define for CPSR's E bit
      kprobes: Add arm64 case in kprobe example module
      arm64: Add kernel return probes support (kretprobes)
      arm64: Add trampoline code for kretprobes
      arm64: kprobes instruction simulation support
      arm64: Treat all entry code as non-kprobe-able
      ...

commit 47c459beabe969c6751e2ea8d1f85c5fa1652d6c
Author: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
Date:   Thu Jul 7 10:18:17 2016 +0530

    arm64: Enable workaround for Cavium erratum 27456 on thunderx-81xx
    
    Cavium erratum 27456 commit 104a0c02e8b1
    ("arm64: Add workaround for Cavium erratum 27456")
    is applicable for thunderx-81xx pass1.0 SoC as well.
    Adding code to enable to 81xx.
    
    Signed-off-by: Ganapatrao Kulkarni <gkulkarni@cavium.com>
    Reviewed-by: Andrew Pinski <apinski@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index d42789499f17..af716b65110d 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -98,6 +98,12 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		MIDR_RANGE(MIDR_THUNDERX, 0x00,
 			   (1 << MIDR_VARIANT_SHIFT) | 1),
 	},
+	{
+	/* Cavium ThunderX, T81 pass 1.0 */
+		.desc = "Cavium erratum 27456",
+		.capability = ARM64_WORKAROUND_CAVIUM_27456,
+		MIDR_RANGE(MIDR_THUNDERX_81XX, 0x00, 0x00),
+	},
 #endif
 	{
 	}

commit 7dd01aef055792260287c6708daf75aac3918f66
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:32 2016 +0100

    arm64: trap userspace "dc cvau" cache operation on errata-affected core
    
    The ARM errata 819472, 826319, 827319 and 824069 for affected
    Cortex-A53 cores demand to promote "dc cvau" instructions to
    "dc civac". Since we allow userspace to also emit those instructions,
    we should make sure that "dc cvau" gets promoted there too.
    So lets grasp the nettle here and actually trap every userland cache
    maintenance instruction once we detect at least one affected core in
    the system.
    We then emulate the instruction by executing it on behalf of userland,
    promoting "dc cvau" to "dc civac" on the way and injecting access
    fault back into userspace.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [catalin.marinas@arm.com: s/set_segfault/arm64_notify_segfault/]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index c2261a7a316e..af647d2cee22 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -46,6 +46,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "ARM errata 826319, 827319, 824069",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x02),
+		.enable = cpu_enable_cache_maint_trap,
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_819472
@@ -54,6 +55,7 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.desc = "ARM errata 819472",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x01),
+		.enable = cpu_enable_cache_maint_trap,
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_832075

commit 8e2318521bf5837dae093413f81292b59d49d030
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:30 2016 +0100

    arm64: errata: Calling enable functions for CPU errata too
    
    Currently we call the (optional) enable function for CPU _features_
    only. As CPU _errata_ descriptions share the same data structure and
    having an enable function is useful for errata as well (for instance
    to set bits in SCTLR), lets call it when enumerating erratas too.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index d42789499f17..c2261a7a316e 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -127,3 +127,8 @@ void check_local_cpu_errata(void)
 {
 	update_cpu_capabilities(arm64_errata, "enabling workaround for");
 }
+
+void __init enable_errata_workarounds(void)
+{
+	enable_cpu_capabilities(arm64_errata);
+}

commit 6a6efbb45b7d95c84840010095367eb06a64f342
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Apr 22 12:25:34 2016 +0100

    arm64: Verify CPU errata work arounds on hotplugged CPU
    
    CPU Errata work arounds are detected and applied to the
    kernel code at boot time and the data is then freed up.
    If a new hotplugged CPU requires a work around which
    was not applied at boot time, there is nothing we can
    do but simply fail the booting.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 2fdecd72ce58..d42789499f17 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -103,6 +103,26 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	}
 };
 
+/*
+ * The CPU Errata work arounds are detected and applied at boot time
+ * and the related information is freed soon after. If the new CPU requires
+ * an errata not detected at boot, fail this CPU.
+ */
+void verify_local_cpu_errata(void)
+{
+	const struct arm64_cpu_capabilities *caps = arm64_errata;
+
+	for (; caps->matches; caps++)
+		if (!cpus_have_cap(caps->capability) &&
+			caps->matches(caps, SCOPE_LOCAL_CPU)) {
+			pr_crit("CPU%d: Requires work around for %s, not detected"
+					" at boot time\n",
+				smp_processor_id(),
+				caps->desc ? : "an erratum");
+			cpu_die_early();
+		}
+}
+
 void check_local_cpu_errata(void)
 {
 	update_cpu_capabilities(arm64_errata, "enabling workaround for");

commit 92406f0cc9e3d5cc77bf3de6d68c9c2373dcd701
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Apr 22 12:25:31 2016 +0100

    arm64: cpufeature: Add scope for capability check
    
    Add scope parameter to the arm64_cpu_capabilities::matches(), so that
    this can be reused for checking the capability on a given CPU vs the
    system wide. The system uses the default scope associated with the
    capability for initialising the CPU_HWCAPs and ELF_HWCAPs.
    
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 06afd04e02c0..2fdecd72ce58 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -22,14 +22,16 @@
 #include <asm/cpufeature.h>
 
 static bool __maybe_unused
-is_affected_midr_range(const struct arm64_cpu_capabilities *entry)
+is_affected_midr_range(const struct arm64_cpu_capabilities *entry, int scope)
 {
+	WARN_ON(scope != SCOPE_LOCAL_CPU || preemptible());
 	return MIDR_IS_CPU_MODEL_RANGE(read_cpuid_id(), entry->midr_model,
 				       entry->midr_range_min,
 				       entry->midr_range_max);
 }
 
 #define MIDR_RANGE(model, min, max) \
+	.def_scope = SCOPE_LOCAL_CPU, \
 	.matches = is_affected_midr_range, \
 	.midr_model = model, \
 	.midr_range_min = min, \

commit 104a0c02e8b1936c049e18a6d4e4ab040fb61213
Author: Andrew Pinski <apinski@cavium.com>
Date:   Wed Feb 24 17:44:57 2016 -0800

    arm64: Add workaround for Cavium erratum 27456
    
    On ThunderX T88 pass 1.x through 2.1 parts, broadcast TLBI
    instructions may cause the icache to become corrupted if it contains
    data for a non-current ASID.
    
    This patch implements the workaround (which invalidates the local
    icache when switching the mm) by using code patching.
    
    Signed-off-by: Andrew Pinski <apinski@cavium.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index e6bc988e8dbf..06afd04e02c0 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -87,6 +87,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_CAVIUM_23154,
 		MIDR_RANGE(MIDR_THUNDERX, 0x00, 0x01),
 	},
+#endif
+#ifdef CONFIG_CAVIUM_ERRATUM_27456
+	{
+	/* Cavium ThunderX, T88 pass 1.x - 2.1 */
+		.desc = "Cavium erratum 27456",
+		.capability = ARM64_WORKAROUND_CAVIUM_27456,
+		MIDR_RANGE(MIDR_THUNDERX, 0x00,
+			   (1 << MIDR_VARIANT_SHIFT) | 1),
+	},
 #endif
 	{
 	}

commit d5370f754875460662abe8561388e019d90dd0c4
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 2 12:46:24 2016 +0000

    arm64: prefetch: add alternative pattern for CPUs without a prefetcher
    
    Most CPUs have a hardware prefetcher which generally performs better
    without explicit prefetch instructions issued by software, however
    some CPUs (e.g. Cavium ThunderX) rely solely on explicit prefetch
    instructions.
    
    This patch adds an alternative pattern (ARM64_HAS_NO_HW_PREFETCH) to
    allow our library code to make use of explicit prefetch instructions
    during things like copy routines only when the CPU does not have the
    capability to perform the prefetching itself.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Andrew Pinski <apinski@cavium.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index feb6b4efa641..e6bc988e8dbf 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -21,24 +21,12 @@
 #include <asm/cputype.h>
 #include <asm/cpufeature.h>
 
-#define MIDR_CORTEX_A53 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A53)
-#define MIDR_CORTEX_A57 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A57)
-#define MIDR_THUNDERX	MIDR_CPU_PART(ARM_CPU_IMP_CAVIUM, CAVIUM_CPU_PART_THUNDERX)
-
-#define CPU_MODEL_MASK (MIDR_IMPLEMENTOR_MASK | MIDR_PARTNUM_MASK | \
-			MIDR_ARCHITECTURE_MASK)
-
 static bool __maybe_unused
 is_affected_midr_range(const struct arm64_cpu_capabilities *entry)
 {
-	u32 midr = read_cpuid_id();
-
-	if ((midr & CPU_MODEL_MASK) != entry->midr_model)
-		return false;
-
-	midr &= MIDR_REVISION_MASK | MIDR_VARIANT_MASK;
-
-	return (midr >= entry->midr_range_min && midr <= entry->midr_range_max);
+	return MIDR_IS_CPU_MODEL_RANGE(read_cpuid_id(), entry->midr_model,
+				       entry->midr_range_min,
+				       entry->midr_range_max);
 }
 
 #define MIDR_RANGE(model, min, max) \

commit 498cd5c32be6e32bc0f8efcad48ab094bb2bfdf3
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Nov 16 10:28:18 2015 +0000

    arm64: KVM: Add workaround for Cortex-A57 erratum 834220
    
    Cortex-A57 parts up to r1p2 can misreport Stage 2 translation faults
    when a Stage 1 permission fault or device alignment fault should
    have been reported.
    
    This patch implements the workaround (which is to validate that the
    Stage-1 translation actually succeeds) by using code patching.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 24926f2504f7..feb6b4efa641 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -75,6 +75,15 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 			   (1 << MIDR_VARIANT_SHIFT) | 2),
 	},
 #endif
+#ifdef CONFIG_ARM64_ERRATUM_834220
+	{
+	/* Cortex-A57 r0p0 - r1p2 */
+		.desc = "ARM erratum 834220",
+		.capability = ARM64_WORKAROUND_834220,
+		MIDR_RANGE(MIDR_CORTEX_A57, 0x00,
+			   (1 << MIDR_VARIANT_SHIFT) | 2),
+	},
+#endif
 #ifdef CONFIG_ARM64_ERRATUM_845719
 	{
 	/* Cortex-A53 r0p[01234] */

commit 2dc10ad81fc017837037e60439662e1b16bdffb9
Merge: e627078a0cbd f8f8bdc48851
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 4 14:47:13 2015 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - "genirq: Introduce generic irq migration for cpu hotunplugged" patch
       merged from tip/irq/for-arm to allow the arm64-specific part to be
       upstreamed via the arm64 tree
    
     - CPU feature detection reworked to cope with heterogeneous systems
       where CPUs may not have exactly the same features.  The features
       reported by the kernel via internal data structures or ELF_HWCAP are
       delayed until all the CPUs are up (and before user space starts)
    
     - Support for 16KB pages, with the additional bonus of a 36-bit VA
       space, though the latter only depending on EXPERT
    
     - Implement native {relaxed, acquire, release} atomics for arm64
    
     - New ASID allocation algorithm which avoids IPI on roll-over, together
       with TLB invalidation optimisations (using local vs global where
       feasible)
    
     - KASan support for arm64
    
     - EFI_STUB clean-up and isolation for the kernel proper (required by
       KASan)
    
     - copy_{to,from,in}_user optimisations (sharing the memcpy template)
    
     - perf: moving arm64 to the arm32/64 shared PMU framework
    
     - L1_CACHE_BYTES increased to 128 to accommodate Cavium hardware
    
     - Support for the contiguous PTE hint on kernel mapping (16 consecutive
       entries may be able to use a single TLB entry)
    
     - Generic CONFIG_HZ now used on arm64
    
     - defconfig updates
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (91 commits)
      arm64/efi: fix libstub build under CONFIG_MODVERSIONS
      ARM64: Enable multi-core scheduler support by default
      arm64/efi: move arm64 specific stub C code to libstub
      arm64: page-align sections for DEBUG_RODATA
      arm64: Fix build with CONFIG_ZONE_DMA=n
      arm64: Fix compat register mappings
      arm64: Increase the max granular size
      arm64: remove bogus TASK_SIZE_64 check
      arm64: make Timer Interrupt Frequency selectable
      arm64/mm: use PAGE_ALIGNED instead of IS_ALIGNED
      arm64: cachetype: fix definitions of ICACHEF_* flags
      arm64: cpufeature: declare enable_cpu_capabilities as static
      genirq: Make the cpuhotplug migration code less noisy
      arm64: Constify hwcap name string arrays
      arm64/kvm: Make use of the system wide safe values
      arm64/debug: Make use of the system wide safe value
      arm64: Move FP/ASIMD hwcap handling to common code
      arm64/HWCAP: Use system wide safe values
      arm64/capabilities: Make use of system wide safe value
      arm64: Delay cpu feature capability checks
      ...

commit ce8b602c694c9482e0ffb7432cd59fa2276673fe
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:49 2015 +0100

    arm64: Refactor check_cpu_capabilities
    
    check_cpu_capabilities runs through a given list of caps and
    checks if the system has the cap, updates the system capability
    bitmap and also runs any enable() methods associated with them.
    All of this is not quite obvious from the name 'check'. This
    patch splits the check_cpu_capabilities into two parts :
    
    1) update_cpu_capabilities
     => Runs through the given list and updates the system
        wide capability map.
    2) enable_cpu_capabilities
     => Runs through the given list and invokes enable() (if any)
        for the caps enabled on the system.
    
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Suggested-by: Catalin Marinas <catalin.marinsa@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 6ffd91438560..09eab326ef93 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -88,5 +88,5 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 
 void check_local_cpu_errata(void)
 {
-	check_cpu_capabilities(arm64_errata, "enabling workaround for");
+	update_cpu_capabilities(arm64_errata, "enabling workaround for");
 }

commit 6d4e11c5e2e8cd54a035ba395bf8ccfa7e22cfd8
Author: Robert Richter <rrichter@cavium.com>
Date:   Mon Sep 21 22:58:35 2015 +0200

    irqchip/gicv3: Workaround for Cavium ThunderX erratum 23154
    
    This patch implements Cavium ThunderX erratum 23154.
    
    The gicv3 of ThunderX requires a modified version for reading the IAR
    status to ensure data synchronization. Since this is in the fast-path
    and called with each interrupt, runtime patching is used using jump
    label patching for smallest overhead (no-op). This is the same
    technique as used for tracepoints.
    
    Signed-off-by: Robert Richter <rrichter@cavium.com>
    Reviewed-by: Marc Zygnier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Tirumalesh Chalamarla <tchalamarla@cavium.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/1442869119-1814-3-git-send-email-rric@kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 6ffd91438560..574450c257a4 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -23,6 +23,7 @@
 
 #define MIDR_CORTEX_A53 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A53)
 #define MIDR_CORTEX_A57 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A57)
+#define MIDR_THUNDERX	MIDR_CPU_PART(ARM_CPU_IMP_CAVIUM, CAVIUM_CPU_PART_THUNDERX)
 
 #define CPU_MODEL_MASK (MIDR_IMPLEMENTOR_MASK | MIDR_PARTNUM_MASK | \
 			MIDR_ARCHITECTURE_MASK)
@@ -81,6 +82,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_845719,
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x04),
 	},
+#endif
+#ifdef CONFIG_CAVIUM_ERRATUM_23154
+	{
+	/* Cavium ThunderX, pass 1.x */
+		.desc = "Cavium erratum 23154",
+		.capability = ARM64_WORKAROUND_CAVIUM_23154,
+		MIDR_RANGE(MIDR_THUNDERX, 0x00, 0x01),
+	},
 #endif
 	{
 	}

commit 6d1966dfd6e0ad2f8aa4b664ae1a62e33abe1998
Author: Bo Yan <byan@nvidia.com>
Date:   Tue Mar 31 21:30:48 2015 +0100

    arm64: fix midr range for Cortex-A57 erratum 832075
    
    Register MIDR_EL1 is masked to get variant and revision fields, then
    compared against midr_range_min and midr_range_max when checking
    whether CPU is affected by any particular erratum. However, variant
    and revision fields in MIDR_EL1 are separated by 16 bits, so the min
    and max of midr range should be constructed accordingly, otherwise
    the patch will not be applied when variant field is non-0.
    
    Cc: stable@vger.kernel.org # 3.19+
    Acked-by: Andre Przywara <andre.przywara@arm.com>
    Reviewed-by: Paul Walmsley <paul@pwsan.com>
    Signed-off-by: Bo Yan <byan@nvidia.com>
    [will: use MIDR_VARIANT_SHIFT to construct upper bound]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 4672860def1f..6ffd91438560 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -70,7 +70,8 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 	/* Cortex-A57 r0p0 - r1p2 */
 		.desc = "ARM erratum 832075",
 		.capability = ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE,
-		MIDR_RANGE(MIDR_CORTEX_A57, 0x00, 0x12),
+		MIDR_RANGE(MIDR_CORTEX_A57, 0x00,
+			   (1 << MIDR_VARIANT_SHIFT) | 2),
 	},
 #endif
 #ifdef CONFIG_ARM64_ERRATUM_845719

commit 905e8c5dcaa147163672b06fe9dcb5abaacbc711
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Mar 23 19:07:02 2015 +0000

    arm64: errata: add workaround for cortex-a53 erratum #845719
    
    When running a compat (AArch32) userspace on Cortex-A53, a load at EL0
    from a virtual address that matches the bottom 32 bits of the virtual
    address used by a recent load at (AArch64) EL1 might return incorrect
    data.
    
    This patch works around the issue by writing to the contextidr_el1
    register on the exception return path when returning to a 32-bit task.
    This workaround is patched in at runtime based on the MIDR value of the
    processor.
    
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index a66f4fa4d541..4672860def1f 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -72,6 +72,14 @@ const struct arm64_cpu_capabilities arm64_errata[] = {
 		.capability = ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE,
 		MIDR_RANGE(MIDR_CORTEX_A57, 0x00, 0x12),
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_845719
+	{
+	/* Cortex-A53 r0p[01234] */
+		.desc = "ARM erratum 845719",
+		.capability = ARM64_WORKAROUND_845719,
+		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x04),
+	},
 #endif
 	{
 	}

commit 359b706473b47da3c93bd99fd10d798fe411ab67
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Mar 27 13:09:23 2015 +0000

    arm64: Extract feature parsing code from cpu_errata.c
    
    As we detect more architectural features at runtime, it makes
    sense to reuse the existing framework whilst avoiding to call
    a feature an erratum...
    
    This patch extract the core capability parsing, moves it into
    a new file (cpufeature.c), and let the CPU errata detection code
    use it.
    
    Reviewed-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index fa62637e63a8..a66f4fa4d541 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -16,8 +16,6 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
-#define pr_fmt(fmt) "alternatives: " fmt
-
 #include <linux/types.h>
 #include <asm/cpu.h>
 #include <asm/cputype.h>
@@ -26,27 +24,11 @@
 #define MIDR_CORTEX_A53 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A53)
 #define MIDR_CORTEX_A57 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A57)
 
-/*
- * Add a struct or another datatype to the union below if you need
- * different means to detect an affected CPU.
- */
-struct arm64_cpu_capabilities {
-	const char *desc;
-	u16 capability;
-	bool (*is_affected)(struct arm64_cpu_capabilities *);
-	union {
-		struct {
-			u32 midr_model;
-			u32 midr_range_min, midr_range_max;
-		};
-	};
-};
-
 #define CPU_MODEL_MASK (MIDR_IMPLEMENTOR_MASK | MIDR_PARTNUM_MASK | \
 			MIDR_ARCHITECTURE_MASK)
 
 static bool __maybe_unused
-is_affected_midr_range(struct arm64_cpu_capabilities *entry)
+is_affected_midr_range(const struct arm64_cpu_capabilities *entry)
 {
 	u32 midr = read_cpuid_id();
 
@@ -59,12 +41,12 @@ is_affected_midr_range(struct arm64_cpu_capabilities *entry)
 }
 
 #define MIDR_RANGE(model, min, max) \
-	.is_affected = is_affected_midr_range, \
+	.matches = is_affected_midr_range, \
 	.midr_model = model, \
 	.midr_range_min = min, \
 	.midr_range_max = max
 
-struct arm64_cpu_capabilities arm64_errata[] = {
+const struct arm64_cpu_capabilities arm64_errata[] = {
 #if	defined(CONFIG_ARM64_ERRATUM_826319) || \
 	defined(CONFIG_ARM64_ERRATUM_827319) || \
 	defined(CONFIG_ARM64_ERRATUM_824069)
@@ -97,15 +79,5 @@ struct arm64_cpu_capabilities arm64_errata[] = {
 
 void check_local_cpu_errata(void)
 {
-	struct arm64_cpu_capabilities *cpus = arm64_errata;
-	int i;
-
-	for (i = 0; cpus[i].desc; i++) {
-		if (!cpus[i].is_affected(&cpus[i]))
-			continue;
-
-		if (!cpus_have_cap(cpus[i].capability))
-			pr_info("enabling workaround for %s\n", cpus[i].desc);
-		cpus_set_cap(cpus[i].capability);
-	}
+	check_cpu_capabilities(arm64_errata, "enabling workaround for");
 }

commit c9453a3ab1a39230a18b3db1d677bbb2bd782baa
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 25 18:27:01 2014 +0000

    arm64: alternatives: fix pr_fmt string for consistency
    
    Consistently use the plural form for alternatives pr_fmt strings.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 5a5226ffcbc8..fa62637e63a8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -16,7 +16,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
-#define pr_fmt(fmt) "alternative: " fmt
+#define pr_fmt(fmt) "alternatives: " fmt
 
 #include <linux/types.h>
 #include <asm/cpu.h>

commit c0a01b84b1fdbd98bff5bca5b201fe73fda7e9d9
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:12 2014 +0000

    arm64: protect alternatives workarounds with Kconfig options
    
    Not all of the errata we have workarounds for apply necessarily to all
    SoCs, so people compiling a kernel for one very specific SoC may not
    need to patch the kernel.
    Introduce a new submenu in the "Platform selection" menu to allow
    people to turn off certain bugs if they are not affected. By default
    all of them are enabled.
    Normal users or distribution kernels shouldn't bother to deselect any
    bugs here, since the alternatives framework will take care of
    patching them in only if needed.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [will: moved kconfig menu under `Kernel Features']
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 30935d2da55a..5a5226ffcbc8 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -65,18 +65,32 @@ is_affected_midr_range(struct arm64_cpu_capabilities *entry)
 	.midr_range_max = max
 
 struct arm64_cpu_capabilities arm64_errata[] = {
+#if	defined(CONFIG_ARM64_ERRATUM_826319) || \
+	defined(CONFIG_ARM64_ERRATUM_827319) || \
+	defined(CONFIG_ARM64_ERRATUM_824069)
 	{
 	/* Cortex-A53 r0p[012] */
 		.desc = "ARM errata 826319, 827319, 824069",
 		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x02),
 	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_819472
+	{
+	/* Cortex-A53 r0p[01] */
+		.desc = "ARM errata 819472",
+		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
+		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x01),
+	},
+#endif
+#ifdef CONFIG_ARM64_ERRATUM_832075
 	{
 	/* Cortex-A57 r0p0 - r1p2 */
 		.desc = "ARM erratum 832075",
 		.capability = ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE,
 		MIDR_RANGE(MIDR_CORTEX_A57, 0x00, 0x12),
 	},
+#endif
 	{
 	}
 };

commit 5afaa1fc1b320cec48affa7e6949f2493f875c12
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:11 2014 +0000

    arm64: add Cortex-A57 erratum 832075 workaround
    
    The ARM erratum 832075 applies to certain revisions of Cortex-A57,
    one of the workarounds is to change device loads into using
    load-aquire semantics.
    This is achieved using the alternatives framework.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index e107ed2d66dc..30935d2da55a 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -24,6 +24,7 @@
 #include <asm/cpufeature.h>
 
 #define MIDR_CORTEX_A53 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A53)
+#define MIDR_CORTEX_A57 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A57)
 
 /*
  * Add a struct or another datatype to the union below if you need
@@ -71,6 +72,12 @@ struct arm64_cpu_capabilities arm64_errata[] = {
 		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x02),
 	},
 	{
+	/* Cortex-A57 r0p0 - r1p2 */
+		.desc = "ARM erratum 832075",
+		.capability = ARM64_WORKAROUND_DEVICE_LOAD_ACQUIRE,
+		MIDR_RANGE(MIDR_CORTEX_A57, 0x00, 0x12),
+	},
+	{
 	}
 };
 

commit 301bcfac42897dbd1b0b3c1be49f24654a1bc49e
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:10 2014 +0000

    arm64: add Cortex-A53 cache errata workaround
    
    The ARM errata 819472, 826319, 827319 and 824069 define the same
    workaround for these hardware issues in certain Cortex-A53 parts.
    Use the new alternatives framework and the CPU MIDR detection to
    patch "cache clean" into "cache clean and invalidate" instructions if
    an affected CPU is detected at runtime.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [will: add __maybe_unused to squash gcc warning]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
index 9332cf7c0826..e107ed2d66dc 100644
--- a/arch/arm64/kernel/cpu_errata.c
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -23,6 +23,8 @@
 #include <asm/cputype.h>
 #include <asm/cpufeature.h>
 
+#define MIDR_CORTEX_A53 MIDR_CPU_PART(ARM_CPU_IMP_ARM, ARM_CPU_PART_CORTEX_A53)
+
 /*
  * Add a struct or another datatype to the union below if you need
  * different means to detect an affected CPU.
@@ -39,8 +41,37 @@ struct arm64_cpu_capabilities {
 	};
 };
 
+#define CPU_MODEL_MASK (MIDR_IMPLEMENTOR_MASK | MIDR_PARTNUM_MASK | \
+			MIDR_ARCHITECTURE_MASK)
+
+static bool __maybe_unused
+is_affected_midr_range(struct arm64_cpu_capabilities *entry)
+{
+	u32 midr = read_cpuid_id();
+
+	if ((midr & CPU_MODEL_MASK) != entry->midr_model)
+		return false;
+
+	midr &= MIDR_REVISION_MASK | MIDR_VARIANT_MASK;
+
+	return (midr >= entry->midr_range_min && midr <= entry->midr_range_max);
+}
+
+#define MIDR_RANGE(model, min, max) \
+	.is_affected = is_affected_midr_range, \
+	.midr_model = model, \
+	.midr_range_min = min, \
+	.midr_range_max = max
+
 struct arm64_cpu_capabilities arm64_errata[] = {
-	{}
+	{
+	/* Cortex-A53 r0p[012] */
+		.desc = "ARM errata 826319, 827319, 824069",
+		.capability = ARM64_WORKAROUND_CLEAN_CACHE,
+		MIDR_RANGE(MIDR_CORTEX_A53, 0x00, 0x02),
+	},
+	{
+	}
 };
 
 void check_local_cpu_errata(void)

commit e116a375423393cdb94714e90a96857005d58428
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:09 2014 +0000

    arm64: detect silicon revisions and set cap bits accordingly
    
    After each CPU has been started, we iterate through a list of
    CPU features or bugs to detect CPUs which need (or could benefit
    from) kernel code patches.
    For each feature/bug there is a function which checks if that
    particular CPU is affected. We will later provide some more generic
    functions for common things like testing for certain MIDR ranges.
    We do this for every CPU to cover big.LITTLE systems properly as
    well.
    If a certain feature/bug has been detected, the capability bit will
    be set, so that later the call to apply_alternatives() will trigger
    the actual code patching.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/cpu_errata.c b/arch/arm64/kernel/cpu_errata.c
new file mode 100644
index 000000000000..9332cf7c0826
--- /dev/null
+++ b/arch/arm64/kernel/cpu_errata.c
@@ -0,0 +1,59 @@
+/*
+ * Contains CPU specific errata definitions
+ *
+ * Copyright (C) 2014 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#define pr_fmt(fmt) "alternative: " fmt
+
+#include <linux/types.h>
+#include <asm/cpu.h>
+#include <asm/cputype.h>
+#include <asm/cpufeature.h>
+
+/*
+ * Add a struct or another datatype to the union below if you need
+ * different means to detect an affected CPU.
+ */
+struct arm64_cpu_capabilities {
+	const char *desc;
+	u16 capability;
+	bool (*is_affected)(struct arm64_cpu_capabilities *);
+	union {
+		struct {
+			u32 midr_model;
+			u32 midr_range_min, midr_range_max;
+		};
+	};
+};
+
+struct arm64_cpu_capabilities arm64_errata[] = {
+	{}
+};
+
+void check_local_cpu_errata(void)
+{
+	struct arm64_cpu_capabilities *cpus = arm64_errata;
+	int i;
+
+	for (i = 0; cpus[i].desc; i++) {
+		if (!cpus[i].is_affected(&cpus[i]))
+			continue;
+
+		if (!cpus_have_cap(cpus[i].capability))
+			pr_info("enabling workaround for %s\n", cpus[i].desc);
+		cpus_set_cap(cpus[i].capability);
+	}
+}
