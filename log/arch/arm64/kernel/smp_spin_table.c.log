commit 80d838122643a09a9f99824adea4b4261e4451e6
Author: Nick Desaulniers <ndesaulniers@google.com>
Date:   Mon Aug 12 14:50:45 2019 -0700

    arm64: prefer __section from compiler_attributes.h
    
    GCC unescapes escaped string section names while Clang does not. Because
    __section uses the `#` stringification operator for the section name, it
    doesn't need to be escaped.
    
    This antipattern was found with:
    $ grep -e __section\(\" -e __section__\(\" -r
    
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Suggested-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 76c2739ba8a4..c8a3fee00c11 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -19,7 +19,7 @@
 #include <asm/smp_plat.h>
 
 extern void secondary_holding_pen(void);
-volatile unsigned long __section(".mmuoff.data.read")
+volatile unsigned long __section(.mmuoff.data.read)
 secondary_holding_pen_release = INVALID_HWID;
 
 static phys_addr_t cpu_release_addr[NR_CPUS];

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 93034651c87e..76c2739ba8a4 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -1,19 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Spin Table SMP initialisation
  *
  * Copyright (C) 2013 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/delay.h>

commit 2077be6783b5936c3daa838d8addbb635667927f
Author: Laura Abbott <labbott@redhat.com>
Date:   Tue Jan 10 13:35:49 2017 -0800

    arm64: Use __pa_symbol for kernel symbols
    
    __pa_symbol is technically the marcro that should be used for kernel
    symbols. Switch to this as a pre-requisite for DEBUG_VIRTUAL which
    will do bounds checking.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 9a00eee9acc8..93034651c87e 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -21,6 +21,7 @@
 #include <linux/of.h>
 #include <linux/smp.h>
 #include <linux/types.h>
+#include <linux/mm.h>
 
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
@@ -98,7 +99,7 @@ static int smp_spin_table_cpu_prepare(unsigned int cpu)
 	 * boot-loader's endianess before jumping. This is mandated by
 	 * the boot protocol.
 	 */
-	writeq_relaxed(__pa(secondary_holding_pen), release_addr);
+	writeq_relaxed(__pa_symbol(secondary_holding_pen), release_addr);
 	__flush_dcache_area((__force void *)release_addr,
 			    sizeof(*release_addr));
 

commit b61130381120398876b86282082ad9f24976dfcf
Author: James Morse <james.morse@arm.com>
Date:   Wed Aug 24 18:27:29 2016 +0100

    arm64: vmlinux.ld: Add mmuoff data sections and move mmuoff text into idmap
    
    Resume from hibernate needs to clean any text executed by the kernel with
    the MMU off to the PoC. Collect these functions together into the
    .idmap.text section as all this code is tightly coupled and also needs
    the same cleaning after resume.
    
    Data is more complicated, secondary_holding_pen_release is written with
    the MMU on, clean and invalidated, then read with the MMU off. In contrast
    __boot_cpu_mode is written with the MMU off, the corresponding cache line
    is invalidated, so when we read it with the MMU on we don't get stale data.
    These cache maintenance operations conflict with each other if the values
    are within a Cache Writeback Granule (CWG) of each other.
    Collect the data into two sections .mmuoff.data.read and .mmuoff.data.write,
    the linker script ensures mmuoff.data.write section is aligned to the
    architectural maximum CWG of 2KB.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 18a71bcd26ee..9a00eee9acc8 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -29,7 +29,8 @@
 #include <asm/smp_plat.h>
 
 extern void secondary_holding_pen(void);
-volatile unsigned long secondary_holding_pen_release = INVALID_HWID;
+volatile unsigned long __section(".mmuoff.data.read")
+secondary_holding_pen_release = INVALID_HWID;
 
 static phys_addr_t cpu_release_addr[NR_CPUS];
 

commit 2fee7d5b08b6419bb59de9e875d895e3a6e7bf5a
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Wed Apr 20 10:23:31 2016 +0900

    arm64: spin-table: add missing of_node_put()
    
    Since of_get_cpu_node() increments refcount, the node should be put.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index aef3605a8c47..18a71bcd26ee 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -52,6 +52,7 @@ static void write_pen_release(u64 val)
 static int smp_spin_table_cpu_init(unsigned int cpu)
 {
 	struct device_node *dn;
+	int ret;
 
 	dn = of_get_cpu_node(cpu, NULL);
 	if (!dn)
@@ -60,15 +61,15 @@ static int smp_spin_table_cpu_init(unsigned int cpu)
 	/*
 	 * Determine the address from which the CPU is polling.
 	 */
-	if (of_property_read_u64(dn, "cpu-release-addr",
-				 &cpu_release_addr[cpu])) {
+	ret = of_property_read_u64(dn, "cpu-release-addr",
+				   &cpu_release_addr[cpu]);
+	if (ret)
 		pr_err("CPU %d: missing or invalid cpu-release-addr property\n",
 		       cpu);
 
-		return -1;
-	}
+	of_node_put(dn);
 
-	return 0;
+	return ret;
 }
 
 static int smp_spin_table_cpu_prepare(unsigned int cpu)

commit 819a88263d5dbe398edd59cc1cf725ed1fdcfd79
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed May 13 14:12:46 2015 +0100

    ARM64: kernel: make cpu_ops hooks DT agnostic
    
    ARM64 CPU operations such as cpu_init and cpu_init_idle take
    a struct device_node pointer as a parameter, which corresponds to
    the device tree node of the logical cpu on which the operation
    has to be applied.
    
    With the advent of ACPI on arm64, where MADT static table entries
    are used to initialize cpus, the device tree node parameter
    in cpu_ops hooks become useless when booting with ACPI, since
    in that case cpu device tree nodes are not present and can not be
    used for cpu initialization.
    
    The current cpu_init hook requires a struct device_node pointer
    parameter because it is called while parsing the device tree to
    initialize CPUs, when the cpu_logical_map (that is used to match
    a cpu node reg property to a device tree node) for a given logical
    cpu id is not set up yet. This means that the cpu_init hook cannot
    rely on the of_get_cpu_node function to retrieve the device tree
    node corresponding to the logical cpu id passed in as parameter,
    so the cpu device tree node must be passed in as a parameter to fix
    this catch-22 dependency cycle.
    
    This patch reshuffles the cpu_logical_map initialization code so
    that the cpu_init cpu_ops hook can safely use the of_get_cpu_node
    function to retrieve the cpu device tree node, removing the need for
    the device tree node pointer parameter.
    
    In the process, the patch removes device tree node parameters
    from all cpu_ops hooks, in preparation for SMP DT/ACPI cpus
    initialization consolidation.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: Mark Rutland <mark.rutland@arm.com> [DT]
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 14944e5b28da..aef3605a8c47 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -49,8 +49,14 @@ static void write_pen_release(u64 val)
 }
 
 
-static int smp_spin_table_cpu_init(struct device_node *dn, unsigned int cpu)
+static int smp_spin_table_cpu_init(unsigned int cpu)
 {
+	struct device_node *dn;
+
+	dn = of_get_cpu_node(cpu, NULL);
+	if (!dn)
+		return -ENODEV;
+
 	/*
 	 * Determine the address from which the CPU is polling.
 	 */

commit 59c68329a00eee7759568bc7a5383407d0d40be1
Author: Paul Walmsley <pwalmsley@nvidia.com>
Date:   Mon Jan 5 17:38:42 2015 -0700

    arm64: fix missing asm/io.h include in kernel/smp_spin_table.c
    
    On next-20150105, defconfig compilation breaks with:
    
    arch/arm64/kernel/smp_spin_table.c:80:2: error: implicit declaration of function ‘ioremap_cache’ [-Werror=implicit-function-declaration]
    arch/arm64/kernel/smp_spin_table.c:92:2: error: implicit declaration of function ‘writeq_relaxed’ [-Werror=implicit-function-declaration]
    arch/arm64/kernel/smp_spin_table.c:101:2: error: implicit declaration of function ‘iounmap’ [-Werror=implicit-function-declaration]
    
    Fix by including asm/io.h, which contains definitions or prototypes
    for these macros or functions.
    
    This second version incorporates a comment from Mark Rutland
    <mark.rutland@arm.com> to keep the includes in alphabetical order
    by filename.
    
    Signed-off-by: Paul Walmsley <paul@pwsan.com>
    Cc: Paul Walmsley <pwalmsley@nvidia.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 4f93c67e63de..14944e5b28da 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -25,6 +25,7 @@
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
 #include <asm/cputype.h>
+#include <asm/io.h>
 #include <asm/smp_plat.h>
 
 extern void secondary_holding_pen(void);

commit 113954c6463d1d80a206e91627ae49711f8b47cd
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 30 11:59:02 2014 +0100

    arm64: spin-table: handle unmapped cpu-release-addrs
    
    In certain cases the cpu-release-addr of a CPU may not fall in the
    linear mapping (e.g. when the kernel is loaded above this address due to
    the presence of other images in memory). This is problematic for the
    spin-table code as it assumes that it can trivially convert a
    cpu-release-addr to a valid VA in the linear map.
    
    This patch modifies the spin-table code to use a temporary cached
    mapping to write to a given cpu-release-addr, enabling us to support
    addresses regardless of whether they are covered by the linear mapping.
    
    Acked-by: Leif Lindholm <leif.lindholm@linaro.org>
    Tested-by: Leif Lindholm <leif.lindholm@linaro.org>
    Tested-by: Mark Salter <msalter@redhat.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    [ardb: added (__force void *) cast]
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 0347d38eea29..4f93c67e63de 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -20,6 +20,7 @@
 #include <linux/init.h>
 #include <linux/of.h>
 #include <linux/smp.h>
+#include <linux/types.h>
 
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
@@ -65,12 +66,21 @@ static int smp_spin_table_cpu_init(struct device_node *dn, unsigned int cpu)
 
 static int smp_spin_table_cpu_prepare(unsigned int cpu)
 {
-	void **release_addr;
+	__le64 __iomem *release_addr;
 
 	if (!cpu_release_addr[cpu])
 		return -ENODEV;
 
-	release_addr = __va(cpu_release_addr[cpu]);
+	/*
+	 * The cpu-release-addr may or may not be inside the linear mapping.
+	 * As ioremap_cache will either give us a new mapping or reuse the
+	 * existing linear mapping, we can use it to cover both cases. In
+	 * either case the memory will be MT_NORMAL.
+	 */
+	release_addr = ioremap_cache(cpu_release_addr[cpu],
+				     sizeof(*release_addr));
+	if (!release_addr)
+		return -ENOMEM;
 
 	/*
 	 * We write the release address as LE regardless of the native
@@ -79,15 +89,17 @@ static int smp_spin_table_cpu_prepare(unsigned int cpu)
 	 * boot-loader's endianess before jumping. This is mandated by
 	 * the boot protocol.
 	 */
-	release_addr[0] = (void *) cpu_to_le64(__pa(secondary_holding_pen));
-
-	__flush_dcache_area(release_addr, sizeof(release_addr[0]));
+	writeq_relaxed(__pa(secondary_holding_pen), release_addr);
+	__flush_dcache_area((__force void *)release_addr,
+			    sizeof(*release_addr));
 
 	/*
 	 * Send an event to wake up the secondary CPU.
 	 */
 	sev();
 
+	iounmap(release_addr);
+
 	return 0;
 }
 

commit 6400111399e16a535231ebd76389c894ea1837ff
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Apr 4 11:49:05 2014 +0100

    arm64: Remove boot thread synchronisation for spin-table release method
    
    The synchronisation with the boot thread already happens in __cpu_up()
    via wait_for_completion_timeout(). In addition, __cpu_up() calls are
    protected by the cpu_add_remove_lock mutex and already serialised.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 7a530d2cc807..0347d38eea29 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -30,7 +30,6 @@ extern void secondary_holding_pen(void);
 volatile unsigned long secondary_holding_pen_release = INVALID_HWID;
 
 static phys_addr_t cpu_release_addr[NR_CPUS];
-static DEFINE_RAW_SPINLOCK(boot_lock);
 
 /*
  * Write secondary_holding_pen_release in a way that is guaranteed to be
@@ -94,14 +93,6 @@ static int smp_spin_table_cpu_prepare(unsigned int cpu)
 
 static int smp_spin_table_cpu_boot(unsigned int cpu)
 {
-	unsigned long timeout;
-
-	/*
-	 * Set synchronisation state between this boot processor
-	 * and the secondary one
-	 */
-	raw_spin_lock(&boot_lock);
-
 	/*
 	 * Update the pen release flag.
 	 */
@@ -112,34 +103,7 @@ static int smp_spin_table_cpu_boot(unsigned int cpu)
 	 */
 	sev();
 
-	timeout = jiffies + (1 * HZ);
-	while (time_before(jiffies, timeout)) {
-		if (secondary_holding_pen_release == INVALID_HWID)
-			break;
-		udelay(10);
-	}
-
-	/*
-	 * Now the secondary core is starting up let it run its
-	 * calibrations, then wait for it to finish
-	 */
-	raw_spin_unlock(&boot_lock);
-
-	return secondary_holding_pen_release != INVALID_HWID ? -ENOSYS : 0;
-}
-
-static void smp_spin_table_cpu_postboot(void)
-{
-	/*
-	 * Let the primary processor know we're out of the pen.
-	 */
-	write_pen_release(INVALID_HWID);
-
-	/*
-	 * Synchronise with the boot thread.
-	 */
-	raw_spin_lock(&boot_lock);
-	raw_spin_unlock(&boot_lock);
+	return 0;
 }
 
 const struct cpu_operations smp_spin_table_ops = {
@@ -147,5 +111,4 @@ const struct cpu_operations smp_spin_table_ops = {
 	.cpu_init	= smp_spin_table_cpu_init,
 	.cpu_prepare	= smp_spin_table_cpu_prepare,
 	.cpu_boot	= smp_spin_table_cpu_boot,
-	.cpu_postboot	= smp_spin_table_cpu_postboot,
 };

commit 7184659bed3090248e382d98a49a3c1bcfe11174
Author: Jingoo Han <jg1.han@samsung.com>
Date:   Wed Mar 5 05:35:45 2014 +0000

    arm64: smp: make local symbol static
    
    Make smp_spin_table_cpu_postboot() static, because this function
    is used only in this file.
    
    Signed-off-by: Jingoo Han <jg1.han@samsung.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 44c22805d2e2..7a530d2cc807 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -128,7 +128,7 @@ static int smp_spin_table_cpu_boot(unsigned int cpu)
 	return secondary_holding_pen_release != INVALID_HWID ? -ENOSYS : 0;
 }
 
-void smp_spin_table_cpu_postboot(void)
+static void smp_spin_table_cpu_postboot(void)
 {
 	/*
 	 * Let the primary processor know we're out of the pen.

commit 710be9ac4ea0d2e02a2c4aa625795e65bf3db5b1
Author: Matthew Leach <matthew.leach@arm.com>
Date:   Fri Oct 11 14:52:18 2013 +0100

    arm64: big-endian: write CPU holding pen address as LE
    
    Currently when CPUs are brought online via a spin-table, the address
    they should jump to is written to the cpu-release-addr in the kernel's
    native endianness. As the kernel may switch endianness, secondaries
    might read the value byte-reversed from what was intended, and they
    would jump to the wrong address.
    
    As the only current arm64 spin-table implementations are
    little-endian, stricten up the arm64 spin-table definition such that
    the value written to cpu-release-addr is _always_ little-endian
    regardless of the endianness of any CPU. If a spinning CPU is
    operating big-endian, it must byte-reverse the value before jumping to
    handle this.
    
    Signed-off-by: Matthew Leach <matthew.leach@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 27f08367a6e7..44c22805d2e2 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -72,7 +72,16 @@ static int smp_spin_table_cpu_prepare(unsigned int cpu)
 		return -ENODEV;
 
 	release_addr = __va(cpu_release_addr[cpu]);
-	release_addr[0] = (void *)__pa(secondary_holding_pen);
+
+	/*
+	 * We write the release address as LE regardless of the native
+	 * endianess of the kernel. Therefore, any boot-loaders that
+	 * read this address need to convert this address to the
+	 * boot-loader's endianess before jumping. This is mandated by
+	 * the boot protocol.
+	 */
+	release_addr[0] = (void *) cpu_to_le64(__pa(secondary_holding_pen));
+
 	__flush_dcache_area(release_addr, sizeof(release_addr[0]));
 
 	/*

commit 652af899799354049b273af897b798b8f03fdd88
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:16 2013 +0100

    arm64: factor out spin-table boot method
    
    The arm64 kernel has an internal holding pen, which is necessary for
    some systems where we can't bring CPUs online individually and must hold
    multiple CPUs in a safe area until the kernel is able to handle them.
    The current SMP infrastructure for arm64 is closely coupled to this
    holding pen, and alternative boot methods must launch CPUs into the pen,
    where they sit before they are launched into the kernel proper.
    
    With PSCI (and possibly other future boot methods), we can bring CPUs
    online individually, and need not perform the secondary_holding_pen
    dance. Instead, this patch factors the holding pen management code out
    to the spin-table boot method code, as it is the only boot method
    requiring the pen.
    
    A new entry point for secondaries, secondary_entry is added for other
    boot methods to use, which bypasses the holding pen and its associated
    overhead when bringing CPUs online. The smp.pen.text section is also
    removed, as the pen can live in head.text without problem.
    
    The cpu_operations structure is extended with two new functions,
    cpu_boot and cpu_postboot, for bringing a cpu into the kernel and
    performing any post-boot cleanup required by a bootmethod (e.g.
    resetting the secondary_holding_pen_release to INVALID_HWID).
    Documentation is added for cpu_operations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index a8b76e4eccff..27f08367a6e7 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -16,14 +16,37 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/of.h>
 #include <linux/smp.h>
 
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
+#include <asm/cputype.h>
+#include <asm/smp_plat.h>
+
+extern void secondary_holding_pen(void);
+volatile unsigned long secondary_holding_pen_release = INVALID_HWID;
 
 static phys_addr_t cpu_release_addr[NR_CPUS];
+static DEFINE_RAW_SPINLOCK(boot_lock);
+
+/*
+ * Write secondary_holding_pen_release in a way that is guaranteed to be
+ * visible to all observers, irrespective of whether they're taking part
+ * in coherency or not.  This is necessary for the hotplug code to work
+ * reliably.
+ */
+static void write_pen_release(u64 val)
+{
+	void *start = (void *)&secondary_holding_pen_release;
+	unsigned long size = sizeof(secondary_holding_pen_release);
+
+	secondary_holding_pen_release = val;
+	__flush_dcache_area(start, size);
+}
+
 
 static int smp_spin_table_cpu_init(struct device_node *dn, unsigned int cpu)
 {
@@ -60,8 +83,60 @@ static int smp_spin_table_cpu_prepare(unsigned int cpu)
 	return 0;
 }
 
+static int smp_spin_table_cpu_boot(unsigned int cpu)
+{
+	unsigned long timeout;
+
+	/*
+	 * Set synchronisation state between this boot processor
+	 * and the secondary one
+	 */
+	raw_spin_lock(&boot_lock);
+
+	/*
+	 * Update the pen release flag.
+	 */
+	write_pen_release(cpu_logical_map(cpu));
+
+	/*
+	 * Send an event, causing the secondaries to read pen_release.
+	 */
+	sev();
+
+	timeout = jiffies + (1 * HZ);
+	while (time_before(jiffies, timeout)) {
+		if (secondary_holding_pen_release == INVALID_HWID)
+			break;
+		udelay(10);
+	}
+
+	/*
+	 * Now the secondary core is starting up let it run its
+	 * calibrations, then wait for it to finish
+	 */
+	raw_spin_unlock(&boot_lock);
+
+	return secondary_holding_pen_release != INVALID_HWID ? -ENOSYS : 0;
+}
+
+void smp_spin_table_cpu_postboot(void)
+{
+	/*
+	 * Let the primary processor know we're out of the pen.
+	 */
+	write_pen_release(INVALID_HWID);
+
+	/*
+	 * Synchronise with the boot thread.
+	 */
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
+}
+
 const struct cpu_operations smp_spin_table_ops = {
 	.name		= "spin-table",
 	.cpu_init	= smp_spin_table_cpu_init,
 	.cpu_prepare	= smp_spin_table_cpu_prepare,
+	.cpu_boot	= smp_spin_table_cpu_boot,
+	.cpu_postboot	= smp_spin_table_cpu_postboot,
 };

commit cd1aebf5277a3a154a9e4c0ea4b3acabb62e5cab
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:15 2013 +0100

    arm64: reorganise smp_enable_ops
    
    For hotplug support, we're going to want a place to store operations
    that do more than bring CPUs online, and it makes sense to group these
    with our current smp_enable_ops. For cpuidle support, we'll want to
    group additional functions, and we may want them even for UP kernels.
    
    This patch renames smp_enable_ops to the more general cpu_operations,
    and pulls the definitions out of smp code such that they can be used in
    UP kernels. While we're at it, fix up instances of the cpu parameter to
    be an unsigned int, drop the init markings and rename the *_cpu
    functions to cpu_* to reduce future churn when cpu_operations is
    extended.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
index 7c35fa682f76..a8b76e4eccff 100644
--- a/arch/arm64/kernel/smp_spin_table.c
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -21,10 +21,11 @@
 #include <linux/smp.h>
 
 #include <asm/cacheflush.h>
+#include <asm/cpu_ops.h>
 
 static phys_addr_t cpu_release_addr[NR_CPUS];
 
-static int __init smp_spin_table_init_cpu(struct device_node *dn, int cpu)
+static int smp_spin_table_cpu_init(struct device_node *dn, unsigned int cpu)
 {
 	/*
 	 * Determine the address from which the CPU is polling.
@@ -40,7 +41,7 @@ static int __init smp_spin_table_init_cpu(struct device_node *dn, int cpu)
 	return 0;
 }
 
-static int __init smp_spin_table_prepare_cpu(int cpu)
+static int smp_spin_table_cpu_prepare(unsigned int cpu)
 {
 	void **release_addr;
 
@@ -59,8 +60,8 @@ static int __init smp_spin_table_prepare_cpu(int cpu)
 	return 0;
 }
 
-const struct smp_enable_ops smp_spin_table_ops __initconst = {
+const struct cpu_operations smp_spin_table_ops = {
 	.name		= "spin-table",
-	.init_cpu 	= smp_spin_table_init_cpu,
-	.prepare_cpu	= smp_spin_table_prepare_cpu,
+	.cpu_init	= smp_spin_table_cpu_init,
+	.cpu_prepare	= smp_spin_table_cpu_prepare,
 };

commit d329de3f2ada413c7cd16e1dc1d70d4abc7309e9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jan 2 15:24:22 2013 +0000

    arm64: SMP: rework the SMP code to be enabling method agnostic
    
    In order to introduce PSCI support, let the SMP code handle
    multiple enabling methods. This also allow CPUs to be booted
    using different methods (though this feels a bit weird...).
    
    In the process, move the spin-table code to its own file.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp_spin_table.c b/arch/arm64/kernel/smp_spin_table.c
new file mode 100644
index 000000000000..7c35fa682f76
--- /dev/null
+++ b/arch/arm64/kernel/smp_spin_table.c
@@ -0,0 +1,66 @@
+/*
+ * Spin Table SMP initialisation
+ *
+ * Copyright (C) 2013 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/init.h>
+#include <linux/of.h>
+#include <linux/smp.h>
+
+#include <asm/cacheflush.h>
+
+static phys_addr_t cpu_release_addr[NR_CPUS];
+
+static int __init smp_spin_table_init_cpu(struct device_node *dn, int cpu)
+{
+	/*
+	 * Determine the address from which the CPU is polling.
+	 */
+	if (of_property_read_u64(dn, "cpu-release-addr",
+				 &cpu_release_addr[cpu])) {
+		pr_err("CPU %d: missing or invalid cpu-release-addr property\n",
+		       cpu);
+
+		return -1;
+	}
+
+	return 0;
+}
+
+static int __init smp_spin_table_prepare_cpu(int cpu)
+{
+	void **release_addr;
+
+	if (!cpu_release_addr[cpu])
+		return -ENODEV;
+
+	release_addr = __va(cpu_release_addr[cpu]);
+	release_addr[0] = (void *)__pa(secondary_holding_pen);
+	__flush_dcache_area(release_addr, sizeof(release_addr[0]));
+
+	/*
+	 * Send an event to wake up the secondary CPU.
+	 */
+	sev();
+
+	return 0;
+}
+
+const struct smp_enable_ops smp_spin_table_ops __initconst = {
+	.name		= "spin-table",
+	.init_cpu 	= smp_spin_table_init_cpu,
+	.prepare_cpu	= smp_spin_table_prepare_cpu,
+};
