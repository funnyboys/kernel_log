commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 4b6f4999d06a..e43a8ff19f0f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -43,7 +43,6 @@
 #include <asm/kvm_mmu.h>
 #include <asm/mmu_context.h>
 #include <asm/numa.h>
-#include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/processor.h>
 #include <asm/smp_plat.h>

commit 039aeb9deb9291f3b19c375a8bc6fa7f768996cc
Merge: 6b2591c21273 13ffbd8db1dd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 3 15:13:47 2020 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Paolo Bonzini:
     "ARM:
       - Move the arch-specific code into arch/arm64/kvm
    
       - Start the post-32bit cleanup
    
       - Cherry-pick a few non-invasive pre-NV patches
    
      x86:
       - Rework of TLB flushing
    
       - Rework of event injection, especially with respect to nested
         virtualization
    
       - Nested AMD event injection facelift, building on the rework of
         generic code and fixing a lot of corner cases
    
       - Nested AMD live migration support
    
       - Optimization for TSC deadline MSR writes and IPIs
    
       - Various cleanups
    
       - Asynchronous page fault cleanups (from tglx, common topic branch
         with tip tree)
    
       - Interrupt-based delivery of asynchronous "page ready" events (host
         side)
    
       - Hyper-V MSRs and hypercalls for guest debugging
    
       - VMX preemption timer fixes
    
      s390:
       - Cleanups
    
      Generic:
       - switch vCPU thread wakeup from swait to rcuwait
    
      The other architectures, and the guest side of the asynchronous page
      fault work, will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (256 commits)
      KVM: selftests: fix rdtsc() for vmx_tsc_adjust_test
      KVM: check userspace_addr for all memslots
      KVM: selftests: update hyperv_cpuid with SynDBG tests
      x86/kvm/hyper-v: Add support for synthetic debugger via hypercalls
      x86/kvm/hyper-v: enable hypercalls regardless of hypercall page
      x86/kvm/hyper-v: Add support for synthetic debugger interface
      x86/hyper-v: Add synthetic debugger definitions
      KVM: selftests: VMX preemption timer migration test
      KVM: nVMX: Fix VMX preemption timer migration
      x86/kvm/hyper-v: Explicitly align hcall param for kvm_hyperv_exit
      KVM: x86/pmu: Support full width counting
      KVM: x86/pmu: Tweak kvm_pmu_get_msr to pass 'struct msr_data' in
      KVM: x86: announce KVM_FEATURE_ASYNC_PF_INT
      KVM: x86: acknowledgment mechanism for async pf page ready notifications
      KVM: x86: interrupt based APF 'page ready' event delivery
      KVM: introduce kvm_read_guest_offset_cached()
      KVM: rename kvm_arch_can_inject_async_page_present() to kvm_arch_can_dequeue_async_page_present()
      KVM: x86: extend struct kvm_vcpu_pv_apf_data with token info
      Revert "KVM: async_pf: Fix #DF due to inject "Page not Present" and "Page Ready" exceptions simultaneously"
      KVM: VMX: Replace zero-length array with flexible-array
      ...

commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit 342403bcb4dfe41324a0f6f4cb5a8d324f31c725
Merge: 09cda9a71350 fd868f148189 b130a8f70cbb 184dbc152e39 357dd8a2aff2 4fc92254bf86 10f6cd2af21b c0fc00ec6304 472de63b0b83 269fd61e15d7 7e9f5e6629f6
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 17:47:34 2020 +0100

    Merge branches 'for-next/acpi', 'for-next/bpf', 'for-next/cpufeature', 'for-next/docs', 'for-next/kconfig', 'for-next/misc', 'for-next/perf', 'for-next/ptr-auth', 'for-next/sdei', 'for-next/smccc' and 'for-next/vdso' into for-next/core
    
    ACPI and IORT updates
    (Lorenzo Pieralisi)
    * for-next/acpi:
      ACPI/IORT: Remove the unused __get_pci_rid()
      ACPI/IORT: Fix PMCG node single ID mapping handling
      ACPI: IORT: Add comments for not calling acpi_put_table()
      ACPI: GTDT: Put GTDT table after parsing
      ACPI: IORT: Add extra message "applying workaround" for off-by-1 issue
      ACPI/IORT: work around num_ids ambiguity
      Revert "ACPI/IORT: Fix 'Number of IDs' handling in iort_id_map()"
      ACPI/IORT: take _DMA methods into account for named components
    
    BPF JIT optimisations for immediate value generation
    (Luke Nelson)
    * for-next/bpf:
      bpf, arm64: Optimize ADD,SUB,JMP BPF_K using arm64 add/sub immediates
      bpf, arm64: Optimize AND,OR,XOR,JSET BPF_K using arm64 logical immediates
      arm64: insn: Fix two bugs in encoding 32-bit logical immediates
    
    Addition of new CPU ID register fields and removal of some benign sanity checks
    (Anshuman Khandual and others)
    * for-next/cpufeature: (27 commits)
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64/cpufeature: Drop open encodings while extracting parange
      arm64/cpufeature: Validate hypervisor capabilities during CPU hotplug
      arm64: cpufeature: Group indexed system register definitions by name
      arm64: cpufeature: Extend comment to describe absence of field info
      arm64: drop duplicate definitions of ID_AA64MMFR0_TGRAN constants
      arm64: cpufeature: Add an overview comment for the cpufeature framework
      ...
    
    Minor documentation tweaks for silicon errata and booting requirements
    (Rob Herring and Will Deacon)
    * for-next/docs:
      arm64: silicon-errata.rst: Sort the Cortex-A55 entries
      arm64: docs: Mandate that the I-cache doesn't hold stale kernel text
    
    Minor Kconfig cleanups
    (Geert Uytterhoeven)
    * for-next/kconfig:
      arm64: cpufeature: Add "or" to mitigations for multiple errata
      arm64: Sort vendor-specific errata
    
    Miscellaneous updates
    (Ard Biesheuvel and others)
    * for-next/misc:
      arm64: mm: Add asid_gen_match() helper
      arm64: stacktrace: Factor out some common code into on_stack()
      arm64: Call debug_traps_init() from trap_init() to help early kgdb
      arm64: cacheflush: Fix KGDB trap detection
      arm64/cpuinfo: Move device_initcall() near cpuinfo_regs_init()
      arm64: kexec_file: print appropriate variable
      arm: mm: use __pfn_to_section() to get mem_section
      arm64: Reorder the macro arguments in the copy routines
      efi/libstub/arm64: align PE/COFF sections to segment alignment
      KVM: arm64: Drop PTE_S2_MEMATTR_MASK
      arm64/kernel: Fix range on invalidating dcache for boot page tables
      arm64: set TEXT_OFFSET to 0x0 in preparation for removing it entirely
      arm64: lib: Consistently enable crc32 extension
      arm64/mm: Use phys_to_page() to access pgtable memory
      arm64: smp: Make cpus_stuck_in_kernel static
      arm64: entry: remove unneeded semicolon in el1_sync_handler()
      arm64/kernel: vmlinux.lds: drop redundant discard/keep macros
      arm64: drop GZFLAGS definition and export
      arm64: kexec_file: Avoid temp buffer for RNG seed
      arm64: rename stext to primary_entry
    
    Perf PMU driver updates
    (Tang Bin and others)
    * for-next/perf:
      pmu/smmuv3: Clear IRQ affinity hint on device removal
      drivers/perf: hisi: Permit modular builds of HiSilicon uncore drivers
      drivers/perf: hisi: Fix typo in events attribute array
      drivers/perf: arm_spe_pmu: Avoid duplicate printouts
      drivers/perf: arm_dsu_pmu: Avoid duplicate printouts
    
    Pointer authentication updates and support for vmcoreinfo
    (Amit Daniel Kachhap and Mark Rutland)
    * for-next/ptr-auth:
      Documentation/vmcoreinfo: Add documentation for 'KERNELPACMASK'
      arm64/crash_core: Export KERNELPACMASK in vmcoreinfo
      arm64: simplify ptrauth initialization
      arm64: remove ptrauth_keys_install_kernel sync arg
    
    SDEI cleanup and non-critical fixes
    (James Morse and others)
    * for-next/sdei:
      firmware: arm_sdei: Document the motivation behind these set_fs() calls
      firmware: arm_sdei: remove unused interfaces
      firmware: arm_sdei: Put the SDEI table after using it
      firmware: arm_sdei: Drop check for /firmware/ node and always register driver
    
    SMCCC updates and refactoring
    (Sudeep Holla)
    * for-next/smccc:
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      firmware: smccc: Add function to fetch SMCCC version
      firmware: smccc: Refactor SMCCC specific bits into separate file
      firmware: smccc: Drop smccc_version enum and use ARM_SMCCC_VERSION_1_x instead
      firmware: smccc: Add the definition for SMCCCv1.2 version/error codes
      firmware: smccc: Update link to latest SMCCC specification
      firmware: smccc: Add HAVE_ARM_SMCCC_DISCOVERY to identify SMCCC v1.1 and above
    
    vDSO cleanup and non-critical fixes
    (Mark Rutland and Vincenzo Frascino)
    * for-next/vdso:
      arm64: vdso: Add --eh-frame-hdr to ldflags
      arm64: vdso: use consistent 'map' nomenclature
      arm64: vdso: use consistent 'abi' nomenclature
      arm64: vdso: simplify arch_vdso_type ifdeffery
      arm64: vdso: remove aarch32_vdso_pages[]
      arm64: vdso: Add '-Bsymbolic' to ldflags

commit ba051f097fc30b00f6b66044386901141351a557
Author: Nobuhiro Iwamatsu <nobuhiro1.iwamatsu@toshiba.co.jp>
Date:   Thu May 28 08:34:57 2020 +0900

    arm64/kernel: Fix return value when cpu_online() fails in __cpu_up()
    
    If boot_secondary() was successful, and cpu_online() was an error in
    __cpu_up(), -EIO was returned, but 0 is returned by commit d22b115cbfbb7
    ("arm64/kernel: Simplify __cpu_up() by bailing out early").
    Therefore, bringup_wait_for_ap() causes the primary core to wait for a
    long time, which may cause boot failure.
    This commit sets -EIO to return code under the same conditions.
    
    Fixes: d22b115cbfbb ("arm64/kernel: Simplify __cpu_up() by bailing out early")
    Signed-off-by: Nobuhiro Iwamatsu <nobuhiro1.iwamatsu@toshiba.co.jp>
    Tested-by: Yuji Ishikawa <yuji2.ishikawa@toshiba.co.jp>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Gavin Shan <gshan@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Link: https://lore.kernel.org/r/20200527233457.2531118-1-nobuhiro1.iwamatsu@toshiba.co.jp
    [catalin.marinas@arm.com: return -EIO at the end of the function]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 061f60fe452f..bb813d06114a 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -176,7 +176,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		panic("CPU%u detected unsupported configuration\n", cpu);
 	}
 
-	return ret;
+	return -EIO;
 }
 
 static void init_gic_priority_masking(void)

commit d82755b2e781c8989614c82df7582f5649e265b8
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 16:45:17 2020 +0100

    KVM: arm64: Kill off CONFIG_KVM_ARM_HOST
    
    CONFIG_KVM_ARM_HOST is just a proxy for CONFIG_KVM, so remove it in favour
    of the latter.
    
    Signed-off-by: Will Deacon <will@kernel.org>
    Signed-off-by: Fuad Tabba <tabba@google.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Link: https://lore.kernel.org/r/20200505154520.194120-2-tabba@google.com

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 061f60fe452f..0a3045d9f33f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -430,7 +430,7 @@ static void __init hyp_mode_check(void)
 			   "CPU: CPUs started in inconsistent modes");
 	else
 		pr_info("CPU: All CPU(s) started at EL1\n");
-	if (IS_ENABLED(CONFIG_KVM_ARM_HOST))
+	if (IS_ENABLED(CONFIG_KVM))
 		kvm_compute_layout();
 }
 

commit 2eaf63ba84dc2fa4cecd717e917ea882be08069b
Author: Zou Wei <zou_wei@huawei.com>
Date:   Thu Apr 23 14:33:26 2020 +0800

    arm64: smp: Make cpus_stuck_in_kernel static
    
    Fix the following sparse warning:
    
    arch/arm64/kernel/smp.c:68:5: warning: symbol 'cpus_stuck_in_kernel'
    was not declared. Should it be static?
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: Zou Wei <zou_wei@huawei.com>
    Link: https://lore.kernel.org/r/1587623606-96698-1-git-send-email-zou_wei@huawei.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 061f60fe452f..1d06af462b93 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -65,7 +65,7 @@ EXPORT_PER_CPU_SYMBOL(cpu_number);
  */
 struct secondary_data secondary_data;
 /* Number of CPUs which aren't online, but looping in kernel text. */
-int cpus_stuck_in_kernel;
+static int cpus_stuck_in_kernel;
 
 enum ipi_msg_type {
 	IPI_RESCHEDULE,

commit 62a679cb2825488387f458c16dff32be41eb3d32
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Apr 23 11:16:06 2020 +0100

    arm64: simplify ptrauth initialization
    
    Currently __cpu_setup conditionally initializes the address
    authentication keys and enables them in SCTLR_EL1, doing so differently
    for the primary CPU and secondary CPUs, and skipping this work for CPUs
    returning from an idle state. For the latter case, cpu_do_resume
    restores the keys and SCTLR_EL1 value after the MMU has been enabled.
    
    This flow is rather difficult to follow, so instead let's move the
    primary and secondary CPU initialization into their respective boot
    paths. By following the example of cpu_do_resume and doing so once the
    MMU is enabled, we can always initialize the keys from the values in
    thread_struct, and avoid the machinery necessary to pass the keys in
    secondary_data or open-coding initialization for the boot CPU.
    
    This means we perform an additional RMW of SCTLR_EL1, but we already do
    this in the cpu_do_resume path, and for other features in cpufeature.c,
    so this isn't a major concern in a bringup path. Note that even while
    the enable bits are clear, the key registers are accessible.
    
    As this now renders the argument to __cpu_setup redundant, let's also
    remove that entirely. Future extensions can follow a similar approach to
    initialize values that differ for primary/secondary CPUs.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200423101606.37601-3-mark.rutland@arm.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 061f60fe452f..d6d337d036f0 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -114,10 +114,6 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	 */
 	secondary_data.task = idle;
 	secondary_data.stack = task_stack_page(idle) + THREAD_SIZE;
-#if defined(CONFIG_ARM64_PTR_AUTH)
-	secondary_data.ptrauth_key.apia.lo = idle->thread.keys_kernel.apia.lo;
-	secondary_data.ptrauth_key.apia.hi = idle->thread.keys_kernel.apia.hi;
-#endif
 	update_cpu_boot_status(CPU_MMU_OFF);
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 
@@ -140,10 +136,6 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	pr_crit("CPU%u: failed to come online\n", cpu);
 	secondary_data.task = NULL;
 	secondary_data.stack = NULL;
-#if defined(CONFIG_ARM64_PTR_AUTH)
-	secondary_data.ptrauth_key.apia.lo = 0;
-	secondary_data.ptrauth_key.apia.hi = 0;
-#endif
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 	status = READ_ONCE(secondary_data.status);
 	if (status == CPU_MMU_OFF)

commit 3cd86a58f7734bf9cef38f6f899608ebcaa3da13
Merge: a8222fd5b80c b2a84de2a2de
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 31 10:05:01 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "The bulk is in-kernel pointer authentication, activity monitors and
      lots of asm symbol annotations. I also queued the sys_mremap() patch
      commenting the asymmetry in the address untagging.
    
      Summary:
    
       - In-kernel Pointer Authentication support (previously only offered
         to user space).
    
       - ARM Activity Monitors (AMU) extension support allowing better CPU
         utilisation numbers for the scheduler (frequency invariance).
    
       - Memory hot-remove support for arm64.
    
       - Lots of asm annotations (SYM_*) in preparation for the in-kernel
         Branch Target Identification (BTI) support.
    
       - arm64 perf updates: ARMv8.5-PMU 64-bit counters, refactoring the
         PMU init callbacks, support for new DT compatibles.
    
       - IPv6 header checksum optimisation.
    
       - Fixes: SDEI (software delegated exception interface) double-lock on
         hibernate with shared events.
    
       - Minor clean-ups and refactoring: cpu_ops accessor,
         cpu_do_switch_mm() converted to C, cpufeature finalisation helper.
    
       - sys_mremap() comment explaining the asymmetric address untagging
         behaviour"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (81 commits)
      mm/mremap: Add comment explaining the untagging behaviour of mremap()
      arm64: head: Convert install_el2_stub to SYM_INNER_LABEL
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
      arm64: move kimage_vaddr to .rodata
      arm64: use mov_q instead of literal ldr
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      ...

commit 44ca0e00b6a05ea9cf89d8a5290a225de19f4a2a
Merge: 806dc825f01f 3b446c7d27dd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:11:08 2020 +0000

    Merge branch 'for-next/kernel-ptrauth' into for-next/core
    
    * for-next/kernel-ptrauth:
      : Return address signing - in-kernel support
      arm64: Kconfig: verify binutils support for ARM64_PTR_AUTH
      lkdtm: arm64: test kernel pointer authentication
      arm64: compile the kernel with ptrauth return address signing
      kconfig: Add support for 'as-option'
      arm64: suspend: restore the kernel ptrauth keys
      arm64: __show_regs: strip PAC from lr in printk
      arm64: unwind: strip PAC from kernel addresses
      arm64: mask PAC bits of __builtin_return_address
      arm64: initialize ptrauth keys for kernel booting task
      arm64: initialize and switch ptrauth kernel keys
      arm64: enable ptrauth earlier
      arm64: cpufeature: handle conflicts based on capability
      arm64: cpufeature: Move cpu capability helpers inside C file
      arm64: ptrauth: Add bootup/runtime flags for __cpu_setup
      arm64: install user ptrauth keys at kernel exit time
      arm64: rename ptrauth key structures to be user-specific
      arm64: cpufeature: add pointer auth meta-capabilities
      arm64: cpufeature: Fix meta-capability cpufeature check

commit da12d2739fb69531bf6bb6eb7e46d73d1dabc814
Merge: bbd6ec605c0f f7d5ef0c654e c265861af2af b5475d8caedb de58ed5e16e6 c17a290f7e7e 8673e02e5841
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 25 11:10:32 2020 +0000

    Merge branches 'for-next/memory-hotremove', 'for-next/arm_sdei', 'for-next/amu', 'for-next/final-cap-helper', 'for-next/cpu_ops-cleanup', 'for-next/misc' and 'for-next/perf' into for-next/core
    
    * for-next/memory-hotremove:
      : Memory hot-remove support for arm64
      arm64/mm: Enable memory hot remove
      arm64/mm: Hold memory hotplug lock while walking for kernel page table dump
    
    * for-next/arm_sdei:
      : SDEI: fix double locking on return from hibernate and clean-up
      firmware: arm_sdei: clean up sdei_event_create()
      firmware: arm_sdei: Use cpus_read_lock() to avoid races with cpuhp
      firmware: arm_sdei: fix possible double-lock on hibernate error path
      firmware: arm_sdei: fix double-lock on hibernate with shared events
    
    * for-next/amu:
      : ARMv8.4 Activity Monitors support
      clocksource/drivers/arm_arch_timer: validate arch_timer_rate
      arm64: use activity monitors for frequency invariance
      cpufreq: add function to get the hardware max frequency
      Documentation: arm64: document support for the AMU extension
      arm64/kvm: disable access to AMU registers from kvm guests
      arm64: trap to EL1 accesses to AMU counters from EL0
      arm64: add support for the AMU extension v1
    
    * for-next/final-cap-helper:
      : Introduce cpus_have_final_cap_helper(), migrate arm64 KVM to it
      arm64: kvm: hyp: use cpus_have_final_cap()
      arm64: cpufeature: add cpus_have_final_cap()
    
    * for-next/cpu_ops-cleanup:
      : cpu_ops[] access code clean-up
      arm64: Introduce get_cpu_ops() helper function
      arm64: Rename cpu_read_ops() to init_cpu_ops()
      arm64: Declare ACPI parking protocol CPU operation if needed
    
    * for-next/misc:
      : Various fixes and clean-ups
      arm64: define __alloc_zeroed_user_highpage
      arm64/kernel: Simplify __cpu_up() by bailing out early
      arm64: remove redundant blank for '=' operator
      arm64: kexec_file: Fixed code style.
      arm64: add blank after 'if'
      arm64: fix spelling mistake "ca not" -> "cannot"
      arm64: entry: unmask IRQ in el0_sp()
      arm64: efi: add efi-entry.o to targets instead of extra-$(CONFIG_EFI)
      arm64: csum: Optimise IPv6 header checksum
      arch/arm64: fix typo in a comment
      arm64: remove gratuitious/stray .ltorg stanzas
      arm64: Update comment for ASID() macro
      arm64: mm: convert cpu_do_switch_mm() to C
      arm64: fix NUMA Kconfig typos
    
    * for-next/perf:
      : arm64 perf updates
      arm64: perf: Add support for ARMv8.5-PMU 64-bit counters
      KVM: arm64: limit PMU version to PMUv3 for ARMv8.1
      arm64: cpufeature: Extract capped perfmon fields
      arm64: perf: Clean up enable/disable calls
      perf: arm-ccn: Use scnprintf() for robustness
      arm64: perf: Support new DT compatibles
      arm64: perf: Refactor PMU init callbacks
      perf: arm_spe: Remove unnecessary zero check on 'nr_pages'

commit de58ed5e16e62f36c7ed05552f18b7f9c647dcaf
Author: Gavin Shan <gshan@redhat.com>
Date:   Thu Mar 19 10:01:44 2020 +1100

    arm64: Introduce get_cpu_ops() helper function
    
    This introduces get_cpu_ops() to return the CPU operations according to
    the given CPU index. For now, it simply returns the @cpu_ops[cpu] as
    before. Also, helper function __cpu_try_die() is introduced to be shared
    by cpu_die() and ipi_cpu_crash_stop(). So it shouldn't introduce any
    functional changes.
    
    Signed-off-by: Gavin Shan <gshan@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 6f8477d7f3be..e5c9862c271b 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -93,8 +93,10 @@ static inline int op_cpu_kill(unsigned int cpu)
  */
 static int boot_secondary(unsigned int cpu, struct task_struct *idle)
 {
-	if (cpu_ops[cpu]->cpu_boot)
-		return cpu_ops[cpu]->cpu_boot(cpu);
+	const struct cpu_operations *ops = get_cpu_ops(cpu);
+
+	if (ops->cpu_boot)
+		return ops->cpu_boot(cpu);
 
 	return -EOPNOTSUPP;
 }
@@ -196,6 +198,7 @@ asmlinkage notrace void secondary_start_kernel(void)
 {
 	u64 mpidr = read_cpuid_mpidr() & MPIDR_HWID_BITMASK;
 	struct mm_struct *mm = &init_mm;
+	const struct cpu_operations *ops;
 	unsigned int cpu;
 
 	cpu = task_cpu(current);
@@ -227,8 +230,9 @@ asmlinkage notrace void secondary_start_kernel(void)
 	 */
 	check_local_cpu_capabilities();
 
-	if (cpu_ops[cpu]->cpu_postboot)
-		cpu_ops[cpu]->cpu_postboot();
+	ops = get_cpu_ops(cpu);
+	if (ops->cpu_postboot)
+		ops->cpu_postboot();
 
 	/*
 	 * Log the CPU info before it is marked online and might get read.
@@ -266,19 +270,21 @@ asmlinkage notrace void secondary_start_kernel(void)
 #ifdef CONFIG_HOTPLUG_CPU
 static int op_cpu_disable(unsigned int cpu)
 {
+	const struct cpu_operations *ops = get_cpu_ops(cpu);
+
 	/*
 	 * If we don't have a cpu_die method, abort before we reach the point
 	 * of no return. CPU0 may not have an cpu_ops, so test for it.
 	 */
-	if (!cpu_ops[cpu] || !cpu_ops[cpu]->cpu_die)
+	if (!ops || !ops->cpu_die)
 		return -EOPNOTSUPP;
 
 	/*
 	 * We may need to abort a hot unplug for some other mechanism-specific
 	 * reason.
 	 */
-	if (cpu_ops[cpu]->cpu_disable)
-		return cpu_ops[cpu]->cpu_disable(cpu);
+	if (ops->cpu_disable)
+		return ops->cpu_disable(cpu);
 
 	return 0;
 }
@@ -314,15 +320,17 @@ int __cpu_disable(void)
 
 static int op_cpu_kill(unsigned int cpu)
 {
+	const struct cpu_operations *ops = get_cpu_ops(cpu);
+
 	/*
 	 * If we have no means of synchronising with the dying CPU, then assume
 	 * that it is really dead. We can only wait for an arbitrary length of
 	 * time and hope that it's dead, so let's skip the wait and just hope.
 	 */
-	if (!cpu_ops[cpu]->cpu_kill)
+	if (!ops->cpu_kill)
 		return 0;
 
-	return cpu_ops[cpu]->cpu_kill(cpu);
+	return ops->cpu_kill(cpu);
 }
 
 /*
@@ -357,6 +365,7 @@ void __cpu_die(unsigned int cpu)
 void cpu_die(void)
 {
 	unsigned int cpu = smp_processor_id();
+	const struct cpu_operations *ops = get_cpu_ops(cpu);
 
 	idle_task_exit();
 
@@ -370,12 +379,22 @@ void cpu_die(void)
 	 * mechanism must perform all required cache maintenance to ensure that
 	 * no dirty lines are lost in the process of shutting down the CPU.
 	 */
-	cpu_ops[cpu]->cpu_die(cpu);
+	ops->cpu_die(cpu);
 
 	BUG();
 }
 #endif
 
+static void __cpu_try_die(int cpu)
+{
+#ifdef CONFIG_HOTPLUG_CPU
+	const struct cpu_operations *ops = get_cpu_ops(cpu);
+
+	if (ops && ops->cpu_die)
+		ops->cpu_die(cpu);
+#endif
+}
+
 /*
  * Kill the calling secondary CPU, early in bringup before it is turned
  * online.
@@ -389,12 +408,11 @@ void cpu_die_early(void)
 	/* Mark this CPU absent */
 	set_cpu_present(cpu, 0);
 
-#ifdef CONFIG_HOTPLUG_CPU
-	update_cpu_boot_status(CPU_KILL_ME);
-	/* Check if we can park ourselves */
-	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
-		cpu_ops[cpu]->cpu_die(cpu);
-#endif
+	if (IS_ENABLED(CONFIG_HOTPLUG_CPU)) {
+		update_cpu_boot_status(CPU_KILL_ME);
+		__cpu_try_die(cpu);
+	}
+
 	update_cpu_boot_status(CPU_STUCK_IN_KERNEL);
 
 	cpu_park_loop();
@@ -488,10 +506,13 @@ static bool __init is_mpidr_duplicate(unsigned int cpu, u64 hwid)
  */
 static int __init smp_cpu_setup(int cpu)
 {
+	const struct cpu_operations *ops;
+
 	if (init_cpu_ops(cpu))
 		return -ENODEV;
 
-	if (cpu_ops[cpu]->cpu_init(cpu))
+	ops = get_cpu_ops(cpu);
+	if (ops->cpu_init(cpu))
 		return -ENODEV;
 
 	set_cpu_possible(cpu, true);
@@ -714,6 +735,7 @@ void __init smp_init_cpus(void)
 
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
+	const struct cpu_operations *ops;
 	int err;
 	unsigned int cpu;
 	unsigned int this_cpu;
@@ -744,10 +766,11 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 		if (cpu == smp_processor_id())
 			continue;
 
-		if (!cpu_ops[cpu])
+		ops = get_cpu_ops(cpu);
+		if (!ops)
 			continue;
 
-		err = cpu_ops[cpu]->cpu_prepare(cpu);
+		err = ops->cpu_prepare(cpu);
 		if (err)
 			continue;
 
@@ -863,10 +886,8 @@ static void ipi_cpu_crash_stop(unsigned int cpu, struct pt_regs *regs)
 	local_irq_disable();
 	sdei_mask_local_cpu();
 
-#ifdef CONFIG_HOTPLUG_CPU
-	if (cpu_ops[cpu]->cpu_die)
-		cpu_ops[cpu]->cpu_die(cpu);
-#endif
+	if (IS_ENABLED(CONFIG_HOTPLUG_CPU))
+		__cpu_try_die(cpu);
 
 	/* just in case */
 	cpu_park_loop();
@@ -1044,8 +1065,9 @@ static bool have_cpu_die(void)
 {
 #ifdef CONFIG_HOTPLUG_CPU
 	int any_cpu = raw_smp_processor_id();
+	const struct cpu_operations *ops = get_cpu_ops(any_cpu);
 
-	if (cpu_ops[any_cpu] && cpu_ops[any_cpu]->cpu_die)
+	if (ops && ops->cpu_die)
 		return true;
 #endif
 	return false;

commit 6885fb129be30c627eb2f5b1498dba498ff6c037
Author: Gavin Shan <gshan@redhat.com>
Date:   Thu Mar 19 10:01:43 2020 +1100

    arm64: Rename cpu_read_ops() to init_cpu_ops()
    
    This renames cpu_read_ops() to init_cpu_ops() as the function is only
    called in initialization phase. Also, we will introduce get_cpu_ops() in
    the subsequent patches, to retireve the CPU operation by the given CPU
    index. The usage of cpu_read_ops() and get_cpu_ops() are difficult to be
    distinguished from their names.
    
    Signed-off-by: Gavin Shan <gshan@redhat.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d4ed9a19d8fe..6f8477d7f3be 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -488,7 +488,7 @@ static bool __init is_mpidr_duplicate(unsigned int cpu, u64 hwid)
  */
 static int __init smp_cpu_setup(int cpu)
 {
-	if (cpu_read_ops(cpu))
+	if (init_cpu_ops(cpu))
 		return -ENODEV;
 
 	if (cpu_ops[cpu]->cpu_init(cpu))

commit 33e45234987ea3ed4b05fc512f4441696478f12d
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:56 2020 +0530

    arm64: initialize and switch ptrauth kernel keys
    
    Set up keys to use pointer authentication within the kernel. The kernel
    will be compiled with APIAKey instructions, the other keys are currently
    unused. Each task is given its own APIAKey, which is initialized during
    fork. The key is changed during context switch and on kernel entry from
    EL0.
    
    The keys for idle threads need to be set before calling any C functions,
    because it is not possible to enter and exit a function with different
    keys.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Modified secondary cores key structure, comments]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d4ed9a19d8fe..08903413f106 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -112,6 +112,10 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	 */
 	secondary_data.task = idle;
 	secondary_data.stack = task_stack_page(idle) + THREAD_SIZE;
+#if defined(CONFIG_ARM64_PTR_AUTH)
+	secondary_data.ptrauth_key.apia.lo = idle->thread.keys_kernel.apia.lo;
+	secondary_data.ptrauth_key.apia.hi = idle->thread.keys_kernel.apia.hi;
+#endif
 	update_cpu_boot_status(CPU_MMU_OFF);
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 
@@ -138,6 +142,10 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 
 	secondary_data.task = NULL;
 	secondary_data.stack = NULL;
+#if defined(CONFIG_ARM64_PTR_AUTH)
+	secondary_data.ptrauth_key.apia.lo = 0;
+	secondary_data.ptrauth_key.apia.hi = 0;
+#endif
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 	status = READ_ONCE(secondary_data.status);
 	if (ret && status) {

commit f50b7dacccbab2b9e3ef18f52a6dcc18ed2050b9
Author: Cristian Marussi <cristian.marussi@arm.com>
Date:   Wed Mar 11 17:12:45 2020 +0000

    arm64: smp: fix crash_smp_send_stop() behaviour
    
    On a system configured to trigger a crash_kexec() reboot, when only one CPU
    is online and another CPU panics while starting-up, crash_smp_send_stop()
    will fail to send any STOP message to the other already online core,
    resulting in fail to freeze and registers not properly saved.
    
    Moreover even if the proper messages are sent (case CPUs > 2)
    it will similarly fail to account for the booting CPU when executing
    the final stop wait-loop, so potentially resulting in some CPU not
    been waited for shutdown before rebooting.
    
    A tangible effect of this behaviour can be observed when, after a panic
    with kexec enabled and loaded, on the following reboot triggered by kexec,
    the cpu that could not be successfully stopped fails to come back online:
    
    [  362.291022] ------------[ cut here ]------------
    [  362.291525] kernel BUG at arch/arm64/kernel/cpufeature.c:886!
    [  362.292023] Internal error: Oops - BUG: 0 [#1] PREEMPT SMP
    [  362.292400] Modules linked in:
    [  362.292970] CPU: 3 PID: 0 Comm: swapper/3 Kdump: loaded Not tainted 5.6.0-rc4-00003-gc780b890948a #105
    [  362.293136] Hardware name: Foundation-v8A (DT)
    [  362.293382] pstate: 200001c5 (nzCv dAIF -PAN -UAO)
    [  362.294063] pc : has_cpuid_feature+0xf0/0x348
    [  362.294177] lr : verify_local_elf_hwcaps+0x84/0xe8
    [  362.294280] sp : ffff800011b1bf60
    [  362.294362] x29: ffff800011b1bf60 x28: 0000000000000000
    [  362.294534] x27: 0000000000000000 x26: 0000000000000000
    [  362.294631] x25: 0000000000000000 x24: ffff80001189a25c
    [  362.294718] x23: 0000000000000000 x22: 0000000000000000
    [  362.294803] x21: ffff8000114aa018 x20: ffff800011156a00
    [  362.294897] x19: ffff800010c944a0 x18: 0000000000000004
    [  362.294987] x17: 0000000000000000 x16: 0000000000000000
    [  362.295073] x15: 00004e53b831ae3c x14: 00004e53b831ae3c
    [  362.295165] x13: 0000000000000384 x12: 0000000000000000
    [  362.295251] x11: 0000000000000000 x10: 00400032b5503510
    [  362.295334] x9 : 0000000000000000 x8 : ffff800010c7e204
    [  362.295426] x7 : 00000000410fd0f0 x6 : 0000000000000001
    [  362.295508] x5 : 00000000410fd0f0 x4 : 0000000000000000
    [  362.295592] x3 : 0000000000000000 x2 : ffff8000100939d8
    [  362.295683] x1 : 0000000000180420 x0 : 0000000000180480
    [  362.296011] Call trace:
    [  362.296257]  has_cpuid_feature+0xf0/0x348
    [  362.296350]  verify_local_elf_hwcaps+0x84/0xe8
    [  362.296424]  check_local_cpu_capabilities+0x44/0x128
    [  362.296497]  secondary_start_kernel+0xf4/0x188
    [  362.296998] Code: 52805001 72a00301 6b01001f 54000ec0 (d4210000)
    [  362.298652] SMP: stopping secondary CPUs
    [  362.300615] Starting crashdump kernel...
    [  362.301168] Bye!
    [    0.000000] Booting Linux on physical CPU 0x0000000003 [0x410fd0f0]
    [    0.000000] Linux version 5.6.0-rc4-00003-gc780b890948a (crimar01@e120937-lin) (gcc version 8.3.0 (GNU Toolchain for the A-profile Architecture 8.3-2019.03 (arm-rel-8.36))) #105 SMP PREEMPT Fri Mar 6 17:00:42 GMT 2020
    [    0.000000] Machine model: Foundation-v8A
    [    0.000000] earlycon: pl11 at MMIO 0x000000001c090000 (options '')
    [    0.000000] printk: bootconsole [pl11] enabled
    .....
    [    0.138024] rcu: Hierarchical SRCU implementation.
    [    0.153472] its@2f020000: unable to locate ITS domain
    [    0.154078] its@2f020000: Unable to locate ITS domain
    [    0.157541] EFI services will not be available.
    [    0.175395] smp: Bringing up secondary CPUs ...
    [    0.209182] psci: failed to boot CPU1 (-22)
    [    0.209377] CPU1: failed to boot: -22
    [    0.274598] Detected PIPT I-cache on CPU2
    [    0.278707] GICv3: CPU2: found redistributor 1 region 0:0x000000002f120000
    [    0.285212] CPU2: Booted secondary processor 0x0000000001 [0x410fd0f0]
    [    0.369053] Detected PIPT I-cache on CPU3
    [    0.372947] GICv3: CPU3: found redistributor 2 region 0:0x000000002f140000
    [    0.378664] CPU3: Booted secondary processor 0x0000000002 [0x410fd0f0]
    [    0.401707] smp: Brought up 1 node, 3 CPUs
    [    0.404057] SMP: Total of 3 processors activated.
    
    Make crash_smp_send_stop() account also for the online status of the
    calling CPU while evaluating how many CPUs are effectively online: this way
    the right number of STOPs is sent and all other stopped-cores's registers
    are properly saved.
    
    Fixes: 78fd584cdec05 ("arm64: kdump: implement machine_crash_shutdown()")
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Cristian Marussi <cristian.marussi@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index e4dc241c5a8e..5407bf5d98ac 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -1012,7 +1012,11 @@ void crash_smp_send_stop(void)
 
 	cpus_stopped = 1;
 
-	if (num_online_cpus() == 1) {
+	/*
+	 * If this cpu is the only one alive at this point in time, online or
+	 * not, there are no stop messages to be sent around, so just back out.
+	 */
+	if (num_other_online_cpus() == 0) {
 		sdei_mask_local_cpu();
 		return;
 	}
@@ -1020,7 +1024,7 @@ void crash_smp_send_stop(void)
 	cpumask_copy(&mask, cpu_online_mask);
 	cpumask_clear_cpu(smp_processor_id(), &mask);
 
-	atomic_set(&waiting_for_crash_ipi, num_online_cpus() - 1);
+	atomic_set(&waiting_for_crash_ipi, num_other_online_cpus());
 
 	pr_crit("SMP: stopping secondary CPUs\n");
 	smp_cross_call(&mask, IPI_CPU_CRASH_STOP);

commit d0bab0c39e32d39a8c5cddca72e5b4a3059fe050
Author: Cristian Marussi <cristian.marussi@arm.com>
Date:   Wed Mar 11 17:12:44 2020 +0000

    arm64: smp: fix smp_send_stop() behaviour
    
    On a system with only one CPU online, when another one CPU panics while
    starting-up, smp_send_stop() will fail to send any STOP message to the
    other already online core, resulting in a system still responsive and
    alive at the end of the panic procedure.
    
    [  186.700083] CPU3: shutdown
    [  187.075462] CPU2: shutdown
    [  187.162869] CPU1: shutdown
    [  188.689998] ------------[ cut here ]------------
    [  188.691645] kernel BUG at arch/arm64/kernel/cpufeature.c:886!
    [  188.692079] Internal error: Oops - BUG: 0 [#1] PREEMPT SMP
    [  188.692444] Modules linked in:
    [  188.693031] CPU: 3 PID: 0 Comm: swapper/3 Not tainted 5.6.0-rc4-00001-g338d25c35a98 #104
    [  188.693175] Hardware name: Foundation-v8A (DT)
    [  188.693492] pstate: 200001c5 (nzCv dAIF -PAN -UAO)
    [  188.694183] pc : has_cpuid_feature+0xf0/0x348
    [  188.694311] lr : verify_local_elf_hwcaps+0x84/0xe8
    [  188.694410] sp : ffff800011b1bf60
    [  188.694536] x29: ffff800011b1bf60 x28: 0000000000000000
    [  188.694707] x27: 0000000000000000 x26: 0000000000000000
    [  188.694801] x25: 0000000000000000 x24: ffff80001189a25c
    [  188.694905] x23: 0000000000000000 x22: 0000000000000000
    [  188.694996] x21: ffff8000114aa018 x20: ffff800011156a38
    [  188.695089] x19: ffff800010c944a0 x18: 0000000000000004
    [  188.695187] x17: 0000000000000000 x16: 0000000000000000
    [  188.695280] x15: 0000249dbde5431e x14: 0262cbe497efa1fa
    [  188.695371] x13: 0000000000000002 x12: 0000000000002592
    [  188.695472] x11: 0000000000000080 x10: 00400032b5503510
    [  188.695572] x9 : 0000000000000000 x8 : ffff800010c80204
    [  188.695659] x7 : 00000000410fd0f0 x6 : 0000000000000001
    [  188.695750] x5 : 00000000410fd0f0 x4 : 0000000000000000
    [  188.695836] x3 : 0000000000000000 x2 : ffff8000100939d8
    [  188.695919] x1 : 0000000000180420 x0 : 0000000000180480
    [  188.696253] Call trace:
    [  188.696410]  has_cpuid_feature+0xf0/0x348
    [  188.696504]  verify_local_elf_hwcaps+0x84/0xe8
    [  188.696591]  check_local_cpu_capabilities+0x44/0x128
    [  188.696666]  secondary_start_kernel+0xf4/0x188
    [  188.697150] Code: 52805001 72a00301 6b01001f 54000ec0 (d4210000)
    [  188.698639] ---[ end trace 3f12ca47652f7b72 ]---
    [  188.699160] Kernel panic - not syncing: Attempted to kill the idle task!
    [  188.699546] Kernel Offset: disabled
    [  188.699828] CPU features: 0x00004,20c02008
    [  188.700012] Memory Limit: none
    [  188.700538] ---[ end Kernel panic - not syncing: Attempted to kill the idle task! ]---
    
    [root@arch ~]# echo Helo
    Helo
    [root@arch ~]# cat /proc/cpuinfo | grep proce
    processor       : 0
    
    Make smp_send_stop() account also for the online status of the calling CPU
    while evaluating how many CPUs are effectively online: this way, the right
    number of STOPs is sent, so enforcing a proper freeze of the system at the
    end of panic even under the above conditions.
    
    Fixes: 08e875c16a16c ("arm64: SMP support")
    Reported-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Cristian Marussi <cristian.marussi@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d4ed9a19d8fe..e4dc241c5a8e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -958,11 +958,22 @@ void tick_broadcast(const struct cpumask *mask)
 }
 #endif
 
+/*
+ * The number of CPUs online, not counting this CPU (which may not be
+ * fully online and so not counted in num_online_cpus()).
+ */
+static inline unsigned int num_other_online_cpus(void)
+{
+	unsigned int this_cpu_online = cpu_online(smp_processor_id());
+
+	return num_online_cpus() - this_cpu_online;
+}
+
 void smp_send_stop(void)
 {
 	unsigned long timeout;
 
-	if (num_online_cpus() > 1) {
+	if (num_other_online_cpus()) {
 		cpumask_t mask;
 
 		cpumask_copy(&mask, cpu_online_mask);
@@ -975,10 +986,10 @@ void smp_send_stop(void)
 
 	/* Wait up to one second for other CPUs to stop */
 	timeout = USEC_PER_SEC;
-	while (num_online_cpus() > 1 && timeout--)
+	while (num_other_online_cpus() && timeout--)
 		udelay(1);
 
-	if (num_online_cpus() > 1)
+	if (num_other_online_cpus())
 		pr_warn("SMP: failed to stop secondary CPUs %*pbl\n",
 			cpumask_pr_args(cpu_online_mask));
 

commit d22b115cbfbb7e4a938f9eb6ea77da9ecac3df5a
Author: Gavin Shan <gshan@redhat.com>
Date:   Mon Mar 2 13:03:40 2020 +1100

    arm64/kernel: Simplify __cpu_up() by bailing out early
    
    The function __cpu_up() is invoked to bring up the target CPU through
    the backend, PSCI for example. The nested if statements won't be needed
    if we bail out early on the following two conditions where the status
    won't be checked. The code looks simplified in that case.
    
       * Error returned from the backend (e.g. PSCI)
       * The target CPU has been marked as onlined
    
    Signed-off-by: Gavin Shan <gshan@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d4ed9a19d8fe..2a9d8f39dc58 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -115,60 +115,55 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	update_cpu_boot_status(CPU_MMU_OFF);
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 
-	/*
-	 * Now bring the CPU into our world.
-	 */
+	/* Now bring the CPU into our world */
 	ret = boot_secondary(cpu, idle);
-	if (ret == 0) {
-		/*
-		 * CPU was successfully started, wait for it to come online or
-		 * time out.
-		 */
-		wait_for_completion_timeout(&cpu_running,
-					    msecs_to_jiffies(5000));
-
-		if (!cpu_online(cpu)) {
-			pr_crit("CPU%u: failed to come online\n", cpu);
-			ret = -EIO;
-		}
-	} else {
+	if (ret) {
 		pr_err("CPU%u: failed to boot: %d\n", cpu, ret);
 		return ret;
 	}
 
+	/*
+	 * CPU was successfully started, wait for it to come online or
+	 * time out.
+	 */
+	wait_for_completion_timeout(&cpu_running,
+				    msecs_to_jiffies(5000));
+	if (cpu_online(cpu))
+		return 0;
+
+	pr_crit("CPU%u: failed to come online\n", cpu);
 	secondary_data.task = NULL;
 	secondary_data.stack = NULL;
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 	status = READ_ONCE(secondary_data.status);
-	if (ret && status) {
-
-		if (status == CPU_MMU_OFF)
-			status = READ_ONCE(__early_cpu_boot_status);
+	if (status == CPU_MMU_OFF)
+		status = READ_ONCE(__early_cpu_boot_status);
 
-		switch (status & CPU_BOOT_STATUS_MASK) {
-		default:
-			pr_err("CPU%u: failed in unknown state : 0x%lx\n",
-					cpu, status);
-			cpus_stuck_in_kernel++;
-			break;
-		case CPU_KILL_ME:
-			if (!op_cpu_kill(cpu)) {
-				pr_crit("CPU%u: died during early boot\n", cpu);
-				break;
-			}
-			pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
-			/* Fall through */
-		case CPU_STUCK_IN_KERNEL:
-			pr_crit("CPU%u: is stuck in kernel\n", cpu);
-			if (status & CPU_STUCK_REASON_52_BIT_VA)
-				pr_crit("CPU%u: does not support 52-bit VAs\n", cpu);
-			if (status & CPU_STUCK_REASON_NO_GRAN)
-				pr_crit("CPU%u: does not support %luK granule \n", cpu, PAGE_SIZE / SZ_1K);
-			cpus_stuck_in_kernel++;
+	switch (status & CPU_BOOT_STATUS_MASK) {
+	default:
+		pr_err("CPU%u: failed in unknown state : 0x%lx\n",
+		       cpu, status);
+		cpus_stuck_in_kernel++;
+		break;
+	case CPU_KILL_ME:
+		if (!op_cpu_kill(cpu)) {
+			pr_crit("CPU%u: died during early boot\n", cpu);
 			break;
-		case CPU_PANIC_KERNEL:
-			panic("CPU%u detected unsupported configuration\n", cpu);
 		}
+		pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
+		/* Fall through */
+	case CPU_STUCK_IN_KERNEL:
+		pr_crit("CPU%u: is stuck in kernel\n", cpu);
+		if (status & CPU_STUCK_REASON_52_BIT_VA)
+			pr_crit("CPU%u: does not support 52-bit VAs\n", cpu);
+		if (status & CPU_STUCK_REASON_NO_GRAN) {
+			pr_crit("CPU%u: does not support %luK granule\n",
+				cpu, PAGE_SIZE / SZ_1K);
+		}
+		cpus_stuck_in_kernel++;
+		break;
+	case CPU_PANIC_KERNEL:
+		panic("CPU%u detected unsupported configuration\n", cpu);
 	}
 
 	return ret;

commit 98884281027d07b93f062b7c5e7aa01e76ba12c6
Merge: 76f6777c9cc0 de858040ee80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 6 14:18:01 2019 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Catalin Marinas:
    
     - ZONE_DMA32 initialisation fix when memblocks fall entirely within the
       first GB (used by ZONE_DMA in 5.5 for Raspberry Pi 4).
    
     - Couple of ftrace fixes following the FTRACE_WITH_REGS patchset.
    
     - access_ok() fix for the Tagged Address ABI when called from from a
       kernel thread (asynchronous I/O): the kthread does not have the TIF
       flags of the mm owner, so untag the user address unconditionally.
    
     - KVM compute_layout() called before the alternatives code patching.
    
     - Minor clean-ups.
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: entry: refine comment of stack overflow check
      arm64: ftrace: fix ifdeffery
      arm64: KVM: Invoke compute_layout() before alternatives are applied
      arm64: Validate tagged addresses in access_ok() called from kernel threads
      arm64: mm: Fix column alignment for UXN in kernel_page_tables
      arm64: insn: consistently handle exit text
      arm64: mm: Fix initialisation of DMA zones on non-NUMA systems

commit 0492747c72a3db0425a234abafb763c5b28c845d
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Thu Nov 28 20:58:05 2019 +0100

    arm64: KVM: Invoke compute_layout() before alternatives are applied
    
    compute_layout() is invoked as part of an alternative fixup under
    stop_machine(). This function invokes get_random_long() which acquires a
    sleeping lock on -RT which can not be acquired in this context.
    
    Rename compute_layout() to kvm_compute_layout() and invoke it before
    stop_machine() applies the alternatives. Add a __init prefix to
    kvm_compute_layout() because the caller has it, too (and so the code can be
    discarded after boot).
    
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index dc9fe879c279..02d41eae3da8 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -31,6 +31,7 @@
 #include <linux/of.h>
 #include <linux/irq_work.h>
 #include <linux/kexec.h>
+#include <linux/kvm_host.h>
 
 #include <asm/alternative.h>
 #include <asm/atomic.h>
@@ -39,6 +40,7 @@
 #include <asm/cputype.h>
 #include <asm/cpu_ops.h>
 #include <asm/daifflags.h>
+#include <asm/kvm_mmu.h>
 #include <asm/mmu_context.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
@@ -408,6 +410,8 @@ static void __init hyp_mode_check(void)
 			   "CPU: CPUs started in inconsistent modes");
 	else
 		pr_info("CPU: All CPU(s) started at EL1\n");
+	if (IS_ENABLED(CONFIG_KVM_ARM_HOST))
+		kvm_compute_layout();
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)

commit 436b2a8039ac00f8dc6ae8f3bd2be83748f72312
Merge: 1b96a41b4209 1d28122131b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 25 19:40:40 2019 -0800

    Merge tag 'printk-for-5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/pmladek/printk
    
    Pull printk updates from Petr Mladek:
    
     - Allow to print symbolic error names via new %pe modifier.
    
     - Use pr_warn() instead of the remaining pr_warning() calls. Fix
       formatting of the related lines.
    
     - Add VSPRINTF entry to MAINTAINERS.
    
    * tag 'printk-for-5.5' of git://git.kernel.org/pub/scm/linux/kernel/git/pmladek/printk: (32 commits)
      checkpatch: don't warn about new vsprintf pointer extension '%pe'
      MAINTAINERS: Add VSPRINTF
      tools lib api: Renaming pr_warning to pr_warn
      ASoC: samsung: Use pr_warn instead of pr_warning
      lib: cpu_rmap: Use pr_warn instead of pr_warning
      trace: Use pr_warn instead of pr_warning
      dma-debug: Use pr_warn instead of pr_warning
      vgacon: Use pr_warn instead of pr_warning
      fs: afs: Use pr_warn instead of pr_warning
      sh/intc: Use pr_warn instead of pr_warning
      scsi: Use pr_warn instead of pr_warning
      platform/x86: intel_oaktrail: Use pr_warn instead of pr_warning
      platform/x86: asus-laptop: Use pr_warn instead of pr_warning
      platform/x86: eeepc-laptop: Use pr_warn instead of pr_warning
      oprofile: Use pr_warn instead of pr_warning
      of: Use pr_warn instead of pr_warning
      macintosh: Use pr_warn instead of pr_warning
      idsn: Use pr_warn instead of pr_warning
      ide: Use pr_warn instead of pr_warning
      crypto: n2: Use pr_warn instead of pr_warning
      ...

commit a74ec64af20a2fc043609339b7a0c8aa2a961c8c
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Fri Oct 18 11:18:19 2019 +0800

    arm64: Use pr_warn instead of pr_warning
    
    As said in commit f2c2cbcc35d4 ("powerpc: Use pr_warn instead of
    pr_warning"), removing pr_warning so all logging messages use a
    consistent <prefix>_warn style. Let's do it.
    
    Link: http://lkml.kernel.org/r/20191018031850.48498-2-wangkefeng.wang@huawei.com
    To: linux-kernel@vger.kernel.org
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Acked-by: Will Deacon <will@kernel.org>
    Reviewed-by: Sergey Senozhatsky <sergey.senozhatsky@gmail.com>
    Signed-off-by: Petr Mladek <pmladek@suse.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 9286ee6749e8..09cf729edb20 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -343,8 +343,7 @@ void __cpu_die(unsigned int cpu)
 	 */
 	err = op_cpu_kill(cpu);
 	if (err)
-		pr_warn("CPU%d may not have shut down cleanly: %d\n",
-			cpu, err);
+		pr_warn("CPU%d may not have shut down cleanly: %d\n", cpu, err);
 }
 
 /*
@@ -979,8 +978,8 @@ void smp_send_stop(void)
 		udelay(1);
 
 	if (num_online_cpus() > 1)
-		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
-			   cpumask_pr_args(cpu_online_mask));
+		pr_warn("SMP: failed to stop secondary CPUs %*pbl\n",
+			cpumask_pr_args(cpu_online_mask));
 
 	sdei_mask_local_cpu();
 }
@@ -1020,8 +1019,8 @@ void crash_smp_send_stop(void)
 		udelay(1);
 
 	if (atomic_read(&waiting_for_crash_ipi) > 0)
-		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
-			   cpumask_pr_args(&mask));
+		pr_warn("SMP: failed to stop secondary CPUs %*pbl\n",
+			cpumask_pr_args(&mask));
 
 	sdei_mask_local_cpu();
 }

commit ebef746543fd1aa162216b0e484eb9062b65741d
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 27 15:54:56 2019 +0100

    arm64: smp: Treat unknown boot failures as being 'stuck in kernel'
    
    When we fail to bring a secondary CPU online and it fails in an unknown
    state, we should assume the worst and increment 'cpus_stuck_in_kernel'
    so that things like kexec() are disabled.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 1f8aeb77cba5..dc9fe879c279 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -147,6 +147,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		default:
 			pr_err("CPU%u: failed in unknown state : 0x%lx\n",
 					cpu, status);
+			cpus_stuck_in_kernel++;
 			break;
 		case CPU_KILL_ME:
 			if (!op_cpu_kill(cpu)) {

commit 5b1cfe3a0ba74c1f2b83b607712a217b9f9463a2
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 27 14:36:38 2019 +0100

    arm64: smp: Don't enter kernel with NULL stack pointer or task struct
    
    Although SMP bringup is inherently racy, we can significantly reduce
    the window during which secondary CPUs can unexpectedly enter the
    kernel by sanity checking the 'stack' and 'task' fields of the
    'secondary_data' structure. If the booting CPU gave up waiting for us,
    then they will have been cleared to NULL and we should spin in a WFE; WFI
    loop instead.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 63c7a7682e93..1f8aeb77cba5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -136,6 +136,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 
 	secondary_data.task = NULL;
 	secondary_data.stack = NULL;
+	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 	status = READ_ONCE(secondary_data.status);
 	if (ret && status) {
 

commit 0e1645557d19fc6d88d3c40431f63a3c3a4c417b
Author: Will Deacon <will@kernel.org>
Date:   Tue Aug 27 14:25:44 2019 +0100

    arm64: smp: Increase secondary CPU boot timeout value
    
    When many debug options are enabled simultaneously (e.g. PROVE_LOCKING,
    KMEMLEAK, DEBUG_PAGE_ALLOC, KASAN etc), it is possible for us to timeout
    when attempting to boot a secondary CPU and give up. Unfortunately, the
    CPU will /eventually/ appear, and sit in the background happily stuck
    in a recursive exception due to a NULL stack pointer.
    
    Increase the timeout to 5s, which will of course be enough for anybody.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 018a33e01b0e..63c7a7682e93 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -123,7 +123,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		 * time out.
 		 */
 		wait_for_completion_timeout(&cpu_running,
-					    msecs_to_jiffies(1000));
+					    msecs_to_jiffies(5000));
 
 		if (!cpu_online(cpu)) {
 			pr_crit("CPU%u: failed to come online\n", cpu);

commit 6655473920129eb2dd1dded147722316294a699a
Author: Anders Roxell <anders.roxell@linaro.org>
Date:   Fri Jul 26 13:27:25 2019 +0200

    arm64: smp: Mark expected switch fall-through
    
    When fall-through warnings was enabled by default the following warning
    was starting to show up:
    
    In file included from ../include/linux/kernel.h:15,
                     from ../include/linux/list.h:9,
                     from ../include/linux/kobject.h:19,
                     from ../include/linux/of.h:17,
                     from ../include/linux/irqdomain.h:35,
                     from ../include/linux/acpi.h:13,
                     from ../arch/arm64/kernel/smp.c:9:
    ../arch/arm64/kernel/smp.c: In function __cpu_up:
    ../include/linux/printk.h:302:2: warning: this statement may fall
     through [-Wimplicit-fallthrough=]
      printk(KERN_CRIT pr_fmt(fmt), ##__VA_ARGS__)
      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    ../arch/arm64/kernel/smp.c:156:4: note: in expansion of macro pr_crit
        pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
        ^~~~~~~
    ../arch/arm64/kernel/smp.c:157:3: note: here
       case CPU_STUCK_IN_KERNEL:
       ^~~~
    
    Rework so that the compiler doesn't warn about fall-through.
    
    Fixes: d93512ef0f0e ("Makefile: Globally enable fall-through warning")
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index ea90d3bd9253..018a33e01b0e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -152,8 +152,8 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 				pr_crit("CPU%u: died during early boot\n", cpu);
 				break;
 			}
-			/* Fall through */
 			pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
+			/* Fall through */
 		case CPU_STUCK_IN_KERNEL:
 			pr_crit("CPU%u: is stuck in kernel\n", cpu);
 			if (status & CPU_STUCK_REASON_52_BIT_VA)

commit ba5c5e4a5da443e80a3722e67515de5e37375b18
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Jul 11 20:59:15 2019 -0700

    arm64: move jump_label_init() before parse_early_param()
    
    While jump_label_init() was moved earlier in the boot process in
    efd9e03facd0 ("arm64: Use static keys for CPU features"), it wasn't early
    enough for early params to use it.  The old state of things was as
    described here...
    
    init/main.c calls out to arch-specific things before general jump label
    and early param handling:
    
      asmlinkage __visible void __init start_kernel(void)
      {
            ...
            setup_arch(&command_line);
            ...
            smp_prepare_boot_cpu();
            ...
            /* parameters may set static keys */
            jump_label_init();
            parse_early_param();
            ...
      }
    
    x86 setup_arch() wants those earlier, so it handles jump label and
    early param:
    
      void __init setup_arch(char **cmdline_p)
      {
            ...
            jump_label_init();
            ...
            parse_early_param();
            ...
      }
    
    arm64 setup_arch() only had early param:
    
      void __init setup_arch(char **cmdline_p)
      {
            ...
            parse_early_param();
            ...
    }
    
    with jump label later in smp_prepare_boot_cpu():
    
      void __init smp_prepare_boot_cpu(void)
      {
            ...
            jump_label_init();
            ...
      }
    
    This moves arm64 jump_label_init() from smp_prepare_boot_cpu() to
    setup_arch(), as done already on x86, in preparation from early param
    usage in the init_on_alloc/free() series:
    https://lkml.kernel.org/r/1561572949.5154.81.camel@lca.pw
    
    Link: http://lkml.kernel.org/r/201906271003.005303B52@keescook
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 9286ee6749e8..ea90d3bd9253 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -420,11 +420,6 @@ void __init smp_cpus_done(unsigned int max_cpus)
 void __init smp_prepare_boot_cpu(void)
 {
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
-	/*
-	 * Initialise the static keys early as they may be enabled by the
-	 * cpufeature code.
-	 */
-	jump_label_init();
 	cpuinfo_store_boot_cpu();
 
 	/*

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit d914d4d4974529da898f2d2618e39df757147c2f
Author: Aaro Koskinen <aaro.koskinen@nokia.com>
Date:   Mon Jun 17 23:35:19 2019 +0300

    arm64: Implement panic_smp_self_stop()
    
    Currently arm64 uses the default implementation of panic_smp_self_stop()
    where the CPU runs in a cpu_relax() loop unable to receive IPIs anymore.
    As a result, when two CPUs panic() simultaneously we get "SMP: failed to
    stop secondary CPUs" warnings and extra delays before a reset, because
    smp_send_stop() still tries to stop the other paniced CPU.
    
    Provide an implementation of panic_smp_self_stop() that is identical to
    the IPI CPU stop handler, so that the online status of stopped CPUs gets
    properly updated.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Aaro Koskinen <aaro.koskinen@nokia.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 932461da5dd7..434d6714358b 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -841,18 +841,25 @@ void arch_irq_work_raise(void)
 }
 #endif
 
-/*
- * ipi_cpu_stop - handle IPI from smp_send_stop()
- */
-static void ipi_cpu_stop(unsigned int cpu)
+static void local_cpu_stop(void)
 {
-	set_cpu_online(cpu, false);
+	set_cpu_online(smp_processor_id(), false);
 
 	local_daif_mask();
 	sdei_mask_local_cpu();
 	cpu_park_loop();
 }
 
+/*
+ * We need to implement panic_smp_self_stop() for parallel panic() calls, so
+ * that cpu_online_mask gets correctly updated and smp_send_stop() can skip
+ * CPUs that have already stopped themselves.
+ */
+void panic_smp_self_stop(void)
+{
+	local_cpu_stop();
+}
+
 #ifdef CONFIG_KEXEC_CORE
 static atomic_t waiting_for_crash_ipi = ATOMIC_INIT(0);
 #endif
@@ -903,7 +910,7 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 
 	case IPI_CPU_STOP:
 		irq_enter();
-		ipi_cpu_stop(cpu);
+		local_cpu_stop();
 		irq_exit();
 		break;
 

commit dccc9da22dedad203acea355b0e4d946b71172e5
Author: Jayachandran C <jnair@caviumnetworks.com>
Date:   Mon Jun 17 23:35:18 2019 +0300

    arm64: Improve parking of stopped CPUs
    
    The current code puts the stopped cpus in an 'yield' instruction loop.
    Using a busy loop here is unnecessary, we can use the cpu_park_loop()
    function here to do a wfi/wfe.
    
    Signed-off-by: Jayachandran C <jnair@caviumnetworks.com>
    Signed-off-by: Aaro Koskinen <aaro.koskinen@nokia.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 83cdb0aa2ff1..932461da5dd7 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -850,9 +850,7 @@ static void ipi_cpu_stop(unsigned int cpu)
 
 	local_daif_mask();
 	sdei_mask_local_cpu();
-
-	while (1)
-		cpu_relax();
+	cpu_park_loop();
 }
 
 #ifdef CONFIG_KEXEC_CORE

commit e1d22385ea6686ff3dcd7092d84465c193849829
Author: Wei Li <liwei391@huawei.com>
Date:   Tue Jun 11 10:38:12 2019 +0100

    arm64: fix kernel stack overflow in kdump capture kernel
    
    When enabling ARM64_PSEUDO_NMI feature in kdump capture kernel, it will
    report a kernel stack overflow exception:
    
    [    0.000000] CPU features: detected: IRQ priority masking
    [    0.000000] alternatives: patching kernel code
    [    0.000000] Insufficient stack space to handle exception!
    [    0.000000] ESR: 0x96000044 -- DABT (current EL)
    [    0.000000] FAR: 0x0000000000000040
    [    0.000000] Task stack:     [0xffff0000097f0000..0xffff0000097f4000]
    [    0.000000] IRQ stack:      [0x0000000000000000..0x0000000000004000]
    [    0.000000] Overflow stack: [0xffff80002b7cf290..0xffff80002b7d0290]
    [    0.000000] CPU: 0 PID: 0 Comm: swapper Not tainted 4.19.34-lw+ #3
    [    0.000000] pstate: 400003c5 (nZcv DAIF -PAN -UAO)
    [    0.000000] pc : el1_sync+0x0/0xb8
    [    0.000000] lr : el1_irq+0xb8/0x140
    [    0.000000] sp : 0000000000000040
    [    0.000000] pmr_save: 00000070
    [    0.000000] x29: ffff0000097f3f60 x28: ffff000009806240
    [    0.000000] x27: 0000000080000000 x26: 0000000000004000
    [    0.000000] x25: 0000000000000000 x24: ffff000009329028
    [    0.000000] x23: 0000000040000005 x22: ffff000008095c6c
    [    0.000000] x21: ffff0000097f3f70 x20: 0000000000000070
    [    0.000000] x19: ffff0000097f3e30 x18: ffffffffffffffff
    [    0.000000] x17: 0000000000000000 x16: 0000000000000000
    [    0.000000] x15: ffff0000097f9708 x14: ffff000089a382ef
    [    0.000000] x13: ffff000009a382fd x12: ffff000009824000
    [    0.000000] x11: ffff0000097fb7b0 x10: ffff000008730028
    [    0.000000] x9 : ffff000009440018 x8 : 000000000000000d
    [    0.000000] x7 : 6b20676e69686374 x6 : 000000000000003b
    [    0.000000] x5 : 0000000000000000 x4 : ffff000008093600
    [    0.000000] x3 : 0000000400000008 x2 : 7db2e689fc2b8e00
    [    0.000000] x1 : 0000000000000000 x0 : ffff0000097f3e30
    [    0.000000] Kernel panic - not syncing: kernel stack overflow
    [    0.000000] CPU: 0 PID: 0 Comm: swapper Not tainted 4.19.34-lw+ #3
    [    0.000000] Call trace:
    [    0.000000]  dump_backtrace+0x0/0x1b8
    [    0.000000]  show_stack+0x24/0x30
    [    0.000000]  dump_stack+0xa8/0xcc
    [    0.000000]  panic+0x134/0x30c
    [    0.000000]  __stack_chk_fail+0x0/0x28
    [    0.000000]  handle_bad_stack+0xfc/0x108
    [    0.000000]  __bad_stack+0x90/0x94
    [    0.000000]  el1_sync+0x0/0xb8
    [    0.000000]  init_gic_priority_masking+0x4c/0x70
    [    0.000000]  smp_prepare_boot_cpu+0x60/0x68
    [    0.000000]  start_kernel+0x1e8/0x53c
    [    0.000000] ---[ end Kernel panic - not syncing: kernel stack overflow ]---
    
    The reason is init_gic_priority_masking() may unmask PSR.I while the
    irq stacks are not inited yet. Some "NMI" could be raised unfortunately
    and it will just go into this exception.
    
    In this patch, we just write the PMR in smp_prepare_boot_cpu(), and delay
    unmasking PSR.I after irq stacks inited in init_IRQ().
    
    Fixes: e79321883842 ("arm64: Switch to PMR masking when starting CPUs")
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Wei Li <liwei391@huawei.com>
    [JT: make init_gic_priority_masking() not modify daif, rebase on other
         priority masking fixes]
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 4deaee3c2a33..83cdb0aa2ff1 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -192,13 +192,7 @@ static void init_gic_priority_masking(void)
 
 	WARN_ON(!(cpuflags & PSR_I_BIT));
 
-	/* We can only unmask PSR.I if we can take aborts */
-	if (!(cpuflags & PSR_A_BIT)) {
-		gic_write_pmr(GIC_PRIO_IRQOFF);
-		write_sysreg(cpuflags & ~PSR_I_BIT, daif);
-	} else {
-		gic_write_pmr(GIC_PRIO_IRQON | GIC_PRIO_PSR_I_SET);
-	}
+	gic_write_pmr(GIC_PRIO_IRQON | GIC_PRIO_PSR_I_SET);
 }
 
 /*

commit bd82d4bd21880b7c4d5f5756be435095d6ae07b5
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:10 2019 +0100

    arm64: Fix incorrect irqflag restore for priority masking
    
    When using IRQ priority masking to disable interrupts, in order to deal
    with the PSR.I state, local_irq_save() would convert the I bit into a
    PMR value (GIC_PRIO_IRQOFF). This resulted in local_irq_restore()
    potentially modifying the value of PMR in undesired location due to the
    state of PSR.I upon flag saving [1].
    
    In an attempt to solve this issue in a less hackish manner, introduce
    a bit (GIC_PRIO_IGNORE_PMR) for the PMR values that can represent
    whether PSR.I is being used to disable interrupts, in which case it
    takes precedence of the status of interrupt masking via PMR.
    
    GIC_PRIO_PSR_I_SET is chosen such that (<pmr_value> |
    GIC_PRIO_PSR_I_SET) does not mask more interrupts than <pmr_value> as
    some sections (e.g. arch_cpu_idle(), interrupt acknowledge path)
    requires PMR not to mask interrupts that could be signaled to the
    CPU when using only PSR.I.
    
    [1] https://www.spinics.net/lists/arm-kernel/msg716956.html
    
    Fixes: 4a503217ce37 ("arm64: irqflags: Use ICC_PMR_EL1 for interrupt masking")
    Cc: <stable@vger.kernel.org> # 5.1.x-
    Reported-by: Zenghui Yu <yuzenghui@huawei.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Wei Li <liwei391@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Pouloze <suzuki.poulose@arm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index bb4b3f07761a..4deaee3c2a33 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -192,11 +192,13 @@ static void init_gic_priority_masking(void)
 
 	WARN_ON(!(cpuflags & PSR_I_BIT));
 
-	gic_write_pmr(GIC_PRIO_IRQOFF);
-
 	/* We can only unmask PSR.I if we can take aborts */
-	if (!(cpuflags & PSR_A_BIT))
+	if (!(cpuflags & PSR_A_BIT)) {
+		gic_write_pmr(GIC_PRIO_IRQOFF);
 		write_sysreg(cpuflags & ~PSR_I_BIT, daif);
+	} else {
+		gic_write_pmr(GIC_PRIO_IRQON | GIC_PRIO_PSR_I_SET);
+	}
 }
 
 /*

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index bb4b3f07761a..6dcf9607d770 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -1,20 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * SMP initialisation and IPI support
  * Based on arch/arm/kernel/smp.c
  *
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/acpi.h>

commit 60574d1e05b094d222162260dd9cac49f4d0996a
Author: Keith Busch <keith.busch@intel.com>
Date:   Mon Mar 11 14:55:57 2019 -0600

    acpi: Create subtable parsing infrastructure
    
    Parsing entries in an ACPI table had assumed a generic header
    structure. There is no standard ACPI header, though, so less common
    layouts with different field sizes required custom parsers to go through
    their subtable entry list.
    
    Create the infrastructure for adding different table types so parsing
    the entries array may be more reused for all ACPI system tables and
    the common code doesn't need to be duplicated.
    
    Reviewed-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Tested-by: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Signed-off-by: Keith Busch <keith.busch@intel.com>
    Tested-by: Brice Goglin <Brice.Goglin@inria.fr>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 824de7038967..bb4b3f07761a 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -586,7 +586,7 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 }
 
 static int __init
-acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
+acpi_parse_gic_cpu_interface(union acpi_subtable_headers *header,
 			     const unsigned long end)
 {
 	struct acpi_madt_generic_interrupt *processor;
@@ -595,7 +595,7 @@ acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
 	if (BAD_MADT_GICC_ENTRY(processor, end))
 		return -EINVAL;
 
-	acpi_table_print_madt_entry(header);
+	acpi_table_print_madt_entry(&header->common);
 
 	acpi_map_gic_cpu_interface(processor);
 

commit e79321883842ca7b77d8a58fe8303e8da35c085e
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:55 2019 +0000

    arm64: Switch to PMR masking when starting CPUs
    
    Once the boot CPU has been prepared or a new secondary CPU has been
    brought up, use ICC_PMR_EL1 to mask interrupts on that CPU and clear
    PSR.I bit.
    
    Since ICC_PMR_EL1 is initialized at CPU bringup, avoid overwriting
    it in the GICv3 driver.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Suggested-by: Daniel Thompson <daniel.thompson@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a944edd39d2d..824de7038967 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -35,6 +35,7 @@
 #include <linux/smp.h>
 #include <linux/seq_file.h>
 #include <linux/irq.h>
+#include <linux/irqchip/arm-gic-v3.h>
 #include <linux/percpu.h>
 #include <linux/clockchips.h>
 #include <linux/completion.h>
@@ -180,6 +181,24 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	return ret;
 }
 
+static void init_gic_priority_masking(void)
+{
+	u32 cpuflags;
+
+	if (WARN_ON(!gic_enable_sre()))
+		return;
+
+	cpuflags = read_sysreg(daif);
+
+	WARN_ON(!(cpuflags & PSR_I_BIT));
+
+	gic_write_pmr(GIC_PRIO_IRQOFF);
+
+	/* We can only unmask PSR.I if we can take aborts */
+	if (!(cpuflags & PSR_A_BIT))
+		write_sysreg(cpuflags & ~PSR_I_BIT, daif);
+}
+
 /*
  * This is the secondary CPU boot entry.  We're using this CPUs
  * idle thread stack, but a set of temporary page tables.
@@ -206,6 +225,9 @@ asmlinkage notrace void secondary_start_kernel(void)
 	 */
 	cpu_uninstall_idmap();
 
+	if (system_uses_irq_prio_masking())
+		init_gic_priority_masking();
+
 	preempt_disable();
 	trace_hardirqs_off();
 
@@ -426,6 +448,10 @@ void __init smp_prepare_boot_cpu(void)
 	 * and/or scheduling is enabled.
 	 */
 	apply_boot_alternatives();
+
+	/* Conditionally switch to GIC PMR for interrupt masking */
+	if (system_uses_irq_prio_masking())
+		init_gic_priority_masking();
 }
 
 static u64 __init of_get_cpu_mpidr(struct device_node *dn)

commit 0ceb0d56905e3d141fae77e5936d00eee9233473
Author: Daniel Thompson <daniel.thompson@linaro.org>
Date:   Thu Jan 31 14:58:53 2019 +0000

    arm64: alternative: Apply alternatives early in boot process
    
    Currently alternatives are applied very late in the boot process (and
    a long time after we enable scheduling). Some alternative sequences,
    such as those that alter the way CPU context is stored, must be applied
    much earlier in the boot sequence.
    
    Introduce apply_boot_alternatives() to allow some alternatives to be
    applied immediately after we detect the CPU features of the boot CPU.
    
    Signed-off-by: Daniel Thompson <daniel.thompson@linaro.org>
    [julien.thierry@arm.com: rename to fit new cpufeature framework better,
                             apply BOOT_SCOPE feature early in boot]
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 1598d6f7200a..a944edd39d2d 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -419,6 +419,13 @@ void __init smp_prepare_boot_cpu(void)
 	 */
 	jump_label_init();
 	cpuinfo_store_boot_cpu();
+
+	/*
+	 * We now know enough about the boot CPU to apply the
+	 * alternatives that cannot wait until interrupt handling
+	 * and/or scheduling is enabled.
+	 */
+	apply_boot_alternatives();
 }
 
 static u64 __init of_get_cpu_mpidr(struct device_node *dn)

commit f357b3a7e17af7736d67d8267edc1ed3d1dd9391
Author: Suzuki K Poulose <Suzuki.Poulose@arm.com>
Date:   Mon Dec 10 18:07:33 2018 +0000

    arm64: smp: Handle errors reported by the firmware
    
    The __cpu_up() routine ignores the errors reported by the firmware
    for a CPU bringup operation and looks for the error status set by the
    booting CPU. If the CPU never entered the kernel, we could end up
    in assuming stale error status, which otherwise would have been
    set/cleared appropriately by the booting CPU.
    
    Reported-by: Steve Capper <steve.capper@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 4e3bfbde829a..1598d6f7200a 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -141,6 +141,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		}
 	} else {
 		pr_err("CPU%u: failed to boot: %d\n", cpu, ret);
+		return ret;
 	}
 
 	secondary_data.task = NULL;

commit 66f16a24512fa44680504effe908df8326885594
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 14:21:13 2018 +0000

    arm64: smp: Rework early feature mismatched detection
    
    Rather than add additional variables to detect specific early feature
    mismatches with secondary CPUs, we can instead dedicate the upper bits
    of the CPU boot status word to flag specific mismatches.
    
    This allows us to communicate both granule and VA-size mismatches back
    to the primary CPU without the need for additional book-keeping.
    
    Tested-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 1ff18f5fbecb..4e3bfbde829a 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -108,7 +108,6 @@ static int boot_secondary(unsigned int cpu, struct task_struct *idle)
 }
 
 static DECLARE_COMPLETION(cpu_running);
-bool va52mismatch __ro_after_init;
 
 int __cpu_up(unsigned int cpu, struct task_struct *idle)
 {
@@ -138,10 +137,6 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 
 		if (!cpu_online(cpu)) {
 			pr_crit("CPU%u: failed to come online\n", cpu);
-
-			if (IS_ENABLED(CONFIG_ARM64_USER_VA_BITS_52) && va52mismatch)
-				pr_crit("CPU%u: does not support 52-bit VAs\n", cpu);
-
 			ret = -EIO;
 		}
 	} else {
@@ -156,7 +151,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		if (status == CPU_MMU_OFF)
 			status = READ_ONCE(__early_cpu_boot_status);
 
-		switch (status) {
+		switch (status & CPU_BOOT_STATUS_MASK) {
 		default:
 			pr_err("CPU%u: failed in unknown state : 0x%lx\n",
 					cpu, status);
@@ -170,6 +165,10 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 			pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
 		case CPU_STUCK_IN_KERNEL:
 			pr_crit("CPU%u: is stuck in kernel\n", cpu);
+			if (status & CPU_STUCK_REASON_52_BIT_VA)
+				pr_crit("CPU%u: does not support 52-bit VAs\n", cpu);
+			if (status & CPU_STUCK_REASON_NO_GRAN)
+				pr_crit("CPU%u: does not support %luK granule \n", cpu, PAGE_SIZE / SZ_1K);
 			cpus_stuck_in_kernel++;
 			break;
 		case CPU_PANIC_KERNEL:

commit 68d23da4373aba76f5300017c4746440f276698e
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 14:15:15 2018 +0000

    arm64: Kconfig: Re-jig CONFIG options for 52-bit VA
    
    Enabling 52-bit VAs for userspace is pretty confusing, since it requires
    you to select "48-bit" virtual addressing in the Kconfig.
    
    Rework the logic so that 52-bit user virtual addressing is advertised in
    the "Virtual address space size" choice, along with some help text to
    describe its interaction with Pointer Authentication. The EXPERT-only
    option to force all user mappings to the 52-bit range is then made
    available immediately below the VA size selection.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index e15b0b64d4d0..1ff18f5fbecb 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -139,7 +139,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		if (!cpu_online(cpu)) {
 			pr_crit("CPU%u: failed to come online\n", cpu);
 
-			if (IS_ENABLED(CONFIG_ARM64_52BIT_VA) && va52mismatch)
+			if (IS_ENABLED(CONFIG_ARM64_USER_VA_BITS_52) && va52mismatch)
 				pr_crit("CPU%u: does not support 52-bit VAs\n", cpu);
 
 			ret = -EIO;

commit a96a33b1ca57dbea4285893dedf290aeb8eb090b
Author: Steve Capper <steve.capper@arm.com>
Date:   Thu Dec 6 22:50:40 2018 +0000

    arm64: mm: Prevent mismatched 52-bit VA support
    
    For cases where there is a mismatch in ARMv8.2-LVA support between CPUs
    we have to be careful in allowing secondary CPUs to boot if 52-bit
    virtual addresses have already been enabled on the boot CPU.
    
    This patch adds code to the secondary startup path. If the boot CPU has
    enabled 52-bit VAs then ID_AA64MMFR2_EL1 is checked to see if the
    secondary can also enable 52-bit support. If not, the secondary is
    prevented from booting and an error message is displayed indicating why.
    
    Technically this patch could be implemented using the cpufeature code
    when considering 52-bit userspace support. However, we employ low level
    checks here as the cpufeature code won't be able to run if we have
    mismatched 52-bit kernel va support.
    
    Signed-off-by: Steve Capper <steve.capper@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 96b8f2f51ab2..e15b0b64d4d0 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -108,6 +108,7 @@ static int boot_secondary(unsigned int cpu, struct task_struct *idle)
 }
 
 static DECLARE_COMPLETION(cpu_running);
+bool va52mismatch __ro_after_init;
 
 int __cpu_up(unsigned int cpu, struct task_struct *idle)
 {
@@ -137,6 +138,10 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 
 		if (!cpu_online(cpu)) {
 			pr_crit("CPU%u: failed to come online\n", cpu);
+
+			if (IS_ENABLED(CONFIG_ARM64_52BIT_VA) && va52mismatch)
+				pr_crit("CPU%u: does not support 52-bit VAs\n", cpu);
+
 			ret = -EIO;
 		}
 	} else {

commit de76e70a8d4ea4518ec46dff2f92e2970af1d512
Author: Rob Herring <robh@kernel.org>
Date:   Mon Aug 27 09:43:01 2018 -0500

    arm64: use for_each_of_cpu_node iterator
    
    Use the for_each_of_cpu_node iterator to iterate over cpu nodes. This
    has the side effect of defaulting to iterating using "cpu" node names in
    preference to the deprecated (for FDT) device_type == "cpu".
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 25fcd22a4bb2..96b8f2f51ab2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -602,7 +602,7 @@ static void __init of_parse_and_init_cpus(void)
 {
 	struct device_node *dn;
 
-	for_each_node_by_type(dn, "cpu") {
+	for_each_of_cpu_node(dn) {
 		u64 hwid = of_get_cpu_mpidr(dn);
 
 		if (hwid == INVALID_HWID)

commit e189624916961c735c18e3c75acc478661403830
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Mon Jun 25 14:05:52 2018 +0100

    arm64: numa: rework ACPI NUMA initialization
    
    Current ACPI ARM64 NUMA initialization code in
    
    acpi_numa_gicc_affinity_init()
    
    carries out NUMA nodes creation and cpu<->node mappings at the same time
    in the arch backend so that a single SRAT walk is needed to parse both
    pieces of information.  This implies that the cpu<->node mappings must
    be stashed in an array (sized NR_CPUS) so that SMP code can later use
    the stashed values to avoid another SRAT table walk to set-up the early
    cpu<->node mappings.
    
    If the kernel is configured with a NR_CPUS value less than the actual
    processor entries in the SRAT (and MADT), the logic in
    acpi_numa_gicc_affinity_init() is broken in that the cpu<->node mapping
    is only carried out (and stashed for future use) only for a number of
    SRAT entries up to NR_CPUS, which do not necessarily correspond to the
    possible cpus detected at SMP initialization in
    acpi_map_gic_cpu_interface() (ie MADT and SRAT processor entries order
    is not enforced), which leaves the kernel with broken cpu<->node
    mappings.
    
    Furthermore, given the current ACPI NUMA code parsing logic in
    acpi_numa_gicc_affinity_init(), PXM domains for CPUs that are not parsed
    because they exceed NR_CPUS entries are not mapped to NUMA nodes (ie the
    PXM corresponding node is not created in the kernel) leaving the system
    with a broken NUMA topology.
    
    Rework the ACPI ARM64 NUMA initialization process so that the NUMA
    nodes creation and cpu<->node mappings are decoupled. cpu<->node
    mappings are moved to SMP initialization code (where they are needed),
    at the cost of an extra SRAT walk so that ACPI NUMA mappings can be
    batched before being applied, fixing current parsing pitfalls.
    
    Acked-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: John Garry <john.garry@huawei.com>
    Fixes: d8b47fca8c23 ("arm64, ACPI, NUMA: NUMA support based on SRAT and
    SLIT")
    Link: http://lkml.kernel.org/r/1527768879-88161-2-git-send-email-xiexiuqi@huawei.com
    Reported-by: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Punit Agrawal <punit.agrawal@arm.com>
    Cc: Jonathan Cameron <jonathan.cameron@huawei.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <guohanjun@huawei.com>
    Cc: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Cc: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a44cc09059b5..25fcd22a4bb2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -522,7 +522,6 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 		}
 		bootcpu_valid = true;
 		cpu_madt_gicc[0] = *processor;
-		early_map_cpu_to_node(0, acpi_numa_get_nid(0, hwid));
 		return;
 	}
 
@@ -545,8 +544,6 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 	 */
 	acpi_set_mailbox_entry(cpu_count, processor);
 
-	early_map_cpu_to_node(cpu_count, acpi_numa_get_nid(cpu_count, hwid));
-
 	cpu_count++;
 }
 
@@ -566,8 +563,34 @@ acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
 
 	return 0;
 }
+
+static void __init acpi_parse_and_init_cpus(void)
+{
+	int i;
+
+	/*
+	 * do a walk of MADT to determine how many CPUs
+	 * we have including disabled CPUs, and get information
+	 * we need for SMP init.
+	 */
+	acpi_table_parse_madt(ACPI_MADT_TYPE_GENERIC_INTERRUPT,
+				      acpi_parse_gic_cpu_interface, 0);
+
+	/*
+	 * In ACPI, SMP and CPU NUMA information is provided in separate
+	 * static tables, namely the MADT and the SRAT.
+	 *
+	 * Thus, it is simpler to first create the cpu logical map through
+	 * an MADT walk and then map the logical cpus to their node ids
+	 * as separate steps.
+	 */
+	acpi_map_cpus_to_nodes();
+
+	for (i = 0; i < nr_cpu_ids; i++)
+		early_map_cpu_to_node(i, acpi_numa_get_nid(i));
+}
 #else
-#define acpi_table_parse_madt(...)	do { } while (0)
+#define acpi_parse_and_init_cpus(...)	do { } while (0)
 #endif
 
 /*
@@ -640,13 +663,7 @@ void __init smp_init_cpus(void)
 	if (acpi_disabled)
 		of_parse_and_init_cpus();
 	else
-		/*
-		 * do a walk of MADT to determine how many CPUs
-		 * we have including disabled CPUs, and get information
-		 * we need for SMP init
-		 */
-		acpi_table_parse_madt(ACPI_MADT_TYPE_GENERIC_INTERRUPT,
-				      acpi_parse_gic_cpu_interface, 0);
+		acpi_parse_and_init_cpus();
 
 	if (cpu_count > nr_cpu_ids)
 		pr_warn("Number of cores (%d) exceeds configured maximum of %u - clipping\n",

commit 7f9545aa1a91a9a4dc8c3e1476dbbfa98dd38b81
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Fri Jul 6 12:02:46 2018 +0100

    arm64: smp: remove cpu and numa topology information when hotplugging out CPU
    
    We already repopulate the information on CPU hotplug-in, so we can safely
    remove the CPU topology and NUMA cpumap information during CPU hotplug
    out operation. This will help to provide the correct cpumask for
    scheduler domains.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Tested-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 2a315f32fad3..a44cc09059b5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -279,6 +279,9 @@ int __cpu_disable(void)
 	if (ret)
 		return ret;
 
+	remove_cpu_topology(cpu);
+	numa_remove_cpu(cpu);
+
 	/*
 	 * Take this CPU offline.  Once we clear this, we can't return,
 	 * and we must not schedule until we're ready to give up the cpu.

commit 97fd6016a7b3df00901b4cfdd883eac01e89fa8a
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Fri Jul 6 12:02:43 2018 +0100

    arm64: numa: separate out updates to percpu nodeid and NUMA node cpumap
    
    Currently numa_clear_node removes both cpu information from the NUMA
    node cpumap as well as the NUMA node id from the cpu. Similarly
    numa_store_cpu_info updates both percpu nodeid and NUMA cpumap.
    
    However we need to retain the numa node id for the cpu and only remove
    the cpu information from the numa node cpumap during CPU hotplug out.
    The same can be extended for hotplugging in the CPU.
    
    This patch separates out numa_{add,remove}_cpu from numa_clear_node and
    numa_store_cpu_info.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Ganapatrao Kulkarni <ganapatrao.kulkarni@cavium.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 2faa9863d2e5..2a315f32fad3 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -225,6 +225,7 @@ asmlinkage notrace void secondary_start_kernel(void)
 	notify_cpu_starting(cpu);
 
 	store_cpu_topology(cpu);
+	numa_add_cpu(cpu);
 
 	/*
 	 * OK, now it's safe to let the boot CPU continue.  Wait for
@@ -679,6 +680,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	this_cpu = smp_processor_id();
 	store_cpu_topology(this_cpu);
 	numa_store_cpu_info(this_cpu);
+	numa_add_cpu(this_cpu);
 
 	/*
 	 * If UP is mandated by "nosmp" (which implies "maxcpus=0"), don't set

commit b154886f7892499d0d3054026e19dfb9a731df61
Author: Zhizhou Zhang <zhizhouzhang@asrmicro.com>
Date:   Tue Jun 12 17:07:37 2018 +0800

    arm64: make secondary_start_kernel() notrace
    
    We can't call function trace hook before setup percpu offset.
    When entering secondary_start_kernel(), percpu offset has not
    been initialized.  So this lead hotplug malfunction.
    Here is the flow to reproduce this bug:
    
    echo 0 > /sys/devices/system/cpu/cpu1/online
    echo function > /sys/kernel/debug/tracing/current_tracer
    echo 1 > /sys/kernel/debug/tracing/tracing_on
    echo 1 > /sys/devices/system/cpu/cpu1/online
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Zhizhou Zhang <zhizhouzhang@asrmicro.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index f3e2e3aec0b0..2faa9863d2e5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -179,7 +179,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
  * This is the secondary CPU boot entry.  We're using this CPUs
  * idle thread stack, but a set of temporary page tables.
  */
-asmlinkage void secondary_start_kernel(void)
+asmlinkage notrace void secondary_start_kernel(void)
 {
 	u64 mpidr = read_cpuid_mpidr() & MPIDR_HWID_BITMASK;
 	struct mm_struct *mm = &init_mm;

commit 830dcc9f9a7cd26a812522a26efaacf7df6fc365
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:42 2018 +0100

    arm64: capabilities: Change scope of VHE to Boot CPU feature
    
    We expect all CPUs to be running at the same EL inside the kernel
    with or without VHE enabled and we have strict checks to ensure
    that any mismatch triggers a kernel panic. If VHE is enabled,
    we use the feature based on the boot CPU and all other CPUs
    should follow. This makes it a perfect candidate for a capability
    based on the boot CPU,  which should be matched by all the CPUs
    (both when is ON and OFF). This saves us some not-so-pretty
    hooks and special code, just for verifying the conflict.
    
    The patch also makes the VHE capability entry depend on
    CONFIG_ARM64_VHE.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 5cef11450183..f3e2e3aec0b0 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -85,43 +85,6 @@ enum ipi_msg_type {
 	IPI_WAKEUP
 };
 
-#ifdef CONFIG_ARM64_VHE
-
-/* Whether the boot CPU is running in HYP mode or not*/
-static bool boot_cpu_hyp_mode;
-
-static inline void save_boot_cpu_run_el(void)
-{
-	boot_cpu_hyp_mode = is_kernel_in_hyp_mode();
-}
-
-static inline bool is_boot_cpu_in_hyp_mode(void)
-{
-	return boot_cpu_hyp_mode;
-}
-
-/*
- * Verify that a secondary CPU is running the kernel at the same
- * EL as that of the boot CPU.
- */
-void verify_cpu_run_el(void)
-{
-	bool in_el2 = is_kernel_in_hyp_mode();
-	bool boot_cpu_el2 = is_boot_cpu_in_hyp_mode();
-
-	if (in_el2 ^ boot_cpu_el2) {
-		pr_crit("CPU%d: mismatched Exception Level(EL%d) with boot CPU(EL%d)\n",
-					smp_processor_id(),
-					in_el2 ? 2 : 1,
-					boot_cpu_el2 ? 2 : 1);
-		cpu_panic_kernel();
-	}
-}
-
-#else
-static inline void save_boot_cpu_run_el(void) {}
-#endif
-
 #ifdef CONFIG_HOTPLUG_CPU
 static int op_cpu_kill(unsigned int cpu);
 #else
@@ -447,7 +410,6 @@ void __init smp_prepare_boot_cpu(void)
 	 */
 	jump_label_init();
 	cpuinfo_store_boot_cpu();
-	save_boot_cpu_run_el();
 }
 
 static u64 __init of_get_cpu_mpidr(struct device_node *dn)

commit 5e91107b06811f0ca147cebbedce53626c9c4443
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Mar 26 15:12:29 2018 +0100

    arm64: capabilities: Move errata work around check on boot CPU
    
    We trigger CPU errata work around check on the boot CPU from
    smp_prepare_boot_cpu() to make sure that we run the checks only
    after the CPU feature infrastructure is initialised. While this
    is correct, we can also do this from init_cpu_features() which
    initilises the infrastructure, and is called only on the
    Boot CPU. This helps to consolidate the CPU capability handling
    to cpufeature.c. No functional changes.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 3b8ad7be9c33..5cef11450183 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -448,12 +448,6 @@ void __init smp_prepare_boot_cpu(void)
 	jump_label_init();
 	cpuinfo_store_boot_cpu();
 	save_boot_cpu_run_el();
-	/*
-	 * Run the errata work around checks on the boot CPU, once we have
-	 * initialised the cpu feature infrastructure from
-	 * cpuinfo_store_boot_cpu() above.
-	 */
-	update_cpu_errata_workarounds();
 }
 
 static u64 __init of_get_cpu_mpidr(struct device_node *dn)

commit f5df26961853d6809d704cedcaf082c57f635a64
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:12 2018 +0000

    arm64: kernel: Add arch-specific SDEI entry code and CPU masking
    
    The Software Delegated Exception Interface (SDEI) is an ARM standard
    for registering callbacks from the platform firmware into the OS.
    This is typically used to implement RAS notifications.
    
    Such notifications enter the kernel at the registered entry-point
    with the register values of the interrupted CPU context. Because this
    is not a CPU exception, it cannot reuse the existing entry code.
    (crucially we don't implicitly know which exception level we interrupted),
    
    Add the entry point to entry.S to set us up for calling into C code. If
    the event interrupted code that had interrupts masked, we always return
    to that location. Otherwise we pretend this was an IRQ, and use SDEI's
    complete_and_resume call to return to vbar_el1 + offset.
    
    This allows the kernel to deliver signals to user space processes. For
    KVM this triggers the world switch, a quick spin round vcpu_run, then
    back into the guest, unless there are pending signals.
    
    Add sdei_mask_local_cpu() calls to the smp_send_stop() code, this covers
    the panic() code-path, which doesn't invoke cpuhotplug notifiers.
    
    Because we can interrupt entry-from/exit-to another EL, we can't trust the
    value in sp_el0 or x29, even if we interrupted the kernel, in this case
    the code in entry.S will save/restore sp_el0 and use the value in
    __entry_task.
    
    When we have VMAP stacks we can interrupt the stack-overflow test, which
    stirs x0 into sp, meaning we have to have our own VMAP stacks. For now
    these are allocated when we probe the interface. Future patches will add
    refcounting hooks to allow the arch code to allocate them lazily.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 551eb07c53b6..3b8ad7be9c33 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -18,6 +18,7 @@
  */
 
 #include <linux/acpi.h>
+#include <linux/arm_sdei.h>
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/spinlock.h>
@@ -836,6 +837,7 @@ static void ipi_cpu_stop(unsigned int cpu)
 	set_cpu_online(cpu, false);
 
 	local_daif_mask();
+	sdei_mask_local_cpu();
 
 	while (1)
 		cpu_relax();
@@ -853,6 +855,7 @@ static void ipi_cpu_crash_stop(unsigned int cpu, struct pt_regs *regs)
 	atomic_dec(&waiting_for_crash_ipi);
 
 	local_irq_disable();
+	sdei_mask_local_cpu();
 
 #ifdef CONFIG_HOTPLUG_CPU
 	if (cpu_ops[cpu]->cpu_die)
@@ -972,6 +975,8 @@ void smp_send_stop(void)
 	if (num_online_cpus() > 1)
 		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
 			   cpumask_pr_args(cpu_online_mask));
+
+	sdei_mask_local_cpu();
 }
 
 #ifdef CONFIG_KEXEC_CORE
@@ -990,8 +995,10 @@ void crash_smp_send_stop(void)
 
 	cpus_stopped = 1;
 
-	if (num_online_cpus() == 1)
+	if (num_online_cpus() == 1) {
+		sdei_mask_local_cpu();
 		return;
+	}
 
 	cpumask_copy(&mask, cpu_online_mask);
 	cpumask_clear_cpu(smp_processor_id(), &mask);
@@ -1009,6 +1016,8 @@ void crash_smp_send_stop(void)
 	if (atomic_read(&waiting_for_crash_ipi) > 0)
 		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
 			   cpumask_pr_args(&mask));
+
+	sdei_mask_local_cpu();
 }
 
 bool smp_crash_stop_failed(void)

commit 41bd5b5d22b77c7300df2a2fa5397cbe785189b4
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:36 2017 +0000

    arm64: Move the async/fiq helpers to explicitly set process context flags
    
    Remove the local_{async,fiq}_{en,dis}able macros as they don't respect
    our newly defined order and are only used to set the flags for process
    context when we bring CPUs online.
    
    Add a helper to do this. The IRQ flag varies as we want it masked on
    the boot CPU until we are ready to handle interrupts.
    The boot CPU unmasks SError during early boot once it can print an error
    message. If we can print an error message about SError, we can do the
    same for FIQ. Debug exceptions are already enabled by __cpu_setup(),
    which has also configured MDSCR_EL1 to disable MDE and KDE.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index c94f4a6515c4..551eb07c53b6 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -274,8 +274,7 @@ asmlinkage void secondary_start_kernel(void)
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 
-	local_irq_enable();
-	local_async_enable();
+	local_daif_restore(DAIF_PROCCTX);
 
 	/*
 	 * OK, it's off to the idle thread for us

commit 0fbeb318754860b37150fd42c2058d636a431426
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:34 2017 +0000

    arm64: explicitly mask all exceptions
    
    There are a few places where we want to mask all exceptions. Today we
    do this in a piecemeal fashion, typically we expect the caller to
    have masked irqs and the arch code masks debug exceptions, ignoring
    serror which is probably masked.
    
    Make it clear that 'mask all exceptions' is the intention by adding
    helpers to do exactly that.
    
    This will let us unmask SError without having to add 'oh and SError'
    to these paths.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 824561ef6b8a..c94f4a6515c4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -47,6 +47,7 @@
 #include <asm/cpu.h>
 #include <asm/cputype.h>
 #include <asm/cpu_ops.h>
+#include <asm/daifflags.h>
 #include <asm/mmu_context.h>
 #include <asm/numa.h>
 #include <asm/pgtable.h>
@@ -370,10 +371,6 @@ void __cpu_die(unsigned int cpu)
 /*
  * Called from the idle thread for the CPU which has been shutdown.
  *
- * Note that we disable IRQs here, but do not re-enable them
- * before returning to the caller. This is also the behaviour
- * of the other hotplug-cpu capable cores, so presumably coming
- * out of idle fixes this.
  */
 void cpu_die(void)
 {
@@ -381,7 +378,7 @@ void cpu_die(void)
 
 	idle_task_exit();
 
-	local_irq_disable();
+	local_daif_mask();
 
 	/* Tell __cpu_die() that this CPU is now safe to dispose of */
 	(void)cpu_report_death();
@@ -839,7 +836,7 @@ static void ipi_cpu_stop(unsigned int cpu)
 {
 	set_cpu_online(cpu, false);
 
-	local_irq_disable();
+	local_daif_mask();
 
 	while (1)
 		cpu_relax();

commit ccaac16287f9b46c58777f5538c4ba3a9d4c3aeb
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Sep 27 14:50:38 2017 +0100

    arm64: consistently log boot/secondary CPU IDs
    
    Currently we inconsistently log identifying information for the boot CPU
    and secondary CPUs. For the boot CPU, we log the MIDR and MPIDR across
    separate messages, whereas for the secondary CPUs we only log the MIDR.
    
    In some cases, it would be useful to know the MPIDR of secondary CPUs,
    and it would be nice for these messages to be consistent.
    
    This patch ensures that in the primary and secondary boot paths, we log
    both the MPIDR and MIDR in a single message, with a consistent format.
    the MPIDR is consistently padded to 10 hex characters to cover Aff3 in
    bits 39:32, so that IDs can be compared easily.
    
    The newly redundant message in setup_arch() is removed.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Al Stone <ahs3@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [will: added '0x' prefixes consistently]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 9f7195a5773e..824561ef6b8a 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -216,6 +216,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
  */
 asmlinkage void secondary_start_kernel(void)
 {
+	u64 mpidr = read_cpuid_mpidr() & MPIDR_HWID_BITMASK;
 	struct mm_struct *mm = &init_mm;
 	unsigned int cpu;
 
@@ -265,8 +266,9 @@ asmlinkage void secondary_start_kernel(void)
 	 * the CPU migration code to notice that the CPU is online
 	 * before we continue.
 	 */
-	pr_info("CPU%u: Booted secondary processor [%08x]\n",
-					 cpu, read_cpuid_id());
+	pr_info("CPU%u: Booted secondary processor 0x%010lx [0x%08x]\n",
+					 cpu, (unsigned long)mpidr,
+					 read_cpuid_id());
 	update_cpu_boot_status(CPU_BOOT_SUCCESS);
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);

commit 9b130ad5bb8255ee8534d92d67e12b2a4887eacb
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Fri Sep 8 16:14:18 2017 -0700

    treewide: make "nr_cpu_ids" unsigned
    
    First, number of CPUs can't be negative number.
    
    Second, different signnnedness leads to suboptimal code in the following
    cases:
    
    1)
            kmalloc(nr_cpu_ids * sizeof(X));
    
    "int" has to be sign extended to size_t.
    
    2)
            while (loff_t *pos < nr_cpu_ids)
    
    MOVSXD is 1 byte longed than the same MOV.
    
    Other cases exist as well. Basically compiler is told that nr_cpu_ids
    can't be negative which can't be deduced if it is "int".
    
    Code savings on allyesconfig kernel: -3KB
    
            add/remove: 0/0 grow/shrink: 25/264 up/down: 261/-3631 (-3370)
            function                                     old     new   delta
            coretemp_cpu_online                          450     512     +62
            rcu_init_one                                1234    1272     +38
            pci_device_probe                             374     399     +25
    
                                    ...
    
            pgdat_reclaimable_pages                      628     556     -72
            select_fallback_rq                           446     369     -77
            task_numa_find_cpu                          1923    1807    -116
    
    Link: http://lkml.kernel.org/r/20170819114959.GA30580@avx2
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index ffe089942ac4..9f7195a5773e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -690,7 +690,7 @@ void __init smp_init_cpus(void)
 				      acpi_parse_gic_cpu_interface, 0);
 
 	if (cpu_count > nr_cpu_ids)
-		pr_warn("Number of cores (%d) exceeds configured maximum of %d - clipping\n",
+		pr_warn("Number of cores (%d) exceeds configured maximum of %u - clipping\n",
 			cpu_count, nr_cpu_ids);
 
 	if (!bootcpu_valid) {

commit a88ce63b642cf8cd82cbc278429ccd9de4455a07
Author: Hoeun Ryu <hoeun.ryu@gmail.com>
Date:   Thu Aug 17 11:24:27 2017 +0900

    arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
    
     Commit 0ee5941 : (x86/panic: replace smp_send_stop() with kdump friendly
    version in panic path) introduced crash_smp_send_stop() which is a weak
    function and can be overridden by architecture codes to fix the side effect
    caused by commit f06e515 : (kernel/panic.c: add "crash_kexec_post_
    notifiers" option).
    
     ARM64 architecture uses the weak version function and the problem is that
    the weak function simply calls smp_send_stop() which makes other CPUs
    offline and takes away the chance to save crash information for nonpanic
    CPUs in machine_crash_shutdown() when crash_kexec_post_notifiers kernel
    option is enabled.
    
     Calling smp_send_crash_stop() in machine_crash_shutdown() is useless
    because all nonpanic CPUs are already offline by smp_send_stop() in this
    case and smp_send_crash_stop() only works against online CPUs.
    
     The result is that secondary CPUs registers are not saved by
    crash_save_cpu() and the vmcore file misreports these CPUs as being
    offline.
    
     crash_smp_send_stop() is implemented to fix this problem by replacing the
    existing smp_send_crash_stop() and adding a check for multiple calling to
    the function. The function (strong symbol version) saves crash information
    for nonpanic CPUs and machine_crash_shutdown() tries to save crash
    information for nonpanic CPUs only when crash_kexec_post_notifiers kernel
    option is disabled.
    
    * crash_kexec_post_notifiers : false
    
      panic()
        __crash_kexec()
          machine_crash_shutdown()
            crash_smp_send_stop()    <= save crash dump for nonpanic cores
    
    * crash_kexec_post_notifiers : true
    
      panic()
        crash_smp_send_stop()        <= save crash dump for nonpanic cores
        __crash_kexec()
          machine_crash_shutdown()
            crash_smp_send_stop()    <= just return.
    
    Signed-off-by: Hoeun Ryu <hoeun.ryu@gmail.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index f13ddb2404f9..ffe089942ac4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -977,11 +977,21 @@ void smp_send_stop(void)
 }
 
 #ifdef CONFIG_KEXEC_CORE
-void smp_send_crash_stop(void)
+void crash_smp_send_stop(void)
 {
+	static int cpus_stopped;
 	cpumask_t mask;
 	unsigned long timeout;
 
+	/*
+	 * This function can be called twice in panic path, but obviously
+	 * we execute this only once.
+	 */
+	if (cpus_stopped)
+		return;
+
+	cpus_stopped = 1;
+
 	if (num_online_cpus() == 1)
 		return;
 

commit 34be98f4944f99076f049a6806fc5f5207a755d3
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Jul 20 17:15:45 2017 +0100

    arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
    
    For historical reasons, we leave the top 16 bytes of our task and IRQ
    stacks unused, a practice used to ensure that the SP can always be
    masked to find the base of the current stack (historically, where
    thread_info could be found).
    
    However, this is not necessary, as:
    
    * When an exception is taken from a task stack, we decrement the SP by
      S_FRAME_SIZE and stash the exception registers before we compare the
      SP against the task stack. In such cases, the SP must be at least
      S_FRAME_SIZE below the limit, and can be safely masked to determine
      whether the task stack is in use.
    
    * When transitioning to an IRQ stack, we'll place a dummy frame onto the
      IRQ stack before enabling asynchronous exceptions, or executing code
      we expect to trigger faults. Thus, if an exception is taken from the
      IRQ stack, the SP must be at least 16 bytes below the limit.
    
    * We no longer mask the SP to find the thread_info, which is now found
      via sp_el0. Note that historically, the offset was critical to ensure
      that cpu_switch_to() found the correct stack for new threads that
      hadn't yet executed ret_from_fork().
    
    Given that, this initial offset serves no purpose, and can be removed.
    This brings us in-line with other architectures (e.g. x86) which do not
    rely on this masking.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [Mark: rebase, kill THREAD_START_SP, commit msg additions]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index dc66e6ec3a99..f13ddb2404f9 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -154,7 +154,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	 * page tables.
 	 */
 	secondary_data.task = idle;
-	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
+	secondary_data.stack = task_stack_page(idle) + THREAD_SIZE;
 	update_cpu_boot_status(CPU_MMU_OFF);
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 

commit a270f32735a20affe325c351c359f13603537d05
Author: Rob Herring <robh@kernel.org>
Date:   Tue Jul 18 16:42:42 2017 -0500

    arm64: Convert to using %pOF instead of full_name
    
    Now that we have a custom printf format specifier, convert users of
    full_name to use %pOF instead. This is preparation to remove storing
    of the full path string for each node.
    
    Signed-off-by: Rob Herring <robh@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 321119881abf..dc66e6ec3a99 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -469,7 +469,7 @@ static u64 __init of_get_cpu_mpidr(struct device_node *dn)
 	 */
 	cell = of_get_property(dn, "reg", NULL);
 	if (!cell) {
-		pr_err("%s: missing reg property\n", dn->full_name);
+		pr_err("%pOF: missing reg property\n", dn);
 		return INVALID_HWID;
 	}
 
@@ -478,7 +478,7 @@ static u64 __init of_get_cpu_mpidr(struct device_node *dn)
 	 * Non affinity bits must be set to 0 in the DT
 	 */
 	if (hwid & ~MPIDR_HWID_BITMASK) {
-		pr_err("%s: invalid reg property\n", dn->full_name);
+		pr_err("%pOF: invalid reg property\n", dn);
 		return INVALID_HWID;
 	}
 	return hwid;
@@ -627,8 +627,8 @@ static void __init of_parse_and_init_cpus(void)
 			goto next;
 
 		if (is_mpidr_duplicate(cpu_count, hwid)) {
-			pr_err("%s: duplicate cpu reg properties in the DT\n",
-				dn->full_name);
+			pr_err("%pOF: duplicate cpu reg properties in the DT\n",
+				dn);
 			goto next;
 		}
 
@@ -640,8 +640,8 @@ static void __init of_parse_and_init_cpus(void)
 		 */
 		if (hwid == cpu_logical_map(0)) {
 			if (bootcpu_valid) {
-				pr_err("%s: duplicate boot cpu reg property in DT\n",
-					dn->full_name);
+				pr_err("%pOF: duplicate boot cpu reg property in DT\n",
+					dn);
 				goto next;
 			}
 

commit ef284f5ca5f102bf855e599305c0c16d6e844635
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 16 20:42:34 2017 +0200

    arm64: Adjust system_state check
    
    To enable smp_processor_id() and might_sleep() debug checks earlier, it's
    required to add system states between SYSTEM_BOOTING and SYSTEM_RUNNING.
    
    Adjust the system_state check in smp_send_stop() to handle the extra states.
    
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: http://lkml.kernel.org/r/20170516184735.112589728@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 6e0e16a3a7d4..321119881abf 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -961,8 +961,7 @@ void smp_send_stop(void)
 		cpumask_copy(&mask, cpu_online_mask);
 		cpumask_clear_cpu(smp_processor_id(), &mask);
 
-		if (system_state == SYSTEM_BOOTING ||
-		    system_state == SYSTEM_RUNNING)
+		if (system_state <= SYSTEM_RUNNING)
 			pr_crit("SMP: stopping secondary CPUs\n");
 		smp_cross_call(&mask, IPI_CPU_STOP);
 	}

commit ab182e67ec99ea0c8d7435a32a4a1ed9bb02559a
Merge: 7246f6006884 92f66f84d969
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 5 12:11:37 2017 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - kdump support, including two necessary memblock additions:
       memblock_clear_nomap() and memblock_cap_memory_range()
    
     - ARMv8.3 HWCAP bits for JavaScript conversion instructions, complex
       numbers and weaker release consistency
    
     - arm64 ACPI platform MSI support
    
     - arm perf updates: ACPI PMU support, L3 cache PMU in some Qualcomm
       SoCs, Cortex-A53 L2 cache events and DTLB refills, MAINTAINERS update
       for DT perf bindings
    
     - architected timer errata framework (the arch/arm64 changes only)
    
     - support for DMA_ATTR_FORCE_CONTIGUOUS in the arm64 iommu DMA API
    
     - arm64 KVM refactoring to use common system register definitions
    
     - remove support for ASID-tagged VIVT I-cache (no ARMv8 implementation
       using it and deprecated in the architecture) together with some
       I-cache handling clean-up
    
     - PE/COFF EFI header clean-up/hardening
    
     - define BUG() instruction without CONFIG_BUG
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      arm64: Fix the DMA mmap and get_sgtable API with DMA_ATTR_FORCE_CONTIGUOUS
      arm64: Print DT machine model in setup_machine_fdt()
      arm64: pmu: Wire-up Cortex A53 L2 cache events and DTLB refills
      arm64: module: split core and init PLT sections
      arm64: pmuv3: handle pmuv3+
      arm64: Add CNTFRQ_EL0 trap handler
      arm64: Silence spurious kbuild warning on menuconfig
      arm64: pmuv3: use arm_pmu ACPI framework
      arm64: pmuv3: handle !PMUv3 when probing
      drivers/perf: arm_pmu: add ACPI framework
      arm64: add function to get a cpu's MADT GICC table
      drivers/perf: arm_pmu: split out platform device probe logic
      drivers/perf: arm_pmu: move irq request/free into probe
      drivers/perf: arm_pmu: split cpu-local irq request/free
      drivers/perf: arm_pmu: rename irq request/free functions
      drivers/perf: arm_pmu: handle no platform_device
      drivers/perf: arm_pmu: simplify cpu_pmu_request_irqs()
      drivers/perf: arm_pmu: factor out pmu registration
      drivers/perf: arm_pmu: fold init into alloc
      drivers/perf: arm_pmu: define armpmu_init_fn
      ...

commit 494bc3cd3dd02e259d5db9372754e993e4a21902
Merge: d91750f12c79 f00fa5f4163b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 12 10:41:13 2017 +0100

    Merge branch 'will/for-next/perf' into for-next/core
    
    * will/for-next/perf:
      arm64: pmuv3: use arm_pmu ACPI framework
      arm64: pmuv3: handle !PMUv3 when probing
      drivers/perf: arm_pmu: add ACPI framework
      arm64: add function to get a cpu's MADT GICC table
      drivers/perf: arm_pmu: split out platform device probe logic
      drivers/perf: arm_pmu: move irq request/free into probe
      drivers/perf: arm_pmu: split cpu-local irq request/free
      drivers/perf: arm_pmu: rename irq request/free functions
      drivers/perf: arm_pmu: handle no platform_device
      drivers/perf: arm_pmu: simplify cpu_pmu_request_irqs()
      drivers/perf: arm_pmu: factor out pmu registration
      drivers/perf: arm_pmu: fold init into alloc
      drivers/perf: arm_pmu: define armpmu_init_fn
      drivers/perf: arm_pmu: remove pointless PMU disabling
      perf: qcom: Add L3 cache PMU driver
      drivers/perf: arm_pmu: split irq request from enable
      drivers/perf: arm_pmu: manage interrupts per-cpu
      drivers/perf: arm_pmu: rework per-cpu allocation
      MAINTAINERS: Add file patterns for perf device tree bindings

commit e0013aed489e7ebbba59d7ada2ff5551ac4b61c6
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 11 09:39:54 2017 +0100

    arm64: add function to get a cpu's MADT GICC table
    
    Currently the ACPI parking protocol code needs to parse each CPU's MADT
    GICC table to extract the mailbox address and so on. Each time we parse
    a GICC table, we call back to the parking protocol code to parse it.
    
    This has been fine so far, but we're about to have more code that needs
    to extract data from the GICC tables, and adding a callback for each
    user is going to get unwieldy.
    
    Instead, this patch ensures that we stash a copy of each CPU's GICC
    table at boot time, such that anything needing to parse it can later
    request it. This will allow for other parsers of GICC, and for
    simplification to the ACPI parking protocol code. Note that we must
    store a copy, rather than a pointer, since the core ACPI code
    temporarily maps/unmaps tables while iterating over them.
    
    Since we parse the MADT before we know how many CPUs we have (and hence
    before we setup the percpu areas), we must use an NR_CPUS sized array.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index ef1caae02110..390c277a98e2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -518,6 +518,13 @@ static bool bootcpu_valid __initdata;
 static unsigned int cpu_count = 1;
 
 #ifdef CONFIG_ACPI
+static struct acpi_madt_generic_interrupt cpu_madt_gicc[NR_CPUS];
+
+struct acpi_madt_generic_interrupt *acpi_cpu_get_madt_gicc(int cpu)
+{
+	return &cpu_madt_gicc[cpu];
+}
+
 /*
  * acpi_map_gic_cpu_interface - parse processor MADT entry
  *
@@ -552,6 +559,7 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 			return;
 		}
 		bootcpu_valid = true;
+		cpu_madt_gicc[0] = *processor;
 		early_map_cpu_to_node(0, acpi_numa_get_nid(0, hwid));
 		return;
 	}
@@ -562,6 +570,8 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 	/* map the logical cpu id to cpu MPIDR */
 	cpu_logical_map(cpu_count) = hwid;
 
+	cpu_madt_gicc[cpu_count] = *processor;
+
 	/*
 	 * Set-up the ACPI parking protocol cpu entries
 	 * while initializing the cpu_logical_map to

commit 78fd584cdec0518075cf3aa75e5ec491cc8f3ff3
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:36 2017 +0900

    arm64: kdump: implement machine_crash_shutdown()
    
    Primary kernel calls machine_crash_shutdown() to shut down non-boot cpus
    and save registers' status in per-cpu ELF notes before starting crash
    dump kernel. See kernel_kexec().
    Even if not all secondary cpus have shut down, we do kdump anyway.
    
    As we don't have to make non-boot(crashed) cpus offline (to preserve
    correct status of cpus at crash dump) before shutting down, this patch
    also adds a variant of smp_send_stop().
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d4739552da28..ffee4e454ac5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -39,6 +39,7 @@
 #include <linux/completion.h>
 #include <linux/of.h>
 #include <linux/irq_work.h>
+#include <linux/kexec.h>
 
 #include <asm/alternative.h>
 #include <asm/atomic.h>
@@ -76,6 +77,7 @@ enum ipi_msg_type {
 	IPI_RESCHEDULE,
 	IPI_CALL_FUNC,
 	IPI_CPU_STOP,
+	IPI_CPU_CRASH_STOP,
 	IPI_TIMER,
 	IPI_IRQ_WORK,
 	IPI_WAKEUP
@@ -756,6 +758,7 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
 	S(IPI_RESCHEDULE, "Rescheduling interrupts"),
 	S(IPI_CALL_FUNC, "Function call interrupts"),
 	S(IPI_CPU_STOP, "CPU stop interrupts"),
+	S(IPI_CPU_CRASH_STOP, "CPU stop (for crash dump) interrupts"),
 	S(IPI_TIMER, "Timer broadcast interrupts"),
 	S(IPI_IRQ_WORK, "IRQ work interrupts"),
 	S(IPI_WAKEUP, "CPU wake-up interrupts"),
@@ -830,6 +833,29 @@ static void ipi_cpu_stop(unsigned int cpu)
 		cpu_relax();
 }
 
+#ifdef CONFIG_KEXEC_CORE
+static atomic_t waiting_for_crash_ipi = ATOMIC_INIT(0);
+#endif
+
+static void ipi_cpu_crash_stop(unsigned int cpu, struct pt_regs *regs)
+{
+#ifdef CONFIG_KEXEC_CORE
+	crash_save_cpu(regs, cpu);
+
+	atomic_dec(&waiting_for_crash_ipi);
+
+	local_irq_disable();
+
+#ifdef CONFIG_HOTPLUG_CPU
+	if (cpu_ops[cpu]->cpu_die)
+		cpu_ops[cpu]->cpu_die(cpu);
+#endif
+
+	/* just in case */
+	cpu_park_loop();
+#endif
+}
+
 /*
  * Main handler for inter-processor interrupts
  */
@@ -860,6 +886,15 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		irq_exit();
 		break;
 
+	case IPI_CPU_CRASH_STOP:
+		if (IS_ENABLED(CONFIG_KEXEC_CORE)) {
+			irq_enter();
+			ipi_cpu_crash_stop(cpu, regs);
+
+			unreachable();
+		}
+		break;
+
 #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
 	case IPI_TIMER:
 		irq_enter();
@@ -932,6 +967,39 @@ void smp_send_stop(void)
 			   cpumask_pr_args(cpu_online_mask));
 }
 
+#ifdef CONFIG_KEXEC_CORE
+void smp_send_crash_stop(void)
+{
+	cpumask_t mask;
+	unsigned long timeout;
+
+	if (num_online_cpus() == 1)
+		return;
+
+	cpumask_copy(&mask, cpu_online_mask);
+	cpumask_clear_cpu(smp_processor_id(), &mask);
+
+	atomic_set(&waiting_for_crash_ipi, num_online_cpus() - 1);
+
+	pr_crit("SMP: stopping secondary CPUs\n");
+	smp_cross_call(&mask, IPI_CPU_CRASH_STOP);
+
+	/* Wait up to one second for other CPUs to stop */
+	timeout = USEC_PER_SEC;
+	while ((atomic_read(&waiting_for_crash_ipi) > 0) && timeout--)
+		udelay(1);
+
+	if (atomic_read(&waiting_for_crash_ipi) > 0)
+		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
+			   cpumask_pr_args(&mask));
+}
+
+bool smp_crash_stop_failed(void)
+{
+	return (atomic_read(&waiting_for_crash_ipi) > 0);
+}
+#endif
+
 /*
  * not supported here
  */

commit 335d2c2d192266358c5dfa64953a4c162f46e464
Author: Mark Salter <msalter@redhat.com>
Date:   Fri Mar 24 09:53:56 2017 -0400

    arm64: fix NULL dereference in have_cpu_die()
    
    Commit 5c492c3f5255 ("arm64: smp: Add function to determine if cpus are
    stuck in the kernel") added a helper function to determine if die() is
    supported in cpu_ops. This function assumes a cpu will have a valid
    cpu_ops entry, but that may not be the case for cpu0 is spin-table or
    parking protocol is used to boot secondary cpus. In that case, there
    is a NULL dereference if have_cpu_die() is called by cpu0. So add a
    check for a valid cpu_ops before dereferencing it.
    
    Fixes: 5c492c3f5255 ("arm64: smp: Add function to determine if cpus are stuck in the kernel")
    Signed-off-by: Mark Salter <msalter@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index ef1caae02110..9b1036570586 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -944,7 +944,7 @@ static bool have_cpu_die(void)
 #ifdef CONFIG_HOTPLUG_CPU
 	int any_cpu = raw_smp_processor_id();
 
-	if (cpu_ops[any_cpu]->cpu_die)
+	if (cpu_ops[any_cpu] && cpu_ops[any_cpu]->cpu_die)
 		return true;
 #endif
 	return false;

commit 5ea5306c3235a157f06040c59730b1133115ed26
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Mar 9 21:52:01 2017 +0100

    arm64: alternatives: apply boot time fixups via the linear mapping
    
    One important rule of thumb when desiging a secure software system is
    that memory should never be writable and executable at the same time.
    We mostly adhere to this rule in the kernel, except at boot time, when
    regions may be mapped RWX until after we are done applying alternatives
    or making other one-off changes.
    
    For the alternative patching, we can improve the situation by applying
    the fixups via the linear mapping, which is never mapped with executable
    permissions. So map the linear alias of .text with RW- permissions
    initially, and remove the write permissions as soon as alternative
    patching has completed.
    
    Reviewed-by: Laura Abbott <labbott@redhat.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index ef1caae02110..d4739552da28 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -434,6 +434,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 	setup_cpu_features();
 	hyp_mode_check();
 	apply_alternatives_all();
+	mark_linear_text_alias_ro();
 }
 
 void __init smp_prepare_boot_cpu(void)

commit 68e21be2916b359fd8afb536c1911dc014cfd03e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 19:08:20 2017 +0100

    sched/headers: Move task->mm handling methods to <linux/sched/mm.h>
    
    Move the following task->mm helper APIs into a new header file,
    <linux/sched/mm.h>, to further reduce the size and complexity
    of <linux/sched.h>.
    
    Here are how the APIs are used in various kernel files:
    
      # mm_alloc():
      arch/arm/mach-rpc/ecard.c
      fs/exec.c
      include/linux/sched/mm.h
      kernel/fork.c
    
      # __mmdrop():
      arch/arc/include/asm/mmu_context.h
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmdrop():
      arch/arm/mach-rpc/ecard.c
      arch/m68k/sun3/mmu_emu.c
      arch/x86/mm/tlb.c
      drivers/gpu/drm/amd/amdkfd/kfd_process.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/hw/hfi1/file_ops.c
      drivers/vfio/vfio_iommu_spapr_tce.c
      fs/exec.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/mmu_notifier.h
      include/linux/sched/mm.h
      kernel/fork.c
      kernel/futex.c
      kernel/sched/core.c
      mm/khugepaged.c
      mm/ksm.c
      mm/mmu_context.c
      mm/mmu_notifier.c
      mm/oom_kill.c
      virt/kvm/kvm_main.c
    
      # mmdrop_async_fn():
      include/linux/sched/mm.h
    
      # mmdrop_async():
      include/linux/sched/mm.h
      kernel/fork.c
    
      # mmget_not_zero():
      fs/userfaultfd.c
      include/linux/sched/mm.h
      mm/oom_kill.c
    
      # mmput():
      arch/arc/include/asm/mmu_context.h
      arch/arc/kernel/troubleshoot.c
      arch/frv/mm/mmu-context.c
      arch/powerpc/platforms/cell/spufs/context.c
      arch/sparc/include/asm/mmu_context_32.h
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/gpu/drm/i915/i915_gem_userptr.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/core/uverbs_main.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/exec.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      fs/proc/task_nommu.c
      fs/userfaultfd.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/events/uprobes.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/oom_kill.c
      mm/process_vm_access.c
      mm/rmap.c
      mm/swapfile.c
      mm/util.c
      virt/kvm/async_pf.c
    
      # mmput_async():
      include/linux/sched/mm.h
      kernel/fork.c
      mm/oom_kill.c
    
      # get_task_mm():
      arch/arc/kernel/troubleshoot.c
      arch/powerpc/platforms/cell/spufs/context.c
      drivers/android/binder.c
      drivers/gpu/drm/etnaviv/etnaviv_gem.c
      drivers/infiniband/core/umem.c
      drivers/infiniband/core/umem_odp.c
      drivers/infiniband/hw/mlx4/main.c
      drivers/infiniband/hw/mlx5/main.c
      drivers/infiniband/hw/usnic/usnic_uiom.c
      drivers/iommu/amd_iommu_v2.c
      drivers/iommu/intel-svm.c
      drivers/lguest/lguest_user.c
      drivers/misc/cxl/fault.c
      drivers/misc/mic/scif/scif_rma.c
      drivers/oprofile/buffer_sync.c
      drivers/vfio/vfio_iommu_type1.c
      drivers/vhost/vhost.c
      drivers/xen/gntdev.c
      fs/proc/array.c
      fs/proc/base.c
      fs/proc/task_mmu.c
      include/linux/sched/mm.h
      kernel/cpuset.c
      kernel/events/core.c
      kernel/exit.c
      kernel/fork.c
      kernel/ptrace.c
      kernel/sys.c
      kernel/trace/trace_output.c
      kernel/tsacct.c
      mm/memcontrol.c
      mm/memory.c
      mm/mempolicy.c
      mm/migrate.c
      mm/mmu_notifier.c
      mm/nommu.c
      mm/util.c
    
      # mm_access():
      fs/proc/base.c
      include/linux/sched/mm.h
      kernel/fork.c
      mm/process_vm_access.c
    
      # mm_release():
      arch/arc/include/asm/mmu_context.h
      fs/exec.c
      include/linux/sched/mm.h
      include/uapi/linux/sched.h
      kernel/exit.c
      kernel/fork.c
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 83c0a839a6ad..ef1caae02110 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -21,7 +21,7 @@
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/spinlock.h>
-#include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/sched/hotplug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/interrupt.h>

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index f66e58523b96..83c0a839a6ad 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -23,6 +23,7 @@
 #include <linux/spinlock.h>
 #include <linux/sched.h>
 #include <linux/sched/hotplug.h>
+#include <linux/sched/task_stack.h>
 #include <linux/interrupt.h>
 #include <linux/cache.h>
 #include <linux/profile.h>

commit ef8bd77f332bb0a4e467d7171bbfc6c57aa08a88
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/hotplug.h>
    
    We are going to split <linux/sched/hotplug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/hotplug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 827d52d78b67..f66e58523b96 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -22,6 +22,7 @@
 #include <linux/init.h>
 #include <linux/spinlock.h>
 #include <linux/sched.h>
+#include <linux/sched/hotplug.h>
 #include <linux/interrupt.h>
 #include <linux/cache.h>
 #include <linux/profile.h>

commit f1f1007644ffc8051a4c11427d58b1967ae7b75a
Author: Vegard Nossum <vegard.nossum@oracle.com>
Date:   Mon Feb 27 14:30:07 2017 -0800

    mm: add new mmgrab() helper
    
    Apart from adding the helper function itself, the rest of the kernel is
    converted mechanically using:
    
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)->mm_count);/mmgrab\(\1\);/'
      git grep -l 'atomic_inc.*mm_count' | xargs sed -i 's/atomic_inc(&\(.*\)\.mm_count);/mmgrab\(\&\1\);/'
    
    This is needed for a later patch that hooks into the helper, but might
    be a worthwhile cleanup on its own.
    
    (Michal Hocko provided most of the kerneldoc comment.)
    
    Link: http://lkml.kernel.org/r/20161218123229.22952-1-vegard.nossum@oracle.com
    Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a8ec5da530af..827d52d78b67 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -222,7 +222,7 @@ asmlinkage void secondary_start_kernel(void)
 	 * All kernel threads share the same mm context; grab a
 	 * reference and switch to it.
 	 */
-	atomic_inc(&mm->mm_count);
+	mmgrab(mm);
 	current->active_mm = mm;
 
 	/*

commit 3d29a9a0f88300b7ccb642ebee61b331ef0a8c27
Author: Dmitry Torokhov <dmitry.torokhov@gmail.com>
Date:   Wed Feb 1 15:01:05 2017 -0800

    arm64: make use of for_each_node_by_type()
    
    Instead of open-coding the loop, let's use canned macro.
    
    Signed-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index cb87234cfcf2..a8ec5da530af 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -603,9 +603,9 @@ acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
  */
 static void __init of_parse_and_init_cpus(void)
 {
-	struct device_node *dn = NULL;
+	struct device_node *dn;
 
-	while ((dn = of_find_node_by_type(dn, "cpu"))) {
+	for_each_node_by_type(dn, "cpu") {
 		u64 hwid = of_get_cpu_mpidr(dn);
 
 		if (hwid == INVALID_HWID)

commit c02433dd6de32f042cf3ffe476746b1115b8c096
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:13 2016 +0000

    arm64: split thread_info from task stack
    
    This patch moves arm64's struct thread_info from the task stack into
    task_struct. This protects thread_info from corruption in the case of
    stack overflows, and makes its address harder to determine if stack
    addresses are leaked, making a number of attacks more difficult. Precise
    detection and handling of overflow is left for subsequent patches.
    
    Largely, this involves changing code to store the task_struct in sp_el0,
    and acquire the thread_info from the task struct. Core code now
    implements current_thread_info(), and as noted in <linux/sched.h> this
    relies on offsetof(task_struct, thread_info) == 0, enforced by core
    code.
    
    This change means that the 'tsk' register used in entry.S now points to
    a task_struct, rather than a thread_info as it used to. To make this
    clear, the TI_* field offsets are renamed to TSK_TI_*, with asm-offsets
    appropriately updated to account for the structural change.
    
    Userspace clobbers sp_el0, and we can no longer restore this from the
    stack. Instead, the current task is cached in a per-cpu variable that we
    can safely access from early assembly as interrupts are disabled (and we
    are thus not preemptible).
    
    Both secondary entry and idle are updated to stash the sp and task
    pointer separately.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 6f42c68e457f..cb87234cfcf2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -149,6 +149,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	 * We need to tell the secondary core where to find its stack and the
 	 * page tables.
 	 */
+	secondary_data.task = idle;
 	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
 	update_cpu_boot_status(CPU_MMU_OFF);
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
@@ -173,6 +174,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 		pr_err("CPU%u: failed to boot: %d\n", cpu, ret);
 	}
 
+	secondary_data.task = NULL;
 	secondary_data.stack = NULL;
 	status = READ_ONCE(secondary_data.status);
 	if (ret && status) {

commit 57c82954e77fa12c1023e87210d2ede77aaa0058
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:11 2016 +0000

    arm64: make cpu number a percpu variable
    
    In the absence of CONFIG_THREAD_INFO_IN_TASK, core code maintains
    thread_info::cpu, and low-level architecture code can access this to
    build raw_smp_processor_id(). With CONFIG_THREAD_INFO_IN_TASK, core code
    maintains task_struct::cpu, which for reasons of hte header soup is not
    accessible to low-level arch code.
    
    Instead, we can maintain a percpu variable containing the cpu number.
    
    For both the old and new implementation of raw_smp_processor_id(), we
    read a syreg into a GPR, add an offset, and load the result. As the
    offset is now larger, it may not be folded into the load, but otherwise
    the assembly shouldn't change much.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index f64401faa091..6f42c68e457f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -58,6 +58,9 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/ipi.h>
 
+DEFINE_PER_CPU_READ_MOSTLY(int, cpu_number);
+EXPORT_PER_CPU_SYMBOL(cpu_number);
+
 /*
  * as from 2.5, kernels no longer have an init_tasks structure
  * so we need some other way of telling a new secondary core
@@ -719,6 +722,8 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	 */
 	for_each_possible_cpu(cpu) {
 
+		per_cpu(cpu_number, cpu) = cpu;
+
 		if (cpu == smp_processor_id())
 			continue;
 

commit 580efaa7ccfb8c0790dce4396434f0e5ac8d86ee
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:10 2016 +0000

    arm64: smp: prepare for smp_processor_id() rework
    
    Subsequent patches will make smp_processor_id() use a percpu variable.
    This will make smp_processor_id() dependent on the percpu offset, and
    thus we cannot use smp_processor_id() to figure out what to initialise
    the offset to.
    
    Prepare for this by initialising the percpu offset based on
    current::cpu, which will work regardless of how smp_processor_id() is
    implemented. Also, make this relationship obvious by placing this code
    together at the start of secondary_start_kernel().
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 8507703dabe4..f64401faa091 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -208,7 +208,10 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 asmlinkage void secondary_start_kernel(void)
 {
 	struct mm_struct *mm = &init_mm;
-	unsigned int cpu = smp_processor_id();
+	unsigned int cpu;
+
+	cpu = task_cpu(current);
+	set_my_cpu_offset(per_cpu_offset(cpu));
 
 	/*
 	 * All kernel threads share the same mm context; grab a
@@ -217,8 +220,6 @@ asmlinkage void secondary_start_kernel(void)
 	atomic_inc(&mm->mm_count);
 	current->active_mm = mm;
 
-	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
-
 	/*
 	 * TTBR0 is only used for the identity mapping at this stage. Make it
 	 * point to zero page to avoid speculatively fetching new entries.

commit baa5567c18d17843815d1d9150424d31f238e363
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Mon Oct 17 15:18:48 2016 +0100

    arm64: kernel: numa: fix ACPI boot cpu numa node mapping
    
    Commit 7ba5f605f3a0 ("arm64/numa: remove the limitation that cpu0 must
    bind to node0") removed the numa cpu<->node mapping restriction whereby
    logical cpu 0 always corresponds to numa node 0; removing the
    restriction was correct, in that it does not really exist in practice
    but the commit only updated the early mapping of logical cpu 0 to its
    real numa node for the DT boot path, missing the ACPI one, leading to
    boot failures on ACPI systems owing to missing node<->cpu map for
    logical cpu 0.
    
    Fix the issue by updating the ACPI boot path with code that carries out
    the early cpu<->node mapping also for the boot cpu (ie cpu 0), mirroring
    what is currently done in the DT boot path.
    
    Fixes: 7ba5f605f3a0 ("arm64/numa: remove the limitation that cpu0 must bind to node0")
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Laszlo Ersek <lersek@redhat.com>
    Reported-by: Laszlo Ersek <lersek@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Laszlo Ersek <lersek@redhat.com>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Andrew Jones <drjones@redhat.com>
    Cc: Zhen Lei <thunder.leizhen@huawei.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d3f151cfd4a1..8507703dabe4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -544,6 +544,7 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 			return;
 		}
 		bootcpu_valid = true;
+		early_map_cpu_to_node(0, acpi_numa_get_nid(0, hwid));
 		return;
 	}
 

commit 7af8a0f8088831428051976cb06cc1e450f8bab5
Merge: c8d2bc9bc39e db68f3e7594a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 08:58:35 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "It's a bit all over the place this time with no "killer feature" to
      speak of.  Support for mismatched cache line sizes should help people
      seeing whacky JIT failures on some SoCs, and the big.LITTLE perf
      updates have been a long time coming, but a lot of the changes here
      are cleanups.
    
      We stray outside arch/arm64 in a few areas: the arch/arm/ arch_timer
      workaround is acked by Russell, the DT/OF bits are acked by Rob, the
      arch_timer clocksource changes acked by Marc, CPU hotplug by tglx and
      jump_label by Peter (all CC'd).
    
      Summary:
    
       - Support for execute-only page permissions
       - Support for hibernate and DEBUG_PAGEALLOC
       - Support for heterogeneous systems with mismatches cache line sizes
       - Errata workarounds (A53 843419 update and QorIQ A-008585 timer bug)
       - arm64 PMU perf updates, including cpumasks for heterogeneous systems
       - Set UTS_MACHINE for building rpm packages
       - Yet another head.S tidy-up
       - Some cleanups and refactoring, particularly in the NUMA code
       - Lots of random, non-critical fixes across the board"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (100 commits)
      arm64: tlbflush.h: add __tlbi() macro
      arm64: Kconfig: remove SMP dependence for NUMA
      arm64: Kconfig: select OF/ACPI_NUMA under NUMA config
      arm64: fix dump_backtrace/unwind_frame with NULL tsk
      arm/arm64: arch_timer: Use archdata to indicate vdso suitability
      arm64: arch_timer: Work around QorIQ Erratum A-008585
      arm64: arch_timer: Add device tree binding for A-008585 erratum
      arm64: Correctly bounds check virt_addr_valid
      arm64: migrate exception table users off module.h and onto extable.h
      arm64: pmu: Hoist pmu platform device name
      arm64: pmu: Probe default hw/cache counters
      arm64: pmu: add fallback probe table
      MAINTAINERS: Update ARM PMU PROFILING AND DEBUGGING entry
      arm64: Improve kprobes test for atomic sequence
      arm64/kvm: use alternative auto-nop
      arm64: use alternative auto-nop
      arm64: alternative: add auto-nop infrastructure
      arm64: lse: convert lse alternatives NOP padding to use __nops
      arm64: barriers: introduce nops and __nops macros for NOP sequences
      arm64: sysreg: replace open-coded mrs_s/msr_s with {read,write}_sysreg_s
      ...

commit c18df0adabf8400c1825b90382d06df5edc303fa
Author: David Daney <david.daney@cavium.com>
Date:   Tue Sep 20 11:46:35 2016 -0700

    arm64: Call numa_store_cpu_info() earlier.
    
    The wq_numa_init() function makes a private CPU to node map by calling
    cpu_to_node() early in the boot process, before the non-boot CPUs are
    brought online.  Since the default implementation of cpu_to_node()
    returns zero for CPUs that have never been brought online, the
    workqueue system's view is that *all* CPUs are on node zero.
    
    When the unbound workqueue for a non-zero node is created, the
    tsk_cpus_allowed() for the worker threads is the empty set because
    there are, in the view of the workqueue system, no CPUs on non-zero
    nodes.  The code in try_to_wake_up() using this empty cpumask ends up
    using the cpumask empty set value of NR_CPUS as an index into the
    per-CPU area pointer array, and gets garbage as it is one past the end
    of the array.  This results in:
    
    [    0.881970] Unable to handle kernel paging request at virtual address fffffb1008b926a4
    [    1.970095] pgd = fffffc00094b0000
    [    1.973530] [fffffb1008b926a4] *pgd=0000000000000000, *pud=0000000000000000, *pmd=0000000000000000
    [    1.982610] Internal error: Oops: 96000004 [#1] SMP
    [    1.987541] Modules linked in:
    [    1.990631] CPU: 48 PID: 295 Comm: cpuhp/48 Tainted: G        W       4.8.0-rc6-preempt-vol+ #9
    [    1.999435] Hardware name: Cavium ThunderX CN88XX board (DT)
    [    2.005159] task: fffffe0fe89cc300 task.stack: fffffe0fe8b8c000
    [    2.011158] PC is at try_to_wake_up+0x194/0x34c
    [    2.015737] LR is at try_to_wake_up+0x150/0x34c
    [    2.020318] pc : [<fffffc00080e7468>] lr : [<fffffc00080e7424>] pstate: 600000c5
    [    2.027803] sp : fffffe0fe8b8fb10
    [    2.031149] x29: fffffe0fe8b8fb10 x28: 0000000000000000
    [    2.036522] x27: fffffc0008c63bc8 x26: 0000000000001000
    [    2.041896] x25: fffffc0008c63c80 x24: fffffc0008bfb200
    [    2.047270] x23: 00000000000000c0 x22: 0000000000000004
    [    2.052642] x21: fffffe0fe89d25bc x20: 0000000000001000
    [    2.058014] x19: fffffe0fe89d1d00 x18: 0000000000000000
    [    2.063386] x17: 0000000000000000 x16: 0000000000000000
    [    2.068760] x15: 0000000000000018 x14: 0000000000000000
    [    2.074133] x13: 0000000000000000 x12: 0000000000000000
    [    2.079505] x11: 0000000000000000 x10: 0000000000000000
    [    2.084879] x9 : 0000000000000000 x8 : 0000000000000000
    [    2.090251] x7 : 0000000000000040 x6 : 0000000000000000
    [    2.095621] x5 : ffffffffffffffff x4 : 0000000000000000
    [    2.100991] x3 : 0000000000000000 x2 : 0000000000000000
    [    2.106364] x1 : fffffc0008be4c24 x0 : ffffff0ffffada80
    [    2.111737]
    [    2.113236] Process cpuhp/48 (pid: 295, stack limit = 0xfffffe0fe8b8c020)
    [    2.120102] Stack: (0xfffffe0fe8b8fb10 to 0xfffffe0fe8b90000)
    [    2.125914] fb00:                                   fffffe0fe8b8fb80 fffffc00080e7648
    .
    .
    .
    [    2.442859] Call trace:
    [    2.445327] Exception stack(0xfffffe0fe8b8f940 to 0xfffffe0fe8b8fa70)
    [    2.451843] f940: fffffe0fe89d1d00 0000040000000000 fffffe0fe8b8fb10 fffffc00080e7468
    [    2.459767] f960: fffffe0fe8b8f980 fffffc00080e4958 ffffff0ff91ab200 fffffc00080e4b64
    [    2.467690] f980: fffffe0fe8b8f9d0 fffffc00080e515c fffffe0fe8b8fa80 0000000000000000
    [    2.475614] f9a0: fffffe0fe8b8f9d0 fffffc00080e58e4 fffffe0fe8b8fa80 0000000000000000
    [    2.483540] f9c0: fffffe0fe8d10000 0000000000000040 fffffe0fe8b8fa50 fffffc00080e5ac4
    [    2.491465] f9e0: ffffff0ffffada80 fffffc0008be4c24 0000000000000000 0000000000000000
    [    2.499387] fa00: 0000000000000000 ffffffffffffffff 0000000000000000 0000000000000040
    [    2.507309] fa20: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [    2.515233] fa40: 0000000000000000 0000000000000000 0000000000000000 0000000000000018
    [    2.523156] fa60: 0000000000000000 0000000000000000
    [    2.528089] [<fffffc00080e7468>] try_to_wake_up+0x194/0x34c
    [    2.533723] [<fffffc00080e7648>] wake_up_process+0x28/0x34
    [    2.539275] [<fffffc00080d3764>] create_worker+0x110/0x19c
    [    2.544824] [<fffffc00080d69dc>] alloc_unbound_pwq+0x3cc/0x4b0
    [    2.550724] [<fffffc00080d6bcc>] wq_update_unbound_numa+0x10c/0x1e4
    [    2.557066] [<fffffc00080d7d78>] workqueue_online_cpu+0x220/0x28c
    [    2.563234] [<fffffc00080bd288>] cpuhp_invoke_callback+0x6c/0x168
    [    2.569398] [<fffffc00080bdf74>] cpuhp_up_callbacks+0x44/0xe4
    [    2.575210] [<fffffc00080be194>] cpuhp_thread_fun+0x13c/0x148
    [    2.581027] [<fffffc00080dfbac>] smpboot_thread_fn+0x19c/0x1a8
    [    2.586929] [<fffffc00080dbd64>] kthread+0xdc/0xf0
    [    2.591776] [<fffffc0008083380>] ret_from_fork+0x10/0x50
    [    2.597147] Code: b00057e1 91304021 91005021 b8626822 (b8606821)
    [    2.603464] ---[ end trace 58c0cd36b88802bc ]---
    [    2.608138] Kernel panic - not syncing: Fatal exception
    
    Fix by moving call to numa_store_cpu_info() for all CPUs into
    smp_prepare_cpus(), which happens before wq_numa_init().  Since
    smp_store_cpu_info() now contains only a single function call,
    simplify by removing the function and out-lining its contents.
    
    Suggested-by: Robert Richter <rric@kernel.org>
    Fixes: 1a2db300348b ("arm64, numa: Add NUMA support for arm64 platforms.")
    Cc: <stable@vger.kernel.org> # 4.7.x-
    Signed-off-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Robert Richter <rrichter@cavium.com>
    Tested-by: Yisheng Xie <xieyisheng1@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d93d43352504..3ff173e92582 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -201,12 +201,6 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	return ret;
 }
 
-static void smp_store_cpu_info(unsigned int cpuid)
-{
-	store_cpu_topology(cpuid);
-	numa_store_cpu_info(cpuid);
-}
-
 /*
  * This is the secondary CPU boot entry.  We're using this CPUs
  * idle thread stack, but a set of temporary page tables.
@@ -254,7 +248,7 @@ asmlinkage void secondary_start_kernel(void)
 	 */
 	notify_cpu_starting(cpu);
 
-	smp_store_cpu_info(cpu);
+	store_cpu_topology(cpu);
 
 	/*
 	 * OK, now it's safe to let the boot CPU continue.  Wait for
@@ -689,10 +683,13 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	int err;
 	unsigned int cpu;
+	unsigned int this_cpu;
 
 	init_cpu_topology();
 
-	smp_store_cpu_info(smp_processor_id());
+	this_cpu = smp_processor_id();
+	store_cpu_topology(this_cpu);
+	numa_store_cpu_info(this_cpu);
 
 	/*
 	 * If UP is mandated by "nosmp" (which implies "maxcpus=0"), don't set
@@ -719,6 +716,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 			continue;
 
 		set_cpu_present(cpu, true);
+		numa_store_cpu_info(cpu);
 	}
 }
 

commit c47a1900ad710fd2c97127e2ba19da1df79cf733
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:10 2016 +0100

    arm64: Rearrange CPU errata workaround checks
    
    Right now we run through the work around checks on a CPU
    from __cpuinfo_store_cpu. There are some problems with that:
    
    1) We initialise the system wide CPU feature registers only after the
    Boot CPU updates its cpuinfo. Now, if a work around depends on the
    variance of a CPU ID feature (e.g, check for Cache Line size mismatch),
    we have no way of performing it cleanly for the boot CPU.
    
    2) It is out of place, invoked from __cpuinfo_store_cpu() in cpuinfo.c. It
    is not an obvious place for that.
    
    This patch rearranges the CPU specific capability(aka work around) checks.
    
    1) At the moment we use verify_local_cpu_capabilities() to check if a new
    CPU has all the system advertised features. Use this for the secondary CPUs
    to perform the work around check. For that we rename
      verify_local_cpu_capabilities() => check_local_cpu_capabilities()
    which:
    
       If the system wide capabilities haven't been initialised (i.e, the CPU
       is activated at the boot), update the system wide detected work arounds.
    
       Otherwise (i.e a CPU hotplugged in later) verify that this CPU conforms to the
       system wide capabilities.
    
    2) Boot CPU updates the work arounds from smp_prepare_boot_cpu() after we have
    initialised the system wide CPU feature values.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 8b048e6ec34a..a8e64095cd69 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -239,7 +239,7 @@ asmlinkage void secondary_start_kernel(void)
 	 * this CPU ticks all of those. If it doesn't, the CPU will
 	 * fail to come online.
 	 */
-	verify_local_cpu_capabilities();
+	check_local_cpu_capabilities();
 
 	if (cpu_ops[cpu]->cpu_postboot)
 		cpu_ops[cpu]->cpu_postboot();
@@ -444,6 +444,12 @@ void __init smp_prepare_boot_cpu(void)
 	jump_label_init();
 	cpuinfo_store_boot_cpu();
 	save_boot_cpu_run_el();
+	/*
+	 * Run the errata work around checks on the boot CPU, once we have
+	 * initialised the cpu feature infrastructure from
+	 * cpuinfo_store_boot_cpu() above.
+	 */
+	update_cpu_errata_workarounds();
 }
 
 static u64 __init of_get_cpu_mpidr(struct device_node *dn)

commit 7ba5f605f3a0d9495aad539eeb8346d726dfc183
Author: Zhen Lei <thunder.leizhen@huawei.com>
Date:   Thu Sep 1 14:55:04 2016 +0800

    arm64/numa: remove the limitation that cpu0 must bind to node0
    
    1. Remove the old binding code.
    2. Read the nid of cpu0 from dts.
    3. Fallback the nid of cpu0 to 0 when numa=off is set in bootargs.
    
    Signed-off-by: Zhen Lei <thunder.leizhen@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index c3c08368a685..8b048e6ec34a 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -624,6 +624,7 @@ static void __init of_parse_and_init_cpus(void)
 			}
 
 			bootcpu_valid = true;
+			early_map_cpu_to_node(0, of_node_to_nid(dn));
 
 			/*
 			 * cpu_logical_map has already been

commit efd9e03facd075f5b76bf82e6c785bd45d5cbf4f
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 5 18:25:48 2016 +0100

    arm64: Use static keys for CPU features
    
    This patch adds static keys transparently for all the cpu_hwcaps
    features by implementing an array of default-false static keys and
    enabling them when detected. The cpus_have_cap() check uses the static
    keys if the feature being checked is a constant, otherwise the compiler
    generates the bitmap test.
    
    Because of the early call to static_branch_enable() via
    check_local_cpu_errata() -> update_cpu_capabilities(), the jump labels
    are initialised in cpuinfo_store_boot_cpu().
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Suzuki K. Poulose <Suzuki.Poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d93d43352504..c3c08368a685 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -437,6 +437,11 @@ void __init smp_cpus_done(unsigned int max_cpus)
 void __init smp_prepare_boot_cpu(void)
 {
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
+	/*
+	 * Initialise the static keys early as they may be enabled by the
+	 * cpufeature code.
+	 */
+	jump_label_init();
 	cpuinfo_store_boot_cpu();
 	save_boot_cpu_run_el();
 }

commit 50ee91bdef41c15b671dcd9446ee007a1d2f5ab7
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Tue Aug 9 10:30:49 2016 +0800

    arm64: Support hard limit of cpu count by nr_cpus
    
    Enable the hard limit of cpu count by set boot options nr_cpus=x
    on arm64, and make a minor change about message when total number
    of cpu exceeds the limit.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Reported-by: Shiyuan Hu <hushiyuan@huawei.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 76a6d9263908..d93d43352504 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -661,9 +661,9 @@ void __init smp_init_cpus(void)
 		acpi_table_parse_madt(ACPI_MADT_TYPE_GENERIC_INTERRUPT,
 				      acpi_parse_gic_cpu_interface, 0);
 
-	if (cpu_count > NR_CPUS)
-		pr_warn("no. of cores (%d) greater than configured maximum of %d - clipping\n",
-			cpu_count, NR_CPUS);
+	if (cpu_count > nr_cpu_ids)
+		pr_warn("Number of cores (%d) exceeds configured maximum of %d - clipping\n",
+			cpu_count, nr_cpu_ids);
 
 	if (!bootcpu_valid) {
 		pr_err("missing boot CPU MPIDR, not enabling secondaries\n");
@@ -677,7 +677,7 @@ void __init smp_init_cpus(void)
 	 * with entries in cpu_logical_map while initializing the cpus.
 	 * If the cpu set-up fails, invalidate the cpu_logical_map entry.
 	 */
-	for (i = 1; i < NR_CPUS; i++) {
+	for (i = 1; i < nr_cpu_ids; i++) {
 		if (cpu_logical_map(i) != INVALID_HWID) {
 			if (smp_cpu_setup(i))
 				cpu_logical_map(i) = INVALID_HWID;

commit e831101a73fbc8339ef1d1909dad3ef64f089e70
Merge: f9abf53af4c7 fd6380b75065
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 11:16:05 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Kexec support for arm64
    
     - Kprobes support
    
     - Expose MIDR_EL1 and REVIDR_EL1 CPU identification registers to sysfs
    
     - Trapping of user space cache maintenance operations and emulation in
       the kernel (CPU errata workaround)
    
     - Clean-up of the early page tables creation (kernel linear mapping,
       EFI run-time maps) to avoid splitting larger blocks (e.g.  pmds) into
       smaller ones (e.g.  ptes)
    
     - VDSO support for CLOCK_MONOTONIC_RAW in clock_gettime()
    
     - ARCH_HAS_KCOV enabled for arm64
    
     - Optimise IP checksum helpers
    
     - SWIOTLB optimisation to only allocate/initialise the buffer if the
       available RAM is beyond the 32-bit mask
    
     - Properly handle the "nosmp" command line argument
    
     - Fix for the initialisation of the CPU debug state during early boot
    
     - vdso-offsets.h build dependency workaround
    
     - Build fix when RANDOMIZE_BASE is enabled with MODULES off
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (64 commits)
      arm64: arm: Fix-up the removal of the arm64 regs_query_register_name() prototype
      arm64: Only select ARM64_MODULE_PLTS if MODULES=y
      arm64: mm: run pgtable_page_ctor() on non-swapper translation table pages
      arm64: mm: make create_mapping_late() non-allocating
      arm64: Honor nosmp kernel command line option
      arm64: Fix incorrect per-cpu usage for boot CPU
      arm64: kprobes: Add KASAN instrumentation around stack accesses
      arm64: kprobes: Cleanup jprobe_return
      arm64: kprobes: Fix overflow when saving stack
      arm64: kprobes: WARN if attempting to step with PSTATE.D=1
      arm64: debug: remove unused local_dbg_{enable, disable} macros
      arm64: debug: remove redundant spsr manipulation
      arm64: debug: unmask PSTATE.D earlier
      arm64: localise Image objcopy flags
      arm64: ptrace: remove extra define for CPSR's E bit
      kprobes: Add arm64 case in kprobe example module
      arm64: Add kernel return probes support (kretprobes)
      arm64: Add trampoline code for kretprobes
      arm64: kprobes instruction simulation support
      arm64: Treat all entry code as non-kprobe-able
      ...

commit d85f4eb6993d8c743ac89e427479cbe9efc14683
Merge: 523d939ef98f 4bac6fa73db7
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Jul 25 13:40:39 2016 +0200

    Merge branch 'acpi-numa'
    
    * acpi-numa:
      ACPI / NUMA: Enable ACPI based NUMA on ARM64
      arm64, ACPI, NUMA: NUMA support based on SRAT and SLIT
      ACPI / processor: Add acpi_map_madt_entry()
      ACPI / NUMA: Improve SRAT error detection and add messages
      ACPI / NUMA: Move acpi_numa_memory_affinity_init() to drivers/acpi/numa.c
      ACPI / NUMA: remove unneeded acpi_numa=1
      ACPI / NUMA: move bad_srat() and srat_disabled() to drivers/acpi/numa.c
      x86 / ACPI / NUMA: cleanup acpi_numa_processor_affinity_init()
      arm64, NUMA: Cleanup NUMA disabled messages
      arm64, NUMA: rework numa_add_memblk()
      ACPI / NUMA: move acpi_numa_slit_init() to drivers/acpi/numa.c
      ACPI / NUMA: Move acpi_numa_arch_fixup() to ia64 only
      ACPI / NUMA: remove duplicate NULL check
      ACPI / NUMA: Replace ACPI_DEBUG_PRINT() with pr_debug()
      ACPI / NUMA: Use pr_fmt() instead of printk

commit e75118a7b581b19b08282c7819c1ec6f68b91b79
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Thu Jul 21 11:15:27 2016 +0100

    arm64: Honor nosmp kernel command line option
    
    Passing "nosmp" should boot the kernel with a single processor, without
    provision to enable secondary CPUs even if they are present. "nosmp" is
    implemented by setting maxcpus=0. At the moment we still mark the secondary
    CPUs present even with nosmp, which allows the userspace to bring them
    up. This patch corrects the smp_prepare_cpus() to honor the maxcpus == 0.
    
    Commit 44dbcc93ab67145 ("arm64: Fix behavior of maxcpus=N") fixed the
    behavior for maxcpus >= 1, but broke maxcpus = 0.
    
    Fixes: 44dbcc93ab67 ("arm64: Fix behavior of maxcpus=N")
    Cc: <stable@vger.kernel.org> # 4.7+
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    [catalin.marinas@arm.com: updated code comment]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 298dd745651e..490db85dec23 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -692,6 +692,13 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 
 	smp_store_cpu_info(smp_processor_id());
 
+	/*
+	 * If UP is mandated by "nosmp" (which implies "maxcpus=0"), don't set
+	 * secondary CPUs present.
+	 */
+	if (max_cpus == 0)
+		return;
+
 	/*
 	 * Initialise the present map (which describes the set of CPUs
 	 * actually populated at the present time) and release the

commit 9113c2aa05e9848cd4f1154abee17d4f265f012d
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Thu Jul 21 11:12:55 2016 +0100

    arm64: Fix incorrect per-cpu usage for boot CPU
    
    In smp_prepare_boot_cpu(), we invoke cpuinfo_store_boot_cpu to  store
    the cpuinfo in a per-cpu ptr, before initialising the per-cpu offset for
    the boot CPU. This patch reorders the sequence to make sure we initialise
    the per-cpu offset before accessing the per-cpu area.
    
    Commit 4b998ff1885eec ("arm64: Delay cpuinfo_store_boot_cpu") fixed the
    issue where we modified the per-cpu area even before the kernel initialises
    the per-cpu areas, but failed to wait until the boot cpu updated it's
    offset.
    
    Fixes: 4b998ff1885e ("arm64: Delay cpuinfo_store_boot_cpu")
    Cc: <stable@vger.kernel.org> # 4.4+
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index c0a772560ab7..298dd745651e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -436,9 +436,9 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 void __init smp_prepare_boot_cpu(void)
 {
+	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 	cpuinfo_store_boot_cpu();
 	save_boot_cpu_run_el();
-	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 
 static u64 __init of_get_cpu_mpidr(struct device_node *dn)

commit 2ce39ad15182604beb6c8fa8bed5e46b59fd1082
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jul 19 15:07:37 2016 +0100

    arm64: debug: unmask PSTATE.D earlier
    
    Clearing PSTATE.D is one of the requirements for generating a debug
    exception. The arm64 booting protocol requires that PSTATE.D is set,
    since many of the debug registers (for example, the hw_breakpoint
    registers) are UNKNOWN out of reset and could potentially generate
    spurious, fatal debug exceptions in early boot code if PSTATE.D was
    clear. Once the debug registers have been safely initialised, PSTATE.D
    is cleared, however this is currently broken for two reasons:
    
    (1) The boot CPU clears PSTATE.D in a postcore_initcall and secondary
        CPUs clear PSTATE.D in secondary_start_kernel. Since the initcall
        runs after SMP (and the scheduler) have been initialised, there is
        no guarantee that it is actually running on the boot CPU. In this
        case, the boot CPU is left with PSTATE.D set and is not capable of
        generating debug exceptions.
    
    (2) In a preemptible kernel, we may explicitly schedule on the IRQ
        return path to EL1. If an IRQ occurs with PSTATE.D set in the idle
        thread, then we may schedule the kthread_init thread, run the
        postcore_initcall to clear PSTATE.D and then context switch back
        to the idle thread before returning from the IRQ. The exception
        return path will then restore PSTATE.D from the stack, and set it
        again.
    
    This patch fixes the problem by moving the clearing of PSTATE.D earlier
    to proc.S. This has the desirable effect of clearing it in one place for
    all CPUs, long before we have to worry about the scheduler or any
    exception handling. We ensure that the previous reset of MDSCR_EL1 has
    completed before unmasking the exception, so that any spurious
    exceptions resulting from UNKNOWN debug registers are not generated.
    
    Without this patch applied, the kprobes selftests have been seen to fail
    under KVM, where we end up attempting to step the OOL instruction buffer
    with PSTATE.D set and therefore fail to complete the step.
    
    Cc: <stable@vger.kernel.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 62ff3c0622e2..c0a772560ab7 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -267,7 +267,6 @@ asmlinkage void secondary_start_kernel(void)
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 
-	local_dbg_enable();
 	local_irq_enable();
 	local_async_enable();
 

commit b69e0dc14ce3c4abbd11725ff98a885d4616f9fe
Author: James Morse <james.morse@arm.com>
Date:   Wed Jun 22 10:06:12 2016 +0100

    arm64: smp: Add function to determine if cpus are stuck in the kernel
    
    kernel/smp.c has a fancy counter that keeps track of the number of CPUs
    it marked as not-present and left in cpu_park_loop(). If there are any
    CPUs spinning in here, features like kexec or hibernate may release them
    by overwriting this memory.
    
    This problem also occurs on machines using spin-tables to release
    secondary cores.
    After commit 44dbcc93ab67 ("arm64: Fix behavior of maxcpus=N")
    we bring all known cpus into the secondary holding pen, meaning this
    memory can't be re-used by kexec or hibernate.
    
    Add a function cpus_are_stuck_in_kernel() to determine if either of these
    cases have occurred.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    [catalin.marinas@arm.com: cherry-picked from mainline for kexec dependency]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 678e0842cb3b..62ff3c0622e2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -909,3 +909,21 @@ int setup_profiling_timer(unsigned int multiplier)
 {
 	return -EINVAL;
 }
+
+static bool have_cpu_die(void)
+{
+#ifdef CONFIG_HOTPLUG_CPU
+	int any_cpu = raw_smp_processor_id();
+
+	if (cpu_ops[any_cpu]->cpu_die)
+		return true;
+#endif
+	return false;
+}
+
+bool cpus_are_stuck_in_kernel(void)
+{
+	bool smp_spin_tables = (num_possible_cpus() > 1 && !have_cpu_die());
+
+	return !!cpus_stuck_in_kernel || smp_spin_tables;
+}

commit 5c492c3f5255bd34f7ff8867515ecf98dcba2a2e
Author: James Morse <james.morse@arm.com>
Date:   Wed Jun 22 10:06:12 2016 +0100

    arm64: smp: Add function to determine if cpus are stuck in the kernel
    
    kernel/smp.c has a fancy counter that keeps track of the number of CPUs
    it marked as not-present and left in cpu_park_loop(). If there are any
    CPUs spinning in here, features like kexec or hibernate may release them
    by overwriting this memory.
    
    This problem also occurs on machines using spin-tables to release
    secondary cores.
    After commit 44dbcc93ab67 ("arm64: Fix behavior of maxcpus=N")
    we bring all known cpus into the secondary holding pen, meaning this
    memory can't be re-used by kexec or hibernate.
    
    Add a function cpus_are_stuck_in_kernel() to determine if either of these
    cases have occurred.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 678e0842cb3b..62ff3c0622e2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -909,3 +909,21 @@ int setup_profiling_timer(unsigned int multiplier)
 {
 	return -EINVAL;
 }
+
+static bool have_cpu_die(void)
+{
+#ifdef CONFIG_HOTPLUG_CPU
+	int any_cpu = raw_smp_processor_id();
+
+	if (cpu_ops[any_cpu]->cpu_die)
+		return true;
+#endif
+	return false;
+}
+
+bool cpus_are_stuck_in_kernel(void)
+{
+	bool smp_spin_tables = (num_possible_cpus() > 1 && !have_cpu_die());
+
+	return !!cpus_stuck_in_kernel || smp_spin_tables;
+}

commit d8b47fca8c233642d1a20fa4025579ebc8be6f1e
Author: Hanjun Guo <hanjun.guo@linaro.org>
Date:   Tue May 24 15:35:44 2016 -0700

    arm64, ACPI, NUMA: NUMA support based on SRAT and SLIT
    
    Introduce a new file to hold ACPI based NUMA information parsing from
    SRAT and SLIT.
    
    SRAT includes the CPU ACPI ID to Proximity Domain mappings and memory
    ranges to Proximity Domain mapping.  SLIT has the information of inter
    node distances(relative number for access latency).
    
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    [rrichter@cavium.com Reworked for numa v10 series ]
    Signed-off-by: Robert Richter <rrichter@cavium.com>
    [david.daney@cavium.com reorderd and combinded with other patches in
    Hanjun Guo's original set, removed get_mpidr_in_madt() and use
    acpi_map_madt_entry() instead.]
    Signed-off-by: David Daney <david.daney@cavium.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Dennis Chen <dennis.chen@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 678e0842cb3b..d0993068c44c 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -560,6 +560,8 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 	 */
 	acpi_set_mailbox_entry(cpu_count, processor);
 
+	early_map_cpu_to_node(cpu_count, acpi_numa_get_nid(cpu_count, hwid));
+
 	cpu_count++;
 }
 

commit 99aa036241ed4a08a71b627bb903b5d7c75d78c1
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue May 10 11:14:41 2016 +0100

    arm64: secondary_start_kernel: Remove unnecessary barrier
    
    Remove the unnecessary smp_wmb(), which was added to make sure
    that the update_cpu_boot_status() completes before we mark the
    CPU online. But update_cpu_boot_status() already has dsb() (required
    for the failing CPUs) to ensure the correct behavior.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Dennis Chen <dennis.chen@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index dc9647521c59..678e0842cb3b 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -264,8 +264,6 @@ asmlinkage void secondary_start_kernel(void)
 	pr_info("CPU%u: Booted secondary processor [%08x]\n",
 					 cpu, read_cpuid_id());
 	update_cpu_boot_status(CPU_BOOT_SUCCESS);
-	/* Make sure the status update is visible before we complete */
-	smp_wmb();
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 

commit 44dbcc93ab67145bf8ebe3e91123303443c4a3bf
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Apr 22 12:25:35 2016 +0100

    arm64: Fix behavior of maxcpus=N
    
    maxcpu=n sets the number of CPUs activated at boot time to a max of n,
    but allowing the remaining CPUs to be brought up later if the user
    decides to do so. However, on arm64 due to various reasons, we disallowed
    hotplugging CPUs beyond n, by marking them not present. Now that
    we have checks in place to make sure the hotplugged CPUs have compatible
    features with system and requires no new errata, relax the restriction.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index fef2e7337fc7..dc9647521c59 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -689,33 +689,18 @@ void __init smp_init_cpus(void)
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
 	int err;
-	unsigned int cpu, ncores = num_possible_cpus();
+	unsigned int cpu;
 
 	init_cpu_topology();
 
 	smp_store_cpu_info(smp_processor_id());
 
-	/*
-	 * are we trying to boot more cores than exist?
-	 */
-	if (max_cpus > ncores)
-		max_cpus = ncores;
-
-	/* Don't bother if we're effectively UP */
-	if (max_cpus <= 1)
-		return;
-
 	/*
 	 * Initialise the present map (which describes the set of CPUs
 	 * actually populated at the present time) and release the
 	 * secondaries from the bootloader.
-	 *
-	 * Make sure we online at most (max_cpus - 1) additional CPUs.
 	 */
-	max_cpus--;
 	for_each_possible_cpu(cpu) {
-		if (max_cpus == 0)
-			break;
 
 		if (cpu == smp_processor_id())
 			continue;
@@ -728,7 +713,6 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 			continue;
 
 		set_cpu_present(cpu, true);
-		max_cpus--;
 	}
 }
 

commit 82611c14c4e479715c071fb0f44ddd72dc632037
Author: Jan Glauber <jan.glauber@caviumnetworks.com>
Date:   Mon Apr 18 09:43:33 2016 +0200

    arm64: Reduce verbosity on SMP CPU stop
    
    When CPUs are stopped during an abnormal operation like panic
    for each CPU a line is printed and the stack trace is dumped.
    
    This information is only interesting for the aborting CPU
    and on systems with many CPUs it only makes it harder to
    debug if after the aborting CPU the log is flooded with data
    about all other CPUs too.
    
    Therefore remove the stack dump and printk of other CPUs
    and only print a single line that the other CPUs are going to be
    stopped and, in case any CPUs remain online list them.
    
    Signed-off-by: Jan Glauber <jglauber@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 622bd6cd86e4..fef2e7337fc7 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -805,21 +805,11 @@ void arch_irq_work_raise(void)
 }
 #endif
 
-static DEFINE_RAW_SPINLOCK(stop_lock);
-
 /*
  * ipi_cpu_stop - handle IPI from smp_send_stop()
  */
 static void ipi_cpu_stop(unsigned int cpu)
 {
-	if (system_state == SYSTEM_BOOTING ||
-	    system_state == SYSTEM_RUNNING) {
-		raw_spin_lock(&stop_lock);
-		pr_crit("CPU%u: stopping\n", cpu);
-		dump_stack();
-		raw_spin_unlock(&stop_lock);
-	}
-
 	set_cpu_online(cpu, false);
 
 	local_irq_disable();
@@ -914,6 +904,9 @@ void smp_send_stop(void)
 		cpumask_copy(&mask, cpu_online_mask);
 		cpumask_clear_cpu(smp_processor_id(), &mask);
 
+		if (system_state == SYSTEM_BOOTING ||
+		    system_state == SYSTEM_RUNNING)
+			pr_crit("SMP: stopping secondary CPUs\n");
 		smp_cross_call(&mask, IPI_CPU_STOP);
 	}
 
@@ -923,7 +916,8 @@ void smp_send_stop(void)
 		udelay(1);
 
 	if (num_online_cpus() > 1)
-		pr_warning("SMP: failed to stop secondary CPUs\n");
+		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
+			   cpumask_pr_args(cpu_online_mask));
 }
 
 /*

commit 1a2db300348b799479d2d22b84d51b27ad0458c7
Author: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
Date:   Fri Apr 8 15:50:27 2016 -0700

    arm64, numa: Add NUMA support for arm64 platforms.
    
    Attempt to get the memory and CPU NUMA node via of_numa.  If that
    fails, default the dummy NUMA node and map all memory and CPUs to node
    0.
    
    Tested-by: Shannon Zhao <shannon.zhao@linaro.org>
    Reviewed-by: Robert Richter <rrichter@cavium.com>
    Signed-off-by: Ganapatrao Kulkarni <gkulkarni@caviumnetworks.com>
    Signed-off-by: David Daney <david.daney@cavium.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 5fbab1c40424..622bd6cd86e4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -45,6 +45,7 @@
 #include <asm/cputype.h>
 #include <asm/cpu_ops.h>
 #include <asm/mmu_context.h>
+#include <asm/numa.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/processor.h>
@@ -203,6 +204,7 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 static void smp_store_cpu_info(unsigned int cpuid)
 {
 	store_cpu_topology(cpuid);
+	numa_store_cpu_info(cpuid);
 }
 
 /*
@@ -633,6 +635,8 @@ static void __init of_parse_and_init_cpus(void)
 
 		pr_debug("cpu logical map 0x%llx\n", hwid);
 		cpu_logical_map(cpu_count) = hwid;
+
+		early_map_cpu_to_node(cpu_count, of_node_to_nid(dn));
 next:
 		cpu_count++;
 	}

commit ac1ad20f9ed73a22b0a72eb83206302f5fbff55c
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Wed Apr 13 14:41:33 2016 +0100

    arm64: vhe: Verify CPU Exception Levels
    
    With a VHE capable CPU, kernel can run at EL2 and is a decided at early
    boot. If some of the CPUs didn't start it EL2 or doesn't have VHE, we
    could have CPUs running at different exception levels, all in the same
    kernel! This patch adds an early check for the secondary CPUs to detect
    such situations.
    
    For each non-boot CPU add a sanity check to make sure we don't have
    different run levels w.r.t the boot CPU. We save the information on
    whether the boot CPU is running in hyp mode or not and ensure the
    remaining CPUs match it.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [will: made boot_cpu_hyp_mode static]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b2d5f4ee9a1c..5fbab1c40424 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -75,6 +75,43 @@ enum ipi_msg_type {
 	IPI_WAKEUP
 };
 
+#ifdef CONFIG_ARM64_VHE
+
+/* Whether the boot CPU is running in HYP mode or not*/
+static bool boot_cpu_hyp_mode;
+
+static inline void save_boot_cpu_run_el(void)
+{
+	boot_cpu_hyp_mode = is_kernel_in_hyp_mode();
+}
+
+static inline bool is_boot_cpu_in_hyp_mode(void)
+{
+	return boot_cpu_hyp_mode;
+}
+
+/*
+ * Verify that a secondary CPU is running the kernel at the same
+ * EL as that of the boot CPU.
+ */
+void verify_cpu_run_el(void)
+{
+	bool in_el2 = is_kernel_in_hyp_mode();
+	bool boot_cpu_el2 = is_boot_cpu_in_hyp_mode();
+
+	if (in_el2 ^ boot_cpu_el2) {
+		pr_crit("CPU%d: mismatched Exception Level(EL%d) with boot CPU(EL%d)\n",
+					smp_processor_id(),
+					in_el2 ? 2 : 1,
+					boot_cpu_el2 ? 2 : 1);
+		cpu_panic_kernel();
+	}
+}
+
+#else
+static inline void save_boot_cpu_run_el(void) {}
+#endif
+
 #ifdef CONFIG_HOTPLUG_CPU
 static int op_cpu_kill(unsigned int cpu);
 #else
@@ -401,6 +438,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 void __init smp_prepare_boot_cpu(void)
 {
 	cpuinfo_store_boot_cpu();
+	save_boot_cpu_run_el();
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit fc6d73d67436e7784758a831227bd019547a3f73
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:40 2016 +0000

    arch/hotplug: Call into idle with a proper state
    
    Let the non boot cpus call into idle with the corresponding hotplug state, so
    the hotplug core can handle the further bringup. That's a first step to
    convert the boot side of the hotplugged cpus to do all the synchronization
    with the other side through the state machine. For now it'll only start the
    hotplug thread and kick the full bringup of the cpu.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.614102639@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b1adc51b2c2e..460765799c64 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -195,7 +195,7 @@ asmlinkage void secondary_start_kernel(void)
 	/*
 	 * OK, it's off to the idle thread for us
 	 */
-	cpu_startup_entry(CPUHP_ONLINE);
+	cpu_startup_entry(CPUHP_AP_ONLINE_IDLE);
 }
 
 #ifdef CONFIG_HOTPLUG_CPU

commit bb9052744f4b7ae11d0061ac9492dd2949981b49
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Feb 23 10:31:42 2016 +0000

    arm64: Handle early CPU boot failures
    
    A secondary CPU could fail to come online due to insufficient
    capabilities and could simply die or loop in the kernel.
    e.g, a CPU with no support for the selected kernel PAGE_SIZE
    loops in kernel with MMU turned off.
    or a hotplugged CPU which doesn't have one of the advertised
    system capability will die during the activation.
    
    There is no way to synchronise the status of the failing CPU
    back to the master. This patch solves the issue by adding a
    field to the secondary_data which can be updated by the failing
    CPU. If the secondary CPU fails even before turning the MMU on,
    it updates the status in a special variable reserved in the head.txt
    section to make sure that the update can be cache invalidated safely
    without possible sharing of cache write back granule.
    
    Here are the possible states :
    
     -1. CPU_MMU_OFF - Initial value set by the master CPU, this value
    indicates that the CPU could not turn the MMU on, hence the status
    could not be reliably updated in the secondary_data. Instead, the
    CPU has updated the status @ __early_cpu_boot_status.
    
     0. CPU_BOOT_SUCCESS - CPU has booted successfully.
    
     1. CPU_KILL_ME - CPU has invoked cpu_ops->die, indicating the
    master CPU to synchronise by issuing a cpu_ops->cpu_kill.
    
     2. CPU_STUCK_IN_KERNEL - CPU couldn't invoke die(), instead is
    looping in the kernel. This information could be used by say,
    kexec to check if it is really safe to do a kexec reboot.
    
     3. CPU_PANIC_KERNEL - CPU detected some serious issues which
    requires kernel to crash immediately. The secondary CPU cannot
    call panic() until it has initialised the GIC. This flag can
    be used to instruct the master to do so.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    [catalin.marinas@arm.com: conflict resolution]
    [catalin.marinas@arm.com: converted "status" from int to long]
    [catalin.marinas@arm.com: updated update_early_cpu_boot_status to use str_l]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 8d09f597024d..d07b19654307 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -63,6 +63,8 @@
  * where to place its SVC stack
  */
 struct secondary_data secondary_data;
+/* Number of CPUs which aren't online, but looping in kernel text. */
+int cpus_stuck_in_kernel;
 
 enum ipi_msg_type {
 	IPI_RESCHEDULE,
@@ -73,6 +75,16 @@ enum ipi_msg_type {
 	IPI_WAKEUP
 };
 
+#ifdef CONFIG_HOTPLUG_CPU
+static int op_cpu_kill(unsigned int cpu);
+#else
+static inline int op_cpu_kill(unsigned int cpu)
+{
+	return -ENOSYS;
+}
+#endif
+
+
 /*
  * Boot a secondary CPU, and assign it the specified idle task.
  * This also gives us the initial stack to use for this CPU.
@@ -90,12 +102,14 @@ static DECLARE_COMPLETION(cpu_running);
 int __cpu_up(unsigned int cpu, struct task_struct *idle)
 {
 	int ret;
+	long status;
 
 	/*
 	 * We need to tell the secondary core where to find its stack and the
 	 * page tables.
 	 */
 	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
+	update_cpu_boot_status(CPU_MMU_OFF);
 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
 
 	/*
@@ -119,6 +133,32 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	}
 
 	secondary_data.stack = NULL;
+	status = READ_ONCE(secondary_data.status);
+	if (ret && status) {
+
+		if (status == CPU_MMU_OFF)
+			status = READ_ONCE(__early_cpu_boot_status);
+
+		switch (status) {
+		default:
+			pr_err("CPU%u: failed in unknown state : 0x%lx\n",
+					cpu, status);
+			break;
+		case CPU_KILL_ME:
+			if (!op_cpu_kill(cpu)) {
+				pr_crit("CPU%u: died during early boot\n", cpu);
+				break;
+			}
+			/* Fall through */
+			pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
+		case CPU_STUCK_IN_KERNEL:
+			pr_crit("CPU%u: is stuck in kernel\n", cpu);
+			cpus_stuck_in_kernel++;
+			break;
+		case CPU_PANIC_KERNEL:
+			panic("CPU%u detected unsupported configuration\n", cpu);
+		}
+	}
 
 	return ret;
 }
@@ -184,6 +224,9 @@ asmlinkage void secondary_start_kernel(void)
 	 */
 	pr_info("CPU%u: Booted secondary processor [%08x]\n",
 					 cpu, read_cpuid_id());
+	update_cpu_boot_status(CPU_BOOT_SUCCESS);
+	/* Make sure the status update is visible before we complete */
+	smp_wmb();
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 
@@ -326,10 +369,12 @@ void cpu_die_early(void)
 	set_cpu_present(cpu, 0);
 
 #ifdef CONFIG_HOTPLUG_CPU
+	update_cpu_boot_status(CPU_KILL_ME);
 	/* Check if we can park ourselves */
 	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
 		cpu_ops[cpu]->cpu_die(cpu);
 #endif
+	update_cpu_boot_status(CPU_STUCK_IN_KERNEL);
 
 	cpu_park_loop();
 }

commit fce6361fe9b0caeba0c05b7d72ceda406f8780df
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Tue Feb 23 10:31:41 2016 +0000

    arm64: Move cpu_die_early to smp.c
    
    This patch moves cpu_die_early to smp.c, where it fits better.
    No functional changes, except for adding the necessary checks
    for CONFIG_HOTPLUG_CPU.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 24cb4f800033..8d09f597024d 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -312,6 +312,28 @@ void cpu_die(void)
 }
 #endif
 
+/*
+ * Kill the calling secondary CPU, early in bringup before it is turned
+ * online.
+ */
+void cpu_die_early(void)
+{
+	int cpu = smp_processor_id();
+
+	pr_crit("CPU%d: will not boot\n", cpu);
+
+	/* Mark this CPU absent */
+	set_cpu_present(cpu, 0);
+
+#ifdef CONFIG_HOTPLUG_CPU
+	/* Check if we can park ourselves */
+	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
+		cpu_ops[cpu]->cpu_die(cpu);
+#endif
+
+	cpu_park_loop();
+}
+
 static void __init hyp_mode_check(void)
 {
 	if (is_hyp_mode_available())

commit 5e89c55e4ed81d7abb1ce8828db35fa389dc0e90
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Tue Jan 26 11:10:38 2016 +0000

    arm64: kernel: implement ACPI parking protocol
    
    The SBBR and ACPI specifications allow ACPI based systems that do not
    implement PSCI (eg systems with no EL3) to boot through the ACPI parking
    protocol specification[1].
    
    This patch implements the ACPI parking protocol CPU operations, and adds
    code that eases parsing the parking protocol data structures to the
    ARM64 SMP initializion carried out at the same time as cpus enumeration.
    
    To wake-up the CPUs from the parked state, this patch implements a
    wakeup IPI for ARM64 (ie arch_send_wakeup_ipi_mask()) that mirrors the
    ARM one, so that a specific IPI is sent for wake-up purpose in order
    to distinguish it from other IPI sources.
    
    Given the current ACPI MADT parsing API, the patch implements a glue
    layer that helps passing MADT GICC data structure from SMP initialization
    code to the parking protocol implementation somewhat overriding the CPU
    operations interfaces. This to avoid creating a completely trasparent
    DT/ACPI CPU operations layer that would require creating opaque
    structure handling for CPUs data (DT represents CPU through DT nodes, ACPI
    through static MADT table entries), which seems overkill given that ACPI
    on ARM64 mandates only two booting protocols (PSCI and parking protocol),
    so there is no need for further protocol additions.
    
    Based on the original work by Mark Salter <msalter@redhat.com>
    
    [1] https://acpica.org/sites/acpica/files/MP%20Startup%20for%20ARM%20platforms.docx
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Loc Ho <lho@apm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Sudeep Holla <sudeep.holla@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Al Stone <ahs3@redhat.com>
    [catalin.marinas@arm.com: Added WARN_ONCE(!acpi_parking_protocol_valid() on the IPI]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 68e7f79630d4..24cb4f800033 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -70,6 +70,7 @@ enum ipi_msg_type {
 	IPI_CPU_STOP,
 	IPI_TIMER,
 	IPI_IRQ_WORK,
+	IPI_WAKEUP
 };
 
 /*
@@ -443,6 +444,17 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 	/* map the logical cpu id to cpu MPIDR */
 	cpu_logical_map(cpu_count) = hwid;
 
+	/*
+	 * Set-up the ACPI parking protocol cpu entries
+	 * while initializing the cpu_logical_map to
+	 * avoid parsing MADT entries multiple times for
+	 * nothing (ie a valid cpu_logical_map entry should
+	 * contain a valid parking protocol data set to
+	 * initialize the cpu if the parking protocol is
+	 * the only available enable method).
+	 */
+	acpi_set_mailbox_entry(cpu_count, processor);
+
 	cpu_count++;
 }
 
@@ -625,6 +637,7 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
 	S(IPI_CPU_STOP, "CPU stop interrupts"),
 	S(IPI_TIMER, "Timer broadcast interrupts"),
 	S(IPI_IRQ_WORK, "IRQ work interrupts"),
+	S(IPI_WAKEUP, "CPU wake-up interrupts"),
 };
 
 static void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
@@ -668,6 +681,13 @@ void arch_send_call_function_single_ipi(int cpu)
 	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC);
 }
 
+#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
+void arch_send_wakeup_ipi_mask(const struct cpumask *mask)
+{
+	smp_cross_call(mask, IPI_WAKEUP);
+}
+#endif
+
 #ifdef CONFIG_IRQ_WORK
 void arch_irq_work_raise(void)
 {
@@ -745,6 +765,14 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		break;
 #endif
 
+#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
+	case IPI_WAKEUP:
+		WARN_ONCE(!acpi_parking_protocol_valid(cpu),
+			  "CPU%u: Wake-up IPI outside the ACPI parking protocol\n",
+			  cpu);
+		break;
+#endif
+
 	default:
 		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
 		break;

commit 9e8e865bbe294a69666a1996bda3e87825b258c0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jan 25 11:44:58 2016 +0000

    arm64: unify idmap removal
    
    We currently open-code the removal of the idmap and restoration of the
    current task's MMU state in a few places.
    
    Before introducing yet more copies of this sequence, unify these to call
    a new helper, cpu_uninstall_idmap.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Jeremy Linton <jeremy.linton@arm.com>
    Cc: Laura Abbott <labbott@fedoraproject.org>
    Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b1adc51b2c2e..68e7f79630d4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -149,9 +149,7 @@ asmlinkage void secondary_start_kernel(void)
 	 * TTBR0 is only used for the identity mapping at this stage. Make it
 	 * point to zero page to avoid speculatively fetching new entries.
 	 */
-	cpu_set_reserved_ttbr0();
-	local_flush_tlb_all();
-	cpu_set_default_tcr_t0sz();
+	cpu_uninstall_idmap();
 
 	preempt_disable();
 	trace_hardirqs_off();

commit 29b8302b1a7baef6b4c71ff368bd14729f26eb0c
Author: Jisheng Zhang <jszhang@marvell.com>
Date:   Thu Nov 12 20:04:42 2015 +0800

    arm64: smp: make of_parse_and_init_cpus static
    
    of_parse_and_init_cpus is only called from within smp.c, so it can be
    declared static.
    
    Signed-off-by: Jisheng Zhang <jszhang@marvell.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 2bbdc0e4fd14..b1adc51b2c2e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -473,7 +473,7 @@ acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
  * cpu logical map array containing MPIDR values related to logical
  * cpus. Assumes that cpu_logical_map(0) has already been initialized.
  */
-void __init of_parse_and_init_cpus(void)
+static void __init of_parse_and_init_cpus(void)
 {
 	struct device_node *dn = NULL;
 

commit dbb4e152b8da1f977d9d8cd7e494ab4ee3622f72
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:50 2015 +0100

    arm64: Delay cpu feature capability checks
    
    At the moment we run through the arm64_features capability list for
    each CPU and set the capability if one of the CPU supports it. This
    could be problematic in a heterogeneous system with differing capabilities.
    Delay the CPU feature checks until all the enabled CPUs are up(i.e,
    smp_cpus_done(), so that we can make better decisions based on the
    overall system capability. Once we decide and advertise the capabilities
    the alternatives can be applied. From this state, we cannot roll back
    a feature to disabled based on the values from a new hotplugged CPU,
    due to the runtime patching and other reasons. So, for all new CPUs,
    we need to make sure that they have the established system capabilities.
    Failing which, we bring the CPU down, preventing it from turning online.
    Once the capabilities are decided, any new CPU booting up goes through
    verification to ensure that it has all the enabled capabilities and also
    invokes the respective enable() method on the CPU.
    
    The CPU errata checks are not delayed and is still executed per-CPU
    to detect the respective capabilities. If we ever come across a non-errata
    capability that needs to be checked on each-CPU, we could introduce them via
    a new capability table(or introduce a flag), which can be processed per CPU.
    
    The next patch will make the feature checks use the system wide
    safe value of a feature register.
    
    NOTE: The enable() methods associated with the capability is scheduled
    on all the CPUs (which is the only use case at the moment). If we need
    a different type of 'enable()' which only needs to be run once on any CPU,
    we should be able to handle that when needed.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    [catalin.marinas@arm.com: static variable and coding style fixes]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d1d00499f0cf..2bbdc0e4fd14 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -156,6 +156,13 @@ asmlinkage void secondary_start_kernel(void)
 	preempt_disable();
 	trace_hardirqs_off();
 
+	/*
+	 * If the system has established the capabilities, make sure
+	 * this CPU ticks all of those. If it doesn't, the CPU will
+	 * fail to come online.
+	 */
+	verify_local_cpu_capabilities();
+
 	if (cpu_ops[cpu]->cpu_postboot)
 		cpu_ops[cpu]->cpu_postboot();
 

commit 4b998ff1885eecd3dc330bf057e24667c1db84a4
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:40 2015 +0100

    arm64: Delay cpuinfo_store_boot_cpu
    
    At the moment the boot CPU stores the cpuinfo long before the
    PERCPU areas are initialised by the kernel. This could be problematic
    as the non-boot CPU data structures might get copied with the data
    from the boot CPU, giving us no chance to detect if a particular CPU
    updated its cpuinfo. This patch delays the boot cpu store to
    smp_prepare_boot_cpu().
    
    Also kills the setup_processor() which no longer does meaningful
    work.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 77763d9cb510..d1d00499f0cf 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -327,6 +327,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 void __init smp_prepare_boot_cpu(void)
 {
+	cpuinfo_store_boot_cpu();
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 

commit 3a75578efae64b94d76eacbf8adf2a3ab13c6aa1
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:39 2015 +0100

    arm64: Delay ELF HWCAP initialisation until all CPUs are up
    
    Delay the ELF HWCAP initialisation until all the (enabled) CPUs are
    up, i.e, smp_cpus_done(). This is in preparation for detecting the
    common features across the CPUS and creating a consistent ELF HWCAP
    for the system.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index c1d044b52f42..77763d9cb510 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -320,6 +320,7 @@ static void __init hyp_mode_check(void)
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());
+	setup_cpu_features();
 	hyp_mode_check();
 	apply_alternatives_all();
 }

commit 64f17818977d0989f7d05347670777611b295799
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Mon Oct 19 14:24:38 2015 +0100

    arm64: Make the CPU information more clear
    
    At early boot, we print the CPU version/revision. On a heterogeneous
    system, we could have different types of CPUs. Print the CPU info for
    all active cpus. Also, the secondary CPUs prints the message only when
    they turn online.
    
    Also, remove the redundant 'revision' information which doesn't
    make any sense without the 'variant' field.
    
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Tested-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b7a973d6861e..c1d044b52f42 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -144,7 +144,6 @@ asmlinkage void secondary_start_kernel(void)
 	current->active_mm = mm;
 
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
-	printk("CPU%u: Booted secondary processor\n", cpu);
 
 	/*
 	 * TTBR0 is only used for the identity mapping at this stage. Make it
@@ -177,6 +176,8 @@ asmlinkage void secondary_start_kernel(void)
 	 * the CPU migration code to notice that the CPU is online
 	 * before we continue.
 	 */
+	pr_info("CPU%u: Booted secondary processor [%08x]\n",
+					 cpu, read_cpuid_id());
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 

commit 217d453d473c5ddfd140a06bf9d8575218551020
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Thu Sep 24 17:32:14 2015 +0800

    arm64: fix a migrating irq bug when hotplug cpu
    
    When cpu is disabled, all irqs will be migratged to another cpu.
    In some cases, a new affinity is different, the old affinity need
    to be updated and if irq_set_affinity's return value is IRQ_SET_MASK_OK_DONE,
    the old affinity can not be updated. Fix it by using irq_do_set_affinity.
    
    And migrating interrupts is a core code matter, so use the generic
    function irq_migrate_all_off_this_cpu() to migrate interrupts in
    kernel/irq/migration.c.
    
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 03b0aa28ea61..b7a973d6861e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -231,7 +231,8 @@ int __cpu_disable(void)
 	/*
 	 * OK - migrate IRQs away from this CPU
 	 */
-	migrate_irqs();
+	irq_migrate_all_off_this_cpu();
+
 	return 0;
 }
 

commit 38d96287504a2478eb538bfecfa1fddd743bb6b2
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:28 2015 +0100

    arm64: mm: kill mm_cpumask usage
    
    mm_cpumask isn't actually used for anything on arm64, so remove all the
    code trying to keep it up-to-date.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index fdd4d4dbd64f..03b0aa28ea61 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -142,7 +142,6 @@ asmlinkage void secondary_start_kernel(void)
 	 */
 	atomic_inc(&mm->mm_count);
 	current->active_mm = mm;
-	cpumask_set_cpu(cpu, mm_cpumask(mm));
 
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 	printk("CPU%u: Booted secondary processor\n", cpu);
@@ -233,12 +232,6 @@ int __cpu_disable(void)
 	 * OK - migrate IRQs away from this CPU
 	 */
 	migrate_irqs();
-
-	/*
-	 * Remove this CPU from the vm mask set of all processes.
-	 */
-	clear_tasks_mm_cpumask(cpu);
-
 	return 0;
 }
 

commit 8e63d38876691756f9bc6930850f1fb77809be1b
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Oct 6 18:46:23 2015 +0100

    arm64: flush: use local TLB and I-cache invalidation
    
    There are a number of places where a single CPU is running with a
    private page-table and we need to perform maintenance on the TLB and
    I-cache in order to ensure correctness, but do not require the operation
    to be broadcast to other CPUs.
    
    This patch adds local variants of tlb_flush_all and __flush_icache_all
    to support these use-cases and updates the callers respectively.
    __local_flush_icache_all also implies an isb, since it is intended to be
    used synchronously.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: David Daney <david.daney@cavium.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index dbdaacddd9a5..fdd4d4dbd64f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -152,7 +152,7 @@ asmlinkage void secondary_start_kernel(void)
 	 * point to zero page to avoid speculatively fetching new entries.
 	 */
 	cpu_set_reserved_ttbr0();
-	flush_tlb_all();
+	local_flush_tlb_all();
 	cpu_set_default_tcr_t0sz();
 
 	preempt_disable();

commit 377bcff9a38a78083d7fff8e8a41cc894cf7813b
Author: Jonas Rabenstein <jonas.rabenstein@studium.uni-erlangen.de>
Date:   Wed Jul 29 12:07:57 2015 +0100

    arm64: remove dead-code depending on CONFIG_UP_LATE_INIT
    
    Commit 4b3dc9679cf7 ("arm64: force CONFIG_SMP=y and remove redundant
    and therfore can not be selected anymore.
    
    Remove dead #ifdef-block depending on UP_LATE_INIT in
    arch/arm64/kernel/setup.c
    
    Signed-off-by: Jonas Rabenstein <jonas.rabenstein@studium.uni-erlangen.de>
    [will: kill do_post_cpus_up_work altogether]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 50fb4696654e..dbdaacddd9a5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -52,6 +52,7 @@
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/ptrace.h>
+#include <asm/virt.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/ipi.h>
@@ -310,10 +311,22 @@ void cpu_die(void)
 }
 #endif
 
+static void __init hyp_mode_check(void)
+{
+	if (is_hyp_mode_available())
+		pr_info("CPU: All CPU(s) started at EL2\n");
+	else if (is_hyp_mode_mismatched())
+		WARN_TAINT(1, TAINT_CPU_OUT_OF_SPEC,
+			   "CPU: CPUs started in inconsistent modes");
+	else
+		pr_info("CPU: All CPU(s) started at EL1\n");
+}
+
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());
-	do_post_cpus_up_work();
+	hyp_mode_check();
+	apply_alternatives_all();
 }
 
 void __init smp_prepare_boot_cpu(void)

commit 99e3e3ae332b6ca91d5c444ea7849f367f5e5a76
Author: Al Stone <al.stone@linaro.org>
Date:   Mon Jul 6 17:16:48 2015 -0600

    ACPI / ARM64 : use the new BAD_MADT_GICC_ENTRY macro
    
    For those parts of the arm64 ACPI code that need to check GICC subtables
    in the MADT, use the new BAD_MADT_GICC_ENTRY macro instead of the previous
    BAD_MADT_ENTRY.  The new macro takes into account differences in the size
    of the GICC subtable that the old macro did not; this caused failures even
    though the subtable entries are valid.
    
    Fixes: aeb823bbacc2 ("ACPICA: ACPI 6.0: Add changes for FADT table.")
    Signed-off-by: Al Stone <al.stone@linaro.org>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 695801a54ca5..50fb4696654e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -438,7 +438,7 @@ acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
 	struct acpi_madt_generic_interrupt *processor;
 
 	processor = (struct acpi_madt_generic_interrupt *)header;
-	if (BAD_MADT_ENTRY(processor, end))
+	if (BAD_MADT_GICC_ENTRY(processor, end))
 		return -EINVAL;
 
 	acpi_table_print_madt_entry(header);

commit f9058929f2acbb273ec83104ebeeab0593595e15
Author: Hanjun Guo <hanjun.guo@linaro.org>
Date:   Fri Jul 3 15:29:06 2015 +0800

    ARM64 / SMP: Switch pr_err() to pr_debug() for disabled GICC entry
    
    It is normal that firmware presents GICC entry or entries (processors)
    with disabled flag in ACPI MADT, taking a system of 16 cpus for example,
    ACPI firmware may present 8 ebabled first with another 8 cpus disabled
    in MADT, the disabled cpus can be hot-added later.
    
    Firmware may also present more cpus than the hardware actually has, but
    disabled the unused ones, and easily enable it when the hardware has such
    cpus to make the firmware code scalable.
    
    So that's not an error for disabled cpus in MADT, we can switch pr_err()
    to pr_debug() to make the boot a little quieter by default.
    
    Since hwid for disabled cpus often are invalid, and we check invalid hwid
    first in the code, for use case that hot add cpus later will be filtered
    out and will not be counted in possible cups, so move this check before
    the hwid one to prepare the code to count for disabeld cpus when cpu
    hot-plug is introduced.
    
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Reviewed-by: Al Stone <ahs3@redhat.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a1883bfdd9d6..695801a54ca5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -396,13 +396,13 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
 {
 	u64 hwid = processor->arm_mpidr;
 
-	if (hwid & ~MPIDR_HWID_BITMASK || hwid == INVALID_HWID) {
-		pr_err("skipping CPU entry with invalid MPIDR 0x%llx\n", hwid);
+	if (!(processor->flags & ACPI_MADT_ENABLED)) {
+		pr_debug("skipping disabled CPU entry with 0x%llx MPIDR\n", hwid);
 		return;
 	}
 
-	if (!(processor->flags & ACPI_MADT_ENABLED)) {
-		pr_err("skipping disabled CPU entry with 0x%llx MPIDR\n", hwid);
+	if (hwid & ~MPIDR_HWID_BITMASK || hwid == INVALID_HWID) {
+		pr_err("skipping CPU entry with invalid MPIDR 0x%llx\n", hwid);
 		return;
 	}
 

commit be081d9bf3e163a9ed1ca2f0f14f08424c7f9016
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Wed Jun 24 13:14:18 2015 -0700

    ARM64: smp: Fix suspicious RCU usage with ipi tracepoints
    
    John Stultz reported an RCU splat on ARM with ipi trace events
    enabled. It looks like the same problem exists on ARM64.
    
    At this point in the IPI handling path we haven't called
    irq_enter() yet, so RCU doesn't know that we're about to exit
    idle and properly warns that we're using RCU from an idle CPU.
    Use trace_ipi_entry_rcuidle() instead of trace_ipi_entry() so
    that RCU is informed about our exit from idle.
    
    Cc: John Stultz <john.stultz@linaro.org>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org> # 3.17+
    Fixes: 45ed695ac10a ("ARM64: add IPI tracepoints")
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 4b2121bd7f9c..a1883bfdd9d6 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -693,7 +693,7 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
 	if ((unsigned)ipinr < NR_IPI) {
-		trace_ipi_entry(ipi_types[ipinr]);
+		trace_ipi_entry_rcuidle(ipi_types[ipinr]);
 		__inc_irq_stat(cpu, ipi_irqs[ipinr]);
 	}
 
@@ -736,7 +736,7 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 	}
 
 	if ((unsigned)ipinr < NR_IPI)
-		trace_ipi_exit(ipi_types[ipinr]);
+		trace_ipi_exit_rcuidle(ipi_types[ipinr]);
 	set_irq_regs(old_regs);
 }
 

commit addc8120a784181cc6410973948eee94ea16f2bd
Merge: eb7c11ee3c5c c5a1330573c1
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jun 5 11:21:23 2015 +0100

    Merge branch 'arm64/psci-rework' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux
    
    * 'arm64/psci-rework' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: psci: remove ACPI coupling
      arm64: psci: kill psci_power_state
      arm64: psci: account for Trusted OS instances
      arm64: psci: support unsigned return values
      arm64: psci: remove unnecessary id indirection
      arm64: smp: consistently use error codes
      arm64: smp_plat: add get_logical_index
      arm/arm64: kvm: add missing PSCI include
    
    Conflicts:
            arch/arm64/kernel/smp.c

commit 6b99c68cb5dd274d79451e5135f9450f7c01ca52
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Apr 20 17:55:30 2015 +0100

    arm64: smp: consistently use error codes
    
    cpu_kill currently returns one for success and zero for failure, which
    is unlike all the other cpu_operations, which return zero for success
    and an error code upon failure. This difference is unnecessarily
    confusing.
    
    Make cpu_kill consistent with the other cpu_operations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b698208848e9..2b503a3186dd 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -249,7 +249,7 @@ static int op_cpu_kill(unsigned int cpu)
 	 * time and hope that it's dead, so let's skip the wait and just hope.
 	 */
 	if (!cpu_ops[cpu]->cpu_kill)
-		return 1;
+		return 0;
 
 	return cpu_ops[cpu]->cpu_kill(cpu);
 }
@@ -262,6 +262,8 @@ static DECLARE_COMPLETION(cpu_died);
  */
 void __cpu_die(unsigned int cpu)
 {
+	int err;
+
 	if (!wait_for_completion_timeout(&cpu_died, msecs_to_jiffies(5000))) {
 		pr_crit("CPU%u: cpu didn't die\n", cpu);
 		return;
@@ -274,8 +276,10 @@ void __cpu_die(unsigned int cpu)
 	 * verify that it has really left the kernel before we consider
 	 * clobbering anything it might still be using.
 	 */
-	if (!op_cpu_kill(cpu))
-		pr_warn("CPU%d may not have shut down cleanly\n", cpu);
+	err = op_cpu_kill(cpu);
+	if (err)
+		pr_warn("CPU%d may not have shut down cleanly: %d\n",
+			cpu, err);
 }
 
 /*

commit 05981277a4de1ad631a4b2c39c4fbc1db69b6f23
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue May 12 14:50:05 2015 -0700

    arm64: Use common outgoing-CPU-notification code
    
    This commit removes the open-coded CPU-offline notification with new
    common code.  In particular, this change avoids calling scheduler code
    using RCU from an offline CPU that RCU is ignoring.  This is a minimal
    change.  A more intrusive change might invoke the cpu_check_up_prepare()
    and cpu_set_state_online() functions at CPU-online time, which would
    allow onlining throw an error if the CPU did not go offline properly.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b698208848e9..65f1a7f72697 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -254,15 +254,13 @@ static int op_cpu_kill(unsigned int cpu)
 	return cpu_ops[cpu]->cpu_kill(cpu);
 }
 
-static DECLARE_COMPLETION(cpu_died);
-
 /*
  * called on the thread which is asking for a CPU to be shutdown -
  * waits until shutdown has completed, or it is timed out.
  */
 void __cpu_die(unsigned int cpu)
 {
-	if (!wait_for_completion_timeout(&cpu_died, msecs_to_jiffies(5000))) {
+	if (!cpu_wait_death(cpu, 5)) {
 		pr_crit("CPU%u: cpu didn't die\n", cpu);
 		return;
 	}
@@ -295,7 +293,7 @@ void cpu_die(void)
 	local_irq_disable();
 
 	/* Tell __cpu_die() that this CPU is now safe to dispose of */
-	complete(&cpu_died);
+	(void)cpu_report_death();
 
 	/*
 	 * Actually shutdown the CPU. This must never fail. The specific hotplug

commit 0f0783365cbb7ec13a8f02198f6e1a146d94a5a9
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed May 13 14:12:47 2015 +0100

    ARM64: kernel: unify ACPI and DT cpus initialization
    
    The code that initializes cpus on arm64 is currently split in two
    different code paths that carry out DT and ACPI cpus initialization.
    
    Most of the code executing SMP initialization is common and should
    be merged to reduce discrepancies between ACPI and DT initialization
    and to have code initializing cpus in a single common place in the
    kernel.
    
    This patch refactors arm64 SMP cpus initialization code to merge
    ACPI and DT boot paths in a common file and to create sanity
    checks that can be reused by both boot methods.
    
    Current code assumes PSCI is the only available boot method
    when arm64 boots with ACPI; this can be easily extended if/when
    the ACPI parking protocol is merged into the kernel.
    
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: Mark Rutland <mark.rutland@arm.com> [DT]
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 98eb68b5660f..b698208848e9 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -17,6 +17,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/acpi.h>
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/spinlock.h>
@@ -318,6 +319,49 @@ void __init smp_prepare_boot_cpu(void)
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 
+static u64 __init of_get_cpu_mpidr(struct device_node *dn)
+{
+	const __be32 *cell;
+	u64 hwid;
+
+	/*
+	 * A cpu node with missing "reg" property is
+	 * considered invalid to build a cpu_logical_map
+	 * entry.
+	 */
+	cell = of_get_property(dn, "reg", NULL);
+	if (!cell) {
+		pr_err("%s: missing reg property\n", dn->full_name);
+		return INVALID_HWID;
+	}
+
+	hwid = of_read_number(cell, of_n_addr_cells(dn));
+	/*
+	 * Non affinity bits must be set to 0 in the DT
+	 */
+	if (hwid & ~MPIDR_HWID_BITMASK) {
+		pr_err("%s: invalid reg property\n", dn->full_name);
+		return INVALID_HWID;
+	}
+	return hwid;
+}
+
+/*
+ * Duplicate MPIDRs are a recipe for disaster. Scan all initialized
+ * entries and check for duplicates. If any is found just ignore the
+ * cpu. cpu_logical_map was initialized to INVALID_HWID to avoid
+ * matching valid MPIDR values.
+ */
+static bool __init is_mpidr_duplicate(unsigned int cpu, u64 hwid)
+{
+	unsigned int i;
+
+	for (i = 1; (i < cpu) && (i < NR_CPUS); i++)
+		if (cpu_logical_map(i) == hwid)
+			return true;
+	return false;
+}
+
 /*
  * Initialize cpu operations for a logical cpu and
  * set it in the possible mask on success
@@ -335,56 +379,97 @@ static int __init smp_cpu_setup(int cpu)
 	return 0;
 }
 
+static bool bootcpu_valid __initdata;
+static unsigned int cpu_count = 1;
+
+#ifdef CONFIG_ACPI
+/*
+ * acpi_map_gic_cpu_interface - parse processor MADT entry
+ *
+ * Carry out sanity checks on MADT processor entry and initialize
+ * cpu_logical_map on success
+ */
+static void __init
+acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
+{
+	u64 hwid = processor->arm_mpidr;
+
+	if (hwid & ~MPIDR_HWID_BITMASK || hwid == INVALID_HWID) {
+		pr_err("skipping CPU entry with invalid MPIDR 0x%llx\n", hwid);
+		return;
+	}
+
+	if (!(processor->flags & ACPI_MADT_ENABLED)) {
+		pr_err("skipping disabled CPU entry with 0x%llx MPIDR\n", hwid);
+		return;
+	}
+
+	if (is_mpidr_duplicate(cpu_count, hwid)) {
+		pr_err("duplicate CPU MPIDR 0x%llx in MADT\n", hwid);
+		return;
+	}
+
+	/* Check if GICC structure of boot CPU is available in the MADT */
+	if (cpu_logical_map(0) == hwid) {
+		if (bootcpu_valid) {
+			pr_err("duplicate boot CPU MPIDR: 0x%llx in MADT\n",
+			       hwid);
+			return;
+		}
+		bootcpu_valid = true;
+		return;
+	}
+
+	if (cpu_count >= NR_CPUS)
+		return;
+
+	/* map the logical cpu id to cpu MPIDR */
+	cpu_logical_map(cpu_count) = hwid;
+
+	cpu_count++;
+}
+
+static int __init
+acpi_parse_gic_cpu_interface(struct acpi_subtable_header *header,
+			     const unsigned long end)
+{
+	struct acpi_madt_generic_interrupt *processor;
+
+	processor = (struct acpi_madt_generic_interrupt *)header;
+	if (BAD_MADT_ENTRY(processor, end))
+		return -EINVAL;
+
+	acpi_table_print_madt_entry(header);
+
+	acpi_map_gic_cpu_interface(processor);
+
+	return 0;
+}
+#else
+#define acpi_table_parse_madt(...)	do { } while (0)
+#endif
+
 /*
  * Enumerate the possible CPU set from the device tree and build the
  * cpu logical map array containing MPIDR values related to logical
  * cpus. Assumes that cpu_logical_map(0) has already been initialized.
  */
-void __init of_smp_init_cpus(void)
+void __init of_parse_and_init_cpus(void)
 {
 	struct device_node *dn = NULL;
-	unsigned int i, cpu = 1;
-	bool bootcpu_valid = false;
 
 	while ((dn = of_find_node_by_type(dn, "cpu"))) {
-		const u32 *cell;
-		u64 hwid;
+		u64 hwid = of_get_cpu_mpidr(dn);
 
-		/*
-		 * A cpu node with missing "reg" property is
-		 * considered invalid to build a cpu_logical_map
-		 * entry.
-		 */
-		cell = of_get_property(dn, "reg", NULL);
-		if (!cell) {
-			pr_err("%s: missing reg property\n", dn->full_name);
+		if (hwid == INVALID_HWID)
 			goto next;
-		}
-		hwid = of_read_number(cell, of_n_addr_cells(dn));
 
-		/*
-		 * Non affinity bits must be set to 0 in the DT
-		 */
-		if (hwid & ~MPIDR_HWID_BITMASK) {
-			pr_err("%s: invalid reg property\n", dn->full_name);
+		if (is_mpidr_duplicate(cpu_count, hwid)) {
+			pr_err("%s: duplicate cpu reg properties in the DT\n",
+				dn->full_name);
 			goto next;
 		}
 
-		/*
-		 * Duplicate MPIDRs are a recipe for disaster. Scan
-		 * all initialized entries and check for
-		 * duplicates. If any is found just ignore the cpu.
-		 * cpu_logical_map was initialized to INVALID_HWID to
-		 * avoid matching valid MPIDR values.
-		 */
-		for (i = 1; (i < cpu) && (i < NR_CPUS); i++) {
-			if (cpu_logical_map(i) == hwid) {
-				pr_err("%s: duplicate cpu reg properties in the DT\n",
-					dn->full_name);
-				goto next;
-			}
-		}
-
 		/*
 		 * The numbering scheme requires that the boot CPU
 		 * must be assigned logical id 0. Record it so that
@@ -409,22 +494,42 @@ void __init of_smp_init_cpus(void)
 			continue;
 		}
 
-		if (cpu >= NR_CPUS)
+		if (cpu_count >= NR_CPUS)
 			goto next;
 
 		pr_debug("cpu logical map 0x%llx\n", hwid);
-		cpu_logical_map(cpu) = hwid;
+		cpu_logical_map(cpu_count) = hwid;
 next:
-		cpu++;
+		cpu_count++;
 	}
+}
+
+/*
+ * Enumerate the possible CPU set from the device tree or ACPI and build the
+ * cpu logical map array containing MPIDR values related to logical
+ * cpus. Assumes that cpu_logical_map(0) has already been initialized.
+ */
+void __init smp_init_cpus(void)
+{
+	int i;
+
+	if (acpi_disabled)
+		of_parse_and_init_cpus();
+	else
+		/*
+		 * do a walk of MADT to determine how many CPUs
+		 * we have including disabled CPUs, and get information
+		 * we need for SMP init
+		 */
+		acpi_table_parse_madt(ACPI_MADT_TYPE_GENERIC_INTERRUPT,
+				      acpi_parse_gic_cpu_interface, 0);
 
-	/* sanity check */
-	if (cpu > NR_CPUS)
-		pr_warning("no. of cores (%d) greater than configured maximum of %d - clipping\n",
-			   cpu, NR_CPUS);
+	if (cpu_count > NR_CPUS)
+		pr_warn("no. of cores (%d) greater than configured maximum of %d - clipping\n",
+			cpu_count, NR_CPUS);
 
 	if (!bootcpu_valid) {
-		pr_err("DT missing boot CPU MPIDR, not enabling secondaries\n");
+		pr_err("missing boot CPU MPIDR, not enabling secondaries\n");
 		return;
 	}
 

commit 819a88263d5dbe398edd59cc1cf725ed1fdcfd79
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed May 13 14:12:46 2015 +0100

    ARM64: kernel: make cpu_ops hooks DT agnostic
    
    ARM64 CPU operations such as cpu_init and cpu_init_idle take
    a struct device_node pointer as a parameter, which corresponds to
    the device tree node of the logical cpu on which the operation
    has to be applied.
    
    With the advent of ACPI on arm64, where MADT static table entries
    are used to initialize cpus, the device tree node parameter
    in cpu_ops hooks become useless when booting with ACPI, since
    in that case cpu device tree nodes are not present and can not be
    used for cpu initialization.
    
    The current cpu_init hook requires a struct device_node pointer
    parameter because it is called while parsing the device tree to
    initialize CPUs, when the cpu_logical_map (that is used to match
    a cpu node reg property to a device tree node) for a given logical
    cpu id is not set up yet. This means that the cpu_init hook cannot
    rely on the of_get_cpu_node function to retrieve the device tree
    node corresponding to the logical cpu id passed in as parameter,
    so the cpu device tree node must be passed in as a parameter to fix
    this catch-22 dependency cycle.
    
    This patch reshuffles the cpu_logical_map initialization code so
    that the cpu_init cpu_ops hook can safely use the of_get_cpu_node
    function to retrieve the cpu device tree node, removing the need for
    the device tree node pointer parameter.
    
    In the process, the patch removes device tree node parameters
    from all cpu_ops hooks, in preparation for SMP DT/ACPI cpus
    initialization consolidation.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Hanjun Guo <hanjun.guo@linaro.org>
    Tested-by: Mark Rutland <mark.rutland@arm.com> [DT]
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 2cb008177252..98eb68b5660f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -318,6 +318,23 @@ void __init smp_prepare_boot_cpu(void)
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 
+/*
+ * Initialize cpu operations for a logical cpu and
+ * set it in the possible mask on success
+ */
+static int __init smp_cpu_setup(int cpu)
+{
+	if (cpu_read_ops(cpu))
+		return -ENODEV;
+
+	if (cpu_ops[cpu]->cpu_init(cpu))
+		return -ENODEV;
+
+	set_cpu_possible(cpu, true);
+
+	return 0;
+}
+
 /*
  * Enumerate the possible CPU set from the device tree and build the
  * cpu logical map array containing MPIDR values related to logical
@@ -395,12 +412,6 @@ void __init of_smp_init_cpus(void)
 		if (cpu >= NR_CPUS)
 			goto next;
 
-		if (cpu_read_ops(dn, cpu) != 0)
-			goto next;
-
-		if (cpu_ops[cpu]->cpu_init(dn, cpu))
-			goto next;
-
 		pr_debug("cpu logical map 0x%llx\n", hwid);
 		cpu_logical_map(cpu) = hwid;
 next:
@@ -418,12 +429,18 @@ void __init of_smp_init_cpus(void)
 	}
 
 	/*
-	 * All the cpus that made it to the cpu_logical_map have been
-	 * validated so set them as possible cpus.
+	 * We need to set the cpu_logical_map entries before enabling
+	 * the cpus so that cpu processor description entries (DT cpu nodes
+	 * and ACPI MADT entries) can be retrieved by matching the cpu hwid
+	 * with entries in cpu_logical_map while initializing the cpus.
+	 * If the cpu set-up fails, invalidate the cpu_logical_map entry.
 	 */
-	for (i = 0; i < NR_CPUS; i++)
-		if (cpu_logical_map(i) != INVALID_HWID)
-			set_cpu_possible(i, true);
+	for (i = 1; i < NR_CPUS; i++) {
+		if (cpu_logical_map(i) != INVALID_HWID) {
+			if (smp_cpu_setup(i))
+				cpu_logical_map(i) = INVALID_HWID;
+		}
+	}
 }
 
 void __init smp_prepare_cpus(unsigned int max_cpus)

commit 836ee4874e201a5907f9658fb2bf3527dd952d30
Merge: fb65d872d7a8 7676fa70feb2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Apr 24 08:23:45 2015 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull initial ACPI support for arm64 from Will Deacon:
     "This series introduces preliminary ACPI 5.1 support to the arm64
      kernel using the "hardware reduced" profile.  We don't support any
      peripherals yet, so it's fairly limited in scope:
    
       - MEMORY init (UEFI)
    
       - ACPI discovery (RSDP via UEFI)
    
       - CPU init (FADT)
    
       - GIC init (MADT)
    
       - SMP boot (MADT + PSCI)
    
       - ACPI Kconfig options (dependent on EXPERT)
    
      ACPI for arm64 has been in development for a while now and hardware
      has been available that can boot with either FDT or ACPI tables.  This
      has been made possible by both changes to the ACPI spec to cater for
      ARM-based machines (known as "hardware-reduced" in ACPI parlance) but
      also a Linaro-driven effort to get this supported on top of the Linux
      kernel.  This pull request is the result of that work.
    
      These changes allow us to initialise the CPUs, interrupt controller,
      and timers via ACPI tables, with memory information and cmdline coming
      from EFI.  We don't support a hybrid ACPI/FDT scheme.  Of course,
      there is still plenty of work to do (a serial console would be nice!)
      but I expect that to happen on a per-driver basis after this core
      series has been merged.
    
      Anyway, the diff stat here is fairly horrible, but splitting this up
      and merging it via all the different subsystems would have been
      extremely painful.  Instead, we've got all the relevant Acks in place
      and I've not seen anything other than trivial (Kconfig) conflicts in
      -next (for completeness, I've included my resolution below).  Nearly
      half of the insertions fall under Documentation/.
    
      So, we'll see how this goes.  Right now, it all depends on EXPERT and
      I fully expect people to use FDT by default for the immediate future"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (31 commits)
      ARM64 / ACPI: make acpi_map_gic_cpu_interface() as void function
      ARM64 / ACPI: Ignore the return error value of acpi_map_gic_cpu_interface()
      ARM64 / ACPI: fix usage of acpi_map_gic_cpu_interface
      ARM64: kernel: acpi: honour acpi=force command line parameter
      ARM64: kernel: acpi: refactor ACPI tables init and checks
      ARM64: kernel: psci: let ACPI probe PSCI version
      ARM64: kernel: psci: factor out probe function
      ACPI: move arm64 GSI IRQ model to generic GSI IRQ layer
      ARM64 / ACPI: Don't unflatten device tree if acpi=force is passed
      ARM64 / ACPI: additions of ACPI documentation for arm64
      Documentation: ACPI for ARM64
      ARM64 / ACPI: Enable ARM64 in Kconfig
      XEN / ACPI: Make XEN ACPI depend on X86
      ARM64 / ACPI: Select ACPI_REDUCED_HARDWARE_ONLY if ACPI is enabled on ARM64
      clocksource / arch_timer: Parse GTDT to initialize arch timer
      irqchip: Add GICv2 specific ACPI boot support
      ARM64 / ACPI: Introduce ACPI_IRQ_MODEL_GIC and register device's gsi
      ACPI / processor: Make it possible to get CPU hardware ID via GICC
      ACPI / processor: Introduce phys_cpuid_t for CPU hardware ID
      ARM64 / ACPI: Parse MADT for SMP initialization
      ...

commit 6496edfce95f943e1da43631c2f437509e56af7f
Merge: b19a42e3cb9e e4afa120c982
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 20 10:19:03 2015 -0700

    Merge tag 'cpumask-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux
    
    Pull final removal of deprecated cpus_* cpumask functions from Rusty Russell:
     "This is the final removal (after several years!) of the obsolete
      cpus_* functions, prompted by their mis-use in staging.
    
      With these function removed, all cpu functions should only iterate to
      nr_cpu_ids, so we finally only allocate that many bits when cpumasks
      are allocated offstack"
    
    * tag 'cpumask-next-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rusty/linux: (25 commits)
      cpumask: remove __first_cpu / __next_cpu
      cpumask: resurrect CPU_MASK_CPU0
      linux/cpumask.h: add typechecking to cpumask_test_cpu
      cpumask: only allocate nr_cpumask_bits.
      Fix weird uses of num_online_cpus().
      cpumask: remove deprecated functions.
      mips: fix obsolete cpumask_of_cpu usage.
      x86: fix more deprecated cpu function usage.
      ia64: remove deprecated cpus_ usage.
      powerpc: fix deprecated CPU_MASK_CPU0 usage.
      CPU_MASK_ALL/CPU_MASK_NONE: remove from deprecated region.
      staging/lustre/o2iblnd: Don't use cpus_weight
      staging/lustre/libcfs: replace deprecated cpus_ calls with cpumask_
      staging/lustre/ptlrpc: Do not use deprecated cpus_* functions
      blackfin: fix up obsolete cpu function usage.
      parisc: fix up obsolete cpu function usage.
      tile: fix up obsolete cpu function usage.
      arm64: fix up obsolete cpu function usage.
      mips: fix up obsolete cpu function usage.
      x86: fix up obsolete cpu function usage.
      ...

commit fccb9a81fd08b61bed91ddef88341694f8ecbfd1
Author: Hanjun Guo <hanjun.guo@linaro.org>
Date:   Tue Mar 24 22:02:45 2015 +0800

    ARM64 / ACPI: Parse MADT for SMP initialization
    
    MADT contains the information for MPIDR which is essential for
    SMP initialization, parse the GIC cpu interface structures to
    get the MPIDR value and map it to cpu_logical_map(), and add
    enabled cpu with valid MPIDR into cpu_possible_map.
    
    ACPI 5.1 only has two explicit methods to boot up SMP, PSCI and
    Parking protocol, but the Parking protocol is only specified for
    ARMv7 now, so make PSCI as the only way for the SMP boot protocol
    before some updates for the ACPI spec or the Parking protocol spec.
    
    Parking protocol patches for SMP boot will be sent to upstream when
    the new version of Parking protocol is ready.
    
    CC: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    CC: Catalin Marinas <catalin.marinas@arm.com>
    CC: Will Deacon <will.deacon@arm.com>
    CC: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Suravee Suthikulpanit <Suravee.Suthikulpanit@amd.com>
    Tested-by: Yijing Wang <wangyijing@huawei.com>
    Tested-by: Mark Langsdorf <mlangsdo@redhat.com>
    Tested-by: Jon Masters <jcm@redhat.com>
    Tested-by: Timur Tabi <timur@codeaurora.org>
    Tested-by: Robert Richter <rrichter@cavium.com>
    Acked-by: Robert Richter <rrichter@cavium.com>
    Acked-by: Olof Johansson <olof@lixom.net>
    Reviewed-by: Grant Likely <grant.likely@linaro.org>
    Signed-off-by: Hanjun Guo <hanjun.guo@linaro.org>
    Signed-off-by: Tomasz Nowicki <tomasz.nowicki@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 328b8ce4b007..52998b7a89b5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -322,7 +322,7 @@ void __init smp_prepare_boot_cpu(void)
  * cpu logical map array containing MPIDR values related to logical
  * cpus. Assumes that cpu_logical_map(0) has already been initialized.
  */
-void __init smp_init_cpus(void)
+void __init of_smp_init_cpus(void)
 {
 	struct device_node *dn = NULL;
 	unsigned int i, cpu = 1;

commit dd006da21646f1c86f0242eb8f527d093303127a
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Mar 19 16:42:27 2015 +0000

    arm64: mm: increase VA range of identity map
    
    The page size and the number of translation levels, and hence the supported
    virtual address range, are build-time configurables on arm64 whose optimal
    values are use case dependent. However, in the current implementation, if
    the system's RAM is located at a very high offset, the virtual address range
    needs to reflect that merely because the identity mapping, which is only used
    to enable or disable the MMU, requires the extended virtual range to map the
    physical memory at an equal virtual offset.
    
    This patch relaxes that requirement, by increasing the number of translation
    levels for the identity mapping only, and only when actually needed, i.e.,
    when system RAM's offset is found to be out of reach at runtime.
    
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 4257369341e4..ffe8e1b814e0 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -151,6 +151,7 @@ asmlinkage void secondary_start_kernel(void)
 	 */
 	cpu_set_reserved_ttbr0();
 	flush_tlb_all();
+	cpu_set_default_tcr_t0sz();
 
 	preempt_disable();
 	trace_hardirqs_off();

commit 137650aad96c9594683445e41afa8ac5a2097520
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Mar 13 16:14:34 2015 +0000

    arm64: apply alternatives for !SMP kernels
    
    Currently we only perform alternative patching for kernels built with
    CONFIG_SMP, as we call apply_alternatives_all() in smp.c, which is only
    built for CONFIG_SMP. Thus !SMP kernels may not have necessary
    alternatives patched in.
    
    This patch ensures that we call apply_alternatives_all() once all CPUs
    are booted, even for !SMP kernels, by having the smp_init_cpus() stub
    call this for !SMP kernels via up_late_init. A new wrapper,
    do_post_cpus_up_work, is added so we can hook other calls here later
    (e.g. boot mode logging).
    
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Fixes: e039ee4ee3fcf174 ("arm64: add alternative runtime patching")
    Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 328b8ce4b007..4257369341e4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -309,7 +309,7 @@ void cpu_die(void)
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());
-	apply_alternatives_all();
+	do_post_cpus_up_work();
 }
 
 void __init smp_prepare_boot_cpu(void)

commit 434ed7f4b08fb8c23bdfabbb1f9427f2ed9ec27c
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Mar 5 10:49:18 2015 +1030

    arm64: fix up obsolete cpu function usage.
    
    Thanks to spatch.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 328b8ce4b007..ccf734ad4961 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -635,7 +635,7 @@ void smp_send_stop(void)
 		cpumask_t mask;
 
 		cpumask_copy(&mask, cpu_online_mask);
-		cpu_clear(smp_processor_id(), mask);
+		cpumask_clear_cpu(smp_processor_id(), &mask);
 
 		smp_cross_call(&mask, IPI_CPU_STOP);
 	}

commit 0aaf0dae81b586134faeb52e28b7ad567629dd68
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Fri Jan 23 05:36:42 2015 +0000

    smp, ARM64: Kill SMP single function call interrupt
    
    Commit 9a46ad6d6df3b54 "smp: make smp_call_function_many() use logic
    similar to smp_call_function_single()" has unified the way to handle
    single and multiple cross-CPU function calls. Now only one interrupt
    is needed for architecture specific code to support generic SMP function
    call interfaces, so kill the redundant single function call interrupt.
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 7ae6ee085261..328b8ce4b007 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -65,7 +65,6 @@ struct secondary_data secondary_data;
 enum ipi_msg_type {
 	IPI_RESCHEDULE,
 	IPI_CALL_FUNC,
-	IPI_CALL_FUNC_SINGLE,
 	IPI_CPU_STOP,
 	IPI_TIMER,
 	IPI_IRQ_WORK,
@@ -483,7 +482,6 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
 #define S(x,s)	[x] = s
 	S(IPI_RESCHEDULE, "Rescheduling interrupts"),
 	S(IPI_CALL_FUNC, "Function call interrupts"),
-	S(IPI_CALL_FUNC_SINGLE, "Single function call interrupts"),
 	S(IPI_CPU_STOP, "CPU stop interrupts"),
 	S(IPI_TIMER, "Timer broadcast interrupts"),
 	S(IPI_IRQ_WORK, "IRQ work interrupts"),
@@ -527,7 +525,7 @@ void arch_send_call_function_ipi_mask(const struct cpumask *mask)
 
 void arch_send_call_function_single_ipi(int cpu)
 {
-	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC_SINGLE);
+	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC);
 }
 
 #ifdef CONFIG_IRQ_WORK
@@ -585,12 +583,6 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		irq_exit();
 		break;
 
-	case IPI_CALL_FUNC_SINGLE:
-		irq_enter();
-		generic_smp_call_function_single_interrupt();
-		irq_exit();
-		break;
-
 	case IPI_CPU_STOP:
 		irq_enter();
 		ipi_cpu_stop(cpu);

commit 932ded4b0b9bf111fbf9d176ec12152a0d29b0fd
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 28 13:40:45 2014 +0000

    arm64: add module support for alternatives fixups
    
    Currently the kernel patches all necessary instructions once at boot
    time, so modules are not covered by this.
    Change the apply_alternatives() function to take a beginning and an
    end pointer and introduce a new variant (apply_alternatives_all()) to
    cover the existing use case for the static kernel image section.
    Add a module_finalize() function to arm64 to check for an
    alternatives section in a module and patch only the instructions from
    that specific area.
    Since that module code is not touched before the module
    initialization has ended, we don't need to halt the machine before
    doing the patching in the module's code.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 0ef87896e4ae..7ae6ee085261 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -310,7 +310,7 @@ void cpu_die(void)
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());
-	apply_alternatives();
+	apply_alternatives_all();
 }
 
 void __init smp_prepare_boot_cpu(void)

commit e039ee4ee3fcf174736f2cb0a2eed6cb908348a6
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Fri Nov 14 15:54:08 2014 +0000

    arm64: add alternative runtime patching
    
    With a blatant copy of some x86 bits we introduce the alternative
    runtime patching "framework" to arm64.
    This is quite basic for now and we only provide the functions we need
    at this time.
    This is connected to the newly introduced feature bits.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b06d1d90ee8c..0ef87896e4ae 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -37,6 +37,7 @@
 #include <linux/of.h>
 #include <linux/irq_work.h>
 
+#include <asm/alternative.h>
 #include <asm/atomic.h>
 #include <asm/cacheflush.h>
 #include <asm/cpu.h>
@@ -309,6 +310,7 @@ void cpu_die(void)
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());
+	apply_alternatives();
 }
 
 void __init smp_prepare_boot_cpu(void)

commit 3631073659d0aafeaa52227bb61a100efaf901dc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Aug 16 18:48:05 2014 +0200

    arm64: Tell irq work about self IPI support
    
    ARM64 irq work self-IPI support depends on __smp_cross_call to point to
    some relevant IRQ controller operations. This information should be
    available after the call to init_IRQ().
    
    Lets implement arch_irq_work_has_interrupt() accordingly.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 474339718105..b06d1d90ee8c 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -470,7 +470,7 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	}
 }
 
-static void (*__smp_cross_call)(const struct cpumask *, unsigned int);
+void (*__smp_cross_call)(const struct cpumask *, unsigned int);
 
 void __init set_smp_cross_call(void (*fn)(const struct cpumask *, unsigned int))
 {

commit c23190c0bf1236e1eb5521a8b10d0102fbc1338c
Merge: fc335c1b68c6 45ed695ac10a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Aug 9 17:33:44 2014 -0700

    Merge tag 'trace-ipi-tracepoints' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull IPI tracepoints for ARM from Steven Rostedt:
     "Nicolas Pitre added generic tracepoints for tracing IPIs and updated
      the arm and arm64 architectures.  It required some minor updates to
      the generic tracepoint system, so it had to wait for me to implement
      them"
    
    * tag 'trace-ipi-tracepoints' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ARM64: add IPI tracepoints
      ARM: add IPI tracepoints
      tracepoint: add generic tracepoint definitions for IPI tracing
      tracing: Do not do anything special with tracepoint_string when tracing is disabled

commit 45ed695ac10a23cb4e60a3e0b68b3f21a8670670
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Jul 25 16:05:32 2014 -0400

    ARM64: add IPI tracepoints
    
    The strings used to list IPIs in /proc/interrupts are reused for tracing
    purposes.
    
    While at it, the code is slightly cleaned up so the ipi_types array
    indices are no longer offset by IPI_RESCHEDULE whose value is 0 anyway.
    
    Link: http://lkml.kernel.org/p/1406318733-26754-5-git-send-email-nicolas.pitre@linaro.org
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 40f38f46c8e0..a89c66f3b4c5 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -50,6 +50,9 @@
 #include <asm/tlbflush.h>
 #include <asm/ptrace.h>
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/ipi.h>
+
 /*
  * as from 2.5, kernels no longer have an init_tasks structure
  * so we need some other way of telling a new secondary core
@@ -307,8 +310,6 @@ void __init smp_prepare_boot_cpu(void)
 	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 
-static void (*smp_cross_call)(const struct cpumask *, unsigned int);
-
 /*
  * Enumerate the possible CPU set from the device tree and build the
  * cpu logical map array containing MPIDR values related to logical
@@ -463,32 +464,15 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	}
 }
 
+static void (*__smp_cross_call)(const struct cpumask *, unsigned int);
 
 void __init set_smp_cross_call(void (*fn)(const struct cpumask *, unsigned int))
 {
-	smp_cross_call = fn;
+	__smp_cross_call = fn;
 }
 
-void arch_send_call_function_ipi_mask(const struct cpumask *mask)
-{
-	smp_cross_call(mask, IPI_CALL_FUNC);
-}
-
-void arch_send_call_function_single_ipi(int cpu)
-{
-	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC_SINGLE);
-}
-
-#ifdef CONFIG_IRQ_WORK
-void arch_irq_work_raise(void)
-{
-	if (smp_cross_call)
-		smp_cross_call(cpumask_of(smp_processor_id()), IPI_IRQ_WORK);
-}
-#endif
-
-static const char *ipi_types[NR_IPI] = {
-#define S(x,s)	[x - IPI_RESCHEDULE] = s
+static const char *ipi_types[NR_IPI] __tracepoint_string = {
+#define S(x,s)	[x] = s
 	S(IPI_RESCHEDULE, "Rescheduling interrupts"),
 	S(IPI_CALL_FUNC, "Function call interrupts"),
 	S(IPI_CALL_FUNC_SINGLE, "Single function call interrupts"),
@@ -497,12 +481,18 @@ static const char *ipi_types[NR_IPI] = {
 	S(IPI_IRQ_WORK, "IRQ work interrupts"),
 };
 
+static void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
+{
+	trace_ipi_raise(target, ipi_types[ipinr]);
+	__smp_cross_call(target, ipinr);
+}
+
 void show_ipi_list(struct seq_file *p, int prec)
 {
 	unsigned int cpu, i;
 
 	for (i = 0; i < NR_IPI; i++) {
-		seq_printf(p, "%*s%u:%s", prec - 1, "IPI", i + IPI_RESCHEDULE,
+		seq_printf(p, "%*s%u:%s", prec - 1, "IPI", i,
 			   prec >= 4 ? " " : "");
 		for_each_online_cpu(cpu)
 			seq_printf(p, "%10u ",
@@ -522,6 +512,24 @@ u64 smp_irq_stat_cpu(unsigned int cpu)
 	return sum;
 }
 
+void arch_send_call_function_ipi_mask(const struct cpumask *mask)
+{
+	smp_cross_call(mask, IPI_CALL_FUNC);
+}
+
+void arch_send_call_function_single_ipi(int cpu)
+{
+	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC_SINGLE);
+}
+
+#ifdef CONFIG_IRQ_WORK
+void arch_irq_work_raise(void)
+{
+	if (__smp_cross_call)
+		smp_cross_call(cpumask_of(smp_processor_id()), IPI_IRQ_WORK);
+}
+#endif
+
 static DEFINE_RAW_SPINLOCK(stop_lock);
 
 /*
@@ -553,8 +561,10 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 	unsigned int cpu = smp_processor_id();
 	struct pt_regs *old_regs = set_irq_regs(regs);
 
-	if (ipinr >= IPI_RESCHEDULE && ipinr < IPI_RESCHEDULE + NR_IPI)
-		__inc_irq_stat(cpu, ipi_irqs[ipinr - IPI_RESCHEDULE]);
+	if ((unsigned)ipinr < NR_IPI) {
+		trace_ipi_entry(ipi_types[ipinr]);
+		__inc_irq_stat(cpu, ipi_irqs[ipinr]);
+	}
 
 	switch (ipinr) {
 	case IPI_RESCHEDULE:
@@ -599,6 +609,9 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
 		break;
 	}
+
+	if ((unsigned)ipinr < NR_IPI)
+		trace_ipi_exit(ipi_types[ipinr]);
 	set_irq_regs(old_regs);
 }
 

commit df857416a13734ed9356f6e4f0152d55e4fb748a
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 16 16:32:44 2014 +0100

    arm64: cpuinfo: record cpu system register values
    
    Several kernel subsystems need to know details about CPU system register
    values, sometimes for CPUs other than that they are executing on. Rather
    than hard-coding system register accesses and cross-calls for these
    cases, this patch adds logic to record various system register values at
    boot-time. This may be used for feature reporting, firmware bug
    detection, etc.
    
    Separate hooks are added for the boot and hotplug paths to enable
    one-time intialisation and cold/warm boot value mismatch detection in
    later patches.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 40f38f46c8e0..3e2f5ebbf63e 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -39,6 +39,7 @@
 
 #include <asm/atomic.h>
 #include <asm/cacheflush.h>
+#include <asm/cpu.h>
 #include <asm/cputype.h>
 #include <asm/cpu_ops.h>
 #include <asm/mmu_context.h>
@@ -154,6 +155,11 @@ asmlinkage void secondary_start_kernel(void)
 	if (cpu_ops[cpu]->cpu_postboot)
 		cpu_ops[cpu]->cpu_postboot();
 
+	/*
+	 * Log the CPU info before it is marked online and might get read.
+	 */
+	cpuinfo_store_cpu();
+
 	/*
 	 * Enable GIC and timers.
 	 */

commit cc07aabc53978ae09a1d539237189f7c9841060a
Merge: 9e47aaef0bd3 9358d755bd5c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 6 10:43:28 2014 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into next
    
    Pull arm64 updates from Catalin Marinas:
     - Optimised assembly string/memory routines (based on the AArch64
       Cortex Strings library contributed to glibc but re-licensed under
       GPLv2)
     - Optimised crypto algorithms making use of the ARMv8 crypto extensions
       (together with kernel API for using FPSIMD instructions in interrupt
       context)
     - Ftrace support
     - CPU topology parsing from DT
     - ESR_EL1 (Exception Syndrome Register) exposed to user space signal
       handlers for SIGSEGV/SIGBUS (useful to emulation tools like Qemu)
     - 1GB section linear mapping if applicable
     - Barriers usage clean-up
     - Default pgprot clean-up
    
    Conflicts as per Catalin.
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (57 commits)
      arm64: kernel: initialize broadcast hrtimer based clock event device
      arm64: ftrace: Add system call tracepoint
      arm64: ftrace: Add CALLER_ADDRx macros
      arm64: ftrace: Add dynamic ftrace support
      arm64: Add ftrace support
      ftrace: Add arm64 support to recordmcount
      arm64: Add 'notrace' attribute to unwind_frame() for ftrace
      arm64: add __ASSEMBLY__ in asm/insn.h
      arm64: Fix linker script entry point
      arm64: lib: Implement optimized string length routines
      arm64: lib: Implement optimized string compare routines
      arm64: lib: Implement optimized memcmp routine
      arm64: lib: Implement optimized memset routine
      arm64: lib: Implement optimized memmove routine
      arm64: lib: Implement optimized memcpy routine
      arm64: defconfig: enable a few more common/useful options in defconfig
      ftrace: Make CALLER_ADDRx macros more generic
      arm64: Fix deadlock scenario with smp_send_stop()
      arm64: Fix machine_shutdown() definition
      arm64: Support arch_irq_work_raise() via self IPIs
      ...

commit eb631bb5bf5b042202aaaee4a8dd8f863ba2a900
Author: Larry Bassel <larry.bassel@linaro.org>
Date:   Mon May 12 16:48:51 2014 +0100

    arm64: Support arch_irq_work_raise() via self IPIs
    
    Support for arch_irq_work_raise() was missing from
    arm64 (a prerequisite for FULL_NOHZ).
    
    This patch is based on the arm32 patch ARM 7872/1.
    
    commit bf18525fd793101df42a1344ecc48b49b62e48c9
    Author: Stephen Boyd <sboyd@codeaurora.org>
    Date:   Tue Oct 29 20:32:56 2013 +0100
    
        ARM: 7872/1: Support arch_irq_work_raise() via self IPIs
    
        By default, IRQ work is run from the tick interrupt (see
        irq_work_run() in update_process_times()). When we're in full
        NOHZ mode, restarting the tick requires the use of IRQ work and
        if the only place we run IRQ work is in the tick interrupt we
        have an unbreakable cycle. Implement arch_irq_work_raise() via
        self IPIs to break this cycle and get the tick started again.
        Note that we implement this via IPIs which are only available on
        SMP builds. This shouldn't be a problem because full NOHZ is only
        supported on SMP builds anyway.
    
        Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
        Reviewed-by: Kevin Hilman <khilman@linaro.org>
        Cc: Frederic Weisbecker <fweisbec@gmail.com>
        Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Signed-off-by: Larry Bassel <larry.bassel@linaro.org>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index f0a141dd5655..049aa8d5c553 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -35,6 +35,7 @@
 #include <linux/clockchips.h>
 #include <linux/completion.h>
 #include <linux/of.h>
+#include <linux/irq_work.h>
 
 #include <asm/atomic.h>
 #include <asm/cacheflush.h>
@@ -62,6 +63,7 @@ enum ipi_msg_type {
 	IPI_CALL_FUNC_SINGLE,
 	IPI_CPU_STOP,
 	IPI_TIMER,
+	IPI_IRQ_WORK,
 };
 
 /*
@@ -455,6 +457,14 @@ void arch_send_call_function_single_ipi(int cpu)
 	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC_SINGLE);
 }
 
+#ifdef CONFIG_IRQ_WORK
+void arch_irq_work_raise(void)
+{
+	if (smp_cross_call)
+		smp_cross_call(cpumask_of(smp_processor_id()), IPI_IRQ_WORK);
+}
+#endif
+
 static const char *ipi_types[NR_IPI] = {
 #define S(x,s)	[x - IPI_RESCHEDULE] = s
 	S(IPI_RESCHEDULE, "Rescheduling interrupts"),
@@ -462,6 +472,7 @@ static const char *ipi_types[NR_IPI] = {
 	S(IPI_CALL_FUNC_SINGLE, "Single function call interrupts"),
 	S(IPI_CPU_STOP, "CPU stop interrupts"),
 	S(IPI_TIMER, "Timer broadcast interrupts"),
+	S(IPI_IRQ_WORK, "IRQ work interrupts"),
 };
 
 void show_ipi_list(struct seq_file *p, int prec)
@@ -554,6 +565,14 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		break;
 #endif
 
+#ifdef CONFIG_IRQ_WORK
+	case IPI_IRQ_WORK:
+		irq_enter();
+		irq_work_run();
+		irq_exit();
+		break;
+#endif
+
 	default:
 		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
 		break;

commit c814ca029e1015bb0ecec312f4bb9751ba1a711a
Author: Ashwin Chaugule <ashwin.chaugule@linaro.org>
Date:   Wed May 7 10:18:36 2014 -0400

    ARM: Check if a CPU has gone offline
    
    PSCIv0.2 adds a new function called AFFINITY_INFO, which
    can be used to query if a specified CPU has actually gone
    offline. Calling this function via cpu_kill ensures that
    a CPU has quiesced after a call to cpu_die. This helps
    prevent the CPU from doing arbitrary bad things when data
    or instructions are clobbered (as happens with kexec)
    in the window between a CPU announcing that it is dead
    and said CPU leaving the kernel.
    
    Signed-off-by: Ashwin Chaugule <ashwin.chaugule@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Rob Herring <robh@kernel.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index f0a141dd5655..c3cb160edc69 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -228,6 +228,19 @@ int __cpu_disable(void)
 	return 0;
 }
 
+static int op_cpu_kill(unsigned int cpu)
+{
+	/*
+	 * If we have no means of synchronising with the dying CPU, then assume
+	 * that it is really dead. We can only wait for an arbitrary length of
+	 * time and hope that it's dead, so let's skip the wait and just hope.
+	 */
+	if (!cpu_ops[cpu]->cpu_kill)
+		return 1;
+
+	return cpu_ops[cpu]->cpu_kill(cpu);
+}
+
 static DECLARE_COMPLETION(cpu_died);
 
 /*
@@ -241,6 +254,15 @@ void __cpu_die(unsigned int cpu)
 		return;
 	}
 	pr_notice("CPU%u: shutdown\n", cpu);
+
+	/*
+	 * Now that the dying CPU is beyond the point of no return w.r.t.
+	 * in-kernel synchronisation, try to get the firwmare to help us to
+	 * verify that it has really left the kernel before we consider
+	 * clobbering anything it might still be using.
+	 */
+	if (!op_cpu_kill(cpu))
+		pr_warn("CPU%d may not have shut down cleanly\n", cpu);
 }
 
 /*

commit f6e763b93a6cd3411fd8df925344022719bcba62
Author: Mark Brown <broonie@linaro.org>
Date:   Tue Mar 4 07:51:17 2014 +0000

    arm64: topology: Implement basic CPU topology support
    
    Add basic CPU topology support to arm64, based on the existing pre-v8
    code and some work done by Mark Hambleton.  This patch does not
    implement any topology discovery support since that should be based on
    information from firmware, it merely implements the scaffolding for
    integration of topology support in the architecture.
    
    No locking of the topology data is done since it is only modified during
    CPU bringup with external serialisation from the SMP code.
    
    The goal is to separate the architecture hookup for providing topology
    information from the DT parsing in order to ease review and avoid
    blocking the architecture code (which will be built on by other work)
    with the DT code review by providing something simple and basic.
    
    Following patches will implement support for interpreting topology
    information from MPIDR and for parsing the DT topology bindings for ARM,
    similar patches will be needed for ACPI.
    
    Signed-off-by: Mark Brown <broonie@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    [catalin.marinas@arm.com: removed CONFIG_CPU_TOPOLOGY, always on if SMP]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 5070dc3b65d2..f0a141dd5655 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -114,6 +114,11 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
 	return ret;
 }
 
+static void smp_store_cpu_info(unsigned int cpuid)
+{
+	store_cpu_topology(cpuid);
+}
+
 /*
  * This is the secondary CPU boot entry.  We're using this CPUs
  * idle thread stack, but a set of temporary page tables.
@@ -152,6 +157,8 @@ asmlinkage void secondary_start_kernel(void)
 	 */
 	notify_cpu_starting(cpu);
 
+	smp_store_cpu_info(cpu);
+
 	/*
 	 * OK, now it's safe to let the boot CPU continue.  Wait for
 	 * the CPU migration code to notice that the CPU is online
@@ -391,6 +398,10 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	int err;
 	unsigned int cpu, ncores = num_possible_cpus();
 
+	init_cpu_topology();
+
+	smp_store_cpu_info(smp_processor_id());
+
 	/*
 	 * are we trying to boot more cores than exist?
 	 */

commit d8ed442a009ecfe155b57d58f231db3d6084633d
Author: Vijaya Kumar K <Vijaya.Kumar@caviumnetworks.com>
Date:   Fri Feb 21 05:13:49 2014 +0000

    arm64: enable processor debug state for secondary cpus
    
    processor debug state PSTATE.D is unmasked in smp call
    clear_os_lock for secondary cpus. So debug state is still
    masked in normal kernel context.  With this patch, unmask
    debug state on secondary boot for the cpus in normal kernel
    context. Now kgdb tests passed with multicore.
    
    Signed-off-by: Vijaya Kumar K <Vijaya.Kumar@caviumnetworks.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 7cfb92a4ab66..5070dc3b65d2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -160,6 +160,7 @@ asmlinkage void secondary_start_kernel(void)
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 
+	local_dbg_enable();
 	local_irq_enable();
 	local_async_enable();
 

commit f864b61ee49bbf3faf9a10b9770c719536328d01
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jan 29 18:00:45 2014 +0000

    arm64: FIQs are unused
    
    So any FIQ handling is superfluous at the moment.  The functions to
    disable/enable FIQs is kept around if ever someone needs them in the
    future, but existing calling sites including arch_cpu_idle_prepare()
    may go for now.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 1b7617ab499b..7cfb92a4ab66 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -161,7 +161,6 @@ asmlinkage void secondary_start_kernel(void)
 	complete(&cpu_running);
 
 	local_irq_enable();
-	local_fiq_enable();
 	local_async_enable();
 
 	/*
@@ -495,7 +494,6 @@ static void ipi_cpu_stop(unsigned int cpu)
 
 	set_cpu_online(cpu, false);
 
-	local_fiq_disable();
 	local_irq_disable();
 
 	while (1)

commit 0a5be743e8c3c3230600fbc0cf923fb5dbefd579
Merge: 6ac2104debc2 1307220d7bb7
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Dec 19 17:57:51 2013 +0000

    Merge tag 'arm64-suspend' of git://linux-arm.org/linux-2.6-lp into upstream
    
    * tag 'arm64-suspend' of git://linux-arm.org/linux-2.6-lp:
      arm64: add CPU power management menu/entries
      arm64: kernel: add PM build infrastructure
      arm64: kernel: add CPU idle call
      arm64: enable generic clockevent broadcast
      arm64: kernel: implement HW breakpoints CPU PM notifier
      arm64: kernel: refactor code to install/uninstall breakpoints
      arm: kvm: implement CPU PM notifier
      arm64: kernel: implement fpsimd CPU PM notifier
      arm64: kernel: cpu_{suspend/resume} implementation
      arm64: kernel: suspend/resume registers save/restore
      arm64: kernel: build MPIDR_EL1 hash function data structure
      arm64: kernel: add MPIDR_EL1 accessors macros
    
    Conflicts:
            arch/arm64/Kconfig

commit 7158627686f02319c50c8d9d78f75d4c8d126ff2
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 5 18:10:47 2013 +0000

    arm64: percpu: implement optimised pcpu access using tpidr_el1
    
    This patch implements optimised percpu variable accesses using the
    el1 r/w thread register (tpidr_el1) along the same lines as arch/arm/.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a0c2ca602cf8..b5d2031c12c6 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -122,8 +122,6 @@ asmlinkage void secondary_start_kernel(void)
 	struct mm_struct *mm = &init_mm;
 	unsigned int cpu = smp_processor_id();
 
-	printk("CPU%u: Booted secondary processor\n", cpu);
-
 	/*
 	 * All kernel threads share the same mm context; grab a
 	 * reference and switch to it.
@@ -132,6 +130,9 @@ asmlinkage void secondary_start_kernel(void)
 	current->active_mm = mm;
 	cpumask_set_cpu(cpu, mm_cpumask(mm));
 
+	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
+	printk("CPU%u: Booted secondary processor\n", cpu);
+
 	/*
 	 * TTBR0 is only used for the identity mapping at this stage. Make it
 	 * point to zero page to avoid speculatively fetching new entries.
@@ -271,6 +272,7 @@ void __init smp_cpus_done(unsigned int max_cpus)
 
 void __init smp_prepare_boot_cpu(void)
 {
+	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
 }
 
 static void (*smp_cross_call)(const struct cpumask *, unsigned int);

commit 1f85008e74768a88e1ddb96cc1fe45bb2378166c
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed Sep 4 10:55:17 2013 +0100

    arm64: enable generic clockevent broadcast
    
    On platforms with power management capabilities, timers that are shut
    down when a CPU enters deep C-states must be emulated using an always-on
    timer and a timer IPI to relay the timer IRQ to target CPUs on an SMP
    system.
    
    This patch enables the generic clockevents broadcast infrastructure for
    arm64, by providing the required Kconfig entries and adding the timer
    IPI infrastructure.
    
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a0c2ca602cf8..0b8c859e744f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -61,6 +61,7 @@ enum ipi_msg_type {
 	IPI_CALL_FUNC,
 	IPI_CALL_FUNC_SINGLE,
 	IPI_CPU_STOP,
+	IPI_TIMER,
 };
 
 /*
@@ -447,6 +448,7 @@ static const char *ipi_types[NR_IPI] = {
 	S(IPI_CALL_FUNC, "Function call interrupts"),
 	S(IPI_CALL_FUNC_SINGLE, "Single function call interrupts"),
 	S(IPI_CPU_STOP, "CPU stop interrupts"),
+	S(IPI_TIMER, "Timer broadcast interrupts"),
 };
 
 void show_ipi_list(struct seq_file *p, int prec)
@@ -532,6 +534,14 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
 		irq_exit();
 		break;
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+	case IPI_TIMER:
+		irq_enter();
+		tick_receive_broadcast();
+		irq_exit();
+		break;
+#endif
+
 	default:
 		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
 		break;
@@ -544,6 +554,13 @@ void smp_send_reschedule(int cpu)
 	smp_cross_call(cpumask_of(cpu), IPI_RESCHEDULE);
 }
 
+#ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+void tick_broadcast(const struct cpumask *mask)
+{
+	smp_cross_call(mask, IPI_TIMER);
+}
+#endif
+
 void smp_send_stop(void)
 {
 	unsigned long timeout;

commit b3bf6aa7e79117419f7eddccf0b7af4382d823c3
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 21 14:46:17 2013 +0000

    arm64: Unmask asynchronous aborts when in kernel mode
    
    The asynchronous aborts are generally fatal for the kernel but they can
    be masked via the pstate A bit. If a system error happens while in
    kernel mode, it won't be visible until returning to user space. This
    patch enables this kind of abort early to help identifying the cause.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a5aeefab03c3..a0c2ca602cf8 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -160,6 +160,7 @@ asmlinkage void secondary_start_kernel(void)
 
 	local_irq_enable();
 	local_fiq_enable();
+	local_async_enable();
 
 	/*
 	 * OK, it's off to the idle thread for us

commit 67317c2689567c24d18e0dd43ab6d409fd42dc6e
Author: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
Date:   Thu Nov 7 15:25:44 2013 +0000

    ARM64: /proc/interrupts: display IPIs of online CPUs only
    
    The non-IPI interrupts are displayed only for the online cpus from
    show_interrupts in kernel/irq/proc.c before calling arch_show_interrupts().
    As a result, the column headers and the IPI count don't match if any
    CPU is offline.
    
    This patch fixes show_ipi_list to display IPIs for online CPUs only.
    
    Signed-off-by: Sudeep KarkadaNagesha <sudeep.karkadanagesha@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 3abb9e797dd4..a5aeefab03c3 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -455,7 +455,7 @@ void show_ipi_list(struct seq_file *p, int prec)
 	for (i = 0; i < NR_IPI; i++) {
 		seq_printf(p, "%*s%u:%s", prec - 1, "IPI", i + IPI_RESCHEDULE,
 			   prec >= 4 ? " " : "");
-		for_each_present_cpu(cpu)
+		for_each_online_cpu(cpu)
 			seq_printf(p, "%10u ",
 				   __get_irq_stat(cpu, ipi_irqs[i]));
 		seq_printf(p, "      %s\n", ipi_types[i]);

commit 7ade67b5984d0a0434462fda733ab5138c63aae1
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Mon Nov 4 16:55:22 2013 +0000

    arm64: move enabling of GIC before CPUs are set online
    
    Commit 53ae3acd (arm64: Only enable local interrupts after the CPU
    is marked online) moved the enabling of the GIC after the CPUs are
    marked online.
    
    This has some interesting effect:
    [...]
    [<ffffffc0002eefd8>] gic_raise_softirq+0xf8/0x160
    [<ffffffc000088f58>] smp_send_reschedule+0x38/0x40
    [<ffffffc0000c8728>] resched_task+0x84/0xc0
    [<ffffffc0000c8cdc>] check_preempt_curr+0x58/0x98
    [<ffffffc0000c8d38>] ttwu_do_wakeup+0x1c/0xf4
    [<ffffffc0000c8f90>] ttwu_do_activate.constprop.84+0x64/0x70
    [<ffffffc0000cad30>] try_to_wake_up+0x1d4/0x2b4
    [<ffffffc0000cae6c>] default_wake_function+0x10/0x18
    [<ffffffc0000c5ca4>] __wake_up_common+0x60/0xa0
    [<ffffffc0000c7784>] complete+0x48/0x64
    [<ffffffc000088bec>] secondary_start_kernel+0xe8/0x110
    [...]
    
    Here, we end-up calling gic_raise_softirq without having initialized
    the interrupt controller for this CPU. While this goes unnoticed
    with GICv2 (the distributor is always accessible), it explodes with
    GICv3.
    
    The fix is to move the call to notify_cpu_starting before we set
    the secondary CPU online.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 5b4ddbd653c3..3abb9e797dd4 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -145,6 +145,11 @@ asmlinkage void secondary_start_kernel(void)
 	if (cpu_ops[cpu]->cpu_postboot)
 		cpu_ops[cpu]->cpu_postboot();
 
+	/*
+	 * Enable GIC and timers.
+	 */
+	notify_cpu_starting(cpu);
+
 	/*
 	 * OK, now it's safe to let the boot CPU continue.  Wait for
 	 * the CPU migration code to notice that the CPU is online
@@ -153,11 +158,6 @@ asmlinkage void secondary_start_kernel(void)
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 
-	/*
-	 * Enable GIC and timers.
-	 */
-	notify_cpu_starting(cpu);
-
 	local_irq_enable();
 	local_fiq_enable();
 

commit 9327e2c6bb8cb0131b38a07847cd58c78dc095e9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:18 2013 +0100

    arm64: add CPU_HOTPLUG infrastructure
    
    This patch adds the basic infrastructure necessary to support
    CPU_HOTPLUG on arm64, based on the arm implementation. Actual hotplug
    support will depend on an implementation's cpu_operations (e.g. PSCI).
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index e992734d21ec..5b4ddbd653c3 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -167,6 +167,102 @@ asmlinkage void secondary_start_kernel(void)
 	cpu_startup_entry(CPUHP_ONLINE);
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+static int op_cpu_disable(unsigned int cpu)
+{
+	/*
+	 * If we don't have a cpu_die method, abort before we reach the point
+	 * of no return. CPU0 may not have an cpu_ops, so test for it.
+	 */
+	if (!cpu_ops[cpu] || !cpu_ops[cpu]->cpu_die)
+		return -EOPNOTSUPP;
+
+	/*
+	 * We may need to abort a hot unplug for some other mechanism-specific
+	 * reason.
+	 */
+	if (cpu_ops[cpu]->cpu_disable)
+		return cpu_ops[cpu]->cpu_disable(cpu);
+
+	return 0;
+}
+
+/*
+ * __cpu_disable runs on the processor to be shutdown.
+ */
+int __cpu_disable(void)
+{
+	unsigned int cpu = smp_processor_id();
+	int ret;
+
+	ret = op_cpu_disable(cpu);
+	if (ret)
+		return ret;
+
+	/*
+	 * Take this CPU offline.  Once we clear this, we can't return,
+	 * and we must not schedule until we're ready to give up the cpu.
+	 */
+	set_cpu_online(cpu, false);
+
+	/*
+	 * OK - migrate IRQs away from this CPU
+	 */
+	migrate_irqs();
+
+	/*
+	 * Remove this CPU from the vm mask set of all processes.
+	 */
+	clear_tasks_mm_cpumask(cpu);
+
+	return 0;
+}
+
+static DECLARE_COMPLETION(cpu_died);
+
+/*
+ * called on the thread which is asking for a CPU to be shutdown -
+ * waits until shutdown has completed, or it is timed out.
+ */
+void __cpu_die(unsigned int cpu)
+{
+	if (!wait_for_completion_timeout(&cpu_died, msecs_to_jiffies(5000))) {
+		pr_crit("CPU%u: cpu didn't die\n", cpu);
+		return;
+	}
+	pr_notice("CPU%u: shutdown\n", cpu);
+}
+
+/*
+ * Called from the idle thread for the CPU which has been shutdown.
+ *
+ * Note that we disable IRQs here, but do not re-enable them
+ * before returning to the caller. This is also the behaviour
+ * of the other hotplug-cpu capable cores, so presumably coming
+ * out of idle fixes this.
+ */
+void cpu_die(void)
+{
+	unsigned int cpu = smp_processor_id();
+
+	idle_task_exit();
+
+	local_irq_disable();
+
+	/* Tell __cpu_die() that this CPU is now safe to dispose of */
+	complete(&cpu_died);
+
+	/*
+	 * Actually shutdown the CPU. This must never fail. The specific hotplug
+	 * mechanism must perform all required cache maintenance to ensure that
+	 * no dirty lines are lost in the process of shutting down the CPU.
+	 */
+	cpu_ops[cpu]->cpu_die(cpu);
+
+	BUG();
+}
+#endif
+
 void __init smp_cpus_done(unsigned int max_cpus)
 {
 	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());

commit e8765b265a69c83504afc6901d6e137b1811d1f0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:17 2013 +0100

    arm64: read enable-method for CPU0
    
    With the advent of CPU_HOTPLUG, the enable-method property for CPU0 may
    tells us something useful (i.e. how to hotplug it back on), so we must
    read it along with all the enable-method for all the other CPUs.  Even
    on UP the enable-method may tell us useful information (e.g. if a core
    has some mechanism that might be usable for cpuidle), so we should
    always read it.
    
    This patch factors out the reading of the enable method, and ensures
    that CPU0's enable method is read regardless of whether the kernel is
    built with SMP support.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 6806bc40b630..e992734d21ec 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -185,7 +185,6 @@ static void (*smp_cross_call)(const struct cpumask *, unsigned int);
  */
 void __init smp_init_cpus(void)
 {
-	const char *enable_method;
 	struct device_node *dn = NULL;
 	unsigned int i, cpu = 1;
 	bool bootcpu_valid = false;
@@ -256,23 +255,8 @@ void __init smp_init_cpus(void)
 		if (cpu >= NR_CPUS)
 			goto next;
 
-		/*
-		 * We currently support only the "spin-table" enable-method.
-		 */
-		enable_method = of_get_property(dn, "enable-method", NULL);
-		if (!enable_method) {
-			pr_err("%s: missing enable-method property\n",
-				dn->full_name);
+		if (cpu_read_ops(dn, cpu) != 0)
 			goto next;
-		}
-
-		cpu_ops[cpu] = cpu_get_ops(enable_method);
-
-		if (!cpu_ops[cpu]) {
-			pr_err("%s: invalid enable-method property: %s\n",
-			       dn->full_name, enable_method);
-			goto next;
-		}
 
 		if (cpu_ops[cpu]->cpu_init(dn, cpu))
 			goto next;

commit 652af899799354049b273af897b798b8f03fdd88
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:16 2013 +0100

    arm64: factor out spin-table boot method
    
    The arm64 kernel has an internal holding pen, which is necessary for
    some systems where we can't bring CPUs online individually and must hold
    multiple CPUs in a safe area until the kernel is able to handle them.
    The current SMP infrastructure for arm64 is closely coupled to this
    holding pen, and alternative boot methods must launch CPUs into the pen,
    where they sit before they are launched into the kernel proper.
    
    With PSCI (and possibly other future boot methods), we can bring CPUs
    online individually, and need not perform the secondary_holding_pen
    dance. Instead, this patch factors the holding pen management code out
    to the spin-table boot method code, as it is the only boot method
    requiring the pen.
    
    A new entry point for secondaries, secondary_entry is added for other
    boot methods to use, which bypasses the holding pen and its associated
    overhead when bringing CPUs online. The smp.pen.text section is also
    removed, as the pen can live in head.text without problem.
    
    The cpu_operations structure is extended with two new functions,
    cpu_boot and cpu_postboot, for bringing a cpu into the kernel and
    performing any post-boot cleanup required by a bootmethod (e.g.
    resetting the secondary_holding_pen_release to INVALID_HWID).
    Documentation is added for cpu_operations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 8965fb7dee89..6806bc40b630 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -55,7 +55,6 @@
  * where to place its SVC stack
  */
 struct secondary_data secondary_data;
-volatile unsigned long secondary_holding_pen_release = INVALID_HWID;
 
 enum ipi_msg_type {
 	IPI_RESCHEDULE,
@@ -64,61 +63,16 @@ enum ipi_msg_type {
 	IPI_CPU_STOP,
 };
 
-static DEFINE_RAW_SPINLOCK(boot_lock);
-
-/*
- * Write secondary_holding_pen_release in a way that is guaranteed to be
- * visible to all observers, irrespective of whether they're taking part
- * in coherency or not.  This is necessary for the hotplug code to work
- * reliably.
- */
-static void write_pen_release(u64 val)
-{
-	void *start = (void *)&secondary_holding_pen_release;
-	unsigned long size = sizeof(secondary_holding_pen_release);
-
-	secondary_holding_pen_release = val;
-	__flush_dcache_area(start, size);
-}
-
 /*
  * Boot a secondary CPU, and assign it the specified idle task.
  * This also gives us the initial stack to use for this CPU.
  */
 static int boot_secondary(unsigned int cpu, struct task_struct *idle)
 {
-	unsigned long timeout;
-
-	/*
-	 * Set synchronisation state between this boot processor
-	 * and the secondary one
-	 */
-	raw_spin_lock(&boot_lock);
-
-	/*
-	 * Update the pen release flag.
-	 */
-	write_pen_release(cpu_logical_map(cpu));
+	if (cpu_ops[cpu]->cpu_boot)
+		return cpu_ops[cpu]->cpu_boot(cpu);
 
-	/*
-	 * Send an event, causing the secondaries to read pen_release.
-	 */
-	sev();
-
-	timeout = jiffies + (1 * HZ);
-	while (time_before(jiffies, timeout)) {
-		if (secondary_holding_pen_release == INVALID_HWID)
-			break;
-		udelay(10);
-	}
-
-	/*
-	 * Now the secondary core is starting up let it run its
-	 * calibrations, then wait for it to finish
-	 */
-	raw_spin_unlock(&boot_lock);
-
-	return secondary_holding_pen_release != INVALID_HWID ? -ENOSYS : 0;
+	return -EOPNOTSUPP;
 }
 
 static DECLARE_COMPLETION(cpu_running);
@@ -188,17 +142,8 @@ asmlinkage void secondary_start_kernel(void)
 	preempt_disable();
 	trace_hardirqs_off();
 
-	/*
-	 * Let the primary processor know we're out of the
-	 * pen, then head off into the C entry point
-	 */
-	write_pen_release(INVALID_HWID);
-
-	/*
-	 * Synchronise with the boot thread.
-	 */
-	raw_spin_lock(&boot_lock);
-	raw_spin_unlock(&boot_lock);
+	if (cpu_ops[cpu]->cpu_postboot)
+		cpu_ops[cpu]->cpu_postboot();
 
 	/*
 	 * OK, now it's safe to let the boot CPU continue.  Wait for

commit cd1aebf5277a3a154a9e4c0ea4b3acabb62e5cab
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:15 2013 +0100

    arm64: reorganise smp_enable_ops
    
    For hotplug support, we're going to want a place to store operations
    that do more than bring CPUs online, and it makes sense to group these
    with our current smp_enable_ops. For cpuidle support, we'll want to
    group additional functions, and we may want them even for UP kernels.
    
    This patch renames smp_enable_ops to the more general cpu_operations,
    and pulls the definitions out of smp code such that they can be used in
    UP kernels. While we're at it, fix up instances of the cpu parameter to
    be an unsigned int, drop the init markings and rename the *_cpu
    functions to cpu_* to reduce future churn when cpu_operations is
    extended.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 78db90dcc910..8965fb7dee89 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -39,6 +39,7 @@
 #include <asm/atomic.h>
 #include <asm/cacheflush.h>
 #include <asm/cputype.h>
+#include <asm/cpu_ops.h>
 #include <asm/mmu_context.h>
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -232,28 +233,6 @@ void __init smp_prepare_boot_cpu(void)
 
 static void (*smp_cross_call)(const struct cpumask *, unsigned int);
 
-static const struct smp_enable_ops *enable_ops[] __initconst = {
-	&smp_spin_table_ops,
-	&smp_psci_ops,
-	NULL,
-};
-
-static const struct smp_enable_ops *smp_enable_ops[NR_CPUS];
-
-static const struct smp_enable_ops * __init smp_get_enable_ops(const char *name)
-{
-	const struct smp_enable_ops **ops = enable_ops;
-
-	while (*ops) {
-		if (!strcmp(name, (*ops)->name))
-			return *ops;
-
-		ops++;
-	}
-
-	return NULL;
-}
-
 /*
  * Enumerate the possible CPU set from the device tree and build the
  * cpu logical map array containing MPIDR values related to logical
@@ -263,7 +242,7 @@ void __init smp_init_cpus(void)
 {
 	const char *enable_method;
 	struct device_node *dn = NULL;
-	int i, cpu = 1;
+	unsigned int i, cpu = 1;
 	bool bootcpu_valid = false;
 
 	while ((dn = of_find_node_by_type(dn, "cpu"))) {
@@ -342,15 +321,15 @@ void __init smp_init_cpus(void)
 			goto next;
 		}
 
-		smp_enable_ops[cpu] = smp_get_enable_ops(enable_method);
+		cpu_ops[cpu] = cpu_get_ops(enable_method);
 
-		if (!smp_enable_ops[cpu]) {
+		if (!cpu_ops[cpu]) {
 			pr_err("%s: invalid enable-method property: %s\n",
 			       dn->full_name, enable_method);
 			goto next;
 		}
 
-		if (smp_enable_ops[cpu]->init_cpu(dn, cpu))
+		if (cpu_ops[cpu]->cpu_init(dn, cpu))
 			goto next;
 
 		pr_debug("cpu logical map 0x%llx\n", hwid);
@@ -380,8 +359,8 @@ void __init smp_init_cpus(void)
 
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
-	int cpu, err;
-	unsigned int ncores = num_possible_cpus();
+	int err;
+	unsigned int cpu, ncores = num_possible_cpus();
 
 	/*
 	 * are we trying to boot more cores than exist?
@@ -408,10 +387,10 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 		if (cpu == smp_processor_id())
 			continue;
 
-		if (!smp_enable_ops[cpu])
+		if (!cpu_ops[cpu])
 			continue;
 
-		err = smp_enable_ops[cpu]->prepare_cpu(cpu);
+		err = cpu_ops[cpu]->cpu_prepare(cpu);
 		if (err)
 			continue;
 

commit 326b16db9f69fd0d279be873c6c00f88c0a4aad5
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Aug 30 18:06:48 2013 +0100

    arm64: delay: don't bother reporting bogomips in /proc/cpuinfo
    
    We always use a timer-backed delay loop for arm64, so don't bother
    reporting a bogomips value which appears to confuse some people.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index fee5cce83450..78db90dcc910 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -223,11 +223,7 @@ asmlinkage void secondary_start_kernel(void)
 
 void __init smp_cpus_done(unsigned int max_cpus)
 {
-	unsigned long bogosum = loops_per_jiffy * num_online_cpus();
-
-	pr_info("SMP: Total of %d processors activated (%lu.%02lu BogoMIPS).\n",
-		num_online_cpus(), bogosum / (500000/HZ),
-		(bogosum / (5000/HZ)) % 100);
+	pr_info("SMP: Total of %d processors activated.\n", num_online_cpus());
 }
 
 void __init smp_prepare_boot_cpu(void)

commit 89d0abe3d695103505c025dde6e07b9c3dd772f4
Merge: 89a8c5940d5c ff701306cd49
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 19 15:08:53 2013 -0700

    Merge tag 'arm64-stable' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull arm64 fixes from Catalin Marinas:
     - Post -rc1 update to the common reboot infrastructure.
     - Fixes (user cache maintenance fault handling, !COMPAT compilation,
       CPU online and interrupt hanlding).
    
    * tag 'arm64-stable' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64:
      arm64: use common reboot infrastructure
      arm64: mm: don't treat user cache maintenance faults as writes
      arm64: add '#ifdef CONFIG_COMPAT' for aarch32_break_handler()
      arm64: Only enable local interrupts after the CPU is marked online

commit 53ae3acd4390ffeecb3a11dbd5be347b5a3d98f2
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 19 15:08:15 2013 +0100

    arm64: Only enable local interrupts after the CPU is marked online
    
    There is a slight chance that (timer) interrupts are triggered before a
    secondary CPU has been marked online with implications on softirq thread
    affinity.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Kirill Tkhai <tkhai@yandex.ru>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 5d54e3717bf8..9c93e126328c 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -199,13 +199,6 @@ asmlinkage void __cpuinit secondary_start_kernel(void)
 	raw_spin_lock(&boot_lock);
 	raw_spin_unlock(&boot_lock);
 
-	/*
-	 * Enable local interrupts.
-	 */
-	notify_cpu_starting(cpu);
-	local_irq_enable();
-	local_fiq_enable();
-
 	/*
 	 * OK, now it's safe to let the boot CPU continue.  Wait for
 	 * the CPU migration code to notice that the CPU is online
@@ -214,6 +207,14 @@ asmlinkage void __cpuinit secondary_start_kernel(void)
 	set_cpu_online(cpu, true);
 	complete(&cpu_running);
 
+	/*
+	 * Enable GIC and timers.
+	 */
+	notify_cpu_starting(cpu);
+
+	local_irq_enable();
+	local_fiq_enable();
+
 	/*
 	 * OK, it's off to the idle thread for us
 	 */

commit b8c6453aaf142620c2e1a4c2da24bbb10cb424bf
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Jun 18 10:18:31 2013 -0400

    arm64: delete __cpuinit usage from all users
    
    The __cpuinit type of throwaway sections might have made sense
    some time ago when RAM was more constrained, but now the savings
    do not offset the cost and complications.  For example, the fix in
    commit 5e427ec2d0 ("x86: Fix bit corruption at CPU resume time")
    is a good example of the nasty type of bugs that can be created
    with improper use of the various __init prefixes.
    
    After a discussion on LKML[1] it was decided that cpuinit should go
    the way of devinit and be phased out.  Once all the users are gone,
    we can then finally remove the macros themselves from linux/init.h.
    
    Note that some harmless section mismatch warnings may result, since
    notify_cpu_starting() and cpu_up() are arch independent (kernel/cpu.c)
    are flagged as __cpuinit  -- so if we remove the __cpuinit from
    arch specific callers, we will also get section mismatch warnings.
    As an intermediate step, we intend to turn the linux/init.h cpuinit
    content into no-ops as early as possible, since that will get rid
    of these warnings.  In any case, they are temporary and harmless.
    
    This removes all the arch/arm64 uses of the __cpuinit macros from
    all C files.  Currently arm64 does not have any __CPUINIT used in
    assembly files.
    
    [1] https://lkml.org/lkml/2013/5/20/589
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 5d54e3717bf8..4a053b3d1728 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -71,7 +71,7 @@ static DEFINE_RAW_SPINLOCK(boot_lock);
  * in coherency or not.  This is necessary for the hotplug code to work
  * reliably.
  */
-static void __cpuinit write_pen_release(u64 val)
+static void write_pen_release(u64 val)
 {
 	void *start = (void *)&secondary_holding_pen_release;
 	unsigned long size = sizeof(secondary_holding_pen_release);
@@ -84,7 +84,7 @@ static void __cpuinit write_pen_release(u64 val)
  * Boot a secondary CPU, and assign it the specified idle task.
  * This also gives us the initial stack to use for this CPU.
  */
-static int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
+static int boot_secondary(unsigned int cpu, struct task_struct *idle)
 {
 	unsigned long timeout;
 
@@ -122,7 +122,7 @@ static int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
 
 static DECLARE_COMPLETION(cpu_running);
 
-int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *idle)
+int __cpu_up(unsigned int cpu, struct task_struct *idle)
 {
 	int ret;
 
@@ -162,7 +162,7 @@ int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *idle)
  * This is the secondary CPU boot entry.  We're using this CPUs
  * idle thread stack, but a set of temporary page tables.
  */
-asmlinkage void __cpuinit secondary_start_kernel(void)
+asmlinkage void secondary_start_kernel(void)
 {
 	struct mm_struct *mm = &init_mm;
 	unsigned int cpu = smp_processor_id();

commit c9ef713993ba168b38d1a97ea0ab00874f1da022
Merge: 87c1f0f8c944 16c85a1fd73e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 10:10:48 2013 -0700

    Merge tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64
    
    Pull arm64 update from Catalin Marinas:
     "Main features:
    
       - Versatile Express SoC (model) support - DT files and Kconfig
         entries (there are no arch/arm64/mach-* directories).  The bulk of
         the code has already been moved to drivers/ as part of the ARM SoC
         clean-up.
    
       - Basic multi-cluster support (CPU logical map initialised from the
         DT)
    
       - Simple earlyprintk support for UART 8250/16550 and FastModel
         console output
    
       - Optimised kernel library bitops and string functions.
    
       - Automatic initialisation of the irqchip and clocks via DT"
    
    * tag 'arm64-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux-aarch64: (26 commits)
      arm64: Use acquire/release semantics instead of explicit DMB
      arm64: klib: bitops: fix unpredictable stxr usage
      arm64: vexpress: Enable ARMv8 RTSM model (SoC) support
      arm64: vexpress: Add dts files for the ARMv8 RTSM models
      arm64: Survive invalid cpu enable-methods
      arm64: mm: Correct show_pte behaviour
      arm64: Fix compat types affecting struct compat_stat
      arm64: Execute DSB during thread switching for TLB/cache maintenance
      arm64: compiling issue, need add include/asm/vga.h file
      arm64: smp: honour #address-size when parsing CPU reg property
      arm64: Define cmpxchg64 and cmpxchg64_local for outside use
      arm64: Define readq and writeq for driver module using
      arm64: Fix task tracing
      arm64: add explicit symbols to ESR_EL1 decoding
      arm64: Use irqchip_init() for interrupt controller initialisation
      arm64: psci: Use the MPIDR values from cpu_logical_map for cpu ids.
      arm64: klib: Optimised atomic bitops
      arm64: klib: Optimised string functions
      arm64: klib: Optimised memory functions
      arm64: head: match all affinity levels in the pen of the secondaries
      ...

commit 39a90ca639db5fcd33064ddf754793ec85764239
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 23 17:22:49 2013 +0100

    arm64: Survive invalid cpu enable-methods
    
    Currently, if you pass the kernel a dtb where a cpu node has an
    unsupported enable-method property (e.g. "not-psci"), it'll explode
    horribly, as it iterates over the enable_ops array incorrectly. It
    increments the pointer *at* the current element, rather than
    incrementing the pointer *to* the current element. As the first two
    elements pointed to structures that were contiguous in memory, this
    happened to be equivalent. However the third element is NULL, so when
    the list is exhausted, smp_get_enable_ops generates the wrong pointer,
    and dereferences an arbitrary portion of memory, which currently happens
    to contain zero.
    
    This patch fixes this by indirecting the pointer one level, so we
    iterate over the array elements correctly, avoiding the below panic:
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a886194e58fd..1e22ff9ae153 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -245,11 +245,11 @@ static const struct smp_enable_ops *smp_enable_ops[NR_CPUS];
 
 static const struct smp_enable_ops * __init smp_get_enable_ops(const char *name)
 {
-	const struct smp_enable_ops *ops = enable_ops[0];
+	const struct smp_enable_ops **ops = enable_ops;
 
-	while (ops) {
-		if (!strcmp(name, ops->name))
-			return ops;
+	while (*ops) {
+		if (!strcmp(name, (*ops)->name))
+			return *ops;
 
 		ops++;
 	}

commit 72aea393a2e7c53a951bc581f18a79315f47036b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 22 18:28:55 2013 +0100

    arm64: smp: honour #address-size when parsing CPU reg property
    
    For systems where the top 32-bits of the MPIDR are all zero, we should
    allow the device-tree to specify an #address-size of 0x1 for the CPU reg
    property and then zero extend the value there.
    
    Without this patch, kvmtool breaks with the recent mpidr parsing code
    introduced in 4c7aa0021356 ("arm64: kernel: initialise cpu_logical_map
    from the DT").
    
    Acked-by: Javi Merino <javi.merino@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index d4dcc6515253..a886194e58fd 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -270,6 +270,7 @@ void __init smp_init_cpus(void)
 	bool bootcpu_valid = false;
 
 	while ((dn = of_find_node_by_type(dn, "cpu"))) {
+		const u32 *cell;
 		u64 hwid;
 
 		/*
@@ -277,10 +278,12 @@ void __init smp_init_cpus(void)
 		 * considered invalid to build a cpu_logical_map
 		 * entry.
 		 */
-		if (of_property_read_u64(dn, "reg", &hwid)) {
+		cell = of_get_property(dn, "reg", NULL);
+		if (!cell) {
 			pr_err("%s: missing reg property\n", dn->full_name);
 			goto next;
 		}
+		hwid = of_read_number(cell, of_n_addr_cells(dn));
 
 		/*
 		 * Non affinity bits must be set to 0 in the DT

commit 0087298f68a726493a637c4f68d148b31102b0d9
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 21 22:49:39 2013 +0100

    arm64: Use generic idle loop
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Cc: Srivatsa S. Bhat <srivatsa.bhat@linux.vnet.ibm.com>
    Cc: Magnus Damm <magnus.damm@gmail.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/r/20130321215233.887563095@linutronix.de

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index bdd34597254b..261445c4666f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -216,7 +216,7 @@ asmlinkage void __cpuinit secondary_start_kernel(void)
 	/*
 	 * OK, it's off to the idle thread for us
 	 */
-	cpu_idle();
+	cpu_startup_entry(CPUHP_ONLINE);
 }
 
 void __init smp_cpus_done(unsigned int max_cpus)

commit 4c7aa0021356ee91b96cea51b8b7fadebaba489e
Author: Javi Merino <javi.merino@arm.com>
Date:   Wed Aug 29 09:47:19 2012 +0100

    arm64: kernel: initialise cpu_logical_map from the DT
    
    When booting the kernel, the cpu logical id map must be initialised
    using device tree data passed by FW or through an embedded blob.
    
    This patch parses the reg property in device tree "cpu" nodes,
    retrieves the corresponding CPUs hardware identifiers (MPIDR) and
    initialises the cpu logical map accordingly.
    
    The device tree HW identifiers are considered valid if all CPU nodes
    contain a "reg" property, there are no duplicate "reg" entries and the
    DT defines a CPU node whose "reg" property defines affinity levels
    that matches those of the boot CPU.
    
    The primary CPU is assigned cpu logical number 0 to keep the current
    convention valid.
    
    Based on a0ae02405076ac32bd17ece976e914b5b6075bb0 (ARM: kernel: add
    device tree init map function).
    
    Signed-off-by: Javi Merino <javi.merino@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index a57a373d305f..d4dcc6515253 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -43,6 +43,7 @@
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
 #include <asm/processor.h>
+#include <asm/smp_plat.h>
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/ptrace.h>
@@ -96,7 +97,7 @@ static int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
 	/*
 	 * Update the pen release flag.
 	 */
-	write_pen_release(cpu);
+	write_pen_release(cpu_logical_map(cpu));
 
 	/*
 	 * Send an event, causing the secondaries to read pen_release.
@@ -257,15 +258,77 @@ static const struct smp_enable_ops * __init smp_get_enable_ops(const char *name)
 }
 
 /*
- * Enumerate the possible CPU set from the device tree.
+ * Enumerate the possible CPU set from the device tree and build the
+ * cpu logical map array containing MPIDR values related to logical
+ * cpus. Assumes that cpu_logical_map(0) has already been initialized.
  */
 void __init smp_init_cpus(void)
 {
 	const char *enable_method;
 	struct device_node *dn = NULL;
-	int cpu = 0;
+	int i, cpu = 1;
+	bool bootcpu_valid = false;
 
 	while ((dn = of_find_node_by_type(dn, "cpu"))) {
+		u64 hwid;
+
+		/*
+		 * A cpu node with missing "reg" property is
+		 * considered invalid to build a cpu_logical_map
+		 * entry.
+		 */
+		if (of_property_read_u64(dn, "reg", &hwid)) {
+			pr_err("%s: missing reg property\n", dn->full_name);
+			goto next;
+		}
+
+		/*
+		 * Non affinity bits must be set to 0 in the DT
+		 */
+		if (hwid & ~MPIDR_HWID_BITMASK) {
+			pr_err("%s: invalid reg property\n", dn->full_name);
+			goto next;
+		}
+
+		/*
+		 * Duplicate MPIDRs are a recipe for disaster. Scan
+		 * all initialized entries and check for
+		 * duplicates. If any is found just ignore the cpu.
+		 * cpu_logical_map was initialized to INVALID_HWID to
+		 * avoid matching valid MPIDR values.
+		 */
+		for (i = 1; (i < cpu) && (i < NR_CPUS); i++) {
+			if (cpu_logical_map(i) == hwid) {
+				pr_err("%s: duplicate cpu reg properties in the DT\n",
+					dn->full_name);
+				goto next;
+			}
+		}
+
+		/*
+		 * The numbering scheme requires that the boot CPU
+		 * must be assigned logical id 0. Record it so that
+		 * the logical map built from DT is validated and can
+		 * be used.
+		 */
+		if (hwid == cpu_logical_map(0)) {
+			if (bootcpu_valid) {
+				pr_err("%s: duplicate boot cpu reg property in DT\n",
+					dn->full_name);
+				goto next;
+			}
+
+			bootcpu_valid = true;
+
+			/*
+			 * cpu_logical_map has already been
+			 * initialized and the boot cpu doesn't need
+			 * the enable-method so continue without
+			 * incrementing cpu.
+			 */
+			continue;
+		}
+
 		if (cpu >= NR_CPUS)
 			goto next;
 
@@ -274,22 +337,24 @@ void __init smp_init_cpus(void)
 		 */
 		enable_method = of_get_property(dn, "enable-method", NULL);
 		if (!enable_method) {
-			pr_err("CPU %d: missing enable-method property\n", cpu);
+			pr_err("%s: missing enable-method property\n",
+				dn->full_name);
 			goto next;
 		}
 
 		smp_enable_ops[cpu] = smp_get_enable_ops(enable_method);
 
 		if (!smp_enable_ops[cpu]) {
-			pr_err("CPU %d: invalid enable-method property: %s\n",
-			       cpu, enable_method);
+			pr_err("%s: invalid enable-method property: %s\n",
+			       dn->full_name, enable_method);
 			goto next;
 		}
 
 		if (smp_enable_ops[cpu]->init_cpu(dn, cpu))
 			goto next;
 
-		set_cpu_possible(cpu, true);
+		pr_debug("cpu logical map 0x%llx\n", hwid);
+		cpu_logical_map(cpu) = hwid;
 next:
 		cpu++;
 	}
@@ -298,6 +363,19 @@ void __init smp_init_cpus(void)
 	if (cpu > NR_CPUS)
 		pr_warning("no. of cores (%d) greater than configured maximum of %d - clipping\n",
 			   cpu, NR_CPUS);
+
+	if (!bootcpu_valid) {
+		pr_err("DT missing boot CPU MPIDR, not enabling secondaries\n");
+		return;
+	}
+
+	/*
+	 * All the cpus that made it to the cpu_logical_map have been
+	 * validated so set them as possible cpus.
+	 */
+	for (i = 0; i < NR_CPUS; i++)
+		if (cpu_logical_map(i) != INVALID_HWID)
+			set_cpu_possible(i, true);
 }
 
 void __init smp_prepare_cpus(unsigned int max_cpus)

commit 3e98fdacc59bbbdbb659be1a144ccc48ed4860fa
Author: Javi Merino <javi.merino@arm.com>
Date:   Thu Jan 31 20:09:04 2013 +0000

    arm64: kernel: make the pen of the secondary a 64-bit unsigned value
    
    Change the prototype of write_pen_release() accordingly and clarify
    that's holding the hardware id of the secondary that's going to boot.
    This is in preparation of getting HWIDs parsed from the DT.
    
    Signed-off-by: Javi Merino <javi.merino@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index bdd34597254b..a57a373d305f 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -53,7 +53,7 @@
  * where to place its SVC stack
  */
 struct secondary_data secondary_data;
-volatile unsigned long secondary_holding_pen_release = -1;
+volatile unsigned long secondary_holding_pen_release = INVALID_HWID;
 
 enum ipi_msg_type {
 	IPI_RESCHEDULE,
@@ -70,7 +70,7 @@ static DEFINE_RAW_SPINLOCK(boot_lock);
  * in coherency or not.  This is necessary for the hotplug code to work
  * reliably.
  */
-static void __cpuinit write_pen_release(int val)
+static void __cpuinit write_pen_release(u64 val)
 {
 	void *start = (void *)&secondary_holding_pen_release;
 	unsigned long size = sizeof(secondary_holding_pen_release);
@@ -105,7 +105,7 @@ static int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
 
 	timeout = jiffies + (1 * HZ);
 	while (time_before(jiffies, timeout)) {
-		if (secondary_holding_pen_release == -1UL)
+		if (secondary_holding_pen_release == INVALID_HWID)
 			break;
 		udelay(10);
 	}
@@ -116,7 +116,7 @@ static int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
 	 */
 	raw_spin_unlock(&boot_lock);
 
-	return secondary_holding_pen_release != -1 ? -ENOSYS : 0;
+	return secondary_holding_pen_release != INVALID_HWID ? -ENOSYS : 0;
 }
 
 static DECLARE_COMPLETION(cpu_running);
@@ -190,7 +190,7 @@ asmlinkage void __cpuinit secondary_start_kernel(void)
 	 * Let the primary processor know we're out of the
 	 * pen, then head off into the C entry point
 	 */
-	write_pen_release(-1);
+	write_pen_release(INVALID_HWID);
 
 	/*
 	 * Synchronise with the boot thread.

commit 0459ca9b7a9f86d0523e008b79c1fcf1afbd3635
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jan 2 15:34:50 2013 +0000

    arm64: SMP: enable PSCI boot method
    
    Wire the PSCI implementation into the SMP secondary startup
    code.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 7776922945af..bdd34597254b 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -236,6 +236,7 @@ static void (*smp_cross_call)(const struct cpumask *, unsigned int);
 
 static const struct smp_enable_ops *enable_ops[] __initconst = {
 	&smp_spin_table_ops,
+	&smp_psci_ops,
 	NULL,
 };
 

commit d329de3f2ada413c7cd16e1dc1d70d4abc7309e9
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Jan 2 15:24:22 2013 +0000

    arm64: SMP: rework the SMP code to be enabling method agnostic
    
    In order to introduce PSCI support, let the SMP code handle
    multiple enabling methods. This also allow CPUs to be booted
    using different methods (though this feels a bit weird...).
    
    In the process, move the spin-table code to its own file.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 538300f2273d..7776922945af 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -233,7 +233,27 @@ void __init smp_prepare_boot_cpu(void)
 }
 
 static void (*smp_cross_call)(const struct cpumask *, unsigned int);
-static phys_addr_t cpu_release_addr[NR_CPUS];
+
+static const struct smp_enable_ops *enable_ops[] __initconst = {
+	&smp_spin_table_ops,
+	NULL,
+};
+
+static const struct smp_enable_ops *smp_enable_ops[NR_CPUS];
+
+static const struct smp_enable_ops * __init smp_get_enable_ops(const char *name)
+{
+	const struct smp_enable_ops *ops = enable_ops[0];
+
+	while (ops) {
+		if (!strcmp(name, ops->name))
+			return ops;
+
+		ops++;
+	}
+
+	return NULL;
+}
 
 /*
  * Enumerate the possible CPU set from the device tree.
@@ -252,22 +272,22 @@ void __init smp_init_cpus(void)
 		 * We currently support only the "spin-table" enable-method.
 		 */
 		enable_method = of_get_property(dn, "enable-method", NULL);
-		if (!enable_method || strcmp(enable_method, "spin-table")) {
-			pr_err("CPU %d: missing or invalid enable-method property: %s\n",
-			       cpu, enable_method);
+		if (!enable_method) {
+			pr_err("CPU %d: missing enable-method property\n", cpu);
 			goto next;
 		}
 
-		/*
-		 * Determine the address from which the CPU is polling.
-		 */
-		if (of_property_read_u64(dn, "cpu-release-addr",
-					 &cpu_release_addr[cpu])) {
-			pr_err("CPU %d: missing or invalid cpu-release-addr property\n",
-			       cpu);
+		smp_enable_ops[cpu] = smp_get_enable_ops(enable_method);
+
+		if (!smp_enable_ops[cpu]) {
+			pr_err("CPU %d: invalid enable-method property: %s\n",
+			       cpu, enable_method);
 			goto next;
 		}
 
+		if (smp_enable_ops[cpu]->init_cpu(dn, cpu))
+			goto next;
+
 		set_cpu_possible(cpu, true);
 next:
 		cpu++;
@@ -281,8 +301,7 @@ void __init smp_init_cpus(void)
 
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
-	int cpu;
-	void **release_addr;
+	int cpu, err;
 	unsigned int ncores = num_possible_cpus();
 
 	/*
@@ -291,30 +310,35 @@ void __init smp_prepare_cpus(unsigned int max_cpus)
 	if (max_cpus > ncores)
 		max_cpus = ncores;
 
+	/* Don't bother if we're effectively UP */
+	if (max_cpus <= 1)
+		return;
+
 	/*
 	 * Initialise the present map (which describes the set of CPUs
 	 * actually populated at the present time) and release the
 	 * secondaries from the bootloader.
+	 *
+	 * Make sure we online at most (max_cpus - 1) additional CPUs.
 	 */
+	max_cpus--;
 	for_each_possible_cpu(cpu) {
 		if (max_cpus == 0)
 			break;
 
-		if (!cpu_release_addr[cpu])
+		if (cpu == smp_processor_id())
+			continue;
+
+		if (!smp_enable_ops[cpu])
 			continue;
 
-		release_addr = __va(cpu_release_addr[cpu]);
-		release_addr[0] = (void *)__pa(secondary_holding_pen);
-		__flush_dcache_area(release_addr, sizeof(release_addr[0]));
+		err = smp_enable_ops[cpu]->prepare_cpu(cpu);
+		if (err)
+			continue;
 
 		set_cpu_present(cpu, true);
 		max_cpus--;
 	}
-
-	/*
-	 * Send an event to wake up the secondaries.
-	 */
-	sev();
 }
 
 

commit b3770b3252589240e50f560197a19531979abba2
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Nov 7 17:00:05 2012 +0000

    arm64: smp: add missing completion for secondary boot
    
    Commit 149c24151e85 ("ARM: SMP: use a timing out completion for cpu
    hotplug") modified arm's CPU up path to use completions. It seems that
    we only got half of this patch for arm64, so add the missing call to
    complete.
    
    Reported-by: Jon Brawn <jon.brawn@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index 226b6bf6e9c2..538300f2273d 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -211,8 +211,7 @@ asmlinkage void __cpuinit secondary_start_kernel(void)
 	 * before we continue.
 	 */
 	set_cpu_online(cpu, true);
-	while (!cpu_active(cpu))
-		cpu_relax();
+	complete(&cpu_running);
 
 	/*
 	 * OK, it's off to the idle thread for us

commit 086e47b6c959ee557e9adefe72b8800c62d0ac34
Author: Sachin Kamat <sachin.kamat@linaro.org>
Date:   Sat Oct 13 09:25:58 2012 +0100

    arm64: Remove duplicate inclusion of mmu_context.h in smp.c
    
    asm/mmu_context.h was included twice.
    
    Signed-off-by: Sachin Kamat <sachin.kamat@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
index b711525be21f..226b6bf6e9c2 100644
--- a/arch/arm64/kernel/smp.c
+++ b/arch/arm64/kernel/smp.c
@@ -46,7 +46,6 @@
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/ptrace.h>
-#include <asm/mmu_context.h>
 
 /*
  * as from 2.5, kernels no longer have an init_tasks structure

commit 08e875c16a16c950e1e6d85755df5f3440844675
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:30 2012 +0000

    arm64: SMP support
    
    This patch adds SMP initialisation and spinlocks implementation for
    AArch64. The spinlock support uses the new load-acquire/store-release
    instructions to avoid explicit barriers. The architecture also specifies
    that an event is automatically generated when clearing the exclusive
    monitor state to wake up processors in WFE, so there is no need for an
    explicit DSB/SEV instruction sequence. The SEVL instruction is used to
    set the exclusive monitor locally as there is no conditional WFE and a
    branch is more expensive.
    
    For the SMP booting protocol, see Documentation/arm64/booting.txt.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
new file mode 100644
index 000000000000..b711525be21f
--- /dev/null
+++ b/arch/arm64/kernel/smp.c
@@ -0,0 +1,469 @@
+/*
+ * SMP initialisation and IPI support
+ * Based on arch/arm/kernel/smp.c
+ *
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/cache.h>
+#include <linux/profile.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/err.h>
+#include <linux/cpu.h>
+#include <linux/smp.h>
+#include <linux/seq_file.h>
+#include <linux/irq.h>
+#include <linux/percpu.h>
+#include <linux/clockchips.h>
+#include <linux/completion.h>
+#include <linux/of.h>
+
+#include <asm/atomic.h>
+#include <asm/cacheflush.h>
+#include <asm/cputype.h>
+#include <asm/mmu_context.h>
+#include <asm/pgtable.h>
+#include <asm/pgalloc.h>
+#include <asm/processor.h>
+#include <asm/sections.h>
+#include <asm/tlbflush.h>
+#include <asm/ptrace.h>
+#include <asm/mmu_context.h>
+
+/*
+ * as from 2.5, kernels no longer have an init_tasks structure
+ * so we need some other way of telling a new secondary core
+ * where to place its SVC stack
+ */
+struct secondary_data secondary_data;
+volatile unsigned long secondary_holding_pen_release = -1;
+
+enum ipi_msg_type {
+	IPI_RESCHEDULE,
+	IPI_CALL_FUNC,
+	IPI_CALL_FUNC_SINGLE,
+	IPI_CPU_STOP,
+};
+
+static DEFINE_RAW_SPINLOCK(boot_lock);
+
+/*
+ * Write secondary_holding_pen_release in a way that is guaranteed to be
+ * visible to all observers, irrespective of whether they're taking part
+ * in coherency or not.  This is necessary for the hotplug code to work
+ * reliably.
+ */
+static void __cpuinit write_pen_release(int val)
+{
+	void *start = (void *)&secondary_holding_pen_release;
+	unsigned long size = sizeof(secondary_holding_pen_release);
+
+	secondary_holding_pen_release = val;
+	__flush_dcache_area(start, size);
+}
+
+/*
+ * Boot a secondary CPU, and assign it the specified idle task.
+ * This also gives us the initial stack to use for this CPU.
+ */
+static int __cpuinit boot_secondary(unsigned int cpu, struct task_struct *idle)
+{
+	unsigned long timeout;
+
+	/*
+	 * Set synchronisation state between this boot processor
+	 * and the secondary one
+	 */
+	raw_spin_lock(&boot_lock);
+
+	/*
+	 * Update the pen release flag.
+	 */
+	write_pen_release(cpu);
+
+	/*
+	 * Send an event, causing the secondaries to read pen_release.
+	 */
+	sev();
+
+	timeout = jiffies + (1 * HZ);
+	while (time_before(jiffies, timeout)) {
+		if (secondary_holding_pen_release == -1UL)
+			break;
+		udelay(10);
+	}
+
+	/*
+	 * Now the secondary core is starting up let it run its
+	 * calibrations, then wait for it to finish
+	 */
+	raw_spin_unlock(&boot_lock);
+
+	return secondary_holding_pen_release != -1 ? -ENOSYS : 0;
+}
+
+static DECLARE_COMPLETION(cpu_running);
+
+int __cpuinit __cpu_up(unsigned int cpu, struct task_struct *idle)
+{
+	int ret;
+
+	/*
+	 * We need to tell the secondary core where to find its stack and the
+	 * page tables.
+	 */
+	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
+	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
+
+	/*
+	 * Now bring the CPU into our world.
+	 */
+	ret = boot_secondary(cpu, idle);
+	if (ret == 0) {
+		/*
+		 * CPU was successfully started, wait for it to come online or
+		 * time out.
+		 */
+		wait_for_completion_timeout(&cpu_running,
+					    msecs_to_jiffies(1000));
+
+		if (!cpu_online(cpu)) {
+			pr_crit("CPU%u: failed to come online\n", cpu);
+			ret = -EIO;
+		}
+	} else {
+		pr_err("CPU%u: failed to boot: %d\n", cpu, ret);
+	}
+
+	secondary_data.stack = NULL;
+
+	return ret;
+}
+
+/*
+ * This is the secondary CPU boot entry.  We're using this CPUs
+ * idle thread stack, but a set of temporary page tables.
+ */
+asmlinkage void __cpuinit secondary_start_kernel(void)
+{
+	struct mm_struct *mm = &init_mm;
+	unsigned int cpu = smp_processor_id();
+
+	printk("CPU%u: Booted secondary processor\n", cpu);
+
+	/*
+	 * All kernel threads share the same mm context; grab a
+	 * reference and switch to it.
+	 */
+	atomic_inc(&mm->mm_count);
+	current->active_mm = mm;
+	cpumask_set_cpu(cpu, mm_cpumask(mm));
+
+	/*
+	 * TTBR0 is only used for the identity mapping at this stage. Make it
+	 * point to zero page to avoid speculatively fetching new entries.
+	 */
+	cpu_set_reserved_ttbr0();
+	flush_tlb_all();
+
+	preempt_disable();
+	trace_hardirqs_off();
+
+	/*
+	 * Let the primary processor know we're out of the
+	 * pen, then head off into the C entry point
+	 */
+	write_pen_release(-1);
+
+	/*
+	 * Synchronise with the boot thread.
+	 */
+	raw_spin_lock(&boot_lock);
+	raw_spin_unlock(&boot_lock);
+
+	/*
+	 * Enable local interrupts.
+	 */
+	notify_cpu_starting(cpu);
+	local_irq_enable();
+	local_fiq_enable();
+
+	/*
+	 * OK, now it's safe to let the boot CPU continue.  Wait for
+	 * the CPU migration code to notice that the CPU is online
+	 * before we continue.
+	 */
+	set_cpu_online(cpu, true);
+	while (!cpu_active(cpu))
+		cpu_relax();
+
+	/*
+	 * OK, it's off to the idle thread for us
+	 */
+	cpu_idle();
+}
+
+void __init smp_cpus_done(unsigned int max_cpus)
+{
+	unsigned long bogosum = loops_per_jiffy * num_online_cpus();
+
+	pr_info("SMP: Total of %d processors activated (%lu.%02lu BogoMIPS).\n",
+		num_online_cpus(), bogosum / (500000/HZ),
+		(bogosum / (5000/HZ)) % 100);
+}
+
+void __init smp_prepare_boot_cpu(void)
+{
+}
+
+static void (*smp_cross_call)(const struct cpumask *, unsigned int);
+static phys_addr_t cpu_release_addr[NR_CPUS];
+
+/*
+ * Enumerate the possible CPU set from the device tree.
+ */
+void __init smp_init_cpus(void)
+{
+	const char *enable_method;
+	struct device_node *dn = NULL;
+	int cpu = 0;
+
+	while ((dn = of_find_node_by_type(dn, "cpu"))) {
+		if (cpu >= NR_CPUS)
+			goto next;
+
+		/*
+		 * We currently support only the "spin-table" enable-method.
+		 */
+		enable_method = of_get_property(dn, "enable-method", NULL);
+		if (!enable_method || strcmp(enable_method, "spin-table")) {
+			pr_err("CPU %d: missing or invalid enable-method property: %s\n",
+			       cpu, enable_method);
+			goto next;
+		}
+
+		/*
+		 * Determine the address from which the CPU is polling.
+		 */
+		if (of_property_read_u64(dn, "cpu-release-addr",
+					 &cpu_release_addr[cpu])) {
+			pr_err("CPU %d: missing or invalid cpu-release-addr property\n",
+			       cpu);
+			goto next;
+		}
+
+		set_cpu_possible(cpu, true);
+next:
+		cpu++;
+	}
+
+	/* sanity check */
+	if (cpu > NR_CPUS)
+		pr_warning("no. of cores (%d) greater than configured maximum of %d - clipping\n",
+			   cpu, NR_CPUS);
+}
+
+void __init smp_prepare_cpus(unsigned int max_cpus)
+{
+	int cpu;
+	void **release_addr;
+	unsigned int ncores = num_possible_cpus();
+
+	/*
+	 * are we trying to boot more cores than exist?
+	 */
+	if (max_cpus > ncores)
+		max_cpus = ncores;
+
+	/*
+	 * Initialise the present map (which describes the set of CPUs
+	 * actually populated at the present time) and release the
+	 * secondaries from the bootloader.
+	 */
+	for_each_possible_cpu(cpu) {
+		if (max_cpus == 0)
+			break;
+
+		if (!cpu_release_addr[cpu])
+			continue;
+
+		release_addr = __va(cpu_release_addr[cpu]);
+		release_addr[0] = (void *)__pa(secondary_holding_pen);
+		__flush_dcache_area(release_addr, sizeof(release_addr[0]));
+
+		set_cpu_present(cpu, true);
+		max_cpus--;
+	}
+
+	/*
+	 * Send an event to wake up the secondaries.
+	 */
+	sev();
+}
+
+
+void __init set_smp_cross_call(void (*fn)(const struct cpumask *, unsigned int))
+{
+	smp_cross_call = fn;
+}
+
+void arch_send_call_function_ipi_mask(const struct cpumask *mask)
+{
+	smp_cross_call(mask, IPI_CALL_FUNC);
+}
+
+void arch_send_call_function_single_ipi(int cpu)
+{
+	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC_SINGLE);
+}
+
+static const char *ipi_types[NR_IPI] = {
+#define S(x,s)	[x - IPI_RESCHEDULE] = s
+	S(IPI_RESCHEDULE, "Rescheduling interrupts"),
+	S(IPI_CALL_FUNC, "Function call interrupts"),
+	S(IPI_CALL_FUNC_SINGLE, "Single function call interrupts"),
+	S(IPI_CPU_STOP, "CPU stop interrupts"),
+};
+
+void show_ipi_list(struct seq_file *p, int prec)
+{
+	unsigned int cpu, i;
+
+	for (i = 0; i < NR_IPI; i++) {
+		seq_printf(p, "%*s%u:%s", prec - 1, "IPI", i + IPI_RESCHEDULE,
+			   prec >= 4 ? " " : "");
+		for_each_present_cpu(cpu)
+			seq_printf(p, "%10u ",
+				   __get_irq_stat(cpu, ipi_irqs[i]));
+		seq_printf(p, "      %s\n", ipi_types[i]);
+	}
+}
+
+u64 smp_irq_stat_cpu(unsigned int cpu)
+{
+	u64 sum = 0;
+	int i;
+
+	for (i = 0; i < NR_IPI; i++)
+		sum += __get_irq_stat(cpu, ipi_irqs[i]);
+
+	return sum;
+}
+
+static DEFINE_RAW_SPINLOCK(stop_lock);
+
+/*
+ * ipi_cpu_stop - handle IPI from smp_send_stop()
+ */
+static void ipi_cpu_stop(unsigned int cpu)
+{
+	if (system_state == SYSTEM_BOOTING ||
+	    system_state == SYSTEM_RUNNING) {
+		raw_spin_lock(&stop_lock);
+		pr_crit("CPU%u: stopping\n", cpu);
+		dump_stack();
+		raw_spin_unlock(&stop_lock);
+	}
+
+	set_cpu_online(cpu, false);
+
+	local_fiq_disable();
+	local_irq_disable();
+
+	while (1)
+		cpu_relax();
+}
+
+/*
+ * Main handler for inter-processor interrupts
+ */
+void handle_IPI(int ipinr, struct pt_regs *regs)
+{
+	unsigned int cpu = smp_processor_id();
+	struct pt_regs *old_regs = set_irq_regs(regs);
+
+	if (ipinr >= IPI_RESCHEDULE && ipinr < IPI_RESCHEDULE + NR_IPI)
+		__inc_irq_stat(cpu, ipi_irqs[ipinr - IPI_RESCHEDULE]);
+
+	switch (ipinr) {
+	case IPI_RESCHEDULE:
+		scheduler_ipi();
+		break;
+
+	case IPI_CALL_FUNC:
+		irq_enter();
+		generic_smp_call_function_interrupt();
+		irq_exit();
+		break;
+
+	case IPI_CALL_FUNC_SINGLE:
+		irq_enter();
+		generic_smp_call_function_single_interrupt();
+		irq_exit();
+		break;
+
+	case IPI_CPU_STOP:
+		irq_enter();
+		ipi_cpu_stop(cpu);
+		irq_exit();
+		break;
+
+	default:
+		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
+		break;
+	}
+	set_irq_regs(old_regs);
+}
+
+void smp_send_reschedule(int cpu)
+{
+	smp_cross_call(cpumask_of(cpu), IPI_RESCHEDULE);
+}
+
+void smp_send_stop(void)
+{
+	unsigned long timeout;
+
+	if (num_online_cpus() > 1) {
+		cpumask_t mask;
+
+		cpumask_copy(&mask, cpu_online_mask);
+		cpu_clear(smp_processor_id(), mask);
+
+		smp_cross_call(&mask, IPI_CPU_STOP);
+	}
+
+	/* Wait up to one second for other CPUs to stop */
+	timeout = USEC_PER_SEC;
+	while (num_online_cpus() > 1 && timeout--)
+		udelay(1);
+
+	if (num_online_cpus() > 1)
+		pr_warning("SMP: failed to stop secondary CPUs\n");
+}
+
+/*
+ * not supported here
+ */
+int setup_profiling_timer(unsigned int multiplier)
+{
+	return -EINVAL;
+}
