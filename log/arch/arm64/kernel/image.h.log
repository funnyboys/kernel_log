commit 90776dd1c427cbb4d381aa4b13338f1fb1d20f5e
Author: Kees Cook <keescook@chromium.org>
Date:   Tue Aug 13 16:04:50 2019 -0700

    arm64/efi: Move variable assignments after SECTIONS
    
    It seems that LLVM's linker does not correctly handle variable assignments
    involving section positions that are updated during the SECTIONS
    parsing. Commit aa69fb62bea1 ("arm64/efi: Mark __efistub_stext_offset as
    an absolute symbol explicitly") ran into this too, but found a different
    workaround.
    
    However, this was not enough, as other variables were also miscalculated
    which manifested as boot failures under UEFI where __efistub__end was
    not taking the correct _end value (they should be the same):
    
    $ ld.lld -EL -maarch64elf --no-undefined -X -shared \
            -Bsymbolic -z notext -z norelro --no-apply-dynamic-relocs \
            -o vmlinux.lld -T poc.lds --whole-archive vmlinux.o && \
      readelf -Ws vmlinux.lld | egrep '\b(__efistub_|)_end\b'
    368272: ffff000002218000     0 NOTYPE  LOCAL  HIDDEN    38 __efistub__end
    368322: ffff000012318000     0 NOTYPE  GLOBAL DEFAULT   38 _end
    
    $ aarch64-linux-gnu-ld.bfd -EL -maarch64elf --no-undefined -X -shared \
            -Bsymbolic -z notext -z norelro --no-apply-dynamic-relocs \
            -o vmlinux.bfd -T poc.lds --whole-archive vmlinux.o && \
      readelf -Ws vmlinux.bfd | egrep '\b(__efistub_|)_end\b'
    338124: ffff000012318000     0 NOTYPE  LOCAL  DEFAULT  ABS __efistub__end
    383812: ffff000012318000     0 NOTYPE  GLOBAL DEFAULT 15325 _end
    
    To work around this, all of the __efistub_-prefixed variable assignments
    need to be moved after the linker script's SECTIONS entry. As it turns
    out, this also solves the problem fixed in commit aa69fb62bea1, so those
    changes are reverted here.
    
    Link: https://github.com/ClangBuiltLinux/linux/issues/634
    Link: https://bugs.llvm.org/show_bug.cgi?id=42990
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 2b85c0d6fa3d..c7d38c660372 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -65,46 +65,4 @@
 	DEFINE_IMAGE_LE64(_kernel_offset_le, TEXT_OFFSET);	\
 	DEFINE_IMAGE_LE64(_kernel_flags_le, __HEAD_FLAGS);
 
-#ifdef CONFIG_EFI
-
-/*
- * Use ABSOLUTE() to avoid ld.lld treating this as a relative symbol:
- * https://github.com/ClangBuiltLinux/linux/issues/561
- */
-__efistub_stext_offset = ABSOLUTE(stext - _text);
-
-/*
- * The EFI stub has its own symbol namespace prefixed by __efistub_, to
- * isolate it from the kernel proper. The following symbols are legally
- * accessed by the stub, so provide some aliases to make them accessible.
- * Only include data symbols here, or text symbols of functions that are
- * guaranteed to be safe when executed at another offset than they were
- * linked at. The routines below are all implemented in assembler in a
- * position independent manner
- */
-__efistub_memcmp		= __pi_memcmp;
-__efistub_memchr		= __pi_memchr;
-__efistub_memcpy		= __pi_memcpy;
-__efistub_memmove		= __pi_memmove;
-__efistub_memset		= __pi_memset;
-__efistub_strlen		= __pi_strlen;
-__efistub_strnlen		= __pi_strnlen;
-__efistub_strcmp		= __pi_strcmp;
-__efistub_strncmp		= __pi_strncmp;
-__efistub_strrchr		= __pi_strrchr;
-__efistub___flush_dcache_area	= __pi___flush_dcache_area;
-
-#ifdef CONFIG_KASAN
-__efistub___memcpy		= __pi_memcpy;
-__efistub___memmove		= __pi_memmove;
-__efistub___memset		= __pi_memset;
-#endif
-
-__efistub__text			= _text;
-__efistub__end			= _end;
-__efistub__edata		= _edata;
-__efistub_screen_info		= screen_info;
-
-#endif
-
 #endif /* __ARM64_KERNEL_IMAGE_H */

commit 4b1fe9b58e9d20f23f6b07d1c2e0dbd921da67bf
Merge: 6fbc7275c7a9 aa69fb62bea1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 15:57:30 2019 +0800

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "Fix a build failure with the LLVM linker and a module allocation
      failure when KASLR is active:
    
       - Fix module allocation when running with KASLR enabled
    
       - Fix broken build due to bug in LLVM linker (ld.lld)"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64/efi: Mark __efistub_stext_offset as an absolute symbol explicitly
      arm64: kaslr: keep modules inside module region when KASAN is enabled

commit aa69fb62bea15126e744af2e02acc0d6cf3ed4da
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Tue Jun 25 21:20:17 2019 -0700

    arm64/efi: Mark __efistub_stext_offset as an absolute symbol explicitly
    
    After r363059 and r363928 in LLVM, a build using ld.lld as the linker
    with CONFIG_RANDOMIZE_BASE enabled fails like so:
    
    ld.lld: error: relocation R_AARCH64_ABS32 cannot be used against symbol
    __efistub_stext_offset; recompile with -fPIC
    
    Fangrui and Peter figured out that ld.lld is incorrectly considering
    __efistub_stext_offset as a relative symbol because of the order in
    which symbols are evaluated. _text is treated as an absolute symbol
    and stext is a relative symbol, making __efistub_stext_offset a
    relative symbol.
    
    Adding ABSOLUTE will force ld.lld to evalute this expression in the
    right context and does not change ld.bfd's behavior. ld.lld will
    need to be fixed but the developers do not see a quick or simple fix
    without some research (see the linked issue for further explanation).
    Add this simple workaround so that ld.lld can continue to link kernels.
    
    Link: https://github.com/ClangBuiltLinux/linux/issues/561
    Link: https://github.com/llvm/llvm-project/commit/025a815d75d2356f2944136269aa5874721ec236
    Link: https://github.com/llvm/llvm-project/commit/249fde85832c33f8b06c6b4ac65d1c4b96d23b83
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Debugged-by: Fangrui Song <maskray@google.com>
    Debugged-by: Peter Smith <peter.smith@linaro.org>
    Suggested-by: Fangrui Song <maskray@google.com>
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    [will: add comment]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 33f14e484040..b22e8ad071b1 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -78,7 +78,11 @@
 
 #ifdef CONFIG_EFI
 
-__efistub_stext_offset = stext - _text;
+/*
+ * Use ABSOLUTE() to avoid ld.lld treating this as a relative symbol:
+ * https://github.com/ClangBuiltLinux/linux/issues/561
+ */
+__efistub_stext_offset = ABSOLUTE(stext - _text);
 
 /*
  * The EFI stub has its own symbol namespace prefixed by __efistub_, to

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 33f14e484040..04ca08086d35 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -1,19 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Linker script macros to generate Image header fields.
  *
  * Copyright (C) 2014 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef __ARM64_KERNEL_IMAGE_H
 #define __ARM64_KERNEL_IMAGE_H

commit d34664f63bba9c884920d86ab67379a08a4ee8e9
Merge: bc84a2d106be 394135c1ff13
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 10 18:57:17 2018 +0000

    Merge branch 'for-next/kexec' into aarch64/for-next/core
    
    Merge in kexec_file_load() support from Akashi Takahiro.

commit f56063c51f9fb3d9af4e7c707926964cf924b814
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Thu Nov 15 14:52:46 2018 +0900

    arm64: add image head flag definitions
    
    Those image head's flags will be used later by kexec_file loader.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index a820ed07fb80..d843f9cbcd92 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -15,13 +15,15 @@
  * You should have received a copy of the GNU General Public License
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
-#ifndef __ASM_IMAGE_H
-#define __ASM_IMAGE_H
+#ifndef __ARM64_KERNEL_IMAGE_H
+#define __ARM64_KERNEL_IMAGE_H
 
 #ifndef LINKER_SCRIPT
 #error This file should only be included in vmlinux.lds.S
 #endif
 
+#include <asm/image.h>
+
 /*
  * There aren't any ELF relocations we can use to endian-swap values known only
  * at link time (e.g. the subtraction of two symbol addresses), so we must get
@@ -47,19 +49,22 @@
 	sym##_lo32 = DATA_LE32((data) & 0xffffffff);		\
 	sym##_hi32 = DATA_LE32((data) >> 32)
 
+#define __HEAD_FLAG(field)	(__HEAD_FLAG_##field << \
+					ARM64_IMAGE_FLAG_##field##_SHIFT)
+
 #ifdef CONFIG_CPU_BIG_ENDIAN
-#define __HEAD_FLAG_BE		1
+#define __HEAD_FLAG_BE		ARM64_IMAGE_FLAG_BE
 #else
-#define __HEAD_FLAG_BE		0
+#define __HEAD_FLAG_BE		ARM64_IMAGE_FLAG_LE
 #endif
 
 #define __HEAD_FLAG_PAGE_SIZE	((PAGE_SHIFT - 10) / 2)
 
 #define __HEAD_FLAG_PHYS_BASE	1
 
-#define __HEAD_FLAGS		((__HEAD_FLAG_BE << 0) |	\
-				 (__HEAD_FLAG_PAGE_SIZE << 1) |	\
-				 (__HEAD_FLAG_PHYS_BASE << 3))
+#define __HEAD_FLAGS		(__HEAD_FLAG(BE)	| \
+				 __HEAD_FLAG(PAGE_SIZE) | \
+				 __HEAD_FLAG(PHYS_BASE))
 
 /*
  * These will output as part of the Image header, which should be little-endian
@@ -119,4 +124,4 @@ __efistub_screen_info		= KALLSYMS_HIDE(screen_info);
 
 #endif
 
-#endif /* __ASM_IMAGE_H */
+#endif /* __ARM64_KERNEL_IMAGE_H */

commit dd6846d774693bfa27d7db4dae5ea67dfe373fa1
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Nov 30 12:35:58 2018 +0100

    arm64: drop linker script hack to hide __efistub_ symbols
    
    Commit 1212f7a16af4 ("scripts/kallsyms: filter arm64's __efistub_
    symbols") updated the kallsyms code to filter out symbols with
    the __efistub_ prefix explicitly, so we no longer require the
    hack in our linker script to emit them as absolute symbols.
    
    Cc: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index a820ed07fb80..8da289dc843a 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -75,16 +75,6 @@
 
 __efistub_stext_offset = stext - _text;
 
-/*
- * Prevent the symbol aliases below from being emitted into the kallsyms
- * table, by forcing them to be absolute symbols (which are conveniently
- * ignored by scripts/kallsyms) rather than section relative symbols.
- * The distinction is only relevant for partial linking, and only for symbols
- * that are defined within a section declaration (which is not the case for
- * the definitions below) so the resulting values will be identical.
- */
-#define KALLSYMS_HIDE(sym)	ABSOLUTE(sym)
-
 /*
  * The EFI stub has its own symbol namespace prefixed by __efistub_, to
  * isolate it from the kernel proper. The following symbols are legally
@@ -94,28 +84,28 @@ __efistub_stext_offset = stext - _text;
  * linked at. The routines below are all implemented in assembler in a
  * position independent manner
  */
-__efistub_memcmp		= KALLSYMS_HIDE(__pi_memcmp);
-__efistub_memchr		= KALLSYMS_HIDE(__pi_memchr);
-__efistub_memcpy		= KALLSYMS_HIDE(__pi_memcpy);
-__efistub_memmove		= KALLSYMS_HIDE(__pi_memmove);
-__efistub_memset		= KALLSYMS_HIDE(__pi_memset);
-__efistub_strlen		= KALLSYMS_HIDE(__pi_strlen);
-__efistub_strnlen		= KALLSYMS_HIDE(__pi_strnlen);
-__efistub_strcmp		= KALLSYMS_HIDE(__pi_strcmp);
-__efistub_strncmp		= KALLSYMS_HIDE(__pi_strncmp);
-__efistub_strrchr		= KALLSYMS_HIDE(__pi_strrchr);
-__efistub___flush_dcache_area	= KALLSYMS_HIDE(__pi___flush_dcache_area);
+__efistub_memcmp		= __pi_memcmp;
+__efistub_memchr		= __pi_memchr;
+__efistub_memcpy		= __pi_memcpy;
+__efistub_memmove		= __pi_memmove;
+__efistub_memset		= __pi_memset;
+__efistub_strlen		= __pi_strlen;
+__efistub_strnlen		= __pi_strnlen;
+__efistub_strcmp		= __pi_strcmp;
+__efistub_strncmp		= __pi_strncmp;
+__efistub_strrchr		= __pi_strrchr;
+__efistub___flush_dcache_area	= __pi___flush_dcache_area;
 
 #ifdef CONFIG_KASAN
-__efistub___memcpy		= KALLSYMS_HIDE(__pi_memcpy);
-__efistub___memmove		= KALLSYMS_HIDE(__pi_memmove);
-__efistub___memset		= KALLSYMS_HIDE(__pi_memset);
+__efistub___memcpy		= __pi_memcpy;
+__efistub___memmove		= __pi_memmove;
+__efistub___memset		= __pi_memset;
 #endif
 
-__efistub__text			= KALLSYMS_HIDE(_text);
-__efistub__end			= KALLSYMS_HIDE(_end);
-__efistub__edata		= KALLSYMS_HIDE(_edata);
-__efistub_screen_info		= KALLSYMS_HIDE(screen_info);
+__efistub__text			= _text;
+__efistub__end			= _end;
+__efistub__edata		= _edata;
+__efistub_screen_info		= screen_info;
 
 #endif
 

commit fdfb69a72522e97f9105a6d39a5be0a465951ed8
Author: Rob Herring <robh@kernel.org>
Date:   Thu Mar 1 10:12:07 2018 -0600

    arm64/efi: Make strrchr() available to the EFI namespace
    
    libfdt gained a new dependency on strrchr, so make it available to the
    EFI namespace before we update libfdt.
    
    Thanks to Ard for providing this fix.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Rob Herring <robh@kernel.org>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index c7fcb232fe47..a820ed07fb80 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -103,6 +103,7 @@ __efistub_strlen		= KALLSYMS_HIDE(__pi_strlen);
 __efistub_strnlen		= KALLSYMS_HIDE(__pi_strnlen);
 __efistub_strcmp		= KALLSYMS_HIDE(__pi_strcmp);
 __efistub_strncmp		= KALLSYMS_HIDE(__pi_strncmp);
+__efistub_strrchr		= KALLSYMS_HIDE(__pi_strrchr);
 __efistub___flush_dcache_area	= KALLSYMS_HIDE(__pi___flush_dcache_area);
 
 #ifdef CONFIG_KASAN

commit be092017b6ffbd013f481f915632db6aa9fc3ca3
Merge: fb6363e9f4ee e6d9a5254333
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 17:17:24 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
    
     - virt_to_page/page_address optimisations
    
     - support for NUMA systems described using device-tree
    
     - support for hibernate/suspend-to-disk
    
     - proper support for maxcpus= command line parameter
    
     - detection and graceful handling of AArch64-only CPUs
    
     - miscellaneous cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      arm64: do not enforce strict 16 byte alignment to stack pointer
      arm64: kernel: Fix incorrect brk randomization
      arm64: cpuinfo: Missing NULL terminator in compat_hwcap_str
      arm64: secondary_start_kernel: Remove unnecessary barrier
      arm64: Ensure pmd_present() returns false after pmd_mknotpresent()
      arm64: Replace hard-coded values in the pmd/pud_bad() macros
      arm64: Implement pmdp_set_access_flags() for hardware AF/DBM
      arm64: Fix typo in the pmdp_huge_get_and_clear() definition
      arm64: mm: remove unnecessary EXPORT_SYMBOL_GPL
      arm64: always use STRICT_MM_TYPECHECKS
      arm64: kvm: Fix kvm teardown for systems using the extended idmap
      arm64: kaslr: increase randomization granularity
      arm64: kconfig: drop CONFIG_RTC_LIB dependency
      arm64: make ARCH_SUPPORTS_DEBUG_PAGEALLOC depend on !HIBERNATION
      arm64: hibernate: Refuse to hibernate if the boot cpu is offline
      arm64: kernel: Add support for hibernate/suspend-to-disk
      PM / Hibernate: Call flush_icache_range() on pages restored in-place
      arm64: Add new asm macro copy_page
      arm64: Promote KERNEL_START/KERNEL_END definitions to a header file
      arm64: kernel: Include _AC definition in page.h
      ...

commit 57fdb89aeb7b0e3aab19847ab7399e5d76f11e6f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Apr 25 21:06:52 2016 +0100

    arm64/efi/libstub: Make screen_info accessible to the UEFI stub
    
    Unlike on 32-bit ARM, where we need to pass the stub's version of struct
    screen_info to the kernel proper via a configuration table, on 64-bit ARM
    it simply involves making the core kernel's copy of struct screen_info
    visible to the stub by exposing an __efistub_ alias for it.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: David Herrmann <dh.herrmann@gmail.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Peter Jones <pjones@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-efi@vger.kernel.org
    Link: http://lkml.kernel.org/r/1461614832-17633-21-git-send-email-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 5e360ce88f10..1428849aece8 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -112,6 +112,7 @@ __efistub___memset		= KALLSYMS_HIDE(__pi_memset);
 __efistub__text			= KALLSYMS_HIDE(_text);
 __efistub__end			= KALLSYMS_HIDE(_end);
 __efistub__edata		= KALLSYMS_HIDE(_edata);
+__efistub_screen_info		= KALLSYMS_HIDE(screen_info);
 
 #endif
 

commit 18b9c0d641938242d8bcdba3c14a8f2beec2a97e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Apr 18 17:09:46 2016 +0200

    arm64: don't map TEXT_OFFSET bytes below the kernel if we can avoid it
    
    For historical reasons, the kernel Image must be loaded into physical
    memory at a 512 KB offset above a 2 MB aligned base address. The region
    between the base address and the start of the kernel Image has no
    significance to the kernel itself, but it is currently mapped explicitly
    into the early kernel VMA range for all translation granules.
    
    In some cases (i.e., 4 KB granule), this is unavoidable, due to the 2 MB
    granularity of the early kernel mappings. However, in other cases, e.g.,
    when running with larger page sizes, or in the future, with more granular
    KASLR, there is no reason to map it explicitly like we do currently.
    
    So update the logic so that the region is mapped only if that happens as
    a side effect of rounding the start address of the kernel to swapper block
    size, and leave it unmapped otherwise.
    
    Since the symbol kernel_img_size now simply resolves to the memory
    footprint of the kernel Image, we can drop its definition from image.h
    and opencode its calculation.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 4fd72da646a3..86d444f9c2c1 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -71,8 +71,6 @@
 	DEFINE_IMAGE_LE64(_kernel_offset_le, TEXT_OFFSET);	\
 	DEFINE_IMAGE_LE64(_kernel_flags_le, __HEAD_FLAGS);
 
-kernel_img_size = _end - (_text - TEXT_OFFSET);
-
 #ifdef CONFIG_EFI
 
 __efistub_stext_offset = stext - _text;

commit 546c8c44f092b2f23291fe499c221efc8cabbb67
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Mar 30 17:43:07 2016 +0200

    arm64: move early boot code to the .init segment
    
    Apart from the arm64/linux and EFI header data structures, there is nothing
    in the .head.text section that must reside at the beginning of the Image.
    So let's move it to the .init section where it belongs.
    
    Note that this involves some minor tweaking of the EFI header, primarily
    because the address of 'stext' no longer coincides with the start of the
    .text section. It also requires a couple of relocated symbol references
    to be slightly rewritten or their definition moved to the linker script.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 5e360ce88f10..4fd72da646a3 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -71,8 +71,12 @@
 	DEFINE_IMAGE_LE64(_kernel_offset_le, TEXT_OFFSET);	\
 	DEFINE_IMAGE_LE64(_kernel_flags_le, __HEAD_FLAGS);
 
+kernel_img_size = _end - (_text - TEXT_OFFSET);
+
 #ifdef CONFIG_EFI
 
+__efistub_stext_offset = stext - _text;
+
 /*
  * Prevent the symbol aliases below from being emitted into the kallsyms
  * table, by forcing them to be absolute symbols (which are conveniently

commit 588ab3f9afdfa1a6b1e5761c858b2c4ab6098285
Merge: 3d15cfdb1b77 2776e0e8ef68
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 17 20:03:47 2016 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
     "Here are the main arm64 updates for 4.6.  There are some relatively
      intrusive changes to support KASLR, the reworking of the kernel
      virtual memory layout and initial page table creation.
    
      Summary:
    
       - Initial page table creation reworked to avoid breaking large block
         mappings (huge pages) into smaller ones.  The ARM architecture
         requires break-before-make in such cases to avoid TLB conflicts but
         that's not always possible on live page tables
    
       - Kernel virtual memory layout: the kernel image is no longer linked
         to the bottom of the linear mapping (PAGE_OFFSET) but at the bottom
         of the vmalloc space, allowing the kernel to be loaded (nearly)
         anywhere in physical RAM
    
       - Kernel ASLR: position independent kernel Image and modules being
         randomly mapped in the vmalloc space with the randomness is
         provided by UEFI (efi_get_random_bytes() patches merged via the
         arm64 tree, acked by Matt Fleming)
    
       - Implement relative exception tables for arm64, required by KASLR
         (initial code for ARCH_HAS_RELATIVE_EXTABLE added to lib/extable.c
         but actual x86 conversion to deferred to 4.7 because of the merge
         dependencies)
    
       - Support for the User Access Override feature of ARMv8.2: this
         allows uaccess functions (get_user etc.) to be implemented using
         LDTR/STTR instructions.  Such instructions, when run by the kernel,
         perform unprivileged accesses adding an extra level of protection.
         The set_fs() macro is used to "upgrade" such instruction to
         privileged accesses via the UAO bit
    
       - Half-precision floating point support (part of ARMv8.2)
    
       - Optimisations for CPUs with or without a hardware prefetcher (using
         run-time code patching)
    
       - copy_page performance improvement to deal with 128 bytes at a time
    
       - Sanity checks on the CPU capabilities (via CPUID) to prevent
         incompatible secondary CPUs from being brought up (e.g.  weird
         big.LITTLE configurations)
    
       - valid_user_regs() reworked for better sanity check of the
         sigcontext information (restored pstate information)
    
       - ACPI parking protocol implementation
    
       - CONFIG_DEBUG_RODATA enabled by default
    
       - VDSO code marked as read-only
    
       - DEBUG_PAGEALLOC support
    
       - ARCH_HAS_UBSAN_SANITIZE_ALL enabled
    
       - Erratum workaround Cavium ThunderX SoC
    
       - set_pte_at() fix for PROT_NONE mappings
    
       - Code clean-ups"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (99 commits)
      arm64: kasan: Fix zero shadow mapping overriding kernel image shadow
      arm64: kasan: Use actual memory node when populating the kernel image shadow
      arm64: Update PTE_RDONLY in set_pte_at() for PROT_NONE permission
      arm64: Fix misspellings in comments.
      arm64: efi: add missing frame pointer assignment
      arm64: make mrs_s prefixing implicit in read_cpuid
      arm64: enable CONFIG_DEBUG_RODATA by default
      arm64: Rework valid_user_regs
      arm64: mm: check at build time that PAGE_OFFSET divides the VA space evenly
      arm64: KVM: Move kvm_call_hyp back to its original localtion
      arm64: mm: treat memstart_addr as a signed quantity
      arm64: mm: list kernel sections in order
      arm64: lse: deal with clobbered IP registers after branch via PLT
      arm64: mm: dump: Use VA_START directly instead of private LOWEST_ADDR
      arm64: kconfig: add submenu for 8.2 architectural features
      arm64: kernel: acpi: fix ioremap in ACPI parking protocol cpu_postboot
      arm64: Add support for Half precision floating point
      arm64: Remove fixmap include fragility
      arm64: Add workaround for Cavium erratum 27456
      arm64: mm: Mark .rodata as RO
      ...

commit 6ad1fe5d9077a1ab40bf74b61994d2e770b00b14
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Dec 26 13:48:02 2015 +0100

    arm64: avoid R_AARCH64_ABS64 relocations for Image header fields
    
    Unfortunately, the current way of using the linker to emit build time
    constants into the Image header will no longer work once we switch to
    the use of PIE executables. The reason is that such constants are emitted
    into the binary using R_AARCH64_ABS64 relocations, which are resolved at
    runtime, not at build time, and the places targeted by those relocations
    will contain zeroes before that.
    
    So refactor the endian swapping linker script constant generation code so
    that it emits the upper and lower 32-bit words separately.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index c9c62cab25a4..db1bf57948f1 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -26,21 +26,27 @@
  * There aren't any ELF relocations we can use to endian-swap values known only
  * at link time (e.g. the subtraction of two symbol addresses), so we must get
  * the linker to endian-swap certain values before emitting them.
+ *
+ * Note that, in order for this to work when building the ELF64 PIE executable
+ * (for KASLR), these values should not be referenced via R_AARCH64_ABS64
+ * relocations, since these are fixed up at runtime rather than at build time
+ * when PIE is in effect. So we need to split them up in 32-bit high and low
+ * words.
  */
 #ifdef CONFIG_CPU_BIG_ENDIAN
-#define DATA_LE64(data)					\
-	((((data) & 0x00000000000000ff) << 56) |	\
-	 (((data) & 0x000000000000ff00) << 40) |	\
-	 (((data) & 0x0000000000ff0000) << 24) |	\
-	 (((data) & 0x00000000ff000000) << 8)  |	\
-	 (((data) & 0x000000ff00000000) >> 8)  |	\
-	 (((data) & 0x0000ff0000000000) >> 24) |	\
-	 (((data) & 0x00ff000000000000) >> 40) |	\
-	 (((data) & 0xff00000000000000) >> 56))
+#define DATA_LE32(data)				\
+	((((data) & 0x000000ff) << 24) |	\
+	 (((data) & 0x0000ff00) << 8)  |	\
+	 (((data) & 0x00ff0000) >> 8)  |	\
+	 (((data) & 0xff000000) >> 24))
 #else
-#define DATA_LE64(data) ((data) & 0xffffffffffffffff)
+#define DATA_LE32(data) ((data) & 0xffffffff)
 #endif
 
+#define DEFINE_IMAGE_LE64(sym, data)				\
+	sym##_lo32 = DATA_LE32((data) & 0xffffffff);		\
+	sym##_hi32 = DATA_LE32((data) >> 32)
+
 #ifdef CONFIG_CPU_BIG_ENDIAN
 #define __HEAD_FLAG_BE		1
 #else
@@ -61,9 +67,9 @@
  * endian swapped in head.S, all are done here for consistency.
  */
 #define HEAD_SYMBOLS						\
-	_kernel_size_le		= DATA_LE64(_end - _text);	\
-	_kernel_offset_le	= DATA_LE64(TEXT_OFFSET);	\
-	_kernel_flags_le	= DATA_LE64(__HEAD_FLAGS);
+	DEFINE_IMAGE_LE64(_kernel_size_le, _end - _text);	\
+	DEFINE_IMAGE_LE64(_kernel_offset_le, TEXT_OFFSET);	\
+	DEFINE_IMAGE_LE64(_kernel_flags_le, __HEAD_FLAGS);
 
 #ifdef CONFIG_EFI
 

commit a7f8de168ace487fa7b88cb154e413cf40e87fc6
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Feb 16 13:52:42 2016 +0100

    arm64: allow kernel Image to be loaded anywhere in physical memory
    
    This relaxes the kernel Image placement requirements, so that it
    may be placed at any 2 MB aligned offset in physical memory.
    
    This is accomplished by ignoring PHYS_OFFSET when installing
    memblocks, and accounting for the apparent virtual offset of
    the kernel Image. As a result, virtual address references
    below PAGE_OFFSET are correctly mapped onto physical references
    into the kernel Image regardless of where it sits in memory.
    
    Special care needs to be taken for dealing with memory limits passed
    via mem=, since the generic implementation clips memory top down, which
    may clip the kernel image itself if it is loaded high up in memory. To
    deal with this case, we simply add back the memory covering the kernel
    image, which may result in more memory to be retained than was passed
    as a mem= parameter.
    
    Since mem= should not be considered a production feature, a panic notifier
    handler is installed that dumps the memory limit at panic time if one was
    set.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 999633bd7294..c9c62cab25a4 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -42,15 +42,18 @@
 #endif
 
 #ifdef CONFIG_CPU_BIG_ENDIAN
-#define __HEAD_FLAG_BE	1
+#define __HEAD_FLAG_BE		1
 #else
-#define __HEAD_FLAG_BE	0
+#define __HEAD_FLAG_BE		0
 #endif
 
-#define __HEAD_FLAG_PAGE_SIZE ((PAGE_SHIFT - 10) / 2)
+#define __HEAD_FLAG_PAGE_SIZE	((PAGE_SHIFT - 10) / 2)
 
-#define __HEAD_FLAGS	((__HEAD_FLAG_BE << 0) |	\
-			 (__HEAD_FLAG_PAGE_SIZE << 1))
+#define __HEAD_FLAG_PHYS_BASE	1
+
+#define __HEAD_FLAGS		((__HEAD_FLAG_BE << 0) |	\
+				 (__HEAD_FLAG_PAGE_SIZE << 1) |	\
+				 (__HEAD_FLAG_PHYS_BASE << 3))
 
 /*
  * These will output as part of the Image header, which should be little-endian

commit 7f4e346263f59ff50b531dda94609fb13ca12401
Author: Thierry Reding <treding@nvidia.com>
Date:   Tue Feb 16 11:16:31 2016 +0100

    arm64/efi: Make strnlen() available to the EFI namespace
    
    Changes introduced in the upstream version of libfdt pulled in by commit
    91feabc2e224 ("scripts/dtc: Update to upstream commit b06e55c88b9b") use
    the strnlen() function, which isn't currently available to the EFI name-
    space. Add it to the EFI namespace to avoid a linker error.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Rob Herring <robh@kernel.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Thierry Reding <treding@nvidia.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 999633bd7294..352f7abd91c9 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -89,6 +89,7 @@ __efistub_memcpy		= KALLSYMS_HIDE(__pi_memcpy);
 __efistub_memmove		= KALLSYMS_HIDE(__pi_memmove);
 __efistub_memset		= KALLSYMS_HIDE(__pi_memset);
 __efistub_strlen		= KALLSYMS_HIDE(__pi_strlen);
+__efistub_strnlen		= KALLSYMS_HIDE(__pi_strnlen);
 __efistub_strcmp		= KALLSYMS_HIDE(__pi_strcmp);
 __efistub_strncmp		= KALLSYMS_HIDE(__pi_strncmp);
 __efistub___flush_dcache_area	= KALLSYMS_HIDE(__pi___flush_dcache_area);

commit 75feee3d9d51775072d3a04f47d4a439a4c4590e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Jan 15 13:28:57 2016 +0100

    arm64: hide __efistub_ aliases from kallsyms
    
    Commit e8f3010f7326 ("arm64/efi: isolate EFI stub from the kernel
    proper") isolated the EFI stub code from the kernel proper by prefixing
    all of its symbols with __efistub_, and selectively allowing access to
    core kernel symbols from the stub by emitting __efistub_ aliases for
    functions and variables that the stub can access legally.
    
    As an unintended side effect, these aliases are emitted into the
    kallsyms symbol table, which means they may turn up in backtraces,
    e.g.,
    
      ...
      PC is at __efistub_memset+0x108/0x200
      LR is at fixup_init+0x3c/0x48
      ...
      [<ffffff8008328608>] __efistub_memset+0x108/0x200
      [<ffffff8008094dcc>] free_initmem+0x2c/0x40
      [<ffffff8008645198>] kernel_init+0x20/0xe0
      [<ffffff8008085cd0>] ret_from_fork+0x10/0x40
    
    The backtrace in question has nothing to do with the EFI stub, but
    simply returns one of the several aliases of memset() that have been
    recorded in the kallsyms table. This is undesirable, since it may
    suggest to people who are not aware of this that the issue they are
    seeing is somehow EFI related.
    
    So hide the __efistub_ aliases from kallsyms, by emitting them as
    absolute linker symbols explicitly. The distinction between those
    and section relative symbols is completely irrelevant to these
    definitions, and to the final link we are performing when these
    definitions are being taken into account (the distinction is only
    relevant to symbols defined inside a section definition when performing
    a partial link), and so the resulting values are identical to the
    original ones. Since absolute symbols are ignored by kallsyms, this
    will result in these values to be omitted from its symbol table.
    
    After this patch, the backtrace generated from the same address looks
    like this:
      ...
      PC is at __memset+0x108/0x200
      LR is at fixup_init+0x3c/0x48
      ...
      [<ffffff8008328608>] __memset+0x108/0x200
      [<ffffff8008094dcc>] free_initmem+0x2c/0x40
      [<ffffff8008645198>] kernel_init+0x20/0xe0
      [<ffffff8008085cd0>] ret_from_fork+0x10/0x40
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index bc2abb8b1599..999633bd7294 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -64,6 +64,16 @@
 
 #ifdef CONFIG_EFI
 
+/*
+ * Prevent the symbol aliases below from being emitted into the kallsyms
+ * table, by forcing them to be absolute symbols (which are conveniently
+ * ignored by scripts/kallsyms) rather than section relative symbols.
+ * The distinction is only relevant for partial linking, and only for symbols
+ * that are defined within a section declaration (which is not the case for
+ * the definitions below) so the resulting values will be identical.
+ */
+#define KALLSYMS_HIDE(sym)	ABSOLUTE(sym)
+
 /*
  * The EFI stub has its own symbol namespace prefixed by __efistub_, to
  * isolate it from the kernel proper. The following symbols are legally
@@ -73,25 +83,25 @@
  * linked at. The routines below are all implemented in assembler in a
  * position independent manner
  */
-__efistub_memcmp		= __pi_memcmp;
-__efistub_memchr		= __pi_memchr;
-__efistub_memcpy		= __pi_memcpy;
-__efistub_memmove		= __pi_memmove;
-__efistub_memset		= __pi_memset;
-__efistub_strlen		= __pi_strlen;
-__efistub_strcmp		= __pi_strcmp;
-__efistub_strncmp		= __pi_strncmp;
-__efistub___flush_dcache_area	= __pi___flush_dcache_area;
+__efistub_memcmp		= KALLSYMS_HIDE(__pi_memcmp);
+__efistub_memchr		= KALLSYMS_HIDE(__pi_memchr);
+__efistub_memcpy		= KALLSYMS_HIDE(__pi_memcpy);
+__efistub_memmove		= KALLSYMS_HIDE(__pi_memmove);
+__efistub_memset		= KALLSYMS_HIDE(__pi_memset);
+__efistub_strlen		= KALLSYMS_HIDE(__pi_strlen);
+__efistub_strcmp		= KALLSYMS_HIDE(__pi_strcmp);
+__efistub_strncmp		= KALLSYMS_HIDE(__pi_strncmp);
+__efistub___flush_dcache_area	= KALLSYMS_HIDE(__pi___flush_dcache_area);
 
 #ifdef CONFIG_KASAN
-__efistub___memcpy		= __pi_memcpy;
-__efistub___memmove		= __pi_memmove;
-__efistub___memset		= __pi_memset;
+__efistub___memcpy		= KALLSYMS_HIDE(__pi_memcpy);
+__efistub___memmove		= KALLSYMS_HIDE(__pi_memmove);
+__efistub___memset		= KALLSYMS_HIDE(__pi_memset);
 #endif
 
-__efistub__text			= _text;
-__efistub__end			= _end;
-__efistub__edata		= _edata;
+__efistub__text			= KALLSYMS_HIDE(_text);
+__efistub__end			= KALLSYMS_HIDE(_end);
+__efistub__edata		= KALLSYMS_HIDE(_edata);
 
 #endif
 

commit 9d372c9fab34cd8803141871195141995f85c7f7
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Oct 19 14:19:36 2015 +0100

    arm64: Add page size to the kernel image header
    
    This patch adds the page size to the arm64 kernel image header
    so that one can infer the PAGESIZE used by the kernel. This will
    be helpful to diagnose failures to boot the kernel with page size
    not supported by the CPU.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 6eb8fee93321..bc2abb8b1599 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -47,7 +47,10 @@
 #define __HEAD_FLAG_BE	0
 #endif
 
-#define __HEAD_FLAGS	(__HEAD_FLAG_BE << 0)
+#define __HEAD_FLAG_PAGE_SIZE ((PAGE_SHIFT - 10) / 2)
+
+#define __HEAD_FLAGS	((__HEAD_FLAG_BE << 0) |	\
+			 (__HEAD_FLAG_PAGE_SIZE << 1))
 
 /*
  * These will output as part of the Image header, which should be little-endian

commit 39d114ddc68223022c12ae3a1573912bc4b585e5
Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
Date:   Mon Oct 12 18:52:58 2015 +0300

    arm64: add KASAN support
    
    This patch adds arch specific code for kernel address sanitizer
    (see Documentation/kasan.txt).
    
    1/8 of kernel addresses reserved for shadow memory. There was no
    big enough hole for this, so virtual addresses for shadow were
    stolen from vmalloc area.
    
    At early boot stage the whole shadow region populated with just
    one physical page (kasan_zero_page). Later, this page reused
    as readonly zero shadow for some memory that KASan currently
    don't track (vmalloc).
    After mapping the physical memory, pages for shadow memory are
    allocated and mapped.
    
    Functions like memset/memmove/memcpy do a lot of memory accesses.
    If bad pointer passed to one of these function it is important
    to catch this. Compiler's instrumentation cannot do this since
    these functions are written in assembly.
    KASan replaces memory functions with manually instrumented variants.
    Original functions declared as weak symbols so strong definitions
    in mm/kasan/kasan.c could replace them. Original functions have aliases
    with '__' prefix in name, so we could call non-instrumented variant
    if needed.
    Some files built without kasan instrumentation (e.g. mm/slub.c).
    Original mem* function replaced (via #define) with prefixed variants
    to disable memory access checks for such files.
    
    Signed-off-by: Andrey Ryabinin <ryabinin.a.a@gmail.com>
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index e083af0dd546..6eb8fee93321 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -80,6 +80,12 @@ __efistub_strcmp		= __pi_strcmp;
 __efistub_strncmp		= __pi_strncmp;
 __efistub___flush_dcache_area	= __pi___flush_dcache_area;
 
+#ifdef CONFIG_KASAN
+__efistub___memcpy		= __pi_memcpy;
+__efistub___memmove		= __pi_memmove;
+__efistub___memset		= __pi_memset;
+#endif
+
 __efistub__text			= _text;
 __efistub__end			= _end;
 __efistub__edata		= _edata;

commit e8f3010f7326c00368dbc057bd052bec80dfc072
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Oct 8 20:02:04 2015 +0100

    arm64/efi: isolate EFI stub from the kernel proper
    
    Since arm64 does not use a builtin decompressor, the EFI stub is built
    into the kernel proper. So far, this has been working fine, but actually,
    since the stub is in fact a PE/COFF relocatable binary that is executed
    at an unknown offset in the 1:1 mapping provided by the UEFI firmware, we
    should not be seamlessly sharing code with the kernel proper, which is a
    position dependent executable linked at a high virtual offset.
    
    So instead, separate the contents of libstub and its dependencies, by
    putting them into their own namespace by prefixing all of its symbols
    with __efistub. This way, we have tight control over what parts of the
    kernel proper are referenced by the stub.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Reviewed-by: Matt Fleming <matt.fleming@intel.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
index 8fae0756e175..e083af0dd546 100644
--- a/arch/arm64/kernel/image.h
+++ b/arch/arm64/kernel/image.h
@@ -59,4 +59,31 @@
 	_kernel_offset_le	= DATA_LE64(TEXT_OFFSET);	\
 	_kernel_flags_le	= DATA_LE64(__HEAD_FLAGS);
 
+#ifdef CONFIG_EFI
+
+/*
+ * The EFI stub has its own symbol namespace prefixed by __efistub_, to
+ * isolate it from the kernel proper. The following symbols are legally
+ * accessed by the stub, so provide some aliases to make them accessible.
+ * Only include data symbols here, or text symbols of functions that are
+ * guaranteed to be safe when executed at another offset than they were
+ * linked at. The routines below are all implemented in assembler in a
+ * position independent manner
+ */
+__efistub_memcmp		= __pi_memcmp;
+__efistub_memchr		= __pi_memchr;
+__efistub_memcpy		= __pi_memcpy;
+__efistub_memmove		= __pi_memmove;
+__efistub_memset		= __pi_memset;
+__efistub_strlen		= __pi_strlen;
+__efistub_strcmp		= __pi_strcmp;
+__efistub_strncmp		= __pi_strncmp;
+__efistub___flush_dcache_area	= __pi___flush_dcache_area;
+
+__efistub__text			= _text;
+__efistub__end			= _end;
+__efistub__edata		= _edata;
+
+#endif
+
 #endif /* __ASM_IMAGE_H */

commit a2c1d73b94ed49f5fac12e95052d7b140783f800
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jun 24 16:51:36 2014 +0100

    arm64: Update the Image header
    
    Currently the kernel Image is stripped of everything past the initial
    stack, and at runtime the memory is initialised and used by the kernel.
    This makes the effective minimum memory footprint of the kernel larger
    than the size of the loaded binary, though bootloaders have no mechanism
    to identify how large this minimum memory footprint is. This makes it
    difficult to choose safe locations to place both the kernel and other
    binaries required at boot (DTB, initrd, etc), such that the kernel won't
    clobber said binaries or other reserved memory during initialisation.
    
    Additionally when big endian support was added the image load offset was
    overlooked, and is currently of an arbitrary endianness, which makes it
    difficult for bootloaders to make use of it. It seems that bootloaders
    aren't respecting the image load offset at present anyway, and are
    assuming that offset 0x80000 will always be correct.
    
    This patch adds an effective image size to the kernel header which
    describes the amount of memory from the start of the kernel Image binary
    which the kernel expects to use before detecting memory and handling any
    memory reservations. This can be used by bootloaders to choose suitable
    locations to load the kernel and/or other binaries such that the kernel
    will not clobber any memory unexpectedly. As before, memory reservations
    are required to prevent the kernel from clobbering these locations
    later.
    
    Both the image load offset and the effective image size are forced to be
    little-endian regardless of the native endianness of the kernel to
    enable bootloaders to load a kernel of arbitrary endianness. Bootloaders
    which wish to make use of the load offset can inspect the effective
    image size field for a non-zero value to determine if the offset is of a
    known endianness. To enable software to determine the endinanness of the
    kernel as may be required for certain use-cases, a new flags field (also
    little-endian) is added to the kernel header to export this information.
    
    The documentation is updated to clarify these details. To discourage
    future assumptions regarding the value of text_offset, the value at this
    point in time is removed from the main flow of the documentation (though
    kept as a compatibility note). Some minor formatting issues in the
    documentation are also corrected.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Tom Rini <trini@ti.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Kevin Hilman <kevin.hilman@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/image.h b/arch/arm64/kernel/image.h
new file mode 100644
index 000000000000..8fae0756e175
--- /dev/null
+++ b/arch/arm64/kernel/image.h
@@ -0,0 +1,62 @@
+/*
+ * Linker script macros to generate Image header fields.
+ *
+ * Copyright (C) 2014 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef __ASM_IMAGE_H
+#define __ASM_IMAGE_H
+
+#ifndef LINKER_SCRIPT
+#error This file should only be included in vmlinux.lds.S
+#endif
+
+/*
+ * There aren't any ELF relocations we can use to endian-swap values known only
+ * at link time (e.g. the subtraction of two symbol addresses), so we must get
+ * the linker to endian-swap certain values before emitting them.
+ */
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define DATA_LE64(data)					\
+	((((data) & 0x00000000000000ff) << 56) |	\
+	 (((data) & 0x000000000000ff00) << 40) |	\
+	 (((data) & 0x0000000000ff0000) << 24) |	\
+	 (((data) & 0x00000000ff000000) << 8)  |	\
+	 (((data) & 0x000000ff00000000) >> 8)  |	\
+	 (((data) & 0x0000ff0000000000) >> 24) |	\
+	 (((data) & 0x00ff000000000000) >> 40) |	\
+	 (((data) & 0xff00000000000000) >> 56))
+#else
+#define DATA_LE64(data) ((data) & 0xffffffffffffffff)
+#endif
+
+#ifdef CONFIG_CPU_BIG_ENDIAN
+#define __HEAD_FLAG_BE	1
+#else
+#define __HEAD_FLAG_BE	0
+#endif
+
+#define __HEAD_FLAGS	(__HEAD_FLAG_BE << 0)
+
+/*
+ * These will output as part of the Image header, which should be little-endian
+ * regardless of the endianness of the kernel. While constant values could be
+ * endian swapped in head.S, all are done here for consistency.
+ */
+#define HEAD_SYMBOLS						\
+	_kernel_size_le		= DATA_LE64(_end - _text);	\
+	_kernel_offset_le	= DATA_LE64(TEXT_OFFSET);	\
+	_kernel_flags_le	= DATA_LE64(__HEAD_FLAGS);
+
+#endif /* __ASM_IMAGE_H */
