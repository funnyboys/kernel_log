commit 84bc1993e28b2d3cf6110bab0b625e8119825403
Merge: 98b769942c69 24ebec25fb27
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 19 12:19:12 2020 -0700

    Merge tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 fixes from Will Deacon:
     "Unfortunately, we still have a number of outstanding issues so there
      will be more fixes to come, but this lot are a good start.
    
       - Fix handling of watchpoints triggered by uaccess routines
    
       - Fix initialisation of gigantic pages for CMA buffers
    
       - Raise minimum clang version for BTI to avoid miscompilation
    
       - Fix data race in SVE vector length configuration code
    
       - Ensure address tags are ignored in kern_addr_valid()
    
       - Dump register state on fatal BTI exception
    
       - kexec_file() cleanup to use struct_size() macro"
    
    * tag 'arm64-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux:
      arm64: hw_breakpoint: Don't invoke overflow handler on uaccess watchpoints
      arm64: kexec_file: Use struct_size() in kmalloc()
      arm64: mm: reserve hugetlb CMA after numa_init
      arm64: bti: Require clang >= 10.0.1 for in-kernel BTI support
      arm64: sve: Fix build failure when ARM64_SVE=y and SYSCTL=n
      arm64: pgtable: Clear the GP bit for non-executable kernel pages
      arm64: mm: reset address tag set by kasan sw tagging
      arm64: traps: Dump registers prior to panic() in bad_mode()
      arm64/sve: Eliminate data races on sve_default_vl
      docs/arm64: Fix typo'd #define in sve.rst
      arm64: remove TEXT_OFFSET randomization

commit 25f12ae45fc1931a1dce3cc59f9989a9d87834b0
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 09:37:55 2020 +0200

    maccess: rename probe_kernel_address to get_kernel_nofault
    
    Better describe what this helper does, and match the naming of
    copy_from_kernel_nofault.
    
    Also switch the argument order around, so that it acts and looks
    like get_user().
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 50cc30acf106..227b2d9bae3d 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -376,7 +376,7 @@ static int call_undef_hook(struct pt_regs *regs)
 
 	if (!user_mode(regs)) {
 		__le32 instr_le;
-		if (probe_kernel_address((__force __le32 *)pc, instr_le))
+		if (get_kernel_nofault(instr_le, (__force __le32 *)pc))
 			goto exit;
 		instr = le32_to_cpu(instr_le);
 	} else if (compat_thumb_mode(regs)) {

commit 413d3ea6b775d77b2057f13a9af75875eb066156
Author: Will Deacon <will@kernel.org>
Date:   Mon Jun 15 12:23:16 2020 +0100

    arm64: traps: Dump registers prior to panic() in bad_mode()
    
    When panicing due to an unknown/unhandled exception at EL1, dump the
    registers of the faulting context so that it's easier to figure out
    what went wrong. In particular, this makes it a lot easier to debug
    in-kernel BTI failures since it pretty-prints PSTATE.BTYPE in the crash
    log.
    
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Link: https://lore.kernel.org/r/20200615113458.2884-1-will@kernel.org
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 50cc30acf106..24f2af70ac2e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -813,6 +813,7 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 		handler[reason], smp_processor_id(), esr,
 		esr_get_class_string(esr));
 
+	__show_regs(regs);
 	local_daif_mask();
 	panic("bad mode");
 }

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3fd9e2731e0d..50cc30acf106 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -448,12 +448,12 @@ void arm64_notify_segfault(unsigned long addr)
 {
 	int code;
 
-	down_read(&current->mm->mmap_sem);
+	mmap_read_lock(current->mm);
 	if (find_vma(current->mm, addr) == NULL)
 		code = SEGV_MAPERR;
 	else
 		code = SEGV_ACCERR;
-	up_read(&current->mm->mmap_sem);
+	mmap_read_unlock(current->mm);
 
 	force_signal_inject(SIGSEGV, code, addr);
 }

commit 9cb8f069deeed708bf19486d5893e297dc467ae0
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:32:29 2020 -0700

    kernel: rename show_stack_loglvl() => show_stack()
    
    Now the last users of show_stack() got converted to use an explicit log
    level, show_stack_loglvl() can drop it's redundant suffix and become once
    again well known show_stack().
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-51-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3621868b2fcc..3fd9e2731e0d 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -137,18 +137,12 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk,
 	put_task_stack(tsk);
 }
 
-void show_stack_loglvl(struct task_struct *tsk, unsigned long *sp,
-		       const char *loglvl)
+void show_stack(struct task_struct *tsk, unsigned long *sp, const char *loglvl)
 {
 	dump_backtrace(NULL, tsk, loglvl);
 	barrier();
 }
 
-void show_stack(struct task_struct *tsk, unsigned long *sp)
-{
-	show_stack_loglvl(tsk, sp, KERN_DEFAULT);
-}
-
 #ifdef CONFIG_PREEMPT
 #define S_PREEMPT " PREEMPT"
 #elif defined(CONFIG_PREEMPT_RT)

commit c0fe096a8aba4b00c2a928c22b7f6a3ce3e9ca97
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:30:26 2020 -0700

    arm64: add show_stack_loglvl()
    
    Currently, the log-level of show_stack() depends on a platform
    realization.  It creates situations where the headers are printed with
    lower log level or higher than the stacktrace (depending on a platform or
    user).
    
    Furthermore, it forces the logic decision from user to an architecture
    side.  In result, some users as sysrq/kdb/etc are doing tricks with
    temporary rising console_loglevel while printing their messages.  And in
    result it not only may print unwanted messages from other CPUs, but also
    omit printing at all in the unlucky case where the printk() was deferred.
    
    Introducing log-level parameter and KERN_UNSUPPRESSED [1] seems an easier
    approach than introducing more printk buffers.  Also, it will consolidate
    printings with headers.
    
    Introduce show_stack_loglvl(), that eventually will substitute
    show_stack().
    
    [1]: https://lore.kernel.org/lkml/20190528002412.1625-1-dima@arista.com/T/#u
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-11-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index f602aca64baa..3621868b2fcc 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -137,12 +137,18 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk,
 	put_task_stack(tsk);
 }
 
-void show_stack(struct task_struct *tsk, unsigned long *sp)
+void show_stack_loglvl(struct task_struct *tsk, unsigned long *sp,
+		       const char *loglvl)
 {
-	dump_backtrace(NULL, tsk, KERN_DEFAULT);
+	dump_backtrace(NULL, tsk, loglvl);
 	barrier();
 }
 
+void show_stack(struct task_struct *tsk, unsigned long *sp)
+{
+	show_stack_loglvl(tsk, sp, KERN_DEFAULT);
+}
+
 #ifdef CONFIG_PREEMPT
 #define S_PREEMPT " PREEMPT"
 #elif defined(CONFIG_PREEMPT_RT)

commit c76898373f9b71586edaf150190c493ae9ed3e77
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:30:23 2020 -0700

    arm64: add loglvl to dump_backtrace()
    
    Currently, the log-level of show_stack() depends on a platform
    realization.  It creates situations where the headers are printed with
    lower log level or higher than the stacktrace (depending on a platform or
    user).
    
    Furthermore, it forces the logic decision from user to an architecture
    side.  In result, some users as sysrq/kdb/etc are doing tricks with
    temporary rising console_loglevel while printing their messages.  And in
    result it not only may print unwanted messages from other CPUs, but also
    omit printing at all in the unlucky case where the printk() was deferred.
    
    Introducing log-level parameter and KERN_UNSUPPRESSED [1] seems an easier
    approach than introducing more printk buffers.  Also, it will consolidate
    printings with headers.
    
    Add log level argument to dump_backtrace() as a preparation for
    introducing show_stack_loglvl().
    
    [1]: https://lore.kernel.org/lkml/20190528002412.1625-1-dima@arista.com/T/#u
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-10-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d332590f5978..f602aca64baa 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -53,9 +53,9 @@ static const char *handler[]= {
 
 int show_unhandled_signals = 0;
 
-static void dump_backtrace_entry(unsigned long where)
+static void dump_backtrace_entry(unsigned long where, const char *loglvl)
 {
-	printk(" %pS\n", (void *)where);
+	printk("%s %pS\n", loglvl, (void *)where);
 }
 
 static void dump_kernel_instr(const char *lvl, struct pt_regs *regs)
@@ -83,7 +83,8 @@ static void dump_kernel_instr(const char *lvl, struct pt_regs *regs)
 	printk("%sCode: %s\n", lvl, str);
 }
 
-void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
+void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk,
+		    const char *loglvl)
 {
 	struct stackframe frame;
 	int skip = 0;
@@ -115,11 +116,11 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 				thread_saved_pc(tsk));
 	}
 
-	printk("Call trace:\n");
+	printk("%sCall trace:\n", loglvl);
 	do {
 		/* skip until specified stack frame */
 		if (!skip) {
-			dump_backtrace_entry(frame.pc);
+			dump_backtrace_entry(frame.pc, loglvl);
 		} else if (frame.fp == regs->regs[29]) {
 			skip = 0;
 			/*
@@ -129,7 +130,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 			 * at which an exception has taken place, use regs->pc
 			 * instead.
 			 */
-			dump_backtrace_entry(regs->pc);
+			dump_backtrace_entry(regs->pc, loglvl);
 		}
 	} while (!unwind_frame(tsk, &frame));
 
@@ -138,7 +139,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 
 void show_stack(struct task_struct *tsk, unsigned long *sp)
 {
-	dump_backtrace(NULL, tsk);
+	dump_backtrace(NULL, tsk, KERN_DEFAULT);
 	barrier();
 }
 

commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit d27865279f12035c730818aa1a0280fada866a37
Merge: 342403bcb4df a4eb355a3fda
Author: Will Deacon <will@kernel.org>
Date:   Thu May 28 18:00:51 2020 +0100

    Merge branch 'for-next/bti' into for-next/core
    
    Support for Branch Target Identification (BTI) in user and kernel
    (Mark Brown and others)
    * for-next/bti: (39 commits)
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      arm64: bti: Fix support for userspace only BTI
      arm64: kconfig: Update and comment GCC version check for kernel BTI
      arm64: vdso: Map the vDSO text with guarded pages when built for BTI
      arm64: vdso: Force the vDSO to be linked as BTI when built for BTI
      arm64: vdso: Annotate for BTI
      arm64: asm: Provide a mechanism for generating ELF note for BTI
      arm64: bti: Provide Kconfig for kernel mode BTI
      arm64: mm: Mark executable text as guarded pages
      arm64: bpf: Annotate JITed code for BTI
      arm64: Set GP bit in kernel page tables to enable BTI for the kernel
      arm64: asm: Override SYM_FUNC_START when building the kernel with BTI
      arm64: bti: Support building kernel C code using BTI
      arm64: Document why we enable PAC support for leaf functions
      arm64: insn: Report PAC and BTI instructions as skippable
      arm64: insn: Don't assume unrecognized HINTs are skippable
      arm64: insn: Provide a better name for aarch64_insn_is_nop()
      arm64: insn: Add constants for new HINT instruction decode
      arm64: Disable old style assembly annotations
      ...

commit 69ea03b56ed2c7189ccd0b5910ad39f3cad1df21
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 19 09:46:47 2020 +0100

    hardirq/nmi: Allow nested nmi_enter()
    
    Since there are already a number of sites (ARM64, PowerPC) that effectively
    nest nmi_enter(), make the primitive support this before adding even more.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lkml.kernel.org/r/20200505134100.864179229@linutronix.de

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index cf402be5c573..c728f163f329 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -906,17 +906,13 @@ bool arm64_is_fatal_ras_serror(struct pt_regs *regs, unsigned int esr)
 
 asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)
 {
-	const bool was_in_nmi = in_nmi();
-
-	if (!was_in_nmi)
-		nmi_enter();
+	nmi_enter();
 
 	/* non-RAS errors are not containable */
 	if (!arm64_is_ras_serror(esr) || arm64_is_fatal_ras_serror(regs, esr))
 		arm64_serror_panic(regs, esr);
 
-	if (!was_in_nmi)
-		nmi_exit();
+	nmi_exit();
 }
 
 asmlinkage void enter_from_user_mode(void)

commit b322c65f8ca37396cfd7d4d0ac2f7f2dc08fa9eb
Author: Douglas Anderson <dianders@chromium.org>
Date:   Wed May 13 16:06:37 2020 -0700

    arm64: Call debug_traps_init() from trap_init() to help early kgdb
    
    A new kgdb feature will soon land (kgdb_earlycon) that lets us run
    kgdb much earlier.  In order for everything to work properly it's
    important that the break hook is setup by the time we process
    "kgdbwait".
    
    Right now the break hook is setup in debug_traps_init() and that's
    called from arch_initcall().  That's a bit too late since
    kgdb_earlycon really needs things to be setup by the time the system
    calls dbg_late_init().
    
    We could fix this by adding call_break_hook() into early_brk64() and
    that works fine.  However, it's a little ugly.  Instead, let's just
    add a call to debug_traps_init() straight from trap_init().  There's
    already a documented dependency between trap_init() and
    debug_traps_init() and this makes the dependency more obvious rather
    than just relying on a comment.
    
    NOTE: this solution isn't early enough to let us select the
    "ARCH_HAS_EARLY_DEBUG" KConfig option that is introduced by the
    kgdb_earlycon patch series.  That would only be set if we could do
    breakpoints when early params are parsed.  This patch only enables
    "late early" breakpoints, AKA breakpoints when dbg_late_init() is
    called.  It's expected that this should be fine for most people.
    
    It should also be noted that if you crash you can still end up in kgdb
    earlier than debug_traps_init().  Since you don't need breakpoints to
    debug a crash that's fine.
    
    Suggested-by: Will Deacon <will@kernel.org>
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Link: https://lore.kernel.org/r/20200513160501.1.I0b5edf030cc6ebef6ab4829f8867cdaea42485d8@changeid
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index cf402be5c573..8408e8670f2e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -1047,11 +1047,11 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
-/* This registration must happen early, before debug_traps_init(). */
 void __init trap_init(void)
 {
 	register_kernel_break_hook(&bug_break_hook);
 #ifdef CONFIG_KASAN_SW_TAGS
 	register_kernel_break_hook(&kasan_break_hook);
 #endif
+	debug_traps_init();
 }

commit 0537c4cd71e3c729c278c82f5b088460fb66fc33
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:51 2020 +0000

    arm64: BTI: Reset BTYPE when skipping emulated instructions
    
    Since normal execution of any non-branch instruction resets the
    PSTATE BTYPE field to 0, so do the same thing when emulating a
    trapped instruction.
    
    Branches don't trap directly, so we should never need to assign a
    non-zero value to BTYPE here.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3c986c8ca204..10d6451b2776 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -340,6 +340,8 @@ void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 
 	if (compat_user_mode(regs))
 		advance_itstate(regs);
+	else
+		regs->pstate &= ~PSR_BTYPE_MASK;
 }
 
 static LIST_HEAD(undef_hook);

commit d2c2ee4cc33bea814eb6739f14d8cb6a4b5265bb
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:50 2020 +0000

    arm64: traps: Shuffle code to eliminate forward declarations
    
    Hoist the IT state handling code earlier in traps.c, to avoid
    accumulating forward declarations.
    
    No functional change.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index bc9f4292bfc3..3c986c8ca204 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -272,7 +272,60 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
-static void advance_itstate(struct pt_regs *regs);
+#ifdef CONFIG_COMPAT
+#define PSTATE_IT_1_0_SHIFT	25
+#define PSTATE_IT_1_0_MASK	(0x3 << PSTATE_IT_1_0_SHIFT)
+#define PSTATE_IT_7_2_SHIFT	10
+#define PSTATE_IT_7_2_MASK	(0x3f << PSTATE_IT_7_2_SHIFT)
+
+static u32 compat_get_it_state(struct pt_regs *regs)
+{
+	u32 it, pstate = regs->pstate;
+
+	it  = (pstate & PSTATE_IT_1_0_MASK) >> PSTATE_IT_1_0_SHIFT;
+	it |= ((pstate & PSTATE_IT_7_2_MASK) >> PSTATE_IT_7_2_SHIFT) << 2;
+
+	return it;
+}
+
+static void compat_set_it_state(struct pt_regs *regs, u32 it)
+{
+	u32 pstate_it;
+
+	pstate_it  = (it << PSTATE_IT_1_0_SHIFT) & PSTATE_IT_1_0_MASK;
+	pstate_it |= ((it >> 2) << PSTATE_IT_7_2_SHIFT) & PSTATE_IT_7_2_MASK;
+
+	regs->pstate &= ~PSR_AA32_IT_MASK;
+	regs->pstate |= pstate_it;
+}
+
+static void advance_itstate(struct pt_regs *regs)
+{
+	u32 it;
+
+	/* ARM mode */
+	if (!(regs->pstate & PSR_AA32_T_BIT) ||
+	    !(regs->pstate & PSR_AA32_IT_MASK))
+		return;
+
+	it  = compat_get_it_state(regs);
+
+	/*
+	 * If this is the last instruction of the block, wipe the IT
+	 * state. Otherwise advance it.
+	 */
+	if (!(it & 7))
+		it = 0;
+	else
+		it = (it & 0xe0) | ((it << 1) & 0x1f);
+
+	compat_set_it_state(regs, it);
+}
+#else
+static void advance_itstate(struct pt_regs *regs)
+{
+}
+#endif
 
 void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
@@ -285,7 +338,7 @@ void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 	if (user_mode(regs))
 		user_fastforward_single_step(current);
 
-	if (regs->pstate & PSR_MODE32_BIT)
+	if (compat_user_mode(regs))
 		advance_itstate(regs);
 }
 
@@ -578,34 +631,7 @@ static const struct sys64_hook sys64_hooks[] = {
 	{},
 };
 
-
 #ifdef CONFIG_COMPAT
-#define PSTATE_IT_1_0_SHIFT	25
-#define PSTATE_IT_1_0_MASK	(0x3 << PSTATE_IT_1_0_SHIFT)
-#define PSTATE_IT_7_2_SHIFT	10
-#define PSTATE_IT_7_2_MASK	(0x3f << PSTATE_IT_7_2_SHIFT)
-
-static u32 compat_get_it_state(struct pt_regs *regs)
-{
-	u32 it, pstate = regs->pstate;
-
-	it  = (pstate & PSTATE_IT_1_0_MASK) >> PSTATE_IT_1_0_SHIFT;
-	it |= ((pstate & PSTATE_IT_7_2_MASK) >> PSTATE_IT_7_2_SHIFT) << 2;
-
-	return it;
-}
-
-static void compat_set_it_state(struct pt_regs *regs, u32 it)
-{
-	u32 pstate_it;
-
-	pstate_it  = (it << PSTATE_IT_1_0_SHIFT) & PSTATE_IT_1_0_MASK;
-	pstate_it |= ((it >> 2) << PSTATE_IT_7_2_SHIFT) & PSTATE_IT_7_2_MASK;
-
-	regs->pstate &= ~PSR_AA32_IT_MASK;
-	regs->pstate |= pstate_it;
-}
-
 static bool cp15_cond_valid(unsigned int esr, struct pt_regs *regs)
 {
 	int cond;
@@ -626,29 +652,6 @@ static bool cp15_cond_valid(unsigned int esr, struct pt_regs *regs)
 	return aarch32_opcode_cond_checks[cond](regs->pstate);
 }
 
-static void advance_itstate(struct pt_regs *regs)
-{
-	u32 it;
-
-	/* ARM mode */
-	if (!(regs->pstate & PSR_AA32_T_BIT) ||
-	    !(regs->pstate & PSR_AA32_IT_MASK))
-		return;
-
-	it  = compat_get_it_state(regs);
-
-	/*
-	 * If this is the last instruction of the block, wipe the IT
-	 * state. Otherwise advance it.
-	 */
-	if (!(it & 7))
-		it = 0;
-	else
-		it = (it & 0xe0) | ((it << 1) & 0x1f);
-
-	compat_set_it_state(regs, it);
-}
-
 static void compat_cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int reg = (esr & ESR_ELx_CP15_32_ISS_RT_MASK) >> ESR_ELx_CP15_32_ISS_RT_SHIFT;

commit 172a797661d95873c4af528c497cb5e1dfa8b91f
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:49 2020 +0000

    arm64: unify native/compat instruction skipping
    
    Skipping of an instruction on AArch32 works a bit differently from
    AArch64, mainly due to the different CPSR/PSTATE semantics.
    
    Currently arm64_skip_faulting_instruction() is only suitable for
    AArch64, and arm64_compat_skip_faulting_instruction() handles the IT
    state machine but is local to traps.c.
    
    Since manual instruction skipping implies a trap, it's a relatively
    slow path.
    
    So, make arm64_skip_faulting_instruction() handle both compat and
    native, and get rid of the arm64_compat_skip_faulting_instruction()
    special case.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index b8c714dda851..bc9f4292bfc3 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -272,6 +272,8 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
+static void advance_itstate(struct pt_regs *regs);
+
 void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 {
 	regs->pc += size;
@@ -282,6 +284,9 @@ void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 	 */
 	if (user_mode(regs))
 		user_fastforward_single_step(current);
+
+	if (regs->pstate & PSR_MODE32_BIT)
+		advance_itstate(regs);
 }
 
 static LIST_HEAD(undef_hook);
@@ -644,19 +649,12 @@ static void advance_itstate(struct pt_regs *regs)
 	compat_set_it_state(regs, it);
 }
 
-static void arm64_compat_skip_faulting_instruction(struct pt_regs *regs,
-						   unsigned int sz)
-{
-	advance_itstate(regs);
-	arm64_skip_faulting_instruction(regs, sz);
-}
-
 static void compat_cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int reg = (esr & ESR_ELx_CP15_32_ISS_RT_MASK) >> ESR_ELx_CP15_32_ISS_RT_SHIFT;
 
 	pt_regs_write_reg(regs, reg, arch_timer_get_rate());
-	arm64_compat_skip_faulting_instruction(regs, 4);
+	arm64_skip_faulting_instruction(regs, 4);
 }
 
 static const struct sys64_hook cp15_32_hooks[] = {
@@ -676,7 +674,7 @@ static void compat_cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 
 	pt_regs_write_reg(regs, rt, lower_32_bits(val));
 	pt_regs_write_reg(regs, rt2, upper_32_bits(val));
-	arm64_compat_skip_faulting_instruction(regs, 4);
+	arm64_skip_faulting_instruction(regs, 4);
 }
 
 static const struct sys64_hook cp15_64_hooks[] = {
@@ -697,7 +695,7 @@ void do_cp15instr(unsigned int esr, struct pt_regs *regs)
 		 * There is no T16 variant of a CP access, so we
 		 * always advance PC by 4 bytes.
 		 */
-		arm64_compat_skip_faulting_instruction(regs, 4);
+		arm64_skip_faulting_instruction(regs, 4);
 		return;
 	}
 

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index cf402be5c573..b8c714dda851 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -411,6 +411,13 @@ void do_undefinstr(struct pt_regs *regs)
 }
 NOKPROBE_SYMBOL(do_undefinstr);
 
+void do_bti(struct pt_regs *regs)
+{
+	BUG_ON(!user_mode(regs));
+	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
+}
+NOKPROBE_SYMBOL(do_bti);
+
 #define __user_cache_maint(insn, address, res)			\
 	if (address >= user_addr_max()) {			\
 		res = -EFAULT;					\
@@ -753,6 +760,7 @@ static const char *esr_class_str[] = {
 	[ESR_ELx_EC_CP10_ID]		= "CP10 MRC/VMRS",
 	[ESR_ELx_EC_PAC]		= "PAC",
 	[ESR_ELx_EC_CP14_64]		= "CP14 MCRR/MRRC",
+	[ESR_ELx_EC_BTI]		= "BTI",
 	[ESR_ELx_EC_ILL]		= "PSTATE.IL",
 	[ESR_ELx_EC_SVC32]		= "SVC (AArch32)",
 	[ESR_ELx_EC_HVC32]		= "HVC (AArch32)",

commit 7ef858dad9fa6cabfe3b78997c3114cd641de6e3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Oct 15 21:17:49 2019 +0200

    sched/rt, arm64: Use CONFIG_PREEMPTION
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by CONFIG_PREEMPT_RT.
    Both PREEMPT and PREEMPT_RT require the same functionality which today
    depends on CONFIG_PREEMPT.
    
    Switch the Kconfig dependency, entry code and preemption handling over
    to use CONFIG_PREEMPTION. Add PREEMPT_RT output in show_stack().
    
    [bigeasy: +traps.c, Kconfig]
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will@kernel.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Link: https://lore.kernel.org/r/20191015191821.11479-3-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 73caf35c2262..cf402be5c573 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -144,9 +144,12 @@ void show_stack(struct task_struct *tsk, unsigned long *sp)
 
 #ifdef CONFIG_PREEMPT
 #define S_PREEMPT " PREEMPT"
+#elif defined(CONFIG_PREEMPT_RT)
+#define S_PREEMPT " PREEMPT_RT"
 #else
 #define S_PREEMPT ""
 #endif
+
 #define S_SMP " SMP"
 
 static int __die(const char *str, int err, struct pt_regs *regs)

commit 8301ae822d8d502b0ecc4b1c557221ecc6d97815
Merge: 346f6a4636f6 bfe298745afc
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Oct 28 17:02:56 2019 +0000

    Merge branch 'for-next/entry-s-to-c' into for-next/core
    
    Move the synchronous exception paths from entry.S into a C file to
    improve the code readability.
    
    * for-next/entry-s-to-c:
      arm64: entry-common: don't touch daif before bp-hardening
      arm64: Remove asmlinkage from updated functions
      arm64: entry: convert el0_sync to C
      arm64: entry: convert el1_sync to C
      arm64: add local_daif_inherit()
      arm64: Add prototypes for functions called by entry.S
      arm64: remove __exception annotations

commit afa7c0e5b965cdb945ad8a2e2973c6d7e19969f9
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:15 2019 +0100

    arm64: Remove asmlinkage from updated functions
    
    Now that the callers of these functions have moved into C, they no longer
    need the asmlinkage annotation. Remove it.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ba1a571a7774..54ebe24ef4b1 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -394,7 +394,7 @@ void arm64_notify_segfault(unsigned long addr)
 	force_signal_inject(SIGSEGV, code, addr);
 }
 
-asmlinkage void do_undefinstr(struct pt_regs *regs)
+void do_undefinstr(struct pt_regs *regs)
 {
 	/* check for AArch32 breakpoint instructions */
 	if (!aarch32_break_handler(regs))
@@ -669,7 +669,7 @@ static const struct sys64_hook cp15_64_hooks[] = {
 	{},
 };
 
-asmlinkage void do_cp15instr(unsigned int esr, struct pt_regs *regs)
+void do_cp15instr(unsigned int esr, struct pt_regs *regs)
 {
 	const struct sys64_hook *hook, *hook_base;
 
@@ -710,7 +710,7 @@ asmlinkage void do_cp15instr(unsigned int esr, struct pt_regs *regs)
 NOKPROBE_SYMBOL(do_cp15instr);
 #endif
 
-asmlinkage void do_sysinstr(unsigned int esr, struct pt_regs *regs)
+void do_sysinstr(unsigned int esr, struct pt_regs *regs)
 {
 	const struct sys64_hook *hook;
 
@@ -797,7 +797,7 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
  * bad_el0_sync handles unexpected, but potentially recoverable synchronous
  * exceptions taken from EL0. Unlike bad_mode, this returns.
  */
-asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
+void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 {
 	void __user *pc = (void __user *)instruction_pointer(regs);
 

commit b6e43c0e3129ffe87e65c85f20fcbdf0eb86fba0
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:10 2019 +0100

    arm64: remove __exception annotations
    
    Since commit 732674980139 ("arm64: unwind: reference pt_regs via embedded
    stack frame") arm64 has not used the __exception annotation to dump
    the pt_regs during stack tracing. in_exception_text() has no callers.
    
    This annotation is only used to blacklist kprobes, it means the same as
    __kprobes.
    
    Section annotations like this require the functions to be grouped
    together between the start/end markers, and placed according to
    the linker script. For kprobes we also have NOKPROBE_SYMBOL() which
    logs the symbol address in a section that kprobes parses and
    blacklists at boot.
    
    Using NOKPROBE_SYMBOL() instead lets kprobes publish the list of
    blacklisted symbols, and saves us from having an arm64 specific
    spelling of __kprobes.
    
    do_debug_exception() already has a NOKPROBE_SYMBOL() annotation.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 34739e80211b..ba1a571a7774 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
 #include <asm/insn.h>
+#include <asm/kprobes.h>
 #include <asm/traps.h>
 #include <asm/smp.h>
 #include <asm/stack_pointer.h>
@@ -393,7 +394,7 @@ void arm64_notify_segfault(unsigned long addr)
 	force_signal_inject(SIGSEGV, code, addr);
 }
 
-asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
+asmlinkage void do_undefinstr(struct pt_regs *regs)
 {
 	/* check for AArch32 breakpoint instructions */
 	if (!aarch32_break_handler(regs))
@@ -405,6 +406,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	BUG_ON(!user_mode(regs));
 	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 }
+NOKPROBE_SYMBOL(do_undefinstr);
 
 #define __user_cache_maint(insn, address, res)			\
 	if (address >= user_addr_max()) {			\
@@ -667,7 +669,7 @@ static const struct sys64_hook cp15_64_hooks[] = {
 	{},
 };
 
-asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
+asmlinkage void do_cp15instr(unsigned int esr, struct pt_regs *regs)
 {
 	const struct sys64_hook *hook, *hook_base;
 
@@ -705,9 +707,10 @@ asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
 	 */
 	do_undefinstr(regs);
 }
+NOKPROBE_SYMBOL(do_cp15instr);
 #endif
 
-asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
+asmlinkage void do_sysinstr(unsigned int esr, struct pt_regs *regs)
 {
 	const struct sys64_hook *hook;
 
@@ -724,6 +727,7 @@ asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
 	 */
 	do_undefinstr(regs);
 }
+NOKPROBE_SYMBOL(do_sysinstr);
 
 static const char *esr_class_str[] = {
 	[0 ... ESR_ELx_EC_MAX]		= "UNRECOGNIZED EC",

commit ee9d90be9ddace01b7fb126567e4b539fbe1f82f
Author: James Morse <james.morse@arm.com>
Date:   Thu Oct 17 18:42:59 2019 +0100

    arm64: Fake the IminLine size on systems affected by Neoverse-N1 #1542419
    
    Systems affected by Neoverse-N1 #1542419 support DIC so do not need to
    perform icache maintenance once new instructions are cleaned to the PoU.
    For the errata workaround, the kernel hides DIC from user-space, so that
    the unnecessary cache maintenance can be trapped by firmware.
    
    To reduce the number of traps, produce a fake IminLine value based on
    PAGE_SIZE.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 465f0a0f8f0a..4e3e9d9c8151 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -470,9 +470,15 @@ static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 	unsigned long val = arm64_ftr_reg_user_value(&arm64_ftr_reg_ctrel0);
 
-	if (cpus_have_const_cap(ARM64_WORKAROUND_1542419))
+	if (cpus_have_const_cap(ARM64_WORKAROUND_1542419)) {
+		/* Hide DIC so that we can trap the unnecessary maintenance...*/
 		val &= ~BIT(CTR_DIC_SHIFT);
 
+		/* ... and fake IminLine to reduce the number of traps. */
+		val &= ~CTR_IMINLINE_MASK;
+		val |= (PAGE_SHIFT - 2) & CTR_IMINLINE_MASK;
+	}
+
 	pt_regs_write_reg(regs, rt, val);
 
 	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

commit 05460849c3b51180d5ada3373d0449aea19075e4
Author: James Morse <james.morse@arm.com>
Date:   Thu Oct 17 18:42:58 2019 +0100

    arm64: errata: Hide CTR_EL0.DIC on systems affected by Neoverse-N1 #1542419
    
    Cores affected by Neoverse-N1 #1542419 could execute a stale instruction
    when a branch is updated to point to freshly generated instructions.
    
    To workaround this issue we need user-space to issue unnecessary
    icache maintenance that we can trap. Start by hiding CTR_EL0.DIC.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 34739e80211b..465f0a0f8f0a 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -470,6 +470,9 @@ static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 	unsigned long val = arm64_ftr_reg_user_value(&arm64_ftr_reg_ctrel0);
 
+	if (cpus_have_const_cap(ARM64_WORKAROUND_1542419))
+		val &= ~BIT(CTR_DIC_SHIFT);
+
 	pt_regs_write_reg(regs, rt, val);
 
 	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

commit e77fafe9afb53b7f4d8176c5cd5c10c43a905bc8
Merge: 52a5525214d0 e376897f424a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 14:31:40 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Although there isn't tonnes of code in terms of line count, there are
      a fair few headline features which I've noted both in the tag and also
      in the merge commits when I pulled everything together.
    
      The part I'm most pleased with is that we had 35 contributors this
      time around, which feels like a big jump from the usual small group of
      core arm64 arch developers. Hopefully they all enjoyed it so much that
      they'll continue to contribute, but we'll see.
    
      It's probably worth highlighting that we've pulled in a branch from
      the risc-v folks which moves our CPU topology code out to where it can
      be shared with others.
    
      Summary:
    
       - 52-bit virtual addressing in the kernel
    
       - New ABI to allow tagged user pointers to be dereferenced by
         syscalls
    
       - Early RNG seeding by the bootloader
    
       - Improve robustness of SMP boot
    
       - Fix TLB invalidation in light of recent architectural
         clarifications
    
       - Support for i.MX8 DDR PMU
    
       - Remove direct LSE instruction patching in favour of static keys
    
       - Function error injection using kprobes
    
       - Support for the PPTT "thread" flag introduced by ACPI 6.3
    
       - Move PSCI idle code into proper cpuidle driver
    
       - Relaxation of implicit I/O memory barriers
    
       - Build with RELR relocations when toolchain supports them
    
       - Numerous cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (114 commits)
      arm64: remove __iounmap
      arm64: atomics: Use K constraint when toolchain appears to support it
      arm64: atomics: Undefine internal macros after use
      arm64: lse: Make ARM64_LSE_ATOMICS depend on JUMP_LABEL
      arm64: asm: Kill 'asm/atomic_arch.h'
      arm64: lse: Remove unused 'alt_lse' assembly macro
      arm64: atomics: Remove atomic_ll_sc compilation unit
      arm64: avoid using hard-coded registers for LSE atomics
      arm64: atomics: avoid out-of-line ll/sc atomics
      arm64: Use correct ll/sc atomic constraints
      jump_label: Don't warn on __exit jump entries
      docs/perf: Add documentation for the i.MX8 DDR PMU
      perf/imx_ddr: Add support for AXI ID filtering
      arm64: kpti: ensure patched kernel text is fetched from PoU
      arm64: fix fixmap copy for 16K pages and 48-bit VA
      perf/smmuv3: Validate groups for global filtering
      perf/smmuv3: Validate group size
      arm64: Relax Documentation/arm64/tagged-pointers.rst
      arm64: kvm: Replace hardcoded '1' with SYS_PAR_EL1_F
      arm64: mm: Ignore spurious translation faults taken from the kernel
      ...

commit 2671828c3ff4ffadf777f793a1f3232d6e51394a
Author: James Morse <james.morse@arm.com>
Date:   Tue Aug 20 18:45:57 2019 +0100

    arm64: entry: Move ct_user_exit before any other exception
    
    When taking an SError or Debug exception from EL0, we run the C
    handler for these exceptions before updating the context tracking
    code and unmasking lower priority interrupts.
    
    When booting with nohz_full lockdep tells us we got this wrong:
    | =============================
    | WARNING: suspicious RCU usage
    | 5.3.0-rc2-00010-gb4b5e9dcb11b-dirty #11271 Not tainted
    | -----------------------------
    | include/linux/rcupdate.h:643 rcu_read_unlock() used illegally wh!
    |
    | other info that might help us debug this:
    |
    |
    | RCU used illegally from idle CPU!
    | rcu_scheduler_active = 2, debug_locks = 1
    | RCU used illegally from extended quiescent state!
    | 1 lock held by a.out/432:
    |  #0: 00000000c7a79515 (rcu_read_lock){....}, at: brk_handler+0x00
    |
    | stack backtrace:
    | CPU: 1 PID: 432 Comm: a.out Not tainted 5.3.0-rc2-00010-gb4b5e9d1
    | Hardware name: ARM LTD ARM Juno Development Platform/ARM Juno De8
    | Call trace:
    |  dump_backtrace+0x0/0x140
    |  show_stack+0x14/0x20
    |  dump_stack+0xbc/0x104
    |  lockdep_rcu_suspicious+0xf8/0x108
    |  brk_handler+0x164/0x1b0
    |  do_debug_exception+0x11c/0x278
    |  el0_dbg+0x14/0x20
    
    Moving the ct_user_exit calls to be before do_debug_exception() means
    they are also before trace_hardirqs_off() has been updated. Add a new
    ct_user_exit_irqoff macro to avoid the context-tracking code using
    irqsave/restore before we've updated trace_hardirqs_off(). To be
    consistent, do this everywhere.
    
    The C helper is called enter_from_user_mode() to match x86 in the hope
    we can merge them into kernel/context_tracking.c later.
    
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Fixes: 6c81fe7925cc4c42 ("arm64: enable context tracking")
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index a5d7ce4297b0..6e950908eb97 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -7,9 +7,11 @@
  */
 
 #include <linux/bug.h>
+#include <linux/context_tracking.h>
 #include <linux/signal.h>
 #include <linux/personality.h>
 #include <linux/kallsyms.h>
+#include <linux/kprobes.h>
 #include <linux/spinlock.h>
 #include <linux/uaccess.h>
 #include <linux/hardirq.h>
@@ -900,6 +902,13 @@ asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)
 		nmi_exit();
 }
 
+asmlinkage void enter_from_user_mode(void)
+{
+	CT_WARN_ON(ct_state() != CONTEXT_USER);
+	user_exit_irqoff();
+}
+NOKPROBE_SYMBOL(enter_from_user_mode);
+
 void __pte_error(const char *file, int line, unsigned long val)
 {
 	pr_err("%s:%d: bad pte %016lx.\n", file, line, val);

commit 37143dcc44f8f3348d47616598137b1580468973
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Aug 13 15:16:39 2019 +0100

    arm64: constify sys64_hook instances
    
    All instances of struct sys64_hook contain compile-time constant data,
    and are never inentionally modified, so let's make them all const.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 42c8422cdf4a..a5d7ce4297b0 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -511,7 +511,7 @@ struct sys64_hook {
 	void (*handler)(unsigned int esr, struct pt_regs *regs);
 };
 
-static struct sys64_hook sys64_hooks[] = {
+static const struct sys64_hook sys64_hooks[] = {
 	{
 		.esr_mask = ESR_ELx_SYS64_ISS_EL0_CACHE_OP_MASK,
 		.esr_val = ESR_ELx_SYS64_ISS_EL0_CACHE_OP_VAL,
@@ -636,7 +636,7 @@ static void compat_cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 	arm64_compat_skip_faulting_instruction(regs, 4);
 }
 
-static struct sys64_hook cp15_32_hooks[] = {
+static const struct sys64_hook cp15_32_hooks[] = {
 	{
 		.esr_mask = ESR_ELx_CP15_32_ISS_SYS_MASK,
 		.esr_val = ESR_ELx_CP15_32_ISS_SYS_CNTFRQ,
@@ -656,7 +656,7 @@ static void compat_cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 	arm64_compat_skip_faulting_instruction(regs, 4);
 }
 
-static struct sys64_hook cp15_64_hooks[] = {
+static const struct sys64_hook cp15_64_hooks[] = {
 	{
 		.esr_mask = ESR_ELx_CP15_64_ISS_SYS_MASK,
 		.esr_val = ESR_ELx_CP15_64_ISS_SYS_CNTVCT,
@@ -667,7 +667,7 @@ static struct sys64_hook cp15_64_hooks[] = {
 
 asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
 {
-	struct sys64_hook *hook, *hook_base;
+	const struct sys64_hook *hook, *hook_base;
 
 	if (!cp15_cond_valid(esr, regs)) {
 		/*
@@ -707,7 +707,7 @@ asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
 
 asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
 {
-	struct sys64_hook *hook;
+	const struct sys64_hook *hook;
 
 	for (hook = sys64_hooks; hook->handler; hook++)
 		if ((hook->esr_mask & esr) == hook->esr_val) {

commit 7f20fd23377ac3356657ce35fcaf19ee2fea8345
Merge: 15abf14202a2 a738b5e75b4c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 9 15:46:29 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm fixes from Paolo Bonzini:
     "Bugfixes (arm and x86) and cleanups"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm:
      selftests: kvm: Adding config fragments
      KVM: selftests: Update gitignore file for latest changes
      kvm: remove unnecessary PageReserved check
      KVM: arm/arm64: vgic: Reevaluate level sensitive interrupts on enable
      KVM: arm: Don't write junk to CP15 registers on reset
      KVM: arm64: Don't write junk to sysregs on reset
      KVM: arm/arm64: Sync ICH_VMCR_EL2 back when about to block
      x86: kvm: remove useless calls to kvm_para_available
      KVM: no need to check return value of debugfs_create functions
      KVM: remove kvm_arch_has_vcpu_debugfs()
      KVM: Fix leak vCPU's VMCS value into other pCPU
      KVM: Check preempted_in_kernel for involuntary preemption
      KVM: LAPIC: Don't need to wakeup vCPU twice afer timer fire
      arm64: KVM: hyp: debug-sr: Mark expected switch fall-through
      KVM: arm64: Update kvm_arm_exception_class and esr_class_str for new EC
      KVM: arm: vgic-v3: Mark expected switch fall-through
      arm64: KVM: regmap: Fix unexpected switch fall-through
      KVM: arm/arm64: Introduce kvm_pmu_vcpu_init() to setup PMU counter index

commit 332e5281a4e8269b96233a7babc98b03596b7e6d
Author: Will Deacon <will@kernel.org>
Date:   Tue Jul 16 08:14:19 2019 +0100

    arm64: esr: Add ESR exception class encoding for trapped ERET
    
    The ESR.EC encoding of 0b011010 (0x1a) describes an exception generated
    by an ERET, ERETAA or ERETAB instruction as a result of a nested
    virtualisation trap to EL2.
    
    Add an encoding for this EC and a string description so that we identify
    it correctly if we take one unexpectedly.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d3313797cca9..42c8422cdf4a 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -743,6 +743,7 @@ static const char *esr_class_str[] = {
 	[ESR_ELx_EC_SMC64]		= "SMC (AArch64)",
 	[ESR_ELx_EC_SYS64]		= "MSR/MRS (AArch64)",
 	[ESR_ELx_EC_SVE]		= "SVE",
+	[ESR_ELx_EC_ERET]		= "ERET/ERETAA/ERETAB",
 	[ESR_ELx_EC_IMP_DEF]		= "EL3 IMP DEF",
 	[ESR_ELx_EC_IABT_LOW]		= "IABT (lower EL)",
 	[ESR_ELx_EC_IABT_CUR]		= "IABT (current EL)",

commit 6701c619fa082e6660ecd7573fbad2177380c7cc
Author: Zenghui Yu <yuzenghui@huawei.com>
Date:   Sat Jul 13 04:40:54 2019 +0000

    KVM: arm64: Update kvm_arm_exception_class and esr_class_str for new EC
    
    We've added two ESR exception classes for new ARM hardware extensions:
    ESR_ELx_EC_PAC and ESR_ELx_EC_SVE, but failed to update the strings
    used in tracing and other debug.
    
    Let's update "kvm_arm_exception_class" for these two EC, which the
    new EC will be visible to user-space via kvm_exit trace events
    Also update to "esr_class_str" for ESR_ELx_EC_PAC, by which we can
    get more readable debug info.
    
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Zenghui Yu <yuzenghui@huawei.com>
    Signed-off-by: Marc Zyngier <maz@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8c03456dade6..969e1565152b 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -734,6 +734,7 @@ static const char *esr_class_str[] = {
 	[ESR_ELx_EC_CP14_LS]		= "CP14 LDC/STC",
 	[ESR_ELx_EC_FP_ASIMD]		= "ASIMD",
 	[ESR_ELx_EC_CP10_ID]		= "CP10 MRC/VMRS",
+	[ESR_ELx_EC_PAC]		= "PAC",
 	[ESR_ELx_EC_CP14_64]		= "CP14 MCRR/MRRC",
 	[ESR_ELx_EC_ILL]		= "PSTATE.IL",
 	[ESR_ELx_EC_SVC32]		= "SVC (AArch32)",

commit f3dcbe67ed424f1cf92065f9ad0cc647f2b44eac
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Jul 2 14:07:28 2019 +0100

    arm64: stacktrace: Factor out backtrace initialisation
    
    Some common code is required by each stacktrace user to initialise
    struct stackframe before the first call to unwind_frame().
    
    In preparation for adding to the common code, this patch factors it
    out into a separate function start_backtrace(), and modifies the
    stacktrace callers appropriately.
    
    No functional change.
    
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [Mark: drop tsk argument, update more callsites]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8c03456dade6..d3313797cca9 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -100,18 +100,17 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		return;
 
 	if (tsk == current) {
-		frame.fp = (unsigned long)__builtin_frame_address(0);
-		frame.pc = (unsigned long)dump_backtrace;
+		start_backtrace(&frame,
+				(unsigned long)__builtin_frame_address(0),
+				(unsigned long)dump_backtrace);
 	} else {
 		/*
 		 * task blocked in __switch_to
 		 */
-		frame.fp = thread_saved_fp(tsk);
-		frame.pc = thread_saved_pc(tsk);
+		start_backtrace(&frame,
+				thread_saved_fp(tsk),
+				thread_saved_pc(tsk));
 	}
-#ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	frame.graph = 0;
-#endif
 
 	printk("Call trace:\n");
 	do {

commit 39d7530d7494b4e47ba1856e741f513dafd17e3d
Merge: 16c97650a56a a45ff5994c9c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 12 15:35:14 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - support for chained PMU counters in guests
       - improved SError handling
       - handle Neoverse N1 erratum #1349291
       - allow side-channel mitigation status to be migrated
       - standardise most AArch64 system register accesses to msr_s/mrs_s
       - fix host MPIDR corruption on 32bit
       - selftests ckleanups
    
      x86:
       - PMU event {white,black}listing
       - ability for the guest to disable host-side interrupt polling
       - fixes for enlightened VMCS (Hyper-V pv nested virtualization),
       - new hypercall to yield to IPI target
       - support for passing cstate MSRs through to the guest
       - lots of cleanups and optimizations
    
      Generic:
       - Some txt->rST conversions for the documentation"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (128 commits)
      Documentation: virtual: Add toctree hooks
      Documentation: kvm: Convert cpuid.txt to .rst
      Documentation: virtual: Convert paravirt_ops.txt to .rst
      KVM: x86: Unconditionally enable irqs in guest context
      KVM: x86: PMU Event Filter
      kvm: x86: Fix -Wmissing-prototypes warnings
      KVM: Properly check if "page" is valid in kvm_vcpu_unmap
      KVM: arm/arm64: Initialise host's MPIDRs by reading the actual register
      KVM: LAPIC: Retry tune per-vCPU timer_advance_ns if adaptive tuning goes insane
      kvm: LAPIC: write down valid APIC registers
      KVM: arm64: Migrate _elx sysreg accessors to msr_s/mrs_s
      KVM: doc: Add API documentation on the KVM_REG_ARM_WORKAROUNDS register
      KVM: arm/arm64: Add save/restore support for firmware workaround state
      arm64: KVM: Propagate full Spectre v2 workaround state to KVM guests
      KVM: arm/arm64: Support chained PMU counters
      KVM: arm/arm64: Remove pmc->bitmask
      KVM: arm/arm64: Re-create event when setting counter value
      KVM: arm/arm64: Extract duplicated code to own function
      KVM: arm/arm64: Rename kvm_pmu_{enable/disable}_counter functions
      KVM: LAPIC: ARBPRI is a reserved register for x2APIC
      ...

commit 5ad18b2e60b75c7297a998dea702451d33a052ed
Merge: 92c1d6522135 318759b4737c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 21:48:15 2019 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull force_sig() argument change from Eric Biederman:
     "A source of error over the years has been that force_sig has taken a
      task parameter when it is only safe to use force_sig with the current
      task.
    
      The force_sig function is built for delivering synchronous signals
      such as SIGSEGV where the userspace application caused a synchronous
      fault (such as a page fault) and the kernel responded with a signal.
    
      Because the name force_sig does not make this clear, and because the
      force_sig takes a task parameter the function force_sig has been
      abused for sending other kinds of signals over the years. Slowly those
      have been fixed when the oopses have been tracked down.
    
      This set of changes fixes the remaining abusers of force_sig and
      carefully rips out the task parameter from force_sig and friends
      making this kind of error almost impossible in the future"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (27 commits)
      signal/x86: Move tsk inside of CONFIG_MEMORY_FAILURE in do_sigbus
      signal: Remove the signal number and task parameters from force_sig_info
      signal: Factor force_sig_info_to_task out of force_sig_info
      signal: Generate the siginfo in force_sig
      signal: Move the computation of force into send_signal and correct it.
      signal: Properly set TRACE_SIGNAL_LOSE_INFO in __send_signal
      signal: Remove the task parameter from force_sig_fault
      signal: Use force_sig_fault_to_task for the two calls that don't deliver to current
      signal: Explicitly call force_sig_fault on current
      signal/unicore32: Remove tsk parameter from __do_user_fault
      signal/arm: Remove tsk parameter from __do_user_fault
      signal/arm: Remove tsk parameter from ptrace_break
      signal/nds32: Remove tsk parameter from send_sigtrap
      signal/riscv: Remove tsk parameter from do_trap
      signal/sh: Remove tsk parameter from force_sig_info_fault
      signal/um: Remove task parameter from send_sigtrap
      signal/x86: Remove task parameter from send_sigtrap
      signal: Remove task parameter from force_sig_mceerr
      signal: Remove task parameter from force_sig
      signal: Remove task parameter from force_sigsegv
      ...

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit 3276cc2489641f7f37e9558f5fe9d6ae17a25528
Author: James Morse <james.morse@arm.com>
Date:   Tue Jun 18 16:17:38 2019 +0100

    arm64: Update silicon-errata.txt for Neoverse-N1 #1349291
    
    Neoverse-N1 affected by #1349291 may report an Uncontained RAS Error
    as Unrecoverable. The kernel's architecture code already considers
    Unrecoverable errors as fatal as without kernel-first support no
    further error-handling is possible.
    
    Now that KVM attributes SError to the host/guest more precisely
    the host's architecture code will always handle host errors that
    become pending during world-switch.
    Errors misclassified by this errata that affected the guest will be
    re-injected to the guest as an implementation-defined SError, which can
    be uncontained.
    
    Until kernel-first support is implemented, no workaround is needed
    for this issue.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 985721a1264c..66743bd1e422 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -880,6 +880,10 @@ bool arm64_is_fatal_ras_serror(struct pt_regs *regs, unsigned int esr)
 		/*
 		 * The CPU can't make progress. The exception may have
 		 * been imprecise.
+		 *
+		 * Neoverse-N1 #1349291 means a non-KVM SError reported as
+		 * Unrecoverable should be treated as Uncontainable. We
+		 * call arm64_serror_panic() in both cases.
 		 */
 		return true;
 

commit 7b71665603bba0fa53e2cd63de9125a0bc3f6e17
Author: jinho lim <jordan.lim@samsung.com>
Date:   Wed Jun 26 20:50:13 2019 +0900

    arm64: rename dump_instr as dump_kernel_instr
    
    In traps.c, only __die calls dump_instr.
    However, this function has sub-function as __dump_instr.
    
    dump_kernel_instr can replace those functions.
    By using aarch64_insn_read, it does not have to change fs to KERNEL_DS.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: jinho lim <jordan.lim@samsung.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 177c0f6ebabf..8a395523adcf 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -66,16 +66,19 @@ static void dump_backtrace_entry(unsigned long where)
 	printk(" %pS\n", (void *)where);
 }
 
-static void __dump_instr(const char *lvl, struct pt_regs *regs)
+static void dump_kernel_instr(const char *lvl, struct pt_regs *regs)
 {
 	unsigned long addr = instruction_pointer(regs);
 	char str[sizeof("00000000 ") * 5 + 2 + 1], *p = str;
 	int i;
 
+	if (user_mode(regs))
+		return;
+
 	for (i = -4; i < 1; i++) {
 		unsigned int val, bad;
 
-		bad = get_user(val, &((u32 *)addr)[i]);
+		bad = aarch64_insn_read(&((u32 *)addr)[i], &val);
 
 		if (!bad)
 			p += sprintf(p, i == 0 ? "(%08x) " : "%08x ", val);
@@ -84,19 +87,8 @@ static void __dump_instr(const char *lvl, struct pt_regs *regs)
 			break;
 		}
 	}
-	printk("%sCode: %s\n", lvl, str);
-}
 
-static void dump_instr(const char *lvl, struct pt_regs *regs)
-{
-	if (!user_mode(regs)) {
-		mm_segment_t fs = get_fs();
-		set_fs(KERNEL_DS);
-		__dump_instr(lvl, regs);
-		set_fs(fs);
-	} else {
-		__dump_instr(lvl, regs);
-	}
+	printk("%sCode: %s\n", lvl, str);
 }
 
 void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
@@ -182,8 +174,7 @@ static int __die(const char *str, int err, struct pt_regs *regs)
 	print_modules();
 	show_regs(regs);
 
-	if (!user_mode(regs))
-		dump_instr(KERN_EMERG, regs);
+	dump_kernel_instr(KERN_EMERG, regs);
 
 	return ret;
 }

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 177c0f6ebabf..985721a1264c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -1,20 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/kernel/traps.c
  *
  * Copyright (C) 1995-2009 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/bug.h>

commit 2e1661d2673667d886cd40ad9f414cb6db48d8da
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 11:04:24 2019 -0500

    signal: Remove the task parameter from force_sig_fault
    
    As synchronous exceptions really only make sense against the current
    task (otherwise how are you synchronous) remove the task parameter
    from from force_sig_fault to make it explicit that is what is going
    on.
    
    The two known exceptions that deliver a synchronous exception to a
    stopped ptraced task have already been changed to
    force_sig_fault_to_task.
    
    The callers have been changed with the following emacs regular expression
    (with obvious variations on the architectures that take more arguments)
    to avoid typos:
    
    force_sig_fault[(]\([^,]+\)[,]\([^,]+\)[,]\([^,]+\)[,]\W+current[)]
    ->
    force_sig_fault(\1,\2,\3)
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c76a64c1bcb3..a490a4a32e77 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -259,7 +259,7 @@ void arm64_force_sig_fault(int signo, int code, void __user *addr,
 	if (signo == SIGKILL)
 		force_sig(SIGKILL);
 	else
-		force_sig_fault(signo, code, addr, current);
+		force_sig_fault(signo, code, addr);
 }
 
 void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,

commit d76cac67db40c172791ce07948367b96a758e45b
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 11:11:19 2019 -0500

    signal/arm64: Use force_sig not force_sig_fault for SIGKILL
    
    I don't think this is userspace visible but SIGKILL does not have
    any si_codes that use the fault member of the siginfo union.  Correct
    this the simple way and call force_sig instead of force_sig_fault when
    the signal is SIGKILL.
    
    The two know places where synchronous SIGKILL are generated are
    do_bad_area and fpsimd_save.  The call paths to force_sig_fault are:
    do_bad_area
      arm64_force_sig_fault
        force_sig_fault
    force_signal_inject
      arm64_notify_die
        arm64_force_sig_fault
           force_sig_fault
    
    Which means correcting this in arm64_force_sig_fault is enough
    to ensure the arm64 code is not misusing the generic code, which
    could lead to maintenance problems later.
    
    Cc: stable@vger.kernel.org
    Cc: Dave Martin <Dave.Martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Fixes: af40ff687bc9 ("arm64: signal: Ensure si_code is valid for all fault signals")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index e6be1a6efc0a..177c0f6ebabf 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -252,7 +252,10 @@ void arm64_force_sig_fault(int signo, int code, void __user *addr,
 			   const char *str)
 {
 	arm64_show_signal(signo, str);
-	force_sig_fault(signo, code, addr, current);
+	if (signo == SIGKILL)
+		force_sig(SIGKILL, current);
+	else
+		force_sig_fault(signo, code, addr, current);
 }
 
 void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,

commit f8eac9011b6be56acfb5d1d0dfd5ee30082a12ee
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Feb 5 18:14:19 2019 -0600

    signal: Remove task parameter from force_sig_mceerr
    
    All of the callers pass current into force_sig_mceer so remove the
    task parameter to make this obvious.
    
    This also makes it clear that force_sig_mceerr passes current
    into force_sig_info.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 64abe8450780..c76a64c1bcb3 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -266,7 +266,7 @@ void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,
 			    const char *str)
 {
 	arm64_show_signal(SIGBUS, str);
-	force_sig_mceerr(code, addr, lsb, current);
+	force_sig_mceerr(code, addr, lsb);
 }
 
 void arm64_force_sig_ptrace_errno_trap(int errno, void __user *addr,

commit 3cf5d076fb4d48979f382bc9452765bf8b79e740
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 10:17:27 2019 -0500

    signal: Remove task parameter from force_sig
    
    All of the remaining callers pass current into force_sig so
    remove the task parameter to make this obvious and to make
    misuse more difficult in the future.
    
    This also makes it clear force_sig passes current into force_sig_info.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index e45d5b440fb1..64abe8450780 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -257,7 +257,7 @@ void arm64_force_sig_fault(int signo, int code, void __user *addr,
 {
 	arm64_show_signal(signo, str);
 	if (signo == SIGKILL)
-		force_sig(SIGKILL, current);
+		force_sig(SIGKILL);
 	else
 		force_sig_fault(signo, code, addr, current);
 }

commit 82e10af2248d2d09c99834613f1b47d5002dc379
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 16 10:55:21 2019 -0500

    signal/arm64: Use force_sig not force_sig_fault for SIGKILL
    
    I don't think this is userspace visible but SIGKILL does not have
    any si_codes that use the fault member of the siginfo union.  Correct
    this the simple way and call force_sig instead of force_sig_fault when
    the signal is SIGKILL.
    
    The two know places where synchronous SIGKILL are generated are
    do_bad_area and fpsimd_save.  The call paths to force_sig_fault are:
    do_bad_area
      arm64_force_sig_fault
        force_sig_fault
    force_signal_inject
      arm64_notify_die
        arm64_force_sig_fault
           force_sig_fault
    
    Which means correcting this in arm64_force_sig_fault is enough
    to ensure the arm64 code is not misusing the generic code, which
    could lead to maintenance problems later.
    
    Cc: stable@vger.kernel.org
    Cc: Dave Martin <Dave.Martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Fixes: af40ff687bc9 ("arm64: signal: Ensure si_code is valid for all fault signals")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ade32046f3fe..e45d5b440fb1 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -256,7 +256,10 @@ void arm64_force_sig_fault(int signo, int code, void __user *addr,
 			   const char *str)
 {
 	arm64_show_signal(signo, str);
-	force_sig_fault(signo, code, addr, current);
+	if (signo == SIGKILL)
+		force_sig(SIGKILL, current);
+	else
+		force_sig_fault(signo, code, addr, current);
 }
 
 void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,

commit 3e29ead500137fe0733eecb901707f986aaf3e30
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue May 14 12:57:54 2019 +0100

    arm64: Remove useless message during oops
    
    During an oops, we print the name of the current task and its pid twice.
    We also helpfully advertise its stack limit as "0x(____ptrval____)".
    
    Drop these useless messages.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ade32046f3fe..e6be1a6efc0a 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -168,7 +168,6 @@ void show_stack(struct task_struct *tsk, unsigned long *sp)
 
 static int __die(const char *str, int err, struct pt_regs *regs)
 {
-	struct task_struct *tsk = current;
 	static int die_counter;
 	int ret;
 
@@ -181,9 +180,6 @@ static int __die(const char *str, int err, struct pt_regs *regs)
 		return ret;
 
 	print_modules();
-	pr_emerg("Process %.*s (pid: %d, stack limit = 0x%p)\n",
-		 TASK_COMM_LEN, tsk->comm, task_pid_nr(tsk),
-		 end_of_stack(tsk));
 	show_regs(regs);
 
 	if (!user_mode(regs))

commit c620f7bd0ba5c882b3e7fc199a8d5c2f6c2f5263
Merge: dd4e5d6106b2 b33f908811b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 6 17:54:22 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "Mostly just incremental improvements here:
    
       - Introduce AT_HWCAP2 for advertising CPU features to userspace
    
       - Expose SVE2 availability to userspace
    
       - Support for "data cache clean to point of deep persistence" (DC PODP)
    
       - Honour "mitigations=off" on the cmdline and advertise status via
         sysfs
    
       - CPU timer erratum workaround (Neoverse-N1 #1188873)
    
       - Introduce perf PMU driver for the SMMUv3 performance counters
    
       - Add config option to disable the kuser helpers page for AArch32 tasks
    
       - Futex modifications to ensure liveness under contention
    
       - Rework debug exception handling to seperate kernel and user
         handlers
    
       - Non-critical fixes and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (92 commits)
      Documentation: Add ARM64 to kernel-parameters.rst
      arm64/speculation: Support 'mitigations=' cmdline option
      arm64: ssbs: Don't treat CPUs with SSBS as unaffected by SSB
      arm64: enable generic CPU vulnerabilites support
      arm64: add sysfs vulnerability show for speculative store bypass
      arm64: Fix size of __early_cpu_boot_status
      clocksource/arm_arch_timer: Use arch_timer_read_counter to access stable counters
      clocksource/arm_arch_timer: Remove use of workaround static key
      clocksource/arm_arch_timer: Drop use of static key in arch_timer_reg_read_stable
      clocksource/arm_arch_timer: Direcly assign set_next_event workaround
      arm64: Use arch_timer_read_counter instead of arch_counter_get_cntvct
      watchdog/sbsa: Use arch_timer_read_counter instead of arch_counter_get_cntvct
      ARM: vdso: Remove dependency with the arch_timer driver internals
      arm64: Apply ARM64_ERRATUM_1188873 to Neoverse-N1
      arm64: Add part number for Neoverse N1
      arm64: Make ARM64_ERRATUM_1188873 depend on COMPAT
      arm64: Restrict ARM64_ERRATUM_1188873 mitigation to AArch32
      arm64: mm: Remove pte_unmap_nested()
      arm64: Fix compiler warning from pte_unmap() with -Wunused-but-set-variable
      arm64: compat: Reduce address limit for 64K pages
      ...

commit 24cf262da1ad303fc940c798aab0bd1bd50e3fc2
Merge: 50abbe19623e 0ea415390cd3
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed May 1 15:45:36 2019 +0100

    Merge branch 'for-next/timers' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux into for-next/core
    
    Conflicts:
            arch/arm64/Kconfig
            arch/arm64/include/asm/arch_timer.h

commit dea86a80033f8b0fb25a805f46dde9f3b1a7c23a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 8 16:49:03 2019 +0100

    arm64: Use arch_timer_read_counter instead of arch_counter_get_cntvct
    
    Only arch_timer_read_counter will guarantee that workarounds are
    applied. So let's use this one instead of arch_counter_get_cntvct.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8ad119c3f665..6190a60388cf 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -493,7 +493,7 @@ static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 
-	pt_regs_write_reg(regs, rt, arch_counter_get_cntvct());
+	pt_regs_write_reg(regs, rt, arch_timer_read_counter());
 	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }
 
@@ -665,7 +665,7 @@ static void compat_cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int rt = (esr & ESR_ELx_CP15_64_ISS_RT_MASK) >> ESR_ELx_CP15_64_ISS_RT_SHIFT;
 	int rt2 = (esr & ESR_ELx_CP15_64_ISS_RT2_MASK) >> ESR_ELx_CP15_64_ISS_RT2_SHIFT;
-	u64 val = arch_counter_get_cntvct();
+	u64 val = arch_timer_read_counter();
 
 	pt_regs_write_reg(regs, rt, lower_32_bits(val));
 	pt_regs_write_reg(regs, rt2, upper_32_bits(val));

commit d16ed4105f5bbd8a34f15053045a6657f4c52c6f
Author: Andrew Murray <andrew.murray@arm.com>
Date:   Tue Apr 9 10:52:42 2019 +0100

    arm64: Handle trapped DC CVADP
    
    The ARMv8.5 DC CVADP instruction may be trapped to EL1 via
    SCTLR_EL1.UCI therefore let's provide a handler for it.
    
    Just like the CVAP instruction we use a 'sys' instruction instead of
    the 'dc' alias to avoid build issues with older toolchains.
    
    Signed-off-by: Andrew Murray <andrew.murray@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 74598396e0bf..21e73954762c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -459,6 +459,9 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 	case ESR_ELx_SYS64_ISS_CRM_DC_CVAC:	/* DC CVAC, gets promoted */
 		__user_cache_maint("dc civac", address, ret);
 		break;
+	case ESR_ELx_SYS64_ISS_CRM_DC_CVADP:	/* DC CVADP */
+		__user_cache_maint("sys 3, c7, c13, 1", address, ret);
+		break;
 	case ESR_ELx_SYS64_ISS_CRM_DC_CVAP:	/* DC CVAP */
 		__user_cache_maint("sys 3, c7, c12, 1", address, ret);
 		break;

commit 453b7740ebfda2d84be7fb583c54f0c91c592869
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 26 15:06:42 2019 +0000

    arm64: probes: Move magic BRK values into brk-imm.h
    
    kprobes and uprobes reserve some BRK immediates for installing their
    probes. Define these along with the other reservations in brk-imm.h
    and rename the ESR definitions to be consistent with the others that we
    already have.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 091379744d2f..74598396e0bf 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -1024,7 +1024,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
 #ifdef CONFIG_KASAN_SW_TAGS
-	unsigned int comment = esr & BRK64_ESR_MASK;
+	unsigned int comment = esr & ESR_ELx_BRK64_ISS_COMMENT_MASK;
 
 	if ((comment & ~KASAN_BRK_MASK) == KASAN_BRK_IMM)
 		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;

commit fb610f2a2006322bebeb30408fefce6a01df09ea
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 26 15:37:09 2019 +0000

    arm64: debug: Remove redundant user_mode(regs) checks from debug handlers
    
    Now that the debug hook dispatching code takes the triggering exception
    level into account, there's no need for the hooks themselves to poke
    around with user_mode(regs).
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 85fdc3d7c556..091379744d2f 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -947,9 +947,6 @@ int is_valid_bugaddr(unsigned long addr)
 
 static int bug_handler(struct pt_regs *regs, unsigned int esr)
 {
-	if (user_mode(regs))
-		return DBG_HOOK_ERROR;
-
 	switch (report_bug(regs->pc, regs)) {
 	case BUG_TRAP_TYPE_BUG:
 		die("Oops - BUG", regs, 0);
@@ -988,9 +985,6 @@ static int kasan_handler(struct pt_regs *regs, unsigned int esr)
 	u64 addr = regs->regs[0];
 	u64 pc = regs->pc;
 
-	if (user_mode(regs))
-		return DBG_HOOK_ERROR;
-
 	kasan_report(addr, size, write, pc);
 
 	/*

commit 26a04d84bc5311d7785b229b353f327e866ab61a
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 26 12:52:47 2019 +0000

    arm64: debug: Separate debug hooks based on target exception level
    
    Mixing kernel and user debug hooks together is highly error-prone as it
    relies on all of the hooks to figure out whether the exception came from
    kernel or user, and then to act accordingly.
    
    Make our debug hook code a little more robust by maintaining separate
    hook lists for user and kernel, with separate registration functions
    to force callers to be explicit about the exception levels that they
    care about.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8ad119c3f665..85fdc3d7c556 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -969,9 +969,8 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 }
 
 static struct break_hook bug_break_hook = {
-	.esr_val = 0xf2000000 | BUG_BRK_IMM,
-	.esr_mask = 0xffffffff,
 	.fn = bug_handler,
+	.imm = BUG_BRK_IMM,
 };
 
 #ifdef CONFIG_KASAN_SW_TAGS
@@ -1016,13 +1015,10 @@ static int kasan_handler(struct pt_regs *regs, unsigned int esr)
 	return DBG_HOOK_HANDLED;
 }
 
-#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
-#define KASAN_ESR_MASK 0xffffff00
-
 static struct break_hook kasan_break_hook = {
-	.esr_val = KASAN_ESR_VAL,
-	.esr_mask = KASAN_ESR_MASK,
-	.fn = kasan_handler,
+	.fn	= kasan_handler,
+	.imm	= KASAN_BRK_IMM,
+	.mask	= KASAN_BRK_MASK,
 };
 #endif
 
@@ -1034,7 +1030,9 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
 #ifdef CONFIG_KASAN_SW_TAGS
-	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+	unsigned int comment = esr & BRK64_ESR_MASK;
+
+	if ((comment & ~KASAN_BRK_MASK) == KASAN_BRK_IMM)
 		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
 #endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
@@ -1043,8 +1041,8 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 /* This registration must happen early, before debug_traps_init(). */
 void __init trap_init(void)
 {
-	register_break_hook(&bug_break_hook);
+	register_kernel_break_hook(&bug_break_hook);
 #ifdef CONFIG_KASAN_SW_TAGS
-	register_break_hook(&kasan_break_hook);
+	register_kernel_break_hook(&kasan_break_hook);
 #endif
 }

commit 1e6f5440a6814d28c32d347f338bfef68bc3e69d
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 8 17:56:34 2019 +0100

    arm64: backtrace: Don't bother trying to unwind the userspace stack
    
    Calling dump_backtrace() with a pt_regs argument corresponding to
    userspace doesn't make any sense and our unwinder will simply print
    "Call trace:" before unwinding the stack looking for user frames.
    
    Rather than go through this song and dance, just return early if we're
    passed a user register state.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 1149aad10b1e ("arm64: Add dump_backtrace() in show_regs")
    Reported-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8ad119c3f665..29755989f616 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -102,10 +102,16 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
-	int skip;
+	int skip = 0;
 
 	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
 
+	if (regs) {
+		if (user_mode(regs))
+			return;
+		skip = 1;
+	}
+
 	if (!tsk)
 		tsk = current;
 
@@ -126,7 +132,6 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	frame.graph = 0;
 #endif
 
-	skip = !!regs;
 	printk("Call trace:\n");
 	do {
 		/* skip until specified stack frame */
@@ -176,15 +181,13 @@ static int __die(const char *str, int err, struct pt_regs *regs)
 		return ret;
 
 	print_modules();
-	__show_regs(regs);
 	pr_emerg("Process %.*s (pid: %d, stack limit = 0x%p)\n",
 		 TASK_COMM_LEN, tsk->comm, task_pid_nr(tsk),
 		 end_of_stack(tsk));
+	show_regs(regs);
 
-	if (!user_mode(regs)) {
-		dump_backtrace(regs, tsk);
+	if (!user_mode(regs))
 		dump_instr(KERN_EMERG, regs);
-	}
 
 	return ret;
 }

commit 7d31464adf20fb8c075a3a3dfe2002a195566510
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:59:00 2019 +0000

    arm64: Handle serror in NMI context
    
    Per definition of the daifflags, Serrors can occur during any interrupt
    context, that includes NMI contexts. Trying to nmi_enter in an nmi context
    will crash.
    
    Skip nmi_enter/nmi_exit when serror occurred during an NMI.
    
    Suggested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 4e2fb877f8d5..8ad119c3f665 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -898,13 +898,17 @@ bool arm64_is_fatal_ras_serror(struct pt_regs *regs, unsigned int esr)
 
 asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)
 {
-	nmi_enter();
+	const bool was_in_nmi = in_nmi();
+
+	if (!was_in_nmi)
+		nmi_enter();
 
 	/* non-RAS errors are not containable */
 	if (!arm64_is_ras_serror(esr) || arm64_is_fatal_ras_serror(regs, esr))
 		arm64_serror_panic(regs, esr);
 
-	nmi_exit();
+	if (!was_in_nmi)
+		nmi_exit();
 }
 
 void __pte_error(const char *file, int line, unsigned long val)

commit 495d714ad140e1732e66c45d0409054b24c1a0d6
Merge: f12e840c819b 3d739c1f6156
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 31 11:46:59 2018 -0800

    Merge tag 'trace-v4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
    
     - Rework of the kprobe/uprobe and synthetic events to consolidate all
       the dynamic event code. This will make changes in the future easier.
    
     - Partial rewrite of the function graph tracing infrastructure. This
       will allow for multiple users of hooking onto functions to get the
       callback (return) of the function. This is the ground work for having
       kprobes and function graph tracer using one code base.
    
     - Clean up of the histogram code that will facilitate adding more
       features to the histograms in the future.
    
     - Addition of str_has_prefix() and a few use cases. There currently is
       a similar function strstart() that is used in a few places, but only
       returns a bool and not a length. These instances will be removed in
       the future to use str_has_prefix() instead.
    
     - A few other various clean ups as well.
    
    * tag 'trace-v4.21' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (57 commits)
      tracing: Use the return of str_has_prefix() to remove open coded numbers
      tracing: Have the historgram use the result of str_has_prefix() for len of prefix
      tracing: Use str_has_prefix() instead of using fixed sizes
      tracing: Use str_has_prefix() helper for histogram code
      string.h: Add str_has_prefix() helper function
      tracing: Make function ‘ftrace_exports’ static
      tracing: Simplify printf'ing in seq_print_sym
      tracing: Avoid -Wformat-nonliteral warning
      tracing: Merge seq_print_sym_short() and seq_print_sym_offset()
      tracing: Add hist trigger comments for variable-related fields
      tracing: Remove hist trigger synth_var_refs
      tracing: Use hist trigger's var_ref array to destroy var_refs
      tracing: Remove open-coding of hist trigger var_ref management
      tracing: Use var_refs[] for hist trigger reference checking
      tracing: Change strlen to sizeof for hist trigger static strings
      tracing: Remove unnecessary hist trigger struct field
      tracing: Fix ftrace_graph_get_ret_stack() to use task and not current
      seq_buf: Use size_t for len in seq_buf_puts()
      seq_buf: Make seq_buf_puts() null-terminate the buffer
      arm64: Use ftrace_graph_get_ret_stack() instead of curr_ret_stack
      ...

commit 41eea9cd239c5b3fff726894f85c97f60e5799a3
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:54 2018 -0800

    kasan, arm64: add brk handler for inline instrumentation
    
    Tag-based KASAN inline instrumentation mode (which embeds checks of shadow
    memory into the generated code, instead of inserting a callback) generates
    a brk instruction when a tag mismatch is detected.
    
    This commit adds a tag-based KASAN specific brk handler, that decodes the
    immediate value passed to the brk instructions (to extract information
    about the memory access that triggered the mismatch), reads the register
    values (x0 contains the guilty address) and reports the bug.
    
    Link: http://lkml.kernel.org/r/c91fe7684070e34dc34b419e6b69498f4dcacc2d.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5f4d9acb32f5..cdc71cf70aad 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -35,6 +35,7 @@
 #include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
+#include <linux/kasan.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
@@ -969,6 +970,58 @@ static struct break_hook bug_break_hook = {
 	.fn = bug_handler,
 };
 
+#ifdef CONFIG_KASAN_SW_TAGS
+
+#define KASAN_ESR_RECOVER	0x20
+#define KASAN_ESR_WRITE	0x10
+#define KASAN_ESR_SIZE_MASK	0x0f
+#define KASAN_ESR_SIZE(esr)	(1 << ((esr) & KASAN_ESR_SIZE_MASK))
+
+static int kasan_handler(struct pt_regs *regs, unsigned int esr)
+{
+	bool recover = esr & KASAN_ESR_RECOVER;
+	bool write = esr & KASAN_ESR_WRITE;
+	size_t size = KASAN_ESR_SIZE(esr);
+	u64 addr = regs->regs[0];
+	u64 pc = regs->pc;
+
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	kasan_report(addr, size, write, pc);
+
+	/*
+	 * The instrumentation allows to control whether we can proceed after
+	 * a crash was detected. This is done by passing the -recover flag to
+	 * the compiler. Disabling recovery allows to generate more compact
+	 * code.
+	 *
+	 * Unfortunately disabling recovery doesn't work for the kernel right
+	 * now. KASAN reporting is disabled in some contexts (for example when
+	 * the allocator accesses slab object metadata; this is controlled by
+	 * current->kasan_depth). All these accesses are detected by the tool,
+	 * even though the reports for them are not printed.
+	 *
+	 * This is something that might be fixed at some point in the future.
+	 */
+	if (!recover)
+		die("Oops - KASAN", regs, 0);
+
+	/* If thread survives, skip over the brk instruction and continue: */
+	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+	return DBG_HOOK_HANDLED;
+}
+
+#define KASAN_ESR_VAL (0xf2000000 | KASAN_BRK_IMM)
+#define KASAN_ESR_MASK 0xffffff00
+
+static struct break_hook kasan_break_hook = {
+	.esr_val = KASAN_ESR_VAL,
+	.esr_mask = KASAN_ESR_MASK,
+	.fn = kasan_handler,
+};
+#endif
+
 /*
  * Initial handler for AArch64 BRK exceptions
  * This handler only used until debug_traps_init().
@@ -976,6 +1029,10 @@ static struct break_hook bug_break_hook = {
 int __init early_brk64(unsigned long addr, unsigned int esr,
 		struct pt_regs *regs)
 {
+#ifdef CONFIG_KASAN_SW_TAGS
+	if ((esr & KASAN_ESR_MASK) == KASAN_ESR_VAL)
+		return kasan_handler(regs, esr) != DBG_HOOK_HANDLED;
+#endif
 	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
 }
 
@@ -983,4 +1040,7 @@ int __init early_brk64(unsigned long addr, unsigned int esr,
 void __init trap_init(void)
 {
 	register_break_hook(&bug_break_hook);
+#ifdef CONFIG_KASAN_SW_TAGS
+	register_break_hook(&kasan_break_hook);
+#endif
 }

commit a448276ce515c91cde4675be497364b91c764d95
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 7 13:13:28 2018 -0500

    arm64: Use ftrace_graph_get_ret_stack() instead of curr_ret_stack
    
    The structure of the ret_stack array on the task struct is going to
    change, and accessing it directly via the curr_ret_stack index will no
    longer give the ret_stack entry that holds the return address. To access
    that, architectures must now use ftrace_graph_get_ret_stack() to get the
    associated ret_stack that matches the saved return address.
    
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5f4d9acb32f5..49ebf3771391 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -122,7 +122,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		frame.pc = thread_saved_pc(tsk);
 	}
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	frame.graph = tsk->curr_ret_stack;
+	frame.graph = 0;
 #endif
 
 	skip = !!regs;

commit ba9f6f8954afa5224e3ed60332f7b92242b7ed0f
Merge: a978a5b8d83f a36700589b85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:22:39 2018 +0100

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "I have been slowly sorting out siginfo and this is the culmination of
      that work.
    
      The primary result is in several ways the signal infrastructure has
      been made less error prone. The code has been updated so that manually
      specifying SEND_SIG_FORCED is never necessary. The conversion to the
      new siginfo sending functions is now complete, which makes it
      difficult to send a signal without filling in the proper siginfo
      fields.
    
      At the tail end of the patchset comes the optimization of decreasing
      the size of struct siginfo in the kernel from 128 bytes to about 48
      bytes on 64bit. The fundamental observation that enables this is by
      definition none of the known ways to use struct siginfo uses the extra
      bytes.
    
      This comes at the cost of a small user space observable difference.
      For the rare case of siginfo being injected into the kernel only what
      can be copied into kernel_siginfo is delivered to the destination, the
      rest of the bytes are set to 0. For cases where the signal and the
      si_code are known this is safe, because we know those bytes are not
      used. For cases where the signal and si_code combination is unknown
      the bits that won't fit into struct kernel_siginfo are tested to
      verify they are zero, and the send fails if they are not.
    
      I made an extensive search through userspace code and I could not find
      anything that would break because of the above change. If it turns out
      I did break something it will take just the revert of a single change
      to restore kernel_siginfo to the same size as userspace siginfo.
    
      Testing did reveal dependencies on preferring the signo passed to
      sigqueueinfo over si->signo, so bit the bullet and added the
      complexity necessary to handle that case.
    
      Testing also revealed bad things can happen if a negative signal
      number is passed into the system calls. Something no sane application
      will do but something a malicious program or a fuzzer might do. So I
      have fixed the code that performs the bounds checks to ensure negative
      signal numbers are handled"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (80 commits)
      signal: Guard against negative signal numbers in copy_siginfo_from_user32
      signal: Guard against negative signal numbers in copy_siginfo_from_user
      signal: In sigqueueinfo prefer sig not si_signo
      signal: Use a smaller struct siginfo in the kernel
      signal: Distinguish between kernel_siginfo and siginfo
      signal: Introduce copy_siginfo_from_user and use it's return value
      signal: Remove the need for __ARCH_SI_PREABLE_SIZE and SI_PAD_SIZE
      signal: Fail sigqueueinfo if si_signo != sig
      signal/sparc: Move EMT_TAGOVF into the generic siginfo.h
      signal/unicore32: Use force_sig_fault where appropriate
      signal/unicore32: Generate siginfo in ucs32_notify_die
      signal/unicore32: Use send_sig_fault where appropriate
      signal/arc: Use force_sig_fault where appropriate
      signal/arc: Push siginfo generation into unhandled_exception
      signal/ia64: Use force_sig_fault where appropriate
      signal/ia64: Use the force_sig(SIGSEGV,...) in ia64_rt_sigreturn
      signal/ia64: Use the generic force_sigsegv in setup_frame
      signal/arm/kvm: Use send_sig_mceerr
      signal/arm: Use send_sig_fault where appropriate
      signal/arm: Use force_sig_fault where appropriate
      ...

commit c219bc4e920518feb025749bdf9623aa57e94b64
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Oct 1 12:19:43 2018 +0100

    arm64: Trap WFI executed in userspace
    
    It recently came to light that userspace can execute WFI, and that
    the arm64 kernel doesn't trap this event. This sounds rather benign,
    but the kernel should decide when it wants to wait for an interrupt,
    and not userspace.
    
    Let's trap WFI and immediately return after having skipped the
    instruction. This effectively makes WFI a rather expensive NOP.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 58134a97928f..4066da7f1e5e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -508,6 +508,11 @@ static void mrs_handler(unsigned int esr, struct pt_regs *regs)
 		force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 }
 
+static void wfi_handler(unsigned int esr, struct pt_regs *regs)
+{
+	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
+}
+
 struct sys64_hook {
 	unsigned int esr_mask;
 	unsigned int esr_val;
@@ -544,6 +549,12 @@ static struct sys64_hook sys64_hooks[] = {
 		.esr_val = ESR_ELx_SYS64_ISS_SYS_MRS_OP_VAL,
 		.handler = mrs_handler,
 	},
+	{
+		/* Trap WFI instructions executed in userspace */
+		.esr_mask = ESR_ELx_WFx_MASK,
+		.esr_val = ESR_ELx_WFx_WFI_VAL,
+		.handler = wfi_handler,
+	},
 	{},
 };
 

commit 32a3e635fb0ecc1b197d54f710e76c6481cf19f0
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:33 2018 +0100

    arm64: compat: Add CNTFRQ trap handler
    
    Just like CNTVCT, we need to handle userspace trapping into the
    kernel if we're decided that the timer wasn't fit for purpose...
    64bit userspace is already dealt with, but we're missing the
    equivalent compat handling.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3602b900ff1c..58134a97928f 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -625,7 +625,20 @@ static void arm64_compat_skip_faulting_instruction(struct pt_regs *regs,
 	arm64_skip_faulting_instruction(regs, sz);
 }
 
+static void compat_cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
+{
+	int reg = (esr & ESR_ELx_CP15_32_ISS_RT_MASK) >> ESR_ELx_CP15_32_ISS_RT_SHIFT;
+
+	pt_regs_write_reg(regs, reg, arch_timer_get_rate());
+	arm64_compat_skip_faulting_instruction(regs, 4);
+}
+
 static struct sys64_hook cp15_32_hooks[] = {
+	{
+		.esr_mask = ESR_ELx_CP15_32_ISS_SYS_MASK,
+		.esr_val = ESR_ELx_CP15_32_ISS_SYS_CNTFRQ,
+		.handler = compat_cntfrq_read_handler,
+	},
 	{},
 };
 

commit 50de013d22e4e112d7b0778a0e7d032f16c46778
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:32 2018 +0100

    arm64: compat: Add CNTVCT trap handler
    
    Since people seem to make a point in breaking the userspace visible
    counter, we have no choice but to trap the access. We already do this
    for 64bit userspace, but this is lacking for compat. Let's provide
    the required handler.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 76ffb9f42aa4..3602b900ff1c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -629,7 +629,23 @@ static struct sys64_hook cp15_32_hooks[] = {
 	{},
 };
 
+static void compat_cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
+{
+	int rt = (esr & ESR_ELx_CP15_64_ISS_RT_MASK) >> ESR_ELx_CP15_64_ISS_RT_SHIFT;
+	int rt2 = (esr & ESR_ELx_CP15_64_ISS_RT2_MASK) >> ESR_ELx_CP15_64_ISS_RT2_SHIFT;
+	u64 val = arch_counter_get_cntvct();
+
+	pt_regs_write_reg(regs, rt, lower_32_bits(val));
+	pt_regs_write_reg(regs, rt2, upper_32_bits(val));
+	arm64_compat_skip_faulting_instruction(regs, 4);
+}
+
 static struct sys64_hook cp15_64_hooks[] = {
+	{
+		.esr_mask = ESR_ELx_CP15_64_ISS_SYS_MASK,
+		.esr_val = ESR_ELx_CP15_64_ISS_SYS_CNTVCT,
+		.handler = compat_cntvct_read_handler,
+	},
 	{},
 };
 

commit 2a8905e18c55d5576d7a53da495b4de0cfcbc459
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:31 2018 +0100

    arm64: compat: Add cp15_32 and cp15_64 handler arrays
    
    We're now ready to start handling CP15 access. Let's add (empty)
    arrays for both 32 and 64bit accessors, and the code that deals
    with them.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 95a646c154fe..76ffb9f42aa4 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -625,8 +625,18 @@ static void arm64_compat_skip_faulting_instruction(struct pt_regs *regs,
 	arm64_skip_faulting_instruction(regs, sz);
 }
 
+static struct sys64_hook cp15_32_hooks[] = {
+	{},
+};
+
+static struct sys64_hook cp15_64_hooks[] = {
+	{},
+};
+
 asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
 {
+	struct sys64_hook *hook, *hook_base;
+
 	if (!cp15_cond_valid(esr, regs)) {
 		/*
 		 * There is no T16 variant of a CP access, so we
@@ -636,6 +646,24 @@ asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
 		return;
 	}
 
+	switch (ESR_ELx_EC(esr)) {
+	case ESR_ELx_EC_CP15_32:
+		hook_base = cp15_32_hooks;
+		break;
+	case ESR_ELx_EC_CP15_64:
+		hook_base = cp15_64_hooks;
+		break;
+	default:
+		do_undefinstr(regs);
+		return;
+	}
+
+	for (hook = hook_base; hook->handler; hook++)
+		if ((hook->esr_mask & esr) == hook->esr_val) {
+			hook->handler(esr, regs);
+			return;
+		}
+
 	/*
 	 * New cp15 instructions may previously have been undefined at
 	 * EL0. Fall back to our usual undefined instruction handler

commit 1f1c014035a8084a768e7e902c6f5857995b1220
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:30 2018 +0100

    arm64: compat: Add condition code checks and IT advance
    
    Here's a /really nice/ part of the architecture: a CP15 access is
    allowed to trap even if it fails its condition check, and SW must
    handle it. This includes decoding the IT state if this happens in
    am IT block. As a consequence, SW must also deal with advancing
    the IT state machine.
    
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 0e2665936493..95a646c154fe 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -549,8 +549,93 @@ static struct sys64_hook sys64_hooks[] = {
 
 
 #ifdef CONFIG_COMPAT
+#define PSTATE_IT_1_0_SHIFT	25
+#define PSTATE_IT_1_0_MASK	(0x3 << PSTATE_IT_1_0_SHIFT)
+#define PSTATE_IT_7_2_SHIFT	10
+#define PSTATE_IT_7_2_MASK	(0x3f << PSTATE_IT_7_2_SHIFT)
+
+static u32 compat_get_it_state(struct pt_regs *regs)
+{
+	u32 it, pstate = regs->pstate;
+
+	it  = (pstate & PSTATE_IT_1_0_MASK) >> PSTATE_IT_1_0_SHIFT;
+	it |= ((pstate & PSTATE_IT_7_2_MASK) >> PSTATE_IT_7_2_SHIFT) << 2;
+
+	return it;
+}
+
+static void compat_set_it_state(struct pt_regs *regs, u32 it)
+{
+	u32 pstate_it;
+
+	pstate_it  = (it << PSTATE_IT_1_0_SHIFT) & PSTATE_IT_1_0_MASK;
+	pstate_it |= ((it >> 2) << PSTATE_IT_7_2_SHIFT) & PSTATE_IT_7_2_MASK;
+
+	regs->pstate &= ~PSR_AA32_IT_MASK;
+	regs->pstate |= pstate_it;
+}
+
+static bool cp15_cond_valid(unsigned int esr, struct pt_regs *regs)
+{
+	int cond;
+
+	/* Only a T32 instruction can trap without CV being set */
+	if (!(esr & ESR_ELx_CV)) {
+		u32 it;
+
+		it = compat_get_it_state(regs);
+		if (!it)
+			return true;
+
+		cond = it >> 4;
+	} else {
+		cond = (esr & ESR_ELx_COND_MASK) >> ESR_ELx_COND_SHIFT;
+	}
+
+	return aarch32_opcode_cond_checks[cond](regs->pstate);
+}
+
+static void advance_itstate(struct pt_regs *regs)
+{
+	u32 it;
+
+	/* ARM mode */
+	if (!(regs->pstate & PSR_AA32_T_BIT) ||
+	    !(regs->pstate & PSR_AA32_IT_MASK))
+		return;
+
+	it  = compat_get_it_state(regs);
+
+	/*
+	 * If this is the last instruction of the block, wipe the IT
+	 * state. Otherwise advance it.
+	 */
+	if (!(it & 7))
+		it = 0;
+	else
+		it = (it & 0xe0) | ((it << 1) & 0x1f);
+
+	compat_set_it_state(regs, it);
+}
+
+static void arm64_compat_skip_faulting_instruction(struct pt_regs *regs,
+						   unsigned int sz)
+{
+	advance_itstate(regs);
+	arm64_skip_faulting_instruction(regs, sz);
+}
+
 asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
 {
+	if (!cp15_cond_valid(esr, regs)) {
+		/*
+		 * There is no T16 variant of a CP access, so we
+		 * always advance PC by 4 bytes.
+		 */
+		arm64_compat_skip_faulting_instruction(regs, 4);
+		return;
+	}
+
 	/*
 	 * New cp15 instructions may previously have been undefined at
 	 * EL0. Fall back to our usual undefined instruction handler

commit 70c63cdfd6ee615714c5453cff370032587723c2
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Thu Sep 27 17:15:29 2018 +0100

    arm64: compat: Add separate CP15 trapping hook
    
    Instead of directly generating an UNDEF when trapping a CP15 access,
    let's add a new entry point to that effect (which only generates an
    UNDEF for now).
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 21689c6a985f..0e2665936493 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -547,6 +547,19 @@ static struct sys64_hook sys64_hooks[] = {
 	{},
 };
 
+
+#ifdef CONFIG_COMPAT
+asmlinkage void __exception do_cp15instr(unsigned int esr, struct pt_regs *regs)
+{
+	/*
+	 * New cp15 instructions may previously have been undefined at
+	 * EL0. Fall back to our usual undefined instruction handler
+	 * so that we handle these consistently.
+	 */
+	do_undefinstr(regs);
+}
+#endif
+
 asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
 {
 	struct sys64_hook *hook;

commit f3a900b34101bb8df10b83f326b3af796c101a05
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:52:41 2018 +0200

    signal/arm64: Add and use arm64_force_sig_ptrace_errno_trap
    
    Add arm64_force_sig_ptrace_errno_trap for consistency with
    arm64_force_sig_fault and use it where appropriate.
    
    This adds the show_signal logic to the force_sig_errno_trap case,
    where it was apparently overlooked earlier.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index de67818258cd..856b32aa03d8 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -262,6 +262,13 @@ void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,
 	force_sig_mceerr(code, addr, lsb, current);
 }
 
+void arm64_force_sig_ptrace_errno_trap(int errno, void __user *addr,
+				       const char *str)
+{
+	arm64_show_signal(SIGTRAP, str);
+	force_sig_ptrace_errno_trap(errno, addr);
+}
+
 void arm64_notify_die(const char *str, struct pt_regs *regs,
 		      int signo, int sicode, void __user *addr,
 		      int err)

commit 009f608ab20a25d01a07e9e75e7d246e81252eb8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:43:01 2018 +0200

    signal/arm64: Remove arm64_force_sig_info
    
    The function has no more callers so remove it.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index baa96dfffeec..de67818258cd 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -262,12 +262,6 @@ void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,
 	force_sig_mceerr(code, addr, lsb, current);
 }
 
-void arm64_force_sig_info(struct siginfo *info, const char *str)
-{
-	arm64_show_signal(info->si_signo, str);
-	force_sig_info(info->si_signo, info, current);
-}
-
 void arm64_notify_die(const char *str, struct pt_regs *regs,
 		      int signo, int sicode, void __user *addr,
 		      int err)

commit b4d5557caa07a01796ca8a2d756eeaa5308f6876
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:37:15 2018 +0200

    signal/arm64: Add and use arm64_force_sig_mceerr as appropriate
    
    Add arm64_force_sig_mceerr for consistency with arm64_force_sig_fault,
    and use it in the one location that can take advantage of it.
    
    This removes the fiddly filling out of siginfo before sending a signal
    reporting an memory error to userspace.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 37a3309863e0..baa96dfffeec 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -255,6 +255,13 @@ void arm64_force_sig_fault(int signo, int code, void __user *addr,
 	force_sig_fault(signo, code, addr, current);
 }
 
+void arm64_force_sig_mceerr(int code, void __user *addr, short lsb,
+			    const char *str)
+{
+	arm64_show_signal(SIGBUS, str);
+	force_sig_mceerr(code, addr, lsb, current);
+}
+
 void arm64_force_sig_info(struct siginfo *info, const char *str)
 {
 	arm64_show_signal(info->si_signo, str);

commit feca355b3d8eba3a2cbca63c97a59a14681983f7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:26:57 2018 +0200

    signal/arm64: Add and use arm64_force_sig_fault where appropriate
    
    Wrap force_sig_fault with a helper that calls arm64_show_signal
    and call arm64_force_sig_fault where appropraite.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index daee8c2ca561..37a3309863e0 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -248,6 +248,13 @@ static void arm64_show_signal(int signo, const char *str)
 	__show_regs(regs);
 }
 
+void arm64_force_sig_fault(int signo, int code, void __user *addr,
+			   const char *str)
+{
+	arm64_show_signal(signo, str);
+	force_sig_fault(signo, code, addr, current);
+}
+
 void arm64_force_sig_info(struct siginfo *info, const char *str)
 {
 	arm64_show_signal(info->si_signo, str);
@@ -259,19 +266,11 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 		      int err)
 {
 	if (user_mode(regs)) {
-		struct siginfo info;
-
 		WARN_ON(regs != current_pt_regs());
 		current->thread.fault_address = 0;
 		current->thread.fault_code = err;
 
-		clear_siginfo(&info);
-		info.si_signo = signo;
-		info.si_errno = 0;
-		info.si_code  = sicode;
-		info.si_addr  = addr;
-
-		arm64_force_sig_info(&info, str);
+		arm64_force_sig_fault(signo, sicode, addr, str);
 	} else {
 		die(str, regs, err);
 	}
@@ -616,19 +615,13 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
  */
 asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 {
-	siginfo_t info;
 	void __user *pc = (void __user *)instruction_pointer(regs);
 
-	clear_siginfo(&info);
-	info.si_signo = SIGILL;
-	info.si_errno = 0;
-	info.si_code  = ILL_ILLOPC;
-	info.si_addr  = pc;
-
 	current->thread.fault_address = 0;
 	current->thread.fault_code = esr;
 
-	arm64_force_sig_info(&info, "Bad EL0 synchronous exception");
+	arm64_force_sig_fault(SIGILL, ILL_ILLOPC, pc,
+			      "Bad EL0 synchronous exception");
 }
 
 #ifdef CONFIG_VMAP_STACK

commit 1628a7cc85db7eced9cd8195fd66574a35470825
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 00:52:21 2018 +0200

    signal/arm64: Factor out arm64_show_signal from arm64_force_sig_info
    
    Filling in siginfo is error prone and so it is wise to use more
    specialized helpers to do that work.  Factor out the arm specific
    unhandled signal reporting from the work of delivering a signal so
    the code can be modified to use functions that take the information
    to fill out siginfo as parameters.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 24035d124608..daee8c2ca561 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -224,24 +224,19 @@ void die(const char *str, struct pt_regs *regs, int err)
 		do_exit(SIGSEGV);
 }
 
-static bool show_unhandled_signals_ratelimited(void)
+static void arm64_show_signal(int signo, const char *str)
 {
 	static DEFINE_RATELIMIT_STATE(rs, DEFAULT_RATELIMIT_INTERVAL,
 				      DEFAULT_RATELIMIT_BURST);
-	return show_unhandled_signals && __ratelimit(&rs);
-}
-
-void arm64_force_sig_info(struct siginfo *info, const char *str)
-{
 	struct task_struct *tsk = current;
 	unsigned int esr = tsk->thread.fault_code;
 	struct pt_regs *regs = task_pt_regs(tsk);
 
-	if (!unhandled_signal(tsk, info->si_signo))
-		goto send_sig;
-
-	if (!show_unhandled_signals_ratelimited())
-		goto send_sig;
+	/* Leave if the signal won't be shown */
+	if (!show_unhandled_signals ||
+	    !unhandled_signal(tsk, signo) ||
+	    !__ratelimit(&rs))
+		return;
 
 	pr_info("%s[%d]: unhandled exception: ", tsk->comm, task_pid_nr(tsk));
 	if (esr)
@@ -251,9 +246,12 @@ void arm64_force_sig_info(struct siginfo *info, const char *str)
 	print_vma_addr(KERN_CONT " in ", regs->pc);
 	pr_cont("\n");
 	__show_regs(regs);
+}
 
-send_sig:
-	force_sig_info(info->si_signo, info, tsk);
+void arm64_force_sig_info(struct siginfo *info, const char *str)
+{
+	arm64_show_signal(info->si_signo, str);
+	force_sig_info(info->si_signo, info, current);
 }
 
 void arm64_notify_die(const char *str, struct pt_regs *regs,

commit 24b8f79dd8e036da618d158b4c0295208d478c5c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 00:38:41 2018 +0200

    signal/arm64: Remove unneeded tsk parameter from arm64_force_sig_info
    
    Every caller passes in current for tsk so there is no need to pass
    tsk.  Instead make tsk a local variable initialized to current.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 459eb6fb7158..24035d124608 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -231,9 +231,9 @@ static bool show_unhandled_signals_ratelimited(void)
 	return show_unhandled_signals && __ratelimit(&rs);
 }
 
-void arm64_force_sig_info(struct siginfo *info, const char *str,
-			  struct task_struct *tsk)
+void arm64_force_sig_info(struct siginfo *info, const char *str)
 {
+	struct task_struct *tsk = current;
 	unsigned int esr = tsk->thread.fault_code;
 	struct pt_regs *regs = task_pt_regs(tsk);
 
@@ -273,7 +273,7 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 		info.si_code  = sicode;
 		info.si_addr  = addr;
 
-		arm64_force_sig_info(&info, str, current);
+		arm64_force_sig_info(&info, str);
 	} else {
 		die(str, regs, err);
 	}
@@ -630,7 +630,7 @@ asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 	current->thread.fault_address = 0;
 	current->thread.fault_code = esr;
 
-	arm64_force_sig_info(&info, "Bad EL0 synchronous exception", current);
+	arm64_force_sig_info(&info, "Bad EL0 synchronous exception");
 }
 
 #ifdef CONFIG_VMAP_STACK

commit 6fa998e83ef9bcc479b0fa088de262a73e139bf8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Sep 21 17:24:40 2018 +0200

    signal/arm64: Push siginfo generation into arm64_notify_die
    
    Instead of generating a struct siginfo before calling arm64_notify_die
    pass the signal number, tne sicode and the fault address into
    arm64_notify_die and have it call force_sig_fault instead of
    force_sig_info to let the generic code generate the struct siginfo.
    
    This keeps code passing just the needed information into
    siginfo generating code, making it easier to see what
    is happening and harder to get wrong.  Further by letting
    the generic code handle the generation of struct siginfo
    it reduces the number of sites generating struct siginfo
    making it possible to review them and verify that all
    of the fiddly details for a structure passed to userspace
    are handled properly.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 039e9ff379cc..459eb6fb7158 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -257,13 +257,23 @@ void arm64_force_sig_info(struct siginfo *info, const char *str,
 }
 
 void arm64_notify_die(const char *str, struct pt_regs *regs,
-		      struct siginfo *info, int err)
+		      int signo, int sicode, void __user *addr,
+		      int err)
 {
 	if (user_mode(regs)) {
+		struct siginfo info;
+
 		WARN_ON(regs != current_pt_regs());
 		current->thread.fault_address = 0;
 		current->thread.fault_code = err;
-		arm64_force_sig_info(info, str, current);
+
+		clear_siginfo(&info);
+		info.si_signo = signo;
+		info.si_errno = 0;
+		info.si_code  = sicode;
+		info.si_addr  = addr;
+
+		arm64_force_sig_info(&info, str, current);
 	} else {
 		die(str, regs, err);
 	}
@@ -348,12 +358,9 @@ static int call_undef_hook(struct pt_regs *regs)
 
 void force_signal_inject(int signal, int code, unsigned long address)
 {
-	siginfo_t info;
 	const char *desc;
 	struct pt_regs *regs = current_pt_regs();
 
-	clear_siginfo(&info);
-
 	switch (signal) {
 	case SIGILL:
 		desc = "undefined instruction";
@@ -372,12 +379,7 @@ void force_signal_inject(int signal, int code, unsigned long address)
 		signal = SIGKILL;
 	}
 
-	info.si_signo = signal;
-	info.si_errno = 0;
-	info.si_code  = code;
-	info.si_addr  = (void __user *)address;
-
-	arm64_notify_die(desc, regs, &info, 0);
+	arm64_notify_die(desc, regs, signal, code, (void __user *)address, 0);
 }
 
 /*

commit 21f84796177443695680180a8493a9e20d254d5e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Thu Sep 20 09:36:21 2018 +0530

    arm64/cpufeatures: Emulate MRS instructions by parsing ESR_ELx.ISS
    
    Armv8.4-A extension enables MRS instruction encodings inside ESR_ELx.ISS
    during exception class ESR_ELx_EC_SYS64 (0x18). This encoding can be used
    to emulate MRS instructions which can avoid fetch/decode from user space
    thus improving performance. This adds a new sys64_hook structure element
    with applicable ESR mask/value pair for MRS instructions on various system
    registers but constrained by sysreg encodings which is currently allowed
    to be emulated.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index abfb304e1025..21689c6a985f 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -497,6 +497,17 @@ static void cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }
 
+static void mrs_handler(unsigned int esr, struct pt_regs *regs)
+{
+	u32 sysreg, rt;
+
+	rt = ESR_ELx_SYS64_ISS_RT(esr);
+	sysreg = esr_sys64_to_sysreg(esr);
+
+	if (do_emulate_mrs(regs, sysreg, rt) != 0)
+		force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
+}
+
 struct sys64_hook {
 	unsigned int esr_mask;
 	unsigned int esr_val;
@@ -527,6 +538,12 @@ static struct sys64_hook sys64_hooks[] = {
 		.esr_val = ESR_ELx_SYS64_ISS_SYS_CNTFRQ,
 		.handler = cntfrq_read_handler,
 	},
+	{
+		/* Trap read access to CPUID registers */
+		.esr_mask = ESR_ELx_SYS64_ISS_SYS_MRS_OP_MASK,
+		.esr_val = ESR_ELx_SYS64_ISS_SYS_MRS_OP_VAL,
+		.handler = mrs_handler,
+	},
 	{},
 };
 

commit 1c8391412d7794e0b38393ed98fef9a974401f05
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Thu Sep 20 09:36:19 2018 +0530

    arm64/cpufeatures: Introduce ESR_ELx_SYS64_ISS_RT()
    
    Extracting target register from ESR.ISS encoding has already been required
    at multiple instances. Just make it a macro definition and replace all the
    existing use cases.
    
    Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ce29973b9bfe..abfb304e1025 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -438,7 +438,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 {
 	unsigned long address;
-	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 	int crm = (esr & ESR_ELx_SYS64_ISS_CRM_MASK) >> ESR_ELx_SYS64_ISS_CRM_SHIFT;
 	int ret = 0;
 
@@ -473,7 +473,7 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 
 static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 {
-	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 	unsigned long val = arm64_ftr_reg_user_value(&arm64_ftr_reg_ctrel0);
 
 	pt_regs_write_reg(regs, rt, val);
@@ -483,7 +483,7 @@ static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 
 static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 {
-	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 
 	pt_regs_write_reg(regs, rt, arch_counter_get_cntvct());
 	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
@@ -491,7 +491,7 @@ static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 
 static void cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 {
-	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+	int rt = ESR_ELx_SYS64_ISS_RT(esr);
 
 	pt_regs_write_reg(regs, rt, arch_timer_get_rate());
 	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);

commit e4ba15debcfd27f60d43da940a58108783bff2a6
Author: Hari Vyas <hari.vyas@broadcom.com>
Date:   Tue Aug 7 16:33:48 2018 +0530

    arm64: fix for bad_mode() handler to always result in panic
    
    The bad_mode() handler is called if we encounter an uunknown exception,
    with the expectation that the subsequent call to panic() will halt the
    system. Unfortunately, if the exception calling bad_mode() is taken from
    EL0, then the call to die() can end up killing the current user task and
    calling schedule() instead of falling through to panic().
    
    Remove the die() call altogether, since we really want to bring down the
    machine in this "impossible" case.
    
    Signed-off-by: Hari Vyas <hari.vyas@broadcom.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 539b470f9526..ce29973b9bfe 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -606,7 +606,6 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 		handler[reason], smp_processor_id(), esr,
 		esr_get_class_string(esr));
 
-	die("Oops - bad mode", regs, 0);
 	local_daif_mask();
 	panic("bad mode");
 }

commit 8a60419d36762a1131c2b29f7bd14371db4df1b5
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 14 16:24:54 2018 +0100

    arm64: force_signal_inject: WARN if called from kernel context
    
    force_signal_inject() is designed to send a fatal signal to userspace,
    so WARN if the current pt_regs indicates a kernel context. This can
    currently happen for the undefined instruction trap, so patch that up so
    we always BUG() if we didn't have a handler.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 148de417ed3e..539b470f9526 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -354,6 +354,9 @@ void force_signal_inject(int signal, int code, unsigned long address)
 	const char *desc;
 	struct pt_regs *regs = current_pt_regs();
 
+	if (WARN_ON(!user_mode(regs)))
+		return;
+
 	clear_siginfo(&info);
 
 	switch (signal) {
@@ -408,8 +411,8 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	if (call_undef_hook(regs) == 0)
 		return;
 
-	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 	BUG_ON(!user_mode(regs));
+	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 }
 
 #define __user_cache_maint(insn, address, res)			\

commit b8925ee2e12d1cb9a11d6f28b5814f2bfa59dce1
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:53:41 2018 +0100

    arm64: cpu: Move errata and feature enable callbacks closer to callers
    
    The cpu errata and feature enable callbacks are only called via their
    respective arm64_cpu_capabilities structure and therefore shouldn't
    exist in the global namespace.
    
    Move the PAN, RAS and cache maintenance emulation enable callbacks into
    the same files as their corresponding arm64_cpu_capabilities structures,
    making them static in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index b9da093e0341..148de417ed3e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -412,11 +412,6 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	BUG_ON(!user_mode(regs));
 }
 
-void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
-{
-	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCI, 0);
-}
-
 #define __user_cache_maint(insn, address, res)			\
 	if (address >= user_addr_max()) {			\
 		res = -EFAULT;					\

commit 0bf0f444b2c49241b2b39aa3cf210d7c95ef6c34
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 7 13:43:06 2018 +0100

    arm64: entry: Allow handling of undefined instructions from EL1
    
    Rather than panic() when taking an undefined instruction exception from
    EL1, allow a hook to be registered in case we want to emulate the
    instruction, like we will for the SSBS PSTATE manipulation instructions.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 039e9ff379cc..b9da093e0341 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -310,10 +310,12 @@ static int call_undef_hook(struct pt_regs *regs)
 	int (*fn)(struct pt_regs *regs, u32 instr) = NULL;
 	void __user *pc = (void __user *)instruction_pointer(regs);
 
-	if (!user_mode(regs))
-		return 1;
-
-	if (compat_thumb_mode(regs)) {
+	if (!user_mode(regs)) {
+		__le32 instr_le;
+		if (probe_kernel_address((__force __le32 *)pc, instr_le))
+			goto exit;
+		instr = le32_to_cpu(instr_le);
+	} else if (compat_thumb_mode(regs)) {
 		/* 16-bit Thumb instruction */
 		__le16 instr_le;
 		if (get_user(instr_le, (__le16 __user *)pc))
@@ -407,6 +409,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 		return;
 
 	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
+	BUG_ON(!user_mode(regs));
 }
 
 void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)

commit 4141c857fd09dbed480f021b3eece4f46c653161
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:43 2018 +0100

    arm64: convert raw syscall invocation to C
    
    As a first step towards invoking syscalls with a pt_regs argument,
    convert the raw syscall invocation logic to C. We end up with a bit more
    register shuffling, but the unified invocation logic means we can unify
    the tracing paths, too.
    
    Previously, assembly had to open-code calls to ni_sys() when the system
    call number was out-of-bounds for the relevant syscall table. This case
    is now handled by invoke_syscall(), and the assembly no longer need to
    handle this case explicitly. This allows the tracing paths to be
    simplified and unified, as we no longer need the __ni_sys_trace path and
    the __sys_trace_return label.
    
    This only converts the invocation of the syscall. The rest of the
    syscall triage and tracing is left in assembly for now, and will be
    converted in subsequent patches.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c27292703bd1..039e9ff379cc 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -547,22 +547,6 @@ asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
 	do_undefinstr(regs);
 }
 
-long compat_arm_syscall(struct pt_regs *regs);
-
-asmlinkage long do_ni_syscall(struct pt_regs *regs)
-{
-#ifdef CONFIG_COMPAT
-	long ret;
-	if (is_compat_task()) {
-		ret = compat_arm_syscall(regs);
-		if (ret != -ENOSYS)
-			return ret;
-	}
-#endif
-
-	return sys_ni_syscall();
-}
-
 static const char *esr_class_str[] = {
 	[0 ... ESR_ELx_EC_MAX]		= "UNRECOGNIZED EC",
 	[ESR_ELx_EC_UNKNOWN]		= "Unknown/Uncategorized",

commit 25be597ada0b49d2748ab520a78a28c1764d69e4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:38 2018 +0100

    arm64: kill config_sctlr_el1()
    
    Now that we have sysreg_clear_set(), we can consistently use this
    instead of config_sctlr_el1().
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d399d459397b..c27292703bd1 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -411,7 +411,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 
 void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 {
-	config_sctlr_el1(SCTLR_EL1_UCI, 0);
+	sysreg_clear_set(sctlr_el1, SCTLR_EL1_UCI, 0);
 }
 
 #define __user_cache_maint(insn, address, res)			\

commit 93e95fa57441b6976b39029bd658b6bbe7ccfe28
Merge: d8aed8415b86 26da35010c6d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 15:23:48 2018 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "This set of changes close the known issues with setting si_code to an
      invalid value, and with not fully initializing struct siginfo. There
      remains work to do on nds32, arc, unicore32, powerpc, arm, arm64, ia64
      and x86 to get the code that generates siginfo into a simpler and more
      maintainable state. Most of that work involves refactoring the signal
      handling code and thus careful code review.
    
      Also not included is the work to shrink the in kernel version of
      struct siginfo. That depends on getting the number of places that
      directly manipulate struct siginfo under control, as it requires the
      introduction of struct kernel_siginfo for the in kernel things.
    
      Overall this set of changes looks like it is making good progress, and
      with a little luck I will be wrapping up the siginfo work next
      development cycle"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (46 commits)
      signal/sh: Stop gcc warning about an impossible case in do_divide_error
      signal/mips: Report FPE_FLTUNK for undiagnosed floating point exceptions
      signal/um: More carefully relay signals in relay_signal.
      signal: Extend siginfo_layout with SIL_FAULT_{MCEERR|BNDERR|PKUERR}
      signal: Remove unncessary #ifdef SEGV_PKUERR in 32bit compat code
      signal/signalfd: Add support for SIGSYS
      signal/signalfd: Remove __put_user from signalfd_copyinfo
      signal/xtensa: Use force_sig_fault where appropriate
      signal/xtensa: Consistenly use SIGBUS in do_unaligned_user
      signal/um: Use force_sig_fault where appropriate
      signal/sparc: Use force_sig_fault where appropriate
      signal/sparc: Use send_sig_fault where appropriate
      signal/sh: Use force_sig_fault where appropriate
      signal/s390: Use force_sig_fault where appropriate
      signal/riscv: Replace do_trap_siginfo with force_sig_fault
      signal/riscv: Use force_sig_fault where appropriate
      signal/parisc: Use force_sig_fault where appropriate
      signal/parisc: Use force_sig_mceerr where appropriate
      signal/openrisc: Use force_sig_fault where appropriate
      signal/nios2: Use force_sig_fault where appropriate
      ...

commit 3eb0f5193b497083391aa05d35210d5645211eef
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 17 15:26:37 2018 -0500

    signal: Ensure every siginfo we send has all bits initialized
    
    Call clear_siginfo to ensure every stack allocated siginfo is properly
    initialized before being passed to the signal sending functions.
    
    Note: It is not safe to depend on C initializers to initialize struct
    siginfo on the stack because C is allowed to skip holes when
    initializing a structure.
    
    The initialization of struct siginfo in tracehook_report_syscall_exit
    was moved from the helper user_single_step_siginfo into
    tracehook_report_syscall_exit itself, to make it clear that the local
    variable siginfo gets fully initialized.
    
    In a few cases the scope of struct siginfo has been reduced to make it
    clear that siginfo siginfo is not used on other paths in the function
    in which it is declared.
    
    Instances of using memset to initialize siginfo have been replaced
    with calls clear_siginfo for clarity.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ba964da31a25..7f476586cacc 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -634,6 +634,7 @@ asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 	siginfo_t info;
 	void __user *pc = (void __user *)instruction_pointer(regs);
 
+	clear_siginfo(&info);
 	info.si_signo = SIGILL;
 	info.si_errno = 0;
 	info.si_code  = ILL_ILLOPC;

commit 9478f1927e6ef9ef5e1ad761af1c98aa8e40b7f5
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 3 11:22:51 2018 +0100

    arm64: only advance singlestep for user instruction traps
    
    Our arm64_skip_faulting_instruction() helper advances the userspace
    singlestep state machine, but this is also called by the kernel BRK
    handler, as used for WARN*().
    
    Thus, if we happen to hit a WARN*() while the user singlestep state
    machine is in the active-no-pending state, we'll advance to the
    active-pending state without having executed a user instruction, and
    will take a step exception earlier than expected when we return to
    userspace.
    
    Let's fix this by only advancing the state machine when skipping a user
    instruction.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Andrey Konovalov <andreyknvl@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 1cb2749a72bf..8bbdc17e49df 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -277,7 +277,8 @@ void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
 	 * If we were single stepping, we want to get the step exception after
 	 * we return from the trap.
 	 */
-	user_fastforward_single_step(current);
+	if (user_mode(regs))
+		user_fastforward_single_step(current);
 }
 
 static LIST_HEAD(undef_hook);

commit b2d71b3cda19831ec67f49d7c6ba0214d9367b29
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Apr 16 16:45:01 2018 +0100

    arm64: signal: don't force known signals to SIGKILL
    
    Since commit:
    
      a7e6f1ca90354a31 ("arm64: signal: Force SIGKILL for unknown signals in force_signal_inject")
    
    ... any signal which is not SIGKILL will be upgraded to a SIGKILL be
    force_signal_inject(). This includes signals we do expect, such as
    SIGILL triggered by do_undefinstr().
    
    Fix the check to use a logical AND rather than a logical OR, permitting
    signals whose layout is SIL_FAULT.
    
    Fixes: a7e6f1ca90354a31 ("arm64: signal: Force SIGKILL for unknown signals in force_signal_inject")
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index ba964da31a25..1cb2749a72bf 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -366,7 +366,7 @@ void force_signal_inject(int signal, int code, unsigned long address)
 	}
 
 	/* Force signals we don't understand to SIGKILL */
-	if (WARN_ON(signal != SIGKILL ||
+	if (WARN_ON(signal != SIGKILL &&
 		    siginfo_layout(signal, code) != SIL_FAULT)) {
 		signal = SIGKILL;
 	}

commit c0cda3b8ee6b4b6851b2fd8b6db91fd7b0e2524a
Author: Dave Martin <dave.martin@arm.com>
Date:   Mon Mar 26 15:12:28 2018 +0100

    arm64: capabilities: Update prototype for enable call back
    
    We issue the enable() call back for all CPU hwcaps capabilities
    available on the system, on all the CPUs. So far we have ignored
    the argument passed to the call back, which had a prototype to
    accept a "void *" for use with on_each_cpu() and later with
    stop_machine(). However, with commit 0a0d111d40fd1
    ("arm64: cpufeature: Pass capability structure to ->enable callback"),
    there are some users of the argument who wants the matching capability
    struct pointer where there are multiple matching criteria for a single
    capability. Clean up the declaration of the call back to make it clear.
    
     1) Renamed to cpu_enable(), to imply taking necessary actions on the
        called CPU for the entry.
     2) Pass const pointer to the capability, to allow the call back to
        check the entry. (e.,g to check if any action is needed on the CPU)
     3) We don't care about the result of the call back, turning this to
        a void.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Dave Martin <dave.martin@arm.com>
    [suzuki: convert more users, rename call back and drop results]
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 2b478565d774..ba964da31a25 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -38,6 +38,7 @@
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
+#include <asm/cpufeature.h>
 #include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
@@ -407,10 +408,9 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 }
 
-int cpu_enable_cache_maint_trap(void *__unused)
+void cpu_enable_cache_maint_trap(const struct arm64_cpu_capabilities *__unused)
 {
 	config_sctlr_el1(SCTLR_EL1_UCI, 0);
-	return 0;
 }
 
 #define __user_cache_maint(insn, address, res)			\

commit 4e829b6735475313016787ec3d256e102167b94d
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 15:18:13 2018 +0000

    arm64: Use arm64_force_sig_info instead of force_sig_info
    
    Using arm64_force_sig_info means that printing messages about unhandled
    signals is dealt with for us, so use that in preference to force_sig_info
    and remove any homebrew printing code.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index b139fe2d2126..2b478565d774 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -633,11 +633,6 @@ asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 {
 	siginfo_t info;
 	void __user *pc = (void __user *)instruction_pointer(regs);
-	console_verbose();
-
-	pr_crit("Bad EL0 synchronous exception detected on CPU%d, code 0x%08x -- %s\n",
-		smp_processor_id(), esr, esr_get_class_string(esr));
-	__show_regs(regs);
 
 	info.si_signo = SIGILL;
 	info.si_errno = 0;
@@ -645,9 +640,9 @@ asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 	info.si_addr  = pc;
 
 	current->thread.fault_address = 0;
-	current->thread.fault_code = 0;
+	current->thread.fault_code = esr;
 
-	force_sig_info(info.si_signo, &info, current);
+	arm64_force_sig_info(&info, "Bad EL0 synchronous exception", current);
 }
 
 #ifdef CONFIG_VMAP_STACK

commit a26731d9d1e3fa93db0b5781d7e8dd9dbff1313e
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 15:08:51 2018 +0000

    arm64: Move show_unhandled_signals_ratelimited into traps.c
    
    show_unhandled_signals_ratelimited is only called in traps.c, so move it
    out of its macro in the dreaded system_misc.h and into a static function
    in traps.c
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 835411cab38c..b139fe2d2126 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -223,6 +223,13 @@ void die(const char *str, struct pt_regs *regs, int err)
 		do_exit(SIGSEGV);
 }
 
+static bool show_unhandled_signals_ratelimited(void)
+{
+	static DEFINE_RATELIMIT_STATE(rs, DEFAULT_RATELIMIT_INTERVAL,
+				      DEFAULT_RATELIMIT_BURST);
+	return show_unhandled_signals && __ratelimit(&rs);
+}
+
 void arm64_force_sig_info(struct siginfo *info, const char *str,
 			  struct task_struct *tsk)
 {

commit 15b67321e7e9671881c7174a651a1c7d74c59f72
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 15:25:19 2018 +0000

    arm64: signal: Don't print anything directly in force_signal_inject
    
    arm64_notify_die deals with printing out information regarding unhandled
    signals, so there's no need to roll our own code here.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 00516f3956e4..835411cab38c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -363,13 +363,6 @@ void force_signal_inject(int signal, int code, unsigned long address)
 		signal = SIGKILL;
 	}
 
-	if (unhandled_signal(current, signal) &&
-	    show_unhandled_signals_ratelimited()) {
-		pr_info("%s[%d]: %s: pc=%08llx\n",
-			current->comm, task_pid_nr(current), desc, regs->pc);
-		dump_instr(KERN_INFO, regs);
-	}
-
 	info.si_signo = signal;
 	info.si_errno = 0;
 	info.si_code  = code;

commit a1ece8216c41c9dbb4040f7b8b3fbcd17662c665
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 13:46:05 2018 +0000

    arm64: Introduce arm64_force_sig_info and hook up in arm64_notify_die
    
    In preparation for consolidating our handling of printing unhandled
    signals, introduce a wrapper around force_sig_info which can act as
    the canonical place for dealing with show_unhandled_signals.
    
    Initially, we just hook this up to arm64_notify_die.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3f52c07b4bf4..00516f3956e4 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -223,13 +223,39 @@ void die(const char *str, struct pt_regs *regs, int err)
 		do_exit(SIGSEGV);
 }
 
+void arm64_force_sig_info(struct siginfo *info, const char *str,
+			  struct task_struct *tsk)
+{
+	unsigned int esr = tsk->thread.fault_code;
+	struct pt_regs *regs = task_pt_regs(tsk);
+
+	if (!unhandled_signal(tsk, info->si_signo))
+		goto send_sig;
+
+	if (!show_unhandled_signals_ratelimited())
+		goto send_sig;
+
+	pr_info("%s[%d]: unhandled exception: ", tsk->comm, task_pid_nr(tsk));
+	if (esr)
+		pr_cont("%s, ESR 0x%08x, ", esr_get_class_string(esr), esr);
+
+	pr_cont("%s", str);
+	print_vma_addr(KERN_CONT " in ", regs->pc);
+	pr_cont("\n");
+	__show_regs(regs);
+
+send_sig:
+	force_sig_info(info->si_signo, info, tsk);
+}
+
 void arm64_notify_die(const char *str, struct pt_regs *regs,
 		      struct siginfo *info, int err)
 {
 	if (user_mode(regs)) {
+		WARN_ON(regs != current_pt_regs());
 		current->thread.fault_address = 0;
 		current->thread.fault_code = err;
-		force_sig_info(info->si_signo, info, current);
+		arm64_force_sig_info(info, str, current);
 	} else {
 		die(str, regs, err);
 	}

commit a7e6f1ca90354a31946873d102cfa999ddf6ecb4
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 18:08:40 2018 +0000

    arm64: signal: Force SIGKILL for unknown signals in force_signal_inject
    
    For signals other than SIGKILL or those with siginfo_layout(signal, code)
    == SIL_FAULT then force_signal_inject does not initialise the siginfo_t
    properly. Since the signal number is determined solely by the caller,
    simply WARN on unknown signals and force to SIGKILL.
    
    Reported-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c478d8e27649..3f52c07b4bf4 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -331,6 +331,12 @@ void force_signal_inject(int signal, int code, unsigned long address)
 		break;
 	}
 
+	/* Force signals we don't understand to SIGKILL */
+	if (WARN_ON(signal != SIGKILL ||
+		    siginfo_layout(signal, code) != SIL_FAULT)) {
+		signal = SIGKILL;
+	}
+
 	if (unhandled_signal(current, signal) &&
 	    show_unhandled_signals_ratelimited()) {
 		pr_info("%s[%d]: %s: pc=%08llx\n",

commit 2c9120f3a86a809518ece1787d76ae07dd01e01b
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 14:16:29 2018 +0000

    arm64: signal: Make force_signal_inject more robust
    
    force_signal_inject is a little flakey:
    
      * It only knows about SIGILL and SIGSEGV, so can potentially deliver
        other signals based on a partially initialised siginfo_t
    
      * It sets si_addr to point at the PC for SIGSEGV
    
      * It always operates on current, so doesn't need the regs argument
    
    This patch fixes these issues by always assigning the si_addr field to
    the address parameter of the function and updates the callers (including
    those that indirectly call via arm64_notify_segfault) accordingly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index eb2d15147e8d..c478d8e27649 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -311,12 +311,13 @@ static int call_undef_hook(struct pt_regs *regs)
 	return fn ? fn(regs, instr) : 1;
 }
 
-void force_signal_inject(int signal, int code, struct pt_regs *regs,
-			 unsigned long address)
+void force_signal_inject(int signal, int code, unsigned long address)
 {
 	siginfo_t info;
-	void __user *pc = (void __user *)instruction_pointer(regs);
 	const char *desc;
+	struct pt_regs *regs = current_pt_regs();
+
+	clear_siginfo(&info);
 
 	switch (signal) {
 	case SIGILL:
@@ -332,15 +333,15 @@ void force_signal_inject(int signal, int code, struct pt_regs *regs,
 
 	if (unhandled_signal(current, signal) &&
 	    show_unhandled_signals_ratelimited()) {
-		pr_info("%s[%d]: %s: pc=%p\n",
-			current->comm, task_pid_nr(current), desc, pc);
+		pr_info("%s[%d]: %s: pc=%08llx\n",
+			current->comm, task_pid_nr(current), desc, regs->pc);
 		dump_instr(KERN_INFO, regs);
 	}
 
 	info.si_signo = signal;
 	info.si_errno = 0;
 	info.si_code  = code;
-	info.si_addr  = pc;
+	info.si_addr  = (void __user *)address;
 
 	arm64_notify_die(desc, regs, &info, 0);
 }
@@ -348,7 +349,7 @@ void force_signal_inject(int signal, int code, struct pt_regs *regs,
 /*
  * Set up process info to signal segmentation fault - called on access error.
  */
-void arm64_notify_segfault(struct pt_regs *regs, unsigned long addr)
+void arm64_notify_segfault(unsigned long addr)
 {
 	int code;
 
@@ -359,7 +360,7 @@ void arm64_notify_segfault(struct pt_regs *regs, unsigned long addr)
 		code = SEGV_ACCERR;
 	up_read(&current->mm->mmap_sem);
 
-	force_signal_inject(SIGSEGV, code, regs, addr);
+	force_signal_inject(SIGSEGV, code, addr);
 }
 
 asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
@@ -371,7 +372,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	if (call_undef_hook(regs) == 0)
 		return;
 
-	force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
+	force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 }
 
 int cpu_enable_cache_maint_trap(void *__unused)
@@ -426,12 +427,12 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 		__user_cache_maint("ic ivau", address, ret);
 		break;
 	default:
-		force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
+		force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 		return;
 	}
 
 	if (ret)
-		arm64_notify_segfault(regs, address);
+		arm64_notify_segfault(address);
 	else
 		arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }

commit 1962682d2b2fbe6cfa995a85c53c069fadda473e
Author: Michael Weiser <michael.weiser@gmx.de>
Date:   Thu Feb 1 23:13:36 2018 +0100

    arm64: Remove unimplemented syscall log message
    
    Stop printing a (ratelimited) kernel message for each instance of an
    unimplemented syscall being called. Userland making an unimplemented
    syscall is not necessarily misbehaviour and to be expected with a
    current userland running on an older kernel. Also, the current message
    looks scary to users but does not actually indicate a real problem nor
    help them narrow down the cause. Just rely on sys_ni_syscall() to return
    -ENOSYS.
    
    Cc: <stable@vger.kernel.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Michael Weiser <michael.weiser@gmx.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c8639f95e59a..eb2d15147e8d 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -526,14 +526,6 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
 	}
 #endif
 
-	if (show_unhandled_signals_ratelimited()) {
-		pr_info("%s[%d]: syscall %d\n", current->comm,
-			task_pid_nr(current), regs->syscallno);
-		dump_instr("", regs);
-		if (user_mode(regs))
-			__show_regs(regs);
-	}
-
 	return sys_ni_syscall();
 }
 

commit 5ee39a71fd89ab7240c5339d04161c44a8e03269
Author: Michael Weiser <michael.weiser@gmx.de>
Date:   Thu Feb 1 23:13:38 2018 +0100

    arm64: Disable unhandled signal log messages by default
    
    aarch64 unhandled signal kernel messages are very verbose, suggesting
    them to be more of a debugging aid:
    
    sigsegv[33]: unhandled level 2 translation fault (11) at 0x00000000, esr
    0x92000046, in sigsegv[400000+71000]
    CPU: 1 PID: 33 Comm: sigsegv Tainted: G        W        4.15.0-rc3+ #3
    Hardware name: linux,dummy-virt (DT)
    pstate: 60000000 (nZCv daif -PAN -UAO)
    pc : 0x4003f4
    lr : 0x4006bc
    sp : 0000fffffe94a060
    x29: 0000fffffe94a070 x28: 0000000000000000
    x27: 0000000000000000 x26: 0000000000000000
    x25: 0000000000000000 x24: 00000000004001b0
    x23: 0000000000486ac8 x22: 00000000004001c8
    x21: 0000000000000000 x20: 0000000000400be8
    x19: 0000000000400b30 x18: 0000000000484728
    x17: 000000000865ffc8 x16: 000000000000270f
    x15: 00000000000000b0 x14: 0000000000000002
    x13: 0000000000000001 x12: 0000000000000000
    x11: 0000000000000000 x10: 0008000020008008
    x9 : 000000000000000f x8 : ffffffffffffffff
    x7 : 0004000000000000 x6 : ffffffffffffffff
    x5 : 0000000000000000 x4 : 0000000000000000
    x3 : 00000000004003e4 x2 : 0000fffffe94a1e8
    x1 : 000000000000000a x0 : 0000000000000000
    
    Disable them by default, so they can be enabled using
    /proc/sys/debug/exception-trace.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Michael Weiser <michael.weiser@gmx.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index bbb0fde2780e..c8639f95e59a 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -57,7 +57,7 @@ static const char *handler[]= {
 	"Error"
 };
 
-int show_unhandled_signals = 1;
+int show_unhandled_signals = 0;
 
 static void dump_backtrace_entry(unsigned long where)
 {

commit 6bf0dcfd713563bd2e13ceb53217305c28a8aa5f
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 15 19:38:57 2018 +0000

    arm64: kernel: Survive corrected RAS errors notified by SError
    
    Prior to v8.2, SError is an uncontainable fatal exception. The v8.2 RAS
    extensions use SError to notify software about RAS errors, these can be
    contained by the Error Syncronization Barrier.
    
    An ACPI system with firmware-first may use SError as its 'SEI'
    notification. Future patches may add code to 'claim' this SError as a
    notification.
    
    Other systems can distinguish these RAS errors from the SError ESR and
    use the AET bits and additional data from RAS-Error registers to handle
    the error. Future patches may add this kernel-first handling.
    
    Without support for either of these we will panic(), even if we received
    a corrected error. Add code to decode the severity of RAS errors. We can
    safely ignore contained errors where the CPU can continue to make
    progress. For all other errors we continue to panic().
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3d3588fcd1c7..bbb0fde2780e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -662,17 +662,58 @@ asmlinkage void handle_bad_stack(struct pt_regs *regs)
 }
 #endif
 
-asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)
+void __noreturn arm64_serror_panic(struct pt_regs *regs, u32 esr)
 {
-	nmi_enter();
-
 	console_verbose();
 
 	pr_crit("SError Interrupt on CPU%d, code 0x%08x -- %s\n",
 		smp_processor_id(), esr, esr_get_class_string(esr));
-	__show_regs(regs);
+	if (regs)
+		__show_regs(regs);
+
+	nmi_panic(regs, "Asynchronous SError Interrupt");
+
+	cpu_park_loop();
+	unreachable();
+}
+
+bool arm64_is_fatal_ras_serror(struct pt_regs *regs, unsigned int esr)
+{
+	u32 aet = arm64_ras_serror_get_severity(esr);
+
+	switch (aet) {
+	case ESR_ELx_AET_CE:	/* corrected error */
+	case ESR_ELx_AET_UEO:	/* restartable, not yet consumed */
+		/*
+		 * The CPU can make progress. We may take UEO again as
+		 * a more severe error.
+		 */
+		return false;
+
+	case ESR_ELx_AET_UEU:	/* Uncorrected Unrecoverable */
+	case ESR_ELx_AET_UER:	/* Uncorrected Recoverable */
+		/*
+		 * The CPU can't make progress. The exception may have
+		 * been imprecise.
+		 */
+		return true;
+
+	case ESR_ELx_AET_UC:	/* Uncontainable or Uncategorized error */
+	default:
+		/* Error has been silently propagated */
+		arm64_serror_panic(regs, esr);
+	}
+}
+
+asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)
+{
+	nmi_enter();
+
+	/* non-RAS errors are not containable */
+	if (!arm64_is_ras_serror(esr) || arm64_is_fatal_ras_serror(regs, esr))
+		arm64_serror_panic(regs, esr);
 
-	panic("Asynchronous SError Interrupt");
+	nmi_exit();
 }
 
 void __pte_error(const char *file, int line, unsigned long val)

commit c9b012e5f4a1d01dfa8abc6318211a67ba7d5db2
Merge: b293fca43be5 6cfa7cc46b1a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 10:56:56 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "The big highlight is support for the Scalable Vector Extension (SVE)
      which required extensive ABI work to ensure we don't break existing
      applications by blowing away their signal stack with the rather large
      new vector context (<= 2 kbit per vector register). There's further
      work to be done optimising things like exception return, but the ABI
      is solid now.
    
      Much of the line count comes from some new PMU drivers we have, but
      they're pretty self-contained and I suspect we'll have more of them in
      future.
    
      Plenty of acronym soup here:
    
       - initial support for the Scalable Vector Extension (SVE)
    
       - improved handling for SError interrupts (required to handle RAS
         events)
    
       - enable GCC support for 128-bit integer types
    
       - remove kernel text addresses from backtraces and register dumps
    
       - use of WFE to implement long delay()s
    
       - ACPI IORT updates from Lorenzo Pieralisi
    
       - perf PMU driver for the Statistical Profiling Extension (SPE)
    
       - perf PMU driver for Hisilicon's system PMUs
    
       - misc cleanups and non-critical fixes"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (97 commits)
      arm64: Make ARMV8_DEPRECATED depend on SYSCTL
      arm64: Implement __lshrti3 library function
      arm64: support __int128 on gcc 5+
      arm64/sve: Add documentation
      arm64/sve: Detect SVE and activate runtime support
      arm64/sve: KVM: Hide SVE from CPU features exposed to guests
      arm64/sve: KVM: Treat guest SVE use as undefined instruction execution
      arm64/sve: KVM: Prevent guests from using SVE
      arm64/sve: Add sysctl to set the default vector length for new processes
      arm64/sve: Add prctl controls for userspace vector length management
      arm64/sve: ptrace and ELF coredump support
      arm64/sve: Preserve SVE registers around EFI runtime service calls
      arm64/sve: Preserve SVE registers around kernel-mode NEON use
      arm64/sve: Probe SVE capabilities and usable vector lengths
      arm64: cpufeature: Move sys_caps_initialised declarations
      arm64/sve: Backend logic for setting the vector length
      arm64/sve: Signal handling support
      arm64/sve: Support vector length resetting for new processes
      arm64/sve: Core task context handling
      arm64/sve: Low-level CPU setup
      ...

commit bc0ee476036478a85beeed51f0d94c8729fd0544
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:05 2017 +0000

    arm64/sve: Core task context handling
    
    This patch adds the core support for switching and managing the SVE
    architectural state of user tasks.
    
    Calls to the existing FPSIMD low-level save/restore functions are
    factored out as new functions task_fpsimd_{save,load}(), since SVE
    now dynamically may or may not need to be handled at these points
    depending on the kernel configuration, hardware features discovered
    at boot, and the runtime state of the task.  To make these
    decisions as fast as possible, const cpucaps are used where
    feasible, via the system_supports_sve() helper.
    
    The SVE registers are only tracked for threads that have explicitly
    used SVE, indicated by the new thread flag TIF_SVE.  Otherwise, the
    FPSIMD view of the architectural state is stored in
    thread.fpsimd_state as usual.
    
    When in use, the SVE registers are not stored directly in
    thread_struct due to their potentially large and variable size.
    Because the task_struct slab allocator must be configured very
    early during kernel boot, it is also tricky to configure it
    correctly to match the maximum vector length provided by the
    hardware, since this depends on examining secondary CPUs as well as
    the primary.  Instead, a pointer sve_state in thread_struct points
    to a dynamically allocated buffer containing the SVE register data,
    and code is added to allocate and free this buffer at appropriate
    times.
    
    TIF_SVE is set when taking an SVE access trap from userspace, if
    suitable hardware support has been detected.  This enables SVE for
    the thread: a subsequent return to userspace will disable the trap
    accordingly.  If such a trap is taken without sufficient system-
    wide hardware support, SIGILL is sent to the thread instead as if
    an undefined instruction had been executed: this may happen if
    userspace tries to use SVE in a system where not all CPUs support
    it for example.
    
    The kernel will clear TIF_SVE and disable SVE for the thread
    whenever an explicit syscall is made by userspace.  For backwards
    compatibility reasons and conformance with the spirit of the base
    AArch64 procedure call standard, the subset of the SVE register
    state that aliases the FPSIMD registers is still preserved across a
    syscall even if this happens.  The remainder of the SVE register
    state logically becomes zero at syscall entry, though the actual
    zeroing work is currently deferred until the thread next tries to
    use SVE, causing another trap to the kernel.  This implementation
    is suboptimal: in the future, the fastpath case may be optimised
    to zero the registers in-place and leave SVE enabled for the task,
    where beneficial.
    
    TIF_SVE is also cleared in the following slowpath cases, which are
    taken as reasonable hints that the task may no longer use SVE:
     * exec
     * fork and clone
    
    Code is added to sync data between thread.fpsimd_state and
    thread.sve_state whenever enabling/disabling SVE, in a manner
    consistent with the SVE architectural programmer's model.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    [will: added #include to fix allnoconfig build]
    [will: use enable_daif in do_sve_acc]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3408b33f6e7c..03f8f8ef542d 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -311,8 +311,8 @@ static int call_undef_hook(struct pt_regs *regs)
 	return fn ? fn(regs, instr) : 1;
 }
 
-static void force_signal_inject(int signal, int code, struct pt_regs *regs,
-				unsigned long address)
+void force_signal_inject(int signal, int code, struct pt_regs *regs,
+			 unsigned long address)
 {
 	siginfo_t info;
 	void __user *pc = (void __user *)instruction_pointer(regs);
@@ -326,7 +326,7 @@ static void force_signal_inject(int signal, int code, struct pt_regs *regs,
 		desc = "illegal memory access";
 		break;
 	default:
-		desc = "bad mode";
+		desc = "unknown or unrecoverable error";
 		break;
 	}
 

commit 672365649ccac68cf6fafecad1a7913951e7493b
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:00 2017 +0000

    arm64/sve: System register and exception syndrome definitions
    
    The SVE architecture adds some system registers, ID register fields
    and a dedicated ESR exception class.
    
    This patch adds the appropriate definitions that will be needed by
    the kernel.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c3a8d9876704..3408b33f6e7c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -556,6 +556,7 @@ static const char *esr_class_str[] = {
 	[ESR_ELx_EC_HVC64]		= "HVC (AArch64)",
 	[ESR_ELx_EC_SMC64]		= "SMC (AArch64)",
 	[ESR_ELx_EC_SYS64]		= "MSR/MRS (AArch64)",
+	[ESR_ELx_EC_SVE]		= "SVE",
 	[ESR_ELx_EC_IMP_DEF]		= "EL3 IMP DEF",
 	[ESR_ELx_EC_IABT_LOW]		= "IABT (lower EL)",
 	[ESR_ELx_EC_IABT_CUR]		= "IABT (current EL)",

commit 7a7003b1da010d2b0d1dc8bf21c10f5c73b389f1
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 2 16:12:03 2017 +0000

    arm64: ensure __dump_instr() checks addr_limit
    
    It's possible for a user to deliberately trigger __dump_instr with a
    chosen kernel address.
    
    Let's avoid problems resulting from this by using get_user() rather than
    __get_user(), ensuring that we don't erroneously access kernel memory.
    
    Where we use __dump_instr() on kernel text, we already switch to
    KERNEL_DS, so this shouldn't adversely affect those cases.
    
    Fixes: 60ffc30d5652810d ("arm64: Exception handling")
    Cc: stable@vger.kernel.org
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5ea4b85aee0e..8383af15a759 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -118,7 +118,7 @@ static void __dump_instr(const char *lvl, struct pt_regs *regs)
 	for (i = -4; i < 1; i++) {
 		unsigned int val, bad;
 
-		bad = __get_user(val, &((u32 *)addr)[i]);
+		bad = get_user(val, &((u32 *)addr)[i]);
 
 		if (!bad)
 			p += sprintf(p, i == 0 ? "(%08x) " : "%08x ", val);

commit a92d4d1454ab8b43b80b89fa31fcedb8821f8164
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Thu Nov 2 12:12:42 2017 +0000

    arm64: entry.S: move SError handling into a C function for future expansion
    
    Today SError is taken using the inv_entry macro that ends up in
    bad_mode.
    
    SError can be used by the RAS Extensions to notify either the OS or
    firmware of CPU problems, some of which may have been corrected.
    
    To allow this handling to be added, add a do_serror() C function
    that just panic()s. Add the entry.S boiler plate to save/restore the
    CPU registers and unmask debug exceptions. Future patches may change
    do_serror() to return if the SError Interrupt was notification of a
    corrected error.
    
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Wang Xiongfeng <wangxiongfengi2@huawei.com>
    [Split out of a bigger patch, added compat path, renamed, enabled debug
     exceptions]
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index fa52aefaf7f2..c3a8d9876704 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -661,6 +661,19 @@ asmlinkage void handle_bad_stack(struct pt_regs *regs)
 }
 #endif
 
+asmlinkage void do_serror(struct pt_regs *regs, unsigned int esr)
+{
+	nmi_enter();
+
+	console_verbose();
+
+	pr_crit("SError Interrupt on CPU%d, code 0x%08x -- %s\n",
+		smp_processor_id(), esr, esr_get_class_string(esr));
+	__show_regs(regs);
+
+	panic("Asynchronous SError Interrupt");
+}
+
 void __pte_error(const char *file, int line, unsigned long val)
 {
 	pr_err("%s:%d: bad pte %016lx.\n", file, line, val);

commit 0fbeb318754860b37150fd42c2058d636a431426
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:34 2017 +0000

    arm64: explicitly mask all exceptions
    
    There are a few places where we want to mask all exceptions. Today we
    do this in a piecemeal fashion, typically we expect the caller to
    have masked irqs and the arch code masks debug exceptions, ignoring
    serror which is probably masked.
    
    Make it clear that 'mask all exceptions' is the intention by adding
    helpers to do exactly that.
    
    This will let us unmask SError without having to add 'oh and SError'
    to these paths.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index afb6b19677dc..fa52aefaf7f2 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -38,6 +38,7 @@
 
 #include <asm/atomic.h>
 #include <asm/bug.h>
+#include <asm/daifflags.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
 #include <asm/insn.h>
@@ -594,7 +595,7 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 		esr_get_class_string(esr));
 
 	die("Oops - bad mode", regs, 0);
-	local_irq_disable();
+	local_daif_mask();
 	panic("bad mode");
 }
 

commit a25ffd3a6302a67814280274d8f1aa4ae2ea4b59
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Oct 19 13:19:20 2017 +0100

    arm64: traps: Don't print stack or raw PC/LR values in backtraces
    
    Printing raw pointer values in backtraces has potential security
    implications and are of questionable value anyway.
    
    This patch follows x86's lead and removes the "Exception stack:" dump
    from kernel backtraces, as well as converting PC/LR values to symbols
    such as "sysrq_handle_crash+0x20/0x30".
    
    Tested-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index a1b7d6420d09..afb6b19677dc 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -58,55 +58,9 @@ static const char *handler[]= {
 
 int show_unhandled_signals = 1;
 
-/*
- * Dump out the contents of some kernel memory nicely...
- */
-static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
-		     unsigned long top)
-{
-	unsigned long first;
-	mm_segment_t fs;
-	int i;
-
-	/*
-	 * We need to switch to kernel mode so that we can use __get_user
-	 * to safely read from kernel space.
-	 */
-	fs = get_fs();
-	set_fs(KERNEL_DS);
-
-	printk("%s%s(0x%016lx to 0x%016lx)\n", lvl, str, bottom, top);
-
-	for (first = bottom & ~31; first < top; first += 32) {
-		unsigned long p;
-		char str[sizeof(" 12345678") * 8 + 1];
-
-		memset(str, ' ', sizeof(str));
-		str[sizeof(str) - 1] = '\0';
-
-		for (p = first, i = 0; i < (32 / 8)
-					&& p < top; i++, p += 8) {
-			if (p >= bottom && p < top) {
-				unsigned long val;
-
-				if (__get_user(val, (unsigned long *)p) == 0)
-					sprintf(str + i * 17, " %016lx", val);
-				else
-					sprintf(str + i * 17, " ????????????????");
-			}
-		}
-		printk("%s%04lx:%s\n", lvl, first & 0xffff, str);
-	}
-
-	set_fs(fs);
-}
-
 static void dump_backtrace_entry(unsigned long where)
 {
-	/*
-	 * Note that 'where' can have a physical address, but it's not handled.
-	 */
-	print_ip_sym(where);
+	printk(" %pS\n", (void *)where);
 }
 
 static void __dump_instr(const char *lvl, struct pt_regs *regs)
@@ -171,10 +125,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 
 	skip = !!regs;
 	printk("Call trace:\n");
-	while (1) {
-		unsigned long stack;
-		int ret;
-
+	do {
 		/* skip until specified stack frame */
 		if (!skip) {
 			dump_backtrace_entry(frame.pc);
@@ -189,17 +140,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 			 */
 			dump_backtrace_entry(regs->pc);
 		}
-		ret = unwind_frame(tsk, &frame);
-		if (ret < 0)
-			break;
-		if (in_entry_text(frame.pc)) {
-			stack = frame.fp - offsetof(struct pt_regs, stackframe);
-
-			if (on_accessible_stack(tsk, stack))
-				dump_mem("", "Exception stack", stack,
-					 stack + sizeof(struct pt_regs));
-		}
-	}
+	} while (!unwind_frame(tsk, &frame));
 
 	put_task_stack(tsk);
 }

commit 6436beeee5721a8e906e9eabf866f12d04470437
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Wed Oct 25 10:04:33 2017 +0100

    arm64: Fix single stepping in kernel traps
    
    Software Step exception is missing after stepping a trapped instruction.
    
    Ensure SPSR.SS gets set to 0 after emulating/skipping a trapped instruction
    before doing ERET.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    [will: replaced AARCH32_INSN_SIZE with 4]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5ea4b85aee0e..a1b7d6420d09 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -293,6 +293,17 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
+void arm64_skip_faulting_instruction(struct pt_regs *regs, unsigned long size)
+{
+	regs->pc += size;
+
+	/*
+	 * If we were single stepping, we want to get the step exception after
+	 * we return from the trap.
+	 */
+	user_fastforward_single_step(current);
+}
+
 static LIST_HEAD(undef_hook);
 static DEFINE_RAW_SPINLOCK(undef_lock);
 
@@ -480,7 +491,7 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 	if (ret)
 		arm64_notify_segfault(regs, address);
 	else
-		regs->pc += 4;
+		arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }
 
 static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
@@ -490,7 +501,7 @@ static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 
 	pt_regs_write_reg(regs, rt, val);
 
-	regs->pc += 4;
+	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }
 
 static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
@@ -498,7 +509,7 @@ static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
 
 	pt_regs_write_reg(regs, rt, arch_counter_get_cntvct());
-	regs->pc += 4;
+	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }
 
 static void cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
@@ -506,7 +517,7 @@ static void cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
 
 	pt_regs_write_reg(regs, rt, arch_timer_get_rate());
-	regs->pc += 4;
+	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 }
 
 struct sys64_hook {
@@ -761,7 +772,7 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 	}
 
 	/* If thread survives, skip over the BUG instruction and continue: */
-	regs->pc += AARCH64_INSN_SIZE;	/* skip BRK and resume */
+	arm64_skip_faulting_instruction(regs, AARCH64_INSN_SIZE);
 	return DBG_HOOK_HANDLED;
 }
 

commit df5b95bee1ed7009a2090e9924e7a96e14850d56
Merge: 969ff73e72fe 872d8327ce89
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 15 18:40:58 2017 +0100

    Merge branch 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: add VMAP_STACK overflow detection
      arm64: add on_accessible_stack()
      arm64: add basic VMAP_STACK support
      arm64: use an irq stack pointer
      arm64: assembler: allow adr_this_cpu to use the stack pointer
      arm64: factor out entry stack manipulation
      efi/arm64: add EFI_KIMG_ALIGN
      arm64: move SEGMENT_ALIGN to <asm/memory.h>
      arm64: clean up irq stack definitions
      arm64: clean up THREAD_* definitions
      arm64: factor out PAGE_* and CONT_* definitions
      arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
      fork: allow arch-override of VMAP stack alignment
      arm64: remove __die()'s stack dump

commit 872d8327ce8982883b8237b2c320c8666f14e561
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 14 20:30:35 2017 +0100

    arm64: add VMAP_STACK overflow detection
    
    This patch adds stack overflow detection to arm64, usable when vmap'd stacks
    are in use.
    
    Overflow is detected in a small preamble executed for each exception entry,
    which checks whether there is enough space on the current stack for the general
    purpose registers to be saved. If there is not enough space, the overflow
    handler is invoked on a per-cpu overflow stack. This approach preserves the
    original exception information in ESR_EL1 (and where appropriate, FAR_EL1).
    
    Task and IRQ stacks are aligned to double their size, enabling overflow to be
    detected with a single bit test. For example, a 16K stack is aligned to 32K,
    ensuring that bit 14 of the SP must be zero. On an overflow (or underflow),
    this bit is flipped. Thus, overflow (of less than the size of the stack) can be
    detected by testing whether this bit is set.
    
    The overflow check is performed before any attempt is made to access the
    stack, avoiding recursive faults (and the loss of exception information
    these would entail). As logical operations cannot be performed on the SP
    directly, the SP is temporarily swapped with a general purpose register
    using arithmetic operations to enable the test to be performed.
    
    This gives us a useful error message on stack overflow, as can be trigger with
    the LKDTM overflow test:
    
    [  305.388749] lkdtm: Performing direct entry OVERFLOW
    [  305.395444] Insufficient stack space to handle exception!
    [  305.395482] ESR: 0x96000047 -- DABT (current EL)
    [  305.399890] FAR: 0xffff00000a5e7f30
    [  305.401315] Task stack:     [0xffff00000a5e8000..0xffff00000a5ec000]
    [  305.403815] IRQ stack:      [0xffff000008000000..0xffff000008004000]
    [  305.407035] Overflow stack: [0xffff80003efce4e0..0xffff80003efcf4e0]
    [  305.409622] CPU: 0 PID: 1219 Comm: sh Not tainted 4.13.0-rc3-00021-g9636aea #5
    [  305.412785] Hardware name: linux,dummy-virt (DT)
    [  305.415756] task: ffff80003d051c00 task.stack: ffff00000a5e8000
    [  305.419221] PC is at recursive_loop+0x10/0x48
    [  305.421637] LR is at recursive_loop+0x38/0x48
    [  305.423768] pc : [<ffff00000859f330>] lr : [<ffff00000859f358>] pstate: 40000145
    [  305.428020] sp : ffff00000a5e7f50
    [  305.430469] x29: ffff00000a5e8350 x28: ffff80003d051c00
    [  305.433191] x27: ffff000008981000 x26: ffff000008f80400
    [  305.439012] x25: ffff00000a5ebeb8 x24: ffff00000a5ebeb8
    [  305.440369] x23: ffff000008f80138 x22: 0000000000000009
    [  305.442241] x21: ffff80003ce65000 x20: ffff000008f80188
    [  305.444552] x19: 0000000000000013 x18: 0000000000000006
    [  305.446032] x17: 0000ffffa2601280 x16: ffff0000081fe0b8
    [  305.448252] x15: ffff000008ff546d x14: 000000000047a4c8
    [  305.450246] x13: ffff000008ff7872 x12: 0000000005f5e0ff
    [  305.452953] x11: ffff000008ed2548 x10: 000000000005ee8d
    [  305.454824] x9 : ffff000008545380 x8 : ffff00000a5e8770
    [  305.457105] x7 : 1313131313131313 x6 : 00000000000000e1
    [  305.459285] x5 : 0000000000000000 x4 : 0000000000000000
    [  305.461781] x3 : 0000000000000000 x2 : 0000000000000400
    [  305.465119] x1 : 0000000000000013 x0 : 0000000000000012
    [  305.467724] Kernel panic - not syncing: kernel stack overflow
    [  305.470561] CPU: 0 PID: 1219 Comm: sh Not tainted 4.13.0-rc3-00021-g9636aea #5
    [  305.473325] Hardware name: linux,dummy-virt (DT)
    [  305.475070] Call trace:
    [  305.476116] [<ffff000008088ad8>] dump_backtrace+0x0/0x378
    [  305.478991] [<ffff000008088e64>] show_stack+0x14/0x20
    [  305.481237] [<ffff00000895a178>] dump_stack+0x98/0xb8
    [  305.483294] [<ffff0000080c3288>] panic+0x118/0x280
    [  305.485673] [<ffff0000080c2e9c>] nmi_panic+0x6c/0x70
    [  305.486216] [<ffff000008089710>] handle_bad_stack+0x118/0x128
    [  305.486612] Exception stack(0xffff80003efcf3a0 to 0xffff80003efcf4e0)
    [  305.487334] f3a0: 0000000000000012 0000000000000013 0000000000000400 0000000000000000
    [  305.488025] f3c0: 0000000000000000 0000000000000000 00000000000000e1 1313131313131313
    [  305.488908] f3e0: ffff00000a5e8770 ffff000008545380 000000000005ee8d ffff000008ed2548
    [  305.489403] f400: 0000000005f5e0ff ffff000008ff7872 000000000047a4c8 ffff000008ff546d
    [  305.489759] f420: ffff0000081fe0b8 0000ffffa2601280 0000000000000006 0000000000000013
    [  305.490256] f440: ffff000008f80188 ffff80003ce65000 0000000000000009 ffff000008f80138
    [  305.490683] f460: ffff00000a5ebeb8 ffff00000a5ebeb8 ffff000008f80400 ffff000008981000
    [  305.491051] f480: ffff80003d051c00 ffff00000a5e8350 ffff00000859f358 ffff00000a5e7f50
    [  305.491444] f4a0: ffff00000859f330 0000000040000145 0000000000000000 0000000000000000
    [  305.492008] f4c0: 0001000000000000 0000000000000000 ffff00000a5e8350 ffff00000859f330
    [  305.493063] [<ffff00000808205c>] __bad_stack+0x88/0x8c
    [  305.493396] [<ffff00000859f330>] recursive_loop+0x10/0x48
    [  305.493731] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494088] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494425] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494649] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.494898] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495205] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495453] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.495708] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496000] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496302] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496644] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.496894] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.497138] [<ffff00000859f358>] recursive_loop+0x38/0x48
    [  305.497325] [<ffff00000859f3dc>] lkdtm_OVERFLOW+0x14/0x20
    [  305.497506] [<ffff00000859f314>] lkdtm_do_action+0x1c/0x28
    [  305.497786] [<ffff00000859f178>] direct_entry+0xe0/0x170
    [  305.498095] [<ffff000008345568>] full_proxy_write+0x60/0xa8
    [  305.498387] [<ffff0000081fb7f4>] __vfs_write+0x1c/0x128
    [  305.498679] [<ffff0000081fcc68>] vfs_write+0xa0/0x1b0
    [  305.498926] [<ffff0000081fe0fc>] SyS_write+0x44/0xa0
    [  305.499182] Exception stack(0xffff00000a5ebec0 to 0xffff00000a5ec000)
    [  305.499429] bec0: 0000000000000001 000000001c4cf5e0 0000000000000009 000000001c4cf5e0
    [  305.499674] bee0: 574f4c465245564f 0000000000000000 0000000000000000 8000000080808080
    [  305.499904] bf00: 0000000000000040 0000000000000038 fefefeff1b4bc2ff 7f7f7f7f7f7fff7f
    [  305.500189] bf20: 0101010101010101 0000000000000000 000000000047a4c8 0000000000000038
    [  305.500712] bf40: 0000000000000000 0000ffffa2601280 0000ffffc63f6068 00000000004b5000
    [  305.501241] bf60: 0000000000000001 000000001c4cf5e0 0000000000000009 000000001c4cf5e0
    [  305.501791] bf80: 0000000000000020 0000000000000000 00000000004b5000 000000001c4cc458
    [  305.502314] bfa0: 0000000000000000 0000ffffc63f7950 000000000040a3c4 0000ffffc63f70e0
    [  305.502762] bfc0: 0000ffffa2601268 0000000080000000 0000000000000001 0000000000000040
    [  305.503207] bfe0: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [  305.503680] [<ffff000008082fb0>] el0_svc_naked+0x24/0x28
    [  305.504720] Kernel Offset: disabled
    [  305.505189] CPU features: 0x002082
    [  305.505473] Memory Limit: none
    [  305.506181] ---[ end Kernel panic - not syncing: kernel stack overflow
    
    This patch was co-authored by Ard Biesheuvel and Mark Rutland.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d01c5988354b..2d591804e46f 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -32,6 +32,7 @@
 #include <linux/sched/signal.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
+#include <linux/sizes.h>
 #include <linux/syscalls.h>
 #include <linux/mm_types.h>
 
@@ -41,6 +42,7 @@
 #include <asm/esr.h>
 #include <asm/insn.h>
 #include <asm/traps.h>
+#include <asm/smp.h>
 #include <asm/stack_pointer.h>
 #include <asm/stacktrace.h>
 #include <asm/exception.h>
@@ -666,6 +668,43 @@ asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
 	force_sig_info(info.si_signo, &info, current);
 }
 
+#ifdef CONFIG_VMAP_STACK
+
+DEFINE_PER_CPU(unsigned long [OVERFLOW_STACK_SIZE/sizeof(long)], overflow_stack)
+	__aligned(16);
+
+asmlinkage void handle_bad_stack(struct pt_regs *regs)
+{
+	unsigned long tsk_stk = (unsigned long)current->stack;
+	unsigned long irq_stk = (unsigned long)this_cpu_read(irq_stack_ptr);
+	unsigned long ovf_stk = (unsigned long)this_cpu_ptr(overflow_stack);
+	unsigned int esr = read_sysreg(esr_el1);
+	unsigned long far = read_sysreg(far_el1);
+
+	console_verbose();
+	pr_emerg("Insufficient stack space to handle exception!");
+
+	pr_emerg("ESR: 0x%08x -- %s\n", esr, esr_get_class_string(esr));
+	pr_emerg("FAR: 0x%016lx\n", far);
+
+	pr_emerg("Task stack:     [0x%016lx..0x%016lx]\n",
+		 tsk_stk, tsk_stk + THREAD_SIZE);
+	pr_emerg("IRQ stack:      [0x%016lx..0x%016lx]\n",
+		 irq_stk, irq_stk + THREAD_SIZE);
+	pr_emerg("Overflow stack: [0x%016lx..0x%016lx]\n",
+		 ovf_stk, ovf_stk + OVERFLOW_STACK_SIZE);
+
+	__show_regs(regs);
+
+	/*
+	 * We use nmi_panic to limit the potential for recusive overflows, and
+	 * to get a better stack trace.
+	 */
+	nmi_panic(NULL, "kernel stack overflow");
+	cpu_park_loop();
+}
+#endif
+
 void __pte_error(const char *file, int line, unsigned long val)
 {
 	pr_err("%s:%d: bad pte %016lx.\n", file, line, val);

commit 12964443e8d1914010f9269f9f9abc4e122bc6ca
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Aug 1 18:51:15 2017 +0100

    arm64: add on_accessible_stack()
    
    Both unwind_frame() and dump_backtrace() try to check whether a stack
    address is sane to access, with very similar logic. Both will need
    updating in order to handle overflow stacks.
    
    Factor out this logic into a helper, so that we can avoid further
    duplication when we add overflow stacks.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 9633773ca42c..d01c5988354b 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -193,8 +193,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		if (in_entry_text(frame.pc)) {
 			stack = frame.fp - offsetof(struct pt_regs, stackframe);
 
-			if (on_task_stack(tsk, stack) ||
-			    (tsk == current && !preemptible() && on_irq_stack(stack)))
+			if (on_accessible_stack(tsk, stack))
 				dump_mem("", "Exception stack", stack,
 					 stack + sizeof(struct pt_regs));
 		}

commit c5bc503cbeee8586395aa541d2b53c69c3dd6930
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Aug 7 12:10:51 2017 +0100

    arm64: remove __die()'s stack dump
    
    Our __die() implementation tries to dump the stack memory, in addition
    to a backtrace, which is problematic.
    
    For contemporary 16K stacks, this can be a lot of data, which can take a
    long time to dump, and can push other useful context out of the kernel's
    printk ringbuffer (and/or a user's scrollback buffer on an attached
    console).
    
    Additionally, the code implicitly assumes that the SP is on the task's
    stack, and tries to dump everything between the SP and the highest task
    stack address. When the SP points at an IRQ stack (or is corrupted),
    this makes the kernel attempt to dump vast amounts of VA space. With
    vmap'd stacks, this may result in erroneous accesses to peripherals.
    
    This patch removes the memory dump, leaving us to rely on the backtrace,
    and other means of dumping stack memory such as kdump.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c2a81bf8827e..9633773ca42c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -237,8 +237,6 @@ static int __die(const char *str, int err, struct pt_regs *regs)
 		 end_of_stack(tsk));
 
 	if (!user_mode(regs)) {
-		dump_mem(KERN_EMERG, "Stack: ", regs->sp,
-			 THREAD_SIZE + (unsigned long)task_stack_page(tsk));
 		dump_backtrace(regs, tsk);
 		dump_instr(KERN_EMERG, regs);
 	}

commit 0553896787353e2526078064ff1cf21ff7bc34ce
Merge: 739586951b8a 31e43ad3b74a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Aug 9 15:37:49 2017 +0100

    Merge branch 'arm64/exception-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/exception-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: unwind: remove sp from struct stackframe
      arm64: unwind: reference pt_regs via embedded stack frame
      arm64: unwind: disregard frame.sp when validating frame pointer
      arm64: unwind: avoid percpu indirection for irq stack
      arm64: move non-entry code out of .entry.text
      arm64: consistently use bl for C exception entry
      arm64: Add ASM_BUG()

commit 31e43ad3b74a5d7b282023b72f25fc677c14c727
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sun Jul 23 09:05:38 2017 +0100

    arm64: unwind: remove sp from struct stackframe
    
    The unwind code sets the sp member of struct stackframe to
    'frame pointer + 0x10' unconditionally, without regard for whether
    doing so produces a legal value. So let's simply remove it now that
    we have stopped using it anyway.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 075c29a24345..c2a81bf8827e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -155,14 +155,12 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 
 	if (tsk == current) {
 		frame.fp = (unsigned long)__builtin_frame_address(0);
-		frame.sp = current_stack_pointer;
 		frame.pc = (unsigned long)dump_backtrace;
 	} else {
 		/*
 		 * task blocked in __switch_to
 		 */
 		frame.fp = thread_saved_fp(tsk);
-		frame.sp = thread_saved_sp(tsk);
 		frame.pc = thread_saved_pc(tsk);
 	}
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER

commit 7326749801396105aef0ed9229df746ac9e24300
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Sat Jul 22 18:45:33 2017 +0100

    arm64: unwind: reference pt_regs via embedded stack frame
    
    As it turns out, the unwind code is slightly broken, and probably has
    been for a while. The problem is in the dumping of the exception stack,
    which is intended to dump the contents of the pt_regs struct at each
    level in the call stack where an exception was taken and routed to a
    routine marked as __exception (which means its stack frame is right
    below the pt_regs struct on the stack).
    
    'Right below the pt_regs struct' is ill defined, though: the unwind
    code assigns 'frame pointer + 0x10' to the .sp member of the stackframe
    struct at each level, and dump_backtrace() happily dereferences that as
    the pt_regs pointer when encountering an __exception routine. However,
    the actual size of the stack frame created by this routine (which could
    be one of many __exception routines we have in the kernel) is not known,
    and so frame.sp is pretty useless to figure out where struct pt_regs
    really is.
    
    So it seems the only way to ensure that we can find our struct pt_regs
    when walking the stack frames is to put it at a known fixed offset of
    the stack frame pointer that is passed to such __exception routines.
    The simplest way to do that is to put it inside pt_regs itself, which is
    the main change implemented by this patch. As a bonus, doing this allows
    us to get rid of a fair amount of cruft related to walking from one stack
    to the other, which is especially nice since we intend to introduce yet
    another stack for overflow handling once we add support for vmapped
    stacks. It also fixes an inconsistency where we only add a stack frame
    pointing to ELR_EL1 if we are executing from the IRQ stack but not when
    we are executing from the task stack.
    
    To consistly identify exceptions regs even in the presence of exceptions
    taken from entry code, we must check whether the next frame was created
    by entry text, rather than whether the current frame was crated by
    exception text.
    
    To avoid backtracing using PCs that fall in the idmap, or are controlled
    by userspace, we must explcitly zero the FP and LR in startup paths, and
    must ensure that the frame embedded in pt_regs is zeroed upon entry from
    EL0. To avoid these NULL entries showin in the backtrace, unwind_frame()
    is updated to avoid them.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    [Mark: compare current frame against .entry.text, avoid bogus PCs]
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5797f5037ec9..075c29a24345 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -143,7 +143,6 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
-	unsigned long irq_stack_ptr;
 	int skip;
 
 	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
@@ -154,15 +153,6 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	if (!try_get_task_stack(tsk))
 		return;
 
-	/*
-	 * Switching between stacks is valid when tracing current and in
-	 * non-preemptible context.
-	 */
-	if (tsk == current && !preemptible())
-		irq_stack_ptr = IRQ_STACK_PTR();
-	else
-		irq_stack_ptr = 0;
-
 	if (tsk == current) {
 		frame.fp = (unsigned long)__builtin_frame_address(0);
 		frame.sp = current_stack_pointer;
@@ -182,13 +172,12 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	skip = !!regs;
 	printk("Call trace:\n");
 	while (1) {
-		unsigned long where = frame.pc;
 		unsigned long stack;
 		int ret;
 
 		/* skip until specified stack frame */
 		if (!skip) {
-			dump_backtrace_entry(where);
+			dump_backtrace_entry(frame.pc);
 		} else if (frame.fp == regs->regs[29]) {
 			skip = 0;
 			/*
@@ -203,20 +192,13 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		ret = unwind_frame(tsk, &frame);
 		if (ret < 0)
 			break;
-		stack = frame.sp;
-		if (in_exception_text(where)) {
-			/*
-			 * If we switched to the irq_stack before calling this
-			 * exception handler, then the pt_regs will be on the
-			 * task stack. The easiest way to tell is if the large
-			 * pt_regs would overlap with the end of the irq_stack.
-			 */
-			if (stack < irq_stack_ptr &&
-			    (stack + sizeof(struct pt_regs)) > irq_stack_ptr)
-				stack = IRQ_STACK_TO_TASK_STACK(irq_stack_ptr);
+		if (in_entry_text(frame.pc)) {
+			stack = frame.fp - offsetof(struct pt_regs, stackframe);
 
-			dump_mem("", "Exception stack", stack,
-				 stack + sizeof(struct pt_regs));
+			if (on_task_stack(tsk, stack) ||
+			    (tsk == current && !preemptible() && on_irq_stack(stack)))
+				dump_mem("", "Exception stack", stack,
+					 stack + sizeof(struct pt_regs));
 		}
 	}
 

commit e1bc5d1b8e0547c258e65dd97a03560f4d69e635
Author: Robin Murphy <robin.murphy@arm.com>
Date:   Tue Jul 25 11:55:41 2017 +0100

    arm64: Handle trapped DC CVAP
    
    Cache clean to PoP is subject to the same access controls as to PoC, so
    if we are trapping userspace cache maintenance with SCTLR_EL1.UCI, we
    need to be prepared to handle it. To avoid getting into complicated
    fights with binutils about ARMv8.2 options, we'll just cheat and use the
    raw SYS instruction rather than the 'proper' DC alias.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 0f047e916cee..ccb9727d67b2 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -484,6 +484,9 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 	case ESR_ELx_SYS64_ISS_CRM_DC_CVAC:	/* DC CVAC, gets promoted */
 		__user_cache_maint("dc civac", address, ret);
 		break;
+	case ESR_ELx_SYS64_ISS_CRM_DC_CVAP:	/* DC CVAP */
+		__user_cache_maint("sys 3, c7, c12, 1", address, ret);
+		break;
 	case ESR_ELx_SYS64_ISS_CRM_DC_CIVAC:	/* DC CIVAC */
 		__user_cache_maint("dc civac", address, ret);
 		break;

commit 096683724cb2eb95fea759a2580996df1039fdd0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 20 14:01:01 2017 +0100

    arm64: unwind: avoid percpu indirection for irq stack
    
    Our IRQ_STACK_PTR() and on_irq_stack() helpers both take a cpu argument,
    used to generate a percpu address. In all cases, they are passed
    {raw_,}smp_processor_id(), so this parameter is redundant.
    
    Since {raw_,}smp_processor_id() use a percpu variable internally, this
    approach means we generate a percpu offset to find the current cpu, then
    use this to index an array of percpu offsets, which we then use to find
    the current CPU's IRQ stack pointer. Thus, most of the work is
    redundant.
    
    Instead, we can consistently use raw_cpu_ptr() to generate the CPU's
    irq_stack pointer by simply adding the percpu offset to the irq_stack
    address, which is simpler in both respects.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d48f47080213..5797f5037ec9 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -159,7 +159,7 @@ void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	 * non-preemptible context.
 	 */
 	if (tsk == current && !preemptible())
-		irq_stack_ptr = IRQ_STACK_PTR(smp_processor_id());
+		irq_stack_ptr = IRQ_STACK_PTR();
 	else
 		irq_stack_ptr = 0;
 

commit 35d0e6fb4d219d64ab3b7cffef7a11a0662140f5
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Aug 1 15:35:53 2017 +0100

    arm64: syscallno is secretly an int, make it official
    
    The upper 32 bits of the syscallno field in thread_struct are
    handled inconsistently, being sometimes zero extended and sometimes
    sign-extended.  In fact, only the lower 32 bits seem to have any
    real significance for the behaviour of the code: it's been OK to
    handle the upper bits inconsistently because they don't matter.
    
    Currently, the only place I can find where those bits are
    significant is in calling trace_sys_enter(), which may be
    unintentional: for example, if a compat tracer attempts to cancel a
    syscall by passing -1 to (COMPAT_)PTRACE_SET_SYSCALL at the
    syscall-enter-stop, it will be traced as syscall 4294967295
    rather than -1 as might be expected (and as occurs for a native
    tracer doing the same thing).  Elsewhere, reads of syscallno cast
    it to an int or truncate it.
    
    There's also a conspicuous amount of code and casting to bodge
    around the fact that although semantically an int, syscallno is
    stored as a u64.
    
    Let's not pretend any more.
    
    In order to preserve the stp x instruction that stores the syscall
    number in entry.S, this patch special-cases the layout of struct
    pt_regs for big endian so that the newly 32-bit syscallno field
    maps onto the low bits of the stored value.  This is not beautiful,
    but benchmarking of the getpid syscall on Juno suggests indicates a
    minor slowdown if the stp is split into an stp x and stp w.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8a62648848e5..0f047e916cee 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -593,7 +593,7 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
 
 	if (show_unhandled_signals_ratelimited()) {
 		pr_info("%s[%d]: syscall %d\n", current->comm,
-			task_pid_nr(current), (int)regs->syscallno);
+			task_pid_nr(current), regs->syscallno);
 		dump_instr("", regs);
 		if (user_mode(regs))
 			__show_regs(regs);

commit c6f97add0f2ac83b98b06dbdda58fa47638ae7b0
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Fri Jul 21 18:15:27 2017 +0100

    arm64: Use arch_timer_get_rate when trapping CNTFRQ_EL0
    
    In an ideal world, CNTFRQ_EL0 always contains the timer frequency
    for the kernel to use. Sadly, we get quite a few broken systems
    where the firmware authors cannot be bothered to program that
    register on all CPUs, and rely on DT to provide that frequency.
    
    So when trapping CNTFRQ_EL0, make sure to return the actual rate
    (as known by the kernel), and not CNTFRQ_EL0.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d48f47080213..8a62648848e5 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -523,7 +523,7 @@ static void cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
 
-	pt_regs_write_reg(regs, rt, read_sysreg(cntfrq_el0));
+	pt_regs_write_reg(regs, rt, arch_timer_get_rate());
 	regs->pc += 4;
 }
 

commit 6f44a0bacb79a03972c83759711832b382b1b8ac
Author: Qiao Zhou <qiaozhou@asrmicro.com>
Date:   Fri Jul 7 17:29:34 2017 +0800

    arm64: traps: disable irq in die()
    
    In current die(), the irq is disabled for __die() handle, not
    including the possible panic() handling. Since the log in __die()
    can take several hundreds ms, new irq might come and interrupt
    current die().
    
    If the process calling die() holds some critical resource, and some
    other process scheduled later also needs it, then it would deadlock.
    The first panic will not be executed.
    
    So here disable irq for the whole flow of die().
    
    Signed-off-by: Qiao Zhou <qiaozhou@asrmicro.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c7c7088097be..d48f47080213 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -274,10 +274,12 @@ static DEFINE_RAW_SPINLOCK(die_lock);
 void die(const char *str, struct pt_regs *regs, int err)
 {
 	int ret;
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&die_lock, flags);
 
 	oops_enter();
 
-	raw_spin_lock_irq(&die_lock);
 	console_verbose();
 	bust_spinlocks(1);
 	ret = __die(str, err, regs);
@@ -287,13 +289,15 @@ void die(const char *str, struct pt_regs *regs, int err)
 
 	bust_spinlocks(0);
 	add_taint(TAINT_DIE, LOCKDEP_NOW_UNRELIABLE);
-	raw_spin_unlock_irq(&die_lock);
 	oops_exit();
 
 	if (in_interrupt())
 		panic("Fatal exception in interrupt");
 	if (panic_on_oops)
 		panic("Fatal exception");
+
+	raw_spin_unlock_irqrestore(&die_lock, flags);
+
 	if (ret != NOTIFY_STOP)
 		do_exit(SIGSEGV);
 }

commit 6cf5d4af83e04f4cfae91bfdefd9d4d6949c09b2
Author: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Date:   Wed Jun 28 16:55:55 2017 +0200

    arm64: fix endianness annotation in call_undef_hook()
    
    Here we're reading thumb or ARM instructions, which are always
    stored in memory in little-endian order. These values are thus
    correctly converted to native order but the intermediate value
    should be annotated as for little-endian values.
    
    Fix this by declaring the intermediate var as __le32 or __le16.
    
    Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3ebfb1d00b53..c7c7088097be 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -344,22 +344,24 @@ static int call_undef_hook(struct pt_regs *regs)
 
 	if (compat_thumb_mode(regs)) {
 		/* 16-bit Thumb instruction */
-		if (get_user(instr, (u16 __user *)pc))
+		__le16 instr_le;
+		if (get_user(instr_le, (__le16 __user *)pc))
 			goto exit;
-		instr = le16_to_cpu(instr);
+		instr = le16_to_cpu(instr_le);
 		if (aarch32_insn_is_wide(instr)) {
 			u32 instr2;
 
-			if (get_user(instr2, (u16 __user *)(pc + 2)))
+			if (get_user(instr_le, (__le16 __user *)(pc + 2)))
 				goto exit;
-			instr2 = le16_to_cpu(instr2);
+			instr2 = le16_to_cpu(instr_le);
 			instr = (instr << 16) | instr2;
 		}
 	} else {
 		/* 32-bit ARM instruction */
-		if (get_user(instr, (u32 __user *)pc))
+		__le32 instr_le;
+		if (get_user(instr_le, (__le32 __user *)pc))
 			goto exit;
-		instr = le32_to_cpu(instr);
+		instr = le32_to_cpu(instr_le);
 	}
 
 	raw_spin_lock_irqsave(&undef_lock, flags);

commit 1149aad10b1e2f2cf1ea023889ac8604ae869c5a
Author: Kefeng Wang <wangkefeng.wang@huawei.com>
Date:   Tue May 9 09:53:37 2017 +0800

    arm64: Add dump_backtrace() in show_regs
    
    Generic code expects show_regs() to dump the stack, but arm64's
    show_regs() does not. This makes it hard to debug softlockups and
    other issues that result in show_regs() being called.
    
    This patch updates arm64's show_regs() to dump the stack, as common
    code expects.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kefeng Wang <wangkefeng.wang@huawei.com>
    [will: folded in bug_handler fix from mrutland]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 0805b44f986a..3ebfb1d00b53 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -140,7 +140,7 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 	}
 }
 
-static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
+void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
 	unsigned long irq_stack_ptr;
@@ -728,8 +728,6 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 		break;
 
 	case BUG_TRAP_TYPE_WARN:
-		/* Ideally, report_bug() should backtrace for us... but no. */
-		dump_backtrace(regs, NULL);
 		break;
 
 	default:

commit 81cddd65b5c82758ea5571a25e31ff6f1f89ff02
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed May 3 16:37:45 2017 +0100

    arm64: traps: fix userspace cache maintenance emulation on a tagged pointer
    
    When we emulate userspace cache maintenance in the kernel, we can
    currently send the task a SIGSEGV even though the maintenance was done
    on a valid address. This happens if the address has a non-zero address
    tag, and happens to not be mapped in.
    
    When we get the address from a user register, we don't currently remove
    the address tag before performing cache maintenance on it. If the
    maintenance faults, we end up in either __do_page_fault, where find_vma
    can't find the VMA if the address has a tag, or in do_translation_fault,
    where the tagged address will appear to be above TASK_SIZE. In both
    cases, the address is not mapped in, and the task is sent a SIGSEGV.
    
    This patch removes the tag from the address before using it. With this
    patch, the fault is handled correctly, the address gets mapped in, and
    the cache maintenance succeeds.
    
    As a second bug, if cache maintenance (correctly) fails on an invalid
    tagged address, the address gets passed into arm64_notify_segfault,
    where find_vma fails to find the VMA due to the tag, and the wrong
    si_code may be sent as part of the siginfo_t of the segfault. With this
    patch, the correct si_code is sent.
    
    Fixes: 7dd01aef0557 ("arm64: trap userspace "dc cvau" cache operation on errata-affected core")
    Cc: <stable@vger.kernel.org> # 4.8.x-
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d4d6ae02cd55..0805b44f986a 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -443,7 +443,7 @@ int cpu_enable_cache_maint_trap(void *__unused)
 }
 
 #define __user_cache_maint(insn, address, res)			\
-	if (untagged_addr(address) >= user_addr_max()) {	\
+	if (address >= user_addr_max()) {			\
 		res = -EFAULT;					\
 	} else {						\
 		uaccess_ttbr0_enable();				\
@@ -469,7 +469,7 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 	int crm = (esr & ESR_ELx_SYS64_ISS_CRM_MASK) >> ESR_ELx_SYS64_ISS_CRM_SHIFT;
 	int ret = 0;
 
-	address = pt_regs_read_reg(regs, rt);
+	address = untagged_addr(pt_regs_read_reg(regs, rt));
 
 	switch (crm) {
 	case ESR_ELx_SYS64_ISS_CRM_DC_CVAU:	/* DC CVAU, gets promoted */

commit 9842119a238bfb92cbab63258dabb54f0e7b111b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 24 09:04:03 2017 +0100

    arm64: Add CNTFRQ_EL0 trap handler
    
    We now trap accesses to CNTVCT_EL0 when the counter is broken
    enough to require the kernel to mediate the access. But it
    turns out that some existing userspace (such as OpenMPI) do
    probe for the counter frequency, leading to an UNDEF exception
    as CNTVCT_EL0 and CNTFRQ_EL0 share the same control bit.
    
    The fix is to handle the exception the same way we do for CNTVCT_EL0.
    
    Fixes: a86bd139f2ae ("arm64: arch_timer: Enable CNTVCT_EL0 trap if workaround is enabled")
    Reported-by: Hanjun Guo <guohanjun@huawei.com>
    Tested-by: Hanjun Guo <guohanjun@huawei.com>
    Reviewed-by: Hanjun Guo <guohanjun@huawei.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 1de444e6c669..d4d6ae02cd55 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -513,6 +513,14 @@ static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
 	regs->pc += 4;
 }
 
+static void cntfrq_read_handler(unsigned int esr, struct pt_regs *regs)
+{
+	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+
+	pt_regs_write_reg(regs, rt, read_sysreg(cntfrq_el0));
+	regs->pc += 4;
+}
+
 struct sys64_hook {
 	unsigned int esr_mask;
 	unsigned int esr_val;
@@ -537,6 +545,12 @@ static struct sys64_hook sys64_hooks[] = {
 		.esr_val = ESR_ELx_SYS64_ISS_SYS_CNTVCT,
 		.handler = cntvct_read_handler,
 	},
+	{
+		/* Trap read access to CNTFRQ_EL0 */
+		.esr_mask = ESR_ELx_SYS64_ISS_SYS_OP_MASK,
+		.esr_val = ESR_ELx_SYS64_ISS_SYS_CNTFRQ,
+		.handler = cntfrq_read_handler,
+	},
 	{},
 };
 

commit 6126ce0588eb5a0752d5c8b5796a7fca324fd887
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Wed Feb 1 11:48:58 2017 +0000

    arm64: Add CNTVCT_EL0 trap handler
    
    Since people seem to make a point in breaking the userspace visible
    counter, we have no choice but to trap the access. Add the required
    handler.
    
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index e52be6aa44ee..1de444e6c669 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -505,6 +505,14 @@ static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 	regs->pc += 4;
 }
 
+static void cntvct_read_handler(unsigned int esr, struct pt_regs *regs)
+{
+	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+
+	pt_regs_write_reg(regs, rt, arch_counter_get_cntvct());
+	regs->pc += 4;
+}
+
 struct sys64_hook {
 	unsigned int esr_mask;
 	unsigned int esr_val;
@@ -523,6 +531,12 @@ static struct sys64_hook sys64_hooks[] = {
 		.esr_val = ESR_ELx_SYS64_ISS_SYS_CTR_READ,
 		.handler = ctr_read_handler,
 	},
+	{
+		/* Trap read access to CNTVCT_EL0 */
+		.esr_mask = ESR_ELx_SYS64_ISS_SYS_OP_MASK,
+		.esr_val = ESR_ELx_SYS64_ISS_SYS_CNTVCT,
+		.handler = cntvct_read_handler,
+	},
 	{},
 };
 

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index bdbd0c3febf4..e52be6aa44ee 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -33,6 +33,7 @@
 #include <linux/sched/debug.h>
 #include <linux/sched/task_stack.h>
 #include <linux/syscalls.h>
+#include <linux/mm_types.h>
 
 #include <asm/atomic.h>
 #include <asm/bug.h>

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5a5d83791837..bdbd0c3febf4 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -31,6 +31,7 @@
 #include <linux/init.h>
 #include <linux/sched/signal.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/task_stack.h>
 #include <linux/syscalls.h>
 
 #include <asm/atomic.h>

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5b8779a849a2..5a5d83791837 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -30,6 +30,7 @@
 #include <linux/delay.h>
 #include <linux/init.h>
 #include <linux/sched/signal.h>
+#include <linux/sched/debug.h>
 #include <linux/syscalls.h>
 
 #include <asm/atomic.h>

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 7d47c2cdfd93..5b8779a849a2 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -29,7 +29,7 @@
 #include <linux/kexec.h>
 #include <linux/delay.h>
 #include <linux/init.h>
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/syscalls.h>
 
 #include <asm/atomic.h>

commit ca78d3173cff3503bcd15723b049757f75762d15
Merge: a4ee7bacd6c0 ffe7afd17135
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:46:44 2017 -0800

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     - Errata workarounds for Qualcomm's Falkor CPU
     - Qualcomm L2 Cache PMU driver
     - Qualcomm SMCCC firmware quirk
     - Support for DEBUG_VIRTUAL
     - CPU feature detection for userspace via MRS emulation
     - Preliminary work for the Statistical Profiling Extension
     - Misc cleanups and non-critical fixes
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (74 commits)
      arm64/kprobes: consistently handle MRS/MSR with XZR
      arm64: cpufeature: correctly handle MRS to XZR
      arm64: traps: correctly handle MRS/MSR with XZR
      arm64: ptrace: add XZR-safe regs accessors
      arm64: include asm/assembler.h in entry-ftrace.S
      arm64: fix warning about swapper_pg_dir overflow
      arm64: Work around Falkor erratum 1003
      arm64: head.S: Enable EL1 (host) access to SPE when entered at EL2
      arm64: arch_timer: document Hisilicon erratum 161010101
      arm64: use is_vmalloc_addr
      arm64: use linux/sizes.h for constants
      arm64: uaccess: consistently check object sizes
      perf: add qcom l2 cache perf events driver
      arm64: remove wrong CONFIG_PROC_SYSCTL ifdef
      ARM: smccc: Update HVC comment to describe new quirk parameter
      arm64: do not trace atomic operations
      ACPI/IORT: Fix the error return code in iort_add_smmu_platform_device()
      ACPI/IORT: Fix iort_node_get_id() mapping entries indexing
      arm64: mm: enable CONFIG_HOLES_IN_ZONE for NUMA
      perf: xgene: Include module.h
      ...

commit 8b6e70fccff27121430114b4507f0adfb790752f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Feb 9 15:19:19 2017 +0000

    arm64: traps: correctly handle MRS/MSR with XZR
    
    Currently we hand-roll XZR-safe register handling in
    user_cache_maint_handler(), though we forget to do the same in
    ctr_read_handler(), and may erroneously write back to the user SP rather
    than XZR.
    
    Use the new helpers to handle these cases correctly and consistently.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 116c81f427ff6c53 ("arm64: Work around systems with mismatched cache line sizes")
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 7c3fc0634aa2..350179becdf7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -466,7 +466,7 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 	int crm = (esr & ESR_ELx_SYS64_ISS_CRM_MASK) >> ESR_ELx_SYS64_ISS_CRM_SHIFT;
 	int ret = 0;
 
-	address = (rt == 31) ? 0 : regs->regs[rt];
+	address = pt_regs_read_reg(regs, rt);
 
 	switch (crm) {
 	case ESR_ELx_SYS64_ISS_CRM_DC_CVAU:	/* DC CVAU, gets promoted */
@@ -495,8 +495,10 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+	unsigned long val = arm64_ftr_reg_user_value(&arm64_ftr_reg_ctrel0);
+
+	pt_regs_write_reg(regs, rt, val);
 
-	regs->regs[rt] = arm64_ftr_reg_user_value(&arm64_ftr_reg_ctrel0);
 	regs->pc += 4;
 }
 

commit 49f6cba617fef4bc097a291e0dfd028cc7073c52
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jan 27 16:15:38 2017 +0000

    arm64: handle sys and undef traps consistently
    
    If an EL0 instruction in the SYS class triggers an exception, do_sysintr
    looks for a sys64_hook matching the instruction, and if none is found,
    injects a SIGILL. This mirrors what we do for undefined instruction
    encodings in do_undefinstr, where we look for an undef_hook matching the
    instruction, and if none is found, inject a SIGILL.
    
    Over time, new SYS instruction encodings may be allocated. Prior to
    allocation, exceptions resulting from these would be handled by
    do_undefinstr, whereas after allocation these may be handled by
    do_sysintr.
    
    To ensure that we have consistent behaviour if and when this happens, it
    would be beneficial to have do_sysinstr fall back to do_undefinstr.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Suzuki Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8187229eb802..7c3fc0634aa2 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -531,7 +531,12 @@ asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
 			return;
 		}
 
-	force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
+	/*
+	 * New SYS instructions may previously have been undefined at EL0. Fall
+	 * back to our usual undefined instruction handler so that we handle
+	 * these consistently.
+	 */
+	do_undefinstr(regs);
 }
 
 long compat_arm_syscall(struct pt_regs *regs);

commit 7d9e8f71b989230bc613d121ca38507d34ada849
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jan 18 17:23:41 2017 +0000

    arm64: avoid returning from bad_mode
    
    Generally, taking an unexpected exception should be a fatal event, and
    bad_mode is intended to cater for this. However, it should be possible
    to contain unexpected synchronous exceptions from EL0 without bringing
    the kernel down, by sending a SIGILL to the task.
    
    We tried to apply this approach in commit 9955ac47f4ba1c95 ("arm64:
    don't kill the kernel on a bad esr from el0"), by sending a signal for
    any bad_mode call resulting from an EL0 exception.
    
    However, this also applies to other unexpected exceptions, such as
    SError and FIQ. The entry paths for these exceptions branch to bad_mode
    without configuring the link register, and have no kernel_exit. Thus, if
    we take one of these exceptions from EL0, bad_mode will eventually
    return to the original user link register value.
    
    This patch fixes this by introducing a new bad_el0_sync handler to cater
    for the recoverable case, and restoring bad_mode to its original state,
    whereby it calls panic() and never returns. The recoverable case
    branches to bad_el0_sync with a bl, and returns to userspace via the
    usual ret_to_user mechanism.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 9955ac47f4ba1c95 ("arm64: don't kill the kernel on a bad esr from el0")
    Reported-by: Mark Salter <msalter@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5b830be79c01..659b2e6b6cf7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -604,17 +604,34 @@ const char *esr_get_class_string(u32 esr)
 }
 
 /*
- * bad_mode handles the impossible case in the exception vector.
+ * bad_mode handles the impossible case in the exception vector. This is always
+ * fatal.
  */
 asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 {
-	siginfo_t info;
-	void __user *pc = (void __user *)instruction_pointer(regs);
 	console_verbose();
 
 	pr_crit("Bad mode in %s handler detected on CPU%d, code 0x%08x -- %s\n",
 		handler[reason], smp_processor_id(), esr,
 		esr_get_class_string(esr));
+
+	die("Oops - bad mode", regs, 0);
+	local_irq_disable();
+	panic("bad mode");
+}
+
+/*
+ * bad_el0_sync handles unexpected, but potentially recoverable synchronous
+ * exceptions taken from EL0. Unlike bad_mode, this returns.
+ */
+asmlinkage void bad_el0_sync(struct pt_regs *regs, int reason, unsigned int esr)
+{
+	siginfo_t info;
+	void __user *pc = (void __user *)instruction_pointer(regs);
+	console_verbose();
+
+	pr_crit("Bad EL0 synchronous exception detected on CPU%d, code 0x%08x -- %s\n",
+		smp_processor_id(), esr, esr_get_class_string(esr));
 	__show_regs(regs);
 
 	info.si_signo = SIGILL;
@@ -622,7 +639,10 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 	info.si_code  = ILL_ILLOPC;
 	info.si_addr  = pc;
 
-	arm64_notify_die("Oops - bad mode", regs, &info, 0);
+	current->thread.fault_address = 0;
+	current->thread.fault_code = 0;
+
+	force_sig_info(info.si_signo, &info, current);
 }
 
 void __pte_error(const char *file, int line, unsigned long val)

commit fe4fbdbcddeaab58a4f9b5297f28b8a4babf6f1f
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 9 17:28:30 2017 +0000

    arm64: cpufeature: Track user visible fields
    
    Track the user visible fields of a CPU feature register. This will be
    used for exposing the value to the userspace. All the user visible
    fields of a feature register will be passed on as it is, while the
    others would be filled with their respective safe value.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5b830be79c01..8187229eb802 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -496,7 +496,7 @@ static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
 {
 	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
 
-	regs->regs[rt] = arm64_ftr_reg_ctrel0.sys_val;
+	regs->regs[rt] = arm64_ftr_reg_user_value(&arm64_ftr_reg_ctrel0);
 	regs->pc += 4;
 }
 

commit 39bc88e5e38e9b213bd7d833ce0df6ec029761ad
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Sep 2 14:54:03 2016 +0100

    arm64: Disable TTBR0_EL1 during normal kernel execution
    
    When the TTBR0 PAN feature is enabled, the kernel entry points need to
    disable access to TTBR0_EL1. The PAN status of the interrupted context
    is stored as part of the saved pstate, reusing the PSR_PAN_BIT (22).
    Restoring access to TTBR0_EL1 is done on exception return if returning
    to user or returning to a context where PAN was disabled.
    
    Context switching via switch_mm() must defer the update of TTBR0_EL1
    until a return to user or an explicit uaccess_enable() call.
    
    Special care needs to be taken for two cases where TTBR0_EL1 is set
    outside the normal kernel context switch operation: EFI run-time
    services (via efi_set_pgd) and CPU suspend (via cpu_(un)install_idmap).
    Code has been added to avoid deferred TTBR0_EL1 switching as in
    switch_mm() and restore the reserved TTBR0_EL1 when uninstalling the
    special TTBR0_EL1.
    
    User cache maintenance (user_cache_maint_handler and
    __flush_cache_user_range) needs the TTBR0_EL1 re-instated since the
    operations are performed by user virtual address.
    
    This patch also removes a stale comment on the switch_mm() function.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 4731133286b3..5b830be79c01 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -440,9 +440,10 @@ int cpu_enable_cache_maint_trap(void *__unused)
 }
 
 #define __user_cache_maint(insn, address, res)			\
-	if (untagged_addr(address) >= user_addr_max())		\
+	if (untagged_addr(address) >= user_addr_max()) {	\
 		res = -EFAULT;					\
-	else							\
+	} else {						\
+		uaccess_ttbr0_enable();				\
 		asm volatile (					\
 			"1:	" insn ", %1\n"			\
 			"	mov	%w0, #0\n"		\
@@ -454,7 +455,9 @@ int cpu_enable_cache_maint_trap(void *__unused)
 			"	.popsection\n"			\
 			_ASM_EXTABLE(1b, 3b)			\
 			: "=r" (res)				\
-			: "r" (address), "i" (-EFAULT) )
+			: "r" (address), "i" (-EFAULT));	\
+		uaccess_ttbr0_disable();			\
+	}
 
 static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 {

commit 9bbd4c56b0b642f04396da378296e68096d5afca
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:08 2016 +0000

    arm64: prep stack walkers for THREAD_INFO_IN_TASK
    
    When CONFIG_THREAD_INFO_IN_TASK is selected, task stacks may be freed
    before a task is destroyed. To account for this, the stacks are
    refcounted, and when manipulating the stack of another task, it is
    necessary to get/put the stack to ensure it isn't freed and/or re-used
    while we do so.
    
    This patch reworks the arm64 stack walking code to account for this.
    When CONFIG_THREAD_INFO_IN_TASK is not selected these perform no
    refcounting, and this should only be a structural change that does not
    affect behaviour.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 7ac30bf943e9..4731133286b3 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -148,6 +148,9 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	if (!tsk)
 		tsk = current;
 
+	if (!try_get_task_stack(tsk))
+		return;
+
 	/*
 	 * Switching between stacks is valid when tracing current and in
 	 * non-preemptible context.
@@ -213,6 +216,8 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 				 stack + sizeof(struct pt_regs));
 		}
 	}
+
+	put_task_stack(tsk);
 }
 
 void show_stack(struct task_struct *tsk, unsigned long *sp)

commit 876e7a38e8788773aac768091aaa3b42e470c03b
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:06 2016 +0000

    arm64: traps: simplify die() and __die()
    
    In arm64's die and __die routines we pass around a thread_info, and
    subsequently use this to determine the relevant task_struct, and the end
    of the thread's stack. Subsequent patches will decouple thread_info from
    the stack, and this approach will no longer work.
    
    To figure out the end of the stack, we can use the new generic
    end_of_stack() helper. As we only call __die() from die(), and die()
    always deals with the current task, we can remove the parameter and have
    both acquire current directly, which also makes it clear that __die
    can't be called for arbitrary tasks.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 95a545200a52..7ac30bf943e9 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -228,10 +228,9 @@ void show_stack(struct task_struct *tsk, unsigned long *sp)
 #endif
 #define S_SMP " SMP"
 
-static int __die(const char *str, int err, struct thread_info *thread,
-		 struct pt_regs *regs)
+static int __die(const char *str, int err, struct pt_regs *regs)
 {
-	struct task_struct *tsk = thread->task;
+	struct task_struct *tsk = current;
 	static int die_counter;
 	int ret;
 
@@ -246,7 +245,8 @@ static int __die(const char *str, int err, struct thread_info *thread,
 	print_modules();
 	__show_regs(regs);
 	pr_emerg("Process %.*s (pid: %d, stack limit = 0x%p)\n",
-		 TASK_COMM_LEN, tsk->comm, task_pid_nr(tsk), thread + 1);
+		 TASK_COMM_LEN, tsk->comm, task_pid_nr(tsk),
+		 end_of_stack(tsk));
 
 	if (!user_mode(regs)) {
 		dump_mem(KERN_EMERG, "Stack: ", regs->sp,
@@ -265,7 +265,6 @@ static DEFINE_RAW_SPINLOCK(die_lock);
  */
 void die(const char *str, struct pt_regs *regs, int err)
 {
-	struct thread_info *thread = current_thread_info();
 	int ret;
 
 	oops_enter();
@@ -273,9 +272,9 @@ void die(const char *str, struct pt_regs *regs, int err)
 	raw_spin_lock_irq(&die_lock);
 	console_verbose();
 	bust_spinlocks(1);
-	ret = __die(str, err, thread, regs);
+	ret = __die(str, err, regs);
 
-	if (regs && kexec_should_crash(thread->task))
+	if (regs && kexec_should_crash(current))
 		crash_kexec(regs);
 
 	bust_spinlocks(0);

commit a9ea0017ebe8889dfa136cac2aa7ae0ee6915e1f
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Nov 3 20:23:05 2016 +0000

    arm64: factor out current_stack_pointer
    
    We define current_stack_pointer in <asm/thread_info.h>, though other
    files and header relying upon it do not have this necessary include, and
    are thus fragile to changes in the header soup.
    
    Subsequent patches will affect the header soup such that directly
    including <asm/thread_info.h> may result in a circular header include in
    some of these cases, so we can't simply include <asm/thread_info.h>.
    
    Instead, factor current_thread_info into its own header, and have all
    existing users include this explicitly.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c9986b3e0a96..95a545200a52 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -38,6 +38,7 @@
 #include <asm/esr.h>
 #include <asm/insn.h>
 #include <asm/traps.h>
+#include <asm/stack_pointer.h>
 #include <asm/stacktrace.h>
 #include <asm/exception.h>
 #include <asm/system_misc.h>

commit 2a6dcb2b5f3e21592ca8dfa198dcce7bec09b020
Author: James Morse <james.morse@arm.com>
Date:   Tue Oct 18 11:27:46 2016 +0100

    arm64: cpufeature: Schedule enable() calls instead of calling them via IPI
    
    The enable() call for a cpufeature/errata is called using on_each_cpu().
    This issues a cross-call IPI to get the work done. Implicitly, this
    stashes the running PSTATE in SPSR when the CPU receives the IPI, and
    restores it when we return. This means an enable() call can never modify
    PSTATE.
    
    To allow PAN to do this, change the on_each_cpu() call to use
    stop_machine(). This schedules the work on each CPU which allows
    us to modify PSTATE.
    
    This involves changing the protype of all the enable() functions.
    
    enable_cpu_capabilities() is called during boot and enables the feature
    on all online CPUs. This path now uses stop_machine(). CPU features for
    hotplug'd CPUs are enabled by verify_local_cpu_features() which only
    acts on the local CPU, and can already modify the running PSTATE as it
    is called from secondary_start_kernel().
    
    Reported-by: Tony Thompson <anthony.thompson@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 7255c9d6cfb7..c9986b3e0a96 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -428,9 +428,10 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
 }
 
-void cpu_enable_cache_maint_trap(void *__unused)
+int cpu_enable_cache_maint_trap(void *__unused)
 {
 	config_sctlr_el1(SCTLR_EL1_UCI, 0);
+	return 0;
 }
 
 #define __user_cache_maint(insn, address, res)			\

commit 87261d19046aeaeed8eb3d2793fde850ae1b5c9e
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Wed Oct 19 14:40:54 2016 +0100

    arm64: Cortex-A53 errata workaround: check for kernel addresses
    
    Commit 7dd01aef0557 ("arm64: trap userspace "dc cvau" cache operation on
    errata-affected core") adds code to execute cache maintenance instructions
    in the kernel on behalf of userland on CPUs with certain ARM CPU errata.
    It turns out that the address hasn't been checked to be a valid user
    space address, allowing userland to clean cache lines in kernel space.
    Fix this by introducing an address check before executing the
    instructions on behalf of userland.
    
    Since the address doesn't come via a syscall parameter, we can't just
    reject tagged pointers and instead have to remove the tag when checking
    against the user address limit.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 7dd01aef0557 ("arm64: trap userspace "dc cvau" cache operation on errata-affected core")
    Reported-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [will: rework commit message + replace access_ok with max_user_addr()]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 5ff020f8fb7f..7255c9d6cfb7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -434,18 +434,21 @@ void cpu_enable_cache_maint_trap(void *__unused)
 }
 
 #define __user_cache_maint(insn, address, res)			\
-	asm volatile (						\
-		"1:	" insn ", %1\n"				\
-		"	mov	%w0, #0\n"			\
-		"2:\n"						\
-		"	.pushsection .fixup,\"ax\"\n"		\
-		"	.align	2\n"				\
-		"3:	mov	%w0, %w2\n"			\
-		"	b	2b\n"				\
-		"	.popsection\n"				\
-		_ASM_EXTABLE(1b, 3b)				\
-		: "=r" (res)					\
-		: "r" (address), "i" (-EFAULT) )
+	if (untagged_addr(address) >= user_addr_max())		\
+		res = -EFAULT;					\
+	else							\
+		asm volatile (					\
+			"1:	" insn ", %1\n"			\
+			"	mov	%w0, #0\n"		\
+			"2:\n"					\
+			"	.pushsection .fixup,\"ax\"\n"	\
+			"	.align	2\n"			\
+			"3:	mov	%w0, %w2\n"		\
+			"	b	2b\n"			\
+			"	.popsection\n"			\
+			_ASM_EXTABLE(1b, 3b)			\
+			: "=r" (res)				\
+			: "r" (address), "i" (-EFAULT) )
 
 static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 {

commit b5e7307d9d5a340d2c9fabbe1cee137d4c682c71
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Sep 23 17:55:05 2016 +0100

    arm64: fix dump_backtrace/unwind_frame with NULL tsk
    
    In some places, dump_backtrace() is called with a NULL tsk parameter,
    e.g. in bug_handler() in arch/arm64, or indirectly via show_stack() in
    core code. The expectation is that this is treated as if current were
    passed instead of NULL. Similar is true of unwind_frame().
    
    Commit a80a0eb70c358f8c ("arm64: make irq_stack_ptr more robust") didn't
    take this into account. In dump_backtrace() it compares tsk against
    current *before* we check if tsk is NULL, and in unwind_frame() we never
    set tsk if it is NULL.
    
    Due to this, we won't initialise irq_stack_ptr in either function. In
    dump_backtrace() this results in calling dump_mem() for memory
    immediately above the IRQ stack range, rather than for the relevant
    range on the task stack. In unwind_frame we'll reject unwinding frames
    on the IRQ stack.
    
    In either case this results in incomplete or misleading backtrace
    information, but is not otherwise problematic. The initial percpu areas
    (including the IRQ stacks) are allocated in the linear map, and dump_mem
    uses __get_user(), so we shouldn't access anything with side-effects,
    and will handle holes safely.
    
    This patch fixes the issue by having both functions handle the NULL tsk
    case before doing anything else with tsk.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: a80a0eb70c358f8c ("arm64: make irq_stack_ptr more robust")
    Acked-by: James Morse <james.morse@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 93445f8b530c..5ff020f8fb7f 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -142,6 +142,11 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	unsigned long irq_stack_ptr;
 	int skip;
 
+	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
+
+	if (!tsk)
+		tsk = current;
+
 	/*
 	 * Switching between stacks is valid when tracing current and in
 	 * non-preemptible context.
@@ -151,11 +156,6 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	else
 		irq_stack_ptr = 0;
 
-	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
-
-	if (!tsk)
-		tsk = current;
-
 	if (tsk == current) {
 		frame.fp = (unsigned long)__builtin_frame_address(0);
 		frame.sp = current_stack_pointer;

commit 116c81f427ff6c5380850963e3fb8798cc821d2b
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:16 2016 +0100

    arm64: Work around systems with mismatched cache line sizes
    
    Systems with differing CPU i-cache/d-cache line sizes can cause
    problems with the cache management by software when the execution
    is migrated from one to another. Usually, the application reads
    the cache size on a CPU and then uses that length to perform cache
    operations. However, if it gets migrated to another CPU with a smaller
    cache line size, things could go completely wrong. To prevent such
    cases, always use the smallest cache line size among the CPUs. The
    kernel CPU feature infrastructure already keeps track of the safe
    value for all CPUID registers including CTR. This patch works around
    the problem by :
    
    For kernel, dynamically patch the kernel to read the cache size
    from the system wide copy of CTR_EL0.
    
    For applications, trap read accesses to CTR_EL0 (by clearing the SCTLR.UCT)
    and emulate the mrs instruction to return the system wide safe value
    of CTR_EL0.
    
    For faster access (i.e, avoiding to lookup the system wide value of CTR_EL0
    via read_system_reg), we keep track of the pointer to table entry for
    CTR_EL0 in the CPU feature infrastructure.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 224f64eddd93..93445f8b530c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -480,6 +480,14 @@ static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 		regs->pc += 4;
 }
 
+static void ctr_read_handler(unsigned int esr, struct pt_regs *regs)
+{
+	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+
+	regs->regs[rt] = arm64_ftr_reg_ctrel0.sys_val;
+	regs->pc += 4;
+}
+
 struct sys64_hook {
 	unsigned int esr_mask;
 	unsigned int esr_val;
@@ -492,6 +500,12 @@ static struct sys64_hook sys64_hooks[] = {
 		.esr_val = ESR_ELx_SYS64_ISS_EL0_CACHE_OP_VAL,
 		.handler = user_cache_maint_handler,
 	},
+	{
+		/* Trap read access to CTR_EL0 */
+		.esr_mask = ESR_ELx_SYS64_ISS_SYS_OP_MASK,
+		.esr_val = ESR_ELx_SYS64_ISS_SYS_CTR_READ,
+		.handler = ctr_read_handler,
+	},
 	{},
 };
 

commit 9dbd5bb25c56e35e6b4c34d968689a1ded850924
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Fri Sep 9 14:07:15 2016 +0100

    arm64: Refactor sysinstr exception handling
    
    Right now we trap some of the user space data cache operations
    based on a few Errata (ARM 819472, 826319, 827319 and 824069).
    We need to trap userspace access to CTR_EL0, if we detect mismatched
    cache line size. Since both these traps share the EC, refactor
    the handler a little bit to make it a bit more reader friendly.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Andre Przywara <andre.przywara@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index e04f83873af7..224f64eddd93 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -447,36 +447,29 @@ void cpu_enable_cache_maint_trap(void *__unused)
 		: "=r" (res)					\
 		: "r" (address), "i" (-EFAULT) )
 
-asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
+static void user_cache_maint_handler(unsigned int esr, struct pt_regs *regs)
 {
 	unsigned long address;
-	int ret;
+	int rt = (esr & ESR_ELx_SYS64_ISS_RT_MASK) >> ESR_ELx_SYS64_ISS_RT_SHIFT;
+	int crm = (esr & ESR_ELx_SYS64_ISS_CRM_MASK) >> ESR_ELx_SYS64_ISS_CRM_SHIFT;
+	int ret = 0;
 
-	/* if this is a write with: Op0=1, Op2=1, Op1=3, CRn=7 */
-	if ((esr & 0x01fffc01) == 0x0012dc00) {
-		int rt = (esr >> 5) & 0x1f;
-		int crm = (esr >> 1) & 0x0f;
+	address = (rt == 31) ? 0 : regs->regs[rt];
 
-		address = (rt == 31) ? 0 : regs->regs[rt];
-
-		switch (crm) {
-		case 11:		/* DC CVAU, gets promoted */
-			__user_cache_maint("dc civac", address, ret);
-			break;
-		case 10:		/* DC CVAC, gets promoted */
-			__user_cache_maint("dc civac", address, ret);
-			break;
-		case 14:		/* DC CIVAC */
-			__user_cache_maint("dc civac", address, ret);
-			break;
-		case 5:			/* IC IVAU */
-			__user_cache_maint("ic ivau", address, ret);
-			break;
-		default:
-			force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
-			return;
-		}
-	} else {
+	switch (crm) {
+	case ESR_ELx_SYS64_ISS_CRM_DC_CVAU:	/* DC CVAU, gets promoted */
+		__user_cache_maint("dc civac", address, ret);
+		break;
+	case ESR_ELx_SYS64_ISS_CRM_DC_CVAC:	/* DC CVAC, gets promoted */
+		__user_cache_maint("dc civac", address, ret);
+		break;
+	case ESR_ELx_SYS64_ISS_CRM_DC_CIVAC:	/* DC CIVAC */
+		__user_cache_maint("dc civac", address, ret);
+		break;
+	case ESR_ELx_SYS64_ISS_CRM_IC_IVAU:	/* IC IVAU */
+		__user_cache_maint("ic ivau", address, ret);
+		break;
+	default:
 		force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
 		return;
 	}
@@ -487,6 +480,34 @@ asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
 		regs->pc += 4;
 }
 
+struct sys64_hook {
+	unsigned int esr_mask;
+	unsigned int esr_val;
+	void (*handler)(unsigned int esr, struct pt_regs *regs);
+};
+
+static struct sys64_hook sys64_hooks[] = {
+	{
+		.esr_mask = ESR_ELx_SYS64_ISS_EL0_CACHE_OP_MASK,
+		.esr_val = ESR_ELx_SYS64_ISS_EL0_CACHE_OP_VAL,
+		.handler = user_cache_maint_handler,
+	},
+	{},
+};
+
+asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
+{
+	struct sys64_hook *hook;
+
+	for (hook = sys64_hooks; hook->handler; hook++)
+		if ((hook->esr_mask & esr) == hook->esr_val) {
+			hook->handler(esr, regs);
+			return;
+		}
+
+	force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
+}
+
 long compat_arm_syscall(struct pt_regs *regs);
 
 asmlinkage long do_ni_syscall(struct pt_regs *regs)

commit 7dd01aef055792260287c6708daf75aac3918f66
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:32 2016 +0100

    arm64: trap userspace "dc cvau" cache operation on errata-affected core
    
    The ARM errata 819472, 826319, 827319 and 824069 for affected
    Cortex-A53 cores demand to promote "dc cvau" instructions to
    "dc civac". Since we allow userspace to also emit those instructions,
    we should make sure that "dc cvau" gets promoted there too.
    So lets grasp the nettle here and actually trap every userland cache
    maintenance instruction once we detect at least one affected core in
    the system.
    We then emulate the instruction by executing it on behalf of userland,
    promoting "dc cvau" to "dc civac" on the way and injecting access
    fault back into userspace.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [catalin.marinas@arm.com: s/set_segfault/arm64_notify_segfault/]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d8a5366dcc24..e04f83873af7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -41,6 +41,7 @@
 #include <asm/stacktrace.h>
 #include <asm/exception.h>
 #include <asm/system_misc.h>
+#include <asm/sysreg.h>
 
 static const char *handler[]= {
 	"Synchronous Abort",
@@ -427,6 +428,65 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
 }
 
+void cpu_enable_cache_maint_trap(void *__unused)
+{
+	config_sctlr_el1(SCTLR_EL1_UCI, 0);
+}
+
+#define __user_cache_maint(insn, address, res)			\
+	asm volatile (						\
+		"1:	" insn ", %1\n"				\
+		"	mov	%w0, #0\n"			\
+		"2:\n"						\
+		"	.pushsection .fixup,\"ax\"\n"		\
+		"	.align	2\n"				\
+		"3:	mov	%w0, %w2\n"			\
+		"	b	2b\n"				\
+		"	.popsection\n"				\
+		_ASM_EXTABLE(1b, 3b)				\
+		: "=r" (res)					\
+		: "r" (address), "i" (-EFAULT) )
+
+asmlinkage void __exception do_sysinstr(unsigned int esr, struct pt_regs *regs)
+{
+	unsigned long address;
+	int ret;
+
+	/* if this is a write with: Op0=1, Op2=1, Op1=3, CRn=7 */
+	if ((esr & 0x01fffc01) == 0x0012dc00) {
+		int rt = (esr >> 5) & 0x1f;
+		int crm = (esr >> 1) & 0x0f;
+
+		address = (rt == 31) ? 0 : regs->regs[rt];
+
+		switch (crm) {
+		case 11:		/* DC CVAU, gets promoted */
+			__user_cache_maint("dc civac", address, ret);
+			break;
+		case 10:		/* DC CVAC, gets promoted */
+			__user_cache_maint("dc civac", address, ret);
+			break;
+		case 14:		/* DC CIVAC */
+			__user_cache_maint("dc civac", address, ret);
+			break;
+		case 5:			/* IC IVAU */
+			__user_cache_maint("ic ivau", address, ret);
+			break;
+		default:
+			force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
+			return;
+		}
+	} else {
+		force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
+		return;
+	}
+
+	if (ret)
+		arm64_notify_segfault(regs, address);
+	else
+		regs->pc += 4;
+}
+
 long compat_arm_syscall(struct pt_regs *regs);
 
 asmlinkage long do_ni_syscall(struct pt_regs *regs)

commit 390bf1773c7eba3b45df62ae82b3d2be911185b7
Author: Andre Przywara <andre.przywara@arm.com>
Date:   Tue Jun 28 18:07:31 2016 +0100

    arm64: consolidate signal injection on emulation errors
    
    The code for injecting a signal into userland if a trapped instruction
    fails emulation due to a _userland_ error (like an illegal address)
    will be used more often with the next patch.
    Factor out the core functionality into a separate function and use
    that both for the existing trap handler and for the deprecated
    instructions emulation.
    
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    [catalin.marinas@arm.com: s/set_segfault/arm64_notify_segfault/]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index a4250a59f2b9..d8a5366dcc24 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -364,30 +364,67 @@ static int call_undef_hook(struct pt_regs *regs)
 	return fn ? fn(regs, instr) : 1;
 }
 
-asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
+static void force_signal_inject(int signal, int code, struct pt_regs *regs,
+				unsigned long address)
 {
 	siginfo_t info;
 	void __user *pc = (void __user *)instruction_pointer(regs);
+	const char *desc;
 
-	/* check for AArch32 breakpoint instructions */
-	if (!aarch32_break_handler(regs))
-		return;
-
-	if (call_undef_hook(regs) == 0)
-		return;
+	switch (signal) {
+	case SIGILL:
+		desc = "undefined instruction";
+		break;
+	case SIGSEGV:
+		desc = "illegal memory access";
+		break;
+	default:
+		desc = "bad mode";
+		break;
+	}
 
-	if (unhandled_signal(current, SIGILL) && show_unhandled_signals_ratelimited()) {
-		pr_info("%s[%d]: undefined instruction: pc=%p\n",
-			current->comm, task_pid_nr(current), pc);
+	if (unhandled_signal(current, signal) &&
+	    show_unhandled_signals_ratelimited()) {
+		pr_info("%s[%d]: %s: pc=%p\n",
+			current->comm, task_pid_nr(current), desc, pc);
 		dump_instr(KERN_INFO, regs);
 	}
 
-	info.si_signo = SIGILL;
+	info.si_signo = signal;
 	info.si_errno = 0;
-	info.si_code  = ILL_ILLOPC;
+	info.si_code  = code;
 	info.si_addr  = pc;
 
-	arm64_notify_die("Oops - undefined instruction", regs, &info, 0);
+	arm64_notify_die(desc, regs, &info, 0);
+}
+
+/*
+ * Set up process info to signal segmentation fault - called on access error.
+ */
+void arm64_notify_segfault(struct pt_regs *regs, unsigned long addr)
+{
+	int code;
+
+	down_read(&current->mm->mmap_sem);
+	if (find_vma(current->mm, addr) == NULL)
+		code = SEGV_MAPERR;
+	else
+		code = SEGV_ACCERR;
+	up_read(&current->mm->mmap_sem);
+
+	force_signal_inject(SIGSEGV, code, regs, addr);
+}
+
+asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
+{
+	/* check for AArch32 breakpoint instructions */
+	if (!aarch32_break_handler(regs))
+		return;
+
+	if (call_undef_hook(regs) == 0)
+		return;
+
+	force_signal_inject(SIGILL, ILL_ILLOPC, regs, 0);
 }
 
 long compat_arm_syscall(struct pt_regs *regs);

commit 275f344bec51e9100bae81f3cc8c6940bbfb24c0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 31 12:33:01 2016 +0100

    arm64: add macro to extract ESR_ELx.EC
    
    Several places open-code extraction of the EC field from an ESR_ELx
    value, in subtly different ways. This is unfortunate duplication and
    variation, and the precise logic used to extract the field is a
    distraction.
    
    This patch adds a new macro, ESR_ELx_EC(), to extract the EC field from
    an ESR_ELx value in a consistent fashion.
    
    Existing open-coded extractions in core arm64 code are moved over to the
    new helper. KVM code is left as-is for the moment.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Huang Shijie <shijie.huang@arm.com>
    Cc: Dave P Martin <dave.martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index d9da2c56e4f8..a4250a59f2b9 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -456,7 +456,7 @@ static const char *esr_class_str[] = {
 
 const char *esr_get_class_string(u32 esr)
 {
-	return esr_class_str[esr >> ESR_ELx_EC_SHIFT];
+	return esr_class_str[ESR_ELx_EC(esr)];
 }
 
 /*

commit 7ceb3a1040524c32aa71df8807427e348fac49da
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jun 13 11:15:15 2016 +0100

    arm64: simplify dump_mem
    
    Currently dump_mem attempts to dump memory in 64-bit chunks when
    reporting a failure in 64-bit code, or 32-bit chunks when reporting a
    failure in 32-bit code. We added code to handle these two cases
    separately in commit e147ae6d7f908412 ("arm64: modify the dump mem for
    64 bit addresses").
    
    However, in all cases dump_mem is called, the failing context is a
    kernel rather than user context. Additionally dump_mem is assumed to
    only be used for kernel contexts, as internally it switches to
    KERNEL_DS, and its callers pass kernel stack bounds.
    
    This patch removes the redundant 32-bit chunk logic and associated
    compat parameter, largely reverting the aforementioned commit. For the
    call in __die(), the check of in_interrupt() is removed also, as __die()
    is only called in response to faults from the kernel's exception level,
    and thus the !user_mode(regs) check is sufficient. Were this not the
    case, the used of task_stack_page(tsk) to generate the stack bounds
    would be erroneous.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 2a43012616b7..d9da2c56e4f8 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -52,15 +52,14 @@ static const char *handler[]= {
 int show_unhandled_signals = 1;
 
 /*
- * Dump out the contents of some memory nicely...
+ * Dump out the contents of some kernel memory nicely...
  */
 static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
-		     unsigned long top, bool compat)
+		     unsigned long top)
 {
 	unsigned long first;
 	mm_segment_t fs;
 	int i;
-	unsigned int width = compat ? 4 : 8;
 
 	/*
 	 * We need to switch to kernel mode so that we can use __get_user
@@ -78,22 +77,15 @@ static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
 		memset(str, ' ', sizeof(str));
 		str[sizeof(str) - 1] = '\0';
 
-		for (p = first, i = 0; i < (32 / width)
-					&& p < top; i++, p += width) {
+		for (p = first, i = 0; i < (32 / 8)
+					&& p < top; i++, p += 8) {
 			if (p >= bottom && p < top) {
 				unsigned long val;
 
-				if (width == 8) {
-					if (__get_user(val, (unsigned long *)p) == 0)
-						sprintf(str + i * 17, " %016lx", val);
-					else
-						sprintf(str + i * 17, " ????????????????");
-				} else {
-					if (__get_user(val, (unsigned int *)p) == 0)
-						sprintf(str + i * 9, " %08lx", val);
-					else
-						sprintf(str + i * 9, " ????????");
-				}
+				if (__get_user(val, (unsigned long *)p) == 0)
+					sprintf(str + i * 17, " %016lx", val);
+				else
+					sprintf(str + i * 17, " ????????????????");
 			}
 		}
 		printk("%s%04lx:%s\n", lvl, first & 0xffff, str);
@@ -216,7 +208,7 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 				stack = IRQ_STACK_TO_TASK_STACK(irq_stack_ptr);
 
 			dump_mem("", "Exception stack", stack,
-				 stack + sizeof(struct pt_regs), false);
+				 stack + sizeof(struct pt_regs));
 		}
 	}
 }
@@ -254,10 +246,9 @@ static int __die(const char *str, int err, struct thread_info *thread,
 	pr_emerg("Process %.*s (pid: %d, stack limit = 0x%p)\n",
 		 TASK_COMM_LEN, tsk->comm, task_pid_nr(tsk), thread + 1);
 
-	if (!user_mode(regs) || in_interrupt()) {
+	if (!user_mode(regs)) {
 		dump_mem(KERN_EMERG, "Stack: ", regs->sp,
-			 THREAD_SIZE + (unsigned long)task_stack_page(tsk),
-			 compat_user_mode(regs));
+			 THREAD_SIZE + (unsigned long)task_stack_page(tsk));
 		dump_backtrace(regs, tsk);
 		dump_instr(KERN_EMERG, regs);
 	}

commit c5cea06be060f38e5400d796e61cfc8c36e52924
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jun 13 11:15:14 2016 +0100

    arm64: fix dump_instr when PAN and UAO are in use
    
    If the kernel is set to show unhandled signals, and a user task does not
    handle a SIGILL as a result of an instruction abort, we will attempt to
    log the offending instruction with dump_instr before killing the task.
    
    We use dump_instr to log the encoding of the offending userspace
    instruction. However, dump_instr is also used to dump instructions from
    kernel space, and internally always switches to KERNEL_DS before dumping
    the instruction with get_user. When both PAN and UAO are in use, reading
    a user instruction via get_user while in KERNEL_DS will result in a
    permission fault, which leads to an Oops.
    
    As we have regs corresponding to the context of the original instruction
    abort, we can inspect this and only flip to KERNEL_DS if the original
    abort was taken from the kernel, avoiding this issue. At the same time,
    remove the redundant (and incorrect) comments regarding the order
    dump_mem and dump_instr are called in.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: <stable@vger.kernel.org> #4.6+
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Tested-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Fixes: 57f4959bad0a154a ("arm64: kernel: Add support for User Access Override")
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index f7cf463107df..2a43012616b7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -64,8 +64,7 @@ static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
 
 	/*
 	 * We need to switch to kernel mode so that we can use __get_user
-	 * to safely read from kernel space.  Note that we now dump the
-	 * code first, just in case the backtrace kills us.
+	 * to safely read from kernel space.
 	 */
 	fs = get_fs();
 	set_fs(KERNEL_DS);
@@ -111,21 +110,12 @@ static void dump_backtrace_entry(unsigned long where)
 	print_ip_sym(where);
 }
 
-static void dump_instr(const char *lvl, struct pt_regs *regs)
+static void __dump_instr(const char *lvl, struct pt_regs *regs)
 {
 	unsigned long addr = instruction_pointer(regs);
-	mm_segment_t fs;
 	char str[sizeof("00000000 ") * 5 + 2 + 1], *p = str;
 	int i;
 
-	/*
-	 * We need to switch to kernel mode so that we can use __get_user
-	 * to safely read from kernel space.  Note that we now dump the
-	 * code first, just in case the backtrace kills us.
-	 */
-	fs = get_fs();
-	set_fs(KERNEL_DS);
-
 	for (i = -4; i < 1; i++) {
 		unsigned int val, bad;
 
@@ -139,8 +129,18 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 		}
 	}
 	printk("%sCode: %s\n", lvl, str);
+}
 
-	set_fs(fs);
+static void dump_instr(const char *lvl, struct pt_regs *regs)
+{
+	if (!user_mode(regs)) {
+		mm_segment_t fs = get_fs();
+		set_fs(KERNEL_DS);
+		__dump_instr(lvl, regs);
+		set_fs(fs);
+	} else {
+		__dump_instr(lvl, regs);
+	}
 }
 
 static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)

commit 8051f4d16ef1d037e7b12abab79c3e0b960f4d36
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 31 12:07:47 2016 +0100

    arm64: report CPU number in bad_mode
    
    If we take an exception we don't expect (e.g. SError), we report this in
    the bad_mode handler with pr_crit. Depending on the configured log
    level, we may or may not log additional information in functions called
    subsequently. Notably, the messages in dump_stack (including the CPU
    number) are printed with KERN_DEFAULT and may not appear.
    
    Some exceptions have an IMPLEMENTATION DEFINED ESR_ELx.ISS encoding, and
    knowing the CPU number is crucial to correctly decode them. To ensure
    that this is always possible, we should log the CPU number along with
    the ESR_ELx value, so we are not reliant on subsequent logs or
    additional printk configuration options.
    
    This patch logs the CPU number in bad_mode such that it is possible for
    a developer to decode these exceptions, provided access to sufficient
    documentation.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Al Grant <Al.Grant@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c5392081b49b..f7cf463107df 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -477,8 +477,9 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 	void __user *pc = (void __user *)instruction_pointer(regs);
 	console_verbose();
 
-	pr_crit("Bad mode in %s handler detected, code 0x%08x -- %s\n",
-		handler[reason], esr, esr_get_class_string(esr));
+	pr_crit("Bad mode in %s handler detected on CPU%d, code 0x%08x -- %s\n",
+		handler[reason], smp_processor_id(), esr,
+		esr_get_class_string(esr));
 	__show_regs(regs);
 
 	info.si_signo = SIGILL;

commit a80a0eb70c358f8c7dda4bb62b2278dc6285217b
Author: Yang Shi <yang.shi@linaro.org>
Date:   Thu Feb 11 13:53:10 2016 -0800

    arm64: make irq_stack_ptr more robust
    
    Switching between stacks is only valid if we are tracing ourselves while on the
    irq_stack, so it is only valid when in current and non-preemptible context,
    otherwise is is just zeroed off.
    
    Fixes: 132cd887b5c5 ("arm64: Modify stack trace and dump for use with irq_stack")
    Acked-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index cbedd724f48e..c5392081b49b 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -146,9 +146,18 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
-	unsigned long irq_stack_ptr = IRQ_STACK_PTR(smp_processor_id());
+	unsigned long irq_stack_ptr;
 	int skip;
 
+	/*
+	 * Switching between stacks is valid when tracing current and in
+	 * non-preemptible context.
+	 */
+	if (tsk == current && !preemptible())
+		irq_stack_ptr = IRQ_STACK_PTR(smp_processor_id());
+	else
+		irq_stack_ptr = 0;
+
 	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
 
 	if (!tsk)

commit c9cd0ed925c0b927283d4739bfe689eb9d1e9dfd
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Dec 21 16:44:27 2015 +0000

    arm64: traps: address fallout from printk -> pr_* conversion
    
    Commit ac7b406c1a9d ("arm64: Use pr_* instead of printk") was a fairly
    mindless s/printk/pr_*/ change driven by a complaint from checkpatch.
    
    As is usual with such changes, this has led to some odd behaviour on
    arm64:
    
      * syslog now picks up the "pr_emerg" line from dump_backtrace, but not
        the actual trace, which leads to a bunch of "kernel:Call trace:"
        lines in the log
    
      * __{pte,pmd,pgd}_error print at KERN_CRIT, as opposed to KERN_ERR
        which is used by other architectures.
    
    This patch restores the original printk behaviour for dump_backtrace
    and downgrade the pgtable error macros to KERN_ERR.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index bdc293f6adc4..cbedd724f48e 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -171,7 +171,7 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 #endif
 
 	skip = !!regs;
-	pr_emerg("Call trace:\n");
+	printk("Call trace:\n");
 	while (1) {
 		unsigned long where = frame.pc;
 		unsigned long stack;
@@ -482,22 +482,22 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 
 void __pte_error(const char *file, int line, unsigned long val)
 {
-	pr_crit("%s:%d: bad pte %016lx.\n", file, line, val);
+	pr_err("%s:%d: bad pte %016lx.\n", file, line, val);
 }
 
 void __pmd_error(const char *file, int line, unsigned long val)
 {
-	pr_crit("%s:%d: bad pmd %016lx.\n", file, line, val);
+	pr_err("%s:%d: bad pmd %016lx.\n", file, line, val);
 }
 
 void __pud_error(const char *file, int line, unsigned long val)
 {
-	pr_crit("%s:%d: bad pud %016lx.\n", file, line, val);
+	pr_err("%s:%d: bad pud %016lx.\n", file, line, val);
 }
 
 void __pgd_error(const char *file, int line, unsigned long val)
 {
-	pr_crit("%s:%d: bad pgd %016lx.\n", file, line, val);
+	pr_err("%s:%d: bad pgd %016lx.\n", file, line, val);
 }
 
 /* GENERIC_BUG traps */

commit 20380bb390a443b2c5c8800cec59743faf8151b4
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:41 2015 +0900

    arm64: ftrace: fix a stack tracer's output under function graph tracer
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function return. This will result in many useless entries
    (return_to_handler) showing up in
     a) a stack tracer's output
     b) perf call graph (with perf record -g)
     c) dump_backtrace (at panic et al.)
    
    For example, in case of a),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo 1 > /proc/sys/kernel/stack_trace_enabled
      $ cat /sys/kernel/debug/tracing/stack_trace
            Depth    Size   Location    (54 entries)
            -----    ----   --------
      0)     4504      16   gic_raise_softirq+0x28/0x150
      1)     4488      80   smp_cross_call+0x38/0xb8
      2)     4408      48   return_to_handler+0x0/0x40
      3)     4360      32   return_to_handler+0x0/0x40
      ...
    
    In case of b),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ perf record -e mem:XXX:x -ag -- sleep 10
      $ perf report
                      ...
                      |          |          |--0.22%-- 0x550f8
                      |          |          |          0x10888
                      |          |          |          el0_svc_naked
                      |          |          |          sys_openat
                      |          |          |          return_to_handler
                      |          |          |          return_to_handler
                      ...
    
    In case of c),
      $ echo function_graph > /sys/kernel/debug/tracing/current_tracer
      $ echo c > /proc/sysrq-trigger
      ...
      Call trace:
      [<ffffffc00044d3ac>] sysrq_handle_crash+0x24/0x30
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      [<ffffffc000092250>] return_to_handler+0x0/0x40
      ...
    
    This patch replaces such entries with real addresses preserved in
    current->ret_stack[] at unwind_frame(). This way, we can cover all
    the cases.
    
    Reviewed-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [will: fixed minor context changes conflicting with irq stack bits]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 937008523fa5..bdc293f6adc4 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -147,17 +147,14 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
 	unsigned long irq_stack_ptr = IRQ_STACK_PTR(smp_processor_id());
+	int skip;
 
 	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
 
 	if (!tsk)
 		tsk = current;
 
-	if (regs) {
-		frame.fp = regs->regs[29];
-		frame.sp = regs->sp;
-		frame.pc = regs->pc;
-	} else if (tsk == current) {
+	if (tsk == current) {
 		frame.fp = (unsigned long)__builtin_frame_address(0);
 		frame.sp = current_stack_pointer;
 		frame.pc = (unsigned long)dump_backtrace;
@@ -169,14 +166,31 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		frame.sp = thread_saved_sp(tsk);
 		frame.pc = thread_saved_pc(tsk);
 	}
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	frame.graph = tsk->curr_ret_stack;
+#endif
 
+	skip = !!regs;
 	pr_emerg("Call trace:\n");
 	while (1) {
 		unsigned long where = frame.pc;
 		unsigned long stack;
 		int ret;
 
-		dump_backtrace_entry(where);
+		/* skip until specified stack frame */
+		if (!skip) {
+			dump_backtrace_entry(where);
+		} else if (frame.fp == regs->regs[29]) {
+			skip = 0;
+			/*
+			 * Mostly, this is the case where this function is
+			 * called in panic/abort. As exception handler's
+			 * stack frame does not contain the corresponding pc
+			 * at which an exception has taken place, use regs->pc
+			 * instead.
+			 */
+			dump_backtrace_entry(regs->pc);
+		}
 		ret = unwind_frame(tsk, &frame);
 		if (ret < 0)
 			break;

commit fe13f95b720075327a761fe6ddb45b0c90cab504
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Tue Dec 15 17:33:40 2015 +0900

    arm64: pass a task parameter to unwind_frame()
    
    Function graph tracer modifies a return address (LR) in a stack frame
    to hook a function's return. This will result in many useless entries
    (return_to_handler) showing up in a call stack list.
    We will fix this problem in a later patch ("arm64: ftrace: fix a stack
    tracer's output under function graph tracer"). But since real return
    addresses are saved in ret_stack[] array in struct task_struct,
    unwind functions need to be notified of, in addition to a stack pointer
    address, which task is being traced in order to find out real return
    addresses.
    
    This patch extends unwind functions' interfaces by adding an extra
    argument of a pointer to task_struct.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 8a0084541f84..937008523fa5 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -177,7 +177,7 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		int ret;
 
 		dump_backtrace_entry(where);
-		ret = unwind_frame(&frame);
+		ret = unwind_frame(tsk, &frame);
 		if (ret < 0)
 			break;
 		stack = frame.sp;

commit 132cd887b5c54758d04bf25c52fa48f45e843a30
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Dec 4 11:02:26 2015 +0000

    arm64: Modify stack trace and dump for use with irq_stack
    
    This patch allows unwind_frame() to traverse from interrupt stack to task
    stack correctly. It requires data from a dummy stack frame, created
    during irq_stack_entry(), added by a later patch.
    
    A similar approach is taken to modify dump_backtrace(), which expects to
    find struct pt_regs underneath any call to functions marked __exception.
    When on an irq_stack, the struct pt_regs is stored on the old task stack,
    the location of which is stored in the dummy stack frame.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [james.morse: merged two patches, reworked for per_cpu irq_stacks, and
     no alignment guarantees, added irq_stack definitions]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index e9b9b5364393..8a0084541f84 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -146,6 +146,7 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
+	unsigned long irq_stack_ptr = IRQ_STACK_PTR(smp_processor_id());
 
 	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
 
@@ -180,9 +181,20 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		if (ret < 0)
 			break;
 		stack = frame.sp;
-		if (in_exception_text(where))
+		if (in_exception_text(where)) {
+			/*
+			 * If we switched to the irq_stack before calling this
+			 * exception handler, then the pt_regs will be on the
+			 * task stack. The easiest way to tell is if the large
+			 * pt_regs would overlap with the end of the irq_stack.
+			 */
+			if (stack < irq_stack_ptr &&
+			    (stack + sizeof(struct pt_regs)) > irq_stack_ptr)
+				stack = IRQ_STACK_TO_TASK_STACK(irq_stack_ptr);
+
 			dump_mem("", "Exception stack", stack,
 				 stack + sizeof(struct pt_regs), false);
+		}
 	}
 }
 

commit 9f93f3e9461a30f425fdba15784db67ce878ce00
Author: Jungseok Lee <jungseoklee85@gmail.com>
Date:   Sat Oct 17 14:28:11 2015 +0000

    arm64: Synchronise dump_backtrace() with perf callchain
    
    Unlike perf callchain relying on walk_stackframe(), dump_backtrace()
    has its own backtrace logic. A major difference between them is the
    moment a symbol is recorded. Perf writes down a symbol *before*
    calling unwind_frame(), but dump_backtrace() prints it out *after*
    unwind_frame(). As a result, the last valid symbol cannot be hooked
    in case of dump_backtrace(). This patch addresses the issue as
    synchronising dump_backtrace() with perf callchain.
    
    A simple test and its results are as follows:
    
    - crash trigger
    
     $ sudo echo c > /proc/sysrq-trigger
    
    - current status
    
     Call trace:
     [<fffffe00003dc738>] sysrq_handle_crash+0x24/0x30
     [<fffffe00003dd2ac>] __handle_sysrq+0x128/0x19c
     [<fffffe00003dd730>] write_sysrq_trigger+0x60/0x74
     [<fffffe0000249fc4>] proc_reg_write+0x84/0xc0
     [<fffffe00001f2638>] __vfs_write+0x44/0x104
     [<fffffe00001f2e60>] vfs_write+0x98/0x1a8
     [<fffffe00001f3730>] SyS_write+0x50/0xb0
    
    - with this change
    
     Call trace:
     [<fffffe00003dc738>] sysrq_handle_crash+0x24/0x30
     [<fffffe00003dd2ac>] __handle_sysrq+0x128/0x19c
     [<fffffe00003dd730>] write_sysrq_trigger+0x60/0x74
     [<fffffe0000249fc4>] proc_reg_write+0x84/0xc0
     [<fffffe00001f2638>] __vfs_write+0x44/0x104
     [<fffffe00001f2e60>] vfs_write+0x98/0x1a8
     [<fffffe00001f3730>] SyS_write+0x50/0xb0
     [<fffffe00000939ec>] el0_svc_naked+0x20/0x28
    
    Note that this patch does not cover a case where MMU is disabled. The
    last stack frame of swapper, for example, has PC in a form of physical
    address. Unfortunately, a simple conversion using phys_to_virt() cannot
    cover all scenarios since PC is retrieved from LR - 4, not LR. It is
    a big tradeoff to change both head.S and unwind_frame() for only a few
    of symbols in *.S. Thus, this hunk does not take care of the case.
    
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: James Morse <james.morse@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Jungseok Lee <jungseoklee85@gmail.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index f93aae5e4307..e9b9b5364393 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -103,12 +103,12 @@ static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
 	set_fs(fs);
 }
 
-static void dump_backtrace_entry(unsigned long where, unsigned long stack)
+static void dump_backtrace_entry(unsigned long where)
 {
+	/*
+	 * Note that 'where' can have a physical address, but it's not handled.
+	 */
 	print_ip_sym(where);
-	if (in_exception_text(where))
-		dump_mem("", "Exception stack", stack,
-			 stack + sizeof(struct pt_regs), false);
 }
 
 static void dump_instr(const char *lvl, struct pt_regs *regs)
@@ -172,12 +172,17 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	pr_emerg("Call trace:\n");
 	while (1) {
 		unsigned long where = frame.pc;
+		unsigned long stack;
 		int ret;
 
+		dump_backtrace_entry(where);
 		ret = unwind_frame(&frame);
 		if (ret < 0)
 			break;
-		dump_backtrace_entry(where, frame.sp);
+		stack = frame.sp;
+		if (in_exception_text(where))
+			dump_mem("", "Exception stack", stack,
+				 stack + sizeof(struct pt_regs), false);
 	}
 }
 

commit a4653228a0f8d0a4a76d03a2dd15beaf6e78c22b
Author: Dave P Martin <Dave.Martin@arm.com>
Date:   Fri Jul 24 16:37:49 2015 +0100

    arm64/BUG: Show explicit backtrace for WARNs
    
    The generic slowpath WARN implementation prints a backtrace, but
    the report_bug() based implementation does not, opting to print the
    registers instead which is generally not as useful.
    
    Ideally, report_bug() should be fixed to make the behaviour more
    consistent, but in the meantime this patch generates a backtrace
    directly from the arm64 backend instead so that this functionality
    is not lost with the migration to report_bug().
    
    As a side-effect, the backtrace will be outside the oops end
    marker, but that's hard to avoid without modifying generic code.
    
    This patch can go away if report_bug() grows the ability in the
    future to generate a backtrace directly or call an arch hook at the
    appropriate time.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 824ba5ac6361..f93aae5e4307 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -494,6 +494,8 @@ static int bug_handler(struct pt_regs *regs, unsigned int esr)
 		break;
 
 	case BUG_TRAP_TYPE_WARN:
+		/* Ideally, report_bug() should backtrace for us... but no. */
+		dump_backtrace(regs, NULL);
 		break;
 
 	default:

commit 9fb7410f955f7a62c1f882ca8f9ffd4525907e28
Author: Dave P Martin <Dave.Martin@arm.com>
Date:   Fri Jul 24 16:37:48 2015 +0100

    arm64/BUG: Use BRK instruction for generic BUG traps
    
    Currently, the minimal default BUG() implementation from asm-
    generic is used for arm64.
    
    This patch uses the BRK software breakpoint instruction to generate
    a trap instead, similarly to most other arches, with the generic
    BUG code generating the dmesg boilerplate.
    
    This allows bug metadata to be moved to a separate table and
    reduces the amount of inline code at BUG and WARN sites.  This also
    avoids clobbering any registers before they can be dumped.
    
    To mitigate the size of the bug table further, this patch makes
    use of the existing infrastructure for encoding addresses within
    the bug table as 32-bit offsets instead of absolute pointers.
    (Note that this limits the kernel size to 2GB.)
    
    Traps are registered at arch_initcall time for aarch64, but BUG
    has minimal real dependencies and it is desirable to be able to
    generate bug splats as early as possible.  This patch redirects
    all debug exceptions caused by BRK directly to bug_handler() until
    the full debug exception support has been initialised.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 1ea920cbd66d..824ba5ac6361 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -17,6 +17,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/bug.h>
 #include <linux/signal.h>
 #include <linux/personality.h>
 #include <linux/kallsyms.h>
@@ -32,8 +33,10 @@
 #include <linux/syscalls.h>
 
 #include <asm/atomic.h>
+#include <asm/bug.h>
 #include <asm/debug-monitors.h>
 #include <asm/esr.h>
+#include <asm/insn.h>
 #include <asm/traps.h>
 #include <asm/stacktrace.h>
 #include <asm/exception.h>
@@ -466,7 +469,61 @@ void __pgd_error(const char *file, int line, unsigned long val)
 	pr_crit("%s:%d: bad pgd %016lx.\n", file, line, val);
 }
 
+/* GENERIC_BUG traps */
+
+int is_valid_bugaddr(unsigned long addr)
+{
+	/*
+	 * bug_handler() only called for BRK #BUG_BRK_IMM.
+	 * So the answer is trivial -- any spurious instances with no
+	 * bug table entry will be rejected by report_bug() and passed
+	 * back to the debug-monitors code and handled as a fatal
+	 * unexpected debug exception.
+	 */
+	return 1;
+}
+
+static int bug_handler(struct pt_regs *regs, unsigned int esr)
+{
+	if (user_mode(regs))
+		return DBG_HOOK_ERROR;
+
+	switch (report_bug(regs->pc, regs)) {
+	case BUG_TRAP_TYPE_BUG:
+		die("Oops - BUG", regs, 0);
+		break;
+
+	case BUG_TRAP_TYPE_WARN:
+		break;
+
+	default:
+		/* unknown/unrecognised bug trap type */
+		return DBG_HOOK_ERROR;
+	}
+
+	/* If thread survives, skip over the BUG instruction and continue: */
+	regs->pc += AARCH64_INSN_SIZE;	/* skip BRK and resume */
+	return DBG_HOOK_HANDLED;
+}
+
+static struct break_hook bug_break_hook = {
+	.esr_val = 0xf2000000 | BUG_BRK_IMM,
+	.esr_mask = 0xffffffff,
+	.fn = bug_handler,
+};
+
+/*
+ * Initial handler for AArch64 BRK exceptions
+ * This handler only used until debug_traps_init().
+ */
+int __init early_brk64(unsigned long addr, unsigned int esr,
+		struct pt_regs *regs)
+{
+	return bug_handler(regs, esr) != DBG_HOOK_HANDLED;
+}
+
+/* This registration must happen early, before debug_traps_init(). */
 void __init trap_init(void)
 {
-	return;
+	register_break_hook(&bug_break_hook);
 }

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 4db6a2574fec..1ea920cbd66d 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -189,11 +189,7 @@ void show_stack(struct task_struct *tsk, unsigned long *sp)
 #else
 #define S_PREEMPT ""
 #endif
-#ifdef CONFIG_SMP
 #define S_SMP " SMP"
-#else
-#define S_SMP ""
-#endif
 
 static int __die(const char *str, int err, struct thread_info *thread,
 		 struct pt_regs *regs)

commit e147ae6d7f908412a013c115e42c3e15dac33ccc
Author: Rohit Thapliyal <r.thapliyal@samsung.com>
Date:   Fri Jul 10 09:23:59 2015 +0100

    arm64: modify the dump mem for 64 bit addresses
    
    On 64bit kernel, the dump_mem gives 32 bit addresses
    on the stack dump. This gives unorganized information regarding
    the 64bit values on the stack. Hence, modified to get a complete 64bit memory
    dump.
    
    With patch:
    [   93.534801] Process insmod (pid: 1587, stack limit = 0xffffffc976be4058)
    [   93.541441] Stack: (0xffffffc976be7cf0 to 0xffffffc976be8000)
    [   93.547136] 7ce0:                                   ffffffc976be7d00 ffffffc00008163c
    [   93.554898] 7d00: ffffffc976be7d40 ffffffc0000f8a44 ffffffc00098ef38 ffffffbffc000088
    [   93.562659] 7d20: ffffffc00098ef50 ffffffbffc0000c0 0000000000000001 ffffffbffc000070
    [   93.570419] 7d40: ffffffc976be7e40 ffffffc0000f935c 0000000000000000 000000002b424090
    [   93.578179] 7d60: 000000002b424010 0000007facc555f4 0000000080000000 0000000000000015
    [   93.585937] 7d80: 0000000000000116 0000000000000069 ffffffc00097b000 ffffffc976be4000
    [   93.593694] 7da0: 0000000000000064 0000000000000072 000000000000006e 000000000000003f
    [   93.601453] 7dc0: 000000000000feff 000000000000fff1 ffffffbffc002028 0000000000000124
    [   93.609211] 7de0: ffffffc976be7e10 0000000000000001 ffffff8000000000 ffffffbbffff0000
    [   93.616969] 7e00: ffffffc976be7e60 0000000000000000 0000000000000000 0000000000000000
    [   93.624726] 7e20: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [   93.632484] 7e40: 0000007fcc474550 ffffffc0000841ec 000000002b424010 0000007facda0710
    [   93.640241] 7e60: ffffffffffffffff ffffffc0000be6dc ffffff80007d2000 000000000001c010
    [   93.647999] 7e80: ffffff80007e0ae0 ffffff80007e09d0 ffffff80007edf70 0000000000000288
    [   93.655757] 7ea0: 00000000000002e8 0000000000000000 0000000000000000 0000001c0000001b
    [   93.663514] 7ec0: 0000000000000009 0000000000000007 000000002b424090 000000000001c010
    [   93.671272] 7ee0: 000000002b424010 0000007faccd3a48 0000000000000000 0000000000000000
    [   93.679030] 7f00: 0000007fcc4743f8 0000007fcc4743f8 0000000000000069 0000000000000003
    [   93.686787] 7f20: 0101010101010101 0000000000000004 0000000000000020 00000000000003f3
    [   93.694544] 7f40: 0000007facb95664 0000007facda7030 0000007facc555d0 0000000000498378
    [   93.702301] 7f60: 0000000000000000 000000002b424010 0000007facda0710 000000002b424090
    [   93.710058] 7f80: 0000007fcc474698 0000000000498000 0000007fcc474ebb 0000000000474f58
    [   93.717815] 7fa0: 0000000000498000 0000000000000000 0000000000000000 0000007fcc474550
    [   93.725573] 7fc0: 00000000004104bc 0000007fcc474430 0000007facc555f4 0000000080000000
    [   93.733330] 7fe0: 000000002b424090 0000000000000069 0950020128000244 4104000008000004
    [   93.741084] Call trace:
    
    The above output makes a debugger life a lot more easier.
    
    Signed-off-by: Rohit Thapliyal <r.thapliyal@samsung.com>
    Signed-off-by: Maninder Singh <maninder1.s@samsung.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 566bc4c35040..4db6a2574fec 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -52,11 +52,12 @@ int show_unhandled_signals = 1;
  * Dump out the contents of some memory nicely...
  */
 static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
-		     unsigned long top)
+		     unsigned long top, bool compat)
 {
 	unsigned long first;
 	mm_segment_t fs;
 	int i;
+	unsigned int width = compat ? 4 : 8;
 
 	/*
 	 * We need to switch to kernel mode so that we can use __get_user
@@ -75,13 +76,22 @@ static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
 		memset(str, ' ', sizeof(str));
 		str[sizeof(str) - 1] = '\0';
 
-		for (p = first, i = 0; i < 8 && p < top; i++, p += 4) {
+		for (p = first, i = 0; i < (32 / width)
+					&& p < top; i++, p += width) {
 			if (p >= bottom && p < top) {
-				unsigned int val;
-				if (__get_user(val, (unsigned int *)p) == 0)
-					sprintf(str + i * 9, " %08x", val);
-				else
-					sprintf(str + i * 9, " ????????");
+				unsigned long val;
+
+				if (width == 8) {
+					if (__get_user(val, (unsigned long *)p) == 0)
+						sprintf(str + i * 17, " %016lx", val);
+					else
+						sprintf(str + i * 17, " ????????????????");
+				} else {
+					if (__get_user(val, (unsigned int *)p) == 0)
+						sprintf(str + i * 9, " %08lx", val);
+					else
+						sprintf(str + i * 9, " ????????");
+				}
 			}
 		}
 		printk("%s%04lx:%s\n", lvl, first & 0xffff, str);
@@ -95,7 +105,7 @@ static void dump_backtrace_entry(unsigned long where, unsigned long stack)
 	print_ip_sym(where);
 	if (in_exception_text(where))
 		dump_mem("", "Exception stack", stack,
-			 stack + sizeof(struct pt_regs));
+			 stack + sizeof(struct pt_regs), false);
 }
 
 static void dump_instr(const char *lvl, struct pt_regs *regs)
@@ -207,7 +217,8 @@ static int __die(const char *str, int err, struct thread_info *thread,
 
 	if (!user_mode(regs) || in_interrupt()) {
 		dump_mem(KERN_EMERG, "Stack: ", regs->sp,
-			 THREAD_SIZE + (unsigned long)task_stack_page(tsk));
+			 THREAD_SIZE + (unsigned long)task_stack_page(tsk),
+			 compat_user_mode(regs));
 		dump_backtrace(regs, tsk);
 		dump_instr(KERN_EMERG, regs);
 	}

commit f871d26807078cf4cc0a64a97ee2c6bb513a4397
Author: Suzuki K. Poulose <suzuki.poulose@arm.com>
Date:   Fri Jul 3 15:08:08 2015 +0100

    arm64: Fix show_unhandled_signal_ratelimited usage
    
    Commit 86dca36e6ba introduced ratelimited usage for
    'unhandled_signal' messages.
    The commit checks the ratelimit irrespective of whether
    the signal is handled or not, which is wrong and leads
    to false reports like the below in dmesg :
    
    __do_user_fault: 127 callbacks suppressed
    
    Do the ratelimit check only if the signal is unhandled.
    
    Fixes: 86dca36e6ba0 ("arm64: use private ratelimit state along with show_unhandled_signals")
    Cc: Vladimir Murzin <Vladimir.Murzin@arm.com>
    Signed-off-by: Suzuki K. Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index a12251c074a8..566bc4c35040 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -335,7 +335,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	if (call_undef_hook(regs) == 0)
 		return;
 
-	if (show_unhandled_signals_ratelimited() && unhandled_signal(current, SIGILL)) {
+	if (unhandled_signal(current, SIGILL) && show_unhandled_signals_ratelimited()) {
 		pr_info("%s[%d]: undefined instruction: pc=%p\n",
 			current->comm, task_pid_nr(current), pc);
 		dump_instr(KERN_INFO, regs);

commit 86dca36e6ba019650a94cadf922ea3d06dec0182
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Fri Jun 19 15:28:03 2015 +0100

    arm64: use private ratelimit state along with show_unhandled_signals
    
    printk_ratelimit() shares the ratelimiting state with other callers what
    may lead to scenarios where at the time we want to print out debug
    information we already limited, so nothing appears in the dmesg - this
    makes exception-trace quite poor helper in debugging.
    
    Additionally, we have imbalance with some messages limited with global
    ratelimit state and other messages limited with their private state
    defined via pr_*_ratelimited().
    
    To address this inconsistency show_unhandled_signals_ratelimited()
    macro is introduced and caller sites are converted to use it.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 1ef2940df13c..a12251c074a8 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -335,8 +335,7 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	if (call_undef_hook(regs) == 0)
 		return;
 
-	if (show_unhandled_signals && unhandled_signal(current, SIGILL) &&
-	    printk_ratelimit()) {
+	if (show_unhandled_signals_ratelimited() && unhandled_signal(current, SIGILL)) {
 		pr_info("%s[%d]: undefined instruction: pc=%p\n",
 			current->comm, task_pid_nr(current), pc);
 		dump_instr(KERN_INFO, regs);
@@ -363,7 +362,7 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
 	}
 #endif
 
-	if (show_unhandled_signals && printk_ratelimit()) {
+	if (show_unhandled_signals_ratelimited()) {
 		pr_info("%s[%d]: syscall %d\n", current->comm,
 			task_pid_nr(current), (int)regs->syscallno);
 		dump_instr("", regs);

commit 60a1f02c9e91e0796b54e83b14fb8a07f7a568b6
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Nov 18 12:16:30 2014 +0000

    arm64: decode ESR_ELx.EC when reporting exceptions
    
    To aid the developer when something triggers an unexpected exception,
    decode the ESR_ELx.EC field when logging an ESR_ELx value. This doesn't
    tell the developer the specifics of the exception encoded in the
    remaining IL and ISS bits, but it can be helpful to distinguish between
    exception classes (e.g. SError and a data abort) without having to
    manually decode the field, which can be tiresome.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 0a801e3743d5..1ef2940df13c 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -33,6 +33,7 @@
 
 #include <asm/atomic.h>
 #include <asm/debug-monitors.h>
+#include <asm/esr.h>
 #include <asm/traps.h>
 #include <asm/stacktrace.h>
 #include <asm/exception.h>
@@ -373,6 +374,51 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
 	return sys_ni_syscall();
 }
 
+static const char *esr_class_str[] = {
+	[0 ... ESR_ELx_EC_MAX]		= "UNRECOGNIZED EC",
+	[ESR_ELx_EC_UNKNOWN]		= "Unknown/Uncategorized",
+	[ESR_ELx_EC_WFx]		= "WFI/WFE",
+	[ESR_ELx_EC_CP15_32]		= "CP15 MCR/MRC",
+	[ESR_ELx_EC_CP15_64]		= "CP15 MCRR/MRRC",
+	[ESR_ELx_EC_CP14_MR]		= "CP14 MCR/MRC",
+	[ESR_ELx_EC_CP14_LS]		= "CP14 LDC/STC",
+	[ESR_ELx_EC_FP_ASIMD]		= "ASIMD",
+	[ESR_ELx_EC_CP10_ID]		= "CP10 MRC/VMRS",
+	[ESR_ELx_EC_CP14_64]		= "CP14 MCRR/MRRC",
+	[ESR_ELx_EC_ILL]		= "PSTATE.IL",
+	[ESR_ELx_EC_SVC32]		= "SVC (AArch32)",
+	[ESR_ELx_EC_HVC32]		= "HVC (AArch32)",
+	[ESR_ELx_EC_SMC32]		= "SMC (AArch32)",
+	[ESR_ELx_EC_SVC64]		= "SVC (AArch64)",
+	[ESR_ELx_EC_HVC64]		= "HVC (AArch64)",
+	[ESR_ELx_EC_SMC64]		= "SMC (AArch64)",
+	[ESR_ELx_EC_SYS64]		= "MSR/MRS (AArch64)",
+	[ESR_ELx_EC_IMP_DEF]		= "EL3 IMP DEF",
+	[ESR_ELx_EC_IABT_LOW]		= "IABT (lower EL)",
+	[ESR_ELx_EC_IABT_CUR]		= "IABT (current EL)",
+	[ESR_ELx_EC_PC_ALIGN]		= "PC Alignment",
+	[ESR_ELx_EC_DABT_LOW]		= "DABT (lower EL)",
+	[ESR_ELx_EC_DABT_CUR]		= "DABT (current EL)",
+	[ESR_ELx_EC_SP_ALIGN]		= "SP Alignment",
+	[ESR_ELx_EC_FP_EXC32]		= "FP (AArch32)",
+	[ESR_ELx_EC_FP_EXC64]		= "FP (AArch64)",
+	[ESR_ELx_EC_SERROR]		= "SError",
+	[ESR_ELx_EC_BREAKPT_LOW]	= "Breakpoint (lower EL)",
+	[ESR_ELx_EC_BREAKPT_CUR]	= "Breakpoint (current EL)",
+	[ESR_ELx_EC_SOFTSTP_LOW]	= "Software Step (lower EL)",
+	[ESR_ELx_EC_SOFTSTP_CUR]	= "Software Step (current EL)",
+	[ESR_ELx_EC_WATCHPT_LOW]	= "Watchpoint (lower EL)",
+	[ESR_ELx_EC_WATCHPT_CUR]	= "Watchpoint (current EL)",
+	[ESR_ELx_EC_BKPT32]		= "BKPT (AArch32)",
+	[ESR_ELx_EC_VECTOR32]		= "Vector catch (AArch32)",
+	[ESR_ELx_EC_BRK64]		= "BRK (AArch64)",
+};
+
+const char *esr_get_class_string(u32 esr)
+{
+	return esr_class_str[esr >> ESR_ELx_EC_SHIFT];
+}
+
 /*
  * bad_mode handles the impossible case in the exception vector.
  */
@@ -382,8 +428,8 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 	void __user *pc = (void __user *)instruction_pointer(regs);
 	console_verbose();
 
-	pr_crit("Bad mode in %s handler detected, code 0x%08x\n",
-		handler[reason], esr);
+	pr_crit("Bad mode in %s handler detected, code 0x%08x -- %s\n",
+		handler[reason], esr, esr_get_class_string(esr));
 	__show_regs(regs);
 
 	info.si_signo = SIGILL;

commit 9b79f52d1a702dd5b160f9d2ee0199c3122809bb
Author: Punit Agrawal <punit.agrawal@arm.com>
Date:   Tue Nov 18 11:41:22 2014 +0000

    arm64: Add support for hooks to handle undefined instructions
    
    Add support to register hooks for undefined instructions. The handlers
    will be called when the undefined instruction and the processor state
    (as contained in pstate) match criteria used at registration.
    
    Signed-off-by: Punit Agrawal <punit.agrawal@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index de1b085e7963..0a801e3743d5 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -259,6 +259,69 @@ void arm64_notify_die(const char *str, struct pt_regs *regs,
 	}
 }
 
+static LIST_HEAD(undef_hook);
+static DEFINE_RAW_SPINLOCK(undef_lock);
+
+void register_undef_hook(struct undef_hook *hook)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&undef_lock, flags);
+	list_add(&hook->node, &undef_hook);
+	raw_spin_unlock_irqrestore(&undef_lock, flags);
+}
+
+void unregister_undef_hook(struct undef_hook *hook)
+{
+	unsigned long flags;
+
+	raw_spin_lock_irqsave(&undef_lock, flags);
+	list_del(&hook->node);
+	raw_spin_unlock_irqrestore(&undef_lock, flags);
+}
+
+static int call_undef_hook(struct pt_regs *regs)
+{
+	struct undef_hook *hook;
+	unsigned long flags;
+	u32 instr;
+	int (*fn)(struct pt_regs *regs, u32 instr) = NULL;
+	void __user *pc = (void __user *)instruction_pointer(regs);
+
+	if (!user_mode(regs))
+		return 1;
+
+	if (compat_thumb_mode(regs)) {
+		/* 16-bit Thumb instruction */
+		if (get_user(instr, (u16 __user *)pc))
+			goto exit;
+		instr = le16_to_cpu(instr);
+		if (aarch32_insn_is_wide(instr)) {
+			u32 instr2;
+
+			if (get_user(instr2, (u16 __user *)(pc + 2)))
+				goto exit;
+			instr2 = le16_to_cpu(instr2);
+			instr = (instr << 16) | instr2;
+		}
+	} else {
+		/* 32-bit ARM instruction */
+		if (get_user(instr, (u32 __user *)pc))
+			goto exit;
+		instr = le32_to_cpu(instr);
+	}
+
+	raw_spin_lock_irqsave(&undef_lock, flags);
+	list_for_each_entry(hook, &undef_hook, node)
+		if ((instr & hook->instr_mask) == hook->instr_val &&
+			(regs->pstate & hook->pstate_mask) == hook->pstate_val)
+			fn = hook->fn;
+
+	raw_spin_unlock_irqrestore(&undef_lock, flags);
+exit:
+	return fn ? fn(regs, instr) : 1;
+}
+
 asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 {
 	siginfo_t info;
@@ -268,6 +331,9 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	if (!aarch32_break_handler(regs))
 		return;
 
+	if (call_undef_hook(regs) == 0)
+		return;
+
 	if (show_unhandled_signals && unhandled_signal(current, SIGILL) &&
 	    printk_ratelimit()) {
 		pr_info("%s[%d]: undefined instruction: pc=%p\n",

commit 2128df143d840a20e12818290eb6e40b95cc4ac0
Author: Behan Webster <behanw@converseincode.com>
Date:   Wed Aug 27 05:29:32 2014 +0100

    arm64: LLVMLinux: Use current_stack_pointer in kernel/traps.c
    
    Use the global current_stack_pointer to get the value of the stack pointer.
    This change supports being able to compile the kernel with both gcc and clang.
    
    Signed-off-by: Behan Webster <behanw@converseincode.com>
    Signed-off-by: Mark Charlebois <charlebm@gmail.com>
    Reviewed-by: Olof Johansson <olof@lixom.net>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 02cd3f023e9a..de1b085e7963 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -132,7 +132,6 @@ static void dump_instr(const char *lvl, struct pt_regs *regs)
 static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 {
 	struct stackframe frame;
-	const register unsigned long current_sp asm ("sp");
 
 	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
 
@@ -145,7 +144,7 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		frame.pc = regs->pc;
 	} else if (tsk == current) {
 		frame.fp = (unsigned long)__builtin_frame_address(0);
-		frame.sp = current_sp;
+		frame.sp = current_stack_pointer;
 		frame.pc = (unsigned long)dump_backtrace;
 	} else {
 		/*

commit c79b954bf6c006f2d3dd9d01f231abeead13a410
Author: Jungseok Lee <jays.lee@samsung.com>
Date:   Mon May 12 18:40:51 2014 +0900

    arm64: mm: Implement 4 levels of translation tables
    
    This patch implements 4 levels of translation tables since 3 levels
    of page tables with 4KB pages cannot support 40-bit physical address
    space described in [1] due to the following issue.
    
    It is a restriction that kernel logical memory map with 4KB + 3 levels
    (0xffffffc000000000-0xffffffffffffffff) cannot cover RAM region from
    544GB to 1024GB in [1]. Specifically, ARM64 kernel fails to create
    mapping for this region in map_mem function since __phys_to_virt for
    this region reaches to address overflow.
    
    If SoC design follows the document, [1], over 32GB RAM would be placed
    from 544GB. Even 64GB system is supposed to use the region from 544GB
    to 576GB for only 32GB RAM. Naturally, it would reach to enable 4 levels
    of page tables to avoid hacking __virt_to_phys and __phys_to_virt.
    
    However, it is recommended 4 levels of page table should be only enabled
    if memory map is too sparse or there is about 512GB RAM.
    
    References
    ----------
    [1]: Principles of ARM Memory Maps, White Paper, Issue C
    
    Signed-off-by: Jungseok Lee <jays.lee@samsung.com>
    Reviewed-by: Sungjinn Chung <sungjinn.chung@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Steve Capper <steve.capper@linaro.org>
    [catalin.marinas@arm.com: MEMBLOCK_INITIAL_LIMIT removed, same as PUD_SIZE]
    [catalin.marinas@arm.com: early_ioremap_init() updated for 4 levels]
    [catalin.marinas@arm.com: 48-bit VA depends on BROKEN until KVM is fixed]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Jungseok Lee <jungseoklee85@gmail.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 506f7814e305..02cd3f023e9a 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -339,6 +339,11 @@ void __pmd_error(const char *file, int line, unsigned long val)
 	pr_crit("%s:%d: bad pmd %016lx.\n", file, line, val);
 }
 
+void __pud_error(const char *file, int line, unsigned long val)
+{
+	pr_crit("%s:%d: bad pud %016lx.\n", file, line, val);
+}
+
 void __pgd_error(const char *file, int line, unsigned long val)
 {
 	pr_crit("%s:%d: bad pgd %016lx.\n", file, line, val);

commit ac7b406c1a9d50ddbf5e5cbce8ca4d68d36ac2db
Author: Jungseok Lee <jays.lee@samsung.com>
Date:   Mon May 12 10:40:30 2014 +0100

    arm64: Use pr_* instead of printk
    
    This patch fixed the following checkpatch complaint as using pr_*
    instead of printk.
    
    WARNING: printk() should include KERN_ facility level
    
    Signed-off-by: Jungseok Lee <jays.lee@samsung.com>
    Reviewed-by: Sungjinn Chung <sungjinn.chung@samsung.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c43cfa9b8304..506f7814e305 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -156,7 +156,7 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 		frame.pc = thread_saved_pc(tsk);
 	}
 
-	printk("Call trace:\n");
+	pr_emerg("Call trace:\n");
 	while (1) {
 		unsigned long where = frame.pc;
 		int ret;
@@ -331,17 +331,17 @@ asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 
 void __pte_error(const char *file, int line, unsigned long val)
 {
-	printk("%s:%d: bad pte %016lx.\n", file, line, val);
+	pr_crit("%s:%d: bad pte %016lx.\n", file, line, val);
 }
 
 void __pmd_error(const char *file, int line, unsigned long val)
 {
-	printk("%s:%d: bad pmd %016lx.\n", file, line, val);
+	pr_crit("%s:%d: bad pmd %016lx.\n", file, line, val);
 }
 
 void __pgd_error(const char *file, int line, unsigned long val)
 {
-	printk("%s:%d: bad pgd %016lx.\n", file, line, val);
+	pr_crit("%s:%d: bad pgd %016lx.\n", file, line, val);
 }
 
 void __init trap_init(void)

commit 9141300a5884b57cea6d32c4e3fd16a337cfc99a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sun Apr 6 23:04:12 2014 +0100

    arm64: Provide read/write fault information in compat signal handlers
    
    For AArch32, bit 11 (WnR) of the FSR/ESR register is set when the fault
    was caused by a write access and applications like Qemu rely on such
    information being provided in sigcontext. This patch introduces the
    ESR_EL1 tracking for the arm64 kernel faults and sets bit 11 accordingly
    in compat sigcontext.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 7ffadddb645d..c43cfa9b8304 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -251,10 +251,13 @@ void die(const char *str, struct pt_regs *regs, int err)
 void arm64_notify_die(const char *str, struct pt_regs *regs,
 		      struct siginfo *info, int err)
 {
-	if (user_mode(regs))
+	if (user_mode(regs)) {
+		current->thread.fault_address = 0;
+		current->thread.fault_code = err;
 		force_sig_info(info->si_signo, info, current);
-	else
+	} else {
 		die(str, regs, err);
+	}
 }
 
 asmlinkage void __exception do_undefinstr(struct pt_regs *regs)

commit 1442b6ed249d2b3d2cfcf45b65ac64393495c96c
Author: Will Deacon <will.deacon@arm.com>
Date:   Sat Mar 16 08:48:13 2013 +0000

    arm64: debug: consolidate software breakpoint handlers
    
    The software breakpoint handlers are hooked in directly from ptrace,
    which makes it difficult to add additional handlers for things like
    kprobes and kgdb.
    
    This patch moves the handling code into debug-monitors.c, where we can
    dispatch to different debug subsystems more easily.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index f30852d28590..7ffadddb645d 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -32,6 +32,7 @@
 #include <linux/syscalls.h>
 
 #include <asm/atomic.h>
+#include <asm/debug-monitors.h>
 #include <asm/traps.h>
 #include <asm/stacktrace.h>
 #include <asm/exception.h>
@@ -261,11 +262,9 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 	siginfo_t info;
 	void __user *pc = (void __user *)instruction_pointer(regs);
 
-#ifdef CONFIG_COMPAT
 	/* check for AArch32 breakpoint instructions */
-	if (compat_user_mode(regs) && aarch32_break_trap(regs) == 0)
+	if (!aarch32_break_handler(regs))
 		return;
-#endif
 
 	if (show_unhandled_signals && unhandled_signal(current, SIGILL) &&
 	    printk_ratelimit()) {

commit 9955ac47f4ba1c95ecb6092aeaefb40a22e99268
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue May 28 15:54:15 2013 +0100

    arm64: don't kill the kernel on a bad esr from el0
    
    Rather than completely killing the kernel if we receive an esr value we
    can't deal with in the el0 handlers, send the process a SIGILL and log
    the esr value in the hope that we can debug it. If we receive a bad esr
    from el1, we'll die() as before.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: stable@vger.kernel.org

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index f1ff9bad00f7..f30852d28590 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -311,14 +311,20 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
  */
 asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
 {
+	siginfo_t info;
+	void __user *pc = (void __user *)instruction_pointer(regs);
 	console_verbose();
 
 	pr_crit("Bad mode in %s handler detected, code 0x%08x\n",
 		handler[reason], esr);
+	__show_regs(regs);
+
+	info.si_signo = SIGILL;
+	info.si_errno = 0;
+	info.si_code  = ILL_ILLOPC;
+	info.si_addr  = pc;
 
-	die("Oops - bad mode", regs, 0);
-	local_irq_disable();
-	panic("bad mode");
+	arm64_notify_die("Oops - bad mode", regs, &info, 0);
 }
 
 void __pte_error(const char *file, int line, unsigned long val)

commit 953dbbed9ee310100bc841cdea8f992d192531c6
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue May 21 12:16:56 2013 +0100

    arm64: Do not report user faults for handled signals
    
    Currently user faults (page, undefined instruction) are always reported
    even though the user may have a signal handler for them. This patch adds
    unhandled_signal() check together with printk_ratelimit() for these
    cases.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 61d7dd29f756..f1ff9bad00f7 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -267,7 +267,8 @@ asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
 		return;
 #endif
 
-	if (show_unhandled_signals) {
+	if (show_unhandled_signals && unhandled_signal(current, SIGILL) &&
+	    printk_ratelimit()) {
 		pr_info("%s[%d]: undefined instruction: pc=%p\n",
 			current->comm, task_pid_nr(current), pc);
 		dump_instr(KERN_INFO, regs);
@@ -294,7 +295,7 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
 	}
 #endif
 
-	if (show_unhandled_signals) {
+	if (show_unhandled_signals && printk_ratelimit()) {
 		pr_info("%s[%d]: syscall %d\n", current->comm,
 			task_pid_nr(current), (int)regs->syscallno);
 		dump_instr("", regs);

commit 196779b9b4ce1922afabdc20d0270720603bd46c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:12 2013 -0700

    dump_stack: consolidate dump_stack() implementations and unify their behaviors
    
    Both dump_stack() and show_stack() are currently implemented by each
    architecture.  show_stack(NULL, NULL) dumps the backtrace for the
    current task as does dump_stack().  On some archs, dump_stack() prints
    extra information - pid, utsname and so on - in addition to the
    backtrace while the two are identical on other archs.
    
    The usages in arch-independent code of the two functions indicate
    show_stack(NULL, NULL) should print out bare backtrace while
    dump_stack() is used for debugging purposes when something went wrong,
    so it does make sense to print additional information on the task which
    triggered dump_stack().
    
    There's no reason to require archs to implement two separate but mostly
    identical functions.  It leads to unnecessary subtle information.
    
    This patch expands the dummy fallback dump_stack() implementation in
    lib/dump_stack.c such that it prints out debug information (taken from
    x86) and invokes show_stack(NULL, NULL) and drops arch-specific
    dump_stack() implementations in all archs except blackfin.  Blackfin's
    dump_stack() does something wonky that I don't understand.
    
    Debug information can be printed separately by calling
    dump_stack_print_info() so that arch-specific dump_stack()
    implementation can still emit the same debug information.  This is used
    in blackfin.
    
    This patch brings the following behavior changes.
    
    * On some archs, an extra level in backtrace for show_stack() could be
      printed.  This is because the top frame was determined in
      dump_stack() on those archs while generic dump_stack() can't do that
      reliably.  It can be compensated by inlining dump_stack() but not
      sure whether that'd be necessary.
    
    * Most archs didn't use to print debug info on dump_stack().  They do
      now.
    
    An example WARN dump follows.
    
     WARNING: at kernel/workqueue.c:4841 init_workqueues+0x35/0x505()
     Hardware name: empty
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.9.0-rc1-work+ #9
      0000000000000009 ffff88007c861e08 ffffffff81c614dc ffff88007c861e48
      ffffffff8108f50f ffffffff82228240 0000000000000040 ffffffff8234a03c
      0000000000000000 0000000000000000 0000000000000000 ffff88007c861e58
     Call Trace:
      [<ffffffff81c614dc>] dump_stack+0x19/0x1b
      [<ffffffff8108f50f>] warn_slowpath_common+0x7f/0xc0
      [<ffffffff8108f56a>] warn_slowpath_null+0x1a/0x20
      [<ffffffff8234a071>] init_workqueues+0x35/0x505
      ...
    
    v2: CPU number added to the generic debug info as requested by s390
        folks and dropped the s390 specific dump_stack().  This loses %ksp
        from the debug message which the maintainers think isn't important
        enough to keep the s390-specific dump_stack() implementation.
    
        dump_stack_print_info() is moved to kernel/printk.c from
        lib/dump_stack.c.  Because linkage is per objecct file,
        dump_stack_print_info() living in the same lib file as generic
        dump_stack() means that archs which implement custom dump_stack()
        - at this point, only blackfin - can't use dump_stack_print_info()
        as that will bring in the generic version of dump_stack() too.  v1
        The v1 patch broke build on blackfin due to this issue.  The build
        breakage was reported by Fengguang Wu.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>   [s390 bits]
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Richard Kuo <rkuo@codeaurora.org>             [hexagon bits]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index b3c5f628bdb4..61d7dd29f756 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -167,13 +167,6 @@ static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
 	}
 }
 
-void dump_stack(void)
-{
-	dump_backtrace(NULL, NULL);
-}
-
-EXPORT_SYMBOL(dump_stack);
-
 void show_stack(struct task_struct *tsk, unsigned long *sp)
 {
 	dump_backtrace(NULL, tsk);

commit 373d4d099761cb1f637bed488ab3871945882273
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Mon Jan 21 17:17:39 2013 +1030

    taint: add explicit flag to show whether lock dep is still OK.
    
    Fix up all callers as they were before, with make one change: an
    unsigned module taints the kernel, but doesn't turn off lockdep.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index 3883f842434f..b3c5f628bdb4 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -242,7 +242,7 @@ void die(const char *str, struct pt_regs *regs, int err)
 		crash_kexec(regs);
 
 	bust_spinlocks(0);
-	add_taint(TAINT_DIE);
+	add_taint(TAINT_DIE, LOCKDEP_NOW_UNRELIABLE);
 	raw_spin_unlock_irq(&die_lock);
 	oops_exit();
 

commit 60ffc30d5652810dd34ea2eec41504222f5d5791
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Mar 5 11:49:27 2012 +0000

    arm64: Exception handling
    
    The patch contains the exception entry code (kernel/entry.S), pt_regs
    structure and related accessors, undefined instruction trapping and
    stack tracing.
    
    AArch64 Linux kernel (including kernel threads) runs in EL1 mode using
    the SP1 stack. The vectors don't have a fixed address, only alignment
    (2^11) requirements.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
new file mode 100644
index 000000000000..3883f842434f
--- /dev/null
+++ b/arch/arm64/kernel/traps.c
@@ -0,0 +1,348 @@
+/*
+ * Based on arch/arm/kernel/traps.c
+ *
+ * Copyright (C) 1995-2009 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/signal.h>
+#include <linux/personality.h>
+#include <linux/kallsyms.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/hardirq.h>
+#include <linux/kdebug.h>
+#include <linux/module.h>
+#include <linux/kexec.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/syscalls.h>
+
+#include <asm/atomic.h>
+#include <asm/traps.h>
+#include <asm/stacktrace.h>
+#include <asm/exception.h>
+#include <asm/system_misc.h>
+
+static const char *handler[]= {
+	"Synchronous Abort",
+	"IRQ",
+	"FIQ",
+	"Error"
+};
+
+int show_unhandled_signals = 1;
+
+/*
+ * Dump out the contents of some memory nicely...
+ */
+static void dump_mem(const char *lvl, const char *str, unsigned long bottom,
+		     unsigned long top)
+{
+	unsigned long first;
+	mm_segment_t fs;
+	int i;
+
+	/*
+	 * We need to switch to kernel mode so that we can use __get_user
+	 * to safely read from kernel space.  Note that we now dump the
+	 * code first, just in case the backtrace kills us.
+	 */
+	fs = get_fs();
+	set_fs(KERNEL_DS);
+
+	printk("%s%s(0x%016lx to 0x%016lx)\n", lvl, str, bottom, top);
+
+	for (first = bottom & ~31; first < top; first += 32) {
+		unsigned long p;
+		char str[sizeof(" 12345678") * 8 + 1];
+
+		memset(str, ' ', sizeof(str));
+		str[sizeof(str) - 1] = '\0';
+
+		for (p = first, i = 0; i < 8 && p < top; i++, p += 4) {
+			if (p >= bottom && p < top) {
+				unsigned int val;
+				if (__get_user(val, (unsigned int *)p) == 0)
+					sprintf(str + i * 9, " %08x", val);
+				else
+					sprintf(str + i * 9, " ????????");
+			}
+		}
+		printk("%s%04lx:%s\n", lvl, first & 0xffff, str);
+	}
+
+	set_fs(fs);
+}
+
+static void dump_backtrace_entry(unsigned long where, unsigned long stack)
+{
+	print_ip_sym(where);
+	if (in_exception_text(where))
+		dump_mem("", "Exception stack", stack,
+			 stack + sizeof(struct pt_regs));
+}
+
+static void dump_instr(const char *lvl, struct pt_regs *regs)
+{
+	unsigned long addr = instruction_pointer(regs);
+	mm_segment_t fs;
+	char str[sizeof("00000000 ") * 5 + 2 + 1], *p = str;
+	int i;
+
+	/*
+	 * We need to switch to kernel mode so that we can use __get_user
+	 * to safely read from kernel space.  Note that we now dump the
+	 * code first, just in case the backtrace kills us.
+	 */
+	fs = get_fs();
+	set_fs(KERNEL_DS);
+
+	for (i = -4; i < 1; i++) {
+		unsigned int val, bad;
+
+		bad = __get_user(val, &((u32 *)addr)[i]);
+
+		if (!bad)
+			p += sprintf(p, i == 0 ? "(%08x) " : "%08x ", val);
+		else {
+			p += sprintf(p, "bad PC value");
+			break;
+		}
+	}
+	printk("%sCode: %s\n", lvl, str);
+
+	set_fs(fs);
+}
+
+static void dump_backtrace(struct pt_regs *regs, struct task_struct *tsk)
+{
+	struct stackframe frame;
+	const register unsigned long current_sp asm ("sp");
+
+	pr_debug("%s(regs = %p tsk = %p)\n", __func__, regs, tsk);
+
+	if (!tsk)
+		tsk = current;
+
+	if (regs) {
+		frame.fp = regs->regs[29];
+		frame.sp = regs->sp;
+		frame.pc = regs->pc;
+	} else if (tsk == current) {
+		frame.fp = (unsigned long)__builtin_frame_address(0);
+		frame.sp = current_sp;
+		frame.pc = (unsigned long)dump_backtrace;
+	} else {
+		/*
+		 * task blocked in __switch_to
+		 */
+		frame.fp = thread_saved_fp(tsk);
+		frame.sp = thread_saved_sp(tsk);
+		frame.pc = thread_saved_pc(tsk);
+	}
+
+	printk("Call trace:\n");
+	while (1) {
+		unsigned long where = frame.pc;
+		int ret;
+
+		ret = unwind_frame(&frame);
+		if (ret < 0)
+			break;
+		dump_backtrace_entry(where, frame.sp);
+	}
+}
+
+void dump_stack(void)
+{
+	dump_backtrace(NULL, NULL);
+}
+
+EXPORT_SYMBOL(dump_stack);
+
+void show_stack(struct task_struct *tsk, unsigned long *sp)
+{
+	dump_backtrace(NULL, tsk);
+	barrier();
+}
+
+#ifdef CONFIG_PREEMPT
+#define S_PREEMPT " PREEMPT"
+#else
+#define S_PREEMPT ""
+#endif
+#ifdef CONFIG_SMP
+#define S_SMP " SMP"
+#else
+#define S_SMP ""
+#endif
+
+static int __die(const char *str, int err, struct thread_info *thread,
+		 struct pt_regs *regs)
+{
+	struct task_struct *tsk = thread->task;
+	static int die_counter;
+	int ret;
+
+	pr_emerg("Internal error: %s: %x [#%d]" S_PREEMPT S_SMP "\n",
+		 str, err, ++die_counter);
+
+	/* trap and error numbers are mostly meaningless on ARM */
+	ret = notify_die(DIE_OOPS, str, regs, err, 0, SIGSEGV);
+	if (ret == NOTIFY_STOP)
+		return ret;
+
+	print_modules();
+	__show_regs(regs);
+	pr_emerg("Process %.*s (pid: %d, stack limit = 0x%p)\n",
+		 TASK_COMM_LEN, tsk->comm, task_pid_nr(tsk), thread + 1);
+
+	if (!user_mode(regs) || in_interrupt()) {
+		dump_mem(KERN_EMERG, "Stack: ", regs->sp,
+			 THREAD_SIZE + (unsigned long)task_stack_page(tsk));
+		dump_backtrace(regs, tsk);
+		dump_instr(KERN_EMERG, regs);
+	}
+
+	return ret;
+}
+
+static DEFINE_RAW_SPINLOCK(die_lock);
+
+/*
+ * This function is protected against re-entrancy.
+ */
+void die(const char *str, struct pt_regs *regs, int err)
+{
+	struct thread_info *thread = current_thread_info();
+	int ret;
+
+	oops_enter();
+
+	raw_spin_lock_irq(&die_lock);
+	console_verbose();
+	bust_spinlocks(1);
+	ret = __die(str, err, thread, regs);
+
+	if (regs && kexec_should_crash(thread->task))
+		crash_kexec(regs);
+
+	bust_spinlocks(0);
+	add_taint(TAINT_DIE);
+	raw_spin_unlock_irq(&die_lock);
+	oops_exit();
+
+	if (in_interrupt())
+		panic("Fatal exception in interrupt");
+	if (panic_on_oops)
+		panic("Fatal exception");
+	if (ret != NOTIFY_STOP)
+		do_exit(SIGSEGV);
+}
+
+void arm64_notify_die(const char *str, struct pt_regs *regs,
+		      struct siginfo *info, int err)
+{
+	if (user_mode(regs))
+		force_sig_info(info->si_signo, info, current);
+	else
+		die(str, regs, err);
+}
+
+asmlinkage void __exception do_undefinstr(struct pt_regs *regs)
+{
+	siginfo_t info;
+	void __user *pc = (void __user *)instruction_pointer(regs);
+
+#ifdef CONFIG_COMPAT
+	/* check for AArch32 breakpoint instructions */
+	if (compat_user_mode(regs) && aarch32_break_trap(regs) == 0)
+		return;
+#endif
+
+	if (show_unhandled_signals) {
+		pr_info("%s[%d]: undefined instruction: pc=%p\n",
+			current->comm, task_pid_nr(current), pc);
+		dump_instr(KERN_INFO, regs);
+	}
+
+	info.si_signo = SIGILL;
+	info.si_errno = 0;
+	info.si_code  = ILL_ILLOPC;
+	info.si_addr  = pc;
+
+	arm64_notify_die("Oops - undefined instruction", regs, &info, 0);
+}
+
+long compat_arm_syscall(struct pt_regs *regs);
+
+asmlinkage long do_ni_syscall(struct pt_regs *regs)
+{
+#ifdef CONFIG_COMPAT
+	long ret;
+	if (is_compat_task()) {
+		ret = compat_arm_syscall(regs);
+		if (ret != -ENOSYS)
+			return ret;
+	}
+#endif
+
+	if (show_unhandled_signals) {
+		pr_info("%s[%d]: syscall %d\n", current->comm,
+			task_pid_nr(current), (int)regs->syscallno);
+		dump_instr("", regs);
+		if (user_mode(regs))
+			__show_regs(regs);
+	}
+
+	return sys_ni_syscall();
+}
+
+/*
+ * bad_mode handles the impossible case in the exception vector.
+ */
+asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
+{
+	console_verbose();
+
+	pr_crit("Bad mode in %s handler detected, code 0x%08x\n",
+		handler[reason], esr);
+
+	die("Oops - bad mode", regs, 0);
+	local_irq_disable();
+	panic("bad mode");
+}
+
+void __pte_error(const char *file, int line, unsigned long val)
+{
+	printk("%s:%d: bad pte %016lx.\n", file, line, val);
+}
+
+void __pmd_error(const char *file, int line, unsigned long val)
+{
+	printk("%s:%d: bad pmd %016lx.\n", file, line, val);
+}
+
+void __pgd_error(const char *file, int line, unsigned long val)
+{
+	printk("%s:%d: bad pgd %016lx.\n", file, line, val);
+}
+
+void __init trap_init(void)
+{
+	return;
+}
