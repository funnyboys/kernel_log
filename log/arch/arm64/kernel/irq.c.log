commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit e1d22385ea6686ff3dcd7092d84465c193849829
Author: Wei Li <liwei391@huawei.com>
Date:   Tue Jun 11 10:38:12 2019 +0100

    arm64: fix kernel stack overflow in kdump capture kernel
    
    When enabling ARM64_PSEUDO_NMI feature in kdump capture kernel, it will
    report a kernel stack overflow exception:
    
    [    0.000000] CPU features: detected: IRQ priority masking
    [    0.000000] alternatives: patching kernel code
    [    0.000000] Insufficient stack space to handle exception!
    [    0.000000] ESR: 0x96000044 -- DABT (current EL)
    [    0.000000] FAR: 0x0000000000000040
    [    0.000000] Task stack:     [0xffff0000097f0000..0xffff0000097f4000]
    [    0.000000] IRQ stack:      [0x0000000000000000..0x0000000000004000]
    [    0.000000] Overflow stack: [0xffff80002b7cf290..0xffff80002b7d0290]
    [    0.000000] CPU: 0 PID: 0 Comm: swapper Not tainted 4.19.34-lw+ #3
    [    0.000000] pstate: 400003c5 (nZcv DAIF -PAN -UAO)
    [    0.000000] pc : el1_sync+0x0/0xb8
    [    0.000000] lr : el1_irq+0xb8/0x140
    [    0.000000] sp : 0000000000000040
    [    0.000000] pmr_save: 00000070
    [    0.000000] x29: ffff0000097f3f60 x28: ffff000009806240
    [    0.000000] x27: 0000000080000000 x26: 0000000000004000
    [    0.000000] x25: 0000000000000000 x24: ffff000009329028
    [    0.000000] x23: 0000000040000005 x22: ffff000008095c6c
    [    0.000000] x21: ffff0000097f3f70 x20: 0000000000000070
    [    0.000000] x19: ffff0000097f3e30 x18: ffffffffffffffff
    [    0.000000] x17: 0000000000000000 x16: 0000000000000000
    [    0.000000] x15: ffff0000097f9708 x14: ffff000089a382ef
    [    0.000000] x13: ffff000009a382fd x12: ffff000009824000
    [    0.000000] x11: ffff0000097fb7b0 x10: ffff000008730028
    [    0.000000] x9 : ffff000009440018 x8 : 000000000000000d
    [    0.000000] x7 : 6b20676e69686374 x6 : 000000000000003b
    [    0.000000] x5 : 0000000000000000 x4 : ffff000008093600
    [    0.000000] x3 : 0000000400000008 x2 : 7db2e689fc2b8e00
    [    0.000000] x1 : 0000000000000000 x0 : ffff0000097f3e30
    [    0.000000] Kernel panic - not syncing: kernel stack overflow
    [    0.000000] CPU: 0 PID: 0 Comm: swapper Not tainted 4.19.34-lw+ #3
    [    0.000000] Call trace:
    [    0.000000]  dump_backtrace+0x0/0x1b8
    [    0.000000]  show_stack+0x24/0x30
    [    0.000000]  dump_stack+0xa8/0xcc
    [    0.000000]  panic+0x134/0x30c
    [    0.000000]  __stack_chk_fail+0x0/0x28
    [    0.000000]  handle_bad_stack+0xfc/0x108
    [    0.000000]  __bad_stack+0x90/0x94
    [    0.000000]  el1_sync+0x0/0xb8
    [    0.000000]  init_gic_priority_masking+0x4c/0x70
    [    0.000000]  smp_prepare_boot_cpu+0x60/0x68
    [    0.000000]  start_kernel+0x1e8/0x53c
    [    0.000000] ---[ end Kernel panic - not syncing: kernel stack overflow ]---
    
    The reason is init_gic_priority_masking() may unmask PSR.I while the
    irq stacks are not inited yet. Some "NMI" could be raised unfortunately
    and it will just go into this exception.
    
    In this patch, we just write the PMR in smp_prepare_boot_cpu(), and delay
    unmasking PSR.I after irq stacks inited in init_IRQ().
    
    Fixes: e79321883842 ("arm64: Switch to PMR masking when starting CPUs")
    Cc: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Wei Li <liwei391@huawei.com>
    [JT: make init_gic_priority_masking() not modify daif, rebase on other
         priority masking fixes]
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index fdd9cb27fed5..e8daa7aa77bc 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -77,6 +77,15 @@ void __init init_IRQ(void)
 	irqchip_init();
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
+
+	if (system_uses_irq_prio_masking()) {
+		/*
+		 * Now that we have a stack for our IRQ handler, set
+		 * the PMR/PSR pair to a consistent state.
+		 */
+		WARN_ON(read_sysreg(daif) & PSR_A_BIT);
+		local_daif_restore(DAIF_PROCCTX_NOIRQ);
+	}
 }
 
 /*

commit 17ce302f3117e9518395847a3120c8a108b587b8
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Jun 11 10:38:09 2019 +0100

    arm64: Fix interrupt tracing in the presence of NMIs
    
    In the presence of any form of instrumentation, nmi_enter() should be
    done before calling any traceable code and any instrumentation code.
    
    Currently, nmi_enter() is done in handle_domain_nmi(), which is much
    too late as instrumentation code might get called before. Move the
    nmi_enter/exit() calls to the arch IRQ vector handler.
    
    On arm64, it is not possible to know if the IRQ vector handler was
    called because of an NMI before acknowledging the interrupt. However, It
    is possible to know whether normal interrupts could be taken in the
    interrupted context (i.e. if taking an NMI in that context could
    introduce a potential race condition).
    
    When interrupting a context with IRQs disabled, call nmi_enter() as soon
    as possible. In contexts with IRQs enabled, defer this to the interrupt
    controller, which is in a better position to know if an interrupt taken
    is an NMI.
    
    Fixes: bc3c03ccb464 ("arm64: Enable the support of pseudo-NMIs")
    Cc: <stable@vger.kernel.org> # 5.1.x-
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jason Cooper <jason@lakedaemon.net>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 92fa81798fb9..fdd9cb27fed5 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -27,8 +27,10 @@
 #include <linux/smp.h>
 #include <linux/init.h>
 #include <linux/irqchip.h>
+#include <linux/kprobes.h>
 #include <linux/seq_file.h>
 #include <linux/vmalloc.h>
+#include <asm/daifflags.h>
 #include <asm/vmap_stack.h>
 
 unsigned long irq_err_count;
@@ -76,3 +78,18 @@ void __init init_IRQ(void)
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
 }
+
+/*
+ * Stubs to make nmi_enter/exit() code callable from ASM
+ */
+asmlinkage void notrace asm_nmi_enter(void)
+{
+	nmi_enter();
+}
+NOKPROBE_SYMBOL(asm_nmi_enter);
+
+asmlinkage void notrace asm_nmi_exit(void)
+{
+	nmi_exit();
+}
+NOKPROBE_SYMBOL(asm_nmi_exit);

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 92fa81798fb9..c70034fbd4ce 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/kernel/irq.c
  *
@@ -7,18 +8,6 @@
  * Dynamic Tick Timer written by Tony Lindgren <tony@atomide.com> and
  * Tuukka Tikkanen <tuukka.tikkanen@elektrobit.com>.
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/kernel_stat.h>

commit 5870970b9a828d8693aa6d15742573289d7dbcd0
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Jan 31 14:58:39 2019 +0000

    arm64: Fix HCR.TGE status for NMI contexts
    
    When using VHE, the host needs to clear HCR_EL2.TGE bit in order
    to interact with guest TLBs, switching from EL2&0 translation regime
    to EL1&0.
    
    However, some non-maskable asynchronous event could happen while TGE is
    cleared like SDEI. Because of this address translation operations
    relying on EL2&0 translation regime could fail (tlb invalidation,
    userspace access, ...).
    
    Fix this by properly setting HCR_EL2.TGE when entering NMI context and
    clear it if necessary when returning to the interrupted context.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Suggested-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 780a12f59a8f..92fa81798fb9 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -33,6 +33,9 @@
 
 unsigned long irq_err_count;
 
+/* Only access this in an NMI enter/exit */
+DEFINE_PER_CPU(struct nmi_ctx, nmi_contexts);
+
 DEFINE_PER_CPU(unsigned long *, irq_stack_ptr);
 
 int arch_show_interrupts(struct seq_file *p, int prec)

commit 78ae2e1cd845480caaa2f181fee64e51f679f5aa
Author: Palmer Dabbelt <palmer@sifive.com>
Date:   Fri Jun 22 10:01:24 2018 -0700

    arm64: Use the new GENERIC_IRQ_MULTI_HANDLER
    
    It appears arm64 copied arm's GENERIC_IRQ_MULTI_HANDLER code, but made
    it unconditional.
    
    Converts the arm64 code to use the new generic code, which simply consists
    of deleting the arm64 code and setting MULTI_IRQ_HANDLER instead.
    
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: linux@armlinux.org.uk
    Cc: catalin.marinas@arm.com
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: jonas@southpole.se
    Cc: stefan.kristiansson@saunalahti.fi
    Cc: shorne@gmail.com
    Cc: jason@lakedaemon.net
    Cc: marc.zyngier@arm.com
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: nicolas.pitre@linaro.org
    Cc: vladimir.murzin@arm.com
    Cc: keescook@chromium.org
    Cc: jinb.park7@gmail.com
    Cc: yamada.masahiro@socionext.com
    Cc: alexandre.belloni@bootlin.com
    Cc: pombredanne@nexb.com
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: kstewart@linuxfoundation.org
    Cc: jhogan@kernel.org
    Cc: mark.rutland@arm.com
    Cc: ard.biesheuvel@linaro.org
    Cc: james.morse@arm.com
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: openrisc@lists.librecores.org
    Link: https://lkml.kernel.org/r/20180622170126.6308-4-palmer@sifive.com

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 60e5fc661f74..780a12f59a8f 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -42,16 +42,6 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 	return 0;
 }
 
-void (*handle_arch_irq)(struct pt_regs *) = NULL;
-
-void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))
-{
-	if (handle_arch_irq)
-		return;
-
-	handle_arch_irq = handle_irq;
-}
-
 #ifdef CONFIG_VMAP_STACK
 static void init_irq_stacks(void)
 {

commit ed8b20d457d72e9e2a30533b436fdb4ea1c70b38
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:10 2018 +0000

    arm64: Add vmap_stack header file
    
    Today the arm64 arch code allocates an extra IRQ stack per-cpu. If we
    also have SDEI and VMAP stacks we need two extra per-cpu VMAP stacks.
    
    Move the VMAP stack allocation out to a helper in a new header file.
    This avoids missing THREADINFO_GFP, or getting the all-important alignment
    wrong.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 713561e5bcab..60e5fc661f74 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -29,6 +29,7 @@
 #include <linux/irqchip.h>
 #include <linux/seq_file.h>
 #include <linux/vmalloc.h>
+#include <asm/vmap_stack.h>
 
 unsigned long irq_err_count;
 
@@ -58,17 +59,7 @@ static void init_irq_stacks(void)
 	unsigned long *p;
 
 	for_each_possible_cpu(cpu) {
-		/*
-		* To ensure that VMAP'd stack overflow detection works
-		* correctly, the IRQ stacks need to have the same
-		* alignment as other stacks.
-		*/
-		p = __vmalloc_node_range(IRQ_STACK_SIZE, THREAD_ALIGN,
-					 VMALLOC_START, VMALLOC_END,
-					 THREADINFO_GFP, PAGE_KERNEL,
-					 0, cpu_to_node(cpu),
-					 __builtin_return_address(0));
-
+		p = arch_alloc_vmap_stack(IRQ_STACK_SIZE, cpu_to_node(cpu));
 		per_cpu(irq_stack_ptr, cpu) = p;
 	}
 }

commit e3067861ba6650a566a6273738c23c956ad55c02
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Jul 21 14:25:33 2017 +0100

    arm64: add basic VMAP_STACK support
    
    This patch enables arm64 to be built with vmap'd task and IRQ stacks.
    
    As vmap'd stacks are mapped at page granularity, stacks must be a multiple of
    PAGE_SIZE. This means that a 64K page kernel must use stacks of at least 64K in
    size.
    
    To minimize the increase in Image size, IRQ stacks are dynamically allocated at
    boot time, rather than embedding the boot CPU's IRQ stack in the kernel image.
    
    This patch was co-authored by Ard Biesheuvel and Mark Rutland.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 5141282e47d5..713561e5bcab 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -23,15 +23,15 @@
 
 #include <linux/kernel_stat.h>
 #include <linux/irq.h>
+#include <linux/memory.h>
 #include <linux/smp.h>
 #include <linux/init.h>
 #include <linux/irqchip.h>
 #include <linux/seq_file.h>
+#include <linux/vmalloc.h>
 
 unsigned long irq_err_count;
 
-/* irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned. */
-DEFINE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack) __aligned(16);
 DEFINE_PER_CPU(unsigned long *, irq_stack_ptr);
 
 int arch_show_interrupts(struct seq_file *p, int prec)
@@ -51,6 +51,31 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))
 	handle_arch_irq = handle_irq;
 }
 
+#ifdef CONFIG_VMAP_STACK
+static void init_irq_stacks(void)
+{
+	int cpu;
+	unsigned long *p;
+
+	for_each_possible_cpu(cpu) {
+		/*
+		* To ensure that VMAP'd stack overflow detection works
+		* correctly, the IRQ stacks need to have the same
+		* alignment as other stacks.
+		*/
+		p = __vmalloc_node_range(IRQ_STACK_SIZE, THREAD_ALIGN,
+					 VMALLOC_START, VMALLOC_END,
+					 THREADINFO_GFP, PAGE_KERNEL,
+					 0, cpu_to_node(cpu),
+					 __builtin_return_address(0));
+
+		per_cpu(irq_stack_ptr, cpu) = p;
+	}
+}
+#else
+/* irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned. */
+DEFINE_PER_CPU_ALIGNED(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack);
+
 static void init_irq_stacks(void)
 {
 	int cpu;
@@ -58,6 +83,7 @@ static void init_irq_stacks(void)
 	for_each_possible_cpu(cpu)
 		per_cpu(irq_stack_ptr, cpu) = per_cpu(irq_stack, cpu);
 }
+#endif
 
 void __init init_IRQ(void)
 {

commit f60fe78f133243e6de0f05fdefc3ed2f3c5085ca
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jul 31 21:17:03 2017 +0100

    arm64: use an irq stack pointer
    
    We allocate our IRQ stacks using a percpu array. This allows us to generate our
    IRQ stack pointers with adr_this_cpu, but bloats the kernel Image with the boot
    CPU's IRQ stack. Additionally, these are packed with other percpu variables,
    and aren't guaranteed to have guard pages.
    
    When we enable VMAP_STACK we'll want to vmap our IRQ stacks also, in order to
    provide guard pages and to permit more stringent alignment requirements. Doing
    so will require that we use a percpu pointer to each IRQ stack, rather than
    allocating a percpu IRQ stack in the kernel image.
    
    This patch updates our IRQ stack code to use a percpu pointer to the base of
    each IRQ stack. This will allow us to change the way the stack is allocated
    with minimal changes elsewhere. In some cases we may try to backtrace before
    the IRQ stack pointers are initialised, so on_irq_stack() is updated to account
    for this.
    
    In testing with cyclictest, there was no measureable difference between using
    adr_this_cpu (for irq_stack) and ldr_this_cpu (for irq_stack_ptr) in the IRQ
    entry path.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 2386b26c0712..5141282e47d5 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -32,6 +32,7 @@ unsigned long irq_err_count;
 
 /* irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned. */
 DEFINE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack) __aligned(16);
+DEFINE_PER_CPU(unsigned long *, irq_stack_ptr);
 
 int arch_show_interrupts(struct seq_file *p, int prec)
 {
@@ -50,8 +51,17 @@ void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))
 	handle_arch_irq = handle_irq;
 }
 
+static void init_irq_stacks(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		per_cpu(irq_stack_ptr, cpu) = per_cpu(irq_stack, cpu);
+}
+
 void __init init_IRQ(void)
 {
+	init_irq_stacks();
 	irqchip_init();
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");

commit d224a69e3d80fe08f285d1f41d21b590bae4fa9f
Author: James Morse <james.morse@arm.com>
Date:   Fri Dec 18 16:01:47 2015 +0000

    arm64: remove irq_count and do_softirq_own_stack()
    
    sysrq_handle_reboot() re-enables interrupts while on the irq stack. The
    irq_stack implementation wrongly assumed this would only ever happen
    via the softirq path, allowing it to update irq_count late, in
    do_softirq_own_stack().
    
    This means if an irq occurs in sysrq_handle_reboot(), during
    emergency_restart() the stack will be corrupted, as irq_count wasn't
    updated.
    
    Lose the optimisation, and instead of moving the adding/subtracting of
    irq_count into irq_stack_entry/irq_stack_exit, remove it, and compare
    sp_el0 (struct thread_info) with sp & ~(THREAD_SIZE - 1). This tells us
    if we are on a task stack, if so, we can safely switch to the irq stack.
    Finally, remove do_softirq_own_stack(), we don't need it anymore.
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    [will: use get_thread_info macro]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index ff7ebb710e51..2386b26c0712 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -25,24 +25,14 @@
 #include <linux/irq.h>
 #include <linux/smp.h>
 #include <linux/init.h>
-#include <linux/interrupt.h>
 #include <linux/irqchip.h>
 #include <linux/seq_file.h>
 
 unsigned long irq_err_count;
 
-/*
- * irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned.
- * irq_stack[0] is used as irq_count, a non-zero value indicates the stack
- * is in use, and el?_irq() shouldn't switch to it. This is used to detect
- * recursive use of the irq_stack, it is lazily updated by
- * do_softirq_own_stack(), which is called on the irq_stack, before
- * re-enabling interrupts to process softirqs.
- */
+/* irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned. */
 DEFINE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack) __aligned(16);
 
-#define IRQ_COUNT()	(*per_cpu(irq_stack, smp_processor_id()))
-
 int arch_show_interrupts(struct seq_file *p, int prec)
 {
 	show_ipi_list(p, prec);
@@ -66,29 +56,3 @@ void __init init_IRQ(void)
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
 }
-
-/*
- * do_softirq_own_stack() is called from irq_exit() before __do_softirq()
- * re-enables interrupts, at which point we may re-enter el?_irq(). We
- * increase irq_count here so that el1_irq() knows that it is already on the
- * irq stack.
- *
- * Called with interrupts disabled, so we don't worry about moving cpu, or
- * being interrupted while modifying irq_count.
- *
- * This function doesn't actually switch stack.
- */
-void do_softirq_own_stack(void)
-{
-	int cpu = smp_processor_id();
-
-	WARN_ON_ONCE(!irqs_disabled());
-
-	if (on_irq_stack(current_stack_pointer, cpu)) {
-		IRQ_COUNT()++;
-		__do_softirq();
-		IRQ_COUNT()--;
-	} else {
-		__do_softirq();
-	}
-}

commit 8e23dacd12a48e58125b84c817da50850b73280a
Author: James Morse <james.morse@arm.com>
Date:   Fri Dec 4 11:02:27 2015 +0000

    arm64: Add do_softirq_own_stack() and enable irq_stacks
    
    entry.S is modified to switch to the per_cpu irq_stack during el{0,1}_irq.
    irq_count is used to detect recursive interrupts on the irq_stack, it is
    updated late by do_softirq_own_stack(), when called on the irq_stack, before
    __do_softirq() re-enables interrupts to process softirqs.
    
    do_softirq_own_stack() is added by this patch, but does not yet switch
    stack.
    
    This patch adds the dummy stack frame and data needed by the previous
    stack tracing patches.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 1e3cef578e21..ff7ebb710e51 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -25,14 +25,24 @@
 #include <linux/irq.h>
 #include <linux/smp.h>
 #include <linux/init.h>
+#include <linux/interrupt.h>
 #include <linux/irqchip.h>
 #include <linux/seq_file.h>
 
 unsigned long irq_err_count;
 
-/* irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned */
+/*
+ * irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned.
+ * irq_stack[0] is used as irq_count, a non-zero value indicates the stack
+ * is in use, and el?_irq() shouldn't switch to it. This is used to detect
+ * recursive use of the irq_stack, it is lazily updated by
+ * do_softirq_own_stack(), which is called on the irq_stack, before
+ * re-enabling interrupts to process softirqs.
+ */
 DEFINE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack) __aligned(16);
 
+#define IRQ_COUNT()	(*per_cpu(irq_stack, smp_processor_id()))
+
 int arch_show_interrupts(struct seq_file *p, int prec)
 {
 	show_ipi_list(p, prec);
@@ -56,3 +66,29 @@ void __init init_IRQ(void)
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
 }
+
+/*
+ * do_softirq_own_stack() is called from irq_exit() before __do_softirq()
+ * re-enables interrupts, at which point we may re-enter el?_irq(). We
+ * increase irq_count here so that el1_irq() knows that it is already on the
+ * irq stack.
+ *
+ * Called with interrupts disabled, so we don't worry about moving cpu, or
+ * being interrupted while modifying irq_count.
+ *
+ * This function doesn't actually switch stack.
+ */
+void do_softirq_own_stack(void)
+{
+	int cpu = smp_processor_id();
+
+	WARN_ON_ONCE(!irqs_disabled());
+
+	if (on_irq_stack(current_stack_pointer, cpu)) {
+		IRQ_COUNT()++;
+		__do_softirq();
+		IRQ_COUNT()--;
+	} else {
+		__do_softirq();
+	}
+}

commit 132cd887b5c54758d04bf25c52fa48f45e843a30
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Dec 4 11:02:26 2015 +0000

    arm64: Modify stack trace and dump for use with irq_stack
    
    This patch allows unwind_frame() to traverse from interrupt stack to task
    stack correctly. It requires data from a dummy stack frame, created
    during irq_stack_entry(), added by a later patch.
    
    A similar approach is taken to modify dump_backtrace(), which expects to
    find struct pt_regs underneath any call to functions marked __exception.
    When on an irq_stack, the struct pt_regs is stored on the old task stack,
    the location of which is stored in the dummy stack frame.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    [james.morse: merged two patches, reworked for per_cpu irq_stacks, and
     no alignment guarantees, added irq_stack definitions]
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 9f17ec071ee0..1e3cef578e21 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -30,6 +30,9 @@
 
 unsigned long irq_err_count;
 
+/* irq stack only needs to be 16 byte aligned - not IRQ_STACK_SIZE aligned */
+DEFINE_PER_CPU(unsigned long [IRQ_STACK_SIZE/sizeof(long)], irq_stack) __aligned(16);
+
 int arch_show_interrupts(struct seq_file *p, int prec)
 {
 	show_ipi_list(p, prec);

commit 217d453d473c5ddfd140a06bf9d8575218551020
Author: Yang Yingliang <yangyingliang@huawei.com>
Date:   Thu Sep 24 17:32:14 2015 +0800

    arm64: fix a migrating irq bug when hotplug cpu
    
    When cpu is disabled, all irqs will be migratged to another cpu.
    In some cases, a new affinity is different, the old affinity need
    to be updated and if irq_set_affinity's return value is IRQ_SET_MASK_OK_DONE,
    the old affinity can not be updated. Fix it by using irq_do_set_affinity.
    
    And migrating interrupts is a core code matter, so use the generic
    function irq_migrate_all_off_this_cpu() to migrate interrupts in
    kernel/irq/migration.c.
    
    Cc: Jiang Liu <jiang.liu@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Russell King - ARM Linux <linux@arm.linux.org.uk>
    Cc: Hanjun Guo <hanjun.guo@linaro.org>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Yang Yingliang <yangyingliang@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 11dc3fd47853..9f17ec071ee0 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -27,7 +27,6 @@
 #include <linux/init.h>
 #include <linux/irqchip.h>
 #include <linux/seq_file.h>
-#include <linux/ratelimit.h>
 
 unsigned long irq_err_count;
 
@@ -54,64 +53,3 @@ void __init init_IRQ(void)
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
 }
-
-#ifdef CONFIG_HOTPLUG_CPU
-static bool migrate_one_irq(struct irq_desc *desc)
-{
-	struct irq_data *d = irq_desc_get_irq_data(desc);
-	const struct cpumask *affinity = irq_data_get_affinity_mask(d);
-	struct irq_chip *c;
-	bool ret = false;
-
-	/*
-	 * If this is a per-CPU interrupt, or the affinity does not
-	 * include this CPU, then we have nothing to do.
-	 */
-	if (irqd_is_per_cpu(d) || !cpumask_test_cpu(smp_processor_id(), affinity))
-		return false;
-
-	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
-		affinity = cpu_online_mask;
-		ret = true;
-	}
-
-	c = irq_data_get_irq_chip(d);
-	if (!c->irq_set_affinity)
-		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
-	else if (c->irq_set_affinity(d, affinity, false) == IRQ_SET_MASK_OK && ret)
-		cpumask_copy(irq_data_get_affinity_mask(d), affinity);
-
-	return ret;
-}
-
-/*
- * The current CPU has been marked offline.  Migrate IRQs off this CPU.
- * If the affinity settings do not allow other CPUs, force them onto any
- * available CPU.
- *
- * Note: we must iterate over all IRQs, whether they have an attached
- * action structure or not, as we need to get chained interrupts too.
- */
-void migrate_irqs(void)
-{
-	unsigned int i;
-	struct irq_desc *desc;
-	unsigned long flags;
-
-	local_irq_save(flags);
-
-	for_each_irq_desc(i, desc) {
-		bool affinity_broken;
-
-		raw_spin_lock(&desc->lock);
-		affinity_broken = migrate_one_irq(desc);
-		raw_spin_unlock(&desc->lock);
-
-		if (affinity_broken)
-			pr_warn_ratelimited("IRQ%u no longer affine to CPU%u\n",
-					    i, smp_processor_id());
-	}
-
-	local_irq_restore(flags);
-}
-#endif /* CONFIG_HOTPLUG_CPU */

commit 4b3dc9679cf779339d9049800803dfc3c83433d1
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 29 18:28:44 2015 +0100

    arm64: force CONFIG_SMP=y and remove redundant #ifdefs
    
    Nobody seems to be producing !SMP systems anymore, so this is just
    becoming a source of kernel bugs, particularly if people want to use
    coherent DMA with non-shared pages.
    
    This patch forces CONFIG_SMP=y for arm64, removing a modest amount of
    code in the process.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 463fa2e7e34c..11dc3fd47853 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -33,9 +33,7 @@ unsigned long irq_err_count;
 
 int arch_show_interrupts(struct seq_file *p, int prec)
 {
-#ifdef CONFIG_SMP
 	show_ipi_list(p, prec);
-#endif
 	seq_printf(p, "%*s: %10lu\n", prec, "Err", irq_err_count);
 	return 0;
 }

commit 3bc38fc110c642c0a455a5214465bb4d62805a75
Author: Jiang Liu <jiang.liu@linux.intel.com>
Date:   Mon Jul 13 20:30:04 2015 +0000

    ARM64/irq: Use access helper irq_data_get_affinity_mask()
    
    This is a preparatory patch for moving irq_data struct members.
    
    Signed-off-by: Jiang Liu <jiang.liu@linux.intel.com>
    Reviewed-by: Hanjun Guo <hanjun.guo@linaro.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 240b75c0e94f..463fa2e7e34c 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -61,7 +61,7 @@ void __init init_IRQ(void)
 static bool migrate_one_irq(struct irq_desc *desc)
 {
 	struct irq_data *d = irq_desc_get_irq_data(desc);
-	const struct cpumask *affinity = d->affinity;
+	const struct cpumask *affinity = irq_data_get_affinity_mask(d);
 	struct irq_chip *c;
 	bool ret = false;
 
@@ -81,7 +81,7 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	if (!c->irq_set_affinity)
 		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
 	else if (c->irq_set_affinity(d, affinity, false) == IRQ_SET_MASK_OK && ret)
-		cpumask_copy(d->affinity, affinity);
+		cpumask_copy(irq_data_get_affinity_mask(d), affinity);
 
 	return ret;
 }

commit fcff588633e848aa728a4437ef96d437299ba03d
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Fri Nov 21 21:50:38 2014 +0000

    arm64: Treat handle_arch_irq as a function pointer
    
    handle_arch_irq isn't actually text, it's just a function pointer.
    It doesn't need to be stored in the text section and doing so
    causes problesm if we ever want to make the kernel text read only.
    Declare handle_arch_irq as a proper function pointer stored in
    the data section.
    
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 071a6ec13bd8..240b75c0e94f 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -40,6 +40,8 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 	return 0;
 }
 
+void (*handle_arch_irq)(struct pt_regs *) = NULL;
+
 void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))
 {
 	if (handle_arch_irq)

commit 782d59c5dfc5ac39ac8cfb4c6dd40597938dde9c
Merge: 47137c6ba1bc 2828c9cdb8bd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 9 06:42:04 2014 -0400

    Merge branch 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull irq updates from Thomas Gleixner:
     "The irq departement delivers:
    
       - a cleanup series to get rid of mindlessly copied code.
    
       - another bunch of new pointlessly different interrupt chip drivers.
    
         Adding homebrewn irq chips (and timers) to SoCs must provide a
         value add which is beyond the imagination of mere mortals.
    
       - the usual SoC irq controller updates, IOW my second cat herding
         project"
    
    * 'irq-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (44 commits)
      irqchip: gic-v3: Implement CPU PM notifier
      irqchip: gic-v3: Refactor gic_enable_redist to support both enabling and disabling
      irqchip: renesas-intc-irqpin: Add minimal runtime PM support
      irqchip: renesas-intc-irqpin: Add helper variable dev = &pdev->dev
      irqchip: atmel-aic5: Add sama5d4 support
      irqchip: atmel-aic5: The sama5d3 has 48 IRQs
      Documentation: bcm7120-l2: Add Broadcom BCM7120-style L2 binding
      irqchip: bcm7120-l2: Add Broadcom BCM7120-style Level 2 interrupt controller
      irqchip: renesas-irqc: Add binding docs for new R-Car Gen2 SoCs
      irqchip: renesas-irqc: Add DT binding documentation
      irqchip: renesas-intc-irqpin: Document SoC-specific bindings
      openrisc: Get rid of handle_IRQ
      arm64: Get rid of handle_IRQ
      ARM: omap2: irq: Convert to handle_domain_irq
      ARM: imx: tzic: Convert to handle_domain_irq
      ARM: imx: avic: Convert to handle_domain_irq
      irqchip: or1k-pic: Convert to handle_domain_irq
      irqchip: atmel-aic5: Convert to handle_domain_irq
      irqchip: atmel-aic: Convert to handle_domain_irq
      irqchip: gic-v3: Convert to handle_domain_irq
      ...

commit 3d8afe3099ebc602848aa7f09235cce3a9a023ce
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Tue Sep 2 11:35:24 2014 +0100

    arm64: use irq_set_affinity with force=false when migrating irqs
    
    The arm64 interrupt migration code on cpu offline calls
    irqchip.irq_set_affinity() with the argument force=true. Originally
    this argument had no effect because it was not used by any interrupt
    chip driver and there was no semantics defined.
    
    This changed with commit 01f8fa4f01d8 ("genirq: Allow forcing cpu
    affinity of interrupts") which made the force argument useful to route
    interrupts to not yet online cpus without checking the target cpu
    against the cpu online mask. The following commit ffde1de64012
    ("irqchip: gic: Support forced affinity setting") implemented this for
    the GIC interrupt controller.
    
    As a consequence the cpu offline irq migration fails if CPU0 is
    offlined, because CPU0 is still set in the affinity mask and the
    validation against cpu online mask is skipped to the force argument
    being true. The following first_cpu(mask) selection always selects
    CPU0 as the target.
    
    Commit 601c942176d8("arm64: use cpu_online_mask when using forced
    irq_set_affinity") intended to fix the above mentioned issue but
    introduced another issue where affinity can be migrated to a wrong
    CPU due to unconditional copy of cpu_online_mask.
    
    As with for arm, solve the issue by calling irq_set_affinity() with
    force=false from the CPU offline irq migration code so the GIC driver
    validates the affinity mask against CPU online mask and therefore
    removes CPU0 from the possible target candidates. Also revert the
    changes done in the commit 601c942176d8 as it's no longer needed.
    
    Tested on Juno platform.
    
    Fixes: 601c942176d8("arm64: use cpu_online_mask when using forced
            irq_set_affinity")
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org> # 3.10.x
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 0f08dfd69ebc..dfa6e3e74fdd 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -97,19 +97,15 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	if (irqd_is_per_cpu(d) || !cpumask_test_cpu(smp_processor_id(), affinity))
 		return false;
 
-	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids)
+	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
+		affinity = cpu_online_mask;
 		ret = true;
+	}
 
-	/*
-	 * when using forced irq_set_affinity we must ensure that the cpu
-	 * being offlined is not present in the affinity mask, it may be
-	 * selected as the target CPU otherwise
-	 */
-	affinity = cpu_online_mask;
 	c = irq_data_get_irq_chip(d);
 	if (!c->irq_set_affinity)
 		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
-	else if (c->irq_set_affinity(d, affinity, true) == IRQ_SET_MASK_OK && ret)
+	else if (c->irq_set_affinity(d, affinity, false) == IRQ_SET_MASK_OK && ret)
 		cpumask_copy(d->affinity, affinity);
 
 	return ret;

commit c59e1ef874e699bb37c8ed20b70113e1e8f45f52
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 26 11:03:40 2014 +0100

    arm64: Get rid of handle_IRQ
    
    All the arm64 irqchip drivers have been converted to handle_domain_irq,
    making it possible to remove the handle_IRQ stub entierely.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: https://lkml.kernel.org/r/1409047421-27649-26-git-send-email-marc.zyngier@arm.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 2c0e2a744723..67ca197277ee 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -40,17 +40,6 @@ int arch_show_interrupts(struct seq_file *p, int prec)
 	return 0;
 }
 
-/*
- * handle_IRQ handles all hardware IRQ's.  Decoded IRQs should
- * not come via this function.  Instead, they should provide their
- * own 'handler'.  Used by platform code implementing C-based 1st
- * level decoding.
- */
-void handle_IRQ(unsigned int irq, struct pt_regs *regs)
-{
-	__handle_domain_irq(NULL, irq, false, regs);
-}
-
 void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))
 {
 	if (handle_arch_irq)

commit a1ddc74a23c89ae236b163a3b0887f8c344aaa4a
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Tue Aug 26 11:03:17 2014 +0100

    arm64: Convert handle_IRQ to use __handle_domain_irq
    
    In order to limit code duplication, convert the architecture specific
    handle_IRQ to use the generic __handle_domain_irq function.
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Link: https://lkml.kernel.org/r/1409047421-27649-3-git-send-email-marc.zyngier@arm.com
    Signed-off-by: Jason Cooper <jason@lakedaemon.net>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 0f08dfd69ebc..2c0e2a744723 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -48,23 +48,7 @@ int arch_show_interrupts(struct seq_file *p, int prec)
  */
 void handle_IRQ(unsigned int irq, struct pt_regs *regs)
 {
-	struct pt_regs *old_regs = set_irq_regs(regs);
-
-	irq_enter();
-
-	/*
-	 * Some hardware gives randomly wrong interrupts.  Rather
-	 * than crashing, do something sensible.
-	 */
-	if (unlikely(irq >= nr_irqs)) {
-		pr_warn_ratelimited("Bad IRQ%u\n", irq);
-		ack_bad_irq(irq);
-	} else {
-		generic_handle_irq(irq);
-	}
-
-	irq_exit();
-	set_irq_regs(old_regs);
+	__handle_domain_irq(NULL, irq, false, regs);
 }
 
 void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))

commit 601c942176d8ad8334118bddb747e3720bed24f8
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Fri May 9 17:37:44 2014 +0100

    arm64: use cpu_online_mask when using forced irq_set_affinity
    
    Commit 01f8fa4f01d8("genirq: Allow forcing cpu affinity of interrupts")
    enabled the forced irq_set_affinity which previously refused to route an
    interrupt to an offline cpu.
    
    Commit ffde1de64012("irqchip: Gic: Support forced affinity setting")
    implements this force logic and disables the cpu online check for GIC
    interrupt controller.
    
    When __cpu_disable calls migrate_irqs, it disables the current cpu in
    cpu_online_mask and uses forced irq_set_affinity to migrate the IRQs
    away from the cpu but passes affinity mask with the cpu being offlined
    also included in it.
    
    When calling irq_set_affinity with force == true in a cpu hotplug path,
    the caller must ensure that the cpu being offlined is not present in the
    affinity mask or it may be selected as the target CPU, leading to the
    interrupt not being migrated.
    
    This patch uses cpu_online_mask when using forced irq_set_affinity so
    that the IRQs are properly migrated away.
    
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 473e5dbf8f39..0f08dfd69ebc 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -97,11 +97,15 @@ static bool migrate_one_irq(struct irq_desc *desc)
 	if (irqd_is_per_cpu(d) || !cpumask_test_cpu(smp_processor_id(), affinity))
 		return false;
 
-	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
-		affinity = cpu_online_mask;
+	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids)
 		ret = true;
-	}
 
+	/*
+	 * when using forced irq_set_affinity we must ensure that the cpu
+	 * being offlined is not present in the affinity mask, it may be
+	 * selected as the target CPU otherwise
+	 */
+	affinity = cpu_online_mask;
 	c = irq_data_get_irq_chip(d);
 	if (!c->irq_set_affinity)
 		pr_debug("IRQ%u: unable to set affinity\n", d->irq);

commit 9327e2c6bb8cb0131b38a07847cd58c78dc095e9
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Oct 24 20:30:18 2013 +0100

    arm64: add CPU_HOTPLUG infrastructure
    
    This patch adds the basic infrastructure necessary to support
    CPU_HOTPLUG on arm64, based on the arm implementation. Actual hotplug
    support will depend on an implementation's cpu_operations (e.g. PSCI).
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index ecb3354292ed..473e5dbf8f39 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -81,3 +81,64 @@ void __init init_IRQ(void)
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
 }
+
+#ifdef CONFIG_HOTPLUG_CPU
+static bool migrate_one_irq(struct irq_desc *desc)
+{
+	struct irq_data *d = irq_desc_get_irq_data(desc);
+	const struct cpumask *affinity = d->affinity;
+	struct irq_chip *c;
+	bool ret = false;
+
+	/*
+	 * If this is a per-CPU interrupt, or the affinity does not
+	 * include this CPU, then we have nothing to do.
+	 */
+	if (irqd_is_per_cpu(d) || !cpumask_test_cpu(smp_processor_id(), affinity))
+		return false;
+
+	if (cpumask_any_and(affinity, cpu_online_mask) >= nr_cpu_ids) {
+		affinity = cpu_online_mask;
+		ret = true;
+	}
+
+	c = irq_data_get_irq_chip(d);
+	if (!c->irq_set_affinity)
+		pr_debug("IRQ%u: unable to set affinity\n", d->irq);
+	else if (c->irq_set_affinity(d, affinity, true) == IRQ_SET_MASK_OK && ret)
+		cpumask_copy(d->affinity, affinity);
+
+	return ret;
+}
+
+/*
+ * The current CPU has been marked offline.  Migrate IRQs off this CPU.
+ * If the affinity settings do not allow other CPUs, force them onto any
+ * available CPU.
+ *
+ * Note: we must iterate over all IRQs, whether they have an attached
+ * action structure or not, as we need to get chained interrupts too.
+ */
+void migrate_irqs(void)
+{
+	unsigned int i;
+	struct irq_desc *desc;
+	unsigned long flags;
+
+	local_irq_save(flags);
+
+	for_each_irq_desc(i, desc) {
+		bool affinity_broken;
+
+		raw_spin_lock(&desc->lock);
+		affinity_broken = migrate_one_irq(desc);
+		raw_spin_unlock(&desc->lock);
+
+		if (affinity_broken)
+			pr_warn_ratelimited("IRQ%u no longer affine to CPU%u\n",
+					    i, smp_processor_id());
+	}
+
+	local_irq_restore(flags);
+}
+#endif /* CONFIG_HOTPLUG_CPU */

commit e851b58cb77b47a5c14267723bd6b76655d21840
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jan 14 12:39:31 2013 +0000

    arm64: Use irqchip_init() for interrupt controller initialisation
    
    This patch uses the generic irqchip_init() function for initialising the
    interrupt controller on arm64. It also adds several definitions required
    by the ARM GIC irqchip driver but does not enable ARM_GIC yet.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
index 0373c6609eaf..ecb3354292ed 100644
--- a/arch/arm64/kernel/irq.c
+++ b/arch/arm64/kernel/irq.c
@@ -25,7 +25,7 @@
 #include <linux/irq.h>
 #include <linux/smp.h>
 #include <linux/init.h>
-#include <linux/of_irq.h>
+#include <linux/irqchip.h>
 #include <linux/seq_file.h>
 #include <linux/ratelimit.h>
 
@@ -67,18 +67,17 @@ void handle_IRQ(unsigned int irq, struct pt_regs *regs)
 	set_irq_regs(old_regs);
 }
 
-/*
- * Interrupt controllers supported by the kernel.
- */
-static const struct of_device_id intctrl_of_match[] __initconst = {
-	/* IRQ controllers { .compatible, .data } info to go here */
-	{}
-};
+void __init set_handle_irq(void (*handle_irq)(struct pt_regs *))
+{
+	if (handle_arch_irq)
+		return;
+
+	handle_arch_irq = handle_irq;
+}
 
 void __init init_IRQ(void)
 {
-	of_irq_init(intctrl_of_match);
-
+	irqchip_init();
 	if (!handle_arch_irq)
 		panic("No interrupt controller found.");
 }

commit fb9bd7d6df81ddf1e7ab6648ac89ddbe0625b26b
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Mar 5 11:49:29 2012 +0000

    arm64: IRQ handling
    
    This patch adds the support for IRQ handling. The actual interrupt
    controller will be part of a separate patch (going into
    drivers/irqchip/).
    
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm64/kernel/irq.c b/arch/arm64/kernel/irq.c
new file mode 100644
index 000000000000..0373c6609eaf
--- /dev/null
+++ b/arch/arm64/kernel/irq.c
@@ -0,0 +1,84 @@
+/*
+ * Based on arch/arm/kernel/irq.c
+ *
+ * Copyright (C) 1992 Linus Torvalds
+ * Modifications for ARM processor Copyright (C) 1995-2000 Russell King.
+ * Support for Dynamic Tick Timer Copyright (C) 2004-2005 Nokia Corporation.
+ * Dynamic Tick Timer written by Tony Lindgren <tony@atomide.com> and
+ * Tuukka Tikkanen <tuukka.tikkanen@elektrobit.com>.
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/kernel_stat.h>
+#include <linux/irq.h>
+#include <linux/smp.h>
+#include <linux/init.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <linux/ratelimit.h>
+
+unsigned long irq_err_count;
+
+int arch_show_interrupts(struct seq_file *p, int prec)
+{
+#ifdef CONFIG_SMP
+	show_ipi_list(p, prec);
+#endif
+	seq_printf(p, "%*s: %10lu\n", prec, "Err", irq_err_count);
+	return 0;
+}
+
+/*
+ * handle_IRQ handles all hardware IRQ's.  Decoded IRQs should
+ * not come via this function.  Instead, they should provide their
+ * own 'handler'.  Used by platform code implementing C-based 1st
+ * level decoding.
+ */
+void handle_IRQ(unsigned int irq, struct pt_regs *regs)
+{
+	struct pt_regs *old_regs = set_irq_regs(regs);
+
+	irq_enter();
+
+	/*
+	 * Some hardware gives randomly wrong interrupts.  Rather
+	 * than crashing, do something sensible.
+	 */
+	if (unlikely(irq >= nr_irqs)) {
+		pr_warn_ratelimited("Bad IRQ%u\n", irq);
+		ack_bad_irq(irq);
+	} else {
+		generic_handle_irq(irq);
+	}
+
+	irq_exit();
+	set_irq_regs(old_regs);
+}
+
+/*
+ * Interrupt controllers supported by the kernel.
+ */
+static const struct of_device_id intctrl_of_match[] __initconst = {
+	/* IRQ controllers { .compatible, .data } info to go here */
+	{}
+};
+
+void __init init_IRQ(void)
+{
+	of_irq_init(intctrl_of_match);
+
+	if (!handle_arch_irq)
+		panic("No interrupt controller found.");
+}
