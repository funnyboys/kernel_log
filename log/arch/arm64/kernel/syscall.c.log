commit 139dbe5d8ed383cbd1ada56c78dbbbd35bf6a9d3
Author: Will Deacon <will@kernel.org>
Date:   Fri Jul 3 09:41:24 2020 +0100

    arm64: syscall: Expand the comment about ptrace and syscall(-1)
    
    If a task executes syscall(-1), we intercept this early and force x0 to
    be -ENOSYS so that we don't need to distinguish this scenario from one
    where the scno is -1 because a tracer wants to skip the system call
    using ptrace. With the return value set, the return path is the same as
    the skip case.
    
    Although there is a one-line comment noting this in el0_svc_common(), it
    misses out most of the detail. Expand the comment to describe a bit more
    about what is going on.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Keno Fischer <keno@juliacomputing.com>
    Cc: Luis Machado <luis.machado@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 98a26d4e7b0c..5f0c04863d2c 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -124,7 +124,21 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 	user_exit();
 
 	if (has_syscall_work(flags)) {
-		/* set default errno for user-issued syscall(-1) */
+		/*
+		 * The de-facto standard way to skip a system call using ptrace
+		 * is to set the system call to -1 (NO_SYSCALL) and set x0 to a
+		 * suitable error code for consumption by userspace. However,
+		 * this cannot be distinguished from a user-issued syscall(-1)
+		 * and so we must set x0 to -ENOSYS here in case the tracer doesn't
+		 * issue the skip and we fall into trace_exit with x0 preserved.
+		 *
+		 * This is slightly odd because it also means that if a tracer
+		 * sets the system call number to -1 but does not initialise x0,
+		 * then x0 will be preserved for all system calls apart from a
+		 * user-issued syscall(-1). However, requesting a skip and not
+		 * setting the return value is unlikely to do anything sensible
+		 * anyway.
+		 */
 		if (scno == NO_SYSCALL)
 			regs->regs[0] = -ENOSYS;
 		scno = syscall_trace_enter(regs);

commit 15956689a0e60aa0c795174f3c310b60d8794235
Author: Will Deacon <will@kernel.org>
Date:   Fri Jul 3 12:08:42 2020 +0100

    arm64: compat: Ensure upper 32 bits of x0 are zero on syscall return
    
    Although we zero the upper bits of x0 on entry to the kernel from an
    AArch32 task, we do not clear them on the exception return path and can
    therefore expose 64-bit sign extended syscall return values to userspace
    via interfaces such as the 'perf_regs' ABI, which deal exclusively with
    64-bit registers.
    
    Explicitly clear the upper 32 bits of x0 on return from a compat system
    call.
    
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Keno Fischer <keno@juliacomputing.com>
    Cc: Luis Machado <luis.machado@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 7c14466a12af..98a26d4e7b0c 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -50,6 +50,9 @@ static void invoke_syscall(struct pt_regs *regs, unsigned int scno,
 		ret = do_ni_syscall(regs, scno);
 	}
 
+	if (is_compat_task())
+		ret = lower_32_bits(ret);
+
 	regs->regs[0] = ret;
 }
 

commit ac2081cdc4d99c57f219c1a6171526e0fa0a6fff
Author: Will Deacon <will@kernel.org>
Date:   Thu Jul 2 21:16:20 2020 +0100

    arm64: ptrace: Consistently use pseudo-singlestep exceptions
    
    Although the arm64 single-step state machine can be fast-forwarded in
    cases where we wish to generate a SIGTRAP without actually executing an
    instruction, this has two major limitations outside of simply skipping
    an instruction due to emulation.
    
    1. Stepping out of a ptrace signal stop into a signal handler where
       SIGTRAP is blocked. Fast-forwarding the stepping state machine in
       this case will result in a forced SIGTRAP, with the handler reset to
       SIG_DFL.
    
    2. The hardware implicitly fast-forwards the state machine when executing
       an SVC instruction for issuing a system call. This can interact badly
       with subsequent ptrace stops signalled during the execution of the
       system call (e.g. SYSCALL_EXIT or seccomp traps), as they may corrupt
       the stepping state by updating the PSTATE for the tracee.
    
    Resolve both of these issues by injecting a pseudo-singlestep exception
    on entry to a signal handler and also on return to userspace following a
    system call.
    
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Luis Machado <luis.machado@linaro.org>
    Reported-by: Keno Fischer <keno@juliacomputing.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 5f5b868292f5..7c14466a12af 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -139,7 +139,7 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 	if (!has_syscall_work(flags) && !IS_ENABLED(CONFIG_DEBUG_RSEQ)) {
 		local_daif_mask();
 		flags = current_thread_info()->flags;
-		if (!has_syscall_work(flags)) {
+		if (!has_syscall_work(flags) && !(flags & _TIF_SINGLESTEP)) {
 			/*
 			 * We're off to userspace, where interrupts are
 			 * always enabled after we restore the flags from

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index a12c0c88d345..5f5b868292f5 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -98,6 +98,24 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 	regs->orig_x0 = regs->regs[0];
 	regs->syscallno = scno;
 
+	/*
+	 * BTI note:
+	 * The architecture does not guarantee that SPSR.BTYPE is zero
+	 * on taking an SVC, so we could return to userspace with a
+	 * non-zero BTYPE after the syscall.
+	 *
+	 * This shouldn't matter except when userspace is explicitly
+	 * doing something stupid, such as setting PROT_BTI on a page
+	 * that lacks conforming BTI/PACIxSP instructions, falling
+	 * through from one executable page to another with differing
+	 * PROT_BTI, or messing with BTYPE via ptrace: in such cases,
+	 * userspace should not be surprised if a SIGILL occurs on
+	 * syscall return.
+	 *
+	 * So, don't touch regs->pstate & PSR_BTYPE_MASK here.
+	 * (Similarly for HVC and SMC elsewhere.)
+	 */
+
 	cortex_a76_erratum_1463225_svc_handler();
 	local_daif_restore(DAIF_PROCCTX);
 	user_exit();

commit 7a2c094464e39a54f5b9228cd78208cd43872bbd
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jan 16 18:35:47 2020 +0000

    arm64: entry: cleanup el0 svc handler naming
    
    For most of the exception entry code, <foo>_handler() is the first C
    function called from the entry assembly in entry-common.c, and external
    functions handling the bulk of the logic are called do_<foo>().
    
    For consistency, apply this scheme to el0_svc_handler and
    el0_svc_compat_handler, renaming them to do_el0_svc and
    do_el0_svc_compat respectively.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 9a9d98a443fc..a12c0c88d345 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -154,14 +154,14 @@ static inline void sve_user_discard(void)
 	sve_user_disable();
 }
 
-void el0_svc_handler(struct pt_regs *regs)
+void do_el0_svc(struct pt_regs *regs)
 {
 	sve_user_discard();
 	el0_svc_common(regs, regs->regs[8], __NR_syscalls, sys_call_table);
 }
 
 #ifdef CONFIG_COMPAT
-void el0_svc_compat_handler(struct pt_regs *regs)
+void do_el0_svc_compat(struct pt_regs *regs)
 {
 	el0_svc_common(regs, regs->regs[7], __NR_compat_syscalls,
 		       compat_sys_call_table);

commit afa7c0e5b965cdb945ad8a2e2973c6d7e19969f9
Author: James Morse <james.morse@arm.com>
Date:   Fri Oct 25 17:42:15 2019 +0100

    arm64: Remove asmlinkage from updated functions
    
    Now that the callers of these functions have moved into C, they no longer
    need the asmlinkage annotation. Remove it.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 871c739f060a..9a9d98a443fc 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -154,14 +154,14 @@ static inline void sve_user_discard(void)
 	sve_user_disable();
 }
 
-asmlinkage void el0_svc_handler(struct pt_regs *regs)
+void el0_svc_handler(struct pt_regs *regs)
 {
 	sve_user_discard();
 	el0_svc_common(regs, regs->regs[8], __NR_syscalls, sys_call_table);
 }
 
 #ifdef CONFIG_COMPAT
-asmlinkage void el0_svc_compat_handler(struct pt_regs *regs)
+void el0_svc_compat_handler(struct pt_regs *regs)
 {
 	el0_svc_common(regs, regs->regs[7], __NR_compat_syscalls,
 		       compat_sys_call_table);

commit 969f5ea627570e91c9d54403287ee3ed657f58fe
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 29 13:03:57 2019 +0100

    arm64: errata: Add workaround for Cortex-A76 erratum #1463225
    
    Revisions of the Cortex-A76 CPU prior to r4p0 are affected by an erratum
    that can prevent interrupts from being taken when single-stepping.
    
    This patch implements a software workaround to prevent userspace from
    effectively being able to disable interrupts.
    
    Cc: <stable@vger.kernel.org>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 5610ac01c1ec..871c739f060a 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -8,6 +8,7 @@
 #include <linux/syscalls.h>
 
 #include <asm/daifflags.h>
+#include <asm/debug-monitors.h>
 #include <asm/fpsimd.h>
 #include <asm/syscall.h>
 #include <asm/thread_info.h>
@@ -60,6 +61,35 @@ static inline bool has_syscall_work(unsigned long flags)
 int syscall_trace_enter(struct pt_regs *regs);
 void syscall_trace_exit(struct pt_regs *regs);
 
+#ifdef CONFIG_ARM64_ERRATUM_1463225
+DECLARE_PER_CPU(int, __in_cortex_a76_erratum_1463225_wa);
+
+static void cortex_a76_erratum_1463225_svc_handler(void)
+{
+	u32 reg, val;
+
+	if (!unlikely(test_thread_flag(TIF_SINGLESTEP)))
+		return;
+
+	if (!unlikely(this_cpu_has_cap(ARM64_WORKAROUND_1463225)))
+		return;
+
+	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 1);
+	reg = read_sysreg(mdscr_el1);
+	val = reg | DBG_MDSCR_SS | DBG_MDSCR_KDE;
+	write_sysreg(val, mdscr_el1);
+	asm volatile("msr daifclr, #8");
+	isb();
+
+	/* We will have taken a single-step exception by this point */
+
+	write_sysreg(reg, mdscr_el1);
+	__this_cpu_write(__in_cortex_a76_erratum_1463225_wa, 0);
+}
+#else
+static void cortex_a76_erratum_1463225_svc_handler(void) { }
+#endif /* CONFIG_ARM64_ERRATUM_1463225 */
+
 static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 			   const syscall_fn_t syscall_table[])
 {
@@ -68,6 +98,7 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 	regs->orig_x0 = regs->regs[0];
 	regs->syscallno = scno;
 
+	cortex_a76_erratum_1463225_svc_handler();
 	local_daif_restore(DAIF_PROCCTX);
 	user_exit();
 

commit 53290432145a8eb143fe29e06e9c1465d43dc723
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jan 3 18:00:39 2019 +0000

    arm64: compat: Don't pull syscall number from regs in arm_compat_syscall
    
    The syscall number may have been changed by a tracer, so we should pass
    the actual number in from the caller instead of pulling it from the
    saved r7 value directly.
    
    Cc: <stable@vger.kernel.org>
    Cc: Pi-Hsun Shih <pihsun@chromium.org>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 032d22312881..5610ac01c1ec 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -13,16 +13,15 @@
 #include <asm/thread_info.h>
 #include <asm/unistd.h>
 
-long compat_arm_syscall(struct pt_regs *regs);
-
+long compat_arm_syscall(struct pt_regs *regs, int scno);
 long sys_ni_syscall(void);
 
-asmlinkage long do_ni_syscall(struct pt_regs *regs)
+static long do_ni_syscall(struct pt_regs *regs, int scno)
 {
 #ifdef CONFIG_COMPAT
 	long ret;
 	if (is_compat_task()) {
-		ret = compat_arm_syscall(regs);
+		ret = compat_arm_syscall(regs, scno);
 		if (ret != -ENOSYS)
 			return ret;
 	}
@@ -47,7 +46,7 @@ static void invoke_syscall(struct pt_regs *regs, unsigned int scno,
 		syscall_fn = syscall_table[array_index_nospec(scno, sc_nr)];
 		ret = __invoke_syscall(regs, syscall_fn);
 	} else {
-		ret = do_ni_syscall(regs);
+		ret = do_ni_syscall(regs, scno);
 	}
 
 	regs->regs[0] = ret;

commit efd112353bf7c0f9d50f928b449ea9da0ee9554b
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jul 30 17:43:39 2018 +0100

    arm64: svc: Ensure hardirq tracing is updated before return
    
    We always run userspace with interrupts enabled, but with the recent
    conversion of the syscall entry/exit code to C, we don't inform the
    hardirq tracing code that interrupts are about to become enabled by
    virtue of restoring the EL0 SPSR.
    
    This patch ensures that trace_hardirqs_on() is called on the syscall
    return path when we return to the assembly code with interrupts still
    disabled.
    
    Fixes: f37099b6992a ("arm64: convert syscall trace logic to C")
    Reported-by: Julien Grall <julien.grall@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index be00c85794db..032d22312881 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -91,8 +91,15 @@ static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 	if (!has_syscall_work(flags) && !IS_ENABLED(CONFIG_DEBUG_RSEQ)) {
 		local_daif_mask();
 		flags = current_thread_info()->flags;
-		if (!has_syscall_work(flags))
+		if (!has_syscall_work(flags)) {
+			/*
+			 * We're off to userspace, where interrupts are
+			 * always enabled after we restore the flags from
+			 * the SPSR.
+			 */
+			trace_hardirqs_on();
 			return;
+		}
 		local_daif_restore(DAIF_PROCCTX);
 	}
 

commit 4378a7d4be30ec6994702b19936f7d1465193541
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:56 2018 +0100

    arm64: implement syscall wrappers
    
    To minimize the risk of userspace-controlled values being used under
    speculation, this patch adds pt_regs based syscall wrappers for arm64,
    which pass the minimum set of required userspace values to syscall
    implementations. For each syscall, a wrapper which takes a pt_regs
    argument is automatically generated, and this extracts the arguments
    before calling the "real" syscall implementation.
    
    Each syscall has three functions generated:
    
    * __do_<compat_>sys_<name> is the "real" syscall implementation, with
      the expected prototype.
    
    * __se_<compat_>sys_<name> is the sign-extension/narrowing wrapper,
      inherited from common code. This takes a series of long parameters,
      casting each to the requisite types required by the "real" syscall
      implementation in __do_<compat_>sys_<name>.
    
      This wrapper *may* not be necessary on arm64 given the AAPCS rules on
      unused register bits, but it seemed safer to keep the wrapper for now.
    
    * __arm64_<compat_>_sys_<name> takes a struct pt_regs pointer, and
      extracts *only* the relevant register values, passing these on to the
      __se_<compat_>sys_<name> wrapper.
    
    The syscall invocation code is updated to handle the calling convention
    required by __arm64_<compat_>_sys_<name>, and passes a single struct
    pt_regs pointer.
    
    The compiler can fold the syscall implementation and its wrappers, such
    that the overhead of this approach is minimized.
    
    Note that we play games with sys_ni_syscall(). It can't be defined with
    SYSCALL_DEFINE0() because we must avoid the possibility of error
    injection. Additionally, there are a couple of locations where we need
    to call it from C code, and we don't (currently) have a
    ksys_ni_syscall().  While it has no wrapper, passing in a redundant
    pt_regs pointer is benign per the AAPCS.
    
    When ARCH_HAS_SYSCALL_WRAPPER is selected, no prototype is defines for
    sys_ni_syscall(). Since we need to treat it differently for in-kernel
    calls and the syscall tables, the prototype is defined as-required.
    
    The wrappers are largely the same as their x86 counterparts, but
    simplified as we don't have a variety of compat calling conventions that
    require separate stubs. Unlike x86, we have some zero-argument compat
    syscalls, and must define COMPAT_SYSCALL_DEFINE0() to ensure that these
    are also given an __arm64_compat_sys_ prefix.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Dominik Brodowski <linux@dominikbrodowski.net>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 7ff2d7b9f517..be00c85794db 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -15,6 +15,8 @@
 
 long compat_arm_syscall(struct pt_regs *regs);
 
+long sys_ni_syscall(void);
+
 asmlinkage long do_ni_syscall(struct pt_regs *regs)
 {
 #ifdef CONFIG_COMPAT
@@ -31,8 +33,7 @@ asmlinkage long do_ni_syscall(struct pt_regs *regs)
 
 static long __invoke_syscall(struct pt_regs *regs, syscall_fn_t syscall_fn)
 {
-	return syscall_fn(regs->regs[0], regs->regs[1], regs->regs[2],
-			  regs->regs[3], regs->regs[4], regs->regs[5]);
+	return syscall_fn(regs);
 }
 
 static void invoke_syscall(struct pt_regs *regs, unsigned int scno,

commit 3b7142752e4bee153df6db4a76ca104ef0d7c0b4
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:45 2018 +0100

    arm64: convert native/compat syscall entry to C
    
    Now that the syscall invocation logic is in C, we can migrate the rest
    of the syscall entry logic over, so that the entry assembly needn't look
    at the register values at all.
    
    The SVE reset across syscall logic now unconditionally clears TIF_SVE,
    but sve_user_disable() will only write back to CPACR_EL1 when SVE is
    actually enabled.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Dave Martin <dave.martin@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 82098e6f6aa3..7ff2d7b9f517 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -8,8 +8,10 @@
 #include <linux/syscalls.h>
 
 #include <asm/daifflags.h>
+#include <asm/fpsimd.h>
 #include <asm/syscall.h>
 #include <asm/thread_info.h>
+#include <asm/unistd.h>
 
 long compat_arm_syscall(struct pt_regs *regs);
 
@@ -58,8 +60,8 @@ static inline bool has_syscall_work(unsigned long flags)
 int syscall_trace_enter(struct pt_regs *regs);
 void syscall_trace_exit(struct pt_regs *regs);
 
-asmlinkage void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
-			       const syscall_fn_t syscall_table[])
+static void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
+			   const syscall_fn_t syscall_table[])
 {
 	unsigned long flags = current_thread_info()->flags;
 
@@ -96,3 +98,34 @@ asmlinkage void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
 trace_exit:
 	syscall_trace_exit(regs);
 }
+
+static inline void sve_user_discard(void)
+{
+	if (!system_supports_sve())
+		return;
+
+	clear_thread_flag(TIF_SVE);
+
+	/*
+	 * task_fpsimd_load() won't be called to update CPACR_EL1 in
+	 * ret_to_user unless TIF_FOREIGN_FPSTATE is still set, which only
+	 * happens if a context switch or kernel_neon_begin() or context
+	 * modification (sigreturn, ptrace) intervenes.
+	 * So, ensure that CPACR_EL1 is already correct for the fast-path case.
+	 */
+	sve_user_disable();
+}
+
+asmlinkage void el0_svc_handler(struct pt_regs *regs)
+{
+	sve_user_discard();
+	el0_svc_common(regs, regs->regs[8], __NR_syscalls, sys_call_table);
+}
+
+#ifdef CONFIG_COMPAT
+asmlinkage void el0_svc_compat_handler(struct pt_regs *regs)
+{
+	el0_svc_common(regs, regs->regs[7], __NR_compat_syscalls,
+		       compat_sys_call_table);
+}
+#endif

commit f37099b6992a0b818c7b51b899e435f4006a9f90
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:44 2018 +0100

    arm64: convert syscall trace logic to C
    
    Currently syscall tracing is a tricky assembly state machine, which can
    be rather difficult to follow, and even harder to modify. Before we
    start fiddling with it for pt_regs syscalls, let's convert it to C.
    
    This is not intended to have any functional change.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
index 93d36f22647e..82098e6f6aa3 100644
--- a/arch/arm64/kernel/syscall.c
+++ b/arch/arm64/kernel/syscall.c
@@ -1,11 +1,15 @@
 // SPDX-License-Identifier: GPL-2.0
 
+#include <linux/compiler.h>
+#include <linux/context_tracking.h>
 #include <linux/errno.h>
 #include <linux/nospec.h>
 #include <linux/ptrace.h>
 #include <linux/syscalls.h>
 
+#include <asm/daifflags.h>
 #include <asm/syscall.h>
+#include <asm/thread_info.h>
 
 long compat_arm_syscall(struct pt_regs *regs);
 
@@ -29,9 +33,9 @@ static long __invoke_syscall(struct pt_regs *regs, syscall_fn_t syscall_fn)
 			  regs->regs[3], regs->regs[4], regs->regs[5]);
 }
 
-asmlinkage void invoke_syscall(struct pt_regs *regs, unsigned int scno,
-			       unsigned int sc_nr,
-			       const syscall_fn_t syscall_table[])
+static void invoke_syscall(struct pt_regs *regs, unsigned int scno,
+			   unsigned int sc_nr,
+			   const syscall_fn_t syscall_table[])
 {
 	long ret;
 
@@ -45,3 +49,50 @@ asmlinkage void invoke_syscall(struct pt_regs *regs, unsigned int scno,
 
 	regs->regs[0] = ret;
 }
+
+static inline bool has_syscall_work(unsigned long flags)
+{
+	return unlikely(flags & _TIF_SYSCALL_WORK);
+}
+
+int syscall_trace_enter(struct pt_regs *regs);
+void syscall_trace_exit(struct pt_regs *regs);
+
+asmlinkage void el0_svc_common(struct pt_regs *regs, int scno, int sc_nr,
+			       const syscall_fn_t syscall_table[])
+{
+	unsigned long flags = current_thread_info()->flags;
+
+	regs->orig_x0 = regs->regs[0];
+	regs->syscallno = scno;
+
+	local_daif_restore(DAIF_PROCCTX);
+	user_exit();
+
+	if (has_syscall_work(flags)) {
+		/* set default errno for user-issued syscall(-1) */
+		if (scno == NO_SYSCALL)
+			regs->regs[0] = -ENOSYS;
+		scno = syscall_trace_enter(regs);
+		if (scno == NO_SYSCALL)
+			goto trace_exit;
+	}
+
+	invoke_syscall(regs, scno, sc_nr, syscall_table);
+
+	/*
+	 * The tracing status may have changed under our feet, so we have to
+	 * check again. However, if we were tracing entry, then we always trace
+	 * exit regardless, as the old entry assembly did.
+	 */
+	if (!has_syscall_work(flags) && !IS_ENABLED(CONFIG_DEBUG_RSEQ)) {
+		local_daif_mask();
+		flags = current_thread_info()->flags;
+		if (!has_syscall_work(flags))
+			return;
+		local_daif_restore(DAIF_PROCCTX);
+	}
+
+trace_exit:
+	syscall_trace_exit(regs);
+}

commit 4141c857fd09dbed480f021b3eece4f46c653161
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Jul 11 14:56:43 2018 +0100

    arm64: convert raw syscall invocation to C
    
    As a first step towards invoking syscalls with a pt_regs argument,
    convert the raw syscall invocation logic to C. We end up with a bit more
    register shuffling, but the unified invocation logic means we can unify
    the tracing paths, too.
    
    Previously, assembly had to open-code calls to ni_sys() when the system
    call number was out-of-bounds for the relevant syscall table. This case
    is now handled by invoke_syscall(), and the assembly no longer need to
    handle this case explicitly. This allows the tracing paths to be
    simplified and unified, as we no longer need the __ni_sys_trace path and
    the __sys_trace_return label.
    
    This only converts the invocation of the syscall. The rest of the
    syscall triage and tracing is left in assembly for now, and will be
    converted in subsequent patches.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
new file mode 100644
index 000000000000..93d36f22647e
--- /dev/null
+++ b/arch/arm64/kernel/syscall.c
@@ -0,0 +1,47 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#include <linux/errno.h>
+#include <linux/nospec.h>
+#include <linux/ptrace.h>
+#include <linux/syscalls.h>
+
+#include <asm/syscall.h>
+
+long compat_arm_syscall(struct pt_regs *regs);
+
+asmlinkage long do_ni_syscall(struct pt_regs *regs)
+{
+#ifdef CONFIG_COMPAT
+	long ret;
+	if (is_compat_task()) {
+		ret = compat_arm_syscall(regs);
+		if (ret != -ENOSYS)
+			return ret;
+	}
+#endif
+
+	return sys_ni_syscall();
+}
+
+static long __invoke_syscall(struct pt_regs *regs, syscall_fn_t syscall_fn)
+{
+	return syscall_fn(regs->regs[0], regs->regs[1], regs->regs[2],
+			  regs->regs[3], regs->regs[4], regs->regs[5]);
+}
+
+asmlinkage void invoke_syscall(struct pt_regs *regs, unsigned int scno,
+			       unsigned int sc_nr,
+			       const syscall_fn_t syscall_table[])
+{
+	long ret;
+
+	if (scno < sc_nr) {
+		syscall_fn_t syscall_fn;
+		syscall_fn = syscall_table[array_index_nospec(scno, sc_nr)];
+		ret = __invoke_syscall(regs, syscall_fn);
+	} else {
+		ret = do_ni_syscall(regs);
+	}
+
+	regs->regs[0] = ret;
+}
