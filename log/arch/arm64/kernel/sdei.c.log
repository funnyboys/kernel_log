commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit 69ea03b56ed2c7189ccd0b5910ad39f3cad1df21
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 19 09:46:47 2020 +0100

    hardirq/nmi: Allow nested nmi_enter()
    
    Since there are already a number of sites (ARM64, PowerPC) that effectively
    nest nmi_enter(), make the primitive support this before adding even more.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Will Deacon <will@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lkml.kernel.org/r/20200505134100.864179229@linutronix.de

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index d6259dac62b6..e396e69e33a1 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -251,22 +251,12 @@ asmlinkage __kprobes notrace unsigned long
 __sdei_handler(struct pt_regs *regs, struct sdei_registered_event *arg)
 {
 	unsigned long ret;
-	bool do_nmi_exit = false;
 
-	/*
-	 * nmi_enter() deals with printk() re-entrance and use of RCU when
-	 * RCU believed this CPU was idle. Because critical events can
-	 * interrupt normal events, we may already be in_nmi().
-	 */
-	if (!in_nmi()) {
-		nmi_enter();
-		do_nmi_exit = true;
-	}
+	nmi_enter();
 
 	ret = _sdei_handler(regs, arg);
 
-	if (do_nmi_exit)
-		nmi_exit();
+	nmi_exit();
 
 	return ret;
 }

commit bd4298c72b56d7faf0ee3671739f3a704a962d0f
Author: Yunfeng Ye <yeyunfeng@huawei.com>
Date:   Fri May 8 11:15:45 2020 +0800

    arm64: stacktrace: Factor out some common code into on_stack()
    
    There are some common codes for stack checking, so factors it out into
    the function on_stack().
    
    No functional change.
    
    Signed-off-by: Yunfeng Ye <yeyunfeng@huawei.com>
    Link: https://lore.kernel.org/r/07b3b0e6-3f58-4fed-07ea-7d17b7508948@huawei.com
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index d6259dac62b6..3afed808b474 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -95,19 +95,7 @@ static bool on_sdei_normal_stack(unsigned long sp, struct stack_info *info)
 	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_normal_ptr);
 	unsigned long high = low + SDEI_STACK_SIZE;
 
-	if (!low)
-		return false;
-
-	if (sp < low || sp >= high)
-		return false;
-
-	if (info) {
-		info->low = low;
-		info->high = high;
-		info->type = STACK_TYPE_SDEI_NORMAL;
-	}
-
-	return true;
+	return on_stack(sp, low, high, STACK_TYPE_SDEI_NORMAL, info);
 }
 
 static bool on_sdei_critical_stack(unsigned long sp, struct stack_info *info)
@@ -115,19 +103,7 @@ static bool on_sdei_critical_stack(unsigned long sp, struct stack_info *info)
 	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_critical_ptr);
 	unsigned long high = low + SDEI_STACK_SIZE;
 
-	if (!low)
-		return false;
-
-	if (sp < low || sp >= high)
-		return false;
-
-	if (info) {
-		info->low = low;
-		info->high = high;
-		info->type = STACK_TYPE_SDEI_CRITICAL;
-	}
-
-	return true;
+	return on_stack(sp, low, high, STACK_TYPE_SDEI_CRITICAL, info);
 }
 
 bool _on_sdei_stack(unsigned long sp, struct stack_info *info)

commit e6ea46511b1ae8c4491904c79411fcd29139af14
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Aug 9 14:22:44 2019 +0100

    firmware: arm_sdei: use common SMCCC_CONDUIT_*
    
    Now that we have common definitions for SMCCC conduits, move the SDEI
    code over to them, and remove the SDEI-specific definitions.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: James Morse <james.morse@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index ea94cf8f9dc6..d6259dac62b6 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -2,6 +2,7 @@
 // Copyright (C) 2017 Arm Ltd.
 #define pr_fmt(fmt) "sdei: " fmt
 
+#include <linux/arm-smccc.h>
 #include <linux/arm_sdei.h>
 #include <linux/hardirq.h>
 #include <linux/irqflags.h>
@@ -161,7 +162,7 @@ unsigned long sdei_arch_get_entry_point(int conduit)
 			return 0;
 	}
 
-	sdei_exit_mode = (conduit == CONDUIT_HVC) ? SDEI_EXIT_HVC : SDEI_EXIT_SMC;
+	sdei_exit_mode = (conduit == SMCCC_CONDUIT_HVC) ? SDEI_EXIT_HVC : SDEI_EXIT_SMC;
 
 #ifdef CONFIG_UNMAP_KERNEL_AT_EL0
 	if (arm64_kernel_unmapped_at_el0()) {

commit 1c41860864c8ae0387ef7d44f0000e99cbb2e06d
Author: Wei Li <liwei391@huawei.com>
Date:   Mon Apr 1 11:55:57 2019 +0800

    arm64: fix wrong check of on_sdei_stack in nmi context
    
    When doing unwind_frame() in the context of pseudo nmi (need enable
    CONFIG_ARM64_PSEUDO_NMI), reaching the bottom of the stack (fp == 0,
    pc != 0), function on_sdei_stack() will return true while the sdei acpi
    table is not inited in fact. This will cause a "NULL pointer dereference"
    oops when going on.
    
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Wei Li <liwei391@huawei.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index 5ba4465e44f0..ea94cf8f9dc6 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -94,6 +94,9 @@ static bool on_sdei_normal_stack(unsigned long sp, struct stack_info *info)
 	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_normal_ptr);
 	unsigned long high = low + SDEI_STACK_SIZE;
 
+	if (!low)
+		return false;
+
 	if (sp < low || sp >= high)
 		return false;
 
@@ -111,6 +114,9 @@ static bool on_sdei_critical_stack(unsigned long sp, struct stack_info *info)
 	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_critical_ptr);
 	unsigned long high = low + SDEI_STACK_SIZE;
 
+	if (!low)
+		return false;
+
 	if (sp < low || sp >= high)
 		return false;
 

commit eab1cecc1223378fee700ab6acebf0f39fa6da81
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jul 31 12:02:18 2018 +0100

    arm64: sdei: Mark sdei stack helper functions as static
    
    The SDEI stack helper functions are only used by _on_sdei_stack() and
    refer to symbols (e.g. sdei_stack_normal_ptr) that are only defined if
    CONFIG_VMAP_STACK=y.
    
    Mark these functions as static, so we don't run into errors at link-time
    due to references to undefined symbols. Stick all the parameters onto
    the same line whilst we're passing through.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index a94a868f0532..5ba4465e44f0 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -89,8 +89,7 @@ static int init_sdei_stacks(void)
 	return err;
 }
 
-bool on_sdei_normal_stack(unsigned long sp,
-			struct stack_info *info)
+static bool on_sdei_normal_stack(unsigned long sp, struct stack_info *info)
 {
 	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_normal_ptr);
 	unsigned long high = low + SDEI_STACK_SIZE;
@@ -107,8 +106,7 @@ bool on_sdei_normal_stack(unsigned long sp,
 	return true;
 }
 
-bool on_sdei_critical_stack(unsigned long sp,
-			struct stack_info *info)
+static bool on_sdei_critical_stack(unsigned long sp, struct stack_info *info)
 {
 	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_critical_ptr);
 	unsigned long high = low + SDEI_STACK_SIZE;
@@ -125,8 +123,7 @@ bool on_sdei_critical_stack(unsigned long sp,
 	return true;
 }
 
-bool _on_sdei_stack(unsigned long sp,
-		struct stack_info *info)
+bool _on_sdei_stack(unsigned long sp, struct stack_info *info)
 {
 	if (!IS_ENABLED(CONFIG_VMAP_STACK))
 		return false;

commit 8a1ccfbc9e0256baafbbce85ccdb72ec89af2aab
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jul 20 14:41:53 2018 -0700

    arm64: Add stack information to on_accessible_stack
    
    In preparation for enabling the stackleak plugin on arm64,
    we need a way to get the bounds of the current stack. Extend
    on_accessible_stack to get this information.
    
    Acked-by: Alexander Popov <alex.popov@linux.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    [will: folded in fix for allmodconfig build breakage w/ sdei]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index 6b8d90d5ceae..a94a868f0532 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -13,6 +13,7 @@
 #include <asm/mmu.h>
 #include <asm/ptrace.h>
 #include <asm/sections.h>
+#include <asm/stacktrace.h>
 #include <asm/sysreg.h>
 #include <asm/vmap_stack.h>
 
@@ -88,23 +89,55 @@ static int init_sdei_stacks(void)
 	return err;
 }
 
-bool _on_sdei_stack(unsigned long sp)
+bool on_sdei_normal_stack(unsigned long sp,
+			struct stack_info *info)
 {
-	unsigned long low, high;
+	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_normal_ptr);
+	unsigned long high = low + SDEI_STACK_SIZE;
 
-	if (!IS_ENABLED(CONFIG_VMAP_STACK))
+	if (sp < low || sp >= high)
 		return false;
 
-	low = (unsigned long)raw_cpu_read(sdei_stack_critical_ptr);
-	high = low + SDEI_STACK_SIZE;
+	if (info) {
+		info->low = low;
+		info->high = high;
+		info->type = STACK_TYPE_SDEI_NORMAL;
+	}
 
-	if (low <= sp && sp < high)
+	return true;
+}
+
+bool on_sdei_critical_stack(unsigned long sp,
+			struct stack_info *info)
+{
+	unsigned long low = (unsigned long)raw_cpu_read(sdei_stack_critical_ptr);
+	unsigned long high = low + SDEI_STACK_SIZE;
+
+	if (sp < low || sp >= high)
+		return false;
+
+	if (info) {
+		info->low = low;
+		info->high = high;
+		info->type = STACK_TYPE_SDEI_CRITICAL;
+	}
+
+	return true;
+}
+
+bool _on_sdei_stack(unsigned long sp,
+		struct stack_info *info)
+{
+	if (!IS_ENABLED(CONFIG_VMAP_STACK))
+		return false;
+
+	if (on_sdei_critical_stack(sp, info))
 		return true;
 
-	low = (unsigned long)raw_cpu_read(sdei_stack_normal_ptr);
-	high = low + SDEI_STACK_SIZE;
+	if (on_sdei_normal_stack(sp, info))
+		return true;
 
-	return (low <= sp && sp < high);
+	return false;
 }
 
 unsigned long sdei_arch_get_entry_point(int conduit)

commit 79e9aa59dc29a995921fb01e64cd36b73cf5abe0
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:18 2018 +0000

    arm64: sdei: Add trampoline code for remapping the kernel
    
    When CONFIG_UNMAP_KERNEL_AT_EL0 is set the SDEI entry point and the rest
    of the kernel may be unmapped when we take an event. If this may be the
    case, use an entry trampoline that can switch to the kernel page tables.
    
    We can't use the provided PSTATE to determine whether to switch page
    tables as we may have interrupted the kernel's entry trampoline, (or a
    normal-priority event that interrupted the kernel's entry trampoline).
    Instead test for a user ASID in ttbr1_el1.
    
    Save a value in regs->addr_limit to indicate whether we need to restore
    the original ASID when returning from this event. This value is only used
    by do_page_fault(), which we don't call with the SDEI regs.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
index f9dffacaa5d6..6b8d90d5ceae 100644
--- a/arch/arm64/kernel/sdei.c
+++ b/arch/arm64/kernel/sdei.c
@@ -10,7 +10,9 @@
 
 #include <asm/alternative.h>
 #include <asm/kprobes.h>
+#include <asm/mmu.h>
 #include <asm/ptrace.h>
+#include <asm/sections.h>
 #include <asm/sysreg.h>
 #include <asm/vmap_stack.h>
 
@@ -124,7 +126,18 @@ unsigned long sdei_arch_get_entry_point(int conduit)
 	}
 
 	sdei_exit_mode = (conduit == CONDUIT_HVC) ? SDEI_EXIT_HVC : SDEI_EXIT_SMC;
-	return (unsigned long)__sdei_asm_handler;
+
+#ifdef CONFIG_UNMAP_KERNEL_AT_EL0
+	if (arm64_kernel_unmapped_at_el0()) {
+		unsigned long offset;
+
+		offset = (unsigned long)__sdei_asm_entry_trampoline -
+			 (unsigned long)__entry_tramp_text_start;
+		return TRAMP_VALIAS + offset;
+	} else
+#endif /* CONFIG_UNMAP_KERNEL_AT_EL0 */
+		return (unsigned long)__sdei_asm_handler;
+
 }
 
 /*
@@ -138,11 +151,14 @@ static __kprobes unsigned long _sdei_handler(struct pt_regs *regs,
 {
 	u32 mode;
 	int i, err = 0;
-	const int clobbered_registers = 4;
+	int clobbered_registers = 4;
 	u64 elr = read_sysreg(elr_el1);
 	u32 kernel_mode = read_sysreg(CurrentEL) | 1;	/* +SPSel */
 	unsigned long vbar = read_sysreg(vbar_el1);
 
+	if (arm64_kernel_unmapped_at_el0())
+		clobbered_registers++;
+
 	/* Retrieve the missing registers values */
 	for (i = 0; i < clobbered_registers; i++) {
 		/* from within the handler, this call always succeeds */

commit f5df26961853d6809d704cedcaf082c57f635a64
Author: James Morse <james.morse@arm.com>
Date:   Mon Jan 8 15:38:12 2018 +0000

    arm64: kernel: Add arch-specific SDEI entry code and CPU masking
    
    The Software Delegated Exception Interface (SDEI) is an ARM standard
    for registering callbacks from the platform firmware into the OS.
    This is typically used to implement RAS notifications.
    
    Such notifications enter the kernel at the registered entry-point
    with the register values of the interrupted CPU context. Because this
    is not a CPU exception, it cannot reuse the existing entry code.
    (crucially we don't implicitly know which exception level we interrupted),
    
    Add the entry point to entry.S to set us up for calling into C code. If
    the event interrupted code that had interrupts masked, we always return
    to that location. Otherwise we pretend this was an IRQ, and use SDEI's
    complete_and_resume call to return to vbar_el1 + offset.
    
    This allows the kernel to deliver signals to user space processes. For
    KVM this triggers the world switch, a quick spin round vcpu_run, then
    back into the guest, unless there are pending signals.
    
    Add sdei_mask_local_cpu() calls to the smp_send_stop() code, this covers
    the panic() code-path, which doesn't invoke cpuhotplug notifiers.
    
    Because we can interrupt entry-from/exit-to another EL, we can't trust the
    value in sp_el0 or x29, even if we interrupted the kernel, in this case
    the code in entry.S will save/restore sp_el0 and use the value in
    __entry_task.
    
    When we have VMAP stacks we can interrupt the stack-overflow test, which
    stirs x0 into sp, meaning we have to have our own VMAP stacks. For now
    these are allocated when we probe the interface. Future patches will add
    refcounting hooks to allow the arch code to allocate them lazily.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/sdei.c b/arch/arm64/kernel/sdei.c
new file mode 100644
index 000000000000..f9dffacaa5d6
--- /dev/null
+++ b/arch/arm64/kernel/sdei.c
@@ -0,0 +1,219 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2017 Arm Ltd.
+#define pr_fmt(fmt) "sdei: " fmt
+
+#include <linux/arm_sdei.h>
+#include <linux/hardirq.h>
+#include <linux/irqflags.h>
+#include <linux/sched/task_stack.h>
+#include <linux/uaccess.h>
+
+#include <asm/alternative.h>
+#include <asm/kprobes.h>
+#include <asm/ptrace.h>
+#include <asm/sysreg.h>
+#include <asm/vmap_stack.h>
+
+unsigned long sdei_exit_mode;
+
+/*
+ * VMAP'd stacks checking for stack overflow on exception using sp as a scratch
+ * register, meaning SDEI has to switch to its own stack. We need two stacks as
+ * a critical event may interrupt a normal event that has just taken a
+ * synchronous exception, and is using sp as scratch register. For a critical
+ * event interrupting a normal event, we can't reliably tell if we were on the
+ * sdei stack.
+ * For now, we allocate stacks when the driver is probed.
+ */
+DECLARE_PER_CPU(unsigned long *, sdei_stack_normal_ptr);
+DECLARE_PER_CPU(unsigned long *, sdei_stack_critical_ptr);
+
+#ifdef CONFIG_VMAP_STACK
+DEFINE_PER_CPU(unsigned long *, sdei_stack_normal_ptr);
+DEFINE_PER_CPU(unsigned long *, sdei_stack_critical_ptr);
+#endif
+
+static void _free_sdei_stack(unsigned long * __percpu *ptr, int cpu)
+{
+	unsigned long *p;
+
+	p = per_cpu(*ptr, cpu);
+	if (p) {
+		per_cpu(*ptr, cpu) = NULL;
+		vfree(p);
+	}
+}
+
+static void free_sdei_stacks(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		_free_sdei_stack(&sdei_stack_normal_ptr, cpu);
+		_free_sdei_stack(&sdei_stack_critical_ptr, cpu);
+	}
+}
+
+static int _init_sdei_stack(unsigned long * __percpu *ptr, int cpu)
+{
+	unsigned long *p;
+
+	p = arch_alloc_vmap_stack(SDEI_STACK_SIZE, cpu_to_node(cpu));
+	if (!p)
+		return -ENOMEM;
+	per_cpu(*ptr, cpu) = p;
+
+	return 0;
+}
+
+static int init_sdei_stacks(void)
+{
+	int cpu;
+	int err = 0;
+
+	for_each_possible_cpu(cpu) {
+		err = _init_sdei_stack(&sdei_stack_normal_ptr, cpu);
+		if (err)
+			break;
+		err = _init_sdei_stack(&sdei_stack_critical_ptr, cpu);
+		if (err)
+			break;
+	}
+
+	if (err)
+		free_sdei_stacks();
+
+	return err;
+}
+
+bool _on_sdei_stack(unsigned long sp)
+{
+	unsigned long low, high;
+
+	if (!IS_ENABLED(CONFIG_VMAP_STACK))
+		return false;
+
+	low = (unsigned long)raw_cpu_read(sdei_stack_critical_ptr);
+	high = low + SDEI_STACK_SIZE;
+
+	if (low <= sp && sp < high)
+		return true;
+
+	low = (unsigned long)raw_cpu_read(sdei_stack_normal_ptr);
+	high = low + SDEI_STACK_SIZE;
+
+	return (low <= sp && sp < high);
+}
+
+unsigned long sdei_arch_get_entry_point(int conduit)
+{
+	/*
+	 * SDEI works between adjacent exception levels. If we booted at EL1 we
+	 * assume a hypervisor is marshalling events. If we booted at EL2 and
+	 * dropped to EL1 because we don't support VHE, then we can't support
+	 * SDEI.
+	 */
+	if (is_hyp_mode_available() && !is_kernel_in_hyp_mode()) {
+		pr_err("Not supported on this hardware/boot configuration\n");
+		return 0;
+	}
+
+	if (IS_ENABLED(CONFIG_VMAP_STACK)) {
+		if (init_sdei_stacks())
+			return 0;
+	}
+
+	sdei_exit_mode = (conduit == CONDUIT_HVC) ? SDEI_EXIT_HVC : SDEI_EXIT_SMC;
+	return (unsigned long)__sdei_asm_handler;
+}
+
+/*
+ * __sdei_handler() returns one of:
+ *  SDEI_EV_HANDLED -  success, return to the interrupted context.
+ *  SDEI_EV_FAILED  -  failure, return this error code to firmare.
+ *  virtual-address -  success, return to this address.
+ */
+static __kprobes unsigned long _sdei_handler(struct pt_regs *regs,
+					     struct sdei_registered_event *arg)
+{
+	u32 mode;
+	int i, err = 0;
+	const int clobbered_registers = 4;
+	u64 elr = read_sysreg(elr_el1);
+	u32 kernel_mode = read_sysreg(CurrentEL) | 1;	/* +SPSel */
+	unsigned long vbar = read_sysreg(vbar_el1);
+
+	/* Retrieve the missing registers values */
+	for (i = 0; i < clobbered_registers; i++) {
+		/* from within the handler, this call always succeeds */
+		sdei_api_event_context(i, &regs->regs[i]);
+	}
+
+	/*
+	 * We didn't take an exception to get here, set PAN. UAO will be cleared
+	 * by sdei_event_handler()s set_fs(USER_DS) call.
+	 */
+	__uaccess_enable_hw_pan();
+
+	err = sdei_event_handler(regs, arg);
+	if (err)
+		return SDEI_EV_FAILED;
+
+	if (elr != read_sysreg(elr_el1)) {
+		/*
+		 * We took a synchronous exception from the SDEI handler.
+		 * This could deadlock, and if you interrupt KVM it will
+		 * hyp-panic instead.
+		 */
+		pr_warn("unsafe: exception during handler\n");
+	}
+
+	mode = regs->pstate & (PSR_MODE32_BIT | PSR_MODE_MASK);
+
+	/*
+	 * If we interrupted the kernel with interrupts masked, we always go
+	 * back to wherever we came from.
+	 */
+	if (mode == kernel_mode && !interrupts_enabled(regs))
+		return SDEI_EV_HANDLED;
+
+	/*
+	 * Otherwise, we pretend this was an IRQ. This lets user space tasks
+	 * receive signals before we return to them, and KVM to invoke it's
+	 * world switch to do the same.
+	 *
+	 * See DDI0487B.a Table D1-7 'Vector offsets from vector table base
+	 * address'.
+	 */
+	if (mode == kernel_mode)
+		return vbar + 0x280;
+	else if (mode & PSR_MODE32_BIT)
+		return vbar + 0x680;
+
+	return vbar + 0x480;
+}
+
+
+asmlinkage __kprobes notrace unsigned long
+__sdei_handler(struct pt_regs *regs, struct sdei_registered_event *arg)
+{
+	unsigned long ret;
+	bool do_nmi_exit = false;
+
+	/*
+	 * nmi_enter() deals with printk() re-entrance and use of RCU when
+	 * RCU believed this CPU was idle. Because critical events can
+	 * interrupt normal events, we may already be in_nmi().
+	 */
+	if (!in_nmi()) {
+		nmi_enter();
+		do_nmi_exit = true;
+	}
+
+	ret = _sdei_handler(regs, arg);
+
+	if (do_nmi_exit)
+		nmi_exit();
+
+	return ret;
+}
