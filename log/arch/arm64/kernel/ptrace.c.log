commit d83ee6e3e75db6f518ef2b0858f163849f2ddeb7
Author: Will Deacon <will@kernel.org>
Date:   Fri Jul 10 13:20:57 2020 +0100

    arm64: ptrace: Use NO_SYSCALL instead of -1 in syscall_trace_enter()
    
    Setting a system call number of -1 is special, as it indicates that the
    current system call should be skipped.
    
    Use NO_SYSCALL instead of -1 when checking for this scenario, which is
    different from the -1 returned due to a seccomp failure.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Keno Fischer <keno@juliacomputing.com>
    Cc: Luis Machado <luis.machado@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 89fbee3991a2..1e02e98e68dd 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1856,12 +1856,12 @@ int syscall_trace_enter(struct pt_regs *regs)
 	if (flags & (_TIF_SYSCALL_EMU | _TIF_SYSCALL_TRACE)) {
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
 		if (!in_syscall(regs) || (flags & _TIF_SYSCALL_EMU))
-			return -1;
+			return NO_SYSCALL;
 	}
 
 	/* Do the secure computing after ptrace; failures should be fast. */
 	if (secure_computing() == -1)
-		return -1;
+		return NO_SYSCALL;
 
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);

commit 59ee987ea47caff8c1e7ba4b89932c6900a35d0c
Author: Will Deacon <will@kernel.org>
Date:   Thu Jul 2 21:14:20 2020 +0100

    arm64: ptrace: Add a comment describing our syscall entry/exit trap ABI
    
    Our tracehook logic for syscall entry/exit raises a SIGTRAP back to the
    tracer following a ptrace request such as PTRACE_SYSCALL. As part of this
    procedure, we clobber the reported value of one of the tracee's general
    purpose registers (x7 for native tasks, r12 for compat) to indicate
    whether the stop occurred on syscall entry or exit. This is a slightly
    unfortunate ABI, as it prevents the tracer from accessing the real
    register value and is at odds with other similar stops such as seccomp
    traps.
    
    Since we're stuck with this ABI, expand the comment in our tracehook
    logic to acknowledge the issue and describe the behaviour in more detail.
    
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Luis Machado <luis.machado@linaro.org>
    Reported-by: Keno Fischer <keno@juliacomputing.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 22f9053b55b6..89fbee3991a2 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1811,8 +1811,20 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 	unsigned long saved_reg;
 
 	/*
-	 * A scratch register (ip(r12) on AArch32, x7 on AArch64) is
-	 * used to denote syscall entry/exit:
+	 * We have some ABI weirdness here in the way that we handle syscall
+	 * exit stops because we indicate whether or not the stop has been
+	 * signalled from syscall entry or syscall exit by clobbering a general
+	 * purpose register (ip/r12 for AArch32, x7 for AArch64) in the tracee
+	 * and restoring its old value after the stop. This means that:
+	 *
+	 * - Any writes by the tracer to this register during the stop are
+	 *   ignored/discarded.
+	 *
+	 * - The actual value of the register is not available during the stop,
+	 *   so the tracer cannot save it and restore it later.
+	 *
+	 * - Syscall stops behave differently to seccomp and pseudo-step traps
+	 *   (the latter do not nobble any registers).
 	 */
 	regno = (is_compat_task() ? 12 : 7);
 	saved_reg = regs->regs[regno];

commit 3a5a4366cecc25daa300b9a9174f7fdd352b9068
Author: Will Deacon <will@kernel.org>
Date:   Thu Feb 13 12:06:26 2020 +0000

    arm64: ptrace: Override SPSR.SS when single-stepping is enabled
    
    Luis reports that, when reverse debugging with GDB, single-step does not
    function as expected on arm64:
    
      | I've noticed, under very specific conditions, that a PTRACE_SINGLESTEP
      | request by GDB won't execute the underlying instruction. As a consequence,
      | the PC doesn't move, but we return a SIGTRAP just like we would for a
      | regular successful PTRACE_SINGLESTEP request.
    
    The underlying problem is that when the CPU register state is restored
    as part of a reverse step, the SPSR.SS bit is cleared and so the hardware
    single-step state can transition to the "active-pending" state, causing
    an unexpected step exception to be taken immediately if a step operation
    is attempted.
    
    In hindsight, we probably shouldn't have exposed SPSR.SS in the pstate
    accessible by the GPR regset, but it's a bit late for that now. Instead,
    simply prevent userspace from configuring the bit to a value which is
    inconsistent with the TIF_SINGLESTEP state for the task being traced.
    
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Keno Fischer <keno@juliacomputing.com>
    Link: https://lore.kernel.org/r/1eed6d69-d53d-9657-1fc9-c089be07f98c@linaro.org
    Reported-by: Luis Machado <luis.machado@linaro.org>
    Tested-by: Luis Machado <luis.machado@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 057d4aa1af4d..22f9053b55b6 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1947,8 +1947,8 @@ static int valid_native_regs(struct user_pt_regs *regs)
  */
 int valid_user_regs(struct user_pt_regs *regs, struct task_struct *task)
 {
-	if (!test_tsk_thread_flag(task, TIF_SINGLESTEP))
-		regs->pstate &= ~DBG_SPSR_SS;
+	/* https://lore.kernel.org/lkml/20191118131525.GA4180@willie-the-truck */
+	user_regs_reset_single_step(regs, task);
 
 	if (is_compat_thread(task_thread_info(task)))
 		return valid_compat_regs(regs);

commit ac2081cdc4d99c57f219c1a6171526e0fa0a6fff
Author: Will Deacon <will@kernel.org>
Date:   Thu Jul 2 21:16:20 2020 +0100

    arm64: ptrace: Consistently use pseudo-singlestep exceptions
    
    Although the arm64 single-step state machine can be fast-forwarded in
    cases where we wish to generate a SIGTRAP without actually executing an
    instruction, this has two major limitations outside of simply skipping
    an instruction due to emulation.
    
    1. Stepping out of a ptrace signal stop into a signal handler where
       SIGTRAP is blocked. Fast-forwarding the stepping state machine in
       this case will result in a forced SIGTRAP, with the handler reset to
       SIG_DFL.
    
    2. The hardware implicitly fast-forwards the state machine when executing
       an SVC instruction for issuing a system call. This can interact badly
       with subsequent ptrace stops signalled during the execution of the
       system call (e.g. SYSCALL_EXIT or seccomp traps), as they may corrupt
       the stepping state by updating the PSTATE for the tracee.
    
    Resolve both of these issues by injecting a pseudo-singlestep exception
    on entry to a signal handler and also on return to userspace following a
    system call.
    
    Cc: <stable@vger.kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Luis Machado <luis.machado@linaro.org>
    Reported-by: Keno Fischer <keno@juliacomputing.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 68b7f34a08f5..057d4aa1af4d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1818,12 +1818,23 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 	saved_reg = regs->regs[regno];
 	regs->regs[regno] = dir;
 
-	if (dir == PTRACE_SYSCALL_EXIT)
+	if (dir == PTRACE_SYSCALL_ENTER) {
+		if (tracehook_report_syscall_entry(regs))
+			forget_syscall(regs);
+		regs->regs[regno] = saved_reg;
+	} else if (!test_thread_flag(TIF_SINGLESTEP)) {
 		tracehook_report_syscall_exit(regs, 0);
-	else if (tracehook_report_syscall_entry(regs))
-		forget_syscall(regs);
+		regs->regs[regno] = saved_reg;
+	} else {
+		regs->regs[regno] = saved_reg;
 
-	regs->regs[regno] = saved_reg;
+		/*
+		 * Signal a pseudo-step exception since we are stepping but
+		 * tracer modifications to the registers may have rewound the
+		 * state machine.
+		 */
+		tracehook_report_syscall_exit(regs, 1);
+	}
 }
 
 int syscall_trace_enter(struct pt_regs *regs)
@@ -1851,12 +1862,14 @@ int syscall_trace_enter(struct pt_regs *regs)
 
 void syscall_trace_exit(struct pt_regs *regs)
 {
+	unsigned long flags = READ_ONCE(current_thread_info()->flags);
+
 	audit_syscall_exit(regs);
 
-	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
+	if (flags & _TIF_SYSCALL_TRACEPOINT)
 		trace_sys_exit(regs, regs_return_value(regs));
 
-	if (test_thread_flag(TIF_SYSCALL_TRACE))
+	if (flags & (_TIF_SYSCALL_TRACE | _TIF_SINGLESTEP))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);
 
 	rseq_syscall(regs);

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 76790a5f2a0d..68b7f34a08f5 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -34,7 +34,6 @@
 #include <asm/cpufeature.h>
 #include <asm/debug-monitors.h>
 #include <asm/fpsimd.h>
-#include <asm/pgtable.h>
 #include <asm/pointer_auth.h>
 #include <asm/stacktrace.h>
 #include <asm/syscall.h>

commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit 1cf6022bd9161081215028203919c33fcfa6debb
Author: Keno Fischer <keno@juliacomputing.com>
Date:   Fri May 15 18:22:53 2020 -0400

    arm64: Fix PTRACE_SYSEMU semantics
    
    Quoth the man page:
    ```
           If the tracee was restarted by PTRACE_SYSCALL or PTRACE_SYSEMU, the
           tracee enters syscall-enter-stop just prior to entering any system
           call (which will not be executed if the restart was using
           PTRACE_SYSEMU, regardless of any change made to registers at this
           point or how the tracee is restarted after this stop).
    ```
    
    The parenthetical comment is currently true on x86 and powerpc,
    but not currently true on arm64. arm64 re-checks the _TIF_SYSCALL_EMU
    flag after the syscall entry ptrace stop. However, at this point,
    it reflects which method was used to re-start the syscall
    at the entry stop, rather than the method that was used to reach it.
    Fix that by recording the original flag before performing the ptrace
    stop, bringing the behavior in line with documentation and x86/powerpc.
    
    Fixes: f086f67485c5 ("arm64: ptrace: add support for syscall emulation")
    Cc: <stable@vger.kernel.org> # 5.3.x-
    Signed-off-by: Keno Fischer <keno@juliacomputing.com>
    Acked-by: Will Deacon <will@kernel.org>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Tested-by: Bin Lu <Bin.Lu@arm.com>
    [catalin.marinas@arm.com: moved 'flags' bit masking]
    [catalin.marinas@arm.com: changed 'flags' type to unsigned long]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index b3d3005d9515..e7b01904f180 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1829,10 +1829,11 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 
 int syscall_trace_enter(struct pt_regs *regs)
 {
-	if (test_thread_flag(TIF_SYSCALL_TRACE) ||
-		test_thread_flag(TIF_SYSCALL_EMU)) {
+	unsigned long flags = READ_ONCE(current_thread_info()->flags);
+
+	if (flags & (_TIF_SYSCALL_EMU | _TIF_SYSCALL_TRACE)) {
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
-		if (!in_syscall(regs) || test_thread_flag(TIF_SYSCALL_EMU))
+		if (!in_syscall(regs) || (flags & _TIF_SYSCALL_EMU))
 			return -1;
 	}
 

commit 80e4e561321595d2e5f4a173e8cf8d8432078995
Merge: 6a8b55ed4056 5d1b631c773f
Author: Will Deacon <will@kernel.org>
Date:   Tue May 5 15:15:58 2020 +0100

    Merge branch 'for-next/bti-user' into for-next/bti
    
    Merge in user support for Branch Target Identification, which narrowly
    missed the cut for 5.7 after a late ABI concern.
    
    * for-next/bti-user:
      arm64: bti: Document behaviour for dynamically linked binaries
      arm64: elf: Fix allnoconfig kernel build with !ARCH_USE_GNU_PROPERTY
      arm64: BTI: Add Kconfig entry for userspace BTI
      mm: smaps: Report arm64 guarded pages in smaps
      arm64: mm: Display guarded pages in ptdump
      KVM: arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: BTI: Reset BTYPE when skipping emulated instructions
      arm64: traps: Shuffle code to eliminate forward declarations
      arm64: unify native/compat instruction skipping
      arm64: BTI: Decode BYTPE bits when printing PSTATE
      arm64: elf: Enable BTI at exec based on ELF program properties
      elf: Allow arch to tweak initial mmap prot flags
      arm64: Basic Branch Target Identification support
      ELF: Add ELF program property parsing support
      ELF: UAPI and Kconfig additions for ELF program properties

commit 91a1b6ccff323e60615e3118eceb2d8cbc4f69ab
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Fri Mar 13 14:34:50 2020 +0530

    arm64: rename ptrauth key structures to be user-specific
    
    We currently enable ptrauth for userspace, but do not use it within the
    kernel. We're going to enable it for the kernel, and will need to manage
    a separate set of ptrauth keys for the kernel.
    
    We currently keep all 5 keys in struct ptrauth_keys. However, as the
    kernel will only need to use 1 key, it is a bit wasteful to allocate a
    whole ptrauth_keys struct for every thread.
    
    Therefore, a subsequent patch will define a separate struct, with only 1
    key, for the kernel. In preparation for that, rename the existing struct
    (and associated macros and functions) to reflect that they are specific
    to userspace.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Vincenzo Frascino <Vincenzo.Frascino@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    [Amit: Re-positioned the patch to reduce the diff]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index cd6e5fa48b9c..b3d3005d9515 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -999,7 +999,7 @@ static struct ptrauth_key pac_key_from_user(__uint128_t ukey)
 }
 
 static void pac_address_keys_to_user(struct user_pac_address_keys *ukeys,
-				     const struct ptrauth_keys *keys)
+				     const struct ptrauth_keys_user *keys)
 {
 	ukeys->apiakey = pac_key_to_user(&keys->apia);
 	ukeys->apibkey = pac_key_to_user(&keys->apib);
@@ -1007,7 +1007,7 @@ static void pac_address_keys_to_user(struct user_pac_address_keys *ukeys,
 	ukeys->apdbkey = pac_key_to_user(&keys->apdb);
 }
 
-static void pac_address_keys_from_user(struct ptrauth_keys *keys,
+static void pac_address_keys_from_user(struct ptrauth_keys_user *keys,
 				       const struct user_pac_address_keys *ukeys)
 {
 	keys->apia = pac_key_from_user(ukeys->apiakey);
@@ -1021,7 +1021,7 @@ static int pac_address_keys_get(struct task_struct *target,
 				unsigned int pos, unsigned int count,
 				void *kbuf, void __user *ubuf)
 {
-	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct ptrauth_keys_user *keys = &target->thread.keys_user;
 	struct user_pac_address_keys user_keys;
 
 	if (!system_supports_address_auth())
@@ -1038,7 +1038,7 @@ static int pac_address_keys_set(struct task_struct *target,
 				unsigned int pos, unsigned int count,
 				const void *kbuf, const void __user *ubuf)
 {
-	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct ptrauth_keys_user *keys = &target->thread.keys_user;
 	struct user_pac_address_keys user_keys;
 	int ret;
 
@@ -1056,12 +1056,12 @@ static int pac_address_keys_set(struct task_struct *target,
 }
 
 static void pac_generic_keys_to_user(struct user_pac_generic_keys *ukeys,
-				     const struct ptrauth_keys *keys)
+				     const struct ptrauth_keys_user *keys)
 {
 	ukeys->apgakey = pac_key_to_user(&keys->apga);
 }
 
-static void pac_generic_keys_from_user(struct ptrauth_keys *keys,
+static void pac_generic_keys_from_user(struct ptrauth_keys_user *keys,
 				       const struct user_pac_generic_keys *ukeys)
 {
 	keys->apga = pac_key_from_user(ukeys->apgakey);
@@ -1072,7 +1072,7 @@ static int pac_generic_keys_get(struct task_struct *target,
 				unsigned int pos, unsigned int count,
 				void *kbuf, void __user *ubuf)
 {
-	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct ptrauth_keys_user *keys = &target->thread.keys_user;
 	struct user_pac_generic_keys user_keys;
 
 	if (!system_supports_generic_auth())
@@ -1089,7 +1089,7 @@ static int pac_generic_keys_set(struct task_struct *target,
 				unsigned int pos, unsigned int count,
 				const void *kbuf, const void __user *ubuf)
 {
-	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct ptrauth_keys_user *keys = &target->thread.keys_user;
 	struct user_pac_generic_keys user_keys;
 	int ret;
 

commit 8ef8f360cf30be12382f89ff48a57fbbd9b31c14
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Mon Mar 16 16:50:45 2020 +0000

    arm64: Basic Branch Target Identification support
    
    This patch adds the bare minimum required to expose the ARMv8.5
    Branch Target Identification feature to userspace.
    
    By itself, this does _not_ automatically enable BTI for any initial
    executable pages mapped by execve().  This will come later, but for
    now it should be possible to enable BTI manually on those pages by
    using mprotect() from within the target process.
    
    Other arches already using the generic mman.h are already using
    0x10 for arch-specific prot flags, so we use that for PROT_BTI
    here.
    
    For consistency, signal handler entry points in BTI guarded pages
    are required to be annotated as such, just like any other function.
    This blocks a relatively minor attack vector, but comforming
    userspace will have the annotations anyway, so we may as well
    enforce them.
    
    Signed-off-by: Mark Brown <broonie@kernel.org>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index cd6e5fa48b9c..fd8ac7cf68e7 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1874,7 +1874,7 @@ void syscall_trace_exit(struct pt_regs *regs)
  */
 #define SPSR_EL1_AARCH64_RES0_BITS \
 	(GENMASK_ULL(63, 32) | GENMASK_ULL(27, 25) | GENMASK_ULL(23, 22) | \
-	 GENMASK_ULL(20, 13) | GENMASK_ULL(11, 10) | GENMASK_ULL(5, 5))
+	 GENMASK_ULL(20, 13) | GENMASK_ULL(5, 5))
 #define SPSR_EL1_AARCH32_RES0_BITS \
 	(GENMASK_ULL(63, 32) | GENMASK_ULL(22, 22) | GENMASK_ULL(20, 20))
 

commit c9d66999f064947e6b577ceacc1eb2fbca6a8d3c
Author: Suzuki K Poulose <suzuki.poulose@arm.com>
Date:   Mon Jan 13 23:30:21 2020 +0000

    arm64: ptrace: nofpsimd: Fail FP/SIMD regset operations
    
    When fp/simd is not supported on the system, fail the operations
    of FP/SIMD regsets.
    
    Fixes: 82e0191a1aa11abf ("arm64: Support systems without FP/ASIMD")
    Cc: Will Deacon <will@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Ard Biesheuvel <ardb@kernel.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6771c399d40c..cd6e5fa48b9c 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -615,6 +615,13 @@ static int gpr_set(struct task_struct *target, const struct user_regset *regset,
 	return 0;
 }
 
+static int fpr_active(struct task_struct *target, const struct user_regset *regset)
+{
+	if (!system_supports_fpsimd())
+		return -ENODEV;
+	return regset->n;
+}
+
 /*
  * TODO: update fp accessors for lazy context switching (sync/flush hwstate)
  */
@@ -637,6 +644,9 @@ static int fpr_get(struct task_struct *target, const struct user_regset *regset,
 		   unsigned int pos, unsigned int count,
 		   void *kbuf, void __user *ubuf)
 {
+	if (!system_supports_fpsimd())
+		return -EINVAL;
+
 	if (target == current)
 		fpsimd_preserve_current_state();
 
@@ -676,6 +686,9 @@ static int fpr_set(struct task_struct *target, const struct user_regset *regset,
 {
 	int ret;
 
+	if (!system_supports_fpsimd())
+		return -EINVAL;
+
 	ret = __fpr_set(target, regset, pos, count, kbuf, ubuf, 0);
 	if (ret)
 		return ret;
@@ -1134,6 +1147,7 @@ static const struct user_regset aarch64_regsets[] = {
 		 */
 		.size = sizeof(u32),
 		.align = sizeof(u32),
+		.active = fpr_active,
 		.get = fpr_get,
 		.set = fpr_set
 	},
@@ -1348,6 +1362,9 @@ static int compat_vfp_get(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret, vregs_end_pos;
 
+	if (!system_supports_fpsimd())
+		return -EINVAL;
+
 	uregs = &target->thread.uw.fpsimd_state;
 
 	if (target == current)
@@ -1381,6 +1398,9 @@ static int compat_vfp_set(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret, vregs_end_pos;
 
+	if (!system_supports_fpsimd())
+		return -EINVAL;
+
 	uregs = &target->thread.uw.fpsimd_state;
 
 	vregs_end_pos = VFP_STATE_SIZE - sizeof(compat_ulong_t);
@@ -1438,6 +1458,7 @@ static const struct user_regset aarch32_regsets[] = {
 		.n = VFP_STATE_SIZE / sizeof(compat_ulong_t),
 		.size = sizeof(compat_ulong_t),
 		.align = sizeof(compat_ulong_t),
+		.active = fpr_active,
 		.get = compat_vfp_get,
 		.set = compat_vfp_set
 	},

commit fefad9ef58ffc228f7b78b667c2aea8267503350
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Tue Sep 24 08:44:20 2019 +0200

    seccomp: simplify secure_computing()
    
    Afaict, the struct seccomp_data argument to secure_computing() is unused
    by all current callers. So let's remove it.
    The argument was added in [1]. It was added because having the arch
    supply the syscall arguments used to be faster than having it done by
    secure_computing() (cf. Andy's comment in [2]). This is not true anymore
    though.
    
    /* References */
    [1]: 2f275de5d1ed ("seccomp: Add a seccomp_data parameter secure_computing()")
    [2]: https://lore.kernel.org/r/CALCETrU_fs_At-hTpr231kpaAd0z7xJN4ku-DvzhRU6cvcJA_w@mail.gmail.com
    
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-um@lists.infradead.org
    Cc: x86@kernel.org
    Acked-by: Borislav Petkov <bp@suse.de>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Link: https://lore.kernel.org/r/20190924064420.6353-1-christian.brauner@ubuntu.com
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 21176d02e21a..6771c399d40c 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1816,7 +1816,7 @@ int syscall_trace_enter(struct pt_regs *regs)
 	}
 
 	/* Do the secure computing after ptrace; failures should be fast. */
-	if (secure_computing(NULL) == -1)
+	if (secure_computing() == -1)
 		return -1;
 
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))

commit 08f103b9a9502974109fab47ea35ca8542c4e57a
Author: Julien Grall <julien.grall@arm.com>
Date:   Wed Aug 7 11:34:45 2019 +0100

    arm64/ptrace: Fix typoes in sve_set() comment
    
    The ptrace trace SVE flags are prefixed with SVE_PT_*. Update the
    comment accordingly.
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Julien Grall <julien.grall@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 3cf3b135027e..21176d02e21a 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -870,7 +870,7 @@ static int sve_set(struct task_struct *target,
 		goto out;
 
 	/*
-	 * Apart from PT_SVE_REGS_MASK, all PT_SVE_* flags are consumed by
+	 * Apart from SVE_PT_REGS_MASK, all SVE_PT_* flags are consumed by
 	 * sve_set_vector_length(), which will also validate them for us:
 	 */
 	ret = sve_set_vector_length(target, header.vl,

commit dfd437a257924484b144ee750e60affc95562c6d
Merge: 0ecfebd2b524 0c61efd322b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 09:54:55 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - arm64 support for syscall emulation via PTRACE_SYSEMU{,_SINGLESTEP}
    
     - Wire up VM_FLUSH_RESET_PERMS for arm64, allowing the core code to
       manage the permissions of executable vmalloc regions more strictly
    
     - Slight performance improvement by keeping softirqs enabled while
       touching the FPSIMD/SVE state (kernel_neon_begin/end)
    
     - Expose a couple of ARMv8.5 features to user (HWCAP): CondM (new
       XAFLAG and AXFLAG instructions for floating point comparison flags
       manipulation) and FRINT (rounding floating point numbers to integers)
    
     - Re-instate ARM64_PSEUDO_NMI support which was previously marked as
       BROKEN due to some bugs (now fixed)
    
     - Improve parking of stopped CPUs and implement an arm64-specific
       panic_smp_self_stop() to avoid warning on not being able to stop
       secondary CPUs during panic
    
     - perf: enable the ARM Statistical Profiling Extensions (SPE) on ACPI
       platforms
    
     - perf: DDR performance monitor support for iMX8QXP
    
     - cache_line_size() can now be set from DT or ACPI/PPTT if provided to
       cope with a system cache info not exposed via the CPUID registers
    
     - Avoid warning on hardware cache line size greater than
       ARCH_DMA_MINALIGN if the system is fully coherent
    
     - arm64 do_page_fault() and hugetlb cleanups
    
     - Refactor set_pte_at() to avoid redundant READ_ONCE(*ptep)
    
     - Ignore ACPI 5.1 FADTs reported as 5.0 (infer from the
       'arm_boot_flags' introduced in 5.1)
    
     - CONFIG_RANDOMIZE_BASE now enabled in defconfig
    
     - Allow the selection of ARM64_MODULE_PLTS, currently only done via
       RANDOMIZE_BASE (and an erratum workaround), allowing modules to spill
       over into the vmalloc area
    
     - Make ZONE_DMA32 configurable
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      perf: arm_spe: Enable ACPI/Platform automatic module loading
      arm_pmu: acpi: spe: Add initial MADT/SPE probing
      ACPI/PPTT: Add function to return ACPI 6.3 Identical tokens
      ACPI/PPTT: Modify node flag detection to find last IDENTICAL
      x86/entry: Simplify _TIF_SYSCALL_EMU handling
      arm64: rename dump_instr as dump_kernel_instr
      arm64/mm: Drop [PTE|PMD]_TYPE_FAULT
      arm64: Implement panic_smp_self_stop()
      arm64: Improve parking of stopped CPUs
      arm64: Expose FRINT capabilities to userspace
      arm64: Expose ARMv8.5 CondM capability to userspace
      arm64: defconfig: enable CONFIG_RANDOMIZE_BASE
      arm64: ARM64_MODULES_PLTS must depend on MODULES
      arm64: bpf: do not allocate executable memory
      arm64/kprobes: set VM_FLUSH_RESET_PERMS on kprobe instruction pages
      arm64/mm: wire up CONFIG_ARCH_HAS_SET_DIRECT_MAP
      arm64: module: create module allocations without exec permissions
      arm64: Allow user selection of ARM64_MODULE_PLTS
      acpi/arm64: ignore 5.1 FADTs that are reported as 5.0
      arm64: Allow selecting Pseudo-NMI again
      ...

commit caab277b1de0a22b675c4c95fc7b285ec2eb5bf5
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 3 07:44:50 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 234
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 503 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190602204653.811534538@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index b82e0a9b3da3..da2441d7b066 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Based on arch/arm/kernel/ptrace.c
  *
@@ -5,18 +6,6 @@
  * edited by Linus Torvalds
  * ARM modifications Copyright (C) 2000 Russell King
  * Copyright (C) 2012 ARM Ltd.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
 #include <linux/audit.h>

commit f086f67485c5c126bcec4b0e96ac7319a2e59ab8
Author: Sudeep Holla <sudeep.holla@arm.com>
Date:   Thu May 23 10:06:18 2019 +0100

    arm64: ptrace: add support for syscall emulation
    
    Add PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP support on arm64.
    We don't need any special handling for PTRACE_SYSEMU_SINGLESTEP.
    
    It's quite difficult to generalize handling PTRACE_SYSEMU cross
    architectures and avoid calls to tracehook_report_syscall_entry twice.
    Different architecture have different mechanism to indicate NO_SYSCALL
    and trying to generalise adds more code for no gain.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index b82e0a9b3da3..9353355cb91a 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1819,8 +1819,12 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 
 int syscall_trace_enter(struct pt_regs *regs)
 {
-	if (test_thread_flag(TIF_SYSCALL_TRACE))
+	if (test_thread_flag(TIF_SYSCALL_TRACE) ||
+		test_thread_flag(TIF_SYSCALL_EMU)) {
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
+		if (!in_syscall(regs) || test_thread_flag(TIF_SYSCALL_EMU))
+			return -1;
+	}
 
 	/* Do the secure computing after ptrace; failures should be fast. */
 	if (secure_computing(NULL) == -1)

commit 3d8dfe75ef69f4dd4ba35c09b20a5aa58b4a5078
Merge: d60752629693 b855b58ac1b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Mar 10 10:17:23 2019 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Catalin Marinas:
    
     - Pseudo NMI support for arm64 using GICv3 interrupt priorities
    
     - uaccess macros clean-up (unsafe user accessors also merged but
       reverted, waiting for objtool support on arm64)
    
     - ptrace regsets for Pointer Authentication (ARMv8.3) key management
    
     - inX() ordering w.r.t. delay() on arm64 and riscv (acks in place by
       the riscv maintainers)
    
     - arm64/perf updates: PMU bindings converted to json-schema, unused
       variable and misleading comment removed
    
     - arm64/debug fixes to ensure checking of the triggering exception
       level and to avoid the propagation of the UNKNOWN FAR value into the
       si_code for debug signals
    
     - Workaround for Fujitsu A64FX erratum 010001
    
     - lib/raid6 ARM NEON optimisations
    
     - NR_CPUS now defaults to 256 on arm64
    
     - Minor clean-ups (documentation/comments, Kconfig warning, unused
       asm-offsets, clang warnings)
    
     - MAINTAINERS update for list information to the ARM64 ACPI entry
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (54 commits)
      arm64: mmu: drop paging_init comments
      arm64: debug: Ensure debug handlers check triggering exception level
      arm64: debug: Don't propagate UNKNOWN FAR into si_code for debug signals
      Revert "arm64: uaccess: Implement unsafe accessors"
      arm64: avoid clang warning about self-assignment
      arm64: Kconfig.platforms: fix warning unmet direct dependencies
      lib/raid6: arm: optimize away a mask operation in NEON recovery routine
      lib/raid6: use vdupq_n_u8 to avoid endianness warnings
      arm64: io: Hook up __io_par() for inX() ordering
      riscv: io: Update __io_[p]ar() macros to take an argument
      asm-generic/io: Pass result of I/O accessor to __io_[p]ar()
      arm64: Add workaround for Fujitsu A64FX erratum 010001
      arm64: Rename get_thread_info()
      arm64: Remove documentation about TIF_USEDFPU
      arm64: irqflags: Fix clang build warnings
      arm64: Enable the support of pseudo-NMIs
      arm64: Skip irqflags tracing for NMI in IRQs disabled context
      arm64: Skip preemption when exiting an NMI
      arm64: Handle serror in NMI context
      irqchip/gic-v3: Allow interrupts to be set as pseudo-NMI
      ...

commit f54dada8274643e3ff4436df0ea124aeedc43cae
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Feb 15 16:34:27 2019 +0000

    arm64: fix SSBS sanitization
    
    In valid_user_regs() we treat SSBS as a RES0 bit, and consequently it is
    unexpectedly cleared when we restore a sigframe or fiddle with GPRs via
    ptrace.
    
    This patch fixes valid_user_regs() to account for this, updating the
    function to refer to the latest ARM ARM (ARM DDI 0487D.a). For AArch32
    tasks, SSBS appears in bit 23 of SPSR_EL1, matching its position in the
    AArch32-native PSR format, and we don't need to translate it as we have
    to for DIT.
    
    There are no other bit assignments that we need to account for today.
    As the recent documentation describes the DIT bit, we can drop our
    comment regarding DIT.
    
    While removing SSBS from the RES0 masks, existing inconsistent
    whitespace is corrected.
    
    Fixes: d71be2b6c0e19180 ("arm64: cpufeature: Detect SSBS and advertise to userspace")
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 9dce33b0e260..ddaea0fd2fa4 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1702,19 +1702,20 @@ void syscall_trace_exit(struct pt_regs *regs)
 }
 
 /*
- * SPSR_ELx bits which are always architecturally RES0 per ARM DDI 0487C.a
- * We also take into account DIT (bit 24), which is not yet documented, and
- * treat PAN and UAO as RES0 bits, as they are meaningless at EL0, and may be
- * allocated an EL0 meaning in future.
+ * SPSR_ELx bits which are always architecturally RES0 per ARM DDI 0487D.a.
+ * We permit userspace to set SSBS (AArch64 bit 12, AArch32 bit 23) which is
+ * not described in ARM DDI 0487D.a.
+ * We treat PAN and UAO as RES0 bits, as they are meaningless at EL0, and may
+ * be allocated an EL0 meaning in future.
  * Userspace cannot use these until they have an architectural meaning.
  * Note that this follows the SPSR_ELx format, not the AArch32 PSR format.
  * We also reserve IL for the kernel; SS is handled dynamically.
  */
 #define SPSR_EL1_AARCH64_RES0_BITS \
-	(GENMASK_ULL(63,32) | GENMASK_ULL(27, 25) | GENMASK_ULL(23, 22) | \
-	 GENMASK_ULL(20, 10) | GENMASK_ULL(5, 5))
+	(GENMASK_ULL(63, 32) | GENMASK_ULL(27, 25) | GENMASK_ULL(23, 22) | \
+	 GENMASK_ULL(20, 13) | GENMASK_ULL(11, 10) | GENMASK_ULL(5, 5))
 #define SPSR_EL1_AARCH32_RES0_BITS \
-	(GENMASK_ULL(63,32) | GENMASK_ULL(23, 22) | GENMASK_ULL(20,20))
+	(GENMASK_ULL(63, 32) | GENMASK_ULL(22, 22) | GENMASK_ULL(20, 20))
 
 static int valid_compat_regs(struct user_pt_regs *regs)
 {

commit d0a060be573bfbf8753a15dca35497db5e968bb0
Author: Kristina Martsenko <kristina.martsenko@arm.com>
Date:   Wed Jan 30 12:02:44 2019 +0000

    arm64: add ptrace regsets for ptrauth key management
    
    Add two new ptrace regsets, which can be used to request and change the
    pointer authentication keys of a thread. NT_ARM_PACA_KEYS gives access
    to the instruction/data address keys, and NT_ARM_PACG_KEYS to the
    generic authentication key. The keys are also part of the core dump file
    of the process.
    
    The regsets are only exposed if the kernel is compiled with
    CONFIG_CHECKPOINT_RESTORE=y, as the only intended use case is
    checkpointing and restoring processes that are using pointer
    authentication. (This can be changed later if there are other use
    cases.)
    
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 9dce33b0e260..a86413be5a2d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -979,6 +979,131 @@ static int pac_mask_get(struct task_struct *target,
 
 	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, &uregs, 0, -1);
 }
+
+#ifdef CONFIG_CHECKPOINT_RESTORE
+static __uint128_t pac_key_to_user(const struct ptrauth_key *key)
+{
+	return (__uint128_t)key->hi << 64 | key->lo;
+}
+
+static struct ptrauth_key pac_key_from_user(__uint128_t ukey)
+{
+	struct ptrauth_key key = {
+		.lo = (unsigned long)ukey,
+		.hi = (unsigned long)(ukey >> 64),
+	};
+
+	return key;
+}
+
+static void pac_address_keys_to_user(struct user_pac_address_keys *ukeys,
+				     const struct ptrauth_keys *keys)
+{
+	ukeys->apiakey = pac_key_to_user(&keys->apia);
+	ukeys->apibkey = pac_key_to_user(&keys->apib);
+	ukeys->apdakey = pac_key_to_user(&keys->apda);
+	ukeys->apdbkey = pac_key_to_user(&keys->apdb);
+}
+
+static void pac_address_keys_from_user(struct ptrauth_keys *keys,
+				       const struct user_pac_address_keys *ukeys)
+{
+	keys->apia = pac_key_from_user(ukeys->apiakey);
+	keys->apib = pac_key_from_user(ukeys->apibkey);
+	keys->apda = pac_key_from_user(ukeys->apdakey);
+	keys->apdb = pac_key_from_user(ukeys->apdbkey);
+}
+
+static int pac_address_keys_get(struct task_struct *target,
+				const struct user_regset *regset,
+				unsigned int pos, unsigned int count,
+				void *kbuf, void __user *ubuf)
+{
+	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct user_pac_address_keys user_keys;
+
+	if (!system_supports_address_auth())
+		return -EINVAL;
+
+	pac_address_keys_to_user(&user_keys, keys);
+
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf,
+				   &user_keys, 0, -1);
+}
+
+static int pac_address_keys_set(struct task_struct *target,
+				const struct user_regset *regset,
+				unsigned int pos, unsigned int count,
+				const void *kbuf, const void __user *ubuf)
+{
+	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct user_pac_address_keys user_keys;
+	int ret;
+
+	if (!system_supports_address_auth())
+		return -EINVAL;
+
+	pac_address_keys_to_user(&user_keys, keys);
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,
+				 &user_keys, 0, -1);
+	if (ret)
+		return ret;
+	pac_address_keys_from_user(keys, &user_keys);
+
+	return 0;
+}
+
+static void pac_generic_keys_to_user(struct user_pac_generic_keys *ukeys,
+				     const struct ptrauth_keys *keys)
+{
+	ukeys->apgakey = pac_key_to_user(&keys->apga);
+}
+
+static void pac_generic_keys_from_user(struct ptrauth_keys *keys,
+				       const struct user_pac_generic_keys *ukeys)
+{
+	keys->apga = pac_key_from_user(ukeys->apgakey);
+}
+
+static int pac_generic_keys_get(struct task_struct *target,
+				const struct user_regset *regset,
+				unsigned int pos, unsigned int count,
+				void *kbuf, void __user *ubuf)
+{
+	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct user_pac_generic_keys user_keys;
+
+	if (!system_supports_generic_auth())
+		return -EINVAL;
+
+	pac_generic_keys_to_user(&user_keys, keys);
+
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf,
+				   &user_keys, 0, -1);
+}
+
+static int pac_generic_keys_set(struct task_struct *target,
+				const struct user_regset *regset,
+				unsigned int pos, unsigned int count,
+				const void *kbuf, const void __user *ubuf)
+{
+	struct ptrauth_keys *keys = &target->thread.keys_user;
+	struct user_pac_generic_keys user_keys;
+	int ret;
+
+	if (!system_supports_generic_auth())
+		return -EINVAL;
+
+	pac_generic_keys_to_user(&user_keys, keys);
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,
+				 &user_keys, 0, -1);
+	if (ret)
+		return ret;
+	pac_generic_keys_from_user(keys, &user_keys);
+
+	return 0;
+}
+#endif /* CONFIG_CHECKPOINT_RESTORE */
 #endif /* CONFIG_ARM64_PTR_AUTH */
 
 enum aarch64_regset {
@@ -995,6 +1120,10 @@ enum aarch64_regset {
 #endif
 #ifdef CONFIG_ARM64_PTR_AUTH
 	REGSET_PAC_MASK,
+#ifdef CONFIG_CHECKPOINT_RESTORE
+	REGSET_PACA_KEYS,
+	REGSET_PACG_KEYS,
+#endif
 #endif
 };
 
@@ -1074,6 +1203,24 @@ static const struct user_regset aarch64_regsets[] = {
 		.get = pac_mask_get,
 		/* this cannot be set dynamically */
 	},
+#ifdef CONFIG_CHECKPOINT_RESTORE
+	[REGSET_PACA_KEYS] = {
+		.core_note_type = NT_ARM_PACA_KEYS,
+		.n = sizeof(struct user_pac_address_keys) / sizeof(__uint128_t),
+		.size = sizeof(__uint128_t),
+		.align = sizeof(__uint128_t),
+		.get = pac_address_keys_get,
+		.set = pac_address_keys_set,
+	},
+	[REGSET_PACG_KEYS] = {
+		.core_note_type = NT_ARM_PACG_KEYS,
+		.n = sizeof(struct user_pac_generic_keys) / sizeof(__uint128_t),
+		.size = sizeof(__uint128_t),
+		.align = sizeof(__uint128_t),
+		.get = pac_generic_keys_get,
+		.set = pac_generic_keys_set,
+	},
+#endif
 #endif
 };
 

commit ec6e822d1a22d0eef1d1fa260dff751dba9a4258
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Dec 7 18:39:26 2018 +0000

    arm64: expose user PAC bit positions via ptrace
    
    When pointer authentication is in use, data/instruction pointers have a
    number of PAC bits inserted into them. The number and position of these
    bits depends on the configured TCR_ELx.TxSZ and whether tagging is
    enabled. ARMv8.3 allows tagging to differ for instruction and data
    pointers.
    
    For userspace debuggers to unwind the stack and/or to follow pointer
    chains, they need to be able to remove the PAC bits before attempting to
    use a pointer.
    
    This patch adds a new structure with masks describing the location of
    the PAC bits in userspace instruction and data pointers (i.e. those
    addressable via TTBR0), which userspace can query via PTRACE_GETREGSET.
    By clearing these bits from pointers (and replacing them with the value
    of bit 55), userspace can acquire the PAC-less versions.
    
    This new regset is exposed when the kernel is built with (user) pointer
    authentication support, and the address authentication feature is
    enabled. Otherwise, the regset is hidden.
    
    Reviewed-by: Richard Henderson <richard.henderson@linaro.org>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    [will: Fix to use vabits_user instead of VA_BITS and rename macro]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 1710a2d01669..9dce33b0e260 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -46,6 +46,7 @@
 #include <asm/debug-monitors.h>
 #include <asm/fpsimd.h>
 #include <asm/pgtable.h>
+#include <asm/pointer_auth.h>
 #include <asm/stacktrace.h>
 #include <asm/syscall.h>
 #include <asm/traps.h>
@@ -956,6 +957,30 @@ static int sve_set(struct task_struct *target,
 
 #endif /* CONFIG_ARM64_SVE */
 
+#ifdef CONFIG_ARM64_PTR_AUTH
+static int pac_mask_get(struct task_struct *target,
+			const struct user_regset *regset,
+			unsigned int pos, unsigned int count,
+			void *kbuf, void __user *ubuf)
+{
+	/*
+	 * The PAC bits can differ across data and instruction pointers
+	 * depending on TCR_EL1.TBID*, which we may make use of in future, so
+	 * we expose separate masks.
+	 */
+	unsigned long mask = ptrauth_user_pac_mask();
+	struct user_pac_mask uregs = {
+		.data_mask = mask,
+		.insn_mask = mask,
+	};
+
+	if (!system_supports_address_auth())
+		return -EINVAL;
+
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, &uregs, 0, -1);
+}
+#endif /* CONFIG_ARM64_PTR_AUTH */
+
 enum aarch64_regset {
 	REGSET_GPR,
 	REGSET_FPR,
@@ -968,6 +993,9 @@ enum aarch64_regset {
 #ifdef CONFIG_ARM64_SVE
 	REGSET_SVE,
 #endif
+#ifdef CONFIG_ARM64_PTR_AUTH
+	REGSET_PAC_MASK,
+#endif
 };
 
 static const struct user_regset aarch64_regsets[] = {
@@ -1037,6 +1065,16 @@ static const struct user_regset aarch64_regsets[] = {
 		.get_size = sve_get_size,
 	},
 #endif
+#ifdef CONFIG_ARM64_PTR_AUTH
+	[REGSET_PAC_MASK] = {
+		.core_note_type = NT_ARM_PAC_MASK,
+		.n = sizeof(struct user_pac_mask) / sizeof(u64),
+		.size = sizeof(u64),
+		.align = sizeof(u64),
+		.get = pac_mask_get,
+		/* this cannot be set dynamically */
+	},
+#endif
 };
 
 static const struct user_regset_view user_aarch64_view = {

commit f3a900b34101bb8df10b83f326b3af796c101a05
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:52:41 2018 +0200

    signal/arm64: Add and use arm64_force_sig_ptrace_errno_trap
    
    Add arm64_force_sig_ptrace_errno_trap for consistency with
    arm64_force_sig_fault and use it where appropriate.
    
    This adds the show_signal logic to the force_sig_errno_trap case,
    where it was apparently overlooked earlier.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 921267f59d0d..1710a2d01669 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -202,7 +202,9 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 				break;
 			}
 		}
-		force_sig_ptrace_errno_trap(si_errno, (void __user *)bkpt->trigger);
+		arm64_force_sig_ptrace_errno_trap(si_errno,
+						  (void __user *)bkpt->trigger,
+						  desc);
 	}
 #endif
 	arm64_force_sig_fault(SIGTRAP, TRAP_HWBKPT,

commit 2627f0347c682d03e8594a81083557d3d798f58f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:47:54 2018 +0200

    signal/arm64: In ptrace_hbptriggered name the signal description string
    
    This will let the description be reused shortly.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 7ab75e78aa08..921267f59d0d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -182,6 +182,7 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 				struct pt_regs *regs)
 {
 	struct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);
+	const char *desc = "Hardware breakpoint trap (ptrace)";
 
 #ifdef CONFIG_COMPAT
 	if (is_compat_task()) {
@@ -206,7 +207,7 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 #endif
 	arm64_force_sig_fault(SIGTRAP, TRAP_HWBKPT,
 			      (void __user *)(bkpt->trigger),
-			      "Hardware breakpoint trap (ptrace)");
+			      desc);
 }
 
 /*

commit feca355b3d8eba3a2cbca63c97a59a14681983f7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 10:26:57 2018 +0200

    signal/arm64: Add and use arm64_force_sig_fault where appropriate
    
    Wrap force_sig_fault with a helper that calls arm64_show_signal
    and call arm64_force_sig_fault where appropraite.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 20b68cb31ecb..7ab75e78aa08 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -182,13 +182,6 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 				struct pt_regs *regs)
 {
 	struct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);
-	siginfo_t info;
-
-	clear_siginfo(&info);
-	info.si_signo	= SIGTRAP;
-	info.si_errno	= 0;
-	info.si_code	= TRAP_HWBKPT;
-	info.si_addr	= (void __user *)(bkpt->trigger);
 
 #ifdef CONFIG_COMPAT
 	if (is_compat_task()) {
@@ -211,7 +204,9 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 		force_sig_ptrace_errno_trap(si_errno, (void __user *)bkpt->trigger);
 	}
 #endif
-	arm64_force_sig_info(&info, "Hardware breakpoint trap (ptrace)");
+	arm64_force_sig_fault(SIGTRAP, TRAP_HWBKPT,
+			      (void __user *)(bkpt->trigger),
+			      "Hardware breakpoint trap (ptrace)");
 }
 
 /*

commit 24b8f79dd8e036da618d158b4c0295208d478c5c
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Sep 22 00:38:41 2018 +0200

    signal/arm64: Remove unneeded tsk parameter from arm64_force_sig_info
    
    Every caller passes in current for tsk so there is no need to pass
    tsk.  Instead make tsk a local variable initialized to current.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6219486fa25f..20b68cb31ecb 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -211,7 +211,7 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 		force_sig_ptrace_errno_trap(si_errno, (void __user *)bkpt->trigger);
 	}
 #endif
-	arm64_force_sig_info(&info, "Hardware breakpoint trap (ptrace)", current);
+	arm64_force_sig_info(&info, "Hardware breakpoint trap (ptrace)");
 }
 
 /*

commit 8a1ccfbc9e0256baafbbce85ccdb72ec89af2aab
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jul 20 14:41:53 2018 -0700

    arm64: Add stack information to on_accessible_stack
    
    In preparation for enabling the stackleak plugin on arm64,
    we need a way to get the bounds of the current stack. Extend
    on_accessible_stack to get this information.
    
    Acked-by: Alexander Popov <alex.popov@linux.com>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    [will: folded in fix for allmodconfig build breakage w/ sdei]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 299758f9b9c4..6219486fa25f 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -132,7 +132,7 @@ static bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
 {
 	return ((addr & ~(THREAD_SIZE - 1))  ==
 		(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1))) ||
-		on_irq_stack(addr);
+		on_irq_stack(addr, NULL);
 }
 
 /**

commit 14d6e289a89780377f8bb09de8926d3c62d763cd
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jul 10 19:01:22 2018 +0100

    arm64: fix possible spectre-v1 write in ptrace_hbp_set_event()
    
    It's possible for userspace to control idx. Sanitize idx when using it
    as an array index, to inhibit the potential spectre-v1 write gadget.
    
    Found by smatch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 379b2b714193..299758f9b9c4 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -277,19 +277,22 @@ static int ptrace_hbp_set_event(unsigned int note_type,
 
 	switch (note_type) {
 	case NT_ARM_HW_BREAK:
-		if (idx < ARM_MAX_BRP) {
-			tsk->thread.debug.hbp_break[idx] = bp;
-			err = 0;
-		}
+		if (idx >= ARM_MAX_BRP)
+			goto out;
+		idx = array_index_nospec(idx, ARM_MAX_BRP);
+		tsk->thread.debug.hbp_break[idx] = bp;
+		err = 0;
 		break;
 	case NT_ARM_HW_WATCH:
-		if (idx < ARM_MAX_WRP) {
-			tsk->thread.debug.hbp_watch[idx] = bp;
-			err = 0;
-		}
+		if (idx >= ARM_MAX_WRP)
+			goto out;
+		idx = array_index_nospec(idx, ARM_MAX_WRP);
+		tsk->thread.debug.hbp_watch[idx] = bp;
+		err = 0;
 		break;
 	}
 
+out:
 	return err;
 }
 

commit 11527b3e0b577513ea318f01e392ccf7feb9d2a4
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jul 12 15:14:23 2018 +0100

    arm64: Drop asmlinkage qualifier from syscall_trace_{enter,exit}
    
    syscall_trace_{enter,exit} are only called from C code, so drop the
    asmlinkage qualifier from their definitions.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 489db3624606..379b2b714193 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1631,7 +1631,7 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 	regs->regs[regno] = saved_reg;
 }
 
-asmlinkage int syscall_trace_enter(struct pt_regs *regs)
+int syscall_trace_enter(struct pt_regs *regs)
 {
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
@@ -1649,7 +1649,7 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	return regs->syscallno;
 }
 
-asmlinkage void syscall_trace_exit(struct pt_regs *regs)
+void syscall_trace_exit(struct pt_regs *regs)
 {
 	audit_syscall_exit(regs);
 

commit 409d5db49867c20d226a1fc50a73d646bd733dae
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jun 20 14:46:50 2018 +0100

    arm64: rseq: Implement backend rseq calls and select HAVE_RSEQ
    
    Implement calls to rseq_signal_deliver, rseq_handle_notify_resume
    and rseq_syscall so that we can select HAVE_RSEQ on arm64.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 42ff28aa5a00..489db3624606 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1658,6 +1658,8 @@ asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);
+
+	rseq_syscall(regs);
 }
 
 /*

commit d64567f67835736d65086e9bfc41a19b2863c32e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 5 15:16:52 2018 +0100

    arm64: use PSR_AA32 definitions
    
    Some code cares about the SPSR_ELx format for exceptions taken from
    AArch32 to inspect or manipulate the SPSR_ELx value, which is already in
    the SPSR_ELx format, and not in the AArch32 PSR format.
    
    To separate these from cases where we care about the AArch32 PSR format,
    migrate these cases to use the PSR_AA32_* definitions rather than
    COMPAT_PSR_*.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 2c4b09f84dd0..42ff28aa5a00 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1681,15 +1681,15 @@ static int valid_compat_regs(struct user_pt_regs *regs)
 
 	if (!system_supports_mixed_endian_el0()) {
 		if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
-			regs->pstate |= COMPAT_PSR_E_BIT;
+			regs->pstate |= PSR_AA32_E_BIT;
 		else
-			regs->pstate &= ~COMPAT_PSR_E_BIT;
+			regs->pstate &= ~PSR_AA32_E_BIT;
 	}
 
 	if (user_mode(regs) && (regs->pstate & PSR_MODE32_BIT) &&
-	    (regs->pstate & COMPAT_PSR_A_BIT) == 0 &&
-	    (regs->pstate & COMPAT_PSR_I_BIT) == 0 &&
-	    (regs->pstate & COMPAT_PSR_F_BIT) == 0) {
+	    (regs->pstate & PSR_AA32_A_BIT) == 0 &&
+	    (regs->pstate & PSR_AA32_I_BIT) == 0 &&
+	    (regs->pstate & PSR_AA32_F_BIT) == 0) {
 		return 1;
 	}
 
@@ -1697,11 +1697,11 @@ static int valid_compat_regs(struct user_pt_regs *regs)
 	 * Force PSR to a valid 32-bit EL0t, preserving the same bits as
 	 * arch/arm.
 	 */
-	regs->pstate &= COMPAT_PSR_N_BIT | COMPAT_PSR_Z_BIT |
-			COMPAT_PSR_C_BIT | COMPAT_PSR_V_BIT |
-			COMPAT_PSR_Q_BIT | COMPAT_PSR_IT_MASK |
-			COMPAT_PSR_GE_MASK | COMPAT_PSR_E_BIT |
-			COMPAT_PSR_T_BIT;
+	regs->pstate &= PSR_AA32_N_BIT | PSR_AA32_Z_BIT |
+			PSR_AA32_C_BIT | PSR_AA32_V_BIT |
+			PSR_AA32_Q_BIT | PSR_AA32_IT_MASK |
+			PSR_AA32_GE_MASK | PSR_AA32_E_BIT |
+			PSR_AA32_T_BIT;
 	regs->pstate |= PSR_MODE32_BIT;
 
 	return 0;

commit 76fc52bd07d3e9cb708f1a50b60c825c96acd606
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 5 15:16:51 2018 +0100

    arm64: ptrace: map SPSR_ELx<->PSR for compat tasks
    
    The SPSR_ELx format for exceptions taken from AArch32 is slightly
    different to the AArch32 PSR format.
    
    Map between the two in the compat ptrace code.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 7206dc93a58fb764 ("arm64: Expose Arm v8.4 features")
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 5df75ab6a3c8..2c4b09f84dd0 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1076,6 +1076,7 @@ static int compat_gpr_get(struct task_struct *target,
 			break;
 		case 16:
 			reg = task_pt_regs(target)->pstate;
+			reg = pstate_to_compat_psr(reg);
 			break;
 		case 17:
 			reg = task_pt_regs(target)->orig_x0;
@@ -1143,6 +1144,7 @@ static int compat_gpr_set(struct task_struct *target,
 			newregs.pc = reg;
 			break;
 		case 16:
+			reg = compat_psr_to_pstate(reg);
 			newregs.pstate = reg;
 			break;
 		case 17:

commit 1265132127b63502d34e0f58c8bdef3a4dc927c2
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 5 15:16:49 2018 +0100

    arm64: don't zero DIT on signal return
    
    Currently valid_user_regs() treats SPSR_ELx.DIT as a RES0 bit, causing
    it to be zeroed upon exception return, rather than preserved. Thus, code
    relying on DIT will not function as expected, and may expose an
    unexpected timing sidechannel.
    
    Let's remove DIT from the set of RES0 bits, such that it is preserved.
    At the same time, the related comment is updated to better describe the
    situation, and to take into account the most recent documentation of
    SPSR_ELx, in ARM DDI 0487C.a.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 7206dc93a58fb764 ("arm64: Expose Arm v8.4 features")
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 5c338ce5a7fa..5df75ab6a3c8 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1659,15 +1659,19 @@ asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 }
 
 /*
- * Bits which are always architecturally RES0 per ARM DDI 0487A.h
+ * SPSR_ELx bits which are always architecturally RES0 per ARM DDI 0487C.a
+ * We also take into account DIT (bit 24), which is not yet documented, and
+ * treat PAN and UAO as RES0 bits, as they are meaningless at EL0, and may be
+ * allocated an EL0 meaning in future.
  * Userspace cannot use these until they have an architectural meaning.
+ * Note that this follows the SPSR_ELx format, not the AArch32 PSR format.
  * We also reserve IL for the kernel; SS is handled dynamically.
  */
 #define SPSR_EL1_AARCH64_RES0_BITS \
-	(GENMASK_ULL(63,32) | GENMASK_ULL(27, 22) | GENMASK_ULL(20, 10) | \
-	 GENMASK_ULL(5, 5))
+	(GENMASK_ULL(63,32) | GENMASK_ULL(27, 25) | GENMASK_ULL(23, 22) | \
+	 GENMASK_ULL(20, 10) | GENMASK_ULL(5, 5))
 #define SPSR_EL1_AARCH32_RES0_BITS \
-	(GENMASK_ULL(63,32) | GENMASK_ULL(24, 22) | GENMASK_ULL(20,20))
+	(GENMASK_ULL(63,32) | GENMASK_ULL(23, 22) | GENMASK_ULL(20,20))
 
 static int valid_compat_regs(struct user_pt_regs *regs)
 {

commit b357bf6023a948cf6a9472f07a1b0caac0e4f8e8
Merge: 0725d4e1b8b0 766d3571d8e5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 11:34:04 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "Small update for KVM:
    
      ARM:
       - lazy context-switching of FPSIMD registers on arm64
       - "split" regions for vGIC redistributor
    
      s390:
       - cleanups for nested
       - clock handling
       - crypto
       - storage keys
       - control register bits
    
      x86:
       - many bugfixes
       - implement more Hyper-V super powers
       - implement lapic_timer_advance_ns even when the LAPIC timer is
         emulated using the processor's VMX preemption timer.
       - two security-related bugfixes at the top of the branch"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (79 commits)
      kvm: fix typo in flag name
      kvm: x86: use correct privilege level for sgdt/sidt/fxsave/fxrstor access
      KVM: x86: pass kvm_vcpu to kvm_read_guest_virt and kvm_write_guest_virt_system
      KVM: x86: introduce linear_{read,write}_system
      kvm: nVMX: Enforce cpl=0 for VMX instructions
      kvm: nVMX: Add support for "VMWRITE to any supported field"
      kvm: nVMX: Restrict VMX capability MSR changes
      KVM: VMX: Optimize tscdeadline timer latency
      KVM: docs: nVMX: Remove known limitations as they do not exist now
      KVM: docs: mmu: KVM support exposing SLAT to guests
      kvm: no need to check return value of debugfs_create functions
      kvm: Make VM ioctl do valloc for some archs
      kvm: Change return type to vm_fault_t
      KVM: docs: mmu: Fix link to NPT presentation from KVM Forum 2008
      kvm: x86: Amend the KVM_GET_SUPPORTED_CPUID API documentation
      KVM: x86: hyperv: declare KVM_CAP_HYPERV_TLBFLUSH capability
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE}_EX implementation
      KVM: x86: hyperv: simplistic HVCALL_FLUSH_VIRTUAL_ADDRESS_{LIST,SPACE} implementation
      KVM: introduce kvm_make_vcpus_request_mask() API
      KVM: x86: hyperv: do rep check for each hypercall separately
      ...

commit 87c021a8143b6133e0085a8162f2a127462b54a3
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Fri Jun 1 11:10:13 2018 +0100

    arm64/sve: Thin out initialisation sanity-checks for sve_max_vl
    
    Now that the kernel SVE support is reasonably mature, it is
    excessive to default sve_max_vl to the invalid value -1 and then
    sprinkle WARN_ON()s around the place to make sure it has been
    initialised before use.  The cpufeatures code already runs pretty
    early, and will ensure sve_max_vl gets initialised.
    
    This patch initialises sve_max_vl to something sane that will be
    supported by every SVE implementation, and removes most of the
    sanity checks.
    
    The checks in find_supported_vector_length() are retained for now.
    If anything goes horribly wrong, we are likely to trip a check here
    sooner or later.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index f847285d96f3..bd732644c2f6 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -766,9 +766,6 @@ static void sve_init_header_from_task(struct user_sve_header *header,
 	vq = sve_vq_from_vl(header->vl);
 
 	header->max_vl = sve_max_vl;
-	if (WARN_ON(!sve_vl_valid(sve_max_vl)))
-		header->max_vl = header->vl;
-
 	header->size = SVE_PT_SIZE(vq, header->flags);
 	header->max_size = SVE_PT_SIZE(sve_vq_from_vl(header->max_vl),
 				      SVE_PT_REGS_SVE);

commit 31dc52b3c8faf47bf3ff5ced661488a20e5d1811
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Apr 12 16:47:20 2018 +0100

    arm64/sve: Move read_zcr_features() out of cpufeature.h
    
    Having read_zcr_features() inline in cpufeature.h results in that
    header requiring #includes which make it hard to include
    <asm/fpsimd.h> elsewhere without triggering header inclusion
    cycles.
    
    This is not a hot-path function and arguably should not be in
    cpufeature.h in the first place, so this patch moves it to
    fpsimd.c, compiled conditionally if CONFIG_ARM64_SVE=y.
    
    This allows some SVE-related #includes to be dropped from
    cpufeature.h, which will ease future maintenance.
    
    A couple of missing #includes of <asm/fpsimd.h> are exposed by this
    change under arch/arm64/.  This patch adds the missing #includes as
    necessary.
    
    No functional change.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Alex Bennée <alex.bennee@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 7ff81fed46e1..78889c4546d7 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -44,6 +44,7 @@
 #include <asm/compat.h>
 #include <asm/cpufeature.h>
 #include <asm/debug-monitors.h>
+#include <asm/fpsimd.h>
 #include <asm/pgtable.h>
 #include <asm/stacktrace.h>
 #include <asm/syscall.h>

commit 92faa7bea3e7592673109e32c75d50f8ce6d5ec6
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Fri Apr 13 15:44:35 2018 +0100

    arm64: Remove duplicate include
    
    "make includecheck" detected few duplicated includes in arch/arm64.
    
    This patch removes the double inclusions.
    
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 7ff81fed46e1..f847285d96f3 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1046,8 +1046,6 @@ static const struct user_regset_view user_aarch64_view = {
 };
 
 #ifdef CONFIG_COMPAT
-#include <linux/compat.h>
-
 enum compat_regset {
 	REGSET_COMPAT_GPR,
 	REGSET_COMPAT_VFP,

commit 19791a7ca674fb3009bb068260e852a2f05b605c
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Apr 25 17:13:40 2018 +0100

    arm64: fix possible spectre-v1 in ptrace_hbp_get_event()
    
    It's possible for userspace to control idx. Sanitize idx when using it
    as an array index.
    
    Found by smatch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 7a832ce6552f..7ff81fed46e1 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -25,6 +25,7 @@
 #include <linux/sched/signal.h>
 #include <linux/sched/task_stack.h>
 #include <linux/mm.h>
+#include <linux/nospec.h>
 #include <linux/smp.h>
 #include <linux/ptrace.h>
 #include <linux/user.h>
@@ -249,15 +250,20 @@ static struct perf_event *ptrace_hbp_get_event(unsigned int note_type,
 
 	switch (note_type) {
 	case NT_ARM_HW_BREAK:
-		if (idx < ARM_MAX_BRP)
-			bp = tsk->thread.debug.hbp_break[idx];
+		if (idx >= ARM_MAX_BRP)
+			goto out;
+		idx = array_index_nospec(idx, ARM_MAX_BRP);
+		bp = tsk->thread.debug.hbp_break[idx];
 		break;
 	case NT_ARM_HW_WATCH:
-		if (idx < ARM_MAX_WRP)
-			bp = tsk->thread.debug.hbp_watch[idx];
+		if (idx >= ARM_MAX_WRP)
+			goto out;
+		idx = array_index_nospec(idx, ARM_MAX_WRP);
+		bp = tsk->thread.debug.hbp_watch[idx];
 		break;
 	}
 
+out:
 	return bp;
 }
 

commit 59275a0c037ed6fabd6354730f1e3104264ab719
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 24 13:11:22 2018 +0100

    arm64: ptrace: remove addr_limit manipulation
    
    We transiently switch to KERNEL_DS in compat_ptrace_gethbpregs() and
    compat_ptrace_sethbpregs(), but in either case this is pointless as we
    don't perform any uaccess during this window.
    
    let's rip out the redundant addr_limit manipulation.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 71d99af24ef2..7a832ce6552f 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1458,9 +1458,7 @@ static int compat_ptrace_gethbpregs(struct task_struct *tsk, compat_long_t num,
 {
 	int ret;
 	u32 kdata;
-	mm_segment_t old_fs = get_fs();
 
-	set_fs(KERNEL_DS);
 	/* Watchpoint */
 	if (num < 0) {
 		ret = compat_ptrace_hbp_get(NT_ARM_HW_WATCH, tsk, num, &kdata);
@@ -1471,7 +1469,6 @@ static int compat_ptrace_gethbpregs(struct task_struct *tsk, compat_long_t num,
 	} else {
 		ret = compat_ptrace_hbp_get(NT_ARM_HW_BREAK, tsk, num, &kdata);
 	}
-	set_fs(old_fs);
 
 	if (!ret)
 		ret = put_user(kdata, data);
@@ -1484,7 +1481,6 @@ static int compat_ptrace_sethbpregs(struct task_struct *tsk, compat_long_t num,
 {
 	int ret;
 	u32 kdata = 0;
-	mm_segment_t old_fs = get_fs();
 
 	if (num == 0)
 		return 0;
@@ -1493,12 +1489,10 @@ static int compat_ptrace_sethbpregs(struct task_struct *tsk, compat_long_t num,
 	if (ret)
 		return ret;
 
-	set_fs(KERNEL_DS);
 	if (num < 0)
 		ret = compat_ptrace_hbp_set(NT_ARM_HW_WATCH, tsk, num, &kdata);
 	else
 		ret = compat_ptrace_hbp_set(NT_ARM_HW_BREAK, tsk, num, &kdata);
-	set_fs(old_fs);
 
 	return ret;
 }

commit 65896545b69ffaac947c12e11d3dcc57fd1fb772
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Mar 28 10:50:49 2018 +0100

    arm64: uaccess: Fix omissions from usercopy whitelist
    
    When the hardend usercopy support was added for arm64, it was
    concluded that all cases of usercopy into and out of thread_struct
    were statically sized and so didn't require explicit whitelisting
    of the appropriate fields in thread_struct.
    
    Testing with usercopy hardening enabled has revealed that this is
    not the case for certain ptrace regset manipulation calls on arm64.
    This occurs because the sizes of usercopies associated with the
    regset API are dynamic by construction, and because arm64 does not
    always stage such copies via the stack: indeed the regset API is
    designed to avoid the need for that by adding some bounds checking.
    
    This is currently believed to affect only the fpsimd and TLS
    registers.
    
    Because the whitelisted fields in thread_struct must be contiguous,
    this patch groups them together in a nested struct.  It is also
    necessary to be able to determine the location and size of that
    struct, so rather than making the struct anonymous (which would
    save on edits elsewhere) or adding an anonymous union containing
    named and unnamed instances of the same struct (gross), this patch
    gives the struct a name and makes the necessary edits to code that
    references it (noisy but simple).
    
    Care is needed to ensure that the new struct does not contain
    padding (which the usercopy hardening would fail to protect).
    
    For this reason, the presence of tp2_value is made unconditional,
    since a padding field would be needed there in any case.  This pads
    up to the 16-byte alignment required by struct user_fpsimd_state.
    
    Acked-by: Kees Cook <keescook@chromium.org>
    Reported-by: Mark Rutland <mark.rutland@arm.com>
    Fixes: 9e8084d3f761 ("arm64: Implement thread_struct whitelist for hardened usercopy")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index fd9e8ed22b70..71d99af24ef2 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -629,7 +629,7 @@ static int __fpr_get(struct task_struct *target,
 
 	sve_sync_to_fpsimd(target);
 
-	uregs = &target->thread.fpsimd_state;
+	uregs = &target->thread.uw.fpsimd_state;
 
 	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs,
 				   start_pos, start_pos + sizeof(*uregs));
@@ -655,19 +655,19 @@ static int __fpr_set(struct task_struct *target,
 	struct user_fpsimd_state newstate;
 
 	/*
-	 * Ensure target->thread.fpsimd_state is up to date, so that a
+	 * Ensure target->thread.uw.fpsimd_state is up to date, so that a
 	 * short copyin can't resurrect stale data.
 	 */
 	sve_sync_to_fpsimd(target);
 
-	newstate = target->thread.fpsimd_state;
+	newstate = target->thread.uw.fpsimd_state;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate,
 				 start_pos, start_pos + sizeof(newstate));
 	if (ret)
 		return ret;
 
-	target->thread.fpsimd_state = newstate;
+	target->thread.uw.fpsimd_state = newstate;
 
 	return ret;
 }
@@ -692,7 +692,7 @@ static int tls_get(struct task_struct *target, const struct user_regset *regset,
 		   unsigned int pos, unsigned int count,
 		   void *kbuf, void __user *ubuf)
 {
-	unsigned long *tls = &target->thread.tp_value;
+	unsigned long *tls = &target->thread.uw.tp_value;
 
 	if (target == current)
 		tls_preserve_current_state();
@@ -705,13 +705,13 @@ static int tls_set(struct task_struct *target, const struct user_regset *regset,
 		   const void *kbuf, const void __user *ubuf)
 {
 	int ret;
-	unsigned long tls = target->thread.tp_value;
+	unsigned long tls = target->thread.uw.tp_value;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
 	if (ret)
 		return ret;
 
-	target->thread.tp_value = tls;
+	target->thread.uw.tp_value = tls;
 	return ret;
 }
 
@@ -842,7 +842,7 @@ static int sve_get(struct task_struct *target,
 	start = end;
 	end = SVE_PT_SVE_FPCR_OFFSET(vq) + SVE_PT_SVE_FPCR_SIZE;
 	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,
-				  &target->thread.fpsimd_state.fpsr,
+				  &target->thread.uw.fpsimd_state.fpsr,
 				  start, end);
 	if (ret)
 		return ret;
@@ -941,7 +941,7 @@ static int sve_set(struct task_struct *target,
 	start = end;
 	end = SVE_PT_SVE_FPCR_OFFSET(vq) + SVE_PT_SVE_FPCR_SIZE;
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,
-				 &target->thread.fpsimd_state.fpsr,
+				 &target->thread.uw.fpsimd_state.fpsr,
 				 start, end);
 
 out:
@@ -1169,7 +1169,7 @@ static int compat_vfp_get(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret, vregs_end_pos;
 
-	uregs = &target->thread.fpsimd_state;
+	uregs = &target->thread.uw.fpsimd_state;
 
 	if (target == current)
 		fpsimd_preserve_current_state();
@@ -1202,7 +1202,7 @@ static int compat_vfp_set(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret, vregs_end_pos;
 
-	uregs = &target->thread.fpsimd_state;
+	uregs = &target->thread.uw.fpsimd_state;
 
 	vregs_end_pos = VFP_STATE_SIZE - sizeof(compat_ulong_t);
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, uregs, 0,
@@ -1225,7 +1225,7 @@ static int compat_tls_get(struct task_struct *target,
 			  const struct user_regset *regset, unsigned int pos,
 			  unsigned int count, void *kbuf, void __user *ubuf)
 {
-	compat_ulong_t tls = (compat_ulong_t)target->thread.tp_value;
+	compat_ulong_t tls = (compat_ulong_t)target->thread.uw.tp_value;
 	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
 }
 
@@ -1235,13 +1235,13 @@ static int compat_tls_set(struct task_struct *target,
 			  const void __user *ubuf)
 {
 	int ret;
-	compat_ulong_t tls = target->thread.tp_value;
+	compat_ulong_t tls = target->thread.uw.tp_value;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
 	if (ret)
 		return ret;
 
-	target->thread.tp_value = tls;
+	target->thread.uw.tp_value = tls;
 	return ret;
 }
 
@@ -1538,7 +1538,7 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 			break;
 
 		case COMPAT_PTRACE_GET_THREAD_AREA:
-			ret = put_user((compat_ulong_t)child->thread.tp_value,
+			ret = put_user((compat_ulong_t)child->thread.uw.tp_value,
 				       (compat_ulong_t __user *)datap);
 			break;
 

commit 20b8547277a6e8ee1d928792c1b2782c9a2a6cf5
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Mar 28 10:50:48 2018 +0100

    arm64: fpsimd: Split cpu field out from struct fpsimd_state
    
    In preparation for using a common representation of the FPSIMD
    state for tasks and KVM vcpus, this patch separates out the "cpu"
    field that is used to track the cpu on which the state was most
    recently loaded.
    
    This will allow common code to operate on task and vcpu contexts
    without requiring the cpu field to be stored at the same offset
    from the FPSIMD register data in both cases.  This should avoid the
    need for messing with the definition of those parts of struct
    vcpu_arch that are exposed in the KVM user ABI.
    
    The resulting change is also convenient for grouping and defining
    the set of thread_struct fields that are supposed to be accessible
    to copy_{to,from}_user(), which includes user_fpsimd_state but
    should exclude the cpu field.  This patch does not amend the
    usercopy whitelist to match: that will be addressed in a subsequent
    patch.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    [will: inline fpsimd_flush_state for now]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6228476e74ba..fd9e8ed22b70 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -629,7 +629,7 @@ static int __fpr_get(struct task_struct *target,
 
 	sve_sync_to_fpsimd(target);
 
-	uregs = &target->thread.fpsimd_state.user_fpsimd;
+	uregs = &target->thread.fpsimd_state;
 
 	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs,
 				   start_pos, start_pos + sizeof(*uregs));
@@ -660,14 +660,14 @@ static int __fpr_set(struct task_struct *target,
 	 */
 	sve_sync_to_fpsimd(target);
 
-	newstate = target->thread.fpsimd_state.user_fpsimd;
+	newstate = target->thread.fpsimd_state;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate,
 				 start_pos, start_pos + sizeof(newstate));
 	if (ret)
 		return ret;
 
-	target->thread.fpsimd_state.user_fpsimd = newstate;
+	target->thread.fpsimd_state = newstate;
 
 	return ret;
 }
@@ -1169,7 +1169,7 @@ static int compat_vfp_get(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret, vregs_end_pos;
 
-	uregs = &target->thread.fpsimd_state.user_fpsimd;
+	uregs = &target->thread.fpsimd_state;
 
 	if (target == current)
 		fpsimd_preserve_current_state();
@@ -1202,7 +1202,7 @@ static int compat_vfp_set(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret, vregs_end_pos;
 
-	uregs = &target->thread.fpsimd_state.user_fpsimd;
+	uregs = &target->thread.fpsimd_state;
 
 	vregs_end_pos = VFP_STATE_SIZE - sizeof(compat_ulong_t);
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, uregs, 0,

commit 4e829b6735475313016787ec3d256e102167b94d
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 20 15:18:13 2018 +0000

    arm64: Use arm64_force_sig_info instead of force_sig_info
    
    Using arm64_force_sig_info means that printing messages about unhandled
    signals is dealt with for us, so use that in preference to force_sig_info
    and remove any homebrew printing code.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 9ae31f7e2243..6228476e74ba 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -209,7 +209,7 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 		force_sig_ptrace_errno_trap(si_errno, (void __user *)bkpt->trigger);
 	}
 #endif
-	force_sig_info(SIGTRAP, &info, current);
+	arm64_force_sig_info(&info, "Hardware breakpoint trap (ptrace)", current);
 }
 
 /*

commit ed7158bae41044ff696e9aafd5ada46d391a5a2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 22 10:54:55 2018 +0100

    treewide/trivial: Remove ';;$' typo noise
    
    On lkml suggestions were made to split up such trivial typo fixes into per subsystem
    patches:
    
      --- a/arch/x86/boot/compressed/eboot.c
      +++ b/arch/x86/boot/compressed/eboot.c
      @@ -439,7 +439,7 @@ setup_uga32(void **uga_handle, unsigned long size, u32 *width, u32 *height)
              struct efi_uga_draw_protocol *uga = NULL, *first_uga;
              efi_guid_t uga_proto = EFI_UGA_PROTOCOL_GUID;
              unsigned long nr_ugas;
      -       u32 *handles = (u32 *)uga_handle;;
      +       u32 *handles = (u32 *)uga_handle;
              efi_status_t status = EFI_INVALID_PARAMETER;
              int i;
    
    This patch is the result of the following script:
    
      $ sed -i 's/;;$/;/g' $(git grep -E ';;$'  | grep "\.[ch]:"  | grep -vwE 'for|ia64' | cut -d: -f1 | sort | uniq)
    
    ... followed by manual review to make sure it's all good.
    
    Splitting this up is just crazy talk, let's get over with this and just do it.
    
    Reported-by: Pavel Machek <pavel@ucw.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6618036ae6d4..9ae31f7e2243 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1419,7 +1419,7 @@ static int compat_ptrace_hbp_get(unsigned int note_type,
 	u64 addr = 0;
 	u32 ctrl = 0;
 
-	int err, idx = compat_ptrace_hbp_num_to_idx(num);;
+	int err, idx = compat_ptrace_hbp_num_to_idx(num);
 
 	if (num & 1) {
 		err = ptrace_hbp_get_addr(note_type, tsk, idx, &addr);

commit f71dd7dc2dc989dc712b246a74d243e4b2c5f8a7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jan 22 14:37:25 2018 -0600

    signal/ptrace: Add force_sig_ptrace_errno_trap and use it where needed
    
    There are so many places that build struct siginfo by hand that at
    least one of them is bound to get it wrong.  A handful of cases in the
    kernel arguably did just that when using the errno field of siginfo to
    pass no errno values to userspace.  The usage is limited to a single
    si_code so at least does not mess up anything else.
    
    Encapsulate this questionable pattern in a helper function so
    that the userspace ABI is preserved.
    
    Update all of the places that use this pattern to use the new helper
    function.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 95daa1478a7c..6618036ae6d4 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -190,21 +190,23 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 
 #ifdef CONFIG_COMPAT
 	if (is_compat_task()) {
+		int si_errno = 0;
 		int i;
 
 		for (i = 0; i < ARM_MAX_BRP; ++i) {
 			if (current->thread.debug.hbp_break[i] == bp) {
-				info.si_errno = (i << 1) + 1;
+				si_errno = (i << 1) + 1;
 				break;
 			}
 		}
 
 		for (i = 0; i < ARM_MAX_WRP; ++i) {
 			if (current->thread.debug.hbp_watch[i] == bp) {
-				info.si_errno = -((i << 1) + 1);
+				si_errno = -((i << 1) + 1);
 				break;
 			}
 		}
+		force_sig_ptrace_errno_trap(si_errno, (void __user *)bkpt->trigger);
 	}
 #endif
 	force_sig_info(SIGTRAP, &info, current);

commit 5f74972ce69fdc6473f74253283408af75a3be15
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jan 22 14:58:57 2018 -0600

    signal: Don't use structure initializers for struct siginfo
    
    The siginfo structure has all manners of holes with the result that a
    structure initializer is not guaranteed to initialize all of the bits.
    As we have to copy the structure to userspace don't even try to use
    a structure initializer.  Instead use clear_siginfo followed by initializing
    selected fields.  This gives a guarantee that uninitialized kernel memory
    is not copied to userspace.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 0a1cf830e4b3..95daa1478a7c 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -180,12 +180,13 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 				struct pt_regs *regs)
 {
 	struct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);
-	siginfo_t info = {
-		.si_signo	= SIGTRAP,
-		.si_errno	= 0,
-		.si_code	= TRAP_HWBKPT,
-		.si_addr	= (void __user *)(bkpt->trigger),
-	};
+	siginfo_t info;
+
+	clear_siginfo(&info);
+	info.si_signo	= SIGTRAP;
+	info.si_errno	= 0;
+	info.si_code	= TRAP_HWBKPT;
+	info.si_addr	= (void __user *)(bkpt->trigger);
 
 #ifdef CONFIG_COMPAT
 	if (is_compat_task()) {

commit 66e0f26315ce7dd3f4efdbdee63f30dac643763f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jan 22 14:50:53 2018 -0600

    signal/arm64: Better isolate the COMPAT_TASK portion of ptrace_hbptriggered
    
    Instead of jumpping while !is_compat_task placee all of the code
    inside of an if (is_compat_task) block.  This allows the int i
    variable to be properly limited to the compat block no matter how the
    rest of ptrace_hbptriggered changes.
    
    In a following change a non-variable declaration will preceed
    was made independent to ensure the code is easy to review.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 7c44658b316d..0a1cf830e4b3 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -188,26 +188,23 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 	};
 
 #ifdef CONFIG_COMPAT
-	int i;
-
-	if (!is_compat_task())
-		goto send_sig;
+	if (is_compat_task()) {
+		int i;
 
-	for (i = 0; i < ARM_MAX_BRP; ++i) {
-		if (current->thread.debug.hbp_break[i] == bp) {
-			info.si_errno = (i << 1) + 1;
-			break;
+		for (i = 0; i < ARM_MAX_BRP; ++i) {
+			if (current->thread.debug.hbp_break[i] == bp) {
+				info.si_errno = (i << 1) + 1;
+				break;
+			}
 		}
-	}
 
-	for (i = 0; i < ARM_MAX_WRP; ++i) {
-		if (current->thread.debug.hbp_watch[i] == bp) {
-			info.si_errno = -((i << 1) + 1);
-			break;
+		for (i = 0; i < ARM_MAX_WRP; ++i) {
+			if (current->thread.debug.hbp_watch[i] == bp) {
+				info.si_errno = -((i << 1) + 1);
+				break;
+			}
 		}
 	}
-
-send_sig:
 #endif
 	force_sig_info(SIGTRAP, &info, current);
 }

commit 43d4da2c45b2f5d62f8a79ff7c6f95089bb24656
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Oct 31 15:51:13 2017 +0000

    arm64/sve: ptrace and ELF coredump support
    
    This patch defines and implements a new regset NT_ARM_SVE, which
    describes a thread's SVE register state.  This allows a debugger to
    manipulate the SVE state, as well as being included in ELF
    coredumps for post-mortem debugging.
    
    Because the regset size and layout are dependent on the thread's
    current vector length, it is not possible to define a C struct to
    describe the regset contents as is done for existing regsets.
    Instead, and for the same reasons, NT_ARM_SVE is based on the
    freeform variable-layout approach used for the SVE signal frame.
    
    Additionally, to reduce debug overhead when debugging threads that
    might or might not have live SVE register state, NT_ARM_SVE may be
    presented in one of two different formats: the old struct
    user_fpsimd_state format is embedded for describing the state of a
    thread with no live SVE state, whereas a new variable-layout
    structure is embedded for describing live SVE state.  This avoids a
    debugger needing to poll NT_PRFPREG in addition to NT_ARM_SVE, and
    allows existing userspace code to handle the non-SVE case without
    too much modification.
    
    For this to work, NT_ARM_SVE is defined with a fixed-format header
    of type struct user_sve_header, which the recipient can use to
    figure out the content, size and layout of the reset of the regset.
    Accessor macros are defined to allow the vector-length-dependent
    parts of the regset to be manipulated.
    
    Signed-off-by: Alan Hayward <alan.hayward@arm.com>
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Alex Bennée <alex.bennee@linaro.org>
    Cc: Okamoto Takayuki <tokamoto@jp.fujitsu.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 9cbb6123208f..7c44658b316d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -32,6 +32,7 @@
 #include <linux/security.h>
 #include <linux/init.h>
 #include <linux/signal.h>
+#include <linux/string.h>
 #include <linux/uaccess.h>
 #include <linux/perf_event.h>
 #include <linux/hw_breakpoint.h>
@@ -40,6 +41,7 @@
 #include <linux/elf.h>
 
 #include <asm/compat.h>
+#include <asm/cpufeature.h>
 #include <asm/debug-monitors.h>
 #include <asm/pgtable.h>
 #include <asm/stacktrace.h>
@@ -618,17 +620,56 @@ static int gpr_set(struct task_struct *target, const struct user_regset *regset,
 /*
  * TODO: update fp accessors for lazy context switching (sync/flush hwstate)
  */
-static int fpr_get(struct task_struct *target, const struct user_regset *regset,
-		   unsigned int pos, unsigned int count,
-		   void *kbuf, void __user *ubuf)
+static int __fpr_get(struct task_struct *target,
+		     const struct user_regset *regset,
+		     unsigned int pos, unsigned int count,
+		     void *kbuf, void __user *ubuf, unsigned int start_pos)
 {
 	struct user_fpsimd_state *uregs;
+
+	sve_sync_to_fpsimd(target);
+
 	uregs = &target->thread.fpsimd_state.user_fpsimd;
 
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs,
+				   start_pos, start_pos + sizeof(*uregs));
+}
+
+static int fpr_get(struct task_struct *target, const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   void *kbuf, void __user *ubuf)
+{
 	if (target == current)
 		fpsimd_preserve_current_state();
 
-	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs, 0, -1);
+	return __fpr_get(target, regset, pos, count, kbuf, ubuf, 0);
+}
+
+static int __fpr_set(struct task_struct *target,
+		     const struct user_regset *regset,
+		     unsigned int pos, unsigned int count,
+		     const void *kbuf, const void __user *ubuf,
+		     unsigned int start_pos)
+{
+	int ret;
+	struct user_fpsimd_state newstate;
+
+	/*
+	 * Ensure target->thread.fpsimd_state is up to date, so that a
+	 * short copyin can't resurrect stale data.
+	 */
+	sve_sync_to_fpsimd(target);
+
+	newstate = target->thread.fpsimd_state.user_fpsimd;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate,
+				 start_pos, start_pos + sizeof(newstate));
+	if (ret)
+		return ret;
+
+	target->thread.fpsimd_state.user_fpsimd = newstate;
+
+	return ret;
 }
 
 static int fpr_set(struct task_struct *target, const struct user_regset *regset,
@@ -636,15 +677,14 @@ static int fpr_set(struct task_struct *target, const struct user_regset *regset,
 		   const void *kbuf, const void __user *ubuf)
 {
 	int ret;
-	struct user_fpsimd_state newstate =
-		target->thread.fpsimd_state.user_fpsimd;
 
-	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate, 0, -1);
+	ret = __fpr_set(target, regset, pos, count, kbuf, ubuf, 0);
 	if (ret)
 		return ret;
 
-	target->thread.fpsimd_state.user_fpsimd = newstate;
+	sve_sync_from_fpsimd_zeropad(target);
 	fpsimd_flush_task_state(target);
+
 	return ret;
 }
 
@@ -702,6 +742,215 @@ static int system_call_set(struct task_struct *target,
 	return ret;
 }
 
+#ifdef CONFIG_ARM64_SVE
+
+static void sve_init_header_from_task(struct user_sve_header *header,
+				      struct task_struct *target)
+{
+	unsigned int vq;
+
+	memset(header, 0, sizeof(*header));
+
+	header->flags = test_tsk_thread_flag(target, TIF_SVE) ?
+		SVE_PT_REGS_SVE : SVE_PT_REGS_FPSIMD;
+	if (test_tsk_thread_flag(target, TIF_SVE_VL_INHERIT))
+		header->flags |= SVE_PT_VL_INHERIT;
+
+	header->vl = target->thread.sve_vl;
+	vq = sve_vq_from_vl(header->vl);
+
+	header->max_vl = sve_max_vl;
+	if (WARN_ON(!sve_vl_valid(sve_max_vl)))
+		header->max_vl = header->vl;
+
+	header->size = SVE_PT_SIZE(vq, header->flags);
+	header->max_size = SVE_PT_SIZE(sve_vq_from_vl(header->max_vl),
+				      SVE_PT_REGS_SVE);
+}
+
+static unsigned int sve_size_from_header(struct user_sve_header const *header)
+{
+	return ALIGN(header->size, SVE_VQ_BYTES);
+}
+
+static unsigned int sve_get_size(struct task_struct *target,
+				 const struct user_regset *regset)
+{
+	struct user_sve_header header;
+
+	if (!system_supports_sve())
+		return 0;
+
+	sve_init_header_from_task(&header, target);
+	return sve_size_from_header(&header);
+}
+
+static int sve_get(struct task_struct *target,
+		   const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   void *kbuf, void __user *ubuf)
+{
+	int ret;
+	struct user_sve_header header;
+	unsigned int vq;
+	unsigned long start, end;
+
+	if (!system_supports_sve())
+		return -EINVAL;
+
+	/* Header */
+	sve_init_header_from_task(&header, target);
+	vq = sve_vq_from_vl(header.vl);
+
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &header,
+				  0, sizeof(header));
+	if (ret)
+		return ret;
+
+	if (target == current)
+		fpsimd_preserve_current_state();
+
+	/* Registers: FPSIMD-only case */
+
+	BUILD_BUG_ON(SVE_PT_FPSIMD_OFFSET != sizeof(header));
+	if ((header.flags & SVE_PT_REGS_MASK) == SVE_PT_REGS_FPSIMD)
+		return __fpr_get(target, regset, pos, count, kbuf, ubuf,
+				 SVE_PT_FPSIMD_OFFSET);
+
+	/* Otherwise: full SVE case */
+
+	BUILD_BUG_ON(SVE_PT_SVE_OFFSET != sizeof(header));
+	start = SVE_PT_SVE_OFFSET;
+	end = SVE_PT_SVE_FFR_OFFSET(vq) + SVE_PT_SVE_FFR_SIZE(vq);
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,
+				  target->thread.sve_state,
+				  start, end);
+	if (ret)
+		return ret;
+
+	start = end;
+	end = SVE_PT_SVE_FPSR_OFFSET(vq);
+	ret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,
+				       start, end);
+	if (ret)
+		return ret;
+
+	/*
+	 * Copy fpsr, and fpcr which must follow contiguously in
+	 * struct fpsimd_state:
+	 */
+	start = end;
+	end = SVE_PT_SVE_FPCR_OFFSET(vq) + SVE_PT_SVE_FPCR_SIZE;
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf,
+				  &target->thread.fpsimd_state.fpsr,
+				  start, end);
+	if (ret)
+		return ret;
+
+	start = end;
+	end = sve_size_from_header(&header);
+	return user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,
+					start, end);
+}
+
+static int sve_set(struct task_struct *target,
+		   const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   const void *kbuf, const void __user *ubuf)
+{
+	int ret;
+	struct user_sve_header header;
+	unsigned int vq;
+	unsigned long start, end;
+
+	if (!system_supports_sve())
+		return -EINVAL;
+
+	/* Header */
+	if (count < sizeof(header))
+		return -EINVAL;
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &header,
+				 0, sizeof(header));
+	if (ret)
+		goto out;
+
+	/*
+	 * Apart from PT_SVE_REGS_MASK, all PT_SVE_* flags are consumed by
+	 * sve_set_vector_length(), which will also validate them for us:
+	 */
+	ret = sve_set_vector_length(target, header.vl,
+		((unsigned long)header.flags & ~SVE_PT_REGS_MASK) << 16);
+	if (ret)
+		goto out;
+
+	/* Actual VL set may be less than the user asked for: */
+	vq = sve_vq_from_vl(target->thread.sve_vl);
+
+	/* Registers: FPSIMD-only case */
+
+	BUILD_BUG_ON(SVE_PT_FPSIMD_OFFSET != sizeof(header));
+	if ((header.flags & SVE_PT_REGS_MASK) == SVE_PT_REGS_FPSIMD) {
+		ret = __fpr_set(target, regset, pos, count, kbuf, ubuf,
+				SVE_PT_FPSIMD_OFFSET);
+		clear_tsk_thread_flag(target, TIF_SVE);
+		goto out;
+	}
+
+	/* Otherwise: full SVE case */
+
+	/*
+	 * If setting a different VL from the requested VL and there is
+	 * register data, the data layout will be wrong: don't even
+	 * try to set the registers in this case.
+	 */
+	if (count && vq != sve_vq_from_vl(header.vl)) {
+		ret = -EIO;
+		goto out;
+	}
+
+	sve_alloc(target);
+
+	/*
+	 * Ensure target->thread.sve_state is up to date with target's
+	 * FPSIMD regs, so that a short copyin leaves trailing registers
+	 * unmodified.
+	 */
+	fpsimd_sync_to_sve(target);
+	set_tsk_thread_flag(target, TIF_SVE);
+
+	BUILD_BUG_ON(SVE_PT_SVE_OFFSET != sizeof(header));
+	start = SVE_PT_SVE_OFFSET;
+	end = SVE_PT_SVE_FFR_OFFSET(vq) + SVE_PT_SVE_FFR_SIZE(vq);
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,
+				 target->thread.sve_state,
+				 start, end);
+	if (ret)
+		goto out;
+
+	start = end;
+	end = SVE_PT_SVE_FPSR_OFFSET(vq);
+	ret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,
+					start, end);
+	if (ret)
+		goto out;
+
+	/*
+	 * Copy fpsr, and fpcr which must follow contiguously in
+	 * struct fpsimd_state:
+	 */
+	start = end;
+	end = SVE_PT_SVE_FPCR_OFFSET(vq) + SVE_PT_SVE_FPCR_SIZE;
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf,
+				 &target->thread.fpsimd_state.fpsr,
+				 start, end);
+
+out:
+	fpsimd_flush_task_state(target);
+	return ret;
+}
+
+#endif /* CONFIG_ARM64_SVE */
+
 enum aarch64_regset {
 	REGSET_GPR,
 	REGSET_FPR,
@@ -711,6 +960,9 @@ enum aarch64_regset {
 	REGSET_HW_WATCH,
 #endif
 	REGSET_SYSTEM_CALL,
+#ifdef CONFIG_ARM64_SVE
+	REGSET_SVE,
+#endif
 };
 
 static const struct user_regset aarch64_regsets[] = {
@@ -768,6 +1020,18 @@ static const struct user_regset aarch64_regsets[] = {
 		.get = system_call_get,
 		.set = system_call_set,
 	},
+#ifdef CONFIG_ARM64_SVE
+	[REGSET_SVE] = { /* Scalable Vector Extension */
+		.core_note_type = NT_ARM_SVE,
+		.n = DIV_ROUND_UP(SVE_PT_SIZE(SVE_VQ_MAX, SVE_PT_REGS_SVE),
+				  SVE_VQ_BYTES),
+		.size = SVE_VQ_BYTES,
+		.align = SVE_VQ_BYTES,
+		.get = sve_get,
+		.set = sve_set,
+		.get_size = sve_get_size,
+	},
+#endif
 };
 
 static const struct user_regset_view user_aarch64_view = {

commit df5b95bee1ed7009a2090e9924e7a96e14850d56
Merge: 969ff73e72fe 872d8327ce89
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 15 18:40:58 2017 +0100

    Merge branch 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/vmap-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: add VMAP_STACK overflow detection
      arm64: add on_accessible_stack()
      arm64: add basic VMAP_STACK support
      arm64: use an irq stack pointer
      arm64: assembler: allow adr_this_cpu to use the stack pointer
      arm64: factor out entry stack manipulation
      efi/arm64: add EFI_KIMG_ALIGN
      arm64: move SEGMENT_ALIGN to <asm/memory.h>
      arm64: clean up irq stack definitions
      arm64: clean up THREAD_* definitions
      arm64: factor out PAGE_* and CONT_* definitions
      arm64: kernel: remove {THREAD,IRQ_STACK}_START_SP
      fork: allow arch-override of VMAP stack alignment
      arm64: remove __die()'s stack dump

commit f60ad4edcf07238a3d2646d65d8d217032452550
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 20 12:26:48 2017 +0100

    arm64: clean up irq stack definitions
    
    Before we add yet another stack to the kernel, it would be nice to
    ensure that we consistently organise stack definitions and related
    helper functions.
    
    This patch moves the basic IRQ stack defintions to <asm/memory.h> to
    live with their task stack counterparts. Helpers used for unwinding are
    moved into <asm/stacktrace.h>, where subsequent patches will add helpers
    for other stacks. Includes are fixed up accordingly.
    
    This patch is a pure refactoring -- there should be no functional
    changes as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Laura Abbott <labbott@redhat.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index baf0838205c7..a9f87157c371 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -42,6 +42,7 @@
 #include <asm/compat.h>
 #include <asm/debug-monitors.h>
 #include <asm/pgtable.h>
+#include <asm/stacktrace.h>
 #include <asm/syscall.h>
 #include <asm/traps.h>
 #include <asm/system_misc.h>

commit 0553896787353e2526078064ff1cf21ff7bc34ce
Merge: 739586951b8a 31e43ad3b74a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Aug 9 15:37:49 2017 +0100

    Merge branch 'arm64/exception-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux into for-next/core
    
    * 'arm64/exception-stack' of git://git.kernel.org/pub/scm/linux/kernel/git/mark/linux:
      arm64: unwind: remove sp from struct stackframe
      arm64: unwind: reference pt_regs via embedded stack frame
      arm64: unwind: disregard frame.sp when validating frame pointer
      arm64: unwind: avoid percpu indirection for irq stack
      arm64: move non-entry code out of .entry.text
      arm64: consistently use bl for C exception entry
      arm64: Add ASM_BUG()

commit 096683724cb2eb95fea759a2580996df1039fdd0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jul 20 14:01:01 2017 +0100

    arm64: unwind: avoid percpu indirection for irq stack
    
    Our IRQ_STACK_PTR() and on_irq_stack() helpers both take a cpu argument,
    used to generate a percpu address. In all cases, they are passed
    {raw_,}smp_processor_id(), so this parameter is redundant.
    
    Since {raw_,}smp_processor_id() use a percpu variable internally, this
    approach means we generate a percpu offset to find the current cpu, then
    use this to index an array of percpu offsets, which we then use to find
    the current CPU's IRQ stack pointer. Thus, most of the work is
    redundant.
    
    Instead, we can consistently use raw_cpu_ptr() to generate the CPU's
    irq_stack pointer by simply adding the percpu offset to the irq_stack
    address, which is simpler in both respects.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 1b38c0150aec..baf0838205c7 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -127,7 +127,7 @@ static bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
 {
 	return ((addr & ~(THREAD_SIZE - 1))  ==
 		(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1))) ||
-		on_irq_stack(addr, raw_smp_processor_id());
+		on_irq_stack(addr);
 }
 
 /**

commit 17c28958600928109049a3bcc814b0d5bfb1ff3a
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Aug 1 15:35:54 2017 +0100

    arm64: Abstract syscallno manipulation
    
    The -1 "no syscall" value is written in various ways, shared with
    the user ABI in some places, and generally obscure.
    
    This patch attempts to make things a little more consistent and
    readable by replacing all these uses with a single #define.  A
    couple of symbolic helpers are provided to clarify the intent
    further.
    
    Because the in-syscall check in do_signal() is changed from >= 0 to
    != NO_SYSCALL by this patch, different behaviour may be observable
    if syscallno is set to values less than -1 by a tracer.  However,
    this is not different from the behaviour that is already observable
    if a tracer sets syscallno to a value >= __NR_(compat_)syscalls.
    
    It appears that this can cause spurious syscall restarting, but
    that is not a new behaviour either, and does not appear harmful.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index de774805f672..28619b5b6746 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1363,7 +1363,7 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 	if (dir == PTRACE_SYSCALL_EXIT)
 		tracehook_report_syscall_exit(regs, 0);
 	else if (tracehook_report_syscall_entry(regs))
-		regs->syscallno = ~0;
+		forget_syscall(regs);
 
 	regs->regs[regno] = saved_reg;
 }

commit 35d0e6fb4d219d64ab3b7cffef7a11a0662140f5
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Tue Aug 1 15:35:53 2017 +0100

    arm64: syscallno is secretly an int, make it official
    
    The upper 32 bits of the syscallno field in thread_struct are
    handled inconsistently, being sometimes zero extended and sometimes
    sign-extended.  In fact, only the lower 32 bits seem to have any
    real significance for the behaviour of the code: it's been OK to
    handle the upper bits inconsistently because they don't matter.
    
    Currently, the only place I can find where those bits are
    significant is in calling trace_sys_enter(), which may be
    unintentional: for example, if a compat tracer attempts to cancel a
    syscall by passing -1 to (COMPAT_)PTRACE_SET_SYSCALL at the
    syscall-enter-stop, it will be traced as syscall 4294967295
    rather than -1 as might be expected (and as occurs for a native
    tracer doing the same thing).  Elsewhere, reads of syscallno cast
    it to an int or truncate it.
    
    There's also a conspicuous amount of code and casting to bodge
    around the fact that although semantically an int, syscallno is
    stored as a u64.
    
    Let's not pretend any more.
    
    In order to preserve the stp x instruction that stores the syscall
    number in entry.S, this patch special-cases the layout of struct
    pt_regs for big endian so that the newly 32-bit syscallno field
    maps onto the low bits of the stored value.  This is not beautiful,
    but benchmarking of the getpid syscall on Juno suggests indicates a
    minor slowdown if the stp is split into an stp x and stp w.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 1b38c0150aec..de774805f672 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1363,7 +1363,7 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 	if (dir == PTRACE_SYSCALL_EXIT)
 		tracehook_report_syscall_exit(regs, 0);
 	else if (tracehook_report_syscall_entry(regs))
-		regs->syscallno = ~0UL;
+		regs->syscallno = ~0;
 
 	regs->regs[regno] = saved_reg;
 }

commit 5fbd5fc49fc39ac8433da62d16682a1d0217ea4f
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Jun 29 15:25:49 2017 +0100

    arm64: ptrace: Fix incorrect get_user() use in compat_vfp_set()
    
    Now that compat_vfp_get() uses the regset API to copy the FPSCR
    value out to userspace, compat_vfp_set() looks inconsistent.  In
    particular, compat_vfp_set() will fail if called with kbuf != NULL
    && ubuf == NULL (which is valid usage according to the regset API).
    
    This patch fixes compat_vfp_set() to use user_regset_copyin(),
    similarly to compat_vfp_get().
    
    This also squashes a sparse warning triggered by the cast that
    drops __user when calling get_user().
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 949ab6bdfbad..1b38c0150aec 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -935,15 +935,17 @@ static int compat_vfp_set(struct task_struct *target,
 {
 	struct user_fpsimd_state *uregs;
 	compat_ulong_t fpscr;
-	int ret;
+	int ret, vregs_end_pos;
 
 	uregs = &target->thread.fpsimd_state.user_fpsimd;
 
+	vregs_end_pos = VFP_STATE_SIZE - sizeof(compat_ulong_t);
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, uregs, 0,
-				 VFP_STATE_SIZE - sizeof(compat_ulong_t));
+				 vregs_end_pos);
 
 	if (count && !ret) {
-		ret = get_user(fpscr, (compat_ulong_t *)ubuf);
+		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &fpscr,
+					 vregs_end_pos, VFP_STATE_SIZE);
 		if (!ret) {
 			uregs->fpsr = fpscr & VFP_FPSCR_STAT_MASK;
 			uregs->fpcr = fpscr & VFP_FPSCR_CTRL_MASK;

commit 16d38acb12d065ebe3494e4e31e8b4438f3214da
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Jun 29 15:25:48 2017 +0100

    arm64: ptrace: Remove redundant overrun check from compat_vfp_set()
    
    compat_vfp_set() checks for userspace trying to write an excessive
    amount of data to the regset.  However this check is conspicuous
    for its absence from every other _set() in the arm64 ptrace
    implementation.  In fact, the core ptrace_regset() already clamps
    userspace's iov_len to the regset size before the individual regset
    .{get,set}() methods get called.
    
    This patch removes the redundant check.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 4c068dcf1977..949ab6bdfbad 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -937,9 +937,6 @@ static int compat_vfp_set(struct task_struct *target,
 	compat_ulong_t fpscr;
 	int ret;
 
-	if (pos + count > VFP_STATE_SIZE)
-		return -EIO;
-
 	uregs = &target->thread.fpsimd_state.user_fpsimd;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, uregs, 0,

commit 53b1a742ed251780267a57415bc955bd50f40c3d
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Jun 29 15:25:47 2017 +0100

    arm64: ptrace: Avoid setting compat FP[SC]R to garbage if get_user fails
    
    If get_user() fails when reading the new FPSCR value from userspace
    in compat_vfp_get(), then garbage* will be written to the task's
    FPSR and FPCR registers.
    
    This patch prevents this by checking the return from get_user()
    first.
    
    [*] Actually, zero, due to the behaviour of get_user() on error, but
    that's still not what userspace expects.
    
    Fixes: 478fcb2cdb23 ("arm64: Debugging support")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 35846f1550db..4c068dcf1977 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -947,8 +947,10 @@ static int compat_vfp_set(struct task_struct *target,
 
 	if (count && !ret) {
 		ret = get_user(fpscr, (compat_ulong_t *)ubuf);
-		uregs->fpsr = fpscr & VFP_FPSCR_STAT_MASK;
-		uregs->fpcr = fpscr & VFP_FPSCR_CTRL_MASK;
+		if (!ret) {
+			uregs->fpsr = fpscr & VFP_FPSCR_STAT_MASK;
+			uregs->fpcr = fpscr & VFP_FPSCR_CTRL_MASK;
+		}
 	}
 
 	fpsimd_flush_task_state(target);

commit 936eb65ca22ad856cb3a995e8cd742e982dc2dd0
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jun 21 16:00:44 2017 +0100

    arm64: ptrace: Flush user-RW TLS reg to thread_struct before reading
    
    When reading current's user-writable TLS register (which occurs
    when dumping core for native tasks), it is possible that userspace
    has modified it since the time the task was last scheduled out.
    The new TLS register value is not guaranteed to have been written
    immediately back to thread_struct in this case.
    
    As a result, a coredump can capture stale data for this register.
    Reading the register for a stopped task via ptrace is unaffected.
    
    For native tasks, this patch explicitly flushes the TPIDR_EL0
    register back to thread_struct before dumping when operating on
    current, thus ensuring that coredump contents are up to date.  For
    compat tasks, the TLS register is not user-writable and so cannot
    be out of sync, so no flush is required in compat_tls_get().
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index eeef01a219a6..35846f1550db 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -652,6 +652,10 @@ static int tls_get(struct task_struct *target, const struct user_regset *regset,
 		   void *kbuf, void __user *ubuf)
 {
 	unsigned long *tls = &target->thread.tp_value;
+
+	if (target == current)
+		tls_preserve_current_state();
+
 	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, tls, 0, -1);
 }
 

commit e1d5a8fb73e6c65280c21ec188180345649ee650
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jun 21 16:00:43 2017 +0100

    arm64: ptrace: Flush FPSIMD regs back to thread_struct before reading
    
    When reading the FPSIMD state of current (which occurs when dumping
    core), it is possible that userspace has modified the FPSIMD
    registers since the time the task was last scheduled out.  Such
    changes are not guaranteed to be reflected immedately in
    thread_struct.
    
    As a result, a coredump can contain stale values for these
    registers.  Reading the registers of a stopped task via ptrace is
    unaffected.
    
    This patch explicitly flushes the CPU state back to thread_struct
    before dumping when operating on current, thus ensuring that
    coredump contents are up to date.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 0e5aaec5b751..eeef01a219a6 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -623,6 +623,10 @@ static int fpr_get(struct task_struct *target, const struct user_regset *regset,
 {
 	struct user_fpsimd_state *uregs;
 	uregs = &target->thread.fpsimd_state.user_fpsimd;
+
+	if (target == current)
+		fpsimd_preserve_current_state();
+
 	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs, 0, -1);
 }
 
@@ -898,6 +902,9 @@ static int compat_vfp_get(struct task_struct *target,
 
 	uregs = &target->thread.fpsimd_state.user_fpsimd;
 
+	if (target == current)
+		fpsimd_preserve_current_state();
+
 	/*
 	 * The VFP registers are packed into the fpsimd_state, so they all sit
 	 * nicely together for us. We just need to create the fpscr separately.

commit af66b2d88a76574d55e81d712292abd34beb6178
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jun 21 16:00:42 2017 +0100

    arm64: ptrace: Fix VFP register dumping in compat coredumps
    
    Currently, VFP registers are omitted from coredumps for compat
    processes, due to a bug in the REGSET_COMPAT_VFP regset
    implementation.
    
    compat_vfp_get() needs to transfer non-contiguous data from
    thread_struct.fpsimd_state, and uses put_user() to handle the
    offending trailing word (FPSCR).  This fails when copying to a
    kernel address (i.e., kbuf && !ubuf), which is what happens when
    dumping core.  As a result, the ELF coredump core code silently
    omits the NT_ARM_VFP note from the dump.
    
    It would be possible to work around this with additional special
    case code for the put_user(), but since user_regset_copyout() is
    explicitly designed to handle this scenario it is cleaner to port
    the put_user() to a user_regset_copyout() call, which this patch
    does.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index c142459a88f3..0e5aaec5b751 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -894,7 +894,7 @@ static int compat_vfp_get(struct task_struct *target,
 {
 	struct user_fpsimd_state *uregs;
 	compat_ulong_t fpscr;
-	int ret;
+	int ret, vregs_end_pos;
 
 	uregs = &target->thread.fpsimd_state.user_fpsimd;
 
@@ -902,13 +902,16 @@ static int compat_vfp_get(struct task_struct *target,
 	 * The VFP registers are packed into the fpsimd_state, so they all sit
 	 * nicely together for us. We just need to create the fpscr separately.
 	 */
-	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs, 0,
-				  VFP_STATE_SIZE - sizeof(compat_ulong_t));
+	vregs_end_pos = VFP_STATE_SIZE - sizeof(compat_ulong_t);
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs,
+				  0, vregs_end_pos);
 
 	if (count && !ret) {
 		fpscr = (uregs->fpsr & VFP_FPSCR_STAT_MASK) |
 			(uregs->fpcr & VFP_FPSCR_CTRL_MASK);
-		ret = put_user(fpscr, (compat_ulong_t *)ubuf);
+
+		ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &fpscr,
+					  vregs_end_pos, VFP_STATE_SIZE);
 	}
 
 	return ret;

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 64fc32ea3422..c142459a88f3 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -23,6 +23,7 @@
 #include <linux/compat.h>
 #include <linux/kernel.h>
 #include <linux/sched/signal.h>
+#include <linux/sched/task_stack.h>
 #include <linux/mm.h>
 #include <linux/smp.h>
 #include <linux/ptrace.h>

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index a22161ccf447..64fc32ea3422 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -22,7 +22,7 @@
 #include <linux/audit.h>
 #include <linux/compat.h>
 #include <linux/kernel.h>
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/mm.h>
 #include <linux/smp.h>
 #include <linux/ptrace.h>

commit ad9e202aa1ce571b1d7fed969d06f66067f8a086
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jan 18 16:25:24 2017 +0000

    arm64/ptrace: Reject attempts to set incomplete hardware breakpoint fields
    
    We cannot preserve partial fields for hardware breakpoints, because
    the values written by userspace to the hardware breakpoint
    registers can't subsequently be recovered intact from the hardware.
    
    So, just reject attempts to write incomplete fields with -EINVAL.
    
    Cc: <stable@vger.kernel.org> # 3.7.x-
    Fixes: 478fcb2cdb23 ("arm64: Debugging support")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <Will.Deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 91c4719dfc00..a22161ccf447 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -551,6 +551,8 @@ static int hw_break_set(struct task_struct *target,
 	/* (address, ctrl) registers */
 	limit = regset->n * regset->size;
 	while (count && offset < limit) {
+		if (count < PTRACE_HBP_ADDR_SZ)
+			return -EINVAL;
 		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &addr,
 					 offset, offset + PTRACE_HBP_ADDR_SZ);
 		if (ret)
@@ -560,6 +562,8 @@ static int hw_break_set(struct task_struct *target,
 			return ret;
 		offset += PTRACE_HBP_ADDR_SZ;
 
+		if (!count)
+			break;
 		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &ctrl,
 					 offset, offset + PTRACE_HBP_CTRL_SZ);
 		if (ret)

commit a672401c00f82e4e19704aff361d9bad18003714
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jan 18 16:25:22 2017 +0000

    arm64/ptrace: Preserve previous registers for short regset write
    
    Ensure that if userspace supplies insufficient data to
    PTRACE_SETREGSET to fill all the registers, the thread's old
    registers are preserved.
    
    Cc: <stable@vger.kernel.org> # 4.3.x-
    Fixes: 5d220ff9420f ("arm64: Better native ptrace support for compat tasks")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <Will.Deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index ead36d5fad6d..91c4719dfc00 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -950,7 +950,7 @@ static int compat_tls_set(struct task_struct *target,
 			  const void __user *ubuf)
 {
 	int ret;
-	compat_ulong_t tls;
+	compat_ulong_t tls = target->thread.tp_value;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
 	if (ret)

commit 9dd73f72f218320c6c90da5f834996e7360dc227
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jan 18 16:25:21 2017 +0000

    arm64/ptrace: Preserve previous registers for short regset write
    
    Ensure that if userspace supplies insufficient data to
    PTRACE_SETREGSET to fill all the registers, the thread's old
    registers are preserved.
    
    Cc: <stable@vger.kernel.org> # 3.19.x-
    Fixes: 766a85d7bc5d ("arm64: ptrace: add NT_ARM_SYSTEM_CALL regset")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <Will.Deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 8c0bc3434f55..ead36d5fad6d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -677,7 +677,8 @@ static int system_call_set(struct task_struct *target,
 			   unsigned int pos, unsigned int count,
 			   const void *kbuf, const void __user *ubuf)
 {
-	int syscallno, ret;
+	int syscallno = task_pt_regs(target)->syscallno;
+	int ret;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &syscallno, 0, -1);
 	if (ret)

commit 9a17b876b573441bfb3387ad55d98bf7184daf9d
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Wed Jan 18 16:25:20 2017 +0000

    arm64/ptrace: Preserve previous registers for short regset write
    
    Ensure that if userspace supplies insufficient data to
    PTRACE_SETREGSET to fill all the registers, the thread's old
    registers are preserved.
    
    Cc: <stable@vger.kernel.org> # 3.7.x-
    Fixes: 478fcb2cdb23 ("arm64: Debugging support")
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Will Deacon <Will.Deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index fc35e06ccaac..8c0bc3434f55 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -596,7 +596,7 @@ static int gpr_set(struct task_struct *target, const struct user_regset *regset,
 		   const void *kbuf, const void __user *ubuf)
 {
 	int ret;
-	struct user_pt_regs newregs;
+	struct user_pt_regs newregs = task_pt_regs(target)->user_regs;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newregs, 0, -1);
 	if (ret)
@@ -626,7 +626,8 @@ static int fpr_set(struct task_struct *target, const struct user_regset *regset,
 		   const void *kbuf, const void __user *ubuf)
 {
 	int ret;
-	struct user_fpsimd_state newstate;
+	struct user_fpsimd_state newstate =
+		target->thread.fpsimd_state.user_fpsimd;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate, 0, -1);
 	if (ret)
@@ -650,7 +651,7 @@ static int tls_set(struct task_struct *target, const struct user_regset *regset,
 		   const void *kbuf, const void __user *ubuf)
 {
 	int ret;
-	unsigned long tls;
+	unsigned long tls = target->thread.tp_value;
 
 	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
 	if (ret)

commit b08fb180bb8802d1c599beb1acd6a3b26163b4f6
Author: Pratyush Anand <panand@redhat.com>
Date:   Mon Nov 14 19:32:43 2016 +0530

    arm64: Allow hw watchpoint at varied offset from base address
    
    ARM64 hardware supports watchpoint at any double word aligned address.
    However, it can select any consecutive bytes from offset 0 to 7 from that
    base address. For example, if base address is programmed as 0x420030 and
    byte select is 0x1C, then access of 0x420032,0x420033 and 0x420034 will
    generate a watchpoint exception.
    
    Currently, we do not have such modularity. We can only program byte,
    halfword, word and double word access exception from any base address.
    
    This patch adds support to overcome above limitations.
    
    Signed-off-by: Pratyush Anand <panand@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index e0c81da60f76..fc35e06ccaac 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -327,13 +327,13 @@ static int ptrace_hbp_fill_attr_ctrl(unsigned int note_type,
 				     struct arch_hw_breakpoint_ctrl ctrl,
 				     struct perf_event_attr *attr)
 {
-	int err, len, type, disabled = !ctrl.enabled;
+	int err, len, type, offset, disabled = !ctrl.enabled;
 
 	attr->disabled = disabled;
 	if (disabled)
 		return 0;
 
-	err = arch_bp_generic_fields(ctrl, &len, &type);
+	err = arch_bp_generic_fields(ctrl, &len, &type, &offset);
 	if (err)
 		return err;
 
@@ -352,6 +352,7 @@ static int ptrace_hbp_fill_attr_ctrl(unsigned int note_type,
 
 	attr->bp_len	= len;
 	attr->bp_type	= type;
+	attr->bp_addr	+= offset;
 
 	return 0;
 }
@@ -404,7 +405,7 @@ static int ptrace_hbp_get_addr(unsigned int note_type,
 	if (IS_ERR(bp))
 		return PTR_ERR(bp);
 
-	*addr = bp ? bp->attr.bp_addr : 0;
+	*addr = bp ? counter_arch_bp(bp)->address : 0;
 	return 0;
 }
 

commit 7a1e8b80fb1e8ead4cec15d1fc494ed290e4d2e9
Merge: a867d7349e94 7616ac70d1bb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 29 17:38:46 2016 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security
    
    Pull security subsystem updates from James Morris:
     "Highlights:
    
       - TPM core and driver updates/fixes
       - IPv6 security labeling (CALIPSO)
       - Lots of Apparmor fixes
       - Seccomp: remove 2-phase API, close hole where ptrace can change
         syscall #"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/jmorris/linux-security: (156 commits)
      apparmor: fix SECURITY_APPARMOR_HASH_DEFAULT parameter handling
      tpm: Add TPM 2.0 support to the Nuvoton i2c driver (NPCT6xx family)
      tpm: Factor out common startup code
      tpm: use devm_add_action_or_reset
      tpm2_i2c_nuvoton: add irq validity check
      tpm: read burstcount from TPM_STS in one 32-bit transaction
      tpm: fix byte-order for the value read by tpm2_get_tpm_pt
      tpm_tis_core: convert max timeouts from msec to jiffies
      apparmor: fix arg_size computation for when setprocattr is null terminated
      apparmor: fix oops, validate buffer size in apparmor_setprocattr()
      apparmor: do not expose kernel stack
      apparmor: fix module parameters can be changed after policy is locked
      apparmor: fix oops in profile_unpack() when policy_db is not present
      apparmor: don't check for vmalloc_addr if kvzalloc() failed
      apparmor: add missing id bounds check on dfa verification
      apparmor: allow SYS_CAP_RESOURCE to be sufficient to prlimit another task
      apparmor: use list_next_entry instead of list_entry_next
      apparmor: fix refcount race when finding a child profile
      apparmor: fix ref count leak when profile sha1 hash is read
      apparmor: check that xindex is in trans_table bounds
      ...

commit 0a8ea52c3eb157dd65e224fc95b7c9c99fcba9f7
Author: David A. Long <dave.long@linaro.org>
Date:   Fri Jul 8 12:35:45 2016 -0400

    arm64: Add HAVE_REGS_AND_STACK_ACCESS_API feature
    
    Add HAVE_REGS_AND_STACK_ACCESS_API feature for arm64, including supporting
    functions and defines.
    
    Signed-off-by: David A. Long <dave.long@linaro.org>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    [catalin.marinas@arm.com: Remove unused functions]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 3f6cd5c5234f..030c1d5aa46d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -48,6 +48,107 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/syscalls.h>
 
+struct pt_regs_offset {
+	const char *name;
+	int offset;
+};
+
+#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}
+#define REG_OFFSET_END {.name = NULL, .offset = 0}
+#define GPR_OFFSET_NAME(r) \
+	{.name = "x" #r, .offset = offsetof(struct pt_regs, regs[r])}
+
+static const struct pt_regs_offset regoffset_table[] = {
+	GPR_OFFSET_NAME(0),
+	GPR_OFFSET_NAME(1),
+	GPR_OFFSET_NAME(2),
+	GPR_OFFSET_NAME(3),
+	GPR_OFFSET_NAME(4),
+	GPR_OFFSET_NAME(5),
+	GPR_OFFSET_NAME(6),
+	GPR_OFFSET_NAME(7),
+	GPR_OFFSET_NAME(8),
+	GPR_OFFSET_NAME(9),
+	GPR_OFFSET_NAME(10),
+	GPR_OFFSET_NAME(11),
+	GPR_OFFSET_NAME(12),
+	GPR_OFFSET_NAME(13),
+	GPR_OFFSET_NAME(14),
+	GPR_OFFSET_NAME(15),
+	GPR_OFFSET_NAME(16),
+	GPR_OFFSET_NAME(17),
+	GPR_OFFSET_NAME(18),
+	GPR_OFFSET_NAME(19),
+	GPR_OFFSET_NAME(20),
+	GPR_OFFSET_NAME(21),
+	GPR_OFFSET_NAME(22),
+	GPR_OFFSET_NAME(23),
+	GPR_OFFSET_NAME(24),
+	GPR_OFFSET_NAME(25),
+	GPR_OFFSET_NAME(26),
+	GPR_OFFSET_NAME(27),
+	GPR_OFFSET_NAME(28),
+	GPR_OFFSET_NAME(29),
+	GPR_OFFSET_NAME(30),
+	{.name = "lr", .offset = offsetof(struct pt_regs, regs[30])},
+	REG_OFFSET_NAME(sp),
+	REG_OFFSET_NAME(pc),
+	REG_OFFSET_NAME(pstate),
+	REG_OFFSET_END,
+};
+
+/**
+ * regs_query_register_offset() - query register offset from its name
+ * @name:	the name of a register
+ *
+ * regs_query_register_offset() returns the offset of a register in struct
+ * pt_regs from its name. If the name is invalid, this returns -EINVAL;
+ */
+int regs_query_register_offset(const char *name)
+{
+	const struct pt_regs_offset *roff;
+
+	for (roff = regoffset_table; roff->name != NULL; roff++)
+		if (!strcmp(roff->name, name))
+			return roff->offset;
+	return -EINVAL;
+}
+
+/**
+ * regs_within_kernel_stack() - check the address in the stack
+ * @regs:      pt_regs which contains kernel stack pointer.
+ * @addr:      address which is checked.
+ *
+ * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
+ * If @addr is within the kernel stack, it returns true. If not, returns false.
+ */
+static bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
+{
+	return ((addr & ~(THREAD_SIZE - 1))  ==
+		(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1))) ||
+		on_irq_stack(addr, raw_smp_processor_id());
+}
+
+/**
+ * regs_get_kernel_stack_nth() - get Nth entry of the stack
+ * @regs:	pt_regs which contains kernel stack pointer.
+ * @n:		stack entry number.
+ *
+ * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
+ * is specified by @regs. If the @n th entry is NOT in the kernel stack,
+ * this returns 0.
+ */
+unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)
+{
+	unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);
+
+	addr += n;
+	if (regs_within_kernel_stack(regs, (unsigned long)addr))
+		return *addr;
+	else
+		return 0;
+}
+
 /*
  * TODO: does not yet catch signals sent when the child dies.
  * in exit.c or in signal.c.

commit a5cd110cb8369d6b37ef5ccfe56b3fa1338c9615
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Jun 2 12:28:52 2016 -0700

    arm64/ptrace: run seccomp after ptrace
    
    Close the hole where ptrace can change a syscall out from under seccomp.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6e2cf046615d..602316c97a47 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1246,13 +1246,13 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 
 asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 {
-	/* Do the secure computing check first; failures should be fast. */
-	if (secure_computing(NULL) == -1)
-		return -1;
-
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
 
+	/* Do the secure computing after ptrace; failures should be fast. */
+	if (secure_computing(NULL) == -1)
+		return -1;
+
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);
 

commit 2f275de5d1ed7269913ef9b4c64a13952c0a38e8
Author: Andy Lutomirski <luto@kernel.org>
Date:   Fri May 27 12:57:02 2016 -0700

    seccomp: Add a seccomp_data parameter secure_computing()
    
    Currently, if arch code wants to supply seccomp_data directly to
    seccomp (which is generally much faster than having seccomp do it
    using the syscall_get_xyz() API), it has to use the two-phase
    seccomp hooks. Add it to the easy hooks, too.
    
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 3f6cd5c5234f..6e2cf046615d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1247,7 +1247,7 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 {
 	/* Do the secure computing check first; failures should be fast. */
-	if (secure_computing() == -1)
+	if (secure_computing(NULL) == -1)
 		return -1;
 
 	if (test_thread_flag(TIF_SYSCALL_TRACE))

commit dbd4d7ca563fd0a8949718d35ce197e5642d5d9d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Mar 1 14:18:50 2016 +0000

    arm64: Rework valid_user_regs
    
    We validate pstate using PSR_MODE32_BIT, which is part of the
    user-provided pstate (and cannot be trusted). Also, we conflate
    validation of AArch32 and AArch64 pstate values, making the code
    difficult to reason about.
    
    Instead, validate the pstate value based on the associated task. The
    task may or may not be current (e.g. when using ptrace), so this must be
    passed explicitly by callers. To avoid circular header dependencies via
    sched.h, is_compat_task is pulled out of asm/ptrace.h.
    
    To make the code possible to reason about, the AArch64 and AArch32
    validation is split into separate functions. Software must respect the
    RES0 policy for SPSR bits, and thus the kernel mirrors the hardware
    policy (RAZ/WI) for bits as-yet unallocated. When these acquire an
    architected meaning writes may be permitted (potentially with additional
    validation).
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Peter Maydell <peter.maydell@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index ff7f13239515..3f6cd5c5234f 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -500,7 +500,7 @@ static int gpr_set(struct task_struct *target, const struct user_regset *regset,
 	if (ret)
 		return ret;
 
-	if (!valid_user_regs(&newregs))
+	if (!valid_user_regs(&newregs, target))
 		return -EINVAL;
 
 	task_pt_regs(target)->user_regs = newregs;
@@ -770,7 +770,7 @@ static int compat_gpr_set(struct task_struct *target,
 
 	}
 
-	if (valid_user_regs(&newregs.user_regs))
+	if (valid_user_regs(&newregs.user_regs, target))
 		*task_pt_regs(target) = newregs;
 	else
 		ret = -EINVAL;
@@ -1272,3 +1272,79 @@ asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);
 }
+
+/*
+ * Bits which are always architecturally RES0 per ARM DDI 0487A.h
+ * Userspace cannot use these until they have an architectural meaning.
+ * We also reserve IL for the kernel; SS is handled dynamically.
+ */
+#define SPSR_EL1_AARCH64_RES0_BITS \
+	(GENMASK_ULL(63,32) | GENMASK_ULL(27, 22) | GENMASK_ULL(20, 10) | \
+	 GENMASK_ULL(5, 5))
+#define SPSR_EL1_AARCH32_RES0_BITS \
+	(GENMASK_ULL(63,32) | GENMASK_ULL(24, 22) | GENMASK_ULL(20,20))
+
+static int valid_compat_regs(struct user_pt_regs *regs)
+{
+	regs->pstate &= ~SPSR_EL1_AARCH32_RES0_BITS;
+
+	if (!system_supports_mixed_endian_el0()) {
+		if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+			regs->pstate |= COMPAT_PSR_E_BIT;
+		else
+			regs->pstate &= ~COMPAT_PSR_E_BIT;
+	}
+
+	if (user_mode(regs) && (regs->pstate & PSR_MODE32_BIT) &&
+	    (regs->pstate & COMPAT_PSR_A_BIT) == 0 &&
+	    (regs->pstate & COMPAT_PSR_I_BIT) == 0 &&
+	    (regs->pstate & COMPAT_PSR_F_BIT) == 0) {
+		return 1;
+	}
+
+	/*
+	 * Force PSR to a valid 32-bit EL0t, preserving the same bits as
+	 * arch/arm.
+	 */
+	regs->pstate &= COMPAT_PSR_N_BIT | COMPAT_PSR_Z_BIT |
+			COMPAT_PSR_C_BIT | COMPAT_PSR_V_BIT |
+			COMPAT_PSR_Q_BIT | COMPAT_PSR_IT_MASK |
+			COMPAT_PSR_GE_MASK | COMPAT_PSR_E_BIT |
+			COMPAT_PSR_T_BIT;
+	regs->pstate |= PSR_MODE32_BIT;
+
+	return 0;
+}
+
+static int valid_native_regs(struct user_pt_regs *regs)
+{
+	regs->pstate &= ~SPSR_EL1_AARCH64_RES0_BITS;
+
+	if (user_mode(regs) && !(regs->pstate & PSR_MODE32_BIT) &&
+	    (regs->pstate & PSR_D_BIT) == 0 &&
+	    (regs->pstate & PSR_A_BIT) == 0 &&
+	    (regs->pstate & PSR_I_BIT) == 0 &&
+	    (regs->pstate & PSR_F_BIT) == 0) {
+		return 1;
+	}
+
+	/* Force PSR to a valid 64-bit EL0t */
+	regs->pstate &= PSR_N_BIT | PSR_Z_BIT | PSR_C_BIT | PSR_V_BIT;
+
+	return 0;
+}
+
+/*
+ * Are the current registers suitable for user mode? (used to maintain
+ * security in signal handlers)
+ */
+int valid_user_regs(struct user_pt_regs *regs, struct task_struct *task)
+{
+	if (!test_tsk_thread_flag(task, TIF_SINGLESTEP))
+		regs->pstate &= ~DBG_SPSR_SS;
+
+	if (is_compat_thread(task_thread_info(task)))
+		return valid_compat_regs(regs);
+	else
+		return valid_native_regs(regs);
+}

commit 5db4fd8c52810bd9740c1240ebf89223b171aa70
Author: John Blackwood <john.blackwood@ccur.com>
Date:   Mon Dec 7 11:50:34 2015 +0000

    arm64: Clear out any singlestep state on a ptrace detach operation
    
    Make sure to clear out any ptrace singlestep state when a ptrace(2)
    PTRACE_DETACH call is made on arm64 systems.
    
    Otherwise, the previously ptraced task will die off with a SIGTRAP
    signal if the debugger just previously singlestepped the ptraced task.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: John Blackwood <john.blackwood@ccur.com>
    [will: added comment to justify why this is in the arch code]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 1971f491bb90..ff7f13239515 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -58,6 +58,12 @@
  */
 void ptrace_disable(struct task_struct *child)
 {
+	/*
+	 * This would be better off in core code, but PTRACE_DETACH has
+	 * grown its fair share of arch-specific worts and changing it
+	 * is likely to cause regressions on obscure architectures.
+	 */
+	user_disable_single_step(child);
 }
 
 #ifdef CONFIG_HAVE_HW_BREAKPOINT

commit 5d220ff9420f8b1689805ba2d938bedf9e0860a4
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 14 16:20:17 2015 +0100

    arm64: Better native ptrace support for compat tasks
    
    The compat ptrace interface allows access to the TLS register, hardware
    breakpoints and watchpoints, syscall number. However, a native task
    using the native ptrace interface to debug compat tasks (e.g. multi-arch
    gdb) only has access to the general and VFP register sets. The compat
    ptrace interface cannot be accessed from a native task.
    
    This patch adds a new user_aarch32_ptrace_view which contains the TLS,
    hardware breakpoint/watchpoint and syscall number regsets in addition to
    the existing GPR and VFP regsets. This view is backwards compatible with
    the previous kernels. Core dumping of 32-bit tasks and compat ptrace are
    not affected since the original user_aarch32_view is preserved.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Yao Qi <yao.qi@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index d882b833dbdb..1971f491bb90 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -826,6 +826,30 @@ static int compat_vfp_set(struct task_struct *target,
 	return ret;
 }
 
+static int compat_tls_get(struct task_struct *target,
+			  const struct user_regset *regset, unsigned int pos,
+			  unsigned int count, void *kbuf, void __user *ubuf)
+{
+	compat_ulong_t tls = (compat_ulong_t)target->thread.tp_value;
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
+}
+
+static int compat_tls_set(struct task_struct *target,
+			  const struct user_regset *regset, unsigned int pos,
+			  unsigned int count, const void *kbuf,
+			  const void __user *ubuf)
+{
+	int ret;
+	compat_ulong_t tls;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
+	if (ret)
+		return ret;
+
+	target->thread.tp_value = tls;
+	return ret;
+}
+
 static const struct user_regset aarch32_regsets[] = {
 	[REGSET_COMPAT_GPR] = {
 		.core_note_type = NT_PRSTATUS,
@@ -850,6 +874,64 @@ static const struct user_regset_view user_aarch32_view = {
 	.regsets = aarch32_regsets, .n = ARRAY_SIZE(aarch32_regsets)
 };
 
+static const struct user_regset aarch32_ptrace_regsets[] = {
+	[REGSET_GPR] = {
+		.core_note_type = NT_PRSTATUS,
+		.n = COMPAT_ELF_NGREG,
+		.size = sizeof(compat_elf_greg_t),
+		.align = sizeof(compat_elf_greg_t),
+		.get = compat_gpr_get,
+		.set = compat_gpr_set
+	},
+	[REGSET_FPR] = {
+		.core_note_type = NT_ARM_VFP,
+		.n = VFP_STATE_SIZE / sizeof(compat_ulong_t),
+		.size = sizeof(compat_ulong_t),
+		.align = sizeof(compat_ulong_t),
+		.get = compat_vfp_get,
+		.set = compat_vfp_set
+	},
+	[REGSET_TLS] = {
+		.core_note_type = NT_ARM_TLS,
+		.n = 1,
+		.size = sizeof(compat_ulong_t),
+		.align = sizeof(compat_ulong_t),
+		.get = compat_tls_get,
+		.set = compat_tls_set,
+	},
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	[REGSET_HW_BREAK] = {
+		.core_note_type = NT_ARM_HW_BREAK,
+		.n = sizeof(struct user_hwdebug_state) / sizeof(u32),
+		.size = sizeof(u32),
+		.align = sizeof(u32),
+		.get = hw_break_get,
+		.set = hw_break_set,
+	},
+	[REGSET_HW_WATCH] = {
+		.core_note_type = NT_ARM_HW_WATCH,
+		.n = sizeof(struct user_hwdebug_state) / sizeof(u32),
+		.size = sizeof(u32),
+		.align = sizeof(u32),
+		.get = hw_break_get,
+		.set = hw_break_set,
+	},
+#endif
+	[REGSET_SYSTEM_CALL] = {
+		.core_note_type = NT_ARM_SYSTEM_CALL,
+		.n = 1,
+		.size = sizeof(int),
+		.align = sizeof(int),
+		.get = system_call_get,
+		.set = system_call_set,
+	},
+};
+
+static const struct user_regset_view user_aarch32_ptrace_view = {
+	.name = "aarch32", .e_machine = EM_ARM,
+	.regsets = aarch32_ptrace_regsets, .n = ARRAY_SIZE(aarch32_ptrace_regsets)
+};
+
 static int compat_ptrace_read_user(struct task_struct *tsk, compat_ulong_t off,
 				   compat_ulong_t __user *ret)
 {
@@ -1109,8 +1191,16 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 const struct user_regset_view *task_user_regset_view(struct task_struct *task)
 {
 #ifdef CONFIG_COMPAT
-	if (is_compat_thread(task_thread_info(task)))
+	/*
+	 * Core dumping of 32-bit tasks or compat ptrace requests must use the
+	 * user_aarch32_view compatible with arm32. Native ptrace requests on
+	 * 32-bit children use an extended user_aarch32_ptrace_view to allow
+	 * access to the TLS register.
+	 */
+	if (is_compat_task())
 		return &user_aarch32_view;
+	else if (is_compat_thread(task_thread_info(task)))
+		return &user_aarch32_ptrace_view;
 #endif
 	return &user_aarch64_view;
 }

commit a1ae65b219416a72c15577bd4c8c11174fffbb8b
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Nov 28 05:26:39 2014 +0000

    arm64: add seccomp support
    
    secure_computing() is called first in syscall_trace_enter() so that
    a system call will be aborted quickly without doing succeeding syscall
    tracing if seccomp rules want to deny that system call.
    
    On compat task, syscall numbers for system calls allowed in seccomp mode 1
    are different from those on normal tasks, and so _NR_seccomp_xxx_32's need
    to be redefined.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index f576781d8d3b..d882b833dbdb 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -27,6 +27,7 @@
 #include <linux/smp.h>
 #include <linux/ptrace.h>
 #include <linux/user.h>
+#include <linux/seccomp.h>
 #include <linux/security.h>
 #include <linux/init.h>
 #include <linux/signal.h>
@@ -1149,6 +1150,10 @@ static void tracehook_report_syscall(struct pt_regs *regs,
 
 asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 {
+	/* Do the secure computing check first; failures should be fast. */
+	if (secure_computing() == -1)
+		return -1;
+
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
 

commit 766a85d7bc5d7f1ddd6de28bdb844eae45ec63b0
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Nov 28 05:26:34 2014 +0000

    arm64: ptrace: add NT_ARM_SYSTEM_CALL regset
    
    This regeset is intended to be used to get and set a system call number
    while tracing.
    There was some discussion about possible approaches to do so:
    
    (1) modify x8 register with ptrace(PTRACE_SETREGSET) indirectly,
        and update regs->syscallno later on in syscall_trace_enter(), or
    (2) define a dedicated regset for this purpose as on s390, or
    (3) support ptrace(PTRACE_SET_SYSCALL) as on arch/arm
    
    Thinking of the fact that user_pt_regs doesn't expose 'syscallno' to
    tracer as well as that secure_computing() expects a changed syscall number,
    especially case of -1, to be visible before this function returns in
    syscall_trace_enter(), (1) doesn't work well.
    We will take (2) since it looks much cleaner.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 8a4ae8e73213..f576781d8d3b 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -551,6 +551,32 @@ static int tls_set(struct task_struct *target, const struct user_regset *regset,
 	return ret;
 }
 
+static int system_call_get(struct task_struct *target,
+			   const struct user_regset *regset,
+			   unsigned int pos, unsigned int count,
+			   void *kbuf, void __user *ubuf)
+{
+	int syscallno = task_pt_regs(target)->syscallno;
+
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf,
+				   &syscallno, 0, -1);
+}
+
+static int system_call_set(struct task_struct *target,
+			   const struct user_regset *regset,
+			   unsigned int pos, unsigned int count,
+			   const void *kbuf, const void __user *ubuf)
+{
+	int syscallno, ret;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &syscallno, 0, -1);
+	if (ret)
+		return ret;
+
+	task_pt_regs(target)->syscallno = syscallno;
+	return ret;
+}
+
 enum aarch64_regset {
 	REGSET_GPR,
 	REGSET_FPR,
@@ -559,6 +585,7 @@ enum aarch64_regset {
 	REGSET_HW_BREAK,
 	REGSET_HW_WATCH,
 #endif
+	REGSET_SYSTEM_CALL,
 };
 
 static const struct user_regset aarch64_regsets[] = {
@@ -608,6 +635,14 @@ static const struct user_regset aarch64_regsets[] = {
 		.set = hw_break_set,
 	},
 #endif
+	[REGSET_SYSTEM_CALL] = {
+		.core_note_type = NT_ARM_SYSTEM_CALL,
+		.n = 1,
+		.size = sizeof(int),
+		.align = sizeof(int),
+		.get = system_call_get,
+		.set = system_call_set,
+	},
 };
 
 static const struct user_regset_view user_aarch64_view = {

commit ab074ade9c33b3585da86d62e87bcb3e897a3f54
Merge: 61ed53deb1c6 2991dd2b0117
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 19 16:25:56 2014 -0700

    Merge git://git.infradead.org/users/eparis/audit
    
    Pull audit updates from Eric Paris:
     "So this change across a whole bunch of arches really solves one basic
      problem.  We want to audit when seccomp is killing a process.  seccomp
      hooks in before the audit syscall entry code.  audit_syscall_entry
      took as an argument the arch of the given syscall.  Since the arch is
      part of what makes a syscall number meaningful it's an important part
      of the record, but it isn't available when seccomp shoots the
      syscall...
    
      For most arch's we have a better way to get the arch (syscall_get_arch)
      So the solution was two fold: Implement syscall_get_arch() everywhere
      there is audit which didn't have it.  Use syscall_get_arch() in the
      seccomp audit code.  Having syscall_get_arch() everywhere meant it was
      a useless flag on the stack and we could get rid of it for the typical
      syscall entry.
    
      The other changes inside the audit system aren't grand, fixed some
      records that had invalid spaces.  Better locking around the task comm
      field.  Removing some dead functions and structs.  Make some things
      static.  Really minor stuff"
    
    * git://git.infradead.org/users/eparis/audit: (31 commits)
      audit: rename audit_log_remove_rule to disambiguate for trees
      audit: cull redundancy in audit_rule_change
      audit: WARN if audit_rule_change called illegally
      audit: put rule existence check in canonical order
      next: openrisc: Fix build
      audit: get comm using lock to avoid race in string printing
      audit: remove open_arg() function that is never used
      audit: correct AUDIT_GET_FEATURE return message type
      audit: set nlmsg_len for multicast messages.
      audit: use union for audit_field values since they are mutually exclusive
      audit: invalid op= values for rules
      audit: use atomic_t to simplify audit_serial()
      kernel/audit.c: use ARRAY_SIZE instead of sizeof/sizeof[0]
      audit: reduce scope of audit_log_fcaps
      audit: reduce scope of audit_net_id
      audit: arm64: Remove the audit arch argument to audit_syscall_entry
      arm64: audit: Add audit hook in syscall_trace_enter/exit()
      audit: x86: drop arch from __audit_syscall_entry() interface
      sparc: implement is_32bit_task
      sparc: properly conditionalize use of TIF_32BIT
      ...

commit 4913c59890b0774990cceb7b0539fee71301dabe
Author: Eric Paris <eparis@redhat.com>
Date:   Tue Sep 23 16:25:34 2014 -0400

    audit: arm64: Remove the audit arch argument to audit_syscall_entry
    
    The arm64 tree added calls to audit_syscall_entry() and rightly included
    the syscall number.  The interface has since been changed to not need
    the syscall number.  As such, arm64 should no longer pass that value.
    
    Signed-off-by: Eric Paris <eparis@redhat.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 70526cfda056..310842e3d477 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1115,8 +1115,8 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);
 
-	audit_syscall_entry(syscall_get_arch(), regs->syscallno,
-		regs->orig_x0, regs->regs[1], regs->regs[2], regs->regs[3]);
+	audit_syscall_entry(regs->syscallno, regs->orig_x0, regs->regs[1],
+			    regs->regs[2], regs->regs[3]);
 
 	return regs->syscallno;
 }

commit 23fed6218d7f6ce6e4cde698579f0f51845f0b25
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Jul 4 08:28:31 2014 +0100

    arm64: audit: Add audit hook in syscall_trace_enter/exit()
    
    This patch adds auditing functions on entry to or exit from
    every system call invocation.
    
    Acked-by: Richard Guy Briggs <rgb@redhat.com>
    Acked-by Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 9fde010c945f..70526cfda056 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -19,6 +19,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/audit.h>
 #include <linux/compat.h>
 #include <linux/kernel.h>
 #include <linux/sched.h>
@@ -39,6 +40,7 @@
 #include <asm/compat.h>
 #include <asm/debug-monitors.h>
 #include <asm/pgtable.h>
+#include <asm/syscall.h>
 #include <asm/traps.h>
 #include <asm/system_misc.h>
 
@@ -1113,11 +1115,16 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);
 
+	audit_syscall_entry(syscall_get_arch(), regs->syscallno,
+		regs->orig_x0, regs->regs[1], regs->regs[2], regs->regs[3]);
+
 	return regs->syscallno;
 }
 
 asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 {
+	audit_syscall_exit(regs);
+
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_exit(regs, regs_return_value(regs));
 

commit 85487edd252fa04718dcd735bc0f41213bbb9546
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Aug 22 14:20:24 2014 +0100

    arm64: ptrace: fix compat reg getter/setter return values
    
    copy_{to,from}_user return the number of bytes remaining on failure, not
    an error code.
    
    This patch returns -EFAULT when the copy operation didn't complete,
    rather than expose the number of bytes not copied directly to userspace.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 2ac998878001..fe63ac5e9bf5 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -663,8 +663,10 @@ static int compat_gpr_get(struct task_struct *target,
 			kbuf += sizeof(reg);
 		} else {
 			ret = copy_to_user(ubuf, &reg, sizeof(reg));
-			if (ret)
+			if (ret) {
+				ret = -EFAULT;
 				break;
+			}
 
 			ubuf += sizeof(reg);
 		}
@@ -702,8 +704,10 @@ static int compat_gpr_set(struct task_struct *target,
 			kbuf += sizeof(reg);
 		} else {
 			ret = copy_from_user(&reg, ubuf, sizeof(reg));
-			if (ret)
-				return ret;
+			if (ret) {
+				ret = -EFAULT;
+				break;
+			}
 
 			ubuf += sizeof(reg);
 		}

commit 27d7ff273c2aad37b28f6ff0cab2cfa35b51e648
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Aug 22 14:13:24 2014 +0100

    arm64: ptrace: fix compat hardware watchpoint reporting
    
    I'm not sure what I was on when I wrote this, but when iterating over
    the hardware watchpoint array (hbp_watch_array), our index is off by
    ARM_MAX_BRP, so we walk off the end of our thread_struct...
    
    ... except, a dodgy condition in the loop means that it never executes
    at all (bp cannot be NULL).
    
    This patch fixes the code so that we remove the bp check and use the
    correct index for accessing the watchpoint structures.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 70526cfda056..2ac998878001 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -87,7 +87,8 @@ static void ptrace_hbptriggered(struct perf_event *bp,
 			break;
 		}
 	}
-	for (i = ARM_MAX_BRP; i < ARM_MAX_HBP_SLOTS && !bp; ++i) {
+
+	for (i = 0; i < ARM_MAX_WRP; ++i) {
 		if (current->thread.debug.hbp_watch[i] == bp) {
 			info.si_errno = -((i << 1) + 1);
 			break;

commit 44b375070fa3defa6bf1010bd0e4e64f7986bfc4
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 19 22:05:45 2014 +0100

    Revert "arm64: Do not invoke audit_syscall_* functions if !CONFIG_AUDIT_SYSCALL"
    
    For some reason, the audit patches didn't make it out of -next this
    merge window, so revert our temporary hack and let the audit guys deal
    with fixing up -next.
    
    This reverts commit 2a8f45b040bcb9b2ad2845f061499d1b6f41cc7b.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 0310811bd77d..70526cfda056 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1115,19 +1115,15 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);
 
-#ifdef CONFIG_AUDITSYSCALL
 	audit_syscall_entry(syscall_get_arch(), regs->syscallno,
 		regs->orig_x0, regs->regs[1], regs->regs[2], regs->regs[3]);
-#endif
 
 	return regs->syscallno;
 }
 
 asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 {
-#ifdef CONFIG_AUDITSYSCALL
 	audit_syscall_exit(regs);
-#endif
 
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_exit(regs, regs_return_value(regs));

commit 2a8f45b040bcb9b2ad2845f061499d1b6f41cc7b
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jul 24 14:12:11 2014 +0100

    arm64: Do not invoke audit_syscall_* functions if !CONFIG_AUDIT_SYSCALL
    
    This is a temporary patch to be able to compile the kernel in linux-next
    where the audit_syscall_* API has been changed. To be reverted once the
    proper arm64 fix can be applied.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 70526cfda056..0310811bd77d 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1115,15 +1115,19 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);
 
+#ifdef CONFIG_AUDITSYSCALL
 	audit_syscall_entry(syscall_get_arch(), regs->syscallno,
 		regs->orig_x0, regs->regs[1], regs->regs[2], regs->regs[3]);
+#endif
 
 	return regs->syscallno;
 }
 
 asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 {
+#ifdef CONFIG_AUDITSYSCALL
 	audit_syscall_exit(regs);
+#endif
 
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_exit(regs, regs_return_value(regs));

commit 5701ede884c2221e6ebbb54aec83dc433287bc50
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Jul 4 08:28:31 2014 +0100

    arm64: audit: Add audit hook in syscall_trace_enter/exit()
    
    This patch adds auditing functions on entry to or exit from
    every system call invocation.
    
    Acked-by: Richard Guy Briggs <rgb@redhat.com>
    Acked-by Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 9fde010c945f..70526cfda056 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -19,6 +19,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/audit.h>
 #include <linux/compat.h>
 #include <linux/kernel.h>
 #include <linux/sched.h>
@@ -39,6 +40,7 @@
 #include <asm/compat.h>
 #include <asm/debug-monitors.h>
 #include <asm/pgtable.h>
+#include <asm/syscall.h>
 #include <asm/traps.h>
 #include <asm/system_misc.h>
 
@@ -1113,11 +1115,16 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_enter(regs, regs->syscallno);
 
+	audit_syscall_entry(syscall_get_arch(), regs->syscallno,
+		regs->orig_x0, regs->regs[1], regs->regs[2], regs->regs[3]);
+
 	return regs->syscallno;
 }
 
 asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 {
+	audit_syscall_exit(regs);
+
 	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
 		trace_sys_exit(regs, regs_return_value(regs));
 

commit 2227901a0230d8fde81ba9c602d649839390f56b
Author: Victor Kamensky <victor.kamensky@linaro.org>
Date:   Tue Jun 3 19:21:30 2014 +0100

    arm64: ptrace: fix empty registers set in prstatus of aarch32 process core
    
    Currently core file of aarch32 process prstatus note has empty
    registers set. As result aarch32 core files create by V8 kernel are
    not very useful.
    
    It happens because compat_gpr_get and compat_gpr_set functions can
    copy registers values to/from either kbuf or ubuf. ELF core file
    collection function fill_thread_core_info calls compat_gpr_get
    with kbuf set and ubuf set to 0. But current compat_gpr_get and
    compat_gpr_set function handle copy to/from only ubuf case.
    
    Fix is to handle kbuf and ubuf as two separate cases in similar
    way as other functions like user_regset_copyout, user_regset_copyin do.
    
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 993cdb79b70e..9fde010c945f 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -655,11 +655,16 @@ static int compat_gpr_get(struct task_struct *target,
 			reg = task_pt_regs(target)->regs[idx];
 		}
 
-		ret = copy_to_user(ubuf, &reg, sizeof(reg));
-		if (ret)
-			break;
-
-		ubuf += sizeof(reg);
+		if (kbuf) {
+			memcpy(kbuf, &reg, sizeof(reg));
+			kbuf += sizeof(reg);
+		} else {
+			ret = copy_to_user(ubuf, &reg, sizeof(reg));
+			if (ret)
+				break;
+
+			ubuf += sizeof(reg);
+		}
 	}
 
 	return ret;
@@ -689,11 +694,16 @@ static int compat_gpr_set(struct task_struct *target,
 		unsigned int idx = start + i;
 		compat_ulong_t reg;
 
-		ret = copy_from_user(&reg, ubuf, sizeof(reg));
-		if (ret)
-			return ret;
+		if (kbuf) {
+			memcpy(&reg, kbuf, sizeof(reg));
+			kbuf += sizeof(reg);
+		} else {
+			ret = copy_from_user(&reg, ubuf, sizeof(reg));
+			if (ret)
+				return ret;
 
-		ubuf += sizeof(reg);
+			ubuf += sizeof(reg);
+		}
 
 		switch (idx) {
 		case 15:

commit c168870704bcde6bb63d05f7882b620dd3985a46
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jun 2 11:47:23 2014 +0100

    arm64: ptrace: change fs when passing kernel pointer to regset code
    
    Our compat PTRACE_POKEUSR implementation simply passes the user data to
    regset_copy_from_user after some simple range checking. Unfortunately,
    the data in question has already been copied to the kernel stack by this
    point, so the subsequent access_ok check fails and the ptrace request
    returns -EFAULT. This causes problems tracing fork() with older versions
    of strace.
    
    This patch briefly changes the fs to KERNEL_DS, so that the access_ok
    check passes even with a kernel address.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 3e926b9c0641..993cdb79b70e 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -827,6 +827,7 @@ static int compat_ptrace_write_user(struct task_struct *tsk, compat_ulong_t off,
 				    compat_ulong_t val)
 {
 	int ret;
+	mm_segment_t old_fs = get_fs();
 
 	if (off & 3 || off >= COMPAT_USER_SZ)
 		return -EIO;
@@ -834,10 +835,13 @@ static int compat_ptrace_write_user(struct task_struct *tsk, compat_ulong_t off,
 	if (off >= sizeof(compat_elf_gregset_t))
 		return 0;
 
+	set_fs(KERNEL_DS);
 	ret = copy_regset_from_user(tsk, &user_aarch32_view,
 				    REGSET_COMPAT_GPR, off,
 				    sizeof(compat_ulong_t),
 				    &val);
+	set_fs(old_fs);
+
 	return ret;
 }
 

commit 055b1212d141f1f398fca548f8147787c0b6253f
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:54:36 2014 +0100

    arm64: ftrace: Add system call tracepoint
    
    This patch allows system call entry or exit to be traced as ftrace events,
    ie. sys_enter_*/sys_exit_*, if CONFIG_FTRACE_SYSCALLS is enabled.
    Those events appear and can be controlled under
        ${sysfs}/tracing/events/syscalls/
    
    Please note that we can't trace compat system calls here because
    AArch32 mode does not share the same syscall table with AArch64.
    Just define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS in order to avoid unexpected
    results (bogus syscalls reported or even hang-up).
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 32d52d3b079c..3e926b9c0641 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -42,6 +42,9 @@
 #include <asm/traps.h>
 #include <asm/system_misc.h>
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/syscalls.h>
+
 /*
  * TODO: does not yet catch signals sent when the child dies.
  * in exit.c or in signal.c.
@@ -1093,11 +1096,17 @@ asmlinkage int syscall_trace_enter(struct pt_regs *regs)
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
 
+	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
+		trace_sys_enter(regs, regs->syscallno);
+
 	return regs->syscallno;
 }
 
 asmlinkage void syscall_trace_exit(struct pt_regs *regs)
 {
+	if (test_thread_flag(TIF_SYSCALL_TRACEPOINT))
+		trace_sys_exit(regs, regs_return_value(regs));
+
 	if (test_thread_flag(TIF_SYSCALL_TRACE))
 		tracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);
 }

commit cf5c95db57ffa02e430c3840c08d1ee0403849d4
Merge: fd92d4a54a06 49788fe2a128
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 16 10:05:11 2014 +0100

    Merge tag 'for-3.16' of git://git.linaro.org/people/ard.biesheuvel/linux-arm into upstream
    
    FPSIMD register bank context switching and crypto algorithms
    optimisations for arm64 from Ard Biesheuvel.
    
    * tag 'for-3.16' of git://git.linaro.org/people/ard.biesheuvel/linux-arm:
      arm64/crypto: AES-ECB/CBC/CTR/XTS using ARMv8 NEON and Crypto Extensions
      arm64: pull in <asm/simd.h> from asm-generic
      arm64/crypto: AES in CCM mode using ARMv8 Crypto Extensions
      arm64/crypto: AES using ARMv8 Crypto Extensions
      arm64/crypto: GHASH secure hash using ARMv8 Crypto Extensions
      arm64/crypto: SHA-224/SHA-256 using ARMv8 Crypto Extensions
      arm64/crypto: SHA-1 using ARMv8 Crypto Extensions
      arm64: add support for kernel mode NEON in interrupt context
      arm64: defer reloading a task's FPSIMD state to userland resume
      arm64: add abstractions for FPSIMD state manipulation
      asm-generic: allow generic unaligned access if the arch supports it
    
    Conflicts:
            arch/arm64/include/asm/thread_info.h

commit fd92d4a54a069953b4679958121317f2a25389cd
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:51:32 2014 +0100

    arm64: is_compat_task is defined both in asm/compat.h and linux/compat.h
    
    Some kernel files may include both linux/compat.h and asm/compat.h directly
    or indirectly. Since both header files contain is_compat_task() under
    !CONFIG_COMPAT, compiling them with !CONFIG_COMPAT will eventually fail.
    Such files include kernel/auditsc.c, kernel/seccomp.c and init/do_mountfs.c
    (do_mountfs.c may read asm/compat.h via asm/ftrace.h once ftrace is
    implemented).
    
    So this patch proactively
    1) removes is_compat_task() under !CONFIG_COMPAT from asm/compat.h
    2) replaces asm/compat.h to linux/compat.h in kernel/*.c,
       but asm/compat.h is still necessary in ptrace.c and process.c because
       they use is_compat_thread().
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6d666dc1cea6..4b58e812cf67 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -19,6 +19,7 @@
  * along with this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 
+#include <linux/compat.h>
 #include <linux/kernel.h>
 #include <linux/sched.h>
 #include <linux/mm.h>

commit 3157858feff89196635b01495d5ec9ebe206639e
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Wed Apr 30 10:51:30 2014 +0100

    arm64: split syscall_trace() into separate functions for enter/exit
    
    As done in arm, this change makes it easy to confirm we invoke syscall
    related hooks, including syscall tracepoint, audit and seccomp which would
    be implemented later, in correct order. That is, undoing operations in the
    opposite order on exit that they were done on entry.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6a8928bba03c..6d666dc1cea6 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1058,35 +1058,43 @@ long arch_ptrace(struct task_struct *child, long request,
 	return ptrace_request(child, request, addr, data);
 }
 
-asmlinkage int syscall_trace(int dir, struct pt_regs *regs)
+enum ptrace_syscall_dir {
+	PTRACE_SYSCALL_ENTER = 0,
+	PTRACE_SYSCALL_EXIT,
+};
+
+static void tracehook_report_syscall(struct pt_regs *regs,
+				     enum ptrace_syscall_dir dir)
 {
+	int regno;
 	unsigned long saved_reg;
 
-	if (!test_thread_flag(TIF_SYSCALL_TRACE))
-		return regs->syscallno;
-
-	if (is_compat_task()) {
-		/* AArch32 uses ip (r12) for scratch */
-		saved_reg = regs->regs[12];
-		regs->regs[12] = dir;
-	} else {
-		/*
-		 * Save X7. X7 is used to denote syscall entry/exit:
-		 *   X7 = 0 -> entry, = 1 -> exit
-		 */
-		saved_reg = regs->regs[7];
-		regs->regs[7] = dir;
-	}
+	/*
+	 * A scratch register (ip(r12) on AArch32, x7 on AArch64) is
+	 * used to denote syscall entry/exit:
+	 */
+	regno = (is_compat_task() ? 12 : 7);
+	saved_reg = regs->regs[regno];
+	regs->regs[regno] = dir;
 
-	if (dir)
+	if (dir == PTRACE_SYSCALL_EXIT)
 		tracehook_report_syscall_exit(regs, 0);
 	else if (tracehook_report_syscall_entry(regs))
 		regs->syscallno = ~0UL;
 
-	if (is_compat_task())
-		regs->regs[12] = saved_reg;
-	else
-		regs->regs[7] = saved_reg;
+	regs->regs[regno] = saved_reg;
+}
+
+asmlinkage int syscall_trace_enter(struct pt_regs *regs)
+{
+	if (test_thread_flag(TIF_SYSCALL_TRACE))
+		tracehook_report_syscall(regs, PTRACE_SYSCALL_ENTER);
 
 	return regs->syscallno;
 }
+
+asmlinkage void syscall_trace_exit(struct pt_regs *regs)
+{
+	if (test_thread_flag(TIF_SYSCALL_TRACE))
+		tracehook_report_syscall(regs, PTRACE_SYSCALL_EXIT);
+}

commit 005f78cd88494457ed38ce817f4e3fe5d372f0cb
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu May 8 11:20:23 2014 +0200

    arm64: defer reloading a task's FPSIMD state to userland resume
    
    If a task gets scheduled out and back in again and nothing has touched
    its FPSIMD state in the mean time, there is really no reason to reload
    it from memory. Similarly, repeated calls to kernel_neon_begin() and
    kernel_neon_end() will preserve and restore the FPSIMD state every time.
    
    This patch defers the FPSIMD state restore to the last possible moment,
    i.e., right before the task returns to userland. If a task does not return to
    userland at all (for any reason), the existing FPSIMD state is preserved
    and may be reused by the owning task if it gets scheduled in again on the
    same CPU.
    
    This patch adds two more functions to abstract away from straight FPSIMD
    register file saves and restores:
    - fpsimd_restore_current_state -> ensure current's FPSIMD state is loaded
    - fpsimd_flush_task_state -> invalidate live copies of a task's FPSIMD state
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6a8928bba03c..f8700eca24e7 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -517,6 +517,7 @@ static int fpr_set(struct task_struct *target, const struct user_regset *regset,
 		return ret;
 
 	target->thread.fpsimd_state.user_fpsimd = newstate;
+	fpsimd_flush_task_state(target);
 	return ret;
 }
 
@@ -764,6 +765,7 @@ static int compat_vfp_set(struct task_struct *target,
 		uregs->fpcr = fpscr & VFP_FPSCR_CTRL_MASK;
 	}
 
+	fpsimd_flush_task_state(target);
 	return ret;
 }
 

commit cdc27c27843248ae7eb0df5fc261dd004eaa5670
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Dec 17 17:09:08 2013 +0000

    arm64: ptrace: avoid using HW_BREAKPOINT_EMPTY for disabled events
    
    Commit 8f34a1da35ae ("arm64: ptrace: use HW_BREAKPOINT_EMPTY type for
    disabled breakpoints") fixed an issue with GDB trying to zero breakpoint
    control registers. The problem there is that the arch hw_breakpoint code
    will attempt to create a (disabled), execute breakpoint of length 0.
    
    This will fail validation and report unexpected failure to GDB. To avoid
    this, we treated disabled breakpoints as HW_BREAKPOINT_EMPTY, but that
    seems to have broken with recent kernels, causing watchpoints to be
    treated as TYPE_INST in the core code and returning ENOSPC for any
    further breakpoints.
    
    This patch fixes the problem by prioritising the `enable' field of the
    breakpoint: if it is cleared, we simply update the perf_event_attr to
    indicate that the thing is disabled and don't bother changing either the
    type or the length. This reinforces the behaviour that the breakpoint
    control register is essentially read-only apart from the enable bit
    when disabling a breakpoint.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Aaron Liu <liucy214@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6777a2192b83..6a8928bba03c 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -214,31 +214,29 @@ static int ptrace_hbp_fill_attr_ctrl(unsigned int note_type,
 {
 	int err, len, type, disabled = !ctrl.enabled;
 
-	if (disabled) {
-		len = 0;
-		type = HW_BREAKPOINT_EMPTY;
-	} else {
-		err = arch_bp_generic_fields(ctrl, &len, &type);
-		if (err)
-			return err;
-
-		switch (note_type) {
-		case NT_ARM_HW_BREAK:
-			if ((type & HW_BREAKPOINT_X) != type)
-				return -EINVAL;
-			break;
-		case NT_ARM_HW_WATCH:
-			if ((type & HW_BREAKPOINT_RW) != type)
-				return -EINVAL;
-			break;
-		default:
+	attr->disabled = disabled;
+	if (disabled)
+		return 0;
+
+	err = arch_bp_generic_fields(ctrl, &len, &type);
+	if (err)
+		return err;
+
+	switch (note_type) {
+	case NT_ARM_HW_BREAK:
+		if ((type & HW_BREAKPOINT_X) != type)
 			return -EINVAL;
-		}
+		break;
+	case NT_ARM_HW_WATCH:
+		if ((type & HW_BREAKPOINT_RW) != type)
+			return -EINVAL;
+		break;
+	default:
+		return -EINVAL;
 	}
 
 	attr->bp_len	= len;
 	attr->bp_type	= type;
-	attr->disabled	= disabled;
 
 	return 0;
 }

commit 6a2e5e521c333a0b56cb60dc5587e3f90859c5e7
Author: Matthew Leach <Matthew.Leach@arm.com>
Date:   Thu Nov 28 12:07:22 2013 +0000

    arm64: ptrace: fix compat registes get/set to be endian clean
    
    On a BE system the wrong half of the X registers is retrieved/written
    when attempting to get/set the value of aarch32 registers through
    ptrace.
    
    Ensure that types are the correct width so that the relevant
    casting occurs.
    
    Signed-off-by: Matthew Leach <matthew.leach@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index fecdbf7de82e..6777a2192b83 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -636,28 +636,27 @@ static int compat_gpr_get(struct task_struct *target,
 
 	for (i = 0; i < num_regs; ++i) {
 		unsigned int idx = start + i;
-		void *reg;
+		compat_ulong_t reg;
 
 		switch (idx) {
 		case 15:
-			reg = (void *)&task_pt_regs(target)->pc;
+			reg = task_pt_regs(target)->pc;
 			break;
 		case 16:
-			reg = (void *)&task_pt_regs(target)->pstate;
+			reg = task_pt_regs(target)->pstate;
 			break;
 		case 17:
-			reg = (void *)&task_pt_regs(target)->orig_x0;
+			reg = task_pt_regs(target)->orig_x0;
 			break;
 		default:
-			reg = (void *)&task_pt_regs(target)->regs[idx];
+			reg = task_pt_regs(target)->regs[idx];
 		}
 
-		ret = copy_to_user(ubuf, reg, sizeof(compat_ulong_t));
-
+		ret = copy_to_user(ubuf, &reg, sizeof(reg));
 		if (ret)
 			break;
-		else
-			ubuf += sizeof(compat_ulong_t);
+
+		ubuf += sizeof(reg);
 	}
 
 	return ret;
@@ -685,28 +684,28 @@ static int compat_gpr_set(struct task_struct *target,
 
 	for (i = 0; i < num_regs; ++i) {
 		unsigned int idx = start + i;
-		void *reg;
+		compat_ulong_t reg;
+
+		ret = copy_from_user(&reg, ubuf, sizeof(reg));
+		if (ret)
+			return ret;
+
+		ubuf += sizeof(reg);
 
 		switch (idx) {
 		case 15:
-			reg = (void *)&newregs.pc;
+			newregs.pc = reg;
 			break;
 		case 16:
-			reg = (void *)&newregs.pstate;
+			newregs.pstate = reg;
 			break;
 		case 17:
-			reg = (void *)&newregs.orig_x0;
+			newregs.orig_x0 = reg;
 			break;
 		default:
-			reg = (void *)&newregs.regs[idx];
+			newregs.regs[idx] = reg;
 		}
 
-		ret = copy_from_user(reg, ubuf, sizeof(compat_ulong_t));
-
-		if (ret)
-			goto out;
-		else
-			ubuf += sizeof(compat_ulong_t);
 	}
 
 	if (valid_user_regs(&newregs.user_regs))
@@ -714,7 +713,6 @@ static int compat_gpr_set(struct task_struct *target,
 	else
 		ret = -EINVAL;
 
-out:
 	return ret;
 }
 

commit 1442b6ed249d2b3d2cfcf45b65ac64393495c96c
Author: Will Deacon <will.deacon@arm.com>
Date:   Sat Mar 16 08:48:13 2013 +0000

    arm64: debug: consolidate software breakpoint handlers
    
    The software breakpoint handlers are hooked in directly from ptrace,
    which makes it difficult to add additional handlers for things like
    kprobes and kgdb.
    
    This patch moves the handling code into debug-monitors.c, where we can
    dispatch to different debug subsystems more easily.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 6e1e77f1831c..fecdbf7de82e 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -53,28 +53,6 @@ void ptrace_disable(struct task_struct *child)
 {
 }
 
-/*
- * Handle hitting a breakpoint.
- */
-static int ptrace_break(struct pt_regs *regs)
-{
-	siginfo_t info = {
-		.si_signo = SIGTRAP,
-		.si_errno = 0,
-		.si_code  = TRAP_BRKPT,
-		.si_addr  = (void __user *)instruction_pointer(regs),
-	};
-
-	force_sig_info(SIGTRAP, &info, current);
-	return 0;
-}
-
-static int arm64_break_trap(unsigned long addr, unsigned int esr,
-			    struct pt_regs *regs)
-{
-	return ptrace_break(regs);
-}
-
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 /*
  * Handle hitting a HW-breakpoint.
@@ -817,33 +795,6 @@ static const struct user_regset_view user_aarch32_view = {
 	.regsets = aarch32_regsets, .n = ARRAY_SIZE(aarch32_regsets)
 };
 
-int aarch32_break_trap(struct pt_regs *regs)
-{
-	unsigned int instr;
-	bool bp = false;
-	void __user *pc = (void __user *)instruction_pointer(regs);
-
-	if (compat_thumb_mode(regs)) {
-		/* get 16-bit Thumb instruction */
-		get_user(instr, (u16 __user *)pc);
-		if (instr == AARCH32_BREAK_THUMB2_LO) {
-			/* get second half of 32-bit Thumb-2 instruction */
-			get_user(instr, (u16 __user *)(pc + 2));
-			bp = instr == AARCH32_BREAK_THUMB2_HI;
-		} else {
-			bp = instr == AARCH32_BREAK_THUMB;
-		}
-	} else {
-		/* 32-bit ARM instruction */
-		get_user(instr, (u32 __user *)pc);
-		bp = (instr & ~0xf0000000) == AARCH32_BREAK_ARM;
-	}
-
-	if (bp)
-		return ptrace_break(regs);
-	return 1;
-}
-
 static int compat_ptrace_read_user(struct task_struct *tsk, compat_ulong_t off,
 				   compat_ulong_t __user *ret)
 {
@@ -1111,16 +1062,6 @@ long arch_ptrace(struct task_struct *child, long request,
 	return ptrace_request(child, request, addr, data);
 }
 
-
-static int __init ptrace_break_init(void)
-{
-	hook_debug_fault_code(DBG_ESR_EVT_BRK, arm64_break_trap, SIGTRAP,
-			      TRAP_BRKPT, "ptrace BRK handler");
-	return 0;
-}
-core_initcall(ptrace_break_init);
-
-
 asmlinkage int syscall_trace(int dir, struct pt_regs *regs)
 {
 	unsigned long saved_reg;

commit 8f34a1da35aed7b438a2de8ac27723a5472e8399
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Oct 18 15:17:00 2012 +0100

    arm64: ptrace: use HW_BREAKPOINT_EMPTY type for disabled breakpoints
    
    If a debugger tries to zero a hardware debug control register, the
    kernel will try to infer both the type and length of the breakpoint
    in order to sanity-check against the requested regset type. This will
    fail because the encoding will appear as a zero-length breakpoint.
    
    This patch changes the control register setting so that disabled
    breakpoints are treated as HW_BREAKPOINT_EMPTY and no further
    sanity-checking is required.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index c62d39d5c99f..6e1e77f1831c 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -234,28 +234,33 @@ static int ptrace_hbp_fill_attr_ctrl(unsigned int note_type,
 				     struct arch_hw_breakpoint_ctrl ctrl,
 				     struct perf_event_attr *attr)
 {
-	int err, len, type;
+	int err, len, type, disabled = !ctrl.enabled;
 
-	err = arch_bp_generic_fields(ctrl, &len, &type);
-	if (err)
-		return err;
-
-	switch (note_type) {
-	case NT_ARM_HW_BREAK:
-		if ((type & HW_BREAKPOINT_X) != type)
-			return -EINVAL;
-		break;
-	case NT_ARM_HW_WATCH:
-		if ((type & HW_BREAKPOINT_RW) != type)
+	if (disabled) {
+		len = 0;
+		type = HW_BREAKPOINT_EMPTY;
+	} else {
+		err = arch_bp_generic_fields(ctrl, &len, &type);
+		if (err)
+			return err;
+
+		switch (note_type) {
+		case NT_ARM_HW_BREAK:
+			if ((type & HW_BREAKPOINT_X) != type)
+				return -EINVAL;
+			break;
+		case NT_ARM_HW_WATCH:
+			if ((type & HW_BREAKPOINT_RW) != type)
+				return -EINVAL;
+			break;
+		default:
 			return -EINVAL;
-		break;
-	default:
-		return -EINVAL;
+		}
 	}
 
 	attr->bp_len	= len;
 	attr->bp_type	= type;
-	attr->disabled	= !ctrl.enabled;
+	attr->disabled	= disabled;
 
 	return 0;
 }

commit 7797d17c591ae62c6f43c6de4fdb8beeb50eb692
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Oct 11 12:10:57 2012 +0100

    arm64: ptrace: make structure padding explicit for debug registers
    
    The user_hwdebug_state structure contains implicit padding to conform to
    the alignment requirements of the AArch64 ABI (namely that aggregates
    must be aligned to their most aligned member).
    
    This patch fixes the ptrace functions operating on struct
    user_hwdebug_state so that the padding is handled correctly.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 2ea3968367c2..c62d39d5c99f 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -372,7 +372,7 @@ static int ptrace_hbp_set_addr(unsigned int note_type,
 
 #define PTRACE_HBP_ADDR_SZ	sizeof(u64)
 #define PTRACE_HBP_CTRL_SZ	sizeof(u32)
-#define PTRACE_HBP_REG_OFF	sizeof(u32)
+#define PTRACE_HBP_PAD_SZ	sizeof(u32)
 
 static int hw_break_get(struct task_struct *target,
 			const struct user_regset *regset,
@@ -380,7 +380,7 @@ static int hw_break_get(struct task_struct *target,
 			void *kbuf, void __user *ubuf)
 {
 	unsigned int note_type = regset->core_note_type;
-	int ret, idx = 0, offset = PTRACE_HBP_REG_OFF, limit;
+	int ret, idx = 0, offset, limit;
 	u32 info, ctrl;
 	u64 addr;
 
@@ -389,11 +389,20 @@ static int hw_break_get(struct task_struct *target,
 	if (ret)
 		return ret;
 
-	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &info, 0, 4);
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &info, 0,
+				  sizeof(info));
+	if (ret)
+		return ret;
+
+	/* Pad */
+	offset = offsetof(struct user_hwdebug_state, pad);
+	ret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf, offset,
+				       offset + PTRACE_HBP_PAD_SZ);
 	if (ret)
 		return ret;
 
 	/* (address, ctrl) registers */
+	offset = offsetof(struct user_hwdebug_state, dbg_regs);
 	limit = regset->n * regset->size;
 	while (count && offset < limit) {
 		ret = ptrace_hbp_get_addr(note_type, target, idx, &addr);
@@ -413,6 +422,13 @@ static int hw_break_get(struct task_struct *target,
 		if (ret)
 			return ret;
 		offset += PTRACE_HBP_CTRL_SZ;
+
+		ret = user_regset_copyout_zero(&pos, &count, &kbuf, &ubuf,
+					       offset,
+					       offset + PTRACE_HBP_PAD_SZ);
+		if (ret)
+			return ret;
+		offset += PTRACE_HBP_PAD_SZ;
 		idx++;
 	}
 
@@ -425,12 +441,13 @@ static int hw_break_set(struct task_struct *target,
 			const void *kbuf, const void __user *ubuf)
 {
 	unsigned int note_type = regset->core_note_type;
-	int ret, idx = 0, offset = PTRACE_HBP_REG_OFF, limit;
+	int ret, idx = 0, offset, limit;
 	u32 ctrl;
 	u64 addr;
 
-	/* Resource info */
-	ret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf, 0, 4);
+	/* Resource info and pad */
+	offset = offsetof(struct user_hwdebug_state, dbg_regs);
+	ret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf, 0, offset);
 	if (ret)
 		return ret;
 
@@ -454,6 +471,13 @@ static int hw_break_set(struct task_struct *target,
 		if (ret)
 			return ret;
 		offset += PTRACE_HBP_CTRL_SZ;
+
+		ret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf,
+						offset,
+						offset + PTRACE_HBP_PAD_SZ);
+		if (ret)
+			return ret;
+		offset += PTRACE_HBP_PAD_SZ;
 		idx++;
 	}
 

commit 7606c37d4a447ea3b0efb2165d3ccf516b7d8696
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Oct 10 15:50:03 2012 +0100

    arm64: Do not export the compat-specific definitions to the user
    
    This patch adds #ifdef __KERNEL__ guards around the COMPAT_* definitions
    to avoid exporting them to user. AArch32 user requiring the kernel
    headers must use those generated with ARCH=arm.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index ac3550ecc7b5..2ea3968367c2 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -823,11 +823,11 @@ static int compat_ptrace_read_user(struct task_struct *tsk, compat_ulong_t off,
 	if (off & 3)
 		return -EIO;
 
-	if (off == PT_TEXT_ADDR)
+	if (off == COMPAT_PT_TEXT_ADDR)
 		tmp = tsk->mm->start_code;
-	else if (off == PT_DATA_ADDR)
+	else if (off == COMPAT_PT_DATA_ADDR)
 		tmp = tsk->mm->start_data;
-	else if (off == PT_TEXT_END_ADDR)
+	else if (off == COMPAT_PT_TEXT_END_ADDR)
 		tmp = tsk->mm->end_code;
 	else if (off < sizeof(compat_elf_gregset_t))
 		return copy_regset_to_user(tsk, &user_aarch32_view,

commit 27aa55c5e5123fa8b8ad0156559d34d7edff58ca
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Sep 27 11:38:12 2012 +0100

    arm64: ptrace: remove obsolete ptrace request numbers from user headers
    
    The use of regsets has removed the need for many private ptrace requests,
    so remove the corresponding definitions from the user-visible ptrace.h
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
index 490f7537a270..ac3550ecc7b5 100644
--- a/arch/arm64/kernel/ptrace.c
+++ b/arch/arm64/kernel/ptrace.c
@@ -1005,7 +1005,7 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 			ret = compat_ptrace_write_user(child, addr, data);
 			break;
 
-		case PTRACE_GETREGS:
+		case COMPAT_PTRACE_GETREGS:
 			ret = copy_regset_to_user(child,
 						  &user_aarch32_view,
 						  REGSET_COMPAT_GPR,
@@ -1013,7 +1013,7 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 						  datap);
 			break;
 
-		case PTRACE_SETREGS:
+		case COMPAT_PTRACE_SETREGS:
 			ret = copy_regset_from_user(child,
 						    &user_aarch32_view,
 						    REGSET_COMPAT_GPR,
@@ -1021,12 +1021,12 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 						    datap);
 			break;
 
-		case PTRACE_GET_THREAD_AREA:
+		case COMPAT_PTRACE_GET_THREAD_AREA:
 			ret = put_user((compat_ulong_t)child->thread.tp_value,
 				       (compat_ulong_t __user *)datap);
 			break;
 
-		case PTRACE_SET_SYSCALL:
+		case COMPAT_PTRACE_SET_SYSCALL:
 			task_pt_regs(child)->syscallno = data;
 			ret = 0;
 			break;
@@ -1048,11 +1048,11 @@ long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
 			break;
 
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
-		case PTRACE_GETHBPREGS:
+		case COMPAT_PTRACE_GETHBPREGS:
 			ret = compat_ptrace_gethbpregs(child, addr, datap);
 			break;
 
-		case PTRACE_SETHBPREGS:
+		case COMPAT_PTRACE_SETHBPREGS:
 			ret = compat_ptrace_sethbpregs(child, addr, datap);
 			break;
 #endif

commit 478fcb2cdb2351dcfc3fb23f42d76f4436ee4149
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Mar 5 11:49:33 2012 +0000

    arm64: Debugging support
    
    This patch adds ptrace, debug monitors and hardware breakpoints support.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
new file mode 100644
index 000000000000..490f7537a270
--- /dev/null
+++ b/arch/arm64/kernel/ptrace.c
@@ -0,0 +1,1126 @@
+/*
+ * Based on arch/arm/kernel/ptrace.c
+ *
+ * By Ross Biro 1/23/92
+ * edited by Linus Torvalds
+ * ARM modifications Copyright (C) 2000 Russell King
+ * Copyright (C) 2012 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/ptrace.h>
+#include <linux/user.h>
+#include <linux/security.h>
+#include <linux/init.h>
+#include <linux/signal.h>
+#include <linux/uaccess.h>
+#include <linux/perf_event.h>
+#include <linux/hw_breakpoint.h>
+#include <linux/regset.h>
+#include <linux/tracehook.h>
+#include <linux/elf.h>
+
+#include <asm/compat.h>
+#include <asm/debug-monitors.h>
+#include <asm/pgtable.h>
+#include <asm/traps.h>
+#include <asm/system_misc.h>
+
+/*
+ * TODO: does not yet catch signals sent when the child dies.
+ * in exit.c or in signal.c.
+ */
+
+/*
+ * Called by kernel/ptrace.c when detaching..
+ */
+void ptrace_disable(struct task_struct *child)
+{
+}
+
+/*
+ * Handle hitting a breakpoint.
+ */
+static int ptrace_break(struct pt_regs *regs)
+{
+	siginfo_t info = {
+		.si_signo = SIGTRAP,
+		.si_errno = 0,
+		.si_code  = TRAP_BRKPT,
+		.si_addr  = (void __user *)instruction_pointer(regs),
+	};
+
+	force_sig_info(SIGTRAP, &info, current);
+	return 0;
+}
+
+static int arm64_break_trap(unsigned long addr, unsigned int esr,
+			    struct pt_regs *regs)
+{
+	return ptrace_break(regs);
+}
+
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+/*
+ * Handle hitting a HW-breakpoint.
+ */
+static void ptrace_hbptriggered(struct perf_event *bp,
+				struct perf_sample_data *data,
+				struct pt_regs *regs)
+{
+	struct arch_hw_breakpoint *bkpt = counter_arch_bp(bp);
+	siginfo_t info = {
+		.si_signo	= SIGTRAP,
+		.si_errno	= 0,
+		.si_code	= TRAP_HWBKPT,
+		.si_addr	= (void __user *)(bkpt->trigger),
+	};
+
+#ifdef CONFIG_COMPAT
+	int i;
+
+	if (!is_compat_task())
+		goto send_sig;
+
+	for (i = 0; i < ARM_MAX_BRP; ++i) {
+		if (current->thread.debug.hbp_break[i] == bp) {
+			info.si_errno = (i << 1) + 1;
+			break;
+		}
+	}
+	for (i = ARM_MAX_BRP; i < ARM_MAX_HBP_SLOTS && !bp; ++i) {
+		if (current->thread.debug.hbp_watch[i] == bp) {
+			info.si_errno = -((i << 1) + 1);
+			break;
+		}
+	}
+
+send_sig:
+#endif
+	force_sig_info(SIGTRAP, &info, current);
+}
+
+/*
+ * Unregister breakpoints from this task and reset the pointers in
+ * the thread_struct.
+ */
+void flush_ptrace_hw_breakpoint(struct task_struct *tsk)
+{
+	int i;
+	struct thread_struct *t = &tsk->thread;
+
+	for (i = 0; i < ARM_MAX_BRP; i++) {
+		if (t->debug.hbp_break[i]) {
+			unregister_hw_breakpoint(t->debug.hbp_break[i]);
+			t->debug.hbp_break[i] = NULL;
+		}
+	}
+
+	for (i = 0; i < ARM_MAX_WRP; i++) {
+		if (t->debug.hbp_watch[i]) {
+			unregister_hw_breakpoint(t->debug.hbp_watch[i]);
+			t->debug.hbp_watch[i] = NULL;
+		}
+	}
+}
+
+void ptrace_hw_copy_thread(struct task_struct *tsk)
+{
+	memset(&tsk->thread.debug, 0, sizeof(struct debug_info));
+}
+
+static struct perf_event *ptrace_hbp_get_event(unsigned int note_type,
+					       struct task_struct *tsk,
+					       unsigned long idx)
+{
+	struct perf_event *bp = ERR_PTR(-EINVAL);
+
+	switch (note_type) {
+	case NT_ARM_HW_BREAK:
+		if (idx < ARM_MAX_BRP)
+			bp = tsk->thread.debug.hbp_break[idx];
+		break;
+	case NT_ARM_HW_WATCH:
+		if (idx < ARM_MAX_WRP)
+			bp = tsk->thread.debug.hbp_watch[idx];
+		break;
+	}
+
+	return bp;
+}
+
+static int ptrace_hbp_set_event(unsigned int note_type,
+				struct task_struct *tsk,
+				unsigned long idx,
+				struct perf_event *bp)
+{
+	int err = -EINVAL;
+
+	switch (note_type) {
+	case NT_ARM_HW_BREAK:
+		if (idx < ARM_MAX_BRP) {
+			tsk->thread.debug.hbp_break[idx] = bp;
+			err = 0;
+		}
+		break;
+	case NT_ARM_HW_WATCH:
+		if (idx < ARM_MAX_WRP) {
+			tsk->thread.debug.hbp_watch[idx] = bp;
+			err = 0;
+		}
+		break;
+	}
+
+	return err;
+}
+
+static struct perf_event *ptrace_hbp_create(unsigned int note_type,
+					    struct task_struct *tsk,
+					    unsigned long idx)
+{
+	struct perf_event *bp;
+	struct perf_event_attr attr;
+	int err, type;
+
+	switch (note_type) {
+	case NT_ARM_HW_BREAK:
+		type = HW_BREAKPOINT_X;
+		break;
+	case NT_ARM_HW_WATCH:
+		type = HW_BREAKPOINT_RW;
+		break;
+	default:
+		return ERR_PTR(-EINVAL);
+	}
+
+	ptrace_breakpoint_init(&attr);
+
+	/*
+	 * Initialise fields to sane defaults
+	 * (i.e. values that will pass validation).
+	 */
+	attr.bp_addr	= 0;
+	attr.bp_len	= HW_BREAKPOINT_LEN_4;
+	attr.bp_type	= type;
+	attr.disabled	= 1;
+
+	bp = register_user_hw_breakpoint(&attr, ptrace_hbptriggered, NULL, tsk);
+	if (IS_ERR(bp))
+		return bp;
+
+	err = ptrace_hbp_set_event(note_type, tsk, idx, bp);
+	if (err)
+		return ERR_PTR(err);
+
+	return bp;
+}
+
+static int ptrace_hbp_fill_attr_ctrl(unsigned int note_type,
+				     struct arch_hw_breakpoint_ctrl ctrl,
+				     struct perf_event_attr *attr)
+{
+	int err, len, type;
+
+	err = arch_bp_generic_fields(ctrl, &len, &type);
+	if (err)
+		return err;
+
+	switch (note_type) {
+	case NT_ARM_HW_BREAK:
+		if ((type & HW_BREAKPOINT_X) != type)
+			return -EINVAL;
+		break;
+	case NT_ARM_HW_WATCH:
+		if ((type & HW_BREAKPOINT_RW) != type)
+			return -EINVAL;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	attr->bp_len	= len;
+	attr->bp_type	= type;
+	attr->disabled	= !ctrl.enabled;
+
+	return 0;
+}
+
+static int ptrace_hbp_get_resource_info(unsigned int note_type, u32 *info)
+{
+	u8 num;
+	u32 reg = 0;
+
+	switch (note_type) {
+	case NT_ARM_HW_BREAK:
+		num = hw_breakpoint_slots(TYPE_INST);
+		break;
+	case NT_ARM_HW_WATCH:
+		num = hw_breakpoint_slots(TYPE_DATA);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	reg |= debug_monitors_arch();
+	reg <<= 8;
+	reg |= num;
+
+	*info = reg;
+	return 0;
+}
+
+static int ptrace_hbp_get_ctrl(unsigned int note_type,
+			       struct task_struct *tsk,
+			       unsigned long idx,
+			       u32 *ctrl)
+{
+	struct perf_event *bp = ptrace_hbp_get_event(note_type, tsk, idx);
+
+	if (IS_ERR(bp))
+		return PTR_ERR(bp);
+
+	*ctrl = bp ? encode_ctrl_reg(counter_arch_bp(bp)->ctrl) : 0;
+	return 0;
+}
+
+static int ptrace_hbp_get_addr(unsigned int note_type,
+			       struct task_struct *tsk,
+			       unsigned long idx,
+			       u64 *addr)
+{
+	struct perf_event *bp = ptrace_hbp_get_event(note_type, tsk, idx);
+
+	if (IS_ERR(bp))
+		return PTR_ERR(bp);
+
+	*addr = bp ? bp->attr.bp_addr : 0;
+	return 0;
+}
+
+static struct perf_event *ptrace_hbp_get_initialised_bp(unsigned int note_type,
+							struct task_struct *tsk,
+							unsigned long idx)
+{
+	struct perf_event *bp = ptrace_hbp_get_event(note_type, tsk, idx);
+
+	if (!bp)
+		bp = ptrace_hbp_create(note_type, tsk, idx);
+
+	return bp;
+}
+
+static int ptrace_hbp_set_ctrl(unsigned int note_type,
+			       struct task_struct *tsk,
+			       unsigned long idx,
+			       u32 uctrl)
+{
+	int err;
+	struct perf_event *bp;
+	struct perf_event_attr attr;
+	struct arch_hw_breakpoint_ctrl ctrl;
+
+	bp = ptrace_hbp_get_initialised_bp(note_type, tsk, idx);
+	if (IS_ERR(bp)) {
+		err = PTR_ERR(bp);
+		return err;
+	}
+
+	attr = bp->attr;
+	decode_ctrl_reg(uctrl, &ctrl);
+	err = ptrace_hbp_fill_attr_ctrl(note_type, ctrl, &attr);
+	if (err)
+		return err;
+
+	return modify_user_hw_breakpoint(bp, &attr);
+}
+
+static int ptrace_hbp_set_addr(unsigned int note_type,
+			       struct task_struct *tsk,
+			       unsigned long idx,
+			       u64 addr)
+{
+	int err;
+	struct perf_event *bp;
+	struct perf_event_attr attr;
+
+	bp = ptrace_hbp_get_initialised_bp(note_type, tsk, idx);
+	if (IS_ERR(bp)) {
+		err = PTR_ERR(bp);
+		return err;
+	}
+
+	attr = bp->attr;
+	attr.bp_addr = addr;
+	err = modify_user_hw_breakpoint(bp, &attr);
+	return err;
+}
+
+#define PTRACE_HBP_ADDR_SZ	sizeof(u64)
+#define PTRACE_HBP_CTRL_SZ	sizeof(u32)
+#define PTRACE_HBP_REG_OFF	sizeof(u32)
+
+static int hw_break_get(struct task_struct *target,
+			const struct user_regset *regset,
+			unsigned int pos, unsigned int count,
+			void *kbuf, void __user *ubuf)
+{
+	unsigned int note_type = regset->core_note_type;
+	int ret, idx = 0, offset = PTRACE_HBP_REG_OFF, limit;
+	u32 info, ctrl;
+	u64 addr;
+
+	/* Resource info */
+	ret = ptrace_hbp_get_resource_info(note_type, &info);
+	if (ret)
+		return ret;
+
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &info, 0, 4);
+	if (ret)
+		return ret;
+
+	/* (address, ctrl) registers */
+	limit = regset->n * regset->size;
+	while (count && offset < limit) {
+		ret = ptrace_hbp_get_addr(note_type, target, idx, &addr);
+		if (ret)
+			return ret;
+		ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &addr,
+					  offset, offset + PTRACE_HBP_ADDR_SZ);
+		if (ret)
+			return ret;
+		offset += PTRACE_HBP_ADDR_SZ;
+
+		ret = ptrace_hbp_get_ctrl(note_type, target, idx, &ctrl);
+		if (ret)
+			return ret;
+		ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, &ctrl,
+					  offset, offset + PTRACE_HBP_CTRL_SZ);
+		if (ret)
+			return ret;
+		offset += PTRACE_HBP_CTRL_SZ;
+		idx++;
+	}
+
+	return 0;
+}
+
+static int hw_break_set(struct task_struct *target,
+			const struct user_regset *regset,
+			unsigned int pos, unsigned int count,
+			const void *kbuf, const void __user *ubuf)
+{
+	unsigned int note_type = regset->core_note_type;
+	int ret, idx = 0, offset = PTRACE_HBP_REG_OFF, limit;
+	u32 ctrl;
+	u64 addr;
+
+	/* Resource info */
+	ret = user_regset_copyin_ignore(&pos, &count, &kbuf, &ubuf, 0, 4);
+	if (ret)
+		return ret;
+
+	/* (address, ctrl) registers */
+	limit = regset->n * regset->size;
+	while (count && offset < limit) {
+		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &addr,
+					 offset, offset + PTRACE_HBP_ADDR_SZ);
+		if (ret)
+			return ret;
+		ret = ptrace_hbp_set_addr(note_type, target, idx, addr);
+		if (ret)
+			return ret;
+		offset += PTRACE_HBP_ADDR_SZ;
+
+		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &ctrl,
+					 offset, offset + PTRACE_HBP_CTRL_SZ);
+		if (ret)
+			return ret;
+		ret = ptrace_hbp_set_ctrl(note_type, target, idx, ctrl);
+		if (ret)
+			return ret;
+		offset += PTRACE_HBP_CTRL_SZ;
+		idx++;
+	}
+
+	return 0;
+}
+#endif	/* CONFIG_HAVE_HW_BREAKPOINT */
+
+static int gpr_get(struct task_struct *target,
+		   const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   void *kbuf, void __user *ubuf)
+{
+	struct user_pt_regs *uregs = &task_pt_regs(target)->user_regs;
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs, 0, -1);
+}
+
+static int gpr_set(struct task_struct *target, const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   const void *kbuf, const void __user *ubuf)
+{
+	int ret;
+	struct user_pt_regs newregs;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newregs, 0, -1);
+	if (ret)
+		return ret;
+
+	if (!valid_user_regs(&newregs))
+		return -EINVAL;
+
+	task_pt_regs(target)->user_regs = newregs;
+	return 0;
+}
+
+/*
+ * TODO: update fp accessors for lazy context switching (sync/flush hwstate)
+ */
+static int fpr_get(struct task_struct *target, const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   void *kbuf, void __user *ubuf)
+{
+	struct user_fpsimd_state *uregs;
+	uregs = &target->thread.fpsimd_state.user_fpsimd;
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs, 0, -1);
+}
+
+static int fpr_set(struct task_struct *target, const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   const void *kbuf, const void __user *ubuf)
+{
+	int ret;
+	struct user_fpsimd_state newstate;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &newstate, 0, -1);
+	if (ret)
+		return ret;
+
+	target->thread.fpsimd_state.user_fpsimd = newstate;
+	return ret;
+}
+
+static int tls_get(struct task_struct *target, const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   void *kbuf, void __user *ubuf)
+{
+	unsigned long *tls = &target->thread.tp_value;
+	return user_regset_copyout(&pos, &count, &kbuf, &ubuf, tls, 0, -1);
+}
+
+static int tls_set(struct task_struct *target, const struct user_regset *regset,
+		   unsigned int pos, unsigned int count,
+		   const void *kbuf, const void __user *ubuf)
+{
+	int ret;
+	unsigned long tls;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, &tls, 0, -1);
+	if (ret)
+		return ret;
+
+	target->thread.tp_value = tls;
+	return ret;
+}
+
+enum aarch64_regset {
+	REGSET_GPR,
+	REGSET_FPR,
+	REGSET_TLS,
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	REGSET_HW_BREAK,
+	REGSET_HW_WATCH,
+#endif
+};
+
+static const struct user_regset aarch64_regsets[] = {
+	[REGSET_GPR] = {
+		.core_note_type = NT_PRSTATUS,
+		.n = sizeof(struct user_pt_regs) / sizeof(u64),
+		.size = sizeof(u64),
+		.align = sizeof(u64),
+		.get = gpr_get,
+		.set = gpr_set
+	},
+	[REGSET_FPR] = {
+		.core_note_type = NT_PRFPREG,
+		.n = sizeof(struct user_fpsimd_state) / sizeof(u32),
+		/*
+		 * We pretend we have 32-bit registers because the fpsr and
+		 * fpcr are 32-bits wide.
+		 */
+		.size = sizeof(u32),
+		.align = sizeof(u32),
+		.get = fpr_get,
+		.set = fpr_set
+	},
+	[REGSET_TLS] = {
+		.core_note_type = NT_ARM_TLS,
+		.n = 1,
+		.size = sizeof(void *),
+		.align = sizeof(void *),
+		.get = tls_get,
+		.set = tls_set,
+	},
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	[REGSET_HW_BREAK] = {
+		.core_note_type = NT_ARM_HW_BREAK,
+		.n = sizeof(struct user_hwdebug_state) / sizeof(u32),
+		.size = sizeof(u32),
+		.align = sizeof(u32),
+		.get = hw_break_get,
+		.set = hw_break_set,
+	},
+	[REGSET_HW_WATCH] = {
+		.core_note_type = NT_ARM_HW_WATCH,
+		.n = sizeof(struct user_hwdebug_state) / sizeof(u32),
+		.size = sizeof(u32),
+		.align = sizeof(u32),
+		.get = hw_break_get,
+		.set = hw_break_set,
+	},
+#endif
+};
+
+static const struct user_regset_view user_aarch64_view = {
+	.name = "aarch64", .e_machine = EM_AARCH64,
+	.regsets = aarch64_regsets, .n = ARRAY_SIZE(aarch64_regsets)
+};
+
+#ifdef CONFIG_COMPAT
+#include <linux/compat.h>
+
+enum compat_regset {
+	REGSET_COMPAT_GPR,
+	REGSET_COMPAT_VFP,
+};
+
+static int compat_gpr_get(struct task_struct *target,
+			  const struct user_regset *regset,
+			  unsigned int pos, unsigned int count,
+			  void *kbuf, void __user *ubuf)
+{
+	int ret = 0;
+	unsigned int i, start, num_regs;
+
+	/* Calculate the number of AArch32 registers contained in count */
+	num_regs = count / regset->size;
+
+	/* Convert pos into an register number */
+	start = pos / regset->size;
+
+	if (start + num_regs > regset->n)
+		return -EIO;
+
+	for (i = 0; i < num_regs; ++i) {
+		unsigned int idx = start + i;
+		void *reg;
+
+		switch (idx) {
+		case 15:
+			reg = (void *)&task_pt_regs(target)->pc;
+			break;
+		case 16:
+			reg = (void *)&task_pt_regs(target)->pstate;
+			break;
+		case 17:
+			reg = (void *)&task_pt_regs(target)->orig_x0;
+			break;
+		default:
+			reg = (void *)&task_pt_regs(target)->regs[idx];
+		}
+
+		ret = copy_to_user(ubuf, reg, sizeof(compat_ulong_t));
+
+		if (ret)
+			break;
+		else
+			ubuf += sizeof(compat_ulong_t);
+	}
+
+	return ret;
+}
+
+static int compat_gpr_set(struct task_struct *target,
+			  const struct user_regset *regset,
+			  unsigned int pos, unsigned int count,
+			  const void *kbuf, const void __user *ubuf)
+{
+	struct pt_regs newregs;
+	int ret = 0;
+	unsigned int i, start, num_regs;
+
+	/* Calculate the number of AArch32 registers contained in count */
+	num_regs = count / regset->size;
+
+	/* Convert pos into an register number */
+	start = pos / regset->size;
+
+	if (start + num_regs > regset->n)
+		return -EIO;
+
+	newregs = *task_pt_regs(target);
+
+	for (i = 0; i < num_regs; ++i) {
+		unsigned int idx = start + i;
+		void *reg;
+
+		switch (idx) {
+		case 15:
+			reg = (void *)&newregs.pc;
+			break;
+		case 16:
+			reg = (void *)&newregs.pstate;
+			break;
+		case 17:
+			reg = (void *)&newregs.orig_x0;
+			break;
+		default:
+			reg = (void *)&newregs.regs[idx];
+		}
+
+		ret = copy_from_user(reg, ubuf, sizeof(compat_ulong_t));
+
+		if (ret)
+			goto out;
+		else
+			ubuf += sizeof(compat_ulong_t);
+	}
+
+	if (valid_user_regs(&newregs.user_regs))
+		*task_pt_regs(target) = newregs;
+	else
+		ret = -EINVAL;
+
+out:
+	return ret;
+}
+
+static int compat_vfp_get(struct task_struct *target,
+			  const struct user_regset *regset,
+			  unsigned int pos, unsigned int count,
+			  void *kbuf, void __user *ubuf)
+{
+	struct user_fpsimd_state *uregs;
+	compat_ulong_t fpscr;
+	int ret;
+
+	uregs = &target->thread.fpsimd_state.user_fpsimd;
+
+	/*
+	 * The VFP registers are packed into the fpsimd_state, so they all sit
+	 * nicely together for us. We just need to create the fpscr separately.
+	 */
+	ret = user_regset_copyout(&pos, &count, &kbuf, &ubuf, uregs, 0,
+				  VFP_STATE_SIZE - sizeof(compat_ulong_t));
+
+	if (count && !ret) {
+		fpscr = (uregs->fpsr & VFP_FPSCR_STAT_MASK) |
+			(uregs->fpcr & VFP_FPSCR_CTRL_MASK);
+		ret = put_user(fpscr, (compat_ulong_t *)ubuf);
+	}
+
+	return ret;
+}
+
+static int compat_vfp_set(struct task_struct *target,
+			  const struct user_regset *regset,
+			  unsigned int pos, unsigned int count,
+			  const void *kbuf, const void __user *ubuf)
+{
+	struct user_fpsimd_state *uregs;
+	compat_ulong_t fpscr;
+	int ret;
+
+	if (pos + count > VFP_STATE_SIZE)
+		return -EIO;
+
+	uregs = &target->thread.fpsimd_state.user_fpsimd;
+
+	ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, uregs, 0,
+				 VFP_STATE_SIZE - sizeof(compat_ulong_t));
+
+	if (count && !ret) {
+		ret = get_user(fpscr, (compat_ulong_t *)ubuf);
+		uregs->fpsr = fpscr & VFP_FPSCR_STAT_MASK;
+		uregs->fpcr = fpscr & VFP_FPSCR_CTRL_MASK;
+	}
+
+	return ret;
+}
+
+static const struct user_regset aarch32_regsets[] = {
+	[REGSET_COMPAT_GPR] = {
+		.core_note_type = NT_PRSTATUS,
+		.n = COMPAT_ELF_NGREG,
+		.size = sizeof(compat_elf_greg_t),
+		.align = sizeof(compat_elf_greg_t),
+		.get = compat_gpr_get,
+		.set = compat_gpr_set
+	},
+	[REGSET_COMPAT_VFP] = {
+		.core_note_type = NT_ARM_VFP,
+		.n = VFP_STATE_SIZE / sizeof(compat_ulong_t),
+		.size = sizeof(compat_ulong_t),
+		.align = sizeof(compat_ulong_t),
+		.get = compat_vfp_get,
+		.set = compat_vfp_set
+	},
+};
+
+static const struct user_regset_view user_aarch32_view = {
+	.name = "aarch32", .e_machine = EM_ARM,
+	.regsets = aarch32_regsets, .n = ARRAY_SIZE(aarch32_regsets)
+};
+
+int aarch32_break_trap(struct pt_regs *regs)
+{
+	unsigned int instr;
+	bool bp = false;
+	void __user *pc = (void __user *)instruction_pointer(regs);
+
+	if (compat_thumb_mode(regs)) {
+		/* get 16-bit Thumb instruction */
+		get_user(instr, (u16 __user *)pc);
+		if (instr == AARCH32_BREAK_THUMB2_LO) {
+			/* get second half of 32-bit Thumb-2 instruction */
+			get_user(instr, (u16 __user *)(pc + 2));
+			bp = instr == AARCH32_BREAK_THUMB2_HI;
+		} else {
+			bp = instr == AARCH32_BREAK_THUMB;
+		}
+	} else {
+		/* 32-bit ARM instruction */
+		get_user(instr, (u32 __user *)pc);
+		bp = (instr & ~0xf0000000) == AARCH32_BREAK_ARM;
+	}
+
+	if (bp)
+		return ptrace_break(regs);
+	return 1;
+}
+
+static int compat_ptrace_read_user(struct task_struct *tsk, compat_ulong_t off,
+				   compat_ulong_t __user *ret)
+{
+	compat_ulong_t tmp;
+
+	if (off & 3)
+		return -EIO;
+
+	if (off == PT_TEXT_ADDR)
+		tmp = tsk->mm->start_code;
+	else if (off == PT_DATA_ADDR)
+		tmp = tsk->mm->start_data;
+	else if (off == PT_TEXT_END_ADDR)
+		tmp = tsk->mm->end_code;
+	else if (off < sizeof(compat_elf_gregset_t))
+		return copy_regset_to_user(tsk, &user_aarch32_view,
+					   REGSET_COMPAT_GPR, off,
+					   sizeof(compat_ulong_t), ret);
+	else if (off >= COMPAT_USER_SZ)
+		return -EIO;
+	else
+		tmp = 0;
+
+	return put_user(tmp, ret);
+}
+
+static int compat_ptrace_write_user(struct task_struct *tsk, compat_ulong_t off,
+				    compat_ulong_t val)
+{
+	int ret;
+
+	if (off & 3 || off >= COMPAT_USER_SZ)
+		return -EIO;
+
+	if (off >= sizeof(compat_elf_gregset_t))
+		return 0;
+
+	ret = copy_regset_from_user(tsk, &user_aarch32_view,
+				    REGSET_COMPAT_GPR, off,
+				    sizeof(compat_ulong_t),
+				    &val);
+	return ret;
+}
+
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+
+/*
+ * Convert a virtual register number into an index for a thread_info
+ * breakpoint array. Breakpoints are identified using positive numbers
+ * whilst watchpoints are negative. The registers are laid out as pairs
+ * of (address, control), each pair mapping to a unique hw_breakpoint struct.
+ * Register 0 is reserved for describing resource information.
+ */
+static int compat_ptrace_hbp_num_to_idx(compat_long_t num)
+{
+	return (abs(num) - 1) >> 1;
+}
+
+static int compat_ptrace_hbp_get_resource_info(u32 *kdata)
+{
+	u8 num_brps, num_wrps, debug_arch, wp_len;
+	u32 reg = 0;
+
+	num_brps	= hw_breakpoint_slots(TYPE_INST);
+	num_wrps	= hw_breakpoint_slots(TYPE_DATA);
+
+	debug_arch	= debug_monitors_arch();
+	wp_len		= 8;
+	reg		|= debug_arch;
+	reg		<<= 8;
+	reg		|= wp_len;
+	reg		<<= 8;
+	reg		|= num_wrps;
+	reg		<<= 8;
+	reg		|= num_brps;
+
+	*kdata = reg;
+	return 0;
+}
+
+static int compat_ptrace_hbp_get(unsigned int note_type,
+				 struct task_struct *tsk,
+				 compat_long_t num,
+				 u32 *kdata)
+{
+	u64 addr = 0;
+	u32 ctrl = 0;
+
+	int err, idx = compat_ptrace_hbp_num_to_idx(num);;
+
+	if (num & 1) {
+		err = ptrace_hbp_get_addr(note_type, tsk, idx, &addr);
+		*kdata = (u32)addr;
+	} else {
+		err = ptrace_hbp_get_ctrl(note_type, tsk, idx, &ctrl);
+		*kdata = ctrl;
+	}
+
+	return err;
+}
+
+static int compat_ptrace_hbp_set(unsigned int note_type,
+				 struct task_struct *tsk,
+				 compat_long_t num,
+				 u32 *kdata)
+{
+	u64 addr;
+	u32 ctrl;
+
+	int err, idx = compat_ptrace_hbp_num_to_idx(num);
+
+	if (num & 1) {
+		addr = *kdata;
+		err = ptrace_hbp_set_addr(note_type, tsk, idx, addr);
+	} else {
+		ctrl = *kdata;
+		err = ptrace_hbp_set_ctrl(note_type, tsk, idx, ctrl);
+	}
+
+	return err;
+}
+
+static int compat_ptrace_gethbpregs(struct task_struct *tsk, compat_long_t num,
+				    compat_ulong_t __user *data)
+{
+	int ret;
+	u32 kdata;
+	mm_segment_t old_fs = get_fs();
+
+	set_fs(KERNEL_DS);
+	/* Watchpoint */
+	if (num < 0) {
+		ret = compat_ptrace_hbp_get(NT_ARM_HW_WATCH, tsk, num, &kdata);
+	/* Resource info */
+	} else if (num == 0) {
+		ret = compat_ptrace_hbp_get_resource_info(&kdata);
+	/* Breakpoint */
+	} else {
+		ret = compat_ptrace_hbp_get(NT_ARM_HW_BREAK, tsk, num, &kdata);
+	}
+	set_fs(old_fs);
+
+	if (!ret)
+		ret = put_user(kdata, data);
+
+	return ret;
+}
+
+static int compat_ptrace_sethbpregs(struct task_struct *tsk, compat_long_t num,
+				    compat_ulong_t __user *data)
+{
+	int ret;
+	u32 kdata = 0;
+	mm_segment_t old_fs = get_fs();
+
+	if (num == 0)
+		return 0;
+
+	ret = get_user(kdata, data);
+	if (ret)
+		return ret;
+
+	set_fs(KERNEL_DS);
+	if (num < 0)
+		ret = compat_ptrace_hbp_set(NT_ARM_HW_WATCH, tsk, num, &kdata);
+	else
+		ret = compat_ptrace_hbp_set(NT_ARM_HW_BREAK, tsk, num, &kdata);
+	set_fs(old_fs);
+
+	return ret;
+}
+#endif	/* CONFIG_HAVE_HW_BREAKPOINT */
+
+long compat_arch_ptrace(struct task_struct *child, compat_long_t request,
+			compat_ulong_t caddr, compat_ulong_t cdata)
+{
+	unsigned long addr = caddr;
+	unsigned long data = cdata;
+	void __user *datap = compat_ptr(data);
+	int ret;
+
+	switch (request) {
+		case PTRACE_PEEKUSR:
+			ret = compat_ptrace_read_user(child, addr, datap);
+			break;
+
+		case PTRACE_POKEUSR:
+			ret = compat_ptrace_write_user(child, addr, data);
+			break;
+
+		case PTRACE_GETREGS:
+			ret = copy_regset_to_user(child,
+						  &user_aarch32_view,
+						  REGSET_COMPAT_GPR,
+						  0, sizeof(compat_elf_gregset_t),
+						  datap);
+			break;
+
+		case PTRACE_SETREGS:
+			ret = copy_regset_from_user(child,
+						    &user_aarch32_view,
+						    REGSET_COMPAT_GPR,
+						    0, sizeof(compat_elf_gregset_t),
+						    datap);
+			break;
+
+		case PTRACE_GET_THREAD_AREA:
+			ret = put_user((compat_ulong_t)child->thread.tp_value,
+				       (compat_ulong_t __user *)datap);
+			break;
+
+		case PTRACE_SET_SYSCALL:
+			task_pt_regs(child)->syscallno = data;
+			ret = 0;
+			break;
+
+		case COMPAT_PTRACE_GETVFPREGS:
+			ret = copy_regset_to_user(child,
+						  &user_aarch32_view,
+						  REGSET_COMPAT_VFP,
+						  0, VFP_STATE_SIZE,
+						  datap);
+			break;
+
+		case COMPAT_PTRACE_SETVFPREGS:
+			ret = copy_regset_from_user(child,
+						    &user_aarch32_view,
+						    REGSET_COMPAT_VFP,
+						    0, VFP_STATE_SIZE,
+						    datap);
+			break;
+
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+		case PTRACE_GETHBPREGS:
+			ret = compat_ptrace_gethbpregs(child, addr, datap);
+			break;
+
+		case PTRACE_SETHBPREGS:
+			ret = compat_ptrace_sethbpregs(child, addr, datap);
+			break;
+#endif
+
+		default:
+			ret = compat_ptrace_request(child, request, addr,
+						    data);
+			break;
+	}
+
+	return ret;
+}
+#endif /* CONFIG_COMPAT */
+
+const struct user_regset_view *task_user_regset_view(struct task_struct *task)
+{
+#ifdef CONFIG_COMPAT
+	if (is_compat_thread(task_thread_info(task)))
+		return &user_aarch32_view;
+#endif
+	return &user_aarch64_view;
+}
+
+long arch_ptrace(struct task_struct *child, long request,
+		 unsigned long addr, unsigned long data)
+{
+	return ptrace_request(child, request, addr, data);
+}
+
+
+static int __init ptrace_break_init(void)
+{
+	hook_debug_fault_code(DBG_ESR_EVT_BRK, arm64_break_trap, SIGTRAP,
+			      TRAP_BRKPT, "ptrace BRK handler");
+	return 0;
+}
+core_initcall(ptrace_break_init);
+
+
+asmlinkage int syscall_trace(int dir, struct pt_regs *regs)
+{
+	unsigned long saved_reg;
+
+	if (!test_thread_flag(TIF_SYSCALL_TRACE))
+		return regs->syscallno;
+
+	if (is_compat_task()) {
+		/* AArch32 uses ip (r12) for scratch */
+		saved_reg = regs->regs[12];
+		regs->regs[12] = dir;
+	} else {
+		/*
+		 * Save X7. X7 is used to denote syscall entry/exit:
+		 *   X7 = 0 -> entry, = 1 -> exit
+		 */
+		saved_reg = regs->regs[7];
+		regs->regs[7] = dir;
+	}
+
+	if (dir)
+		tracehook_report_syscall_exit(regs, 0);
+	else if (tracehook_report_syscall_entry(regs))
+		regs->syscallno = ~0UL;
+
+	if (is_compat_task())
+		regs->regs[12] = saved_reg;
+	else
+		regs->regs[7] = saved_reg;
+
+	return regs->syscallno;
+}
