commit d51c214541c5154dda3037289ee895ea3ded5ebd
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun May 10 09:54:41 2020 +0200

    arm64: fix the flush_icache_range arguments in machine_kexec
    
    The second argument is the end "pointer", not the length.
    
    Fixes: d28f6df1305a ("arm64/kexec: Add core kexec support")
    Cc: <stable@vger.kernel.org> # 4.8.x-
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 8e9c924423b4..a0b144cfaea7 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -177,6 +177,7 @@ void machine_kexec(struct kimage *kimage)
 	 * the offline CPUs. Therefore, we must use the __* variant here.
 	 */
 	__flush_icache_range((uintptr_t)reboot_code_buffer,
+			     (uintptr_t)reboot_code_buffer +
 			     arm64_relocate_new_kernel_size);
 
 	/* Flush the kimage list and its buffers. */

commit 1595fe299eb5a664c754eaf48bc178c0d664e1cf
Author: Will Deacon <will@kernel.org>
Date:   Fri Jan 10 16:00:50 2020 +0000

    Revert "arm64: kexec: make dtb_mem always enabled"
    
    Adding crash dump support to 'kexec_file' is going to extend 'struct
    kimage_arch' with more 'kexec_file'-specific members. The cleanup here
    then starts to get in the way, so revert it.
    
    This reverts commit 621516789ee6e285cb2088fe4706eedd030d38bf.
    
    Reported-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index ae1bad0156cd..8e9c924423b4 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -203,7 +203,11 @@ void machine_kexec(struct kimage *kimage)
 	 * In kexec_file case, the kernel starts directly without purgatory.
 	 */
 	cpu_soft_restart(reboot_code_buffer_phys, kimage->head, kimage->start,
-			 kimage->arch.dtb_mem);
+#ifdef CONFIG_KEXEC_FILE
+						kimage->arch.dtb_mem);
+#else
+						0);
+#endif
 
 	BUG(); /* Should never get here. */
 }

commit 621516789ee6e285cb2088fe4706eedd030d38bf
Author: Pavel Tatashin <pasha.tatashin@soleen.com>
Date:   Wed Dec 4 10:59:17 2019 -0500

    arm64: kexec: make dtb_mem always enabled
    
    Currently, dtb_mem is enabled only when CONFIG_KEXEC_FILE is
    enabled. This adds ugly ifdefs to c files.
    
    Always enabled dtb_mem, when it is not used, it is NULL.
    Change the dtb_mem to phys_addr_t, as it is a physical address.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 8e9c924423b4..ae1bad0156cd 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -203,11 +203,7 @@ void machine_kexec(struct kimage *kimage)
 	 * In kexec_file case, the kernel starts directly without purgatory.
 	 */
 	cpu_soft_restart(reboot_code_buffer_phys, kimage->head, kimage->start,
-#ifdef CONFIG_KEXEC_FILE
-						kimage->arch.dtb_mem);
-#else
-						0);
-#endif
+			 kimage->arch.dtb_mem);
 
 	BUG(); /* Should never get here. */
 }

commit 3b54b743397eb51f879f2bf24b9938b80e5a2092
Author: Pavel Tatashin <pasha.tatashin@soleen.com>
Date:   Wed Dec 4 10:59:16 2019 -0500

    arm64: kexec: remove unnecessary debug prints
    
    The kexec_image_info() outputs all the necessary information about the
    upcoming kexec. The extra debug printfs in machine_kexec() are not
    needed.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 0df8493624e0..8e9c924423b4 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -160,18 +160,6 @@ void machine_kexec(struct kimage *kimage)
 
 	kexec_image_info(kimage);
 
-	pr_debug("%s:%d: control_code_page:        %p\n", __func__, __LINE__,
-		kimage->control_code_page);
-	pr_debug("%s:%d: reboot_code_buffer_phys:  %pa\n", __func__, __LINE__,
-		&reboot_code_buffer_phys);
-	pr_debug("%s:%d: reboot_code_buffer:       %p\n", __func__, __LINE__,
-		reboot_code_buffer);
-	pr_debug("%s:%d: relocate_new_kernel:      %p\n", __func__, __LINE__,
-		arm64_relocate_new_kernel);
-	pr_debug("%s:%d: relocate_new_kernel_size: 0x%lx(%lu) bytes\n",
-		__func__, __LINE__, arm64_relocate_new_kernel_size,
-		arm64_relocate_new_kernel_size);
-
 	/*
 	 * Copy arm64_relocate_new_kernel to the reboot_code_buffer for use
 	 * after the kernel is shut down.

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 66b5d697d943..0df8493624e0 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * kexec for arm64
  *
  * Copyright (C) Linaro.
  * Copyright (C) Huawei Futurewei Technologies.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #include <linux/interrupt.h>

commit d9fa9d951779eb8110879f796434876a58321ae9
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Mar 5 15:47:28 2019 -0800

    arm64: kdump: no need to mark crashkernel pages manually PG_reserved
    
    The crashkernel is reserved via memblock_reserve().  memblock_free_all()
    will call free_low_memory_core_early(), which will go over all reserved
    memblocks, marking the pages as PG_reserved.
    
    So manually marking pages as PG_reserved is not necessary, they are
    already in the desired state (otherwise they would have been handed over
    to the buddy as free pages and bad things would happen).
    
    Link: http://lkml.kernel.org/r/20190114125903.24845-8-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Matthias Brugger <mbrugger@suse.com>
    Reviewed-by: Bhupesh Sharma <bhsharma@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Florian Fainelli <f.fainelli@gmail.com>
    Cc: Stefan Agner <stefan@agner.ch>
    Cc: Laura Abbott <labbott@redhat.com>
    Cc: Greg Hackmann <ghackmann@android.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kristina Martsenko <kristina.martsenko@arm.com>
    Cc: CHANDAN VN <chandan.vn@samsung.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 6f0587b5e941..66b5d697d943 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -321,7 +321,7 @@ void crash_post_resume(void)
  * but does not hold any data of loaded kernel image.
  *
  * Note that all the pages in crash dump kernel memory have been initially
- * marked as Reserved in kexec_reserve_crashkres_pages().
+ * marked as Reserved as memory was allocated via memblock_reserve().
  *
  * In hibernation, the pages which are Reserved and yet "nosave" are excluded
  * from the hibernation iamge. crash_is_nosave() does thich check for crash

commit aee494424414aa6f511bb837624557e9d3b84823
Author: David Hildenbrand <david@redhat.com>
Date:   Tue Mar 5 15:47:25 2019 -0800

    arm64: kexec: no need to ClearPageReserved()
    
    This will be done by free_reserved_page().
    
    Link: http://lkml.kernel.org/r/20190114125903.24845-7-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: James Morse <james.morse@arm.com>
    Reviewed-by: Bhupesh Sharma <bhsharma@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index aa9c94113700..6f0587b5e941 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -361,7 +361,6 @@ void crash_free_reserved_phys_range(unsigned long begin, unsigned long end)
 
 	for (addr = begin; addr < end; addr += PAGE_SIZE) {
 		page = phys_to_page(addr);
-		ClearPageReserved(page);
 		free_reserved_page(page);
 	}
 }

commit 4c9e7e649a3f291e1b939299458e6844c16afe70
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Thu Nov 15 14:52:52 2018 +0900

    arm64: kexec_file: invoke the kernel without purgatory
    
    On arm64, purgatory would do almost nothing. So just invoke secondary
    kernel directly by jumping into its entry code.
    
    While, in this case, cpu_soft_restart() must be called with dtb address
    in the fifth argument, the behavior still stays compatible with kexec_load
    case as long as the argument is null.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 922add8adb74..aa9c94113700 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -212,9 +212,17 @@ void machine_kexec(struct kimage *kimage)
 	 * uses physical addressing to relocate the new image to its final
 	 * position and transfers control to the image entry point when the
 	 * relocation is complete.
+	 * In kexec case, kimage->start points to purgatory assuming that
+	 * kernel entry and dtb address are embedded in purgatory by
+	 * userspace (kexec-tools).
+	 * In kexec_file case, the kernel starts directly without purgatory.
 	 */
-
-	cpu_soft_restart(reboot_code_buffer_phys, kimage->head, kimage->start, 0);
+	cpu_soft_restart(reboot_code_buffer_phys, kimage->head, kimage->start,
+#ifdef CONFIG_KEXEC_FILE
+						kimage->arch.dtb_mem);
+#else
+						0);
+#endif
 
 	BUG(); /* Should never get here. */
 }

commit 84c57dbd3c480fb2730c393a2cef994ddb4f42cc
Author: James Morse <james.morse@arm.com>
Date:   Mon Sep 10 15:20:54 2018 +0100

    arm64: kernel: arch_crash_save_vmcoreinfo() should depend on CONFIG_CRASH_CORE
    
    Since commit 23c85094fe18 ("proc/kcore: add vmcoreinfo note to /proc/kcore")
    the kernel has exported the vmcoreinfo PT_NOTE on /proc/kcore as well
    as /proc/vmcore.
    
    arm64 only exposes it's additional arch information via
    arch_crash_save_vmcoreinfo() if built with CONFIG_KEXEC, as kdump was
    previously the only user of vmcoreinfo.
    
    Move this weak function to a separate file that is built at the same
    time as its caller in kernel/crash_core.c. This ensures values like
    'kimage_voffset' are always present in the vmcoreinfo PT_NOTE.
    
    CC: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: Bhupesh Sharma <bhsharma@redhat.com>
    Signed-off-by: James Morse <james.morse@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index f6a5c6bc1434..922add8adb74 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -358,14 +358,3 @@ void crash_free_reserved_phys_range(unsigned long begin, unsigned long end)
 	}
 }
 #endif /* CONFIG_HIBERNATION */
-
-void arch_crash_save_vmcoreinfo(void)
-{
-	VMCOREINFO_NUMBER(VA_BITS);
-	/* Please note VMCOREINFO_NUMBER() uses "%d", not "%x" */
-	vmcoreinfo_append_str("NUMBER(kimage_voffset)=0x%llx\n",
-						kimage_voffset);
-	vmcoreinfo_append_str("NUMBER(PHYS_OFFSET)=0x%llx\n",
-						PHYS_OFFSET);
-	vmcoreinfo_append_str("KERNELOFFSET=%lx\n", kaslr_offset());
-}

commit dcab90d90935f990407c86b671a7f1ac285d106c
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Jul 31 12:09:03 2018 +0100

    arm64: kexec: Add comment to explain use of __flush_icache_range()
    
    Now that we understand the deadlock arising from flush_icache_range()
    on the kexec crash kernel path, add a comment to justify the use of
    __flush_icache_range() here.
    
    Reported-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 4c0eb30bfede..f6a5c6bc1434 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -184,8 +184,15 @@ void machine_kexec(struct kimage *kimage)
 
 	/* Flush the reboot_code_buffer in preparation for its execution. */
 	__flush_dcache_area(reboot_code_buffer, arm64_relocate_new_kernel_size);
+
+	/*
+	 * Although we've killed off the secondary CPUs, we don't update
+	 * the online mask if we're handling a crash kernel and consequently
+	 * need to avoid flush_icache_range(), which will attempt to IPI
+	 * the offline CPUs. Therefore, we must use the __* variant here.
+	 */
 	__flush_icache_range((uintptr_t)reboot_code_buffer,
-		arm64_relocate_new_kernel_size);
+			     arm64_relocate_new_kernel_size);
 
 	/* Flush the kimage list and its buffers. */
 	kexec_list_flush(kimage);

commit e401b7c2c69008ad2fcdc154f7c5421281c90042
Author: Bhupesh Sharma <bhsharma@redhat.com>
Date:   Mon Jul 30 11:54:43 2018 +0530

    arm64, kaslr: export offset in VMCOREINFO ELF notes
    
    Include KASLR offset in arm64 VMCOREINFO ELF notes to assist in
    debugging. vmcore parsing in user-space already expects this value in
    the notes and we are providing it for portability of those existing
    tools with x86.
    
    Ideally we would like core code to do this (so that way this
    information won't be missed when an architecture adds KASLR support),
    but mips has CONFIG_RANDOMIZE_BASE, and doesn't provide kaslr_offset(),
    so I am not sure if this is needed for mips (and other such similar arch
    cases in future). So, lets keep this architecture specific for now.
    
    As an example of a user-space use-case, consider the
    makedumpfile user-space utility which will need fixup to use this
    KASLR offset to work with cases where we need to find a way to
    translate symbol address from vmlinux to kernel run time address
    in case of KASLR boot on arm64.
    
    I have already submitted the makedumpfile user-space patch upstream
    and the maintainer has suggested to wait for the kernel changes to be
    included (see [0]).
    
    I tested this on my qualcomm amberwing board both for KASLR and
    non-KASLR boot cases:
    
    Without this patch:
       # cat > scrub.conf << EOF
       [vmlinux]
       erase jiffies
       erase init_task.utime
       for tsk in init_task.tasks.next within task_struct:tasks
           erase tsk.utime
       endfor
       EOF
    
      # makedumpfile --split -d 31 -x vmlinux --config scrub.conf vmcore dumpfile_{1,2,3}
      readpage_elf: Attempt to read non-existent page at 0xffffa8a5bf180000.
      readmem: type_addr: 1, addr:ffffa8a5bf180000, size:8
      vaddr_to_paddr_arm64: Can't read pgd
      readmem: Can't convert a virtual address(ffff0000092a542c) to physical
      address.
      readmem: type_addr: 0, addr:ffff0000092a542c, size:390
      check_release: Can't get the address of system_utsname
    
    After this patch check_release() is ok, and also we are able to erase
    symbol from vmcore (I checked this with kernel 4.18.0-rc4+):
    
      # makedumpfile --split -d 31 -x vmlinux --config scrub.conf vmcore dumpfile_{1,2,3}
      The kernel version is not supported.
      The makedumpfile operation may be incomplete.
      Checking for memory holes                         : [100.0 %] \
      Checking for memory holes                         : [100.0 %] |
      Checking foExcluding unnecessary pages                       : [100.0 %]
      \
      Excluding unnecessary pages                       : [100.0 %] \
    
      The dumpfiles are saved to dumpfile_1, dumpfile_2, and dumpfile_3.
    
      makedumpfile Completed.
    
    [0] https://www.spinics.net/lists/kexec/msg21195.html
    
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: James Morse <james.morse@arm.com>
    Signed-off-by: Bhupesh Sharma <bhsharma@redhat.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index e8c02832b7ad..4c0eb30bfede 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -360,4 +360,5 @@ void arch_crash_save_vmcoreinfo(void)
 						kimage_voffset);
 	vmcoreinfo_append_str("NUMBER(PHYS_OFFSET)=0x%llx\n",
 						PHYS_OFFSET);
+	vmcoreinfo_append_str("KERNELOFFSET=%lx\n", kaslr_offset());
 }

commit 140aada48b5f1a8bed3ba4afb5fc59220651657f
Author: Dave Kleikamp <dave.kleikamp@oracle.com>
Date:   Mon Jul 30 10:29:21 2018 -0500

    arm64: kexec: machine_kexec should call __flush_icache_range
    
    machine_kexec flushes the reboot_code_buffer from the icache
    after stopping the other cpus.
    
    Commit 3b8c9f1cdfc5 ("arm64: IPI each CPU after invalidating the I-cache
    for kernel mappings") added an IPI call to flush_icache_range, which
    causes a hang here, so replace the call with __flush_icache_range
    
    Signed-off-by: Dave Kleikamp <dave.kleikamp@oracle.com>
    Cc: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index f62effc6e064..e8c02832b7ad 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -184,7 +184,7 @@ void machine_kexec(struct kimage *kimage)
 
 	/* Flush the reboot_code_buffer in preparation for its execution. */
 	__flush_dcache_area(reboot_code_buffer, arm64_relocate_new_kernel_size);
-	flush_icache_range((uintptr_t)reboot_code_buffer,
+	__flush_icache_range((uintptr_t)reboot_code_buffer,
 		arm64_relocate_new_kernel_size);
 
 	/* Flush the kimage list and its buffers. */

commit 76f4e2da45b44bf70f61c28fcbc91668492463e0
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Jul 2 14:17:53 2018 +0100

    arm64: kexec: always reset to EL2 if present
    
    Currently machine_kexec() doesn't reset to EL2 in the case of a
    crashdump kernel. This leaves potentially dodgy state active at EL2, and
    means that if the crashdump kernel attempts to online secondary CPUs,
    these will be booted as mismatched ELs.
    
    Let's reset to EL2, as we do in all other cases, and simplify things. If
    EL2 state is corrupt, things are already sufficiently bad that kdump is
    unlikely to work, and it's best-effort regardless.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: James Morse <james.morse@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index f76ea92dff91..f62effc6e064 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -207,8 +207,7 @@ void machine_kexec(struct kimage *kimage)
 	 * relocation is complete.
 	 */
 
-	cpu_soft_restart(kimage != kexec_crash_image,
-		reboot_code_buffer_phys, kimage->head, kimage->start, 0);
+	cpu_soft_restart(reboot_code_buffer_phys, kimage->head, kimage->start, 0);
 
 	BUG(); /* Should never get here. */
 }

commit 0fbeb318754860b37150fd42c2058d636a431426
Author: James Morse <james.morse@arm.com>
Date:   Thu Nov 2 12:12:34 2017 +0000

    arm64: explicitly mask all exceptions
    
    There are a few places where we want to mask all exceptions. Today we
    do this in a piecemeal fashion, typically we expect the caller to
    have masked irqs and the arch code masks debug exceptions, ignoring
    serror which is probably masked.
    
    Make it clear that 'mask all exceptions' is the intention by adding
    helpers to do exactly that.
    
    This will let us unmask SError without having to add 'oh and SError'
    to these paths.
    
    Signed-off-by: James Morse <james.morse@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 11121f608eb5..f76ea92dff91 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -18,6 +18,7 @@
 
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
+#include <asm/daifflags.h>
 #include <asm/memory.h>
 #include <asm/mmu.h>
 #include <asm/mmu_context.h>
@@ -195,8 +196,7 @@ void machine_kexec(struct kimage *kimage)
 
 	pr_info("Bye!\n");
 
-	/* Disable all DAIF exceptions. */
-	asm volatile ("msr daifset, #0xf" : : : "memory");
+	local_daif_mask();
 
 	/*
 	 * cpu_soft_restart will shutdown the MMU, disable data caches, then

commit a88ce63b642cf8cd82cbc278429ccd9de4455a07
Author: Hoeun Ryu <hoeun.ryu@gmail.com>
Date:   Thu Aug 17 11:24:27 2017 +0900

    arm64: kexec: have own crash_smp_send_stop() for crash dump for nonpanic cores
    
     Commit 0ee5941 : (x86/panic: replace smp_send_stop() with kdump friendly
    version in panic path) introduced crash_smp_send_stop() which is a weak
    function and can be overridden by architecture codes to fix the side effect
    caused by commit f06e515 : (kernel/panic.c: add "crash_kexec_post_
    notifiers" option).
    
     ARM64 architecture uses the weak version function and the problem is that
    the weak function simply calls smp_send_stop() which makes other CPUs
    offline and takes away the chance to save crash information for nonpanic
    CPUs in machine_crash_shutdown() when crash_kexec_post_notifiers kernel
    option is enabled.
    
     Calling smp_send_crash_stop() in machine_crash_shutdown() is useless
    because all nonpanic CPUs are already offline by smp_send_stop() in this
    case and smp_send_crash_stop() only works against online CPUs.
    
     The result is that secondary CPUs registers are not saved by
    crash_save_cpu() and the vmcore file misreports these CPUs as being
    offline.
    
     crash_smp_send_stop() is implemented to fix this problem by replacing the
    existing smp_send_crash_stop() and adding a check for multiple calling to
    the function. The function (strong symbol version) saves crash information
    for nonpanic CPUs and machine_crash_shutdown() tries to save crash
    information for nonpanic CPUs only when crash_kexec_post_notifiers kernel
    option is disabled.
    
    * crash_kexec_post_notifiers : false
    
      panic()
        __crash_kexec()
          machine_crash_shutdown()
            crash_smp_send_stop()    <= save crash dump for nonpanic cores
    
    * crash_kexec_post_notifiers : true
    
      panic()
        crash_smp_send_stop()        <= save crash dump for nonpanic cores
        __crash_kexec()
          machine_crash_shutdown()
            crash_smp_send_stop()    <= just return.
    
    Signed-off-by: Hoeun Ryu <hoeun.ryu@gmail.com>
    Reviewed-by: James Morse <james.morse@arm.com>
    Tested-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 481f54a866c5..11121f608eb5 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -252,7 +252,7 @@ void machine_crash_shutdown(struct pt_regs *regs)
 	local_irq_disable();
 
 	/* shutdown non-crashing cpus */
-	smp_send_crash_stop();
+	crash_smp_send_stop();
 
 	/* for crashing cpu */
 	crash_save_cpu(regs, smp_processor_id());

commit 20a166243328c14a0c24bd8c7919223ab4174917
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:37 2017 +0900

    arm64: kdump: add VMCOREINFO's for user-space tools
    
    In addition to common VMCOREINFO's defined in
    crash_save_vmcoreinfo_init(), we need to know, for crash utility,
      - kimage_voffset
      - PHYS_OFFSET
    to examine the contents of a dump file (/proc/vmcore) correctly
    due to the introduction of KASLR (CONFIG_RANDOMIZE_BASE) in v4.6.
    
      - VA_BITS
    is also required for makedumpfile command.
    
    arch_crash_save_vmcoreinfo() appends them to the dump file.
    More VMCOREINFO's may be added later.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index 779a80046066..481f54a866c5 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -18,6 +18,7 @@
 
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
+#include <asm/memory.h>
 #include <asm/mmu.h>
 #include <asm/mmu_context.h>
 #include <asm/page.h>
@@ -351,3 +352,13 @@ void crash_free_reserved_phys_range(unsigned long begin, unsigned long end)
 	}
 }
 #endif /* CONFIG_HIBERNATION */
+
+void arch_crash_save_vmcoreinfo(void)
+{
+	VMCOREINFO_NUMBER(VA_BITS);
+	/* Please note VMCOREINFO_NUMBER() uses "%d", not "%x" */
+	vmcoreinfo_append_str("NUMBER(kimage_voffset)=0x%llx\n",
+						kimage_voffset);
+	vmcoreinfo_append_str("NUMBER(PHYS_OFFSET)=0x%llx\n",
+						PHYS_OFFSET);
+}

commit 78fd584cdec0518075cf3aa75e5ec491cc8f3ff3
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:36 2017 +0900

    arm64: kdump: implement machine_crash_shutdown()
    
    Primary kernel calls machine_crash_shutdown() to shut down non-boot cpus
    and save registers' status in per-cpu ELF notes before starting crash
    dump kernel. See kernel_kexec().
    Even if not all secondary cpus have shut down, we do kdump anyway.
    
    As we don't have to make non-boot(crashed) cpus offline (to preserve
    correct status of cpus at crash dump) before shutting down, this patch
    also adds a variant of smp_send_stop().
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index a6d66b98d795..779a80046066 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -9,6 +9,9 @@
  * published by the Free Software Foundation.
  */
 
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/kernel.h>
 #include <linux/kexec.h>
 #include <linux/page-flags.h>
 #include <linux/smp.h>
@@ -143,11 +146,15 @@ void machine_kexec(struct kimage *kimage)
 {
 	phys_addr_t reboot_code_buffer_phys;
 	void *reboot_code_buffer;
+	bool in_kexec_crash = (kimage == kexec_crash_image);
+	bool stuck_cpus = cpus_are_stuck_in_kernel();
 
 	/*
 	 * New cpus may have become stuck_in_kernel after we loaded the image.
 	 */
-	BUG_ON(cpus_are_stuck_in_kernel() || (num_online_cpus() > 1));
+	BUG_ON(!in_kexec_crash && (stuck_cpus || (num_online_cpus() > 1)));
+	WARN(in_kexec_crash && (stuck_cpus || smp_crash_stop_failed()),
+		"Some CPUs may be stale, kdump will be unreliable.\n");
 
 	reboot_code_buffer_phys = page_to_phys(kimage->control_code_page);
 	reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys);
@@ -199,15 +206,58 @@ void machine_kexec(struct kimage *kimage)
 	 * relocation is complete.
 	 */
 
-	cpu_soft_restart(1, reboot_code_buffer_phys, kimage->head,
-		kimage->start, 0);
+	cpu_soft_restart(kimage != kexec_crash_image,
+		reboot_code_buffer_phys, kimage->head, kimage->start, 0);
 
 	BUG(); /* Should never get here. */
 }
 
+static void machine_kexec_mask_interrupts(void)
+{
+	unsigned int i;
+	struct irq_desc *desc;
+
+	for_each_irq_desc(i, desc) {
+		struct irq_chip *chip;
+		int ret;
+
+		chip = irq_desc_get_chip(desc);
+		if (!chip)
+			continue;
+
+		/*
+		 * First try to remove the active state. If this
+		 * fails, try to EOI the interrupt.
+		 */
+		ret = irq_set_irqchip_state(i, IRQCHIP_STATE_ACTIVE, false);
+
+		if (ret && irqd_irq_inprogress(&desc->irq_data) &&
+		    chip->irq_eoi)
+			chip->irq_eoi(&desc->irq_data);
+
+		if (chip->irq_mask)
+			chip->irq_mask(&desc->irq_data);
+
+		if (chip->irq_disable && !irqd_irq_disabled(&desc->irq_data))
+			chip->irq_disable(&desc->irq_data);
+	}
+}
+
+/**
+ * machine_crash_shutdown - shutdown non-crashing cpus and save registers
+ */
 void machine_crash_shutdown(struct pt_regs *regs)
 {
-	/* Empty routine needed to avoid build errors. */
+	local_irq_disable();
+
+	/* shutdown non-crashing cpus */
+	smp_send_crash_stop();
+
+	/* for crashing cpu */
+	crash_save_cpu(regs, smp_processor_id());
+	machine_kexec_mask_interrupts();
+
+	pr_info("Starting crashdump kernel...\n");
 }
 
 void arch_kexec_protect_crashkres(void)

commit 254a41c0ba0573fa23272945d3fbe39efcc5d07d
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:35 2017 +0900

    arm64: hibernate: preserve kdump image around hibernation
    
    Since arch_kexec_protect_crashkres() removes a mapping for crash dump
    kernel image, the loaded data won't be preserved around hibernation.
    
    In this patch, helper functions, crash_prepare_suspend()/
    crash_post_resume(), are additionally called before/after hibernation so
    that the relevant memory segments will be mapped again and preserved just
    as the others are.
    
    In addition, to minimize the size of hibernation image, crash_is_nosave()
    is added to pfn_is_nosave() in order to recognize only the pages that hold
    loaded crash dump kernel image as saveable. Hibernation excludes any pages
    that are marked as Reserved and yet "nosave."
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index b63baa749609..a6d66b98d795 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -10,6 +10,7 @@
  */
 
 #include <linux/kexec.h>
+#include <linux/page-flags.h>
 #include <linux/smp.h>
 
 #include <asm/cacheflush.h>
@@ -230,3 +231,73 @@ void arch_kexec_unprotect_crashkres(void)
 			__phys_to_virt(kexec_crash_image->segment[i].mem),
 			kexec_crash_image->segment[i].memsz >> PAGE_SHIFT, 1);
 }
+
+#ifdef CONFIG_HIBERNATION
+/*
+ * To preserve the crash dump kernel image, the relevant memory segments
+ * should be mapped again around the hibernation.
+ */
+void crash_prepare_suspend(void)
+{
+	if (kexec_crash_image)
+		arch_kexec_unprotect_crashkres();
+}
+
+void crash_post_resume(void)
+{
+	if (kexec_crash_image)
+		arch_kexec_protect_crashkres();
+}
+
+/*
+ * crash_is_nosave
+ *
+ * Return true only if a page is part of reserved memory for crash dump kernel,
+ * but does not hold any data of loaded kernel image.
+ *
+ * Note that all the pages in crash dump kernel memory have been initially
+ * marked as Reserved in kexec_reserve_crashkres_pages().
+ *
+ * In hibernation, the pages which are Reserved and yet "nosave" are excluded
+ * from the hibernation iamge. crash_is_nosave() does thich check for crash
+ * dump kernel and will reduce the total size of hibernation image.
+ */
+
+bool crash_is_nosave(unsigned long pfn)
+{
+	int i;
+	phys_addr_t addr;
+
+	if (!crashk_res.end)
+		return false;
+
+	/* in reserved memory? */
+	addr = __pfn_to_phys(pfn);
+	if ((addr < crashk_res.start) || (crashk_res.end < addr))
+		return false;
+
+	if (!kexec_crash_image)
+		return true;
+
+	/* not part of loaded kernel image? */
+	for (i = 0; i < kexec_crash_image->nr_segments; i++)
+		if (addr >= kexec_crash_image->segment[i].mem &&
+				addr < (kexec_crash_image->segment[i].mem +
+					kexec_crash_image->segment[i].memsz))
+			return false;
+
+	return true;
+}
+
+void crash_free_reserved_phys_range(unsigned long begin, unsigned long end)
+{
+	unsigned long addr;
+	struct page *page;
+
+	for (addr = begin; addr < end; addr += PAGE_SIZE) {
+		page = phys_to_page(addr);
+		ClearPageReserved(page);
+		free_reserved_page(page);
+	}
+}
+#endif /* CONFIG_HIBERNATION */

commit 98d2e1539b84abddce4b3c2ca8733f6aeacdee47
Author: Takahiro Akashi <takahiro.akashi@linaro.org>
Date:   Mon Apr 3 11:24:34 2017 +0900

    arm64: kdump: protect crash dump kernel memory
    
    arch_kexec_protect_crashkres() and arch_kexec_unprotect_crashkres()
    are meant to be called by kexec_load() in order to protect the memory
    allocated for crash dump kernel once the image is loaded.
    
    The protection is implemented by unmapping the relevant segments in crash
    dump kernel memory, rather than making it read-only as other archs do,
    to prevent coherency issues due to potential cache aliasing (with
    mismatched attributes).
    
    Page-level mappings are consistently used here so that we can change
    the attributes of segments in page granularity as well as shrink the region
    also in page granularity through /sys/kernel/kexec_crash_size, putting
    the freed memory back to buddy system.
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index bc96c8a7fc79..b63baa749609 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -14,7 +14,9 @@
 
 #include <asm/cacheflush.h>
 #include <asm/cpu_ops.h>
+#include <asm/mmu.h>
 #include <asm/mmu_context.h>
+#include <asm/page.h>
 
 #include "cpu-reset.h"
 
@@ -22,8 +24,6 @@
 extern const unsigned char arm64_relocate_new_kernel[];
 extern const unsigned long arm64_relocate_new_kernel_size;
 
-static unsigned long kimage_start;
-
 /**
  * kexec_image_info - For debugging output.
  */
@@ -64,8 +64,6 @@ void machine_kexec_cleanup(struct kimage *kimage)
  */
 int machine_kexec_prepare(struct kimage *kimage)
 {
-	kimage_start = kimage->start;
-
 	kexec_image_info(kimage);
 
 	if (kimage->type != KEXEC_TYPE_CRASH && cpus_are_stuck_in_kernel()) {
@@ -183,7 +181,7 @@ void machine_kexec(struct kimage *kimage)
 	kexec_list_flush(kimage);
 
 	/* Flush the new image if already in place. */
-	if (kimage->head & IND_DONE)
+	if ((kimage != kexec_crash_image) && (kimage->head & IND_DONE))
 		kexec_segment_flush(kimage);
 
 	pr_info("Bye!\n");
@@ -201,7 +199,7 @@ void machine_kexec(struct kimage *kimage)
 	 */
 
 	cpu_soft_restart(1, reboot_code_buffer_phys, kimage->head,
-		kimage_start, 0);
+		kimage->start, 0);
 
 	BUG(); /* Should never get here. */
 }
@@ -210,3 +208,25 @@ void machine_crash_shutdown(struct pt_regs *regs)
 {
 	/* Empty routine needed to avoid build errors. */
 }
+
+void arch_kexec_protect_crashkres(void)
+{
+	int i;
+
+	kexec_segment_flush(kexec_crash_image);
+
+	for (i = 0; i < kexec_crash_image->nr_segments; i++)
+		set_memory_valid(
+			__phys_to_virt(kexec_crash_image->segment[i].mem),
+			kexec_crash_image->segment[i].memsz >> PAGE_SHIFT, 0);
+}
+
+void arch_kexec_unprotect_crashkres(void)
+{
+	int i;
+
+	for (i = 0; i < kexec_crash_image->nr_segments; i++)
+		set_memory_valid(
+			__phys_to_virt(kexec_crash_image->segment[i].mem),
+			kexec_crash_image->segment[i].memsz >> PAGE_SHIFT, 1);
+}

commit 221f2c770e10d3f8dca71e1e14e24e61dc8988dd
Author: Geoff Levand <geoff@infradead.org>
Date:   Thu Jun 23 17:54:48 2016 +0000

    arm64/kexec: Add pr_debug output
    
    To aid in debugging kexec problems or when adding new functionality to
    kexec add a new routine kexec_image_info() and several inline pr_debug
    statements.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
index c40e64607545..bc96c8a7fc79 100644
--- a/arch/arm64/kernel/machine_kexec.c
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -24,6 +24,32 @@ extern const unsigned long arm64_relocate_new_kernel_size;
 
 static unsigned long kimage_start;
 
+/**
+ * kexec_image_info - For debugging output.
+ */
+#define kexec_image_info(_i) _kexec_image_info(__func__, __LINE__, _i)
+static void _kexec_image_info(const char *func, int line,
+	const struct kimage *kimage)
+{
+	unsigned long i;
+
+	pr_debug("%s:%d:\n", func, line);
+	pr_debug("  kexec kimage info:\n");
+	pr_debug("    type:        %d\n", kimage->type);
+	pr_debug("    start:       %lx\n", kimage->start);
+	pr_debug("    head:        %lx\n", kimage->head);
+	pr_debug("    nr_segments: %lu\n", kimage->nr_segments);
+
+	for (i = 0; i < kimage->nr_segments; i++) {
+		pr_debug("      segment[%lu]: %016lx - %016lx, 0x%lx bytes, %lu pages\n",
+			i,
+			kimage->segment[i].mem,
+			kimage->segment[i].mem + kimage->segment[i].memsz,
+			kimage->segment[i].memsz,
+			kimage->segment[i].memsz /  PAGE_SIZE);
+	}
+}
+
 void machine_kexec_cleanup(struct kimage *kimage)
 {
 	/* Empty routine needed to avoid build errors. */
@@ -40,6 +66,8 @@ int machine_kexec_prepare(struct kimage *kimage)
 {
 	kimage_start = kimage->start;
 
+	kexec_image_info(kimage);
+
 	if (kimage->type != KEXEC_TYPE_CRASH && cpus_are_stuck_in_kernel()) {
 		pr_err("Can't kexec: CPUs are stuck in the kernel.\n");
 		return -EBUSY;
@@ -125,6 +153,20 @@ void machine_kexec(struct kimage *kimage)
 	reboot_code_buffer_phys = page_to_phys(kimage->control_code_page);
 	reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys);
 
+	kexec_image_info(kimage);
+
+	pr_debug("%s:%d: control_code_page:        %p\n", __func__, __LINE__,
+		kimage->control_code_page);
+	pr_debug("%s:%d: reboot_code_buffer_phys:  %pa\n", __func__, __LINE__,
+		&reboot_code_buffer_phys);
+	pr_debug("%s:%d: reboot_code_buffer:       %p\n", __func__, __LINE__,
+		reboot_code_buffer);
+	pr_debug("%s:%d: relocate_new_kernel:      %p\n", __func__, __LINE__,
+		arm64_relocate_new_kernel);
+	pr_debug("%s:%d: relocate_new_kernel_size: 0x%lx(%lu) bytes\n",
+		__func__, __LINE__, arm64_relocate_new_kernel_size,
+		arm64_relocate_new_kernel_size);
+
 	/*
 	 * Copy arm64_relocate_new_kernel to the reboot_code_buffer for use
 	 * after the kernel is shut down.

commit d28f6df1305a86715e4e7ea0f043ba01c0a0e8d9
Author: Geoff Levand <geoff@infradead.org>
Date:   Thu Jun 23 17:54:48 2016 +0000

    arm64/kexec: Add core kexec support
    
    Add three new files, kexec.h, machine_kexec.c and relocate_kernel.S to the
    arm64 architecture that add support for the kexec re-boot mechanism
    (CONFIG_KEXEC) on arm64 platforms.
    
    Signed-off-by: Geoff Levand <geoff@infradead.org>
    Reviewed-by: James Morse <james.morse@arm.com>
    [catalin.marinas@arm.com: removed dead code following James Morse's comments]
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
new file mode 100644
index 000000000000..c40e64607545
--- /dev/null
+++ b/arch/arm64/kernel/machine_kexec.c
@@ -0,0 +1,170 @@
+/*
+ * kexec for arm64
+ *
+ * Copyright (C) Linaro.
+ * Copyright (C) Huawei Futurewei Technologies.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/kexec.h>
+#include <linux/smp.h>
+
+#include <asm/cacheflush.h>
+#include <asm/cpu_ops.h>
+#include <asm/mmu_context.h>
+
+#include "cpu-reset.h"
+
+/* Global variables for the arm64_relocate_new_kernel routine. */
+extern const unsigned char arm64_relocate_new_kernel[];
+extern const unsigned long arm64_relocate_new_kernel_size;
+
+static unsigned long kimage_start;
+
+void machine_kexec_cleanup(struct kimage *kimage)
+{
+	/* Empty routine needed to avoid build errors. */
+}
+
+/**
+ * machine_kexec_prepare - Prepare for a kexec reboot.
+ *
+ * Called from the core kexec code when a kernel image is loaded.
+ * Forbid loading a kexec kernel if we have no way of hotplugging cpus or cpus
+ * are stuck in the kernel. This avoids a panic once we hit machine_kexec().
+ */
+int machine_kexec_prepare(struct kimage *kimage)
+{
+	kimage_start = kimage->start;
+
+	if (kimage->type != KEXEC_TYPE_CRASH && cpus_are_stuck_in_kernel()) {
+		pr_err("Can't kexec: CPUs are stuck in the kernel.\n");
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+/**
+ * kexec_list_flush - Helper to flush the kimage list and source pages to PoC.
+ */
+static void kexec_list_flush(struct kimage *kimage)
+{
+	kimage_entry_t *entry;
+
+	for (entry = &kimage->head; ; entry++) {
+		unsigned int flag;
+		void *addr;
+
+		/* flush the list entries. */
+		__flush_dcache_area(entry, sizeof(kimage_entry_t));
+
+		flag = *entry & IND_FLAGS;
+		if (flag == IND_DONE)
+			break;
+
+		addr = phys_to_virt(*entry & PAGE_MASK);
+
+		switch (flag) {
+		case IND_INDIRECTION:
+			/* Set entry point just before the new list page. */
+			entry = (kimage_entry_t *)addr - 1;
+			break;
+		case IND_SOURCE:
+			/* flush the source pages. */
+			__flush_dcache_area(addr, PAGE_SIZE);
+			break;
+		case IND_DESTINATION:
+			break;
+		default:
+			BUG();
+		}
+	}
+}
+
+/**
+ * kexec_segment_flush - Helper to flush the kimage segments to PoC.
+ */
+static void kexec_segment_flush(const struct kimage *kimage)
+{
+	unsigned long i;
+
+	pr_debug("%s:\n", __func__);
+
+	for (i = 0; i < kimage->nr_segments; i++) {
+		pr_debug("  segment[%lu]: %016lx - %016lx, 0x%lx bytes, %lu pages\n",
+			i,
+			kimage->segment[i].mem,
+			kimage->segment[i].mem + kimage->segment[i].memsz,
+			kimage->segment[i].memsz,
+			kimage->segment[i].memsz /  PAGE_SIZE);
+
+		__flush_dcache_area(phys_to_virt(kimage->segment[i].mem),
+			kimage->segment[i].memsz);
+	}
+}
+
+/**
+ * machine_kexec - Do the kexec reboot.
+ *
+ * Called from the core kexec code for a sys_reboot with LINUX_REBOOT_CMD_KEXEC.
+ */
+void machine_kexec(struct kimage *kimage)
+{
+	phys_addr_t reboot_code_buffer_phys;
+	void *reboot_code_buffer;
+
+	/*
+	 * New cpus may have become stuck_in_kernel after we loaded the image.
+	 */
+	BUG_ON(cpus_are_stuck_in_kernel() || (num_online_cpus() > 1));
+
+	reboot_code_buffer_phys = page_to_phys(kimage->control_code_page);
+	reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys);
+
+	/*
+	 * Copy arm64_relocate_new_kernel to the reboot_code_buffer for use
+	 * after the kernel is shut down.
+	 */
+	memcpy(reboot_code_buffer, arm64_relocate_new_kernel,
+		arm64_relocate_new_kernel_size);
+
+	/* Flush the reboot_code_buffer in preparation for its execution. */
+	__flush_dcache_area(reboot_code_buffer, arm64_relocate_new_kernel_size);
+	flush_icache_range((uintptr_t)reboot_code_buffer,
+		arm64_relocate_new_kernel_size);
+
+	/* Flush the kimage list and its buffers. */
+	kexec_list_flush(kimage);
+
+	/* Flush the new image if already in place. */
+	if (kimage->head & IND_DONE)
+		kexec_segment_flush(kimage);
+
+	pr_info("Bye!\n");
+
+	/* Disable all DAIF exceptions. */
+	asm volatile ("msr daifset, #0xf" : : : "memory");
+
+	/*
+	 * cpu_soft_restart will shutdown the MMU, disable data caches, then
+	 * transfer control to the reboot_code_buffer which contains a copy of
+	 * the arm64_relocate_new_kernel routine.  arm64_relocate_new_kernel
+	 * uses physical addressing to relocate the new image to its final
+	 * position and transfers control to the image entry point when the
+	 * relocation is complete.
+	 */
+
+	cpu_soft_restart(1, reboot_code_buffer_phys, kimage->head,
+		kimage_start, 0);
+
+	BUG(); /* Should never get here. */
+}
+
+void machine_crash_shutdown(struct pt_regs *regs)
+{
+	/* Empty routine needed to avoid build errors. */
+}
