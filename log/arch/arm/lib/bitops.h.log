commit c001899a5d6c2d7a0f3b75b2307ddef137fb46a6
Author: Stefan Agner <stefan@agner.ch>
Date:   Mon Feb 18 00:56:58 2019 +0100

    ARM: 8843/1: use unified assembler in headers
    
    Use unified assembler syntax (UAL) in headers. Divided syntax is
    considered deprecated. This will also allow to build the kernel
    using LLVM's integrated assembler.
    
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 93cddab73072..95bd35991288 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -7,7 +7,7 @@
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
 	ands	ip, r1, #3
-	strneb	r1, [ip]		@ assert word-aligned
+	strbne	r1, [ip]		@ assert word-aligned
 	mov	r2, #1
 	and	r3, r0, #31		@ Get bit offset
 	mov	r0, r0, lsr #5
@@ -32,7 +32,7 @@ ENDPROC(\name		)
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
 	ands	ip, r1, #3
-	strneb	r1, [ip]		@ assert word-aligned
+	strbne	r1, [ip]		@ assert word-aligned
 	mov	r2, #1
 	and	r3, r0, #31		@ Get bit offset
 	mov	r0, r0, lsr #5
@@ -62,7 +62,7 @@ ENDPROC(\name		)
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
 	ands	ip, r1, #3
-	strneb	r1, [ip]		@ assert word-aligned
+	strbne	r1, [ip]		@ assert word-aligned
 	and	r2, r0, #31
 	mov	r0, r0, lsr #5
 	mov	r3, #1
@@ -89,7 +89,7 @@ ENDPROC(\name		)
 ENTRY(	\name		)
 UNWIND(	.fnstart	)
 	ands	ip, r1, #3
-	strneb	r1, [ip]		@ assert word-aligned
+	strbne	r1, [ip]		@ assert word-aligned
 	and	r3, r0, #31
 	mov	r0, r0, lsr #5
 	save_and_disable_irqs ip

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 7d807cfd8ef5..93cddab73072 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #include <asm/assembler.h>
 #include <asm/unwind.h>
 

commit 8478132a8784605fe07ede555f7277d989368d73
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Wed Nov 23 10:00:03 2016 +0000

    Revert "arm: move exports to definitions"
    
    This reverts commit 4dd1837d7589f468ed109556513f476e7a7f9121.
    
    Moving the exports for assembly code into the assembly files breaks
    KSYM trimming, but also breaks modversions.
    
    While fixing the KSYM trimming is trivial, fixing modversions brings
    us to a technically worse position that we had prior to the above
    change:
    
    - We end up with the prototype definitions divorsed from everything
      else, which means that adding or removing assembly level ksyms
      become more fragile:
      * if adding a new assembly ksyms export, a missed prototype in
        asm-prototypes.h results in a successful build if no module in
        the selected configuration makes use of the symbol.
      * when removing a ksyms export, asm-prototypes.h will get forgotten,
        with armksyms.c, you'll get a build error if you forget to touch
        the file.
    
    - We end up with the same amount of include files and prototypes,
      they're just in a header file instead of a .c file with their
      exports.
    
    As for lines of code, we don't get much of a size reduction:
     (original commit)
     47 files changed, 131 insertions(+), 208 deletions(-)
     (fix for ksyms trimming)
     7 files changed, 18 insertions(+), 5 deletions(-)
     (two fixes for modversions)
     1 file changed, 34 insertions(+)
     3 files changed, 7 insertions(+), 2 deletions(-)
    which results in a net total of only 25 lines deleted.
    
    As there does not seem to be much benefit from this change of approach,
    revert the change.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index df06638b327c..7d807cfd8ef5 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,6 +1,5 @@
 #include <asm/assembler.h>
 #include <asm/unwind.h>
-#include <asm/export.h>
 
 #if __LINUX_ARM_ARCH__ >= 6
 	.macro	bitop, name, instr
@@ -26,7 +25,6 @@ UNWIND(	.fnstart	)
 	bx	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
-EXPORT_SYMBOL(\name	)
 	.endm
 
 	.macro	testop, name, instr, store
@@ -57,7 +55,6 @@ UNWIND(	.fnstart	)
 2:	bx	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
-EXPORT_SYMBOL(\name	)
 	.endm
 #else
 	.macro	bitop, name, instr
@@ -77,7 +74,6 @@ UNWIND(	.fnstart	)
 	ret	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
-EXPORT_SYMBOL(\name	)
 	.endm
 
 /**
@@ -106,6 +102,5 @@ UNWIND(	.fnstart	)
 	ret	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
-EXPORT_SYMBOL(\name	)
 	.endm
 #endif

commit 4dd1837d7589f468ed109556513f476e7a7f9121
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Jan 13 13:46:22 2016 -0500

    arm: move exports to definitions
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 7d807cfd8ef5..df06638b327c 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,5 +1,6 @@
 #include <asm/assembler.h>
 #include <asm/unwind.h>
+#include <asm/export.h>
 
 #if __LINUX_ARM_ARCH__ >= 6
 	.macro	bitop, name, instr
@@ -25,6 +26,7 @@ UNWIND(	.fnstart	)
 	bx	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
+EXPORT_SYMBOL(\name	)
 	.endm
 
 	.macro	testop, name, instr, store
@@ -55,6 +57,7 @@ UNWIND(	.fnstart	)
 2:	bx	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
+EXPORT_SYMBOL(\name	)
 	.endm
 #else
 	.macro	bitop, name, instr
@@ -74,6 +77,7 @@ UNWIND(	.fnstart	)
 	ret	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
+EXPORT_SYMBOL(\name	)
 	.endm
 
 /**
@@ -102,5 +106,6 @@ UNWIND(	.fnstart	)
 	ret	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
+EXPORT_SYMBOL(\name	)
 	.endm
 #endif

commit 6ebbf2ce437b33022d30badd49dc94d33ecfa498
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jun 30 16:29:12 2014 +0100

    ARM: convert all "mov.* pc, reg" to "bx reg" for ARMv6+
    
    ARMv6 and greater introduced a new instruction ("bx") which can be used
    to return from function calls.  Recent CPUs perform better when the
    "bx lr" instruction is used rather than the "mov pc, lr" instruction,
    and this sequence is strongly recommended to be used by the ARM
    architecture manual (section A.4.1.1).
    
    We provide a new macro "ret" with all its variants for the condition
    code which will resolve to the appropriate instruction.
    
    Rather than doing this piecemeal, and miss some instances, change all
    the "mov pc" instances to use the new macro, with the exception of
    the "movs" instruction and the kprobes code.  This allows us to detect
    the "mov pc, lr" case and fix it up - and also gives us the possibility
    of deploying this for other registers depending on the CPU selection.
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> # Tegra Jetson TK1
    Tested-by: Robert Jarzmik <robert.jarzmik@free.fr> # mioa701_bootresume.S
    Tested-by: Andrew Lunn <andrew@lunn.ch> # Kirkwood
    Tested-by: Shawn Guo <shawn.guo@freescale.com>
    Tested-by: Tony Lindgren <tony@atomide.com> # OMAPs
    Tested-by: Gregory CLEMENT <gregory.clement@free-electrons.com> # Armada XP, 375, 385
    Acked-by: Sekhar Nori <nsekhar@ti.com> # DaVinci
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org> # kvm/hyp
    Acked-by: Haojian Zhuang <haojian.zhuang@gmail.com> # PXA3xx
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com> # Xen
    Tested-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de> # ARMv7M
    Tested-by: Simon Horman <horms+renesas@verge.net.au> # Shmobile
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 9f12ed1eea86..7d807cfd8ef5 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,3 +1,4 @@
+#include <asm/assembler.h>
 #include <asm/unwind.h>
 
 #if __LINUX_ARM_ARCH__ >= 6
@@ -70,7 +71,7 @@ UNWIND(	.fnstart	)
 	\instr	r2, r2, r3
 	str	r2, [r1, r0, lsl #2]
 	restore_irqs ip
-	mov	pc, lr
+	ret	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
 	.endm
@@ -98,7 +99,7 @@ UNWIND(	.fnstart	)
 	\store	r2, [r1]
 	moveq	r0, #0
 	restore_irqs ip
-	mov	pc, lr
+	ret	lr
 UNWIND(	.fnend		)
 ENDPROC(\name		)
 	.endm

commit c32ffce0f66e5d1d4856254516e24f5ef275cd00
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 21 17:01:48 2014 +0100

    ARM: 7984/1: prefetch: add prefetchw invocations for barriered atomics
    
    After a bunch of benchmarking on the interaction between dmb and pldw,
    it turns out that issuing the pldw *after* the dmb instruction can
    give modest performance gains (~3% atomic_add_return improvement on a
    dual A15).
    
    This patch adds prefetchw invocations to our barriered atomic operations
    including cmpxchg, test_and_xxx and futexes.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 52886b89706c..9f12ed1eea86 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -37,6 +37,11 @@ UNWIND(	.fnstart	)
 	add	r1, r1, r0, lsl #2	@ Get word offset
 	mov	r3, r2, lsl r3		@ create mask
 	smp_dmb
+#if __LINUX_ARM_ARCH__ >= 7 && defined(CONFIG_SMP)
+	.arch_extension	mp
+	ALT_SMP(W(pldw)	[r1])
+	ALT_UP(W(nop))
+#endif
 1:	ldrex	r2, [r1]
 	ands	r0, r2, r3		@ save old value of bit
 	\instr	r2, r2, r3		@ toggle bit

commit b7ec699405f55667caeb46d96229d75bf33a83ad
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 19 15:46:11 2013 +0100

    ARM: 7893/1: bitops: only emit .arch_extension mp if CONFIG_SMP
    
    Uwe reported a build failure when targetting a NOMMU platform with my
    recent prefetch changes:
    
      arch/arm/lib/changebit.S: Assembler messages:
      arch/arm/lib/changebit.S:15: Error: architectural extension `mp' is
                            not allowed for the current base architecture
    
    This is due to use of the .arch_extension mp directive immediately prior
    to an ALT_SMP(...) instruction. Whilst the ALT_SMP macro will expand to
    nothing if !CONFIG_SMP, gas will still choke on the directive.
    
    This patch fixes the issue by only emitting the sequence (including the
    directive) if CONFIG_SMP=y.
    
    Tested-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index e0c68d5bb7dc..52886b89706c 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -10,7 +10,7 @@ UNWIND(	.fnstart	)
 	and	r3, r0, #31		@ Get bit offset
 	mov	r0, r0, lsr #5
 	add	r1, r1, r0, lsl #2	@ Get word offset
-#if __LINUX_ARM_ARCH__ >= 7
+#if __LINUX_ARM_ARCH__ >= 7 && defined(CONFIG_SMP)
 	.arch_extension	mp
 	ALT_SMP(W(pldw)	[r1])
 	ALT_UP(W(nop))

commit d779c07dd72098a7416d907494f958213b7726f3
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jun 27 12:01:51 2013 +0100

    ARM: bitops: prefetch the destination word for write prior to strex
    
    The cost of changing a cacheline from shared to exclusive state can be
    significant, especially when this is triggered by an exclusive store,
    since it may result in having to retry the transaction.
    
    This patch prefixes our atomic bitops implementation with prefetchw,
    to try and grab the line in exclusive state from the start. The testop
    macro is left alone, since the barrier semantics limit the usefulness
    of prefetching data.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index d6408d1ee543..e0c68d5bb7dc 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -10,6 +10,11 @@ UNWIND(	.fnstart	)
 	and	r3, r0, #31		@ Get bit offset
 	mov	r0, r0, lsr #5
 	add	r1, r1, r0, lsl #2	@ Get word offset
+#if __LINUX_ARM_ARCH__ >= 7
+	.arch_extension	mp
+	ALT_SMP(W(pldw)	[r1])
+	ALT_UP(W(nop))
+#endif
 	mov	r3, r2, lsl r3
 1:	ldrex	r2, [r1]
 	\instr	r2, r2, r3

commit c36ef4b1762302a493c6cb754073bded084700e2
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Nov 23 11:28:25 2011 +0100

    ARM: 7171/1: unwind: add unwind directives to bitops assembly macros
    
    The bitops functions (e.g. _test_and_set_bit) on ARM do not have unwind
    annotations and therefore the kernel cannot backtrace out of them on a
    fatal error (for example, NULL pointer dereference).
    
    This patch annotates the bitops assembly macros with UNWIND annotations
    so that we can produce a meaningful backtrace on error. Callers of the
    macros are modified to pass their function name as a macro parameter,
    enforcing that the macros are used as standalone function implementations.
    
    Acked-by: Dave Martin <dave.martin@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 10d868a5a481..d6408d1ee543 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,5 +1,9 @@
+#include <asm/unwind.h>
+
 #if __LINUX_ARM_ARCH__ >= 6
-	.macro	bitop, instr
+	.macro	bitop, name, instr
+ENTRY(	\name		)
+UNWIND(	.fnstart	)
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
 	mov	r2, #1
@@ -13,9 +17,13 @@
 	cmp	r0, #0
 	bne	1b
 	bx	lr
+UNWIND(	.fnend		)
+ENDPROC(\name		)
 	.endm
 
-	.macro	testop, instr, store
+	.macro	testop, name, instr, store
+ENTRY(	\name		)
+UNWIND(	.fnstart	)
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
 	mov	r2, #1
@@ -34,9 +42,13 @@
 	cmp	r0, #0
 	movne	r0, #1
 2:	bx	lr
+UNWIND(	.fnend		)
+ENDPROC(\name		)
 	.endm
 #else
-	.macro	bitop, instr
+	.macro	bitop, name, instr
+ENTRY(	\name		)
+UNWIND(	.fnstart	)
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
 	and	r2, r0, #31
@@ -49,6 +61,8 @@
 	str	r2, [r1, r0, lsl #2]
 	restore_irqs ip
 	mov	pc, lr
+UNWIND(	.fnend		)
+ENDPROC(\name		)
 	.endm
 
 /**
@@ -59,7 +73,9 @@
  * Note: we can trivially conditionalise the store instruction
  * to avoid dirtying the data cache.
  */
-	.macro	testop, instr, store
+	.macro	testop, name, instr, store
+ENTRY(	\name		)
+UNWIND(	.fnstart	)
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
 	and	r3, r0, #31
@@ -73,5 +89,7 @@
 	moveq	r0, #0
 	restore_irqs ip
 	mov	pc, lr
+UNWIND(	.fnend		)
+ENDPROC(\name		)
 	.endm
 #endif

commit 3ba6e69ad887f8a814267ed36fd4bfbddf8855a9
Author: Dave Martin <dave.martin@linaro.org>
Date:   Tue Feb 8 12:09:52 2011 +0100

    ARM: 6653/1: bitops: Use BX instead of MOV PC,LR
    
    The kernel doesn't officially need to interwork, but using BX
    wherever appropriate will help educate people into good assembler
    coding habits.
    
    BX is appropriate here because this code is predicated on
    __LINUX_ARM_ARCH__ >= 6
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index a9d9d152a751..10d868a5a481 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -12,7 +12,7 @@
 	strex	r0, r2, [r1]
 	cmp	r0, #0
 	bne	1b
-	mov	pc, lr
+	bx	lr
 	.endm
 
 	.macro	testop, instr, store
@@ -33,7 +33,7 @@
 	smp_dmb
 	cmp	r0, #0
 	movne	r0, #1
-2:	mov	pc, lr
+2:	bx	lr
 	.endm
 #else
 	.macro	bitop, instr

commit 6323f0ccedf756dfe5f46549cec69a2d6d97937b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jan 16 18:02:17 2011 +0000

    ARM: bitops: switch set/clear/change bitops to use ldrex/strex
    
    Switch the set/clear/change bitops to use the word-based exclusive
    operations, which are only present in a wider range of ARM architectures
    than the byte-based exclusive operations.
    
    Tested record:
    - Nicolas Pitre: ext3,rw,le
    - Sourav Poddar: nfs,le
    - Will Deacon: ext3,rw,le
    - Tony Lindgren: ext3+nfs,le
    
    Reviewed-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Sourav Poddar <sourav.poddar@ti.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index bd00551fb797..a9d9d152a751 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,15 +1,15 @@
-
-#if __LINUX_ARM_ARCH__ >= 6 && defined(CONFIG_CPU_32v6K)
+#if __LINUX_ARM_ARCH__ >= 6
 	.macro	bitop, instr
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
 	mov	r2, #1
-	and	r3, r0, #7		@ Get bit offset
-	add	r1, r1, r0, lsr #3	@ Get byte offset
+	and	r3, r0, #31		@ Get bit offset
+	mov	r0, r0, lsr #5
+	add	r1, r1, r0, lsl #2	@ Get word offset
 	mov	r3, r2, lsl r3
-1:	ldrexb	r2, [r1]
+1:	ldrex	r2, [r1]
 	\instr	r2, r2, r3
-	strexb	r0, r2, [r1]
+	strex	r0, r2, [r1]
 	cmp	r0, #0
 	bne	1b
 	mov	pc, lr
@@ -18,15 +18,16 @@
 	.macro	testop, instr, store
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
-	and	r3, r0, #7		@ Get bit offset
 	mov	r2, #1
-	add	r1, r1, r0, lsr #3	@ Get byte offset
+	and	r3, r0, #31		@ Get bit offset
+	mov	r0, r0, lsr #5
+	add	r1, r1, r0, lsl #2	@ Get word offset
 	mov	r3, r2, lsl r3		@ create mask
 	smp_dmb
-1:	ldrexb	r2, [r1]
+1:	ldrex	r2, [r1]
 	ands	r0, r2, r3		@ save old value of bit
-	\instr	r2, r2, r3			@ toggle bit
-	strexb	ip, r2, [r1]
+	\instr	r2, r2, r3		@ toggle bit
+	strex	ip, r2, [r1]
 	cmp	ip, #0
 	bne	1b
 	smp_dmb
@@ -38,13 +39,14 @@
 	.macro	bitop, instr
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
-	and	r2, r0, #7
+	and	r2, r0, #31
+	mov	r0, r0, lsr #5
 	mov	r3, #1
 	mov	r3, r3, lsl r2
 	save_and_disable_irqs ip
-	ldrb	r2, [r1, r0, lsr #3]
+	ldr	r2, [r1, r0, lsl #2]
 	\instr	r2, r2, r3
-	strb	r2, [r1, r0, lsr #3]
+	str	r2, [r1, r0, lsl #2]
 	restore_irqs ip
 	mov	pc, lr
 	.endm
@@ -60,11 +62,11 @@
 	.macro	testop, instr, store
 	ands	ip, r1, #3
 	strneb	r1, [ip]		@ assert word-aligned
-	add	r1, r1, r0, lsr #3
-	and	r3, r0, #7
-	mov	r0, #1
+	and	r3, r0, #31
+	mov	r0, r0, lsr #5
 	save_and_disable_irqs ip
-	ldrb	r2, [r1]
+	ldr	r2, [r1, r0, lsl #2]!
+	mov	r0, #1
 	tst	r2, r0, lsl r3
 	\instr	r2, r2, r0, lsl r3
 	\store	r2, [r1]

commit a16ede35a2659170c855c5d267776666c0630f1f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jan 16 17:59:44 2011 +0000

    ARM: bitops: ensure set/clear/change bitops take a word-aligned pointer
    
    Add additional instructions to our assembly bitops functions to ensure
    that they only operate on word-aligned pointers.  This will be necessary
    when we switch these operations to use the word-based exclusive
    operations.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index d42252918bfb..bd00551fb797 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,6 +1,8 @@
 
 #if __LINUX_ARM_ARCH__ >= 6 && defined(CONFIG_CPU_32v6K)
 	.macro	bitop, instr
+	ands	ip, r1, #3
+	strneb	r1, [ip]		@ assert word-aligned
 	mov	r2, #1
 	and	r3, r0, #7		@ Get bit offset
 	add	r1, r1, r0, lsr #3	@ Get byte offset
@@ -14,6 +16,8 @@
 	.endm
 
 	.macro	testop, instr, store
+	ands	ip, r1, #3
+	strneb	r1, [ip]		@ assert word-aligned
 	and	r3, r0, #7		@ Get bit offset
 	mov	r2, #1
 	add	r1, r1, r0, lsr #3	@ Get byte offset
@@ -32,6 +36,8 @@
 	.endm
 #else
 	.macro	bitop, instr
+	ands	ip, r1, #3
+	strneb	r1, [ip]		@ assert word-aligned
 	and	r2, r0, #7
 	mov	r3, #1
 	mov	r3, r3, lsl r2
@@ -52,6 +58,8 @@
  * to avoid dirtying the data cache.
  */
 	.macro	testop, instr, store
+	ands	ip, r1, #3
+	strneb	r1, [ip]		@ assert word-aligned
 	add	r1, r1, r0, lsr #3
 	and	r3, r0, #7
 	mov	r0, #1

commit 0d928b0b616d1c5c5fe76019a87cba171ca91633
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu Aug 13 20:38:17 2009 +0200

    Complete irq tracing support for ARM
    
    Before this patch enabling and disabling irqs in assembler code and by
    the hardware wasn't tracked completly.
    
    I had to transpose two instructions in arch/arm/lib/bitops.h because
    restore_irqs doesn't preserve the flags with CONFIG_TRACE_IRQFLAGS=y
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index c7f2627385e7..d42252918bfb 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -60,8 +60,8 @@
 	tst	r2, r0, lsl r3
 	\instr	r2, r2, r0, lsl r3
 	\store	r2, [r1]
-	restore_irqs ip
 	moveq	r0, #0
+	restore_irqs ip
 	mov	pc, lr
 	.endm
 #endif

commit bac4e960b5ce2453d862beaf20e59aa68af3b43a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon May 25 20:58:00 2009 +0100

    [ARM] barriers: improve xchg, bitops and atomic SMP barriers
    
    Mathieu Desnoyers pointed out that the ARM barriers were lacking:
    
    - cmpxchg, xchg and atomic add return need memory barriers on
      architectures which can reorder the relative order in which memory
      read/writes can be seen between CPUs, which seems to include recent
      ARM architectures. Those barriers are currently missing on ARM.
    
    - test_and_xxx_bit were missing SMP barriers.
    
    So put these barriers in.  Provide separate atomic_add/atomic_sub
    operations which do not require barriers.
    
    Reported-Reviewed-and-Acked-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 2e787d40d599..c7f2627385e7 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -18,12 +18,14 @@
 	mov	r2, #1
 	add	r1, r1, r0, lsr #3	@ Get byte offset
 	mov	r3, r2, lsl r3		@ create mask
+	smp_dmb
 1:	ldrexb	r2, [r1]
 	ands	r0, r2, r3		@ save old value of bit
 	\instr	r2, r2, r3			@ toggle bit
 	strexb	ip, r2, [r1]
 	cmp	ip, #0
 	bne	1b
+	smp_dmb
 	cmp	r0, #0
 	movne	r0, #1
 2:	mov	pc, lr

commit 6cbdc8c5357276307a77deeada3f04626ff17da6
Author: Simon Arlott <simon@fire.lp0.eu>
Date:   Fri May 11 20:40:30 2007 +0100

    [ARM] spelling fixes
    
    Spelling fixes in arch/arm/.
    
    Signed-off-by: Simon Arlott <simon@fire.lp0.eu>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 542251021744..2e787d40d599 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -47,7 +47,7 @@
  * @store: store instruction
  *
  * Note: we can trivially conditionalise the store instruction
- * to avoid dirting the data cache.
+ * to avoid dirtying the data cache.
  */
 	.macro	testop, instr, store
 	add	r1, r1, r0, lsr #3

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index b8c14e936697..542251021744 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,4 +1,3 @@
-#include <linux/config.h>
 
 #if __LINUX_ARM_ARCH__ >= 6 && defined(CONFIG_CPU_32v6K)
 	.macro	bitop, instr

commit 59d1ff3bfb56d9b8cf3ec864857e6a4dfd9d2dba
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Nov 9 15:04:22 2005 +0000

    [ARM] Clean up save_and_disable_irqs macro and allow use of ARMv6 CPSID
    
    save_and_disable_irqs does not need to use mov + msr (which was
    introduced to work around a documentation bug which was propagated
    into binutils.)  Use msr with an immediate constant, and if we're
    building for ARMv6 or later, use the new CPSID instruction.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index f35d91fbe117..b8c14e936697 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -34,7 +34,7 @@
 	and	r2, r0, #7
 	mov	r3, #1
 	mov	r3, r3, lsl r2
-	save_and_disable_irqs ip, r2
+	save_and_disable_irqs ip
 	ldrb	r2, [r1, r0, lsr #3]
 	\instr	r2, r2, r3
 	strb	r2, [r1, r0, lsr #3]
@@ -54,7 +54,7 @@
 	add	r1, r1, r0, lsr #3
 	and	r3, r0, #7
 	mov	r0, #1
-	save_and_disable_irqs ip, r2
+	save_and_disable_irqs ip
 	ldrb	r2, [r1]
 	tst	r2, r0, lsl r3
 	\instr	r2, r2, r0, lsl r3

commit 4a5f79e7e65d24d2fa9eb6e6208672571704d337
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Nov 3 15:48:21 2005 +0000

    [ARM SMP] Add configuration option for ARMv6K processors
    
    The 'K' extension adds several new instructions to the ARMv6 ISA
    which are primerily useful for SMP.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 64a988c1ad44..f35d91fbe117 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,6 +1,6 @@
 #include <linux/config.h>
 
-#if __LINUX_ARM_ARCH__ >= 6 && defined(CONFIG_CPU_MPCORE)
+#if __LINUX_ARM_ARCH__ >= 6 && defined(CONFIG_CPU_32v6K)
 	.macro	bitop, instr
 	mov	r2, #1
 	and	r3, r0, #7		@ Get bit offset

commit 3c4ee4e2520775896efc6ab850c4c27971fbcf2a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Aug 10 14:41:45 2005 +0100

    [ARM SMP] Only enable V6K instructions on V6 MP core CPUs
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 2036ff15bda9..64a988c1ad44 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,4 +1,6 @@
-#if __LINUX_ARM_ARCH__ >= 6
+#include <linux/config.h>
+
+#if __LINUX_ARM_ARCH__ >= 6 && defined(CONFIG_CPU_MPCORE)
 	.macro	bitop, instr
 	mov	r2, #1
 	and	r3, r0, #7		@ Get bit offset

commit e7ec02938dbe8ca35b750f29eaa4b12de0b52754
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Jul 28 20:36:26 2005 +0100

    [ARM SMP] Fix another ARMv6 bitop problem
    
    We sometimes forgot to check whether the exclusive store succeeded.
    Ensure that we always check.  Also ensure that we always use the
    out of line versions, since the inline versions are not SMP safe.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 5382a3023602..2036ff15bda9 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -7,7 +7,7 @@
 1:	ldrexb	r2, [r1]
 	\instr	r2, r2, r3
 	strexb	r0, r2, [r1]
-	cmpne	r0, #0
+	cmp	r0, #0
 	bne	1b
 	mov	pc, lr
 	.endm

commit 614d73edae68836f7659ee8efec90878e6215fb1
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Jul 27 23:00:05 2005 +0100

    [ARM SMP] Fix data corruption in test_* bitops
    
    If we found that the bit was already in the desired state, we
    would skip performing the operation, and write random data back.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 6976e60e47cb..5382a3023602 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -19,9 +19,9 @@
 	mov	r3, r2, lsl r3		@ create mask
 1:	ldrexb	r2, [r1]
 	ands	r0, r2, r3		@ save old value of bit
-	\instr	ip, r2, r3			@ toggle bit
-	strexb	r2, ip, [r1]
-	cmp	r2, #0
+	\instr	r2, r2, r3			@ toggle bit
+	strexb	ip, r2, [r1]
+	cmp	ip, #0
 	bne	1b
 	cmp	r0, #0
 	movne	r0, #1

commit 54ea06f6afe85aaf419e51343d4e4b5599197113
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Jul 16 15:21:51 2005 +0100

    [PATCH] ARM: Convert bitops to use ARMv6 ldrex/strex instructions
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
index 4a83ab6cd565..6976e60e47cb 100644
--- a/arch/arm/lib/bitops.h
+++ b/arch/arm/lib/bitops.h
@@ -1,3 +1,33 @@
+#if __LINUX_ARM_ARCH__ >= 6
+	.macro	bitop, instr
+	mov	r2, #1
+	and	r3, r0, #7		@ Get bit offset
+	add	r1, r1, r0, lsr #3	@ Get byte offset
+	mov	r3, r2, lsl r3
+1:	ldrexb	r2, [r1]
+	\instr	r2, r2, r3
+	strexb	r0, r2, [r1]
+	cmpne	r0, #0
+	bne	1b
+	mov	pc, lr
+	.endm
+
+	.macro	testop, instr, store
+	and	r3, r0, #7		@ Get bit offset
+	mov	r2, #1
+	add	r1, r1, r0, lsr #3	@ Get byte offset
+	mov	r3, r2, lsl r3		@ create mask
+1:	ldrexb	r2, [r1]
+	ands	r0, r2, r3		@ save old value of bit
+	\instr	ip, r2, r3			@ toggle bit
+	strexb	r2, ip, [r1]
+	cmp	r2, #0
+	bne	1b
+	cmp	r0, #0
+	movne	r0, #1
+2:	mov	pc, lr
+	.endm
+#else
 	.macro	bitop, instr
 	and	r2, r0, #7
 	mov	r3, #1
@@ -31,3 +61,4 @@
 	moveq	r0, #0
 	mov	pc, lr
 	.endm
+#endif

commit 7a55fd0bb31eb369149b89fdf9e0c7bc73486ee1
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Apr 18 22:50:01 2005 +0100

    [PATCH] ARM: Add missing new file for bitops patch
    
    Signed-off-by: Russell King <rmk@arm.linux.org.uk>

diff --git a/arch/arm/lib/bitops.h b/arch/arm/lib/bitops.h
new file mode 100644
index 000000000000..4a83ab6cd565
--- /dev/null
+++ b/arch/arm/lib/bitops.h
@@ -0,0 +1,33 @@
+	.macro	bitop, instr
+	and	r2, r0, #7
+	mov	r3, #1
+	mov	r3, r3, lsl r2
+	save_and_disable_irqs ip, r2
+	ldrb	r2, [r1, r0, lsr #3]
+	\instr	r2, r2, r3
+	strb	r2, [r1, r0, lsr #3]
+	restore_irqs ip
+	mov	pc, lr
+	.endm
+
+/**
+ * testop - implement a test_and_xxx_bit operation.
+ * @instr: operational instruction
+ * @store: store instruction
+ *
+ * Note: we can trivially conditionalise the store instruction
+ * to avoid dirting the data cache.
+ */
+	.macro	testop, instr, store
+	add	r1, r1, r0, lsr #3
+	and	r3, r0, #7
+	mov	r0, #1
+	save_and_disable_irqs ip, r2
+	ldrb	r2, [r1]
+	tst	r2, r0, lsl r3
+	\instr	r2, r2, r0, lsl r3
+	\store	r2, [r1]
+	restore_irqs ip
+	moveq	r0, #0
+	mov	pc, lr
+	.endm
