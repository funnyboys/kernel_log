commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 9af0701f7094..d4908b3736d8 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -1,11 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/arch/arm/kernel/smp_tlb.c
  *
  *  Copyright (C) 2002 ARM Limited, All Rights Reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include <linux/preempt.h>
 #include <linux/smp.h>

commit 90f92c631b210c1e97080b53a9d863783281a932
Author: Linus Walleij <linus.walleij@linaro.org>
Date:   Tue Sep 13 12:31:17 2016 +0100

    ARM: 8613/1: Fix the uaccess crash on PB11MPCore
    
    The following patch was sketched by Russell in response to my
    crashes on the PB11MPCore after the patch for software-based
    priviledged no access support for ARMv8.1. See this thread:
    http://marc.info/?l=linux-arm-kernel&m=144051749807214&w=2
    
    I am unsure what is going on, I suspect everyone involved in
    the discussion is. I just want to repost this to get the
    discussion restarted, as I still have to apply this patch
    with every kernel iteration to get my PB11MPCore Realview
    running.
    
    Testing by Neil Armstrong on the Oxnas NAS has revealed that
    this bug exist also on that widely deployed hardware, so
    we are probably currently regressing all ARM11MPCore systems.
    
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Fixes: a5e090acbf54 ("ARM: software-based priviledged-no-access support")
    Tested-by: Neil Armstrong <narmstrong@baylibre.com>
    Signed-off-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 22313cb53362..9af0701f7094 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -9,6 +9,7 @@
  */
 #include <linux/preempt.h>
 #include <linux/smp.h>
+#include <linux/uaccess.h>
 
 #include <asm/smp_plat.h>
 #include <asm/tlbflush.h>
@@ -40,8 +41,11 @@ static inline void ipi_flush_tlb_mm(void *arg)
 static inline void ipi_flush_tlb_page(void *arg)
 {
 	struct tlb_args *ta = (struct tlb_args *)arg;
+	unsigned int __ua_flags = uaccess_save_and_enable();
 
 	local_flush_tlb_page(ta->ta_vma, ta->ta_start);
+
+	uaccess_restore(__ua_flags);
 }
 
 static inline void ipi_flush_tlb_kernel_page(void *arg)
@@ -54,8 +58,11 @@ static inline void ipi_flush_tlb_kernel_page(void *arg)
 static inline void ipi_flush_tlb_range(void *arg)
 {
 	struct tlb_args *ta = (struct tlb_args *)arg;
+	unsigned int __ua_flags = uaccess_save_and_enable();
 
 	local_flush_tlb_range(ta->ta_vma, ta->ta_start, ta->ta_end);
+
+	uaccess_restore(__ua_flags);
 }
 
 static inline void ipi_flush_tlb_kernel_range(void *arg)

commit f6492164ecb19d43687875fdb456845acc504590
Author: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
Date:   Tue Jun 7 16:19:16 2016 +0100

    ARM: 8577/1: Fix Cortex-A15 798181 errata initialization
    
    Current errata initialization doesn't take properly revision and REVIDR
    into account. Depending on the core revision, revidr bits should not be
    taken into account. Errata misleadingly declares r3p3 to be error-free,
    but this is not the case. Include rp3p3 in errata initialization.
    
    Here are possible fixes defined in revidr register for r2 and r3 [1,2]:
    
    r0p0-r2p1: No fixes applied
    
    r2p2,r2p3:
    
    REVIDR[4]: 798181 Moving a virtual page that is being accessed by
        an active process can lead to unexpected behavior
    REVIDR[9]: Not defined
    
    r2p4,r3p0,r3p1,r3p2:
    
    REVIDR[4]: 798181 Moving a virtual page that is being accessed by
       an active process can lead to unexpected behavior
    REVIDR[9]: 798181 Moving a virtual page that is being accessed by
       an active process can lead to unexpected behavior
       - This is an update to a previously released ECO.
    
    r3p3:
    
    REVIDR[4]: Reserved
    REVIDR[9]: 798181 Moving a virtual page that is being accessed by
       an active process can lead to unexpected behavior
       - This is an update to a previously released ECO.
    
    And here is proposed handling from the same document:
    
    * In r3p2 and earlier versions with REVIDR[4]= 0,the full workaround is
      required.
    * In r3p2 and earlier versions with REVIDR[4]=1, REVIDR[9]=0, only the
      portion of the workaround up to the end of step 6 is required.
    * In r3p2 and earlier versions with REVIDR[4]=1, REVIDR[9]=1, no
      workaround is required.
    * In r3p3, if REVIDR[9]=0, only the portion of the workaround up
      to the end of step 6 is required.
    * In r3p3, if REVIDR[9]=1, no workaround is required.
    
    These imply following:
    
    REVIDR[9] set -> No WA
    REVIDR[4] set, REVIDR[9] cleared -> Partial WA
    Both cleared -> Full WA
    
    Where certain bits should and should not be taken into account
    depending on whether they are defined for the revision.
    
    Although not explicitly mentioned in the errata note, REVIDR[9] set,
    with REVIDR[4] cleared is valid combination which requires no WA. This
    is confirmed by ARM support and errata will be updated.
    
    [1] ARM CortexTM-A15 MPCore - NEON
        Product revision r3
        Software Developers Errata Notice
        ARM-EPM-028093 v20.0 Released
    
    [2] ARM CortexTM-A15 MPCore - NEON
        Product Revision r2
        Software Developers Errata Notice
        ARM-EPM-028090 v19.3 Released
    
    Signed-off-by: Matija Glavinic Pecotic <matija.glavinic-pecotic.ext@nokia.com>
    Reviewed-by: Alexander Sverdlin <alexander.sverdlin@nokia.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 2e72be4f623e..22313cb53362 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -93,17 +93,53 @@ void erratum_a15_798181_init(void)
 	unsigned int revidr = read_cpuid(CPUID_REVIDR);
 
 	/* Brahma-B15 r0p0..r0p2 affected
-	 * Cortex-A15 r0p0..r3p2 w/o ECO fix affected */
-	if ((midr & 0xff0ffff0) == 0x420f00f0 && midr <= 0x420f00f2)
+	 * Cortex-A15 r0p0..r3p3 w/o ECO fix affected
+	 * Fixes applied to A15 with respect to the revision and revidr are:
+	 *
+	 * r0p0-r2p1: No fixes applied
+	 * r2p2,r2p3:
+	 *	REVIDR[4]: 798181 Moving a virtual page that is being accessed
+	 *		   by an active process can lead to unexpected behavior
+	 *	REVIDR[9]: Not defined
+	 * r2p4,r3p0,r3p1,r3p2:
+	 *	REVIDR[4]: 798181 Moving a virtual page that is being accessed
+	 *		   by an active process can lead to unexpected behavior
+	 *	REVIDR[9]: 798181 Moving a virtual page that is being accessed
+	 *		   by an active process can lead to unexpected behavior
+	 *		   - This is an update to a previously released ECO.
+	 * r3p3:
+	 *	REVIDR[4]: Reserved
+	 *	REVIDR[9]: 798181 Moving a virtual page that is being accessed
+	 *		   by an active process can lead to unexpected behavior
+	 *		   - This is an update to a previously released ECO.
+	 *
+	 * Handling:
+	 *	REVIDR[9] set -> No WA
+	 *	REVIDR[4] set, REVIDR[9] cleared -> Partial WA
+	 *	Both cleared -> Full WA
+	 */
+	if ((midr & 0xff0ffff0) == 0x420f00f0 && midr <= 0x420f00f2) {
 		erratum_a15_798181_handler = erratum_a15_798181_broadcast;
-	else if ((midr & 0xff0ffff0) == 0x410fc0f0 && midr <= 0x413fc0f2 &&
-		 (revidr & 0x210) != 0x210) {
+	} else if ((midr & 0xff0ffff0) == 0x410fc0f0 && midr < 0x412fc0f2) {
+		erratum_a15_798181_handler = erratum_a15_798181_broadcast;
+	} else if ((midr & 0xff0ffff0) == 0x410fc0f0 && midr < 0x412fc0f4) {
 		if (revidr & 0x10)
 			erratum_a15_798181_handler =
 				erratum_a15_798181_partial;
 		else
 			erratum_a15_798181_handler =
 				erratum_a15_798181_broadcast;
+	} else if ((midr & 0xff0ffff0) == 0x410fc0f0 && midr < 0x413fc0f3) {
+		if ((revidr & 0x210) == 0)
+			erratum_a15_798181_handler =
+				erratum_a15_798181_broadcast;
+		else if (revidr & 0x10)
+			erratum_a15_798181_handler =
+				erratum_a15_798181_partial;
+	} else if ((midr & 0xff0ffff0) == 0x410fc0f0 && midr < 0x414fc0f0) {
+		if ((revidr & 0x200) == 0)
+			erratum_a15_798181_handler =
+				erratum_a15_798181_partial;
 	}
 }
 #endif

commit 04fcab32d3fa1d3f6afe97e0ab431c5572e07a2c
Author: Gregory Fong <gregory.0xf0@gmail.com>
Date:   Wed Jul 23 00:34:23 2014 +0100

    ARM: 8111/1: Enable erratum 798181 for Broadcom Brahma-B15
    
    Broadcom Brahma-B15 (r0p0..r0p2) is also affected by Cortex-A15
    erratum 798181, so enable the workaround for Brahma-B15.
    
    Signed-off-by: Gregory Fong <gregory.0xf0@gmail.com>
    Acked-by: Marc Carino <marc.ceeeee@gmail.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Brian Norris <computersforpeace@gmail.com>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 95d063620b76..2e72be4f623e 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -92,15 +92,19 @@ void erratum_a15_798181_init(void)
 	unsigned int midr = read_cpuid_id();
 	unsigned int revidr = read_cpuid(CPUID_REVIDR);
 
-	/* Cortex-A15 r0p0..r3p2 w/o ECO fix affected */
-	if ((midr & 0xff0ffff0) != 0x410fc0f0 || midr > 0x413fc0f2 ||
-	    (revidr & 0x210) == 0x210) {
-		return;
-	}
-	if (revidr & 0x10)
-		erratum_a15_798181_handler = erratum_a15_798181_partial;
-	else
+	/* Brahma-B15 r0p0..r0p2 affected
+	 * Cortex-A15 r0p0..r3p2 w/o ECO fix affected */
+	if ((midr & 0xff0ffff0) == 0x420f00f0 && midr <= 0x420f00f2)
 		erratum_a15_798181_handler = erratum_a15_798181_broadcast;
+	else if ((midr & 0xff0ffff0) == 0x410fc0f0 && midr <= 0x413fc0f2 &&
+		 (revidr & 0x210) != 0x210) {
+		if (revidr & 0x10)
+			erratum_a15_798181_handler =
+				erratum_a15_798181_partial;
+		else
+			erratum_a15_798181_handler =
+				erratum_a15_798181_broadcast;
+	}
 }
 #endif
 

commit 92871b94a5f9892e324c31960678387922c75049
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Wed Oct 9 17:26:44 2013 +0100

    ARM: 7855/1: Add check for Cortex-A15 errata 798181 ECO
    
    The work-around for A15 errata 798181 is not needed if appropriate ECO
    fixes have been applied to r3p2 and earlier core revisions. This can be
    checked by reading REVIDR register bits 4 and 9. If only bit 4 is set,
    then the IPI broadcast can be skipped.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 83ccca303df8..95d063620b76 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -70,6 +70,40 @@ static inline void ipi_flush_bp_all(void *ignored)
 	local_flush_bp_all();
 }
 
+#ifdef CONFIG_ARM_ERRATA_798181
+bool (*erratum_a15_798181_handler)(void);
+
+static bool erratum_a15_798181_partial(void)
+{
+	asm("mcr p15, 0, %0, c8, c3, 1" : : "r" (0));
+	dsb(ish);
+	return false;
+}
+
+static bool erratum_a15_798181_broadcast(void)
+{
+	asm("mcr p15, 0, %0, c8, c3, 1" : : "r" (0));
+	dsb(ish);
+	return true;
+}
+
+void erratum_a15_798181_init(void)
+{
+	unsigned int midr = read_cpuid_id();
+	unsigned int revidr = read_cpuid(CPUID_REVIDR);
+
+	/* Cortex-A15 r0p0..r3p2 w/o ECO fix affected */
+	if ((midr & 0xff0ffff0) != 0x410fc0f0 || midr > 0x413fc0f2 ||
+	    (revidr & 0x210) == 0x210) {
+		return;
+	}
+	if (revidr & 0x10)
+		erratum_a15_798181_handler = erratum_a15_798181_partial;
+	else
+		erratum_a15_798181_handler = erratum_a15_798181_broadcast;
+}
+#endif
+
 static void ipi_flush_tlb_a15_erratum(void *arg)
 {
 	dmb();
@@ -80,7 +114,6 @@ static void broadcast_tlb_a15_erratum(void)
 	if (!erratum_a15_798181())
 		return;
 
-	dummy_flush_tlb_a15_erratum();
 	smp_call_function(ipi_flush_tlb_a15_erratum, NULL, 1);
 }
 
@@ -92,7 +125,6 @@ static void broadcast_tlb_mm_a15_erratum(struct mm_struct *mm)
 	if (!erratum_a15_798181())
 		return;
 
-	dummy_flush_tlb_a15_erratum();
 	this_cpu = get_cpu();
 	a15_erratum_get_cpumask(this_cpu, mm, &mask);
 	smp_call_function_many(&mask, ipi_flush_tlb_a15_erratum, NULL, 1);

commit 2c813980c6113ac2c407fbed99f53242088c3038
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 18 22:07:47 2013 +0000

    ARM: tlb: don't perform inner-shareable invalidation for local BP ops
    
    Now that the ASID allocator doesn't require inner-shareable maintenance,
    we can convert the local_bp_flush_all function to perform only
    non-shareable flushing, in a similar manner to the TLB invalidation
    routines.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 5883b8ae77c8..83ccca303df8 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -173,5 +173,5 @@ void flush_bp_all(void)
 	if (tlb_ops_need_broadcast())
 		on_each_cpu(ipi_flush_bp_all, NULL, 1);
 	else
-		local_flush_bp_all();
+		__flush_bp_all();
 }

commit f0915781bd5edf78b1154e61efe962dc15872d09
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Feb 11 13:47:48 2013 +0000

    ARM: tlb: don't perform inner-shareable invalidation for local TLB ops
    
    Inner-shareable TLB invalidation is typically more expensive than local
    (non-shareable) invalidation, so performing the broadcasting for
    local_flush_tlb_* operations is a waste of cycles and needlessly
    clobbers entries in the TLBs of other CPUs.
    
    This patch introduces __flush_tlb_* versions for many of the TLB
    invalidation functions, which only respect inner-shareable variants of
    the invalidation instructions when presented with the TLB_V7_UIS_FULL
    flag. The local version is also inlined to prevent SMP_ON_UP kernels
    from missing flushes, where the __flush variant would be called with
    the UP flags.
    
    This gains us around 0.5% in hackbench scores for a dual-core A15, but I
    would expect this to improve as more cores (and clusters) are added to
    the equation.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Albin Tonnerre <Albin.Tonnerre@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index c2edfff573c2..5883b8ae77c8 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -104,7 +104,7 @@ void flush_tlb_all(void)
 	if (tlb_ops_need_broadcast())
 		on_each_cpu(ipi_flush_tlb_all, NULL, 1);
 	else
-		local_flush_tlb_all();
+		__flush_tlb_all();
 	broadcast_tlb_a15_erratum();
 }
 
@@ -113,7 +113,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 	if (tlb_ops_need_broadcast())
 		on_each_cpu_mask(mm_cpumask(mm), ipi_flush_tlb_mm, mm, 1);
 	else
-		local_flush_tlb_mm(mm);
+		__flush_tlb_mm(mm);
 	broadcast_tlb_mm_a15_erratum(mm);
 }
 
@@ -126,7 +126,7 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 		on_each_cpu_mask(mm_cpumask(vma->vm_mm), ipi_flush_tlb_page,
 					&ta, 1);
 	} else
-		local_flush_tlb_page(vma, uaddr);
+		__flush_tlb_page(vma, uaddr);
 	broadcast_tlb_mm_a15_erratum(vma->vm_mm);
 }
 
@@ -137,7 +137,7 @@ void flush_tlb_kernel_page(unsigned long kaddr)
 		ta.ta_start = kaddr;
 		on_each_cpu(ipi_flush_tlb_kernel_page, &ta, 1);
 	} else
-		local_flush_tlb_kernel_page(kaddr);
+		__flush_tlb_kernel_page(kaddr);
 	broadcast_tlb_a15_erratum();
 }
 

commit 1f49856bb029779d8f1b63517a3a3b34ffe672c7
Author: Fabio Estevam <festevam@gmail.com>
Date:   Tue Jul 23 15:13:06 2013 +0100

    ARM: 7789/1: Do not run dummy_flush_tlb_a15_erratum() on non-Cortex-A15
    
    Commit 93dc688 (ARM: 7684/1: errata: Workaround for Cortex-A15 erratum 798181 (TLBI/DSB operations)) causes the following undefined instruction error on a mx53 (Cortex-A8):
    
    Internal error: Oops - undefined instruction: 0 [#1] SMP ARM
    CPU: 0 PID: 275 Comm: modprobe Not tainted 3.11.0-rc2-next-20130722-00009-g9b0f371 #881
    task: df46cc00 ti: df48e000 task.ti: df48e000
    PC is at check_and_switch_context+0x17c/0x4d0
    LR is at check_and_switch_context+0xdc/0x4d0
    
    This problem happens because check_and_switch_context() calls dummy_flush_tlb_a15_erratum() without checking if we are really running on a Cortex-A15 or not.
    
    To avoid this issue, only call dummy_flush_tlb_a15_erratum() inside
    check_and_switch_context() if erratum_a15_798181() returns true, which means that we are really running on a Cortex-A15.
    
    Signed-off-by: Fabio Estevam <fabio.estevam@freescale.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Roger Quadros <rogerq@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index a98b62dca2fa..c2edfff573c2 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -70,23 +70,6 @@ static inline void ipi_flush_bp_all(void *ignored)
 	local_flush_bp_all();
 }
 
-#ifdef CONFIG_ARM_ERRATA_798181
-static int erratum_a15_798181(void)
-{
-	unsigned int midr = read_cpuid_id();
-
-	/* Cortex-A15 r0p0..r3p2 affected */
-	if ((midr & 0xff0ffff0) != 0x410fc0f0 || midr > 0x413fc0f2)
-		return 0;
-	return 1;
-}
-#else
-static int erratum_a15_798181(void)
-{
-	return 0;
-}
-#endif
-
 static void ipi_flush_tlb_a15_erratum(void *arg)
 {
 	dmb();

commit 0d0752bca1f9a91fb646647aa4abbb21156f316c
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Fri Jun 21 12:07:27 2013 +0100

    ARM: 7769/1: Cortex-A15: fix erratum 798181 implementation
    
    Looking into the active_asids array is not enough, as we also need
    to look into the reserved_asids array (they both represent processes
    that are currently running).
    
    Also, not holding the ASID allocator lock is racy, as another CPU
    could schedule that process and trigger a rollover, making the erratum
    workaround miss an IPI.
    
    Exposing this outside of context.c is a little ugly on the side, so
    let's define a new entry point that the erratum workaround can call
    to obtain the cpumask.
    
    Cc: <stable@vger.kernel.org> # 3.9
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 9a52a07aa40e..a98b62dca2fa 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -103,7 +103,7 @@ static void broadcast_tlb_a15_erratum(void)
 
 static void broadcast_tlb_mm_a15_erratum(struct mm_struct *mm)
 {
-	int cpu, this_cpu;
+	int this_cpu;
 	cpumask_t mask = { CPU_BITS_NONE };
 
 	if (!erratum_a15_798181())
@@ -111,21 +111,7 @@ static void broadcast_tlb_mm_a15_erratum(struct mm_struct *mm)
 
 	dummy_flush_tlb_a15_erratum();
 	this_cpu = get_cpu();
-	for_each_online_cpu(cpu) {
-		if (cpu == this_cpu)
-			continue;
-		/*
-		 * We only need to send an IPI if the other CPUs are running
-		 * the same ASID as the one being invalidated. There is no
-		 * need for locking around the active_asids check since the
-		 * switch_mm() function has at least one dmb() (as required by
-		 * this workaround) in case a context switch happens on
-		 * another CPU after the condition below.
-		 */
-		if (atomic64_read(&mm->context.id) ==
-		    atomic64_read(&per_cpu(active_asids, cpu)))
-			cpumask_set_cpu(cpu, &mask);
-	}
+	a15_erratum_get_cpumask(this_cpu, mm, &mask);
 	smp_call_function_many(&mask, ipi_flush_tlb_a15_erratum, NULL, 1);
 	put_cpu();
 }

commit 3eb0be304273054fd2ae34367f96127d082e310c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 24 14:41:37 2013 +0100

    ARM: 7703/1: Disable preemption in broadcast_tlb*_a15_erratum()
    
    Commit 93dc688 (ARM: 7684/1: errata: Workaround for Cortex-A15 erratum
    798181 (TLBI/DSB operations)) introduces calls to smp_processor_id() and
    smp_call_function_many() with preemption enabled. This patch disables
    preemption and also optimises the smp_processor_id() call in
    broadcast_tlb_mm_a15_erratum(). The broadcast_tlb_a15_erratum() function
    is changed to use smp_call_function() which disables preemption.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Geoff Levand <geoff@infradead.org>
    Reported-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index e82e1d248772..9a52a07aa40e 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -98,21 +98,21 @@ static void broadcast_tlb_a15_erratum(void)
 		return;
 
 	dummy_flush_tlb_a15_erratum();
-	smp_call_function_many(cpu_online_mask, ipi_flush_tlb_a15_erratum,
-			       NULL, 1);
+	smp_call_function(ipi_flush_tlb_a15_erratum, NULL, 1);
 }
 
 static void broadcast_tlb_mm_a15_erratum(struct mm_struct *mm)
 {
-	int cpu;
+	int cpu, this_cpu;
 	cpumask_t mask = { CPU_BITS_NONE };
 
 	if (!erratum_a15_798181())
 		return;
 
 	dummy_flush_tlb_a15_erratum();
+	this_cpu = get_cpu();
 	for_each_online_cpu(cpu) {
-		if (cpu == smp_processor_id())
+		if (cpu == this_cpu)
 			continue;
 		/*
 		 * We only need to send an IPI if the other CPUs are running
@@ -127,6 +127,7 @@ static void broadcast_tlb_mm_a15_erratum(struct mm_struct *mm)
 			cpumask_set_cpu(cpu, &mask);
 	}
 	smp_call_function_many(&mask, ipi_flush_tlb_a15_erratum, NULL, 1);
+	put_cpu();
 }
 
 void flush_tlb_all(void)

commit 93dc68876b608da041fe40ed39424b0fcd5aa2fb
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Mar 26 23:35:04 2013 +0100

    ARM: 7684/1: errata: Workaround for Cortex-A15 erratum 798181 (TLBI/DSB operations)
    
    On Cortex-A15 (r0p0..r3p2) the TLBI/DSB are not adequately shooting down
    all use of the old entries. This patch implements the erratum workaround
    which consists of:
    
    1. Dummy TLBIMVAIS and DSB on the CPU doing the TLBI operation.
    2. Send IPI to the CPUs that are running the same mm (and ASID) as the
       one being invalidated (or all the online CPUs for global pages).
    3. CPU receiving the IPI executes a DMB and CLREX (part of the exception
       return code already).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index bd0300531399..e82e1d248772 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -12,6 +12,7 @@
 
 #include <asm/smp_plat.h>
 #include <asm/tlbflush.h>
+#include <asm/mmu_context.h>
 
 /**********************************************************************/
 
@@ -69,12 +70,72 @@ static inline void ipi_flush_bp_all(void *ignored)
 	local_flush_bp_all();
 }
 
+#ifdef CONFIG_ARM_ERRATA_798181
+static int erratum_a15_798181(void)
+{
+	unsigned int midr = read_cpuid_id();
+
+	/* Cortex-A15 r0p0..r3p2 affected */
+	if ((midr & 0xff0ffff0) != 0x410fc0f0 || midr > 0x413fc0f2)
+		return 0;
+	return 1;
+}
+#else
+static int erratum_a15_798181(void)
+{
+	return 0;
+}
+#endif
+
+static void ipi_flush_tlb_a15_erratum(void *arg)
+{
+	dmb();
+}
+
+static void broadcast_tlb_a15_erratum(void)
+{
+	if (!erratum_a15_798181())
+		return;
+
+	dummy_flush_tlb_a15_erratum();
+	smp_call_function_many(cpu_online_mask, ipi_flush_tlb_a15_erratum,
+			       NULL, 1);
+}
+
+static void broadcast_tlb_mm_a15_erratum(struct mm_struct *mm)
+{
+	int cpu;
+	cpumask_t mask = { CPU_BITS_NONE };
+
+	if (!erratum_a15_798181())
+		return;
+
+	dummy_flush_tlb_a15_erratum();
+	for_each_online_cpu(cpu) {
+		if (cpu == smp_processor_id())
+			continue;
+		/*
+		 * We only need to send an IPI if the other CPUs are running
+		 * the same ASID as the one being invalidated. There is no
+		 * need for locking around the active_asids check since the
+		 * switch_mm() function has at least one dmb() (as required by
+		 * this workaround) in case a context switch happens on
+		 * another CPU after the condition below.
+		 */
+		if (atomic64_read(&mm->context.id) ==
+		    atomic64_read(&per_cpu(active_asids, cpu)))
+			cpumask_set_cpu(cpu, &mask);
+	}
+	smp_call_function_many(&mask, ipi_flush_tlb_a15_erratum, NULL, 1);
+}
+
 void flush_tlb_all(void)
 {
 	if (tlb_ops_need_broadcast())
 		on_each_cpu(ipi_flush_tlb_all, NULL, 1);
 	else
 		local_flush_tlb_all();
+	broadcast_tlb_a15_erratum();
 }
 
 void flush_tlb_mm(struct mm_struct *mm)
@@ -83,6 +144,7 @@ void flush_tlb_mm(struct mm_struct *mm)
 		on_each_cpu_mask(mm_cpumask(mm), ipi_flush_tlb_mm, mm, 1);
 	else
 		local_flush_tlb_mm(mm);
+	broadcast_tlb_mm_a15_erratum(mm);
 }
 
 void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
@@ -95,6 +157,7 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 					&ta, 1);
 	} else
 		local_flush_tlb_page(vma, uaddr);
+	broadcast_tlb_mm_a15_erratum(vma->vm_mm);
 }
 
 void flush_tlb_kernel_page(unsigned long kaddr)
@@ -105,6 +168,7 @@ void flush_tlb_kernel_page(unsigned long kaddr)
 		on_each_cpu(ipi_flush_tlb_kernel_page, &ta, 1);
 	} else
 		local_flush_tlb_kernel_page(kaddr);
+	broadcast_tlb_a15_erratum();
 }
 
 void flush_tlb_range(struct vm_area_struct *vma,
@@ -119,6 +183,7 @@ void flush_tlb_range(struct vm_area_struct *vma,
 					&ta, 1);
 	} else
 		local_flush_tlb_range(vma, start, end);
+	broadcast_tlb_mm_a15_erratum(vma->vm_mm);
 }
 
 void flush_tlb_kernel_range(unsigned long start, unsigned long end)
@@ -130,6 +195,7 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 		on_each_cpu(ipi_flush_tlb_kernel_range, &ta, 1);
 	} else
 		local_flush_tlb_kernel_range(start, end);
+	broadcast_tlb_a15_erratum();
 }
 
 void flush_bp_all(void)

commit 862c588f062fe9339a180cf6429e4df1855c376a
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Feb 28 17:48:11 2013 +0100

    ARM: 7660/1: tlb: add branch predictor maintenance operations
    
    The ARM architecture requires explicit branch predictor maintenance
    when updating an instruction stream for a given virtual address. In
    reality, this isn't so much of a burden because the branch predictor
    is flushed during the cache maintenance required to make the new
    instructions visible to the I-side of the processor.
    
    However, there are still some cases where explicit flushing is required,
    so add a local_bp_flush_all operation to deal with this.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 02c5d2ce23bf..bd0300531399 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -64,6 +64,11 @@ static inline void ipi_flush_tlb_kernel_range(void *arg)
 	local_flush_tlb_kernel_range(ta->ta_start, ta->ta_end);
 }
 
+static inline void ipi_flush_bp_all(void *ignored)
+{
+	local_flush_bp_all();
+}
+
 void flush_tlb_all(void)
 {
 	if (tlb_ops_need_broadcast())
@@ -127,3 +132,10 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 		local_flush_tlb_kernel_range(start, end);
 }
 
+void flush_bp_all(void)
+{
+	if (tlb_ops_need_broadcast())
+		on_each_cpu(ipi_flush_bp_all, NULL, 1);
+	else
+		local_flush_bp_all();
+}

commit 3fc498f165304dc913f1d13b5ac9ab4c758ee7ab
Author: Gilad Ben-Yossef <gilad@benyossef.com>
Date:   Wed Mar 28 14:42:43 2012 -0700

    smp: introduce a generic on_each_cpu_mask() function
    
    We have lots of infrastructure in place to partition multi-core systems
    such that we have a group of CPUs that are dedicated to specific task:
    cgroups, scheduler and interrupt affinity, and cpuisol= boot parameter.
    Still, kernel code will at times interrupt all CPUs in the system via IPIs
    for various needs.  These IPIs are useful and cannot be avoided
    altogether, but in certain cases it is possible to interrupt only specific
    CPUs that have useful work to do and not the entire system.
    
    This patch set, inspired by discussions with Peter Zijlstra and Frederic
    Weisbecker when testing the nohz task patch set, is a first stab at trying
    to explore doing this by locating the places where such global IPI calls
    are being made and turning the global IPI into an IPI for a specific group
    of CPUs.  The purpose of the patch set is to get feedback if this is the
    right way to go for dealing with this issue and indeed, if the issue is
    even worth dealing with at all.  Based on the feedback from this patch set
    I plan to offer further patches that address similar issue in other code
    paths.
    
    This patch creates an on_each_cpu_mask() and on_each_cpu_cond()
    infrastructure API (the former derived from existing arch specific
    versions in Tile and Arm) and uses them to turn several global IPI
    invocation to per CPU group invocations.
    
    Core kernel:
    
    on_each_cpu_mask() calls a function on processors specified by cpumask,
    which may or may not include the local processor.
    
    You must not call this function with disabled interrupts or from a
    hardware interrupt handler or from a bottom half handler.
    
    arch/arm:
    
    Note that the generic version is a little different then the Arm one:
    
    1. It has the mask as first parameter
    2. It calls the function on the calling CPU with interrupts disabled,
       but this should be OK since the function is called on the other CPUs
       with interrupts disabled anyway.
    
    arch/tile:
    
    The API is the same as the tile private one, but the generic version
    also calls the function on the with interrupts disabled in UP case
    
    This is OK since the function is called on the other CPUs
    with interrupts disabled.
    
    Signed-off-by: Gilad Ben-Yossef <gilad@benyossef.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Matt Mackall <mpm@selenic.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Sasha Levin <levinsasha928@gmail.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Avi Kivity <avi@redhat.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.org>
    Cc: Kosaki Motohiro <kosaki.motohiro@gmail.com>
    Cc: Milton Miller <miltonm@bga.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
index 7dcb35285be7..02c5d2ce23bf 100644
--- a/arch/arm/kernel/smp_tlb.c
+++ b/arch/arm/kernel/smp_tlb.c
@@ -13,18 +13,6 @@
 #include <asm/smp_plat.h>
 #include <asm/tlbflush.h>
 
-static void on_each_cpu_mask(void (*func)(void *), void *info, int wait,
-	const struct cpumask *mask)
-{
-	preempt_disable();
-
-	smp_call_function_many(mask, func, info, wait);
-	if (cpumask_test_cpu(smp_processor_id(), mask))
-		func(info);
-
-	preempt_enable();
-}
-
 /**********************************************************************/
 
 /*
@@ -87,7 +75,7 @@ void flush_tlb_all(void)
 void flush_tlb_mm(struct mm_struct *mm)
 {
 	if (tlb_ops_need_broadcast())
-		on_each_cpu_mask(ipi_flush_tlb_mm, mm, 1, mm_cpumask(mm));
+		on_each_cpu_mask(mm_cpumask(mm), ipi_flush_tlb_mm, mm, 1);
 	else
 		local_flush_tlb_mm(mm);
 }
@@ -98,7 +86,8 @@ void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 		struct tlb_args ta;
 		ta.ta_vma = vma;
 		ta.ta_start = uaddr;
-		on_each_cpu_mask(ipi_flush_tlb_page, &ta, 1, mm_cpumask(vma->vm_mm));
+		on_each_cpu_mask(mm_cpumask(vma->vm_mm), ipi_flush_tlb_page,
+					&ta, 1);
 	} else
 		local_flush_tlb_page(vma, uaddr);
 }
@@ -121,7 +110,8 @@ void flush_tlb_range(struct vm_area_struct *vma,
 		ta.ta_vma = vma;
 		ta.ta_start = start;
 		ta.ta_end = end;
-		on_each_cpu_mask(ipi_flush_tlb_range, &ta, 1, mm_cpumask(vma->vm_mm));
+		on_each_cpu_mask(mm_cpumask(vma->vm_mm), ipi_flush_tlb_range,
+					&ta, 1);
 	} else
 		local_flush_tlb_range(vma, start, end);
 }

commit 03b505eae6a276b8c38b6222694afb6cea10b1cc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Dec 20 14:44:32 2010 +0000

    ARM: SMP: split out software TLB maintainence broadcasting
    
    smp.c is becoming too large, so split out the TLB maintainence
    broadcasting into a separate smp_tlb.c file.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/kernel/smp_tlb.c b/arch/arm/kernel/smp_tlb.c
new file mode 100644
index 000000000000..7dcb35285be7
--- /dev/null
+++ b/arch/arm/kernel/smp_tlb.c
@@ -0,0 +1,139 @@
+/*
+ *  linux/arch/arm/kernel/smp_tlb.c
+ *
+ *  Copyright (C) 2002 ARM Limited, All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/preempt.h>
+#include <linux/smp.h>
+
+#include <asm/smp_plat.h>
+#include <asm/tlbflush.h>
+
+static void on_each_cpu_mask(void (*func)(void *), void *info, int wait,
+	const struct cpumask *mask)
+{
+	preempt_disable();
+
+	smp_call_function_many(mask, func, info, wait);
+	if (cpumask_test_cpu(smp_processor_id(), mask))
+		func(info);
+
+	preempt_enable();
+}
+
+/**********************************************************************/
+
+/*
+ * TLB operations
+ */
+struct tlb_args {
+	struct vm_area_struct *ta_vma;
+	unsigned long ta_start;
+	unsigned long ta_end;
+};
+
+static inline void ipi_flush_tlb_all(void *ignored)
+{
+	local_flush_tlb_all();
+}
+
+static inline void ipi_flush_tlb_mm(void *arg)
+{
+	struct mm_struct *mm = (struct mm_struct *)arg;
+
+	local_flush_tlb_mm(mm);
+}
+
+static inline void ipi_flush_tlb_page(void *arg)
+{
+	struct tlb_args *ta = (struct tlb_args *)arg;
+
+	local_flush_tlb_page(ta->ta_vma, ta->ta_start);
+}
+
+static inline void ipi_flush_tlb_kernel_page(void *arg)
+{
+	struct tlb_args *ta = (struct tlb_args *)arg;
+
+	local_flush_tlb_kernel_page(ta->ta_start);
+}
+
+static inline void ipi_flush_tlb_range(void *arg)
+{
+	struct tlb_args *ta = (struct tlb_args *)arg;
+
+	local_flush_tlb_range(ta->ta_vma, ta->ta_start, ta->ta_end);
+}
+
+static inline void ipi_flush_tlb_kernel_range(void *arg)
+{
+	struct tlb_args *ta = (struct tlb_args *)arg;
+
+	local_flush_tlb_kernel_range(ta->ta_start, ta->ta_end);
+}
+
+void flush_tlb_all(void)
+{
+	if (tlb_ops_need_broadcast())
+		on_each_cpu(ipi_flush_tlb_all, NULL, 1);
+	else
+		local_flush_tlb_all();
+}
+
+void flush_tlb_mm(struct mm_struct *mm)
+{
+	if (tlb_ops_need_broadcast())
+		on_each_cpu_mask(ipi_flush_tlb_mm, mm, 1, mm_cpumask(mm));
+	else
+		local_flush_tlb_mm(mm);
+}
+
+void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
+{
+	if (tlb_ops_need_broadcast()) {
+		struct tlb_args ta;
+		ta.ta_vma = vma;
+		ta.ta_start = uaddr;
+		on_each_cpu_mask(ipi_flush_tlb_page, &ta, 1, mm_cpumask(vma->vm_mm));
+	} else
+		local_flush_tlb_page(vma, uaddr);
+}
+
+void flush_tlb_kernel_page(unsigned long kaddr)
+{
+	if (tlb_ops_need_broadcast()) {
+		struct tlb_args ta;
+		ta.ta_start = kaddr;
+		on_each_cpu(ipi_flush_tlb_kernel_page, &ta, 1);
+	} else
+		local_flush_tlb_kernel_page(kaddr);
+}
+
+void flush_tlb_range(struct vm_area_struct *vma,
+                     unsigned long start, unsigned long end)
+{
+	if (tlb_ops_need_broadcast()) {
+		struct tlb_args ta;
+		ta.ta_vma = vma;
+		ta.ta_start = start;
+		ta.ta_end = end;
+		on_each_cpu_mask(ipi_flush_tlb_range, &ta, 1, mm_cpumask(vma->vm_mm));
+	} else
+		local_flush_tlb_range(vma, start, end);
+}
+
+void flush_tlb_kernel_range(unsigned long start, unsigned long end)
+{
+	if (tlb_ops_need_broadcast()) {
+		struct tlb_args ta;
+		ta.ta_start = start;
+		ta.ta_end = end;
+		on_each_cpu(ipi_flush_tlb_kernel_range, &ta, 1);
+	} else
+		local_flush_tlb_kernel_range(start, end);
+}
+
