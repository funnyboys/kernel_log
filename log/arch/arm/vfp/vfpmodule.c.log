commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index ee7b07938dd5..8c9e7f9f0277 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/arch/arm/vfp/vfpmodule.c
  *
  *  Copyright (C) 2004 ARM Limited.
  *  Written by Deep Blue Solutions Limited.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include <linux/types.h>
 #include <linux/cpu.h>

commit e2f8b472a70827348ce2007e121d67f49047ba62
Merge: ccda4af0f4b9 5df7a99bdd0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 14 13:40:22 2018 -0600

    Merge branch 'spectre' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM fix from Russell King:
     "It was noticed that one of Julien's patches contained an error, this
      fixes that up"
    
    * 'spectre' of git://git.armlinux.org.uk/~rmk/linux-arm:
      ARM: 8810/1: vfp: Fix wrong assignement to ufp_exc

commit 5df7a99bdd0de4a0480320264c44c04543c29d5a
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Thu Nov 8 17:25:28 2018 +0100

    ARM: 8810/1: vfp: Fix wrong assignement to ufp_exc
    
    In vfp_preserve_user_clear_hwstate, ufp_exc->fpinst2 gets assigned to
    itself. It should actually be hwstate->fpinst2 that gets assigned to the
    ufp_exc field.
    
    Fixes commit 3aa2df6ec2ca6bc143a65351cca4266d03a8bc41 ("ARM: 8791/1:
    vfp: use __copy_to_user() when saving VFP state").
    
    Reported-by: David Binderman <dcb314@hotmail.com>
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 3b75f1d8a491..15bc3cf2a7fd 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -579,7 +579,7 @@ int vfp_preserve_user_clear_hwstate(struct user_vfp *ufp,
 	 */
 	ufp_exc->fpexc = hwstate->fpexc;
 	ufp_exc->fpinst = hwstate->fpinst;
-	ufp_exc->fpinst2 = ufp_exc->fpinst2;
+	ufp_exc->fpinst2 = hwstate->fpinst2;
 
 	/* Ensure that VFP is disabled. */
 	vfp_flush_hwstate(thread);

commit ba9f6f8954afa5224e3ed60332f7b92242b7ed0f
Merge: a978a5b8d83f a36700589b85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:22:39 2018 +0100

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "I have been slowly sorting out siginfo and this is the culmination of
      that work.
    
      The primary result is in several ways the signal infrastructure has
      been made less error prone. The code has been updated so that manually
      specifying SEND_SIG_FORCED is never necessary. The conversion to the
      new siginfo sending functions is now complete, which makes it
      difficult to send a signal without filling in the proper siginfo
      fields.
    
      At the tail end of the patchset comes the optimization of decreasing
      the size of struct siginfo in the kernel from 128 bytes to about 48
      bytes on 64bit. The fundamental observation that enables this is by
      definition none of the known ways to use struct siginfo uses the extra
      bytes.
    
      This comes at the cost of a small user space observable difference.
      For the rare case of siginfo being injected into the kernel only what
      can be copied into kernel_siginfo is delivered to the destination, the
      rest of the bytes are set to 0. For cases where the signal and the
      si_code are known this is safe, because we know those bytes are not
      used. For cases where the signal and si_code combination is unknown
      the bits that won't fit into struct kernel_siginfo are tested to
      verify they are zero, and the send fails if they are not.
    
      I made an extensive search through userspace code and I could not find
      anything that would break because of the above change. If it turns out
      I did break something it will take just the revert of a single change
      to restore kernel_siginfo to the same size as userspace siginfo.
    
      Testing did reveal dependencies on preferring the signo passed to
      sigqueueinfo over si->signo, so bit the bullet and added the
      complexity necessary to handle that case.
    
      Testing also revealed bad things can happen if a negative signal
      number is passed into the system calls. Something no sane application
      will do but something a malicious program or a fuzzer might do. So I
      have fixed the code that performs the bounds checks to ensure negative
      signal numbers are handled"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (80 commits)
      signal: Guard against negative signal numbers in copy_siginfo_from_user32
      signal: Guard against negative signal numbers in copy_siginfo_from_user
      signal: In sigqueueinfo prefer sig not si_signo
      signal: Use a smaller struct siginfo in the kernel
      signal: Distinguish between kernel_siginfo and siginfo
      signal: Introduce copy_siginfo_from_user and use it's return value
      signal: Remove the need for __ARCH_SI_PREABLE_SIZE and SI_PAD_SIZE
      signal: Fail sigqueueinfo if si_signo != sig
      signal/sparc: Move EMT_TAGOVF into the generic siginfo.h
      signal/unicore32: Use force_sig_fault where appropriate
      signal/unicore32: Generate siginfo in ucs32_notify_die
      signal/unicore32: Use send_sig_fault where appropriate
      signal/arc: Use force_sig_fault where appropriate
      signal/arc: Push siginfo generation into unhandled_exception
      signal/ia64: Use force_sig_fault where appropriate
      signal/ia64: Use the force_sig(SIGSEGV,...) in ia64_rt_sigreturn
      signal/ia64: Use the generic force_sigsegv in setup_frame
      signal/arm/kvm: Use send_sig_mceerr
      signal/arm: Use send_sig_fault where appropriate
      signal/arm: Use force_sig_fault where appropriate
      ...

commit 3e98d240981a33290afc9435d01ec248e5880354
Merge: 3a58ac65e2d7 f18aef742c8f a1d09e074250
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Wed Oct 10 13:53:33 2018 +0100

    Merge branches 'fixes', 'misc' and 'spectre' into for-next

commit 3aa2df6ec2ca6bc143a65351cca4266d03a8bc41
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Sep 11 10:12:18 2018 +0100

    ARM: 8791/1: vfp: use __copy_to_user() when saving VFP state
    
    Use __copy_to_user() rather than __put_user_error() for individual
    members when saving VFP state.
    This has the benefit of disabling/enabling PAN once per copied struct
    intead of once per write.
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 859d50ea17d3..3b75f1d8a491 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -554,12 +554,11 @@ void vfp_flush_hwstate(struct thread_info *thread)
  * Save the current VFP state into the provided structures and prepare
  * for entry into a new function (signal handler).
  */
-int vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,
-				    struct user_vfp_exc __user *ufp_exc)
+int vfp_preserve_user_clear_hwstate(struct user_vfp *ufp,
+				    struct user_vfp_exc *ufp_exc)
 {
 	struct thread_info *thread = current_thread_info();
 	struct vfp_hard_struct *hwstate = &thread->vfpstate.hard;
-	int err = 0;
 
 	/* Ensure that the saved hwstate is up-to-date. */
 	vfp_sync_hwstate(thread);
@@ -568,22 +567,19 @@ int vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,
 	 * Copy the floating point registers. There can be unused
 	 * registers see asm/hwcap.h for details.
 	 */
-	err |= __copy_to_user(&ufp->fpregs, &hwstate->fpregs,
-			      sizeof(hwstate->fpregs));
+	memcpy(&ufp->fpregs, &hwstate->fpregs, sizeof(hwstate->fpregs));
+
 	/*
 	 * Copy the status and control register.
 	 */
-	__put_user_error(hwstate->fpscr, &ufp->fpscr, err);
+	ufp->fpscr = hwstate->fpscr;
 
 	/*
 	 * Copy the exception registers.
 	 */
-	__put_user_error(hwstate->fpexc, &ufp_exc->fpexc, err);
-	__put_user_error(hwstate->fpinst, &ufp_exc->fpinst, err);
-	__put_user_error(hwstate->fpinst2, &ufp_exc->fpinst2, err);
-
-	if (err)
-		return -EFAULT;
+	ufp_exc->fpexc = hwstate->fpexc;
+	ufp_exc->fpinst = hwstate->fpinst;
+	ufp_exc->fpinst2 = ufp_exc->fpinst2;
 
 	/* Ensure that VFP is disabled. */
 	vfp_flush_hwstate(thread);

commit b059454846141c2daacffe8a99366ef31cc1f23f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Apr 16 13:35:45 2018 -0500

    signal/arm: Use send_sig_fault where appropriate
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index dc7e6b50ef67..aa96cc5c9fe9 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -216,13 +216,6 @@ static struct notifier_block vfp_notifier_block = {
  */
 static void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 {
-	siginfo_t info;
-
-	clear_siginfo(&info);
-	info.si_signo = SIGFPE;
-	info.si_code = sicode;
-	info.si_addr = (void __user *)(instruction_pointer(regs) - 4);
-
 	/*
 	 * This is the same as NWFPE, because it's not clear what
 	 * this is used for
@@ -230,7 +223,9 @@ static void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 	current->thread.error_code = 0;
 	current->thread.trap_no = 6;
 
-	send_sig_info(SIGFPE, &info, current);
+	send_sig_fault(SIGFPE, sicode,
+		       (void __user *)(instruction_pointer(regs) - 4),
+		       current);
 }
 
 static void vfp_panic(char *reason, u32 inst)

commit c61b466d4f886613c7e71de8282701646a4d999d
Merge: afc9f65e01cd 001a30c4d0e8 a3c0f84765bb
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Mon Aug 13 16:28:50 2018 +0100

    Merge branches 'fixes', 'misc' and 'spectre' into for-linus
    
    Conflicts:
            arch/arm/include/asm/uaccess.h
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit 42019fc50dfadb219f9e6ddf4c354f3837057d80
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Mon Jul 9 10:13:36 2018 +0100

    ARM: vfp: use __copy_from_user() when restoring VFP state
    
    __get_user_error() is used as a fast accessor to make copying structure
    members in the signal handling path as efficient as possible.  However,
    with software PAN and the recent Spectre variant 1, the efficiency is
    reduced as these are no longer fast accessors.
    
    In the case of software PAN, it has to switch the domain register around
    each access, and with Spectre variant 1, it would have to repeat the
    access_ok() check for each access.
    
    Use __copy_from_user() rather than __get_user_err() for individual
    members when restoring VFP state.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 4c375e11ae95..859d50ea17d3 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -597,13 +597,11 @@ int vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,
 }
 
 /* Sanitise and restore the current VFP state from the provided structures. */
-int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
-			     struct user_vfp_exc __user *ufp_exc)
+int vfp_restore_user_hwstate(struct user_vfp *ufp, struct user_vfp_exc *ufp_exc)
 {
 	struct thread_info *thread = current_thread_info();
 	struct vfp_hard_struct *hwstate = &thread->vfpstate.hard;
 	unsigned long fpexc;
-	int err = 0;
 
 	/* Disable VFP to avoid corrupting the new thread state. */
 	vfp_flush_hwstate(thread);
@@ -612,17 +610,16 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
 	 * Copy the floating point registers. There can be unused
 	 * registers see asm/hwcap.h for details.
 	 */
-	err |= __copy_from_user(&hwstate->fpregs, &ufp->fpregs,
-				sizeof(hwstate->fpregs));
+	memcpy(&hwstate->fpregs, &ufp->fpregs, sizeof(hwstate->fpregs));
 	/*
 	 * Copy the status and control register.
 	 */
-	__get_user_error(hwstate->fpscr, &ufp->fpscr, err);
+	hwstate->fpscr = ufp->fpscr;
 
 	/*
 	 * Sanitise and restore the exception registers.
 	 */
-	__get_user_error(fpexc, &ufp_exc->fpexc, err);
+	fpexc = ufp_exc->fpexc;
 
 	/* Ensure the VFP is enabled. */
 	fpexc |= FPEXC_EN;
@@ -631,10 +628,10 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
 	fpexc &= ~(FPEXC_EX | FPEXC_FP2V);
 	hwstate->fpexc = fpexc;
 
-	__get_user_error(hwstate->fpinst, &ufp_exc->fpinst, err);
-	__get_user_error(hwstate->fpinst2, &ufp_exc->fpinst2, err);
+	hwstate->fpinst = ufp_exc->fpinst;
+	hwstate->fpinst2 = ufp_exc->fpinst2;
 
-	return err ? -EFAULT : 0;
+	return 0;
 }
 
 /*

commit 93e95fa57441b6976b39029bd658b6bbe7ccfe28
Merge: d8aed8415b86 26da35010c6d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 15:23:48 2018 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "This set of changes close the known issues with setting si_code to an
      invalid value, and with not fully initializing struct siginfo. There
      remains work to do on nds32, arc, unicore32, powerpc, arm, arm64, ia64
      and x86 to get the code that generates siginfo into a simpler and more
      maintainable state. Most of that work involves refactoring the signal
      handling code and thus careful code review.
    
      Also not included is the work to shrink the in kernel version of
      struct siginfo. That depends on getting the number of places that
      directly manipulate struct siginfo under control, as it requires the
      introduction of struct kernel_siginfo for the in kernel things.
    
      Overall this set of changes looks like it is making good progress, and
      with a little luck I will be wrapping up the siginfo work next
      development cycle"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (46 commits)
      signal/sh: Stop gcc warning about an impossible case in do_divide_error
      signal/mips: Report FPE_FLTUNK for undiagnosed floating point exceptions
      signal/um: More carefully relay signals in relay_signal.
      signal: Extend siginfo_layout with SIL_FAULT_{MCEERR|BNDERR|PKUERR}
      signal: Remove unncessary #ifdef SEGV_PKUERR in 32bit compat code
      signal/signalfd: Add support for SIGSYS
      signal/signalfd: Remove __put_user from signalfd_copyinfo
      signal/xtensa: Use force_sig_fault where appropriate
      signal/xtensa: Consistenly use SIGBUS in do_unaligned_user
      signal/um: Use force_sig_fault where appropriate
      signal/sparc: Use force_sig_fault where appropriate
      signal/sparc: Use send_sig_fault where appropriate
      signal/sh: Use force_sig_fault where appropriate
      signal/s390: Use force_sig_fault where appropriate
      signal/riscv: Replace do_trap_siginfo with force_sig_fault
      signal/riscv: Use force_sig_fault where appropriate
      signal/parisc: Use force_sig_fault where appropriate
      signal/parisc: Use force_sig_mceerr where appropriate
      signal/openrisc: Use force_sig_fault where appropriate
      signal/nios2: Use force_sig_fault where appropriate
      ...

commit 92d44a42af81e850a038c38278ff4f434b2871df
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri Apr 13 00:22:47 2018 +0100

    ARM: fix kill( ,SIGFPE) breakage
    
    Commit 7771c6645700 ("signal/arm: Document conflicts with SI_USER and
    SIGFPE") broke the siginfo structure for userspace triggered signals,
    causing the strace testsuite to regress.  Fix this by eliminating
    the FPE_FIXME definition (which is at the root of the breakage) and
    use FPE_FLTINV instead for the case where the hardware appears to be
    reporting nonsense.
    
    Fixes: 7771c6645700 ("signal/arm: Document conflicts with SI_USER and SIGFPE")
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 4c375e11ae95..af4ee2cef2f9 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -257,7 +257,7 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 
 	if (exceptions == VFP_EXCEPTION_ERROR) {
 		vfp_panic("unhandled bounce", inst);
-		vfp_raise_sigfpe(FPE_FIXME, regs);
+		vfp_raise_sigfpe(FPE_FLTINV, regs);
 		return;
 	}
 

commit 3eb0f5193b497083391aa05d35210d5645211eef
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 17 15:26:37 2018 -0500

    signal: Ensure every siginfo we send has all bits initialized
    
    Call clear_siginfo to ensure every stack allocated siginfo is properly
    initialized before being passed to the signal sending functions.
    
    Note: It is not safe to depend on C initializers to initialize struct
    siginfo on the stack because C is allowed to skip holes when
    initializing a structure.
    
    The initialization of struct siginfo in tracehook_report_syscall_exit
    was moved from the helper user_single_step_siginfo into
    tracehook_report_syscall_exit itself, to make it clear that the local
    variable siginfo gets fully initialized.
    
    In a few cases the scope of struct siginfo has been reduced to make it
    clear that siginfo siginfo is not used on other paths in the function
    in which it is declared.
    
    Instances of using memset to initialize siginfo have been replaced
    with calls clear_siginfo for clarity.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 4c375e11ae95..adda3fc2dde8 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -218,8 +218,7 @@ static void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 {
 	siginfo_t info;
 
-	memset(&info, 0, sizeof(info));
-
+	clear_siginfo(&info);
 	info.si_signo = SIGFPE;
 	info.si_code = sicode;
 	info.si_addr = (void __user *)(instruction_pointer(regs) - 4);

commit 1328f02005bbbaed15b9d5b7f3ab5ec9d4d5268a
Author: Fabio Estevam <festevam@gmail.com>
Date:   Mon Jan 22 12:20:26 2018 +0100

    ARM: 8746/1: vfp: Go back to clearing vfp_current_hw_state[]
    
    Commit 384b38b66947 ("ARM: 7873/1: vfp: clear vfp_current_hw_state
    for dying cpu") fixed the cpu dying notifier by clearing
    vfp_current_hw_state[]. However commit e5b61bafe704 ("arm: Convert VFP
    hotplug notifiers to state machine") incorrectly used the original
    vfp_force_reload() function in the cpu dying notifier.
    
    Fix it by going back to clearing vfp_current_hw_state[].
    
    Fixes: e5b61bafe704 ("arm: Convert VFP hotplug notifiers to state machine")
    Cc: linux-stable <stable@vger.kernel.org>
    Reported-by: Kohji Okuno <okuno.kohji@jp.panasonic.com>
    Signed-off-by: Fabio Estevam <fabio.estevam@nxp.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 03c6a3c72f9c..4c375e11ae95 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -648,7 +648,7 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
  */
 static int vfp_dying_cpu(unsigned int cpu)
 {
-	vfp_force_reload(cpu, current_thread_info());
+	vfp_current_hw_state[cpu] = NULL;
 	return 0;
 }
 

commit 7771c66457004977b616bab785209f49d164f527
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Aug 17 17:07:46 2017 -0500

    signal/arm: Document conflicts with SI_USER and SIGFPE
    
    Setting si_code to 0 results in a userspace seeing an si_code of 0.
    This is the same si_code as SI_USER.  Posix and common sense requires
    that SI_USER not be a signal specific si_code.  As such this use of 0
    for the si_code is a pretty horribly broken ABI.
    
    Further use of si_code == 0 guaranteed that copy_siginfo_to_user saw a
    value of __SI_KILL and now sees a value of SIL_KILL with the result
    that uid and pid fields are copied and which might copying the si_addr
    field by accident but certainly not by design.  Making this a very
    flakey implementation.
    
    Utilizing FPE_FIXME, siginfo_layout will now return SIL_FAULT and the
    appropriate fields will be reliably copied.
    
    Possible ABI fixes includee:
    - Send the signal without siginfo
    - Don't generate a signal
    - Possibly assign and use an appropriate si_code
    - Don't handle cases which can't happen
    
    Cc: Russell King <rmk@flint.arm.linux.org.uk>
    Cc: linux-arm-kernel@lists.infradead.org
    Ref: 451436b7bbb2 ("[ARM] Add support code for ARM hardware vector floating point")
    History Tree: https://git.kernel.org/pub/scm/linux/kernel/git/tglx/history.git
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index a71a48e71fff..03c6a3c72f9c 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -257,7 +257,7 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 
 	if (exceptions == VFP_EXCEPTION_ERROR) {
 		vfp_panic("unhandled bounce", inst);
-		vfp_raise_sigfpe(0, regs);
+		vfp_raise_sigfpe(FPE_FIXME, regs);
 		return;
 	}
 

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 569d5a650a4a..a71a48e71fff 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -15,7 +15,7 @@
 #include <linux/kernel.h>
 #include <linux/notifier.h>
 #include <linux/signal.h>
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/smp.h>
 #include <linux/init.h>
 #include <linux/uaccess.h>

commit 73c1b41e63f040e92669e61a02c7893933bfe743
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:19:54 2016 +0100

    cpu/hotplug: Cleanup state names
    
    When the state names got added a script was used to add the extra argument
    to the calls. The script basically converted the state constant to a
    string, but the cleanup to convert these strings into meaningful ones did
    not happen.
    
    Replace all the useless strings with 'subsys/xxx/yyy:state' strings which
    are used in all the other places already.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Link: http://lkml.kernel.org/r/20161221192112.085444152@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 0351f5645fb1..569d5a650a4a 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -799,7 +799,7 @@ static int __init vfp_init(void)
 	}
 
 	cpuhp_setup_state_nocalls(CPUHP_AP_ARM_VFP_STARTING,
-				  "AP_ARM_VFP_STARTING", vfp_starting_cpu,
+				  "arm/vfp:starting", vfp_starting_cpu,
 				  vfp_dying_cpu);
 
 	vfp_vector = vfp_support_entry;

commit a85b2257a57d5b3328fdecec9255c6d30c67c8bb
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Nov 16 22:20:14 2016 +0100

    ARM: 8629/1: vfp: properly tag assembly function declarations in C code
    
    This is good for consistency even if there is no difference in compiled
    code. LTO might rely on this eventually. No need to preserve the extern
    attribute as it is the default with function prototypes.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index da0b33deba6d..0351f5645fb1 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -34,11 +34,11 @@
 /*
  * Our undef handlers (in entry.S)
  */
-void vfp_testing_entry(void);
-void vfp_support_entry(void);
-void vfp_null_entry(void);
+asmlinkage void vfp_testing_entry(void);
+asmlinkage void vfp_support_entry(void);
+asmlinkage void vfp_null_entry(void);
 
-void (*vfp_vector)(void) = vfp_null_entry;
+asmlinkage void (*vfp_vector)(void) = vfp_null_entry;
 
 /*
  * Dual-use variable.

commit e5b61bafe70477e05e1dce0d6ca4ec181e23cb2a
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Jul 13 17:16:35 2016 +0000

    arm: Convert VFP hotplug notifiers to state machine
    
    Straight forward conversion plus commentary why code which is executed
    in hotplug callbacks needs to be invoked before installing them.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Reviewed-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160713153335.713612993@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 73085d3482ed..da0b33deba6d 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -643,19 +643,19 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
  * hardware state at every thread switch.  We clear our held state when
  * a CPU has been killed, indicating that the VFP hardware doesn't contain
  * a threads VFP state.  When a CPU starts up, we re-enable access to the
- * VFP hardware.
- *
- * Both CPU_DYING and CPU_STARTING are called on the CPU which
+ * VFP hardware. The callbacks below are called on the CPU which
  * is being offlined/onlined.
  */
-static int vfp_hotplug(struct notifier_block *b, unsigned long action,
-	void *hcpu)
+static int vfp_dying_cpu(unsigned int cpu)
 {
-	if (action == CPU_DYING || action == CPU_DYING_FROZEN)
-		vfp_current_hw_state[(long)hcpu] = NULL;
-	else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
-		vfp_enable(NULL);
-	return NOTIFY_OK;
+	vfp_force_reload(cpu, current_thread_info());
+	return 0;
+}
+
+static int vfp_starting_cpu(unsigned int unused)
+{
+	vfp_enable(NULL);
+	return 0;
 }
 
 void vfp_kmode_exception(void)
@@ -732,6 +732,10 @@ static int __init vfp_init(void)
 	unsigned int vfpsid;
 	unsigned int cpu_arch = cpu_architecture();
 
+	/*
+	 * Enable the access to the VFP on all online CPUs so the
+	 * following test on FPSID will succeed.
+	 */
 	if (cpu_arch >= CPU_ARCH_ARMv6)
 		on_each_cpu(vfp_enable, NULL, 1);
 
@@ -794,7 +798,9 @@ static int __init vfp_init(void)
 		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;
 	}
 
-	hotcpu_notifier(vfp_hotplug, 0);
+	cpuhp_setup_state_nocalls(CPUHP_AP_ARM_VFP_STARTING,
+				  "AP_ARM_VFP_STARTING", vfp_starting_cpu,
+				  vfp_dying_cpu);
 
 	vfp_vector = vfp_support_entry;
 

commit e64646946ed32902fd597fa6e514b1da84642de3
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:20 2016 -0700

    exit_thread: accept a task parameter to be exited
    
    We need to call exit_thread from copy_process in a fail path.  So make it
    accept task_struct as a parameter.
    
    [v2]
    * s390: exit_thread_runtime_instr doesn't make sense to be called for
      non-current tasks.
    * arm: fix the comment in vfp_thread_copy
    * change 'me' to 'tsk' for task_struct
    * now we can change only archs that actually have exit_thread
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 2a61e4b04600..73085d3482ed 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -156,10 +156,6 @@ static void vfp_thread_copy(struct thread_info *thread)
  *   - we could be preempted if tree preempt rcu is enabled, so
  *	it is unsafe to use thread->cpu.
  *  THREAD_NOTIFY_EXIT
- *   - the thread (v) will be running on the local CPU, so
- *	v === current_thread_info()
- *   - thread->cpu is the local CPU number at the time it is accessed,
- *	but may change at any time.
  *   - we could be preempted if tree preempt rcu is enabled, so
  *	it is unsafe to use thread->cpu.
  */

commit 7d7d7a413c5b8dddfde56dce1dd42e2199033c6c
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Fri Apr 17 16:53:58 2015 -0700

    ARM: vfp: Add vfp_disable for problematic platforms
    
    Some platforms might not be able to fully utilize VFP when e.g: one CPU
    out of two in a SMP complex lacks a VFP unit. Adding code to migrate
    task to the CPU which has a VFP unit would be cumbersome and not
    performant, instead, just add the ability to disable VFP.
    
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f6e4d56eda00..2a61e4b04600 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -445,6 +445,19 @@ static void vfp_enable(void *unused)
 	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
 }
 
+/* Called by platforms on which we want to disable VFP because it may not be
+ * present on all CPUs within a SMP complex. Needs to be called prior to
+ * vfp_init().
+ */
+void vfp_disable(void)
+{
+	if (VFP_arch) {
+		pr_debug("%s: should be called prior to vfp_init\n", __func__);
+		return;
+	}
+	VFP_arch = 1;
+}
+
 #ifdef CONFIG_CPU_PM
 static int vfp_pm_suspend(void)
 {

commit 2b94fe2ac97fdd2ae7521004e857e33016720eb7
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Thu Nov 20 20:29:09 2014 +0100

    ARM: 8215/1: vfp: Silence mvfr0 variable unused warning
    
    Stephen Rothwell reports that commit 3f4c9f8f0a20 ("ARM: 8197/1:
    vfp: Fix VFPv3 hwcap detection on CPUID based cpus") introduced a
    variable unused warning.
    
    arch/arm/vfp/vfpmodule.c: In function 'vfp_init':
    arch/arm/vfp/vfpmodule.c:725:6: warning: unused variable 'mvfr0'
    [-Wunused-variable]
      u32 mvfr0;
    
    Silence this warning by using IS_ENABLED instead of ifdefs.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 5002d002f6e3..f6e4d56eda00 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -722,7 +722,6 @@ static int __init vfp_init(void)
 {
 	unsigned int vfpsid;
 	unsigned int cpu_arch = cpu_architecture();
-	u32 mvfr0;
 
 	if (cpu_arch >= CPU_ARCH_ARMv6)
 		on_each_cpu(vfp_enable, NULL, 1);
@@ -752,30 +751,30 @@ static int __init vfp_init(void)
 		 * precision floating point operations. Only check
 		 * for NEON if the hardware has the MVFR registers.
 		 */
-#ifdef CONFIG_NEON
-		if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
+		if (IS_ENABLED(CONFIG_NEON) &&
+		   (fmrx(MVFR1) & 0x000fff00) == 0x00011100)
 			elf_hwcap |= HWCAP_NEON;
-#endif
-#ifdef CONFIG_VFPv3
-		mvfr0 = fmrx(MVFR0);
-		if (((mvfr0 & MVFR0_DP_MASK) >> MVFR0_DP_BIT) == 0x2 ||
-		    ((mvfr0 & MVFR0_SP_MASK) >> MVFR0_SP_BIT) == 0x2) {
-			elf_hwcap |= HWCAP_VFPv3;
-			/*
-			 * Check for VFPv3 D16 and VFPv4 D16.  CPUs in
-			 * this configuration only have 16 x 64bit
-			 * registers.
-			 */
-			if ((mvfr0 & MVFR0_A_SIMD_MASK) == 1)
-				/* also v4-D16 */
-				elf_hwcap |= HWCAP_VFPv3D16;
-			else
-				elf_hwcap |= HWCAP_VFPD32;
-		}
 
-		if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
-			elf_hwcap |= HWCAP_VFPv4;
-#endif
+		if (IS_ENABLED(CONFIG_VFPv3)) {
+			u32 mvfr0 = fmrx(MVFR0);
+			if (((mvfr0 & MVFR0_DP_MASK) >> MVFR0_DP_BIT) == 0x2 ||
+			    ((mvfr0 & MVFR0_SP_MASK) >> MVFR0_SP_BIT) == 0x2) {
+				elf_hwcap |= HWCAP_VFPv3;
+				/*
+				 * Check for VFPv3 D16 and VFPv4 D16.  CPUs in
+				 * this configuration only have 16 x 64bit
+				 * registers.
+				 */
+				if ((mvfr0 & MVFR0_A_SIMD_MASK) == 1)
+					/* also v4-D16 */
+					elf_hwcap |= HWCAP_VFPv3D16;
+				else
+					elf_hwcap |= HWCAP_VFPD32;
+			}
+
+			if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
+				elf_hwcap |= HWCAP_VFPv4;
+		}
 	/* Extract the architecture version on pre-cpuid scheme */
 	} else {
 		if (vfpsid & FPSID_NODOUBLE) {

commit 6c96a4a6e249a0580b32893583771149e0611375
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Mon Nov 10 21:56:41 2014 +0100

    ARM: 8197/1: vfp: Fix VFPv3 hwcap detection on CPUID based cpus
    
    The subarchitecture field in the fpsid register is 7 bits wide on
    ARM CPUs using the CPUID identification scheme, spanning bits 22
    to 16. The topmost bit is used to designate that the
    subarchitecture designer is not ARM when it is set to 1. On
    non-CPUID scheme CPUs the subarchitecture field is only 4 bits
    wide and the higher bits are used to indicate no double precision
    support (bit 20) and the FTSMX/FLDMX format (bits 21-22).
    
    The VFP support code only looks at bits 19-16 to determine the
    VFP version. On Qualcomm's processors (Krait and Scorpion) we
    should see that we have HWCAP_VFPv3 but we don't because bit 22
    is set to 1 to indicate that the subarchitecture is not
    implemented by ARM and the rest of the bits are left as 0 because
    this is the first subarchitecture that Qualcomm has designed.
    Unfortunately we can't just widen the FPSID subarchitecture
    bitmask to consider all the bits on a CPUID scheme because there
    may be CPUs without the CPUID scheme that have VFP without double
    precision support and then the version would be a very wrong and
    large number. Instead, update the version detection logic to
    consider if the CPU is using the CPUID scheme.
    
    If the CPU is using CPUID scheme, use the MVFR registers to
    determine what version of VFP is supported. We already do this
    for VFPv4, so do something similar for VFPv3 and look for single
    or double precision support in MVFR0. Otherwise fall back to
    using FPSID to detect VFP support on non-CPUID scheme CPUs. We
    know that VFPv3 is only present in CPUs that have support for the
    CPUID scheme so this should be equivalent.
    
    Tested-by: Rob Clark <robdclark@gmail.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 2f37e1d6cb45..5002d002f6e3 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -722,6 +722,7 @@ static int __init vfp_init(void)
 {
 	unsigned int vfpsid;
 	unsigned int cpu_arch = cpu_architecture();
+	u32 mvfr0;
 
 	if (cpu_arch >= CPU_ARCH_ARMv6)
 		on_each_cpu(vfp_enable, NULL, 1);
@@ -738,63 +739,73 @@ static int __init vfp_init(void)
 	vfp_vector = vfp_null_entry;
 
 	pr_info("VFP support v0.3: ");
-	if (VFP_arch)
+	if (VFP_arch) {
 		pr_cont("not present\n");
-	else if (vfpsid & FPSID_NODOUBLE) {
-		pr_cont("no double precision support\n");
-	} else {
-		hotcpu_notifier(vfp_hotplug, 0);
-
-		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */
-		pr_cont("implementor %02x architecture %d part %02x variant %x rev %x\n",
-			(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,
-			(vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT,
-			(vfpsid & FPSID_PART_MASK) >> FPSID_PART_BIT,
-			(vfpsid & FPSID_VARIANT_MASK) >> FPSID_VARIANT_BIT,
-			(vfpsid & FPSID_REV_MASK) >> FPSID_REV_BIT);
-
-		vfp_vector = vfp_support_entry;
-
-		thread_register_notifier(&vfp_notifier_block);
-		vfp_pm_init();
-
+		return 0;
+	/* Extract the architecture on CPUID scheme */
+	} else if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {
+		VFP_arch = vfpsid & FPSID_CPUID_ARCH_MASK;
+		VFP_arch >>= FPSID_ARCH_BIT;
 		/*
-		 * We detected VFP, and the support code is
-		 * in place; report VFP support to userspace.
+		 * Check for the presence of the Advanced SIMD
+		 * load/store instructions, integer and single
+		 * precision floating point operations. Only check
+		 * for NEON if the hardware has the MVFR registers.
 		 */
-		elf_hwcap |= HWCAP_VFP;
+#ifdef CONFIG_NEON
+		if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
+			elf_hwcap |= HWCAP_NEON;
+#endif
 #ifdef CONFIG_VFPv3
-		if (VFP_arch >= 2) {
+		mvfr0 = fmrx(MVFR0);
+		if (((mvfr0 & MVFR0_DP_MASK) >> MVFR0_DP_BIT) == 0x2 ||
+		    ((mvfr0 & MVFR0_SP_MASK) >> MVFR0_SP_BIT) == 0x2) {
 			elf_hwcap |= HWCAP_VFPv3;
-
 			/*
 			 * Check for VFPv3 D16 and VFPv4 D16.  CPUs in
 			 * this configuration only have 16 x 64bit
 			 * registers.
 			 */
-			if (((fmrx(MVFR0) & MVFR0_A_SIMD_MASK)) == 1)
-				elf_hwcap |= HWCAP_VFPv3D16; /* also v4-D16 */
+			if ((mvfr0 & MVFR0_A_SIMD_MASK) == 1)
+				/* also v4-D16 */
+				elf_hwcap |= HWCAP_VFPv3D16;
 			else
 				elf_hwcap |= HWCAP_VFPD32;
 		}
+
+		if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
+			elf_hwcap |= HWCAP_VFPv4;
 #endif
-		/*
-		 * Check for the presence of the Advanced SIMD
-		 * load/store instructions, integer and single
-		 * precision floating point operations. Only check
-		 * for NEON if the hardware has the MVFR registers.
-		 */
-		if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {
-#ifdef CONFIG_NEON
-			if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
-				elf_hwcap |= HWCAP_NEON;
-#endif
-#ifdef CONFIG_VFPv3
-			if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
-				elf_hwcap |= HWCAP_VFPv4;
-#endif
+	/* Extract the architecture version on pre-cpuid scheme */
+	} else {
+		if (vfpsid & FPSID_NODOUBLE) {
+			pr_cont("no double precision support\n");
+			return 0;
 		}
+
+		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;
 	}
+
+	hotcpu_notifier(vfp_hotplug, 0);
+
+	vfp_vector = vfp_support_entry;
+
+	thread_register_notifier(&vfp_notifier_block);
+	vfp_pm_init();
+
+	/*
+	 * We detected VFP, and the support code is
+	 * in place; report VFP support to userspace.
+	 */
+	elf_hwcap |= HWCAP_VFP;
+
+	pr_cont("implementor %02x architecture %d part %02x variant %x rev %x\n",
+		(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,
+		VFP_arch,
+		(vfpsid & FPSID_PART_MASK) >> FPSID_PART_BIT,
+		(vfpsid & FPSID_VARIANT_MASK) >> FPSID_VARIANT_BIT,
+		(vfpsid & FPSID_REV_MASK) >> FPSID_REV_BIT);
+
 	return 0;
 }
 

commit 384b38b66947b06999b3e39a596d4f2fb94f77e4
Author: Yuanyuan Zhong <zyy@motorola.com>
Date:   Wed Oct 30 17:31:49 2013 +0100

    ARM: 7873/1: vfp: clear vfp_current_hw_state for dying cpu
    
    The CPU_DYING notifier is called by cpu stopper task which
    does not own the context held in the VFP hardware. Calling
    vfp_force_reload() has no effect.
    Replace it with clearing vfp_current_hw_state.
    
    Signed-off-by: Yuanyuan Zhong <zyy@motorola.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 52b8f40b1c73..2f37e1d6cb45 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -642,9 +642,9 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
 static int vfp_hotplug(struct notifier_block *b, unsigned long action,
 	void *hcpu)
 {
-	if (action == CPU_DYING || action == CPU_DYING_FROZEN) {
-		vfp_force_reload((long)hcpu, current_thread_info());
-	} else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
+	if (action == CPU_DYING || action == CPU_DYING_FROZEN)
+		vfp_current_hw_state[(long)hcpu] = NULL;
+	else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
 		vfp_enable(NULL);
 	return NOTIFY_OK;
 }

commit 73c132c15da504789b924871e2491479a18e4f6a
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu May 16 11:41:48 2013 +0200

    ARM: add support for kernel mode NEON
    
    In order to safely support the use of NEON instructions in
    kernel mode, some precautions need to be taken:
    - the userland context that may be present in the registers (even
      if the NEON/VFP is currently disabled) must be stored under the
      correct task (which may not be 'current' in the UP case),
    - to avoid having to keep track of additional vfpstates for the
      kernel side, disallow the use of NEON in interrupt context
      and run with preemption disabled,
    - after use, re-enable preemption and re-enable the lazy restore
      machinery by disabling the NEON/VFP unit.
    
    This patch adds the functions kernel_neon_begin() and
    kernel_neon_end() which take care of the above. It also adds
    the Kconfig symbol KERNEL_MODE_NEON to enable it.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 7620831a0c66..52b8f40b1c73 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -20,6 +20,7 @@
 #include <linux/init.h>
 #include <linux/uaccess.h>
 #include <linux/user.h>
+#include <linux/export.h>
 
 #include <asm/cp15.h>
 #include <asm/cputype.h>
@@ -668,6 +669,52 @@ void vfp_kmode_exception(void)
 		pr_crit("BUG: FP instruction issued in kernel mode with FP unit disabled\n");
 }
 
+#ifdef CONFIG_KERNEL_MODE_NEON
+
+/*
+ * Kernel-side NEON support functions
+ */
+void kernel_neon_begin(void)
+{
+	struct thread_info *thread = current_thread_info();
+	unsigned int cpu;
+	u32 fpexc;
+
+	/*
+	 * Kernel mode NEON is only allowed outside of interrupt context
+	 * with preemption disabled. This will make sure that the kernel
+	 * mode NEON register contents never need to be preserved.
+	 */
+	BUG_ON(in_interrupt());
+	cpu = get_cpu();
+
+	fpexc = fmrx(FPEXC) | FPEXC_EN;
+	fmxr(FPEXC, fpexc);
+
+	/*
+	 * Save the userland NEON/VFP state. Under UP,
+	 * the owner could be a task other than 'current'
+	 */
+	if (vfp_state_in_hw(cpu, thread))
+		vfp_save_state(&thread->vfpstate, fpexc);
+#ifndef CONFIG_SMP
+	else if (vfp_current_hw_state[cpu] != NULL)
+		vfp_save_state(vfp_current_hw_state[cpu], fpexc);
+#endif
+	vfp_current_hw_state[cpu] = NULL;
+}
+EXPORT_SYMBOL(kernel_neon_begin);
+
+void kernel_neon_end(void)
+{
+	/* Disable the NEON/VFP unit. */
+	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+	put_cpu();
+}
+EXPORT_SYMBOL(kernel_neon_end);
+
+#endif /* CONFIG_KERNEL_MODE_NEON */
+
 /*
  * VFP support code initialisation.
  */

commit ab3da15643469ab2d206dee3d9cfa4194ba77f25
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri May 24 16:23:28 2013 +0200

    ARM: be strict about FP exceptions in kernel mode
    
    The support code in vfp_support_entry does not care whether the
    exception that caused it to be invoked occurred in kernel mode or
    in user mode. However, neither condition that could trigger this
    exception (lazy restore and VFP bounce to support code) is
    currently allowable in kernel mode.
    
    In either case, print a message describing the condition before
    letting the undefined instruction handler run its course and trigger
    an oops.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 791993aed75a..7620831a0c66 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -648,6 +648,26 @@ static int vfp_hotplug(struct notifier_block *b, unsigned long action,
 	return NOTIFY_OK;
 }
 
+void vfp_kmode_exception(void)
+{
+	/*
+	 * If we reach this point, a floating point exception has been raised
+	 * while running in kernel mode. If the NEON/VFP unit was enabled at the
+	 * time, it means a VFP instruction has been issued that requires
+	 * software assistance to complete, something which is not currently
+	 * supported in kernel mode.
+	 * If the NEON/VFP unit was disabled, and the location pointed to below
+	 * is properly preceded by a call to kernel_neon_begin(), something has
+	 * caused the task to be scheduled out and back in again. In this case,
+	 * rebuilding and running with CONFIG_DEBUG_ATOMIC_SLEEP enabled should
+	 * be helpful in localizing the problem.
+	 */
+	if (fmrx(FPEXC) & FPEXC_EN)
+		pr_crit("BUG: unsupported FP instruction in kernel mode\n");
+	else
+		pr_crit("BUG: FP instruction issued in kernel mode with FP unit disabled\n");
+}
+
 /*
  * VFP support code initialisation.
  */

commit 0773d73d818702191dd568e1e20bcafc5c64661a
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed May 22 10:38:53 2013 +0200

    ARM: move VFP init to an earlier boot stage
    
    In order to use the NEON unit in the kernel, we should
    initialize it a bit earlier in the boot process so NEON users
    that like to do a quick benchmark at load time (like the
    xor_blocks or RAID-6 code) find the NEON/VFP unit already
    enabled.
    
    Replaced late_initcall() with core_initcall().
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 5dfbb0b8e7f4..791993aed75a 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -731,4 +731,4 @@ static int __init vfp_init(void)
 	return 0;
 }
 
-late_initcall(vfp_init);
+core_initcall(vfp_init);

commit 5e4ba617c1b584b2e376f31a63bd4e734109318a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Feb 25 16:09:12 2013 +0000

    ARM: VFP: fix emulation of second VFP instruction
    
    Martin Storsjö reports that the sequence:
    
            ee312ac1        vsub.f32        s4, s3, s2
            ee702ac0        vsub.f32        s5, s1, s0
            e59f0028        ldr             r0, [pc, #40]
            ee111a90        vmov            r1, s3
    
    on Raspberry Pi (implementor 41 architecture 1 part 20 variant b rev 5)
    where s3 is a denormal and s2 is zero results in incorrect behaviour -
    the instruction "vsub.f32 s5, s1, s0" is not executed:
    
            VFP: bounce: trigger ee111a90 fpexc d0000780
            VFP: emulate: INST=0xee312ac1 SCR=0x00000000
            ...
    
    As we can see, the instruction triggering the exception is the "vmov"
    instruction, and we emulate the "vsub.f32 s4, s3, s2" but fail to
    properly take account of the FPEXC_FP2V flag in FPEXC.  This is because
    the test for the second instruction register being valid is bogus, and
    will always skip emulation of the second instruction.
    
    Cc: <stable@vger.kernel.org>
    Reported-by: Martin Storsjö <martin@martin.st>
    Tested-by: Martin Storsjö <martin@martin.st>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 3b44e0dd0a93..5dfbb0b8e7f4 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -413,7 +413,7 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	 * If there isn't a second FP instruction, exit now. Note that
 	 * the FPEXC.FP2V bit is valid only if FPEXC.EX is 1.
 	 */
-	if (fpexc ^ (FPEXC_EX | FPEXC_FP2V))
+	if ((fpexc & (FPEXC_EX | FPEXC_FP2V)) != (FPEXC_EX | FPEXC_FP2V))
 		goto exit;
 
 	/*

commit 39141ddfb63a664f26d3f42f64ee386e879b492c
Author: Paul Walmsley <paul@pwsan.com>
Date:   Tue Oct 23 20:32:59 2012 +0100

    ARM: 7566/1: vfp: fix save and restore when running on pre-VFPv3 and CONFIG_VFPv3 set
    
    After commit 846a136881b8f73c1f74250bf6acfaa309cab1f2 ("ARM: vfp: fix
    saving d16-d31 vfp registers on v6+ kernels"), the OMAP 2430SDP board
    started crashing during boot with omap2plus_defconfig:
    
    [    3.875122] mmcblk0: mmc0:e624 SD04G 3.69 GiB
    [    3.915954]  mmcblk0: p1
    [    4.086639] Internal error: Oops - undefined instruction: 0 [#1] SMP ARM
    [    4.093719] Modules linked in:
    [    4.096954] CPU: 0    Not tainted  (3.6.0-02232-g759e00b #570)
    [    4.103149] PC is at vfp_reload_hw+0x1c/0x44
    [    4.107666] LR is at __und_usr_fault_32+0x0/0x8
    
    It turns out that the context save/restore fix unmasked a latent bug
    in commit 5aaf254409f8d58229107b59507a8235b715a960 ("ARM: 6203/1: Make
    VFPv3 usable on ARMv6").  When CONFIG_VFPv3 is set, but the kernel is
    booted on a pre-VFPv3 core, the code attempts to save and restore the
    d16-d31 VFP registers.  These are only present on non-D16 VFPv3+, so
    this results in an undefined instruction exception.  The code didn't
    crash before commit 846a136 because the save and restore code was
    only touching d0-d15, present on all VFP.
    
    Fix by implementing a request from Russell King to add a new HWCAP
    flag that affirmatively indicates the presence of the d16-d31
    registers:
    
       http://marc.info/?l=linux-arm-kernel&m=135013547905283&w=2
    
    and some feedback from Måns to clarify the name of the HWCAP flag.
    
    Signed-off-by: Paul Walmsley <paul@pwsan.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dave Martin <dave.martin@linaro.org>
    Cc: Måns Rullgård <mans.rullgard@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index c834b32af275..3b44e0dd0a93 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -701,11 +701,14 @@ static int __init vfp_init(void)
 			elf_hwcap |= HWCAP_VFPv3;
 
 			/*
-			 * Check for VFPv3 D16. CPUs in this configuration
-			 * only have 16 x 64bit registers.
+			 * Check for VFPv3 D16 and VFPv4 D16.  CPUs in
+			 * this configuration only have 16 x 64bit
+			 * registers.
 			 */
 			if (((fmrx(MVFR0) & MVFR0_A_SIMD_MASK)) == 1)
-				elf_hwcap |= HWCAP_VFPv3D16;
+				elf_hwcap |= HWCAP_VFPv3D16; /* also v4-D16 */
+			else
+				elf_hwcap |= HWCAP_VFPD32;
 		}
 #endif
 		/*

commit 3d9fb0038a9b02febb01efc79a4a5d97f1822a90
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Aug 3 17:24:14 2012 +0100

    ARM: 7483/1: vfp: only advertise VFPv4 in hwcaps if CONFIG_VFPv3 is enabled
    
    VFPv4 support depends on the VFPv3 context save/restore code, so only
    advertise support in the hwcaps if the kernel can actually handle it.
    
    Cc: <stable@vger.kernel.org> # 3.1+
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index fb849d044bde..c834b32af275 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -719,8 +719,10 @@ static int __init vfp_init(void)
 			if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
 				elf_hwcap |= HWCAP_NEON;
 #endif
+#ifdef CONFIG_VFPv3
 			if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
 				elf_hwcap |= HWCAP_VFPv4;
+#endif
 		}
 	}
 	return 0;

commit 24b35521b8ddf088531258f06f681bb7b227bf47
Author: Colin Cross <ccross@android.com>
Date:   Fri Jul 20 02:03:42 2012 +0100

    ARM: 7477/1: vfp: Always save VFP state in vfp_pm_suspend on UP
    
    vfp_pm_suspend should save the VFP state in suspend after
    any lazy context switch.  If it only saves when the VFP is enabled,
    the state can get lost when, on a UP system:
      Thread 1 uses the VFP
      Context switch occurs to thread 2, VFP is disabled but the
         VFP context is not saved
      Thread 2 initiates suspend
      vfp_pm_suspend is called with the VFP disabled, and the unsaved
         VFP context of Thread 1 in the registers
    
    Modify vfp_pm_suspend to save the VFP context whenever
    vfp_current_hw_state is not NULL.
    
    Includes a fix from Ido Yariv <ido@wizery.com>, who pointed out that on
    SMP systems, the state pointer can be pointing to a freed task struct if
    a task exited on another cpu, fixed by using #ifndef CONFIG_SMP in the
    new if clause.
    
    Cc: Barry Song <bs14@csr.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Ido Yariv <ido@wizery.com>
    Cc: Daniel Drake <dsd@laptop.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 9428006728f2..fb849d044bde 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -457,6 +457,12 @@ static int vfp_pm_suspend(void)
 
 		/* disable, just in case */
 		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+	} else if (vfp_current_hw_state[ti->cpu]) {
+#ifndef CONFIG_SMP
+		fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_save_state(vfp_current_hw_state[ti->cpu], fpexc);
+		fmxr(FPEXC, fpexc);
+#endif
 	}
 
 	/* clear any information we had about last context state */

commit a84b895a2348f0dbff31b71ddf954f70a6cde368
Author: Colin Cross <ccross@android.com>
Date:   Fri Jul 20 02:03:43 2012 +0100

    ARM: 7476/1: vfp: only clear vfp state for current cpu in vfp_pm_suspend
    
    vfp_pm_suspend runs on each cpu, only clear the hardware state
    pointer for the current cpu.  Prevents a possible crash if one
    cpu clears the hw state pointer when another cpu has already
    checked if it is valid.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 586961929e96..9428006728f2 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -460,7 +460,7 @@ static int vfp_pm_suspend(void)
 	}
 
 	/* clear any information we had about last context state */
-	memset(vfp_current_hw_state, 0, sizeof(vfp_current_hw_state));
+	vfp_current_hw_state[ti->cpu] = NULL;
 
 	return 0;
 }

commit 4175160b065e74572819a320dcd34129224a4e1c
Merge: ddf90a2ff2c4 0ec8e7aa8f63
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon May 21 15:15:24 2012 +0100

    Merge branch 'misc' into for-linus
    
    Conflicts:
            arch/arm/kernel/ptrace.c

commit 56cb248428ead13a6b423ed3f3cf9e4aa01244b1
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue May 15 15:51:54 2012 +0100

    ARM: 7419/1: vfp: fix VFP flushing regression on sigreturn path
    
    Commit ff9a184c ("ARM: 7400/1: vfp: clear fpscr length and stride bits
    on entry to sig handler") flushes the VFP state prior to entering a
    signal handler so that a VFP operation inside the handler will trap and
    force a restore of ABI-compliant registers. Reflushing and disabling VFP
    on the sigreturn path is predicated on the saved thread state indicating
    that VFP was used by the handler -- however for SMP platforms this is
    only set on context-switch, making the check unreliable and causing VFP
    register corruption in userspace since the register values are not
    necessarily those restored from the sigframe.
    
    This patch unconditionally flushes the VFP state after a signal handler.
    Since we already perform the flush before the handler and the flushing
    itself happens lazily, the redundant flush when VFP is not used by the
    handler is essentially a nop.
    
    Reported-by: Jon Medhurst <tixy@linaro.org>
    Signed-off-by: Jon Medhurst <tixy@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index c5767b5a4318..b0197b2c857d 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -577,12 +577,6 @@ int vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,
 	 * entry.
 	 */
 	hwstate->fpscr &= ~(FPSCR_LENGTH_MASK | FPSCR_STRIDE_MASK);
-
-	/*
-	 * Disable VFP in the hwstate so that we can detect if it gets
-	 * used.
-	 */
-	hwstate->fpexc &= ~FPEXC_EN;
 	return 0;
 }
 
@@ -595,12 +589,8 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
 	unsigned long fpexc;
 	int err = 0;
 
-	/*
-	 * If VFP has been used, then disable it to avoid corrupting
-	 * the new thread state.
-	 */
-	if (hwstate->fpexc & FPEXC_EN)
-		vfp_flush_hwstate(thread);
+	/* Disable VFP to avoid corrupting the new thread state. */
+	vfp_flush_hwstate(thread);
 
 	/*
 	 * Copy the floating point registers. There can be unused

commit 998de4acb2ba188d20768d1065658377a2e7d29b
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 11 17:42:37 2012 +0100

    ARM: 7417/1: vfp: ensure preemption is disabled when enabling VFP access
    
    The vfp_enable function enables access to the VFP co-processor register
    space (cp10 and cp11) on the current CPU and must be called with
    preemption disabled. Unfortunately, the vfp_init late initcall does not
    disable preemption and can lead to an oops during boot if thread
    migration occurs at the wrong time and we end up attempting to access
    the FPSID on a CPU with VFP access disabled.
    
    This patch fixes the initcall to call vfp_enable from a non-preemptible
    context on each CPU and adds a BUG_ON(preemptible) to ensure that any
    similar problems are easily spotted in the future.
    
    Cc: stable@vger.kernel.org
    Reported-by: Hyungwoo Yang <hwoo.yang@gmail.com>
    Signed-off-by: Hyungwoo Yang <hyungwooy@nvidia.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index bc683b8219b5..c5767b5a4318 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -11,6 +11,7 @@
 #include <linux/types.h>
 #include <linux/cpu.h>
 #include <linux/cpu_pm.h>
+#include <linux/hardirq.h>
 #include <linux/kernel.h>
 #include <linux/notifier.h>
 #include <linux/signal.h>
@@ -432,7 +433,10 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 
 static void vfp_enable(void *unused)
 {
-	u32 access = get_copro_access();
+	u32 access;
+
+	BUG_ON(preemptible());
+	access = get_copro_access();
 
 	/*
 	 * Enable full access to VFP (cp10 and cp11)
@@ -657,7 +661,7 @@ static int __init vfp_init(void)
 	unsigned int cpu_arch = cpu_architecture();
 
 	if (cpu_arch >= CPU_ARCH_ARMv6)
-		vfp_enable(NULL);
+		on_each_cpu(vfp_enable, NULL, 1);
 
 	/*
 	 * First check that there is a VFP that we can use.
@@ -678,8 +682,6 @@ static int __init vfp_init(void)
 	} else {
 		hotcpu_notifier(vfp_hotplug, 0);
 
-		smp_call_function(vfp_enable, NULL, 1);
-
 		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */
 		printk("implementor %02x architecture %d part %02x variant %x rev %x\n",
 			(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,

commit dc457078d4da76e3951e55e00d3801bf8217c683
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed May 9 22:32:43 2012 +0100

    ARM: 7415/1: vfp: convert printk's to pr_*'s
    
    This is mainly to get rid of the "vfp_pm_suspend: saving vfp state"
    message flooding the kernel message ring by default.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 858748eaa144..a8f541b8fe9f 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -238,11 +238,11 @@ static void vfp_panic(char *reason, u32 inst)
 {
 	int i;
 
-	printk(KERN_ERR "VFP: Error: %s\n", reason);
-	printk(KERN_ERR "VFP: EXC 0x%08x SCR 0x%08x INST 0x%08x\n",
+	pr_err("VFP: Error: %s\n", reason);
+	pr_err("VFP: EXC 0x%08x SCR 0x%08x INST 0x%08x\n",
 		fmrx(FPEXC), fmrx(FPSCR), inst);
 	for (i = 0; i < 32; i += 2)
-		printk(KERN_ERR "VFP: s%2u: 0x%08x s%2u: 0x%08x\n",
+		pr_err("VFP: s%2u: 0x%08x s%2u: 0x%08x\n",
 		       i, vfp_get_float(i), i+1, vfp_get_float(i+1));
 }
 
@@ -446,7 +446,7 @@ static int vfp_pm_suspend(void)
 
 	/* if vfp is on, then save state for resumption */
 	if (fpexc & FPEXC_EN) {
-		printk(KERN_DEBUG "%s: saving vfp state\n", __func__);
+		pr_debug("%s: saving vfp state\n", __func__);
 		vfp_save_state(&ti->vfpstate, fpexc);
 
 		/* disable, just in case */
@@ -571,18 +571,18 @@ static int __init vfp_init(void)
 	barrier();
 	vfp_vector = vfp_null_entry;
 
-	printk(KERN_INFO "VFP support v0.3: ");
+	pr_info("VFP support v0.3: ");
 	if (VFP_arch)
-		printk("not present\n");
+		pr_cont("not present\n");
 	else if (vfpsid & FPSID_NODOUBLE) {
-		printk("no double precision support\n");
+		pr_cont("no double precision support\n");
 	} else {
 		hotcpu_notifier(vfp_hotplug, 0);
 
 		smp_call_function(vfp_enable, NULL, 1);
 
 		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */
-		printk("implementor %02x architecture %d part %02x variant %x rev %x\n",
+		pr_cont("implementor %02x architecture %d part %02x variant %x rev %x\n",
 			(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,
 			(vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT,
 			(vfpsid & FPSID_PART_MASK) >> FPSID_PART_BIT,

commit ff9a184cfb6542bef98aff1789481284e122a4b3
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 23 15:42:16 2012 +0100

    ARM: 7400/1: vfp: clear fpscr length and stride bits on entry to sig handler
    
    The ARM PCS mandates that the length and stride bits of the fpscr are
    cleared on entry to and return from a public interface. Although signal
    handlers run asynchronously with respect to the interrupted function,
    the handler itself expects to run as though it has been called like a
    normal function.
    
    This patch updates the state mirroring the VFP hardware before entry to
    a signal handler so that it adheres to the PCS. Furthermore, we disable
    VFP to ensure that we trap on any floating point operation performed by
    the signal handler and synchronise the hardware appropriately. A check
    is inserted after the signal handler to avoid redundant flushing if VFP
    was not used.
    
    Reported-by: Peter Maydell <peter.maydell@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 05872d92fca2..bc683b8219b5 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -564,6 +564,21 @@ int vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,
 
 	if (err)
 		return -EFAULT;
+
+	/* Ensure that VFP is disabled. */
+	vfp_flush_hwstate(thread);
+
+	/*
+	 * As per the PCS, clear the length and stride bits for function
+	 * entry.
+	 */
+	hwstate->fpscr &= ~(FPSCR_LENGTH_MASK | FPSCR_STRIDE_MASK);
+
+	/*
+	 * Disable VFP in the hwstate so that we can detect if it gets
+	 * used.
+	 */
+	hwstate->fpexc &= ~FPEXC_EN;
 	return 0;
 }
 
@@ -576,7 +591,12 @@ int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
 	unsigned long fpexc;
 	int err = 0;
 
-	vfp_flush_hwstate(thread);
+	/*
+	 * If VFP has been used, then disable it to avoid corrupting
+	 * the new thread state.
+	 */
+	if (hwstate->fpexc & FPEXC_EN)
+		vfp_flush_hwstate(thread);
 
 	/*
 	 * Copy the floating point registers. There can be unused

commit 2498814fcb3068f19b82b1519b4038721f61af43
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Apr 23 15:38:28 2012 +0100

    ARM: 7399/1: vfp: move user vfp state save/restore code out of signal.c
    
    The user VFP state must be preserved (subject to ucontext modifications)
    across invocation of a signal handler and this is currently handled by
    vfp_{preserve,restore}_context in signal.c
    
    Since this code requires intimate low-level knowledge of the VFP state,
    this patch moves it into vfpmodule.c.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 858748eaa144..05872d92fca2 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -17,6 +17,8 @@
 #include <linux/sched.h>
 #include <linux/smp.h>
 #include <linux/init.h>
+#include <linux/uaccess.h>
+#include <linux/user.h>
 
 #include <asm/cp15.h>
 #include <asm/cputype.h>
@@ -528,6 +530,83 @@ void vfp_flush_hwstate(struct thread_info *thread)
 	put_cpu();
 }
 
+/*
+ * Save the current VFP state into the provided structures and prepare
+ * for entry into a new function (signal handler).
+ */
+int vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,
+				    struct user_vfp_exc __user *ufp_exc)
+{
+	struct thread_info *thread = current_thread_info();
+	struct vfp_hard_struct *hwstate = &thread->vfpstate.hard;
+	int err = 0;
+
+	/* Ensure that the saved hwstate is up-to-date. */
+	vfp_sync_hwstate(thread);
+
+	/*
+	 * Copy the floating point registers. There can be unused
+	 * registers see asm/hwcap.h for details.
+	 */
+	err |= __copy_to_user(&ufp->fpregs, &hwstate->fpregs,
+			      sizeof(hwstate->fpregs));
+	/*
+	 * Copy the status and control register.
+	 */
+	__put_user_error(hwstate->fpscr, &ufp->fpscr, err);
+
+	/*
+	 * Copy the exception registers.
+	 */
+	__put_user_error(hwstate->fpexc, &ufp_exc->fpexc, err);
+	__put_user_error(hwstate->fpinst, &ufp_exc->fpinst, err);
+	__put_user_error(hwstate->fpinst2, &ufp_exc->fpinst2, err);
+
+	if (err)
+		return -EFAULT;
+	return 0;
+}
+
+/* Sanitise and restore the current VFP state from the provided structures. */
+int vfp_restore_user_hwstate(struct user_vfp __user *ufp,
+			     struct user_vfp_exc __user *ufp_exc)
+{
+	struct thread_info *thread = current_thread_info();
+	struct vfp_hard_struct *hwstate = &thread->vfpstate.hard;
+	unsigned long fpexc;
+	int err = 0;
+
+	vfp_flush_hwstate(thread);
+
+	/*
+	 * Copy the floating point registers. There can be unused
+	 * registers see asm/hwcap.h for details.
+	 */
+	err |= __copy_from_user(&hwstate->fpregs, &ufp->fpregs,
+				sizeof(hwstate->fpregs));
+	/*
+	 * Copy the status and control register.
+	 */
+	__get_user_error(hwstate->fpscr, &ufp->fpscr, err);
+
+	/*
+	 * Sanitise and restore the exception registers.
+	 */
+	__get_user_error(fpexc, &ufp_exc->fpexc, err);
+
+	/* Ensure the VFP is enabled. */
+	fpexc |= FPEXC_EN;
+
+	/* Ensure FPINST2 is invalid and the exception flag is cleared. */
+	fpexc &= ~(FPEXC_EX | FPEXC_FP2V);
+	hwstate->fpexc = fpexc;
+
+	__get_user_error(hwstate->fpinst, &ufp_exc->fpinst, err);
+	__get_user_error(hwstate->fpinst2, &ufp_exc->fpinst2, err);
+
+	return err ? -EFAULT : 0;
+}
+
 /*
  * VFP hardware can lose all context when a CPU goes offline.
  * As we will be running in SMP mode with CPU hotplug, we will save the

commit 9f97da78bf018206fb623cd351d454af2f105fe0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:01 2012 +0100

    Disintegrate asm/system.h for ARM
    
    Disintegrate asm/system.h for ARM.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Russell King <linux@arm.linux.org.uk>
    cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index d89068f6d6e5..858748eaa144 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -20,6 +20,7 @@
 
 #include <asm/cp15.h>
 #include <asm/cputype.h>
+#include <asm/system_info.h>
 #include <asm/thread_notify.h>
 #include <asm/vfp.h>
 

commit 15d07dc9c59eae51219c40253bdf920f62bb10f2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Mar 28 18:30:01 2012 +0100

    ARM: move CP15 definitions to separate header file
    
    Avoid namespace conflicts with drivers over the CP15 definitions by
    moving CP15 related prototypes and definitions to a private header
    file.
    
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> [Tegra]
    Acked-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com> [EP93xx]
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 8f3ccddbdafd..d89068f6d6e5 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -18,6 +18,7 @@
 #include <linux/smp.h>
 #include <linux/init.h>
 
+#include <asm/cp15.h>
 #include <asm/cputype.h>
 #include <asm/thread_notify.h>
 #include <asm/vfp.h>

commit d91ef63bd5ae52f642c0a0369d57671977508e3d
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 10:29:08 2011 -0400

    arm: remove several unnecessary module.h include instances
    
    Building these files does not reveal a hidden need for
    any of these.  Since module.h brings in the whole kitchen
    sink, it just needlessly adds 30k+ lines to the cpp burden.
    
    There are probably lots more, but ARM files of mach-* and plat-*
    don't get coverage via a simple yesconfig build.  They will have
    to be cleaned up and tested via using their respective configs.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 0cbd5a0a9332..8f3ccddbdafd 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -8,7 +8,6 @@
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
-#include <linux/module.h>
 #include <linux/types.h>
 #include <linux/cpu.h>
 #include <linux/cpu_pm.h>

commit 48af9feab5e3bdf21af3a929ecc7c0b79d9a4a4e
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Fri Sep 2 21:12:36 2011 +0530

    ARM: vfp: Fix the comment to make it consistent with the code.
    
    Function vfp_force_reload() clears vfp_current_hw_state, so
    update the comment accordingly.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index fe4b60b1e6c7..0cbd5a0a9332 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -69,7 +69,7 @@ static bool vfp_state_in_hw(unsigned int cpu, struct thread_info *thread)
 /*
  * Force a reload of the VFP context from the thread structure.  We do
  * this by ensuring that access to the VFP hardware is disabled, and
- * clear last_VFP_context.  Must be called from non-preemptible context.
+ * clear vfp_current_hw_state.  Must be called from non-preemptible context.
  */
 static void vfp_force_reload(unsigned int cpu, struct thread_info *thread)
 {

commit 746a9d1963f1c7a3cd22662065d7dcd3d93ee62e
Author: Colin Cross <ccross@android.com>
Date:   Thu Feb 10 02:08:32 2011 -0800

    ARM: vfp: Use cpu pm notifiers to save vfp state
    
    When the cpu is powered down in a low power mode, the vfp
    registers may be reset.
    
    This patch uses CPU_PM_ENTER and CPU_PM_EXIT notifiers to save
    and restore the cpu's vfp registers.
    
    Signed-off-by: Colin Cross <ccross@android.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Kevin Hilman <khilman@ti.com>
    Tested-and-Acked-by: Shawn Guo <shawn.guo@linaro.org>
    Tested-by: Vishwanath BS <vishwanath.bs@ti.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 79bcb4316930..fe4b60b1e6c7 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -11,6 +11,7 @@
 #include <linux/module.h>
 #include <linux/types.h>
 #include <linux/cpu.h>
+#include <linux/cpu_pm.h>
 #include <linux/kernel.h>
 #include <linux/notifier.h>
 #include <linux/signal.h>
@@ -436,9 +437,7 @@ static void vfp_enable(void *unused)
 	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
 }
 
-#ifdef CONFIG_PM
-#include <linux/syscore_ops.h>
-
+#ifdef CONFIG_CPU_PM
 static int vfp_pm_suspend(void)
 {
 	struct thread_info *ti = current_thread_info();
@@ -468,19 +467,33 @@ static void vfp_pm_resume(void)
 	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
 }
 
-static struct syscore_ops vfp_pm_syscore_ops = {
-	.suspend	= vfp_pm_suspend,
-	.resume		= vfp_pm_resume,
+static int vfp_cpu_pm_notifier(struct notifier_block *self, unsigned long cmd,
+	void *v)
+{
+	switch (cmd) {
+	case CPU_PM_ENTER:
+		vfp_pm_suspend();
+		break;
+	case CPU_PM_ENTER_FAILED:
+	case CPU_PM_EXIT:
+		vfp_pm_resume();
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+static struct notifier_block vfp_cpu_pm_notifier_block = {
+	.notifier_call = vfp_cpu_pm_notifier,
 };
 
 static void vfp_pm_init(void)
 {
-	register_syscore_ops(&vfp_pm_syscore_ops);
+	cpu_pm_register_notifier(&vfp_cpu_pm_notifier_block);
 }
 
 #else
 static inline void vfp_pm_init(void) { }
-#endif /* CONFIG_PM */
+#endif /* CONFIG_CPU_PM */
 
 /*
  * Ensure that the VFP state stored in 'thread->vfpstate' is up to date

commit 3ad55155b222f2a901405dea20ff7c68828ecd92
Merge: 06f365acef5c 6645cb61f3a1
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 22 23:09:07 2011 +0100

    Merge branch 'devel-stable' into for-next
    
    Conflicts:
            arch/arm/kernel/entry-armv.S

commit 19dad35fe0f10df8f0524f98037469e3a0dd1ec2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jul 9 17:41:33 2011 +0100

    ARM: vfp: ensure that thread flushing works if preempted
    
    Prevent a preemption event causing the initialized VFP state being
    overwritten by ensuring that the VFP hardware access is disabled
    prior to starting initialization.  We can then do this in safety
    while still allowing preemption to occur.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 08ff93fa533c..0a96f71f0abd 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -89,24 +89,27 @@ static void vfp_thread_flush(struct thread_info *thread)
 	union vfp_state *vfp = &thread->vfpstate;
 	unsigned int cpu;
 
-	memset(vfp, 0, sizeof(union vfp_state));
-
-	vfp->hard.fpexc = FPEXC_EN;
-	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
-#ifdef CONFIG_SMP
-	vfp->hard.cpu = NR_CPUS;
-#endif
-
 	/*
 	 * Disable VFP to ensure we initialize it first.  We must ensure
-	 * that the modification of vfp_current_hw_state[] and hardware disable
-	 * are done for the same CPU and without preemption.
+	 * that the modification of vfp_current_hw_state[] and hardware
+	 * disable are done for the same CPU and without preemption.
+	 *
+	 * Do this first to ensure that preemption won't overwrite our
+	 * state saving should access to the VFP be enabled at this point.
 	 */
 	cpu = get_cpu();
 	if (vfp_current_hw_state[cpu] == vfp)
 		vfp_current_hw_state[cpu] = NULL;
 	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
 	put_cpu();
+
+	memset(vfp, 0, sizeof(union vfp_state));
+
+	vfp->hard.fpexc = FPEXC_EN;
+	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
+#ifdef CONFIG_SMP
+	vfp->hard.cpu = NR_CPUS;
+#endif
 }
 
 static void vfp_thread_exit(struct thread_info *thread)

commit f8f2a8522a88aacd62a310ce49e8dac530d1b403
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jul 9 16:09:43 2011 +0100

    ARM: vfp: fix a hole in VFP thread migration
    
    Fix a hole in the VFP thread migration.  Lets define two threads.
    
    Thread 1, we'll call 'interesting_thread' which is a thread which is
    running on CPU0, using VFP (so vfp_current_hw_state[0] =
    &interesting_thread->vfpstate) and gets migrated off to CPU1, where
    it continues execution of VFP instructions.
    
    Thread 2, we'll call 'new_cpu0_thread' which is the thread which takes
    over on CPU0.  This has also been using VFP, and last used VFP on CPU0,
    but doesn't use it again.
    
    The following code will be executed twice:
    
                    cpu = thread->cpu;
    
                    /*
                     * On SMP, if VFP is enabled, save the old state in
                     * case the thread migrates to a different CPU. The
                     * restoring is done lazily.
                     */
                    if ((fpexc & FPEXC_EN) && vfp_current_hw_state[cpu]) {
                            vfp_save_state(vfp_current_hw_state[cpu], fpexc);
                            vfp_current_hw_state[cpu]->hard.cpu = cpu;
                    }
                    /*
                     * Thread migration, just force the reloading of the
                     * state on the new CPU in case the VFP registers
                     * contain stale data.
                     */
                    if (thread->vfpstate.hard.cpu != cpu)
                            vfp_current_hw_state[cpu] = NULL;
    
    The first execution will be on CPU0 to switch away from 'interesting_thread'.
    interesting_thread->cpu will be 0.
    
    So, vfp_current_hw_state[0] points at interesting_thread->vfpstate.
    The hardware state will be saved, along with the CPU number (0) that
    it was executing on.
    
    'thread' will be 'new_cpu0_thread' with new_cpu0_thread->cpu = 0.
    Also, because it was executing on CPU0, new_cpu0_thread->vfpstate.hard.cpu = 0,
    and so the thread migration check is not triggered.
    
    This means that vfp_current_hw_state[0] remains pointing at interesting_thread.
    
    The second execution will be on CPU1 to switch _to_ 'interesting_thread'.
    So, 'thread' will be 'interesting_thread' and interesting_thread->cpu now
    will be 1.  The previous thread executing on CPU1 is not relevant to this
    so we shall ignore that.
    
    We get to the thread migration check.  Here, we discover that
    interesting_thread->vfpstate.hard.cpu = 0, yet interesting_thread->cpu is
    now 1, indicating thread migration.  We set vfp_current_hw_state[1] to
    NULL.
    
    So, at this point vfp_current_hw_state[] contains the following:
    
    [0] = &interesting_thread->vfpstate
    [1] = NULL
    
    Our interesting thread now executes a VFP instruction, takes a fault
    which loads the state into the VFP hardware.  Now, through the assembly
    we now have:
    
    [0] = &interesting_thread->vfpstate
    [1] = &interesting_thread->vfpstate
    
    CPU1 stops due to ptrace (and so saves its VFP state) using the thread
    switch code above), and CPU0 calls vfp_sync_hwstate().
    
            if (vfp_current_hw_state[cpu] == &thread->vfpstate) {
                    vfp_save_state(&thread->vfpstate, fpexc | FPEXC_EN);
    
    BANG, we corrupt interesting_thread's VFP state by overwriting the
    more up-to-date state saved by CPU1 with the old VFP state from CPU0.
    
    Fix this by ensuring that we have sane semantics for the various state
    describing variables:
    
    1. vfp_current_hw_state[] points to the current owner of the context
       information stored in each CPUs hardware, or NULL if that state
       information is invalid.
    2. thread->vfpstate.hard.cpu always contains the most recent CPU number
       which the state was loaded into or NR_CPUS if no CPU owns the state.
    
    So, for a particular CPU to be a valid owner of the VFP state for a
    particular thread t, two things must be true:
    
     vfp_current_hw_state[cpu] == &t->vfpstate && t->vfpstate.hard.cpu == cpu.
    
    and that is valid from the moment a CPU loads the saved VFP context
    into the hardware.  This gives clear and consistent semantics to
    interpreting these variables.
    
    This patch also fixes thread copying, ensuring that t->vfpstate.hard.cpu
    is invalidated, otherwise CPU0 may believe it was the last owner.  The
    hole can happen thus:
    
    - thread1 runs on CPU2 using VFP, migrates to CPU3, exits and thread_info
      freed.
    - New thread allocated from a previously running thread on CPU2, reusing
      memory for thread1 and copying vfp.hard.cpu.
    
    At this point, the following are true:
    
            new_thread1->vfpstate.hard.cpu == 2
            &new_thread1->vfpstate == vfp_current_hw_state[2]
    
    Lastly, this also addresses thread flushing in a similar way to thread
    copying.  Hole is:
    
    - thread runs on CPU0, using VFP, migrates to CPU1 but does not use VFP.
    - thread calls execve(), so thread flush happens, leaving
      vfp_current_hw_state[0] intact.  This vfpstate is memset to 0 causing
      thread->vfpstate.hard.cpu = 0.
    - thread migrates back to CPU0 before using VFP.
    
    At this point, the following are true:
    
            thread->vfpstate.hard.cpu == 0
            &thread->vfpstate == vfp_current_hw_state[0]
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 3640351171b8..08ff93fa533c 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -34,19 +34,52 @@ void vfp_null_entry(void);
 
 void (*vfp_vector)(void) = vfp_null_entry;
 
+/*
+ * Dual-use variable.
+ * Used in startup: set to non-zero if VFP checks fail
+ * After startup, holds VFP architecture
+ */
+unsigned int VFP_arch;
+
 /*
  * The pointer to the vfpstate structure of the thread which currently
  * owns the context held in the VFP hardware, or NULL if the hardware
  * context is invalid.
+ *
+ * For UP, this is sufficient to tell which thread owns the VFP context.
+ * However, for SMP, we also need to check the CPU number stored in the
+ * saved state too to catch migrations.
  */
 union vfp_state *vfp_current_hw_state[NR_CPUS];
 
 /*
- * Dual-use variable.
- * Used in startup: set to non-zero if VFP checks fail
- * After startup, holds VFP architecture
+ * Is 'thread's most up to date state stored in this CPUs hardware?
+ * Must be called from non-preemptible context.
  */
-unsigned int VFP_arch;
+static bool vfp_state_in_hw(unsigned int cpu, struct thread_info *thread)
+{
+#ifdef CONFIG_SMP
+	if (thread->vfpstate.hard.cpu != cpu)
+		return false;
+#endif
+	return vfp_current_hw_state[cpu] == &thread->vfpstate;
+}
+
+/*
+ * Force a reload of the VFP context from the thread structure.  We do
+ * this by ensuring that access to the VFP hardware is disabled, and
+ * clear last_VFP_context.  Must be called from non-preemptible context.
+ */
+static void vfp_force_reload(unsigned int cpu, struct thread_info *thread)
+{
+	if (vfp_state_in_hw(cpu, thread)) {
+		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+		vfp_current_hw_state[cpu] = NULL;
+	}
+#ifdef CONFIG_SMP
+	thread->vfpstate.hard.cpu = NR_CPUS;
+#endif
+}
 
 /*
  * Per-thread VFP initialization.
@@ -60,6 +93,9 @@ static void vfp_thread_flush(struct thread_info *thread)
 
 	vfp->hard.fpexc = FPEXC_EN;
 	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
+#ifdef CONFIG_SMP
+	vfp->hard.cpu = NR_CPUS;
+#endif
 
 	/*
 	 * Disable VFP to ensure we initialize it first.  We must ensure
@@ -90,6 +126,9 @@ static void vfp_thread_copy(struct thread_info *thread)
 
 	vfp_sync_hwstate(parent);
 	thread->vfpstate = parent->vfpstate;
+#ifdef CONFIG_SMP
+	thread->vfpstate.hard.cpu = NR_CPUS;
+#endif
 }
 
 /*
@@ -135,17 +174,8 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * case the thread migrates to a different CPU. The
 		 * restoring is done lazily.
 		 */
-		if ((fpexc & FPEXC_EN) && vfp_current_hw_state[cpu]) {
+		if ((fpexc & FPEXC_EN) && vfp_current_hw_state[cpu])
 			vfp_save_state(vfp_current_hw_state[cpu], fpexc);
-			vfp_current_hw_state[cpu]->hard.cpu = cpu;
-		}
-		/*
-		 * Thread migration, just force the reloading of the
-		 * state on the new CPU in case the VFP registers
-		 * contain stale data.
-		 */
-		if (thread->vfpstate.hard.cpu != cpu)
-			vfp_current_hw_state[cpu] = NULL;
 #endif
 
 		/*
@@ -449,15 +479,15 @@ static void vfp_pm_init(void)
 static inline void vfp_pm_init(void) { }
 #endif /* CONFIG_PM */
 
+/*
+ * Ensure that the VFP state stored in 'thread->vfpstate' is up to date
+ * with the hardware state.
+ */
 void vfp_sync_hwstate(struct thread_info *thread)
 {
 	unsigned int cpu = get_cpu();
 
-	/*
-	 * If the thread we're interested in is the current owner of the
-	 * hardware VFP state, then we need to save its state.
-	 */
-	if (vfp_current_hw_state[cpu] == &thread->vfpstate) {
+	if (vfp_state_in_hw(cpu, thread)) {
 		u32 fpexc = fmrx(FPEXC);
 
 		/*
@@ -471,36 +501,13 @@ void vfp_sync_hwstate(struct thread_info *thread)
 	put_cpu();
 }
 
+/* Ensure that the thread reloads the hardware VFP state on the next use. */
 void vfp_flush_hwstate(struct thread_info *thread)
 {
 	unsigned int cpu = get_cpu();
 
-	/*
-	 * If the thread we're interested in is the current owner of the
-	 * hardware VFP state, then we need to save its state.
-	 */
-	if (vfp_current_hw_state[cpu] == &thread->vfpstate) {
-		u32 fpexc = fmrx(FPEXC);
+	vfp_force_reload(cpu, thread);
 
-		fmxr(FPEXC, fpexc & ~FPEXC_EN);
-
-		/*
-		 * Set the context to NULL to force a reload the next time
-		 * the thread uses the VFP.
-		 */
-		vfp_current_hw_state[cpu] = NULL;
-	}
-
-#ifdef CONFIG_SMP
-	/*
-	 * For SMP we still have to take care of the case where the thread
-	 * migrates to another CPU and then back to the original CPU on which
-	 * the last VFP user is still the same thread. Mark the thread VFP
-	 * state as belonging to a non-existent CPU so that the saved one will
-	 * be reloaded in the above case.
-	 */
-	thread->vfpstate.hard.cpu = NR_CPUS;
-#endif
 	put_cpu();
 }
 
@@ -519,8 +526,7 @@ static int vfp_hotplug(struct notifier_block *b, unsigned long action,
 	void *hcpu)
 {
 	if (action == CPU_DYING || action == CPU_DYING_FROZEN) {
-		unsigned int cpu = (long)hcpu;
-		vfp_current_hw_state[cpu] = NULL;
+		vfp_force_reload((long)hcpu, current_thread_info());
 	} else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
 		vfp_enable(NULL);
 	return NOTIFY_OK;

commit af61bdf035e2e4dd646b37b270bd558188a127c0
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jul 9 13:44:04 2011 +0100

    ARM: vfp: rename last_VFP_context to vfp_current_hw_state
    
    Rename the slightly confusing 'last_VFP_context' variable to be more
    descriptive of what it actually is.  This variable stores a pointer
    to the current owner's vfpstate structure for the context held in the
    VFP hardware.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f25e7ec89416..3640351171b8 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -33,7 +33,13 @@ void vfp_support_entry(void);
 void vfp_null_entry(void);
 
 void (*vfp_vector)(void) = vfp_null_entry;
-union vfp_state *last_VFP_context[NR_CPUS];
+
+/*
+ * The pointer to the vfpstate structure of the thread which currently
+ * owns the context held in the VFP hardware, or NULL if the hardware
+ * context is invalid.
+ */
+union vfp_state *vfp_current_hw_state[NR_CPUS];
 
 /*
  * Dual-use variable.
@@ -57,12 +63,12 @@ static void vfp_thread_flush(struct thread_info *thread)
 
 	/*
 	 * Disable VFP to ensure we initialize it first.  We must ensure
-	 * that the modification of last_VFP_context[] and hardware disable
+	 * that the modification of vfp_current_hw_state[] and hardware disable
 	 * are done for the same CPU and without preemption.
 	 */
 	cpu = get_cpu();
-	if (last_VFP_context[cpu] == vfp)
-		last_VFP_context[cpu] = NULL;
+	if (vfp_current_hw_state[cpu] == vfp)
+		vfp_current_hw_state[cpu] = NULL;
 	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
 	put_cpu();
 }
@@ -73,8 +79,8 @@ static void vfp_thread_exit(struct thread_info *thread)
 	union vfp_state *vfp = &thread->vfpstate;
 	unsigned int cpu = get_cpu();
 
-	if (last_VFP_context[cpu] == vfp)
-		last_VFP_context[cpu] = NULL;
+	if (vfp_current_hw_state[cpu] == vfp)
+		vfp_current_hw_state[cpu] = NULL;
 	put_cpu();
 }
 
@@ -129,9 +135,9 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * case the thread migrates to a different CPU. The
 		 * restoring is done lazily.
 		 */
-		if ((fpexc & FPEXC_EN) && last_VFP_context[cpu]) {
-			vfp_save_state(last_VFP_context[cpu], fpexc);
-			last_VFP_context[cpu]->hard.cpu = cpu;
+		if ((fpexc & FPEXC_EN) && vfp_current_hw_state[cpu]) {
+			vfp_save_state(vfp_current_hw_state[cpu], fpexc);
+			vfp_current_hw_state[cpu]->hard.cpu = cpu;
 		}
 		/*
 		 * Thread migration, just force the reloading of the
@@ -139,7 +145,7 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * contain stale data.
 		 */
 		if (thread->vfpstate.hard.cpu != cpu)
-			last_VFP_context[cpu] = NULL;
+			vfp_current_hw_state[cpu] = NULL;
 #endif
 
 		/*
@@ -415,7 +421,7 @@ static int vfp_pm_suspend(void)
 	}
 
 	/* clear any information we had about last context state */
-	memset(last_VFP_context, 0, sizeof(last_VFP_context));
+	memset(vfp_current_hw_state, 0, sizeof(vfp_current_hw_state));
 
 	return 0;
 }
@@ -451,7 +457,7 @@ void vfp_sync_hwstate(struct thread_info *thread)
 	 * If the thread we're interested in is the current owner of the
 	 * hardware VFP state, then we need to save its state.
 	 */
-	if (last_VFP_context[cpu] == &thread->vfpstate) {
+	if (vfp_current_hw_state[cpu] == &thread->vfpstate) {
 		u32 fpexc = fmrx(FPEXC);
 
 		/*
@@ -473,7 +479,7 @@ void vfp_flush_hwstate(struct thread_info *thread)
 	 * If the thread we're interested in is the current owner of the
 	 * hardware VFP state, then we need to save its state.
 	 */
-	if (last_VFP_context[cpu] == &thread->vfpstate) {
+	if (vfp_current_hw_state[cpu] == &thread->vfpstate) {
 		u32 fpexc = fmrx(FPEXC);
 
 		fmxr(FPEXC, fpexc & ~FPEXC_EN);
@@ -482,7 +488,7 @@ void vfp_flush_hwstate(struct thread_info *thread)
 		 * Set the context to NULL to force a reload the next time
 		 * the thread uses the VFP.
 		 */
-		last_VFP_context[cpu] = NULL;
+		vfp_current_hw_state[cpu] = NULL;
 	}
 
 #ifdef CONFIG_SMP
@@ -514,7 +520,7 @@ static int vfp_hotplug(struct notifier_block *b, unsigned long action,
 {
 	if (action == CPU_DYING || action == CPU_DYING_FROZEN) {
 		unsigned int cpu = (long)hcpu;
-		last_VFP_context[cpu] = NULL;
+		vfp_current_hw_state[cpu] = NULL;
 	} else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
 		vfp_enable(NULL);
 	return NOTIFY_OK;

commit 18b9dc130c33de2d1fd46bd668e67d0e1a544b16
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jun 8 11:42:43 2011 +0100

    ARM: vfp: add VFPv4 capability detection and populate elf_hwcap
    
    The presence of VFPv4 cannot be detected simply by looking at the FPSID
    subarchitecture field, as a value >= 2 signifies the architecture as
    VFPv3 or later.
    
    This patch reads from MVFR1 to check whether or not the fused multiply
    accumulate instructions are supported. Since these are introduced with
    VFPv4, this tells us what we need to know.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f25e7ec89416..650d90be0f9f 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -582,7 +582,6 @@ static int __init vfp_init(void)
 				elf_hwcap |= HWCAP_VFPv3D16;
 		}
 #endif
-#ifdef CONFIG_NEON
 		/*
 		 * Check for the presence of the Advanced SIMD
 		 * load/store instructions, integer and single
@@ -590,10 +589,13 @@ static int __init vfp_init(void)
 		 * for NEON if the hardware has the MVFR registers.
 		 */
 		if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {
+#ifdef CONFIG_NEON
 			if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
 				elf_hwcap |= HWCAP_NEON;
-		}
 #endif
+			if ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)
+				elf_hwcap |= HWCAP_VFPv4;
+		}
 	}
 	return 0;
 }

commit 328f5cc30290a92ea3ca62b2a63d2b9ebcb0d334
Author: Rafael J. Wysocki <rjw@sisk.pl>
Date:   Fri Apr 22 22:02:33 2011 +0200

    ARM: Use struct syscore_ops instead of sysdevs for PM in common code
    
    Convert some ARM architecture's common code to using
    struct syscore_ops objects for power management instead of sysdev
    classes and sysdevs.
    
    This simplifies the code and reduces the kernel's memory footprint.
    It also is necessary for removing sysdevs from the kernel entirely in
    the future.
    
    Signed-off-by: Rafael J. Wysocki <rjw@sisk.pl>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f74695075e64..f25e7ec89416 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -398,9 +398,9 @@ static void vfp_enable(void *unused)
 }
 
 #ifdef CONFIG_PM
-#include <linux/sysdev.h>
+#include <linux/syscore_ops.h>
 
-static int vfp_pm_suspend(struct sys_device *dev, pm_message_t state)
+static int vfp_pm_suspend(void)
 {
 	struct thread_info *ti = current_thread_info();
 	u32 fpexc = fmrx(FPEXC);
@@ -420,34 +420,25 @@ static int vfp_pm_suspend(struct sys_device *dev, pm_message_t state)
 	return 0;
 }
 
-static int vfp_pm_resume(struct sys_device *dev)
+static void vfp_pm_resume(void)
 {
 	/* ensure we have access to the vfp */
 	vfp_enable(NULL);
 
 	/* and disable it to ensure the next usage restores the state */
 	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
-
-	return 0;
 }
 
-static struct sysdev_class vfp_pm_sysclass = {
-	.name		= "vfp",
+static struct syscore_ops vfp_pm_syscore_ops = {
 	.suspend	= vfp_pm_suspend,
 	.resume		= vfp_pm_resume,
 };
 
-static struct sys_device vfp_pm_sysdev = {
-	.cls	= &vfp_pm_sysclass,
-};
-
 static void vfp_pm_init(void)
 {
-	sysdev_class_register(&vfp_pm_sysclass);
-	sysdev_register(&vfp_pm_sysdev);
+	register_syscore_ops(&vfp_pm_syscore_ops);
 }
 
-
 #else
 static inline void vfp_pm_init(void) { }
 #endif /* CONFIG_PM */

commit c98c09773d80db93cae349f0496fef109feab54d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 6 16:17:17 2011 +0100

    ARM: 6868/1: Preserve the VFP state during fork
    
    VFP registers d16-d31 are callee saved registers and must be preserved
    during function calls, including fork(). The VFP configuration should
    also be preserved. The patch copies the full VFP state to the child
    process.
    
    Reported-by: Paul Wright <paul.wright@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 1c88bbdfd34d..f74695075e64 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -78,6 +78,14 @@ static void vfp_thread_exit(struct thread_info *thread)
 	put_cpu();
 }
 
+static void vfp_thread_copy(struct thread_info *thread)
+{
+	struct thread_info *parent = current_thread_info();
+
+	vfp_sync_hwstate(parent);
+	thread->vfpstate = parent->vfpstate;
+}
+
 /*
  * When this function is called with the following 'cmd's, the following
  * is true while this function is being run:
@@ -148,6 +156,10 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 	case THREAD_NOTIFY_EXIT:
 		vfp_thread_exit(thread);
 		break;
+
+	case THREAD_NOTIFY_COPY:
+		vfp_thread_copy(thread);
+		break;
 	}
 
 	return NOTIFY_DONE;

commit 2e82669acf03e5bf2080f5d3ef005168e67d8a51
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 6 16:16:29 2011 +0100

    ARM: 6867/1: Introduce THREAD_NOTIFY_COPY for copy_thread() hooks
    
    This patch adds THREAD_NOTIFY_COPY for calling registered handlers
    during the copy_thread() function call. It also changes the VFP handler
    to use a switch statement rather than if..else and ignore this event.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index bbf3da012afd..1c88bbdfd34d 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -104,12 +104,17 @@ static void vfp_thread_exit(struct thread_info *thread)
 static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
 	struct thread_info *thread = v;
+	u32 fpexc;
+#ifdef CONFIG_SMP
+	unsigned int cpu;
+#endif
 
-	if (likely(cmd == THREAD_NOTIFY_SWITCH)) {
-		u32 fpexc = fmrx(FPEXC);
+	switch (cmd) {
+	case THREAD_NOTIFY_SWITCH:
+		fpexc = fmrx(FPEXC);
 
 #ifdef CONFIG_SMP
-		unsigned int cpu = thread->cpu;
+		cpu = thread->cpu;
 
 		/*
 		 * On SMP, if VFP is enabled, save the old state in
@@ -134,13 +139,16 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * old state.
 		 */
 		fmxr(FPEXC, fpexc & ~FPEXC_EN);
-		return NOTIFY_DONE;
-	}
+		break;
 
-	if (cmd == THREAD_NOTIFY_FLUSH)
+	case THREAD_NOTIFY_FLUSH:
 		vfp_thread_flush(thread);
-	else
+		break;
+
+	case THREAD_NOTIFY_EXIT:
 		vfp_thread_exit(thread);
+		break;
+	}
 
 	return NOTIFY_DONE;
 }

commit 74c25beeb30fa32badf575a908902cbdef4d4eb4
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jan 31 14:43:25 2011 +0000

    ARM: vfp: improve commentry for hotplug events
    
    Improve the documentation for the VFP hotplug notifier handler, so
    that people better understand what's going on there and what has
    been done for them.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 25b89d817105..bbf3da012afd 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -489,8 +489,11 @@ void vfp_flush_hwstate(struct thread_info *thread)
 
 /*
  * VFP hardware can lose all context when a CPU goes offline.
- * Safely clear our held state when a CPU has been killed, and
- * re-enable access to VFP when the CPU comes back online.
+ * As we will be running in SMP mode with CPU hotplug, we will save the
+ * hardware state at every thread switch.  We clear our held state when
+ * a CPU has been killed, indicating that the VFP hardware doesn't contain
+ * a threads VFP state.  When a CPU starts up, we re-enable access to the
+ * VFP hardware.
  *
  * Both CPU_DYING and CPU_STARTING are called on the CPU which
  * is being offlined/onlined.

commit 2bbd7e9b74271b2d6a14b4840fc44afbea83774d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jan 8 12:05:09 2011 +0000

    ARM: fix some sparse errors in generic ARM code
    
    arch/arm/kernel/return_address.c:37:6: warning: symbol 'return_address' was not declared. Should it be static?
    arch/arm/kernel/setup.c:76:14: warning: symbol 'processor_id' was not declared. Should it be static?
    arch/arm/kernel/traps.c:259:1: warning: symbol 'die_lock' was not declared. Should it be static?
    arch/arm/vfp/vfpmodule.c:156:6: warning: symbol 'vfp_raise_sigfpe' was not declared. Should it be static?
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 0797cb528b46..25b89d817105 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -153,7 +153,7 @@ static struct notifier_block vfp_notifier_block = {
  * Raise a SIGFPE for the current process.
  * sicode describes the signal being raised.
  */
-void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
+static void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 {
 	siginfo_t info;
 

commit 90b44199e83ec780d6def11a602d825dc68438e3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Dec 18 10:59:49 2010 +0000

    ARM: VFP: re-initialize VFP coprocessor access enables on CPU hotplug
    
    We can not guarantee that VFP will be enabled when CPU hotplug brings
    a CPU back online from a reset state.  Add a hotplug CPU notifier to
    ensure that the VFP coprocessor access is enabled whenever a CPU comes
    back online.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 8063a322c790..0797cb528b46 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -10,9 +10,12 @@
  */
 #include <linux/module.h>
 #include <linux/types.h>
+#include <linux/cpu.h>
 #include <linux/kernel.h>
+#include <linux/notifier.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
+#include <linux/smp.h>
 #include <linux/init.h>
 
 #include <asm/cputype.h>
@@ -484,7 +487,24 @@ void vfp_flush_hwstate(struct thread_info *thread)
 	put_cpu();
 }
 
-#include <linux/smp.h>
+/*
+ * VFP hardware can lose all context when a CPU goes offline.
+ * Safely clear our held state when a CPU has been killed, and
+ * re-enable access to VFP when the CPU comes back online.
+ *
+ * Both CPU_DYING and CPU_STARTING are called on the CPU which
+ * is being offlined/onlined.
+ */
+static int vfp_hotplug(struct notifier_block *b, unsigned long action,
+	void *hcpu)
+{
+	if (action == CPU_DYING || action == CPU_DYING_FROZEN) {
+		unsigned int cpu = (long)hcpu;
+		last_VFP_context[cpu] = NULL;
+	} else if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)
+		vfp_enable(NULL);
+	return NOTIFY_OK;
+}
 
 /*
  * VFP support code initialisation.
@@ -514,6 +534,8 @@ static int __init vfp_init(void)
 	else if (vfpsid & FPSID_NODOUBLE) {
 		printk("no double precision support\n");
 	} else {
+		hotcpu_notifier(vfp_hotplug, 0);
+
 		smp_call_function(vfp_enable, NULL, 1);
 
 		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */

commit 5aaf254409f8d58229107b59507a8235b715a960
Author: Tony Lindgren <tony@atomide.com>
Date:   Thu Jul 1 13:41:05 2010 +0100

    ARM: 6203/1: Make VFPv3 usable on ARMv6
    
    MVFR0 and MVFR1 are only available starting with ARM1136 r1p0 release
    according to "B.5 VFP changes" in DDI0211F_arm1136_r1p0_trm.pdf. This is
    also when TLS register got added, so we can use HAS_TLS also to test for
    MVFR0 and MVFR1.
    
    Otherwise VFPFMRX and VFPFMXR access fails and we get:
    
    Internal error: Oops - undefined instruction: 0 [#1]
    PC is at no_old_VFP_process+0x8/0x3c
    LR is at __und_svc+0x48/0x80
    ...
    
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 315a540c7ce5..8063a322c790 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -15,6 +15,7 @@
 #include <linux/sched.h>
 #include <linux/init.h>
 
+#include <asm/cputype.h>
 #include <asm/thread_notify.h>
 #include <asm/vfp.h>
 
@@ -549,10 +550,13 @@ static int __init vfp_init(void)
 		/*
 		 * Check for the presence of the Advanced SIMD
 		 * load/store instructions, integer and single
-		 * precision floating point operations.
+		 * precision floating point operations. Only check
+		 * for NEON if the hardware has the MVFR registers.
 		 */
-		if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
-			elf_hwcap |= HWCAP_NEON;
+		if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {
+			if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
+				elf_hwcap |= HWCAP_NEON;
+		}
 #endif
 	}
 	return 0;

commit 5c5cac63851f347d8308d69f1892c4af51d7c1a4
Author: Imre Deak <imre.deak@nokia.com>
Date:   Sun Apr 11 15:57:07 2010 +0100

    ARM: 6050/1: VFP: fix the SMP versions of vfp_{sync,flush}_hwstate
    
    From: Imre Deak <imre.deak@nokia.com>
    
    Recently the UP versions of these functions were refactored and as
    a side effect it became possible to call them for the current thread.
    This isn't true for the SMP versions however, so fix this up.
    
    Signed-off-by: Imre Deak <imre.deak@nokia.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index a420cb949328..315a540c7ce5 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -428,26 +428,6 @@ static void vfp_pm_init(void)
 static inline void vfp_pm_init(void) { }
 #endif /* CONFIG_PM */
 
-/*
- * Synchronise the hardware VFP state of a thread other than current with the
- * saved one. This function is used by the ptrace mechanism.
- */
-#ifdef CONFIG_SMP
-void vfp_sync_hwstate(struct thread_info *thread)
-{
-}
-
-void vfp_flush_hwstate(struct thread_info *thread)
-{
-	/*
-	 * On SMP systems, the VFP state is automatically saved at every
-	 * context switch. We mark the thread VFP state as belonging to a
-	 * non-existent CPU so that the saved one will be reloaded when
-	 * needed.
-	 */
-	thread->vfpstate.hard.cpu = NR_CPUS;
-}
-#else
 void vfp_sync_hwstate(struct thread_info *thread)
 {
 	unsigned int cpu = get_cpu();
@@ -490,9 +470,18 @@ void vfp_flush_hwstate(struct thread_info *thread)
 		last_VFP_context[cpu] = NULL;
 	}
 
+#ifdef CONFIG_SMP
+	/*
+	 * For SMP we still have to take care of the case where the thread
+	 * migrates to another CPU and then back to the original CPU on which
+	 * the last VFP user is still the same thread. Mark the thread VFP
+	 * state as belonging to a non-existent CPU so that the saved one will
+	 * be reloaded in the above case.
+	 */
+	thread->vfpstate.hard.cpu = NR_CPUS;
+#endif
 	put_cpu();
 }
-#endif
 
 #include <linux/smp.h>
 

commit 325ffc3633f1c30ef89d98d619f7e1497366e77c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Mar 26 15:44:57 2010 +0100

    ARM: 5997/1: ARM: Correct the VFPv3 detection
    
    A CPU has VFPv3 hardware if the FPSID[19:16] bits are 2 or more.
    Currently Linux was only checking for 3 or more.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 7f3f59fcaa21..a420cb949328 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -545,7 +545,7 @@ static int __init vfp_init(void)
 		 */
 		elf_hwcap |= HWCAP_VFP;
 #ifdef CONFIG_VFPv3
-		if (VFP_arch >= 3) {
+		if (VFP_arch >= 2) {
 			elf_hwcap |= HWCAP_VFPv3;
 
 			/*

commit ac0f6f927db539e03e1f3f61bcd4ed57d5cde7a9
Merge: b1bf9368407a 9f33be2c3a80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 1 09:15:15 2010 -0800

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm: (100 commits)
      ARM: Eliminate decompressor -Dstatic= PIC hack
      ARM: 5958/1: ARM: U300: fix inverted clk round rate
      ARM: 5956/1: misplaced parentheses
      ARM: 5955/1: ep93xx: move timer defines into core.c and document
      ARM: 5954/1: ep93xx: move gpio interrupt support to gpio.c
      ARM: 5953/1: ep93xx: fix broken build of clock.c
      ARM: 5952/1: ARM: MM: Add ARM_L1_CACHE_SHIFT_6 for handle inside each ARCH Kconfig
      ARM: 5949/1: NUC900 add gpio virtual memory map
      ARM: 5948/1: Enable timer0 to time4 clock support for nuc910
      ARM: 5940/2: ARM: MMCI: remove custom DBG macro and printk
      ARM: make_coherent(): fix problems with highpte, part 2
      MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself
      ARM: 5945/1: ep93xx: include correct irq.h in core.c
      ARM: 5933/1: amba-pl011: support hardware flow control
      ARM: 5930/1: Add PKMAP area description to memory.txt.
      ARM: 5929/1: Add checks to detect overlap of memory regions.
      ARM: 5928/1: Change type of VMALLOC_END to unsigned long.
      ARM: 5927/1: Make delimiters of DMA area globally visibly.
      ARM: 5926/1: Add "Virtual kernel memory..." printout.
      ARM: 5920/1: OMAP4: Enable L2 Cache
      ...
    
    Fix up trivial conflict in arch/arm/mach-mx25/clock.c

commit ad187f956108e1c56b444706212bf08d84c0bee0
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Feb 6 11:36:23 2010 +0000

    ARM: vfp ptrace: no point flushing hw context for PTRACE_GETVFPREGS
    
    If we're only reading the VFP context via the ptrace call, there's
    no need to invalidate the hardware context - we only need to do that
    on PTRACE_SETVFPREGS.  This allows more efficient monitoring of a
    traced task.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 86a57aeeda4a..def19f83d812 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -430,7 +430,11 @@ static inline void vfp_pm_init(void) { }
  * saved one. This function is used by the ptrace mechanism.
  */
 #ifdef CONFIG_SMP
-void vfp_sync_state(struct thread_info *thread)
+void vfp_sync_hwstate(struct thread_info *thread)
+{
+}
+
+void vfp_flush_hwstate(struct thread_info *thread)
 {
 	/*
 	 * On SMP systems, the VFP state is automatically saved at every
@@ -441,7 +445,7 @@ void vfp_sync_state(struct thread_info *thread)
 	thread->vfpstate.hard.cpu = NR_CPUS;
 }
 #else
-void vfp_sync_state(struct thread_info *thread)
+void vfp_sync_hwstate(struct thread_info *thread)
 {
 	unsigned int cpu = get_cpu();
 
@@ -457,6 +461,23 @@ void vfp_sync_state(struct thread_info *thread)
 		 */
 		fmxr(FPEXC, fpexc | FPEXC_EN);
 		vfp_save_state(&thread->vfpstate, fpexc | FPEXC_EN);
+		fmxr(FPEXC, fpexc);
+	}
+
+	put_cpu();
+}
+
+void vfp_flush_hwstate(struct thread_info *thread)
+{
+	unsigned int cpu = get_cpu();
+
+	/*
+	 * If the thread we're interested in is the current owner of the
+	 * hardware VFP state, then we need to save its state.
+	 */
+	if (last_VFP_context[cpu] == &thread->vfpstate) {
+		u32 fpexc = fmrx(FPEXC);
+
 		fmxr(FPEXC, fpexc & ~FPEXC_EN);
 
 		/*

commit 54cb3dbb4a1cccd6b1bffc169ff638f8e74daf44
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Feb 6 11:27:45 2010 +0000

    ARM: vfp: fix vfp_sync_state()
    
    The more I look at vfp_sync_state(), the more I believe it's trying
    to do its job in a really obscure way.
    
    Essentially, last_VFP_context[] tracks who owns the state in the VFP
    hardware.  If last_VFP_context[] is the context for the thread which
    we're interested in, then the VFP hardware has context which is not
    saved in the software state - so we need to bring the software state
    up to date.
    
    If last_VFP_context[] is for some other thread, we really don't care
    what state the VFP hardware is in; it doesn't contain any information
    pertinent to the thread we're trying to deal with - so don't touch
    the hardware.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f60a5400a25b..86a57aeeda4a 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -444,32 +444,28 @@ void vfp_sync_state(struct thread_info *thread)
 void vfp_sync_state(struct thread_info *thread)
 {
 	unsigned int cpu = get_cpu();
-	u32 fpexc = fmrx(FPEXC);
 
 	/*
-	 * If VFP is enabled, the previous state was already saved and
-	 * last_VFP_context updated.
+	 * If the thread we're interested in is the current owner of the
+	 * hardware VFP state, then we need to save its state.
 	 */
-	if (fpexc & FPEXC_EN)
-		goto out;
-
-	if (!last_VFP_context[cpu])
-		goto out;
+	if (last_VFP_context[cpu] == &thread->vfpstate) {
+		u32 fpexc = fmrx(FPEXC);
 
-	/*
-	 * Save the last VFP state on this CPU.
-	 */
-	fmxr(FPEXC, fpexc | FPEXC_EN);
-	vfp_save_state(last_VFP_context[cpu], fpexc);
-	fmxr(FPEXC, fpexc);
+		/*
+		 * Save the last VFP state on this CPU.
+		 */
+		fmxr(FPEXC, fpexc | FPEXC_EN);
+		vfp_save_state(&thread->vfpstate, fpexc | FPEXC_EN);
+		fmxr(FPEXC, fpexc & ~FPEXC_EN);
 
-	/*
-	 * Set the context to NULL to force a reload the next time the thread
-	 * uses the VFP.
-	 */
-	last_VFP_context[cpu] = NULL;
+		/*
+		 * Set the context to NULL to force a reload the next time
+		 * the thread uses the VFP.
+		 */
+		last_VFP_context[cpu] = NULL;
+	}
 
-out:
 	put_cpu();
 }
 #endif

commit dbead405801c8d5aa1bc21ab6e2a47f060d47c06
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Feb 1 18:50:40 2010 +0100

    ARM: 5909/1: ARM: Correct the FPSCR bits setting when raising exceptions
    
    Commit c98929c07a removed the clearing of the FPSCR[31:28] bits from the
    vfp_raise_exceptions() function and the new bits are or'ed with the old
    FPSCR bits leading to unexpected results (the original commit was
    referring to the cumulative bits - FPSCR[4:0]).
    
    Reported-by: Tom Hameenanttila <tmhameen@marvell.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f60a5400a25b..a63c4be99b36 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -197,10 +197,13 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 	}
 
 	/*
-	 * Update the FPSCR with the additional exception flags.
+	 * If any of the status flags are set, update the FPSCR.
 	 * Comparison instructions always return at least one of
 	 * these flags set.
 	 */
+	if (exceptions & (FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V))
+		fpscr &= ~(FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V);
+
 	fpscr |= exceptions;
 
 	fmxr(FPSCR, fpscr);

commit 797245f5da543074ee7db0e0516da744c89aa17f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Dec 18 14:34:43 2009 +0000

    ARM: Convert VFP/Crunch/XscaleCP thread_release() to exit_thread()
    
    This avoids races in the VFP code where the dead thread may have
    state on another CPU.  By moving this code to exit_thread(), we
    will be running as the thread, and therefore be running on the
    current CPU.
    
    This means that we can ensure that the only local state is accessed
    in the thread notifiers.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index aed05bc3c2ea..f60a5400a25b 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -63,14 +63,15 @@ static void vfp_thread_flush(struct thread_info *thread)
 	put_cpu();
 }
 
-static void vfp_thread_release(struct thread_info *thread)
+static void vfp_thread_exit(struct thread_info *thread)
 {
 	/* release case: Per-thread VFP cleanup. */
 	union vfp_state *vfp = &thread->vfpstate;
-	unsigned int cpu = thread->cpu;
+	unsigned int cpu = get_cpu();
 
 	if (last_VFP_context[cpu] == vfp)
 		last_VFP_context[cpu] = NULL;
+	put_cpu();
 }
 
 /*
@@ -88,11 +89,13 @@ static void vfp_thread_release(struct thread_info *thread)
  *	but may change at any time.
  *   - we could be preempted if tree preempt rcu is enabled, so
  *	it is unsafe to use thread->cpu.
- *  THREAD_NOTIFY_RELEASE:
- *   - the thread (v) will not be running on any CPU; it is a dead thread.
- *   - thread->cpu will be the last CPU the thread ran on, which may not
- *	be the current CPU.
- *   - we could be preempted if tree preempt rcu is enabled.
+ *  THREAD_NOTIFY_EXIT
+ *   - the thread (v) will be running on the local CPU, so
+ *	v === current_thread_info()
+ *   - thread->cpu is the local CPU number at the time it is accessed,
+ *	but may change at any time.
+ *   - we could be preempted if tree preempt rcu is enabled, so
+ *	it is unsafe to use thread->cpu.
  */
 static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
@@ -133,7 +136,7 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 	if (cmd == THREAD_NOTIFY_FLUSH)
 		vfp_thread_flush(thread);
 	else
-		vfp_thread_release(thread);
+		vfp_thread_exit(thread);
 
 	return NOTIFY_DONE;
 }

commit 0d782dc430d94dc36b47cb11c2e33ecb1bb38234
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Dec 12 14:47:40 2009 +0000

    ARM: VFP: fix vfp thread init bug and document vfp notifier entry conditions
    
    When the VFP notifier is called for flush_thread(), we may be
    preemptible, meaning we might migrate to another CPU, which means
    referencing the current CPU number without some form of locking is
    invalid, and can cause data corruption.
    
    For the most cases, this isn't a problem since atomic notifiers are run
    under rcu lock, which for most configurations results in preemption
    being disabled - except when the preemptable tree-based rcu
    implementation is selected.
    
    Let's make it safe anyway.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 2d7423af1197..aed05bc3c2ea 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -38,16 +38,72 @@ union vfp_state *last_VFP_context[NR_CPUS];
  */
 unsigned int VFP_arch;
 
+/*
+ * Per-thread VFP initialization.
+ */
+static void vfp_thread_flush(struct thread_info *thread)
+{
+	union vfp_state *vfp = &thread->vfpstate;
+	unsigned int cpu;
+
+	memset(vfp, 0, sizeof(union vfp_state));
+
+	vfp->hard.fpexc = FPEXC_EN;
+	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
+
+	/*
+	 * Disable VFP to ensure we initialize it first.  We must ensure
+	 * that the modification of last_VFP_context[] and hardware disable
+	 * are done for the same CPU and without preemption.
+	 */
+	cpu = get_cpu();
+	if (last_VFP_context[cpu] == vfp)
+		last_VFP_context[cpu] = NULL;
+	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+	put_cpu();
+}
+
+static void vfp_thread_release(struct thread_info *thread)
+{
+	/* release case: Per-thread VFP cleanup. */
+	union vfp_state *vfp = &thread->vfpstate;
+	unsigned int cpu = thread->cpu;
+
+	if (last_VFP_context[cpu] == vfp)
+		last_VFP_context[cpu] = NULL;
+}
+
+/*
+ * When this function is called with the following 'cmd's, the following
+ * is true while this function is being run:
+ *  THREAD_NOFTIFY_SWTICH:
+ *   - the previously running thread will not be scheduled onto another CPU.
+ *   - the next thread to be run (v) will not be running on another CPU.
+ *   - thread->cpu is the local CPU number
+ *   - not preemptible as we're called in the middle of a thread switch
+ *  THREAD_NOTIFY_FLUSH:
+ *   - the thread (v) will be running on the local CPU, so
+ *	v === current_thread_info()
+ *   - thread->cpu is the local CPU number at the time it is accessed,
+ *	but may change at any time.
+ *   - we could be preempted if tree preempt rcu is enabled, so
+ *	it is unsafe to use thread->cpu.
+ *  THREAD_NOTIFY_RELEASE:
+ *   - the thread (v) will not be running on any CPU; it is a dead thread.
+ *   - thread->cpu will be the last CPU the thread ran on, which may not
+ *	be the current CPU.
+ *   - we could be preempted if tree preempt rcu is enabled.
+ */
 static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
 	struct thread_info *thread = v;
-	union vfp_state *vfp;
-	__u32 cpu = thread->cpu;
 
 	if (likely(cmd == THREAD_NOTIFY_SWITCH)) {
 		u32 fpexc = fmrx(FPEXC);
 
 #ifdef CONFIG_SMP
+		unsigned int cpu = thread->cpu;
+
 		/*
 		 * On SMP, if VFP is enabled, save the old state in
 		 * case the thread migrates to a different CPU. The
@@ -74,25 +130,10 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		return NOTIFY_DONE;
 	}
 
-	vfp = &thread->vfpstate;
-	if (cmd == THREAD_NOTIFY_FLUSH) {
-		/*
-		 * Per-thread VFP initialisation.
-		 */
-		memset(vfp, 0, sizeof(union vfp_state));
-
-		vfp->hard.fpexc = FPEXC_EN;
-		vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
-
-		/*
-		 * Disable VFP to ensure we initialise it first.
-		 */
-		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
-	}
-
-	/* flush and release case: Per-thread VFP cleanup. */
-	if (last_VFP_context[cpu] == vfp)
-		last_VFP_context[cpu] = NULL;
+	if (cmd == THREAD_NOTIFY_FLUSH)
+		vfp_thread_flush(thread);
+	else
+		vfp_thread_release(thread);
 
 	return NOTIFY_DONE;
 }

commit 85d6943af50537d3aec58b967ffbd3fec88453e9
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sat May 30 14:00:18 2009 +0100

    Fix the VFP handling on the Feroceon CPU
    
    This CPU generates synchronous VFP exceptions in a non-standard way -
    the FPEXC.EX bit set but without the FPSCR.IXE bit being set like in the
    VFP subarchitecture 1 or just the FPEXC.DEX bit like in VFP
    subarchitecture 2. The main problem is that the faulty instruction
    (which needs to be emulated in software) will be restarted several times
    (normally until a context switch disables the VFP). This patch ensures
    that the VFP exception is treated as synchronous.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Nicolas Pitre <nico@cam.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 01599c4ef726..2d7423af1197 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -253,12 +253,14 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	}
 
 	if (fpexc & FPEXC_EX) {
+#ifndef CONFIG_CPU_FEROCEON
 		/*
 		 * Asynchronous exception. The instruction is read from FPINST
 		 * and the interrupted instruction has to be restarted.
 		 */
 		trigger = fmrx(FPINST);
 		regs->ARM_pc -= 4;
+#endif
 	} else if (!(fpexc & FPEXC_DEX)) {
 		/*
 		 * Illegal combination of bits. It can be caused by an

commit f2255be8126e86142901c61dd482c1c2a5ffdda7
Author: George G. Davis <gdavis@mvista.com>
Date:   Wed Apr 1 20:27:18 2009 +0100

    [ARM] 5440/1: Fix VFP state corruption due to preemption during VFP exceptions
    
    We've observed that ARM VFP state can be corrupted during VFP exception
    handling when PREEMPT is enabled.  The exact conditions are difficult
    to reproduce but appear to occur during VFP exception handling when a
    task causes a VFP exception which is handled via VFP_bounce and is then
    preempted by yet another task which in turn causes yet another VFP
    exception.  Since the VFP_bounce code is not preempt safe, VFP state then
    becomes corrupt.  In order to prevent preemption from occuring while
    handling a VFP exception, this patch disables preemption while handling
    VFP exceptions.
    
    Signed-off-by: George G. Davis <gdavis@mvista.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 75457b30d813..01599c4ef726 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -266,7 +266,7 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 		 * on VFP subarch 1.
 		 */
 		 vfp_raise_exceptions(VFP_EXCEPTION_ERROR, trigger, fpscr, regs);
-		 return;
+		goto exit;
 	}
 
 	/*
@@ -297,7 +297,7 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	 * the FPEXC.FP2V bit is valid only if FPEXC.EX is 1.
 	 */
 	if (fpexc ^ (FPEXC_EX | FPEXC_FP2V))
-		return;
+		goto exit;
 
 	/*
 	 * The barrier() here prevents fpinst2 being read
@@ -310,6 +310,8 @@ void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	exceptions = vfp_emulate_instruction(trigger, orig_fpscr, regs);
 	if (exceptions)
 		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
+ exit:
+	preempt_enable();
 }
 
 static void vfp_enable(void *unused)

commit 7279dc3e914635ab4b288ec39383272a06c466f3
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Feb 11 13:13:56 2009 +0100

    [ARM] 5388/1: Add hwcap bits for VFPv3 and VFPv3D16
    
    The VFPv3D16 is a VFPv3 CPU configuration where only 16 double registers
    are present, as the VFPv2 configuration. This patch adds the
    corresponding hwcap bits so that applications or debuggers have more
    information about the supported features.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 7e1239041b39..75457b30d813 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -476,6 +476,18 @@ static int __init vfp_init(void)
 		 * in place; report VFP support to userspace.
 		 */
 		elf_hwcap |= HWCAP_VFP;
+#ifdef CONFIG_VFPv3
+		if (VFP_arch >= 3) {
+			elf_hwcap |= HWCAP_VFPv3;
+
+			/*
+			 * Check for VFPv3 D16. CPUs in this configuration
+			 * only have 16 x 64bit registers.
+			 */
+			if (((fmrx(MVFR0) & MVFR0_A_SIMD_MASK)) == 1)
+				elf_hwcap |= HWCAP_VFPv3D16;
+		}
+#endif
 #ifdef CONFIG_NEON
 		/*
 		 * Check for the presence of the Advanced SIMD

commit 3d1228ead618b88e8606015cbabc49019981805d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Feb 11 13:12:56 2009 +0100

    [ARM] 5387/1: Add ptrace VFP support on ARM
    
    This patch adds ptrace support for setting and getting the VFP registers
    using PTRACE_SETVFPREGS and PTRACE_GETVFPREGS. The user_vfp structure
    defined in asm/user.h contains 32 double registers (to cover VFPv3 and
    Neon hardware) and the FPSCR register.
    
    Cc: Paul Brook <paul@codesourcery.com>
    Cc: Daniel Jacobowitz <dan@codesourcery.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 9f476a1be2ca..7e1239041b39 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -377,6 +377,55 @@ static void vfp_pm_init(void)
 static inline void vfp_pm_init(void) { }
 #endif /* CONFIG_PM */
 
+/*
+ * Synchronise the hardware VFP state of a thread other than current with the
+ * saved one. This function is used by the ptrace mechanism.
+ */
+#ifdef CONFIG_SMP
+void vfp_sync_state(struct thread_info *thread)
+{
+	/*
+	 * On SMP systems, the VFP state is automatically saved at every
+	 * context switch. We mark the thread VFP state as belonging to a
+	 * non-existent CPU so that the saved one will be reloaded when
+	 * needed.
+	 */
+	thread->vfpstate.hard.cpu = NR_CPUS;
+}
+#else
+void vfp_sync_state(struct thread_info *thread)
+{
+	unsigned int cpu = get_cpu();
+	u32 fpexc = fmrx(FPEXC);
+
+	/*
+	 * If VFP is enabled, the previous state was already saved and
+	 * last_VFP_context updated.
+	 */
+	if (fpexc & FPEXC_EN)
+		goto out;
+
+	if (!last_VFP_context[cpu])
+		goto out;
+
+	/*
+	 * Save the last VFP state on this CPU.
+	 */
+	fmxr(FPEXC, fpexc | FPEXC_EN);
+	vfp_save_state(last_VFP_context[cpu], fpexc);
+	fmxr(FPEXC, fpexc);
+
+	/*
+	 * Set the context to NULL to force a reload the next time the thread
+	 * uses the VFP.
+	 */
+	last_VFP_context[cpu] = NULL;
+
+out:
+	put_cpu();
+}
+#endif
+
 #include <linux/smp.h>
 
 /*

commit fc0b7a20368193e1f7cf1376624eed419b9c33e6
Author: Ben Dooks <ben-linux@fluff.org>
Date:   Thu Dec 18 12:26:54 2008 +0100

    [ARM] 5349/1: VFP: Add PM code to save and restore current VFP state
    
    When CONFIG_PM is selected, the VFP code does not have any handler
    installed to deal with either saving the VFP state of the current
    task, nor does it do anything to try and restore the VFP after a
    resume.
    
    On resume, the VFP will have been reset and the co-processor access
    control registers are in an indeterminate state (very probably the
    CP10 and CP11 the VFP uses will have been disabled by the ARM core
    reset). When this happens, resume will break as soon as it tries to
    unfreeze the tasks and restart scheduling.
    
    Add a sys device to allow us to hook the suspend call to save the
    current thread state if the thread is using VFP and a resume hook
    which restores the CP10/CP11 access and ensures the VFP is disabled
    so that the lazy swapping will take place on next access.
    
    Signed-off-by: Ben Dooks <ben-linux@fluff.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 67ca340a7c85..9f476a1be2ca 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -322,6 +322,61 @@ static void vfp_enable(void *unused)
 	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
 }
 
+#ifdef CONFIG_PM
+#include <linux/sysdev.h>
+
+static int vfp_pm_suspend(struct sys_device *dev, pm_message_t state)
+{
+	struct thread_info *ti = current_thread_info();
+	u32 fpexc = fmrx(FPEXC);
+
+	/* if vfp is on, then save state for resumption */
+	if (fpexc & FPEXC_EN) {
+		printk(KERN_DEBUG "%s: saving vfp state\n", __func__);
+		vfp_save_state(&ti->vfpstate, fpexc);
+
+		/* disable, just in case */
+		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+	}
+
+	/* clear any information we had about last context state */
+	memset(last_VFP_context, 0, sizeof(last_VFP_context));
+
+	return 0;
+}
+
+static int vfp_pm_resume(struct sys_device *dev)
+{
+	/* ensure we have access to the vfp */
+	vfp_enable(NULL);
+
+	/* and disable it to ensure the next usage restores the state */
+	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
+
+	return 0;
+}
+
+static struct sysdev_class vfp_pm_sysclass = {
+	.name		= "vfp",
+	.suspend	= vfp_pm_suspend,
+	.resume		= vfp_pm_resume,
+};
+
+static struct sys_device vfp_pm_sysdev = {
+	.cls	= &vfp_pm_sysclass,
+};
+
+static void vfp_pm_init(void)
+{
+	sysdev_class_register(&vfp_pm_sysclass);
+	sysdev_register(&vfp_pm_sysdev);
+}
+
+
+#else
+static inline void vfp_pm_init(void) { }
+#endif /* CONFIG_PM */
+
 #include <linux/smp.h>
 
 /*
@@ -365,6 +420,7 @@ static int __init vfp_init(void)
 		vfp_vector = vfp_support_entry;
 
 		thread_register_notifier(&vfp_notifier_block);
+		vfp_pm_init();
 
 		/*
 		 * We detected VFP, and the support code is

commit 2bedbdf4148ebbe48c7a89449ab52e475a788f42
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 6 13:23:07 2008 +0000

    Add HWCAP_NEON to the ARM hwcap.h file
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index c0d2c9bb952b..67ca340a7c85 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -371,6 +371,15 @@ static int __init vfp_init(void)
 		 * in place; report VFP support to userspace.
 		 */
 		elf_hwcap |= HWCAP_VFP;
+#ifdef CONFIG_NEON
+		/*
+		 * Check for the presence of the Advanced SIMD
+		 * load/store instructions, integer and single
+		 * precision floating point operations.
+		 */
+		if ((fmrx(MVFR1) & 0x000fff00) == 0x00011100)
+			elf_hwcap |= HWCAP_NEON;
+#endif
 	}
 	return 0;
 }

commit 8691e5a8f691cc2a4fda0651e8d307aaba0e7d68
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Fri Jun 6 11:18:06 2008 +0200

    smp_call_function: get rid of the unused nonatomic/retry argument
    
    It's never used and the comments refer to nonatomic and retry
    interchangably. So get rid of it.
    
    Acked-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 32455c633f1c..c0d2c9bb952b 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -352,7 +352,7 @@ static int __init vfp_init(void)
 	else if (vfpsid & FPSID_NODOUBLE) {
 		printk("no double precision support\n");
 	} else {
-		smp_call_function(vfp_enable, NULL, 1, 1);
+		smp_call_function(vfp_enable, NULL, 1);
 
 		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */
 		printk("implementor %02x architecture %d part %02x variant %x rev %x\n",

commit c98929c07a01c9ec2e1e5253456acc7168da8b66
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 22 18:32:01 2007 +0100

    [ARM] 4582/2: Add support for the common VFP subarchitecture
    
    This patch allows the VFP support code to run correctly on CPUs
    compatible with the common VFP subarchitecture specification (Appendix
    B in the ARM ARM v7-A and v7-R edition). It implements support for VFP
    subarchitecture 2 while being backwards compatible with
    subarchitecture 1.
    
    On VFP subarchitecture 1, the arithmetic exceptions are asynchronous
    (or imprecise as described in the old ARM ARM) unless the FPSCR.IXE
    bit is 1. The exceptional instructions can be read from FPINST and
    FPINST2 registers. With VFP subarchitecture 2, the arithmetic
    exceptions can also be synchronous and marked by the FPEXC.DEX bit
    (the FPEXC.EX bit is cleared). CPUs implementing the synchronous
    arithmetic exceptions don't have the FPINST and FPINST2 registers and
    accessing them would trigger and undefined exception.
    
    Note that FPEXC.EX bit has an additional meaning on subarchitecture 1
    - if it isn't set, there is no additional information in FPINST and
    FPINST2 that needs to be saved at context switch or when lazy-loading
    the VFP state of a different thread.
    
    The patch also removes the clearing of the cumulative exception flags in
    FPSCR when additional exceptions were raised. It is up to the user
    application to clear these bits.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index b4e210df92f2..32455c633f1c 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -125,13 +125,13 @@ void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 	send_sig_info(SIGFPE, &info, current);
 }
 
-static void vfp_panic(char *reason)
+static void vfp_panic(char *reason, u32 inst)
 {
 	int i;
 
 	printk(KERN_ERR "VFP: Error: %s\n", reason);
 	printk(KERN_ERR "VFP: EXC 0x%08x SCR 0x%08x INST 0x%08x\n",
-		fmrx(FPEXC), fmrx(FPSCR), fmrx(FPINST));
+		fmrx(FPEXC), fmrx(FPSCR), inst);
 	for (i = 0; i < 32; i += 2)
 		printk(KERN_ERR "VFP: s%2u: 0x%08x s%2u: 0x%08x\n",
 		       i, vfp_get_float(i), i+1, vfp_get_float(i+1));
@@ -147,19 +147,16 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 	pr_debug("VFP: raising exceptions %08x\n", exceptions);
 
 	if (exceptions == VFP_EXCEPTION_ERROR) {
-		vfp_panic("unhandled bounce");
+		vfp_panic("unhandled bounce", inst);
 		vfp_raise_sigfpe(0, regs);
 		return;
 	}
 
 	/*
-	 * If any of the status flags are set, update the FPSCR.
+	 * Update the FPSCR with the additional exception flags.
 	 * Comparison instructions always return at least one of
 	 * these flags set.
 	 */
-	if (exceptions & (FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V))
-		fpscr &= ~(FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V);
-
 	fpscr |= exceptions;
 
 	fmxr(FPSCR, fpscr);
@@ -220,35 +217,64 @@ static u32 vfp_emulate_instruction(u32 inst, u32 fpscr, struct pt_regs *regs)
 /*
  * Package up a bounce condition.
  */
-void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
+void VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 {
-	u32 fpscr, orig_fpscr, exceptions, inst;
+	u32 fpscr, orig_fpscr, fpsid, exceptions;
 
 	pr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);
 
 	/*
-	 * Enable access to the VFP so we can handle the bounce.
+	 * At this point, FPEXC can have the following configuration:
+	 *
+	 *  EX DEX IXE
+	 *  0   1   x   - synchronous exception
+	 *  1   x   0   - asynchronous exception
+	 *  1   x   1   - sychronous on VFP subarch 1 and asynchronous on later
+	 *  0   0   1   - synchronous on VFP9 (non-standard subarch 1
+	 *                implementation), undefined otherwise
+	 *
+	 * Clear various bits and enable access to the VFP so we can
+	 * handle the bounce.
 	 */
-	fmxr(FPEXC, fpexc & ~(FPEXC_EX|FPEXC_FPV2|FPEXC_INV|FPEXC_UFC|FPEXC_OFC|FPEXC_IOC));
+	fmxr(FPEXC, fpexc & ~(FPEXC_EX|FPEXC_DEX|FPEXC_FP2V|FPEXC_VV|FPEXC_TRAP_MASK));
 
+	fpsid = fmrx(FPSID);
 	orig_fpscr = fpscr = fmrx(FPSCR);
 
 	/*
-	 * If we are running with inexact exceptions enabled, we need to
-	 * emulate the trigger instruction.  Note that as we're emulating
-	 * the trigger instruction, we need to increment PC.
+	 * Check for the special VFP subarch 1 and FPSCR.IXE bit case
 	 */
-	if (fpscr & FPSCR_IXE) {
-		regs->ARM_pc += 4;
+	if ((fpsid & FPSID_ARCH_MASK) == (1 << FPSID_ARCH_BIT)
+	    && (fpscr & FPSCR_IXE)) {
+		/*
+		 * Synchronous exception, emulate the trigger instruction
+		 */
 		goto emulate;
 	}
 
-	barrier();
+	if (fpexc & FPEXC_EX) {
+		/*
+		 * Asynchronous exception. The instruction is read from FPINST
+		 * and the interrupted instruction has to be restarted.
+		 */
+		trigger = fmrx(FPINST);
+		regs->ARM_pc -= 4;
+	} else if (!(fpexc & FPEXC_DEX)) {
+		/*
+		 * Illegal combination of bits. It can be caused by an
+		 * unallocated VFP instruction but with FPSCR.IXE set and not
+		 * on VFP subarch 1.
+		 */
+		 vfp_raise_exceptions(VFP_EXCEPTION_ERROR, trigger, fpscr, regs);
+		 return;
+	}
 
 	/*
-	 * Modify fpscr to indicate the number of iterations remaining
+	 * Modify fpscr to indicate the number of iterations remaining.
+	 * If FPEXC.EX is 0, FPEXC.DEX is 1 and the FPEXC.VV bit indicates
+	 * whether FPEXC.VECITR or FPSCR.LEN is used.
 	 */
-	if (fpexc & FPEXC_EX) {
+	if (fpexc & (FPEXC_EX | FPEXC_VV)) {
 		u32 len;
 
 		len = fpexc + (1 << FPEXC_LENGTH_BIT);
@@ -262,15 +288,15 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	 * FPEXC bounce reason, but this appears to be unreliable.
 	 * Emulate the bounced instruction instead.
 	 */
-	inst = fmrx(FPINST);
-	exceptions = vfp_emulate_instruction(inst, fpscr, regs);
+	exceptions = vfp_emulate_instruction(trigger, fpscr, regs);
 	if (exceptions)
-		vfp_raise_exceptions(exceptions, inst, orig_fpscr, regs);
+		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
 
 	/*
-	 * If there isn't a second FP instruction, exit now.
+	 * If there isn't a second FP instruction, exit now. Note that
+	 * the FPEXC.FP2V bit is valid only if FPEXC.EX is 1.
 	 */
-	if (!(fpexc & FPEXC_FPV2))
+	if (fpexc ^ (FPEXC_EX | FPEXC_FP2V))
 		return;
 
 	/*
@@ -279,10 +305,9 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	 */
 	barrier();
 	trigger = fmrx(FPINST2);
-	orig_fpscr = fpscr = fmrx(FPSCR);
 
  emulate:
-	exceptions = vfp_emulate_instruction(trigger, fpscr, regs);
+	exceptions = vfp_emulate_instruction(trigger, orig_fpscr, regs);
 	if (exceptions)
 		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
 }
@@ -306,16 +331,9 @@ static int __init vfp_init(void)
 {
 	unsigned int vfpsid;
 	unsigned int cpu_arch = cpu_architecture();
-	u32 access = 0;
 
-	if (cpu_arch >= CPU_ARCH_ARMv6) {
-		access = get_copro_access();
-
-		/*
-		 * Enable full access to VFP (cp10 and cp11)
-		 */
-		set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
-	}
+	if (cpu_arch >= CPU_ARCH_ARMv6)
+		vfp_enable(NULL);
 
 	/*
 	 * First check that there is a VFP that we can use.
@@ -329,15 +347,9 @@ static int __init vfp_init(void)
 	vfp_vector = vfp_null_entry;
 
 	printk(KERN_INFO "VFP support v0.3: ");
-	if (VFP_arch) {
+	if (VFP_arch)
 		printk("not present\n");
-
-		/*
-		 * Restore the copro access register.
-		 */
-		if (cpu_arch >= CPU_ARCH_ARMv6)
-			set_copro_access(access);
-	} else if (vfpsid & FPSID_NODOUBLE) {
+	else if (vfpsid & FPSID_NODOUBLE) {
 		printk("no double precision support\n");
 	} else {
 		smp_call_function(vfp_enable, NULL, 1, 1);

commit 67f18f34583c9dda0dbcd4088df9070926f7e611
Author: Takashi Ohmasa <ohmasa.takashi@jp.panasonic.com>
Date:   Fri Oct 19 13:58:51 2007 +0100

    [ARM] 4629/1: Fix VFP emulation code to clear all exception flags of FPEXC
    
    All exception flags of the FPEXC register must be cleared before
    returning from exception code to user code, including FP2V and OFC.
    
    Signed-off-by: Takashi Ohmasa <ohmasa.takashi@jp.panasonic.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index eea3f50743d5..b4e210df92f2 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -229,7 +229,7 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	/*
 	 * Enable access to the VFP so we can handle the bounce.
 	 */
-	fmxr(FPEXC, fpexc & ~(FPEXC_EX|FPEXC_INV|FPEXC_UFC|FPEXC_IOC));
+	fmxr(FPEXC, fpexc & ~(FPEXC_EX|FPEXC_FPV2|FPEXC_INV|FPEXC_UFC|FPEXC_OFC|FPEXC_IOC));
 
 	orig_fpscr = fpscr = fmrx(FPSCR);
 

commit b9338a78fc21e980d33c58b31f3bb37cd48a68f6
Author: Tzachi Perelstein <tzachi@marvell.com>
Date:   Sun Sep 9 14:24:59 2007 +0100

    [ARM] 4567/1: Fix 'Oops - undefined instruction' when CONFIG_VFP=y on non VFP device
    
    vfp_init() takes care of the condition when CONFIG_VFP=y but no real VFP
    device exists. However, when this condition is true, a compiler might
    misplace code lines in a way that will break this support. (To be more
    specific - fmrx(FPSID) might be executed before vfp_testing_entry
    assignment, which will end up with Oops - undefined instruction).
    This patch adds a barrier() to guarantee the right execution ordering.
    
    Signed-off-by: Assaf Hoffman
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 04ddab2bd876..eea3f50743d5 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -323,6 +323,7 @@ static int __init vfp_init(void)
 	 * we just need to read the VFPSID register.
 	 */
 	vfp_vector = vfp_testing_entry;
+	barrier();
 	vfpsid = fmrx(FPSID);
 	barrier();
 	vfp_vector = vfp_null_entry;

commit 228adef16d6e7b7725ef6b9ba760810d5966afa5
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Jul 18 09:37:10 2007 +0100

    [ARM] vfp: make fpexc bit names less verbose
    
    Use the fpexc abbreviated names instead of long verbose names
    for fpexc bits.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 1106b5f9cf19..04ddab2bd876 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -53,7 +53,7 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * case the thread migrates to a different CPU. The
 		 * restoring is done lazily.
 		 */
-		if ((fpexc & FPEXC_ENABLE) && last_VFP_context[cpu]) {
+		if ((fpexc & FPEXC_EN) && last_VFP_context[cpu]) {
 			vfp_save_state(last_VFP_context[cpu], fpexc);
 			last_VFP_context[cpu]->hard.cpu = cpu;
 		}
@@ -70,7 +70,7 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * Always disable VFP so we can lazily save/restore the
 		 * old state.
 		 */
-		fmxr(FPEXC, fpexc & ~FPEXC_ENABLE);
+		fmxr(FPEXC, fpexc & ~FPEXC_EN);
 		return NOTIFY_DONE;
 	}
 
@@ -81,13 +81,13 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 */
 		memset(vfp, 0, sizeof(union vfp_state));
 
-		vfp->hard.fpexc = FPEXC_ENABLE;
+		vfp->hard.fpexc = FPEXC_EN;
 		vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
 
 		/*
 		 * Disable VFP to ensure we initialise it first.
 		 */
-		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
+		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);
 	}
 
 	/* flush and release case: Per-thread VFP cleanup. */
@@ -229,7 +229,7 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	/*
 	 * Enable access to the VFP so we can handle the bounce.
 	 */
-	fmxr(FPEXC, fpexc & ~(FPEXC_EXCEPTION|FPEXC_INV|FPEXC_UFC|FPEXC_IOC));
+	fmxr(FPEXC, fpexc & ~(FPEXC_EX|FPEXC_INV|FPEXC_UFC|FPEXC_IOC));
 
 	orig_fpscr = fpscr = fmrx(FPSCR);
 
@@ -248,7 +248,7 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	/*
 	 * Modify fpscr to indicate the number of iterations remaining
 	 */
-	if (fpexc & FPEXC_EXCEPTION) {
+	if (fpexc & FPEXC_EX) {
 		u32 len;
 
 		len = fpexc + (1 << FPEXC_LENGTH_BIT);

commit 5d4cae5fe2ea1a0974962e2c49dca5c9c4b14cc0
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Jun 10 12:22:20 2007 +0100

    [ARM] VFP: fix section mismatch error
    
    Fix a real section mismatch issue; the test code is thrown away after
    initialisation, but if we do not detect the VFP hardware, it is left
    hooked into the exception handler.  Any VFP instructions which are
    subsequently executed risk calling the discarded exception handler.
    
    Introduce a new "null" handler which returns to the "unrecognised
    fault" return address.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f1e5951dc721..1106b5f9cf19 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -26,8 +26,9 @@
  */
 void vfp_testing_entry(void);
 void vfp_support_entry(void);
+void vfp_null_entry(void);
 
-void (*vfp_vector)(void) = vfp_testing_entry;
+void (*vfp_vector)(void) = vfp_null_entry;
 union vfp_state *last_VFP_context[NR_CPUS];
 
 /*
@@ -321,8 +322,10 @@ static int __init vfp_init(void)
 	 * The handler is already setup to just log calls, so
 	 * we just need to read the VFPSID register.
 	 */
+	vfp_vector = vfp_testing_entry;
 	vfpsid = fmrx(FPSID);
 	barrier();
+	vfp_vector = vfp_null_entry;
 
 	printk(KERN_INFO "VFP support v0.3: ");
 	if (VFP_arch) {

commit c6428464894889e110418928e6b37dc2eb4cee56
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jan 24 18:47:08 2007 +0100

    [ARM] 4111/1: Allow VFP to work with thread migration on SMP
    
    The current lazy saving of the VFP registers is no longer possible
    with thread migration on SMP. This patch implements a per-CPU
    vfp-state pointer and the saving of the VFP registers at every context
    switch. The registers restoring is still performed in a lazy way.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 490d9d18a7d1..f1e5951dc721 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -28,7 +28,7 @@ void vfp_testing_entry(void);
 void vfp_support_entry(void);
 
 void (*vfp_vector)(void) = vfp_testing_entry;
-union vfp_state *last_VFP_context;
+union vfp_state *last_VFP_context[NR_CPUS];
 
 /*
  * Dual-use variable.
@@ -41,13 +41,35 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
 	struct thread_info *thread = v;
 	union vfp_state *vfp;
+	__u32 cpu = thread->cpu;
 
 	if (likely(cmd == THREAD_NOTIFY_SWITCH)) {
+		u32 fpexc = fmrx(FPEXC);
+
+#ifdef CONFIG_SMP
+		/*
+		 * On SMP, if VFP is enabled, save the old state in
+		 * case the thread migrates to a different CPU. The
+		 * restoring is done lazily.
+		 */
+		if ((fpexc & FPEXC_ENABLE) && last_VFP_context[cpu]) {
+			vfp_save_state(last_VFP_context[cpu], fpexc);
+			last_VFP_context[cpu]->hard.cpu = cpu;
+		}
+		/*
+		 * Thread migration, just force the reloading of the
+		 * state on the new CPU in case the VFP registers
+		 * contain stale data.
+		 */
+		if (thread->vfpstate.hard.cpu != cpu)
+			last_VFP_context[cpu] = NULL;
+#endif
+
 		/*
 		 * Always disable VFP so we can lazily save/restore the
 		 * old state.
 		 */
-		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
+		fmxr(FPEXC, fpexc & ~FPEXC_ENABLE);
 		return NOTIFY_DONE;
 	}
 
@@ -68,8 +90,8 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 	}
 
 	/* flush and release case: Per-thread VFP cleanup. */
-	if (last_VFP_context == vfp)
-		last_VFP_context = NULL;
+	if (last_VFP_context[cpu] == vfp)
+		last_VFP_context[cpu] = NULL;
 
 	return NOTIFY_DONE;
 }

commit 8e140362f71ed801c69c5df63a68fc2cd07da3a9
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Jan 2 23:40:30 2007 +0000

    [ARM] Fix VFP initialisation issue for SMP systems
    
    When we install the handlers for context switching, we must enable
    VFP on all CPU cores, otherwise undefined (and random) effects
    occur.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index e26cc1f59948..490d9d18a7d1 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -264,6 +264,18 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
 }
 
+static void vfp_enable(void *unused)
+{
+	u32 access = get_copro_access();
+
+	/*
+	 * Enable full access to VFP (cp10 and cp11)
+	 */
+	set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
+}
+
+#include <linux/smp.h>
+
 /*
  * VFP support code initialisation.
  */
@@ -288,6 +300,7 @@ static int __init vfp_init(void)
 	 * we just need to read the VFPSID register.
 	 */
 	vfpsid = fmrx(FPSID);
+	barrier();
 
 	printk(KERN_INFO "VFP support v0.3: ");
 	if (VFP_arch) {
@@ -301,6 +314,8 @@ static int __init vfp_init(void)
 	} else if (vfpsid & FPSID_NODOUBLE) {
 		printk("no double precision support\n");
 	} else {
+		smp_call_function(vfp_enable, NULL, 1, 1);
+
 		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */
 		printk("implementor %02x architecture %d part %02x variant %x rev %x\n",
 			(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,

commit efe90d273b6f365d37c0f82fbbd68a40982c3265
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Dec 8 15:22:20 2006 +0000

    [ARM] Handle HWCAP_VFP in VFP support code
    
    Don't set HWCAP_VFP in the processor support file; not only does it
    depend on the processor features, but it also depends on the support
    code being present.  Therefore, only set it if the support code
    detects that we have a VFP coprocessor attached.
    
    Also, move the VFP handling of the coprocessor access register into
    the VFP support code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index f08eafbddcc1..e26cc1f59948 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -263,13 +263,24 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	if (exceptions)
 		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
 }
- 
+
 /*
  * VFP support code initialisation.
  */
 static int __init vfp_init(void)
 {
 	unsigned int vfpsid;
+	unsigned int cpu_arch = cpu_architecture();
+	u32 access = 0;
+
+	if (cpu_arch >= CPU_ARCH_ARMv6) {
+		access = get_copro_access();
+
+		/*
+		 * Enable full access to VFP (cp10 and cp11)
+		 */
+		set_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));
+	}
 
 	/*
 	 * First check that there is a VFP that we can use.
@@ -281,6 +292,12 @@ static int __init vfp_init(void)
 	printk(KERN_INFO "VFP support v0.3: ");
 	if (VFP_arch) {
 		printk("not present\n");
+
+		/*
+		 * Restore the copro access register.
+		 */
+		if (cpu_arch >= CPU_ARCH_ARMv6)
+			set_copro_access(access);
 	} else if (vfpsid & FPSID_NODOUBLE) {
 		printk("no double precision support\n");
 	} else {
@@ -291,9 +308,16 @@ static int __init vfp_init(void)
 			(vfpsid & FPSID_PART_MASK) >> FPSID_PART_BIT,
 			(vfpsid & FPSID_VARIANT_MASK) >> FPSID_VARIANT_BIT,
 			(vfpsid & FPSID_REV_MASK) >> FPSID_REV_BIT);
+
 		vfp_vector = vfp_support_entry;
 
 		thread_register_notifier(&vfp_notifier_block);
+
+		/*
+		 * We detected VFP, and the support code is
+		 * in place; report VFP support to userspace.
+		 */
+		elf_hwcap |= HWCAP_VFP;
 	}
 	return 0;
 }

commit e0f205d9c656da9dad6340f75e33a96014b7d23f
Author: Takashi Ohmasa <ohmasa.takashi@com.rmk.(none)>
Date:   Mon Oct 23 11:19:40 2006 +0100

    [ARM] 3900/1: Fix VFP Division by Zero exception handling.
    
    The SIGFPE signal should be generated if Division by Zero exception is detected.
    
    Signed-off-by: Takashi Ohmasa <ohmasa.takashi@jp.panasonic.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index a657a28f08db..f08eafbddcc1 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -148,6 +148,7 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 	/*
 	 * These are arranged in priority order, least to highest.
 	 */
+	RAISE(FPSCR_DZC, FPSCR_DZE, FPE_FLTDIV);
 	RAISE(FPSCR_IXC, FPSCR_IXE, FPE_FLTRES);
 	RAISE(FPSCR_UFC, FPSCR_UFE, FPE_FLTUND);
 	RAISE(FPSCR_OFC, FPSCR_OFE, FPE_FLTOVF);

commit 35d59fc5d6f318a28a99c5936171afd4edef28c8
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Wed Oct 11 17:22:44 2006 +0100

    [PATCH] arm __user annotations
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index dedbb449632e..a657a28f08db 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -90,7 +90,7 @@ void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 
 	info.si_signo = SIGFPE;
 	info.si_code = sicode;
-	info.si_addr = (void *)(instruction_pointer(regs) - 4);
+	info.si_addr = (void __user *)(instruction_pointer(regs) - 4);
 
 	/*
 	 * This is the same as NWFPE, because it's not clear what

commit 681a4991f83742a0d2325afbf7b7f22045ad5b30
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 27 12:38:34 2006 +0100

    [ARM] Optimise VFP thread notify function a little
    
    The common case for the thread notifier is a context switch.  Tell
    gcc that this is the most likely condition so it can optimise the
    function for this case.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 4178f6cc3d37..dedbb449632e 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -40,10 +40,19 @@ unsigned int VFP_arch;
 static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
 	struct thread_info *thread = v;
-	union vfp_state *vfp = &thread->vfpstate;
+	union vfp_state *vfp;
 
-	switch (cmd) {
-	case THREAD_NOTIFY_FLUSH:
+	if (likely(cmd == THREAD_NOTIFY_SWITCH)) {
+		/*
+		 * Always disable VFP so we can lazily save/restore the
+		 * old state.
+		 */
+		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
+		return NOTIFY_DONE;
+	}
+
+	vfp = &thread->vfpstate;
+	if (cmd == THREAD_NOTIFY_FLUSH) {
 		/*
 		 * Per-thread VFP initialisation.
 		 */
@@ -56,29 +65,12 @@ static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 		 * Disable VFP to ensure we initialise it first.
 		 */
 		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
-
-		/*
-		 * FALLTHROUGH: Ensure we don't try to overwrite our newly
-		 * initialised state information on the first fault.
-		 */
-
-	case THREAD_NOTIFY_RELEASE:
-		/*
-		 * Per-thread VFP cleanup.
-		 */
-		if (last_VFP_context == vfp)
-			last_VFP_context = NULL;
-		break;
-
-	case THREAD_NOTIFY_SWITCH:
-		/*
-		 * Always disable VFP so we can lazily save/restore the
-		 * old state.
-		 */
-		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
-		break;
 	}
 
+	/* flush and release case: Per-thread VFP cleanup. */
+	if (last_VFP_context == vfp)
+		last_VFP_context = NULL;
+
 	return NOTIFY_DONE;
 }
 

commit 7c6f25141b88b5e926f9d43932ce668602d611ae
Author: Daniel Jacobowitz <drow@false.org>
Date:   Sun Aug 27 12:42:08 2006 +0100

    [ARM] 3748/3: Correct error check in vfp_raise_exceptions
    
    Patch from Daniel Jacobowitz
    
    The recent fix to hide VFP_NAN_FLAG broke the check in vfp_raise_exceptions;
    it would attempt to deliver an exception mask of 0xfffffeff instead of reporting
    a serious error condition using printk.  Define a safe constant to use for
    an invalid exception maskm, and use it at both ends.
    
    Signed-off-by: Daniel Jacobowitz <dan@codesourcery.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 9d265d5e748c..4178f6cc3d37 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -131,7 +131,7 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
 
 	pr_debug("VFP: raising exceptions %08x\n", exceptions);
 
-	if (exceptions == (u32)-1) {
+	if (exceptions == VFP_EXCEPTION_ERROR) {
 		vfp_panic("unhandled bounce");
 		vfp_raise_sigfpe(0, regs);
 		return;
@@ -170,7 +170,7 @@ static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_
  */
 static u32 vfp_emulate_instruction(u32 inst, u32 fpscr, struct pt_regs *regs)
 {
-	u32 exceptions = (u32)-1;
+	u32 exceptions = VFP_EXCEPTION_ERROR;
 
 	pr_debug("VFP: emulate: INST=0x%08x SCR=0x%08x\n", inst, fpscr);
 

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 2476f4c2e760..9d265d5e748c 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -9,7 +9,6 @@
  * published by the Free Software Foundation.
  */
 #include <linux/module.h>
-#include <linux/config.h>
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/signal.h>

commit d6551e884cf66de072b81f8b6d23259462c40baf
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Jun 21 13:31:52 2006 +0100

    [ARM] Add thread_notify infrastructure
    
    Some machine classes need to allow VFP support to be built into the
    kernel, but still allow the kernel to run even though VFP isn't
    present.  Unfortunately, the kernel hard-codes VFP instructions
    into the thread switch, which prevents this being run-time selectable.
    
    Solve this by introducing a notifier which things such as VFP can
    hook into to be informed of events which affect the VFP subsystem
    (eg, creation and destruction of threads, switches between threads.)
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 03486be04193..2476f4c2e760 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -15,6 +15,8 @@
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/init.h>
+
+#include <asm/thread_notify.h>
 #include <asm/vfp.h>
 
 #include "vfpinstr.h"
@@ -36,38 +38,55 @@ union vfp_state *last_VFP_context;
  */
 unsigned int VFP_arch;
 
-/*
- * Per-thread VFP initialisation.
- */
-void vfp_flush_thread(union vfp_state *vfp)
+static int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)
 {
-	memset(vfp, 0, sizeof(union vfp_state));
+	struct thread_info *thread = v;
+	union vfp_state *vfp = &thread->vfpstate;
 
-	vfp->hard.fpexc = FPEXC_ENABLE;
-	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
+	switch (cmd) {
+	case THREAD_NOTIFY_FLUSH:
+		/*
+		 * Per-thread VFP initialisation.
+		 */
+		memset(vfp, 0, sizeof(union vfp_state));
 
-	/*
-	 * Disable VFP to ensure we initialise it first.
-	 */
-	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
+		vfp->hard.fpexc = FPEXC_ENABLE;
+		vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
 
-	/*
-	 * Ensure we don't try to overwrite our newly initialised
-	 * state information on the first fault.
-	 */
-	if (last_VFP_context == vfp)
-		last_VFP_context = NULL;
-}
+		/*
+		 * Disable VFP to ensure we initialise it first.
+		 */
+		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
 
-/*
- * Per-thread VFP cleanup.
- */
-void vfp_release_thread(union vfp_state *vfp)
-{
-	if (last_VFP_context == vfp)
-		last_VFP_context = NULL;
+		/*
+		 * FALLTHROUGH: Ensure we don't try to overwrite our newly
+		 * initialised state information on the first fault.
+		 */
+
+	case THREAD_NOTIFY_RELEASE:
+		/*
+		 * Per-thread VFP cleanup.
+		 */
+		if (last_VFP_context == vfp)
+			last_VFP_context = NULL;
+		break;
+
+	case THREAD_NOTIFY_SWITCH:
+		/*
+		 * Always disable VFP so we can lazily save/restore the
+		 * old state.
+		 */
+		fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
+		break;
+	}
+
+	return NOTIFY_DONE;
 }
 
+static struct notifier_block vfp_notifier_block = {
+	.notifier_call	= vfp_notifier,
+};
+
 /*
  * Raise a SIGFPE for the current process.
  * sicode describes the signal being raised.
@@ -281,6 +300,8 @@ static int __init vfp_init(void)
 			(vfpsid & FPSID_VARIANT_MASK) >> FPSID_VARIANT_BIT,
 			(vfpsid & FPSID_REV_MASK) >> FPSID_REV_BIT);
 		vfp_vector = vfp_support_entry;
+
+		thread_register_notifier(&vfp_notifier_block);
 	}
 	return 0;
 }

commit b7d7ef87e15dea105be59ec8f14e2f92182dd421
Author: George G. Davis <davis_g@mvista.com>
Date:   Fri May 5 22:32:23 2006 +0100

    [ARM] 3499/1: Fix VFP FPSCR corruption for double exception case
    
    Patch from George G. Davis
    
    The ARM VFP FPSCR register is corrupted when a condition flags modifying
    VFP instruction is followed by a non-condition flags modifying VFP
    instruction and both instructions raise exceptions.  The fix is to
    read the current FPSCR in between emulation of these two instructions
    and use the current FPSCR value when handling the second exception.
    
    Signed-off-by: George G. Davis <gdavis@mvista.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 37ff8145b5b5..03486be04193 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -245,7 +245,7 @@ void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
 	 */
 	barrier();
 	trigger = fmrx(FPINST2);
-	fpscr = fmrx(FPSCR);
+	orig_fpscr = fpscr = fmrx(FPSCR);
 
  emulate:
 	exceptions = vfp_emulate_instruction(trigger, fpscr, regs);

commit 928bd1b4709045355fad8bf858904884c0a1e87f
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Apr 25 20:41:27 2006 +0100

    [ARM] vfp: fix leak of VFP_NAN_FLAG into FPSCR
    
    The VFP code can leak VFP_NAN_FLAG into the FPSCR. It doesn't correspond
    to any real FPSCR bit (and overlaps one of the exception flags).
    
    Bug report from Daniel Jacobowitz
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 22f3da4e0829..37ff8145b5b5 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -180,7 +180,7 @@ static u32 vfp_emulate_instruction(u32 inst, u32 fpscr, struct pt_regs *regs)
 		 * emulate it.
 		 */
 	}
-	return exceptions;
+	return exceptions & ~VFP_NAN_FLAG;
 }
 
 /*

commit da41119af78864d27ccbf505949df788d5e8aaf5
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Jun 29 23:02:02 2005 +0100

    [PATCH] ARM: Don't force SIGFPE
    
    We were forcing SIGFPE on to a user program for no good reason.
    Use send_sig_info() instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
index 3aeedd2afc70..22f3da4e0829 100644
--- a/arch/arm/vfp/vfpmodule.c
+++ b/arch/arm/vfp/vfpmodule.c
@@ -89,7 +89,7 @@ void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
 	current->thread.error_code = 0;
 	current->thread.trap_no = 6;
 
-	force_sig_info(SIGFPE, &info, current);
+	send_sig_info(SIGFPE, &info, current);
 }
 
 static void vfp_panic(char *reason)

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/arm/vfp/vfpmodule.c b/arch/arm/vfp/vfpmodule.c
new file mode 100644
index 000000000000..3aeedd2afc70
--- /dev/null
+++ b/arch/arm/vfp/vfpmodule.c
@@ -0,0 +1,288 @@
+/*
+ *  linux/arch/arm/vfp/vfpmodule.c
+ *
+ *  Copyright (C) 2004 ARM Limited.
+ *  Written by Deep Blue Solutions Limited.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/module.h>
+#include <linux/config.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <asm/vfp.h>
+
+#include "vfpinstr.h"
+#include "vfp.h"
+
+/*
+ * Our undef handlers (in entry.S)
+ */
+void vfp_testing_entry(void);
+void vfp_support_entry(void);
+
+void (*vfp_vector)(void) = vfp_testing_entry;
+union vfp_state *last_VFP_context;
+
+/*
+ * Dual-use variable.
+ * Used in startup: set to non-zero if VFP checks fail
+ * After startup, holds VFP architecture
+ */
+unsigned int VFP_arch;
+
+/*
+ * Per-thread VFP initialisation.
+ */
+void vfp_flush_thread(union vfp_state *vfp)
+{
+	memset(vfp, 0, sizeof(union vfp_state));
+
+	vfp->hard.fpexc = FPEXC_ENABLE;
+	vfp->hard.fpscr = FPSCR_ROUND_NEAREST;
+
+	/*
+	 * Disable VFP to ensure we initialise it first.
+	 */
+	fmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_ENABLE);
+
+	/*
+	 * Ensure we don't try to overwrite our newly initialised
+	 * state information on the first fault.
+	 */
+	if (last_VFP_context == vfp)
+		last_VFP_context = NULL;
+}
+
+/*
+ * Per-thread VFP cleanup.
+ */
+void vfp_release_thread(union vfp_state *vfp)
+{
+	if (last_VFP_context == vfp)
+		last_VFP_context = NULL;
+}
+
+/*
+ * Raise a SIGFPE for the current process.
+ * sicode describes the signal being raised.
+ */
+void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)
+{
+	siginfo_t info;
+
+	memset(&info, 0, sizeof(info));
+
+	info.si_signo = SIGFPE;
+	info.si_code = sicode;
+	info.si_addr = (void *)(instruction_pointer(regs) - 4);
+
+	/*
+	 * This is the same as NWFPE, because it's not clear what
+	 * this is used for
+	 */
+	current->thread.error_code = 0;
+	current->thread.trap_no = 6;
+
+	force_sig_info(SIGFPE, &info, current);
+}
+
+static void vfp_panic(char *reason)
+{
+	int i;
+
+	printk(KERN_ERR "VFP: Error: %s\n", reason);
+	printk(KERN_ERR "VFP: EXC 0x%08x SCR 0x%08x INST 0x%08x\n",
+		fmrx(FPEXC), fmrx(FPSCR), fmrx(FPINST));
+	for (i = 0; i < 32; i += 2)
+		printk(KERN_ERR "VFP: s%2u: 0x%08x s%2u: 0x%08x\n",
+		       i, vfp_get_float(i), i+1, vfp_get_float(i+1));
+}
+
+/*
+ * Process bitmask of exception conditions.
+ */
+static void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_regs *regs)
+{
+	int si_code = 0;
+
+	pr_debug("VFP: raising exceptions %08x\n", exceptions);
+
+	if (exceptions == (u32)-1) {
+		vfp_panic("unhandled bounce");
+		vfp_raise_sigfpe(0, regs);
+		return;
+	}
+
+	/*
+	 * If any of the status flags are set, update the FPSCR.
+	 * Comparison instructions always return at least one of
+	 * these flags set.
+	 */
+	if (exceptions & (FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V))
+		fpscr &= ~(FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V);
+
+	fpscr |= exceptions;
+
+	fmxr(FPSCR, fpscr);
+
+#define RAISE(stat,en,sig)				\
+	if (exceptions & stat && fpscr & en)		\
+		si_code = sig;
+
+	/*
+	 * These are arranged in priority order, least to highest.
+	 */
+	RAISE(FPSCR_IXC, FPSCR_IXE, FPE_FLTRES);
+	RAISE(FPSCR_UFC, FPSCR_UFE, FPE_FLTUND);
+	RAISE(FPSCR_OFC, FPSCR_OFE, FPE_FLTOVF);
+	RAISE(FPSCR_IOC, FPSCR_IOE, FPE_FLTINV);
+
+	if (si_code)
+		vfp_raise_sigfpe(si_code, regs);
+}
+
+/*
+ * Emulate a VFP instruction.
+ */
+static u32 vfp_emulate_instruction(u32 inst, u32 fpscr, struct pt_regs *regs)
+{
+	u32 exceptions = (u32)-1;
+
+	pr_debug("VFP: emulate: INST=0x%08x SCR=0x%08x\n", inst, fpscr);
+
+	if (INST_CPRTDO(inst)) {
+		if (!INST_CPRT(inst)) {
+			/*
+			 * CPDO
+			 */
+			if (vfp_single(inst)) {
+				exceptions = vfp_single_cpdo(inst, fpscr);
+			} else {
+				exceptions = vfp_double_cpdo(inst, fpscr);
+			}
+		} else {
+			/*
+			 * A CPRT instruction can not appear in FPINST2, nor
+			 * can it cause an exception.  Therefore, we do not
+			 * have to emulate it.
+			 */
+		}
+	} else {
+		/*
+		 * A CPDT instruction can not appear in FPINST2, nor can
+		 * it cause an exception.  Therefore, we do not have to
+		 * emulate it.
+		 */
+	}
+	return exceptions;
+}
+
+/*
+ * Package up a bounce condition.
+ */
+void VFP9_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)
+{
+	u32 fpscr, orig_fpscr, exceptions, inst;
+
+	pr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);
+
+	/*
+	 * Enable access to the VFP so we can handle the bounce.
+	 */
+	fmxr(FPEXC, fpexc & ~(FPEXC_EXCEPTION|FPEXC_INV|FPEXC_UFC|FPEXC_IOC));
+
+	orig_fpscr = fpscr = fmrx(FPSCR);
+
+	/*
+	 * If we are running with inexact exceptions enabled, we need to
+	 * emulate the trigger instruction.  Note that as we're emulating
+	 * the trigger instruction, we need to increment PC.
+	 */
+	if (fpscr & FPSCR_IXE) {
+		regs->ARM_pc += 4;
+		goto emulate;
+	}
+
+	barrier();
+
+	/*
+	 * Modify fpscr to indicate the number of iterations remaining
+	 */
+	if (fpexc & FPEXC_EXCEPTION) {
+		u32 len;
+
+		len = fpexc + (1 << FPEXC_LENGTH_BIT);
+
+		fpscr &= ~FPSCR_LENGTH_MASK;
+		fpscr |= (len & FPEXC_LENGTH_MASK) << (FPSCR_LENGTH_BIT - FPEXC_LENGTH_BIT);
+	}
+
+	/*
+	 * Handle the first FP instruction.  We used to take note of the
+	 * FPEXC bounce reason, but this appears to be unreliable.
+	 * Emulate the bounced instruction instead.
+	 */
+	inst = fmrx(FPINST);
+	exceptions = vfp_emulate_instruction(inst, fpscr, regs);
+	if (exceptions)
+		vfp_raise_exceptions(exceptions, inst, orig_fpscr, regs);
+
+	/*
+	 * If there isn't a second FP instruction, exit now.
+	 */
+	if (!(fpexc & FPEXC_FPV2))
+		return;
+
+	/*
+	 * The barrier() here prevents fpinst2 being read
+	 * before the condition above.
+	 */
+	barrier();
+	trigger = fmrx(FPINST2);
+	fpscr = fmrx(FPSCR);
+
+ emulate:
+	exceptions = vfp_emulate_instruction(trigger, fpscr, regs);
+	if (exceptions)
+		vfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);
+}
+ 
+/*
+ * VFP support code initialisation.
+ */
+static int __init vfp_init(void)
+{
+	unsigned int vfpsid;
+
+	/*
+	 * First check that there is a VFP that we can use.
+	 * The handler is already setup to just log calls, so
+	 * we just need to read the VFPSID register.
+	 */
+	vfpsid = fmrx(FPSID);
+
+	printk(KERN_INFO "VFP support v0.3: ");
+	if (VFP_arch) {
+		printk("not present\n");
+	} else if (vfpsid & FPSID_NODOUBLE) {
+		printk("no double precision support\n");
+	} else {
+		VFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;  /* Extract the architecture version */
+		printk("implementor %02x architecture %d part %02x variant %x rev %x\n",
+			(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,
+			(vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT,
+			(vfpsid & FPSID_PART_MASK) >> FPSID_PART_BIT,
+			(vfpsid & FPSID_VARIANT_MASK) >> FPSID_VARIANT_BIT,
+			(vfpsid & FPSID_REV_MASK) >> FPSID_REV_BIT);
+		vfp_vector = vfp_support_entry;
+	}
+	return 0;
+}
+
+late_initcall(vfp_init);
