commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 3851bb396442..1fb10733305a 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/arch/arm/mm/copypage-v4wt.S
  *
  *  Copyright (C) 1995-1999 Russell King
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
  *  This is for CPUs with a writethrough cache and 'flush ID cache' is
  *  the only supported cache operation.
  */

commit b7e8c9397cd4efe6567d2728f091f1b728025533
Author: Stefan Agner <stefan@agner.ch>
Date:   Mon Feb 18 00:58:29 2019 +0100

    ARM: 8845/1: use unified assembler in c files
    
    Use unified assembler syntax (UAL) in inline assembler. Divided
    syntax is considered deprecated. This will also allow to build
    the kernel using LLVM's integrated assembler.
    
    When compiling non-Thumb2 GCC always emits a ".syntax divided"
    at the beginning of the inline assembly which makes the
    assembler fail. Since GCC 5 there is the -masm-syntax-unified
    GCC option which make GCC assume unified syntax asm and hence
    emits ".syntax unified" even in ARM mode. However, the option
    is broken since GCC version 6 (see GCC PR88648 [1]). Work
    around by adding ".syntax unified" as part of the inline
    assembly.
    
    [0] https://gcc.gnu.org/onlinedocs/gcc/ARM-Options.html#index-masm-syntax-unified
    [1] https://gcc.gnu.org/bugzilla/show_bug.cgi?id=88648
    
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 8614572e1296..3851bb396442 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -25,6 +25,7 @@ static void v4wt_copy_user_page(void *kto, const void *kfrom)
 	int tmp;
 
 	asm volatile ("\
+	.syntax unified\n\
 	ldmia	%1!, {r3, r4, ip, lr}		@ 4\n\
 1:	stmia	%0!, {r3, r4, ip, lr}		@ 4\n\
 	ldmia	%1!, {r3, r4, ip, lr}		@ 4+1\n\
@@ -34,7 +35,7 @@ static void v4wt_copy_user_page(void *kto, const void *kfrom)
 	ldmia	%1!, {r3, r4, ip, lr}		@ 4\n\
 	subs	%2, %2, #1			@ 1\n\
 	stmia	%0!, {r3, r4, ip, lr}		@ 4\n\
-	ldmneia	%1!, {r3, r4, ip, lr}		@ 4\n\
+	ldmiane	%1!, {r3, r4, ip, lr}		@ 4\n\
 	bne	1b				@ 1\n\
 	mcr	p15, 0, %2, c7, c7, 0		@ flush ID cache"
 	: "+&r" (kto), "+&r" (kfrom), "=&r" (tmp)

commit b99afae1390140f5b0039e6b37a7380de31ae874
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Nov 7 17:49:00 2018 +0100

    ARM: 8805/2: remove unneeded naked function usage
    
    The naked attribute is known to confuse some old gcc versions when
    function arguments aren't explicitly listed as inline assembly operands
    despite the gcc documentation. That resulted in commit 9a40ac86152c
    ("ARM: 6164/1: Add kto and kfrom to input operands list.").
    
    Yet that commit has problems of its own by having assembly operand
    constraints completely wrong. If the generated code has been OK since
    then, it is due to luck rather than correctness. So this patch also
    provides proper assembly operand constraints, and removes two instances
    of redundant register usages in the implementation while at it.
    
    Inspection of the generated code with this patch doesn't show any
    obvious quality degradation either, so not relying on __naked at all
    will make the code less fragile, and avoid some issues with clang.
    
    The only remaining __naked instances (excluding the kprobes test cases)
    are exynos_pm_power_up_setup(), tc2_pm_power_up_setup() and
    
    cci_enable_port_for_self(. But in the first two cases, only the function
    address is used by the compiler with no chance of inlining it by
    mistake, and the third case is called from assembly code only. And the
    fact that no stack is available when the corresponding code is executed
    does warrant the __naked usage in those cases.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Stefan Agner <stefan@agner.ch>
    Tested-by: Stefan Agner <stefan@agner.ch>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index b85c5da2e510..8614572e1296 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -20,27 +20,26 @@
  * dirty data in the cache.  However, we do have to ensure that
  * subsequent reads are up to date.
  */
-static void __naked
-v4wt_copy_user_page(void *kto, const void *kfrom)
+static void v4wt_copy_user_page(void *kto, const void *kfrom)
 {
-	asm("\
-	stmfd	sp!, {r4, lr}			@ 2\n\
-	mov	r2, %2				@ 1\n\
-	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
-1:	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
-	ldmia	r1!, {r3, r4, ip, lr}		@ 4+1\n\
-	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
-	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
-	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
-	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
-	subs	r2, r2, #1			@ 1\n\
-	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
-	ldmneia	r1!, {r3, r4, ip, lr}		@ 4\n\
+	int tmp;
+
+	asm volatile ("\
+	ldmia	%1!, {r3, r4, ip, lr}		@ 4\n\
+1:	stmia	%0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmia	%1!, {r3, r4, ip, lr}		@ 4+1\n\
+	stmia	%0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmia	%1!, {r3, r4, ip, lr}		@ 4\n\
+	stmia	%0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmia	%1!, {r3, r4, ip, lr}		@ 4\n\
+	subs	%2, %2, #1			@ 1\n\
+	stmia	%0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmneia	%1!, {r3, r4, ip, lr}		@ 4\n\
 	bne	1b				@ 1\n\
-	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache\n\
-	ldmfd	sp!, {r4, pc}			@ 3"
-	:
-	: "r" (kto), "r" (kfrom), "I" (PAGE_SIZE / 64));
+	mcr	p15, 0, %2, c7, c7, 0		@ flush ID cache"
+	: "+&r" (kto), "+&r" (kfrom), "=&r" (tmp)
+	: "2" (PAGE_SIZE / 64)
+	: "r3", "r4", "ip", "lr");
 }
 
 void v4wt_copy_user_highpage(struct page *to, struct page *from,

commit 5472e862de2bc4a47f18d216a4a626d5c7eeef90
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:15 2011 +0800

    arm: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 30c7d048a324..b85c5da2e510 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -48,11 +48,11 @@ void v4wt_copy_user_highpage(struct page *to, struct page *from,
 {
 	void *kto, *kfrom;
 
-	kto = kmap_atomic(to, KM_USER0);
-	kfrom = kmap_atomic(from, KM_USER1);
+	kto = kmap_atomic(to);
+	kfrom = kmap_atomic(from);
 	v4wt_copy_user_page(kto, kfrom);
-	kunmap_atomic(kfrom, KM_USER1);
-	kunmap_atomic(kto, KM_USER0);
+	kunmap_atomic(kfrom);
+	kunmap_atomic(kto);
 }
 
 /*
@@ -62,7 +62,7 @@ void v4wt_copy_user_highpage(struct page *to, struct page *from,
  */
 void v4wt_clear_user_highpage(struct page *page, unsigned long vaddr)
 {
-	void *ptr, *kaddr = kmap_atomic(page, KM_USER0);
+	void *ptr, *kaddr = kmap_atomic(page);
 	asm volatile("\
 	mov	r1, %2				@ 1\n\
 	mov	r2, #0				@ 1\n\
@@ -79,7 +79,7 @@ void v4wt_clear_user_highpage(struct page *page, unsigned long vaddr)
 	: "=r" (ptr)
 	: "0" (kaddr), "I" (PAGE_SIZE / 64)
 	: "r1", "r2", "r3", "ip", "lr");
-	kunmap_atomic(kaddr, KM_USER0);
+	kunmap_atomic(kaddr);
 }
 
 struct cpu_user_fns v4wt_user_fns __initdata = {

commit 9a40ac86152c9cffd3dca482a15ddf9a8c5716b3
Author: Khem Raj <raj.khem@gmail.com>
Date:   Fri Jun 4 04:05:15 2010 +0100

    ARM: 6164/1: Add kto and kfrom to input operands list.
    
    When functions incoming parameters are not in input operands list gcc
    4.5 does not load the parameters into registers before calling this
    function but the inline assembly assumes valid addresses inside this
    function. This breaks the code because r0 and r1 are invalid when
    execution enters v4wb_copy_user_page ()
    
    Also the constant needs to be used as third input operand so account
    for that as well.
    
    Tested on qemu arm.
    
    CC: <stable@kernel.org>
    Signed-off-by: Khem Raj <raj.khem@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 172e6a55458e..30c7d048a324 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -25,7 +25,7 @@ v4wt_copy_user_page(void *kto, const void *kfrom)
 {
 	asm("\
 	stmfd	sp!, {r4, lr}			@ 2\n\
-	mov	r2, %0				@ 1\n\
+	mov	r2, %2				@ 1\n\
 	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
 1:	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
 	ldmia	r1!, {r3, r4, ip, lr}		@ 4+1\n\
@@ -40,7 +40,7 @@ v4wt_copy_user_page(void *kto, const void *kfrom)
 	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache\n\
 	ldmfd	sp!, {r4, pc}			@ 3"
 	:
-	: "I" (PAGE_SIZE / 64));
+	: "r" (kto), "r" (kfrom), "I" (PAGE_SIZE / 64));
 }
 
 void v4wt_copy_user_highpage(struct page *to, struct page *from,

commit f00a75c094c340c4e7435665816c3273c870e849
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 5 15:17:45 2009 +0100

    ARM: Pass VMA to copy_user_highpage() implementations
    
    Our copy_user_highpage() implementations may require cache maintainence.
    Ensure that implementations have all necessary details to perform this
    maintainence.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 300efafd6643..172e6a55458e 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -44,7 +44,7 @@ v4wt_copy_user_page(void *kto, const void *kfrom)
 }
 
 void v4wt_copy_user_highpage(struct page *to, struct page *from,
-	unsigned long vaddr)
+	unsigned long vaddr, struct vm_area_struct *vma)
 {
 	void *kto, *kfrom;
 

commit 446c92b2901bedb3725d29b4e73def8aba623ffc
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu Mar 12 18:03:16 2009 +0100

    [ARM] 5421/1: ftrace: fix crash due to tracing of __naked functions
    
    This is a fix for the following crash observed in 2.6.29-rc3:
    http://lkml.org/lkml/2009/1/29/150
    
    On ARM it doesn't make sense to trace a naked function because then
    mcount is called without stack and frame pointer being set up and there
    is no chance to restore the lr register to the value before mcount was
    called.
    
    Reported-by: Matthias Kaehlcke <matthias@kaehlcke.net>
    Tested-by: Matthias Kaehlcke <matthias@kaehlcke.net>
    
    Cc: Abhishek Sagar <sagar.abhishek@gmail.com>
    Cc: Steven Rostedt <rostedt@home.goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 0f1188efae45..300efafd6643 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -20,7 +20,7 @@
  * dirty data in the cache.  However, we do have to ensure that
  * subsequent reads are up to date.
  */
-static void __attribute__((naked))
+static void __naked
 v4wt_copy_user_page(void *kto, const void *kfrom)
 {
 	asm("\

commit 43ae286b7d4d8c4983bc263ef2e3cccc10dedb2b
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Nov 4 02:42:27 2008 -0500

    [ARM] fix a couple clear_user_highpage assembly constraints
    
    In all cases the kaddr is assigned an input register even though it is
    modified in the assembly code.  Let's assign a new variable to the
    modified value and mark those inline asm with volatile otherwise they
    get optimized away because the output variable is otherwise not used.
    
    Also fix a few conversion errors in copypage-feroceon.c and
    copypage-v4mc.c.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index b8a345d6e77e..0f1188efae45 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -62,9 +62,9 @@ void v4wt_copy_user_highpage(struct page *to, struct page *from,
  */
 void v4wt_clear_user_highpage(struct page *page, unsigned long vaddr)
 {
-	void *kaddr = kmap_atomic(page, KM_USER0);
-	asm("\
-	mov	r1, %1				@ 1\n\
+	void *ptr, *kaddr = kmap_atomic(page, KM_USER0);
+	asm volatile("\
+	mov	r1, %2				@ 1\n\
 	mov	r2, #0				@ 1\n\
 	mov	r3, #0				@ 1\n\
 	mov	ip, #0				@ 1\n\
@@ -76,8 +76,8 @@ void v4wt_clear_user_highpage(struct page *page, unsigned long vaddr)
 	subs	r1, r1, #1			@ 1\n\
 	bne	1b				@ 1\n\
 	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache"
-	:
-	: "r" (kaddr), "I" (PAGE_SIZE / 64)
+	: "=r" (ptr)
+	: "0" (kaddr), "I" (PAGE_SIZE / 64)
 	: "r1", "r2", "r3", "ip", "lr");
 	kunmap_atomic(kaddr, KM_USER0);
 }

commit 303c6443659bc1dc911356f5de149f48ff1d97b8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 16:32:19 2008 +0000

    [ARM] clearpage: provide our own clear_user_highpage()
    
    For similar reasons as copy_user_page(), we want to avoid the
    additional kmap_atomic if it's unnecessary.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index 86c2cfdbde03..b8a345d6e77e 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -60,29 +60,29 @@ void v4wt_copy_user_highpage(struct page *to, struct page *from,
  *
  * Same story as above.
  */
-void __attribute__((naked))
-v4wt_clear_user_page(void *kaddr, unsigned long vaddr)
+void v4wt_clear_user_highpage(struct page *page, unsigned long vaddr)
 {
+	void *kaddr = kmap_atomic(page, KM_USER0);
 	asm("\
-	str	lr, [sp, #-4]!\n\
-	mov	r1, %0				@ 1\n\
+	mov	r1, %1				@ 1\n\
 	mov	r2, #0				@ 1\n\
 	mov	r3, #0				@ 1\n\
 	mov	ip, #0				@ 1\n\
 	mov	lr, #0				@ 1\n\
-1:	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
-	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
-	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
-	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
+1:	stmia	%0!, {r2, r3, ip, lr}		@ 4\n\
+	stmia	%0!, {r2, r3, ip, lr}		@ 4\n\
+	stmia	%0!, {r2, r3, ip, lr}		@ 4\n\
+	stmia	%0!, {r2, r3, ip, lr}		@ 4\n\
 	subs	r1, r1, #1			@ 1\n\
 	bne	1b				@ 1\n\
-	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache\n\
-	ldr	pc, [sp], #4"
+	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache"
 	:
-	: "I" (PAGE_SIZE / 64));
+	: "r" (kaddr), "I" (PAGE_SIZE / 64)
+	: "r1", "r2", "r3", "ip", "lr");
+	kunmap_atomic(kaddr, KM_USER0);
 }
 
 struct cpu_user_fns v4wt_user_fns __initdata = {
-	.cpu_clear_user_page	= v4wt_clear_user_page,
+	.cpu_clear_user_highpage = v4wt_clear_user_highpage,
 	.cpu_copy_user_highpage	= v4wt_copy_user_highpage,
 };

commit 063b0a4207e43acbeff3d4b09f43e750e0212b48
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 15:08:35 2008 +0000

    [ARM] copypage: provide our own copy_user_highpage()
    
    We used to override the copy_user_page() function.  However, this
    is not only inefficient, it also causes additional complexity for
    highmem support, since we convert from a struct page to a kernel
    direct mapped address and back to a struct page again.
    
    Moreover, with highmem support, we end up pointlessly setting up
    kmap entries for pages which we're going to remap.  So, push the
    kmapping down into the copypage implementation files where it's
    required.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
index d8ef39503ff0..86c2cfdbde03 100644
--- a/arch/arm/mm/copypage-v4wt.c
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -11,18 +11,17 @@
  *  the only supported cache operation.
  */
 #include <linux/init.h>
-
-#include <asm/page.h>
+#include <linux/highmem.h>
 
 /*
- * ARMv4 optimised copy_user_page
+ * ARMv4 optimised copy_user_highpage
  *
  * Since we have writethrough caches, we don't have to worry about
  * dirty data in the cache.  However, we do have to ensure that
  * subsequent reads are up to date.
  */
-void __attribute__((naked))
-v4wt_copy_user_page(void *kto, const void *kfrom, unsigned long vaddr)
+static void __attribute__((naked))
+v4wt_copy_user_page(void *kto, const void *kfrom)
 {
 	asm("\
 	stmfd	sp!, {r4, lr}			@ 2\n\
@@ -44,6 +43,18 @@ v4wt_copy_user_page(void *kto, const void *kfrom, unsigned long vaddr)
 	: "I" (PAGE_SIZE / 64));
 }
 
+void v4wt_copy_user_highpage(struct page *to, struct page *from,
+	unsigned long vaddr)
+{
+	void *kto, *kfrom;
+
+	kto = kmap_atomic(to, KM_USER0);
+	kfrom = kmap_atomic(from, KM_USER1);
+	v4wt_copy_user_page(kto, kfrom);
+	kunmap_atomic(kfrom, KM_USER1);
+	kunmap_atomic(kto, KM_USER0);
+}
+
 /*
  * ARMv4 optimised clear_user_page
  *
@@ -73,5 +84,5 @@ v4wt_clear_user_page(void *kaddr, unsigned long vaddr)
 
 struct cpu_user_fns v4wt_user_fns __initdata = {
 	.cpu_clear_user_page	= v4wt_clear_user_page,
-	.cpu_copy_user_page	= v4wt_copy_user_page,
+	.cpu_copy_user_highpage	= v4wt_copy_user_highpage,
 };

commit d73e60b7144a86baf0fdfcc9537a70bb4f72e11c
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 13:08:02 2008 +0000

    [ARM] copypage: convert assembly files to C
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-v4wt.c b/arch/arm/mm/copypage-v4wt.c
new file mode 100644
index 000000000000..d8ef39503ff0
--- /dev/null
+++ b/arch/arm/mm/copypage-v4wt.c
@@ -0,0 +1,77 @@
+/*
+ *  linux/arch/arm/mm/copypage-v4wt.S
+ *
+ *  Copyright (C) 1995-1999 Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *  This is for CPUs with a writethrough cache and 'flush ID cache' is
+ *  the only supported cache operation.
+ */
+#include <linux/init.h>
+
+#include <asm/page.h>
+
+/*
+ * ARMv4 optimised copy_user_page
+ *
+ * Since we have writethrough caches, we don't have to worry about
+ * dirty data in the cache.  However, we do have to ensure that
+ * subsequent reads are up to date.
+ */
+void __attribute__((naked))
+v4wt_copy_user_page(void *kto, const void *kfrom, unsigned long vaddr)
+{
+	asm("\
+	stmfd	sp!, {r4, lr}			@ 2\n\
+	mov	r2, %0				@ 1\n\
+	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
+1:	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmia	r1!, {r3, r4, ip, lr}		@ 4+1\n\
+	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
+	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmia	r1!, {r3, r4, ip, lr}		@ 4\n\
+	subs	r2, r2, #1			@ 1\n\
+	stmia	r0!, {r3, r4, ip, lr}		@ 4\n\
+	ldmneia	r1!, {r3, r4, ip, lr}		@ 4\n\
+	bne	1b				@ 1\n\
+	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache\n\
+	ldmfd	sp!, {r4, pc}			@ 3"
+	:
+	: "I" (PAGE_SIZE / 64));
+}
+
+/*
+ * ARMv4 optimised clear_user_page
+ *
+ * Same story as above.
+ */
+void __attribute__((naked))
+v4wt_clear_user_page(void *kaddr, unsigned long vaddr)
+{
+	asm("\
+	str	lr, [sp, #-4]!\n\
+	mov	r1, %0				@ 1\n\
+	mov	r2, #0				@ 1\n\
+	mov	r3, #0				@ 1\n\
+	mov	ip, #0				@ 1\n\
+	mov	lr, #0				@ 1\n\
+1:	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
+	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
+	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
+	stmia	r0!, {r2, r3, ip, lr}		@ 4\n\
+	subs	r1, r1, #1			@ 1\n\
+	bne	1b				@ 1\n\
+	mcr	p15, 0, r2, c7, c7, 0		@ flush ID cache\n\
+	ldr	pc, [sp], #4"
+	:
+	: "I" (PAGE_SIZE / 64));
+}
+
+struct cpu_user_fns v4wt_user_fns __initdata = {
+	.cpu_clear_user_page	= v4wt_clear_user_page,
+	.cpu_copy_user_page	= v4wt_copy_user_page,
+};
