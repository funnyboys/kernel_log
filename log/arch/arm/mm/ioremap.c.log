commit e05c7b1f2bc4b7b28199b9a7572f73436d97317e
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:33:05 2020 -0700

    mm: pgtable: add shortcuts for accessing kernel PMD and PTE
    
    The powerpc 32-bit implementation of pgtable has nice shortcuts for
    accessing kernel PMD and PTE for a given virtual address.  Make these
    helpers available for all architectures.
    
    [rppt@linux.ibm.com: microblaze: fix page table traversal in setup_rt_frame()]
      Link: http://lkml.kernel.org/r/20200518191511.GD1118872@kernel.org
    [akpm@linux-foundation.org: s/pmd_ptr_k/pmd_off_k/ in various powerpc places]
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-9-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 75529d76d28c..000e8210000b 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -141,16 +141,8 @@ void __check_vmalloc_seq(struct mm_struct *mm)
 static void unmap_area_sections(unsigned long virt, unsigned long size)
 {
 	unsigned long addr = virt, end = virt + (size & ~(SZ_1M - 1));
-	pgd_t *pgd;
-	p4d_t *p4d;
-	pud_t *pud;
-	pmd_t *pmdp;
-
-	flush_cache_vunmap(addr, end);
-	pgd = pgd_offset_k(addr);
-	p4d = p4d_offset(pgd, addr);
-	pud = pud_offset(p4d, addr);
-	pmdp = pmd_offset(pud, addr);
+	pmd_t *pmdp = pmd_off_k(addr);
+
 	do {
 		pmd_t pmd = *pmdp;
 
@@ -191,10 +183,7 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 		    size_t size, const struct mem_type *type)
 {
 	unsigned long addr = virt, end = virt + size;
-	pgd_t *pgd;
-	p4d_t *p4d;
-	pud_t *pud;
-	pmd_t *pmd;
+	pmd_t *pmd = pmd_off_k(addr);
 
 	/*
 	 * Remove and free any PTE-based mapping, and
@@ -202,10 +191,6 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 	 */
 	unmap_area_sections(virt, size);
 
-	pgd = pgd_offset_k(addr);
-	p4d = p4d_offset(pgd, addr);
-	pud = pud_offset(p4d, addr);
-	pmd = pmd_offset(pud, addr);
 	do {
 		pmd[0] = __pmd(__pfn_to_phys(pfn) | type->prot_sect);
 		pfn += SZ_1M >> PAGE_SHIFT;
@@ -225,21 +210,13 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 			 size_t size, const struct mem_type *type)
 {
 	unsigned long addr = virt, end = virt + size;
-	pgd_t *pgd;
-	p4d_t *p4d;
-	pud_t *pud;
-	pmd_t *pmd;
+	pmd_t *pmd = pmd_off_k(addr);
 
 	/*
 	 * Remove and free any PTE-based mapping, and
 	 * sync the current kernel mapping.
 	 */
 	unmap_area_sections(virt, size);
-
-	pgd = pgd_offset_k(virt);
-	p4d = p4d_offset(pgd, addr);
-	pud = pud_offset(p4d, addr);
-	pmd = pmd_offset(pud, addr);
 	do {
 		unsigned long super_pmd_val, i;
 

commit 84e6ffb2c49c7901a9efb54b497d2eb84c3bef8c
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Jun 4 16:46:19 2020 -0700

    arm: add support for folded p4d page tables
    
    Implement primitives necessary for the 4th level folding, add walks of p4d
    level where appropriate, and remove __ARCH_USE_5LEVEL_HACK.
    
    [rppt@linux.ibm.com: fix kexec]
      Link: http://lkml.kernel.org/r/20200508174232.GA759899@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200414153455.21744-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 72286f9a4d30..75529d76d28c 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -142,12 +142,14 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 {
 	unsigned long addr = virt, end = virt + (size & ~(SZ_1M - 1));
 	pgd_t *pgd;
+	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmdp;
 
 	flush_cache_vunmap(addr, end);
 	pgd = pgd_offset_k(addr);
-	pud = pud_offset(pgd, addr);
+	p4d = p4d_offset(pgd, addr);
+	pud = pud_offset(p4d, addr);
 	pmdp = pmd_offset(pud, addr);
 	do {
 		pmd_t pmd = *pmdp;
@@ -190,6 +192,7 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 {
 	unsigned long addr = virt, end = virt + size;
 	pgd_t *pgd;
+	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 
@@ -200,7 +203,8 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 	unmap_area_sections(virt, size);
 
 	pgd = pgd_offset_k(addr);
-	pud = pud_offset(pgd, addr);
+	p4d = p4d_offset(pgd, addr);
+	pud = pud_offset(p4d, addr);
 	pmd = pmd_offset(pud, addr);
 	do {
 		pmd[0] = __pmd(__pfn_to_phys(pfn) | type->prot_sect);
@@ -222,6 +226,7 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 {
 	unsigned long addr = virt, end = virt + size;
 	pgd_t *pgd;
+	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 
@@ -232,7 +237,8 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 	unmap_area_sections(virt, size);
 
 	pgd = pgd_offset_k(virt);
-	pud = pud_offset(pgd, addr);
+	p4d = p4d_offset(pgd, addr);
+	pud = pud_offset(p4d, addr);
 	pmd = pmd_offset(pud, addr);
 	do {
 		unsigned long super_pmd_val, i;

commit 6a22d824a4154004c25850b0e1b710c2a88fb348
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Aug 11 14:03:29 2019 +0200

    arm: remove ioremap_cached
    
    No users of ioremap_cached are left, remove it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index d42b93316183..72286f9a4d30 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -382,15 +382,11 @@ void __iomem *ioremap(resource_size_t res_cookie, size_t size)
 EXPORT_SYMBOL(ioremap);
 
 void __iomem *ioremap_cache(resource_size_t res_cookie, size_t size)
-	__alias(ioremap_cached);
-
-void __iomem *ioremap_cached(resource_size_t res_cookie, size_t size)
 {
 	return arch_ioremap_caller(res_cookie, size, MT_DEVICE_CACHED,
 				   __builtin_return_address(0));
 }
 EXPORT_SYMBOL(ioremap_cache);
-EXPORT_SYMBOL(ioremap_cached);
 
 void __iomem *ioremap_wc(resource_size_t res_cookie, size_t size)
 {

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 5bf9443cfbaa..d42b93316183 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/arch/arm/mm/ioremap.c
  *

commit 3a58ac65e2d7969bcdf1b6acb70fa4d12a88e53e
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Thu Sep 13 16:48:08 2018 +0100

    ARM: 8799/1: mm: fix pci_ioremap_io() offset check
    
    IO_SPACE_LIMIT is the ending address of the PCI IO space, i.e
    something like 0xfffff (and not 0x100000).
    
    Therefore, when offset = 0xf0000 is passed as argument, this function
    fails even though the offset + SZ_64K fits below the
    IO_SPACE_LIMIT. This makes the last chunk of 64 KB of the I/O space
    not usable as it cannot be mapped.
    
    This patch fixes that by substracing 1 to offset + SZ_64K, so that we
    compare the addrss of the last byte of the I/O space against
    IO_SPACE_LIMIT instead of the address of the first byte of what is
    after the I/O space.
    
    Fixes: c2794437091a4 ("ARM: Add fixed PCI i/o mapping")
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@bootlin.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index fc91205ff46c..5bf9443cfbaa 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -473,7 +473,7 @@ void pci_ioremap_set_mem_type(int mem_type)
 
 int pci_ioremap_io(unsigned int offset, phys_addr_t phys_addr)
 {
-	BUG_ON(offset + SZ_64K > IO_SPACE_LIMIT);
+	BUG_ON(offset + SZ_64K - 1 > IO_SPACE_LIMIT);
 
 	return ioremap_page_range(PCI_IO_VIRT_BASE + offset,
 				  PCI_IO_VIRT_BASE + offset + SZ_64K,

commit b9cdbe6e39351f0ba6cc0c5bc218443f0898e123
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Wed Apr 19 17:48:53 2017 +0100

    ARM: Implement pci_remap_cfgspace() interface
    
    The PCI bus specification (rev 3.0, 3.2.5 "Transaction Ordering and
    Posting") defines rules for PCI configuration space transactions ordering
    and posting, that state that configuration writes have to be non-posted
    transactions.
    
    Current ioremap interface on ARM provides mapping functions that provide
    "bufferable" writes transactions (ie ioremap uses MT_DEVICE memory type)
    aka posted writes, so PCI host controller drivers have no arch interface to
    remap PCI configuration space with memory attributes that comply with the
    PCI specifications for configuration space.
    
    Implement an ARM specific pci_remap_cfgspace() interface that allows to map
    PCI config memory regions with MT_UNCACHED memory type (ie strongly ordered
    - non-posted writes), providing a remap function that complies with PCI
    specifications for config space transactions.
    
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@armlinux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index ff0eed23ddf1..fc91205ff46c 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -481,6 +481,13 @@ int pci_ioremap_io(unsigned int offset, phys_addr_t phys_addr)
 				  __pgprot(get_mem_type(pci_ioremap_mem_type)->prot_pte));
 }
 EXPORT_SYMBOL_GPL(pci_ioremap_io);
+
+void __iomem *pci_remap_cfgspace(resource_size_t res_cookie, size_t size)
+{
+	return arch_ioremap_caller(res_cookie, size, MT_UNCACHED,
+				   __builtin_return_address(0));
+}
+EXPORT_SYMBOL_GPL(pci_remap_cfgspace);
 #endif
 
 /*

commit 9ab9e4fce45379cb6a7dbf87cf8f8e6ba01853c2
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Mon Feb 22 15:02:08 2016 +0100

    ARM: memremap: implement arch_memremap_wb()
    
    The generic memremap() falls back to using ioremap_cache() to create
    MEMREMAP_WB mappings if the requested region is not already covered
    by the linear mapping, unless the architecture provides an implementation
    of arch_memremap_wb().
    
    Since ioremap_cache() is not appropriate on ARM to map memory with the
    same attributes used for the linear mapping, implement arch_memremap_wb()
    which does exactly that. Also, relax the WARN() check to allow MT_MEMORY_RW
    mappings of pfn_valid() pages.
    
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index d5350f6af089..ff0eed23ddf1 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -297,9 +297,10 @@ static void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	}
 
 	/*
-	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
+	 * Don't allow RAM to be mapped with mismatched attributes - this
+	 * causes problems with ARMv6+
 	 */
-	if (WARN_ON(pfn_valid(pfn)))
+	if (WARN_ON(pfn_valid(pfn) && mtype != MT_MEMORY_RW))
 		return NULL;
 
 	area = get_vm_area_caller(size, VM_IOREMAP, caller);
@@ -418,6 +419,13 @@ __arm_ioremap_exec(phys_addr_t phys_addr, size_t size, bool cached)
 			__builtin_return_address(0));
 }
 
+void *arch_memremap_wb(phys_addr_t phys_addr, size_t size)
+{
+	return (__force void *)arch_ioremap_caller(phys_addr, size,
+						   MT_MEMORY_RW,
+						   __builtin_return_address(0));
+}
+
 void __iounmap(volatile void __iomem *io_addr)
 {
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);

commit 20c5ea4fc131dc45c2639653b5b7aeeb2d4d0d1e
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Fri Mar 4 10:05:39 2016 +0100

    ARM: reintroduce ioremap_cached() for creating cached I/O mappings
    
    The original ARM-only ioremap flavor 'ioremap_cached' has been renamed
    to 'ioremap_cache' to align with other architectures, and subsequently
    abused in generic code to map things like firmware tables in memory.
    For that reason, there is currently an effort underway to deprecate
    ioremap_cache, whose semantics are poorly defined, and which is typed
    with an __iomem annotation that is inappropriate for mappings of ordinary
    memory.
    
    However, original users of ioremap_cached() used it in a context where
    the I/O connotation is appropriate, and replacing those instances with
    memremap() does not make sense. So let's revive ioremap_cached(), so
    that we can change back those original users before we drop ioremap_cache
    entirely in favor of memremap.
    
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 66a978d05958..d5350f6af089 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -380,11 +380,15 @@ void __iomem *ioremap(resource_size_t res_cookie, size_t size)
 EXPORT_SYMBOL(ioremap);
 
 void __iomem *ioremap_cache(resource_size_t res_cookie, size_t size)
+	__alias(ioremap_cached);
+
+void __iomem *ioremap_cached(resource_size_t res_cookie, size_t size)
 {
 	return arch_ioremap_caller(res_cookie, size, MT_DEVICE_CACHED,
 				   __builtin_return_address(0));
 }
 EXPORT_SYMBOL(ioremap_cache);
+EXPORT_SYMBOL(ioremap_cached);
 
 void __iomem *ioremap_wc(resource_size_t res_cookie, size_t size)
 {

commit 2937367b8a4b0d46ce3312cb997e4a240b02cf15
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 1 08:59:28 2015 +0200

    ARM: add support for generic early_ioremap/early_memremap
    
    This enables the generic early_ioremap implementation for ARM.
    
    It uses the fixmap region reserved for kmap. Since early_ioremap
    is only supported before paging_init(), and kmap is only supported
    afterwards, this is guaranteed not to cause any clashes.
    
    Tested-by: Ryan Harkin <ryan.harkin@linaro.org>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 0c81056c1dd7..66a978d05958 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -30,6 +30,7 @@
 #include <asm/cp15.h>
 #include <asm/cputype.h>
 #include <asm/cacheflush.h>
+#include <asm/early_ioremap.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
@@ -469,3 +470,11 @@ int pci_ioremap_io(unsigned int offset, phys_addr_t phys_addr)
 }
 EXPORT_SYMBOL_GPL(pci_ioremap_io);
 #endif
+
+/*
+ * Must be called after early_fixmap_init
+ */
+void __init early_ioremap_init(void)
+{
+	early_ioremap_setup();
+}

commit 20a1080dff2f1be8933baa0d910c41882c7279ee
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jul 1 10:06:32 2015 +0100

    ARM: io: convert ioremap*() to functions
    
    Convert the ioremap*() preprocessor macros to real functions, moving
    them out of line.  This allows us to kill off __arm_ioremap(), and
    __arm_iounmap() helpers, and remove __arm_ioremap_pfn_caller() from
    global view.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index d1e5ad7ab3bc..0c81056c1dd7 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -255,7 +255,7 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 }
 #endif
 
-void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
+static void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	unsigned long offset, size_t size, unsigned int mtype, void *caller)
 {
 	const struct mem_type *type;
@@ -363,7 +363,7 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 		  unsigned int mtype)
 {
 	return __arm_ioremap_pfn_caller(pfn, offset, size, mtype,
-			__builtin_return_address(0));
+					__builtin_return_address(0));
 }
 EXPORT_SYMBOL(__arm_ioremap_pfn);
 
@@ -371,13 +371,26 @@ void __iomem * (*arch_ioremap_caller)(phys_addr_t, size_t,
 				      unsigned int, void *) =
 	__arm_ioremap_caller;
 
-void __iomem *
-__arm_ioremap(phys_addr_t phys_addr, size_t size, unsigned int mtype)
+void __iomem *ioremap(resource_size_t res_cookie, size_t size)
+{
+	return arch_ioremap_caller(res_cookie, size, MT_DEVICE,
+				   __builtin_return_address(0));
+}
+EXPORT_SYMBOL(ioremap);
+
+void __iomem *ioremap_cache(resource_size_t res_cookie, size_t size)
+{
+	return arch_ioremap_caller(res_cookie, size, MT_DEVICE_CACHED,
+				   __builtin_return_address(0));
+}
+EXPORT_SYMBOL(ioremap_cache);
+
+void __iomem *ioremap_wc(resource_size_t res_cookie, size_t size)
 {
-	return arch_ioremap_caller(phys_addr, size, mtype,
-		__builtin_return_address(0));
+	return arch_ioremap_caller(res_cookie, size, MT_DEVICE_WC,
+				   __builtin_return_address(0));
 }
-EXPORT_SYMBOL(__arm_ioremap);
+EXPORT_SYMBOL(ioremap_wc);
 
 /*
  * Remap an arbitrary physical address space into the kernel virtual
@@ -431,11 +444,11 @@ void __iounmap(volatile void __iomem *io_addr)
 
 void (*arch_iounmap)(volatile void __iomem *) = __iounmap;
 
-void __arm_iounmap(volatile void __iomem *io_addr)
+void iounmap(volatile void __iomem *cookie)
 {
-	arch_iounmap(io_addr);
+	arch_iounmap(cookie);
 }
-EXPORT_SYMBOL(__arm_iounmap);
+EXPORT_SYMBOL(iounmap);
 
 #ifdef CONFIG_PCI
 static int pci_ioremap_mem_type = MT_DEVICE;

commit 1c8c3cf0b5239388e712508a85821f4718f4d889
Author: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
Date:   Mon May 19 11:04:39 2014 +0100

    ARM: 8060/1: mm: allow sub-architectures to override PCI I/O memory type
    
    Due to a design incompatibility between the PCIe Marvell controller
    and the Cortex-A9, stressing PCIe devices with a lot of traffic
    quickly causes a deadlock.
    
    One part of the workaround for this is to have all PCIe regions mapped
    as strongly-ordered (MT_UNCACHED) instead of the default
    MT_DEVICE. While the arch_ioremap_caller() mechanism allows
    sub-architecture code to override ioremap(), used to map PCIe memory
    regions, there isn't such a mechanism to override the behavior of
    pci_ioremap_io().
    
    This commit adds the arch_pci_ioremap_mem_type variable, initialized
    to MT_DEVICE by default, and that sub-architecture code can
    override. We have chosen to expose a single variable rather than
    offering the possibility of overriding the entire pci_ioremap_io(),
    because implementing pci_ioremap_io() requires calling functions
    (get_mem_type()) that are private to the arch/arm/mm/ code.
    
    Signed-off-by: Thomas Petazzoni <thomas.petazzoni@free-electrons.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index f9c32ba73544..d1e5ad7ab3bc 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -438,6 +438,13 @@ void __arm_iounmap(volatile void __iomem *io_addr)
 EXPORT_SYMBOL(__arm_iounmap);
 
 #ifdef CONFIG_PCI
+static int pci_ioremap_mem_type = MT_DEVICE;
+
+void pci_ioremap_set_mem_type(int mem_type)
+{
+	pci_ioremap_mem_type = mem_type;
+}
+
 int pci_ioremap_io(unsigned int offset, phys_addr_t phys_addr)
 {
 	BUG_ON(offset + SZ_64K > IO_SPACE_LIMIT);
@@ -445,7 +452,7 @@ int pci_ioremap_io(unsigned int offset, phys_addr_t phys_addr)
 	return ioremap_page_range(PCI_IO_VIRT_BASE + offset,
 				  PCI_IO_VIRT_BASE + offset + SZ_64K,
 				  phys_addr,
-				  __pgprot(get_mem_type(MT_DEVICE)->prot_pte));
+				  __pgprot(get_mem_type(pci_ioremap_mem_type)->prot_pte));
 }
 EXPORT_SYMBOL_GPL(pci_ioremap_io);
 #endif

commit 2e2c9de207be043ee80161971c814d740759d3bc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Oct 24 10:26:40 2013 +0100

    ARM: add permission annotations to MT_MEMORY* mapping types
    
    Document the permissions which the various MT_MEMORY* mapping types
    will provide.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index f123d6eb074b..f9c32ba73544 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -392,9 +392,9 @@ __arm_ioremap_exec(phys_addr_t phys_addr, size_t size, bool cached)
 	unsigned int mtype;
 
 	if (cached)
-		mtype = MT_MEMORY;
+		mtype = MT_MEMORY_RWX;
 	else
-		mtype = MT_MEMORY_NONCACHED;
+		mtype = MT_MEMORY_RWX_NONCACHED;
 
 	return __arm_ioremap_caller(phys_addr, size, mtype,
 			__builtin_return_address(0));

commit 9b97173e785a54c5df0aa23d1e1f680f61e36e43
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Thu May 16 19:40:22 2013 +0100

    ARM: 7728/1: mm: Use phys_addr_t properly for ioremap functions
    
    Several of the ioremap functions use unsigned long in places
    resulting in truncation if physical addresses greater than
    4G are passed in. Change the types of the functions and the
    callers accordingly.
    
    Cc: Krzysztof Halasa <khc@pm.waw.pl>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 04d9006eab1f..f123d6eb074b 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -331,10 +331,10 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	return (void __iomem *) (offset + addr);
 }
 
-void __iomem *__arm_ioremap_caller(unsigned long phys_addr, size_t size,
+void __iomem *__arm_ioremap_caller(phys_addr_t phys_addr, size_t size,
 	unsigned int mtype, void *caller)
 {
-	unsigned long last_addr;
+	phys_addr_t last_addr;
  	unsigned long offset = phys_addr & ~PAGE_MASK;
  	unsigned long pfn = __phys_to_pfn(phys_addr);
 
@@ -367,12 +367,12 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 }
 EXPORT_SYMBOL(__arm_ioremap_pfn);
 
-void __iomem * (*arch_ioremap_caller)(unsigned long, size_t,
+void __iomem * (*arch_ioremap_caller)(phys_addr_t, size_t,
 				      unsigned int, void *) =
 	__arm_ioremap_caller;
 
 void __iomem *
-__arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
+__arm_ioremap(phys_addr_t phys_addr, size_t size, unsigned int mtype)
 {
 	return arch_ioremap_caller(phys_addr, size, mtype,
 		__builtin_return_address(0));
@@ -387,7 +387,7 @@ EXPORT_SYMBOL(__arm_ioremap);
  * CONFIG_GENERIC_ALLOCATOR for allocating external memory.
  */
 void __iomem *
-__arm_ioremap_exec(unsigned long phys_addr, size_t size, bool cached)
+__arm_ioremap_exec(phys_addr_t phys_addr, size_t size, bool cached)
 {
 	unsigned int mtype;
 

commit 101eeda38c0ab8a4f916176e325d9e036d981a24
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Feb 9 06:28:06 2013 +0100

    ARM: 7646/1: mm: use static_vm for managing static mapped areas
    
    A static mapped area is ARM-specific, so it is better not to use
    generic vmalloc data structure, that is, vmlist and vmlist_lock
    for managing static mapped area. And it causes some needless overhead and
    reducing this overhead is better idea.
    
    Now, we have newly introduced static_vm infrastructure.
    With it, we don't need to iterate all mapped areas. Instead, we just
    iterate static mapped areas. It helps to reduce an overhead of finding
    matched area. And architecture dependency on vmalloc layer is removed,
    so it will help to maintainability for vmalloc layer.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 904c15e86063..04d9006eab1f 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -261,13 +261,14 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	const struct mem_type *type;
 	int err;
 	unsigned long addr;
- 	struct vm_struct * area;
+	struct vm_struct *area;
+	phys_addr_t paddr = __pfn_to_phys(pfn);
 
 #ifndef CONFIG_ARM_LPAE
 	/*
 	 * High mappings must be supersection aligned
 	 */
-	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
+	if (pfn >= 0x100000 && (paddr & ~SUPERSECTION_MASK))
 		return NULL;
 #endif
 
@@ -283,24 +284,16 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	/*
 	 * Try to reuse one of the static mapping whenever possible.
 	 */
-	read_lock(&vmlist_lock);
-	for (area = vmlist; area; area = area->next) {
-		if (!size || (sizeof(phys_addr_t) == 4 && pfn >= 0x100000))
-			break;
-		if (!(area->flags & VM_ARM_STATIC_MAPPING))
-			continue;
-		if ((area->flags & VM_ARM_MTYPE_MASK) != VM_ARM_MTYPE(mtype))
-			continue;
-		if (__phys_to_pfn(area->phys_addr) > pfn ||
-		    __pfn_to_phys(pfn) + size-1 > area->phys_addr + area->size-1)
-			continue;
-		/* we can drop the lock here as we know *area is static */
-		read_unlock(&vmlist_lock);
-		addr = (unsigned long)area->addr;
-		addr += __pfn_to_phys(pfn) - area->phys_addr;
-		return (void __iomem *) (offset + addr);
+	if (size && !(sizeof(phys_addr_t) == 4 && pfn >= 0x100000)) {
+		struct static_vm *svm;
+
+		svm = find_static_vm_paddr(paddr, size, mtype);
+		if (svm) {
+			addr = (unsigned long)svm->vm.addr;
+			addr += paddr - svm->vm.phys_addr;
+			return (void __iomem *) (offset + addr);
+		}
 	}
-	read_unlock(&vmlist_lock);
 
 	/*
 	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
@@ -312,21 +305,21 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
  	if (!area)
  		return NULL;
  	addr = (unsigned long)area->addr;
-	area->phys_addr = __pfn_to_phys(pfn);
+	area->phys_addr = paddr;
 
 #if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 	if (DOMAIN_IO == 0 &&
 	    (((cpu_architecture() >= CPU_ARCH_ARMv6) && (get_cr() & CR_XP)) ||
 	       cpu_is_xsc3()) && pfn >= 0x100000 &&
-	       !((__pfn_to_phys(pfn) | size | addr) & ~SUPERSECTION_MASK)) {
+	       !((paddr | size | addr) & ~SUPERSECTION_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;
 		err = remap_area_supersections(addr, pfn, size, type);
-	} else if (!((__pfn_to_phys(pfn) | size | addr) & ~PMD_MASK)) {
+	} else if (!((paddr | size | addr) & ~PMD_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;
 		err = remap_area_sections(addr, pfn, size, type);
 	} else
 #endif
-		err = ioremap_page_range(addr, addr + size, __pfn_to_phys(pfn),
+		err = ioremap_page_range(addr, addr + size, paddr,
 					 __pgprot(type->prot_pte));
 
 	if (err) {
@@ -410,34 +403,28 @@ __arm_ioremap_exec(unsigned long phys_addr, size_t size, bool cached)
 void __iounmap(volatile void __iomem *io_addr)
 {
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);
-	struct vm_struct *vm;
+	struct static_vm *svm;
+
+	/* If this is a static mapping, we must leave it alone */
+	svm = find_static_vm_vaddr(addr);
+	if (svm)
+		return;
 
-	read_lock(&vmlist_lock);
-	for (vm = vmlist; vm; vm = vm->next) {
-		if (vm->addr > addr)
-			break;
-		if (!(vm->flags & VM_IOREMAP))
-			continue;
-		/* If this is a static mapping we must leave it alone */
-		if ((vm->flags & VM_ARM_STATIC_MAPPING) &&
-		    (vm->addr <= addr) && (vm->addr + vm->size > addr)) {
-			read_unlock(&vmlist_lock);
-			return;
-		}
 #if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
+	{
+		struct vm_struct *vm;
+
+		vm = find_vm_area(addr);
+
 		/*
 		 * If this is a section based mapping we need to handle it
 		 * specially as the VM subsystem does not know how to handle
 		 * such a beast.
 		 */
-		if ((vm->addr == addr) &&
-		    (vm->flags & VM_ARM_SECTION_MAPPING)) {
+		if (vm && (vm->flags & VM_ARM_SECTION_MAPPING))
 			unmap_area_sections((unsigned long)vm->addr, vm->size);
-			break;
-		}
-#endif
 	}
-	read_unlock(&vmlist_lock);
+#endif
 
 	vunmap(addr);
 }

commit ed8fd2186a4e4f3b98434093b56f9b793d48443e
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Feb 9 06:28:05 2013 +0100

    ARM: 7645/1: ioremap: introduce an infrastructure for static mapped area
    
    In current implementation, we used ARM-specific flag, that is,
    VM_ARM_STATIC_MAPPING, for distinguishing ARM specific static mapped area.
    The purpose of static mapped area is to re-use static mapped area when
    entire physical address range of the ioremap request can be covered
    by this area.
    
    This implementation causes needless overhead for some cases.
    For example, assume that there is only one static mapped area and
    vmlist has 300 areas. Every time we call ioremap, we check 300 areas for
    deciding whether it is matched or not. Moreover, even if there is
    no static mapped area and vmlist has 300 areas, every time we call
    ioremap, we check 300 areas in now.
    
    If we construct a extra list for static mapped area, we can eliminate
    above mentioned overhead.
    With a extra list, if there is one static mapped area,
    we just check only one area and proceed next operation quickly.
    
    In fact, it is not a critical problem, because ioremap is not frequently
    used. But reducing overhead is better idea.
    
    Another reason for doing this work is for removing architecture dependency
    on vmalloc layer. I think that vmlist and vmlist_lock is internal data
    structure for vmalloc layer. Some codes for debugging and stat inevitably
    use vmlist and vmlist_lock. But it is preferable that they are used
    as least as possible in outside of vmalloc.c
    
    Now, I introduce an ARM-specific infrastructure for static mapped area. In
    the following patch, we will use this and resolve above mentioned problem.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 88fd86cf3d9a..904c15e86063 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -39,6 +39,70 @@
 #include <asm/mach/pci.h>
 #include "mm.h"
 
+
+LIST_HEAD(static_vmlist);
+
+static struct static_vm *find_static_vm_paddr(phys_addr_t paddr,
+			size_t size, unsigned int mtype)
+{
+	struct static_vm *svm;
+	struct vm_struct *vm;
+
+	list_for_each_entry(svm, &static_vmlist, list) {
+		vm = &svm->vm;
+		if (!(vm->flags & VM_ARM_STATIC_MAPPING))
+			continue;
+		if ((vm->flags & VM_ARM_MTYPE_MASK) != VM_ARM_MTYPE(mtype))
+			continue;
+
+		if (vm->phys_addr > paddr ||
+			paddr + size - 1 > vm->phys_addr + vm->size - 1)
+			continue;
+
+		return svm;
+	}
+
+	return NULL;
+}
+
+struct static_vm *find_static_vm_vaddr(void *vaddr)
+{
+	struct static_vm *svm;
+	struct vm_struct *vm;
+
+	list_for_each_entry(svm, &static_vmlist, list) {
+		vm = &svm->vm;
+
+		/* static_vmlist is ascending order */
+		if (vm->addr > vaddr)
+			break;
+
+		if (vm->addr <= vaddr && vm->addr + vm->size > vaddr)
+			return svm;
+	}
+
+	return NULL;
+}
+
+void __init add_static_vm_early(struct static_vm *svm)
+{
+	struct static_vm *curr_svm;
+	struct vm_struct *vm;
+	void *vaddr;
+
+	vm = &svm->vm;
+	vm_area_add_early(vm);
+	vaddr = vm->addr;
+
+	list_for_each_entry(curr_svm, &static_vmlist, list) {
+		vm = &curr_svm->vm;
+
+		if (vm->addr > vaddr)
+			break;
+	}
+	list_add_tail(&svm->list, &curr_svm->list);
+}
+
 int ioremap_page(unsigned long virt, unsigned long phys,
 		 const struct mem_type *mtype)
 {

commit 3e99675af1b25a191c467700499b1cbe5585a778
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sun Nov 25 03:24:32 2012 +0100

    ARM: 7582/2: rename kvm_seq to vmalloc_seq so to avoid confusion with KVM
    
    The kvm_seq value has nothing to do what so ever with this other KVM.
    Given that KVM support on ARM is imminent, it's best to rename kvm_seq
    into something else to clearly identify what it is about i.e. a sequence
    number for vmalloc section mappings.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 5dcc2fd46c46..88fd86cf3d9a 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -47,18 +47,18 @@ int ioremap_page(unsigned long virt, unsigned long phys,
 }
 EXPORT_SYMBOL(ioremap_page);
 
-void __check_kvm_seq(struct mm_struct *mm)
+void __check_vmalloc_seq(struct mm_struct *mm)
 {
 	unsigned int seq;
 
 	do {
-		seq = init_mm.context.kvm_seq;
+		seq = init_mm.context.vmalloc_seq;
 		memcpy(pgd_offset(mm, VMALLOC_START),
 		       pgd_offset_k(VMALLOC_START),
 		       sizeof(pgd_t) * (pgd_index(VMALLOC_END) -
 					pgd_index(VMALLOC_START)));
-		mm->context.kvm_seq = seq;
-	} while (seq != init_mm.context.kvm_seq);
+		mm->context.vmalloc_seq = seq;
+	} while (seq != init_mm.context.vmalloc_seq);
 }
 
 #if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
@@ -89,13 +89,13 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 		if (!pmd_none(pmd)) {
 			/*
 			 * Clear the PMD from the page table, and
-			 * increment the kvm sequence so others
+			 * increment the vmalloc sequence so others
 			 * notice this change.
 			 *
 			 * Note: this is still racy on SMP machines.
 			 */
 			pmd_clear(pmdp);
-			init_mm.context.kvm_seq++;
+			init_mm.context.vmalloc_seq++;
 
 			/*
 			 * Free the page table, if there was one.
@@ -112,8 +112,8 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 	 * Ensure that the active_mm is up to date - we want to
 	 * catch any use-after-iounmap cases.
 	 */
-	if (current->active_mm->context.kvm_seq != init_mm.context.kvm_seq)
-		__check_kvm_seq(current->active_mm);
+	if (current->active_mm->context.vmalloc_seq != init_mm.context.vmalloc_seq)
+		__check_vmalloc_seq(current->active_mm);
 
 	flush_tlb_kernel_range(virt, end);
 }

commit 0e51793e162ca432fc5f04178cf82b80a92c2659
Merge: 5cad3598ea0c b4874a3d2986
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 7 21:20:57 2012 +0900

    Merge branch 'for-linus' of git://git.linaro.org/people/rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "This is the first chunk of ARM updates for this merge window.
      Conflicts are expected in two files - asm/timex.h and
      mach-integrator/integrator_cp.c.  Nothing particularly stands out more
      than anything else.
    
      Most of the growth is down to the opcodes stuff from Dave Martin,
      which is countered by Rob's patches to use more of the asm-generic
      headers on ARM."
    
    (A few more conflicts grew since then, but it all looked fairly trivial)
    
    * 'for-linus' of git://git.linaro.org/people/rmk/linux-arm: (44 commits)
      ARM: 7548/1: include linux/sched.h in syscall.h
      ARM: 7541/1: Add ARM ERRATA 775420 workaround
      ARM: ensure vm_struct has its phys_addr member filled in
      ARM: 7540/1: kexec: Check segment memory addresses
      ARM: 7539/1: kexec: scan for dtb magic in segments
      ARM: 7538/1: delay: add registration mechanism for delay timer sources
      ARM: 7536/1: smp: Formalize an IPI for wakeup
      ARM: 7525/1: ptrace: use updated syscall number for syscall auditing
      ARM: 7524/1: support syscall tracing
      ARM: 7519/1: integrator: convert platform devices to Device Tree
      ARM: 7518/1: integrator: convert AMBA devices to device tree
      ARM: 7517/1: integrator: initial device tree support
      ARM: 7516/1: plat-versatile: add DT support to FPGA IRQ
      ARM: 7515/1: integrator: check PL010 base address from resource
      ARM: 7514/1: integrator: call common init function from machine
      ARM: 7522/1: arch_timers: register a time/cycle counter
      ARM: 7523/1: arch_timers: enable the use of the virtual timer
      ARM: 7531/1: mark kernelmode mem{cpy,set} non-experimental
      ARM: 7520/1: Build dtb files in all target
      ARM: Fix build warning in arch/arm/mm/alignment.c
      ...

commit a3d7193e3c5d0b2365b9247ef0d4cb549187f32f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Sep 28 13:48:48 2012 +0100

    ARM: ensure vm_struct has its phys_addr member filled in
    
    This allows /proc/vmallocinfo to show the physical address for
    ioremap mappings.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 566750fa57d4..491f6b3336f5 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -247,6 +247,7 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
  	if (!area)
  		return NULL;
  	addr = (unsigned long)area->addr;
+	area->phys_addr = __pfn_to_phys(pfn);
 
 #if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 	if (DOMAIN_IO == 0 &&

commit 19ec6caca2da706f11646249ba280177fec359fa
Merge: 0d7614f09c1e dd9bf78040fa
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Aug 13 16:56:29 2012 +0200

    Merge branch 'cleanup/io-pci' into next/cleanups
    
    From Rob Herring <robherring2@gmail.com>:
    
    This is the 2nd part of mach/io.h removals. This series removes io.h on
    platforms with PCI by creating a fixed virtual I/O mapping and a common
    __io() macro.
    
    This version has changed a bit to accommodate Tegra converting its PCIe
    host to a platform driver. Now the virtual space is only reserved during
    early boot before .map_io() is called. The mapping is not created until
    calling pci_ioremap_io which can be done at any point after vmalloc is
    initialized.
    
    I've gone back to fixed 64K windows for each PCI bus. This allows
    removing all the i/o resource setup from the individually platforms and
    placing it within the common ARM PCI code.
    
    I've only tested versatilepb under qemu (with the model hacked up to
    actually enable i/o space), so any testing is appreciated. iop3xx and
    mv78xx0 have some risk of breaking as the PCI bus addresses are moved
    to 0 from matching the cpu host bus addesss.
    
    * cleanup/io-pci:
      ARM: iop3xx: use fixed PCI i/o mapping
      ARM: mv78xx0: use fixed pci i/o mapping
      ARM: iop13xx: use fixed PCI i/o mapping
      iop13xx: use more regular PCI I/O space handling
      ARM: orion5x: use fixed PCI i/o mapping
      ARM: kirkwood: use fixed PCI i/o mapping
      ARM: dove: use fixed PCI i/o mapping
      ARM: footbridge: use fixed PCI i/o mapping
      ARM: shark: use fixed PCI i/o mapping
      ARM: integrator: remove trailing whitespace on pci_v3.c
      ARM: integrator: use fixed PCI i/o mapping
      ARM: tegra: use fixed PCI i/o mapping
      ARM: versatile: use fixed PCI i/o mapping
      ARM: move PCI i/o resource setup into common code
      ARM: Add fixed PCI i/o mapping
      i2c: iop3xx: use standard gpiolib functions
      i2c: iop3xx: clean-up trailing whitespace
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

commit c2794437091a4fda72c4a4f3567dd728dcc0c3c9
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Wed Feb 29 18:10:58 2012 -0600

    ARM: Add fixed PCI i/o mapping
    
    This adds a fixed virtual mapping for PCI i/o addresses. The mapping is
    located at the last 2MB of vmalloc region (0xfee00000-0xff000000). 2MB
    is used to align with PMD size, but IO_SPACE_LIMIT is 1MB. The space
    is reserved after .map_io and can be mapped at any time later with
    pci_ioremap_io. Platforms which need early i/o mapping (e.g. for vga
    console) can call pci_map_io_early in their .map_io function.
    
    This has changed completely from the 1st implementation which only
    supported creating the static mapping at .map_io.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 4f55f5062ab7..8727802f8661 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -36,6 +36,7 @@
 #include <asm/system_info.h>
 
 #include <asm/mach/map.h>
+#include <asm/mach/pci.h>
 #include "mm.h"
 
 int ioremap_page(unsigned long virt, unsigned long phys,
@@ -383,3 +384,16 @@ void __arm_iounmap(volatile void __iomem *io_addr)
 	arch_iounmap(io_addr);
 }
 EXPORT_SYMBOL(__arm_iounmap);
+
+#ifdef CONFIG_PCI
+int pci_ioremap_io(unsigned int offset, phys_addr_t phys_addr)
+{
+	BUG_ON(offset + SZ_64K > IO_SPACE_LIMIT);
+
+	return ioremap_page_range(PCI_IO_VIRT_BASE + offset,
+				  PCI_IO_VIRT_BASE + offset + SZ_64K,
+				  phys_addr,
+				  __pgprot(get_mem_type(MT_DEVICE)->prot_pte));
+}
+EXPORT_SYMBOL_GPL(pci_ioremap_io);
+#endif

commit 158e8bfe802f730f9ea7cde32eee8b43285bdd4a
Author: Alessandro Rubini <rubini@gnudd.com>
Date:   Sun Jun 24 12:46:26 2012 +0100

    ARM: 7432/1: use the new linux/sizes.h
    
    Signed-off-by: Alessandro Rubini <rubini@gnudd.com>
    Acked-by: Giancarlo Asnaghi <giancarlo.asnaghi@st.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Cc: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 4f55f5062ab7..566750fa57d4 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -25,6 +25,7 @@
 #include <linux/mm.h>
 #include <linux/vmalloc.h>
 #include <linux/io.h>
+#include <linux/sizes.h>
 
 #include <asm/cp15.h>
 #include <asm/cputype.h>
@@ -32,7 +33,6 @@
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
-#include <asm/sizes.h>
 #include <asm/system_info.h>
 
 #include <asm/mach/map.h>

commit 820d41cf0cd0e94a5661e093821e2e5c6b36a9d8
Merge: 6268b325c306 88b48684fe2d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:02:10 2012 -0700

    Merge tag 'cleanup2' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull "ARM: cleanups of io includes" from Olof Johansson:
     "Rob Herring has done a sweeping change cleaning up all of the
      mach/io.h includes, moving some of the oft-repeated macros to a common
      location and removing a bunch of boiler plate.  This is another step
      closer to a common zImage for multiple platforms."
    
    Fix up various fairly trivial conflicts (<mach/io.h> removal vs changes
    around it, tegra localtimer.o is *still* gone, yadda-yadda).
    
    * tag 'cleanup2' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (29 commits)
      ARM: tegra: Include assembler.h in sleep.S to fix build break
      ARM: pxa: use common IOMEM definition
      ARM: dma-mapping: convert ARCH_HAS_DMA_SET_COHERENT_MASK to kconfig symbol
      ARM: __io abuse cleanup
      ARM: create a common IOMEM definition
      ARM: iop13xx: fix missing declaration of iop13xx_init_early
      ARM: fix ioremap/iounmap for !CONFIG_MMU
      ARM: kill off __mem_pci
      ARM: remove bunch of now unused mach/io.h files
      ARM: make mach/io.h include optional
      ARM: clps711x: remove unneeded include of mach/io.h
      ARM: dove: add explicit include of dove.h to addr-map.c
      ARM: at91: add explicit include of hardware.h to uncompressor
      ARM: ep93xx: clean-up mach/io.h
      ARM: tegra: clean-up mach/io.h
      ARM: orion5x: clean-up mach/io.h
      ARM: davinci: remove unneeded mach/io.h include
      [media] davinci: remove includes of mach/io.h
      ARM: OMAP: Remove remaining includes for mach/io.h
      ARM: msm: clean-up mach/io.h
      ...

commit 9f97da78bf018206fb623cd351d454af2f105fe0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:01 2012 +0100

    Disintegrate asm/system.h for ARM
    
    Disintegrate asm/system.h for ARM.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Russell King <linux@arm.linux.org.uk>
    cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 66daf17b5e33..6780b49f2c69 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -33,6 +33,7 @@
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
 #include <asm/sizes.h>
+#include <asm/system_info.h>
 
 #include <asm/mach/map.h>
 #include "mm.h"

commit 15d07dc9c59eae51219c40253bdf920f62bb10f2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Mar 28 18:30:01 2012 +0100

    ARM: move CP15 definitions to separate header file
    
    Avoid namespace conflicts with drivers over the CP15 definitions by
    moving CP15 related prototypes and definitions to a private header
    file.
    
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> [Tegra]
    Acked-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com> [EP93xx]
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 80632e8d7538..66daf17b5e33 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -26,6 +26,7 @@
 #include <linux/vmalloc.h>
 #include <linux/io.h>
 
+#include <asm/cp15.h>
 #include <asm/cputype.h>
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>

commit 4fe7ef3a0811c33137ace0ed424dd0c01dd2d75e
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Fri Feb 10 17:05:13 2012 -0600

    ARM: provide runtime hook for ioremap/iounmap
    
    We have compile time over-ride of ioremap and iounmap, but an run-time
    override is needed for multi-platform builds. This adds an extra function
    pointer check, but ioremap is not peformance critical. The option for
    compile time selection remains.
    
    The caller variant is used here to provide correct caller information as
    ARM can only support level 0 for __builtin_return_address.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 80632e8d7538..024629046f1f 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -306,11 +306,15 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 }
 EXPORT_SYMBOL(__arm_ioremap_pfn);
 
+void __iomem * (*arch_ioremap_caller)(unsigned long, size_t,
+				      unsigned int, void *) =
+	__arm_ioremap_caller;
+
 void __iomem *
 __arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
 {
-	return __arm_ioremap_caller(phys_addr, size, mtype,
-			__builtin_return_address(0));
+	return arch_ioremap_caller(phys_addr, size, mtype,
+		__builtin_return_address(0));
 }
 EXPORT_SYMBOL(__arm_ioremap);
 
@@ -369,4 +373,11 @@ void __iounmap(volatile void __iomem *io_addr)
 
 	vunmap(addr);
 }
-EXPORT_SYMBOL(__iounmap);
+
+void (*arch_iounmap)(volatile void __iomem *) = __iounmap;
+
+void __arm_iounmap(volatile void __iomem *io_addr)
+{
+	arch_iounmap(io_addr);
+}
+EXPORT_SYMBOL(__arm_iounmap);

commit 97f1040982d7935716e9a45a26ccd5cc8fe92f8c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jan 29 14:55:21 2012 +0000

    Revert "ARM: 7304/1: ioremap: fix boundary check when reusing static mapping"
    
    This reverts commit 3c424f359898aff48c3d5bed608ac706f8a528c3.
    
    Joachim Eastwood reports:
    | "ARM: 7304/1: ioremap: fix boundary check when reusing static mapping"
    | Commit: 3c424f359898aff48c3d5bed608ac706f8a528c3 in Linus master
    |
    | Breaks booting on my custom AT91RM9200 board.
    | There isn't any error messages or anything that indicates what goes
    | wrong it just stops after; Uncompressing Linux... done, booting the
    | kernel.
    |
    | Reverting it makes my board boot again.
    
    and further debugging reveals:
    
    ioremap: pfn=fffff phys=fffff000 offset=400 size=1000
    ioremap: area c3ffdfc0: phys_addr=200000 pfn=200 size=4000
    ioremap: found: addr fef74000 => fed73000 => fed73400
    
    Clearly, an area for pfn 0x200, 16K can't ever satisfy a request for pfn
    0xfffff.  This happens because the changed if statement becomes:
    
                    if (0x00200 > 0xfffff ||
                        0xfffff000 + 0x400 + 0x1000-1 > 0x00200000 + 0x4000-1)
    and therefore:
                    if (0x00200 > 0xfffff ||
                        0x000003ff > 0x00203fff)
    
    The if condition fails, and so we _believe_ that the SRAM mapping fits
    our request.  Clearly that's totally bogus.
    
    Moreover, the original premise of the 'fix' patch was wrong:
    |    The condition checking boundaries of the requested and existing
    |    mappings didn't take in-page offset into consideration though,
    |    which lead to obscure and hard to debug problems when requested
    |    mapping crossed end of the static one.
    
    as the code immediately above this loop does:
    
            size = PAGE_ALIGN(offset + size);
    
    so 'size' already contains the requested offset into the page.
    
    So, revert the broken 'fix'.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index ba159370fa5f..80632e8d7538 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -225,8 +225,7 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 		if ((area->flags & VM_ARM_MTYPE_MASK) != VM_ARM_MTYPE(mtype))
 			continue;
 		if (__phys_to_pfn(area->phys_addr) > pfn ||
-		    __pfn_to_phys(pfn) + offset + size-1 >
-		    area->phys_addr + area->size-1)
+		    __pfn_to_phys(pfn) + size-1 > area->phys_addr + area->size-1)
 			continue;
 		/* we can drop the lock here as we know *area is static */
 		read_unlock(&vmlist_lock);

commit 3c424f359898aff48c3d5bed608ac706f8a528c3
Author: Pawel Moll <pawel.moll@arm.com>
Date:   Thu Jan 26 11:47:11 2012 +0100

    ARM: 7304/1: ioremap: fix boundary check when reusing static mapping
    
    Since commit 576d2f2525612ecb5af029a76f21f22a3b82563d "ARM: add
    generic ioremap optimization by reusing static mappings" ioremap()
    is trying to reuse existing static mapping when possible.
    
    The condition checking boundaries of the requested and existing
    mappings didn't take in-page offset into consideration though,
    which lead to obscure and hard to debug problems when requested
    mapping crossed end of the static one.
    
    Signed-off-by: Pawel Moll <pawel.moll@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 80632e8d7538..ba159370fa5f 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -225,7 +225,8 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 		if ((area->flags & VM_ARM_MTYPE_MASK) != VM_ARM_MTYPE(mtype))
 			continue;
 		if (__phys_to_pfn(area->phys_addr) > pfn ||
-		    __pfn_to_phys(pfn) + size-1 > area->phys_addr + area->size-1)
+		    __pfn_to_phys(pfn) + offset + size-1 >
+		    area->phys_addr + area->size-1)
 			continue;
 		/* we can drop the lock here as we know *area is static */
 		read_unlock(&vmlist_lock);

commit 6ae25a5b9d7ba86d6ac19c403dfa57dae6caa73d
Merge: 3ee0fc5ca129 497b7e943d0d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Dec 8 18:02:04 2011 +0000

    Merge branch 'for-rmk' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux into devel-stable
    
    Conflicts:
            arch/arm/mm/ioremap.c

commit da02877987e6e173ebba137d4e1e155e1f1151cd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Nov 22 17:30:29 2011 +0000

    ARM: LPAE: Page table maintenance for the 3-level format
    
    This patch modifies the pgd/pmd/pte manipulation functions to support
    the 3-level page table format. Since there is no need for an 'ext'
    argument to cpu_set_pte_ext(), this patch conditionally defines a
    different prototype for this function when CONFIG_ARM_LPAE.
    
    The patch also introduces the L_PGD_SWAPPER flag to mark pgd entries
    pointing to pmd tables pre-allocated in the swapper_pg_dir and avoid
    trying to free them at run-time. This flag is 0 with the classic page
    table format.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index c3fa40da3b75..d1f78bacb015 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -64,7 +64,7 @@ void __check_kvm_seq(struct mm_struct *mm)
 	} while (seq != init_mm.context.kvm_seq);
 }
 
-#ifndef CONFIG_SMP
+#if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 /*
  * Section support is unsafe on SMP - If you iounmap and ioremap a region,
  * the other CPUs will not see this change until their next context switch.
@@ -202,11 +202,13 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	unsigned long addr;
  	struct vm_struct * area;
 
+#ifndef CONFIG_ARM_LPAE
 	/*
 	 * High mappings must be supersection aligned
 	 */
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
+#endif
 
 	/*
 	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
@@ -228,7 +230,7 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
  		return NULL;
  	addr = (unsigned long)area->addr;
 
-#ifndef CONFIG_SMP
+#if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 	if (DOMAIN_IO == 0 &&
 	    (((cpu_architecture() >= CPU_ARCH_ARMv6) && (get_cr() & CR_XP)) ||
 	       cpu_is_xsc3()) && pfn >= 0x100000 &&
@@ -320,7 +322,7 @@ __arm_ioremap_exec(unsigned long phys_addr, size_t size, bool cached)
 void __iounmap(volatile void __iomem *io_addr)
 {
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);
-#ifndef CONFIG_SMP
+#if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 	struct vm_struct **p, *tmp;
 
 	/*

commit 03a6b8274cc61fb9bb77aaa102e63840461c5f3a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Nov 22 17:30:27 2011 +0000

    ARM: pgtable: Fix compiler warning in ioremap.c introduced by nopud
    
    With the arch/arm code conversion to pgtable-nopud.h, the section and
    supersection (un|re)map code triggers compiler warnings on UP systems.
    This is caused by pmd_offset() being given a pgd_t argument rather than
    a pud_t one. This patch makes the necessary conversion with the
    assumption that the pud is folded into the pgd. The page table setting
    code only loops over the pmd which is enough with the classic page
    tables. This code is not compiled when LPAE is enabled.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index bdb248c4f55c..c3fa40da3b75 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -79,13 +79,16 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 {
 	unsigned long addr = virt, end = virt + (size & ~(SZ_1M - 1));
 	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmdp;
 
 	flush_cache_vunmap(addr, end);
 	pgd = pgd_offset_k(addr);
+	pud = pud_offset(pgd, addr);
+	pmdp = pmd_offset(pud, addr);
 	do {
-		pmd_t pmd, *pmdp = pmd_offset(pgd, addr);
+		pmd_t pmd = *pmdp;
 
-		pmd = *pmdp;
 		if (!pmd_none(pmd)) {
 			/*
 			 * Clear the PMD from the page table, and
@@ -104,8 +107,8 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 				pte_free_kernel(&init_mm, pmd_page_vaddr(pmd));
 		}
 
-		addr += PGDIR_SIZE;
-		pgd++;
+		addr += PMD_SIZE;
+		pmdp += 2;
 	} while (addr < end);
 
 	/*
@@ -124,6 +127,8 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 {
 	unsigned long addr = virt, end = virt + size;
 	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
 
 	/*
 	 * Remove and free any PTE-based mapping, and
@@ -132,17 +137,17 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 	unmap_area_sections(virt, size);
 
 	pgd = pgd_offset_k(addr);
+	pud = pud_offset(pgd, addr);
+	pmd = pmd_offset(pud, addr);
 	do {
-		pmd_t *pmd = pmd_offset(pgd, addr);
-
 		pmd[0] = __pmd(__pfn_to_phys(pfn) | type->prot_sect);
 		pfn += SZ_1M >> PAGE_SHIFT;
 		pmd[1] = __pmd(__pfn_to_phys(pfn) | type->prot_sect);
 		pfn += SZ_1M >> PAGE_SHIFT;
 		flush_pmd_entry(pmd);
 
-		addr += PGDIR_SIZE;
-		pgd++;
+		addr += PMD_SIZE;
+		pmd += 2;
 	} while (addr < end);
 
 	return 0;
@@ -154,6 +159,8 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 {
 	unsigned long addr = virt, end = virt + size;
 	pgd_t *pgd;
+	pud_t *pud;
+	pmd_t *pmd;
 
 	/*
 	 * Remove and free any PTE-based mapping, and
@@ -162,6 +169,8 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 	unmap_area_sections(virt, size);
 
 	pgd = pgd_offset_k(virt);
+	pud = pud_offset(pgd, addr);
+	pmd = pmd_offset(pud, addr);
 	do {
 		unsigned long super_pmd_val, i;
 
@@ -170,14 +179,12 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 		super_pmd_val |= ((pfn >> (32 - PAGE_SHIFT)) & 0xf) << 20;
 
 		for (i = 0; i < 8; i++) {
-			pmd_t *pmd = pmd_offset(pgd, addr);
-
 			pmd[0] = __pmd(super_pmd_val);
 			pmd[1] = __pmd(super_pmd_val);
 			flush_pmd_entry(pmd);
 
-			addr += PGDIR_SIZE;
-			pgd++;
+			addr += PMD_SIZE;
+			pmd += 2;
 		}
 
 		pfn += SUPERSECTION_SIZE >> PAGE_SHIFT;

commit 576d2f2525612ecb5af029a76f21f22a3b82563d
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Sep 16 01:14:23 2011 -0400

    ARM: add generic ioremap optimization by reusing static mappings
    
    Now that we have all the static mappings from iotable_init() located
    in the vmalloc area, it is trivial to optimize ioremap by reusing those
    static mappings when the requested physical area fits in one of them,
    and so in a generic way for all platforms.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index bc7d9bd766d1..12c7ad215ce7 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -36,12 +36,6 @@
 #include <asm/mach/map.h>
 #include "mm.h"
 
-/*
- * Used by ioremap() and iounmap() code to mark (super)section-mapped
- * I/O regions in vm_struct->flags field.
- */
-#define VM_ARM_SECTION_MAPPING	0x80000000
-
 int ioremap_page(unsigned long virt, unsigned long phys,
 		 const struct mem_type *mtype)
 {
@@ -201,12 +195,6 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
 
-	/*
-	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
-	 */
-	if (WARN_ON(pfn_valid(pfn)))
-		return NULL;
-
 	type = get_mem_type(mtype);
 	if (!type)
 		return NULL;
@@ -216,6 +204,34 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	 */
 	size = PAGE_ALIGN(offset + size);
 
+	/*
+	 * Try to reuse one of the static mapping whenever possible.
+	 */
+	read_lock(&vmlist_lock);
+	for (area = vmlist; area; area = area->next) {
+		if (!size || (sizeof(phys_addr_t) == 4 && pfn >= 0x100000))
+			break;
+		if (!(area->flags & VM_ARM_STATIC_MAPPING))
+			continue;
+		if ((area->flags & VM_ARM_MTYPE_MASK) != VM_ARM_MTYPE(mtype))
+			continue;
+		if (__phys_to_pfn(area->phys_addr) > pfn ||
+		    __pfn_to_phys(pfn) + size-1 > area->phys_addr + area->size-1)
+			continue;
+		/* we can drop the lock here as we know *area is static */
+		read_unlock(&vmlist_lock);
+		addr = (unsigned long)area->addr;
+		addr += __pfn_to_phys(pfn) - area->phys_addr;
+		return (void __iomem *) (offset + addr);
+	}
+	read_unlock(&vmlist_lock);
+
+	/*
+	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
+	 */
+	if (WARN_ON(pfn_valid(pfn)))
+		return NULL;
+
 	area = get_vm_area_caller(size, VM_IOREMAP, caller);
  	if (!area)
  		return NULL;
@@ -313,26 +329,34 @@ __arm_ioremap_exec(unsigned long phys_addr, size_t size, bool cached)
 void __iounmap(volatile void __iomem *io_addr)
 {
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);
-#ifndef CONFIG_SMP
 	struct vm_struct *vm;
 
-	/*
-	 * If this is a section based mapping we need to handle it
-	 * specially as the VM subsystem does not know how to handle
-	 * such a beast.
-	 */
 	read_lock(&vmlist_lock);
 	for (vm = vmlist; vm; vm = vm->next) {
-		if ((vm->flags & VM_IOREMAP) && (vm->addr == addr)) {
-			if (vm->flags & VM_ARM_SECTION_MAPPING) {
-				unmap_area_sections((unsigned long)vm->addr,
-						    vm->size);
-			}
+		if (vm->addr > addr)
 			break;
+		if (!(vm->flags & VM_IOREMAP))
+			continue;
+		/* If this is a static mapping we must leave it alone */
+		if ((vm->flags & VM_ARM_STATIC_MAPPING) &&
+		    (vm->addr <= addr) && (vm->addr + vm->size > addr)) {
+			read_unlock(&vmlist_lock);
+			return;
 		}
+#ifndef CONFIG_SMP
+		/*
+		 * If this is a section based mapping we need to handle it
+		 * specially as the VM subsystem does not know how to handle
+		 * such a beast.
+		 */
+		if ((vm->addr == addr) &&
+		    (vm->flags & VM_ARM_SECTION_MAPPING)) {
+			unmap_area_sections((unsigned long)vm->addr, vm->size);
+			break;
+		}
+#endif
 	}
 	read_unlock(&vmlist_lock);
-#endif
 
 	vunmap(addr);
 }

commit 6ee723a6570a897208b76ab3e9a495e9106b2f8c
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Sep 15 22:12:19 2011 -0400

    ARM: simplify __iounmap() when dealing with section based mapping
    
    Firstly, there is no need to have a double pointer here as we're only
    walking the vmlist and not modifying it.
    
    Secondly, for the same reason, we don't need a write lock but only a
    read lock here, since the lock only protects the coherency of the list
    nothing else.
    
    Lastly, the reason for holding a lock is not what the comment says, so
    let's remove that misleading piece of information.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index bdb248c4f55c..bc7d9bd766d1 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -314,26 +314,24 @@ void __iounmap(volatile void __iomem *io_addr)
 {
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);
 #ifndef CONFIG_SMP
-	struct vm_struct **p, *tmp;
+	struct vm_struct *vm;
 
 	/*
 	 * If this is a section based mapping we need to handle it
 	 * specially as the VM subsystem does not know how to handle
-	 * such a beast. We need the lock here b/c we need to clear
-	 * all the mappings before the area can be reclaimed
-	 * by someone else.
+	 * such a beast.
 	 */
-	write_lock(&vmlist_lock);
-	for (p = &vmlist ; (tmp = *p) ; p = &tmp->next) {
-		if ((tmp->flags & VM_IOREMAP) && (tmp->addr == addr)) {
-			if (tmp->flags & VM_ARM_SECTION_MAPPING) {
-				unmap_area_sections((unsigned long)tmp->addr,
-						    tmp->size);
+	read_lock(&vmlist_lock);
+	for (vm = vmlist; vm; vm = vm->next) {
+		if ((vm->flags & VM_IOREMAP) && (vm->addr == addr)) {
+			if (vm->flags & VM_ARM_SECTION_MAPPING) {
+				unmap_area_sections((unsigned long)vm->addr,
+						    vm->size);
 			}
 			break;
 		}
 	}
-	write_unlock(&vmlist_lock);
+	read_unlock(&vmlist_lock);
 #endif
 
 	vunmap(addr);

commit 6c5482d53f195d3ca61c9ec1be25b0f4a92575fe
Author: Tony Lindgren <tony@atomide.com>
Date:   Wed Oct 12 01:02:50 2011 +0100

    ARM: 7129/1: Add __arm_ioremap_exec for mapping external memory as MT_MEMORY
    
    This allows mapping external memory such as SRAM for use.
    
    This is needed for some small chunks of code, such as reprogramming
    SDRAM memory source clocks that can't be executed in SDRAM. Other
    use cases include some PM related code.
    
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Acked-by: Andres Salomon <dilinger@queued.net>
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index ab506272b2d3..bdb248c4f55c 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -289,6 +289,27 @@ __arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
 }
 EXPORT_SYMBOL(__arm_ioremap);
 
+/*
+ * Remap an arbitrary physical address space into the kernel virtual
+ * address space as memory. Needed when the kernel wants to execute
+ * code in external memory. This is needed for reprogramming source
+ * clocks that would affect normal memory for example. Please see
+ * CONFIG_GENERIC_ALLOCATOR for allocating external memory.
+ */
+void __iomem *
+__arm_ioremap_exec(unsigned long phys_addr, size_t size, bool cached)
+{
+	unsigned int mtype;
+
+	if (cached)
+		mtype = MT_MEMORY;
+	else
+		mtype = MT_MEMORY_NONCACHED;
+
+	return __arm_ioremap_caller(phys_addr, size, mtype,
+			__builtin_return_address(0));
+}
+
 void __iounmap(volatile void __iomem *io_addr)
 {
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);

commit 67cfa23ac9df810d1fbf3a06b7f408243350ecfe
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Dec 21 10:42:20 2010 +0000

    Revert "ARM: relax ioremap prohibition (309caa9) for -final and -stable"
    
    This reverts commit 06c1088, as promised in the warning message.

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 17e7b0b57e49..ab506272b2d3 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -204,12 +204,8 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	/*
 	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
 	 */
-	if (pfn_valid(pfn)) {
-		printk(KERN_WARNING "BUG: Your driver calls ioremap() on system memory.  This leads\n"
-		       KERN_WARNING "to architecturally unpredictable behaviour on ARMv6+, and ioremap()\n"
-		       KERN_WARNING "will fail in the next kernel release.  Please fix your driver.\n");
-		WARN_ON(1);
-	}
+	if (WARN_ON(pfn_valid(pfn)))
+		return NULL;
 
 	type = get_mem_type(mtype);
 	if (!type)

commit 06c10884486a63a1e4ff657aaa51e848e64b9dc3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Oct 13 00:15:25 2010 +0100

    ARM: relax ioremap prohibition (309caa9) for -final and -stable
    
    ... but produce a big warning about the problem as encouragement
    for people to fix their drivers.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index ab506272b2d3..17e7b0b57e49 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -204,8 +204,12 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	/*
 	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
 	 */
-	if (WARN_ON(pfn_valid(pfn)))
-		return NULL;
+	if (pfn_valid(pfn)) {
+		printk(KERN_WARNING "BUG: Your driver calls ioremap() on system memory.  This leads\n"
+		       KERN_WARNING "to architecturally unpredictable behaviour on ARMv6+, and ioremap()\n"
+		       KERN_WARNING "will fail in the next kernel release.  Please fix your driver.\n");
+		WARN_ON(1);
+	}
 
 	type = get_mem_type(mtype);
 	if (!type)

commit d746196361c9c635128249bb6cf13e709ae6abe1
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jul 26 10:29:13 2010 +0100

    ARM: use generic ioremap_page_range()
    
    We don't need our own implementation of this, use the generic
    library implementation instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 03f11935ed08..ab506272b2d3 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -42,78 +42,11 @@
  */
 #define VM_ARM_SECTION_MAPPING	0x80000000
 
-static int remap_area_pte(pmd_t *pmd, unsigned long addr, unsigned long end,
-			  unsigned long phys_addr, const struct mem_type *type)
-{
-	pgprot_t prot = __pgprot(type->prot_pte);
-	pte_t *pte;
-
-	pte = pte_alloc_kernel(pmd, addr);
-	if (!pte)
-		return -ENOMEM;
-
-	do {
-		if (!pte_none(*pte))
-			goto bad;
-
-		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot), 0);
-		phys_addr += PAGE_SIZE;
-	} while (pte++, addr += PAGE_SIZE, addr != end);
-	return 0;
-
- bad:
-	printk(KERN_CRIT "remap_area_pte: page already exists\n");
-	BUG();
-}
-
-static inline int remap_area_pmd(pgd_t *pgd, unsigned long addr,
-				 unsigned long end, unsigned long phys_addr,
-				 const struct mem_type *type)
-{
-	unsigned long next;
-	pmd_t *pmd;
-	int ret = 0;
-
-	pmd = pmd_alloc(&init_mm, pgd, addr);
-	if (!pmd)
-		return -ENOMEM;
-
-	do {
-		next = pmd_addr_end(addr, end);
-		ret = remap_area_pte(pmd, addr, next, phys_addr, type);
-		if (ret)
-			return ret;
-		phys_addr += next - addr;
-	} while (pmd++, addr = next, addr != end);
-	return ret;
-}
-
-static int remap_area_pages(unsigned long start, unsigned long pfn,
-			    size_t size, const struct mem_type *type)
-{
-	unsigned long addr = start;
-	unsigned long next, end = start + size;
-	unsigned long phys_addr = __pfn_to_phys(pfn);
-	pgd_t *pgd;
-	int err = 0;
-
-	BUG_ON(addr >= end);
-	pgd = pgd_offset_k(addr);
-	do {
-		next = pgd_addr_end(addr, end);
-		err = remap_area_pmd(pgd, addr, next, phys_addr, type);
-		if (err)
-			break;
-		phys_addr += next - addr;
-	} while (pgd++, addr = next, addr != end);
-
-	return err;
-}
-
 int ioremap_page(unsigned long virt, unsigned long phys,
 		 const struct mem_type *mtype)
 {
-	return remap_area_pages(virt, __phys_to_pfn(phys), PAGE_SIZE, mtype);
+	return ioremap_page_range(virt, virt + PAGE_SIZE, phys,
+				  __pgprot(mtype->prot_pte));
 }
 EXPORT_SYMBOL(ioremap_page);
 
@@ -300,7 +233,8 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 		err = remap_area_sections(addr, pfn, size, type);
 	} else
 #endif
-		err = remap_area_pages(addr, pfn, size, type);
+		err = ioremap_page_range(addr, addr + size, __pfn_to_phys(pfn),
+					 __pgprot(type->prot_pte));
 
 	if (err) {
  		vunmap((void *)addr);

commit 309caa9cc6ff39d261264ec4ff10e29489afc8f8
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jun 21 21:03:18 2010 +0100

    ARM: Prohibit ioremap() on kernel managed RAM
    
    ARMv6 and above have a restriction whereby aliasing virtual:physical
    mappings must not have differing memory type and sharability
    attributes.  Strictly, this covers the memory type (strongly ordered,
    device, memory), cache attributes (uncached, write combine, write
    through, write back read alloc, write back write alloc) and the
    shared bit.
    
    However, using ioremap() and its variants on system RAM results in
    mappings which differ in these attributes from the main system RAM
    mapping.  Other architectures which similar restrictions approch this
    problem in the same way - they do not permit ioremap on main system
    RAM.
    
    Make ARM behave in the same way, with a WARN_ON() such that users can
    be traced and an alternative approach found.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 28c8b950ef04..03f11935ed08 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -268,6 +268,12 @@ void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
 
+	/*
+	 * Don't allow RAM to be mapped - this causes problems with ARMv6+
+	 */
+	if (WARN_ON(pfn_valid(pfn)))
+		return NULL;
+
 	type = get_mem_type(mtype);
 	if (!type)
 		return NULL;

commit 31aa8fd6fd30b0f36416df7d09619768d26b4332
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Dec 18 11:10:03 2009 +0000

    ARM: Add caller information to ioremap
    
    This allows the procfs vmallocinfo file to show who created the ioremap
    regions.  Note: __builtin_return_address(0) doesn't do what's expected
    if its used in an inline function, so we leave __arm_ioremap callers
    in such places alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 0ab75c60f7cf..28c8b950ef04 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -139,8 +139,8 @@ void __check_kvm_seq(struct mm_struct *mm)
  * which requires the new ioremap'd region to be referenced, the CPU will
  * reference the _old_ region.
  *
- * Note that get_vm_area() allocates a guard 4K page, so we need to mask
- * the size back to 1MB aligned or we will overflow in the loop below.
+ * Note that get_vm_area_caller() allocates a guard 4K page, so we need to
+ * mask the size back to 1MB aligned or we will overflow in the loop below.
  */
 static void unmap_area_sections(unsigned long virt, unsigned long size)
 {
@@ -254,22 +254,8 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 }
 #endif
 
-
-/*
- * Remap an arbitrary physical address space into the kernel virtual
- * address space. Needed when the kernel wants to access high addresses
- * directly.
- *
- * NOTE! We need to allow non-page-aligned mappings too: we will obviously
- * have to convert them into an offset in a page-aligned mapping, but the
- * caller shouldn't need to know that small detail.
- *
- * 'flags' are the extra L_PTE_ flags that you want to specify for this
- * mapping.  See <asm/pgtable.h> for more information.
- */
-void __iomem *
-__arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
-		  unsigned int mtype)
+void __iomem * __arm_ioremap_pfn_caller(unsigned long pfn,
+	unsigned long offset, size_t size, unsigned int mtype, void *caller)
 {
 	const struct mem_type *type;
 	int err;
@@ -291,7 +277,7 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	 */
 	size = PAGE_ALIGN(offset + size);
 
- 	area = get_vm_area(size, VM_IOREMAP);
+	area = get_vm_area_caller(size, VM_IOREMAP, caller);
  	if (!area)
  		return NULL;
  	addr = (unsigned long)area->addr;
@@ -318,10 +304,9 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	flush_cache_vmap(addr, addr + size);
 	return (void __iomem *) (offset + addr);
 }
-EXPORT_SYMBOL(__arm_ioremap_pfn);
 
-void __iomem *
-__arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
+void __iomem *__arm_ioremap_caller(unsigned long phys_addr, size_t size,
+	unsigned int mtype, void *caller)
 {
 	unsigned long last_addr;
  	unsigned long offset = phys_addr & ~PAGE_MASK;
@@ -334,7 +319,33 @@ __arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
 	if (!size || last_addr < phys_addr)
 		return NULL;
 
- 	return __arm_ioremap_pfn(pfn, offset, size, mtype);
+	return __arm_ioremap_pfn_caller(pfn, offset, size, mtype,
+			caller);
+}
+
+/*
+ * Remap an arbitrary physical address space into the kernel virtual
+ * address space. Needed when the kernel wants to access high addresses
+ * directly.
+ *
+ * NOTE! We need to allow non-page-aligned mappings too: we will obviously
+ * have to convert them into an offset in a page-aligned mapping, but the
+ * caller shouldn't need to know that small detail.
+ */
+void __iomem *
+__arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
+		  unsigned int mtype)
+{
+	return __arm_ioremap_pfn_caller(pfn, offset, size, mtype,
+			__builtin_return_address(0));
+}
+EXPORT_SYMBOL(__arm_ioremap_pfn);
+
+void __iomem *
+__arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
+{
+	return __arm_ioremap_caller(phys_addr, size, mtype,
+			__builtin_return_address(0));
 }
 EXPORT_SYMBOL(__arm_ioremap);
 

commit 69d3a84a646d6ad6cd693a7a3d5b9af414113d2c
Author: Hiroshi DOYU <Hiroshi.DOYU@nokia.com>
Date:   Wed Jan 28 21:32:08 2009 +0200

    omap iommu: simple virtual address space management
    
    This patch provides a device drivers, which has a omap iommu, with
    address mapping APIs between device virtual address(iommu), physical
    address and MPU virtual address.
    
    There are 4 possible patterns for iommu virtual address(iova/da) mapping.
    
        |iova/                        mapping               iommu_          page
        | da        pa      va      (d)-(p)-(v)             function        type
      ---------------------------------------------------------------------------
      1 | c         c       c        1 - 1 - 1        _kmap() / _kunmap()   s
      2 | c         c,a     c        1 - 1 - 1      _kmalloc()/ _kfree()    s
      3 | c         d       c        1 - n - 1        _vmap() / _vunmap()   s
      4 | c         d,a     c        1 - n - 1      _vmalloc()/ _vfree()    n*
    
        'iova':     device iommu virtual address
        'da':       alias of 'iova'
        'pa':       physical address
        'va':       mpu virtual address
    
        'c':        contiguous memory area
        'd':        dicontiguous memory area
        'a':        anonymous memory allocation
        '()':       optional feature
    
        'n':        a normal page(4KB) size is used.
        's':        multiple iommu superpage(16MB, 1MB, 64KB, 4KB) size is used.
    
        '*':        not yet, but feasible.
    
    Signed-off-by: Hiroshi DOYU <Hiroshi.DOYU@nokia.com>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 9f88dd3be601..0ab75c60f7cf 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -110,6 +110,12 @@ static int remap_area_pages(unsigned long start, unsigned long pfn,
 	return err;
 }
 
+int ioremap_page(unsigned long virt, unsigned long phys,
+		 const struct mem_type *mtype)
+{
+	return remap_area_pages(virt, __phys_to_pfn(phys), PAGE_SIZE, mtype);
+}
+EXPORT_SYMBOL(ioremap_page);
 
 void __check_kvm_seq(struct mm_struct *mm)
 {

commit 24f11ec001920f1cfaeeed8e8b55725d900bbb56
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Jan 25 17:36:34 2009 +0000

    [ARM] fix section-based ioremap
    
    Tomi Valkeinen reports:
      Running with latest linux-omap kernel on OMAP3 SDP board, I have
      problem with iounmap(). It looks like iounmap() does not properly
      free large areas. Below is a test which fails for me in 6-7 loops.
    
            for (i = 0; i < 200; ++i) {
                    vaddr = ioremap(paddr, size);
                    if (!vaddr) {
                            printk("couldn't ioremap\n");
                            break;
                    }
                    iounmap(vaddr);
            }
    
    The changes to vmalloc.c weren't reflected in the ARM ioremap
    implementation.  Turns out the fix is rather simple.
    
    Tested-by: Tomi Valkeinen <tomi.valkeinen@nokia.com>
    Tested-by: Matt Gerassimoff <mgeras@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 18373f73f2fc..9f88dd3be601 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -138,7 +138,7 @@ void __check_kvm_seq(struct mm_struct *mm)
  */
 static void unmap_area_sections(unsigned long virt, unsigned long size)
 {
-	unsigned long addr = virt, end = virt + (size & ~SZ_1M);
+	unsigned long addr = virt, end = virt + (size & ~(SZ_1M - 1));
 	pgd_t *pgd;
 
 	flush_cache_vunmap(addr, end);
@@ -337,10 +337,7 @@ void __iounmap(volatile void __iomem *io_addr)
 	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);
 #ifndef CONFIG_SMP
 	struct vm_struct **p, *tmp;
-#endif
-	unsigned int section_mapping = 0;
 
-#ifndef CONFIG_SMP
 	/*
 	 * If this is a section based mapping we need to handle it
 	 * specially as the VM subsystem does not know how to handle
@@ -352,11 +349,8 @@ void __iounmap(volatile void __iomem *io_addr)
 	for (p = &vmlist ; (tmp = *p) ; p = &tmp->next) {
 		if ((tmp->flags & VM_IOREMAP) && (tmp->addr == addr)) {
 			if (tmp->flags & VM_ARM_SECTION_MAPPING) {
-				*p = tmp->next;
 				unmap_area_sections((unsigned long)tmp->addr,
 						    tmp->size);
-				kfree(tmp);
-				section_mapping = 1;
 			}
 			break;
 		}
@@ -364,7 +358,6 @@ void __iounmap(volatile void __iomem *io_addr)
 	write_unlock(&vmlist_lock);
 #endif
 
-	if (!section_mapping)
-		vunmap(addr);
+	vunmap(addr);
 }
 EXPORT_SYMBOL(__iounmap);

commit 6a4690c22f5da1eb1c898b61b6a80da52fbd976f
Merge: 90bb28b0644f 8ec53663d269
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Oct 9 21:31:56 2008 +0100

    Merge branch 'ptebits' into devel
    
    Conflicts:
    
            arch/arm/Kconfig

commit 40d192b63d079db1f76cec9ae8ccbf461fda23e4
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 6 21:15:56 2008 +0100

    [ARM] remove 'prot_pte_ext' from memory type table
    
    This member is now redundant; the memory type is encoded in the Linux
    PTE bits.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index b81dbf9ffb77..33eeab0ce3d1 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -55,8 +55,7 @@ static int remap_area_pte(pmd_t *pmd, unsigned long addr, unsigned long end,
 		if (!pte_none(*pte))
 			goto bad;
 
-		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot),
-			    type->prot_pte_ext);
+		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot), 0);
 		phys_addr += PAGE_SIZE;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	return 0;

commit fced80c735941fa518ac67c0b61bbe153fb8c050
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 6 12:10:45 2008 +0100

    [ARM] Convert asm/io.h to linux/io.h
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 52e5a4d28eb7..8a41912ec7c5 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -24,10 +24,10 @@
 #include <linux/errno.h>
 #include <linux/mm.h>
 #include <linux/vmalloc.h>
+#include <linux/io.h>
 
 #include <asm/cputype.h>
 #include <asm/cacheflush.h>
-#include <asm/io.h>
 #include <asm/mmu_context.h>
 #include <asm/pgalloc.h>
 #include <asm/tlbflush.h>

commit 09d9bae064724635df3920bcca47e077cfb23e76
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Sep 5 14:08:44 2008 +0100

    [ARM] sparse: fix several warnings
    
    arch/arm/kernel/process.c:270:6: warning: symbol 'show_fpregs' was not declared. Should it be static?
    
    This function isn't used, so can be removed.
    
    arch/arm/kernel/setup.c:532:9: warning: symbol 'len' shadows an earlier one
    arch/arm/kernel/setup.c:524:6: originally declared here
    
    A function containing two 'len's.
    
    arch/arm/mm/fault-armv.c:188:13: warning: symbol 'check_writebuffer_bugs' was not declared. Should it be static?
    arch/arm/mm/mmap.c:122:5: warning: symbol 'valid_phys_addr_range' was not declared. Should it be static?
    arch/arm/mm/mmap.c:137:5: warning: symbol 'valid_mmap_phys_addr_range' was not declared. Should it be static?
    
    Missing includes.
    
    arch/arm/kernel/traps.c:71:77: warning: Using plain integer as NULL pointer
    arch/arm/mm/ioremap.c:355:46: error: incompatible types in comparison expression (different address spaces)
    
    Sillies.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 20e4454e452e..52e5a4d28eb7 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -333,15 +333,14 @@ __arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
 }
 EXPORT_SYMBOL(__arm_ioremap);
 
-void __iounmap(volatile void __iomem *addr)
+void __iounmap(volatile void __iomem *io_addr)
 {
+	void *addr = (void *)(PAGE_MASK & (unsigned long)io_addr);
 #ifndef CONFIG_SMP
 	struct vm_struct **p, *tmp;
 #endif
 	unsigned int section_mapping = 0;
 
-	addr = (volatile void __iomem *)(PAGE_MASK & (unsigned long)addr);
-
 #ifndef CONFIG_SMP
 	/*
 	 * If this is a section based mapping we need to handle it
@@ -352,7 +351,7 @@ void __iounmap(volatile void __iomem *addr)
 	 */
 	write_lock(&vmlist_lock);
 	for (p = &vmlist ; (tmp = *p) ; p = &tmp->next) {
-		if((tmp->flags & VM_IOREMAP) && (tmp->addr == addr)) {
+		if ((tmp->flags & VM_IOREMAP) && (tmp->addr == addr)) {
 			if (tmp->flags & VM_ARM_SECTION_MAPPING) {
 				*p = tmp->next;
 				unmap_area_sections((unsigned long)tmp->addr,
@@ -367,6 +366,6 @@ void __iounmap(volatile void __iomem *addr)
 #endif
 
 	if (!section_mapping)
-		vunmap((void __force *)addr);
+		vunmap(addr);
 }
 EXPORT_SYMBOL(__iounmap);

commit 0ba8b9b273c45dd23f60ff700e265a0069b33758
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 18:08:10 2008 +0100

    [ARM] cputype: separate definitions, use them
    
    Add asm/cputype.h, moving functions and definitions from asm/system.h
    there.  Convert all users of 'processor_id' to the more efficient
    read_cpuid_id() function.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index b81dbf9ffb77..20e4454e452e 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -25,6 +25,7 @@
 #include <linux/mm.h>
 #include <linux/vmalloc.h>
 
+#include <asm/cputype.h>
 #include <asm/cacheflush.h>
 #include <asm/io.h>
 #include <asm/mmu_context.h>

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 303a7ff6bfd2..b81dbf9ffb77 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -259,7 +259,7 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
  * caller shouldn't need to know that small detail.
  *
  * 'flags' are the extra L_PTE_ flags that you want to specify for this
- * mapping.  See include/asm-arm/proc-armv/pgtable.h for more information.
+ * mapping.  See <asm/pgtable.h> for more information.
  */
 void __iomem *
 __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,

commit 5e5419734c8719cbc01af959ad9c0844002c0df5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Feb 4 22:29:14 2008 -0800

    add mm argument to pte/pmd/pud/pgd_free
    
    (with Martin Schwidefsky <schwidefsky@de.ibm.com>)
    
    The pgd/pud/pmd/pte page table allocation functions get a mm_struct pointer as
    first argument.  The free functions do not get the mm_struct argument.  This
    is 1) asymmetrical and 2) to do mm related page table allocations the mm
    argument is needed on the free function as well.
    
    [kamalesh@linux.vnet.ibm.com: i386 fix]
    [akpm@linux-foundation.org: coding-syle fixes]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 75952779ce19..303a7ff6bfd2 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -162,7 +162,7 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 			 * Free the page table, if there was one.
 			 */
 			if ((pmd_val(pmd) & PMD_TYPE_MASK) == PMD_TYPE_TABLE)
-				pte_free_kernel(pmd_page_vaddr(pmd));
+				pte_free_kernel(&init_mm, pmd_page_vaddr(pmd));
 		}
 
 		addr += PGDIR_SIZE;

commit 6d78b5f9c6cf59c98d3833e09d0ed6aebd6a33d3
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Jun 3 19:26:04 2007 +0100

    [ARM] Fix bounding error in ioremap_pfn()
    
    If size=16M offset=2K then we should map two supersections
    rather than just one.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index f3ade18862aa..75952779ce19 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -280,7 +280,10 @@ __arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	if (!type)
 		return NULL;
 
-	size = PAGE_ALIGN(size);
+	/*
+	 * Page align the mapping size, taking account of any offset.
+	 */
+	size = PAGE_ALIGN(offset + size);
 
  	area = get_vm_area(size, VM_IOREMAP);
  	if (!area)
@@ -325,11 +328,6 @@ __arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
 	if (!size || last_addr < phys_addr)
 		return NULL;
 
-	/*
- 	 * Page align the mapping size
-	 */
-	size = PAGE_ALIGN(last_addr + 1) - phys_addr;
-
  	return __arm_ioremap_pfn(pfn, offset, size, mtype);
 }
 EXPORT_SYMBOL(__arm_ioremap);

commit 6cbdc8c5357276307a77deeada3f04626ff17da6
Author: Simon Arlott <simon@fire.lp0.eu>
Date:   Fri May 11 20:40:30 2007 +0100

    [ARM] spelling fixes
    
    Spelling fixes in arch/arm/.
    
    Signed-off-by: Simon Arlott <simon@fire.lp0.eu>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index d6167ad4e011..f3ade18862aa 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -346,7 +346,7 @@ void __iounmap(volatile void __iomem *addr)
 #ifndef CONFIG_SMP
 	/*
 	 * If this is a section based mapping we need to handle it
-	 * specially as the VM subysystem does not know how to handle
+	 * specially as the VM subsystem does not know how to handle
 	 * such a beast. We need the lock here b/c we need to clear
 	 * all the mappings before the area can be reclaimed
 	 * by someone else.

commit 3603ab2b62ad8372fc93816b080b370dd55d7cec
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat May 5 20:59:27 2007 +0100

    [ARM] mm 10: allow memory type to be specified with ioremap
    
    __ioremap() took a set of page table flags (specifically the cacheable
    and bufferable bits) to control the mapping type.  However, with
    the advent of ARMv6, this is far too limited.
    
    Replace the page table flags with a memory type index, so that the
    desired attributes can be selected from the mem_type table.
    
    Finally, to prevent silent miscompilation due to the differing
    arguments, rename the __ioremap() and __ioremap_pfn() functions.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 216623eece35..d6167ad4e011 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -262,11 +262,10 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
  * mapping.  See include/asm-arm/proc-armv/pgtable.h for more information.
  */
 void __iomem *
-__ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
-	      unsigned long flags)
+__arm_ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
+		  unsigned int mtype)
 {
 	const struct mem_type *type;
-	struct mem_type t;
 	int err;
 	unsigned long addr;
  	struct vm_struct * area;
@@ -277,10 +276,9 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
 
-	t = *get_mem_type(MT_DEVICE);
-	t.prot_sect |= flags;
-	t.prot_pte |= flags;
-	type = &t;
+	type = get_mem_type(mtype);
+	if (!type)
+		return NULL;
 
 	size = PAGE_ALIGN(size);
 
@@ -311,10 +309,10 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	flush_cache_vmap(addr, addr + size);
 	return (void __iomem *) (offset + addr);
 }
-EXPORT_SYMBOL(__ioremap_pfn);
+EXPORT_SYMBOL(__arm_ioremap_pfn);
 
 void __iomem *
-__ioremap(unsigned long phys_addr, size_t size, unsigned long flags)
+__arm_ioremap(unsigned long phys_addr, size_t size, unsigned int mtype)
 {
 	unsigned long last_addr;
  	unsigned long offset = phys_addr & ~PAGE_MASK;
@@ -332,9 +330,9 @@ __ioremap(unsigned long phys_addr, size_t size, unsigned long flags)
 	 */
 	size = PAGE_ALIGN(last_addr + 1) - phys_addr;
 
- 	return __ioremap_pfn(pfn, offset, size, flags);
+ 	return __arm_ioremap_pfn(pfn, offset, size, mtype);
 }
-EXPORT_SYMBOL(__ioremap);
+EXPORT_SYMBOL(__arm_ioremap);
 
 void __iounmap(volatile void __iomem *addr)
 {

commit c172cc92c87103c98b5cd359205b684bf99b5067
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:52:32 2007 +0100

    [ARM] mm 6: allow mem_types table to specify extended pte attributes
    
    Add prot_pte_ext to the mem_types table to allow the extended pte
    attributes to be passed to set_pte_ext(), thereby permitting us to
    specify memory type information for the hardware PTE entries.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index b26b36109d54..216623eece35 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -55,7 +55,8 @@ static int remap_area_pte(pmd_t *pmd, unsigned long addr, unsigned long end,
 		if (!pte_none(*pte))
 			goto bad;
 
-		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot), 0);
+		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot),
+			    type->prot_pte_ext);
 		phys_addr += PAGE_SIZE;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	return 0;

commit b29e9f5e64fb90d2e4be1c7ef8c925b56669c74a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:47:29 2007 +0100

    [ARM] mm 5: Use mem_types table in ioremap
    
    We really want to be using the memory type table in ioremap, so we
    only have to do the CPU type fixups in one place.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 800855b2dc83..b26b36109d54 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -32,6 +32,9 @@
 #include <asm/tlbflush.h>
 #include <asm/sizes.h>
 
+#include <asm/mach/map.h>
+#include "mm.h"
+
 /*
  * Used by ioremap() and iounmap() code to mark (super)section-mapped
  * I/O regions in vm_struct->flags field.
@@ -39,8 +42,9 @@
 #define VM_ARM_SECTION_MAPPING	0x80000000
 
 static int remap_area_pte(pmd_t *pmd, unsigned long addr, unsigned long end,
-			  unsigned long phys_addr, pgprot_t prot)
+			  unsigned long phys_addr, const struct mem_type *type)
 {
+	pgprot_t prot = __pgprot(type->prot_pte);
 	pte_t *pte;
 
 	pte = pte_alloc_kernel(pmd, addr);
@@ -63,7 +67,7 @@ static int remap_area_pte(pmd_t *pmd, unsigned long addr, unsigned long end,
 
 static inline int remap_area_pmd(pgd_t *pgd, unsigned long addr,
 				 unsigned long end, unsigned long phys_addr,
-				 pgprot_t prot)
+				 const struct mem_type *type)
 {
 	unsigned long next;
 	pmd_t *pmd;
@@ -75,7 +79,7 @@ static inline int remap_area_pmd(pgd_t *pgd, unsigned long addr,
 
 	do {
 		next = pmd_addr_end(addr, end);
-		ret = remap_area_pte(pmd, addr, next, phys_addr, prot);
+		ret = remap_area_pte(pmd, addr, next, phys_addr, type);
 		if (ret)
 			return ret;
 		phys_addr += next - addr;
@@ -84,13 +88,11 @@ static inline int remap_area_pmd(pgd_t *pgd, unsigned long addr,
 }
 
 static int remap_area_pages(unsigned long start, unsigned long pfn,
-			    unsigned long size, unsigned long flags)
+			    size_t size, const struct mem_type *type)
 {
 	unsigned long addr = start;
 	unsigned long next, end = start + size;
 	unsigned long phys_addr = __pfn_to_phys(pfn);
-	pgprot_t prot = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
-				 L_PTE_DIRTY | L_PTE_WRITE | flags);
 	pgd_t *pgd;
 	int err = 0;
 
@@ -98,7 +100,7 @@ static int remap_area_pages(unsigned long start, unsigned long pfn,
 	pgd = pgd_offset_k(addr);
 	do {
 		next = pgd_addr_end(addr, end);
-		err = remap_area_pmd(pgd, addr, next, phys_addr, prot);
+		err = remap_area_pmd(pgd, addr, next, phys_addr, type);
 		if (err)
 			break;
 		phys_addr += next - addr;
@@ -178,9 +180,9 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 
 static int
 remap_area_sections(unsigned long virt, unsigned long pfn,
-		    unsigned long size, unsigned long flags)
+		    size_t size, const struct mem_type *type)
 {
-	unsigned long prot, addr = virt, end = virt + size;
+	unsigned long addr = virt, end = virt + size;
 	pgd_t *pgd;
 
 	/*
@@ -189,23 +191,13 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 	 */
 	unmap_area_sections(virt, size);
 
-	prot = PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_DOMAIN(DOMAIN_IO) |
-	       (flags & (L_PTE_CACHEABLE | L_PTE_BUFFERABLE));
-
-	/*
-	 * ARMv6 and above need XN set to prevent speculative prefetches
-	 * hitting IO.
-	 */
-	if (cpu_architecture() >= CPU_ARCH_ARMv6)
-		prot |= PMD_SECT_XN;
-
 	pgd = pgd_offset_k(addr);
 	do {
 		pmd_t *pmd = pmd_offset(pgd, addr);
 
-		pmd[0] = __pmd(__pfn_to_phys(pfn) | prot);
+		pmd[0] = __pmd(__pfn_to_phys(pfn) | type->prot_sect);
 		pfn += SZ_1M >> PAGE_SHIFT;
-		pmd[1] = __pmd(__pfn_to_phys(pfn) | prot);
+		pmd[1] = __pmd(__pfn_to_phys(pfn) | type->prot_sect);
 		pfn += SZ_1M >> PAGE_SHIFT;
 		flush_pmd_entry(pmd);
 
@@ -218,9 +210,9 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 
 static int
 remap_area_supersections(unsigned long virt, unsigned long pfn,
-			 unsigned long size, unsigned long flags)
+			 size_t size, const struct mem_type *type)
 {
-	unsigned long prot, addr = virt, end = virt + size;
+	unsigned long addr = virt, end = virt + size;
 	pgd_t *pgd;
 
 	/*
@@ -229,22 +221,12 @@ remap_area_supersections(unsigned long virt, unsigned long pfn,
 	 */
 	unmap_area_sections(virt, size);
 
-	prot = PMD_TYPE_SECT | PMD_SECT_SUPER | PMD_SECT_AP_WRITE |
-			PMD_DOMAIN(DOMAIN_IO) |
-			(flags & (L_PTE_CACHEABLE | L_PTE_BUFFERABLE));
-
-	/*
-	 * ARMv6 and above need XN set to prevent speculative prefetches
-	 * hitting IO.
-	 */
-	if (cpu_architecture() >= CPU_ARCH_ARMv6)
-		prot |= PMD_SECT_XN;
-
 	pgd = pgd_offset_k(virt);
 	do {
 		unsigned long super_pmd_val, i;
 
-		super_pmd_val = __pfn_to_phys(pfn) | prot;
+		super_pmd_val = __pfn_to_phys(pfn) | type->prot_sect |
+				PMD_SECT_SUPER;
 		super_pmd_val |= ((pfn >> (32 - PAGE_SHIFT)) & 0xf) << 20;
 
 		for (i = 0; i < 8; i++) {
@@ -282,6 +264,8 @@ void __iomem *
 __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	      unsigned long flags)
 {
+	const struct mem_type *type;
+	struct mem_type t;
 	int err;
 	unsigned long addr;
  	struct vm_struct * area;
@@ -292,6 +276,11 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
 
+	t = *get_mem_type(MT_DEVICE);
+	t.prot_sect |= flags;
+	t.prot_pte |= flags;
+	type = &t;
+
 	size = PAGE_ALIGN(size);
 
  	area = get_vm_area(size, VM_IOREMAP);
@@ -305,13 +294,13 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	       cpu_is_xsc3()) && pfn >= 0x100000 &&
 	       !((__pfn_to_phys(pfn) | size | addr) & ~SUPERSECTION_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;
-		err = remap_area_supersections(addr, pfn, size, flags);
+		err = remap_area_supersections(addr, pfn, size, type);
 	} else if (!((__pfn_to_phys(pfn) | size | addr) & ~PMD_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;
-		err = remap_area_sections(addr, pfn, size, flags);
+		err = remap_area_sections(addr, pfn, size, type);
 	} else
 #endif
-		err = remap_area_pages(addr, pfn, size, flags);
+		err = remap_area_pages(addr, pfn, size, type);
 
 	if (err) {
  		vunmap((void *)addr);

commit 4a56c1e41f19393577bdd5c774c289c199b7269d
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:16:48 2007 +0100

    [ARM] mm 3: separate out supersection mappings, avoid for <4GB
    
    Catalin Marinas at ARM Ltd says:
    > The CPU architects in ARM intended supersections only as a way to map
    > addresses >= 4GB. Supersections are not mandated by the architecture
    > and there is no easy way to detect their hardware support at run-time
    > (other than checking for a specific core). From the analysis done in
    > ARM, there wasn't a clear performance gain by using supersections
    > rather than sections (no significant improvement in the TLB misses).
    
    Therefore, we should avoid using supersections unless there's a real
    need (iow, we're mapping addresses >= 4GB).
    
    This means that we can simplify create_mapping() a bit since we will
    only use supersection mappings for addresses >= 4GB, which means that
    the physical, virtual and length must be multiples of the supersection
    mapping size.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 0ac615c0f798..800855b2dc83 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -302,7 +302,7 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 #ifndef CONFIG_SMP
 	if (DOMAIN_IO == 0 &&
 	    (((cpu_architecture() >= CPU_ARCH_ARMv6) && (get_cr() & CR_XP)) ||
-	       cpu_is_xsc3()) &&
+	       cpu_is_xsc3()) && pfn >= 0x100000 &&
 	       !((__pfn_to_phys(pfn) | size | addr) & ~SUPERSECTION_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;
 		err = remap_area_supersections(addr, pfn, size, flags);

commit 412489af76b5c0e4029d4406d93554c22a88fc73
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Jan 25 14:16:47 2007 +0100

    [ARM] 4112/1: Only ioremap to supersections if DOMAIN_IO is zero
    
    Supersections do not have a field for the domain and it is always
    0. This patch prevents the creation of supersections during ioremap
    when DOMAIN_IO is not zero (i.e. !defined(CONFIG_IO_36)).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 251685fe73a8..0ac615c0f798 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -300,7 +300,8 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
  	addr = (unsigned long)area->addr;
 
 #ifndef CONFIG_SMP
-	if ((((cpu_architecture() >= CPU_ARCH_ARMv6) && (get_cr() & CR_XP)) ||
+	if (DOMAIN_IO == 0 &&
+	    (((cpu_architecture() >= CPU_ARCH_ARMv6) && (get_cr() & CR_XP)) ||
 	       cpu_is_xsc3()) &&
 	       !((__pfn_to_phys(pfn) | size | addr) & ~SUPERSECTION_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;

commit c924aff853bdc1c9841dd22440f931fba5ab3a59
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Dec 17 23:29:57 2006 +0000

    [ARM] Fix BUG()s in ioremap() code
    
    We need to ensure that the area size is page aligned so that
    remap_area_pte() doesn't increment the address past the end of
    the desired area.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 3bb3951920bc..251685fe73a8 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -292,6 +292,8 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
 		return NULL;
 
+	size = PAGE_ALIGN(size);
+
  	area = get_vm_area(size, VM_IOREMAP);
  	if (!area)
  		return NULL;

commit da2c12a279ae225f3d4696f76cb3b32a5bec5bfb
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Dec 13 14:35:58 2006 +0000

    [ARM] Clean up ioremap code
    
    Since we're keeping the ioremap code, we might as well keep it as
    close to the standard kernel as possible.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index a43d2800a9cd..3bb3951920bc 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -38,89 +38,71 @@
  */
 #define VM_ARM_SECTION_MAPPING	0x80000000
 
-static inline void
-remap_area_pte(pte_t * pte, unsigned long address, unsigned long size,
-	       unsigned long phys_addr, pgprot_t prot)
+static int remap_area_pte(pmd_t *pmd, unsigned long addr, unsigned long end,
+			  unsigned long phys_addr, pgprot_t prot)
 {
-	unsigned long end;
+	pte_t *pte;
+
+	pte = pte_alloc_kernel(pmd, addr);
+	if (!pte)
+		return -ENOMEM;
 
-	address &= ~PMD_MASK;
-	end = address + size;
-	if (end > PMD_SIZE)
-		end = PMD_SIZE;
-	BUG_ON(address >= end);
 	do {
 		if (!pte_none(*pte))
 			goto bad;
 
 		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot), 0);
-		address += PAGE_SIZE;
 		phys_addr += PAGE_SIZE;
-		pte++;
-	} while (address && (address < end));
-	return;
+	} while (pte++, addr += PAGE_SIZE, addr != end);
+	return 0;
 
  bad:
-	printk("remap_area_pte: page already exists\n");
+	printk(KERN_CRIT "remap_area_pte: page already exists\n");
 	BUG();
 }
 
-static inline int
-remap_area_pmd(pmd_t * pmd, unsigned long address, unsigned long size,
-	       unsigned long phys_addr, unsigned long flags)
+static inline int remap_area_pmd(pgd_t *pgd, unsigned long addr,
+				 unsigned long end, unsigned long phys_addr,
+				 pgprot_t prot)
 {
-	unsigned long end;
-	pgprot_t pgprot;
-
-	address &= ~PGDIR_MASK;
-	end = address + size;
+	unsigned long next;
+	pmd_t *pmd;
+	int ret = 0;
 
-	if (end > PGDIR_SIZE)
-		end = PGDIR_SIZE;
+	pmd = pmd_alloc(&init_mm, pgd, addr);
+	if (!pmd)
+		return -ENOMEM;
 
-	phys_addr -= address;
-	BUG_ON(address >= end);
-
-	pgprot = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY | L_PTE_WRITE | flags);
 	do {
-		pte_t * pte = pte_alloc_kernel(pmd, address);
-		if (!pte)
-			return -ENOMEM;
-		remap_area_pte(pte, address, end - address, address + phys_addr, pgprot);
-		address = (address + PMD_SIZE) & PMD_MASK;
-		pmd++;
-	} while (address && (address < end));
-	return 0;
+		next = pmd_addr_end(addr, end);
+		ret = remap_area_pte(pmd, addr, next, phys_addr, prot);
+		if (ret)
+			return ret;
+		phys_addr += next - addr;
+	} while (pmd++, addr = next, addr != end);
+	return ret;
 }
 
-static int
-remap_area_pages(unsigned long start, unsigned long pfn,
-		 unsigned long size, unsigned long flags)
+static int remap_area_pages(unsigned long start, unsigned long pfn,
+			    unsigned long size, unsigned long flags)
 {
-	unsigned long address = start;
-	unsigned long end = start + size;
+	unsigned long addr = start;
+	unsigned long next, end = start + size;
 	unsigned long phys_addr = __pfn_to_phys(pfn);
+	pgprot_t prot = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
+				 L_PTE_DIRTY | L_PTE_WRITE | flags);
+	pgd_t *pgd;
 	int err = 0;
-	pgd_t * dir;
 
-	phys_addr -= address;
-	dir = pgd_offset(&init_mm, address);
-	BUG_ON(address >= end);
+	BUG_ON(addr >= end);
+	pgd = pgd_offset_k(addr);
 	do {
-		pmd_t *pmd = pmd_alloc(&init_mm, dir, address);
-		if (!pmd) {
-			err = -ENOMEM;
-			break;
-		}
-		if (remap_area_pmd(pmd, address, end - address,
-					 phys_addr + address, flags)) {
-			err = -ENOMEM;
+		next = pgd_addr_end(addr, end);
+		err = remap_area_pmd(pgd, addr, next, phys_addr, prot);
+		if (err)
 			break;
-		}
-
-		address = (address + PGDIR_SIZE) & PGDIR_MASK;
-		dir++;
-	} while (address && (address < end));
+		phys_addr += next - addr;
+	} while (pgd++, addr = next, addr != end);
 
 	return err;
 }

commit ad1ae2fe7fe68414ef29eab3c87b48841f8b72f2
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Dec 13 14:34:43 2006 +0000

    [ARM] Unuse another Linux PTE bit
    
    L_PTE_ASID is not really required to be stored in every PTE, since we
    can identify it via the address passed to set_pte_at().  So, create
    set_pte_ext() which takes the address of the PTE to set, the Linux
    PTE value, and the additional CPU PTE bits which aren't encoded in
    the Linux PTE value.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 465440592791..a43d2800a9cd 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -40,7 +40,7 @@
 
 static inline void
 remap_area_pte(pte_t * pte, unsigned long address, unsigned long size,
-	       unsigned long phys_addr, pgprot_t pgprot)
+	       unsigned long phys_addr, pgprot_t prot)
 {
 	unsigned long end;
 
@@ -53,7 +53,7 @@ remap_area_pte(pte_t * pte, unsigned long address, unsigned long size,
 		if (!pte_none(*pte))
 			goto bad;
 
-		set_pte(pte, pfn_pte(phys_addr >> PAGE_SHIFT, pgprot));
+		set_pte_ext(pte, pfn_pte(phys_addr >> PAGE_SHIFT, prot), 0);
 		address += PAGE_SIZE;
 		phys_addr += PAGE_SIZE;
 		pte++;

commit 1622605cf6e15bfdc55a3dc78b792018edded435
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Mon Oct 9 02:09:49 2006 +0100

    [PATCH] arm: it's OK to pass pointer to volatile as iounmap() argument...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 591fc3187c7f..465440592791 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -361,14 +361,14 @@ __ioremap(unsigned long phys_addr, size_t size, unsigned long flags)
 }
 EXPORT_SYMBOL(__ioremap);
 
-void __iounmap(void __iomem *addr)
+void __iounmap(volatile void __iomem *addr)
 {
 #ifndef CONFIG_SMP
 	struct vm_struct **p, *tmp;
 #endif
 	unsigned int section_mapping = 0;
 
-	addr = (void __iomem *)(PAGE_MASK & (unsigned long)addr);
+	addr = (volatile void __iomem *)(PAGE_MASK & (unsigned long)addr);
 
 #ifndef CONFIG_SMP
 	/*
@@ -395,6 +395,6 @@ void __iounmap(void __iomem *addr)
 #endif
 
 	if (!section_mapping)
-		vunmap(addr);
+		vunmap((void __force *)addr);
 }
 EXPORT_SYMBOL(__iounmap);

commit 46a82b2d5591335277ed2930611f6acb4ce654ed
Author: Dave McCracken <dmccr@us.ibm.com>
Date:   Mon Sep 25 23:31:48 2006 -0700

    [PATCH] Standardize pxx_page macros
    
    One of the changes necessary for shared page tables is to standardize the
    pxx_page macros.  pte_page and pmd_page have always returned the struct
    page associated with their entry, while pte_page_kernel and pmd_page_kernel
    have returned the kernel virtual address.  pud_page and pgd_page, on the
    other hand, return the kernel virtual address.
    
    Shared page tables needs pud_page and pgd_page to return the actual page
    structures.  There are very few actual users of these functions, so it is
    simple to standardize their usage.
    
    Since this is basic cleanup, I am submitting these changes as a standalone
    patch.  Per Hugh Dickins' comments about it, I am also changing the
    pxx_page_kernel macros to pxx_page_vaddr to clarify their meaning.
    
    Signed-off-by: Dave McCracken <dmccr@us.ibm.com>
    Cc: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 88a999df0ab3..591fc3187c7f 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -177,7 +177,7 @@ static void unmap_area_sections(unsigned long virt, unsigned long size)
 			 * Free the page table, if there was one.
 			 */
 			if ((pmd_val(pmd) & PMD_TYPE_MASK) == PMD_TYPE_TABLE)
-				pte_free_kernel(pmd_page_kernel(pmd));
+				pte_free_kernel(pmd_page_vaddr(pmd));
 		}
 
 		addr += PGDIR_SIZE;

commit ceaccbd2a6a82b97bd15938fc7ffe180754cbe6c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sat Jul 29 08:29:30 2006 +0100

    [ARM] 3734/1: Fix the unused variable warning in __iounmap()
    
    Patch from Catalin Marinas
    
    This patch adds #ifdef around some variables in the arch/arm/mm/ioremap.c
    file.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index dba7dddfe57d..88a999df0ab3 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -363,7 +363,9 @@ EXPORT_SYMBOL(__ioremap);
 
 void __iounmap(void __iomem *addr)
 {
+#ifndef CONFIG_SMP
 	struct vm_struct **p, *tmp;
+#endif
 	unsigned int section_mapping = 0;
 
 	addr = (void __iomem *)(PAGE_MASK & (unsigned long)addr);

commit 67f3a58856b6a41a46e9256a79a8ca3809f47cc6
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Jul 3 13:30:52 2006 +0100

    [ARM] Fix warning in consistent.c
    
    No need for 'cr' to be a local variable, which is unused in the
    SMP case, and only used once in the UP case.  Just call get_cr()
    directly.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 3e86fe7c333d..dba7dddfe57d 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -303,7 +303,6 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	int err;
 	unsigned long addr;
  	struct vm_struct * area;
-	unsigned int cr = get_cr();
 
 	/*
 	 * High mappings must be supersection aligned
@@ -317,7 +316,7 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
  	addr = (unsigned long)area->addr;
 
 #ifndef CONFIG_SMP
-	if ((((cpu_architecture() >= CPU_ARCH_ARMv6) && (cr & CR_XP)) ||
+	if ((((cpu_architecture() >= CPU_ARCH_ARMv6) && (get_cr() & CR_XP)) ||
 	       cpu_is_xsc3()) &&
 	       !((__pfn_to_phys(pfn) | size | addr) & ~SUPERSECTION_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;

commit 7cddc397027ddf80b2d916f6e8fb15a21e9791c5
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Mon Jul 3 12:26:02 2006 +0100

    [ARM] 3708/2: fix SMP build after section ioremap changes
    
    Patch from Lennert Buytenhek
    
    Commit ff0daca525dde796382b9ccd563f169df2571211 broke the SMP build,
    this patch fixes it up again.
    
    Signed-off-by: Lennert Buytenhek <buytenh@wantstofly.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 7eac87f05180..3e86fe7c333d 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -369,6 +369,7 @@ void __iounmap(void __iomem *addr)
 
 	addr = (void __iomem *)(PAGE_MASK & (unsigned long)addr);
 
+#ifndef CONFIG_SMP
 	/*
 	 * If this is a section based mapping we need to handle it
 	 * specially as the VM subysystem does not know how to handle
@@ -390,6 +391,7 @@ void __iounmap(void __iomem *addr)
 		}
 	}
 	write_unlock(&vmlist_lock);
+#endif
 
 	if (!section_mapping)
 		vunmap(addr);

commit a069c896d0d6c028581089da7a9a9037a63c2803
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Sat Jul 1 19:58:20 2006 +0100

    [ARM] 3705/1: add supersection support to ioremap()
    
    Patch from Lennert Buytenhek
    
    Analogous to the previous patch that allows ioremap() to use section
    mappings, this patch allows ioremap() to use supersection mappings.
    Original patch by Deepak Saxena.
    
    Signed-off-by: Lennert Buytenhek <buytenh@wantstofly.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 6aa13d59c858..7eac87f05180 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -33,8 +33,8 @@
 #include <asm/sizes.h>
 
 /*
- * Used by ioremap() and iounmap() code to mark section-mapped I/O regions
- * in vm_struct->flags field.
+ * Used by ioremap() and iounmap() code to mark (super)section-mapped
+ * I/O regions in vm_struct->flags field.
  */
 #define VM_ARM_SECTION_MAPPING	0x80000000
 
@@ -233,6 +233,54 @@ remap_area_sections(unsigned long virt, unsigned long pfn,
 
 	return 0;
 }
+
+static int
+remap_area_supersections(unsigned long virt, unsigned long pfn,
+			 unsigned long size, unsigned long flags)
+{
+	unsigned long prot, addr = virt, end = virt + size;
+	pgd_t *pgd;
+
+	/*
+	 * Remove and free any PTE-based mapping, and
+	 * sync the current kernel mapping.
+	 */
+	unmap_area_sections(virt, size);
+
+	prot = PMD_TYPE_SECT | PMD_SECT_SUPER | PMD_SECT_AP_WRITE |
+			PMD_DOMAIN(DOMAIN_IO) |
+			(flags & (L_PTE_CACHEABLE | L_PTE_BUFFERABLE));
+
+	/*
+	 * ARMv6 and above need XN set to prevent speculative prefetches
+	 * hitting IO.
+	 */
+	if (cpu_architecture() >= CPU_ARCH_ARMv6)
+		prot |= PMD_SECT_XN;
+
+	pgd = pgd_offset_k(virt);
+	do {
+		unsigned long super_pmd_val, i;
+
+		super_pmd_val = __pfn_to_phys(pfn) | prot;
+		super_pmd_val |= ((pfn >> (32 - PAGE_SHIFT)) & 0xf) << 20;
+
+		for (i = 0; i < 8; i++) {
+			pmd_t *pmd = pmd_offset(pgd, addr);
+
+			pmd[0] = __pmd(super_pmd_val);
+			pmd[1] = __pmd(super_pmd_val);
+			flush_pmd_entry(pmd);
+
+			addr += PGDIR_SIZE;
+			pgd++;
+		}
+
+		pfn += SUPERSECTION_SIZE >> PAGE_SHIFT;
+	} while (addr < end);
+
+	return 0;
+}
 #endif
 
 
@@ -255,6 +303,13 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	int err;
 	unsigned long addr;
  	struct vm_struct * area;
+	unsigned int cr = get_cr();
+
+	/*
+	 * High mappings must be supersection aligned
+	 */
+	if (pfn >= 0x100000 && (__pfn_to_phys(pfn) & ~SUPERSECTION_MASK))
+		return NULL;
 
  	area = get_vm_area(size, VM_IOREMAP);
  	if (!area)
@@ -262,7 +317,12 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
  	addr = (unsigned long)area->addr;
 
 #ifndef CONFIG_SMP
-	if (!((__pfn_to_phys(pfn) | size | addr) & ~PMD_MASK)) {
+	if ((((cpu_architecture() >= CPU_ARCH_ARMv6) && (cr & CR_XP)) ||
+	       cpu_is_xsc3()) &&
+	       !((__pfn_to_phys(pfn) | size | addr) & ~SUPERSECTION_MASK)) {
+		area->flags |= VM_ARM_SECTION_MAPPING;
+		err = remap_area_supersections(addr, pfn, size, flags);
+	} else if (!((__pfn_to_phys(pfn) | size | addr) & ~PMD_MASK)) {
 		area->flags |= VM_ARM_SECTION_MAPPING;
 		err = remap_area_sections(addr, pfn, size, flags);
 	} else

commit ff0daca525dde796382b9ccd563f169df2571211
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Jun 29 20:17:15 2006 +0100

    [ARM] Add section support to ioremap
    
    Allow section mappings to be setup using ioremap() and torn down
    with iounmap().  This requires additional support in the MM
    context switch to ensure that mappings are properly synchronised
    when mapped in.
    
    Based an original implementation by Deepak Saxena, reworked and
    ARMv6 support added by rmk.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 7691cfdba567..6aa13d59c858 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -27,7 +27,16 @@
 
 #include <asm/cacheflush.h>
 #include <asm/io.h>
+#include <asm/mmu_context.h>
+#include <asm/pgalloc.h>
 #include <asm/tlbflush.h>
+#include <asm/sizes.h>
+
+/*
+ * Used by ioremap() and iounmap() code to mark section-mapped I/O regions
+ * in vm_struct->flags field.
+ */
+#define VM_ARM_SECTION_MAPPING	0x80000000
 
 static inline void
 remap_area_pte(pte_t * pte, unsigned long address, unsigned long size,
@@ -113,10 +122,120 @@ remap_area_pages(unsigned long start, unsigned long pfn,
 		dir++;
 	} while (address && (address < end));
 
-	flush_cache_vmap(start, end);
 	return err;
 }
 
+
+void __check_kvm_seq(struct mm_struct *mm)
+{
+	unsigned int seq;
+
+	do {
+		seq = init_mm.context.kvm_seq;
+		memcpy(pgd_offset(mm, VMALLOC_START),
+		       pgd_offset_k(VMALLOC_START),
+		       sizeof(pgd_t) * (pgd_index(VMALLOC_END) -
+					pgd_index(VMALLOC_START)));
+		mm->context.kvm_seq = seq;
+	} while (seq != init_mm.context.kvm_seq);
+}
+
+#ifndef CONFIG_SMP
+/*
+ * Section support is unsafe on SMP - If you iounmap and ioremap a region,
+ * the other CPUs will not see this change until their next context switch.
+ * Meanwhile, (eg) if an interrupt comes in on one of those other CPUs
+ * which requires the new ioremap'd region to be referenced, the CPU will
+ * reference the _old_ region.
+ *
+ * Note that get_vm_area() allocates a guard 4K page, so we need to mask
+ * the size back to 1MB aligned or we will overflow in the loop below.
+ */
+static void unmap_area_sections(unsigned long virt, unsigned long size)
+{
+	unsigned long addr = virt, end = virt + (size & ~SZ_1M);
+	pgd_t *pgd;
+
+	flush_cache_vunmap(addr, end);
+	pgd = pgd_offset_k(addr);
+	do {
+		pmd_t pmd, *pmdp = pmd_offset(pgd, addr);
+
+		pmd = *pmdp;
+		if (!pmd_none(pmd)) {
+			/*
+			 * Clear the PMD from the page table, and
+			 * increment the kvm sequence so others
+			 * notice this change.
+			 *
+			 * Note: this is still racy on SMP machines.
+			 */
+			pmd_clear(pmdp);
+			init_mm.context.kvm_seq++;
+
+			/*
+			 * Free the page table, if there was one.
+			 */
+			if ((pmd_val(pmd) & PMD_TYPE_MASK) == PMD_TYPE_TABLE)
+				pte_free_kernel(pmd_page_kernel(pmd));
+		}
+
+		addr += PGDIR_SIZE;
+		pgd++;
+	} while (addr < end);
+
+	/*
+	 * Ensure that the active_mm is up to date - we want to
+	 * catch any use-after-iounmap cases.
+	 */
+	if (current->active_mm->context.kvm_seq != init_mm.context.kvm_seq)
+		__check_kvm_seq(current->active_mm);
+
+	flush_tlb_kernel_range(virt, end);
+}
+
+static int
+remap_area_sections(unsigned long virt, unsigned long pfn,
+		    unsigned long size, unsigned long flags)
+{
+	unsigned long prot, addr = virt, end = virt + size;
+	pgd_t *pgd;
+
+	/*
+	 * Remove and free any PTE-based mapping, and
+	 * sync the current kernel mapping.
+	 */
+	unmap_area_sections(virt, size);
+
+	prot = PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_DOMAIN(DOMAIN_IO) |
+	       (flags & (L_PTE_CACHEABLE | L_PTE_BUFFERABLE));
+
+	/*
+	 * ARMv6 and above need XN set to prevent speculative prefetches
+	 * hitting IO.
+	 */
+	if (cpu_architecture() >= CPU_ARCH_ARMv6)
+		prot |= PMD_SECT_XN;
+
+	pgd = pgd_offset_k(addr);
+	do {
+		pmd_t *pmd = pmd_offset(pgd, addr);
+
+		pmd[0] = __pmd(__pfn_to_phys(pfn) | prot);
+		pfn += SZ_1M >> PAGE_SHIFT;
+		pmd[1] = __pmd(__pfn_to_phys(pfn) | prot);
+		pfn += SZ_1M >> PAGE_SHIFT;
+		flush_pmd_entry(pmd);
+
+		addr += PGDIR_SIZE;
+		pgd++;
+	} while (addr < end);
+
+	return 0;
+}
+#endif
+
+
 /*
  * Remap an arbitrary physical address space into the kernel virtual
  * address space. Needed when the kernel wants to access high addresses
@@ -133,6 +252,7 @@ void __iomem *
 __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
 	      unsigned long flags)
 {
+	int err;
 	unsigned long addr;
  	struct vm_struct * area;
 
@@ -140,11 +260,22 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
  	if (!area)
  		return NULL;
  	addr = (unsigned long)area->addr;
- 	if (remap_area_pages(addr, pfn, size, flags)) {
+
+#ifndef CONFIG_SMP
+	if (!((__pfn_to_phys(pfn) | size | addr) & ~PMD_MASK)) {
+		area->flags |= VM_ARM_SECTION_MAPPING;
+		err = remap_area_sections(addr, pfn, size, flags);
+	} else
+#endif
+		err = remap_area_pages(addr, pfn, size, flags);
+
+	if (err) {
  		vunmap((void *)addr);
  		return NULL;
  	}
- 	return (void __iomem *) (offset + (char *)addr);
+
+	flush_cache_vmap(addr, addr + size);
+	return (void __iomem *) (offset + addr);
 }
 EXPORT_SYMBOL(__ioremap_pfn);
 
@@ -173,6 +304,34 @@ EXPORT_SYMBOL(__ioremap);
 
 void __iounmap(void __iomem *addr)
 {
-	vunmap((void *)(PAGE_MASK & (unsigned long)addr));
+	struct vm_struct **p, *tmp;
+	unsigned int section_mapping = 0;
+
+	addr = (void __iomem *)(PAGE_MASK & (unsigned long)addr);
+
+	/*
+	 * If this is a section based mapping we need to handle it
+	 * specially as the VM subysystem does not know how to handle
+	 * such a beast. We need the lock here b/c we need to clear
+	 * all the mappings before the area can be reclaimed
+	 * by someone else.
+	 */
+	write_lock(&vmlist_lock);
+	for (p = &vmlist ; (tmp = *p) ; p = &tmp->next) {
+		if((tmp->flags & VM_IOREMAP) && (tmp->addr == addr)) {
+			if (tmp->flags & VM_ARM_SECTION_MAPPING) {
+				*p = tmp->next;
+				unmap_area_sections((unsigned long)tmp->addr,
+						    tmp->size);
+				kfree(tmp);
+				section_mapping = 1;
+			}
+			break;
+		}
+	}
+	write_unlock(&vmlist_lock);
+
+	if (!section_mapping)
+		vunmap(addr);
 }
 EXPORT_SYMBOL(__iounmap);

commit 5924486dc0f205ebc2bbf4c262eec902ff38e802
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Jun 22 15:05:36 2006 +0100

    [ARM] nommu: add stubs for ioremap and friends
    
    nommu doesn't have any form of remapping support, so ioremap, etc
    become stubs which just return the casted address, doing nothing
    else.
    
    Move ioport_map(), ioport_unmap(), pci_iomap(), pci_iounmap()
    into a separate file which is always built.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index c1f7180c7bed..7691cfdba567 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -176,50 +176,3 @@ void __iounmap(void __iomem *addr)
 	vunmap((void *)(PAGE_MASK & (unsigned long)addr));
 }
 EXPORT_SYMBOL(__iounmap);
-
-#ifdef __io
-void __iomem *ioport_map(unsigned long port, unsigned int nr)
-{
-	return __io(port);
-}
-EXPORT_SYMBOL(ioport_map);
-
-void ioport_unmap(void __iomem *addr)
-{
-}
-EXPORT_SYMBOL(ioport_unmap);
-#endif
-
-#ifdef CONFIG_PCI
-#include <linux/pci.h>
-#include <linux/ioport.h>
-
-void __iomem *pci_iomap(struct pci_dev *dev, int bar, unsigned long maxlen)
-{
-	unsigned long start = pci_resource_start(dev, bar);
-	unsigned long len   = pci_resource_len(dev, bar);
-	unsigned long flags = pci_resource_flags(dev, bar);
-
-	if (!len || !start)
-		return NULL;
-	if (maxlen && len > maxlen)
-		len = maxlen;
-	if (flags & IORESOURCE_IO)
-		return ioport_map(start, len);
-	if (flags & IORESOURCE_MEM) {
-		if (flags & IORESOURCE_CACHEABLE)
-			return ioremap(start, len);
-		return ioremap_nocache(start, len);
-	}
-	return NULL;
-}
-EXPORT_SYMBOL(pci_iomap);
-
-void pci_iounmap(struct pci_dev *dev, void __iomem *addr)
-{
-	if ((unsigned long)addr >= VMALLOC_START &&
-	    (unsigned long)addr < VMALLOC_END)
-		iounmap(addr);
-}
-EXPORT_SYMBOL(pci_iounmap);
-#endif

commit 478922c2b3c4ec8844ff2dec7eb1eba6f89a10ee
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue May 16 11:30:26 2006 +0100

    [ARM] 3526/1: ioremap should use vunmap instead of vfree on ARM
    
    Patch from Catalin Marinas
    
    This patch modifies the __ioremap_pfn and __iounmap functions in
    arch/arm/mm/ioremap.c to use vunmap instead of vfree.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 25e0ca3e598c..c1f7180c7bed 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -141,7 +141,7 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
  		return NULL;
  	addr = (unsigned long)area->addr;
  	if (remap_area_pages(addr, pfn, size, flags)) {
- 		vfree((void *)addr);
+ 		vunmap((void *)addr);
  		return NULL;
  	}
  	return (void __iomem *) (offset + (char *)addr);
@@ -173,7 +173,7 @@ EXPORT_SYMBOL(__ioremap);
 
 void __iounmap(void __iomem *addr)
 {
-	vfree((void *) (PAGE_MASK & (unsigned long) addr));
+	vunmap((void *)(PAGE_MASK & (unsigned long)addr));
 }
 EXPORT_SYMBOL(__iounmap);
 

commit f78f10436806660f39440a729acbaf03e3a01023
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Mar 4 11:04:12 2006 +0000

    [ARM] Remove unnecessary asm/hardware.h includes
    
    asm/hardware.h is not required for the majority of processor support
    files, ioremap support, mm initialisation, acorn IO support, nor
    the debug code (which picks up its machine specific includes via
    debug-macros.S)
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index da9b35974118..25e0ca3e598c 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -26,7 +26,6 @@
 #include <linux/vmalloc.h>
 
 #include <asm/cacheflush.h>
-#include <asm/hardware.h>
 #include <asm/io.h>
 #include <asm/tlbflush.h>
 

commit 20a2c88f5039b8b17f0aa3fbc2ac3e9257961123
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Jan 20 20:52:50 2006 +0000

    [ARM] Fix ioremap.c vfree type warning
    
    arch/arm/mm/ioremap.c:145: warning: passing argument 1 of 'vfree' makes pointer from integer without a cast
    
    resulted from commit id 9d4ae7276ae26c5bfba6207cf05340af1931d8d4
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index de3ce1eec2ec..da9b35974118 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -142,7 +142,7 @@ __ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
  		return NULL;
  	addr = (unsigned long)area->addr;
  	if (remap_area_pages(addr, pfn, size, flags)) {
- 		vfree(addr);
+ 		vfree((void *)addr);
  		return NULL;
  	}
  	return (void __iomem *) (offset + (char *)addr);

commit 9d4ae7276ae26c5bfba6207cf05340af1931d8d4
Author: Deepak Saxena <dsaxena@plexity.net>
Date:   Mon Jan 9 19:23:11 2006 +0000

    [ARM] 3070/2: Add __ioremap_pfn() API
    
    Patch from Deepak Saxena
    
    In working on adding 36-bit addressed supersection support to ioremap(),
    I came to the conclusion that it would be far simpler to do so by just
    splitting __ioremap() into a main external interface and adding an
    __ioremap_pfn() function that takes a pfn + offset into the page that
    __ioremap() can call. This way existing callers of __ioremap() won't have
    to change their code and 36-bit systems will just call __ioremap_pfn()
    and we will not have to deal with unsigned long long variables.
    
    Note that __ioremap_pfn() should _NOT_ be called directly by drivers
    but is reserved for use by arch_ioremap() implementations that map
    32-bit resource regions into the real 36-bit address and then call
    this new function.
    
    Signed-off-by: Deepak Saxena <dsaxena@plexity.net>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 10901398e4a2..de3ce1eec2ec 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -86,11 +86,12 @@ remap_area_pmd(pmd_t * pmd, unsigned long address, unsigned long size,
 }
 
 static int
-remap_area_pages(unsigned long start, unsigned long phys_addr,
+remap_area_pages(unsigned long start, unsigned long pfn,
 		 unsigned long size, unsigned long flags)
 {
 	unsigned long address = start;
 	unsigned long end = start + size;
+	unsigned long phys_addr = __pfn_to_phys(pfn);
 	int err = 0;
 	pgd_t * dir;
 
@@ -129,37 +130,45 @@ remap_area_pages(unsigned long start, unsigned long phys_addr,
  * 'flags' are the extra L_PTE_ flags that you want to specify for this
  * mapping.  See include/asm-arm/proc-armv/pgtable.h for more information.
  */
+void __iomem *
+__ioremap_pfn(unsigned long pfn, unsigned long offset, size_t size,
+	      unsigned long flags)
+{
+	unsigned long addr;
+ 	struct vm_struct * area;
+
+ 	area = get_vm_area(size, VM_IOREMAP);
+ 	if (!area)
+ 		return NULL;
+ 	addr = (unsigned long)area->addr;
+ 	if (remap_area_pages(addr, pfn, size, flags)) {
+ 		vfree(addr);
+ 		return NULL;
+ 	}
+ 	return (void __iomem *) (offset + (char *)addr);
+}
+EXPORT_SYMBOL(__ioremap_pfn);
+
 void __iomem *
 __ioremap(unsigned long phys_addr, size_t size, unsigned long flags)
 {
-	void * addr;
-	struct vm_struct * area;
-	unsigned long offset, last_addr;
+	unsigned long last_addr;
+ 	unsigned long offset = phys_addr & ~PAGE_MASK;
+ 	unsigned long pfn = __phys_to_pfn(phys_addr);
 
-	/* Don't allow wraparound or zero size */
+ 	/*
+ 	 * Don't allow wraparound or zero size
+	 */
 	last_addr = phys_addr + size - 1;
 	if (!size || last_addr < phys_addr)
 		return NULL;
 
 	/*
-	 * Mappings have to be page-aligned
+ 	 * Page align the mapping size
 	 */
-	offset = phys_addr & ~PAGE_MASK;
-	phys_addr &= PAGE_MASK;
 	size = PAGE_ALIGN(last_addr + 1) - phys_addr;
 
-	/*
-	 * Ok, go for it..
-	 */
-	area = get_vm_area(size, VM_IOREMAP);
-	if (!area)
-		return NULL;
-	addr = area->addr;
-	if (remap_area_pages((unsigned long) addr, phys_addr, size, flags)) {
-		vfree(addr);
-		return NULL;
-	}
-	return (void __iomem *) (offset + (char *)addr);
+ 	return __ioremap_pfn(pfn, offset, size, flags);
 }
 EXPORT_SYMBOL(__ioremap);
 

commit 67a1901ff498363e253b90ba132e336c925203ed
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Nov 17 16:48:00 2005 +0000

    [ARM] __ioremap doesn't use 4th argument
    
    The "align" argument in ARMs __ioremap is unused and provides a
    misleading expectation that it might do something.  It doesn't.
    Remove it.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 0f128c28fee4..10901398e4a2 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -130,8 +130,7 @@ remap_area_pages(unsigned long start, unsigned long phys_addr,
  * mapping.  See include/asm-arm/proc-armv/pgtable.h for more information.
  */
 void __iomem *
-__ioremap(unsigned long phys_addr, size_t size, unsigned long flags,
-	  unsigned long align)
+__ioremap(unsigned long phys_addr, size_t size, unsigned long flags)
 {
 	void * addr;
 	struct vm_struct * area;

commit 872fec16d9a0ed3b75b8893aa217e49cca575ee5
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Oct 29 18:16:21 2005 -0700

    [PATCH] mm: init_mm without ptlock
    
    First step in pushing down the page_table_lock.  init_mm.page_table_lock has
    been used throughout the architectures (usually for ioremap): not to serialize
    kernel address space allocation (that's usually vmlist_lock), but because
    pud_alloc,pmd_alloc,pte_alloc_kernel expect caller holds it.
    
    Reverse that: don't lock or unlock init_mm.page_table_lock in any of the
    architectures; instead rely on pud_alloc,pmd_alloc,pte_alloc_kernel to take
    and drop it when allocating a new one, to check lest a racing task already
    did.  Similarly no page_table_lock in vmalloc's map_vm_area.
    
    Some temporary ugliness in __pud_alloc and __pmd_alloc: since they also handle
    user mms, which are converted only by a later patch, for now they have to lock
    differently according to whether or not it's init_mm.
    
    If sources get muddled, there's a danger that an arch source taking
    init_mm.page_table_lock will be mixed with common source also taking it (or
    neither take it).  So break the rules and make another change, which should
    break the build for such a mismatch: remove the redundant mm arg from
    pte_alloc_kernel (ppc64 scrapped its distinct ioremap_mm in 2.6.13).
    
    Exceptions: arm26 used pte_alloc_kernel on user mm, now pte_alloc_map; ia64
    used pte_alloc_map on init_mm, now pte_alloc_kernel; parisc had bad args to
    pmd_alloc and pte_alloc_kernel in unused USE_HPPA_IOREMAP code; ppc64
    map_io_page forgot to unlock on failure; ppc mmu_mapin_ram and ppc64 im_free
    took page_table_lock for no good reason.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 6fb1258df1b5..0f128c28fee4 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -75,7 +75,7 @@ remap_area_pmd(pmd_t * pmd, unsigned long address, unsigned long size,
 
 	pgprot = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY | L_PTE_WRITE | flags);
 	do {
-		pte_t * pte = pte_alloc_kernel(&init_mm, pmd, address);
+		pte_t * pte = pte_alloc_kernel(pmd, address);
 		if (!pte)
 			return -ENOMEM;
 		remap_area_pte(pte, address, end - address, address + phys_addr, pgprot);
@@ -97,7 +97,6 @@ remap_area_pages(unsigned long start, unsigned long phys_addr,
 	phys_addr -= address;
 	dir = pgd_offset(&init_mm, address);
 	BUG_ON(address >= end);
-	spin_lock(&init_mm.page_table_lock);
 	do {
 		pmd_t *pmd = pmd_alloc(&init_mm, dir, address);
 		if (!pmd) {
@@ -114,7 +113,6 @@ remap_area_pages(unsigned long start, unsigned long phys_addr,
 		dir++;
 	} while (address && (address < end));
 
-	spin_unlock(&init_mm.page_table_lock);
 	flush_cache_vmap(start, end);
 	return err;
 }

commit 674c04538284736c4a44224c78cb784b2c972f98
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 28 14:25:28 2005 +0100

    [ARM] 3/4: Remove asm/hardware.h from SA1100 io.h
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 7110e54182b1..6fb1258df1b5 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -26,6 +26,7 @@
 #include <linux/vmalloc.h>
 
 #include <asm/cacheflush.h>
+#include <asm/hardware.h>
 #include <asm/io.h>
 #include <asm/tlbflush.h>
 

commit 09f0551d20ddf6d22c333adcc59f2b1148734273
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Jun 20 18:44:37 2005 +0100

    [PATCH] ARM: Add iomap support for ARM
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
index 00bb8fd37a59..7110e54182b1 100644
--- a/arch/arm/mm/ioremap.c
+++ b/arch/arm/mm/ioremap.c
@@ -170,3 +170,50 @@ void __iounmap(void __iomem *addr)
 	vfree((void *) (PAGE_MASK & (unsigned long) addr));
 }
 EXPORT_SYMBOL(__iounmap);
+
+#ifdef __io
+void __iomem *ioport_map(unsigned long port, unsigned int nr)
+{
+	return __io(port);
+}
+EXPORT_SYMBOL(ioport_map);
+
+void ioport_unmap(void __iomem *addr)
+{
+}
+EXPORT_SYMBOL(ioport_unmap);
+#endif
+
+#ifdef CONFIG_PCI
+#include <linux/pci.h>
+#include <linux/ioport.h>
+
+void __iomem *pci_iomap(struct pci_dev *dev, int bar, unsigned long maxlen)
+{
+	unsigned long start = pci_resource_start(dev, bar);
+	unsigned long len   = pci_resource_len(dev, bar);
+	unsigned long flags = pci_resource_flags(dev, bar);
+
+	if (!len || !start)
+		return NULL;
+	if (maxlen && len > maxlen)
+		len = maxlen;
+	if (flags & IORESOURCE_IO)
+		return ioport_map(start, len);
+	if (flags & IORESOURCE_MEM) {
+		if (flags & IORESOURCE_CACHEABLE)
+			return ioremap(start, len);
+		return ioremap_nocache(start, len);
+	}
+	return NULL;
+}
+EXPORT_SYMBOL(pci_iomap);
+
+void pci_iounmap(struct pci_dev *dev, void __iomem *addr)
+{
+	if ((unsigned long)addr >= VMALLOC_START &&
+	    (unsigned long)addr < VMALLOC_END)
+		iounmap(addr);
+}
+EXPORT_SYMBOL(pci_iounmap);
+#endif

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/arm/mm/ioremap.c b/arch/arm/mm/ioremap.c
new file mode 100644
index 000000000000..00bb8fd37a59
--- /dev/null
+++ b/arch/arm/mm/ioremap.c
@@ -0,0 +1,172 @@
+/*
+ *  linux/arch/arm/mm/ioremap.c
+ *
+ * Re-map IO memory to kernel address space so that we can access it.
+ *
+ * (C) Copyright 1995 1996 Linus Torvalds
+ *
+ * Hacked for ARM by Phil Blundell <philb@gnu.org>
+ * Hacked to allow all architectures to build, and various cleanups
+ * by Russell King
+ *
+ * This allows a driver to remap an arbitrary region of bus memory into
+ * virtual space.  One should *only* use readl, writel, memcpy_toio and
+ * so on with such remapped areas.
+ *
+ * Because the ARM only has a 32-bit address space we can't address the
+ * whole of the (physical) PCI space at once.  PCI huge-mode addressing
+ * allows us to circumvent this restriction by splitting PCI space into
+ * two 2GB chunks and mapping only one at a time into processor memory.
+ * We use MMU protection domains to trap any attempt to access the bank
+ * that is not currently mapped.  (This isn't fully implemented yet.)
+ */
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/vmalloc.h>
+
+#include <asm/cacheflush.h>
+#include <asm/io.h>
+#include <asm/tlbflush.h>
+
+static inline void
+remap_area_pte(pte_t * pte, unsigned long address, unsigned long size,
+	       unsigned long phys_addr, pgprot_t pgprot)
+{
+	unsigned long end;
+
+	address &= ~PMD_MASK;
+	end = address + size;
+	if (end > PMD_SIZE)
+		end = PMD_SIZE;
+	BUG_ON(address >= end);
+	do {
+		if (!pte_none(*pte))
+			goto bad;
+
+		set_pte(pte, pfn_pte(phys_addr >> PAGE_SHIFT, pgprot));
+		address += PAGE_SIZE;
+		phys_addr += PAGE_SIZE;
+		pte++;
+	} while (address && (address < end));
+	return;
+
+ bad:
+	printk("remap_area_pte: page already exists\n");
+	BUG();
+}
+
+static inline int
+remap_area_pmd(pmd_t * pmd, unsigned long address, unsigned long size,
+	       unsigned long phys_addr, unsigned long flags)
+{
+	unsigned long end;
+	pgprot_t pgprot;
+
+	address &= ~PGDIR_MASK;
+	end = address + size;
+
+	if (end > PGDIR_SIZE)
+		end = PGDIR_SIZE;
+
+	phys_addr -= address;
+	BUG_ON(address >= end);
+
+	pgprot = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY | L_PTE_WRITE | flags);
+	do {
+		pte_t * pte = pte_alloc_kernel(&init_mm, pmd, address);
+		if (!pte)
+			return -ENOMEM;
+		remap_area_pte(pte, address, end - address, address + phys_addr, pgprot);
+		address = (address + PMD_SIZE) & PMD_MASK;
+		pmd++;
+	} while (address && (address < end));
+	return 0;
+}
+
+static int
+remap_area_pages(unsigned long start, unsigned long phys_addr,
+		 unsigned long size, unsigned long flags)
+{
+	unsigned long address = start;
+	unsigned long end = start + size;
+	int err = 0;
+	pgd_t * dir;
+
+	phys_addr -= address;
+	dir = pgd_offset(&init_mm, address);
+	BUG_ON(address >= end);
+	spin_lock(&init_mm.page_table_lock);
+	do {
+		pmd_t *pmd = pmd_alloc(&init_mm, dir, address);
+		if (!pmd) {
+			err = -ENOMEM;
+			break;
+		}
+		if (remap_area_pmd(pmd, address, end - address,
+					 phys_addr + address, flags)) {
+			err = -ENOMEM;
+			break;
+		}
+
+		address = (address + PGDIR_SIZE) & PGDIR_MASK;
+		dir++;
+	} while (address && (address < end));
+
+	spin_unlock(&init_mm.page_table_lock);
+	flush_cache_vmap(start, end);
+	return err;
+}
+
+/*
+ * Remap an arbitrary physical address space into the kernel virtual
+ * address space. Needed when the kernel wants to access high addresses
+ * directly.
+ *
+ * NOTE! We need to allow non-page-aligned mappings too: we will obviously
+ * have to convert them into an offset in a page-aligned mapping, but the
+ * caller shouldn't need to know that small detail.
+ *
+ * 'flags' are the extra L_PTE_ flags that you want to specify for this
+ * mapping.  See include/asm-arm/proc-armv/pgtable.h for more information.
+ */
+void __iomem *
+__ioremap(unsigned long phys_addr, size_t size, unsigned long flags,
+	  unsigned long align)
+{
+	void * addr;
+	struct vm_struct * area;
+	unsigned long offset, last_addr;
+
+	/* Don't allow wraparound or zero size */
+	last_addr = phys_addr + size - 1;
+	if (!size || last_addr < phys_addr)
+		return NULL;
+
+	/*
+	 * Mappings have to be page-aligned
+	 */
+	offset = phys_addr & ~PAGE_MASK;
+	phys_addr &= PAGE_MASK;
+	size = PAGE_ALIGN(last_addr + 1) - phys_addr;
+
+	/*
+	 * Ok, go for it..
+	 */
+	area = get_vm_area(size, VM_IOREMAP);
+	if (!area)
+		return NULL;
+	addr = area->addr;
+	if (remap_area_pages((unsigned long) addr, phys_addr, size, flags)) {
+		vfree(addr);
+		return NULL;
+	}
+	return (void __iomem *) (offset + (char *)addr);
+}
+EXPORT_SYMBOL(__ioremap);
+
+void __iounmap(void __iomem *addr)
+{
+	vfree((void *) (PAGE_MASK & (unsigned long) addr));
+}
+EXPORT_SYMBOL(__iounmap);
