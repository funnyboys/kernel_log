commit e05c7b1f2bc4b7b28199b9a7572f73436d97317e
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:33:05 2020 -0700

    mm: pgtable: add shortcuts for accessing kernel PMD and PTE
    
    The powerpc 32-bit implementation of pgtable has nice shortcuts for
    accessing kernel PMD and PTE for a given virtual address.  Make these
    helpers available for all architectures.
    
    [rppt@linux.ibm.com: microblaze: fix page table traversal in setup_rt_frame()]
      Link: http://lkml.kernel.org/r/20200518191511.GD1118872@kernel.org
    [akpm@linux-foundation.org: s/pmd_ptr_k/pmd_off_k/ in various powerpc places]
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-9-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c425288f1a86..628028bfbb92 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -356,12 +356,7 @@ static pte_t *pte_offset_late_fixmap(pmd_t *dir, unsigned long addr)
 
 static inline pmd_t * __init fixmap_pmd(unsigned long addr)
 {
-	pgd_t *pgd = pgd_offset_k(addr);
-	p4d_t *p4d = p4d_offset(pgd, addr);
-	pud_t *pud = pud_offset(p4d, addr);
-	pmd_t *pmd = pmd_offset(pud, addr);
-
-	return pmd;
+	return pmd_off_k(addr);
 }
 
 void __init early_fixmap_init(void)

commit 84e6ffb2c49c7901a9efb54b497d2eb84c3bef8c
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Jun 4 16:46:19 2020 -0700

    arm: add support for folded p4d page tables
    
    Implement primitives necessary for the 4th level folding, add walks of p4d
    level where appropriate, and remove __ARCH_USE_5LEVEL_HACK.
    
    [rppt@linux.ibm.com: fix kexec]
      Link: http://lkml.kernel.org/r/20200508174232.GA759899@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert+renesas@glider.be>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: James Morse <james.morse@arm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Julien Thierry <julien.thierry.kdev@gmail.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Marc Zyngier <maz@kernel.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Suzuki K Poulose <suzuki.poulose@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200414153455.21744-3-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ec8d0008bfa1..c425288f1a86 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -357,7 +357,8 @@ static pte_t *pte_offset_late_fixmap(pmd_t *dir, unsigned long addr)
 static inline pmd_t * __init fixmap_pmd(unsigned long addr)
 {
 	pgd_t *pgd = pgd_offset_k(addr);
-	pud_t *pud = pud_offset(pgd, addr);
+	p4d_t *p4d = p4d_offset(pgd, addr);
+	pud_t *pud = pud_offset(p4d, addr);
 	pmd_t *pmd = pmd_offset(pud, addr);
 
 	return pmd;
@@ -801,12 +802,12 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 	} while (pmd++, addr = next, addr != end);
 }
 
-static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
+static void __init alloc_init_pud(p4d_t *p4d, unsigned long addr,
 				  unsigned long end, phys_addr_t phys,
 				  const struct mem_type *type,
 				  void *(*alloc)(unsigned long sz), bool ng)
 {
-	pud_t *pud = pud_offset(pgd, addr);
+	pud_t *pud = pud_offset(p4d, addr);
 	unsigned long next;
 
 	do {
@@ -816,6 +817,21 @@ static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
 	} while (pud++, addr = next, addr != end);
 }
 
+static void __init alloc_init_p4d(pgd_t *pgd, unsigned long addr,
+				  unsigned long end, phys_addr_t phys,
+				  const struct mem_type *type,
+				  void *(*alloc)(unsigned long sz), bool ng)
+{
+	p4d_t *p4d = p4d_offset(pgd, addr);
+	unsigned long next;
+
+	do {
+		next = p4d_addr_end(addr, end);
+		alloc_init_pud(p4d, addr, next, phys, type, alloc, ng);
+		phys += next - addr;
+	} while (p4d++, addr = next, addr != end);
+}
+
 #ifndef CONFIG_ARM_LPAE
 static void __init create_36bit_mapping(struct mm_struct *mm,
 					struct map_desc *md,
@@ -863,7 +879,8 @@ static void __init create_36bit_mapping(struct mm_struct *mm,
 	pgd = pgd_offset(mm, addr);
 	end = addr + length;
 	do {
-		pud_t *pud = pud_offset(pgd, addr);
+		p4d_t *p4d = p4d_offset(pgd, addr);
+		pud_t *pud = pud_offset(p4d, addr);
 		pmd_t *pmd = pmd_offset(pud, addr);
 		int i;
 
@@ -914,7 +931,7 @@ static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md,
 	do {
 		unsigned long next = pgd_addr_end(addr, end);
 
-		alloc_init_pud(pgd, addr, next, phys, type, alloc, ng);
+		alloc_init_p4d(pgd, addr, next, phys, type, alloc, ng);
 
 		phys += next - addr;
 		addr = next;
@@ -950,7 +967,13 @@ void __init create_mapping_late(struct mm_struct *mm, struct map_desc *md,
 				bool ng)
 {
 #ifdef CONFIG_ARM_LPAE
-	pud_t *pud = pud_alloc(mm, pgd_offset(mm, md->virtual), md->virtual);
+	p4d_t *p4d;
+	pud_t *pud;
+
+	p4d = p4d_alloc(mm, pgd_offset(mm, md->virtual), md->virtual);
+	if (!WARN_ON(!p4d))
+		return;
+	pud = pud_alloc(mm, p4d, md->virtual);
 	if (WARN_ON(!pud))
 		return;
 	pmd_alloc(mm, pud, 0);

commit 78e7c5af080b86e9f28afac5a8307ddab1d2c1a3
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:13 2020 -0700

    mm/special: create generic fallbacks for pte_special() and pte_mkspecial()
    
    Currently there are many platforms that dont enable ARCH_HAS_PTE_SPECIAL
    but required to define quite similar fallback stubs for special page
    table entry helpers such as pte_special() and pte_mkspecial(), as they
    get build in generic MM without a config check.  This creates two
    generic fallback stub definitions for these helpers, eliminating much
    code duplication.
    
    mips platform has a special case where pte_special() and pte_mkspecial()
    visibility is wider than what ARCH_HAS_PTE_SPECIAL enablement requires.
    This restricts those symbol visibility in order to avoid redefinitions
    which is now exposed through this new generic stubs and subsequent build
    failure.  arm platform set_pte_at() definition needs to be moved into a
    C file just to prevent a build failure.
    
    [anshuman.khandual@arm.com: use defined(CONFIG_ARCH_HAS_PTE_SPECIAL) in mips per Thomas]
      Link: http://lkml.kernel.org/r/1583851924-21603-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Guo Ren <guoren@kernel.org>                   [csky]
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>     [m68k]
    Acked-by: Stafford Horne <shorne@gmail.com>             [openrisc]
    Acked-by: Helge Deller <deller@gmx.de>                  [parisc]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Sam Creasey <sammy@sammy.net>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Link: http://lkml.kernel.org/r/1583802551-15406-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 69a337df619f..ec8d0008bfa1 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1646,3 +1646,17 @@ void __init early_mm_init(const struct machine_desc *mdesc)
 	build_mem_type_table();
 	early_paging_init(mdesc);
 }
+
+void set_pte_at(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep, pte_t pteval)
+{
+	unsigned long ext = 0;
+
+	if (addr < TASK_SIZE && pte_valid_user(pteval)) {
+		if (!pte_special(pteval))
+			__sync_icache_dcache(pteval);
+		ext |= PTE_EXT_NG;
+	}
+
+	set_pte_ext(ptep, pteval, ext);
+}

commit 3fbb96c054e28d9f7d63535ef7df9968d747426e
Author: Marc Zyngier <maz@kernel.org>
Date:   Fri Jan 24 23:11:46 2020 +0000

    arm: Remove HYP/Stage-2 page-table support
    
    Remove all traces of Stage-2 and HYP page table support.
    
    Signed-off-by: Marc Zyngier <maz@kernel.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Will Deacon <will@kernel.org>
    Acked-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 5d0d0f86e790..69a337df619f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -63,9 +63,6 @@ static unsigned int cachepolicy __initdata = CPOLICY_WRITEBACK;
 static unsigned int ecc_mask __initdata = 0;
 pgprot_t pgprot_user;
 pgprot_t pgprot_kernel;
-pgprot_t pgprot_hyp_device;
-pgprot_t pgprot_s2;
-pgprot_t pgprot_s2_device;
 
 EXPORT_SYMBOL(pgprot_user);
 EXPORT_SYMBOL(pgprot_kernel);
@@ -75,15 +72,8 @@ struct cachepolicy {
 	unsigned int	cr_mask;
 	pmdval_t	pmd;
 	pteval_t	pte;
-	pteval_t	pte_s2;
 };
 
-#ifdef CONFIG_ARM_LPAE
-#define s2_policy(policy)	policy
-#else
-#define s2_policy(policy)	0
-#endif
-
 unsigned long kimage_voffset __ro_after_init;
 
 static struct cachepolicy cache_policies[] __initdata = {
@@ -92,31 +82,26 @@ static struct cachepolicy cache_policies[] __initdata = {
 		.cr_mask	= CR_W|CR_C,
 		.pmd		= PMD_SECT_UNCACHED,
 		.pte		= L_PTE_MT_UNCACHED,
-		.pte_s2		= s2_policy(L_PTE_S2_MT_UNCACHED),
 	}, {
 		.policy		= "buffered",
 		.cr_mask	= CR_C,
 		.pmd		= PMD_SECT_BUFFERED,
 		.pte		= L_PTE_MT_BUFFERABLE,
-		.pte_s2		= s2_policy(L_PTE_S2_MT_UNCACHED),
 	}, {
 		.policy		= "writethrough",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WT,
 		.pte		= L_PTE_MT_WRITETHROUGH,
-		.pte_s2		= s2_policy(L_PTE_S2_MT_WRITETHROUGH),
 	}, {
 		.policy		= "writeback",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WB,
 		.pte		= L_PTE_MT_WRITEBACK,
-		.pte_s2		= s2_policy(L_PTE_S2_MT_WRITEBACK),
 	}, {
 		.policy		= "writealloc",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WBWA,
 		.pte		= L_PTE_MT_WRITEALLOC,
-		.pte_s2		= s2_policy(L_PTE_S2_MT_WRITEBACK),
 	}
 };
 
@@ -246,9 +231,6 @@ static struct mem_type mem_types[] __ro_after_init = {
 	[MT_DEVICE] = {		  /* Strongly ordered / ARMv6 shared device */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_SHARED |
 				  L_PTE_SHARED,
-		.prot_pte_s2	= s2_policy(PROT_PTE_S2_DEVICE) |
-				  s2_policy(L_PTE_S2_MT_DEV_SHARED) |
-				  L_PTE_SHARED,
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_S,
 		.domain		= DOMAIN_IO,
@@ -434,7 +416,6 @@ static void __init build_mem_type_table(void)
 	struct cachepolicy *cp;
 	unsigned int cr = get_cr();
 	pteval_t user_pgprot, kern_pgprot, vecs_pgprot;
-	pteval_t hyp_device_pgprot, s2_pgprot, s2_device_pgprot;
 	int cpu_arch = cpu_architecture();
 	int i;
 
@@ -558,9 +539,6 @@ static void __init build_mem_type_table(void)
 	 */
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
-	s2_pgprot = cp->pte_s2;
-	hyp_device_pgprot = mem_types[MT_DEVICE].prot_pte;
-	s2_device_pgprot = mem_types[MT_DEVICE].prot_pte_s2;
 
 #ifndef CONFIG_ARM_LPAE
 	/*
@@ -604,7 +582,6 @@ static void __init build_mem_type_table(void)
 			user_pgprot |= L_PTE_SHARED;
 			kern_pgprot |= L_PTE_SHARED;
 			vecs_pgprot |= L_PTE_SHARED;
-			s2_pgprot |= L_PTE_SHARED;
 			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_S;
 			mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
@@ -666,9 +643,6 @@ static void __init build_mem_type_table(void)
 	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
 				 L_PTE_DIRTY | kern_pgprot);
-	pgprot_s2  = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | s2_pgprot);
-	pgprot_s2_device  = __pgprot(s2_device_pgprot);
-	pgprot_hyp_device  = __pgprot(hyp_device_pgprot);
 
 	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;

commit 6a22d824a4154004c25850b0e1b710c2a88fb348
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Aug 11 14:03:29 2019 +0200

    arm: remove ioremap_cached
    
    No users of ioremap_cached are left, remove it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 48c2888297dd..5d0d0f86e790 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -259,7 +259,7 @@ static struct mem_type mem_types[] __ro_after_init = {
 		.prot_sect	= PROT_SECT_DEVICE,
 		.domain		= DOMAIN_IO,
 	},
-	[MT_DEVICE_CACHED] = {	  /* ioremap_cached */
+	[MT_DEVICE_CACHED] = {	  /* ioremap_cache */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_CACHED,
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_WB,

commit b4ed71f557e458257e0f71b11969954acb389240
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Sep 25 16:49:46 2019 -0700

    mm: treewide: clarify pgtable_page_{ctor,dtor}() naming
    
    The naming of pgtable_page_{ctor,dtor}() seems to have confused a few
    people, and until recently arm64 used these erroneously/pointlessly for
    other levels of page table.
    
    To make it incredibly clear that these only apply to the PTE level, and to
    align with the naming of pgtable_pmd_page_{ctor,dtor}(), let's rename them
    to pgtable_pte_page_{ctor,dtor}().
    
    These changes were generated with the following shell script:
    
    ----
    git grep -lw 'pgtable_page_.tor' | while read FILE; do
        sed -i '{s/pgtable_page_ctor/pgtable_pte_page_ctor/}' $FILE;
        sed -i '{s/pgtable_page_dtor/pgtable_pte_page_dtor/}' $FILE;
    done
    ----
    
    ... with the documentation re-flowed to remain under 80 columns, and
    whitespace fixed up in macros to keep backslashes aligned.
    
    There should be no functional change as a result of this patch.
    
    Link: http://lkml.kernel.org/r/20190722141133.3116-1-mark.rutland@arm.com
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>     [m68k]
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Yu Zhao <yuzhao@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 25da9b2d9610..48c2888297dd 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -731,7 +731,7 @@ static void *__init late_alloc(unsigned long sz)
 {
 	void *ptr = (void *)__get_free_pages(GFP_PGTABLE_KERNEL, get_order(sz));
 
-	if (!ptr || !pgtable_page_ctor(virt_to_page(ptr)))
+	if (!ptr || !pgtable_pte_page_ctor(virt_to_page(ptr)))
 		BUG();
 	return ptr;
 }

commit 1d31999cf04c21709f72ceb17e65b54a401330da
Author: Chester Lin <clin@suse.com>
Date:   Fri Aug 30 14:30:07 2019 +0100

    ARM: 8904/1: skip nomap memblocks while finding the lowmem/highmem boundary
    
    adjust_lowmem_bounds() checks every memblocks in order to find the boundary
    between lowmem and highmem. However some memblocks could be marked as NOMAP
    so they are not used by kernel, which should be skipped while calculating
    the boundary.
    
    Signed-off-by: Chester Lin <clin@suse.com>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d5e0b908f0ba..25da9b2d9610 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1197,6 +1197,9 @@ void __init adjust_lowmem_bounds(void)
 		phys_addr_t block_start = reg->base;
 		phys_addr_t block_end = reg->base + reg->size;
 
+		if (memblock_is_nomap(reg))
+			continue;
+
 		if (reg->base < vmalloc_limit) {
 			if (block_end > lowmem_limit)
 				/*

commit 00d2ec1e6bd82c0538e6dd3e4a4040de93ba4fef
Author: Mike Rapoport <mike.rapoport@gmail.com>
Date:   Fri Aug 30 14:27:56 2019 +0100

    ARM: 8903/1: ensure that usable memory in bank 0 starts from a PMD-aligned address
    
    The calculation of memblock_limit in adjust_lowmem_bounds() assumes that
    bank 0 starts from a PMD-aligned address. However, the beginning of the
    first bank may be NOMAP memory and the start of usable memory
    will be not aligned to PMD boundary. In such case the memblock_limit will
    be set to the end of the NOMAP region, which will prevent any memblock
    allocations.
    
    Mark the region between the end of the NOMAP area and the next PMD-aligned
    address as NOMAP as well, so that the usable memory will start at
    PMD-aligned address.
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d9a0038774a6..d5e0b908f0ba 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1177,6 +1177,22 @@ void __init adjust_lowmem_bounds(void)
 	 */
 	vmalloc_limit = (u64)(uintptr_t)vmalloc_min - PAGE_OFFSET + PHYS_OFFSET;
 
+	/*
+	 * The first usable region must be PMD aligned. Mark its start
+	 * as MEMBLOCK_NOMAP if it isn't
+	 */
+	for_each_memblock(memory, reg) {
+		if (!memblock_is_nomap(reg)) {
+			if (!IS_ALIGNED(reg->base, PMD_SIZE)) {
+				phys_addr_t len;
+
+				len = round_up(reg->base, PMD_SIZE) - reg->base;
+				memblock_mark_nomap(reg->base, len);
+			}
+			break;
+		}
+	}
+
 	for_each_memblock(memory, reg) {
 		phys_addr_t block_start = reg->base;
 		phys_addr_t block_end = reg->base + reg->size;

commit 28bcf5937536062d96ee0b581a76a0b1b652eec6
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Jul 11 20:57:57 2019 -0700

    arm: switch to generic version of pte allocation
    
    Replace __get_free_page() and alloc_pages() calls with the generic
    __pte_alloc_one_kernel() and __pte_alloc_one().
    
    There is no functional change for the kernel PTE allocation.
    
    The difference for the user PTEs, is that the clear_pte_table() is now
    called after pgtable_page_ctor() and the addition of __GFP_ACCOUNT to the
    GFP flags.
    
    The conversion to the generic version of pte_free_kernel() removes the NULL
    check for pte.
    
    The pte_free() version on arm is identical to the generic one and can be
    simply dropped.
    
    Link: http://lkml.kernel.org/r/1557296232-15361-4-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Guo Ren <ren_guo@c-sky.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Sam Creasey <sammy@sammy.net>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 1aa2586fa597..d9a0038774a6 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -729,7 +729,7 @@ static void __init *early_alloc(unsigned long sz)
 
 static void *__init late_alloc(unsigned long sz)
 {
-	void *ptr = (void *)__get_free_pages(PGALLOC_GFP, get_order(sz));
+	void *ptr = (void *)__get_free_pages(GFP_PGTABLE_KERNEL, get_order(sz));
 
 	if (!ptr || !pgtable_page_ctor(virt_to_page(ptr)))
 		BUG();

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f3ce34113f89..1aa2586fa597 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1,11 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/arch/arm/mm/mmu.c
  *
  *  Copyright (C) 1995-2005 Russell King
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include <linux/module.h>
 #include <linux/kernel.h>

commit 8a7f97b902f4fb0d94b355b6b3f1fbd7154cafb9
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Mar 11 23:30:31 2019 -0700

    treewide: add checks for the return value of memblock_alloc*()
    
    Add check for the return value of memblock_alloc*() functions and call
    panic() in case of error.  The panic message repeats the one used by
    panicing memblock allocators with adjustment of parameters to include
    only relevant ones.
    
    The replacement was mostly automated with semantic patches like the one
    below with manual massaging of format strings.
    
      @@
      expression ptr, size, align;
      @@
      ptr = memblock_alloc(size, align);
      + if (!ptr)
      +     panic("%s: Failed to allocate %lu bytes align=0x%lx\n", __func__, size, align);
    
    [anders.roxell@linaro.org: use '%pa' with 'phys_addr_t' type]
      Link: http://lkml.kernel.org/r/20190131161046.21886-1-anders.roxell@linaro.org
    [rppt@linux.ibm.com: fix format strings for panics after memblock_alloc]
      Link: http://lkml.kernel.org/r/1548950940-15145-1-git-send-email-rppt@linux.ibm.com
    [rppt@linux.ibm.com: don't panic if the allocation in sparse_buffer_init fails]
      Link: http://lkml.kernel.org/r/20190131074018.GD28876@rapoport-lnx
    [akpm@linux-foundation.org: fix xtensa printk warning]
    Link: http://lkml.kernel.org/r/1548057848-15136-20-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Anders Roxell <anders.roxell@linaro.org>
    Reviewed-by: Guo Ren <ren_guo@c-sky.com>                [c-sky]
    Acked-by: Paul Burton <paul.burton@mips.com>            [MIPS]
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>    [s390]
    Reviewed-by: Juergen Gross <jgross@suse.com>            [Xen]
    Reviewed-by: Geert Uytterhoeven <geert@linux-m68k.org>  [m68k]
    Acked-by: Max Filippov <jcmvbkbc@gmail.com>             [xtensa]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Dennis Zhou <dennis@kernel.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Rob Herring <robh+dt@kernel.org>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 57de0dde3ae0..f3ce34113f89 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -721,7 +721,13 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 
 static void __init *early_alloc(unsigned long sz)
 {
-	return memblock_alloc(sz, sz);
+	void *ptr = memblock_alloc(sz, sz);
+
+	if (!ptr)
+		panic("%s: Failed to allocate %lu bytes align=0x%lx\n",
+		      __func__, sz, sz);
+
+	return ptr;
 }
 
 static void *__init late_alloc(unsigned long sz)
@@ -994,6 +1000,9 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		return;
 
 	svm = memblock_alloc(sizeof(*svm) * nr, __alignof__(*svm));
+	if (!svm)
+		panic("%s: Failed to allocate %zu bytes align=0x%zx\n",
+		      __func__, sizeof(*svm) * nr, __alignof__(*svm));
 
 	for (md = io_desc; nr; md++, nr--) {
 		create_mapping(md);
@@ -1016,6 +1025,9 @@ void __init vm_reserve_area_early(unsigned long addr, unsigned long size,
 	struct static_vm *svm;
 
 	svm = memblock_alloc(sizeof(*svm), __alignof__(*svm));
+	if (!svm)
+		panic("%s: Failed to allocate %zu bytes align=0x%zx\n",
+		      __func__, sizeof(*svm), __alignof__(*svm));
 
 	vm = &svm->vm;
 	vm->addr = (void *)addr;

commit c2938eeb8888f0af8862ca1369e89edf9bfc47f3
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Mar 7 16:31:10 2019 -0800

    arm, s390, unicore32: remove oneliner wrappers for memblock_alloc()
    
    arm, s390 and unicore32 use oneliner wrappers for memblock_alloc().
    Replace their usage with direct call to memblock_alloc().
    
    Link: http://lkml.kernel.org/r/1546248566-14910-7-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 0a04c9a5f042..57de0dde3ae0 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -719,14 +719,9 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 
 #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
 
-static void __init *early_alloc_aligned(unsigned long sz, unsigned long align)
-{
-	return memblock_alloc(sz, align);
-}
-
 static void __init *early_alloc(unsigned long sz)
 {
-	return early_alloc_aligned(sz, sz);
+	return memblock_alloc(sz, sz);
 }
 
 static void *__init late_alloc(unsigned long sz)
@@ -998,7 +993,7 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 	if (!nr)
 		return;
 
-	svm = early_alloc_aligned(sizeof(*svm) * nr, __alignof__(*svm));
+	svm = memblock_alloc(sizeof(*svm) * nr, __alignof__(*svm));
 
 	for (md = io_desc; nr; md++, nr--) {
 		create_mapping(md);
@@ -1020,7 +1015,7 @@ void __init vm_reserve_area_early(unsigned long addr, unsigned long size,
 	struct vm_struct *vm;
 	struct static_vm *svm;
 
-	svm = early_alloc_aligned(sizeof(*svm), __alignof__(*svm));
+	svm = memblock_alloc(sizeof(*svm), __alignof__(*svm));
 
 	vm = &svm->vm;
 	vm->addr = (void *)addr;

commit b63a07d69d404435125e77286620891ef8f9d719
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Thu Mar 7 16:31:06 2019 -0800

    arch: simplify several early memory allocations
    
    There are several early memory allocations in arch/ code that use
    memblock_phys_alloc() to allocate memory, convert the returned physical
    address to the virtual address and then set the allocated memory to
    zero.
    
    Exactly the same behaviour can be achieved simply by calling
    memblock_alloc(): it allocates the memory in the same way as
    memblock_phys_alloc(), then it performs the phys_to_virt() conversion
    and clears the allocated memory.
    
    Replace the longer sequence with a simpler call to memblock_alloc().
    
    Link: http://lkml.kernel.org/r/1546248566-14910-6-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Michal Simek <michal.simek@xilinx.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f5cc1ccfea3d..0a04c9a5f042 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -721,9 +721,7 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 
 static void __init *early_alloc_aligned(unsigned long sz, unsigned long align)
 {
-	void *ptr = __va(memblock_phys_alloc(sz, align));
-	memset(ptr, 0, sz);
-	return ptr;
+	return memblock_alloc(sz, align);
 }
 
 static void __init *early_alloc(unsigned long sz)

commit 9a8dd708d547268c899f1cb443c49bd4d8c84eb3
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:07:59 2018 -0700

    memblock: rename memblock_alloc{_nid,_try_nid} to memblock_phys_alloc*
    
    Make it explicit that the caller gets a physical address rather than a
    virtual one.
    
    This will also allow using meblock_alloc prefix for memblock allocations
    returning virtual address, which is done in the following patches.
    
    The conversion is done using the following semantic patch:
    
    @@
    expression e1, e2, e3;
    @@
    (
    - memblock_alloc(e1, e2)
    + memblock_phys_alloc(e1, e2)
    |
    - memblock_alloc_nid(e1, e2, e3)
    + memblock_phys_alloc_nid(e1, e2, e3)
    |
    - memblock_alloc_try_nid(e1, e2, e3)
    + memblock_phys_alloc_try_nid(e1, e2, e3)
    )
    
    Link: http://lkml.kernel.org/r/1536927045-23536-7-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e46a6a446cdd..f5cc1ccfea3d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -721,7 +721,7 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 
 static void __init *early_alloc_aligned(unsigned long sz, unsigned long align)
 {
-	void *ptr = __va(memblock_alloc(sz, align));
+	void *ptr = __va(memblock_phys_alloc(sz, align));
 	memset(ptr, 0, sz);
 	return ptr;
 }

commit 9e25ebfe56ece7541cd10a20d715cbdd148a2e06
Author: Doug Berger <opendmb@gmail.com>
Date:   Thu Jun 29 18:41:36 2017 +0100

    ARM: 8685/1: ensure memblock-limit is pmd-aligned
    
    The pmd containing memblock_limit is cleared by prepare_page_table()
    which creates the opportunity for early_alloc() to allocate unmapped
    memory if memblock_limit is not pmd aligned causing a boot-time hang.
    
    Commit 965278dcb8ab ("ARM: 8356/1: mm: handle non-pmd-aligned end of RAM")
    attempted to resolve this problem, but there is a path through the
    adjust_lowmem_bounds() routine where if all memory regions start and
    end on pmd-aligned addresses the memblock_limit will be set to
    arm_lowmem_limit.
    
    Since arm_lowmem_limit can be affected by the vmalloc early parameter,
    the value of arm_lowmem_limit may not be pmd-aligned. This commit
    corrects this oversight such that memblock_limit is always rounded
    down to pmd-alignment.
    
    Fixes: 965278dcb8ab ("ARM: 8356/1: mm: handle non-pmd-aligned end of RAM")
    Signed-off-by: Doug Berger <opendmb@gmail.com>
    Suggested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 31af3cb59a60..e46a6a446cdd 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1218,15 +1218,15 @@ void __init adjust_lowmem_bounds(void)
 
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 
+	if (!memblock_limit)
+		memblock_limit = arm_lowmem_limit;
+
 	/*
 	 * Round the memblock limit down to a pmd size.  This
 	 * helps to ensure that we will allocate memory from the
 	 * last full pmd, which should be mapped.
 	 */
-	if (memblock_limit)
-		memblock_limit = round_down(memblock_limit, PMD_SIZE);
-	if (!memblock_limit)
-		memblock_limit = arm_lowmem_limit;
+	memblock_limit = round_down(memblock_limit, PMD_SIZE);
 
 	if (!IS_ENABLED(CONFIG_HIGHMEM) || cache_is_vipt_aliasing()) {
 		if (memblock_end_of_DRAM() > arm_lowmem_limit) {

commit 2d3e4866dea96b0506395b47bfefb234f2088dac
Merge: 9c6ee01ed5bb 2e5b0bd9cc61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 8 12:37:56 2017 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "ARM:
       - HYP mode stub supports kexec/kdump on 32-bit
       - improved PMU support
       - virtual interrupt controller performance improvements
       - support for userspace virtual interrupt controller (slower, but
         necessary for KVM on the weird Broadcom SoCs used by the Raspberry
         Pi 3)
    
      MIPS:
       - basic support for hardware virtualization (ImgTec P5600/P6600/I6400
         and Cavium Octeon III)
    
      PPC:
       - in-kernel acceleration for VFIO
    
      s390:
       - support for guests without storage keys
       - adapter interruption suppression
    
      x86:
       - usual range of nVMX improvements, notably nested EPT support for
         accessed and dirty bits
       - emulation of CPL3 CPUID faulting
    
      generic:
       - first part of VCPU thread request API
       - kvm_stat improvements"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (227 commits)
      kvm: nVMX: Don't validate disabled secondary controls
      KVM: put back #ifndef CONFIG_S390 around kvm_vcpu_kick
      Revert "KVM: Support vCPU-based gfn->hva cache"
      tools/kvm: fix top level makefile
      KVM: x86: don't hold kvm->lock in KVM_SET_GSI_ROUTING
      KVM: Documentation: remove VM mmap documentation
      kvm: nVMX: Remove superfluous VMX instruction fault checks
      KVM: x86: fix emulation of RSM and IRET instructions
      KVM: mark requests that need synchronization
      KVM: return if kvm_vcpu_wake_up() did wake up the VCPU
      KVM: add explicit barrier to kvm_vcpu_kick
      KVM: perform a wake_up in kvm_make_all_cpus_request
      KVM: mark requests that do not need a wakeup
      KVM: remove #ifndef CONFIG_S390 around kvm_vcpu_wake_up
      KVM: x86: always use kvm_make_request instead of set_bit
      KVM: add kvm_{test,clear}_request to replace {test,clear}_bit
      s390: kvm: Cpu model support for msa6, msa7 and msa8
      KVM: x86: remove irq disablement around KVM_SET_CLOCK/KVM_GET_CLOCK
      kvm: better MWAIT emulation for guests
      KVM: x86: virtualize cpuid faulting
      ...

commit b089c31c519c3906c14801b6ec483e18a5152a50
Author: Jon Medhurst <tixy@linaro.org>
Date:   Mon Apr 10 11:13:59 2017 +0100

    ARM: 8667/3: Fix memory attribute inconsistencies when using fixmap
    
    To cope with the variety in ARM architectures and configurations, the
    pagetable attributes for kernel memory are generated at runtime to match
    the system the kernel finds itself on. This calculated value is stored
    in pgprot_kernel.
    
    However, when early fixmap support was added for ARM (commit
    a5f4c561b3b1) the attributes used for mappings were hard coded because
    pgprot_kernel is not set up early enough. Unfortunately, when fixmap is
    used after early boot this means the memory being mapped can have
    different attributes to existing mappings, potentially leading to
    unpredictable behaviour. A specific problem also exists due to the hard
    coded values not include the 'shareable' attribute which means on
    systems where this matters (e.g. those with multiple CPU clusters) the
    cache contents for a memory location can become inconsistent between
    CPUs.
    
    To resolve these issues we change fixmap to use the same memory
    attributes (from pgprot_kernel) that the rest of the kernel uses. To
    enable this we need to refactor the initialisation code so
    build_mem_type_table() is called early enough. Note, that relies on early
    param parsing for memory type overrides passed via the kernel command
    line, so we need to make sure this call is still after
    parse_early_params().
    
    [ardb: keep early_fixmap_init() before param parsing, for earlycon]
    
    Fixes: a5f4c561b3b1 ("ARM: 8415/1: early fixmap support for earlycon")
    Cc: <stable@vger.kernel.org> # v4.3+
    Tested-by: afzal mohammed <afzal.mohd.ma@gmail.com>
    Signed-off-by: Jon Medhurst <tixy@linaro.org>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4e016d7f37b3..347cca965783 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -414,6 +414,11 @@ void __set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t prot)
 		     FIXADDR_END);
 	BUG_ON(idx >= __end_of_fixed_addresses);
 
+	/* we only support device mappings until pgprot_kernel has been set */
+	if (WARN_ON(pgprot_val(prot) != pgprot_val(FIXMAP_PAGE_IO) &&
+		    pgprot_val(pgprot_kernel) == 0))
+		return;
+
 	if (pgprot_val(prot))
 		set_pte_at(NULL, vaddr, pte,
 			pfn_pte(phys >> PAGE_SHIFT, prot));
@@ -1492,7 +1497,7 @@ pgtables_remap lpae_pgtables_remap_asm;
  * early_paging_init() recreates boot time page table setup, allowing machines
  * to switch over to a high (>4G) address space on LPAE systems
  */
-void __init early_paging_init(const struct machine_desc *mdesc)
+static void __init early_paging_init(const struct machine_desc *mdesc)
 {
 	pgtables_remap *lpae_pgtables_remap;
 	unsigned long pa_pgd;
@@ -1560,7 +1565,7 @@ void __init early_paging_init(const struct machine_desc *mdesc)
 
 #else
 
-void __init early_paging_init(const struct machine_desc *mdesc)
+static void __init early_paging_init(const struct machine_desc *mdesc)
 {
 	long long offset;
 
@@ -1616,7 +1621,6 @@ void __init paging_init(const struct machine_desc *mdesc)
 {
 	void *zero_page;
 
-	build_mem_type_table();
 	prepare_page_table();
 	map_lowmem();
 	memblock_set_current_limit(arm_lowmem_limit);
@@ -1636,3 +1640,9 @@ void __init paging_init(const struct machine_desc *mdesc)
 	empty_zero_page = virt_to_page(zero_page);
 	__flush_dcache_page(NULL, empty_zero_page);
 }
+
+void __init early_mm_init(const struct machine_desc *mdesc)
+{
+	build_mem_type_table();
+	early_paging_init(mdesc);
+}

commit cf763e4ede9deca45c83d7a59c8e798c4d079da2
Author: Marc Zyngier <marc.zyngier@arm.com>
Date:   Mon Apr 3 19:37:50 2017 +0100

    ARM: Expose the VA/IDMAP offset
    
    The KVM code needs to be able to compute the address of
    symbols in its idmap page (the equivalent of a virt_to_idmap()
    call). Unfortunately, virt_to_idmap is slightly complicated,
    depending on the use of arch_phys_to_idmap_offset or not, and
    none of that is readily available at HYP.
    
    Instead, expose a single kimage_voffset variable which contains the
    offset between a kernel VA and its idmap address, enabling the
    VA->IDMAP conversion. This allows the KVM code to behave similarily
    to its arm64 counterpart.
    
    Tested-by: Keerthy <j-keerthy@ti.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4e016d7f37b3..e98a2b5c4e85 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -87,6 +87,8 @@ struct cachepolicy {
 #define s2_policy(policy)	0
 #endif
 
+unsigned long kimage_voffset __ro_after_init;
+
 static struct cachepolicy cache_policies[] __initdata = {
 	{
 		.policy		= "uncached",
@@ -1635,4 +1637,7 @@ void __init paging_init(const struct machine_desc *mdesc)
 
 	empty_zero_page = virt_to_page(zero_page);
 	__flush_dcache_page(NULL, empty_zero_page);
+
+	/* Compute the virt/idmap offset, mostly for the sake of KVM */
+	kimage_voffset = (unsigned long)&kimage_voffset - virt_to_idmap(&kimage_voffset);
 }

commit a09975bf6c756e4555a95258ff4b2286dcfddc4e
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Sun Jan 15 03:57:40 2017 +0100

    ARM: 8639/1: Define KERNEL_START and KERNEL_END
    
    In preparation for adding CONFIG_DEBUG_VIRTUAL support, define a set of
    common constants: KERNEL_START and KERNEL_END which abstract
    CONFIG_XIP_KERNEL vs. !CONFIG_XIP_KERNEL. Update the code where
    relevant.
    
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 5cbfd9f86412..4e016d7f37b3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1422,11 +1422,7 @@ static void __init kmap_init(void)
 static void __init map_lowmem(void)
 {
 	struct memblock_region *reg;
-#ifdef CONFIG_XIP_KERNEL
-	phys_addr_t kernel_x_start = round_down(__pa(_sdata), SECTION_SIZE);
-#else
-	phys_addr_t kernel_x_start = round_down(__pa(_stext), SECTION_SIZE);
-#endif
+	phys_addr_t kernel_x_start = round_down(__pa(KERNEL_START), SECTION_SIZE);
 	phys_addr_t kernel_x_end = round_up(__pa(__init_end), SECTION_SIZE);
 
 	/* Map all the lowmem memory banks. */

commit 985626564eedc470ce2866e53938303368ad41b7
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jan 13 22:51:45 2017 +0100

    ARM: 8637/1: Adjust memory boundaries after reservations
    
    adjust_lowmem_bounds is responsible for setting up the boundary for
    lowmem/highmem. This needs to be setup before memblock reservations can
    occur. At the time memblock reservations can occur, memory can also be
    removed from the system. The lowmem/highmem boundary and end of memory
    may be affected by this but it is currently not recalculated. On some
    systems this may be harmless, on others this may result in incorrect
    ranges being passed to the main memory allocator. Correct this by
    recalculating the lowmem/highmem boundary after all reservations have
    been made.
    
    Tested-by: Magnus Lilja <lilja.magnus@gmail.com>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b8f70a3bb7f8..5cbfd9f86412 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1157,6 +1157,7 @@ void __init adjust_lowmem_bounds(void)
 	phys_addr_t memblock_limit = 0;
 	u64 vmalloc_limit;
 	struct memblock_region *reg;
+	phys_addr_t lowmem_limit = 0;
 
 	/*
 	 * Let's use our own (unoptimized) equivalent of __pa() that is
@@ -1172,14 +1173,14 @@ void __init adjust_lowmem_bounds(void)
 		phys_addr_t block_end = reg->base + reg->size;
 
 		if (reg->base < vmalloc_limit) {
-			if (block_end > arm_lowmem_limit)
+			if (block_end > lowmem_limit)
 				/*
 				 * Compare as u64 to ensure vmalloc_limit does
 				 * not get truncated. block_end should always
 				 * fit in phys_addr_t so there should be no
 				 * issue with assignment.
 				 */
-				arm_lowmem_limit = min_t(u64,
+				lowmem_limit = min_t(u64,
 							 vmalloc_limit,
 							 block_end);
 
@@ -1200,12 +1201,14 @@ void __init adjust_lowmem_bounds(void)
 				if (!IS_ALIGNED(block_start, PMD_SIZE))
 					memblock_limit = block_start;
 				else if (!IS_ALIGNED(block_end, PMD_SIZE))
-					memblock_limit = arm_lowmem_limit;
+					memblock_limit = lowmem_limit;
 			}
 
 		}
 	}
 
+	arm_lowmem_limit = lowmem_limit;
+
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 
 	/*

commit 374d446d25d6271ee615952a3b7f123ba4983c35
Author: Laura Abbott <labbott@redhat.com>
Date:   Fri Jan 13 22:51:08 2017 +0100

    ARM: 8636/1: Cleanup sanity_check_meminfo
    
    The logic for sanity_check_meminfo has become difficult to
    follow. Clean up the code so it's more obvious what the code
    is actually trying to do. Additionally, meminfo is now removed
    so rename the function to better describe its purpose.
    
    Tested-by: Magnus Lilja <lilja.magnus@gmail.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4001dd15818d..b8f70a3bb7f8 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1152,13 +1152,11 @@ early_param("vmalloc", early_vmalloc);
 
 phys_addr_t arm_lowmem_limit __initdata = 0;
 
-void __init sanity_check_meminfo(void)
+void __init adjust_lowmem_bounds(void)
 {
 	phys_addr_t memblock_limit = 0;
-	int highmem = 0;
 	u64 vmalloc_limit;
 	struct memblock_region *reg;
-	bool should_use_highmem = false;
 
 	/*
 	 * Let's use our own (unoptimized) equivalent of __pa() that is
@@ -1172,43 +1170,18 @@ void __init sanity_check_meminfo(void)
 	for_each_memblock(memory, reg) {
 		phys_addr_t block_start = reg->base;
 		phys_addr_t block_end = reg->base + reg->size;
-		phys_addr_t size_limit = reg->size;
 
-		if (reg->base >= vmalloc_limit)
-			highmem = 1;
-		else
-			size_limit = vmalloc_limit - reg->base;
-
-
-		if (!IS_ENABLED(CONFIG_HIGHMEM) || cache_is_vipt_aliasing()) {
-
-			if (highmem) {
-				pr_notice("Ignoring RAM at %pa-%pa (!CONFIG_HIGHMEM)\n",
-					  &block_start, &block_end);
-				memblock_remove(reg->base, reg->size);
-				should_use_highmem = true;
-				continue;
-			}
-
-			if (reg->size > size_limit) {
-				phys_addr_t overlap_size = reg->size - size_limit;
-
-				pr_notice("Truncating RAM at %pa-%pa",
-					  &block_start, &block_end);
-				block_end = vmalloc_limit;
-				pr_cont(" to -%pa", &block_end);
-				memblock_remove(vmalloc_limit, overlap_size);
-				should_use_highmem = true;
-			}
-		}
-
-		if (!highmem) {
-			if (block_end > arm_lowmem_limit) {
-				if (reg->size > size_limit)
-					arm_lowmem_limit = vmalloc_limit;
-				else
-					arm_lowmem_limit = block_end;
-			}
+		if (reg->base < vmalloc_limit) {
+			if (block_end > arm_lowmem_limit)
+				/*
+				 * Compare as u64 to ensure vmalloc_limit does
+				 * not get truncated. block_end should always
+				 * fit in phys_addr_t so there should be no
+				 * issue with assignment.
+				 */
+				arm_lowmem_limit = min_t(u64,
+							 vmalloc_limit,
+							 block_end);
 
 			/*
 			 * Find the first non-pmd-aligned page, and point
@@ -1233,9 +1206,6 @@ void __init sanity_check_meminfo(void)
 		}
 	}
 
-	if (should_use_highmem)
-		pr_notice("Consider using a HIGHMEM enabled kernel.\n");
-
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 
 	/*
@@ -1248,6 +1218,18 @@ void __init sanity_check_meminfo(void)
 	if (!memblock_limit)
 		memblock_limit = arm_lowmem_limit;
 
+	if (!IS_ENABLED(CONFIG_HIGHMEM) || cache_is_vipt_aliasing()) {
+		if (memblock_end_of_DRAM() > arm_lowmem_limit) {
+			phys_addr_t end = memblock_end_of_DRAM();
+
+			pr_notice("Ignoring RAM at %pa-%pa\n",
+				  &memblock_limit, &end);
+			pr_notice("Consider using a HIGHMEM enabled kernel.\n");
+
+			memblock_remove(memblock_limit, end - memblock_limit);
+		}
+	}
+
 	memblock_set_current_limit(memblock_limit);
 }
 

commit 301a36fa700f9add6e14f5a95c7573e01578343a
Merge: b828f960215f b60752f2b20c
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Thu Oct 6 08:56:43 2016 +0100

    Merge branches 'misc' and 'sa1111-base' into for-linus

commit 6b3142b2b852cd5e3216d1aa800a0a49377e6e1c
Author: Stefan Agner <stefan@agner.ch>
Date:   Wed Sep 7 21:56:09 2016 +0100

    ARM: 8612/1: LPAE: initialize cache policy correctly
    
    The cachepolicy variable gets initialized using a masked pmd
    value. So far, the pmd has been masked with flags valid for the
    2-page table format, but the 3-page table format requires a
    different mask. On LPAE, this lead to a wrong assumption of what
    initial cache policy has been used. Later a check forces the
    cache policy to writealloc and prints the following warning:
    Forcing write-allocate cache policy for SMP
    
    This patch introduces a new definition PMD_SECT_CACHE_MASK for
    both page table formats which masks in all cache flags in both
    cases.
    
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6344913f0804..30fe03f95c85 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -137,7 +137,7 @@ void __init init_default_cache_policy(unsigned long pmd)
 
 	initial_pmd_value = pmd;
 
-	pmd &= PMD_SECT_TEX(1) | PMD_SECT_BUFFERABLE | PMD_SECT_CACHEABLE;
+	pmd &= PMD_SECT_CACHE_MASK;
 
 	for (i = 0; i < ARRAY_SIZE(cache_policies); i++)
 		if (cache_policies[i].pmd == pmd) {

commit 7619751f8c900fa5fdd76db06f4caf095c56de8e
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Aug 10 22:46:49 2016 +0100

    ARM: 8595/2: apply more __ro_after_init
    
    Guided by grsecurity's analogous __read_only markings in arch/arm,
    this applies several uses of __ro_after_init to structures that are
    only updated during __init.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 62f4d01941f7..2396935e715c 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -243,7 +243,7 @@ __setup("noalign", noalign_setup);
 #define PROT_PTE_S2_DEVICE	PROT_PTE_DEVICE
 #define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_AP_WRITE
 
-static struct mem_type mem_types[] = {
+static struct mem_type mem_types[] __ro_after_init = {
 	[MT_DEVICE] = {		  /* Strongly ordered / ARMv6 shared device */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_SHARED |
 				  L_PTE_SHARED,

commit 61444cde9170e256c238a02c9a4861930db04f5f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Jul 28 19:48:44 2016 +0100

    ARM: 8591/1: mm: use fully constructed struct pages for EFI pgd allocations
    
    The late_alloc() PTE allocation function used by create_mapping_late()
    does not call pgtable_page_ctor() on PTE pages it allocates, leaving
    the per-page spinlock uninitialized.
    
    Since generic page table manipulation code may assume that translation
    table pages that are not owned by init_mm are covered by fully
    constructed struct pages, the following crash may occur with the new
    UEFI memory attributes table code.
    
      efi: memattr: Processing EFI Memory Attributes table:
      efi: memattr:  0x0000ffa16000-0x0000ffa82fff [Runtime Code       |RUN|  |  |XP|  |  |  |   |  |  |  |  ]
      Unable to handle kernel NULL pointer dereference at virtual address 00000010
      pgd = c0204000
      [00000010] *pgd=00000000
      Internal error: Oops: 5 [#1] SMP ARM
      Modules linked in:
      CPU: 0 PID: 1 Comm: swapper/0 Not tainted 4.7.0-rc4-00063-g3882aa7b340b #361
      Hardware name: Generic DT based system
      task: ed858000 ti: ed842000 task.ti: ed842000
      PC is at __lock_acquire+0xa0/0x19a8
      ...
      [<c038c830>] (__lock_acquire) from [<c038e4f8>] (lock_acquire+0x6c/0x88)
      [<c038e4f8>] (lock_acquire) from [<c0c06134>] (_raw_spin_lock+0x2c/0x3c)
      [<c0c06134>] (_raw_spin_lock) from [<c0410384>] (apply_to_page_range+0xe8/0x238)
      [<c0410384>] (apply_to_page_range) from [<c1205f34>] (efi_set_mapping_permissions+0x54/0x5c)
      [<c1205f34>] (efi_set_mapping_permissions) from [<c1247474>] (efi_memattr_apply_permissions+0x2b8/0x378)
      [<c1247474>] (efi_memattr_apply_permissions) from [<c1248258>] (arm_enable_runtime_services+0x1f0/0x22c)
      [<c1248258>] (arm_enable_runtime_services) from [<c0301f0c>] (do_one_initcall+0x44/0x174)
      [<c0301f0c>] (do_one_initcall) from [<c1200d10>] (kernel_init_freeable+0x90/0x1e8)
      [<c1200d10>] (kernel_init_freeable) from [<c0bff690>] (kernel_init+0x8/0x114)
      [<c0bff690>] (kernel_init) from [<c0307ed0>] (ret_from_fork+0x14/0x24)
    
    The crash is due to the fact that the UEFI page tables are not owned by
    init_mm, but are not covered by fully constructed struct pages.
    
    Given that the UEFI subsystem is currently the only user of
    create_mapping_late(), add an unconditional call to pgtable_page_ctor() to
    late_alloc().
    
    Fixes: 9fc68b717c24 ("ARM/efi: Apply strict permissions for UEFI Runtime Services regions")
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 12774c8e770c..6344913f0804 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -728,7 +728,8 @@ static void *__init late_alloc(unsigned long sz)
 {
 	void *ptr = (void *)__get_free_pages(PGALLOC_GFP, get_order(sz));
 
-	BUG_ON(!ptr);
+	if (!ptr || !pgtable_page_ctor(virt_to_page(ptr)))
+		BUG();
 	return ptr;
 }
 

commit b9a019899f61acca18df5fb5e38a8fcdfea86fcd
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Jul 28 19:38:07 2016 +0100

    ARM: 8590/1: sanity_check_meminfo(): avoid overflow on vmalloc_limit
    
    To limit the amount of mapped low memory, we determine a physical address
    boundary based on the start of the vmalloc area using __pa().
    Strictly speaking, the vmalloc area location is arbitrary and does not
    necessarily corresponds to a valid physical address. For example, if
    
            PAGE_OFFSET = 0x80000000
            PHYS_OFFSET = 0x90000000
            vmalloc_min = 0xf0000000
    
    then __pa(vmalloc_min) overflows and returns a wrapped 0 when phys_addr_t
    is a 32-bit type. Then the code that follows determines that the entire
    physical memory is above that boundary and no low memory gets mapped at
    all:
    
    |[...]
    |Machine model: Freescale i.MX51 NA04 Board
    |Ignoring RAM at 0x90000000-0xb0000000 (!CONFIG_HIGHMEM)
    |Consider using a HIGHMEM enabled kernel.
    
    To avoid this problem let's make vmalloc_limit a 64-bit value all the
    time and determine that boundary explicitly without using __pa().
    
    Reported-by: Emil Renner Berthing <kernel@esmil.dk>
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Emil Renner Berthing <kernel@esmil.dk>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 62f4d01941f7..12774c8e770c 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1155,10 +1155,19 @@ void __init sanity_check_meminfo(void)
 {
 	phys_addr_t memblock_limit = 0;
 	int highmem = 0;
-	phys_addr_t vmalloc_limit = __pa(vmalloc_min - 1) + 1;
+	u64 vmalloc_limit;
 	struct memblock_region *reg;
 	bool should_use_highmem = false;
 
+	/*
+	 * Let's use our own (unoptimized) equivalent of __pa() that is
+	 * not affected by wrap-arounds when sizeof(phys_addr_t) == 4.
+	 * The result is used as the upper bound on physical memory address
+	 * and may itself be outside the valid range for which phys_addr_t
+	 * and therefore __pa() is defined.
+	 */
+	vmalloc_limit = (u64)(uintptr_t)vmalloc_min - PAGE_OFFSET + PHYS_OFFSET;
+
 	for_each_memblock(memory, reg) {
 		phys_addr_t block_start = reg->base;
 		phys_addr_t block_end = reg->base + reg->size;
@@ -1183,10 +1192,11 @@ void __init sanity_check_meminfo(void)
 			if (reg->size > size_limit) {
 				phys_addr_t overlap_size = reg->size - size_limit;
 
-				pr_notice("Truncating RAM at %pa-%pa to -%pa",
-					  &block_start, &block_end, &vmalloc_limit);
-				memblock_remove(vmalloc_limit, overlap_size);
+				pr_notice("Truncating RAM at %pa-%pa",
+					  &block_start, &block_end);
 				block_end = vmalloc_limit;
+				pr_cont(" to -%pa", &block_end);
+				memblock_remove(vmalloc_limit, overlap_size);
 				should_use_highmem = true;
 			}
 		}

commit de06dbfa7861c9019eedefc0c356ba86e5098f1b
Merge: b31a3bc3dbd2 1b3bf847977c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 16:31:54 2016 -0700

    Merge branch 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "Another mixture of changes this time around:
    
       - Split XIP linker file from main linker file to make it more
         maintainable, and various XIP fixes, and clean up a resulting
         macro.
    
       - Decompressor cleanups from Masahiro Yamada
    
       - Avoid printing an error for a missing L2 cache
    
       - Remove some duplicated symbols in System.map, and move
         vectors/stubs back into kernel VMA
    
       - Various low priority fixes from Arnd
    
       - Updates to allow bus match functions to return negative errno
         values, touching some drivers and the driver core.  Greg has acked
         these changes.
    
       - Virtualisation platform udpates form Jean-Philippe Brucker.
    
       - Security enhancements from Kees Cook
    
       - Rework some Kconfig dependencies and move PSCI idle management code
         out of arch/arm into drivers/firmware/psci.c
    
       - ARM DMA mapping updates, touching media, acked by Mauro.
    
       - Fix places in ARM code which should be using virt_to_idmap() so
         that Keystone2 can work.
    
       - Fix Marvell Tauros2 to work again with non-DT boots.
    
       - Provide a delay timer for ARM Orion platforms"
    
    * 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm: (45 commits)
      ARM: 8546/1: dma-mapping: refactor to fix coherent+cma+gfp=0
      ARM: 8547/1: dma-mapping: store buffer information
      ARM: 8543/1: decompressor: rename suffix_y to compress-y
      ARM: 8542/1: decompressor: merge piggy.*.S and simplify Makefile
      ARM: 8541/1: decompressor: drop redundant FORCE in Makefile
      ARM: 8540/1: decompressor: use clean-files instead of extra-y to clean files
      ARM: 8539/1: decompressor: drop more unneeded assignments to "targets"
      ARM: 8538/1: decompressor: drop unneeded assignments to "targets"
      ARM: 8532/1: uncompress: mark putc as inline
      ARM: 8531/1: turn init_new_context into an inline function
      ARM: 8530/1: remove VIRT_TO_BUS
      ARM: 8537/1: drop unused DEBUG_RODATA from XIP_KERNEL
      ARM: 8536/1: mm: hide __start_rodata_section_aligned for non-debug builds
      ARM: 8535/1: mm: DEBUG_RODATA makes no sense with XIP_KERNEL
      ARM: 8534/1: virt: fix hyp-stub build for pre-ARMv7 CPUs
      ARM: make the physical-relative calculation more obvious
      ARM: 8512/1: proc-v7.S: Adjust stack address when XIP_KERNEL
      ARM: 8411/1: Add default SPARSEMEM settings
      ARM: 8503/1: clk_register_clkdev: remove format string interface
      ARM: 8529/1: remove 'i' and 'zi' targets
      ...

commit 3ed3a4f0ddffece942bb2661924d87be4ce63cb7
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 17 14:19:11 2016 -0700

    mm: cleanup *pte_alloc* interfaces
    
    There are few things about *pte_alloc*() helpers worth cleaning up:
    
     - 'vma' argument is unused, let's drop it;
    
     - most __pte_alloc() callers do speculative check for pmd_none(),
       before taking ptl: let's introduce pte_alloc() macro which does
       the check.
    
       The only direct user of __pte_alloc left is userfaultfd, which has
       different expectation about atomicity wrt pmd.
    
     - pte_alloc_map() and pte_alloc_map_lock() are redefined using
       pte_alloc().
    
    [sudeep.holla@arm.com: fix build for arm64 hugetlbpage]
    [sfr@canb.auug.org.au: fix arch/arm/mm/mmu.c some more]
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 434d76f0b363..88fbe0d23ca6 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -732,7 +732,7 @@ static void *__init late_alloc(unsigned long sz)
 	return ptr;
 }
 
-static pte_t * __init pte_alloc(pmd_t *pmd, unsigned long addr,
+static pte_t * __init arm_pte_alloc(pmd_t *pmd, unsigned long addr,
 				unsigned long prot,
 				void *(*alloc)(unsigned long sz))
 {
@@ -747,7 +747,7 @@ static pte_t * __init pte_alloc(pmd_t *pmd, unsigned long addr,
 static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr,
 				      unsigned long prot)
 {
-	return pte_alloc(pmd, addr, prot, early_alloc);
+	return arm_pte_alloc(pmd, addr, prot, early_alloc);
 }
 
 static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
@@ -756,7 +756,7 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 				  void *(*alloc)(unsigned long sz),
 				  bool ng)
 {
-	pte_t *pte = pte_alloc(pmd, addr, type->prot_l1, alloc);
+	pte_t *pte = arm_pte_alloc(pmd, addr, type->prot_l1, alloc);
 	do {
 		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)),
 			    ng ? PTE_EXT_NG : 0);

commit 02afa9a87b232bca15bc30808b9310c6388ca1a8
Author: Chris Brandt <chris.brandt@renesas.com>
Date:   Tue Feb 9 19:34:43 2016 +0100

    ARM: 8518/1: Use correct symbols for XIP_KERNEL
    
    For an XIP build, _etext does not represent the end of the
    binary image that needs to stay mapped into the MODULES_VADDR area.
    Years ago, data came before text in the memory map. However,
    now that the order is text/init/data, an XIP_KERNEL needs to map
    up to the data location in order to keep from cutting off
    parts of the kernel that are needed.
    We only map up to the beginning of data because data has already been
    copied, so there's no reason to keep it around anymore.
    A new symbol is created to make it clear what it is we are referring
    to.
    
    This fixes the bug where you might lose the end of your kernel area
    after page table setup is complete.
    
    Signed-off-by: Chris Brandt <chris.brandt@renesas.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 434d76f0b363..e4b681aafd6d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1253,7 +1253,7 @@ static inline void prepare_page_table(void)
 
 #ifdef CONFIG_XIP_KERNEL
 	/* The XIP kernel is mapped in the module area -- skip over it */
-	addr = ((unsigned long)_etext + PMD_SIZE - 1) & PMD_MASK;
+	addr = ((unsigned long)_exiprom + PMD_SIZE - 1) & PMD_MASK;
 #endif
 	for ( ; addr < PAGE_OFFSET; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
@@ -1335,7 +1335,7 @@ static void __init devicemaps_init(const struct machine_desc *mdesc)
 #ifdef CONFIG_XIP_KERNEL
 	map.pfn = __phys_to_pfn(CONFIG_XIP_PHYS_ADDR & SECTION_MASK);
 	map.virtual = MODULES_VADDR;
-	map.length = ((unsigned long)_etext - map.virtual + ~SECTION_MASK) & SECTION_MASK;
+	map.length = ((unsigned long)_exiprom - map.virtual + ~SECTION_MASK) & SECTION_MASK;
 	map.type = MT_ROM;
 	create_mapping(&map);
 #endif
@@ -1426,7 +1426,11 @@ static void __init kmap_init(void)
 static void __init map_lowmem(void)
 {
 	struct memblock_region *reg;
+#ifdef CONFIG_XIP_KERNEL
+	phys_addr_t kernel_x_start = round_down(__pa(_sdata), SECTION_SIZE);
+#else
 	phys_addr_t kernel_x_start = round_down(__pa(_stext), SECTION_SIZE);
+#endif
 	phys_addr_t kernel_x_end = round_up(__pa(__init_end), SECTION_SIZE);
 
 	/* Map all the lowmem memory banks. */

commit 6b5a12dbca7a8681ecb78dbebaedc1f8364ebd10
Merge: 5083c54264d2 2cb58c016673
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 20 18:03:56 2016 -0800

    Merge tag 'armsoc-multiplatform' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC multiplatform code updates from Arnd Bergmann:
     "This branch is the culmination of 5 years of effort to bring the ARMv6
      and ARMv7 platforms together such that they can all be enabled and
      boot the same kernel.  It has been a tremendous amount of cleanup and
      refactoring by a huge number of people, and creation of several new
      (and major) subsystems to better abstract out all the platform details
      in an appropriate manner.
    
      The bulk of this branch is a large patchset from Arnd that brings
      several of the more minor and older platforms we have closer to
      multiplatform support.  Among these are MMP, S3C64xx, Orion5x, mv78xx0
      and realview Much of this is moving around header files from old mach
      directories, but there are also some cleanup patches of debug_ll
      (lowlevel debug per-platform options) and other parts.
    
      Linus Walleij also has some patchs to clean up the older ARM Realview
      platforms by finally introducing DT support, and Rob Herring has some
      for ARM Versatile which is now DT-only.  Both of these platforms are
      now multiplatform.
    
      Finally, a couple of patches from Russell for Dove PMU, and a fix from
      Valentin Rothberg for Exynos ADC, which were rebased on top of the
      series to avoid conflicts"
    
    * tag 'armsoc-multiplatform' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (75 commits)
      ARM: realview: don't select SMP_ON_UP for UP builds
      ARM: s3c: simplify s3c_irqwake_{e,}intallow definition
      ARM: s3c64xx: fix pm-debug compilation
      iio: exynos-adc: fix irqf_oneshot.cocci warnings
      ARM: realview: build realview-dt SMP support only when used
      ARM: realview: select apropriate targets
      ARM: realview: clean up header files
      ARM: realview: make all header files local
      ARM: no longer make CPU targets visible separately
      ARM: integrator: use explicit core module options
      ARM: realview: enable multiplatform
      ARM: make default platform work for NOMMU
      ARM: debug-ll: move DEBUG_LL_UART_EFM32 to correct Kconfig location
      ARM: defconfig: use correct debug_ll settings
      ARM: versatile: convert to multi-platform
      ARM: versatile: merge mach code into a single file
      ARM: versatile: switch to DT only booting and remove legacy code
      ARM: versatile: add DT based PCI detection
      ARM: pxa: mark ezx structures as __maybe_unused
      ARM: pxa: mark raumfeld init functions as __maybe_unused
      ...

commit 6660800fb7fd0f66faecb3c550fe59709220ade5
Merge: 598bcc6ea6ec 06312f44ad63
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jan 12 13:41:03 2016 +0000

    Merge branch 'devel-stable' into for-linus

commit ad84f56bf6d620fe6ed4d57ce6ec9945684d7f35
Author: Jungseung Lee <js07.lee@samsung.com>
Date:   Tue Dec 29 05:47:00 2015 +0100

    ARM: 8494/1: mm: Enable PXN when running non-LPAE kernel on LPAE processor
    
    The VMSA field of MMFR0 (bottom 4 bits) is incremented for each
    added feature.  PXN is supported if the value is >= 4 and LPAE
    is supported if it is >= 5.
    
    In case a kernel with CONFIG_ARM_LPAE disabled is used on a
    processor that supports LPAE, we can still use PXN in short
    descriptors.  So check for >= 4 not == 4.
    
    Signed-off-by: Jungseung Lee <js07.lee@samsung.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Ben Hutchings <ben@decadent.org.uk>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4867f5daf82c..de9f8921e407 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -572,7 +572,7 @@ static void __init build_mem_type_table(void)
 	 * in the Short-descriptor translation table format descriptors.
 	 */
 	if (cpu_arch == CPU_ARCH_ARMv7 &&
-		(read_cpuid_ext(CPUID_EXT_MMFR0) & 0xF) == 4) {
+		(read_cpuid_ext(CPUID_EXT_MMFR0) & 0xF) >= 4) {
 		user_pmd_table |= PMD_PXNTABLE;
 	}
 #endif

commit 09414d00a137cf7f42b6dc7415f346258d60e8da
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Thu Oct 1 17:58:11 2015 +0200

    ARM: only consider memblocks with NOMAP cleared for linear mapping
    
    Take the new memblock attribute MEMBLOCK_NOMAP into account when
    deciding whether a certain region is or should be covered by the
    kernel direct mapping.
    
    Tested-by: Ryan Harkin <ryan.harkin@linaro.org>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 8c69830e791a..c615d2eb9232 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1435,6 +1435,9 @@ static void __init map_lowmem(void)
 		phys_addr_t end = start + reg->size;
 		struct map_desc map;
 
+		if (memblock_is_nomap(reg))
+			continue;
+
 		if (end > arm_lowmem_limit)
 			end = arm_lowmem_limit;
 		if (start >= end)

commit c7936206b9715d7a3075b53789c1fed0cab9147f
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Wed Apr 29 10:04:17 2015 +0200

    ARM: implement create_mapping_late() for EFI use
    
    This implements create_mapping_late(), which we will use to populate
    the UEFI Runtime Services page tables.
    
    Tested-by: Ryan Harkin <ryan.harkin@linaro.org>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 2d9f628a7fe8..8c69830e791a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -724,6 +724,14 @@ static void __init *early_alloc(unsigned long sz)
 	return early_alloc_aligned(sz, sz);
 }
 
+static void *__init late_alloc(unsigned long sz)
+{
+	void *ptr = (void *)__get_free_pages(PGALLOC_GFP, get_order(sz));
+
+	BUG_ON(!ptr);
+	return ptr;
+}
+
 static pte_t * __init pte_alloc(pmd_t *pmd, unsigned long addr,
 				unsigned long prot,
 				void *(*alloc)(unsigned long sz))
@@ -960,6 +968,18 @@ static void __init create_mapping(struct map_desc *md)
 	__create_mapping(&init_mm, md, early_alloc, false);
 }
 
+void __init create_mapping_late(struct mm_struct *mm, struct map_desc *md,
+				bool ng)
+{
+#ifdef CONFIG_ARM_LPAE
+	pud_t *pud = pud_alloc(mm, pgd_offset(mm, md->virtual), md->virtual);
+	if (WARN_ON(!pud))
+		return;
+	pmd_alloc(mm, pud, 0);
+#endif
+	__create_mapping(mm, md, late_alloc, ng);
+}
+
 /*
  * Create the architecture specific mappings
  */

commit b430e55b2318ca4523b0e39ff6c0d5a2109159b4
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Nov 17 08:46:47 2015 +0100

    ARM: add support for non-global kernel mappings
    
    Add support to the kernel translation table population routines for
    creating non-global mappings. This will be used by the UEFI runtime
    services, which will use temporary mappings in the userland range.
    
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 87dc49dbe231..2d9f628a7fe8 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -745,18 +745,20 @@ static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr,
 static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 				  unsigned long end, unsigned long pfn,
 				  const struct mem_type *type,
-				  void *(*alloc)(unsigned long sz))
+				  void *(*alloc)(unsigned long sz),
+				  bool ng)
 {
 	pte_t *pte = pte_alloc(pmd, addr, type->prot_l1, alloc);
 	do {
-		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
+		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)),
+			    ng ? PTE_EXT_NG : 0);
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }
 
 static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
 			unsigned long end, phys_addr_t phys,
-			const struct mem_type *type)
+			const struct mem_type *type, bool ng)
 {
 	pmd_t *p = pmd;
 
@@ -774,7 +776,7 @@ static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
 		pmd++;
 #endif
 	do {
-		*pmd = __pmd(phys | type->prot_sect);
+		*pmd = __pmd(phys | type->prot_sect | (ng ? PMD_SECT_nG : 0));
 		phys += SECTION_SIZE;
 	} while (pmd++, addr += SECTION_SIZE, addr != end);
 
@@ -784,7 +786,7 @@ static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
 static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 				      unsigned long end, phys_addr_t phys,
 				      const struct mem_type *type,
-				      void *(*alloc)(unsigned long sz))
+				      void *(*alloc)(unsigned long sz), bool ng)
 {
 	pmd_t *pmd = pmd_offset(pud, addr);
 	unsigned long next;
@@ -802,10 +804,10 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 		 */
 		if (type->prot_sect &&
 				((addr | next | phys) & ~SECTION_MASK) == 0) {
-			__map_init_section(pmd, addr, next, phys, type);
+			__map_init_section(pmd, addr, next, phys, type, ng);
 		} else {
 			alloc_init_pte(pmd, addr, next,
-				       __phys_to_pfn(phys), type, alloc);
+				       __phys_to_pfn(phys), type, alloc, ng);
 		}
 
 		phys += next - addr;
@@ -816,14 +818,14 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
 				  unsigned long end, phys_addr_t phys,
 				  const struct mem_type *type,
-				  void *(*alloc)(unsigned long sz))
+				  void *(*alloc)(unsigned long sz), bool ng)
 {
 	pud_t *pud = pud_offset(pgd, addr);
 	unsigned long next;
 
 	do {
 		next = pud_addr_end(addr, end);
-		alloc_init_pmd(pud, addr, next, phys, type, alloc);
+		alloc_init_pmd(pud, addr, next, phys, type, alloc, ng);
 		phys += next - addr;
 	} while (pud++, addr = next, addr != end);
 }
@@ -831,7 +833,8 @@ static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
 #ifndef CONFIG_ARM_LPAE
 static void __init create_36bit_mapping(struct mm_struct *mm,
 					struct map_desc *md,
-					const struct mem_type *type)
+					const struct mem_type *type,
+					bool ng)
 {
 	unsigned long addr, length, end;
 	phys_addr_t phys;
@@ -879,7 +882,8 @@ static void __init create_36bit_mapping(struct mm_struct *mm,
 		int i;
 
 		for (i = 0; i < 16; i++)
-			*pmd++ = __pmd(phys | type->prot_sect | PMD_SECT_SUPER);
+			*pmd++ = __pmd(phys | type->prot_sect | PMD_SECT_SUPER |
+				       (ng ? PMD_SECT_nG : 0));
 
 		addr += SUPERSECTION_SIZE;
 		phys += SUPERSECTION_SIZE;
@@ -889,7 +893,8 @@ static void __init create_36bit_mapping(struct mm_struct *mm,
 #endif	/* !CONFIG_ARM_LPAE */
 
 static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md,
-				    void *(*alloc)(unsigned long sz))
+				    void *(*alloc)(unsigned long sz),
+				    bool ng)
 {
 	unsigned long addr, length, end;
 	phys_addr_t phys;
@@ -903,7 +908,7 @@ static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md,
 	 * Catch 36-bit addresses
 	 */
 	if (md->pfn >= 0x100000) {
-		create_36bit_mapping(mm, md, type);
+		create_36bit_mapping(mm, md, type, ng);
 		return;
 	}
 #endif
@@ -923,7 +928,7 @@ static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md,
 	do {
 		unsigned long next = pgd_addr_end(addr, end);
 
-		alloc_init_pud(pgd, addr, next, phys, type, alloc);
+		alloc_init_pud(pgd, addr, next, phys, type, alloc, ng);
 
 		phys += next - addr;
 		addr = next;
@@ -952,7 +957,7 @@ static void __init create_mapping(struct map_desc *md)
 			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 	}
 
-	__create_mapping(&init_mm, md, early_alloc);
+	__create_mapping(&init_mm, md, early_alloc, false);
 }
 
 /*

commit f579b2b10412771ad5eaa785ddaa7b62b97a6e8d
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 15 14:59:14 2015 +0200

    ARM: factor out allocation routine from __create_mapping()
    
    To allow __create_mapping() to be used for populating UEFI Runtime
    Services page tables, factor out the allocation routine 'early_alloc'
    and pass it down as a function pointer into alloc_init_[pud|pmd|pte].
    This way, new users of __create_mapping() can supply another allocation
    function.
    
    Tested-by: Ryan Harkin <ryan.harkin@linaro.org>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 3100de92148b..87dc49dbe231 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -724,21 +724,30 @@ static void __init *early_alloc(unsigned long sz)
 	return early_alloc_aligned(sz, sz);
 }
 
-static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
+static pte_t * __init pte_alloc(pmd_t *pmd, unsigned long addr,
+				unsigned long prot,
+				void *(*alloc)(unsigned long sz))
 {
 	if (pmd_none(*pmd)) {
-		pte_t *pte = early_alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
+		pte_t *pte = alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
 		__pmd_populate(pmd, __pa(pte), prot);
 	}
 	BUG_ON(pmd_bad(*pmd));
 	return pte_offset_kernel(pmd, addr);
 }
 
+static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr,
+				      unsigned long prot)
+{
+	return pte_alloc(pmd, addr, prot, early_alloc);
+}
+
 static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 				  unsigned long end, unsigned long pfn,
-				  const struct mem_type *type)
+				  const struct mem_type *type,
+				  void *(*alloc)(unsigned long sz))
 {
-	pte_t *pte = early_pte_alloc(pmd, addr, type->prot_l1);
+	pte_t *pte = pte_alloc(pmd, addr, type->prot_l1, alloc);
 	do {
 		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
 		pfn++;
@@ -774,7 +783,8 @@ static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
 
 static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 				      unsigned long end, phys_addr_t phys,
-				      const struct mem_type *type)
+				      const struct mem_type *type,
+				      void *(*alloc)(unsigned long sz))
 {
 	pmd_t *pmd = pmd_offset(pud, addr);
 	unsigned long next;
@@ -795,7 +805,7 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 			__map_init_section(pmd, addr, next, phys, type);
 		} else {
 			alloc_init_pte(pmd, addr, next,
-						__phys_to_pfn(phys), type);
+				       __phys_to_pfn(phys), type, alloc);
 		}
 
 		phys += next - addr;
@@ -805,14 +815,15 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 
 static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
 				  unsigned long end, phys_addr_t phys,
-				  const struct mem_type *type)
+				  const struct mem_type *type,
+				  void *(*alloc)(unsigned long sz))
 {
 	pud_t *pud = pud_offset(pgd, addr);
 	unsigned long next;
 
 	do {
 		next = pud_addr_end(addr, end);
-		alloc_init_pmd(pud, addr, next, phys, type);
+		alloc_init_pmd(pud, addr, next, phys, type, alloc);
 		phys += next - addr;
 	} while (pud++, addr = next, addr != end);
 }
@@ -877,7 +888,8 @@ static void __init create_36bit_mapping(struct mm_struct *mm,
 }
 #endif	/* !CONFIG_ARM_LPAE */
 
-static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md)
+static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md,
+				    void *(*alloc)(unsigned long sz))
 {
 	unsigned long addr, length, end;
 	phys_addr_t phys;
@@ -911,7 +923,7 @@ static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md)
 	do {
 		unsigned long next = pgd_addr_end(addr, end);
 
-		alloc_init_pud(pgd, addr, next, phys, type);
+		alloc_init_pud(pgd, addr, next, phys, type, alloc);
 
 		phys += next - addr;
 		addr = next;
@@ -940,7 +952,7 @@ static void __init create_mapping(struct map_desc *md)
 			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 	}
 
-	__create_mapping(&init_mm, md);
+	__create_mapping(&init_mm, md, early_alloc);
 }
 
 /*

commit 1bdb2d4ee05f2fdad4d8a82d7e0ce8d6d91ec4ac
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 15 14:50:22 2015 +0200

    ARM: split off core mapping logic from create_mapping
    
    In order to be able to reuse the core mapping logic of create_mapping
    for mapping the UEFI Runtime Services into a private set of page tables,
    split it off from create_mapping() into a separate function
    __create_mapping which we will wire up in a subsequent patch.
    
    Tested-by: Ryan Harkin <ryan.harkin@linaro.org>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index de19f90221e2..3100de92148b 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -818,7 +818,8 @@ static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
 }
 
 #ifndef CONFIG_ARM_LPAE
-static void __init create_36bit_mapping(struct map_desc *md,
+static void __init create_36bit_mapping(struct mm_struct *mm,
+					struct map_desc *md,
 					const struct mem_type *type)
 {
 	unsigned long addr, length, end;
@@ -859,7 +860,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	 */
 	phys |= (((md->pfn >> (32 - PAGE_SHIFT)) & 0xF) << 20);
 
-	pgd = pgd_offset_k(addr);
+	pgd = pgd_offset(mm, addr);
 	end = addr + length;
 	do {
 		pud_t *pud = pud_offset(pgd, addr);
@@ -876,33 +877,13 @@ static void __init create_36bit_mapping(struct map_desc *md,
 }
 #endif	/* !CONFIG_ARM_LPAE */
 
-/*
- * Create the page directory entries and any necessary
- * page tables for the mapping specified by `md'.  We
- * are able to cope here with varying sizes and address
- * offsets, and we take full advantage of sections and
- * supersections.
- */
-static void __init create_mapping(struct map_desc *md)
+static void __init __create_mapping(struct mm_struct *mm, struct map_desc *md)
 {
 	unsigned long addr, length, end;
 	phys_addr_t phys;
 	const struct mem_type *type;
 	pgd_t *pgd;
 
-	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
-		pr_warn("BUG: not creating mapping for 0x%08llx at 0x%08lx in user region\n",
-			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
-		return;
-	}
-
-	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
-	    md->virtual >= PAGE_OFFSET && md->virtual < FIXADDR_START &&
-	    (md->virtual < VMALLOC_START || md->virtual >= VMALLOC_END)) {
-		pr_warn("BUG: mapping for 0x%08llx at 0x%08lx out of vmalloc space\n",
-			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
-	}
-
 	type = &mem_types[md->type];
 
 #ifndef CONFIG_ARM_LPAE
@@ -910,7 +891,7 @@ static void __init create_mapping(struct map_desc *md)
 	 * Catch 36-bit addresses
 	 */
 	if (md->pfn >= 0x100000) {
-		create_36bit_mapping(md, type);
+		create_36bit_mapping(mm, md, type);
 		return;
 	}
 #endif
@@ -925,7 +906,7 @@ static void __init create_mapping(struct map_desc *md)
 		return;
 	}
 
-	pgd = pgd_offset_k(addr);
+	pgd = pgd_offset(mm, addr);
 	end = addr + length;
 	do {
 		unsigned long next = pgd_addr_end(addr, end);
@@ -937,6 +918,31 @@ static void __init create_mapping(struct map_desc *md)
 	} while (pgd++, addr != end);
 }
 
+/*
+ * Create the page directory entries and any necessary
+ * page tables for the mapping specified by `md'.  We
+ * are able to cope here with varying sizes and address
+ * offsets, and we take full advantage of sections and
+ * supersections.
+ */
+static void __init create_mapping(struct map_desc *md)
+{
+	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
+		pr_warn("BUG: not creating mapping for 0x%08llx at 0x%08lx in user region\n",
+			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
+		return;
+	}
+
+	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
+	    md->virtual >= PAGE_OFFSET && md->virtual < FIXADDR_START &&
+	    (md->virtual < VMALLOC_START || md->virtual >= VMALLOC_END)) {
+		pr_warn("BUG: mapping for 0x%08llx at 0x%08lx out of vmalloc space\n",
+			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
+	}
+
+	__create_mapping(&init_mm, md);
+}
+
 /*
  * Create the architecture specific mappings
  */

commit 2937367b8a4b0d46ce3312cb997e4a240b02cf15
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Sep 1 08:59:28 2015 +0200

    ARM: add support for generic early_ioremap/early_memremap
    
    This enables the generic early_ioremap implementation for ARM.
    
    It uses the fixmap region reserved for kmap. Since early_ioremap
    is only supported before paging_init(), and kmap is only supported
    afterwards, this is guaranteed not to cause any clashes.
    
    Tested-by: Ryan Harkin <ryan.harkin@linaro.org>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4867f5daf82c..de19f90221e2 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -390,7 +390,7 @@ void __init early_fixmap_init(void)
 	 * The early fixmap range spans multiple pmds, for which
 	 * we are not prepared:
 	 */
-	BUILD_BUG_ON((__fix_to_virt(__end_of_permanent_fixed_addresses) >> PMD_SHIFT)
+	BUILD_BUG_ON((__fix_to_virt(__end_of_early_ioremap_region) >> PMD_SHIFT)
 		     != FIXADDR_TOP >> PMD_SHIFT);
 
 	pmd = fixmap_pmd(FIXADDR_TOP);

commit d33c43ac185e2921e0f541872719588c3d491c60
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Apr 15 15:38:39 2014 +0200

    ARM: make xscale iwmmxt code multiplatform aware
    
    In a multiplatform configuration, we may end up building a kernel for
    both Marvell PJ1 and an ARMv4 CPU implementation. In that case, the
    xscale-cp0 code is built with gcc -march=armv4{,t}, which results in a
    build error from the coprocessor instructions.
    
    Since we know this code will only have to run on an actual xscale
    processor, we can simply build the entire file for ARMv5TE.
    
    Related to this, we need to handle the iWMMXT initialization sequence
    differently during boot, to ensure we don't try to touch xscale
    specific registers on other CPUs from the xscale_cp0_init initcall.
    cpu_is_xscale() used to be hardcoded to '1' in any configuration that
    enables any XScale-compatible core, but this breaks once we can have a
    combined kernel with MMP1 and something else.
    
    In this patch, I replace the existing cpu_is_xscale() macro with a new
    cpu_is_xscale_family() macro that evaluates true for xscale, xsc3 and
    mohawk, which makes the behavior more deterministic.
    
    The two existing users of cpu_is_xscale() are modified accordingly,
    but slightly change behavior for kernels that enable CPU_MOHAWK without
    also enabling CPU_XSCALE or CPU_XSC3. Previously, these would leave leave
    PMD_BIT4 in the page tables untouched, now they clear it as we've always
    done for kernels that enable both MOHAWK and the support for the older
    CPU types.
    
    Since the previous behavior was inconsistent, I assume it was
    unintentional.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4867f5daf82c..e0e911ea32e6 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -477,7 +477,7 @@ static void __init build_mem_type_table(void)
 	 * "update-able on write" bit on ARM610).  However, Xscale and
 	 * Xscale3 require this bit to be cleared.
 	 */
-	if (cpu_is_xscale() || cpu_is_xsc3()) {
+	if (cpu_is_xscale_family()) {
 		for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
 			mem_types[i].prot_sect &= ~PMD_BIT4;
 			mem_types[i].prot_l1 &= ~PMD_BIT4;

commit 9254970cbbf542a0085e491810f0144a27885702
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Mon Oct 19 13:38:09 2015 +0100

    ARM: 8447/1: catch pending imprecise abort on unmask
    
    Install a non-faulting handler just before unmasking imprecise aborts
    and switch back to the regular one after unmasking is done.
    
    This catches any pending imprecise abort that the firmware/bootloader
    may have left behind that would normally crash the kernel at that point.
    As there are apparently a lot of bootlaoders out there that do such a
    thing it makes sense to handle it in the common startup code.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Tested-by: Tyler Baker <tyler.baker@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f65a6f344b6d..4867f5daf82c 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -38,6 +38,7 @@
 #include <asm/mach/pci.h>
 #include <asm/fixmap.h>
 
+#include "fault.h"
 #include "mm.h"
 #include "tcm.h"
 
@@ -1365,7 +1366,7 @@ static void __init devicemaps_init(const struct machine_desc *mdesc)
 	flush_cache_all();
 
 	/* Enable asynchronous aborts */
-	local_abt_enable();
+	early_abt_enable();
 }
 
 static void __init kmap_init(void)

commit bbeb9209515989ff47802d4e5d5702178c8e42c4
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Tue Aug 25 13:52:09 2015 +0100

    ARM: 8422/1: enable imprecise aborts during early kernel startup
    
    This patch adds imprecise abort enable/disable macros and uses them to
    enable imprecise aborts early when starting the kernel.
    
    This helps in tracking down the real cause for such imprecise abort, as
    they are handled as soon as they occur. Until now those aborts would
    only be enabled when entering the userspace and as a consequence crash
    the first userspace process if any abort had been raised during kernel
    startup.
    
    Signed-off-by: Fabrice Gasnier <fabrice.gasnier@st.com>
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 7cd15143a507..f65a6f344b6d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1363,6 +1363,9 @@ static void __init devicemaps_init(const struct machine_desc *mdesc)
 	 */
 	local_flush_tlb_all();
 	flush_cache_all();
+
+	/* Enable asynchronous aborts */
+	local_abt_enable();
 }
 
 static void __init kmap_init(void)

commit 40d3f02851577da27b5cbb1538888301245ef1e7
Merge: e0aa3a665782 3939f3345050 9205b797dbe5 3fa609755c11 a5e090acbf54
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Sep 3 15:28:37 2015 +0100

    Merge branches 'cleanup', 'fixes', 'misc', 'omap-barrier' and 'uaccess' into for-linus

commit a02d8dfd54cdf3b1b0464ccc2c1c4afe2c003a35
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Aug 21 09:38:31 2015 +0100

    ARM: domains: keep vectors in separate domain
    
    Keep the machine vectors in its own domain to avoid software based
    user access control from making the vector code inaccessible, and
    thereby deadlocking the machine.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6ca7d9aa896f..a016de248034 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -291,13 +291,13 @@ static struct mem_type mem_types[] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_RDONLY,
 		.prot_l1   = PMD_TYPE_TABLE,
-		.domain    = DOMAIN_USER,
+		.domain    = DOMAIN_VECTORS,
 	},
 	[MT_HIGH_VECTORS] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_USER | L_PTE_RDONLY,
 		.prot_l1   = PMD_TYPE_TABLE,
-		.domain    = DOMAIN_USER,
+		.domain    = DOMAIN_VECTORS,
 	},
 	[MT_MEMORY_RWX] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,

commit a5f4c561b3b19a9bc43a81da6382b0098ebbc1fb
Author: Stefan Agner <stefan@agner.ch>
Date:   Thu Aug 13 00:01:52 2015 +0100

    ARM: 8415/1: early fixmap support for earlycon
    
    Add early fixmap support, initially to support permanent, fixed
    mapping support for early console. A temporary, early pte is
    created which is migrated to a permanent mapping in paging_init.
    This is also needed since the attributes may change as the memory
    types are initialized. The 3MiB range of fixmap spans two pte
    tables, but currently only one pte is created for early fixmap
    support.
    
    Re-add FIX_KMAP_BEGIN to the index calculation in highmem.c since
    the index for kmap does not start at zero anymore. This reverts
    4221e2e6b316 ("ARM: 8031/1: fixmap: remove FIX_KMAP_BEGIN and
    FIX_KMAP_END") to some extent.
    
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6ca7d9aa896f..fb9e817d08bb 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -357,6 +357,47 @@ const struct mem_type *get_mem_type(unsigned int type)
 }
 EXPORT_SYMBOL(get_mem_type);
 
+static pte_t *(*pte_offset_fixmap)(pmd_t *dir, unsigned long addr);
+
+static pte_t bm_pte[PTRS_PER_PTE + PTE_HWTABLE_PTRS]
+	__aligned(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE) __initdata;
+
+static pte_t * __init pte_offset_early_fixmap(pmd_t *dir, unsigned long addr)
+{
+	return &bm_pte[pte_index(addr)];
+}
+
+static pte_t *pte_offset_late_fixmap(pmd_t *dir, unsigned long addr)
+{
+	return pte_offset_kernel(dir, addr);
+}
+
+static inline pmd_t * __init fixmap_pmd(unsigned long addr)
+{
+	pgd_t *pgd = pgd_offset_k(addr);
+	pud_t *pud = pud_offset(pgd, addr);
+	pmd_t *pmd = pmd_offset(pud, addr);
+
+	return pmd;
+}
+
+void __init early_fixmap_init(void)
+{
+	pmd_t *pmd;
+
+	/*
+	 * The early fixmap range spans multiple pmds, for which
+	 * we are not prepared:
+	 */
+	BUILD_BUG_ON((__fix_to_virt(__end_of_permanent_fixed_addresses) >> PMD_SHIFT)
+		     != FIXADDR_TOP >> PMD_SHIFT);
+
+	pmd = fixmap_pmd(FIXADDR_TOP);
+	pmd_populate_kernel(&init_mm, pmd, bm_pte);
+
+	pte_offset_fixmap = pte_offset_early_fixmap;
+}
+
 /*
  * To avoid TLB flush broadcasts, this uses local_flush_tlb_kernel_range().
  * As a result, this can only be called with preemption disabled, as under
@@ -365,7 +406,7 @@ EXPORT_SYMBOL(get_mem_type);
 void __set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t prot)
 {
 	unsigned long vaddr = __fix_to_virt(idx);
-	pte_t *pte = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
+	pte_t *pte = pte_offset_fixmap(pmd_off_k(vaddr), vaddr);
 
 	/* Make sure fixmap region does not exceed available allocation. */
 	BUILD_BUG_ON(FIXADDR_START + (__end_of_fixed_addresses * PAGE_SIZE) >
@@ -855,7 +896,7 @@ static void __init create_mapping(struct map_desc *md)
 	}
 
 	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
-	    md->virtual >= PAGE_OFFSET &&
+	    md->virtual >= PAGE_OFFSET && md->virtual < FIXADDR_START &&
 	    (md->virtual < VMALLOC_START || md->virtual >= VMALLOC_END)) {
 		pr_warn("BUG: mapping for 0x%08llx at 0x%08lx out of vmalloc space\n",
 			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
@@ -1213,10 +1254,10 @@ void __init arm_mm_memblock_reserve(void)
 
 /*
  * Set up the device mappings.  Since we clear out the page tables for all
- * mappings above VMALLOC_START, we will remove any debug device mappings.
- * This means you have to be careful how you debug this function, or any
- * called function.  This means you can't use any function or debugging
- * method which may touch any device, otherwise the kernel _will_ crash.
+ * mappings above VMALLOC_START, except early fixmap, we might remove debug
+ * device mappings.  This means earlycon can be used to debug this function
+ * Any other function or debugging method which may touch any device _will_
+ * crash the kernel.
  */
 static void __init devicemaps_init(const struct machine_desc *mdesc)
 {
@@ -1231,7 +1272,10 @@ static void __init devicemaps_init(const struct machine_desc *mdesc)
 
 	early_trap_init(vectors);
 
-	for (addr = VMALLOC_START; addr; addr += PMD_SIZE)
+	/*
+	 * Clear page table except top pmd used by early fixmaps
+	 */
+	for (addr = VMALLOC_START; addr < (FIXADDR_TOP & PMD_MASK); addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
 	/*
@@ -1483,6 +1527,35 @@ void __init early_paging_init(const struct machine_desc *mdesc)
 
 #endif
 
+static void __init early_fixmap_shutdown(void)
+{
+	int i;
+	unsigned long va = fix_to_virt(__end_of_permanent_fixed_addresses - 1);
+
+	pte_offset_fixmap = pte_offset_late_fixmap;
+	pmd_clear(fixmap_pmd(va));
+	local_flush_tlb_kernel_page(va);
+
+	for (i = 0; i < __end_of_permanent_fixed_addresses; i++) {
+		pte_t *pte;
+		struct map_desc map;
+
+		map.virtual = fix_to_virt(i);
+		pte = pte_offset_early_fixmap(pmd_off_k(map.virtual), map.virtual);
+
+		/* Only i/o device mappings are supported ATM */
+		if (pte_none(*pte) ||
+		    (pte_val(*pte) & L_PTE_MT_MASK) != L_PTE_MT_DEV_SHARED)
+			continue;
+
+		map.pfn = pte_pfn(*pte);
+		map.type = MT_DEVICE;
+		map.length = PAGE_SIZE;
+
+		create_mapping(&map);
+	}
+}
+
 /*
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.
@@ -1495,6 +1568,7 @@ void __init paging_init(const struct machine_desc *mdesc)
 	prepare_page_table();
 	map_lowmem();
 	dma_contiguous_remap();
+	early_fixmap_shutdown();
 	devicemaps_init(mdesc);
 	kmap_init();
 	tcm_init();

commit 06be5eefe1192eb8ce8d07497f67595b6bfe9741
Merge: 11b8b25ce4f8 1bd46782d08b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jul 7 12:35:33 2015 +0100

    Merge branches 'fixes' and 'ioremap' into for-linus

commit eeb3fee8f6cca5e7bf1647d9e327c7b40e384578
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jun 25 10:52:58 2015 +0100

    ARM: add helpful message when truncating physical memory
    
    Add a nmessage to suggest that HIGHMEM is enabled when physical memory
    is truncated due to lack of virtual address space to map it in the low
    memory mapping.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 904d1532e6d0..79de062c6077 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1072,6 +1072,7 @@ void __init sanity_check_meminfo(void)
 	int highmem = 0;
 	phys_addr_t vmalloc_limit = __pa(vmalloc_min - 1) + 1;
 	struct memblock_region *reg;
+	bool should_use_highmem = false;
 
 	for_each_memblock(memory, reg) {
 		phys_addr_t block_start = reg->base;
@@ -1090,6 +1091,7 @@ void __init sanity_check_meminfo(void)
 				pr_notice("Ignoring RAM at %pa-%pa (!CONFIG_HIGHMEM)\n",
 					  &block_start, &block_end);
 				memblock_remove(reg->base, reg->size);
+				should_use_highmem = true;
 				continue;
 			}
 
@@ -1100,6 +1102,7 @@ void __init sanity_check_meminfo(void)
 					  &block_start, &block_end, &vmalloc_limit);
 				memblock_remove(vmalloc_limit, overlap_size);
 				block_end = vmalloc_limit;
+				should_use_highmem = true;
 			}
 		}
 
@@ -1134,6 +1137,9 @@ void __init sanity_check_meminfo(void)
 		}
 	}
 
+	if (should_use_highmem)
+		pr_notice("Consider using a HIGHMEM enabled kernel.\n");
+
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 
 	/*

commit 3de1f52a3ae823265299409e276f56cdadddc310
Author: Laura Abbott <labbott@redhat.com>
Date:   Thu Jun 25 01:04:20 2015 +0100

    ARM: 8394/1: update memblock limit after mapping lowmem
    
    The memblock limit is currently used in find_limits
    to find the bounds for ZONE_NORMAL. The memblock
    limit may need to be rounded down a PMD size to ensure
    allocations are fully mapped though. This has the side
    effect of reducing the amount of memory in ZONE_NORMAL.
    Once all lowmem is mapped, it's safe to change the memblock
    limit back to include the unaligned section. Adjust the
    memblock limit after lowmem mapping is complete.
    
    Before:
     # cat /proc/zoneinfo | grep managed
            managed  62907
            managed  424
    
    After:
     # cat /proc/zoneinfo | grep managed
            managed  63331
    
    Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Tested-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 7186382672b5..904d1532e6d0 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1519,6 +1519,7 @@ void __init paging_init(const struct machine_desc *mdesc)
 	build_mem_type_table();
 	prepare_page_table();
 	map_lowmem();
+	memblock_set_current_limit(arm_lowmem_limit);
 	dma_contiguous_remap();
 	devicemaps_init(mdesc);
 	kmap_init();

commit 9de44aa4dc969e4a46c2bfbd33d65bfa6ad2a15d
Merge: 6fb18ac9366b 079ed3681d74 ec3bd0e68a67 c76f238e261b 0bbe6b5a73c0
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jun 12 21:18:08 2015 +0100

    Merge branches 'arnd-fixes', 'clk', 'misc', 'v7' and 'fixes' into for-next

commit 1221ed10f2a56ecdd8ff75f436f52aca5ba0f1d3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Apr 4 17:25:20 2015 +0100

    ARM: cleanup early_paging_init() calling
    
    Eliminate the needless nommu version of this function, and get rid of
    the proc_info_list structure argument - we no longer need this in order
    to fix up the page table entries.
    
    Acked-by: Santosh Shilimkar <ssantosh@kernel.org>
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index a5d3180a6886..5aefa9533684 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1396,8 +1396,7 @@ pgtables_remap lpae_pgtables_remap_asm;
  * early_paging_init() recreates boot time page table setup, allowing machines
  * to switch over to a high (>4G) address space on LPAE systems
  */
-void __init early_paging_init(const struct machine_desc *mdesc,
-			      struct proc_info_list *procinfo)
+void __init early_paging_init(const struct machine_desc *mdesc)
 {
 	pgtables_remap *lpae_pgtables_remap;
 	unsigned long pa_pgd;
@@ -1465,8 +1464,7 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 
 #else
 
-void __init early_paging_init(const struct machine_desc *mdesc,
-			      struct proc_info_list *procinfo)
+void __init early_paging_init(const struct machine_desc *mdesc)
 {
 	long long offset;
 

commit d8dc7fbd53eeb329a1dda5a19df7058b9c1c413e
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Apr 4 16:58:38 2015 +0100

    ARM: re-implement physical address space switching
    
    Re-implement the physical address space switching to be architecturally
    compliant.  This involves flushing the caches, disabling the MMU, and
    only then updating the page tables.  Once that is complete, the system
    can be brought back up again.
    
    Since we disable the MMU, we need to do the update in assembly code.
    Luckily, the entries which need updating are fairly trivial, and are
    all setup by the early assembly code.  We can merely adjust each entry
    by the delta required.
    
    Not only does this fix the code to be architecturally compliant, but it
    fixes a couple of bugs too:
    
    1. The original code would only ever update the first L2 entry covering
       a fraction of the kernel; the remainder were left untouched.
    2. The L2 entries covering the DTB blob were likewise untouched.
    
    This solution fixes up all entries.
    
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 0e5ed87221dd..a5d3180a6886 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1387,7 +1387,11 @@ static void __init map_lowmem(void)
 	}
 }
 
-#if defined(CONFIG_ARM_LPAE) && defined(CONFIG_ARM_PATCH_PHYS_VIRT)
+#ifdef CONFIG_ARM_PV_FIXUP
+extern unsigned long __atags_pointer;
+typedef void pgtables_remap(long long offset, unsigned long pgd, void *bdata);
+pgtables_remap lpae_pgtables_remap_asm;
+
 /*
  * early_paging_init() recreates boot time page table setup, allowing machines
  * to switch over to a high (>4G) address space on LPAE systems
@@ -1395,35 +1399,30 @@ static void __init map_lowmem(void)
 void __init early_paging_init(const struct machine_desc *mdesc,
 			      struct proc_info_list *procinfo)
 {
-	pmdval_t pmdprot = procinfo->__cpu_mm_mmu_flags;
-	unsigned long map_start, map_end;
+	pgtables_remap *lpae_pgtables_remap;
+	unsigned long pa_pgd;
+	unsigned int cr, ttbcr;
 	long long offset;
-	pgd_t *pgd0, *pgdk;
-	pud_t *pud0, *pudk, *pud_start;
-	pmd_t *pmd0, *pmdk;
-	phys_addr_t phys;
-	int i;
+	void *boot_data;
 
 	if (!mdesc->pv_fixup)
 		return;
 
-	/* remap kernel code and data */
-	map_start = init_mm.start_code & PMD_MASK;
-	map_end   = ALIGN(init_mm.brk, PMD_SIZE);
-
-	/* get a handle on things... */
-	pgd0 = pgd_offset_k(0);
-	pud_start = pud0 = pud_offset(pgd0, 0);
-	pmd0 = pmd_offset(pud0, 0);
-
-	pgdk = pgd_offset_k(map_start);
-	pudk = pud_offset(pgdk, map_start);
-	pmdk = pmd_offset(pudk, map_start);
-
 	offset = mdesc->pv_fixup();
 	if (offset == 0)
 		return;
 
+	/*
+	 * Get the address of the remap function in the 1:1 identity
+	 * mapping setup by the early page table assembly code.  We
+	 * must get this prior to the pv update.  The following barrier
+	 * ensures that this is complete before we fixup any P:V offsets.
+	 */
+	lpae_pgtables_remap = (pgtables_remap *)(unsigned long)__pa(lpae_pgtables_remap_asm);
+	pa_pgd = __pa(swapper_pg_dir);
+	boot_data = __va(__atags_pointer);
+	barrier();
+
 	pr_info("Switching physical address space to 0x%08llx\n",
 		(u64)PHYS_OFFSET + offset);
 
@@ -1436,75 +1435,32 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 		(&__pv_table_end - &__pv_table_begin) << 2);
 
 	/*
-	 * Cache cleaning operations for self-modifying code
-	 * We should clean the entries by MVA but running a
-	 * for loop over every pv_table entry pointer would
-	 * just complicate the code.
-	 */
-	flush_cache_louis();
-	dsb(ishst);
-	isb();
-
-	/*
-	 * FIXME: This code is not architecturally compliant: we modify
-	 * the mappings in-place, indeed while they are in use by this
-	 * very same code.  This may lead to unpredictable behaviour of
-	 * the CPU.
-	 *
-	 * Even modifying the mappings in a separate page table does
-	 * not resolve this.
-	 *
-	 * The architecture strongly recommends that when a mapping is
-	 * changed, that it is changed by first going via an invalid
-	 * mapping and back to the new mapping.  This is to ensure that
-	 * no TLB conflicts (caused by the TLB having more than one TLB
-	 * entry match a translation) can occur.  However, doing that
-	 * here will result in unmapping the code we are running.
-	 */
-	pr_warn("WARNING: unsafe modification of in-place page tables - tainting kernel\n");
-	add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_STILL_OK);
-
-	/*
-	 * Remap level 1 table.  This changes the physical addresses
-	 * used to refer to the level 2 page tables to the high
-	 * physical address alias, leaving everything else the same.
-	 */
-	for (i = 0; i < PTRS_PER_PGD; pud0++, i++) {
-		set_pud(pud0,
-			__pud(__pa(pmd0) | PMD_TYPE_TABLE | L_PGD_SWAPPER));
-		pmd0 += PTRS_PER_PMD;
-	}
-
-	/*
-	 * Remap the level 2 table, pointing the mappings at the high
-	 * physical address alias of these pages.
-	 */
-	phys = __pa(map_start);
-	do {
-		*pmdk++ = __pmd(phys | pmdprot);
-		phys += PMD_SIZE;
-	} while (phys < map_end);
-
-	/*
-	 * Ensure that the above updates are flushed out of the cache.
-	 * This is not strictly correct; on a system where the caches
-	 * are coherent with each other, but the MMU page table walks
-	 * may not be coherent, flush_cache_all() may be a no-op, and
-	 * this will fail.
+	 * We changing not only the virtual to physical mapping, but also
+	 * the physical addresses used to access memory.  We need to flush
+	 * all levels of cache in the system with caching disabled to
+	 * ensure that all data is written back, and nothing is prefetched
+	 * into the caches.  We also need to prevent the TLB walkers
+	 * allocating into the caches too.  Note that this is ARMv7 LPAE
+	 * specific.
 	 */
+	cr = get_cr();
+	set_cr(cr & ~(CR_I | CR_C));
+	asm("mrc p15, 0, %0, c2, c0, 2" : "=r" (ttbcr));
+	asm volatile("mcr p15, 0, %0, c2, c0, 2"
+		: : "r" (ttbcr & ~(3 << 8 | 3 << 10)));
 	flush_cache_all();
 
 	/*
-	 * Re-write the TTBR values to point them at the high physical
-	 * alias of the page tables.  We expect __va() will work on
-	 * cpu_get_pgd(), which returns the value of TTBR0.
+	 * Fixup the page tables - this must be in the idmap region as
+	 * we need to disable the MMU to do this safely, and hence it
+	 * needs to be assembly.  It's fairly simple, as we're using the
+	 * temporary tables setup by the initial assembly code.
 	 */
-	cpu_switch_mm(pgd0, &init_mm);
-	cpu_set_ttbr(1, __pa(pgd0) + TTBR1_OFFSET);
+	lpae_pgtables_remap(offset, pa_pgd, boot_data);
 
-	/* Finally flush any stale TLB values. */
-	local_flush_bp_all();
-	local_flush_tlb_all();
+	/* Re-enable the caches and cacheable TLB walks */
+	asm volatile("mcr p15, 0, %0, c2, c0, 2" : : "r" (ttbcr));
+	set_cr(cr);
 }
 
 #else

commit c0b759d87eab301af0380f5459057656178e78cf
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Apr 4 10:01:10 2015 +0100

    ARM: keystone2: rename init_meminfo to pv_fixup
    
    The init_meminfo() method is not about initialising meminfo - it's about
    fixing up the physical to virtual translation so that we use a different
    physical address space, possibly above the 4GB physical address space.
    Therefore, the name "init_meminfo()" is confusing.
    
    Rename it to pv_fixup() instead.
    
    Acked-by: Santosh Shilimkar <ssantosh@kernel.org>
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 91262d28a4c9..0e5ed87221dd 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1404,7 +1404,7 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 	phys_addr_t phys;
 	int i;
 
-	if (!(mdesc->init_meminfo))
+	if (!mdesc->pv_fixup)
 		return;
 
 	/* remap kernel code and data */
@@ -1420,7 +1420,7 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 	pudk = pud_offset(pgdk, map_start);
 	pmdk = pmd_offset(pudk, map_start);
 
-	offset = mdesc->init_meminfo();
+	offset = mdesc->pv_fixup();
 	if (offset == 0)
 		return;
 
@@ -1514,10 +1514,10 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 {
 	long long offset;
 
-	if (!mdesc->init_meminfo)
+	if (!mdesc->pv_fixup)
 		return;
 
-	offset = mdesc->init_meminfo();
+	offset = mdesc->pv_fixup();
 	if (offset == 0)
 		return;
 

commit 39b74fe82f734ac0bec726733805bb7d46c95933
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Apr 4 10:25:28 2015 +0100

    ARM: keystone2: move address space switch printk into generic code
    
    There is no point platform code doing this, let's move it into the
    generic code so it doesn't get duplicated.
    
    Acked-by: Santosh Shilimkar <ssantosh@kernel.org>
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 38ccbdf6c322..91262d28a4c9 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1424,6 +1424,9 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 	if (offset == 0)
 		return;
 
+	pr_info("Switching physical address space to 0x%08llx\n",
+		(u64)PHYS_OFFSET + offset);
+
 	/* Re-set the phys pfn offset, and the pv offset */
 	__pv_offset += offset;
 	__pv_phys_pfn_offset += PFN_DOWN(offset);

commit c8ca2b4b29282605698968d15667939b23e208e2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Apr 4 09:53:38 2015 +0100

    ARM: keystone2: move update of the phys-to-virt constants into generic code
    
    Make the init_meminfo function return the offset to be applied to the
    phys-to-virt translation constants.  This allows us to move the update
    into generic code, along with the requirements for this update.
    
    This avoids platforms having to know the details of the phys-to-virt
    translation support.
    
    Acked-by: Santosh Shilimkar <ssantosh@kernel.org>
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4e6ef896c619..38ccbdf6c322 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1387,7 +1387,7 @@ static void __init map_lowmem(void)
 	}
 }
 
-#ifdef CONFIG_ARM_LPAE
+#if defined(CONFIG_ARM_LPAE) && defined(CONFIG_ARM_PATCH_PHYS_VIRT)
 /*
  * early_paging_init() recreates boot time page table setup, allowing machines
  * to switch over to a high (>4G) address space on LPAE systems
@@ -1397,6 +1397,7 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 {
 	pmdval_t pmdprot = procinfo->__cpu_mm_mmu_flags;
 	unsigned long map_start, map_end;
+	long long offset;
 	pgd_t *pgd0, *pgdk;
 	pud_t *pud0, *pudk, *pud_start;
 	pmd_t *pmd0, *pmdk;
@@ -1419,7 +1420,13 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 	pudk = pud_offset(pgdk, map_start);
 	pmdk = pmd_offset(pudk, map_start);
 
-	mdesc->init_meminfo();
+	offset = mdesc->init_meminfo();
+	if (offset == 0)
+		return;
+
+	/* Re-set the phys pfn offset, and the pv offset */
+	__pv_offset += offset;
+	__pv_phys_pfn_offset += PFN_DOWN(offset);
 
 	/* Run the patch stub to update the constants */
 	fixup_pv_table(&__pv_table_begin,
@@ -1502,8 +1509,19 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 void __init early_paging_init(const struct machine_desc *mdesc,
 			      struct proc_info_list *procinfo)
 {
-	if (mdesc->init_meminfo)
-		mdesc->init_meminfo();
+	long long offset;
+
+	if (!mdesc->init_meminfo)
+		return;
+
+	offset = mdesc->init_meminfo();
+	if (offset == 0)
+		return;
+
+	pr_crit("Physical address space modification is only to support Keystone2.\n");
+	pr_crit("Please enable ARM_LPAE and ARM_PATCH_PHYS_VIRT support to use this\n");
+	pr_crit("feature. Your kernel may crash now, have a good day.\n");
+	add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_STILL_OK);
 }
 
 #endif

commit 965278dcb8ab0b1f666cc47937933c4be4aea48d
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed May 13 15:07:54 2015 +0100

    ARM: 8356/1: mm: handle non-pmd-aligned end of RAM
    
    At boot time we round the memblock limit down to section size in an
    attempt to ensure that we will have mapped this RAM with section
    mappings prior to allocating from it. When mapping RAM we iterate over
    PMD-sized chunks, creating these section mappings.
    
    Section mappings are only created when the end of a chunk is aligned to
    section size. Unfortunately, with classic page tables (where PMD_SIZE is
    2 * SECTION_SIZE) this means that if a chunk is between 1M and 2M in
    size the first 1M will not be mapped despite having been accounted for
    in the memblock limit. This has been observed to result in page tables
    being allocated from unmapped memory, causing boot-time hangs.
    
    This patch modifies the memblock limit rounding to always round down to
    PMD_SIZE instead of SECTION_SIZE. For classic MMU this means that we
    will round the memblock limit down to a 2M boundary, matching the limits
    on section mappings, and preventing allocations from unmapped memory.
    For LPAE there should be no change as PMD_SIZE == SECTION_SIZE.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: Stefan Agner <stefan@agner.ch>
    Tested-by: Stefan Agner <stefan@agner.ch>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Tested-by: Hans de Goede <hdegoede@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4e6ef896c619..7186382672b5 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1112,22 +1112,22 @@ void __init sanity_check_meminfo(void)
 			}
 
 			/*
-			 * Find the first non-section-aligned page, and point
+			 * Find the first non-pmd-aligned page, and point
 			 * memblock_limit at it. This relies on rounding the
-			 * limit down to be section-aligned, which happens at
-			 * the end of this function.
+			 * limit down to be pmd-aligned, which happens at the
+			 * end of this function.
 			 *
 			 * With this algorithm, the start or end of almost any
-			 * bank can be non-section-aligned. The only exception
-			 * is that the start of the bank 0 must be section-
+			 * bank can be non-pmd-aligned. The only exception is
+			 * that the start of the bank 0 must be section-
 			 * aligned, since otherwise memory would need to be
 			 * allocated when mapping the start of bank 0, which
 			 * occurs before any free memory is mapped.
 			 */
 			if (!memblock_limit) {
-				if (!IS_ALIGNED(block_start, SECTION_SIZE))
+				if (!IS_ALIGNED(block_start, PMD_SIZE))
 					memblock_limit = block_start;
-				else if (!IS_ALIGNED(block_end, SECTION_SIZE))
+				else if (!IS_ALIGNED(block_end, PMD_SIZE))
 					memblock_limit = arm_lowmem_limit;
 			}
 
@@ -1137,12 +1137,12 @@ void __init sanity_check_meminfo(void)
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 
 	/*
-	 * Round the memblock limit down to a section size.  This
+	 * Round the memblock limit down to a pmd size.  This
 	 * helps to ensure that we will allocate memory from the
-	 * last full section, which should be mapped.
+	 * last full pmd, which should be mapped.
 	 */
 	if (memblock_limit)
-		memblock_limit = round_down(memblock_limit, SECTION_SIZE);
+		memblock_limit = round_down(memblock_limit, PMD_SIZE);
 	if (!memblock_limit)
 		memblock_limit = arm_lowmem_limit;
 

commit ac08468867e99bc02b22baf4e58bc3537e9d852c
Author: Grygorii Strashko <grygorii.strashko@linaro.org>
Date:   Tue Dec 23 19:36:55 2014 +0100

    ARM: 8253/1: mm: use phys_addr_t type in map_lowmem() for kernel mem region
    
    Now local variables kernel_x_start and kernel_x_end defined using
    'unsigned long' type which is wrong because they represent physical
    memory range and will be calculated wrongly if LPAE is enabled.
    As result, all following code in map_lowmem() will not work correctly.
    
    For example, Keystone 2 boot is broken because
     kernel_x_start == 0x0000 0000
     kernel_x_end   == 0x0080 0000
    
    instead of
     kernel_x_start == 0x0000 0008 0000 0000
     kernel_x_end   == 0x0000 0008 0080 0000
    and as result whole low memory will be mapped with MT_MEMORY_RW
    permissions by code (start > kernel_x_end):
                    } else if (start >= kernel_x_end) {
                            map.pfn = __phys_to_pfn(start);
                            map.virtual = __phys_to_virt(start);
                            map.length = end - start;
                            map.type = MT_MEMORY_RW;
    
                            create_mapping(&map);
                    }
    
    Hence, fix it by using phys_addr_t type for variables kernel_x_start
    and kernel_x_end.
    
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Grygorii Strashko <grygorii.strashko@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index cda7c40999b6..4e6ef896c619 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1329,8 +1329,8 @@ static void __init kmap_init(void)
 static void __init map_lowmem(void)
 {
 	struct memblock_region *reg;
-	unsigned long kernel_x_start = round_down(__pa(_stext), SECTION_SIZE);
-	unsigned long kernel_x_end = round_up(__pa(__init_end), SECTION_SIZE);
+	phys_addr_t kernel_x_start = round_down(__pa(_stext), SECTION_SIZE);
+	phys_addr_t kernel_x_end = round_up(__pa(__init_end), SECTION_SIZE);
 
 	/* Map all the lowmem memory banks. */
 	for_each_memblock(memory, reg) {

commit e9f2d6d66037cdf97487491e04053f411abc5d16
Merge: fbe4dd088f44 06e944b8e5fc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Dec 5 16:30:54 2014 +0000

    Merge branch 'devel-stable' into for-next

commit 4e802cfd74e6351cb937997996de9ca2e67df9f2
Author: Jungseung Lee <js07.lee@gmail.com>
Date:   Sat Nov 29 03:02:11 2014 +0100

    ARM: 8238/1: mm: Refine set_memory_* functions
    
    set_memory_* functions have same implementation
    except memory attribute.
    
    This patch makes to use common function for these, and pull out
    the functions into arch/arm/mm/pageattr.c like arm64 did.
    It will reduce code size and enhance the readability.
    
    Signed-off-by: Jungseung Lee <js07.lee@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b590a4c92462..f86ce1a9f525 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -356,44 +356,6 @@ const struct mem_type *get_mem_type(unsigned int type)
 }
 EXPORT_SYMBOL(get_mem_type);
 
-#define PTE_SET_FN(_name, pteop) \
-static int pte_set_##_name(pte_t *ptep, pgtable_t token, unsigned long addr, \
-			void *data) \
-{ \
-	pte_t pte = pteop(*ptep); \
-\
-	set_pte_ext(ptep, pte, 0); \
-	return 0; \
-} \
-
-#define SET_MEMORY_FN(_name, callback) \
-int set_memory_##_name(unsigned long addr, int numpages) \
-{ \
-	unsigned long start = addr; \
-	unsigned long size = PAGE_SIZE*numpages; \
-	unsigned end = start + size; \
-\
-	if (start < MODULES_VADDR || start >= MODULES_END) \
-		return -EINVAL;\
-\
-	if (end < MODULES_VADDR || end >= MODULES_END) \
-		return -EINVAL; \
-\
-	apply_to_page_range(&init_mm, start, size, callback, NULL); \
-	flush_tlb_kernel_range(start, end); \
-	return 0;\
-}
-
-PTE_SET_FN(ro, pte_wrprotect)
-PTE_SET_FN(rw, pte_mkwrite)
-PTE_SET_FN(x, pte_mkexec)
-PTE_SET_FN(nx, pte_mknexec)
-
-SET_MEMORY_FN(ro, pte_set_ro)
-SET_MEMORY_FN(rw, pte_set_rw)
-SET_MEMORY_FN(x, pte_set_x)
-SET_MEMORY_FN(nx, pte_set_nx)
-
 /*
  * Adjust the PMD section entries according to the CPU in use.
  */

commit 1d4d37159d013a4c54d785407dd8902f901d7bc5
Author: Jungseung Lee <js07.lee@gmail.com>
Date:   Sat Nov 29 02:33:30 2014 +0100

    ARM: 8235/1: Support for the PXN CPU feature on ARMv7
    
    Modern ARMv7-A/R cores optionally implement below new
    hardware feature:
    
    - PXN:
    Privileged execute-never(PXN) is a security feature. PXN bit
    determines whether the processor can execute software from
    the region. This is effective solution against ret2usr attack.
    On an implementation that does not include the LPAE, PXN is
    optionally supported.
    
    This patch set PXN bit on user page table for preventing
    user code execution with privilege mode.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Jungseung Lee <js07.lee@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d028dc74a9d5..b590a4c92462 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -52,6 +52,8 @@ EXPORT_SYMBOL(empty_zero_page);
  */
 pmd_t *top_pmd;
 
+pmdval_t user_pmd_table = _PAGE_USER_TABLE;
+
 #define CPOLICY_UNCACHED	0
 #define CPOLICY_BUFFERED	1
 #define CPOLICY_WRITETHROUGH	2
@@ -528,14 +530,23 @@ static void __init build_mem_type_table(void)
 	hyp_device_pgprot = mem_types[MT_DEVICE].prot_pte;
 	s2_device_pgprot = mem_types[MT_DEVICE].prot_pte_s2;
 
+#ifndef CONFIG_ARM_LPAE
 	/*
 	 * We don't use domains on ARMv6 (since this causes problems with
 	 * v6/v7 kernels), so we must use a separate memory type for user
 	 * r/o, kernel r/w to map the vectors page.
 	 */
-#ifndef CONFIG_ARM_LPAE
 	if (cpu_arch == CPU_ARCH_ARMv6)
 		vecs_pgprot |= L_PTE_MT_VECTORS;
+
+	/*
+	 * Check is it with support for the PXN bit
+	 * in the Short-descriptor translation table format descriptors.
+	 */
+	if (cpu_arch == CPU_ARCH_ARMv7 &&
+		(read_cpuid_ext(CPUID_EXT_MMFR0) & 0xF) == 4) {
+		user_pmd_table |= PMD_PXNTABLE;
+	}
 #endif
 
 	/*
@@ -605,6 +616,11 @@ static void __init build_mem_type_table(void)
 	}
 	kern_pgprot |= PTE_EXT_AF;
 	vecs_pgprot |= PTE_EXT_AF;
+
+	/*
+	 * Set PXN for user mappings
+	 */
+	user_pgprot |= PTE_EXT_PXN;
 #endif
 
 	for (i = 0; i < 16; i++) {

commit 4ed89f2228061422ce5f62545fd0b6f6648bd2cc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Oct 28 11:26:42 2014 +0000

    ARM: convert printk(KERN_* to pr_*
    
    Convert many (but not all) printk(KERN_* to pr_* to simplify the code.
    We take the opportunity to join some printk lines together so we don't
    split the message across several lines, and we also add a few levels
    to some messages which were previously missing them.
    
    Tested-by: Andrew Lunn <andrew@lunn.ch>
    Tested-by: Felipe Balbi <balbi@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9f98cec7fe1e..d028dc74a9d5 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -192,7 +192,7 @@ early_param("cachepolicy", early_cachepolicy);
 static int __init early_nocache(char *__unused)
 {
 	char *p = "buffered";
-	printk(KERN_WARNING "nocache is deprecated; use cachepolicy=%s\n", p);
+	pr_warn("nocache is deprecated; use cachepolicy=%s\n", p);
 	early_cachepolicy(p);
 	return 0;
 }
@@ -201,7 +201,7 @@ early_param("nocache", early_nocache);
 static int __init early_nowrite(char *__unused)
 {
 	char *p = "uncached";
-	printk(KERN_WARNING "nowb is deprecated; use cachepolicy=%s\n", p);
+	pr_warn("nowb is deprecated; use cachepolicy=%s\n", p);
 	early_cachepolicy(p);
 	return 0;
 }
@@ -786,8 +786,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	length = PAGE_ALIGN(md->length);
 
 	if (!(cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())) {
-		printk(KERN_ERR "MM: CPU does not support supersection "
-		       "mapping for 0x%08llx at 0x%08lx\n",
+		pr_err("MM: CPU does not support supersection mapping for 0x%08llx at 0x%08lx\n",
 		       (long long)__pfn_to_phys((u64)md->pfn), addr);
 		return;
 	}
@@ -799,15 +798,13 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	 *	of the actual domain assignments in use.
 	 */
 	if (type->domain) {
-		printk(KERN_ERR "MM: invalid domain in supersection "
-		       "mapping for 0x%08llx at 0x%08lx\n",
+		pr_err("MM: invalid domain in supersection mapping for 0x%08llx at 0x%08lx\n",
 		       (long long)__pfn_to_phys((u64)md->pfn), addr);
 		return;
 	}
 
 	if ((addr | length | __pfn_to_phys(md->pfn)) & ~SUPERSECTION_MASK) {
-		printk(KERN_ERR "MM: cannot create mapping for 0x%08llx"
-		       " at 0x%08lx invalid alignment\n",
+		pr_err("MM: cannot create mapping for 0x%08llx at 0x%08lx invalid alignment\n",
 		       (long long)__pfn_to_phys((u64)md->pfn), addr);
 		return;
 	}
@@ -850,18 +847,16 @@ static void __init create_mapping(struct map_desc *md)
 	pgd_t *pgd;
 
 	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
-		printk(KERN_WARNING "BUG: not creating mapping for 0x%08llx"
-		       " at 0x%08lx in user region\n",
-		       (long long)__pfn_to_phys((u64)md->pfn), md->virtual);
+		pr_warn("BUG: not creating mapping for 0x%08llx at 0x%08lx in user region\n",
+			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 		return;
 	}
 
 	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
 	    md->virtual >= PAGE_OFFSET &&
 	    (md->virtual < VMALLOC_START || md->virtual >= VMALLOC_END)) {
-		printk(KERN_WARNING "BUG: mapping for 0x%08llx"
-		       " at 0x%08lx out of vmalloc space\n",
-		       (long long)__pfn_to_phys((u64)md->pfn), md->virtual);
+		pr_warn("BUG: mapping for 0x%08llx at 0x%08lx out of vmalloc space\n",
+			(long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 	}
 
 	type = &mem_types[md->type];
@@ -881,9 +876,8 @@ static void __init create_mapping(struct map_desc *md)
 	length = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
 
 	if (type->prot_l1 == 0 && ((addr | phys | length) & ~SECTION_MASK)) {
-		printk(KERN_WARNING "BUG: map for 0x%08llx at 0x%08lx can not "
-		       "be mapped using pages, ignoring.\n",
-		       (long long)__pfn_to_phys(md->pfn), addr);
+		pr_warn("BUG: map for 0x%08llx at 0x%08lx can not be mapped using pages, ignoring.\n",
+			(long long)__pfn_to_phys(md->pfn), addr);
 		return;
 	}
 
@@ -1053,15 +1047,13 @@ static int __init early_vmalloc(char *arg)
 
 	if (vmalloc_reserve < SZ_16M) {
 		vmalloc_reserve = SZ_16M;
-		printk(KERN_WARNING
-			"vmalloc area too small, limiting to %luMB\n",
+		pr_warn("vmalloc area too small, limiting to %luMB\n",
 			vmalloc_reserve >> 20);
 	}
 
 	if (vmalloc_reserve > VMALLOC_END - (PAGE_OFFSET + SZ_32M)) {
 		vmalloc_reserve = VMALLOC_END - (PAGE_OFFSET + SZ_32M);
-		printk(KERN_WARNING
-			"vmalloc area is too big, limiting to %luMB\n",
+		pr_warn("vmalloc area is too big, limiting to %luMB\n",
 			vmalloc_reserve >> 20);
 	}
 
@@ -1094,7 +1086,7 @@ void __init sanity_check_meminfo(void)
 
 			if (highmem) {
 				pr_notice("Ignoring RAM at %pa-%pa (!CONFIG_HIGHMEM)\n",
-					&block_start, &block_end);
+					  &block_start, &block_end);
 				memblock_remove(reg->base, reg->size);
 				continue;
 			}
@@ -1103,7 +1095,7 @@ void __init sanity_check_meminfo(void)
 				phys_addr_t overlap_size = reg->size - size_limit;
 
 				pr_notice("Truncating RAM at %pa-%pa to -%pa",
-				      &block_start, &block_end, &vmalloc_limit);
+					  &block_start, &block_end, &vmalloc_limit);
 				memblock_remove(vmalloc_limit, overlap_size);
 				block_end = vmalloc_limit;
 			}

commit 06e944b8e5fc4bec83f102f98c1ee4f972f5f072
Merge: f114040e3ea6 80d6b0c2eed2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Nov 3 10:12:13 2014 +0000

    Merge tag 'ronx-next' of git://git.kernel.org/pub/scm/linux/kernel/git/kees/linux into devel-stable
    
    generic fixmaps
    ARM support for CONFIG_DEBUG_RODATA

commit 1e6b48116a95046ec51f3d40f83aff8b006674d7
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Apr 3 17:28:11 2014 -0700

    ARM: mm: allow non-text sections to be non-executable
    
    Adds CONFIG_ARM_KERNMEM_PERMS to separate the kernel memory regions
    into section-sized areas that can have different permisions. Performs
    the NX permission changes during free_initmem, so that init memory can be
    reclaimed.
    
    This uses section size instead of PMD size to reduce memory lost to
    padding on non-LPAE systems.
    
    Based on work by Brad Spengler, Larry Bassel, and Laura Abbott.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index bdf5c94f7c36..1c52c8e94372 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1373,12 +1373,19 @@ static void __init map_lowmem(void)
 		if (start >= end)
 			break;
 
-		if (end < kernel_x_start || start >= kernel_x_end) {
+		if (end < kernel_x_start) {
 			map.pfn = __phys_to_pfn(start);
 			map.virtual = __phys_to_virt(start);
 			map.length = end - start;
 			map.type = MT_MEMORY_RWX;
 
+			create_mapping(&map);
+		} else if (start >= kernel_x_end) {
+			map.pfn = __phys_to_pfn(start);
+			map.virtual = __phys_to_virt(start);
+			map.length = end - start;
+			map.type = MT_MEMORY_RW;
+
 			create_mapping(&map);
 		} else {
 			/* This better cover the entire kernel */

commit 99b4ac9afce4129323b5b4c7002a942a9489914c
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Apr 4 23:27:49 2014 +0200

    arm: fixmap: implement __set_fixmap()
    
    This is used from set_fixmap() and clear_fixmap() via asm-generic/fixmap.h.
    Also makes sure that the fixmap allocation fits into the expected range.
    
    Based on patch by Rabin Vincent.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Cc: Rabin Vincent <rabin@rab.in>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 7fa0966cd15f..bdf5c94f7c36 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -22,6 +22,7 @@
 #include <asm/cputype.h>
 #include <asm/sections.h>
 #include <asm/cachetype.h>
+#include <asm/fixmap.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/smp_plat.h>
@@ -392,6 +393,29 @@ SET_MEMORY_FN(rw, pte_set_rw)
 SET_MEMORY_FN(x, pte_set_x)
 SET_MEMORY_FN(nx, pte_set_nx)
 
+/*
+ * To avoid TLB flush broadcasts, this uses local_flush_tlb_kernel_range().
+ * As a result, this can only be called with preemption disabled, as under
+ * stop_machine().
+ */
+void __set_fixmap(enum fixed_addresses idx, phys_addr_t phys, pgprot_t prot)
+{
+	unsigned long vaddr = __fix_to_virt(idx);
+	pte_t *pte = pte_offset_kernel(pmd_off_k(vaddr), vaddr);
+
+	/* Make sure fixmap region does not exceed available allocation. */
+	BUILD_BUG_ON(FIXADDR_START + (__end_of_fixed_addresses * PAGE_SIZE) >
+		     FIXADDR_END);
+	BUG_ON(idx >= __end_of_fixed_addresses);
+
+	if (pgprot_val(prot))
+		set_pte_at(NULL, vaddr, pte,
+			pfn_pte(phys >> PAGE_SHIFT, prot));
+	else
+		pte_clear(NULL, vaddr, pte);
+	local_flush_tlb_kernel_range(vaddr, vaddr + PAGE_SIZE);
+}
+
 /*
  * Adjust the PMD section entries according to the CPU in use.
  */

commit 836a24183273e9db1c092246bd8e306b297d9917
Author: Rob Herring <robh@kernel.org>
Date:   Wed Jul 2 02:01:15 2014 -0500

    ARM: expand fixmap region to 3MB
    
    With commit a05e54c103b0 ("ARM: 8031/2: change fixmap mapping region to
    support 32 CPUs"), the fixmap region was expanded to 2MB, but it
    precluded any other uses of the fixmap region. In order to support other
    uses the fixmap region needs to be expanded beyond 2MB. Fortunately, the
    adjacent 1MB range 0xffe00000-0xfff00000 is availabe.
    
    Remove fixmap_page_table ptr and lookup the page table via the virtual
    address so that the fixmap region can span more that one pmd. The 2nd
    pmd is already created since it is shared with the vector page.
    
    Signed-off-by: Rob Herring <robh@kernel.org>
    [kees: fixed CONFIG_DEBUG_HIGHMEM get_fixmap() calls]
    [kees: moved pte allocation outside of CONFIG_HIGHMEM]
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 8348ed6b2efe..7fa0966cd15f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1326,10 +1326,10 @@ static void __init kmap_init(void)
 #ifdef CONFIG_HIGHMEM
 	pkmap_page_table = early_pte_alloc(pmd_off_k(PKMAP_BASE),
 		PKMAP_BASE, _PAGE_KERNEL_TABLE);
-
-	fixmap_page_table = early_pte_alloc(pmd_off_k(FIXADDR_START),
-		FIXADDR_START, _PAGE_KERNEL_TABLE);
 #endif
+
+	early_pte_alloc(pmd_off_k(FIXADDR_START), FIXADDR_START,
+			_PAGE_KERNEL_TABLE);
 }
 
 static void __init map_lowmem(void)

commit 8b521cb2947d8811b4cf7fc6a7a6ebde35218243
Author: Joe Perches <joe@perches.com>
Date:   Tue Sep 16 20:41:43 2014 +0100

    ARM: 8152/1: Convert pr_warning to pr_warn
    
    Use the more common pr_warn.
    
    Other miscellanea:
    
    o Coalesce formats
    o Realign arguments
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 8348ed6b2efe..9f98cec7fe1e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -223,13 +223,13 @@ early_param("ecc", early_ecc);
 
 static int __init early_cachepolicy(char *p)
 {
-	pr_warning("cachepolicy kernel parameter not supported without cp15\n");
+	pr_warn("cachepolicy kernel parameter not supported without cp15\n");
 }
 early_param("cachepolicy", early_cachepolicy);
 
 static int __init noalign_setup(char *__unused)
 {
-	pr_warning("noalign kernel parameter not supported without cp15\n");
+	pr_warn("noalign kernel parameter not supported without cp15\n");
 }
 __setup("noalign", noalign_setup);
 

commit 3bb70de692f70861f5c5729cd2b870d0104a7cc9
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jul 29 09:27:13 2014 +0100

    ARM: add comments to the early page table remap code
    
    Add further comments to the early page table remap code to explain what
    the code is doing, why it is doing it, but more importantly to explain
    that the code is not architecturally compliant and is squarely in
    "UNPREDICTABLE" behaviour territory.
    
    Add a warning and tainting of the kernel too.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ab14b79b03f0..8348ed6b2efe 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1406,8 +1406,8 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 		return;
 
 	/* remap kernel code and data */
-	map_start = init_mm.start_code;
-	map_end   = init_mm.brk;
+	map_start = init_mm.start_code & PMD_MASK;
+	map_end   = ALIGN(init_mm.brk, PMD_SIZE);
 
 	/* get a handle on things... */
 	pgd0 = pgd_offset_k(0);
@@ -1434,23 +1434,64 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 	dsb(ishst);
 	isb();
 
-	/* remap level 1 table */
+	/*
+	 * FIXME: This code is not architecturally compliant: we modify
+	 * the mappings in-place, indeed while they are in use by this
+	 * very same code.  This may lead to unpredictable behaviour of
+	 * the CPU.
+	 *
+	 * Even modifying the mappings in a separate page table does
+	 * not resolve this.
+	 *
+	 * The architecture strongly recommends that when a mapping is
+	 * changed, that it is changed by first going via an invalid
+	 * mapping and back to the new mapping.  This is to ensure that
+	 * no TLB conflicts (caused by the TLB having more than one TLB
+	 * entry match a translation) can occur.  However, doing that
+	 * here will result in unmapping the code we are running.
+	 */
+	pr_warn("WARNING: unsafe modification of in-place page tables - tainting kernel\n");
+	add_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_STILL_OK);
+
+	/*
+	 * Remap level 1 table.  This changes the physical addresses
+	 * used to refer to the level 2 page tables to the high
+	 * physical address alias, leaving everything else the same.
+	 */
 	for (i = 0; i < PTRS_PER_PGD; pud0++, i++) {
 		set_pud(pud0,
 			__pud(__pa(pmd0) | PMD_TYPE_TABLE | L_PGD_SWAPPER));
 		pmd0 += PTRS_PER_PMD;
 	}
 
-	/* remap pmds for kernel mapping */
-	phys = __pa(map_start) & PMD_MASK;
+	/*
+	 * Remap the level 2 table, pointing the mappings at the high
+	 * physical address alias of these pages.
+	 */
+	phys = __pa(map_start);
 	do {
 		*pmdk++ = __pmd(phys | pmdprot);
 		phys += PMD_SIZE;
 	} while (phys < map_end);
 
+	/*
+	 * Ensure that the above updates are flushed out of the cache.
+	 * This is not strictly correct; on a system where the caches
+	 * are coherent with each other, but the MMU page table walks
+	 * may not be coherent, flush_cache_all() may be a no-op, and
+	 * this will fail.
+	 */
 	flush_cache_all();
+
+	/*
+	 * Re-write the TTBR values to point them at the high physical
+	 * alias of the page tables.  We expect __va() will work on
+	 * cpu_get_pgd(), which returns the value of TTBR0.
+	 */
 	cpu_switch_mm(pgd0, &init_mm);
 	cpu_set_ttbr(1, __pa(pgd0) + TTBR1_OFFSET);
+
+	/* Finally flush any stale TLB values. */
 	local_flush_bp_all();
 	local_flush_tlb_all();
 }

commit 1fb333489fb917c704ad43e51b45c12f52215a9c
Merge: 20e7e364331d 3f8517e7937d 8ef418c7178f 1c2f87c22566
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jun 5 12:35:52 2014 +0100

    Merge branches 'alignment', 'fixes', 'l2c' (early part) and 'misc' into for-next

commit 20e7e364331d7b5590695a839a18a00547403f18
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jun 2 09:29:37 2014 +0100

    ARM: ensure C page table setup code follows assembly code (part II)
    
    This does the same as the previous commit, but for the S bit, which also
    needs to match the initial value which the assembly code used for the
    same reasons.  Again, we add a check for SMP to ensure that the page
    tables are correctly setup for SMP.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 92df149c88a8..df875c457068 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -117,6 +117,8 @@ static struct cachepolicy cache_policies[] __initdata = {
 };
 
 #ifdef CONFIG_CPU_CP15
+static unsigned long initial_pmd_value __initdata = 0;
+
 /*
  * Initialise the cache_policy variable with the initial state specified
  * via the "pmd" value.  This is used to ensure that on ARMv6 and later,
@@ -128,6 +130,8 @@ void __init init_default_cache_policy(unsigned long pmd)
 {
 	int i;
 
+	initial_pmd_value = pmd;
+
 	pmd &= PMD_SECT_TEX(1) | PMD_SECT_BUFFERABLE | PMD_SECT_CACHEABLE;
 
 	for (i = 0; i < ARRAY_SIZE(cache_policies); i++)
@@ -414,9 +418,15 @@ static void __init build_mem_type_table(void)
 		ecc_mask = 0;
 	}
 
-	if (is_smp() && cachepolicy != CPOLICY_WRITEALLOC) {
-		pr_warn("Forcing write-allocate cache policy for SMP\n");
-		cachepolicy = CPOLICY_WRITEALLOC;
+	if (is_smp()) {
+		if (cachepolicy != CPOLICY_WRITEALLOC) {
+			pr_warn("Forcing write-allocate cache policy for SMP\n");
+			cachepolicy = CPOLICY_WRITEALLOC;
+		}
+		if (!(initial_pmd_value & PMD_SECT_S)) {
+			pr_warn("Forcing shared mappings for SMP\n");
+			initial_pmd_value |= PMD_SECT_S;
+		}
 	}
 
 	/*
@@ -541,11 +551,12 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 #endif
 
-		if (is_smp()) {
-			/*
-			 * Mark memory with the "shared" attribute
-			 * for SMP systems
-			 */
+		/*
+		 * If the initial page tables were created with the S bit
+		 * set, then we need to do the same here for the same
+		 * reasons given in early_cachepolicy().
+		 */
+		if (initial_pmd_value & PMD_SECT_S) {
 			user_pgprot |= L_PTE_SHARED;
 			kern_pgprot |= L_PTE_SHARED;
 			vecs_pgprot |= L_PTE_SHARED;

commit ca8f0b0a545f55b3dc6877cda24d609a8979c951
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue May 27 20:34:28 2014 +0100

    ARM: ensure C page table setup code follows assembly code
    
    Fix a long standing bug where, for ARMv6+, we don't fully ensure that
    the C code sets the same cache policy as the assembly code.  This was
    introduced partially by commit 11179d8ca28d ([ARM] 4497/1: Only allow
    safe cache configurations on ARMv6 and later) and also by adding SMP
    support.
    
    This patch sets the default cache policy based on the flags used by the
    assembly code, and then ensures that when a cache policy command line
    argument is used, we verify that on ARMv6, it matches the initial setup.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9c8fec02c274..92df149c88a8 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -118,27 +118,49 @@ static struct cachepolicy cache_policies[] __initdata = {
 
 #ifdef CONFIG_CPU_CP15
 /*
- * These are useful for identifying cache coherency
- * problems by allowing the cache or the cache and
- * writebuffer to be turned off.  (Note: the write
- * buffer should not be on and the cache off).
+ * Initialise the cache_policy variable with the initial state specified
+ * via the "pmd" value.  This is used to ensure that on ARMv6 and later,
+ * the C code sets the page tables up with the same policy as the head
+ * assembly code, which avoids an illegal state where the TLBs can get
+ * confused.  See comments in early_cachepolicy() for more information.
  */
-static int __init early_cachepolicy(char *p)
+void __init init_default_cache_policy(unsigned long pmd)
 {
-	unsigned long cr = get_cr();
 	int i;
 
+	pmd &= PMD_SECT_TEX(1) | PMD_SECT_BUFFERABLE | PMD_SECT_CACHEABLE;
+
+	for (i = 0; i < ARRAY_SIZE(cache_policies); i++)
+		if (cache_policies[i].pmd == pmd) {
+			cachepolicy = i;
+			break;
+		}
+
+	if (i == ARRAY_SIZE(cache_policies))
+		pr_err("ERROR: could not find cache policy\n");
+}
+
+/*
+ * These are useful for identifying cache coherency problems by allowing
+ * the cache or the cache and writebuffer to be turned off.  (Note: the
+ * write buffer should not be on and the cache off).
+ */
+static int __init early_cachepolicy(char *p)
+{
+	int i, selected = -1;
+
 	for (i = 0; i < ARRAY_SIZE(cache_policies); i++) {
 		int len = strlen(cache_policies[i].policy);
 
 		if (memcmp(p, cache_policies[i].policy, len) == 0) {
-			cachepolicy = i;
-			cr = __clear_cr(cache_policies[i].cr_mask);
+			selected = i;
 			break;
 		}
 	}
-	if (i == ARRAY_SIZE(cache_policies))
-		printk(KERN_ERR "ERROR: unknown or unsupported cache policy\n");
+
+	if (selected == -1)
+		pr_err("ERROR: unknown or unsupported cache policy\n");
+
 	/*
 	 * This restriction is partly to do with the way we boot; it is
 	 * unpredictable to have memory mapped using two different sets of
@@ -146,12 +168,18 @@ static int __init early_cachepolicy(char *p)
 	 * change these attributes once the initial assembly has setup the
 	 * page tables.
 	 */
-	if (cpu_architecture() >= CPU_ARCH_ARMv6) {
-		printk(KERN_WARNING "Only cachepolicy=writeback supported on ARMv6 and later\n");
-		cachepolicy = CPOLICY_WRITEBACK;
+	if (cpu_architecture() >= CPU_ARCH_ARMv6 && selected != cachepolicy) {
+		pr_warn("Only cachepolicy=%s supported on ARMv6 and later\n",
+			cache_policies[cachepolicy].policy);
+		return 0;
+	}
+
+	if (selected != cachepolicy) {
+		unsigned long cr = __clear_cr(cache_policies[selected].cr_mask);
+		cachepolicy = selected;
+		flush_cache_all();
+		set_cr(cr);
 	}
-	flush_cache_all();
-	set_cr(cr);
 	return 0;
 }
 early_param("cachepolicy", early_cachepolicy);
@@ -385,8 +413,11 @@ static void __init build_mem_type_table(void)
 			cachepolicy = CPOLICY_WRITEBACK;
 		ecc_mask = 0;
 	}
-	if (is_smp())
+
+	if (is_smp() && cachepolicy != CPOLICY_WRITEALLOC) {
+		pr_warn("Forcing write-allocate cache policy for SMP\n");
 		cachepolicy = CPOLICY_WRITEALLOC;
+	}
 
 	/*
 	 * Strip out features not present on earlier architectures.

commit c6e13600d3b2137ff583767d7f8688a41e34ac49
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Apr 13 19:01:14 2014 +0100

    ARM: remove unused adjust_cr() function
    
    adjust_cr() is not used anymore, so let's get rid of it.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ec57f792aed4..9c8fec02c274 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -186,26 +186,6 @@ static int __init early_ecc(char *p)
 early_param("ecc", early_ecc);
 #endif
 
-#ifndef CONFIG_SMP
-void adjust_cr(unsigned long mask, unsigned long set)
-{
-	unsigned long flags;
-
-	mask &= ~CR_A;
-
-	set &= mask;
-
-	local_irq_save(flags);
-
-	cr_no_alignment = (cr_no_alignment & ~mask) | set;
-	cr_alignment = (cr_alignment & ~mask) | set;
-
-	set_cr((get_cr() & ~mask) | set);
-
-	local_irq_restore(flags);
-}
-#endif
-
 #else /* ifdef CONFIG_CPU_CP15 */
 
 static int __init early_cachepolicy(char *p)

commit 175352a51cfc275b8590d27be6fdd4b14f416c32
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Apr 13 19:00:17 2014 +0100

    ARM: move "noalign" command line option to alignment.c
    
    Keep all bits of alignment handling together.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d97cb2d8953a..ec57f792aed4 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -186,13 +186,6 @@ static int __init early_ecc(char *p)
 early_param("ecc", early_ecc);
 #endif
 
-static int __init noalign_setup(char *__unused)
-{
-	set_cr(__clear_cr(CR_A));
-	return 1;
-}
-__setup("noalign", noalign_setup);
-
 #ifndef CONFIG_SMP
 void adjust_cr(unsigned long mask, unsigned long set)
 {

commit b4b20ad881f5a5c19ae9199547ddbb00fa4825eb
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Apr 13 18:57:29 2014 +0100

    ARM: provide common method to clear bits in CPU control register
    
    Several places open-code this manipulation, let's consolidate this.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b68c6b22e1c8..d97cb2d8953a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -125,6 +125,7 @@ static struct cachepolicy cache_policies[] __initdata = {
  */
 static int __init early_cachepolicy(char *p)
 {
+	unsigned long cr = get_cr();
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(cache_policies); i++) {
@@ -132,8 +133,7 @@ static int __init early_cachepolicy(char *p)
 
 		if (memcmp(p, cache_policies[i].policy, len) == 0) {
 			cachepolicy = i;
-			cr_alignment &= ~cache_policies[i].cr_mask;
-			cr_no_alignment &= ~cache_policies[i].cr_mask;
+			cr = __clear_cr(cache_policies[i].cr_mask);
 			break;
 		}
 	}
@@ -151,7 +151,7 @@ static int __init early_cachepolicy(char *p)
 		cachepolicy = CPOLICY_WRITEBACK;
 	}
 	flush_cache_all();
-	set_cr(cr_alignment);
+	set_cr(cr);
 	return 0;
 }
 early_param("cachepolicy", early_cachepolicy);
@@ -188,9 +188,7 @@ early_param("ecc", early_ecc);
 
 static int __init noalign_setup(char *__unused)
 {
-	cr_alignment &= ~CR_A;
-	cr_no_alignment &= ~CR_A;
-	set_cr(cr_alignment);
+	set_cr(__clear_cr(CR_A));
 	return 1;
 }
 __setup("noalign", noalign_setup);

commit 1c2f87c22566cd057bc8cde10c37ae9da1a1bb76
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Sun Apr 13 22:54:58 2014 +0100

    ARM: 8025/1: Get rid of meminfo
    
    memblock is now fully integrated into the kernel and is the prefered
    method for tracking memory. Rather than reinvent the wheel with
    meminfo, migrate to using memblock directly instead of meminfo as
    an intermediate.
    
    Acked-by: Jason Cooper <jason@lakedaemon.net>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Tested-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Leif Lindholm <leif.lindholm@linaro.org>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index a991ce2f18d4..82ea2b3fb9b5 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1062,74 +1062,47 @@ phys_addr_t arm_lowmem_limit __initdata = 0;
 void __init sanity_check_meminfo(void)
 {
 	phys_addr_t memblock_limit = 0;
-	int i, j, highmem = 0;
+	int highmem = 0;
 	phys_addr_t vmalloc_limit = __pa(vmalloc_min - 1) + 1;
+	struct memblock_region *reg;
 
-	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
-		struct membank *bank = &meminfo.bank[j];
-		phys_addr_t size_limit;
-
-		*bank = meminfo.bank[i];
-		size_limit = bank->size;
+	for_each_memblock(memory, reg) {
+		phys_addr_t block_start = reg->base;
+		phys_addr_t block_end = reg->base + reg->size;
+		phys_addr_t size_limit = reg->size;
 
-		if (bank->start >= vmalloc_limit)
+		if (reg->base >= vmalloc_limit)
 			highmem = 1;
 		else
-			size_limit = vmalloc_limit - bank->start;
+			size_limit = vmalloc_limit - reg->base;
 
-		bank->highmem = highmem;
 
-#ifdef CONFIG_HIGHMEM
-		/*
-		 * Split those memory banks which are partially overlapping
-		 * the vmalloc area greatly simplifying things later.
-		 */
-		if (!highmem && bank->size > size_limit) {
-			if (meminfo.nr_banks >= NR_BANKS) {
-				printk(KERN_CRIT "NR_BANKS too low, "
-						 "ignoring high memory\n");
-			} else {
-				memmove(bank + 1, bank,
-					(meminfo.nr_banks - i) * sizeof(*bank));
-				meminfo.nr_banks++;
-				i++;
-				bank[1].size -= size_limit;
-				bank[1].start = vmalloc_limit;
-				bank[1].highmem = highmem = 1;
-				j++;
+		if (!IS_ENABLED(CONFIG_HIGHMEM) || cache_is_vipt_aliasing()) {
+
+			if (highmem) {
+				pr_notice("Ignoring RAM at %pa-%pa (!CONFIG_HIGHMEM)\n",
+					&block_start, &block_end);
+				memblock_remove(reg->base, reg->size);
+				continue;
 			}
-			bank->size = size_limit;
-		}
-#else
-		/*
-		 * Highmem banks not allowed with !CONFIG_HIGHMEM.
-		 */
-		if (highmem) {
-			printk(KERN_NOTICE "Ignoring RAM at %.8llx-%.8llx "
-			       "(!CONFIG_HIGHMEM).\n",
-			       (unsigned long long)bank->start,
-			       (unsigned long long)bank->start + bank->size - 1);
-			continue;
-		}
 
-		/*
-		 * Check whether this memory bank would partially overlap
-		 * the vmalloc area.
-		 */
-		if (bank->size > size_limit) {
-			printk(KERN_NOTICE "Truncating RAM at %.8llx-%.8llx "
-			       "to -%.8llx (vmalloc region overlap).\n",
-			       (unsigned long long)bank->start,
-			       (unsigned long long)bank->start + bank->size - 1,
-			       (unsigned long long)bank->start + size_limit - 1);
-			bank->size = size_limit;
+			if (reg->size > size_limit) {
+				phys_addr_t overlap_size = reg->size - size_limit;
+
+				pr_notice("Truncating RAM at %pa-%pa to -%pa",
+				      &block_start, &block_end, &vmalloc_limit);
+				memblock_remove(vmalloc_limit, overlap_size);
+				block_end = vmalloc_limit;
+			}
 		}
-#endif
-		if (!bank->highmem) {
-			phys_addr_t bank_end = bank->start + bank->size;
 
-			if (bank_end > arm_lowmem_limit)
-				arm_lowmem_limit = bank_end;
+		if (!highmem) {
+			if (block_end > arm_lowmem_limit) {
+				if (reg->size > size_limit)
+					arm_lowmem_limit = vmalloc_limit;
+				else
+					arm_lowmem_limit = block_end;
+			}
 
 			/*
 			 * Find the first non-section-aligned page, and point
@@ -1145,35 +1118,15 @@ void __init sanity_check_meminfo(void)
 			 * occurs before any free memory is mapped.
 			 */
 			if (!memblock_limit) {
-				if (!IS_ALIGNED(bank->start, SECTION_SIZE))
-					memblock_limit = bank->start;
-				else if (!IS_ALIGNED(bank_end, SECTION_SIZE))
-					memblock_limit = bank_end;
+				if (!IS_ALIGNED(block_start, SECTION_SIZE))
+					memblock_limit = block_start;
+				else if (!IS_ALIGNED(block_end, SECTION_SIZE))
+					memblock_limit = arm_lowmem_limit;
 			}
-		}
-		j++;
-	}
-#ifdef CONFIG_HIGHMEM
-	if (highmem) {
-		const char *reason = NULL;
 
-		if (cache_is_vipt_aliasing()) {
-			/*
-			 * Interactions between kmap and other mappings
-			 * make highmem support with aliasing VIPT caches
-			 * rather difficult.
-			 */
-			reason = "with VIPT aliasing cache";
-		}
-		if (reason) {
-			printk(KERN_CRIT "HIGHMEM is not supported %s, ignoring high memory\n",
-				reason);
-			while (j > 0 && meminfo.bank[j - 1].highmem)
-				j--;
 		}
 	}
-#endif
-	meminfo.nr_banks = j;
+
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
 
 	/*

commit 9581960a40ab0e281b695bf03744c8924ec3b5d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 9 18:36:27 2014 +0100

    ARM: 8055/1: cacheflush: use -st dsb option for ensuring completion
    
    dsb st can be used to ensure completion of pending cache maintenance
    operations, so use it for the v7 cache maintenance operations.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 09c0a16165dc..a991ce2f18d4 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1465,7 +1465,7 @@ void __init early_paging_init(const struct machine_desc *mdesc,
 	 * just complicate the code.
 	 */
 	flush_cache_louis();
-	dsb();
+	dsb(ishst);
 	isb();
 
 	/* remap level 1 table */

commit a05e54c103b0b8e1dab5d04b411f1d48387c4903
Author: Liu Hua <sdu.liu@huawei.com>
Date:   Fri Apr 18 09:43:32 2014 +0100

    ARM: 8031/2: change fixmap mapping region to support 32 CPUs
    
    In 32-bit ARM systems, the fixmap mapping region can support no more
    than 14 CPUs(total: 896k; one CPU: 64K). And we can configure NR_CPUS
    up to 32. So there is a mismatch.
    
    This patch moves fixmapping region downwards to region 0xffc00000-
    0xffe00000. Then the fixmap mapping region can support up to 32 CPUs.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Liu Hua <sdu.liu@huawei.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b68c6b22e1c8..09c0a16165dc 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -35,6 +35,7 @@
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
 #include <asm/mach/pci.h>
+#include <asm/fixmap.h>
 
 #include "mm.h"
 #include "tcm.h"
@@ -1359,6 +1360,9 @@ static void __init kmap_init(void)
 #ifdef CONFIG_HIGHMEM
 	pkmap_page_table = early_pte_alloc(pmd_off_k(PKMAP_BASE),
 		PKMAP_BASE, _PAGE_KERNEL_TABLE);
+
+	fixmap_page_table = early_pte_alloc(pmd_off_k(FIXADDR_START),
+		FIXADDR_START, _PAGE_KERNEL_TABLE);
 #endif
 }
 

commit 95959e6a06720834fc80a210e37898341c63cb91
Merge: f210c53a82ab 95c52fe06335 e26a9e00afc4 dfdf5f63b438 aa4c5b962a7a 1ef2bf8227ab
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 4 00:33:32 2014 +0100

    Merge branches 'amba', 'fixes', 'misc', 'mmci', 'unstable/omap-dma' and 'unstable/sa11x0' into for-next

commit b6ccb9803e90c16b212cf4ed62913a7591e79a39
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 7 19:12:27 2014 +0100

    ARM: 7954/1: mm: remove remaining domain support from ARMv6
    
    CPU_32v6 currently selects CPU_USE_DOMAINS if CPU_V6 and MMU. This is
    because ARM 1136 r0pX CPUs lack the v6k extensions, and therefore do
    not have hardware thread registers. The lack of these registers requires
    the kernel to update the vectors page at each context switch in order to
    write a new TLS pointer. This write must be done via the userspace
    mapping, since aliasing caches can lead to expensive flushing when using
    kmap. Finally, this requires the vectors page to be mapped r/w for
    kernel and r/o for user, which has implications for things like put_user
    which must trigger CoW appropriately when targetting user pages.
    
    The upshot of all this is that a v6/v7 kernel makes use of domains to
    segregate kernel and user memory accesses. This has the nasty
    side-effect of making device mappings executable, which has been
    observed to cause subtle bugs on recent cores (e.g. Cortex-A15
    performing a speculative instruction fetch from the GIC and acking an
    interrupt in the process).
    
    This patch solves this problem by removing the remaining domain support
    from ARMv6. A new memory type is added specifically for the vectors page
    which allows that page (and only that page) to be mapped as user r/o,
    kernel r/w. All other user r/o pages are mapped also as kernel r/o.
    Patch co-developed with Russell King.
    
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4f08c133cc25..6ec07a84f759 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -510,6 +510,16 @@ static void __init build_mem_type_table(void)
 	s2_pgprot = cp->pte_s2;
 	hyp_device_pgprot = s2_device_pgprot = mem_types[MT_DEVICE].prot_pte;
 
+	/*
+	 * We don't use domains on ARMv6 (since this causes problems with
+	 * v6/v7 kernels), so we must use a separate memory type for user
+	 * r/o, kernel r/w to map the vectors page.
+	 */
+#ifndef CONFIG_ARM_LPAE
+	if (cpu_arch == CPU_ARCH_ARMv6)
+		vecs_pgprot |= L_PTE_MT_VECTORS;
+#endif
+
 	/*
 	 * ARMv6 and above have extended page tables.
 	 */

commit 4d9c5b89cf3605bbc39c6e274351ff25f0d83e6a
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Sun Feb 2 22:21:31 2014 +0100

    ARM: 7950/1: mm: Fix stage-2 device memory attributes
    
    The stage-2 memory attributes are distinct from the Hyp memory
    attributes and the Stage-1 memory attributes.  We were using the stage-1
    memory attributes for stage-2 mappings causing device mappings to be
    mapped as normal memory.  Add the S2 equivalent defines for memory
    attributes and fix the comments explaining the defines while at it.
    
    Add a prot_pte_s2 field to the mem_type struct and fill out the field
    for device mappings accordingly.
    
    Cc: <stable@vger.kernel.org>    [3.9+]
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4f08c133cc25..a623cb3ad012 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -232,12 +232,16 @@ __setup("noalign", noalign_setup);
 #endif /* ifdef CONFIG_CPU_CP15 / else */
 
 #define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_XN
+#define PROT_PTE_S2_DEVICE	PROT_PTE_DEVICE
 #define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_AP_WRITE
 
 static struct mem_type mem_types[] = {
 	[MT_DEVICE] = {		  /* Strongly ordered / ARMv6 shared device */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_SHARED |
 				  L_PTE_SHARED,
+		.prot_pte_s2	= s2_policy(PROT_PTE_S2_DEVICE) |
+				  s2_policy(L_PTE_S2_MT_DEV_SHARED) |
+				  L_PTE_SHARED,
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_S,
 		.domain		= DOMAIN_IO,
@@ -508,7 +512,8 @@ static void __init build_mem_type_table(void)
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
 	s2_pgprot = cp->pte_s2;
-	hyp_device_pgprot = s2_device_pgprot = mem_types[MT_DEVICE].prot_pte;
+	hyp_device_pgprot = mem_types[MT_DEVICE].prot_pte;
+	s2_device_pgprot = mem_types[MT_DEVICE].prot_pte_s2;
 
 	/*
 	 * ARMv6 and above have extended page tables.

commit 71b55663c5d0a6f8473045db1c8b376803780191
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Nov 25 12:01:03 2013 +0000

    ARM: fix executability of CMA mappings
    
    The CMA region was being marked executable:
    
    0xdc04e000-0xdc050000           8K     RW x      MEM/CACHED/WBRA
    0xdc060000-0xdc100000         640K     RW x      MEM/CACHED/WBRA
    0xdc4f5000-0xdc500000          44K     RW x      MEM/CACHED/WBRA
    0xdcce9000-0xe0000000       52316K     RW x      MEM/CACHED/WBRA
    
    This is mainly due to the badly worded MT_MEMORY_DMA_READY symbol, but
    there are also a few other places in dma-mapping which should be
    corrected to use the right constant.  Fix all these places:
    
    0xdc04e000-0xdc050000           8K     RW NX     MEM/CACHED/WBRA
    0xdc060000-0xdc100000         640K     RW NX     MEM/CACHED/WBRA
    0xdc280000-0xdc300000         512K     RW NX     MEM/CACHED/WBRA
    0xdc6fc000-0xe0000000       58384K     RW NX     MEM/CACHED/WBRA
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 09033890c8b0..4f08c133cc25 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -333,7 +333,8 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_DMA_READY] = {
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_XN,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_KERNEL,
 	},

commit 75374ad47c6465de6c098d7b913fe02d6131195c
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Jun 17 10:29:13 2013 -0700

    ARM: mm: Define set_memory_* functions for ARM
    
    Other architectures define various set_memory functions to allow
    attributes to be changed (e.g. set_memory_x, set_memory_rw, etc.)
    Currently, these functions are missing on ARM. Define these in an
    appropriate manner for ARM.
    
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9ec715f12224..09033890c8b0 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -345,6 +345,44 @@ const struct mem_type *get_mem_type(unsigned int type)
 }
 EXPORT_SYMBOL(get_mem_type);
 
+#define PTE_SET_FN(_name, pteop) \
+static int pte_set_##_name(pte_t *ptep, pgtable_t token, unsigned long addr, \
+			void *data) \
+{ \
+	pte_t pte = pteop(*ptep); \
+\
+	set_pte_ext(ptep, pte, 0); \
+	return 0; \
+} \
+
+#define SET_MEMORY_FN(_name, callback) \
+int set_memory_##_name(unsigned long addr, int numpages) \
+{ \
+	unsigned long start = addr; \
+	unsigned long size = PAGE_SIZE*numpages; \
+	unsigned end = start + size; \
+\
+	if (start < MODULES_VADDR || start >= MODULES_END) \
+		return -EINVAL;\
+\
+	if (end < MODULES_VADDR || end >= MODULES_END) \
+		return -EINVAL; \
+\
+	apply_to_page_range(&init_mm, start, size, callback, NULL); \
+	flush_tlb_kernel_range(start, end); \
+	return 0;\
+}
+
+PTE_SET_FN(ro, pte_wrprotect)
+PTE_SET_FN(rw, pte_mkwrite)
+PTE_SET_FN(x, pte_mkexec)
+PTE_SET_FN(nx, pte_mknexec)
+
+SET_MEMORY_FN(ro, pte_set_ro)
+SET_MEMORY_FN(rw, pte_set_rw)
+SET_MEMORY_FN(x, pte_set_x)
+SET_MEMORY_FN(nx, pte_set_nx)
+
 /*
  * Adjust the PMD section entries according to the CPU in use.
  */

commit ebd4922ecc38e31e662cd641dd04099a9652c9b3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Oct 24 08:12:39 2013 +0100

    ARM: implement basic NX support for kernel lowmem mappings
    
    Add basic NX support for kernel lowmem mappings.  We mark any section
    which does not overlap kernel text as non-executable, preventing it
    from being used to write code and then execute directly from there.
    
    This does not change the alignment of the sections, so the kernel
    image doesn't grow significantly via this change, so we can do this
    without needing a config option.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index fce2e7388098..9ec715f12224 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -22,6 +22,7 @@
 #include <asm/cputype.h>
 #include <asm/sections.h>
 #include <asm/cachetype.h>
+#include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/smp_plat.h>
 #include <asm/tlb.h>
@@ -293,6 +294,13 @@ static struct mem_type mem_types[] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
+	[MT_MEMORY_RW] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+			     L_PTE_XN,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
+		.domain    = DOMAIN_KERNEL,
+	},
 	[MT_ROM] = {
 		.prot_sect = PMD_TYPE_SECT,
 		.domain    = DOMAIN_KERNEL,
@@ -410,6 +418,9 @@ static void __init build_mem_type_table(void)
 			mem_types[MT_DEVICE_NONSHARED].prot_sect |= PMD_SECT_XN;
 			mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_XN;
 			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_XN;
+
+			/* Also setup NX memory mapping */
+			mem_types[MT_MEMORY_RW].prot_sect |= PMD_SECT_XN;
 		}
 		if (cpu_arch >= CPU_ARCH_ARMv7 && (cr & CR_TRE)) {
 			/*
@@ -489,6 +500,8 @@ static void __init build_mem_type_table(void)
 			mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_MEMORY_RWX].prot_sect |= PMD_SECT_S;
 			mem_types[MT_MEMORY_RWX].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_MEMORY_RW].prot_sect |= PMD_SECT_S;
+			mem_types[MT_MEMORY_RW].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_MEMORY_DMA_READY].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |= PMD_SECT_S;
 			mem_types[MT_MEMORY_RWX_NONCACHED].prot_pte |= L_PTE_SHARED;
@@ -545,6 +558,8 @@ static void __init build_mem_type_table(void)
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_MEMORY_RWX].prot_sect |= ecc_mask | cp->pmd;
 	mem_types[MT_MEMORY_RWX].prot_pte |= kern_pgprot;
+	mem_types[MT_MEMORY_RW].prot_sect |= ecc_mask | cp->pmd;
+	mem_types[MT_MEMORY_RW].prot_pte |= kern_pgprot;
 	mem_types[MT_MEMORY_DMA_READY].prot_pte |= kern_pgprot;
 	mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |= ecc_mask;
 	mem_types[MT_ROM].prot_sect |= cp->pmd;
@@ -1296,6 +1311,8 @@ static void __init kmap_init(void)
 static void __init map_lowmem(void)
 {
 	struct memblock_region *reg;
+	unsigned long kernel_x_start = round_down(__pa(_stext), SECTION_SIZE);
+	unsigned long kernel_x_end = round_up(__pa(__init_end), SECTION_SIZE);
 
 	/* Map all the lowmem memory banks. */
 	for_each_memblock(memory, reg) {
@@ -1308,12 +1325,40 @@ static void __init map_lowmem(void)
 		if (start >= end)
 			break;
 
-		map.pfn = __phys_to_pfn(start);
-		map.virtual = __phys_to_virt(start);
-		map.length = end - start;
-		map.type = MT_MEMORY;
+		if (end < kernel_x_start || start >= kernel_x_end) {
+			map.pfn = __phys_to_pfn(start);
+			map.virtual = __phys_to_virt(start);
+			map.length = end - start;
+			map.type = MT_MEMORY_RWX;
 
-		create_mapping(&map);
+			create_mapping(&map);
+		} else {
+			/* This better cover the entire kernel */
+			if (start < kernel_x_start) {
+				map.pfn = __phys_to_pfn(start);
+				map.virtual = __phys_to_virt(start);
+				map.length = kernel_x_start - start;
+				map.type = MT_MEMORY_RW;
+
+				create_mapping(&map);
+			}
+
+			map.pfn = __phys_to_pfn(kernel_x_start);
+			map.virtual = __phys_to_virt(kernel_x_start);
+			map.length = kernel_x_end - kernel_x_start;
+			map.type = MT_MEMORY_RWX;
+
+			create_mapping(&map);
+
+			if (kernel_x_end < end) {
+				map.pfn = __phys_to_pfn(kernel_x_end);
+				map.virtual = __phys_to_virt(kernel_x_end);
+				map.length = end - kernel_x_end;
+				map.type = MT_MEMORY_RW;
+
+				create_mapping(&map);
+			}
+		}
 	}
 }
 

commit 2e2c9de207be043ee80161971c814d740759d3bc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Oct 24 10:26:40 2013 +0100

    ARM: add permission annotations to MT_MEMORY* mapping types
    
    Document the permissions which the various MT_MEMORY* mapping types
    will provide.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 580ef2de82d7..fce2e7388098 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -287,7 +287,7 @@ static struct mem_type mem_types[] = {
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_USER,
 	},
-	[MT_MEMORY] = {
+	[MT_MEMORY_RWX] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
@@ -297,26 +297,26 @@ static struct mem_type mem_types[] = {
 		.prot_sect = PMD_TYPE_SECT,
 		.domain    = DOMAIN_KERNEL,
 	},
-	[MT_MEMORY_NONCACHED] = {
+	[MT_MEMORY_RWX_NONCACHED] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_MT_BUFFERABLE,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
-	[MT_MEMORY_DTCM] = {
+	[MT_MEMORY_RW_DTCM] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_XN,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
 	},
-	[MT_MEMORY_ITCM] = {
+	[MT_MEMORY_RWX_ITCM] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_KERNEL,
 	},
-	[MT_MEMORY_SO] = {
+	[MT_MEMORY_RW_SO] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_MT_UNCACHED | L_PTE_XN,
 		.prot_l1   = PMD_TYPE_TABLE,
@@ -487,11 +487,11 @@ static void __init build_mem_type_table(void)
 			mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
 			mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
-			mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
-			mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_MEMORY_RWX].prot_sect |= PMD_SECT_S;
+			mem_types[MT_MEMORY_RWX].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_MEMORY_DMA_READY].prot_pte |= L_PTE_SHARED;
-			mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
-			mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |= PMD_SECT_S;
+			mem_types[MT_MEMORY_RWX_NONCACHED].prot_pte |= L_PTE_SHARED;
 		}
 	}
 
@@ -502,15 +502,15 @@ static void __init build_mem_type_table(void)
 	if (cpu_arch >= CPU_ARCH_ARMv6) {
 		if (cpu_arch >= CPU_ARCH_ARMv7 && (cr & CR_TRE)) {
 			/* Non-cacheable Normal is XCB = 001 */
-			mem_types[MT_MEMORY_NONCACHED].prot_sect |=
+			mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |=
 				PMD_SECT_BUFFERED;
 		} else {
 			/* For both ARMv6 and non-TEX-remapping ARMv7 */
-			mem_types[MT_MEMORY_NONCACHED].prot_sect |=
+			mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |=
 				PMD_SECT_TEX(1);
 		}
 	} else {
-		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_BUFFERABLE;
+		mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |= PMD_SECT_BUFFERABLE;
 	}
 
 #ifdef CONFIG_ARM_LPAE
@@ -543,10 +543,10 @@ static void __init build_mem_type_table(void)
 
 	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;
-	mem_types[MT_MEMORY].prot_sect |= ecc_mask | cp->pmd;
-	mem_types[MT_MEMORY].prot_pte |= kern_pgprot;
+	mem_types[MT_MEMORY_RWX].prot_sect |= ecc_mask | cp->pmd;
+	mem_types[MT_MEMORY_RWX].prot_pte |= kern_pgprot;
 	mem_types[MT_MEMORY_DMA_READY].prot_pte |= kern_pgprot;
-	mem_types[MT_MEMORY_NONCACHED].prot_sect |= ecc_mask;
+	mem_types[MT_MEMORY_RWX_NONCACHED].prot_sect |= ecc_mask;
 	mem_types[MT_ROM].prot_sect |= cp->pmd;
 
 	switch (cp->pmd) {

commit 905b57972149f205aa73b0a3b6935b0e5470ab24
Author: Michal Simek <michal.simek@xilinx.com>
Date:   Thu Nov 7 12:49:53 2013 +0100

    ARM: 7884/1: mm: Fix ECC mem policy printk
    
    ECC policy can be applied to the whole system
    when this bit is implemented by SoC vendor
    (IMP - bit 9 - in L1 page table entry format).
    When this bit is not implemented by SoC vendor
    it doesn't mean that system has no other way
    how to do ECC.
    This patch ensures to show this message only when ECC
    is requested via cmd line ecc=on and runs on
    appropriate ARM core.
    
    Signed-off-by: Michal Simek <michal.simek@xilinx.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 78eeeca78f5a..580ef2de82d7 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -558,8 +558,8 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_WB;
 		break;
 	}
-	printk("Memory policy: ECC %sabled, Data cache %s\n",
-		ecc_mask ? "en" : "dis", cp->policy);
+	pr_info("Memory policy: %sData cache %s\n",
+		ecc_mask ? "ECC enabled, " : "", cp->policy);
 
 	for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
 		struct mem_type *t = &mem_types[i];

commit a77e0c7b2774fd52ce6bf25c2c3ffdccb7b110ff
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Wed Jul 31 12:44:46 2013 -0400

    ARM: mm: Recreate kernel mappings in early_paging_init()
    
    This patch adds a step in the init sequence, in order to recreate
    the kernel code/data page table mappings prior to full paging
    initialization.  This is necessary on LPAE systems that run out of
    a physical address space outside the 4G limit.  On these systems,
    this implementation provides a machine descriptor hook that allows
    the PHYS_OFFSET to be overridden in a machine specific fashion.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: R Sricharan <r.sricharan@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b1d17eeb59b8..78eeeca78f5a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -28,6 +28,8 @@
 #include <asm/highmem.h>
 #include <asm/system_info.h>
 #include <asm/traps.h>
+#include <asm/procinfo.h>
+#include <asm/memory.h>
 
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
@@ -1315,6 +1317,86 @@ static void __init map_lowmem(void)
 	}
 }
 
+#ifdef CONFIG_ARM_LPAE
+/*
+ * early_paging_init() recreates boot time page table setup, allowing machines
+ * to switch over to a high (>4G) address space on LPAE systems
+ */
+void __init early_paging_init(const struct machine_desc *mdesc,
+			      struct proc_info_list *procinfo)
+{
+	pmdval_t pmdprot = procinfo->__cpu_mm_mmu_flags;
+	unsigned long map_start, map_end;
+	pgd_t *pgd0, *pgdk;
+	pud_t *pud0, *pudk, *pud_start;
+	pmd_t *pmd0, *pmdk;
+	phys_addr_t phys;
+	int i;
+
+	if (!(mdesc->init_meminfo))
+		return;
+
+	/* remap kernel code and data */
+	map_start = init_mm.start_code;
+	map_end   = init_mm.brk;
+
+	/* get a handle on things... */
+	pgd0 = pgd_offset_k(0);
+	pud_start = pud0 = pud_offset(pgd0, 0);
+	pmd0 = pmd_offset(pud0, 0);
+
+	pgdk = pgd_offset_k(map_start);
+	pudk = pud_offset(pgdk, map_start);
+	pmdk = pmd_offset(pudk, map_start);
+
+	mdesc->init_meminfo();
+
+	/* Run the patch stub to update the constants */
+	fixup_pv_table(&__pv_table_begin,
+		(&__pv_table_end - &__pv_table_begin) << 2);
+
+	/*
+	 * Cache cleaning operations for self-modifying code
+	 * We should clean the entries by MVA but running a
+	 * for loop over every pv_table entry pointer would
+	 * just complicate the code.
+	 */
+	flush_cache_louis();
+	dsb();
+	isb();
+
+	/* remap level 1 table */
+	for (i = 0; i < PTRS_PER_PGD; pud0++, i++) {
+		set_pud(pud0,
+			__pud(__pa(pmd0) | PMD_TYPE_TABLE | L_PGD_SWAPPER));
+		pmd0 += PTRS_PER_PMD;
+	}
+
+	/* remap pmds for kernel mapping */
+	phys = __pa(map_start) & PMD_MASK;
+	do {
+		*pmdk++ = __pmd(phys | pmdprot);
+		phys += PMD_SIZE;
+	} while (phys < map_end);
+
+	flush_cache_all();
+	cpu_switch_mm(pgd0, &init_mm);
+	cpu_set_ttbr(1, __pa(pgd0) + TTBR1_OFFSET);
+	local_flush_bp_all();
+	local_flush_tlb_all();
+}
+
+#else
+
+void __init early_paging_init(const struct machine_desc *mdesc,
+			      struct proc_info_list *procinfo)
+{
+	if (mdesc->init_meminfo)
+		mdesc->init_meminfo();
+}
+
+#endif
+
 /*
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.

commit 141b97433d77e39ac3ac111a7b3852192035259c
Merge: d8dfad3876e4 8d258beb76e3 5cc91e046088 9fc2105aeaaf
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Sep 5 10:34:15 2013 +0100

    Merge branches 'debug-choice', 'devel-stable' and 'misc' into for-linus

commit 24195cad3e00557da166d629c8b0fd2f984f2170
Merge: 2449189bb7c7 a5463cd34354
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Aug 1 20:51:13 2013 +0100

    Merge branch 'security-fixes' into fixes

commit a5463cd3435475386cbbe7b06e01292ac169d36f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jul 31 21:58:56 2013 +0100

    ARM: make vectors page inaccessible from userspace
    
    If kuser helpers are not provided by the kernel, disable user access to
    the vectors page.  With the kuser helpers gone, there is no reason for
    this page to be visible to userspace.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9ea274d1af69..ca46f413d867 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1205,7 +1205,11 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	map.pfn = __phys_to_pfn(virt_to_phys(vectors));
 	map.virtual = 0xffff0000;
 	map.length = PAGE_SIZE;
+#ifdef CONFIG_KUSER_HELPERS
 	map.type = MT_HIGH_VECTORS;
+#else
+	map.type = MT_LOW_VECTORS;
+#endif
 	create_mapping(&map);
 
 	if (!vectors_high()) {

commit 19accfd373847ac3d10623c5d20f948846299741
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 4 11:40:32 2013 +0100

    ARM: move vector stubs
    
    Move the machine vector stubs into the page above the vector page,
    which we can prevent from being visible to userspace.  Also move
    the reset stub, and place the swi vector at a location that the
    'ldr' can get to it.
    
    This hides pointers into the kernel which could give valuable
    information to attackers, and reduces the number of exploitable
    instructions at a fixed address.
    
    Cc: <stable@vger.kernel.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4f56617a2392..9ea274d1af69 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1160,7 +1160,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	/*
 	 * Allocate the vector page early.
 	 */
-	vectors = early_alloc(PAGE_SIZE);
+	vectors = early_alloc(PAGE_SIZE * 2);
 
 	early_trap_init(vectors);
 
@@ -1210,10 +1210,18 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 
 	if (!vectors_high()) {
 		map.virtual = 0;
+		map.length = PAGE_SIZE * 2;
 		map.type = MT_LOW_VECTORS;
 		create_mapping(&map);
 	}
 
+	/* Now create a kernel read-only mapping */
+	map.pfn += 1;
+	map.virtual = 0xffff0000 + PAGE_SIZE;
+	map.length = PAGE_SIZE;
+	map.type = MT_LOW_VECTORS;
+	create_mapping(&map);
+
 	/*
 	 * Ask the machine support to map in the statically mapped devices.
 	 */

commit ff69a4c855066592f9e293cff8f54813614dd544
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 26 14:55:59 2013 +0100

    ARM: constify machine_desc structure uses
    
    struct machine_desc records are defined everywhere as a 'const'
    structure, but unfortuantely it loses its const-ness through the use of
    linker magic - the symbols which surround the section are not declared
    const so it becomes possible not to use 'const' for pointers to these
    const structures.
    
    Let's fix this oversight - all pointers to these structures should be
    marked const too.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4f56617a2392..56054ac8348e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1151,7 +1151,7 @@ void __init arm_mm_memblock_reserve(void)
  * called function.  This means you can't use any function or debugging
  * method which may touch any device, otherwise the kernel _will_ crash.
  */
-static void __init devicemaps_init(struct machine_desc *mdesc)
+static void __init devicemaps_init(const struct machine_desc *mdesc)
 {
 	struct map_desc map;
 	unsigned long addr;
@@ -1272,7 +1272,7 @@ static void __init map_lowmem(void)
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.
  */
-void __init paging_init(struct machine_desc *mdesc)
+void __init paging_init(const struct machine_desc *mdesc)
 {
 	void *zero_page;
 

commit c65b7e98b4edce7faf534154b28eae8fb579144b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jul 17 17:53:04 2013 +0100

    ARM: 7785/1: mm: restrict early_alloc to section-aligned memory
    
    When map_lowmem() runs, and processes a memory bank whose start or end
    is not section-aligned, memory must be allocated to store the 2nd-level
    page tables. Those allocations are made by calling memblock_alloc().
    
    At this point, the only memory that is free *and* mapped is memory which
    has already been mapped by map_lowmem() itself. For this reason, we must
    calculate the first point at which map_lowmem() will need to allocate
    memory, and set the memblock allocation limit to a lower address, so that
    memblock_alloc() is guaranteed to return memory that is already mapped.
    
    This patch enhances sanity_check_meminfo() to calculate that memory
    address, and pass it to memblock_set_current_limit(), rather than just
    assuming the limit is arm_lowmem_limit.
    
    The algorithm applied is:
    
    * Default memblock_limit to arm_lowmem_limit in the absence of any other
      limit; arm_lowmem_limit is the highest memory that is mapped by
      map_lowmem().
    
    * While walking the list of memblocks, if the start of a block is not
      aligned, 2nd-level page tables will need to be allocated to map the
      first few pages of the block. Hence, the memblock_limit must be before
      the start of the block.
    
    * Similarly, if the end of any block is not aligned, 2nd-level page
      tables will need to be allocated to map the last few pages of the
      block. Hence, the memblock_limit must point at the end of the block,
      rounded down to section-alignment.
    
    * The memory blocks are assumed to be sorted in address order, so the
      first unaligned block start or end is used to set the limit.
    
    With this algorithm, the start or end of almost any bank can be non-
    section-aligned. The only exception is that the start of bank 0 must
    be section-aligned, since otherwise memory would need to be allocated
    when mapping the start of bank 0, which occurs before any free memory
    is mapped.
    
    [swarren, wrote commit description, rewrote calculation of memblock_limit]
    
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4f56617a2392..b3fdb63783e3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -989,6 +989,7 @@ phys_addr_t arm_lowmem_limit __initdata = 0;
 
 void __init sanity_check_meminfo(void)
 {
+	phys_addr_t memblock_limit = 0;
 	int i, j, highmem = 0;
 	phys_addr_t vmalloc_limit = __pa(vmalloc_min - 1) + 1;
 
@@ -1052,9 +1053,32 @@ void __init sanity_check_meminfo(void)
 			bank->size = size_limit;
 		}
 #endif
-		if (!bank->highmem && bank->start + bank->size > arm_lowmem_limit)
-			arm_lowmem_limit = bank->start + bank->size;
+		if (!bank->highmem) {
+			phys_addr_t bank_end = bank->start + bank->size;
 
+			if (bank_end > arm_lowmem_limit)
+				arm_lowmem_limit = bank_end;
+
+			/*
+			 * Find the first non-section-aligned page, and point
+			 * memblock_limit at it. This relies on rounding the
+			 * limit down to be section-aligned, which happens at
+			 * the end of this function.
+			 *
+			 * With this algorithm, the start or end of almost any
+			 * bank can be non-section-aligned. The only exception
+			 * is that the start of the bank 0 must be section-
+			 * aligned, since otherwise memory would need to be
+			 * allocated when mapping the start of bank 0, which
+			 * occurs before any free memory is mapped.
+			 */
+			if (!memblock_limit) {
+				if (!IS_ALIGNED(bank->start, SECTION_SIZE))
+					memblock_limit = bank->start;
+				else if (!IS_ALIGNED(bank_end, SECTION_SIZE))
+					memblock_limit = bank_end;
+			}
+		}
 		j++;
 	}
 #ifdef CONFIG_HIGHMEM
@@ -1079,7 +1103,18 @@ void __init sanity_check_meminfo(void)
 #endif
 	meminfo.nr_banks = j;
 	high_memory = __va(arm_lowmem_limit - 1) + 1;
-	memblock_set_current_limit(arm_lowmem_limit);
+
+	/*
+	 * Round the memblock limit down to a section size.  This
+	 * helps to ensure that we will allocate memory from the
+	 * last full section, which should be mapped.
+	 */
+	if (memblock_limit)
+		memblock_limit = round_down(memblock_limit, SECTION_SIZE);
+	if (!memblock_limit)
+		memblock_limit = arm_lowmem_limit;
+
+	memblock_set_current_limit(memblock_limit);
 }
 
 static inline void prepare_page_table(void)
@@ -1276,8 +1311,6 @@ void __init paging_init(struct machine_desc *mdesc)
 {
 	void *zero_page;
 
-	memblock_set_current_limit(arm_lowmem_limit);
-
 	build_mem_type_table();
 	prepare_page_table();
 	map_lowmem();

commit bfd65dd9fe42b54c6f4eabbabc40bda9e25dcf93
Merge: d14474647899 cbbe6f82b489
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 13 14:58:36 2013 -0700

    Merge branch 'fixes' of git://git.linaro.org/people/rmk/linux-arm
    
    Pull ARM fixes from Russell King:
     "A few fixes for ARM, mostly just one liners with the exception of the
      missing section specification.  We decided not to rely on .previous to
      fix this but to explicitly state the section we want the code to be
      in."
    
    * 'fixes' of git://git.linaro.org/people/rmk/linux-arm:
      ARM: 7778/1: smp_twd: twd_update_frequency need be run on all online CPUs
      ARM: 7782/1: Kconfig: Let ARM_ERRATA_364296 not depend on CONFIG_SMP
      ARM: mm: fix boot on SA1110 Assabet
      ARM: 7781/1: mmu: Add debug_ll_io_init() mappings to early mappings
      ARM: 7780/1: add missing linker section markup to head-common.S

commit ee4de5d99aeac44f4507b7538b2b3faedc5205b9
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Sat Jul 6 00:25:51 2013 +0100

    ARM: 7781/1: mmu: Add debug_ll_io_init() mappings to early mappings
    
    Failure to add the mapping created in debug_ll_io_init() can lead
    to the BUG_ON() triggering in lib/ioremap.c:27 if the static
    virtual address decided for the debug_ll mapping overlaps with
    another mapping that is created later. This happens because the
    generic ioremap code has no idea there is a mapping there and it
    tries to place a mapping in the same location and blows up when
    it sees that there is a pte already present.
    
    kernel BUG at lib/ioremap.c:27!
    Internal error: Oops - BUG: 0 [#1] PREEMPT SMP ARM
    Modules linked in:
    CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.10.0-rc2-00042-g2af0c67-dirty #316
    task: ef088000 ti: ef082000 task.ti: ef082000
    PC is at ioremap_page_range+0x16c/0x198
    LR is at ioremap_page_range+0xf0/0x198
    pc : [<c04cb874>]    lr : [<c04cb7f8>]    psr: 20000113
    sp : ef083e78  ip : af140000  fp : ef083ebc
    r10: ef7fc100  r9 : ef7fc104  r8 : 000af174
    r7 : 00000647  r6 : beffffff  r5 : f004c000  r4 : f0040000
    r3 : af173417  r2 : 16440653  r1 : af173e07  r0 : ef7fc8fc
    Flags: nzCv  IRQs on  FIQs on  Mode SVC_32  ISA ARM  Segment kernel
    Control: 10c5787d  Table: 8020406a  DAC: 00000015
    Process swapper/0 (pid: 1, stack limit = 0xef082238)
    Stack: (0xef083e78 to 0xef084000)
    3e60:                                                       00040000 ef083eec
    3e80: bf134000 f004bfff c0207c00 f004c000 c02fc120 f000c000 c15e7800 00040000
    3ea0: ef083eec 00000647 c098ba9c c0953544 ef083edc ef083ec0 c021b82c c04cb714
    3ec0: c09cdc50 00000040 ef0f1e00 ef1003c0 ef083f14 ef083ee0 c09535bc c021b7bc
    3ee0: c0953544 c04d0c6c c094e2cc c1600be4 c07440c4 c09a6888 00000002 c0a15f00
    3f00: ef082000 00000000 ef083f54 ef083f18 c0208728 c0953550 00000002 c1600bfc
    3f20: c08e3fac c0839918 ef083f54 c1600b80 c09a6888 c0a15f00 0000008b c094e2cc
    3f40: c098ba9c c098bab8 ef083f94 ef083f58 c094ea0c c020865c 00000002 00000002
    3f60: c094e2cc 00000000 c025b674 00000000 c06ff860 00000000 00000000 00000000
    3f80: 00000000 00000000 ef083fac ef083f98 c06ff878 c094e910 00000000 00000000
    3fa0: 00000000 ef083fb0 c020efe8 c06ff86c 00000000 00000000 00000000 00000000
    3fc0: 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000
    3fe0: 00000000 00000000 00000000 00000000 00000013 00000000 00000000 c0595108
    [<c04cb874>] (ioremap_page_range+0x16c/0x198) from [<c021b82c>] (__alloc_remap_buffer.isra.18+0x7c/0xc4)
    [<c021b82c>] (__alloc_remap_buffer.isra.18+0x7c/0xc4) from [<c09535bc>] (atomic_pool_init+0x78/0x128)
    [<c09535bc>] (atomic_pool_init+0x78/0x128) from [<c0208728>] (do_one_initcall+0xd8/0x198)
    [<c0208728>] (do_one_initcall+0xd8/0x198) from [<c094ea0c>] (kernel_init_freeable+0x108/0x1d0)
    [<c094ea0c>] (kernel_init_freeable+0x108/0x1d0) from [<c06ff878>] (kernel_init+0x18/0xf4)
    [<c06ff878>] (kernel_init+0x18/0xf4) from [<c020efe8>] (ret_from_fork+0x14/0x20)
    Code: e50b0040 ebf54b2f e51b0040 eaffffee (e7f001f2)
    
    Fix it by telling generic layers about the static mapping via
    iotable_init().  This also has the nice side effect of letting
    you see the mapping in procfs' vmallocinfo file.
    
    Cc: Rob Herring <rob.herring@calxeda.com>
    Cc: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4d409e6a552d..f6ad0182df40 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -949,7 +949,7 @@ void __init debug_ll_io_init(void)
 	map.virtual &= PAGE_MASK;
 	map.length = PAGE_SIZE;
 	map.type = MT_DEVICE;
-	create_mapping(&map);
+	iotable_init(&map, 1);
 }
 #endif
 

commit fb2af0020a51709ad87ea8055c325d3fbde04158
Merge: 790eac5640ab 3c0c01ab742d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 3 09:46:29 2013 -0700

    Merge branch 'for-linus' of git://git.linaro.org/people/rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "This contains the usual updates from other people (listed below) and
      the usual random muddle of miscellaneous ARM updates which cover some
      low priority bug fixes and performance improvements.
    
      I've started to put the pull request wording into the merge commits,
      which are:
    
       - NoMMU stuff:
    
         This includes the following series sent earlier to the list:
          - nommu-fixes
          - R7 Support
          - MPU support
    
         I've left out the ARCH_MULTIPLATFORM/!MMU stuff that Arnd and I
         were discussing today until we've reached a conclusion/that's had
         some more review.
    
         This is rebased (and re-tested) on your devel-stable branch because
         otherwise there were going to be conflicts with Uwe's V7M work now
         that you've merged that.  I've included the fix for limiting MPU to
         CPU_V7.
    
       - Huge page support
    
         These changes bring both HugeTLB support and Transparent HugePage
         (THP) support to ARM.  Only long descriptors (LPAE) are supported
         in this series.
    
         The code has been tested on an Arndale board (Exynos 5250).
    
       - LPAE updates
    
         Please pull these miscellaneous LPAE fixes I've been collecting for
         a while now for 3.11.  They've been tested and reviewed by quite a
         few people, and most of the patches are pretty trivial.  -- Will Deacon.
    
       - arch_timer cleanups
    
         Please pull these arch_timer cleanups I've been holding onto for a
         while.  They're the same as my last posting, but have been rebased
         to v3.10-rc3.
    
       - mpidr linearisation (multiprocessor id register - identifies which
         CPU number we are in the system)
    
         This patch series that implements MPIDR linearization through a
         simple hashing algorithm and updates current cpu_{suspend}/{resume}
         code to use the newly created hash structures to retrieve context
         pointers.  It represents a stepping stone for the implementation of
         power management code on forthcoming multi-cluster ARM systems.
    
         It has been tested on TC2 (dual cluster A15xA7 system), iMX6q,
         OMAP4 and Tegra, with processors hitting low-power states requiring
         warm-boot resume through the cpu_resume code path"
    
    * 'for-linus' of git://git.linaro.org/people/rmk/linux-arm: (77 commits)
      ARM: 7775/1: mm: Remove do_sect_fault from LPAE code
      ARM: 7777/1: Avoid extra calls to the C compiler
      ARM: 7774/1: Fix dtb dependency to use order-only prerequisites
      ARM: 7770/1: remove residual ARMv2 support from decompressor
      ARM: 7769/1: Cortex-A15: fix erratum 798181 implementation
      ARM: 7768/1: prevent risks of out-of-bound access in ASID allocator
      ARM: 7767/1: let the ASID allocator handle suspended animation
      ARM: 7766/1: versatile: don't mark pen as __INIT
      ARM: 7765/1: perf: Record the user-mode PC in the call chain.
      ARM: 7735/2: Preserve the user r/w register TPIDRURW on context switch and fork
      ARM: kernel: implement stack pointer save array through MPIDR hashing
      ARM: kernel: build MPIDR hash function data structure
      ARM: mpu: Ensure that MPU depends on CPU_V7
      ARM: mpu: protect the vectors page with an MPU region
      ARM: mpu: Allow enabling of the MPU via kconfig
      ARM: 7758/1: introduce config HAS_BANDGAP
      ARM: 7757/1: mm: don't flush icache in switch_mm with hardware broadcasting
      ARM: 7751/1: zImage: don't overwrite ourself with a page table
      ARM: 7749/1: spinlock: retry trylock operation if strex fails on free lock
      ARM: 7748/1: oabi: handle faults when loading swi instruction from userspace
      ...

commit d2033f2c1d1de2239ded15e478ddb4028f192a15
Merge: 22237d5a588c 0ee8090c1d05
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jul 2 13:25:35 2013 -0700

    Merge tag 'cleanup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC cleanups from Arnd Bergmann:
     "This contains cleanups as preparation for other branches adding new
      features, we pulled 16 branches for 9 platforms into this one.
    
      Most notable here is the removal of support for ATAGS based OMAP4
      systems.  Since all OMAP4 machines are fully functional with DT based
      booting in 3.10, we can remove a lot of code here.
    
      Also noteworthy is Maxime Ripard's cleanup of the machine descriptors,
      which means we need no machine descriptors in a lot more cases and can
      boot additional machines by just having the respective device drivers
      enabled."
    
    * tag 'cleanup-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (76 commits)
      ARM: picoxcell: remove .nr_irqs reference
      ARM: s5p64x0: avoid build warning for uncompress.h
      ARM: SAMSUNG: Remove unused plat/regs-watchdog.h header
      ARM: SAMSUNG: Remove legacy watchdog reset code
      ARM: SAMSUNG: Let platforms use the new watchdog reset driver
      ARM: SAMSUNG: Add watchdog reset driver
      ARM: SAMSUNG: Use local definitions of watchdog registers
      watchdog: s3c2410_wdt: Use local register definitions
      ARM: S5P64X0: Use common uncompress.h part for plat-samsung
      ARM: SAMSUNG: Consolidate uncompress subroutine
      ARM: at91: drop rm9200dk board support
      ARM: dts: msm: Fix merge resolution
      ARM: OMAP1: Remove dma.h
      ARM: OMAP1: Remove legacy irda.h and irda setup from board files
      ARM: OMAP1: Remove duplicated DMA channel definitions
      ARM: OMAP1: Remove McBSP DMA channel definitions
      ARM: OMAP2+: Remove dma.h
      ARM: OMAP2+: hwmod: Remove remaining DMA channel definitions
      ARM: OMAP2+: Remove duplicated DMA channel definitions
      ARM: OMAP2+: Remove AES crypto device DMA channel definitions
      ...

commit 3c0c01ab742ddfaf6b6f2d64b890e77cda4b7727
Merge: cbd379b10019 809e660f438f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jun 29 11:44:43 2013 +0100

    Merge branch 'devel-stable' into for-next
    
    Conflicts:
            arch/arm/Makefile
            arch/arm/include/asm/glue-proc.h

commit 37468b30a3948bbbdf9d664678f611510d987e65
Author: Po-Yu Chuang <ratbert.chuang@gmail.com>
Date:   Fri Jun 7 12:15:45 2013 +0100

    ARM: 7753/1: map_init_section flushes incorrect pmd
    
    This bug was introduced in commit e651eab0.
    Some v4/v5 platforms failed to boot due to this.
    
    Signed-off-by: Po-Yu Chuang <ratbert.chuang@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e0d8565671a6..4d409e6a552d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -616,10 +616,12 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }
 
-static void __init map_init_section(pmd_t *pmd, unsigned long addr,
+static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
 			unsigned long end, phys_addr_t phys,
 			const struct mem_type *type)
 {
+	pmd_t *p = pmd;
+
 #ifndef CONFIG_ARM_LPAE
 	/*
 	 * In classic MMU format, puds and pmds are folded in to
@@ -638,7 +640,7 @@ static void __init map_init_section(pmd_t *pmd, unsigned long addr,
 		phys += SECTION_SIZE;
 	} while (pmd++, addr += SECTION_SIZE, addr != end);
 
-	flush_pmd_entry(pmd);
+	flush_pmd_entry(p);
 }
 
 static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
@@ -661,7 +663,7 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 		 */
 		if (type->prot_sect &&
 				((addr | next | phys) & ~SECTION_MASK) == 0) {
-			map_init_section(pmd, addr, next, phys, type);
+			__map_init_section(pmd, addr, next, phys, type);
 		} else {
 			alloc_init_pte(pmd, addr, next,
 						__phys_to_pfn(phys), type);

commit 28d4bf7a2929c5e525171d249e12662e21130ec3
Author: Cyril Chemparathy <cyril@ti.com>
Date:   Fri Jul 20 13:16:41 2012 -0400

    ARM: mm: clean up membank size limit checks
    
    This patch cleans up the highmem sanity check code by simplifying the range
    checks with a pre-calculated size_limit.  This patch should otherwise have no
    functional impact on behavior.
    
    This patch also removes a redundant (bank->start < vmalloc_limit) check, since
    this is already covered by the !highmem condition.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ae249d1ab1d3..280f91d02de2 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -992,10 +992,15 @@ void __init sanity_check_meminfo(void)
 
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];
+		phys_addr_t size_limit;
+
 		*bank = meminfo.bank[i];
+		size_limit = bank->size;
 
 		if (bank->start >= vmalloc_limit)
 			highmem = 1;
+		else
+			size_limit = vmalloc_limit - bank->start;
 
 		bank->highmem = highmem;
 
@@ -1004,8 +1009,7 @@ void __init sanity_check_meminfo(void)
 		 * Split those memory banks which are partially overlapping
 		 * the vmalloc area greatly simplifying things later.
 		 */
-		if (!highmem && bank->start < vmalloc_limit &&
-		    bank->size > vmalloc_limit - bank->start) {
+		if (!highmem && bank->size > size_limit) {
 			if (meminfo.nr_banks >= NR_BANKS) {
 				printk(KERN_CRIT "NR_BANKS too low, "
 						 "ignoring high memory\n");
@@ -1014,12 +1018,12 @@ void __init sanity_check_meminfo(void)
 					(meminfo.nr_banks - i) * sizeof(*bank));
 				meminfo.nr_banks++;
 				i++;
-				bank[1].size -= vmalloc_limit - bank->start;
+				bank[1].size -= size_limit;
 				bank[1].start = vmalloc_limit;
 				bank[1].highmem = highmem = 1;
 				j++;
 			}
-			bank->size = vmalloc_limit - bank->start;
+			bank->size = size_limit;
 		}
 #else
 		/*
@@ -1037,14 +1041,13 @@ void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would partially overlap
 		 * the vmalloc area.
 		 */
-		if (bank->start + bank->size > vmalloc_limit)
-			unsigned long newsize = vmalloc_limit - bank->start;
+		if (bank->size > size_limit) {
 			printk(KERN_NOTICE "Truncating RAM at %.8llx-%.8llx "
 			       "to -%.8llx (vmalloc region overlap).\n",
 			       (unsigned long long)bank->start,
 			       (unsigned long long)bank->start + bank->size - 1,
-			       (unsigned long long)bank->start + newsize - 1);
-			bank->size = newsize;
+			       (unsigned long long)bank->start + size_limit - 1);
+			bank->size = size_limit;
 		}
 #endif
 		if (!bank->highmem && bank->start + bank->size > arm_lowmem_limit)

commit adf2e9fda34c1cfff2ee4e47078b1e142adb2c30
Author: Cyril Chemparathy <cyril@ti.com>
Date:   Fri Jul 20 12:24:45 2012 -0400

    ARM: mm: cleanup checks for membank overlap with vmalloc area
    
    On Keystone platforms, physical memory is entirely outside the 32-bit
    addressible range.  Therefore, the (bank->start > ULONG_MAX) check below marks
    the entire system memory as highmem, and this causes unpleasentness all over.
    
    This patch eliminates the extra bank start check (against ULONG_MAX) by
    checking bank->start against the physical address corresponding to vmalloc_min
    instead.
    
    In the process, this patch also cleans up parts of the highmem sanity check
    code by removing what has now become a redundant check for banks that entirely
    overlap with the vmalloc range.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index fc6ff1a9db50..ae249d1ab1d3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -994,15 +994,12 @@ void __init sanity_check_meminfo(void)
 		struct membank *bank = &meminfo.bank[j];
 		*bank = meminfo.bank[i];
 
-		if (bank->start > ULONG_MAX)
-			highmem = 1;
-
-#ifdef CONFIG_HIGHMEM
 		if (bank->start >= vmalloc_limit)
 			highmem = 1;
 
 		bank->highmem = highmem;
 
+#ifdef CONFIG_HIGHMEM
 		/*
 		 * Split those memory banks which are partially overlapping
 		 * the vmalloc area greatly simplifying things later.
@@ -1025,8 +1022,6 @@ void __init sanity_check_meminfo(void)
 			bank->size = vmalloc_limit - bank->start;
 		}
 #else
-		bank->highmem = highmem;
-
 		/*
 		 * Highmem banks not allowed with !CONFIG_HIGHMEM.
 		 */
@@ -1038,18 +1033,6 @@ void __init sanity_check_meminfo(void)
 			continue;
 		}
 
-		/*
-		 * Check whether this memory bank would entirely overlap
-		 * the vmalloc area.
-		 */
-		if (bank->start >= vmalloc_limit) {
-			printk(KERN_NOTICE "Ignoring RAM at %.8llx-%.8llx "
-			       "(vmalloc region overlap).\n",
-			       (unsigned long long)bank->start,
-			       (unsigned long long)bank->start + bank->size - 1);
-			continue;
-		}
-
 		/*
 		 * Check whether this memory bank would partially overlap
 		 * the vmalloc area.

commit 82f667046ec895552caf2f7a4c6841c530bfc215
Author: Cyril Chemparathy <cyril@ti.com>
Date:   Fri Jul 20 12:01:23 2012 -0400

    ARM: mm: use physical addresses in highmem sanity checks
    
    This patch modifies the highmem sanity checking code to use physical addresses
    instead.  This change eliminates the wrap-around problems associated with the
    original virtual address based checks, and this simplifies the code a bit.
    
    The one constraint imposed here is that low physical memory must be mapped in
    a monotonically increasing fashion if there are multiple banks of memory,
    i.e., x < y must => pa(x) < pa(y).
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b7ce65a82371..fc6ff1a9db50 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -988,6 +988,7 @@ phys_addr_t arm_lowmem_limit __initdata = 0;
 void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
+	phys_addr_t vmalloc_limit = __pa(vmalloc_min - 1) + 1;
 
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];
@@ -997,8 +998,7 @@ void __init sanity_check_meminfo(void)
 			highmem = 1;
 
 #ifdef CONFIG_HIGHMEM
-		if (__va(bank->start) >= vmalloc_min ||
-		    __va(bank->start) < (void *)PAGE_OFFSET)
+		if (bank->start >= vmalloc_limit)
 			highmem = 1;
 
 		bank->highmem = highmem;
@@ -1007,8 +1007,8 @@ void __init sanity_check_meminfo(void)
 		 * Split those memory banks which are partially overlapping
 		 * the vmalloc area greatly simplifying things later.
 		 */
-		if (!highmem && __va(bank->start) < vmalloc_min &&
-		    bank->size > vmalloc_min - __va(bank->start)) {
+		if (!highmem && bank->start < vmalloc_limit &&
+		    bank->size > vmalloc_limit - bank->start) {
 			if (meminfo.nr_banks >= NR_BANKS) {
 				printk(KERN_CRIT "NR_BANKS too low, "
 						 "ignoring high memory\n");
@@ -1017,12 +1017,12 @@ void __init sanity_check_meminfo(void)
 					(meminfo.nr_banks - i) * sizeof(*bank));
 				meminfo.nr_banks++;
 				i++;
-				bank[1].size -= vmalloc_min - __va(bank->start);
-				bank[1].start = __pa(vmalloc_min - 1) + 1;
+				bank[1].size -= vmalloc_limit - bank->start;
+				bank[1].start = vmalloc_limit;
 				bank[1].highmem = highmem = 1;
 				j++;
 			}
-			bank->size = vmalloc_min - __va(bank->start);
+			bank->size = vmalloc_limit - bank->start;
 		}
 #else
 		bank->highmem = highmem;
@@ -1042,8 +1042,7 @@ void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would entirely overlap
 		 * the vmalloc area.
 		 */
-		if (__va(bank->start) >= vmalloc_min ||
-		    __va(bank->start) < (void *)PAGE_OFFSET) {
+		if (bank->start >= vmalloc_limit) {
 			printk(KERN_NOTICE "Ignoring RAM at %.8llx-%.8llx "
 			       "(vmalloc region overlap).\n",
 			       (unsigned long long)bank->start,
@@ -1055,9 +1054,8 @@ void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would partially overlap
 		 * the vmalloc area.
 		 */
-		if (__va(bank->start + bank->size - 1) >= vmalloc_min ||
-		    __va(bank->start + bank->size - 1) <= __va(bank->start)) {
-			unsigned long newsize = vmalloc_min - __va(bank->start);
+		if (bank->start + bank->size > vmalloc_limit)
+			unsigned long newsize = vmalloc_limit - bank->start;
 			printk(KERN_NOTICE "Truncating RAM at %.8llx-%.8llx "
 			       "to -%.8llx (vmalloc region overlap).\n",
 			       (unsigned long long)bank->start,

commit 20d6956d8cd2452cec0889ff040f18afc03c2e6b
Author: Vitaly Andrianov <vitalya@ti.com>
Date:   Tue Jul 10 14:41:17 2012 -0400

    ARM: LPAE: use phys_addr_t in alloc_init_pud()
    
    This patch fixes the alloc_init_pud() function to use phys_addr_t instead of
    unsigned long when passing in the phys argument.
    
    This is an extension to commit 97092e0c56830457af0639f6bd904537a150ea4a (ARM:
    pgtable: use phys_addr_t for physical addresses), which applied similar changes
    elsewhere in the ARM memory management code.
    
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e0d8565671a6..b7ce65a82371 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -673,7 +673,8 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 }
 
 static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
-	unsigned long end, unsigned long phys, const struct mem_type *type)
+				  unsigned long end, phys_addr_t phys,
+				  const struct mem_type *type)
 {
 	pud_t *pud = pud_offset(pgd, addr);
 	unsigned long next;

commit bc37324e820e4a23a0bccef79ae797ce4d939c4f
Author: Maxime Ripard <maxime.ripard@free-electrons.com>
Date:   Thu Apr 18 21:52:23 2013 +0200

    ARM: mmu: Call debug_ll_io_init if no map_io function is specified
    
    More and more sub-architectures are using only the debug_ll_io_init
    function as the map_io function. Make the core code call this function
    if no function is specified in the machine description to remove some
    boilerplate code.
    
    Signed-off-by: Maxime Ripard <maxime.ripard@free-electrons.com>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e0d8565671a6..faa36d7b8786 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1232,6 +1232,8 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 */
 	if (mdesc->map_io)
 		mdesc->map_io();
+	else
+		debug_ll_io_init();
 	fill_pmd_gaps();
 
 	/* Reserve fixed i/o space in VMALLOC region */

commit 946342d03ec4e8367bba7bb99e7155e97f69058a
Merge: 70100a022684 f8f02ec25ce3 1783d4574619 848a7b13aef0 6aaa189f8712 5e45a0a1975d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu May 2 21:30:36 2013 +0100

    Merge branches 'devel-stable', 'entry', 'fixes', 'mach-types', 'misc' and 'smp-hotplug' into for-linus

commit de40614e92bf1b0308d953387b0cb9d3a5710186
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Fri Apr 5 03:16:51 2013 +0100

    ARM: 7694/1: ARM, TCM: initialize TCM in paging_init(), instead of setup_arch()
    
    tcm_init() call iotable_init() and it use early_alloc variants which
    do memblock allocation. Directly using memblock allocation after
    initializing bootmem should not permitted, because bootmem can't know
    where are additinally reserved.
    So move tcm_init() to a safe place before initalizing bootmem.
    
    (On the U300)
    
    Tested-by: Linus Walleij <linus.walleij@linaro.org>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 78978945492a..a84ff763ac39 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -34,6 +34,7 @@
 #include <asm/mach/pci.h>
 
 #include "mm.h"
+#include "tcm.h"
 
 /*
  * empty_zero_page is a special page that is used for
@@ -1277,6 +1278,7 @@ void __init paging_init(struct machine_desc *mdesc)
 	dma_contiguous_remap();
 	devicemaps_init(mdesc);
 	kmap_init();
+	tcm_init();
 
 	top_pmd = pmd_off_k(0xffff0000);
 

commit e651eab0af88aa7a281fe9e8c36c0846552aa7fc
Author: Sricharan R <r.sricharan@ti.com>
Date:   Mon Mar 18 12:24:04 2013 +0100

    ARM: 7677/1: LPAE: Fix mapping in alloc_init_section for unaligned addresses
    
    With LPAE enabled, alloc_init_section() does not map the entire
    address space for unaligned addresses.
    
    The issue also reproduced with CMA + LPAE. CMA tries to map 16MB
    with page granularity mappings during boot. alloc_init_pte()
    is called and out of 16MB, only 2MB gets mapped and rest remains
    unaccessible.
    
    Because of this OMAP5 boot is broken with CMA + LPAE enabled.
    Fix the issue by ensuring that the entire addresses are
    mapped.
    
    Signed-off-by: R Sricharan <r.sricharan@ti.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoffer Dall <chris@cloudcar.com>
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Christoffer Dall <chris@cloudcar.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e95a996ab78f..78978945492a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -598,39 +598,60 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }
 
-static void __init alloc_init_section(pud_t *pud, unsigned long addr,
-				      unsigned long end, phys_addr_t phys,
-				      const struct mem_type *type)
+static void __init map_init_section(pmd_t *pmd, unsigned long addr,
+			unsigned long end, phys_addr_t phys,
+			const struct mem_type *type)
 {
-	pmd_t *pmd = pmd_offset(pud, addr);
-
+#ifndef CONFIG_ARM_LPAE
 	/*
-	 * Try a section mapping - end, addr and phys must all be aligned
-	 * to a section boundary.  Note that PMDs refer to the individual
-	 * L1 entries, whereas PGDs refer to a group of L1 entries making
-	 * up one logical pointer to an L2 table.
+	 * In classic MMU format, puds and pmds are folded in to
+	 * the pgds. pmd_offset gives the PGD entry. PGDs refer to a
+	 * group of L1 entries making up one logical pointer to
+	 * an L2 table (2MB), where as PMDs refer to the individual
+	 * L1 entries (1MB). Hence increment to get the correct
+	 * offset for odd 1MB sections.
+	 * (See arch/arm/include/asm/pgtable-2level.h)
 	 */
-	if (type->prot_sect && ((addr | end | phys) & ~SECTION_MASK) == 0) {
-		pmd_t *p = pmd;
-
-#ifndef CONFIG_ARM_LPAE
-		if (addr & SECTION_SIZE)
-			pmd++;
+	if (addr & SECTION_SIZE)
+		pmd++;
 #endif
+	do {
+		*pmd = __pmd(phys | type->prot_sect);
+		phys += SECTION_SIZE;
+	} while (pmd++, addr += SECTION_SIZE, addr != end);
 
-		do {
-			*pmd = __pmd(phys | type->prot_sect);
-			phys += SECTION_SIZE;
-		} while (pmd++, addr += SECTION_SIZE, addr != end);
+	flush_pmd_entry(pmd);
+}
 
-		flush_pmd_entry(p);
-	} else {
+static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
+				      unsigned long end, phys_addr_t phys,
+				      const struct mem_type *type)
+{
+	pmd_t *pmd = pmd_offset(pud, addr);
+	unsigned long next;
+
+	do {
 		/*
-		 * No need to loop; pte's aren't interested in the
-		 * individual L1 entries.
+		 * With LPAE, we must loop over to map
+		 * all the pmds for the given range.
 		 */
-		alloc_init_pte(pmd, addr, end, __phys_to_pfn(phys), type);
-	}
+		next = pmd_addr_end(addr, end);
+
+		/*
+		 * Try a section mapping - addr, next and phys must all be
+		 * aligned to a section boundary.
+		 */
+		if (type->prot_sect &&
+				((addr | next | phys) & ~SECTION_MASK) == 0) {
+			map_init_section(pmd, addr, next, phys, type);
+		} else {
+			alloc_init_pte(pmd, addr, next,
+						__phys_to_pfn(phys), type);
+		}
+
+		phys += next - addr;
+
+	} while (pmd++, addr = next, addr != end);
 }
 
 static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
@@ -641,7 +662,7 @@ static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
 
 	do {
 		next = pud_addr_end(addr, end);
-		alloc_init_section(pud, addr, next, phys, type);
+		alloc_init_pmd(pud, addr, next, phys, type);
 		phys += next - addr;
 	} while (pud++, addr = next, addr != end);
 }

commit 73a09d212ec65b7068a283e6034fa05649d3d075
Merge: 6dbe51c251a3 6ebd4d038dbb
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Mar 9 15:49:32 2013 +0000

    Merge branch 'for-next' of git://git.pengutronix.de/git/ukl/linux into devel-stable
    
    Conflicts:
            arch/arm/include/asm/cputype.h
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 6db167dfc013b0e114c81077ac091ba26a69f4ed
Merge: 32f9aab8ebd8 9cb543124a2d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 20 14:29:37 2013 -0800

    Merge branch 'for-linus-2' of git://git.linaro.org/people/rmk/linux-arm
    
    Pull ARM updates (part two) from Russell King:
    
     - breakpoint and perf updates from Will Deacon.
    
     - hypervisor boot mode updates from Will.
    
     - support for Power State Coordination Interface via the Hypervisor
    
     - core ARM support for KVM
    
    * 'for-linus-2' of git://git.linaro.org/people/rmk/linux-arm: (32 commits)
      KVM: ARM: Add maintainer entry for KVM/ARM
      KVM: ARM: Power State Coordination Interface implementation
      KVM: ARM: Handle I/O aborts
      KVM: ARM: Handle guest faults in KVM
      KVM: ARM: VFP userspace interface
      KVM: ARM: Demux CCSIDR in the userspace API
      KVM: ARM: User space API for getting/setting co-proc registers
      KVM: ARM: Emulation framework and CP15 emulation
      KVM: ARM: World-switch implementation
      KVM: ARM: Inject IRQs and FIQs from userspace
      KVM: ARM: Memory virtualization setup
      KVM: ARM: Hypervisor initialization
      KVM: ARM: Initial skeleton to compile KVM support
      ARM: Section based HYP idmap
      ARM: Add page table and page defines needed by KVM
      ARM: perf: simplify __hw_perf_event_init err handling
      ARM: perf: remove unnecessary checks for idx < 0
      ARM: perf: handle armpmu_register failing
      ARM: perf: don't pretend to support counting of L1I writes
      ARM: perf: remove redundant NULL check on cpu_pmu
      ...

commit 1b1c7409b75a8b62906b78b84c8469002072b738
Merge: 573f8c8d1337 b28748fb5d21
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Feb 20 14:35:58 2013 +0000

    Merge branch 'misc' into for-linus
    
    Conflicts:
            arch/arm/include/asm/memory.h

commit bce2bd3cda4e73cd92cf08e017c073058c151ddc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Feb 14 10:50:04 2013 +0000

    ARM: fix warnings introduced by previous patch
    
    869486d5f51 (ARM: 7646/1: mm: use static_vm for managing static mapped
    areas) introduced new warnings:
    
    arch/arm/mm/mmu.c: In function 'pci_reserve_io':
    arch/arm/mm/mmu.c:888:16: warning: unused variable 'addr'
    arch/arm/mm/mmu.c:887:20: warning: unused variable 'vm'
    
    because it failed to delete the two local variables it no longer used.
    Fix this.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index a35b314d270d..3da1d3e0af24 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -862,8 +862,6 @@ static void __init fill_pmd_gaps(void)
 #if defined(CONFIG_PCI) && !defined(CONFIG_NEED_MACH_IO_H)
 static void __init pci_reserve_io(void)
 {
-	struct vm_struct *vm;
-	unsigned long addr;
 	struct static_vm *svm;
 
 	svm = find_static_vm_vaddr((void *)PCI_IO_VIRT_BASE);

commit 101eeda38c0ab8a4f916176e325d9e036d981a24
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Sat Feb 9 06:28:06 2013 +0100

    ARM: 7646/1: mm: use static_vm for managing static mapped areas
    
    A static mapped area is ARM-specific, so it is better not to use
    generic vmalloc data structure, that is, vmlist and vmlist_lock
    for managing static mapped area. And it causes some needless overhead and
    reducing this overhead is better idea.
    
    Now, we have newly introduced static_vm infrastructure.
    With it, we don't need to iterate all mapped areas. Instead, we just
    iterate static mapped areas. It helps to reduce an overhead of finding
    matched area. And architecture dependency on vmalloc layer is removed,
    so it will help to maintainability for vmalloc layer.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Rob Herring <rob.herring@calxeda.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9f0610243bd6..a35b314d270d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -757,21 +757,24 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 {
 	struct map_desc *md;
 	struct vm_struct *vm;
+	struct static_vm *svm;
 
 	if (!nr)
 		return;
 
-	vm = early_alloc_aligned(sizeof(*vm) * nr, __alignof__(*vm));
+	svm = early_alloc_aligned(sizeof(*svm) * nr, __alignof__(*svm));
 
 	for (md = io_desc; nr; md++, nr--) {
 		create_mapping(md);
+
+		vm = &svm->vm;
 		vm->addr = (void *)(md->virtual & PAGE_MASK);
 		vm->size = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
 		vm->phys_addr = __pfn_to_phys(md->pfn);
 		vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
 		vm->flags |= VM_ARM_MTYPE(md->type);
 		vm->caller = iotable_init;
-		vm_area_add_early(vm++);
+		add_static_vm_early(svm++);
 	}
 }
 
@@ -779,13 +782,16 @@ void __init vm_reserve_area_early(unsigned long addr, unsigned long size,
 				  void *caller)
 {
 	struct vm_struct *vm;
+	struct static_vm *svm;
+
+	svm = early_alloc_aligned(sizeof(*svm), __alignof__(*svm));
 
-	vm = early_alloc_aligned(sizeof(*vm), __alignof__(*vm));
+	vm = &svm->vm;
 	vm->addr = (void *)addr;
 	vm->size = size;
 	vm->flags = VM_IOREMAP | VM_ARM_EMPTY_MAPPING;
 	vm->caller = caller;
-	vm_area_add_early(vm);
+	add_static_vm_early(svm);
 }
 
 #ifndef CONFIG_ARM_LPAE
@@ -810,14 +816,13 @@ static void __init pmd_empty_section_gap(unsigned long addr)
 
 static void __init fill_pmd_gaps(void)
 {
+	struct static_vm *svm;
 	struct vm_struct *vm;
 	unsigned long addr, next = 0;
 	pmd_t *pmd;
 
-	/* we're still single threaded hence no lock needed here */
-	for (vm = vmlist; vm; vm = vm->next) {
-		if (!(vm->flags & (VM_ARM_STATIC_MAPPING | VM_ARM_EMPTY_MAPPING)))
-			continue;
+	list_for_each_entry(svm, &static_vmlist, list) {
+		vm = &svm->vm;
 		addr = (unsigned long)vm->addr;
 		if (addr < next)
 			continue;
@@ -859,17 +864,12 @@ static void __init pci_reserve_io(void)
 {
 	struct vm_struct *vm;
 	unsigned long addr;
+	struct static_vm *svm;
 
-	/* we're still single threaded hence no lock needed here */
-	for (vm = vmlist; vm; vm = vm->next) {
-		if (!(vm->flags & VM_ARM_STATIC_MAPPING))
-			continue;
-		addr = (unsigned long)vm->addr;
-		addr &= ~(SZ_2M - 1);
-		if (addr == PCI_IO_VIRT_BASE)
-			return;
+	svm = find_static_vm_vaddr((void *)PCI_IO_VIRT_BASE);
+	if (svm)
+		return;
 
-	}
 	vm_reserve_area_early(PCI_IO_VIRT_BASE, SZ_2M, pci_reserve_io);
 }
 #else

commit b849a60e0903b1c5430c3859864554662e127a8d
Author: Uwe Kleine-Knig <u.kleine-koenig@pengutronix.de>
Date:   Mon Jan 16 10:34:31 2012 +0100

    ARM: make cr_alignment read-only #ifndef CONFIG_CPU_CP15
    
    This makes cr_alignment a constant 0 to break code that tries to modify
    the value as it's likely that it's built on wrong assumption when
    CONFIG_CPU_CP15 isn't defined. For code that is only reading the value 0
    is more or less a fine value to report.
    
    Signed-off-by: Uwe Kleine-Knig <u.kleine-koenig@pengutronix.de>
    Message-Id: 1358413196-5609-2-git-send-email-u.kleine-koenig@pengutronix.de (v8)

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ce328c7f5c94..7c347bcc9421 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -97,6 +97,7 @@ static struct cachepolicy cache_policies[] __initdata = {
 	}
 };
 
+#ifdef CONFIG_CPU_CP15
 /*
  * These are useful for identifying cache coherency
  * problems by allowing the cache or the cache and
@@ -195,6 +196,22 @@ void adjust_cr(unsigned long mask, unsigned long set)
 }
 #endif
 
+#else /* ifdef CONFIG_CPU_CP15 */
+
+static int __init early_cachepolicy(char *p)
+{
+	pr_warning("cachepolicy kernel parameter not supported without cp15\n");
+}
+early_param("cachepolicy", early_cachepolicy);
+
+static int __init noalign_setup(char *__unused)
+{
+	pr_warning("noalign kernel parameter not supported without cp15\n");
+}
+__setup("noalign", noalign_setup);
+
+#endif /* ifdef CONFIG_CPU_CP15 / else */
+
 #define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_XN
 #define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_AP_WRITE
 

commit cc577c26e2e9740b046591a72e77213c556bff19
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Sun Jan 20 18:28:04 2013 -0500

    ARM: Add page table and page defines needed by KVM
    
    KVM uses the stage-2 page tables and the Hyp page table format,
    so we define the fields and page protection flags needed by KVM.
    
    The nomenclature is this:
     - page_hyp:        PL2 code/data mappings
     - page_hyp_device: PL2 device mappings (vgic access)
     - page_s2:         Stage-2 code/data page mappings
     - page_s2_device:  Stage-2 device mappings (vgic access)
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Marcelo Tosatti <mtosatti@redhat.com>
    Christoffer Dall <c.dall@virtualopensystems.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9f0610243bd6..1f51d712b55a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -57,6 +57,9 @@ static unsigned int cachepolicy __initdata = CPOLICY_WRITEBACK;
 static unsigned int ecc_mask __initdata = 0;
 pgprot_t pgprot_user;
 pgprot_t pgprot_kernel;
+pgprot_t pgprot_hyp_device;
+pgprot_t pgprot_s2;
+pgprot_t pgprot_s2_device;
 
 EXPORT_SYMBOL(pgprot_user);
 EXPORT_SYMBOL(pgprot_kernel);
@@ -66,34 +69,46 @@ struct cachepolicy {
 	unsigned int	cr_mask;
 	pmdval_t	pmd;
 	pteval_t	pte;
+	pteval_t	pte_s2;
 };
 
+#ifdef CONFIG_ARM_LPAE
+#define s2_policy(policy)	policy
+#else
+#define s2_policy(policy)	0
+#endif
+
 static struct cachepolicy cache_policies[] __initdata = {
 	{
 		.policy		= "uncached",
 		.cr_mask	= CR_W|CR_C,
 		.pmd		= PMD_SECT_UNCACHED,
 		.pte		= L_PTE_MT_UNCACHED,
+		.pte_s2		= s2_policy(L_PTE_S2_MT_UNCACHED),
 	}, {
 		.policy		= "buffered",
 		.cr_mask	= CR_C,
 		.pmd		= PMD_SECT_BUFFERED,
 		.pte		= L_PTE_MT_BUFFERABLE,
+		.pte_s2		= s2_policy(L_PTE_S2_MT_UNCACHED),
 	}, {
 		.policy		= "writethrough",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WT,
 		.pte		= L_PTE_MT_WRITETHROUGH,
+		.pte_s2		= s2_policy(L_PTE_S2_MT_WRITETHROUGH),
 	}, {
 		.policy		= "writeback",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WB,
 		.pte		= L_PTE_MT_WRITEBACK,
+		.pte_s2		= s2_policy(L_PTE_S2_MT_WRITEBACK),
 	}, {
 		.policy		= "writealloc",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WBWA,
 		.pte		= L_PTE_MT_WRITEALLOC,
+		.pte_s2		= s2_policy(L_PTE_S2_MT_WRITEBACK),
 	}
 };
 
@@ -310,6 +325,7 @@ static void __init build_mem_type_table(void)
 	struct cachepolicy *cp;
 	unsigned int cr = get_cr();
 	pteval_t user_pgprot, kern_pgprot, vecs_pgprot;
+	pteval_t hyp_device_pgprot, s2_pgprot, s2_device_pgprot;
 	int cpu_arch = cpu_architecture();
 	int i;
 
@@ -421,6 +437,8 @@ static void __init build_mem_type_table(void)
 	 */
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
+	s2_pgprot = cp->pte_s2;
+	hyp_device_pgprot = s2_device_pgprot = mem_types[MT_DEVICE].prot_pte;
 
 	/*
 	 * ARMv6 and above have extended page tables.
@@ -444,6 +462,7 @@ static void __init build_mem_type_table(void)
 			user_pgprot |= L_PTE_SHARED;
 			kern_pgprot |= L_PTE_SHARED;
 			vecs_pgprot |= L_PTE_SHARED;
+			s2_pgprot |= L_PTE_SHARED;
 			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_S;
 			mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
@@ -498,6 +517,9 @@ static void __init build_mem_type_table(void)
 	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
 				 L_PTE_DIRTY | kern_pgprot);
+	pgprot_s2  = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | s2_pgprot);
+	pgprot_s2_device  = __pgprot(s2_device_pgprot);
+	pgprot_hyp_device  = __pgprot(hyp_device_pgprot);
 
 	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;

commit 93d5bf073a1e01035be66dc41860b9ae9aa9ccfa
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Thu Jan 17 07:18:04 2013 +0100

    ARM: 7629/1: mm: Fix missing XN flag for for MT_MEMORY_SO
    
    Commit 8fb54284ba6a {ARM: mm: Add strongly ordered descriptor support}
    added XN flag at section level but missed it at PTE level.
    
    Fix it by adding the L_PTE_XN to MT_MEMORY_SO PTE descriptor.
    
    Reported-by: Richard Woodruff <r-woodruff2@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9f0610243bd6..ce328c7f5c94 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -283,7 +283,7 @@ static struct mem_type mem_types[] = {
 	},
 	[MT_MEMORY_SO] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_MT_UNCACHED,
+				L_PTE_MT_UNCACHED | L_PTE_XN,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_SECT_S |
 				PMD_SECT_UNCACHED | PMD_SECT_XN,

commit d027db132b395dabfac208e52a7e510e441bb9d2
Merge: d01e4afdbb65 5faf7cbb848d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 12 12:05:15 2012 -0800

    Merge tag 'soc' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC updates from Olof Johansson:
     "This contains the bulk of new SoC development for this merge window.
    
      Two new platforms have been added, the sunxi platforms (Allwinner A1x
      SoCs) by Maxime Ripard, and a generic Broadcom platform for a new
      series of ARMv7 platforms from them, where the hope is that we can
      keep the platform code generic enough to have them all share one mach
      directory.  The new Broadcom platform is contributed by Christian
      Daudt.
    
      Highbank has grown support for Calxeda's next generation of hardware,
      ECX-2000.
    
      clps711x has seen a lot of cleanup from Alexander Shiyan, and he's
      also taken on maintainership of the platform.
    
      Beyond this there has been a bunch of work from a number of people on
      converting more platforms to IRQ domains, pinctrl conversion, cleanup
      and general feature enablement across most of the active platforms."
    
    Fix up trivial conflicts as per Olof.
    
    * tag 'soc' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (174 commits)
      mfd: vexpress-sysreg: Remove LEDs code
      irqchip: irq-sunxi: Add terminating entry for sunxi_irq_dt_ids
      clocksource: sunxi_timer: Add terminating entry for sunxi_timer_dt_ids
      irq: versatile: delete dangling variable
      ARM: sunxi: add missing include for mdelay()
      ARM: EXYNOS: Avoid early use of of_machine_is_compatible()
      ARM: dts: add node for PL330 MDMA1 controller for exynos4
      ARM: EXYNOS: Add support for secondary CPU bring-up on Exynos4412
      ARM: EXYNOS: add UART3 to DEBUG_LL ports
      ARM: S3C24XX: Add clkdev entry for camif-upll clock
      ARM: SAMSUNG: Add s3c24xx/s3c64xx CAMIF GPIO setup helpers
      ARM: sunxi: Add missing sun4i.dtsi file
      pinctrl: samsung: Do not initialise statics to 0
      ARM i.MX6: remove gate_mask from pllv3
      ARM i.MX6: Fix ethernet PLL clocks
      ARM i.MX6: rename PLLs according to datasheet
      ARM i.MX6: Add pwm support
      ARM i.MX51: Add pwm support
      ARM i.MX53: Add pwm support
      ARM: mx5: Replace clk_register_clkdev with clock DT lookup
      ...

commit 864aa04cd02979c2c755cb28b5f4fe56039171c0
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Sep 18 19:18:35 2012 +0100

    ARM: mm: use pteval_t to represent page protection values
    
    When updating the page protection map after calculating the user_pgprot
    value, the base protection map is temporarily stored in an unsigned long
    type, causing truncation of the protection bits when LPAE is enabled.
    This effectively means that calls to mprotect() will corrupt the upper
    page attributes, clearing the XN bit unconditionally.
    
    This patch uses pteval_t to store the intermediate protection values,
    preserving the upper bits for 64-bit descriptors.
    
    Cc: stable@vger.kernel.org
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 941dfb9e9a78..99b47b950efc 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -488,7 +488,7 @@ static void __init build_mem_type_table(void)
 #endif
 
 	for (i = 0; i < 16; i++) {
-		unsigned long v = pgprot_val(protection_map[i]);
+		pteval_t v = pgprot_val(protection_map[i]);
 		protection_map[i] = __pgprot(v | user_pgprot);
 	}
 

commit e5c5f2adeb370559f4b221d57214db85858b786a
Author: Rob Herring <robherring2@gmail.com>
Date:   Mon Oct 22 11:42:54 2012 -0600

    ARM: implement debug_ll_io_init()
    
    When using DEBUG_LL, the UART's (or other HW's) registers are mapped
    into early page tables based on the results of assembly macro addruart.
    Later, when the page tables are replaced, the same virtual address must
    remain valid. Historically, this has been ensured by using defines from
    <mach/iomap.h> in both the implementation of addruart, and the machine's
    .map_io() function. However, with the move to single zImage, we wish to
    remove <mach/iomap.h>. To enable this, the macro addruart may be used
    when constructing the late page tables too; addruart is exposed as a
    C function debug_ll_addr(), and used to set up the required mapping in
    debug_ll_io_init(), which may called on an opt-in basis from a machine's
    .map_io() function.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    [swarren: Mask map.virtual with PAGE_MASK. Checked for NULL results from
     debug_ll_addr (e.g. when selected UART isn't valid). Fixed compile when
     either !CONFIG_DEBUG_LL or CONFIG_DEBUG_SEMIHOSTING.]
    Signed-off-by: Stephen Warren <swarren@nvidia.com>
    Signed-off-by: Olof Johansson <olof@lixom.net>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 941dfb9e9a78..39719bb93caa 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -876,6 +876,22 @@ static void __init pci_reserve_io(void)
 #define pci_reserve_io() do { } while (0)
 #endif
 
+#ifdef CONFIG_DEBUG_LL
+void __init debug_ll_io_init(void)
+{
+	struct map_desc map;
+
+	debug_ll_addr(&map.pfn, &map.virtual);
+	if (!map.pfn || !map.virtual)
+		return;
+	map.pfn = __phys_to_pfn(map.pfn);
+	map.virtual &= PAGE_MASK;
+	map.length = PAGE_SIZE;
+	map.type = MT_DEVICE;
+	create_mapping(&map);
+}
+#endif
+
 static void * __initdata vmalloc_min =
 	(void *)(VMALLOC_END - (240 << 20) - VMALLOC_OFFSET);
 

commit ca41cc96b2813221b05af57d0355157924de5a07
Merge: 3151367f8778 461b6f0d3d7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 19:13:12 2012 -0700

    Merge branch 'for-v3.7' of git://git.linaro.org/people/mszyprowski/linux-dma-mapping
    
    Pull CMA and DMA-mapping updates from Marek Szyprowski:
     "This time the pull request is rather small, because the further
      redesign patches were not ready on time.
    
      This pull request consists of the patches which extend ARM DMA-mapping
      subsystem with support for CPU coherent (ACP) DMA busses.  The first
      client of the new version is HighBank SATA driver.  The second part of
      the pull request includes various cleanup for both CMA common code and
      ARM DMA-mapping subsystem."
    
    Fix up trivial add-add conflict due to the "dma-coherent" DT property
    being added next to the "calxeda,port-phys" property for the Calxeda
    AHCI controller.
    
    * 'for-v3.7' of git://git.linaro.org/people/mszyprowski/linux-dma-mapping:
      ARM: dma-mapping: Remove unsed var at arm_coherent_iommu_unmap_page
      ARM: highbank: add coherent DMA setup
      ARM: kill off arch_is_coherent
      ARM: add coherent iommu dma ops
      ARM: add coherent dma ops
      ARM: dma-mapping: Refrain noisy console message
      ARM: dma-mapping: Small logical clean up
      drivers: dma-contiguous: refactor dma_alloc_from_contiguous()

commit 48aa820f1e3824e46dde6251db98e5961abf605d
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Tue Aug 21 12:26:24 2012 +0200

    ARM: kill off arch_is_coherent
    
    With ixp2xxx removed, there are no platforms that define arch_is_coherent,
    so the last occurrences of arch_is_coherent can be removed. Any new
    platform with coherent i/o should use coherent dma mapping functions.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c2fa21d0103e..8fd039929ae8 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -216,7 +216,7 @@ static struct mem_type mem_types[] = {
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_WB,
 		.domain		= DOMAIN_IO,
-	},	
+	},
 	[MT_DEVICE_WC] = {	/* ioremap_wc */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_WC,
 		.prot_l1	= PMD_TYPE_TABLE,
@@ -421,17 +421,6 @@ static void __init build_mem_type_table(void)
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
 
-	/*
-	 * Enable CPU-specific coherency if supported.
-	 * (Only available on XSC3 at the moment.)
-	 */
-	if (arch_is_coherent() && cpu_is_xsc3()) {
-		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
-		mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
-		mem_types[MT_MEMORY_DMA_READY].prot_pte |= L_PTE_SHARED;
-		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
-		mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
-	}
 	/*
 	 * ARMv6 and above have extended page tables.
 	 */
@@ -777,8 +766,8 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		create_mapping(md);
 		vm->addr = (void *)(md->virtual & PAGE_MASK);
 		vm->size = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
-		vm->phys_addr = __pfn_to_phys(md->pfn); 
-		vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING; 
+		vm->phys_addr = __pfn_to_phys(md->pfn);
+		vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
 		vm->flags |= VM_ARM_MTYPE(md->type);
 		vm->caller = iotable_init;
 		vm_area_add_early(vm++);

commit 32dec75349da4e68b53f099ce3a96469cdc334d6
Merge: 740418ef19fd 9891e3240543
Author: Olof Johansson <olof@lixom.net>
Date:   Thu Sep 20 19:57:38 2012 -0700

    Merge tag 'tegra-for-3.7-dmaengine' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra into next/cleanup
    
    ARM: tegra: switch to dmaengine
    
    The Tegra code-base has contained both a legacy DMA and a dmaengine
    driver since v3.6-rcX. This series flips Tegra's defconfig to enable
    dmaengine rather than the legacy driver, and removes the legacy driver
    and all client code.
    
    * tag 'tegra-for-3.7-dmaengine' of git://git.kernel.org/pub/scm/linux/kernel/git/swarren/linux-tegra:
      ASoC: tegra: remove support of legacy DMA driver based access
      spi: tegra: remove support of legacy DMA driver based access
      ARM: tegra: apbio: remove support of legacy DMA driver based access
      ARM: tegra: dma: remove legacy APB DMA driver
      ARM: tegra: config: enable dmaengine based APB DMA driver
      + sync to 3.6-rc6

commit 863e99a8c1ea2b0391491904297f57a0f6a1fdd6
Merge: dd9bf78040fa a849088aa155
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue Sep 4 15:01:37 2012 +0200

    Merge commit 'a849088aa1' from rmk/fixes into cleanup/io-pci
    
    As Stephen Rothwell reports, a849088aa155 ("ARM: Fix ioremap() of
    address zero") from the arm-current tree and commit c2794437091a ("ARM:
    Add fixed PCI i/o mapping") from the arm-soc tree conflict in
    a nontrivial way in arch/arm/mm/mmu.c.
    
    Rob Herring explains:
    The PCI i/o reserved area has a dummy physical address of 0 and
    needs to be skipped by ioremap searches. So we don't set
    VM_ARM_STATIC_MAPPING to prevent matches by ioremap. The vm_struct
    settings don't really matter when we do the real mapping of the
    i/o space.
    
    Since commit a849088aa155 is at the start of the fixes branch
    in the arm tree, we can merge it into the branch that contains
    the other ioremap changes.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>

commit 36418c516b31bff4ff949c7c618430a1a514debe
Author: Jonathan Austin <Jonathan.Austin@arm.com>
Date:   Thu Aug 23 14:02:59 2012 +0100

    ARM: 7499/1: mm: Fix vmalloc overlap check for !HIGHMEM
    
    With !HIGHMEM, sanity_check_meminfo checks for banks that completely or
    partially overlap the vmalloc region. The test for partial overlap checks
    __va(bank->start + bank->size) > vmalloc_min. This is not appropriate if
    there is a non-linear translation between virtual and physical addresses,
    as bank->start + bank->size is actually in the bank following the one being
    interrogated.
    
    In most cases, even when using SPARSEMEM, this is not problematic as the
    subsequent bank will start at a higher va than the one in question. However
    if the physical to virtual address conversion is not monotonic increasing,
    the incorrect test could result in a bank not being truncated when it
    should be.
    
    This patch ensures we perform the va-pa conversion on memory from the
    bank we are interested in, not the following one.
    
    Reported-by: ??? (Steve) <zhanzhenbo@gmail.com>
    Signed-off-by: Jonathan Austin <jonathan.austin@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index eab94bc6f805..c2fa21d0103e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -961,8 +961,8 @@ void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would partially overlap
 		 * the vmalloc area.
 		 */
-		if (__va(bank->start + bank->size) > vmalloc_min ||
-		    __va(bank->start + bank->size) < __va(bank->start)) {
+		if (__va(bank->start + bank->size - 1) >= vmalloc_min ||
+		    __va(bank->start + bank->size - 1) <= __va(bank->start)) {
 			unsigned long newsize = vmalloc_min - __va(bank->start);
 			printk(KERN_NOTICE "Truncating RAM at %.8llx-%.8llx "
 			       "to -%.8llx (vmalloc region overlap).\n",

commit a849088aa1552b1a28eea3daff599ee22a734ae3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Aug 25 09:03:15 2012 +0100

    ARM: Fix ioremap() of address zero
    
    Murali Nalajala reports a regression that ioremapping address zero
    results in an oops dump:
    
    Unable to handle kernel paging request at virtual address fa200000
    pgd = d4f80000
    [fa200000] *pgd=00000000
    Internal error: Oops: 5 [#1] PREEMPT SMP ARM
    Modules linked in:
    CPU: 0    Tainted: G        W (3.4.0-g3b5f728-00009-g638207a #13)
    PC is at msm_pm_config_rst_vector_before_pc+0x8/0x30
    LR is at msm_pm_boot_config_before_pc+0x18/0x20
    pc : [<c0078f84>]    lr : [<c007903c>]    psr: a0000093
    sp : c0837ef0  ip : cfe00000  fp : 0000000d
    r10: da7efc17  r9 : 225c4278  r8 : 00000006
    r7 : 0003c000  r6 : c085c824  r5 : 00000001  r4 : fa101000
    r3 : fa200000  r2 : c095080c  r1 : 002250fc  r0 : 00000000
    Flags: NzCv  IRQs off  FIQs on  Mode SVC_32  ISA ARM Segment kernel
    Control: 10c5387d  Table: 25180059  DAC: 00000015
    [<c0078f84>] (msm_pm_config_rst_vector_before_pc+0x8/0x30) from [<c007903c>] (msm_pm_boot_config_before_pc+0x18/0x20)
    [<c007903c>] (msm_pm_boot_config_before_pc+0x18/0x20) from [<c007a55c>] (msm_pm_power_collapse+0x410/0xb04)
    [<c007a55c>] (msm_pm_power_collapse+0x410/0xb04) from [<c007b17c>] (arch_idle+0x294/0x3e0)
    [<c007b17c>] (arch_idle+0x294/0x3e0) from [<c000eed8>] (default_idle+0x18/0x2c)
    [<c000eed8>] (default_idle+0x18/0x2c) from [<c000f254>] (cpu_idle+0x90/0xe4)
    [<c000f254>] (cpu_idle+0x90/0xe4) from [<c057231c>] (rest_init+0x88/0xa0)
    [<c057231c>] (rest_init+0x88/0xa0) from [<c07ff890>] (start_kernel+0x3a8/0x40c)
    Code: c0704256 e12fff1e e59f2020 e5923000 (e5930000)
    
    This is caused by the 'reserved' entries which we insert (see
    19b52abe3c5d7 - ARM: 7438/1: fill possible PMD empty section gaps)
    which get matched for physical address zero.
    
    Resolve this by marking these reserved entries with a different flag.
    
    Cc: <stable@vger.kernel.org>
    Tested-by: Murali Nalajala <mnalajal@codeaurora.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4c2d0451e84a..eab94bc6f805 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -807,7 +807,7 @@ static void __init pmd_empty_section_gap(unsigned long addr)
 	vm = early_alloc_aligned(sizeof(*vm), __alignof__(*vm));
 	vm->addr = (void *)addr;
 	vm->size = SECTION_SIZE;
-	vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
+	vm->flags = VM_IOREMAP | VM_ARM_EMPTY_MAPPING;
 	vm->caller = pmd_empty_section_gap;
 	vm_area_add_early(vm);
 }
@@ -820,7 +820,7 @@ static void __init fill_pmd_gaps(void)
 
 	/* we're still single threaded hence no lock needed here */
 	for (vm = vmlist; vm; vm = vm->next) {
-		if (!(vm->flags & VM_ARM_STATIC_MAPPING))
+		if (!(vm->flags & (VM_ARM_STATIC_MAPPING | VM_ARM_EMPTY_MAPPING)))
 			continue;
 		addr = (unsigned long)vm->addr;
 		if (addr < next)

commit 91b006def384d8f07f9f324ab211fefe2b085c90
Merge: 28a33cbc24e4 ad722541147e d0a533b18235 ff081e05bfba 339ca09d7ada cb70706c822c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 27 23:06:32 2012 +0100

    Merge branches 'audit', 'delay', 'fixes', 'misc' and 'sta2x11' into for-linus

commit c2794437091a4fda72c4a4f3567dd728dcc0c3c9
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Wed Feb 29 18:10:58 2012 -0600

    ARM: Add fixed PCI i/o mapping
    
    This adds a fixed virtual mapping for PCI i/o addresses. The mapping is
    located at the last 2MB of vmalloc region (0xfee00000-0xff000000). 2MB
    is used to align with PMD size, but IO_SPACE_LIMIT is 1MB. The space
    is reserved after .map_io and can be mapped at any time later with
    pci_ioremap_io. Platforms which need early i/o mapping (e.g. for vga
    console) can call pci_map_io_early in their .map_io function.
    
    This has changed completely from the 1st implementation which only
    supported creating the static mapping at .map_io.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index cf4528d51774..714a7fd99ca3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -31,6 +31,7 @@
 
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
+#include <asm/mach/pci.h>
 
 #include "mm.h"
 
@@ -216,7 +217,7 @@ static struct mem_type mem_types[] = {
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_WB,
 		.domain		= DOMAIN_IO,
-	},	
+	},
 	[MT_DEVICE_WC] = {	/* ioremap_wc */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_WC,
 		.prot_l1	= PMD_TYPE_TABLE,
@@ -783,14 +784,27 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		create_mapping(md);
 		vm->addr = (void *)(md->virtual & PAGE_MASK);
 		vm->size = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
-		vm->phys_addr = __pfn_to_phys(md->pfn); 
-		vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING; 
+		vm->phys_addr = __pfn_to_phys(md->pfn);
+		vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
 		vm->flags |= VM_ARM_MTYPE(md->type);
 		vm->caller = iotable_init;
 		vm_area_add_early(vm++);
 	}
 }
 
+void __init vm_reserve_area_early(unsigned long addr, unsigned long size,
+				  void *caller)
+{
+	struct vm_struct *vm;
+
+	vm = early_alloc_aligned(sizeof(*vm), __alignof__(*vm));
+	vm->addr = (void *)addr;
+	vm->size = size;
+	vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
+	vm->caller = caller;
+	vm_area_add_early(vm);
+}
+
 #ifndef CONFIG_ARM_LPAE
 
 /*
@@ -808,14 +822,7 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 
 static void __init pmd_empty_section_gap(unsigned long addr)
 {
-	struct vm_struct *vm;
-
-	vm = early_alloc_aligned(sizeof(*vm), __alignof__(*vm));
-	vm->addr = (void *)addr;
-	vm->size = SECTION_SIZE;
-	vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
-	vm->caller = pmd_empty_section_gap;
-	vm_area_add_early(vm);
+	vm_reserve_area_early(addr, SECTION_SIZE, pmd_empty_section_gap);
 }
 
 static void __init fill_pmd_gaps(void)
@@ -864,6 +871,28 @@ static void __init fill_pmd_gaps(void)
 #define fill_pmd_gaps() do { } while (0)
 #endif
 
+#if defined(CONFIG_PCI) && !defined(CONFIG_NEED_MACH_IO_H)
+static void __init pci_reserve_io(void)
+{
+	struct vm_struct *vm;
+	unsigned long addr;
+
+	/* we're still single threaded hence no lock needed here */
+	for (vm = vmlist; vm; vm = vm->next) {
+		if (!(vm->flags & VM_ARM_STATIC_MAPPING))
+			continue;
+		addr = (unsigned long)vm->addr;
+		addr &= ~(SZ_2M - 1);
+		if (addr == PCI_IO_VIRT_BASE)
+			return;
+
+	}
+	vm_reserve_area_early(PCI_IO_VIRT_BASE, SZ_2M, pci_reserve_io);
+}
+#else
+#define pci_reserve_io() do { } while (0)
+#endif
+
 static void * __initdata vmalloc_min =
 	(void *)(VMALLOC_END - (240 << 20) - VMALLOC_OFFSET);
 
@@ -1147,6 +1176,9 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 		mdesc->map_io();
 	fill_pmd_gaps();
 
+	/* Reserve fixed i/o space in VMALLOC region */
+	pci_reserve_io();
+
 	/*
 	 * Finally flush the caches and tlb to ensure that we're in a
 	 * consistent state wrt the writebuffer.  This also ensures that

commit 9ad86ddde0345e903eb74837e52cf19fe0c7f825
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Jun 25 14:59:38 2012 +0100

    ARM: 7436/1: Do not map the vectors page as write-through on UP systems
    
    The vectors page has been traditionally mapped as WT on UP systems but
    this creates a mismatched alias with the directly mapped RAM that is
    using WB attributes. On newer processors like Cortex-A15 this has
    implications on the data/instructions coherency at the point of
    unification (usually L2).
    
    This patch removes such restriction.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e5dad60b558b..f37dc1856a69 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -421,12 +421,6 @@ static void __init build_mem_type_table(void)
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
 
-	/*
-	 * Only use write-through for non-SMP systems
-	 */
-	if (!is_smp() && cpu_arch >= CPU_ARCH_ARMv5 && cachepolicy > CPOLICY_WRITETHROUGH)
-		vecs_pgprot = cache_policies[CPOLICY_WRITETHROUGH].pte;
-
 	/*
 	 * Enable CPU-specific coherency if supported.
 	 * (Only available on XSC3 at the moment.)

commit 19b52abe3c5d759661500a1dc810924369b2ad46
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jun 27 17:28:57 2012 +0100

    ARM: 7438/1: fill possible PMD empty section gaps
    
    On ARM with the 2-level page table format, a PMD entry is represented by
    two consecutive section entries covering 2MB of virtual space.
    
    However, static mappings always were allowed to use separate 1MB section
    entries.  This means in practice that a static mapping may create half
    populated PMDs via create_mapping().
    
    Since commit 0536bdf33f (ARM: move iotable mappings within the vmalloc
    region) those static mappings are located in the vmalloc area. We must
    ensure no such half populated PMDs are accessible once vmalloc() or
    ioremap() start looking at the vmalloc area for nearby free virtual
    address ranges, or various things leading to a kernel crash will happen.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reported-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: "R, Sricharan" <r.sricharan@ti.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e5dad60b558b..cf4528d51774 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -791,6 +791,79 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 	}
 }
 
+#ifndef CONFIG_ARM_LPAE
+
+/*
+ * The Linux PMD is made of two consecutive section entries covering 2MB
+ * (see definition in include/asm/pgtable-2level.h).  However a call to
+ * create_mapping() may optimize static mappings by using individual
+ * 1MB section mappings.  This leaves the actual PMD potentially half
+ * initialized if the top or bottom section entry isn't used, leaving it
+ * open to problems if a subsequent ioremap() or vmalloc() tries to use
+ * the virtual space left free by that unused section entry.
+ *
+ * Let's avoid the issue by inserting dummy vm entries covering the unused
+ * PMD halves once the static mappings are in place.
+ */
+
+static void __init pmd_empty_section_gap(unsigned long addr)
+{
+	struct vm_struct *vm;
+
+	vm = early_alloc_aligned(sizeof(*vm), __alignof__(*vm));
+	vm->addr = (void *)addr;
+	vm->size = SECTION_SIZE;
+	vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING;
+	vm->caller = pmd_empty_section_gap;
+	vm_area_add_early(vm);
+}
+
+static void __init fill_pmd_gaps(void)
+{
+	struct vm_struct *vm;
+	unsigned long addr, next = 0;
+	pmd_t *pmd;
+
+	/* we're still single threaded hence no lock needed here */
+	for (vm = vmlist; vm; vm = vm->next) {
+		if (!(vm->flags & VM_ARM_STATIC_MAPPING))
+			continue;
+		addr = (unsigned long)vm->addr;
+		if (addr < next)
+			continue;
+
+		/*
+		 * Check if this vm starts on an odd section boundary.
+		 * If so and the first section entry for this PMD is free
+		 * then we block the corresponding virtual address.
+		 */
+		if ((addr & ~PMD_MASK) == SECTION_SIZE) {
+			pmd = pmd_off_k(addr);
+			if (pmd_none(*pmd))
+				pmd_empty_section_gap(addr & PMD_MASK);
+		}
+
+		/*
+		 * Then check if this vm ends on an odd section boundary.
+		 * If so and the second section entry for this PMD is empty
+		 * then we block the corresponding virtual address.
+		 */
+		addr += vm->size;
+		if ((addr & ~PMD_MASK) == SECTION_SIZE) {
+			pmd = pmd_off_k(addr) + 1;
+			if (pmd_none(*pmd))
+				pmd_empty_section_gap(addr);
+		}
+
+		/* no need to look at any vm entry until we hit the next PMD */
+		next = (addr + PMD_SIZE - 1) & PMD_MASK;
+	}
+}
+
+#else
+#define fill_pmd_gaps() do { } while (0)
+#endif
+
 static void * __initdata vmalloc_min =
 	(void *)(VMALLOC_END - (240 << 20) - VMALLOC_OFFSET);
 
@@ -1072,6 +1145,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 */
 	if (mdesc->map_io)
 		mdesc->map_io();
+	fill_pmd_gaps();
 
 	/*
 	 * Finally flush the caches and tlb to ensure that we're in a

commit 158e8bfe802f730f9ea7cde32eee8b43285bdd4a
Author: Alessandro Rubini <rubini@gnudd.com>
Date:   Sun Jun 24 12:46:26 2012 +0100

    ARM: 7432/1: use the new linux/sizes.h
    
    Signed-off-by: Alessandro Rubini <rubini@gnudd.com>
    Acked-by: Giancarlo Asnaghi <giancarlo.asnaghi@st.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Cc: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e5dad60b558b..2196116c882f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -16,13 +16,13 @@
 #include <linux/memblock.h>
 #include <linux/fs.h>
 #include <linux/vmalloc.h>
+#include <linux/sizes.h>
 
 #include <asm/cp15.h>
 #include <asm/cputype.h>
 #include <asm/sections.h>
 #include <asm/cachetype.h>
 #include <asm/setup.h>
-#include <asm/sizes.h>
 #include <asm/smp_plat.h>
 #include <asm/tlb.h>
 #include <asm/highmem.h>

commit c79095092834a18ae74cfc08def1a5a101dc106c
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Dec 29 13:09:51 2011 +0100

    ARM: integrate CMA with DMA-mapping subsystem
    
    This patch adds support for CMA to dma-mapping subsystem for ARM
    architecture. By default a global CMA area is used, but specific devices
    are allowed to have their private memory areas if required (they can be
    created with dma_declare_contiguous() function during board
    initialisation).
    
    Contiguous memory areas reserved for DMA are remapped with 2-level page
    tables on boot. Once a buffer is requested, a low memory kernel mapping
    is updated to to match requested memory access type.
    
    GFP_ATOMIC allocations are performed from special pool which is created
    early during boot. This way remapping page attributes is not needed on
    allocation time.
    
    CMA has been enabled unconditionally for ARMv6+ systems.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    CC: Michal Nazarewicz <mina86@mina86.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Tested-by: Rob Clark <rob.clark@linaro.org>
    Tested-by: Ohad Ben-Cohen <ohad@wizery.com>
    Tested-by: Benjamin Gaignard <benjamin.gaignard@linaro.org>
    Tested-by: Robert Nelson <robertcnelson@gmail.com>
    Tested-by: Barry Song <Baohua.Song@csr.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index aa78de8bfdd3..e5dad60b558b 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -288,6 +288,11 @@ static struct mem_type mem_types[] = {
 				PMD_SECT_UNCACHED | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
 	},
+	[MT_MEMORY_DMA_READY] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.domain    = DOMAIN_KERNEL,
+	},
 };
 
 const struct mem_type *get_mem_type(unsigned int type)
@@ -429,6 +434,7 @@ static void __init build_mem_type_table(void)
 	if (arch_is_coherent() && cpu_is_xsc3()) {
 		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
 		mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
+		mem_types[MT_MEMORY_DMA_READY].prot_pte |= L_PTE_SHARED;
 		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
 		mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
 	}
@@ -460,6 +466,7 @@ static void __init build_mem_type_table(void)
 			mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
 			mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_MEMORY_DMA_READY].prot_pte |= L_PTE_SHARED;
 			mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
 			mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
 		}
@@ -512,6 +519,7 @@ static void __init build_mem_type_table(void)
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_MEMORY].prot_sect |= ecc_mask | cp->pmd;
 	mem_types[MT_MEMORY].prot_pte |= kern_pgprot;
+	mem_types[MT_MEMORY_DMA_READY].prot_pte |= kern_pgprot;
 	mem_types[MT_MEMORY_NONCACHED].prot_sect |= ecc_mask;
 	mem_types[MT_ROM].prot_sect |= cp->pmd;
 
@@ -596,7 +604,7 @@ static void __init alloc_init_section(pud_t *pud, unsigned long addr,
 	 * L1 entries, whereas PGDs refer to a group of L1 entries making
 	 * up one logical pointer to an L2 table.
 	 */
-	if (((addr | end | phys) & ~SECTION_MASK) == 0) {
+	if (type->prot_sect && ((addr | end | phys) & ~SECTION_MASK) == 0) {
 		pmd_t *p = pmd;
 
 #ifndef CONFIG_ARM_LPAE
@@ -814,7 +822,7 @@ static int __init early_vmalloc(char *arg)
 }
 early_param("vmalloc", early_vmalloc);
 
-static phys_addr_t lowmem_limit __initdata = 0;
+phys_addr_t arm_lowmem_limit __initdata = 0;
 
 void __init sanity_check_meminfo(void)
 {
@@ -897,8 +905,8 @@ void __init sanity_check_meminfo(void)
 			bank->size = newsize;
 		}
 #endif
-		if (!bank->highmem && bank->start + bank->size > lowmem_limit)
-			lowmem_limit = bank->start + bank->size;
+		if (!bank->highmem && bank->start + bank->size > arm_lowmem_limit)
+			arm_lowmem_limit = bank->start + bank->size;
 
 		j++;
 	}
@@ -923,8 +931,8 @@ void __init sanity_check_meminfo(void)
 	}
 #endif
 	meminfo.nr_banks = j;
-	high_memory = __va(lowmem_limit - 1) + 1;
-	memblock_set_current_limit(lowmem_limit);
+	high_memory = __va(arm_lowmem_limit - 1) + 1;
+	memblock_set_current_limit(arm_lowmem_limit);
 }
 
 static inline void prepare_page_table(void)
@@ -949,8 +957,8 @@ static inline void prepare_page_table(void)
 	 * Find the end of the first block of lowmem.
 	 */
 	end = memblock.memory.regions[0].base + memblock.memory.regions[0].size;
-	if (end >= lowmem_limit)
-		end = lowmem_limit;
+	if (end >= arm_lowmem_limit)
+		end = arm_lowmem_limit;
 
 	/*
 	 * Clear out all the kernel space mappings, except for the first
@@ -1093,8 +1101,8 @@ static void __init map_lowmem(void)
 		phys_addr_t end = start + reg->size;
 		struct map_desc map;
 
-		if (end > lowmem_limit)
-			end = lowmem_limit;
+		if (end > arm_lowmem_limit)
+			end = arm_lowmem_limit;
 		if (start >= end)
 			break;
 
@@ -1115,11 +1123,12 @@ void __init paging_init(struct machine_desc *mdesc)
 {
 	void *zero_page;
 
-	memblock_set_current_limit(lowmem_limit);
+	memblock_set_current_limit(arm_lowmem_limit);
 
 	build_mem_type_table();
 	prepare_page_table();
 	map_lowmem();
+	dma_contiguous_remap();
 	devicemaps_init(mdesc);
 	kmap_init();
 

commit 1a3abcf41f13666d4ed241c8cc7f48bd38e7b543
Author: Vitaly Andrianov <vitalya@ti.com>
Date:   Tue May 15 15:01:16 2012 +0100

    ARM: 7418/1: LPAE: fix access flag setup in mem_type_table
    
    A zero value for prot_sect in the memory types table implies that
    section mappings should never be created for the memory type in question.
    This is checked for in alloc_init_section().
    
    With LPAE, we set a bit to mask access flag faults for kernel mappings.
    This breaks the aforementioned (!prot_sect) check in alloc_init_section().
    
    This patch fixes this bug by first checking for a non-zero
    prot_sect before setting the PMD_SECT_AF flag.
    
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 2c7cf2f9c837..aa78de8bfdd3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -489,7 +489,8 @@ static void __init build_mem_type_table(void)
 	 */
 	for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
 		mem_types[i].prot_pte |= PTE_EXT_AF;
-		mem_types[i].prot_sect |= PMD_SECT_AF;
+		if (mem_types[i].prot_sect)
+			mem_types[i].prot_sect |= PMD_SECT_AF;
 	}
 	kern_pgprot |= PTE_EXT_AF;
 	vecs_pgprot |= PTE_EXT_AF;

commit 14904927fcef6bb881fd995b478a0d2e700c1818
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Fri Apr 27 01:40:10 2012 +0100

    ARM: 7401/1: mm: Fix section mismatches
    
    WARNING: vmlinux.o(.text+0x111b8): Section mismatch in reference
    from the function arm_memory_present() to the function
    .init.text:memory_present()
    The function arm_memory_present() references
    the function __init memory_present().
    This is often because arm_memory_present lacks a __init
    annotation or the annotation of memory_present is wrong.
    
    WARNING: arch/arm/mm/built-in.o(.text+0x1edc): Section mismatch
    in reference from the function alloc_init_pud() to the function
    .init.text:alloc_init_section()
    The function alloc_init_pud() references
    the function __init alloc_init_section().
    This is often because alloc_init_pud lacks a __init
    annotation or the annotation of alloc_init_section is wrong.
    
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b86f8933ff91..2c7cf2f9c837 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -618,8 +618,8 @@ static void __init alloc_init_section(pud_t *pud, unsigned long addr,
 	}
 }
 
-static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,
-	unsigned long phys, const struct mem_type *type)
+static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
+	unsigned long end, unsigned long phys, const struct mem_type *type)
 {
 	pud_t *pud = pud_offset(pgd, addr);
 	unsigned long next;

commit 12679a2d7e3bfbdc7586e3e86d1ca90c46659363
Merge: 1c036588772d b0df89868006
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 16:53:48 2012 -0700

    Merge branch 'for-linus' of git://git.linaro.org/people/rmk/linux-arm
    
    Pull more ARM updates from Russell King.
    
    This got a fair number of conflicts with the <asm/system.h> split, but
    also with some other sparse-irq and header file include cleanups.  They
    all looked pretty trivial, though.
    
    * 'for-linus' of git://git.linaro.org/people/rmk/linux-arm: (59 commits)
      ARM: fix Kconfig warning for HAVE_BPF_JIT
      ARM: 7361/1: provide XIP_VIRT_ADDR for no-MMU builds
      ARM: 7349/1: integrator: convert to sparse irqs
      ARM: 7259/3: net: JIT compiler for packet filters
      ARM: 7334/1: add jump label support
      ARM: 7333/2: jump label: detect %c support for ARM
      ARM: 7338/1: add support for early console output via semihosting
      ARM: use set_current_blocked() and block_sigmask()
      ARM: exec: remove redundant set_fs(USER_DS)
      ARM: 7332/1: extract out code patch function from kprobes
      ARM: 7331/1: extract out insn generation code from ftrace
      ARM: 7330/1: ftrace: use canonical Thumb-2 wide instruction format
      ARM: 7351/1: ftrace: remove useless memory checks
      ARM: 7316/1: kexec: EOI active and mask all interrupts in kexec crash path
      ARM: Versatile Express: add NO_IOPORT
      ARM: get rid of asm/irq.h in asm/prom.h
      ARM: 7319/1: Print debug info for SIGBUS in user faults
      ARM: 7318/1: gic: refactor irq_start assignment
      ARM: 7317/1: irq: avoid NULL check in for_each_irq_desc loop
      ARM: 7315/1: perf: add support for the Cortex-A7 PMU
      ...

commit 9f97da78bf018206fb623cd351d454af2f105fe0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:01 2012 +0100

    Disintegrate asm/system.h for ARM
    
    Disintegrate asm/system.h for ARM.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Russell King <linux@arm.linux.org.uk>
    cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 163a69af1fbf..cd439c1dd506 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -26,6 +26,7 @@
 #include <asm/smp_plat.h>
 #include <asm/tlb.h>
 #include <asm/highmem.h>
+#include <asm/system_info.h>
 #include <asm/traps.h>
 
 #include <asm/mach/arch.h>

commit 15d07dc9c59eae51219c40253bdf920f62bb10f2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Mar 28 18:30:01 2012 +0100

    ARM: move CP15 definitions to separate header file
    
    Avoid namespace conflicts with drivers over the CP15 definitions by
    moving CP15 related prototypes and definitions to a private header
    file.
    
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> [Tegra]
    Acked-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com> [EP93xx]
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 94c5a0c94f5e..163a69af1fbf 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -17,6 +17,7 @@
 #include <linux/fs.h>
 #include <linux/vmalloc.h>
 
+#include <asm/cp15.h>
 #include <asm/cputype.h>
 #include <asm/sections.h>
 #include <asm/cachetype.h>

commit 195864cf3d6f5b6b743793bda3aaa2ff65d322ae
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jan 19 10:05:41 2012 +0000

    ARM: move CP15 definitions to separate header file
    
    Avoid namespace conflicts with drivers over the CP15 definitions by
    moving CP15 related prototypes and definitions to a private header
    file.
    
    Acked-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> [Tegra]
    Acked-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com> [EP93xx]
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c1263adc2a26..f77f1dbbdf59 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -17,6 +17,7 @@
 #include <linux/fs.h>
 #include <linux/vmalloc.h>
 
+#include <asm/cp15.h>
 #include <asm/cputype.h>
 #include <asm/sections.h>
 #include <asm/cachetype.h>

commit 94e5a85b3be0ce109d26aa6812b2a02c518a0e4b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jan 18 15:32:49 2012 +0000

    ARM: earlier initialization of vectors page
    
    Initialize the contents of the vectors page immediately after we
    allocate the page, but before we map it.  This avoids any possible
    aliases with other mappings which may need to be flushed after the
    page has been mapped irrespective of the cache type.
    
    We follow this later with a flush_cache_all() after all static memory
    mappings have been initialized, which ensures that this is safe from
    any cache effects.
    
    Tested-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 94c5a0c94f5e..c1263adc2a26 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -997,11 +997,14 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 {
 	struct map_desc map;
 	unsigned long addr;
+	void *vectors;
 
 	/*
 	 * Allocate the vector page early.
 	 */
-	vectors_page = early_alloc(PAGE_SIZE);
+	vectors = early_alloc(PAGE_SIZE);
+
+	early_trap_init(vectors);
 
 	for (addr = VMALLOC_START; addr; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
@@ -1041,7 +1044,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 * location (0xffff0000).  If we aren't using high-vectors, also
 	 * create a mapping at the low-vectors virtual address.
 	 */
-	map.pfn = __phys_to_pfn(virt_to_phys(vectors_page));
+	map.pfn = __phys_to_pfn(virt_to_phys(vectors));
 	map.virtual = 0xffff0000;
 	map.length = PAGE_SIZE;
 	map.type = MT_HIGH_VECTORS;

commit 6ae25a5b9d7ba86d6ac19c403dfa57dae6caa73d
Merge: 3ee0fc5ca129 497b7e943d0d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Dec 8 18:02:04 2011 +0000

    Merge branch 'for-rmk' of git://git.kernel.org/pub/scm/linux/kernel/git/cmarinas/linux into devel-stable
    
    Conflicts:
            arch/arm/mm/ioremap.c

commit 77f73a2c8e869b035e71eea5cae07c30fe4bded0
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 22 17:30:32 2011 +0000

    ARM: LPAE: mark memory banks with start > ULONG_MAX as highmem
    
    Memory banks living outside of the 32-bit physical address
    space do not have a 1:1 pa <-> va mapping and therefore the
    __va macro may wrap.
    
    This patch ensures that such banks are marked as highmem so
    that the Kernel doesn't try to split them up when it sees that
    the wrapped virtual address overlaps the vmalloc space.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 1935311e17fc..b836d6b2258b 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -799,6 +799,9 @@ void __init sanity_check_meminfo(void)
 		struct membank *bank = &meminfo.bank[j];
 		*bank = meminfo.bank[i];
 
+		if (bank->start > ULONG_MAX)
+			highmem = 1;
+
 #ifdef CONFIG_HIGHMEM
 		if (__va(bank->start) >= vmalloc_min ||
 		    __va(bank->start) < (void *)PAGE_OFFSET)
@@ -810,7 +813,7 @@ void __init sanity_check_meminfo(void)
 		 * Split those memory banks which are partially overlapping
 		 * the vmalloc area greatly simplifying things later.
 		 */
-		if (__va(bank->start) < vmalloc_min &&
+		if (!highmem && __va(bank->start) < vmalloc_min &&
 		    bank->size > vmalloc_min - __va(bank->start)) {
 			if (meminfo.nr_banks >= NR_BANKS) {
 				printk(KERN_CRIT "NR_BANKS too low, "
@@ -830,6 +833,17 @@ void __init sanity_check_meminfo(void)
 #else
 		bank->highmem = highmem;
 
+		/*
+		 * Highmem banks not allowed with !CONFIG_HIGHMEM.
+		 */
+		if (highmem) {
+			printk(KERN_NOTICE "Ignoring RAM at %.8llx-%.8llx "
+			       "(!CONFIG_HIGHMEM).\n",
+			       (unsigned long long)bank->start,
+			       (unsigned long long)bank->start + bank->size - 1);
+			continue;
+		}
+
 		/*
 		 * Check whether this memory bank would entirely overlap
 		 * the vmalloc area.

commit 1b6ba46b7efa31055eb993a6f2c6bbcb8b35b001
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Nov 22 17:30:29 2011 +0000

    ARM: LPAE: MMU setup for the 3-level page table format
    
    This patch adds the MMU initialisation for the LPAE page table format.
    The swapper_pg_dir size with LPAE is 5 rather than 4 pages. A new
    proc-v7-3level.S file contains the TTB initialisation, context switch
    and PTE setting code with the LPAE. The TTBRx split is based on the
    PAGE_OFFSET with TTBR1 used for the kernel mappings. The 36-bit mappings
    (supersections) and a few other memory types in mmu.c are conditionally
    compiled.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index dc8c550e6cbd..1935311e17fc 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -150,6 +150,7 @@ static int __init early_nowrite(char *__unused)
 }
 early_param("nowb", early_nowrite);
 
+#ifndef CONFIG_ARM_LPAE
 static int __init early_ecc(char *p)
 {
 	if (memcmp(p, "on", 2) == 0)
@@ -159,6 +160,7 @@ static int __init early_ecc(char *p)
 	return 0;
 }
 early_param("ecc", early_ecc);
+#endif
 
 static int __init noalign_setup(char *__unused)
 {
@@ -228,10 +230,12 @@ static struct mem_type mem_types[] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
 	},
+#ifndef CONFIG_ARM_LPAE
 	[MT_MINICLEAN] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_MINICACHE,
 		.domain    = DOMAIN_KERNEL,
 	},
+#endif
 	[MT_LOW_VECTORS] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_RDONLY,
@@ -429,6 +433,7 @@ static void __init build_mem_type_table(void)
 	 * ARMv6 and above have extended page tables.
 	 */
 	if (cpu_arch >= CPU_ARCH_ARMv6 && (cr & CR_XP)) {
+#ifndef CONFIG_ARM_LPAE
 		/*
 		 * Mark cache clean areas and XIP ROM read only
 		 * from SVC mode and no access from userspace.
@@ -436,6 +441,7 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_ROM].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 		mem_types[MT_MINICLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
+#endif
 
 		if (is_smp()) {
 			/*
@@ -474,6 +480,18 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_BUFFERABLE;
 	}
 
+#ifdef CONFIG_ARM_LPAE
+	/*
+	 * Do not generate access flag faults for the kernel mappings.
+	 */
+	for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
+		mem_types[i].prot_pte |= PTE_EXT_AF;
+		mem_types[i].prot_sect |= PMD_SECT_AF;
+	}
+	kern_pgprot |= PTE_EXT_AF;
+	vecs_pgprot |= PTE_EXT_AF;
+#endif
+
 	for (i = 0; i < 16; i++) {
 		unsigned long v = pgprot_val(protection_map[i]);
 		protection_map[i] = __pgprot(v | user_pgprot);
@@ -572,8 +590,10 @@ static void __init alloc_init_section(pud_t *pud, unsigned long addr,
 	if (((addr | end | phys) & ~SECTION_MASK) == 0) {
 		pmd_t *p = pmd;
 
+#ifndef CONFIG_ARM_LPAE
 		if (addr & SECTION_SIZE)
 			pmd++;
+#endif
 
 		do {
 			*pmd = __pmd(phys | type->prot_sect);
@@ -603,6 +623,7 @@ static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,
 	} while (pud++, addr = next, addr != end);
 }
 
+#ifndef CONFIG_ARM_LPAE
 static void __init create_36bit_mapping(struct map_desc *md,
 					const struct mem_type *type)
 {
@@ -662,6 +683,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
 		pgd += SUPERSECTION_SIZE >> PGDIR_SHIFT;
 	} while (addr != end);
 }
+#endif	/* !CONFIG_ARM_LPAE */
 
 /*
  * Create the page directory entries and any necessary
@@ -693,6 +715,7 @@ static void __init create_mapping(struct map_desc *md)
 
 	type = &mem_types[md->type];
 
+#ifndef CONFIG_ARM_LPAE
 	/*
 	 * Catch 36-bit addresses
 	 */
@@ -700,6 +723,7 @@ static void __init create_mapping(struct map_desc *md)
 		create_36bit_mapping(md, type);
 		return;
 	}
+#endif
 
 	addr = md->virtual & PAGE_MASK;
 	phys = __pfn_to_phys(md->pfn);
@@ -897,7 +921,13 @@ static inline void prepare_page_table(void)
 		pmd_clear(pmd_off_k(addr));
 }
 
+#ifdef CONFIG_ARM_LPAE
+/* the first page is reserved for pgd */
+#define SWAPPER_PG_DIR_SIZE	(PAGE_SIZE + \
+				 PTRS_PER_PGD * PTRS_PER_PMD * sizeof(pmd_t))
+#else
 #define SWAPPER_PG_DIR_SIZE	(PTRS_PER_PGD * sizeof(pgd_t))
+#endif
 
 /*
  * Reserve the special regions of memory

commit 576d2f2525612ecb5af029a76f21f22a3b82563d
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Sep 16 01:14:23 2011 -0400

    ARM: add generic ioremap optimization by reusing static mappings
    
    Now that we have all the static mappings from iotable_init() located
    in the vmalloc area, it is trivial to optimize ioremap by reusing those
    static mappings when the requested physical area fits in one of them,
    and so in a generic way for all platforms.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c61481577ae1..27e366af67f9 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -749,7 +749,8 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		vm->addr = (void *)(md->virtual & PAGE_MASK);
 		vm->size = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
 		vm->phys_addr = __pfn_to_phys(md->pfn); 
-		vm->flags = VM_IOREMAP;
+		vm->flags = VM_IOREMAP | VM_ARM_STATIC_MAPPING; 
+		vm->flags |= VM_ARM_MTYPE(md->type);
 		vm->caller = iotable_init;
 		vm_area_add_early(vm++);
 	}

commit 0536bdf33faff4d940ac094c77998cfac368cfff
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Aug 25 00:35:59 2011 -0400

    ARM: move iotable mappings within the vmalloc region
    
    In order to remove the build time variation between different SOCs with
    regards to VMALLOC_END, the iotable mappings are now allocated inside
    the vmalloc region.  This allows for VMALLOC_END to be identical across
    all machines.
    
    The value for VMALLOC_END is now set to 0xff000000 which is right where
    the consistent DMA area starts.
    
    To accommodate all static mappings on machines with possible highmem usage,
    the default vmalloc area size is changed to 240 MB so that VMALLOC_START
    is no higher than 0xf0000000 by default.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Stephen Warren <swarren@nvidia.com>
    Tested-by: Kevin Hilman <khilman@ti.com>
    Tested-by: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 0aa8b7d5b21d..c61481577ae1 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -15,6 +15,7 @@
 #include <linux/nodemask.h>
 #include <linux/memblock.h>
 #include <linux/fs.h>
+#include <linux/vmalloc.h>
 
 #include <asm/cputype.h>
 #include <asm/sections.h>
@@ -529,13 +530,18 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 
 #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
 
-static void __init *early_alloc(unsigned long sz)
+static void __init *early_alloc_aligned(unsigned long sz, unsigned long align)
 {
-	void *ptr = __va(memblock_alloc(sz, sz));
+	void *ptr = __va(memblock_alloc(sz, align));
 	memset(ptr, 0, sz);
 	return ptr;
 }
 
+static void __init *early_alloc(unsigned long sz)
+{
+	return early_alloc_aligned(sz, sz);
+}
+
 static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
 {
 	if (pmd_none(*pmd)) {
@@ -685,9 +691,10 @@ static void __init create_mapping(struct map_desc *md)
 	}
 
 	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
-	    md->virtual >= PAGE_OFFSET && md->virtual < VMALLOC_END) {
+	    md->virtual >= PAGE_OFFSET &&
+	    (md->virtual < VMALLOC_START || md->virtual >= VMALLOC_END)) {
 		printk(KERN_WARNING "BUG: mapping for 0x%08llx"
-		       " at 0x%08lx overlaps vmalloc space\n",
+		       " at 0x%08lx out of vmalloc space\n",
 		       (long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 	}
 
@@ -729,18 +736,32 @@ static void __init create_mapping(struct map_desc *md)
  */
 void __init iotable_init(struct map_desc *io_desc, int nr)
 {
-	int i;
+	struct map_desc *md;
+	struct vm_struct *vm;
+
+	if (!nr)
+		return;
 
-	for (i = 0; i < nr; i++)
-		create_mapping(io_desc + i);
+	vm = early_alloc_aligned(sizeof(*vm) * nr, __alignof__(*vm));
+
+	for (md = io_desc; nr; md++, nr--) {
+		create_mapping(md);
+		vm->addr = (void *)(md->virtual & PAGE_MASK);
+		vm->size = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
+		vm->phys_addr = __pfn_to_phys(md->pfn); 
+		vm->flags = VM_IOREMAP;
+		vm->caller = iotable_init;
+		vm_area_add_early(vm++);
+	}
 }
 
-static void * __initdata vmalloc_min = (void *)(VMALLOC_END - SZ_128M);
+static void * __initdata vmalloc_min =
+	(void *)(VMALLOC_END - (240 << 20) - VMALLOC_OFFSET);
 
 /*
  * vmalloc=size forces the vmalloc area to be exactly 'size'
  * bytes. This can be used to increase (or decrease) the vmalloc
- * area - the default is 128m.
+ * area - the default is 240m.
  */
 static int __init early_vmalloc(char *arg)
 {
@@ -891,10 +912,10 @@ static inline void prepare_page_table(void)
 
 	/*
 	 * Clear out all the kernel space mappings, except for the first
-	 * memory bank, up to the end of the vmalloc region.
+	 * memory bank, up to the vmalloc region.
 	 */
 	for (addr = __phys_to_virt(end);
-	     addr < VMALLOC_END; addr += PMD_SIZE)
+	     addr < VMALLOC_START; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 }
 
@@ -921,8 +942,8 @@ void __init arm_mm_memblock_reserve(void)
 }
 
 /*
- * Set up device the mappings.  Since we clear out the page tables for all
- * mappings above VMALLOC_END, we will remove any debug device mappings.
+ * Set up the device mappings.  Since we clear out the page tables for all
+ * mappings above VMALLOC_START, we will remove any debug device mappings.
  * This means you have to be careful how you debug this function, or any
  * called function.  This means you can't use any function or debugging
  * method which may touch any device, otherwise the kernel _will_ crash.
@@ -937,7 +958,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 */
 	vectors_page = early_alloc(PAGE_SIZE);
 
-	for (addr = VMALLOC_END; addr; addr += PMD_SIZE)
+	for (addr = VMALLOC_START; addr; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
 	/*

commit 55a8173cfe1c6b489f8f5705282c762aed2e265e
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sun Sep 18 22:40:00 2011 -0400

    ARM: move initialization of the high_memory variable earlier
    
    Some upcoming changes must know the VMALLOC_START value, which is based
    on high_memory, before bootmem_init() is called.
    
    The best location to set it is in sanity_check_meminfo() where the needed
    computation is already done, and in the non MMU case it is trivial to do
    now that the meminfo array is already sorted at that point.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index dc8c550e6cbd..0aa8b7d5b21d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -860,6 +860,7 @@ void __init sanity_check_meminfo(void)
 	}
 #endif
 	meminfo.nr_banks = j;
+	high_memory = __va(lowmem_limit - 1) + 1;
 	memblock_set_current_limit(lowmem_limit);
 }
 

commit 1fdb24e969110fafea36d3b393bea438f702c87f
Merge: f362f98e7c44 531a6a941745
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 28 12:02:27 2011 -0700

    Merge branch 'devel-stable' of http://ftp.arm.linux.org.uk/pub/linux/arm/kernel/git-cur/linux-2.6-arm
    
    * 'devel-stable' of http://ftp.arm.linux.org.uk/pub/linux/arm/kernel/git-cur/linux-2.6-arm: (178 commits)
      ARM: 7139/1: fix compilation with CONFIG_ARM_ATAG_DTB_COMPAT and large TEXT_OFFSET
      ARM: gic, local timers: use the request_percpu_irq() interface
      ARM: gic: consolidate PPI handling
      ARM: switch from NO_MACH_MEMORY_H to NEED_MACH_MEMORY_H
      ARM: mach-s5p64x0: remove mach/memory.h
      ARM: mach-s3c64xx: remove mach/memory.h
      ARM: plat-mxc: remove mach/memory.h
      ARM: mach-prima2: remove mach/memory.h
      ARM: mach-zynq: remove mach/memory.h
      ARM: mach-bcmring: remove mach/memory.h
      ARM: mach-davinci: remove mach/memory.h
      ARM: mach-pxa: remove mach/memory.h
      ARM: mach-ixp4xx: remove mach/memory.h
      ARM: mach-h720x: remove mach/memory.h
      ARM: mach-vt8500: remove mach/memory.h
      ARM: mach-s5pc100: remove mach/memory.h
      ARM: mach-tegra: remove mach/memory.h
      ARM: plat-tcc: remove mach/memory.h
      ARM: mach-mmp: remove mach/memory.h
      ARM: mach-cns3xxx: remove mach/memory.h
      ...
    
    Fix up mostly pretty trivial conflicts in:
     - arch/arm/Kconfig
     - arch/arm/include/asm/localtimer.h
     - arch/arm/kernel/Makefile
     - arch/arm/mach-shmobile/board-ap4evb.c
     - arch/arm/mach-u300/core.c
     - arch/arm/mm/dma-mapping.c
     - arch/arm/mm/proc-v7.S
     - arch/arm/plat-omap/Kconfig
    largely due to some CONFIG option renaming (ie CONFIG_PM_SLEEP ->
    CONFIG_ARM_CPU_SUSPEND for the arm-specific suspend code etc) and
    addition of NEED_MACH_MEMORY_H next to HAVE_IDE.

commit 442e70c0b3536e832547eed89629435462f4b515
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 5 17:51:56 2011 +0100

    ARM: 7076/1: LPAE: Add (pte|pmd)val_t type definitions as u32
    
    This patch defines the (pte|pmd)val_t as u32 and changes the page table
    types to be based on these. The PMD bits are converted to the
    corresponding type using the _AT macro.
    
    The flush_pmd_entry/clean_pmd_entry argument was changed to (void *) to
    allow them to be used with both PGD and PMD pointers and avoid code
    duplication.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 36e983d98315..226f1804be12 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -60,7 +60,7 @@ EXPORT_SYMBOL(pgprot_kernel);
 struct cachepolicy {
 	const char	policy[16];
 	unsigned int	cr_mask;
-	unsigned int	pmd;
+	pmdval_t	pmd;
 	pteval_t	pte;
 };
 
@@ -288,7 +288,7 @@ static void __init build_mem_type_table(void)
 {
 	struct cachepolicy *cp;
 	unsigned int cr = get_cr();
-	unsigned int user_pgprot, kern_pgprot, vecs_pgprot;
+	pteval_t user_pgprot, kern_pgprot, vecs_pgprot;
 	int cpu_arch = cpu_architecture();
 	int i;
 

commit 8fb54284ba6aa1f0d832ec015fde64ecf4bb0f4f
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Tue Jun 28 12:42:56 2011 -0700

    ARM: mm: Add strongly ordered descriptor support.
    
    On certain architectures, there might be a need to mark certain
    addresses with strongly ordered memory attributes to avoid ordering
    issues at the interconnect level.
    
    On OMAP4, the asynchronous bridge buffers can only be drained
    with strongly ordered accesses and hence the need to mark the
    memory strongly ordered.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Woodruff Richard <r-woodruff2@ti.com>
    Tested-by: Vishwanath BS <vishwanath.bs@ti.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 594d677b92c8..ea9c9f3e48bf 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -273,6 +273,14 @@ static struct mem_type mem_types[] = {
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_KERNEL,
 	},
+	[MT_MEMORY_SO] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_MT_UNCACHED,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE | PMD_SECT_S |
+				PMD_SECT_UNCACHED | PMD_SECT_XN,
+		.domain    = DOMAIN_KERNEL,
+	},
 };
 
 const struct mem_type *get_mem_type(unsigned int type)

commit e73fc88e19d74fd4dd664cff45b88caab8cde45c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 23 14:07:23 2011 +0100

    ARM: 7059/1: LPAE: Use PMD_(SHIFT|SIZE|MASK) instead of PGDIR_*
    
    PGDIR_SHIFT and PMD_SHIFT for the classic 2-level page table format have
    the same value (21). This patch converts the PGDIR_* uses in the kernel
    to the PMD_* equivalent so that LPAE builds can reuse the same code.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 594d677b92c8..36e983d98315 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -863,14 +863,14 @@ static inline void prepare_page_table(void)
 	/*
 	 * Clear out all the mappings below the kernel image.
 	 */
-	for (addr = 0; addr < MODULES_VADDR; addr += PGDIR_SIZE)
+	for (addr = 0; addr < MODULES_VADDR; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
 #ifdef CONFIG_XIP_KERNEL
 	/* The XIP kernel is mapped in the module area -- skip over it */
-	addr = ((unsigned long)_etext + PGDIR_SIZE - 1) & PGDIR_MASK;
+	addr = ((unsigned long)_etext + PMD_SIZE - 1) & PMD_MASK;
 #endif
-	for ( ; addr < PAGE_OFFSET; addr += PGDIR_SIZE)
+	for ( ; addr < PAGE_OFFSET; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
 	/*
@@ -885,10 +885,12 @@ static inline void prepare_page_table(void)
 	 * memory bank, up to the end of the vmalloc region.
 	 */
 	for (addr = __phys_to_virt(end);
-	     addr < VMALLOC_END; addr += PGDIR_SIZE)
+	     addr < VMALLOC_END; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 }
 
+#define SWAPPER_PG_DIR_SIZE	(PTRS_PER_PGD * sizeof(pgd_t))
+
 /*
  * Reserve the special regions of memory
  */
@@ -898,7 +900,7 @@ void __init arm_mm_memblock_reserve(void)
 	 * Reserve the page tables.  These are already in use,
 	 * and can only be in node 0.
 	 */
-	memblock_reserve(__pa(swapper_pg_dir), PTRS_PER_PGD * sizeof(pgd_t));
+	memblock_reserve(__pa(swapper_pg_dir), SWAPPER_PG_DIR_SIZE);
 
 #ifdef CONFIG_SA1111
 	/*
@@ -926,7 +928,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 */
 	vectors_page = early_alloc(PAGE_SIZE);
 
-	for (addr = VMALLOC_END; addr; addr += PGDIR_SIZE)
+	for (addr = VMALLOC_END; addr; addr += PMD_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
 	/*

commit 0371d3f7e8f1cddaee1f215e42c09a40e235d810
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jul 5 19:58:29 2011 +0100

    ARM: move memory layout sanity checking before meminfo initialization
    
    Ensure that the meminfo array is sanity checked before we pass the
    memory to memblock.  This helps to ensure that memblock and meminfo
    agree on the dimensions of memory, especially when more memory is
    passed than the kernel can deal with.
    
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9d9e736c2b4f..594d677b92c8 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -759,7 +759,7 @@ early_param("vmalloc", early_vmalloc);
 
 static phys_addr_t lowmem_limit __initdata = 0;
 
-static void __init sanity_check_meminfo(void)
+void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
 
@@ -1032,8 +1032,9 @@ void __init paging_init(struct machine_desc *mdesc)
 {
 	void *zero_page;
 
+	memblock_set_current_limit(lowmem_limit);
+
 	build_mem_type_table();
-	sanity_check_meminfo();
 	prepare_page_table();
 	map_lowmem();
 	devicemaps_init(mdesc);

commit 40f7bfe4f1c2761abeceb3b2b9dc1feec3c47ed9
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu May 19 13:22:48 2011 +0100

    ARM: 6914/1: sparsemem: fix highmem detection when using SPARSEMEM
    
    sanity_check_meminfo walks over the registered memory banks and attempts
    to split banks across lowmem and highmem when they would otherwise
    overlap with the vmalloc space.
    
    When SPARSEMEM is used, there are two potential problems that occur
    when the virtual address of the start of a bank is equal to vmalloc_min.
    
     1.) The end of lowmem is calculated as __pa(vmalloc_min - 1) + 1.
         In the above scenario, this will give the end address of the
         previous bank, rather than the actual bank we are interested in.
         This value is later used as the memblock limit and artificially
         restricts the total amount of available memory.
    
     2.) The checks to determine whether or not a bank belongs to highmem
         or not only check if __va(bank->start) is greater or less than
         vmalloc_min. In the case that it is equal, the bank is incorrectly
         treated as lowmem, which hoses the vmalloc area.
    
    This patch fixes these two problems by checking whether the virtual
    start address of a bank is >= vmalloc_min and then calculating
    lowmem_end by finding the virtual end address of the highest lowmem
    bank.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 08a92368d9d3..9d9e736c2b4f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -763,15 +763,12 @@ static void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
 
-	lowmem_limit = __pa(vmalloc_min - 1) + 1;
-	memblock_set_current_limit(lowmem_limit);
-
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];
 		*bank = meminfo.bank[i];
 
 #ifdef CONFIG_HIGHMEM
-		if (__va(bank->start) > vmalloc_min ||
+		if (__va(bank->start) >= vmalloc_min ||
 		    __va(bank->start) < (void *)PAGE_OFFSET)
 			highmem = 1;
 
@@ -829,6 +826,9 @@ static void __init sanity_check_meminfo(void)
 			bank->size = newsize;
 		}
 #endif
+		if (!bank->highmem && bank->start + bank->size > lowmem_limit)
+			lowmem_limit = bank->start + bank->size;
+
 		j++;
 	}
 #ifdef CONFIG_HIGHMEM
@@ -852,6 +852,7 @@ static void __init sanity_check_meminfo(void)
 	}
 #endif
 	meminfo.nr_banks = j;
+	memblock_set_current_limit(lowmem_limit);
 }
 
 static inline void prepare_page_table(void)

commit 1c395176962176660bb108f90e97e1686cfe0d85
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 24 17:11:58 2011 -0700

    mm: now that all old mmu_gather code is gone, remove the storage
    
    Fold all the mmu_gather rework patches into one for submission
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reported-by: Hugh Dickins <hughd@google.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6cf76b3b68d1..08a92368d9d3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -31,8 +31,6 @@
 
 #include "mm.h"
 
-DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
-
 /*
  * empty_zero_page is a special page that is used for
  * zero-initialized data and COW.

commit 196f020fbbb83d246960548e73a40fd08f3e7866
Merge: 6d7ed21d17e6 7d85d61f6ad6 516295e5ab4b 3de4ade3d696
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Mar 20 09:32:12 2011 +0000

    Merge branches 'fixes', 'pgt-next' and 'versatile' into devel

commit aaa50048f6ce44af66ce0389d4cc6a8348333271
Author: Nicolas Pitre <nico@fluxnic.net>
Date:   Tue Jan 25 21:35:38 2011 +0100

    ARM: 6639/1: allow highmem on SMP platforms without h/w TLB ops broadcast
    
    In commit e616c591405c168f6dc3dfd1221e105adfe49b8d, highmem support was
    deactivated for SMP platforms without hardware TLB ops broadcast because
    usage of kmap_high_get() requires that IRQs be disabled when kmap_lock
    is locked which is incompatible with the IPI mechanism used by the
    software TLB ops broadcast invoked through flush_all_zero_pkmaps().
    
    The reason for kmap_high_get() is to ensure that the currently kmap'd
    page usage count does not decrease to zero while we're using its
    existing virtual mapping in an atomic context.  With a VIVT cache this
    is essential to do due to cache coherency issues, but with a VIPT cache
    this is only an optimization so not to pay the price of establishing a
    second mapping if an existing one can be used.  However, on VIPT
    platforms without hardware TLB maintenance we can give up on that
    optimization in order to be able to use highmem.
    
    From ARMv7 onwards the TLB ops are broadcasted in hardware, so let's
    disable ARCH_NEEDS_KMAP_HIGH_GET only when CONFIG_SMP and
    CONFIG_CPU_TLB_V6 are defined.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Saeed Bishara <saeed.bishara@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 3c67e92f7d59..ff7b43b5885a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -827,16 +827,6 @@ static void __init sanity_check_meminfo(void)
 			 * rather difficult.
 			 */
 			reason = "with VIPT aliasing cache";
-		} else if (is_smp() && tlb_ops_need_broadcast()) {
-			/*
-			 * kmap_high needs to occasionally flush TLB entries,
-			 * however, if the TLB entries need to be broadcast
-			 * we may deadlock:
-			 *  kmap_high(irqs off)->flush_all_zero_pkmaps->
-			 *  flush_tlb_kernel_range->smp_call_function_many
-			 *   (must not be called with irqs off)
-			 */
-			reason = "without hardware TLB ops broadcasting";
 		}
 		if (reason) {
 			printk(KERN_CRIT "HIGHMEM is not supported %s, ignoring high memory\n",

commit 516295e5ab4bf986865cfff856d484ec678e3b0b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Nov 21 16:27:49 2010 +0000

    ARM: pgtable: add pud-level code
    
    Add pud_offset() et.al. between the pgd and pmd code in preparation of
    using pgtable-nopud.h rather than 4level-fixup.h.
    
    This incorporates a fix from Jamie Iles <jamie@jamieiles.com> for
    uaccess_with_memcpy.c.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 176749010935..82ef6966ae09 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -550,11 +550,11 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }
 
-static void __init alloc_init_section(pgd_t *pgd, unsigned long addr,
+static void __init alloc_init_section(pud_t *pud, unsigned long addr,
 				      unsigned long end, phys_addr_t phys,
 				      const struct mem_type *type)
 {
-	pmd_t *pmd = pmd_offset(pgd, addr);
+	pmd_t *pmd = pmd_offset(pud, addr);
 
 	/*
 	 * Try a section mapping - end, addr and phys must all be aligned
@@ -583,6 +583,19 @@ static void __init alloc_init_section(pgd_t *pgd, unsigned long addr,
 	}
 }
 
+static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,
+	unsigned long phys, const struct mem_type *type)
+{
+	pud_t *pud = pud_offset(pgd, addr);
+	unsigned long next;
+
+	do {
+		next = pud_addr_end(addr, end);
+		alloc_init_section(pud, addr, next, phys, type);
+		phys += next - addr;
+	} while (pud++, addr = next, addr != end);
+}
+
 static void __init create_36bit_mapping(struct map_desc *md,
 					const struct mem_type *type)
 {
@@ -630,7 +643,8 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	pgd = pgd_offset_k(addr);
 	end = addr + length;
 	do {
-		pmd_t *pmd = pmd_offset(pgd, addr);
+		pud_t *pud = pud_offset(pgd, addr);
+		pmd_t *pmd = pmd_offset(pud, addr);
 		int i;
 
 		for (i = 0; i < 16; i++)
@@ -696,7 +710,7 @@ static void __init create_mapping(struct map_desc *md)
 	do {
 		unsigned long next = pgd_addr_end(addr, end);
 
-		alloc_init_section(pgd, addr, next, phys, type);
+		alloc_init_pud(pgd, addr, next, phys, type);
 
 		phys += next - addr;
 		addr = next;

commit e33b9d0878ff6b8a543b3423c3019a5dbbf5689a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Feb 20 11:47:41 2011 +0000

    ARM: Use long long format when printing meminfo physical addresses
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e09744e82d50..176749010935 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -794,9 +794,10 @@ static void __init sanity_check_meminfo(void)
 		 */
 		if (__va(bank->start) >= vmalloc_min ||
 		    __va(bank->start) < (void *)PAGE_OFFSET) {
-			printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
+			printk(KERN_NOTICE "Ignoring RAM at %.8llx-%.8llx "
 			       "(vmalloc region overlap).\n",
-			       bank->start, bank->start + bank->size - 1);
+			       (unsigned long long)bank->start,
+			       (unsigned long long)bank->start + bank->size - 1);
 			continue;
 		}
 
@@ -807,10 +808,11 @@ static void __init sanity_check_meminfo(void)
 		if (__va(bank->start + bank->size) > vmalloc_min ||
 		    __va(bank->start + bank->size) < __va(bank->start)) {
 			unsigned long newsize = vmalloc_min - __va(bank->start);
-			printk(KERN_NOTICE "Truncating RAM at %.8lx-%.8lx "
-			       "to -%.8lx (vmalloc region overlap).\n",
-			       bank->start, bank->start + bank->size - 1,
-			       bank->start + newsize - 1);
+			printk(KERN_NOTICE "Truncating RAM at %.8llx-%.8llx "
+			       "to -%.8llx (vmalloc region overlap).\n",
+			       (unsigned long long)bank->start,
+			       (unsigned long long)bank->start + bank->size - 1,
+			       (unsigned long long)bank->start + newsize - 1);
 			bank->size = newsize;
 		}
 #endif

commit cae6292b653f5e3308bf2787a54b7dcd2cc7e2b3
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 15 12:42:57 2011 +0100

    ARM: 6672/1: LPAE: use phys_addr_t instead of unsigned long in mapping functions
    
    The unsigned long datatype is not sufficient for mapping physical addresses
    >= 4GB.
    
    This patch ensures that the phys_addr_t datatype is used to represent physical
    addresses when converting from a PFN.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f512ad97dafe..e09744e82d50 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -591,7 +591,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	pgd_t *pgd;
 
 	addr = md->virtual;
-	phys = (unsigned long)__pfn_to_phys(md->pfn);
+	phys = __pfn_to_phys(md->pfn);
 	length = PAGE_ALIGN(md->length);
 
 	if (!(cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())) {
@@ -651,7 +651,8 @@ static void __init create_36bit_mapping(struct map_desc *md,
  */
 static void __init create_mapping(struct map_desc *md)
 {
-	unsigned long phys, addr, length, end;
+	unsigned long addr, length, end;
+	phys_addr_t phys;
 	const struct mem_type *type;
 	pgd_t *pgd;
 
@@ -680,7 +681,7 @@ static void __init create_mapping(struct map_desc *md)
 	}
 
 	addr = md->virtual & PAGE_MASK;
-	phys = (unsigned long)__pfn_to_phys(md->pfn);
+	phys = __pfn_to_phys(md->pfn);
 	length = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
 
 	if (type->prot_l1 == 0 && ((addr | phys | length) & ~SECTION_MASK)) {

commit 29a38193c15bd72ba96c57a805443ef46ea6a6ac
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 15 14:31:37 2011 +0100

    ARM: 6674/1: LPAE: use long long format when printing physical addresses and ptes
    
    For the Kernel to support 2 level and 3 level page tables, physical
    addresses (and also page table entries) need to be 32 or 64-bits depending
    upon the configuration.
    
    This patch uses the %08llx conversion specifier for physical addresses
    and page table entries, ensuring that they are cast to (long long) so
    that common code can be used regardless of the datatype widths.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 7515d4718645..f512ad97dafe 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -597,7 +597,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	if (!(cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())) {
 		printk(KERN_ERR "MM: CPU does not support supersection "
 		       "mapping for 0x%08llx at 0x%08lx\n",
-		       __pfn_to_phys((u64)md->pfn), addr);
+		       (long long)__pfn_to_phys((u64)md->pfn), addr);
 		return;
 	}
 
@@ -610,14 +610,14 @@ static void __init create_36bit_mapping(struct map_desc *md,
 	if (type->domain) {
 		printk(KERN_ERR "MM: invalid domain in supersection "
 		       "mapping for 0x%08llx at 0x%08lx\n",
-		       __pfn_to_phys((u64)md->pfn), addr);
+		       (long long)__pfn_to_phys((u64)md->pfn), addr);
 		return;
 	}
 
 	if ((addr | length | __pfn_to_phys(md->pfn)) & ~SUPERSECTION_MASK) {
-		printk(KERN_ERR "MM: cannot create mapping for "
-		       "0x%08llx at 0x%08lx invalid alignment\n",
-		       __pfn_to_phys((u64)md->pfn), addr);
+		printk(KERN_ERR "MM: cannot create mapping for 0x%08llx"
+		       " at 0x%08lx invalid alignment\n",
+		       (long long)__pfn_to_phys((u64)md->pfn), addr);
 		return;
 	}
 
@@ -656,17 +656,17 @@ static void __init create_mapping(struct map_desc *md)
 	pgd_t *pgd;
 
 	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
-		printk(KERN_WARNING "BUG: not creating mapping for "
-		       "0x%08llx at 0x%08lx in user region\n",
-		       __pfn_to_phys((u64)md->pfn), md->virtual);
+		printk(KERN_WARNING "BUG: not creating mapping for 0x%08llx"
+		       " at 0x%08lx in user region\n",
+		       (long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 		return;
 	}
 
 	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
 	    md->virtual >= PAGE_OFFSET && md->virtual < VMALLOC_END) {
-		printk(KERN_WARNING "BUG: mapping for 0x%08llx at 0x%08lx "
-		       "overlaps vmalloc space\n",
-		       __pfn_to_phys((u64)md->pfn), md->virtual);
+		printk(KERN_WARNING "BUG: mapping for 0x%08llx"
+		       " at 0x%08lx overlaps vmalloc space\n",
+		       (long long)__pfn_to_phys((u64)md->pfn), md->virtual);
 	}
 
 	type = &mem_types[md->type];
@@ -684,9 +684,9 @@ static void __init create_mapping(struct map_desc *md)
 	length = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
 
 	if (type->prot_l1 == 0 && ((addr | phys | length) & ~SECTION_MASK)) {
-		printk(KERN_WARNING "BUG: map for 0x%08lx at 0x%08lx can not "
+		printk(KERN_WARNING "BUG: map for 0x%08llx at 0x%08lx can not "
 		       "be mapped using pages, ignoring.\n",
-		       __pfn_to_phys(md->pfn), addr);
+		       (long long)__pfn_to_phys(md->pfn), addr);
 		return;
 	}
 

commit 410f14837a92177242a403c20d0a0906fe357019
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Feb 14 12:58:04 2011 +0100

    ARM: 6667/1: Fix early_pte_alloc() assumption about the Linux PTE
    
    With LPAE we no longer have software bits in a separate Linux PTE and
    the early_pte_alloc() function should pass PTE_HWTABLE_OFF +
    PTE_HWTABLE_SIZE to early_alloc() to avoid allocating extra memory.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 546e44734db0..7515d4718645 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -532,7 +532,7 @@ static void __init *early_alloc(unsigned long sz)
 static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
 {
 	if (pmd_none(*pmd)) {
-		pte_t *pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
+		pte_t *pte = early_alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
 		__pmd_populate(pmd, __pa(pte), prot);
 	}
 	BUG_ON(pmd_bad(*pmd));

commit 28cdac6690cb113856293bf79b40de33dbd8f974
Merge: 4073723acb9c 36bb94ba36f3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jan 6 22:33:19 2011 +0000

    Merge branch 'pgt' (early part) into devel

commit 36bb94ba36f332de767cfaa3af6a5136435a3a9c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 16 08:40:36 2010 +0000

    ARM: pgtable: provide RDONLY page table bit rather than WRITE bit
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index bd5a94b2d610..546e44734db0 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -190,7 +190,7 @@ void adjust_cr(unsigned long mask, unsigned long set)
 }
 #endif
 
-#define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_WRITE|L_PTE_XN
+#define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_XN
 #define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_AP_WRITE
 
 static struct mem_type mem_types[] = {
@@ -234,19 +234,19 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_LOW_VECTORS] = {
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_RDONLY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_USER,
 	},
 	[MT_HIGH_VECTORS] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_USER,
+				L_PTE_USER | L_PTE_RDONLY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_USER,
 	},
 	[MT_MEMORY] = {
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE,
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
@@ -257,21 +257,20 @@ static struct mem_type mem_types[] = {
 	},
 	[MT_MEMORY_NONCACHED] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE| L_PTE_MT_BUFFERABLE,
+				L_PTE_MT_BUFFERABLE,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_DTCM] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE | L_PTE_XN,
+				L_PTE_XN,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_ITCM] = {
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE,
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_KERNEL,
 	},
@@ -478,7 +477,7 @@ static void __init build_mem_type_table(void)
 
 	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
-				 L_PTE_DIRTY | L_PTE_WRITE | kern_pgprot);
+				 L_PTE_DIRTY | kern_pgprot);
 
 	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;

commit 9522d7e4cb5e0858122fc55d33a2c07728f0b10d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 16 00:23:31 2010 +0000

    ARM: pgtable: invert L_PTE_EXEC to L_PTE_XN
    
    The hardware page tables use an XN bit 'execute never'.  Historically,
    we've had a Linux 'execute allow' bit, in the positive sense.  Get rid
    of this artifact as future hardware will continue to have the XN sense.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index bd1a11e62f4e..bd5a94b2d610 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -190,7 +190,7 @@ void adjust_cr(unsigned long mask, unsigned long set)
 }
 #endif
 
-#define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_WRITE
+#define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_WRITE|L_PTE_XN
 #define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_AP_WRITE
 
 static struct mem_type mem_types[] = {
@@ -234,20 +234,19 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_LOW_VECTORS] = {
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_EXEC,
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_USER,
 	},
 	[MT_HIGH_VECTORS] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_USER | L_PTE_EXEC,
+				L_PTE_USER,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_USER,
 	},
 	[MT_MEMORY] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE | L_PTE_EXEC,
+				L_PTE_WRITE,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
@@ -258,21 +257,21 @@ static struct mem_type mem_types[] = {
 	},
 	[MT_MEMORY_NONCACHED] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE | L_PTE_EXEC | L_PTE_MT_BUFFERABLE,
+				L_PTE_WRITE| L_PTE_MT_BUFFERABLE,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_DTCM] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE,
+				L_PTE_WRITE | L_PTE_XN,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_ITCM] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE | L_PTE_EXEC,
+				L_PTE_WRITE,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.domain    = DOMAIN_KERNEL,
 	},

commit 614dd0585f376a25c638abbed9c5fbd21d7baece
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Nov 21 11:41:57 2010 +0000

    ARM: pgtable: collect up identity mapping functions
    
    We have two places where we create identity mappings - one when we bring
    secondary CPUs online, and one where we setup some mappings for soft-
    reboot.  Combine these two into a single implementation.  Also collect
    the identity mapping deletion function.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 94ee0930d690..bd1a11e62f4e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1045,38 +1045,3 @@ void __init paging_init(struct machine_desc *mdesc)
 	empty_zero_page = virt_to_page(zero_page);
 	__flush_dcache_page(NULL, empty_zero_page);
 }
-
-/*
- * In order to soft-boot, we need to insert a 1:1 mapping in place of
- * the user-mode pages.  This will then ensure that we have predictable
- * results when turning the mmu off
- */
-void setup_mm_for_reboot(char mode)
-{
-	unsigned long base_pmdval;
-	pgd_t *pgd;
-	int i;
-
-	/*
-	 * We need to access to user-mode page tables here. For kernel threads
-	 * we don't have any user-mode mappings so we use the context that we
-	 * "borrowed".
-	 */
-	pgd = current->active_mm->pgd;
-
-	base_pmdval = PMD_SECT_AP_WRITE | PMD_SECT_AP_READ | PMD_TYPE_SECT;
-	if (cpu_architecture() <= CPU_ARCH_ARMv5TEJ && !cpu_is_xscale())
-		base_pmdval |= PMD_BIT4;
-
-	for (i = 0; i < FIRST_USER_PGD_NR + USER_PTRS_PER_PGD; i++, pgd++) {
-		unsigned long pmdval = (i << PGDIR_SHIFT) | base_pmdval;
-		pmd_t *pmd;
-
-		pmd = pmd_off(pgd, i << PGDIR_SHIFT);
-		pmd[0] = __pmd(pmdval);
-		pmd[1] = __pmd(pmdval + (1 << (PGDIR_SHIFT - 1)));
-		flush_pmd_entry(pmd);
-	}
-
-	local_flush_tlb_all();
-}

commit f6e3354d02aa1f30672e3671098c12cb49c7da25
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 16 00:22:09 2010 +0000

    ARM: pgtable: introduce pteval_t to represent a pte value
    
    This makes everywhere dealing with pte values use the same type.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9568f8632ae3..94ee0930d690 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -62,7 +62,7 @@ struct cachepolicy {
 	const char	policy[16];
 	unsigned int	cr_mask;
 	unsigned int	pmd;
-	unsigned int	pte;
+	pteval_t	pte;
 };
 
 static struct cachepolicy cache_policies[] __initdata = {

commit 97092e0c56830457af0639f6bd904537a150ea4a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 16 00:16:01 2010 +0000

    ARM: pgtable: use phys_addr_t for physical addresses
    
    Ensure that physical addresses are typed as phys_addr_t
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 72ad3e1f56cf..9568f8632ae3 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -535,7 +535,7 @@ static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned l
 {
 	if (pmd_none(*pmd)) {
 		pte_t *pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
-		__pmd_populate(pmd, __pa(pte) | prot);
+		__pmd_populate(pmd, __pa(pte), prot);
 	}
 	BUG_ON(pmd_bad(*pmd));
 	return pte_offset_kernel(pmd, addr);
@@ -553,7 +553,7 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 }
 
 static void __init alloc_init_section(pgd_t *pgd, unsigned long addr,
-				      unsigned long end, unsigned long phys,
+				      unsigned long end, phys_addr_t phys,
 				      const struct mem_type *type)
 {
 	pmd_t *pmd = pmd_offset(pgd, addr);
@@ -588,7 +588,8 @@ static void __init alloc_init_section(pgd_t *pgd, unsigned long addr,
 static void __init create_36bit_mapping(struct map_desc *md,
 					const struct mem_type *type)
 {
-	unsigned long phys, addr, length, end;
+	unsigned long addr, length, end;
+	phys_addr_t phys;
 	pgd_t *pgd;
 
 	addr = md->virtual;

commit 247055aa21ffef1c49dd64710d5e94c2aee19b58
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 13 16:03:21 2010 +0100

    ARM: 6384/1: Remove the domain switching on ARMv6k/v7 CPUs
    
    This patch removes the domain switching functionality via the set_fs and
    __switch_to functions on cores that have a TLS register.
    
    Currently, the ioremap and vmalloc areas share the same level 1 page
    tables and therefore have the same domain (DOMAIN_KERNEL). When the
    kernel domain is modified from Client to Manager (via the __set_fs or in
    the __switch_to function), the XN (eXecute Never) bit is overridden and
    newer CPUs can speculatively prefetch the ioremap'ed memory.
    
    Linux performs the kernel domain switching to allow user-specific
    functions (copy_to/from_user, get/put_user etc.) to access kernel
    memory. In order for these functions to work with the kernel domain set
    to Client, the patch modifies the LDRT/STRT and related instructions to
    the LDR/STR ones.
    
    The user pages access rights are also modified for kernel read-only
    access rather than read/write so that the copy-on-write mechanism still
    works. CPU_USE_DOMAINS gets disabled only if the hardware has a TLS register
    (CPU_32v6K is defined) since writing the TLS value to the high vectors page
    isn't possible.
    
    The user addresses passed to the kernel are checked by the access_ok()
    function so that they do not point to the kernel space.
    
    Tested-by: Anton Vorontsov <cbouatmailru@gmail.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 72ad3e1f56cf..79c01f540cbe 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -24,6 +24,7 @@
 #include <asm/smp_plat.h>
 #include <asm/tlb.h>
 #include <asm/highmem.h>
+#include <asm/traps.h>
 
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
@@ -914,12 +915,11 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 {
 	struct map_desc map;
 	unsigned long addr;
-	void *vectors;
 
 	/*
 	 * Allocate the vector page early.
 	 */
-	vectors = early_alloc(PAGE_SIZE);
+	vectors_page = early_alloc(PAGE_SIZE);
 
 	for (addr = VMALLOC_END; addr; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
@@ -959,7 +959,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 * location (0xffff0000).  If we aren't using high-vectors, also
 	 * create a mapping at the low-vectors virtual address.
 	 */
-	map.pfn = __phys_to_pfn(virt_to_phys(vectors));
+	map.pfn = __phys_to_pfn(virt_to_phys(vectors_page));
 	map.virtual = 0xffff0000;
 	map.length = PAGE_SIZE;
 	map.type = MT_HIGH_VECTORS;

commit 8df6516864462cb7a6f87d5a46df68fb0faebbb5
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Oct 27 19:57:38 2010 +0100

    ARM: memblock: setup lowmem mappings using memblock
    
    Use memblock information to setup lowmem mappings rather than the
    membank array.
    
    This allows platforms to manipulate the memblock information during
    initialization to reserve (and remove) memory from the kernel's view
    of memory - and thus allowing platforms to setup their own private
    mappings for this memory without causing problems with multiple
    aliasing mappings:
    
            size = min(size, SZ_2M);
            base = memblock_alloc(size, min(align, SZ_2M));
            memblock_free(base, size);
            memblock_remove(base, size);
    
    This is needed because multiple mappings of regions with differing
    attributes (sharability, type, cache) are not permitted with ARMv6
    and above.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 5222d3850960..72ad3e1f56cf 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -744,11 +744,14 @@ static int __init early_vmalloc(char *arg)
 }
 early_param("vmalloc", early_vmalloc);
 
+static phys_addr_t lowmem_limit __initdata = 0;
+
 static void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
 
-	memblock_set_current_limit(__pa(vmalloc_min - 1) + 1);
+	lowmem_limit = __pa(vmalloc_min - 1) + 1;
+	memblock_set_current_limit(lowmem_limit);
 
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];
@@ -849,6 +852,7 @@ static void __init sanity_check_meminfo(void)
 static inline void prepare_page_table(void)
 {
 	unsigned long addr;
+	phys_addr_t end;
 
 	/*
 	 * Clear out all the mappings below the kernel image.
@@ -863,11 +867,18 @@ static inline void prepare_page_table(void)
 	for ( ; addr < PAGE_OFFSET; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
+	/*
+	 * Find the end of the first block of lowmem.
+	 */
+	end = memblock.memory.regions[0].base + memblock.memory.regions[0].size;
+	if (end >= lowmem_limit)
+		end = lowmem_limit;
+
 	/*
 	 * Clear out all the kernel space mappings, except for the first
 	 * memory bank, up to the end of the vmalloc region.
 	 */
-	for (addr = __phys_to_virt(bank_phys_end(&meminfo.bank[0]));
+	for (addr = __phys_to_virt(end);
 	     addr < VMALLOC_END; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
 }
@@ -984,29 +995,27 @@ static void __init kmap_init(void)
 #endif
 }
 
-static inline void map_memory_bank(struct membank *bank)
-{
-	struct map_desc map;
-
-	map.pfn = bank_pfn_start(bank);
-	map.virtual = __phys_to_virt(bank_phys_start(bank));
-	map.length = bank_phys_size(bank);
-	map.type = MT_MEMORY;
-
-	create_mapping(&map);
-}
-
 static void __init map_lowmem(void)
 {
-	struct meminfo *mi = &meminfo;
-	int i;
+	struct memblock_region *reg;
 
 	/* Map all the lowmem memory banks. */
-	for (i = 0; i < mi->nr_banks; i++) {
-		struct membank *bank = &mi->bank[i];
+	for_each_memblock(memory, reg) {
+		phys_addr_t start = reg->base;
+		phys_addr_t end = start + reg->size;
+		struct map_desc map;
+
+		if (end > lowmem_limit)
+			end = lowmem_limit;
+		if (start >= end)
+			break;
+
+		map.pfn = __phys_to_pfn(start);
+		map.virtual = __phys_to_virt(start);
+		map.length = end - start;
+		map.type = MT_MEMORY;
 
-		if (!bank->highmem)
-			map_memory_bank(bank);
+		create_mapping(&map);
 	}
 }
 

commit 7dc50ec7283391dd7a29a80e2a0fb76731a6a7c7
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Oct 27 18:14:56 2010 +0100

    ARM: ensure membank array is always sorted
    
    This was missing from the noMMU code, so there was the possibility
    of things not working as expected if out of order memory information
    was passed.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 8e21b4e721e4..5222d3850960 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -14,7 +14,6 @@
 #include <linux/mman.h>
 #include <linux/nodemask.h>
 #include <linux/memblock.h>
-#include <linux/sort.h>
 #include <linux/fs.h>
 
 #include <asm/cputype.h>
@@ -1011,13 +1010,6 @@ static void __init map_lowmem(void)
 	}
 }
 
-static int __init meminfo_cmp(const void *_a, const void *_b)
-{
-	const struct membank *a = _a, *b = _b;
-	long cmp = bank_pfn_start(a) - bank_pfn_start(b);
-	return cmp < 0 ? -1 : cmp > 0 ? 1 : 0;
-}
-
 /*
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.
@@ -1026,8 +1018,6 @@ void __init paging_init(struct machine_desc *mdesc)
 {
 	void *zero_page;
 
-	sort(&meminfo.bank, meminfo.nr_banks, sizeof(meminfo.bank[0]), meminfo_cmp, NULL);
-
 	build_mem_type_table();
 	sanity_check_meminfo();
 	prepare_page_table();

commit 4e929d2bcf13eeaa9636448c55690b383a910391
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Oct 27 18:09:59 2010 +0100

    ARM: fix memblock breakage
    
    Will says:
    | Commit e63075a3 removed the explicit MEMBLOCK_REAL_LIMIT #define
    | and introduced the requirement that arch code calls
    | memblock_set_current_limit to ensure that the __va macro can
    | be used on physical addresses returned from memblock_alloc.
    
    Unfortunately, ARM was missed out of this change.  Fix this.
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d184d25e9a5f..8e21b4e721e4 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -745,13 +745,11 @@ static int __init early_vmalloc(char *arg)
 }
 early_param("vmalloc", early_vmalloc);
 
-phys_addr_t lowmem_end_addr;
-
 static void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
 
-	lowmem_end_addr = __pa(vmalloc_min - 1) + 1;
+	memblock_set_current_limit(__pa(vmalloc_min - 1) + 1);
 
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];

commit f444fce3c0aa199facc8c2be7635a4a82b84b71e
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Mon Oct 18 09:03:03 2010 +0100

    ARM: 6445/1: fixup TCM memory types
    
    After Santosh's fixup of the generic MT_MEMORY and
    MT_MEMORY_NONCACHED I add this fix to the TCM memory types.
    The main change is that the ITCM memory is L_PTE_WRITE and
    DOMAIN_KERNEL which works just fine. The changed to the DTCM
    is just cosmetic to fit with surrounding code.
    
    Cc: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Cc: Rickard Andersson <rickard.andersson@stericsson.com>
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c32f731d56d3..d184d25e9a5f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -265,17 +265,17 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_DTCM] = {
-		.prot_pte	= L_PTE_PRESENT | L_PTE_YOUNG |
-		                  L_PTE_DIRTY | L_PTE_WRITE,
-		.prot_l1	= PMD_TYPE_TABLE,
-		.prot_sect	= PMD_TYPE_SECT | PMD_SECT_XN,
-		.domain		= DOMAIN_KERNEL,
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_WRITE,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
+		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_ITCM] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_USER | L_PTE_EXEC,
+				L_PTE_WRITE | L_PTE_EXEC,
 		.prot_l1   = PMD_TYPE_TABLE,
-		.domain    = DOMAIN_IO,
+		.domain    = DOMAIN_KERNEL,
 	},
 };
 

commit 809b4e00baf006a990a73329ba381d536c6fa277
Merge: a0a55682b83f 79a94c3538bd
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Oct 19 22:06:36 2010 +0100

    Merge branch 'devel-stable' into devel

commit f00ec48fadf5e37e7889f14cff900aa70d18b644
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Sep 4 10:47:48 2010 +0100

    ARM: Allow SMP kernels to boot on UP systems
    
    UP systems do not implement all the instructions that SMP systems have,
    so in order to boot a SMP kernel on a UP system, we need to rewrite
    parts of the kernel.
    
    Do this using an 'alternatives' scheme, where the kernel code and data
    is modified prior to initialization to replace the SMP instructions,
    thereby rendering the problematical code ineffectual.  We use the linker
    to generate a list of 32-bit word locations and their replacement values,
    and run through these replacements when we detect a UP system.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6a3a2d0cd6db..e2335811c02e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -310,9 +310,8 @@ static void __init build_mem_type_table(void)
 			cachepolicy = CPOLICY_WRITEBACK;
 		ecc_mask = 0;
 	}
-#ifdef CONFIG_SMP
-	cachepolicy = CPOLICY_WRITEALLOC;
-#endif
+	if (is_smp())
+		cachepolicy = CPOLICY_WRITEALLOC;
 
 	/*
 	 * Strip out features not present on earlier architectures.
@@ -406,13 +405,11 @@ static void __init build_mem_type_table(void)
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
 
-#ifndef CONFIG_SMP
 	/*
 	 * Only use write-through for non-SMP systems
 	 */
-	if (cpu_arch >= CPU_ARCH_ARMv5 && cachepolicy > CPOLICY_WRITETHROUGH)
+	if (!is_smp() && cpu_arch >= CPU_ARCH_ARMv5 && cachepolicy > CPOLICY_WRITETHROUGH)
 		vecs_pgprot = cache_policies[CPOLICY_WRITETHROUGH].pte;
-#endif
 
 	/*
 	 * Enable CPU-specific coherency if supported.
@@ -436,22 +433,23 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_MINICLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 
-#ifdef CONFIG_SMP
-		/*
-		 * Mark memory with the "shared" attribute for SMP systems
-		 */
-		user_pgprot |= L_PTE_SHARED;
-		kern_pgprot |= L_PTE_SHARED;
-		vecs_pgprot |= L_PTE_SHARED;
-		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_S;
-		mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_SHARED;
-		mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
-		mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
-		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
-		mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
-		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
-		mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
-#endif
+		if (is_smp()) {
+			/*
+			 * Mark memory with the "shared" attribute
+			 * for SMP systems
+			 */
+			user_pgprot |= L_PTE_SHARED;
+			kern_pgprot |= L_PTE_SHARED;
+			vecs_pgprot |= L_PTE_SHARED;
+			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_S;
+			mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
+			mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
+			mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
+			mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
+			mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
+		}
 	}
 
 	/*
@@ -829,8 +827,7 @@ static void __init sanity_check_meminfo(void)
 			 * rather difficult.
 			 */
 			reason = "with VIPT aliasing cache";
-#ifdef CONFIG_SMP
-		} else if (tlb_ops_need_broadcast()) {
+		} else if (is_smp() && tlb_ops_need_broadcast()) {
 			/*
 			 * kmap_high needs to occasionally flush TLB entries,
 			 * however, if the TLB entries need to be broadcast
@@ -840,7 +837,6 @@ static void __init sanity_check_meminfo(void)
 			 *   (must not be called with irqs off)
 			 */
 			reason = "without hardware TLB ops broadcasting";
-#endif
 		}
 		if (reason) {
 			printk(KERN_CRIT "HIGHMEM is not supported %s, ignoring high memory\n",

commit 7f58217bb6d39edac68ae0988cdb96a520ad916f
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Thu Sep 30 09:02:17 2010 +0100

    ARM: 6419/1: mmu: Fix MT_MEMORY and MT_MEMORY_NONCACHED pte flags
    
    The commit f1a2481c0 sets up the default flags for MT_MEMORY and
    MT_MEMORY_NONCACHED memory types. L_PTE_USER flag is wrongly
    set as default for these entries so remove it. Also adding
    the 'L_PTE_WRITE' flag so that these pages become read-write
    instead of just being read-only
    
    [this stops them being exposed to userspace, which is the main
    concern here --rmk]
    
    Reported-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6a3a2d0cd6db..e8ed9dc461fe 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -248,7 +248,7 @@ static struct mem_type mem_types[] = {
 	},
 	[MT_MEMORY] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_USER | L_PTE_EXEC,
+				L_PTE_WRITE | L_PTE_EXEC,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
@@ -259,7 +259,7 @@ static struct mem_type mem_types[] = {
 	},
 	[MT_MEMORY_NONCACHED] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_USER | L_PTE_EXEC | L_PTE_MT_BUFFERABLE,
+				L_PTE_WRITE | L_PTE_EXEC | L_PTE_MT_BUFFERABLE,
 		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,

commit f1a2481c0ad3aebd94d11b317c488deaadc25002
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Fri Sep 24 07:18:22 2010 +0100

    ARM: 6407/1: mmu: Setup MT_MEMORY and MT_MEMORY_NONCACHED L1 entries
    
    This patch populates the L1 entries for MT_MEMORY and MT_MEMORY_NONCACHED
    types so that at boot-up, we can map memories outside system memory
    at page level granularity
    
    Previously the mapping was limiting to section level, which creates
    unnecessary additional mapping for which physical memory may not
    present. On the newer ARM with speculation, this is dangerous and can
    result in untraceable aborts.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index a486bd0d97dc..6a3a2d0cd6db 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -247,6 +247,9 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_USER,
 	},
 	[MT_MEMORY] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_USER | L_PTE_EXEC,
+		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
@@ -255,6 +258,9 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MEMORY_NONCACHED] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_USER | L_PTE_EXEC | L_PTE_MT_BUFFERABLE,
+		.prot_l1   = PMD_TYPE_TABLE,
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
@@ -412,9 +418,12 @@ static void __init build_mem_type_table(void)
 	 * Enable CPU-specific coherency if supported.
 	 * (Only available on XSC3 at the moment.)
 	 */
-	if (arch_is_coherent() && cpu_is_xsc3())
+	if (arch_is_coherent() && cpu_is_xsc3()) {
 		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
-
+		mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
+		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
+		mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
+	}
 	/*
 	 * ARMv6 and above have extended page tables.
 	 */
@@ -439,7 +448,9 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
 		mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
 		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
+		mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
 		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
+		mem_types[MT_MEMORY_NONCACHED].prot_pte |= L_PTE_SHARED;
 #endif
 	}
 
@@ -476,6 +487,8 @@ static void __init build_mem_type_table(void)
 	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_MEMORY].prot_sect |= ecc_mask | cp->pmd;
+	mem_types[MT_MEMORY].prot_pte |= kern_pgprot;
+	mem_types[MT_MEMORY_NONCACHED].prot_sect |= ecc_mask;
 	mem_types[MT_ROM].prot_sect |= cp->pmd;
 
 	switch (cp->pmd) {

commit d907387c42e9e39261629890e45a08ef4c3ed3fe
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 13 16:01:24 2010 +0100

    ARM: 6383/1: Implement phys_mem_access_prot() to avoid attributes aliasing
    
    ARMv7 onwards requires that there are no aliases to the same physical
    location using different memory types (i.e. Normal vs Strongly Ordered).
    Access to SO mappings when the unaligned accesses are handled in
    hardware is also Unpredictable (pgprot_noncached() mappings in user
    space).
    
    The /dev/mem driver requires uncached mappings with O_SYNC. The patch
    implements the phys_mem_access_prot() function which generates Strongly
    Ordered memory attributes if !pfn_valid() (independent of O_SYNC) and
    Normal Noncacheable (writecombine) if O_SYNC.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6e1c4f6a2b3f..a486bd0d97dc 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -15,6 +15,7 @@
 #include <linux/nodemask.h>
 #include <linux/memblock.h>
 #include <linux/sort.h>
+#include <linux/fs.h>
 
 #include <asm/cputype.h>
 #include <asm/sections.h>
@@ -498,6 +499,19 @@ static void __init build_mem_type_table(void)
 	}
 }
 
+#ifdef CONFIG_ARM_DMA_MEM_BUFFERABLE
+pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
+			      unsigned long size, pgprot_t vma_prot)
+{
+	if (!pfn_valid(pfn))
+		return pgprot_noncached(vma_prot);
+	else if (file->f_flags & O_SYNC)
+		return pgprot_writecombine(vma_prot);
+	return vma_prot;
+}
+EXPORT_SYMBOL(phys_mem_access_prot);
+#endif
+
 #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
 
 static void __init *early_alloc(unsigned long sz)

commit ceb0885d3b01bb2e2f18765770e212914f2864be
Merge: b31fc7af78e1 08458ef6eede
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jul 31 14:20:02 2010 +0100

    Merge branch 'misc' into devel
    
    Conflicts:
            arch/arm/mm/init.c

commit 8d717a52d1b0959128be5134dd12608e8e4f2115
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat May 22 19:47:18 2010 +0100

    ARM: Convert platform reservations to use LMB rather than bootmem
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 833a6c3f70c0..d5541adc3520 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1006,7 +1006,7 @@ void __init paging_init(struct machine_desc *mdesc)
 	/* allocate the zero page. */
 	zero_page = early_alloc(PAGE_SIZE);
 
-	bootmem_init(mdesc);
+	bootmem_init();
 
 	empty_zero_page = virt_to_page(zero_page);
 	__flush_dcache_page(NULL, empty_zero_page);

commit 2778f62056ada442414392d7ccd41188bb631619
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 9 16:27:52 2010 +0100

    ARM: initial LMB trial
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 05dbb956175b..833a6c3f70c0 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -11,9 +11,9 @@
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/init.h>
-#include <linux/bootmem.h>
 #include <linux/mman.h>
 #include <linux/nodemask.h>
+#include <linux/memblock.h>
 #include <linux/sort.h>
 
 #include <asm/cputype.h>
@@ -489,7 +489,9 @@ static void __init build_mem_type_table(void)
 
 static void __init *early_alloc(unsigned long sz)
 {
-	return alloc_bootmem_low_pages(sz);
+	void *ptr = __va(memblock_alloc(sz, sz));
+	memset(ptr, 0, sz);
+	return ptr;
 }
 
 static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
@@ -705,10 +707,14 @@ static int __init early_vmalloc(char *arg)
 }
 early_param("vmalloc", early_vmalloc);
 
+phys_addr_t lowmem_end_addr;
+
 static void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
 
+	lowmem_end_addr = __pa(vmalloc_min - 1) + 1;
+
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];
 		*bank = meminfo.bank[i];
@@ -834,34 +840,22 @@ static inline void prepare_page_table(void)
 }
 
 /*
- * Reserve the various regions
+ * Reserve the special regions of memory
  */
-void __init reserve_special_regions(void)
+void __init arm_mm_memblock_reserve(void)
 {
-	/*
-	 * Register the kernel text and data with bootmem.
-	 * Note that this can only be in node 0.
-	 */
-#ifdef CONFIG_XIP_KERNEL
-	reserve_bootmem(__pa(_data), _end - _data, BOOTMEM_DEFAULT);
-#else
-	reserve_bootmem(__pa(_stext), _end - _stext, BOOTMEM_DEFAULT);
-#endif
-
 	/*
 	 * Reserve the page tables.  These are already in use,
 	 * and can only be in node 0.
 	 */
-	reserve_bootmem(__pa(swapper_pg_dir),
-			PTRS_PER_PGD * sizeof(pgd_t), BOOTMEM_DEFAULT);
+	memblock_reserve(__pa(swapper_pg_dir), PTRS_PER_PGD * sizeof(pgd_t));
 
 #ifdef CONFIG_SA1111
 	/*
 	 * Because of the SA1111 DMA bug, we want to preserve our
 	 * precious DMA-able memory...
 	 */
-	reserve_bootmem(PHYS_OFFSET, __pa(swapper_pg_dir) - PHYS_OFFSET,
-			BOOTMEM_DEFAULT);
+	memblock_reserve(PHYS_OFFSET, __pa(swapper_pg_dir) - PHYS_OFFSET);
 #endif
 }
 
@@ -1004,7 +998,6 @@ void __init paging_init(struct machine_desc *mdesc)
 	sanity_check_meminfo();
 	prepare_page_table();
 	map_lowmem();
-	bootmem_init(mdesc);
 	devicemaps_init(mdesc);
 	kmap_init();
 
@@ -1012,6 +1005,9 @@ void __init paging_init(struct machine_desc *mdesc)
 
 	/* allocate the zero page. */
 	zero_page = early_alloc(PAGE_SIZE);
+
+	bootmem_init(mdesc);
+
 	empty_zero_page = virt_to_page(zero_page);
 	__flush_dcache_page(NULL, empty_zero_page);
 }

commit 4bb2e27db10abc555dfabd73661485fb75e4e97d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jul 1 18:33:29 2010 +0100

    ARM: early_pte_alloc()
    
    Provide a common function for allocating early PTE tables.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 3079d0fd9cea..05dbb956175b 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -492,18 +492,21 @@ static void __init *early_alloc(unsigned long sz)
 	return alloc_bootmem_low_pages(sz);
 }
 
-static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
-				  unsigned long end, unsigned long pfn,
-				  const struct mem_type *type)
+static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
 {
-	pte_t *pte;
-
 	if (pmd_none(*pmd)) {
-		pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
-		__pmd_populate(pmd, __pa(pte) | type->prot_l1);
+		pte_t *pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
+		__pmd_populate(pmd, __pa(pte) | prot);
 	}
+	BUG_ON(pmd_bad(*pmd));
+	return pte_offset_kernel(pmd, addr);
+}
 
-	pte = pte_offset_kernel(pmd, addr);
+static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
+				  unsigned long end, unsigned long pfn,
+				  const struct mem_type *type)
+{
+	pte_t *pte = early_pte_alloc(pmd, addr, type->prot_l1);
 	do {
 		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
 		pfn++;
@@ -949,11 +952,8 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 static void __init kmap_init(void)
 {
 #ifdef CONFIG_HIGHMEM
-	pmd_t *pmd = pmd_off_k(PKMAP_BASE);
-	pte_t *pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
-	BUG_ON(!pmd_none(*pmd) || !pte);
-	__pmd_populate(pmd, __pa(pte) | _PAGE_KERNEL_TABLE);
-	pkmap_page_table = pte + PTRS_PER_PTE;
+	pkmap_page_table = early_pte_alloc(pmd_off_k(PKMAP_BASE),
+		PKMAP_BASE, _PAGE_KERNEL_TABLE);
 #endif
 }
 

commit cb9d7707cd9be57830f31616233f6a872ca8416d
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Mon Jul 12 21:50:59 2010 +0100

    ARM: 6222/1: add memory types for the TCMs
    
    The earlier TCM memory regions were mapped as MT_MEMORY_UNCACHED
    which doesn't really work on platforms supporting the new v6
    features like the NX bit. Add unique MT_MEMORY_[I|D]TCM types
    instead.
    
    Cc: Nicolas Pitre <nico@fluxnic.net>
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 285894171186..e53480148c05 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -258,6 +258,19 @@ static struct mem_type mem_types[] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
+	[MT_MEMORY_DTCM] = {
+		.prot_pte	= L_PTE_PRESENT | L_PTE_YOUNG |
+		                  L_PTE_DIRTY | L_PTE_WRITE,
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PMD_TYPE_SECT | PMD_SECT_XN,
+		.domain		= DOMAIN_KERNEL,
+	},
+	[MT_MEMORY_ITCM] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_USER | L_PTE_EXEC,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.domain    = DOMAIN_IO,
+	},
 };
 
 const struct mem_type *get_mem_type(unsigned int type)

commit 3abe9d33b382cb9eee7bfee8d90b10078f4baa4d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Mar 25 17:02:59 2010 +0000

    ARM: early_alloc()
    
    Add a common early allocator function, in preparation for switching
    over to LMB.  When we do, this function will need to do a little more
    than just allocating memory; we need it zero initialized too.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 1676d017a93a..3079d0fd9cea 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -487,6 +487,11 @@ static void __init build_mem_type_table(void)
 
 #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
 
+static void __init *early_alloc(unsigned long sz)
+{
+	return alloc_bootmem_low_pages(sz);
+}
+
 static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 				  unsigned long end, unsigned long pfn,
 				  const struct mem_type *type)
@@ -494,7 +499,7 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 	pte_t *pte;
 
 	if (pmd_none(*pmd)) {
-		pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+		pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
 		__pmd_populate(pmd, __pa(pte) | type->prot_l1);
 	}
 
@@ -873,7 +878,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	/*
 	 * Allocate the vector page early.
 	 */
-	vectors = alloc_bootmem_low_pages(PAGE_SIZE);
+	vectors = early_alloc(PAGE_SIZE);
 
 	for (addr = VMALLOC_END; addr; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
@@ -945,7 +950,7 @@ static void __init kmap_init(void)
 {
 #ifdef CONFIG_HIGHMEM
 	pmd_t *pmd = pmd_off_k(PKMAP_BASE);
-	pte_t *pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+	pte_t *pte = early_alloc(2 * PTRS_PER_PTE * sizeof(pte_t));
 	BUG_ON(!pmd_none(*pmd) || !pte);
 	__pmd_populate(pmd, __pa(pte) | _PAGE_KERNEL_TABLE);
 	pkmap_page_table = pte + PTRS_PER_PTE;
@@ -1005,11 +1010,8 @@ void __init paging_init(struct machine_desc *mdesc)
 
 	top_pmd = pmd_off_k(0xffff0000);
 
-	/*
-	 * allocate the zero page.  Note that this always succeeds and
-	 * returns a zeroed result.
-	 */
-	zero_page = alloc_bootmem_low_pages(PAGE_SIZE);
+	/* allocate the zero page. */
+	zero_page = early_alloc(PAGE_SIZE);
 	empty_zero_page = virt_to_page(zero_page);
 	__flush_dcache_page(NULL, empty_zero_page);
 }

commit 98c672cf1fa2a56f6f43e3f48b1208b83845582c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat May 22 18:18:57 2010 +0100

    ARM: Move platform memory reservations out of generic code
    
    Move the platform specific bootmem memory reservations out of
    arch/arm/mm/mmu.c into their respective platform files.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ddb1bee6de73..1676d017a93a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -17,7 +17,6 @@
 #include <linux/sort.h>
 
 #include <asm/cputype.h>
-#include <asm/mach-types.h>
 #include <asm/sections.h>
 #include <asm/cachetype.h>
 #include <asm/setup.h>
@@ -831,8 +830,6 @@ static inline void prepare_page_table(void)
  */
 void __init reserve_special_regions(void)
 {
-	unsigned long res_size = 0;
-
 	/*
 	 * Register the kernel text and data with bootmem.
 	 * Note that this can only be in node 0.
@@ -850,68 +847,14 @@ void __init reserve_special_regions(void)
 	reserve_bootmem(__pa(swapper_pg_dir),
 			PTRS_PER_PGD * sizeof(pgd_t), BOOTMEM_DEFAULT);
 
-	/*
-	 * Hmm... This should go elsewhere, but we really really need to
-	 * stop things allocating the low memory; ideally we need a better
-	 * implementation of GFP_DMA which does not assume that DMA-able
-	 * memory starts at zero.
-	 */
-	if (machine_is_integrator() || machine_is_cintegrator())
-		res_size = __pa(swapper_pg_dir) - PHYS_OFFSET;
-
-	/*
-	 * These should likewise go elsewhere.  They pre-reserve the
-	 * screen memory region at the start of main system memory.
-	 */
-	if (machine_is_edb7211())
-		res_size = 0x00020000;
-	if (machine_is_p720t())
-		res_size = 0x00014000;
-
-	/* H1940, RX3715 and RX1950 need to reserve this for suspend */
-
-	if (machine_is_h1940() || machine_is_rx3715()
-		|| machine_is_rx1950()) {
-		reserve_bootmem(0x30003000, 0x1000, BOOTMEM_DEFAULT);
-		reserve_bootmem(0x30081000, 0x1000, BOOTMEM_DEFAULT);
-	}
-
-	if (machine_is_palmld() || machine_is_palmtx()) {
-		reserve_bootmem(0xa0000000, 0x1000, BOOTMEM_EXCLUSIVE);
-		reserve_bootmem(0xa0200000, 0x1000, BOOTMEM_EXCLUSIVE);
-	}
-
-	if (machine_is_treo680() || machine_is_centro()) {
-		reserve_bootmem(0xa0000000, 0x1000, BOOTMEM_EXCLUSIVE);
-		reserve_bootmem(0xa2000000, 0x1000, BOOTMEM_EXCLUSIVE);
-	}
-
-	if (machine_is_palmt5())
-		reserve_bootmem(0xa0200000, 0x1000, BOOTMEM_EXCLUSIVE);
-
-	/*
-	 * U300 - This platform family can share physical memory
-	 * between two ARM cpus, one running Linux and the other
-	 * running another OS.
-	 */
-	if (machine_is_u300()) {
-#ifdef CONFIG_MACH_U300_SINGLE_RAM
-#if ((CONFIG_MACH_U300_ACCESS_MEM_SIZE & 1) == 1) &&	\
-	CONFIG_MACH_U300_2MB_ALIGNMENT_FIX
-		res_size = 0x00100000;
-#endif
-#endif
-	}
-
 #ifdef CONFIG_SA1111
 	/*
 	 * Because of the SA1111 DMA bug, we want to preserve our
 	 * precious DMA-able memory...
 	 */
-	res_size = __pa(swapper_pg_dir) - PHYS_OFFSET;
+	reserve_bootmem(PHYS_OFFSET, __pa(swapper_pg_dir) - PHYS_OFFSET,
+			BOOTMEM_DEFAULT);
 #endif
-	if (res_size)
-		reserve_bootmem(PHYS_OFFSET, res_size, BOOTMEM_DEFAULT);
 }
 
 /*
@@ -1056,7 +999,7 @@ void __init paging_init(struct machine_desc *mdesc)
 	sanity_check_meminfo();
 	prepare_page_table();
 	map_lowmem();
-	bootmem_init();
+	bootmem_init(mdesc);
 	devicemaps_init(mdesc);
 	kmap_init();
 

commit be370302742ff9948f2a42b15cb2ba174d97b930
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri May 7 17:40:33 2010 +0100

    ARM: Remove DISCONTIGMEM support
    
    Everything should now be using sparsemem rather than discontigmem, so
    remove the code supporting discontigmem from ARM.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6a08087ab022..ddb1bee6de73 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -827,9 +827,9 @@ static inline void prepare_page_table(void)
 }
 
 /*
- * Reserve the various regions of node 0
+ * Reserve the various regions
  */
-void __init reserve_node_zero(pg_data_t *pgdat)
+void __init reserve_special_regions(void)
 {
 	unsigned long res_size = 0;
 
@@ -838,19 +838,17 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	 * Note that this can only be in node 0.
 	 */
 #ifdef CONFIG_XIP_KERNEL
-	reserve_bootmem_node(pgdat, __pa(_data), _end - _data,
-			BOOTMEM_DEFAULT);
+	reserve_bootmem(__pa(_data), _end - _data, BOOTMEM_DEFAULT);
 #else
-	reserve_bootmem_node(pgdat, __pa(_stext), _end - _stext,
-			BOOTMEM_DEFAULT);
+	reserve_bootmem(__pa(_stext), _end - _stext, BOOTMEM_DEFAULT);
 #endif
 
 	/*
 	 * Reserve the page tables.  These are already in use,
 	 * and can only be in node 0.
 	 */
-	reserve_bootmem_node(pgdat, __pa(swapper_pg_dir),
-			     PTRS_PER_PGD * sizeof(pgd_t), BOOTMEM_DEFAULT);
+	reserve_bootmem(__pa(swapper_pg_dir),
+			PTRS_PER_PGD * sizeof(pgd_t), BOOTMEM_DEFAULT);
 
 	/*
 	 * Hmm... This should go elsewhere, but we really really need to
@@ -874,29 +872,22 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 
 	if (machine_is_h1940() || machine_is_rx3715()
 		|| machine_is_rx1950()) {
-		reserve_bootmem_node(pgdat, 0x30003000, 0x1000,
-				BOOTMEM_DEFAULT);
-		reserve_bootmem_node(pgdat, 0x30081000, 0x1000,
-				BOOTMEM_DEFAULT);
+		reserve_bootmem(0x30003000, 0x1000, BOOTMEM_DEFAULT);
+		reserve_bootmem(0x30081000, 0x1000, BOOTMEM_DEFAULT);
 	}
 
 	if (machine_is_palmld() || machine_is_palmtx()) {
-		reserve_bootmem_node(pgdat, 0xa0000000, 0x1000,
-				BOOTMEM_EXCLUSIVE);
-		reserve_bootmem_node(pgdat, 0xa0200000, 0x1000,
-				BOOTMEM_EXCLUSIVE);
+		reserve_bootmem(0xa0000000, 0x1000, BOOTMEM_EXCLUSIVE);
+		reserve_bootmem(0xa0200000, 0x1000, BOOTMEM_EXCLUSIVE);
 	}
 
 	if (machine_is_treo680() || machine_is_centro()) {
-		reserve_bootmem_node(pgdat, 0xa0000000, 0x1000,
-				BOOTMEM_EXCLUSIVE);
-		reserve_bootmem_node(pgdat, 0xa2000000, 0x1000,
-				BOOTMEM_EXCLUSIVE);
+		reserve_bootmem(0xa0000000, 0x1000, BOOTMEM_EXCLUSIVE);
+		reserve_bootmem(0xa2000000, 0x1000, BOOTMEM_EXCLUSIVE);
 	}
 
 	if (machine_is_palmt5())
-		reserve_bootmem_node(pgdat, 0xa0200000, 0x1000,
-				BOOTMEM_EXCLUSIVE);
+		reserve_bootmem(0xa0200000, 0x1000, BOOTMEM_EXCLUSIVE);
 
 	/*
 	 * U300 - This platform family can share physical memory
@@ -920,8 +911,7 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	res_size = __pa(swapper_pg_dir) - PHYS_OFFSET;
 #endif
 	if (res_size)
-		reserve_bootmem_node(pgdat, PHYS_OFFSET, res_size,
-				BOOTMEM_DEFAULT);
+		reserve_bootmem(PHYS_OFFSET, res_size, BOOTMEM_DEFAULT);
 }
 
 /*

commit 7961239599de71130c852ecfa9a4140f3f60547b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat May 22 16:20:14 2010 +0100

    ARM: Precalculate vmalloc_min
    
    Rather than storing the minimum size of the vmalloc area, store the
    maximum permitted address of the vmalloc area instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 285894171186..6a08087ab022 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -668,7 +668,7 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		create_mapping(io_desc + i);
 }
 
-static unsigned long __initdata vmalloc_reserve = SZ_128M;
+static void * __initdata vmalloc_min = (void *)(VMALLOC_END - SZ_128M);
 
 /*
  * vmalloc=size forces the vmalloc area to be exactly 'size'
@@ -677,7 +677,7 @@ static unsigned long __initdata vmalloc_reserve = SZ_128M;
  */
 static int __init early_vmalloc(char *arg)
 {
-	vmalloc_reserve = memparse(arg, NULL);
+	unsigned long vmalloc_reserve = memparse(arg, NULL);
 
 	if (vmalloc_reserve < SZ_16M) {
 		vmalloc_reserve = SZ_16M;
@@ -692,12 +692,12 @@ static int __init early_vmalloc(char *arg)
 			"vmalloc area is too big, limiting to %luMB\n",
 			vmalloc_reserve >> 20);
 	}
+
+	vmalloc_min = (void *)(VMALLOC_END - vmalloc_reserve);
 	return 0;
 }
 early_param("vmalloc", early_vmalloc);
 
-#define VMALLOC_MIN	(void *)(VMALLOC_END - vmalloc_reserve)
-
 static void __init sanity_check_meminfo(void)
 {
 	int i, j, highmem = 0;
@@ -707,7 +707,7 @@ static void __init sanity_check_meminfo(void)
 		*bank = meminfo.bank[i];
 
 #ifdef CONFIG_HIGHMEM
-		if (__va(bank->start) > VMALLOC_MIN ||
+		if (__va(bank->start) > vmalloc_min ||
 		    __va(bank->start) < (void *)PAGE_OFFSET)
 			highmem = 1;
 
@@ -717,8 +717,8 @@ static void __init sanity_check_meminfo(void)
 		 * Split those memory banks which are partially overlapping
 		 * the vmalloc area greatly simplifying things later.
 		 */
-		if (__va(bank->start) < VMALLOC_MIN &&
-		    bank->size > VMALLOC_MIN - __va(bank->start)) {
+		if (__va(bank->start) < vmalloc_min &&
+		    bank->size > vmalloc_min - __va(bank->start)) {
 			if (meminfo.nr_banks >= NR_BANKS) {
 				printk(KERN_CRIT "NR_BANKS too low, "
 						 "ignoring high memory\n");
@@ -727,12 +727,12 @@ static void __init sanity_check_meminfo(void)
 					(meminfo.nr_banks - i) * sizeof(*bank));
 				meminfo.nr_banks++;
 				i++;
-				bank[1].size -= VMALLOC_MIN - __va(bank->start);
-				bank[1].start = __pa(VMALLOC_MIN - 1) + 1;
+				bank[1].size -= vmalloc_min - __va(bank->start);
+				bank[1].start = __pa(vmalloc_min - 1) + 1;
 				bank[1].highmem = highmem = 1;
 				j++;
 			}
-			bank->size = VMALLOC_MIN - __va(bank->start);
+			bank->size = vmalloc_min - __va(bank->start);
 		}
 #else
 		bank->highmem = highmem;
@@ -741,7 +741,7 @@ static void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would entirely overlap
 		 * the vmalloc area.
 		 */
-		if (__va(bank->start) >= VMALLOC_MIN ||
+		if (__va(bank->start) >= vmalloc_min ||
 		    __va(bank->start) < (void *)PAGE_OFFSET) {
 			printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
 			       "(vmalloc region overlap).\n",
@@ -753,9 +753,9 @@ static void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would partially overlap
 		 * the vmalloc area.
 		 */
-		if (__va(bank->start + bank->size) > VMALLOC_MIN ||
+		if (__va(bank->start + bank->size) > vmalloc_min ||
 		    __va(bank->start + bank->size) < __va(bank->start)) {
-			unsigned long newsize = VMALLOC_MIN - __va(bank->start);
+			unsigned long newsize = vmalloc_min - __va(bank->start);
 			printk(KERN_NOTICE "Truncating RAM at %.8lx-%.8lx "
 			       "to -%.8lx (vmalloc region overlap).\n",
 			       bank->start, bank->start + bank->size - 1,

commit 349e1fba7e63b1067b5915337986060c13d9edd0
Merge: fa9dc265ace9 504d36e91ac1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 19 11:49:33 2010 -0700

    Merge branch 'for-linus/samsung4' of git://git.fluff.org/bjdooks/linux
    
    * 'for-linus/samsung4' of git://git.fluff.org/bjdooks/linux: (98 commits)
      Input: s3c24xx_ts - depend on SAMSUNG_DEV_TS and update menu entry
      Input: s3c24xx_ts - Add FEAT for Samsung touchscreen support
      Input: s3c24xx_ts - Implement generic GPIO configuration callback
      ARM: SAMSUNG: Move s3c64xx dev-ts.c to plat-samsung and rename configuration
      ARM: SAMSUNG: Implements cfg_gpio function for Samsung touchscreen
      ARM: S3C64XX: Add touchscreen platform device definition
      ARM: SAMSUNG: Move mach/ts.h to plat/ts.h
      ARM: S5PC100: Move i2c helpers from plat-s5pc1xx to mach-s5pc100
      ARM: S5PC100: Move frame buffer helpers from plat-s5pc1xx to mach-s5pc100
      ARM: S5PC100: gpio.h cleanup
      ARM: S5PC100: Move gpio support from plat-s5pc1xx to mach-s5pc100
      ARM: S5PC100: Use common functions for gpiolib implementation
      drivers: serial: S5PC100 serial driver cleanup
      ARM: S5PC100: Pre-requisite clock patch for plat-s5pc1xx to plat-s5p move
      ARM: SAMSUNG: Copy common I2C0 device helpers to machine directories
      ARM: SAMSUNG: move driver strength gpio configuration helper to common dir
      ARM: S5PV210: Add GPIOlib support
      ARM: SAMSUNGy: fix broken timer irq base
      ARM: SMDK6440: Add audio devices on board
      ARM: S5P6440: Add audio platform devices
      ...

commit ac1d426e825ab5778995f2f6f053ca2e6b45c622
Merge: fda0e18c8a7a a3685f00652a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon May 17 17:24:04 2010 +0100

    Merge branch 'devel-stable' into devel
    
    Conflicts:
            arch/arm/Kconfig
            arch/arm/include/asm/system.h
            arch/arm/mm/Kconfig

commit a2227120eead4ea7d2ea04d8ce0947f1dd23dedf
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Mar 25 18:56:05 2010 +0000

    ARM: Move memory mapping into mmu.c
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 91547196915c..69852003675f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -600,7 +600,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
  * offsets, and we take full advantage of sections and
  * supersections.
  */
-void __init create_mapping(struct map_desc *md)
+static void __init create_mapping(struct map_desc *md)
 {
 	unsigned long phys, addr, length, end;
 	const struct mem_type *type;
@@ -1014,6 +1014,32 @@ static void __init kmap_init(void)
 #endif
 }
 
+static inline void map_memory_bank(struct membank *bank)
+{
+	struct map_desc map;
+
+	map.pfn = bank_pfn_start(bank);
+	map.virtual = __phys_to_virt(bank_phys_start(bank));
+	map.length = bank_phys_size(bank);
+	map.type = MT_MEMORY;
+
+	create_mapping(&map);
+}
+
+static void __init map_lowmem(void)
+{
+	struct meminfo *mi = &meminfo;
+	int i;
+
+	/* Map all the lowmem memory banks. */
+	for (i = 0; i < mi->nr_banks; i++) {
+		struct membank *bank = &mi->bank[i];
+
+		if (!bank->highmem)
+			map_memory_bank(bank);
+	}
+}
+
 static int __init meminfo_cmp(const void *_a, const void *_b)
 {
 	const struct membank *a = _a, *b = _b;
@@ -1034,6 +1060,7 @@ void __init paging_init(struct machine_desc *mdesc)
 	build_mem_type_table();
 	sanity_check_meminfo();
 	prepare_page_table();
+	map_lowmem();
 	bootmem_init();
 	devicemaps_init(mdesc);
 	kmap_init();

commit ceb683d3bc36f213aeef0c5d79e6fbb1e16bd459
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Mar 25 18:47:20 2010 +0000

    ARM: Ensure meminfo is sorted prior to sanity_check_meminfo
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9d4da6ac28eb..91547196915c 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -14,6 +14,7 @@
 #include <linux/bootmem.h>
 #include <linux/mman.h>
 #include <linux/nodemask.h>
+#include <linux/sort.h>
 
 #include <asm/cputype.h>
 #include <asm/mach-types.h>
@@ -1013,6 +1014,13 @@ static void __init kmap_init(void)
 #endif
 }
 
+static int __init meminfo_cmp(const void *_a, const void *_b)
+{
+	const struct membank *a = _a, *b = _b;
+	long cmp = bank_pfn_start(a) - bank_pfn_start(b);
+	return cmp < 0 ? -1 : cmp > 0 ? 1 : 0;
+}
+
 /*
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.
@@ -1021,6 +1029,8 @@ void __init paging_init(struct machine_desc *mdesc)
 {
 	void *zero_page;
 
+	sort(&meminfo.bank, meminfo.nr_banks, sizeof(meminfo.bank[0]), meminfo_cmp, NULL);
+
 	build_mem_type_table();
 	sanity_check_meminfo();
 	prepare_page_table();

commit 0741b7d2699d1e7b4ceab27522c9e5518b2dbd2c
Author: Vasily Khoruzhick <anarsoul@gmail.com>
Date:   Tue May 11 09:55:10 2010 +0300

    ARM: RX1950: Add suspend/resume support for RX1950
    
    Signed-off-by: Vasily Khoruzhick <anarsoul@gmail.com>
    Signed-off-by: Ben Dooks <ben-linux@fluff.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 241c24a1c18f..45a1bc275f0a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -869,9 +869,10 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	if (machine_is_p720t())
 		res_size = 0x00014000;
 
-	/* H1940 and RX3715 need to reserve this for suspend */
+	/* H1940, RX3715 and RX1950 need to reserve this for suspend */
 
-	if (machine_is_h1940() || machine_is_rx3715()) {
+	if (machine_is_h1940() || machine_is_rx3715()
+		|| machine_is_rx1950()) {
 		reserve_bootmem_node(pgdat, 0x30003000, 0x1000,
 				BOOTMEM_DEFAULT);
 		reserve_bootmem_node(pgdat, 0x30081000, 0x1000,

commit 3f2d4f561fab4588344cc519fd323382ab950928
Author: Mika Westerberg <mika.westerberg@iki.fi>
Date:   Tue Apr 13 07:01:46 2010 +0100

    ARM: 6052/1: kdump: make kexec work in interrupt context
    
    When crash happens in interrupt context there is no userspace context.
    We always use current->active_mm in those cases.
    
    Signed-off-by: Mika Westerberg <ext-mika.1.westerberg@nokia.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4223d086aa17..241c24a1c18f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1054,10 +1054,12 @@ void setup_mm_for_reboot(char mode)
 	pgd_t *pgd;
 	int i;
 
-	if (current->mm && current->mm->pgd)
-		pgd = current->mm->pgd;
-	else
-		pgd = init_mm.pgd;
+	/*
+	 * We need to access to user-mode page tables here. For kernel threads
+	 * we don't have any user-mode mappings so we use the context that we
+	 * "borrowed".
+	 */
+	pgd = current->active_mm->pgd;
 
 	base_pmdval = PMD_SECT_AP_WRITE | PMD_SECT_AP_READ | PMD_TYPE_SECT;
 	if (cpu_architecture() <= CPU_ARCH_ARMv5TEJ && !cpu_is_xscale())

commit 85b3cce880a19e78286570d5fd004cc3cac06f57
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 9 15:00:11 2010 +0100

    ARM: Fix ioremap_cached()/ioremap_wc() for SMP platforms
    
    Write combining/cached device mappings are not setting the shared bit,
    which could potentially cause problems on SMP systems since the cache
    lines won't participate in the cache coherency protocol.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9d4da6ac28eb..4223d086aa17 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -420,6 +420,10 @@ static void __init build_mem_type_table(void)
 		user_pgprot |= L_PTE_SHARED;
 		kern_pgprot |= L_PTE_SHARED;
 		vecs_pgprot |= L_PTE_SHARED;
+		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_S;
+		mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_SHARED;
+		mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_S;
+		mem_types[MT_DEVICE_CACHED].prot_pte |= L_PTE_SHARED;
 		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
 		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
 #endif

commit 9f33be2c3a80bdc2cc08342dd77fac87652e0548
Merge: 2741ecb4ce5c eed18b5fa4d2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Feb 25 22:10:38 2010 +0000

    Merge branches 'clks' and 'pnx' into devel

commit 2b0d8c251b8876d530a6bf671eb5425838fa698a
Author: Jeremy Kerr <jeremy.kerr@canonical.com>
Date:   Mon Jan 11 23:17:34 2010 +0100

    ARM: 5880/1: arm: use generic infrastructure for early params
    
    The ARM setup code includes its own parser for early params, there's
    also one in the generic init code.
    
    This patch removes __early_init (and related code) from
    arch/arm/kernel/setup.c, and changes users to the generic early_init
    macro instead.
    
    The generic macro takes a char * argument, rather than char **, so we
    need to update the parser functions a little.
    
    Signed-off-by: Jeremy Kerr <jeremy.kerr@canonical.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 1708da82da96..88f5d71248d9 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -100,18 +100,17 @@ static struct cachepolicy cache_policies[] __initdata = {
  * writebuffer to be turned off.  (Note: the write
  * buffer should not be on and the cache off).
  */
-static void __init early_cachepolicy(char **p)
+static int __init early_cachepolicy(char *p)
 {
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(cache_policies); i++) {
 		int len = strlen(cache_policies[i].policy);
 
-		if (memcmp(*p, cache_policies[i].policy, len) == 0) {
+		if (memcmp(p, cache_policies[i].policy, len) == 0) {
 			cachepolicy = i;
 			cr_alignment &= ~cache_policies[i].cr_mask;
 			cr_no_alignment &= ~cache_policies[i].cr_mask;
-			*p += len;
 			break;
 		}
 	}
@@ -130,36 +129,37 @@ static void __init early_cachepolicy(char **p)
 	}
 	flush_cache_all();
 	set_cr(cr_alignment);
+	return 0;
 }
-__early_param("cachepolicy=", early_cachepolicy);
+early_param("cachepolicy", early_cachepolicy);
 
-static void __init early_nocache(char **__unused)
+static int __init early_nocache(char *__unused)
 {
 	char *p = "buffered";
 	printk(KERN_WARNING "nocache is deprecated; use cachepolicy=%s\n", p);
-	early_cachepolicy(&p);
+	early_cachepolicy(p);
+	return 0;
 }
-__early_param("nocache", early_nocache);
+early_param("nocache", early_nocache);
 
-static void __init early_nowrite(char **__unused)
+static int __init early_nowrite(char *__unused)
 {
 	char *p = "uncached";
 	printk(KERN_WARNING "nowb is deprecated; use cachepolicy=%s\n", p);
-	early_cachepolicy(&p);
+	early_cachepolicy(p);
+	return 0;
 }
-__early_param("nowb", early_nowrite);
+early_param("nowb", early_nowrite);
 
-static void __init early_ecc(char **p)
+static int __init early_ecc(char *p)
 {
-	if (memcmp(*p, "on", 2) == 0) {
+	if (memcmp(p, "on", 2) == 0)
 		ecc_mask = PMD_PROTECTION;
-		*p += 2;
-	} else if (memcmp(*p, "off", 3) == 0) {
+	else if (memcmp(p, "off", 3) == 0)
 		ecc_mask = 0;
-		*p += 3;
-	}
+	return 0;
 }
-__early_param("ecc=", early_ecc);
+early_param("ecc", early_ecc);
 
 static int __init noalign_setup(char *__unused)
 {
@@ -670,9 +670,9 @@ static unsigned long __initdata vmalloc_reserve = SZ_128M;
  * bytes. This can be used to increase (or decrease) the vmalloc
  * area - the default is 128m.
  */
-static void __init early_vmalloc(char **arg)
+static int __init early_vmalloc(char *arg)
 {
-	vmalloc_reserve = memparse(*arg, arg);
+	vmalloc_reserve = memparse(arg, NULL);
 
 	if (vmalloc_reserve < SZ_16M) {
 		vmalloc_reserve = SZ_16M;
@@ -687,8 +687,9 @@ static void __init early_vmalloc(char **arg)
 			"vmalloc area is too big, limiting to %luMB\n",
 			vmalloc_reserve >> 20);
 	}
+	return 0;
 }
-__early_param("vmalloc=", early_vmalloc);
+early_param("vmalloc", early_vmalloc);
 
 #define VMALLOC_MIN	(void *)(VMALLOC_END - vmalloc_reserve)
 

commit ad3e6c0b1fad31282a8384903ed811671d840a9c
Author: Tony Lindgren <tony@atomide.com>
Date:   Tue Jan 19 16:42:12 2010 +0100

    ARM: 5885/1: arm: Flush TLB entries in setup_mm_for_reboot()
    
    We need to do that if we tinker with the MMU entries.
    
    This fixes the occasional bug with kexec where the new
    fails to uncompress with "crc error". Most likely at
    least kexec on v6 and v7 need this fix.
    
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 1708da82da96..761ffede6a23 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1067,4 +1067,6 @@ void setup_mm_for_reboot(char mode)
 		pmd[1] = __pmd(pmdval + (1 << (PGDIR_SHIFT - 1)));
 		flush_pmd_entry(pmd);
 	}
+
+	local_flush_tlb_all();
 }

commit 6dc995a3da9adfa83d61ccf06aa1afa5f6ab764f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Dec 24 10:16:21 2009 +0000

    ARM: fix PAGE_KERNEL
    
    PAGE_KERNEL should not be executable; any area marked executable can
    be prefetched into the instruction cache.  We don't want vmalloc areas
    to be read in this way.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 8c7fbd19a4b3..1708da82da96 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -453,8 +453,7 @@ static void __init build_mem_type_table(void)
 
 	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
-				 L_PTE_DIRTY | L_PTE_WRITE |
-				 L_PTE_EXEC | kern_pgprot);
+				 L_PTE_DIRTY | L_PTE_WRITE | kern_pgprot);
 
 	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
 	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;

commit 0719dc341389882cc834ed18fc9b7fc6006b2b85
Merge: e28edb723e64 677f4f64e4b2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Dec 5 10:35:33 2009 +0000

    Merge branch 'devel-stable' into devel

commit 421fe93cc4b06b2f5e875cbe0f692800d4862ee5
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Oct 25 10:23:04 2009 +0000

    ARM: ZERO_PAGE: Avoid flush_dcache_page() for zero page
    
    The zero page is read-only, and has its cache state cleared during
    boot.  No further maintanence for this page is required.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ea67be0223ac..2427cdcd9098 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -1036,7 +1036,7 @@ void __init paging_init(struct machine_desc *mdesc)
 	 */
 	zero_page = alloc_bootmem_low_pages(PAGE_SIZE);
 	empty_zero_page = virt_to_page(zero_page);
-	flush_dcache_page(empty_zero_page);
+	__flush_dcache_page(NULL, empty_zero_page);
 }
 
 /*

commit d0a92fd3b84bf707f6b32f31d0f09d2b7bb1ad67
Author: Tom ech <sleep_walker@suse.cz>
Date:   Fri Sep 11 13:57:02 2009 +0200

    [ARM] pxa/treo: add Palm Centro 685 support
    
    Signed-off-by: Tom ech <sleep_walker@suse.cz>
    Signed-off-by: Eric Miao <eric.y.miao@gmail.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ea67be0223ac..3e8556b16a9f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -881,7 +881,7 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 				BOOTMEM_EXCLUSIVE);
 	}
 
-	if (machine_is_treo680()) {
+	if (machine_is_treo680() || machine_is_centro()) {
 		reserve_bootmem_node(pgdat, 0xa0000000, 0x1000,
 				BOOTMEM_EXCLUSIVE);
 		reserve_bootmem_node(pgdat, 0xa2000000, 0x1000,

commit 4b46d6416548fb6a0940dfd9911fd895eb6247b3
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Nov 1 17:44:24 2009 +0000

    ARM: ensure initial page tables are setup for SMP systems
    
    Mapping the same memory using two different attributes (memory
    type, shareability, cacheability) is unpredictable.  During boot,
    we encounter a situation when we're updating the kernel's page
    tables which can lead to dirty cache lines existing in the cache
    which are subsequently missed.  This causes stack corruption,
    and therefore a crash.
    
    Therefore, ensure that the shared and cacheability settings
    matches the configuration that will be used later; this together
    with the restriction in early_cachepolicy() ensures that we won't
    create a mismatch during boot.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 02243eeccf50..ea67be0223ac 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -117,6 +117,13 @@ static void __init early_cachepolicy(char **p)
 	}
 	if (i == ARRAY_SIZE(cache_policies))
 		printk(KERN_ERR "ERROR: unknown or unsupported cache policy\n");
+	/*
+	 * This restriction is partly to do with the way we boot; it is
+	 * unpredictable to have memory mapped using two different sets of
+	 * memory attributes (shared, type, and cache attribs).  We can not
+	 * change these attributes once the initial assembly has setup the
+	 * page tables.
+	 */
 	if (cpu_architecture() >= CPU_ARCH_ARMv6) {
 		printk(KERN_WARNING "Only cachepolicy=writeback supported on ARMv6 and later\n");
 		cachepolicy = CPOLICY_WRITEBACK;

commit e616c591405c168f6dc3dfd1221e105adfe49b8d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Sep 27 20:55:43 2009 +0100

    ARM: Don't allow highmem on SMP platforms without h/w TLB ops broadcast
    
    We suffer an unfortunate combination of "features" which makes highmem
    support on platforms without hardware TLB maintainence broadcast difficult:
    
    - we need kmap_high_get() support for DMA cache coherence
    - this requires kmap_high() to take a spinlock with IRQs disabled
    - kmap_high() occasionally calls flush_all_zero_pkmaps() to clear
      out old mappings
    - flush_all_zero_pkmaps() calls flush_tlb_kernel_range(), which
      on s/w IPI'd systems eventually calls smp_call_function_many()
    - smp_call_function_many() must not be called with IRQs disabled:
    
    WARNING: at kernel/smp.c:380 smp_call_function_many+0xc4/0x240()
    Modules linked in:
    Backtrace:
    [<c00306f0>] (dump_backtrace+0x0/0x108) from [<c0286e6c>] (dump_stack+0x18/0x1c)
     r6:c007cd18 r5:c02ff228 r4:0000017c
    [<c0286e54>] (dump_stack+0x0/0x1c) from [<c0053e08>] (warn_slowpath_common+0x50/0x80)
    [<c0053db8>] (warn_slowpath_common+0x0/0x80) from [<c0053e50>] (warn_slowpath_null+0x18/0x1c)
     r7:00000003 r6:00000001 r5:c1ff4000 r4:c035fa34
    [<c0053e38>] (warn_slowpath_null+0x0/0x1c) from [<c007cd18>] (smp_call_function_many+0xc4/0x240)
    [<c007cc54>] (smp_call_function_many+0x0/0x240) from [<c007cec0>] (smp_call_function+0x2c/0x38)
    [<c007ce94>] (smp_call_function+0x0/0x38) from [<c005980c>] (on_each_cpu+0x1c/0x38)
    [<c00597f0>] (on_each_cpu+0x0/0x38) from [<c0031788>] (flush_tlb_kernel_range+0x50/0x58)
     r6:00000001 r5:00000800 r4:c05f3590
    [<c0031738>] (flush_tlb_kernel_range+0x0/0x58) from [<c009c600>] (flush_all_zero_pkmaps+0xc0/0xe8)
    [<c009c540>] (flush_all_zero_pkmaps+0x0/0xe8) from [<c009c6b4>] (kmap_high+0x8c/0x1e0)
    [<c009c628>] (kmap_high+0x0/0x1e0) from [<c00364a8>] (kmap+0x44/0x5c)
    [<c0036464>] (kmap+0x0/0x5c) from [<c0109dfc>] (cramfs_readpage+0x3c/0x194)
    [<c0109dc0>] (cramfs_readpage+0x0/0x194) from [<c0090c14>] (__do_page_cache_readahead+0x1f0/0x290)
    [<c0090a24>] (__do_page_cache_readahead+0x0/0x290) from [<c0090ce4>] (ra_submit+0x30/0x38)
    [<c0090cb4>] (ra_submit+0x0/0x38) from [<c0089384>] (filemap_fault+0x3dc/0x438)
     r4:c1819988
    [<c0088fa8>] (filemap_fault+0x0/0x438) from [<c009d21c>] (__do_fault+0x58/0x43c)
    [<c009d1c4>] (__do_fault+0x0/0x43c) from [<c009e8cc>] (handle_mm_fault+0x104/0x318)
    [<c009e7c8>] (handle_mm_fault+0x0/0x318) from [<c0033c98>] (do_page_fault+0x188/0x1e4)
    [<c0033b10>] (do_page_fault+0x0/0x1e4) from [<c0033ddc>] (do_translation_fault+0x7c/0x84)
    [<c0033d60>] (do_translation_fault+0x0/0x84) from [<c002b474>] (do_DataAbort+0x40/0xa4)
     r8:c1ff5e20 r7:c0340120 r6:00000805 r5:c1ff5e54 r4:c03400d0
    [<c002b434>] (do_DataAbort+0x0/0xa4) from [<c002bcac>] (__dabt_svc+0x4c/0x60)
    ...
    
    So we disable highmem support on these systems.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ce551ec2cb23..02243eeccf50 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -21,6 +21,7 @@
 #include <asm/cachetype.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>
+#include <asm/smp_plat.h>
 #include <asm/tlb.h>
 #include <asm/highmem.h>
 
@@ -709,10 +710,6 @@ static void __init sanity_check_meminfo(void)
 			if (meminfo.nr_banks >= NR_BANKS) {
 				printk(KERN_CRIT "NR_BANKS too low, "
 						 "ignoring high memory\n");
-			} else if (cache_is_vipt_aliasing()) {
-				printk(KERN_CRIT "HIGHMEM is not yet supported "
-						 "with VIPT aliasing cache, "
-						 "ignoring high memory\n");
 			} else {
 				memmove(bank + 1, bank,
 					(meminfo.nr_banks - i) * sizeof(*bank));
@@ -756,6 +753,38 @@ static void __init sanity_check_meminfo(void)
 #endif
 		j++;
 	}
+#ifdef CONFIG_HIGHMEM
+	if (highmem) {
+		const char *reason = NULL;
+
+		if (cache_is_vipt_aliasing()) {
+			/*
+			 * Interactions between kmap and other mappings
+			 * make highmem support with aliasing VIPT caches
+			 * rather difficult.
+			 */
+			reason = "with VIPT aliasing cache";
+#ifdef CONFIG_SMP
+		} else if (tlb_ops_need_broadcast()) {
+			/*
+			 * kmap_high needs to occasionally flush TLB entries,
+			 * however, if the TLB entries need to be broadcast
+			 * we may deadlock:
+			 *  kmap_high(irqs off)->flush_all_zero_pkmaps->
+			 *  flush_tlb_kernel_range->smp_call_function_many
+			 *   (must not be called with irqs off)
+			 */
+			reason = "without hardware TLB ops broadcasting";
+#endif
+		}
+		if (reason) {
+			printk(KERN_CRIT "HIGHMEM is not supported %s, ignoring high memory\n",
+				reason);
+			while (j > 0 && meminfo.bank[j - 1].highmem)
+				j--;
+		}
+	}
+#endif
 	meminfo.nr_banks = j;
 }
 

commit 041d785f807c8e92e11cc1bd2b3438f7823e63d2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Sep 27 17:40:42 2009 +0100

    ARM: Fix warning: unused variable 'highmem'
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4426ee67ceca..ce551ec2cb23 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -726,6 +726,8 @@ static void __init sanity_check_meminfo(void)
 			bank->size = VMALLOC_MIN - __va(bank->start);
 		}
 #else
+		bank->highmem = highmem;
+
 		/*
 		 * Check whether this memory bank would entirely overlap
 		 * the vmalloc area.

commit dde5828f56cb2c1aa70365c476e6830482127258
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 15 12:36:00 2009 +0100

    ARM: Fix broken highmem support
    
    Currently, highmem is selectable, and you can request an increased
    vmalloc area.  However, none of this has any effect on the memory
    layout since a patch in the highmem series was accidentally dropped.
    Moreover, even if you did want highmem, all memory would still be
    registered as lowmem, possibly resulting in overflow of the available
    virtual mapping space.
    
    The highmem boundary is determined by the highest allowed beginning
    of the vmalloc area, which depends on its configurable minimum size
    (see commit 60296c71f6c5063e3c1f1d2619ca0b60940162e7 for details on
    this).
    
    We should create mappings and initialize bootmem only for low memory,
    while the zone allocator must still be told about highmem.
    
    Currently, memory nodes which are completely located in high memory
    are not supported.  This is not a huge limitation since systems
    relying on highmem support are unlikely to have discontiguous memory
    with large holes.
    
    [ A similar patch was meant to be merged before commit 5f0fbf9ecaf3
      and be available  in Linux v2.6.30, however some git rebase screw-up
      of mine dropped the first commit of the series, and that goofage
      escaped testing somehow as well. -- Nico ]
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Reviewed-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4722582b17b8..4426ee67ceca 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -687,13 +687,19 @@ __early_param("vmalloc=", early_vmalloc);
 
 static void __init sanity_check_meminfo(void)
 {
-	int i, j;
+	int i, j, highmem = 0;
 
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
 		struct membank *bank = &meminfo.bank[j];
 		*bank = meminfo.bank[i];
 
 #ifdef CONFIG_HIGHMEM
+		if (__va(bank->start) > VMALLOC_MIN ||
+		    __va(bank->start) < (void *)PAGE_OFFSET)
+			highmem = 1;
+
+		bank->highmem = highmem;
+
 		/*
 		 * Split those memory banks which are partially overlapping
 		 * the vmalloc area greatly simplifying things later.
@@ -714,6 +720,7 @@ static void __init sanity_check_meminfo(void)
 				i++;
 				bank[1].size -= VMALLOC_MIN - __va(bank->start);
 				bank[1].start = __pa(VMALLOC_MIN - 1) + 1;
+				bank[1].highmem = highmem = 1;
 				j++;
 			}
 			bank->size = VMALLOC_MIN - __va(bank->start);

commit 187f81b3d8d315c35c73ac0d05b15a04a0ac3ce7
Merge: 1d89b30cc9be d78ff0a50aac
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Jun 18 23:09:52 2009 +0100

    Merge branch 'for-rmk' of git://git.kernel.org/pub/scm/linux/kernel/git/ycmiao/pxa-linux-2.6 into devel

commit e6c3f4b89bf3698a6994d30de7a16ae395a81dab
Author: Tomas 'Sleep_Walker' Cech <sleep_walker@suse.cz>
Date:   Mon May 18 15:24:14 2009 +0200

    [ARM] pxa/treo680: initial support
    
    Signed-off-by: Tom ech <sleep_walker@suse.cz>
    Acked-by: Marek Vasut <marek.vasut@gmail.com>
    Signed-off-by: Eric Miao <eric.miao@marvell.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e6344ece00ce..0b91bb20cf95 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -835,6 +835,13 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 				BOOTMEM_EXCLUSIVE);
 	}
 
+	if (machine_is_treo680()) {
+		reserve_bootmem_node(pgdat, 0xa0000000, 0x1000,
+				BOOTMEM_EXCLUSIVE);
+		reserve_bootmem_node(pgdat, 0xa2000000, 0x1000,
+				BOOTMEM_EXCLUSIVE);
+	}
+
 	if (machine_is_palmt5())
 		reserve_bootmem_node(pgdat, 0xa0200000, 0x1000,
 				BOOTMEM_EXCLUSIVE);

commit b7c11ec9f1c9f4272f032215ddb0b65d64b22ecc
Merge: 06099961002a 7666c17e2b09
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Jun 14 11:01:44 2009 +0100

    Merge branch 'u300' into devel
    
    Conflicts:
            arch/arm/Makefile
    Updates:
            arch/arm/mach-u300/core.c
            arch/arm/mach-u300/timer.c

commit 69d3a84a646d6ad6cd693a7a3d5b9af414113d2c
Author: Hiroshi DOYU <Hiroshi.DOYU@nokia.com>
Date:   Wed Jan 28 21:32:08 2009 +0200

    omap iommu: simple virtual address space management
    
    This patch provides a device drivers, which has a omap iommu, with
    address mapping APIs between device virtual address(iommu), physical
    address and MPU virtual address.
    
    There are 4 possible patterns for iommu virtual address(iova/da) mapping.
    
        |iova/                        mapping               iommu_          page
        | da        pa      va      (d)-(p)-(v)             function        type
      ---------------------------------------------------------------------------
      1 | c         c       c        1 - 1 - 1        _kmap() / _kunmap()   s
      2 | c         c,a     c        1 - 1 - 1      _kmalloc()/ _kfree()    s
      3 | c         d       c        1 - n - 1        _vmap() / _vunmap()   s
      4 | c         d,a     c        1 - n - 1      _vmalloc()/ _vfree()    n*
    
        'iova':     device iommu virtual address
        'da':       alias of 'iova'
        'pa':       physical address
        'va':       mpu virtual address
    
        'c':        contiguous memory area
        'd':        dicontiguous memory area
        'a':        anonymous memory allocation
        '()':       optional feature
    
        'n':        a normal page(4KB) size is used.
        's':        multiple iommu superpage(16MB, 1MB, 64KB, 4KB) size is used.
    
        '*':        not yet, but feasible.
    
    Signed-off-by: Hiroshi DOYU <Hiroshi.DOYU@nokia.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e6344ece00ce..70974d75a075 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -255,6 +255,7 @@ const struct mem_type *get_mem_type(unsigned int type)
 {
 	return type < ARRAY_SIZE(mem_types) ? &mem_types[type] : NULL;
 }
+EXPORT_SYMBOL(get_mem_type);
 
 /*
  * Adjust the PMD section entries according to the CPU in use.

commit d98aac7592114241f378bc8d5b3e424cced7ded2
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Mon Apr 27 10:21:46 2009 +0100

    [ARM] 5480/1: U300-v5 integrate into the ARM architecture
    
    This hooks the U300 support into Kbuild and makes a small hook
    in mmu.c for supporting an odd memory alignment with shared memory
    on these systems.
    
    This is rebased to RMK:s GIT HEAD. This patch tries to add the
    Kconfig option in alphabetic order by option text and the Makefile
    entry after config symbol.
    
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e6344ece00ce..39fca4e416e2 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -839,6 +839,20 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 		reserve_bootmem_node(pgdat, 0xa0200000, 0x1000,
 				BOOTMEM_EXCLUSIVE);
 
+	/*
+	 * U300 - This platform family can share physical memory
+	 * between two ARM cpus, one running Linux and the other
+	 * running another OS.
+	 */
+	if (machine_is_u300()) {
+#ifdef CONFIG_MACH_U300_SINGLE_RAM
+#if ((CONFIG_MACH_U300_ACCESS_MEM_SIZE & 1) == 1) &&	\
+	CONFIG_MACH_U300_2MB_ALIGNMENT_FIX
+		res_size = 0x00100000;
+#endif
+#endif
+	}
+
 #ifdef CONFIG_SA1111
 	/*
 	 * Because of the SA1111 DMA bug, we want to preserve our

commit 81854f82c5c1a203b2f5c94f6aa2ed8b8e19f025
Author: Marek Vasut <marek.vasut@gmail.com>
Date:   Sat Mar 28 12:37:42 2009 +0100

    [ARM] pxa: Add support for suspend on PalmTX, T5 and LD
    
    Signed-off-by: Marek Vasut <marek.vasut@gmail.com>
    Signed-off-by: Eric Miao <eric.miao@marvell.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b438fc4fb77b..e6344ece00ce 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -828,6 +828,17 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 				BOOTMEM_DEFAULT);
 	}
 
+	if (machine_is_palmld() || machine_is_palmtx()) {
+		reserve_bootmem_node(pgdat, 0xa0000000, 0x1000,
+				BOOTMEM_EXCLUSIVE);
+		reserve_bootmem_node(pgdat, 0xa0200000, 0x1000,
+				BOOTMEM_EXCLUSIVE);
+	}
+
+	if (machine_is_palmt5())
+		reserve_bootmem_node(pgdat, 0xa0200000, 0x1000,
+				BOOTMEM_EXCLUSIVE);
+
 #ifdef CONFIG_SA1111
 	/*
 	 * Because of the SA1111 DMA bug, we want to preserve our

commit 9759d22c8348343b0da4e25d6150c41712686c14
Merge: ed40d0c472b1 f0bba9f93451
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Mar 28 20:30:18 2009 +0000

    Merge branch 'master' into devel
    
    Conflicts:
            arch/arm/include/asm/elf.h
            arch/arm/kernel/module.c

commit f0bba9f934517533acbda7329be93f55d5a01c03
Author: Mikael Pettersson <mikpe@it.uu.se>
Date:   Sat Mar 28 19:18:05 2009 +0100

    [ARM] 5435/1: fix compile warning in sanity_check_meminfo()
    
    Compiling recent 2.6.29-rc kernels for ARM gives me the following warning:
    
    arch/arm/mm/mmu.c: In function 'sanity_check_meminfo':
    arch/arm/mm/mmu.c:697: warning: comparison between pointer and integer
    
    This is because commit 3fd9825c42c784a59b3b90bdf073f49d4bb42a8d
    "[ARM] 5402/1: fix a case of wrap-around in sanity_check_meminfo()"
    in 2.6.29-rc5-git4 added a comparison of a pointer with PAGE_OFFSET,
    which is an integer.
    
    Fixed by casting PAGE_OFFSET to void *.
    
    Signed-off-by: Mikael Pettersson <mikpe@it.uu.se>
    Acked-by: Nicolas Pitre <nico@cam.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d4d082c5c2d4..5a89e57e342d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -694,7 +694,7 @@ static void __init sanity_check_meminfo(void)
 		 * the vmalloc area.
 		 */
 		if (__va(bank->start) >= VMALLOC_MIN ||
-		    __va(bank->start) < PAGE_OFFSET) {
+		    __va(bank->start) < (void *)PAGE_OFFSET) {
 			printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
 			       "(vmalloc region overlap).\n",
 			       bank->start, bank->start + bank->size - 1);

commit fbf2b1f9cfdb4e4b5d042839142ed19ff5d46679
Merge: 9a38e989b8ce 053a96ca11a9
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Mar 24 22:47:45 2009 +0000

    Merge branch 'highmem' into devel

commit 3f973e22160257c5bda85815be5b1540d391a671
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Nov 4 00:48:42 2008 -0500

    [ARM] ignore high memory with VIPT aliasing caches
    
    VIPT aliasing caches have issues of their own which are not yet handled.
    Usage of discard_old_kernel_data() in copypage-v6.c is not highmem ready,
    kmap/fixmap stuff doesn't take account of cache colouring, etc.
    If/when those issues are handled then this could be reverted.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 4810a4c9ffce..cf504885a5fb 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -18,6 +18,7 @@
 #include <asm/cputype.h>
 #include <asm/mach-types.h>
 #include <asm/sections.h>
+#include <asm/cachetype.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>
 #include <asm/tlb.h>
@@ -678,6 +679,10 @@ static void __init sanity_check_meminfo(void)
 			if (meminfo.nr_banks >= NR_BANKS) {
 				printk(KERN_CRIT "NR_BANKS too low, "
 						 "ignoring high memory\n");
+			} else if (cache_is_vipt_aliasing()) {
+				printk(KERN_CRIT "HIGHMEM is not yet supported "
+						 "with VIPT aliasing cache, "
+						 "ignoring high memory\n");
 			} else {
 				memmove(bank + 1, bank,
 					(meminfo.nr_banks - i) * sizeof(*bank));

commit d73cd42893f4cdc06e6829fea2347bb92cb789d1
Author: Nicolas Pitre <nico@cam.org>
Date:   Mon Sep 15 16:44:55 2008 -0400

    [ARM] kmap support
    
    The kmap virtual area borrows a 2MB range at the top of the 16MB area
    below PAGE_OFFSET currently reserved for kernel modules and/or the
    XIP kernel.  This 2MB corresponds to the range covered by 2 consecutive
    second-level page tables, or a single pmd entry as seen by the Linux
    page table abstraction.  Because XIP kernels are unlikely to be seen
    on systems needing highmem support, there shouldn't be any shortage of
    VM space for modules (14 MB for modules is still way more than twice the
    typical usage).
    
    Because the virtual mapping of highmem pages can go away at any moment
    after kunmap() is called on them, we need to bypass the delayed cache
    flushing provided by flush_dcache_page() in that case.
    
    The atomic kmap versions are based on fixmaps, and
    __cpuc_flush_dcache_page() is used directly in that case.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d4d082c5c2d4..4810a4c9ffce 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -21,6 +21,7 @@
 #include <asm/setup.h>
 #include <asm/sizes.h>
 #include <asm/tlb.h>
+#include <asm/highmem.h>
 
 #include <asm/mach/arch.h>
 #include <asm/mach/map.h>
@@ -895,6 +896,17 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	flush_cache_all();
 }
 
+static void __init kmap_init(void)
+{
+#ifdef CONFIG_HIGHMEM
+	pmd_t *pmd = pmd_off_k(PKMAP_BASE);
+	pte_t *pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+	BUG_ON(!pmd_none(*pmd) || !pte);
+	__pmd_populate(pmd, __pa(pte) | _PAGE_KERNEL_TABLE);
+	pkmap_page_table = pte + PTRS_PER_PTE;
+#endif
+}
+
 /*
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.
@@ -908,6 +920,7 @@ void __init paging_init(struct machine_desc *mdesc)
 	prepare_page_table();
 	bootmem_init();
 	devicemaps_init(mdesc);
+	kmap_init();
 
 	top_pmd = pmd_off_k(0xffff0000);
 

commit 97fb44eb6bc01f4ffed4300e475aa15e44877375
Merge: e4707dd3e9d0 148854c65ea8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Mar 13 21:44:51 2009 +0000

    Merge branch 'for-rmk' of git://git.pengutronix.de/git/imx/linux-2.6 into devel
    
    Conflicts:
    
            arch/arm/mach-at91/gpio.c

commit e4707dd3e9d0cb57597b6568a5e51fea5d6fca41
Author: Paul Walmsley <paul@pwsan.com>
Date:   Thu Mar 12 20:11:43 2009 +0100

    [ARM] 5422/1: ARM: MMU: add a Non-cacheable Normal executable memory type
    
    This patch adds a Non-cacheable Normal ARM executable memory type,
    MT_MEMORY_NONCACHED.
    
    On OMAP3, this is used for rapid dynamic voltage/frequency scaling in
    the VDD2 voltage domain. OMAP3's SDRAM controller (SDRC) is in the
    VDD2 voltage domain, and its clock frequency must change along with
    voltage. The SDRC clock change code cannot run from SDRAM itself,
    since SDRAM accesses are paused during the clock change. So the
    current implementation of the DVFS code executes from OMAP on-chip
    SRAM, aka "OCM RAM."
    
    If the OCM RAM pages are marked as Cacheable, the ARM cache controller
    will attempt to flush dirty cache lines to the SDRC, so it can fill
    those lines with OCM RAM instruction code. The problem is that the
    SDRC is paused during DVFS, and so any SDRAM access causes the ARM MPU
    subsystem to hang.
    
    TI's original solution to this problem was to mark the OCM RAM
    sections as Strongly Ordered memory, thus preventing caching. This is
    overkill: since the memory is marked as non-bufferable, OCM RAM writes
    become needlessly slow. The idea of "Strongly Ordered SRAM" is also
    conceptually disturbing. Previous LAKML list discussion is here:
    
    http://www.spinics.net/lists/arm-kernel/msg54312.html
    
    This memory type MT_MEMORY_NONCACHED is used for OCM RAM by a future
    patch.
    
    Cc: Richard Woodruff <r-woodruff2@ti.com>
    Signed-off-by: Paul Walmsley <paul@pwsan.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9b36c5cb5e9f..aa424e1da8a1 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -243,6 +243,10 @@ static struct mem_type mem_types[] = {
 		.prot_sect = PMD_TYPE_SECT,
 		.domain    = DOMAIN_KERNEL,
 	},
+	[MT_MEMORY_NONCACHED] = {
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
+		.domain    = DOMAIN_KERNEL,
+	},
 };
 
 const struct mem_type *get_mem_type(unsigned int type)
@@ -406,9 +410,28 @@ static void __init build_mem_type_table(void)
 		kern_pgprot |= L_PTE_SHARED;
 		vecs_pgprot |= L_PTE_SHARED;
 		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
+		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_S;
 #endif
 	}
 
+	/*
+	 * Non-cacheable Normal - intended for memory areas that must
+	 * not cause dirty cache line writebacks when used
+	 */
+	if (cpu_arch >= CPU_ARCH_ARMv6) {
+		if (cpu_arch >= CPU_ARCH_ARMv7 && (cr & CR_TRE)) {
+			/* Non-cacheable Normal is XCB = 001 */
+			mem_types[MT_MEMORY_NONCACHED].prot_sect |=
+				PMD_SECT_BUFFERED;
+		} else {
+			/* For both ARMv6 and non-TEX-remapping ARMv7 */
+			mem_types[MT_MEMORY_NONCACHED].prot_sect |=
+				PMD_SECT_TEX(1);
+		}
+	} else {
+		mem_types[MT_MEMORY_NONCACHED].prot_sect |= PMD_SECT_BUFFERABLE;
+	}
+
 	for (i = 0; i < 16; i++) {
 		unsigned long v = pgprot_val(protection_map[i]);
 		protection_map[i] = __pgprot(v | user_pgprot);

commit 3fd9825c42c784a59b3b90bdf073f49d4bb42a8d
Author: Nicolas Pitre <nico@cam.org>
Date:   Wed Feb 18 22:29:22 2009 +0100

    [ARM] 5402/1: fix a case of wrap-around in sanity_check_meminfo()
    
    In the non highmem case, if two memory banks of 1GB each are provided,
    the second bank would evade suppression since its virtual base would
    be 0.  Fix this by disallowing any memory bank which virtual base
    address is found to be lower than PAGE_OFFSET.
    
    Reported-by: Lennert Buytenhek <buytenh@marvell.com>
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9b36c5cb5e9f..d4d082c5c2d4 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -693,7 +693,8 @@ static void __init sanity_check_meminfo(void)
 		 * Check whether this memory bank would entirely overlap
 		 * the vmalloc area.
 		 */
-		if (__va(bank->start) >= VMALLOC_MIN) {
+		if (__va(bank->start) >= VMALLOC_MIN ||
+		    __va(bank->start) < PAGE_OFFSET) {
 			printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
 			       "(vmalloc region overlap).\n",
 			       bank->start, bank->start + bank->size - 1);

commit 6ce1b871db6a3ca69ed1e35956d89d3cacd7ba08
Author: Julia Lawall <julia@diku.dk>
Date:   Mon Dec 1 14:15:41 2008 -0800

    [ARM] eliminate NULL test and memset after alloc_bootmem
    
    As noted by Akinobu Mita in patch b1fceac2b9e04d278316b2faddf276015fc06e3b,
    alloc_bootmem and related functions never return NULL and always return a
    zeroed region of memory.  Thus a NULL test or memset after calls to these
    functions is unnecessary.
    
    This was fixed using the following semantic patch.
    (http://www.emn.fr/x-info/coccinelle/)
    
    // <smpl>
    @@
    expression E;
    statement S;
    @@
    
    E = \(alloc_bootmem\|alloc_bootmem_low\|alloc_bootmem_pages\|alloc_bootmem_low_pages\|alloc_bootmem_node\|alloc_bootmem_low_pages_node\|alloc_bootmem_pages_node\)(...)
    ... when != E
    (
    - BUG_ON (E == NULL);
    |
    - if (E == NULL) S
    )
    
    @@
    expression E,E1;
    @@
    
    E = \(alloc_bootmem\|alloc_bootmem_low\|alloc_bootmem_pages\|alloc_bootmem_low_pages\|alloc_bootmem_node\|alloc_bootmem_low_pages_node\|alloc_bootmem_pages_node\)(...)
    ... when != E
    - memset(E,0,E1);
    // </smpl>
    
    Signed-off-by: Julia Lawall <julia@diku.dk>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 2ab5f962a053..9b36c5cb5e9f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -827,7 +827,6 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 * Allocate the vector page early.
 	 */
 	vectors = alloc_bootmem_low_pages(PAGE_SIZE);
-	BUG_ON(!vectors);
 
 	for (addr = VMALLOC_END; addr; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
@@ -912,10 +911,10 @@ void __init paging_init(struct machine_desc *mdesc)
 	top_pmd = pmd_off_k(0xffff0000);
 
 	/*
-	 * allocate the zero page.  Note that we count on this going ok.
+	 * allocate the zero page.  Note that this always succeeds and
+	 * returns a zeroed result.
 	 */
 	zero_page = alloc_bootmem_low_pages(PAGE_SIZE);
-	memset(zero_page, 0, PAGE_SIZE);
 	empty_zero_page = virt_to_page(zero_page);
 	flush_dcache_page(empty_zero_page);
 }

commit 37efe6427dd50e889473fb3c7fcec02dbbd098eb
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Dec 1 11:53:07 2008 +0000

    [ARM] use asm/sections.h
    
    Update to use the asm/sections.h header rather than declaring these
    symbols ourselves.  Change __data_start to _data to conform with the
    naming found within asm/sections.h.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c0b9a78d7b87..2ab5f962a053 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -17,6 +17,7 @@
 
 #include <asm/cputype.h>
 #include <asm/mach-types.h>
+#include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>
 #include <asm/tlb.h>
@@ -730,7 +731,7 @@ static inline void prepare_page_table(void)
 
 #ifdef CONFIG_XIP_KERNEL
 	/* The XIP kernel is mapped in the module area -- skip over it */
-	addr = ((unsigned long)&_etext + PGDIR_SIZE - 1) & PGDIR_MASK;
+	addr = ((unsigned long)_etext + PGDIR_SIZE - 1) & PGDIR_MASK;
 #endif
 	for ( ; addr < PAGE_OFFSET; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
@@ -756,10 +757,10 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	 * Note that this can only be in node 0.
 	 */
 #ifdef CONFIG_XIP_KERNEL
-	reserve_bootmem_node(pgdat, __pa(&__data_start), &_end - &__data_start,
+	reserve_bootmem_node(pgdat, __pa(_data), _end - _data,
 			BOOTMEM_DEFAULT);
 #else
-	reserve_bootmem_node(pgdat, __pa(&_stext), &_end - &_stext,
+	reserve_bootmem_node(pgdat, __pa(_stext), _end - _stext,
 			BOOTMEM_DEFAULT);
 #endif
 
@@ -838,7 +839,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 #ifdef CONFIG_XIP_KERNEL
 	map.pfn = __phys_to_pfn(CONFIG_XIP_PHYS_ADDR & SECTION_MASK);
 	map.virtual = MODULES_VADDR;
-	map.length = ((unsigned long)&_etext - map.virtual + ~SECTION_MASK) & SECTION_MASK;
+	map.length = ((unsigned long)_etext - map.virtual + ~SECTION_MASK) & SECTION_MASK;
 	map.type = MT_ROM;
 	create_mapping(&map);
 #endif

commit 7ef4de17cc55a3c3b8d093743b1e3b845d8eba47
Merge: f412b09f4ed7 b5ee9002583f
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Nov 28 15:39:02 2008 +0000

    Merge branch 'highmem' into devel
    
    Conflicts:
    
            arch/arm/mach-clps7500/include/mach/memory.h

commit 9210807cb5a3f19a0e954dd401e3a2c3626d1b48
Author: Nicolas Pitre <nico@cam.org>
Date:   Fri Sep 19 10:43:06 2008 -0400

    [ARM] prevent the vmalloc cmdline argument from eating all memory
    
    Commit 8d5796d2ec6b5a4e7a52861144e63af438d6f8f7 allows for the vmalloc
    area to be resized from the kernel cmdline.  Make sure it cannot overlap
    with RAM entirely.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ab511d94d917..636cf8fc70ef 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -646,6 +646,13 @@ static void __init early_vmalloc(char **arg)
 			"vmalloc area too small, limiting to %luMB\n",
 			vmalloc_reserve >> 20);
 	}
+
+	if (vmalloc_reserve > VMALLOC_END - (PAGE_OFFSET + SZ_32M)) {
+		vmalloc_reserve = VMALLOC_END - (PAGE_OFFSET + SZ_32M);
+		printk(KERN_WARNING
+			"vmalloc area is too big, limiting to %luMB\n",
+			vmalloc_reserve >> 20);
+	}
 }
 __early_param("vmalloc=", early_vmalloc);
 

commit a1bbaec0cd2a59d4bb09b72e4541a8a12e480d5d
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Sep 2 11:44:21 2008 -0400

    [ARM] split highmem into its own memory bank
    
    Doing so will greatly simplify the bootmem initialization code as each
    bank is therefore entirely lowmem or highmem with no crossing between
    those zones.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6870805c31dd..ab511d94d917 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -651,44 +651,62 @@ __early_param("vmalloc=", early_vmalloc);
 
 #define VMALLOC_MIN	(void *)(VMALLOC_END - vmalloc_reserve)
 
-static int __init check_membank_valid(struct membank *mb)
-{
-	/*
-	 * Check whether this memory region would entirely overlap
-	 * the vmalloc area.
-	 */
-	if (phys_to_virt(mb->start) >= VMALLOC_MIN) {
-		printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
-			"(vmalloc region overlap).\n",
-			mb->start, mb->start + mb->size - 1);
-		return 0;
-	}
-
-	/*
-	 * Check whether this memory region would partially overlap
-	 * the vmalloc area.
-	 */
-	if (phys_to_virt(mb->start + mb->size) < phys_to_virt(mb->start) ||
-	    phys_to_virt(mb->start + mb->size) > VMALLOC_MIN) {
-		unsigned long newsize = VMALLOC_MIN - phys_to_virt(mb->start);
-
-		printk(KERN_NOTICE "Truncating RAM at %.8lx-%.8lx "
-			"to -%.8lx (vmalloc region overlap).\n",
-			mb->start, mb->start + mb->size - 1,
-			mb->start + newsize - 1);
-		mb->size = newsize;
-	}
-
-	return 1;
-}
-
 static void __init sanity_check_meminfo(void)
 {
 	int i, j;
 
 	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
-		if (check_membank_valid(&meminfo.bank[i]))
-			meminfo.bank[j++] = meminfo.bank[i];
+		struct membank *bank = &meminfo.bank[j];
+		*bank = meminfo.bank[i];
+
+#ifdef CONFIG_HIGHMEM
+		/*
+		 * Split those memory banks which are partially overlapping
+		 * the vmalloc area greatly simplifying things later.
+		 */
+		if (__va(bank->start) < VMALLOC_MIN &&
+		    bank->size > VMALLOC_MIN - __va(bank->start)) {
+			if (meminfo.nr_banks >= NR_BANKS) {
+				printk(KERN_CRIT "NR_BANKS too low, "
+						 "ignoring high memory\n");
+			} else {
+				memmove(bank + 1, bank,
+					(meminfo.nr_banks - i) * sizeof(*bank));
+				meminfo.nr_banks++;
+				i++;
+				bank[1].size -= VMALLOC_MIN - __va(bank->start);
+				bank[1].start = __pa(VMALLOC_MIN - 1) + 1;
+				j++;
+			}
+			bank->size = VMALLOC_MIN - __va(bank->start);
+		}
+#else
+		/*
+		 * Check whether this memory bank would entirely overlap
+		 * the vmalloc area.
+		 */
+		if (__va(bank->start) >= VMALLOC_MIN) {
+			printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
+			       "(vmalloc region overlap).\n",
+			       bank->start, bank->start + bank->size - 1);
+			continue;
+		}
+
+		/*
+		 * Check whether this memory bank would partially overlap
+		 * the vmalloc area.
+		 */
+		if (__va(bank->start + bank->size) > VMALLOC_MIN ||
+		    __va(bank->start + bank->size) < __va(bank->start)) {
+			unsigned long newsize = VMALLOC_MIN - __va(bank->start);
+			printk(KERN_NOTICE "Truncating RAM at %.8lx-%.8lx "
+			       "to -%.8lx (vmalloc region overlap).\n",
+			       bank->start, bank->start + bank->size - 1,
+			       bank->start + newsize - 1);
+			bank->size = newsize;
+		}
+#endif
+		j++;
 	}
 	meminfo.nr_banks = j;
 }

commit 4b5f32cee0cce7b9783ced5cbeabd17aa53c51fb
Author: Nicolas Pitre <nico@cam.org>
Date:   Mon Oct 6 13:24:40 2008 -0400

    [ARM] rationalize memory configuration code some more
    
    Currently there are two instances of struct meminfo: one in
    kernel/setup.c marked __initdata, and another in mm/init.c with
    permanent storage.  Let's keep only the later to directly populate
    the permanent version from arm_add_memory().
    
    Also move common validation tests between the MMU and non-MMU cases
    into arm_add_memory() to remove some duplication.  Protection against
    overflowing the membank array is also moved in there in order to cover
    the kernel cmdline parsing path as well.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 7f36c825718d..6870805c31dd 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -653,13 +653,6 @@ __early_param("vmalloc=", early_vmalloc);
 
 static int __init check_membank_valid(struct membank *mb)
 {
-	/*
-	 * Check whether this memory region has non-zero size or
-	 * invalid node number.
-	 */
-	if (mb->size == 0 || mb->node >= MAX_NUMNODES)
-		return 0;
-
 	/*
 	 * Check whether this memory region would entirely overlap
 	 * the vmalloc area.
@@ -689,18 +682,18 @@ static int __init check_membank_valid(struct membank *mb)
 	return 1;
 }
 
-static void __init sanity_check_meminfo(struct meminfo *mi)
+static void __init sanity_check_meminfo(void)
 {
 	int i, j;
 
-	for (i = 0, j = 0; i < mi->nr_banks; i++) {
-		if (check_membank_valid(&mi->bank[i]))
-			mi->bank[j++] = mi->bank[i];
+	for (i = 0, j = 0; i < meminfo.nr_banks; i++) {
+		if (check_membank_valid(&meminfo.bank[i]))
+			meminfo.bank[j++] = meminfo.bank[i];
 	}
-	mi->nr_banks = j;
+	meminfo.nr_banks = j;
 }
 
-static inline void prepare_page_table(struct meminfo *mi)
+static inline void prepare_page_table(void)
 {
 	unsigned long addr;
 
@@ -721,7 +714,7 @@ static inline void prepare_page_table(struct meminfo *mi)
 	 * Clear out all the kernel space mappings, except for the first
 	 * memory bank, up to the end of the vmalloc region.
 	 */
-	for (addr = __phys_to_virt(mi->bank[0].start + mi->bank[0].size);
+	for (addr = __phys_to_virt(bank_phys_end(&meminfo.bank[0]));
 	     addr < VMALLOC_END; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
 }
@@ -880,14 +873,14 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
  * paging_init() sets up the page tables, initialises the zone memory
  * maps, and sets up the zero page, bad page and bad page tables.
  */
-void __init paging_init(struct meminfo *mi, struct machine_desc *mdesc)
+void __init paging_init(struct machine_desc *mdesc)
 {
 	void *zero_page;
 
 	build_mem_type_table();
-	sanity_check_meminfo(mi);
-	prepare_page_table(mi);
-	bootmem_init(mi);
+	sanity_check_meminfo();
+	prepare_page_table();
+	bootmem_init();
 	devicemaps_init(mdesc);
 
 	top_pmd = pmd_off_k(0xffff0000);

commit 59f0cb0fddc14ffc6676ae62e911f8115ebc8ccf
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Oct 27 11:24:09 2008 +0000

    [ARM] remove memzero()
    
    As suggested by Andrew Morton, remove memzero() - it's not supported
    on other architectures so use of it is a potential build breaking bug.
    Since the compiler optimizes memset(x,0,n) to __memzero() perfectly
    well, we don't miss out on the underlying benefits of memzero().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 7f36c825718d..f24803c1fb0b 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -896,7 +896,7 @@ void __init paging_init(struct meminfo *mi, struct machine_desc *mdesc)
 	 * allocate the zero page.  Note that we count on this going ok.
 	 */
 	zero_page = alloc_bootmem_low_pages(PAGE_SIZE);
-	memzero(zero_page, PAGE_SIZE);
+	memset(zero_page, 0, PAGE_SIZE);
 	empty_zero_page = virt_to_page(zero_page);
 	flush_dcache_page(empty_zero_page);
 }

commit ebb4c65869db7213280ad9c510637683939b5ff8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Nov 9 11:18:36 2008 +0000

    [ARM] iop: iop3xx needs registers mapped uncached+unbuffered
    
    Mikael Pettersson reported:
    
       The 2.6.28-rc kernels fail to detect PCI device 0000:00:01.0
       (the first ethernet port) on my Thecus n2100 XScale box.
    
       There is however still a strange "ghost" device that gets partially
       detected in 2.6.28-rc2 vanilla.
    
    The IOP321 manual says:
    
      The user designates the memory region containing the OCCDR as
      non-cacheable and non-bufferable from the IntelR XScaleTM core.
      This guarantees that all load/stores to the OCCDR are only of
      DWORD quantities.
    
    Ensure that the OCCDR is so mapped.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e63db11f16a8..7f36c825718d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -208,6 +208,12 @@ static struct mem_type mem_types[] = {
 		.prot_sect	= PROT_SECT_DEVICE,
 		.domain		= DOMAIN_IO,
 	},
+	[MT_UNCACHED] = {
+		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PMD_TYPE_SECT | PMD_SECT_XN,
+		.domain		= DOMAIN_IO,
+	},
 	[MT_CACHECLEAN] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,

commit b1cce6b1b2785fd61454b47ceacb461815407662
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Nov 4 10:52:28 2008 +0000

    [ARM] mm: fix page table initialization
    
    As a result of the ptebits changes, we ended up marking device mappings
    as normal memory on ARMv7 CPUs, resulting in undesirable behaviour with
    serial ports and the like.  While reviewing the section mapping table
    entries, other errors in the memory type settings for devices were
    detected and confirmed to prevent Xscale3 platforms booting.
    
    Tested on:
            OMAP34xx (ARMv7),
            OMAP24xx (ARMv6),
            OMAP16xx (ARM926T, ARMv5),
            PXA311 (Xscale3),
            PXA272 (Xscale),
            PXA255 (Xscale),
            IXP42x (Xscale),
            S3C2410 (ARM920T, ARMv4T),
            ARM720T (ARMv4T)
            StrongARM-110 (ARMv4)
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Tested-by: Robert Jarzmik <robert.jarzmik@free.fr>
    Tested-by: Mike Rapoport <mike@compulab.co.il>
    Tested-by: Ben Dooks <ben-linux@fluff.org>
    Tested-by: Anders Grafstrm <grfstrm@users.sourceforge.net>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 34e53596ff1d..e63db11f16a8 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -180,20 +180,20 @@ void adjust_cr(unsigned long mask, unsigned long set)
 #endif
 
 #define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_WRITE
-#define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_XN|PMD_SECT_AP_WRITE
+#define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_AP_WRITE
 
 static struct mem_type mem_types[] = {
 	[MT_DEVICE] = {		  /* Strongly ordered / ARMv6 shared device */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_SHARED |
 				  L_PTE_SHARED,
 		.prot_l1	= PMD_TYPE_TABLE,
-		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_UNCACHED,
+		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_S,
 		.domain		= DOMAIN_IO,
 	},
 	[MT_DEVICE_NONSHARED] = { /* ARMv6 non-shared device */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_NONSHARED,
 		.prot_l1	= PMD_TYPE_TABLE,
-		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_TEX(2),
+		.prot_sect	= PROT_SECT_DEVICE,
 		.domain		= DOMAIN_IO,
 	},
 	[MT_DEVICE_CACHED] = {	  /* ioremap_cached */
@@ -205,7 +205,7 @@ static struct mem_type mem_types[] = {
 	[MT_DEVICE_WC] = {	/* ioremap_wc */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_WC,
 		.prot_l1	= PMD_TYPE_TABLE,
-		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_BUFFERABLE,
+		.prot_sect	= PROT_SECT_DEVICE,
 		.domain		= DOMAIN_IO,
 	},
 	[MT_CACHECLEAN] = {
@@ -273,22 +273,23 @@ static void __init build_mem_type_table(void)
 #endif
 
 	/*
-	 * On non-Xscale3 ARMv5-and-older systems, use CB=01
-	 * (Uncached/Buffered) for ioremap_wc() mappings.  On XScale3
-	 * and ARMv6+, use TEXCB=00100 mappings (Inner/Outer Uncacheable
-	 * in xsc3 parlance, Uncached Normal in ARMv6 parlance).
+	 * Strip out features not present on earlier architectures.
+	 * Pre-ARMv5 CPUs don't have TEX bits.  Pre-ARMv6 CPUs or those
+	 * without extended page tables don't have the 'Shared' bit.
 	 */
-	if (cpu_is_xsc3() || cpu_arch >= CPU_ARCH_ARMv6) {
-		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_TEX(1);
-		mem_types[MT_DEVICE_WC].prot_sect &= ~PMD_SECT_BUFFERABLE;
-	}
+	if (cpu_arch < CPU_ARCH_ARMv5)
+		for (i = 0; i < ARRAY_SIZE(mem_types); i++)
+			mem_types[i].prot_sect &= ~PMD_SECT_TEX(7);
+	if ((cpu_arch < CPU_ARCH_ARMv6 || !(cr & CR_XP)) && !cpu_is_xsc3())
+		for (i = 0; i < ARRAY_SIZE(mem_types); i++)
+			mem_types[i].prot_sect &= ~PMD_SECT_S;
 
 	/*
-	 * ARMv5 and lower, bit 4 must be set for page tables.
-	 * (was: cache "update-able on write" bit on ARM610)
-	 * However, Xscale cores require this bit to be cleared.
+	 * ARMv5 and lower, bit 4 must be set for page tables (was: cache
+	 * "update-able on write" bit on ARM610).  However, Xscale and
+	 * Xscale3 require this bit to be cleared.
 	 */
-	if (cpu_is_xscale()) {
+	if (cpu_is_xscale() || cpu_is_xsc3()) {
 		for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
 			mem_types[i].prot_sect &= ~PMD_BIT4;
 			mem_types[i].prot_l1 &= ~PMD_BIT4;
@@ -302,6 +303,64 @@ static void __init build_mem_type_table(void)
 		}
 	}
 
+	/*
+	 * Mark the device areas according to the CPU/architecture.
+	 */
+	if (cpu_is_xsc3() || (cpu_arch >= CPU_ARCH_ARMv6 && (cr & CR_XP))) {
+		if (!cpu_is_xsc3()) {
+			/*
+			 * Mark device regions on ARMv6+ as execute-never
+			 * to prevent speculative instruction fetches.
+			 */
+			mem_types[MT_DEVICE].prot_sect |= PMD_SECT_XN;
+			mem_types[MT_DEVICE_NONSHARED].prot_sect |= PMD_SECT_XN;
+			mem_types[MT_DEVICE_CACHED].prot_sect |= PMD_SECT_XN;
+			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_XN;
+		}
+		if (cpu_arch >= CPU_ARCH_ARMv7 && (cr & CR_TRE)) {
+			/*
+			 * For ARMv7 with TEX remapping,
+			 * - shared device is SXCB=1100
+			 * - nonshared device is SXCB=0100
+			 * - write combine device mem is SXCB=0001
+			 * (Uncached Normal memory)
+			 */
+			mem_types[MT_DEVICE].prot_sect |= PMD_SECT_TEX(1);
+			mem_types[MT_DEVICE_NONSHARED].prot_sect |= PMD_SECT_TEX(1);
+			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_BUFFERABLE;
+		} else if (cpu_is_xsc3()) {
+			/*
+			 * For Xscale3,
+			 * - shared device is TEXCB=00101
+			 * - nonshared device is TEXCB=01000
+			 * - write combine device mem is TEXCB=00100
+			 * (Inner/Outer Uncacheable in xsc3 parlance)
+			 */
+			mem_types[MT_DEVICE].prot_sect |= PMD_SECT_TEX(1) | PMD_SECT_BUFFERED;
+			mem_types[MT_DEVICE_NONSHARED].prot_sect |= PMD_SECT_TEX(2);
+			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_TEX(1);
+		} else {
+			/*
+			 * For ARMv6 and ARMv7 without TEX remapping,
+			 * - shared device is TEXCB=00001
+			 * - nonshared device is TEXCB=01000
+			 * - write combine device mem is TEXCB=00100
+			 * (Uncached Normal in ARMv6 parlance).
+			 */
+			mem_types[MT_DEVICE].prot_sect |= PMD_SECT_BUFFERED;
+			mem_types[MT_DEVICE_NONSHARED].prot_sect |= PMD_SECT_TEX(2);
+			mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_TEX(1);
+		}
+	} else {
+		/*
+		 * On others, write combining is "Uncached/Buffered"
+		 */
+		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_BUFFERABLE;
+	}
+
+	/*
+	 * Now deal with the memory-type mappings
+	 */
 	cp = &cache_policies[cachepolicy];
 	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
 
@@ -317,12 +376,8 @@ static void __init build_mem_type_table(void)
 	 * Enable CPU-specific coherency if supported.
 	 * (Only available on XSC3 at the moment.)
 	 */
-	if (arch_is_coherent()) {
-		if (cpu_is_xsc3()) {
-			mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
-			mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
-		}
-	}
+	if (arch_is_coherent() && cpu_is_xsc3())
+		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
 
 	/*
 	 * ARMv6 and above have extended page tables.
@@ -336,11 +391,6 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_MINICLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
 
-		/*
-		 * Mark the device area as "shared device"
-		 */
-		mem_types[MT_DEVICE].prot_sect |= PMD_SECT_BUFFERED;
-
 #ifdef CONFIG_SMP
 		/*
 		 * Mark memory with the "shared" attribute for SMP systems
@@ -360,9 +410,6 @@ static void __init build_mem_type_table(void)
 	mem_types[MT_LOW_VECTORS].prot_pte |= vecs_pgprot;
 	mem_types[MT_HIGH_VECTORS].prot_pte |= vecs_pgprot;
 
-	if (cpu_arch < CPU_ARCH_ARMv5)
-		mem_types[MT_MINICLEAN].prot_sect &= ~PMD_SECT_TEX(1);
-
 	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
 				 L_PTE_DIRTY | L_PTE_WRITE |

commit ab4f2ee130d5ffcf35616e1f5c6ab75af5b463b6
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Nov 6 17:11:07 2008 +0000

    [ARM] fix naming of MODULE_START / MODULE_END
    
    As of 73bdf0a60e607f4b8ecc5aec597105976565a84f, the kernel needs
    to know where modules are located in the virtual address space.
    On ARM, we located this region between MODULE_START and MODULE_END.
    Unfortunately, everyone else calls it MODULES_VADDR and MODULES_END.
    Update ARM to use the same naming, so is_vmalloc_or_module_addr()
    can work properly.  Also update the comment on mm/vmalloc.c to
    reflect that ARM also places modules in a separate region from the
    vmalloc space.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 8ba754064559..34e53596ff1d 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -654,7 +654,7 @@ static inline void prepare_page_table(struct meminfo *mi)
 	/*
 	 * Clear out all the mappings below the kernel image.
 	 */
-	for (addr = 0; addr < MODULE_START; addr += PGDIR_SIZE)
+	for (addr = 0; addr < MODULES_VADDR; addr += PGDIR_SIZE)
 		pmd_clear(pmd_off_k(addr));
 
 #ifdef CONFIG_XIP_KERNEL
@@ -766,7 +766,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	 */
 #ifdef CONFIG_XIP_KERNEL
 	map.pfn = __phys_to_pfn(CONFIG_XIP_PHYS_ADDR & SECTION_MASK);
-	map.virtual = MODULE_START;
+	map.virtual = MODULES_VADDR;
 	map.length = ((unsigned long)&_etext - map.virtual + ~SECTION_MASK) & SECTION_MASK;
 	map.type = MT_ROM;
 	create_mapping(&map);

commit 6a4690c22f5da1eb1c898b61b6a80da52fbd976f
Merge: 90bb28b0644f 8ec53663d269
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Oct 9 21:31:56 2008 +0100

    Merge branch 'ptebits' into devel
    
    Conflicts:
    
            arch/arm/Kconfig

commit db5b7169474882fabbd811a4cf5c1bae3157e677
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Sep 7 12:42:51 2008 +0100

    [ARM] Remove MT_DEVICE_IXP2000 and associated definitions
    
    As of the previous commit, MT_DEVICE_IXP2000 encodes to the same
    PTE bit encoding as MT_DEVICE, so it's now redundant.  Convert
    MT_DEVICE_IXP2000 to use MT_DEVICE instead, and remove its aliases.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 685dc1bf0083..06ad914928cf 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -204,13 +204,6 @@ static struct mem_type mem_types[] = {
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_WB,
 		.domain		= DOMAIN_IO,
 	},	
-	[MT_DEVICE_IXP2000] = {	  /* IXP2400 requires XCB=101 for on-chip I/O */
-		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_IXP2000,
-		.prot_l1	= PMD_TYPE_TABLE,
-		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_BUFFERABLE |
-				  PMD_SECT_TEX(1),
-		.domain		= DOMAIN_IO,
-	},
 	[MT_DEVICE_WC] = {	/* ioremap_wc */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_WC,
 		.prot_l1	= PMD_TYPE_TABLE,

commit 40d192b63d079db1f76cec9ae8ccbf461fda23e4
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 6 21:15:56 2008 +0100

    [ARM] remove 'prot_pte_ext' from memory type table
    
    This member is now redundant; the memory type is encoded in the Linux
    PTE bits.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 04602288da2c..685dc1bf0083 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -418,8 +418,7 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 
 	pte = pte_offset_kernel(pmd, addr);
 	do {
-		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)),
-			    type->prot_pte_ext);
+		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }

commit 639b0ae7f5bcd645862a9c3ea2d4321475c71d7a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 6 21:07:45 2008 +0100

    [ARM] Convert ARMv6 and ARMv7 to use new memory types
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index cfc0add4874e..04602288da2c 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -194,7 +194,6 @@ static struct mem_type mem_types[] = {
 	},
 	[MT_DEVICE_NONSHARED] = { /* ARMv6 non-shared device */
 		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_NONSHARED,
-		.prot_pte_ext	= PTE_EXT_TEX(2),
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_TEX(2),
 		.domain		= DOMAIN_IO,
@@ -289,8 +288,6 @@ static void __init build_mem_type_table(void)
 	 * in xsc3 parlance, Uncached Normal in ARMv6 parlance).
 	 */
 	if (cpu_is_xsc3() || cpu_arch >= CPU_ARCH_ARMv6) {
-		mem_types[MT_DEVICE_WC].prot_pte_ext |= PTE_EXT_TEX(1);
-		mem_types[MT_DEVICE_WC].prot_pte &= ~L_PTE_BUFFERABLE;
 		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_TEX(1);
 		mem_types[MT_DEVICE_WC].prot_sect &= ~PMD_SECT_BUFFERABLE;
 	}
@@ -351,7 +348,6 @@ static void __init build_mem_type_table(void)
 		/*
 		 * Mark the device area as "shared device"
 		 */
-		mem_types[MT_DEVICE].prot_pte |= L_PTE_BUFFERABLE;
 		mem_types[MT_DEVICE].prot_sect |= PMD_SECT_BUFFERED;
 
 #ifdef CONFIG_SMP

commit bb30f36f9b71c31dc8fe3483bba4c9884fc86080
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 6 20:04:59 2008 +0100

    [ARM] Introduce new PTE memory type bits
    
    Provide L_PTE_MT_xxx definitions to describe the memory types that we
    use in Linux/ARM.  These definitions are carefully picked such that:
    
    1. their LSBs match what is required for pre-ARMv6 CPUs.
    2. they all have a unique encoding, including after modification
       by build_mem_type_table() (the result being that some have more
       than one combination.)
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index a713e40e1f1a..cfc0add4874e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -68,27 +68,27 @@ static struct cachepolicy cache_policies[] __initdata = {
 		.policy		= "uncached",
 		.cr_mask	= CR_W|CR_C,
 		.pmd		= PMD_SECT_UNCACHED,
-		.pte		= 0,
+		.pte		= L_PTE_MT_UNCACHED,
 	}, {
 		.policy		= "buffered",
 		.cr_mask	= CR_C,
 		.pmd		= PMD_SECT_BUFFERED,
-		.pte		= PTE_BUFFERABLE,
+		.pte		= L_PTE_MT_BUFFERABLE,
 	}, {
 		.policy		= "writethrough",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WT,
-		.pte		= PTE_CACHEABLE,
+		.pte		= L_PTE_MT_WRITETHROUGH,
 	}, {
 		.policy		= "writeback",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WB,
-		.pte		= PTE_BUFFERABLE|PTE_CACHEABLE,
+		.pte		= L_PTE_MT_WRITEBACK,
 	}, {
 		.policy		= "writealloc",
 		.cr_mask	= 0,
 		.pmd		= PMD_SECT_WBWA,
-		.pte		= PTE_BUFFERABLE|PTE_CACHEABLE,
+		.pte		= L_PTE_MT_WRITEALLOC,
 	}
 };
 
@@ -186,35 +186,36 @@ void adjust_cr(unsigned long mask, unsigned long set)
 
 static struct mem_type mem_types[] = {
 	[MT_DEVICE] = {		  /* Strongly ordered / ARMv6 shared device */
-		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_SHARED |
+				  L_PTE_SHARED,
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_UNCACHED,
 		.domain		= DOMAIN_IO,
 	},
 	[MT_DEVICE_NONSHARED] = { /* ARMv6 non-shared device */
-		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_NONSHARED,
 		.prot_pte_ext	= PTE_EXT_TEX(2),
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_TEX(2),
 		.domain		= DOMAIN_IO,
 	},
 	[MT_DEVICE_CACHED] = {	  /* ioremap_cached */
-		.prot_pte	= PROT_PTE_DEVICE | L_PTE_CACHEABLE | L_PTE_BUFFERABLE,
+		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_CACHED,
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_WB,
 		.domain		= DOMAIN_IO,
 	},	
 	[MT_DEVICE_IXP2000] = {	  /* IXP2400 requires XCB=101 for on-chip I/O */
-		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_IXP2000,
 		.prot_l1	= PMD_TYPE_TABLE,
 		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_BUFFERABLE |
 				  PMD_SECT_TEX(1),
 		.domain		= DOMAIN_IO,
 	},
 	[MT_DEVICE_WC] = {	/* ioremap_wc */
-		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_pte	= PROT_PTE_DEVICE | L_PTE_MT_DEV_WC,
 		.prot_l1	= PMD_TYPE_TABLE,
-		.prot_sect	= PROT_SECT_DEVICE,
+		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_BUFFERABLE,
 		.domain		= DOMAIN_IO,
 	},
 	[MT_CACHECLEAN] = {
@@ -259,7 +260,7 @@ static void __init build_mem_type_table(void)
 {
 	struct cachepolicy *cp;
 	unsigned int cr = get_cr();
-	unsigned int user_pgprot, kern_pgprot;
+	unsigned int user_pgprot, kern_pgprot, vecs_pgprot;
 	int cpu_arch = cpu_architecture();
 	int i;
 
@@ -277,6 +278,9 @@ static void __init build_mem_type_table(void)
 			cachepolicy = CPOLICY_WRITEBACK;
 		ecc_mask = 0;
 	}
+#ifdef CONFIG_SMP
+	cachepolicy = CPOLICY_WRITEALLOC;
+#endif
 
 	/*
 	 * On non-Xscale3 ARMv5-and-older systems, use CB=01
@@ -286,10 +290,9 @@ static void __init build_mem_type_table(void)
 	 */
 	if (cpu_is_xsc3() || cpu_arch >= CPU_ARCH_ARMv6) {
 		mem_types[MT_DEVICE_WC].prot_pte_ext |= PTE_EXT_TEX(1);
+		mem_types[MT_DEVICE_WC].prot_pte &= ~L_PTE_BUFFERABLE;
 		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_TEX(1);
-	} else {
-		mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_BUFFERABLE;
-		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_BUFFERABLE;
+		mem_types[MT_DEVICE_WC].prot_sect &= ~PMD_SECT_BUFFERABLE;
 	}
 
 	/*
@@ -312,7 +315,15 @@ static void __init build_mem_type_table(void)
 	}
 
 	cp = &cache_policies[cachepolicy];
-	kern_pgprot = user_pgprot = cp->pte;
+	vecs_pgprot = kern_pgprot = user_pgprot = cp->pte;
+
+#ifndef CONFIG_SMP
+	/*
+	 * Only use write-through for non-SMP systems
+	 */
+	if (cpu_arch >= CPU_ARCH_ARMv5 && cachepolicy > CPOLICY_WRITETHROUGH)
+		vecs_pgprot = cache_policies[CPOLICY_WRITETHROUGH].pte;
+#endif
 
 	/*
 	 * Enable CPU-specific coherency if supported.
@@ -349,30 +360,21 @@ static void __init build_mem_type_table(void)
 		 */
 		user_pgprot |= L_PTE_SHARED;
 		kern_pgprot |= L_PTE_SHARED;
+		vecs_pgprot |= L_PTE_SHARED;
 		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
 #endif
 	}
 
 	for (i = 0; i < 16; i++) {
 		unsigned long v = pgprot_val(protection_map[i]);
-		v = (v & ~(L_PTE_BUFFERABLE|L_PTE_CACHEABLE)) | user_pgprot;
-		protection_map[i] = __pgprot(v);
+		protection_map[i] = __pgprot(v | user_pgprot);
 	}
 
-	mem_types[MT_LOW_VECTORS].prot_pte |= kern_pgprot;
-	mem_types[MT_HIGH_VECTORS].prot_pte |= kern_pgprot;
+	mem_types[MT_LOW_VECTORS].prot_pte |= vecs_pgprot;
+	mem_types[MT_HIGH_VECTORS].prot_pte |= vecs_pgprot;
 
-	if (cpu_arch >= CPU_ARCH_ARMv5) {
-#ifndef CONFIG_SMP
-		/*
-		 * Only use write-through for non-SMP systems
-		 */
-		mem_types[MT_LOW_VECTORS].prot_pte &= ~L_PTE_BUFFERABLE;
-		mem_types[MT_HIGH_VECTORS].prot_pte &= ~L_PTE_BUFFERABLE;
-#endif
-	} else {
+	if (cpu_arch < CPU_ARCH_ARMv5)
 		mem_types[MT_MINICLEAN].prot_sect &= ~PMD_SECT_TEX(1);
-	}
 
 	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |

commit 6c5da7aced798c7781f054a76c769b85f0173561
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Sep 30 19:31:44 2008 +0100

    [ARM] mm: move vmalloc= parsing to arch/arm/mm/mmu.c
    
    There's no point scattering this around the tree, the parsing
    of the parameter might as well live beside the code which uses
    it.  That also means we can make vmalloc_reserve a static
    variable.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f03ad87f6baa..e7af83e569d7 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -566,6 +566,28 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		create_mapping(io_desc + i);
 }
 
+static unsigned long __initdata vmalloc_reserve = SZ_128M;
+
+/*
+ * vmalloc=size forces the vmalloc area to be exactly 'size'
+ * bytes. This can be used to increase (or decrease) the vmalloc
+ * area - the default is 128m.
+ */
+static void __init early_vmalloc(char **arg)
+{
+	vmalloc_reserve = memparse(*arg, arg);
+
+	if (vmalloc_reserve < SZ_16M) {
+		vmalloc_reserve = SZ_16M;
+		printk(KERN_WARNING
+			"vmalloc area too small, limiting to %luMB\n",
+			vmalloc_reserve >> 20);
+	}
+}
+__early_param("vmalloc=", early_vmalloc);
+
+#define VMALLOC_MIN	(void *)(VMALLOC_END - vmalloc_reserve)
+
 static int __init check_membank_valid(struct membank *mb)
 {
 	/*

commit eca73214c9c50e290b8dc823b41730b01788872d
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Sep 30 19:29:25 2008 +0100

    [ARM] mm: move validation of membanks to one place
    
    The newly introduced sanity_check_meminfo() function should be
    used to collect all validation of the meminfo array, which we
    have in bootmem_init().  Move it there.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 94d81a2ba008..f03ad87f6baa 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -569,9 +569,10 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 static int __init check_membank_valid(struct membank *mb)
 {
 	/*
-	 * Check whether this memory region has non-zero size.
+	 * Check whether this memory region has non-zero size or
+	 * invalid node number.
 	 */
-	if (mb->size == 0)
+	if (mb->size == 0 || mb->node >= MAX_NUMNODES)
 		return 0;
 
 	/*
@@ -605,8 +606,7 @@ static int __init check_membank_valid(struct membank *mb)
 
 static void __init sanity_check_meminfo(struct meminfo *mi)
 {
-	int i;
-	int j;
+	int i, j;
 
 	for (i = 0, j = 0; i < mi->nr_banks; i++) {
 		if (check_membank_valid(&mi->bank[i]))

commit 1ad77a876da48331451698cc4172c90ab9b6372f
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Fri Sep 5 13:17:11 2008 +0100

    [ARM] 5241/1: provide ioremap_wc()
    
    This patch provides an ARM implementation of ioremap_wc().
    
    We use different page table attributes depending on which CPU we
    are running on:
    
    - Non-XScale ARMv5 and earlier systems: The ARMv5 ARM documents four
      possible mapping types (CB=00/01/10/11).  We can't use any of the
      cached memory types (CB=10/11), since that breaks coherency with
      peripheral devices.  Both CB=00 and CB=01 are suitable for _wc, and
      CB=01 (Uncached/Buffered) allows the hardware more freedom than
      CB=00, so we'll use that.
    
      (The ARMv5 ARM seems to suggest that CB=01 is allowed to delay stores
      but isn't allowed to merge them, but there is no other mapping type
      we can use that allows the hardware to delay and merge stores, so
      we'll go with CB=01.)
    
    - XScale v1/v2 (ARMv5): same as the ARMv5 case above, with the slight
      difference that on these platforms, CB=01 actually _does_ allow
      merging stores.  (If you want noncoalescing bufferable behavior
      on Xscale v1/v2, you need to use XCB=101.)
    
    - Xscale v3 (ARMv5) and ARMv6+: on these systems, we use TEXCB=00100
      mappings (Inner/Outer Uncacheable in xsc3 parlance, Uncached Normal
      in ARMv6 parlance).
    
      The ARMv6 ARM explicitly says that any accesses to Normal memory can
      be merged, which makes Normal memory more suitable for _wc mappings
      than Device or Strongly Ordered memory, as the latter two mapping
      types are guaranteed to maintain transaction number, size and order.
      We use the Uncached variety of Normal mappings for the same reason
      that we can't use C=1 mappings on ARMv5.
    
      The xsc3 Architecture Specification documents TEXCB=00100 as being
      Uncacheable and allowing coalescing of writes, which is also just
      what we need.
    
    Signed-off-by: Lennert Buytenhek <buytenh@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 25d9a11eb617..a713e40e1f1a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -211,6 +211,12 @@ static struct mem_type mem_types[] = {
 				  PMD_SECT_TEX(1),
 		.domain		= DOMAIN_IO,
 	},
+	[MT_DEVICE_WC] = {	/* ioremap_wc */
+		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PROT_SECT_DEVICE,
+		.domain		= DOMAIN_IO,
+	},
 	[MT_CACHECLEAN] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
@@ -272,6 +278,20 @@ static void __init build_mem_type_table(void)
 		ecc_mask = 0;
 	}
 
+	/*
+	 * On non-Xscale3 ARMv5-and-older systems, use CB=01
+	 * (Uncached/Buffered) for ioremap_wc() mappings.  On XScale3
+	 * and ARMv6+, use TEXCB=00100 mappings (Inner/Outer Uncacheable
+	 * in xsc3 parlance, Uncached Normal in ARMv6 parlance).
+	 */
+	if (cpu_is_xsc3() || cpu_arch >= CPU_ARCH_ARMv6) {
+		mem_types[MT_DEVICE_WC].prot_pte_ext |= PTE_EXT_TEX(1);
+		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_TEX(1);
+	} else {
+		mem_types[MT_DEVICE_WC].prot_pte |= L_PTE_BUFFERABLE;
+		mem_types[MT_DEVICE_WC].prot_sect |= PMD_SECT_BUFFERABLE;
+	}
+
 	/*
 	 * ARMv5 and lower, bit 4 must be set for page tables.
 	 * (was: cache "update-able on write" bit on ARM610)

commit 5ed5fdf50cbe8195522e2176d6356b357c0c963f
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 6 11:23:30 2008 +0100

    [ARM] clean up a load of old declarations
    
    ... some of which are now in linux/*.h headers.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index c2bbe3f311f0..94d81a2ba008 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -28,9 +28,6 @@
 
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
 
-extern void _stext, _etext, __data_start, _end;
-extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
-
 /*
  * empty_zero_page is a special page that is used for
  * zero-initialized data and COW.

commit 0ba8b9b273c45dd23f60ff700e265a0069b33758
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 18:08:10 2008 +0100

    [ARM] cputype: separate definitions, use them
    
    Add asm/cputype.h, moving functions and definitions from asm/system.h
    there.  Convert all users of 'processor_id' to the more efficient
    read_cpuid_id() function.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 25d9a11eb617..c2bbe3f311f0 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -15,6 +15,7 @@
 #include <linux/mman.h>
 #include <linux/nodemask.h>
 
+#include <asm/cputype.h>
 #include <asm/mach-types.h>
 #include <asm/setup.h>
 #include <asm/sizes.h>

commit 60296c71f6c5063e3c1f1d2619ca0b60940162e7
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Tue Aug 5 01:56:13 2008 +0200

    [ARM] prevent crashing when too much RAM installed
    
    This patch will truncate and/or ignore memory banks if their kernel
    direct mappings would (partially) overlap with the vmalloc area or
    the mappings between the vmalloc area and the address space top, to
    prevent crashing during early boot if there happens to be more RAM
    installed than we are expecting.
    
    Since the start of the vmalloc area is not at a fixed address (but
    the vmalloc end address is, via the per-platform VMALLOC_END define),
    a default area of 128M is reserved for vmalloc mappings, which can
    be shrunk or enlarged by passing an appropriate vmalloc= command line
    option as it is done on x86.
    
    On a board with a 3:1 user:kernel split, VMALLOC_END at 0xfe000000,
    two 512M RAM banks and vmalloc=128M (the default), this patch gives:
    
            Truncating RAM at 20000000-3fffffff to -35ffffff (vmalloc region overlap).
            Memory: 512MB 352MB = 864MB total
    
    On a board with a 3:1 user:kernel split, VMALLOC_END at 0xfe800000,
    two 256M RAM banks and vmalloc=768M, this patch gives:
    
            Truncating RAM at 00000000-0fffffff to -0e7fffff (vmalloc region overlap).
            Ignoring RAM at 10000000-1fffffff (vmalloc region overlap).
    
    Signed-off-by: Lennert Buytenhek <buytenh@marvell.com>
    Tested-by: Riku Voipio <riku.voipio@iki.fi>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 2d6d682c206a..25d9a11eb617 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -568,6 +568,55 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 		create_mapping(io_desc + i);
 }
 
+static int __init check_membank_valid(struct membank *mb)
+{
+	/*
+	 * Check whether this memory region has non-zero size.
+	 */
+	if (mb->size == 0)
+		return 0;
+
+	/*
+	 * Check whether this memory region would entirely overlap
+	 * the vmalloc area.
+	 */
+	if (phys_to_virt(mb->start) >= VMALLOC_MIN) {
+		printk(KERN_NOTICE "Ignoring RAM at %.8lx-%.8lx "
+			"(vmalloc region overlap).\n",
+			mb->start, mb->start + mb->size - 1);
+		return 0;
+	}
+
+	/*
+	 * Check whether this memory region would partially overlap
+	 * the vmalloc area.
+	 */
+	if (phys_to_virt(mb->start + mb->size) < phys_to_virt(mb->start) ||
+	    phys_to_virt(mb->start + mb->size) > VMALLOC_MIN) {
+		unsigned long newsize = VMALLOC_MIN - phys_to_virt(mb->start);
+
+		printk(KERN_NOTICE "Truncating RAM at %.8lx-%.8lx "
+			"to -%.8lx (vmalloc region overlap).\n",
+			mb->start, mb->start + mb->size - 1,
+			mb->start + newsize - 1);
+		mb->size = newsize;
+	}
+
+	return 1;
+}
+
+static void __init sanity_check_meminfo(struct meminfo *mi)
+{
+	int i;
+	int j;
+
+	for (i = 0, j = 0; i < mi->nr_banks; i++) {
+		if (check_membank_valid(&mi->bank[i]))
+			mi->bank[j++] = mi->bank[i];
+	}
+	mi->nr_banks = j;
+}
+
 static inline void prepare_page_table(struct meminfo *mi)
 {
 	unsigned long addr;
@@ -753,6 +802,7 @@ void __init paging_init(struct meminfo *mi, struct machine_desc *mdesc)
 	void *zero_page;
 
 	build_mem_type_table();
+	sanity_check_meminfo(mi);
 	prepare_page_table(mi);
 	bootmem_init(mi);
 	devicemaps_init(mdesc);

commit 3653f3abe37f334659eea9d889cf8dc798fc4baa
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Tue Apr 29 08:11:12 2008 -0400

    arm: Export empty_zero_page for ZERO_PAGE usage in modules.
    
    ext4 uses ZERO_PAGE(0) to zero out blocks.  We need to export
    different symbols in different arches for the usage of ZERO_PAGE
    in modules.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: "Theodore Ts'o" <tytso@mit.edu>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index d41a75ed3dce..2d6d682c206a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -35,6 +35,7 @@ extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
  * zero-initialized data and COW.
  */
 struct page *empty_zero_page;
+EXPORT_SYMBOL(empty_zero_page);
 
 /*
  * The pmd table for the upper-most set of pages.

commit 72a7fe3967dbf86cb34e24fbf1d957fe24d2f246
Author: Bernhard Walle <bwalle@suse.de>
Date:   Thu Feb 7 00:15:17 2008 -0800

    Introduce flags for reserve_bootmem()
    
    This patchset adds a flags variable to reserve_bootmem() and uses the
    BOOTMEM_EXCLUSIVE flag in crashkernel reservation code to detect collisions
    between crashkernel area and already used memory.
    
    This patch:
    
    Change the reserve_bootmem() function to accept a new flag BOOTMEM_EXCLUSIVE.
    If that flag is set, the function returns with -EBUSY if the memory already
    has been reserved in the past.  This is to avoid conflicts.
    
    Because that code runs before SMP initialisation, there's no race condition
    inside reserve_bootmem_core().
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: fix powerpc build]
    Signed-off-by: Bernhard Walle <bwalle@suse.de>
    Cc: <linux-arch@vger.kernel.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Vivek Goyal <vgoyal@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e5d61ee3d4a1..d41a75ed3dce 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -605,9 +605,11 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	 * Note that this can only be in node 0.
 	 */
 #ifdef CONFIG_XIP_KERNEL
-	reserve_bootmem_node(pgdat, __pa(&__data_start), &_end - &__data_start);
+	reserve_bootmem_node(pgdat, __pa(&__data_start), &_end - &__data_start,
+			BOOTMEM_DEFAULT);
 #else
-	reserve_bootmem_node(pgdat, __pa(&_stext), &_end - &_stext);
+	reserve_bootmem_node(pgdat, __pa(&_stext), &_end - &_stext,
+			BOOTMEM_DEFAULT);
 #endif
 
 	/*
@@ -615,7 +617,7 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	 * and can only be in node 0.
 	 */
 	reserve_bootmem_node(pgdat, __pa(swapper_pg_dir),
-			     PTRS_PER_PGD * sizeof(pgd_t));
+			     PTRS_PER_PGD * sizeof(pgd_t), BOOTMEM_DEFAULT);
 
 	/*
 	 * Hmm... This should go elsewhere, but we really really need to
@@ -638,8 +640,10 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	/* H1940 and RX3715 need to reserve this for suspend */
 
 	if (machine_is_h1940() || machine_is_rx3715()) {
-		reserve_bootmem_node(pgdat, 0x30003000, 0x1000);
-		reserve_bootmem_node(pgdat, 0x30081000, 0x1000);
+		reserve_bootmem_node(pgdat, 0x30003000, 0x1000,
+				BOOTMEM_DEFAULT);
+		reserve_bootmem_node(pgdat, 0x30081000, 0x1000,
+				BOOTMEM_DEFAULT);
 	}
 
 #ifdef CONFIG_SA1111
@@ -650,7 +654,8 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	res_size = __pa(swapper_pg_dir) - PHYS_OFFSET;
 #endif
 	if (res_size)
-		reserve_bootmem_node(pgdat, PHYS_OFFSET, res_size);
+		reserve_bootmem_node(pgdat, PHYS_OFFSET, res_size,
+				BOOTMEM_DEFAULT);
 }
 
 /*

commit 11179d8ca28d669e3d4cc7573a5f5fdda3e6f02d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 20 11:42:24 2007 +0100

    [ARM] 4497/1: Only allow safe cache configurations on ARMv6 and later
    
    Currently, Linux doesn't generate correct page tables for ARMv6 and
    later cores if the cache policy is different from the default one (it
    may lead to strongly ordered or shared device mappings). This patch
    disallows cache policies other than writeback and the
    CPU_[ID]CACHE_DISABLE options only affect the CP15 system control
    register rather than the page tables.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 3b5e47dc0c97..e5d61ee3d4a1 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -114,6 +114,10 @@ static void __init early_cachepolicy(char **p)
 	}
 	if (i == ARRAY_SIZE(cache_policies))
 		printk(KERN_ERR "ERROR: unknown or unsupported cache policy\n");
+	if (cpu_architecture() >= CPU_ARCH_ARMv6) {
+		printk(KERN_WARNING "Only cachepolicy=writeback supported on ARMv6 and later\n");
+		cachepolicy = CPOLICY_WRITEBACK;
+	}
 	flush_cache_all();
 	set_cr(cr_alignment);
 }
@@ -252,13 +256,15 @@ static void __init build_mem_type_table(void)
 	int cpu_arch = cpu_architecture();
 	int i;
 
+	if (cpu_arch < CPU_ARCH_ARMv6) {
 #if defined(CONFIG_CPU_DCACHE_DISABLE)
-	if (cachepolicy > CPOLICY_BUFFERED)
-		cachepolicy = CPOLICY_BUFFERED;
+		if (cachepolicy > CPOLICY_BUFFERED)
+			cachepolicy = CPOLICY_BUFFERED;
 #elif defined(CONFIG_CPU_DCACHE_WRITETHROUGH)
-	if (cachepolicy > CPOLICY_WRITETHROUGH)
-		cachepolicy = CPOLICY_WRITETHROUGH;
+		if (cachepolicy > CPOLICY_WRITETHROUGH)
+			cachepolicy = CPOLICY_WRITETHROUGH;
 #endif
+	}
 	if (cpu_arch < CPU_ARCH_ARMv5) {
 		if (cachepolicy >= CPOLICY_WRITEALLOC)
 			cachepolicy = CPOLICY_WRITEBACK;

commit 7b9c7b4d07fd8981193a2c4ecb650566f42d1219
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Jul 4 21:16:33 2007 +0100

    [ARM] Fix non-page aligned boot time mappings
    
    AT91SAM9260 stopped booting with the recent changes to MM
    initialisation - it was asking for a non-aligned virtual address
    which caused loops to be non-terminal.  Fix this by rounding
    virtual addresses down, but remember to include the offset in
    the length, and round the length up to the following page.
    
    This means that asking for a mapping of 4K starting at 2K into
    a page maps two pages as one would expect.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 02e050ae59f6..3b5e47dc0c97 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -527,9 +527,9 @@ void __init create_mapping(struct map_desc *md)
 		return;
 	}
 
-	addr = md->virtual;
+	addr = md->virtual & PAGE_MASK;
 	phys = (unsigned long)__pfn_to_phys(md->pfn);
-	length = PAGE_ALIGN(md->length);
+	length = PAGE_ALIGN(md->length + (md->virtual & ~PAGE_MASK));
 
 	if (type->prot_l1 == 0 && ((addr | phys | length) & ~SECTION_MASK)) {
 		printk(KERN_WARNING "BUG: map for 0x%08lx at 0x%08lx can not "

commit 6cbdc8c5357276307a77deeada3f04626ff17da6
Author: Simon Arlott <simon@fire.lp0.eu>
Date:   Fri May 11 20:40:30 2007 +0100

    [ARM] spelling fixes
    
    Spelling fixes in arch/arm/.
    
    Signed-off-by: Simon Arlott <simon@fire.lp0.eu>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 2ba1530d1ce1..02e050ae59f6 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -92,7 +92,7 @@ static struct cachepolicy cache_policies[] __initdata = {
 };
 
 /*
- * These are useful for identifing cache coherency
+ * These are useful for identifying cache coherency
  * problems by allowing the cache or the cache and
  * writebuffer to be turned off.  (Note: the write
  * buffer should not be on and the cache off).

commit 0af92befeb4b330c46cce6b520b2cc775cd6931f
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat May 5 20:28:16 2007 +0100

    [ARM] mm 9: add additional device memory types
    
    Add cached device type for ioremap_cached().  Group all device memory
    types together, and ensure that they all have a "MT_DEVICE" prefix.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 44f385a3eb3f..2ba1530d1ce1 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -176,14 +176,35 @@ void adjust_cr(unsigned long mask, unsigned long set)
 }
 #endif
 
+#define PROT_PTE_DEVICE		L_PTE_PRESENT|L_PTE_YOUNG|L_PTE_DIRTY|L_PTE_WRITE
+#define PROT_SECT_DEVICE	PMD_TYPE_SECT|PMD_SECT_XN|PMD_SECT_AP_WRITE
+
 static struct mem_type mem_types[] = {
-	[MT_DEVICE] = {
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE,
-		.prot_l1   = PMD_TYPE_TABLE,
-		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_UNCACHED |
-				PMD_SECT_AP_WRITE,
-		.domain    = DOMAIN_IO,
+	[MT_DEVICE] = {		  /* Strongly ordered / ARMv6 shared device */
+		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_UNCACHED,
+		.domain		= DOMAIN_IO,
+	},
+	[MT_DEVICE_NONSHARED] = { /* ARMv6 non-shared device */
+		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_pte_ext	= PTE_EXT_TEX(2),
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_TEX(2),
+		.domain		= DOMAIN_IO,
+	},
+	[MT_DEVICE_CACHED] = {	  /* ioremap_cached */
+		.prot_pte	= PROT_PTE_DEVICE | L_PTE_CACHEABLE | L_PTE_BUFFERABLE,
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_WB,
+		.domain		= DOMAIN_IO,
+	},	
+	[MT_DEVICE_IXP2000] = {	  /* IXP2400 requires XCB=101 for on-chip I/O */
+		.prot_pte	= PROT_PTE_DEVICE,
+		.prot_l1	= PMD_TYPE_TABLE,
+		.prot_sect	= PROT_SECT_DEVICE | PMD_SECT_BUFFERABLE |
+				  PMD_SECT_TEX(1),
+		.domain		= DOMAIN_IO,
 	},
 	[MT_CACHECLEAN] = {
 		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
@@ -213,21 +234,6 @@ static struct mem_type mem_types[] = {
 		.prot_sect = PMD_TYPE_SECT,
 		.domain    = DOMAIN_KERNEL,
 	},
-	[MT_IXP2000_DEVICE] = { /* IXP2400 requires XCB=101 for on-chip I/O */
-		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
-				L_PTE_WRITE,
-		.prot_l1   = PMD_TYPE_TABLE,
-		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_UNCACHED |
-				PMD_SECT_AP_WRITE | PMD_SECT_BUFFERABLE |
-				PMD_SECT_TEX(1),
-		.domain    = DOMAIN_IO,
-	},
-	[MT_NONSHARED_DEVICE] = {
-		.prot_l1   = PMD_TYPE_TABLE,
-		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_NONSHARED_DEV |
-				PMD_SECT_AP_WRITE,
-		.domain    = DOMAIN_IO,
-	}
 };
 
 const struct mem_type *get_mem_type(unsigned int type)

commit 9ef7963503abd3287943125681c2dc17879e8d4e
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat May 5 20:03:35 2007 +0100

    [ARM] mm 8: define mem_types table L1 bit 4 to be for ARMv6
    
    Change the memory types table to define the L1 descriptor bit 4 to
    be in terms of the ARMv6 definition - execute never.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 360405515bbd..44f385a3eb3f 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -181,16 +181,16 @@ static struct mem_type mem_types[] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_WRITE,
 		.prot_l1   = PMD_TYPE_TABLE,
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_UNCACHED |
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_UNCACHED |
 				PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_IO,
 	},
 	[MT_CACHECLEAN] = {
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4,
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_MINICLEAN] = {
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_MINICACHE,
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_MINICACHE,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_LOW_VECTORS] = {
@@ -206,25 +206,25 @@ static struct mem_type mem_types[] = {
 		.domain    = DOMAIN_USER,
 	},
 	[MT_MEMORY] = {
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_AP_WRITE,
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_ROM] = {
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4,
+		.prot_sect = PMD_TYPE_SECT,
 		.domain    = DOMAIN_KERNEL,
 	},
 	[MT_IXP2000_DEVICE] = { /* IXP2400 requires XCB=101 for on-chip I/O */
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_WRITE,
 		.prot_l1   = PMD_TYPE_TABLE,
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_UNCACHED |
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_UNCACHED |
 				PMD_SECT_AP_WRITE | PMD_SECT_BUFFERABLE |
 				PMD_SECT_TEX(1),
 		.domain    = DOMAIN_IO,
 	},
 	[MT_NONSHARED_DEVICE] = {
 		.prot_l1   = PMD_TYPE_TABLE,
-		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_NONSHARED_DEV |
+		.prot_sect = PMD_TYPE_SECT | PMD_SECT_XN | PMD_SECT_NONSHARED_DEV |
 				PMD_SECT_AP_WRITE,
 		.domain    = DOMAIN_IO,
 	}
@@ -260,20 +260,23 @@ static void __init build_mem_type_table(void)
 	}
 
 	/*
-	 * Xscale must not have PMD bit 4 set for section mappings.
+	 * ARMv5 and lower, bit 4 must be set for page tables.
+	 * (was: cache "update-able on write" bit on ARM610)
+	 * However, Xscale cores require this bit to be cleared.
 	 */
-	if (cpu_is_xscale())
-		for (i = 0; i < ARRAY_SIZE(mem_types); i++)
+	if (cpu_is_xscale()) {
+		for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
 			mem_types[i].prot_sect &= ~PMD_BIT4;
-
-	/*
-	 * ARMv5 and lower, excluding Xscale, bit 4 must be set for
-	 * page tables.
-	 */
-	if (cpu_arch < CPU_ARCH_ARMv6 && !cpu_is_xscale())
-		for (i = 0; i < ARRAY_SIZE(mem_types); i++)
+			mem_types[i].prot_l1 &= ~PMD_BIT4;
+		}
+	} else if (cpu_arch < CPU_ARCH_ARMv6) {
+		for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
 			if (mem_types[i].prot_l1)
 				mem_types[i].prot_l1 |= PMD_BIT4;
+			if (mem_types[i].prot_sect)
+				mem_types[i].prot_sect |= PMD_BIT4;
+		}
+	}
 
 	cp = &cache_policies[cachepolicy];
 	kern_pgprot = user_pgprot = cp->pte;
@@ -293,13 +296,6 @@ static void __init build_mem_type_table(void)
 	 * ARMv6 and above have extended page tables.
 	 */
 	if (cpu_arch >= CPU_ARCH_ARMv6 && (cr & CR_XP)) {
-		/*
-		 * bit 4 becomes XN which we must clear for the
-		 * kernel memory mapping.
-		 */
-		mem_types[MT_MEMORY].prot_sect &= ~PMD_SECT_XN;
-		mem_types[MT_ROM].prot_sect &= ~PMD_SECT_XN;
-
 		/*
 		 * Mark cache clean areas and XIP ROM read only
 		 * from SVC mode and no access from userspace.

commit c172cc92c87103c98b5cd359205b684bf99b5067
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:52:32 2007 +0100

    [ARM] mm 6: allow mem_types table to specify extended pte attributes
    
    Add prot_pte_ext to the mem_types table to allow the extended pte
    attributes to be passed to set_pte_ext(), thereby permitting us to
    specify memory type information for the hardware PTE entries.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6cb80b4973d2..360405515bbd 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -391,7 +391,8 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 
 	pte = pte_offset_kernel(pmd, addr);
 	do {
-		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
+		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)),
+			    type->prot_pte_ext);
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }

commit b29e9f5e64fb90d2e4be1c7ef8c925b56669c74a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:47:29 2007 +0100

    [ARM] mm 5: Use mem_types table in ioremap
    
    We really want to be using the memory type table in ioremap, so we
    only have to do the CPU type fixups in one place.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 5821e67cf8c2..6cb80b4973d2 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -176,14 +176,7 @@ void adjust_cr(unsigned long mask, unsigned long set)
 }
 #endif
 
-struct mem_type {
-	unsigned int	prot_pte;
-	unsigned int	prot_l1;
-	unsigned int	prot_sect;
-	unsigned int	domain;
-};
-
-static struct mem_type mem_types[] __initdata = {
+static struct mem_type mem_types[] = {
 	[MT_DEVICE] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_WRITE,
@@ -237,6 +230,11 @@ static struct mem_type mem_types[] __initdata = {
 	}
 };
 
+const struct mem_type *get_mem_type(unsigned int type)
+{
+	return type < ARRAY_SIZE(mem_types) ? &mem_types[type] : NULL;
+}
+
 /*
  * Adjust the PMD section entries according to the CPU in use.
  */

commit 24e6c6996fb6e0e716c1dda1def1bb023a0fe43b
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:21:28 2007 +0100

    [ARM] mm 4: make create_mapping() more conventional
    
    Rather than our three separate loops to setup mappings (by page
    mappings up to a section boundary, then section mappings, and the
    remainder by page mappings) convert this to a more conventional
    Linux style of a loop over each page table level.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 32139800d939..5821e67cf8c2 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -380,45 +380,55 @@ static void __init build_mem_type_table(void)
 
 #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
 
-/*
- * Create a SECTION PGD between VIRT and PHYS in domain
- * DOMAIN with protection PROT.  This operates on half-
- * pgdir entry increments.
- */
-static inline void
-alloc_init_section(unsigned long virt, unsigned long phys, int prot)
+static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
+				  unsigned long end, unsigned long pfn,
+				  const struct mem_type *type)
 {
-	pmd_t *pmdp = pmd_off_k(virt);
+	pte_t *pte;
 
-	if (virt & (1 << 20))
-		pmdp++;
+	if (pmd_none(*pmd)) {
+		pte = alloc_bootmem_low_pages(2 * PTRS_PER_PTE * sizeof(pte_t));
+		__pmd_populate(pmd, __pa(pte) | type->prot_l1);
+	}
 
-	*pmdp = __pmd(phys | prot);
-	flush_pmd_entry(pmdp);
+	pte = pte_offset_kernel(pmd, addr);
+	do {
+		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
+		pfn++;
+	} while (pte++, addr += PAGE_SIZE, addr != end);
 }
 
-/*
- * Add a PAGE mapping between VIRT and PHYS in domain
- * DOMAIN with protection PROT.  Note that due to the
- * way we map the PTEs, we must allocate two PTE_SIZE'd
- * blocks - one for the Linux pte table, and one for
- * the hardware pte table.
- */
-static inline void
-alloc_init_page(unsigned long virt, unsigned long phys, const struct mem_type *type)
+static void __init alloc_init_section(pgd_t *pgd, unsigned long addr,
+				      unsigned long end, unsigned long phys,
+				      const struct mem_type *type)
 {
-	pmd_t *pmdp = pmd_off_k(virt);
-	pte_t *ptep;
+	pmd_t *pmd = pmd_offset(pgd, addr);
 
-	if (pmd_none(*pmdp)) {
-		ptep = alloc_bootmem_low_pages(2 * PTRS_PER_PTE *
-					       sizeof(pte_t));
+	/*
+	 * Try a section mapping - end, addr and phys must all be aligned
+	 * to a section boundary.  Note that PMDs refer to the individual
+	 * L1 entries, whereas PGDs refer to a group of L1 entries making
+	 * up one logical pointer to an L2 table.
+	 */
+	if (((addr | end | phys) & ~SECTION_MASK) == 0) {
+		pmd_t *p = pmd;
 
-		__pmd_populate(pmdp, __pa(ptep) | type->prot_l1);
-	}
-	ptep = pte_offset_kernel(pmdp, virt);
+		if (addr & SECTION_SIZE)
+			pmd++;
+
+		do {
+			*pmd = __pmd(phys | type->prot_sect);
+			phys += SECTION_SIZE;
+		} while (pmd++, addr += SECTION_SIZE, addr != end);
 
-	set_pte_ext(ptep, pfn_pte(phys >> PAGE_SHIFT, __pgprot(type->prot_pte)), 0);
+		flush_pmd_entry(p);
+	} else {
+		/*
+		 * No need to loop; pte's aren't interested in the
+		 * individual L1 entries.
+		 */
+		alloc_init_pte(pmd, addr, end, __phys_to_pfn(phys), type);
+	}
 }
 
 static void __init create_36bit_mapping(struct map_desc *md,
@@ -488,9 +498,9 @@ static void __init create_36bit_mapping(struct map_desc *md,
  */
 void __init create_mapping(struct map_desc *md)
 {
-	unsigned long virt, length;
-	unsigned long off = (u32)__pfn_to_phys(md->pfn);
+	unsigned long phys, addr, length, end;
 	const struct mem_type *type;
+	pgd_t *pgd;
 
 	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
 		printk(KERN_WARNING "BUG: not creating mapping for "
@@ -516,41 +526,27 @@ void __init create_mapping(struct map_desc *md)
 		return;
 	}
 
-	virt   = md->virtual;
-	off   -= virt;
-	length = md->length;
+	addr = md->virtual;
+	phys = (unsigned long)__pfn_to_phys(md->pfn);
+	length = PAGE_ALIGN(md->length);
 
-	if (type->prot_l1 == 0 &&
-	    (virt & 0xfffff || (virt + off) & 0xfffff || (virt + length) & 0xfffff)) {
+	if (type->prot_l1 == 0 && ((addr | phys | length) & ~SECTION_MASK)) {
 		printk(KERN_WARNING "BUG: map for 0x%08lx at 0x%08lx can not "
 		       "be mapped using pages, ignoring.\n",
-		       __pfn_to_phys(md->pfn), md->virtual);
+		       __pfn_to_phys(md->pfn), addr);
 		return;
 	}
 
-	while ((virt & 0xfffff || (virt + off) & 0xfffff) && length >= PAGE_SIZE) {
-		alloc_init_page(virt, virt + off, type);
-
-		virt   += PAGE_SIZE;
-		length -= PAGE_SIZE;
-	}
-
-	/*
-	 * A section mapping covers half a "pgdir" entry.
-	 */
-	while (length >= (PGDIR_SIZE / 2)) {
-		alloc_init_section(virt, virt + off, type->prot_sect);
-
-		virt   += (PGDIR_SIZE / 2);
-		length -= (PGDIR_SIZE / 2);
-	}
+	pgd = pgd_offset_k(addr);
+	end = addr + length;
+	do {
+		unsigned long next = pgd_addr_end(addr, end);
 
-	while (length >= PAGE_SIZE) {
-		alloc_init_page(virt, virt + off, type);
+		alloc_init_section(pgd, addr, next, phys, type);
 
-		virt   += PAGE_SIZE;
-		length -= PAGE_SIZE;
-	}
+		phys += next - addr;
+		addr = next;
+	} while (pgd++, addr != end);
 }
 
 /*

commit 4a56c1e41f19393577bdd5c774c289c199b7269d
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:16:48 2007 +0100

    [ARM] mm 3: separate out supersection mappings, avoid for <4GB
    
    Catalin Marinas at ARM Ltd says:
    > The CPU architects in ARM intended supersections only as a way to map
    > addresses >= 4GB. Supersections are not mandated by the architecture
    > and there is no easy way to detect their hardware support at run-time
    > (other than checking for a specific core). From the analysis done in
    > ARM, there wasn't a clear performance gain by using supersections
    > rather than sections (no significant improvement in the TLB misses).
    
    Therefore, we should avoid using supersections unless there's a real
    need (iow, we're mapping addresses >= 4GB).
    
    This means that we can simplify create_mapping() a bit since we will
    only use supersection mappings for addresses >= 4GB, which means that
    the physical, virtual and length must be multiples of the supersection
    mapping size.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e359f3685433..32139800d939 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -397,21 +397,6 @@ alloc_init_section(unsigned long virt, unsigned long phys, int prot)
 	flush_pmd_entry(pmdp);
 }
 
-/*
- * Create a SUPER SECTION PGD between VIRT and PHYS with protection PROT
- */
-static inline void
-alloc_init_supersection(unsigned long virt, unsigned long phys, int prot)
-{
-	int i;
-
-	for (i = 0; i < 16; i += 1) {
-		alloc_init_section(virt, phys, prot | PMD_SECT_SUPER);
-
-		virt += (PGDIR_SIZE / 2);
-	}
-}
-
 /*
  * Add a PAGE mapping between VIRT and PHYS in domain
  * DOMAIN with protection PROT.  Note that due to the
@@ -436,6 +421,64 @@ alloc_init_page(unsigned long virt, unsigned long phys, const struct mem_type *t
 	set_pte_ext(ptep, pfn_pte(phys >> PAGE_SHIFT, __pgprot(type->prot_pte)), 0);
 }
 
+static void __init create_36bit_mapping(struct map_desc *md,
+					const struct mem_type *type)
+{
+	unsigned long phys, addr, length, end;
+	pgd_t *pgd;
+
+	addr = md->virtual;
+	phys = (unsigned long)__pfn_to_phys(md->pfn);
+	length = PAGE_ALIGN(md->length);
+
+	if (!(cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())) {
+		printk(KERN_ERR "MM: CPU does not support supersection "
+		       "mapping for 0x%08llx at 0x%08lx\n",
+		       __pfn_to_phys((u64)md->pfn), addr);
+		return;
+	}
+
+	/* N.B.	ARMv6 supersections are only defined to work with domain 0.
+	 *	Since domain assignments can in fact be arbitrary, the
+	 *	'domain == 0' check below is required to insure that ARMv6
+	 *	supersections are only allocated for domain 0 regardless
+	 *	of the actual domain assignments in use.
+	 */
+	if (type->domain) {
+		printk(KERN_ERR "MM: invalid domain in supersection "
+		       "mapping for 0x%08llx at 0x%08lx\n",
+		       __pfn_to_phys((u64)md->pfn), addr);
+		return;
+	}
+
+	if ((addr | length | __pfn_to_phys(md->pfn)) & ~SUPERSECTION_MASK) {
+		printk(KERN_ERR "MM: cannot create mapping for "
+		       "0x%08llx at 0x%08lx invalid alignment\n",
+		       __pfn_to_phys((u64)md->pfn), addr);
+		return;
+	}
+
+	/*
+	 * Shift bits [35:32] of address into bits [23:20] of PMD
+	 * (See ARMv6 spec).
+	 */
+	phys |= (((md->pfn >> (32 - PAGE_SHIFT)) & 0xF) << 20);
+
+	pgd = pgd_offset_k(addr);
+	end = addr + length;
+	do {
+		pmd_t *pmd = pmd_offset(pgd, addr);
+		int i;
+
+		for (i = 0; i < 16; i++)
+			*pmd++ = __pmd(phys | type->prot_sect | PMD_SECT_SUPER);
+
+		addr += SUPERSECTION_SIZE;
+		phys += SUPERSECTION_SIZE;
+		pgd += SUPERSECTION_SIZE >> PGDIR_SHIFT;
+	} while (addr != end);
+}
+
 /*
  * Create the page directory entries and any necessary
  * page tables for the mapping specified by `md'.  We
@@ -468,26 +511,9 @@ void __init create_mapping(struct map_desc *md)
 	/*
 	 * Catch 36-bit addresses
 	 */
-	if(md->pfn >= 0x100000) {
-		if (type->domain) {
-			printk(KERN_ERR "MM: invalid domain in supersection "
-				"mapping for 0x%08llx at 0x%08lx\n",
-				__pfn_to_phys((u64)md->pfn), md->virtual);
-			return;
-		}
-		if((md->virtual | md->length | __pfn_to_phys(md->pfn))
-			& ~SUPERSECTION_MASK) {
-			printk(KERN_ERR "MM: cannot create mapping for "
-				"0x%08llx at 0x%08lx invalid alignment\n",
-				__pfn_to_phys((u64)md->pfn), md->virtual);
-			return;
-		}
-
-		/*
-		 * Shift bits [35:32] of address into bits [23:20] of PMD
-		 * (See ARMv6 spec).
-		 */
-		off |= (((md->pfn >> (32 - PAGE_SHIFT)) & 0xF) << 20);
+	if (md->pfn >= 0x100000) {
+		create_36bit_mapping(md, type);
+		return;
 	}
 
 	virt   = md->virtual;
@@ -509,40 +535,6 @@ void __init create_mapping(struct map_desc *md)
 		length -= PAGE_SIZE;
 	}
 
-	/* N.B.	ARMv6 supersections are only defined to work with domain 0.
-	 *	Since domain assignments can in fact be arbitrary, the
-	 *	'domain == 0' check below is required to insure that ARMv6
-	 *	supersections are only allocated for domain 0 regardless
-	 *	of the actual domain assignments in use.
-	 */
-	if ((cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())
-		&& type->domain == 0) {
-		/*
-		 * Align to supersection boundary if !high pages.
-		 * High pages have already been checked for proper
-		 * alignment above and they will fail the SUPSERSECTION_MASK
-		 * check because of the way the address is encoded into
-		 * offset.
-		 */
-		if (md->pfn <= 0x100000) {
-			while ((virt & ~SUPERSECTION_MASK ||
-			        (virt + off) & ~SUPERSECTION_MASK) &&
-				length >= (PGDIR_SIZE / 2)) {
-				alloc_init_section(virt, virt + off, type->prot_sect);
-
-				virt   += (PGDIR_SIZE / 2);
-				length -= (PGDIR_SIZE / 2);
-			}
-		}
-
-		while (length >= SUPERSECTION_SIZE) {
-			alloc_init_supersection(virt, virt + off, type->prot_sect);
-
-			virt   += SUPERSECTION_SIZE;
-			length -= SUPERSECTION_SIZE;
-		}
-	}
-
 	/*
 	 * A section mapping covers half a "pgdir" entry.
 	 */

commit d5c98176ef34b8b78645646593c17e10f62f53ff
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 10:05:32 2007 +0100

    [ARM] mm 2: clean up create_mapping()
    
    There's now no need to carry around each protection separately.
    Instead, pass around the pointer to the entry in the mem_types
    array which we're interested in.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 6178be0242f2..e359f3685433 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -420,7 +420,7 @@ alloc_init_supersection(unsigned long virt, unsigned long phys, int prot)
  * the hardware pte table.
  */
 static inline void
-alloc_init_page(unsigned long virt, unsigned long phys, unsigned int prot_l1, pgprot_t prot)
+alloc_init_page(unsigned long virt, unsigned long phys, const struct mem_type *type)
 {
 	pmd_t *pmdp = pmd_off_k(virt);
 	pte_t *ptep;
@@ -429,11 +429,11 @@ alloc_init_page(unsigned long virt, unsigned long phys, unsigned int prot_l1, pg
 		ptep = alloc_bootmem_low_pages(2 * PTRS_PER_PTE *
 					       sizeof(pte_t));
 
-		__pmd_populate(pmdp, __pa(ptep) | prot_l1);
+		__pmd_populate(pmdp, __pa(ptep) | type->prot_l1);
 	}
 	ptep = pte_offset_kernel(pmdp, virt);
 
-	set_pte_ext(ptep, pfn_pte(phys >> PAGE_SHIFT, prot), 0);
+	set_pte_ext(ptep, pfn_pte(phys >> PAGE_SHIFT, __pgprot(type->prot_pte)), 0);
 }
 
 /*
@@ -446,9 +446,8 @@ alloc_init_page(unsigned long virt, unsigned long phys, unsigned int prot_l1, pg
 void __init create_mapping(struct map_desc *md)
 {
 	unsigned long virt, length;
-	int prot_sect, prot_l1, domain;
-	pgprot_t prot_pte;
 	unsigned long off = (u32)__pfn_to_phys(md->pfn);
+	const struct mem_type *type;
 
 	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
 		printk(KERN_WARNING "BUG: not creating mapping for "
@@ -464,16 +463,13 @@ void __init create_mapping(struct map_desc *md)
 		       __pfn_to_phys((u64)md->pfn), md->virtual);
 	}
 
-	domain	  = mem_types[md->type].domain;
-	prot_pte  = __pgprot(mem_types[md->type].prot_pte);
-	prot_l1   = mem_types[md->type].prot_l1;
-	prot_sect = mem_types[md->type].prot_sect;
+	type = &mem_types[md->type];
 
 	/*
 	 * Catch 36-bit addresses
 	 */
 	if(md->pfn >= 0x100000) {
-		if(domain) {
+		if (type->domain) {
 			printk(KERN_ERR "MM: invalid domain in supersection "
 				"mapping for 0x%08llx at 0x%08lx\n",
 				__pfn_to_phys((u64)md->pfn), md->virtual);
@@ -498,7 +494,7 @@ void __init create_mapping(struct map_desc *md)
 	off   -= virt;
 	length = md->length;
 
-	if (mem_types[md->type].prot_l1 == 0 &&
+	if (type->prot_l1 == 0 &&
 	    (virt & 0xfffff || (virt + off) & 0xfffff || (virt + length) & 0xfffff)) {
 		printk(KERN_WARNING "BUG: map for 0x%08lx at 0x%08lx can not "
 		       "be mapped using pages, ignoring.\n",
@@ -507,7 +503,7 @@ void __init create_mapping(struct map_desc *md)
 	}
 
 	while ((virt & 0xfffff || (virt + off) & 0xfffff) && length >= PAGE_SIZE) {
-		alloc_init_page(virt, virt + off, prot_l1, prot_pte);
+		alloc_init_page(virt, virt + off, type);
 
 		virt   += PAGE_SIZE;
 		length -= PAGE_SIZE;
@@ -520,7 +516,7 @@ void __init create_mapping(struct map_desc *md)
 	 *	of the actual domain assignments in use.
 	 */
 	if ((cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())
-		&& domain == 0) {
+		&& type->domain == 0) {
 		/*
 		 * Align to supersection boundary if !high pages.
 		 * High pages have already been checked for proper
@@ -532,7 +528,7 @@ void __init create_mapping(struct map_desc *md)
 			while ((virt & ~SUPERSECTION_MASK ||
 			        (virt + off) & ~SUPERSECTION_MASK) &&
 				length >= (PGDIR_SIZE / 2)) {
-				alloc_init_section(virt, virt + off, prot_sect);
+				alloc_init_section(virt, virt + off, type->prot_sect);
 
 				virt   += (PGDIR_SIZE / 2);
 				length -= (PGDIR_SIZE / 2);
@@ -540,7 +536,7 @@ void __init create_mapping(struct map_desc *md)
 		}
 
 		while (length >= SUPERSECTION_SIZE) {
-			alloc_init_supersection(virt, virt + off, prot_sect);
+			alloc_init_supersection(virt, virt + off, type->prot_sect);
 
 			virt   += SUPERSECTION_SIZE;
 			length -= SUPERSECTION_SIZE;
@@ -551,14 +547,14 @@ void __init create_mapping(struct map_desc *md)
 	 * A section mapping covers half a "pgdir" entry.
 	 */
 	while (length >= (PGDIR_SIZE / 2)) {
-		alloc_init_section(virt, virt + off, prot_sect);
+		alloc_init_section(virt, virt + off, type->prot_sect);
 
 		virt   += (PGDIR_SIZE / 2);
 		length -= (PGDIR_SIZE / 2);
 	}
 
 	while (length >= PAGE_SIZE) {
-		alloc_init_page(virt, virt + off, prot_l1, prot_pte);
+		alloc_init_page(virt, virt + off, type);
 
 		virt   += PAGE_SIZE;
 		length -= PAGE_SIZE;

commit 2497f0a8125e307cf1fd4222bab53f66305eba27
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Apr 21 09:59:44 2007 +0100

    [ARM] mm 1: Combine mem_type domain into prot_* at init time
    
    Rather than combining the domain for a particular memory type with
    the protection information each time we want to use it, do so when
    we fix up the mem_type array at initialisation time.
    
    Rename struct mem_types to be mem_type - each structure is one
    memory type description, not several.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 94fd4bf5cb9e..6178be0242f2 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -176,14 +176,14 @@ void adjust_cr(unsigned long mask, unsigned long set)
 }
 #endif
 
-struct mem_types {
+struct mem_type {
 	unsigned int	prot_pte;
 	unsigned int	prot_l1;
 	unsigned int	prot_sect;
 	unsigned int	domain;
 };
 
-static struct mem_types mem_types[] __initdata = {
+static struct mem_type mem_types[] __initdata = {
 	[MT_DEVICE] = {
 		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
 				L_PTE_WRITE,
@@ -368,6 +368,14 @@ static void __init build_mem_type_table(void)
 	}
 	printk("Memory policy: ECC %sabled, Data cache %s\n",
 		ecc_mask ? "en" : "dis", cp->policy);
+
+	for (i = 0; i < ARRAY_SIZE(mem_types); i++) {
+		struct mem_type *t = &mem_types[i];
+		if (t->prot_l1)
+			t->prot_l1 |= PMD_DOMAIN(t->domain);
+		if (t->prot_sect)
+			t->prot_sect |= PMD_DOMAIN(t->domain);
+	}
 }
 
 #define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
@@ -458,8 +466,8 @@ void __init create_mapping(struct map_desc *md)
 
 	domain	  = mem_types[md->type].domain;
 	prot_pte  = __pgprot(mem_types[md->type].prot_pte);
-	prot_l1   = mem_types[md->type].prot_l1 | PMD_DOMAIN(domain);
-	prot_sect = mem_types[md->type].prot_sect | PMD_DOMAIN(domain);
+	prot_l1   = mem_types[md->type].prot_l1;
+	prot_sect = mem_types[md->type].prot_sect;
 
 	/*
 	 * Catch 36-bit addresses

commit 44b18693904e65d840ae999ac8aa717551cf509f
Author: Imre_Deak <imre.deak@solidboot.com>
Date:   Sun Feb 11 13:45:13 2007 +0100

    [ARM] 4158/1: Fix user page protection macros
    
    The PAGE_* user page protection macros don't take into account the
    configured memory policy and other architecture specific bits like
    the global/ASID and shared mapping bits. Instead of constants let
    these depend on a variable fixed up at init just like PAGE_KERNEL.
    
    Signed-off-by: Imre Deak <imre.deak@solidboot.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 655c8376f0b5..94fd4bf5cb9e 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -49,8 +49,10 @@ pmd_t *top_pmd;
 
 static unsigned int cachepolicy __initdata = CPOLICY_WRITEBACK;
 static unsigned int ecc_mask __initdata = 0;
+pgprot_t pgprot_user;
 pgprot_t pgprot_kernel;
 
+EXPORT_SYMBOL(pgprot_user);
 EXPORT_SYMBOL(pgprot_kernel);
 
 struct cachepolicy {
@@ -345,6 +347,7 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_MINICLEAN].prot_sect &= ~PMD_SECT_TEX(1);
 	}
 
+	pgprot_user   = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG | user_pgprot);
 	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
 				 L_PTE_DIRTY | L_PTE_WRITE |
 				 L_PTE_EXEC | kern_pgprot);

commit 255d1f8639f5877381545d0da6821079ebad1c21
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Dec 18 00:12:47 2006 +0000

    [ARM] Fix warnings from asm/system.h
    
    Move adjust_cr() into arch/arm/mm/mmu.c, and move irqflags.h to
    a more appropriate place in the header file.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f028aef9a861..655c8376f0b5 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -154,6 +154,26 @@ static int __init noalign_setup(char *__unused)
 }
 __setup("noalign", noalign_setup);
 
+#ifndef CONFIG_SMP
+void adjust_cr(unsigned long mask, unsigned long set)
+{
+	unsigned long flags;
+
+	mask &= ~CR_A;
+
+	set &= mask;
+
+	local_irq_save(flags);
+
+	cr_no_alignment = (cr_no_alignment & ~mask) | set;
+	cr_alignment = (cr_alignment & ~mask) | set;
+
+	set_cr((get_cr() & ~mask) | set);
+
+	local_irq_restore(flags);
+}
+#endif
+
 struct mem_types {
 	unsigned int	prot_pte;
 	unsigned int	prot_l1;

commit ad1ae2fe7fe68414ef29eab3c87b48841f8b72f2
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Dec 13 14:34:43 2006 +0000

    [ARM] Unuse another Linux PTE bit
    
    L_PTE_ASID is not really required to be stored in every PTE, since we
    can identify it via the address passed to set_pte_at().  So, create
    set_pte_ext() which takes the address of the PTE to set, the Linux
    PTE value, and the additional CPU PTE bits which aren't encoded in
    the Linux PTE value.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index b7f194af20b4..f028aef9a861 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -294,12 +294,6 @@ static void __init build_mem_type_table(void)
 		mem_types[MT_DEVICE].prot_pte |= L_PTE_BUFFERABLE;
 		mem_types[MT_DEVICE].prot_sect |= PMD_SECT_BUFFERED;
 
-		/*
-		 * User pages need to be mapped with the ASID
-		 * (iow, non-global)
-		 */
-		user_pgprot |= L_PTE_ASID;
-
 #ifdef CONFIG_SMP
 		/*
 		 * Mark memory with the "shared" attribute for SMP systems
@@ -408,7 +402,7 @@ alloc_init_page(unsigned long virt, unsigned long phys, unsigned int prot_l1, pg
 	}
 	ptep = pte_offset_kernel(pmdp, virt);
 
-	set_pte(ptep, pfn_pte(phys >> PAGE_SHIFT, prot));
+	set_pte_ext(ptep, pfn_pte(phys >> PAGE_SHIFT, prot), 0);
 }
 
 /*

commit bbf6f2809dbadc2bacfd73a052d8b0893dbf1762
Author: Ben Dooks <ben-linux@fluff.org>
Date:   Thu Dec 7 20:47:58 2006 +0100

    [ARM] 3999/1: RX3715: suspend to RAM support
    
    The RX3715 is similar to the H1940 in the way
    that suspend to RAM works, so we can use most
    of the extant support for the H1940 with only
    a few modifictions
    
    Signed-off-by: Ben Dooks <ben-linux@fluff.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f2d0d6f78973..b7f194af20b4 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -619,7 +619,9 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	if (machine_is_p720t())
 		res_size = 0x00014000;
 
-	if (machine_is_h1940()) {
+	/* H1940 and RX3715 need to reserve this for suspend */
+
+	if (machine_is_h1940() || machine_is_rx3715()) {
 		reserve_bootmem_node(pgdat, 0x30003000, 0x1000);
 		reserve_bootmem_node(pgdat, 0x30081000, 0x1000);
 	}

commit 9073341c2ba5d5e77b3d05d84cf9e3a16e8a7902
Author: Ben Dooks <ben-linux@fluff.org>
Date:   Wed Dec 6 01:50:24 2006 +0100

    [ARM] 3986/1: H1940: suspend to RAM support
    
    Add support to suspend and resume, using the
    H1940's bootloader
    
    Signed-off-by: Ben Dooks <ben-linux@fluff.org>
    Signed-off-by: Arnaud Patard <arnaud.patard@rtp-net.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 445bc3b951e6..f2d0d6f78973 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -619,6 +619,11 @@ void __init reserve_node_zero(pg_data_t *pgdat)
 	if (machine_is_p720t())
 		res_size = 0x00014000;
 
+	if (machine_is_h1940()) {
+		reserve_bootmem_node(pgdat, 0x30003000, 0x1000);
+		reserve_bootmem_node(pgdat, 0x30081000, 0x1000);
+	}
+
 #ifdef CONFIG_SA1111
 	/*
 	 * Because of the SA1111 DMA bug, we want to preserve our

commit 0e5fdca7622eb950f42f97a1970670e00a739175
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Sat Dec 2 00:03:47 2006 +0100

    [ARM] 3971/1: xsc3: get rid of L_PTE_COHERENT
    
    Merge L_PTE_COHERENT with L_PTE_SHARED and free up a L_PTE_* bit.
    
    Signed-off-by: Lennert Buytenhek <buytenh@wantstofly.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index f866bf6b97d4..445bc3b951e6 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -265,7 +265,7 @@ static void __init build_mem_type_table(void)
 	if (arch_is_coherent()) {
 		if (cpu_is_xsc3()) {
 			mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
-			mem_types[MT_MEMORY].prot_pte |= L_PTE_COHERENT;
+			mem_types[MT_MEMORY].prot_pte |= L_PTE_SHARED;
 		}
 	}
 

commit 6ae5a6ef0340908fb7065a75231ae9980a910fbe
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 30 10:50:05 2006 +0100

    [ARM] Fix XIP_KERNEL build error in arch/arm/mm/mmu.c
    
    XIP kernels need to know the start/end of text, but we were
    missing the declaration of _etext in mmu.c.  Add it.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index e566cbe4b222..f866bf6b97d4 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -27,7 +27,7 @@
 
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
 
-extern void _stext, __data_start, _end;
+extern void _stext, _etext, __data_start, _end;
 extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
 
 /*

commit ae8f154129e4d965771c2d6adbe36210b3913d72
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Sep 27 15:38:34 2006 +0100

    [ARM] Move rest of MMU setup code from mm-armv.c to mmu.c
    
    If we're going to have mmu.c for code which is specific to the MMU
    machines, we might as well move the other MMU initialisation
    specific code from mm-armv.c into this new file.  This also allows
    us to make some functions static.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index 9648e6800ffe..e566cbe4b222 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -7,6 +7,7 @@
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
+#include <linux/module.h>
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/init.h>
@@ -40,6 +41,516 @@ struct page *empty_zero_page;
  */
 pmd_t *top_pmd;
 
+#define CPOLICY_UNCACHED	0
+#define CPOLICY_BUFFERED	1
+#define CPOLICY_WRITETHROUGH	2
+#define CPOLICY_WRITEBACK	3
+#define CPOLICY_WRITEALLOC	4
+
+static unsigned int cachepolicy __initdata = CPOLICY_WRITEBACK;
+static unsigned int ecc_mask __initdata = 0;
+pgprot_t pgprot_kernel;
+
+EXPORT_SYMBOL(pgprot_kernel);
+
+struct cachepolicy {
+	const char	policy[16];
+	unsigned int	cr_mask;
+	unsigned int	pmd;
+	unsigned int	pte;
+};
+
+static struct cachepolicy cache_policies[] __initdata = {
+	{
+		.policy		= "uncached",
+		.cr_mask	= CR_W|CR_C,
+		.pmd		= PMD_SECT_UNCACHED,
+		.pte		= 0,
+	}, {
+		.policy		= "buffered",
+		.cr_mask	= CR_C,
+		.pmd		= PMD_SECT_BUFFERED,
+		.pte		= PTE_BUFFERABLE,
+	}, {
+		.policy		= "writethrough",
+		.cr_mask	= 0,
+		.pmd		= PMD_SECT_WT,
+		.pte		= PTE_CACHEABLE,
+	}, {
+		.policy		= "writeback",
+		.cr_mask	= 0,
+		.pmd		= PMD_SECT_WB,
+		.pte		= PTE_BUFFERABLE|PTE_CACHEABLE,
+	}, {
+		.policy		= "writealloc",
+		.cr_mask	= 0,
+		.pmd		= PMD_SECT_WBWA,
+		.pte		= PTE_BUFFERABLE|PTE_CACHEABLE,
+	}
+};
+
+/*
+ * These are useful for identifing cache coherency
+ * problems by allowing the cache or the cache and
+ * writebuffer to be turned off.  (Note: the write
+ * buffer should not be on and the cache off).
+ */
+static void __init early_cachepolicy(char **p)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(cache_policies); i++) {
+		int len = strlen(cache_policies[i].policy);
+
+		if (memcmp(*p, cache_policies[i].policy, len) == 0) {
+			cachepolicy = i;
+			cr_alignment &= ~cache_policies[i].cr_mask;
+			cr_no_alignment &= ~cache_policies[i].cr_mask;
+			*p += len;
+			break;
+		}
+	}
+	if (i == ARRAY_SIZE(cache_policies))
+		printk(KERN_ERR "ERROR: unknown or unsupported cache policy\n");
+	flush_cache_all();
+	set_cr(cr_alignment);
+}
+__early_param("cachepolicy=", early_cachepolicy);
+
+static void __init early_nocache(char **__unused)
+{
+	char *p = "buffered";
+	printk(KERN_WARNING "nocache is deprecated; use cachepolicy=%s\n", p);
+	early_cachepolicy(&p);
+}
+__early_param("nocache", early_nocache);
+
+static void __init early_nowrite(char **__unused)
+{
+	char *p = "uncached";
+	printk(KERN_WARNING "nowb is deprecated; use cachepolicy=%s\n", p);
+	early_cachepolicy(&p);
+}
+__early_param("nowb", early_nowrite);
+
+static void __init early_ecc(char **p)
+{
+	if (memcmp(*p, "on", 2) == 0) {
+		ecc_mask = PMD_PROTECTION;
+		*p += 2;
+	} else if (memcmp(*p, "off", 3) == 0) {
+		ecc_mask = 0;
+		*p += 3;
+	}
+}
+__early_param("ecc=", early_ecc);
+
+static int __init noalign_setup(char *__unused)
+{
+	cr_alignment &= ~CR_A;
+	cr_no_alignment &= ~CR_A;
+	set_cr(cr_alignment);
+	return 1;
+}
+__setup("noalign", noalign_setup);
+
+struct mem_types {
+	unsigned int	prot_pte;
+	unsigned int	prot_l1;
+	unsigned int	prot_sect;
+	unsigned int	domain;
+};
+
+static struct mem_types mem_types[] __initdata = {
+	[MT_DEVICE] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_WRITE,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_UNCACHED |
+				PMD_SECT_AP_WRITE,
+		.domain    = DOMAIN_IO,
+	},
+	[MT_CACHECLEAN] = {
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4,
+		.domain    = DOMAIN_KERNEL,
+	},
+	[MT_MINICLEAN] = {
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_MINICACHE,
+		.domain    = DOMAIN_KERNEL,
+	},
+	[MT_LOW_VECTORS] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_EXEC,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.domain    = DOMAIN_USER,
+	},
+	[MT_HIGH_VECTORS] = {
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_USER | L_PTE_EXEC,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.domain    = DOMAIN_USER,
+	},
+	[MT_MEMORY] = {
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_AP_WRITE,
+		.domain    = DOMAIN_KERNEL,
+	},
+	[MT_ROM] = {
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4,
+		.domain    = DOMAIN_KERNEL,
+	},
+	[MT_IXP2000_DEVICE] = { /* IXP2400 requires XCB=101 for on-chip I/O */
+		.prot_pte  = L_PTE_PRESENT | L_PTE_YOUNG | L_PTE_DIRTY |
+				L_PTE_WRITE,
+		.prot_l1   = PMD_TYPE_TABLE,
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_UNCACHED |
+				PMD_SECT_AP_WRITE | PMD_SECT_BUFFERABLE |
+				PMD_SECT_TEX(1),
+		.domain    = DOMAIN_IO,
+	},
+	[MT_NONSHARED_DEVICE] = {
+		.prot_l1   = PMD_TYPE_TABLE,
+		.prot_sect = PMD_TYPE_SECT | PMD_BIT4 | PMD_SECT_NONSHARED_DEV |
+				PMD_SECT_AP_WRITE,
+		.domain    = DOMAIN_IO,
+	}
+};
+
+/*
+ * Adjust the PMD section entries according to the CPU in use.
+ */
+static void __init build_mem_type_table(void)
+{
+	struct cachepolicy *cp;
+	unsigned int cr = get_cr();
+	unsigned int user_pgprot, kern_pgprot;
+	int cpu_arch = cpu_architecture();
+	int i;
+
+#if defined(CONFIG_CPU_DCACHE_DISABLE)
+	if (cachepolicy > CPOLICY_BUFFERED)
+		cachepolicy = CPOLICY_BUFFERED;
+#elif defined(CONFIG_CPU_DCACHE_WRITETHROUGH)
+	if (cachepolicy > CPOLICY_WRITETHROUGH)
+		cachepolicy = CPOLICY_WRITETHROUGH;
+#endif
+	if (cpu_arch < CPU_ARCH_ARMv5) {
+		if (cachepolicy >= CPOLICY_WRITEALLOC)
+			cachepolicy = CPOLICY_WRITEBACK;
+		ecc_mask = 0;
+	}
+
+	/*
+	 * Xscale must not have PMD bit 4 set for section mappings.
+	 */
+	if (cpu_is_xscale())
+		for (i = 0; i < ARRAY_SIZE(mem_types); i++)
+			mem_types[i].prot_sect &= ~PMD_BIT4;
+
+	/*
+	 * ARMv5 and lower, excluding Xscale, bit 4 must be set for
+	 * page tables.
+	 */
+	if (cpu_arch < CPU_ARCH_ARMv6 && !cpu_is_xscale())
+		for (i = 0; i < ARRAY_SIZE(mem_types); i++)
+			if (mem_types[i].prot_l1)
+				mem_types[i].prot_l1 |= PMD_BIT4;
+
+	cp = &cache_policies[cachepolicy];
+	kern_pgprot = user_pgprot = cp->pte;
+
+	/*
+	 * Enable CPU-specific coherency if supported.
+	 * (Only available on XSC3 at the moment.)
+	 */
+	if (arch_is_coherent()) {
+		if (cpu_is_xsc3()) {
+			mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
+			mem_types[MT_MEMORY].prot_pte |= L_PTE_COHERENT;
+		}
+	}
+
+	/*
+	 * ARMv6 and above have extended page tables.
+	 */
+	if (cpu_arch >= CPU_ARCH_ARMv6 && (cr & CR_XP)) {
+		/*
+		 * bit 4 becomes XN which we must clear for the
+		 * kernel memory mapping.
+		 */
+		mem_types[MT_MEMORY].prot_sect &= ~PMD_SECT_XN;
+		mem_types[MT_ROM].prot_sect &= ~PMD_SECT_XN;
+
+		/*
+		 * Mark cache clean areas and XIP ROM read only
+		 * from SVC mode and no access from userspace.
+		 */
+		mem_types[MT_ROM].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
+		mem_types[MT_MINICLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
+		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_APX|PMD_SECT_AP_WRITE;
+
+		/*
+		 * Mark the device area as "shared device"
+		 */
+		mem_types[MT_DEVICE].prot_pte |= L_PTE_BUFFERABLE;
+		mem_types[MT_DEVICE].prot_sect |= PMD_SECT_BUFFERED;
+
+		/*
+		 * User pages need to be mapped with the ASID
+		 * (iow, non-global)
+		 */
+		user_pgprot |= L_PTE_ASID;
+
+#ifdef CONFIG_SMP
+		/*
+		 * Mark memory with the "shared" attribute for SMP systems
+		 */
+		user_pgprot |= L_PTE_SHARED;
+		kern_pgprot |= L_PTE_SHARED;
+		mem_types[MT_MEMORY].prot_sect |= PMD_SECT_S;
+#endif
+	}
+
+	for (i = 0; i < 16; i++) {
+		unsigned long v = pgprot_val(protection_map[i]);
+		v = (v & ~(L_PTE_BUFFERABLE|L_PTE_CACHEABLE)) | user_pgprot;
+		protection_map[i] = __pgprot(v);
+	}
+
+	mem_types[MT_LOW_VECTORS].prot_pte |= kern_pgprot;
+	mem_types[MT_HIGH_VECTORS].prot_pte |= kern_pgprot;
+
+	if (cpu_arch >= CPU_ARCH_ARMv5) {
+#ifndef CONFIG_SMP
+		/*
+		 * Only use write-through for non-SMP systems
+		 */
+		mem_types[MT_LOW_VECTORS].prot_pte &= ~L_PTE_BUFFERABLE;
+		mem_types[MT_HIGH_VECTORS].prot_pte &= ~L_PTE_BUFFERABLE;
+#endif
+	} else {
+		mem_types[MT_MINICLEAN].prot_sect &= ~PMD_SECT_TEX(1);
+	}
+
+	pgprot_kernel = __pgprot(L_PTE_PRESENT | L_PTE_YOUNG |
+				 L_PTE_DIRTY | L_PTE_WRITE |
+				 L_PTE_EXEC | kern_pgprot);
+
+	mem_types[MT_LOW_VECTORS].prot_l1 |= ecc_mask;
+	mem_types[MT_HIGH_VECTORS].prot_l1 |= ecc_mask;
+	mem_types[MT_MEMORY].prot_sect |= ecc_mask | cp->pmd;
+	mem_types[MT_ROM].prot_sect |= cp->pmd;
+
+	switch (cp->pmd) {
+	case PMD_SECT_WT:
+		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_WT;
+		break;
+	case PMD_SECT_WB:
+	case PMD_SECT_WBWA:
+		mem_types[MT_CACHECLEAN].prot_sect |= PMD_SECT_WB;
+		break;
+	}
+	printk("Memory policy: ECC %sabled, Data cache %s\n",
+		ecc_mask ? "en" : "dis", cp->policy);
+}
+
+#define vectors_base()	(vectors_high() ? 0xffff0000 : 0)
+
+/*
+ * Create a SECTION PGD between VIRT and PHYS in domain
+ * DOMAIN with protection PROT.  This operates on half-
+ * pgdir entry increments.
+ */
+static inline void
+alloc_init_section(unsigned long virt, unsigned long phys, int prot)
+{
+	pmd_t *pmdp = pmd_off_k(virt);
+
+	if (virt & (1 << 20))
+		pmdp++;
+
+	*pmdp = __pmd(phys | prot);
+	flush_pmd_entry(pmdp);
+}
+
+/*
+ * Create a SUPER SECTION PGD between VIRT and PHYS with protection PROT
+ */
+static inline void
+alloc_init_supersection(unsigned long virt, unsigned long phys, int prot)
+{
+	int i;
+
+	for (i = 0; i < 16; i += 1) {
+		alloc_init_section(virt, phys, prot | PMD_SECT_SUPER);
+
+		virt += (PGDIR_SIZE / 2);
+	}
+}
+
+/*
+ * Add a PAGE mapping between VIRT and PHYS in domain
+ * DOMAIN with protection PROT.  Note that due to the
+ * way we map the PTEs, we must allocate two PTE_SIZE'd
+ * blocks - one for the Linux pte table, and one for
+ * the hardware pte table.
+ */
+static inline void
+alloc_init_page(unsigned long virt, unsigned long phys, unsigned int prot_l1, pgprot_t prot)
+{
+	pmd_t *pmdp = pmd_off_k(virt);
+	pte_t *ptep;
+
+	if (pmd_none(*pmdp)) {
+		ptep = alloc_bootmem_low_pages(2 * PTRS_PER_PTE *
+					       sizeof(pte_t));
+
+		__pmd_populate(pmdp, __pa(ptep) | prot_l1);
+	}
+	ptep = pte_offset_kernel(pmdp, virt);
+
+	set_pte(ptep, pfn_pte(phys >> PAGE_SHIFT, prot));
+}
+
+/*
+ * Create the page directory entries and any necessary
+ * page tables for the mapping specified by `md'.  We
+ * are able to cope here with varying sizes and address
+ * offsets, and we take full advantage of sections and
+ * supersections.
+ */
+void __init create_mapping(struct map_desc *md)
+{
+	unsigned long virt, length;
+	int prot_sect, prot_l1, domain;
+	pgprot_t prot_pte;
+	unsigned long off = (u32)__pfn_to_phys(md->pfn);
+
+	if (md->virtual != vectors_base() && md->virtual < TASK_SIZE) {
+		printk(KERN_WARNING "BUG: not creating mapping for "
+		       "0x%08llx at 0x%08lx in user region\n",
+		       __pfn_to_phys((u64)md->pfn), md->virtual);
+		return;
+	}
+
+	if ((md->type == MT_DEVICE || md->type == MT_ROM) &&
+	    md->virtual >= PAGE_OFFSET && md->virtual < VMALLOC_END) {
+		printk(KERN_WARNING "BUG: mapping for 0x%08llx at 0x%08lx "
+		       "overlaps vmalloc space\n",
+		       __pfn_to_phys((u64)md->pfn), md->virtual);
+	}
+
+	domain	  = mem_types[md->type].domain;
+	prot_pte  = __pgprot(mem_types[md->type].prot_pte);
+	prot_l1   = mem_types[md->type].prot_l1 | PMD_DOMAIN(domain);
+	prot_sect = mem_types[md->type].prot_sect | PMD_DOMAIN(domain);
+
+	/*
+	 * Catch 36-bit addresses
+	 */
+	if(md->pfn >= 0x100000) {
+		if(domain) {
+			printk(KERN_ERR "MM: invalid domain in supersection "
+				"mapping for 0x%08llx at 0x%08lx\n",
+				__pfn_to_phys((u64)md->pfn), md->virtual);
+			return;
+		}
+		if((md->virtual | md->length | __pfn_to_phys(md->pfn))
+			& ~SUPERSECTION_MASK) {
+			printk(KERN_ERR "MM: cannot create mapping for "
+				"0x%08llx at 0x%08lx invalid alignment\n",
+				__pfn_to_phys((u64)md->pfn), md->virtual);
+			return;
+		}
+
+		/*
+		 * Shift bits [35:32] of address into bits [23:20] of PMD
+		 * (See ARMv6 spec).
+		 */
+		off |= (((md->pfn >> (32 - PAGE_SHIFT)) & 0xF) << 20);
+	}
+
+	virt   = md->virtual;
+	off   -= virt;
+	length = md->length;
+
+	if (mem_types[md->type].prot_l1 == 0 &&
+	    (virt & 0xfffff || (virt + off) & 0xfffff || (virt + length) & 0xfffff)) {
+		printk(KERN_WARNING "BUG: map for 0x%08lx at 0x%08lx can not "
+		       "be mapped using pages, ignoring.\n",
+		       __pfn_to_phys(md->pfn), md->virtual);
+		return;
+	}
+
+	while ((virt & 0xfffff || (virt + off) & 0xfffff) && length >= PAGE_SIZE) {
+		alloc_init_page(virt, virt + off, prot_l1, prot_pte);
+
+		virt   += PAGE_SIZE;
+		length -= PAGE_SIZE;
+	}
+
+	/* N.B.	ARMv6 supersections are only defined to work with domain 0.
+	 *	Since domain assignments can in fact be arbitrary, the
+	 *	'domain == 0' check below is required to insure that ARMv6
+	 *	supersections are only allocated for domain 0 regardless
+	 *	of the actual domain assignments in use.
+	 */
+	if ((cpu_architecture() >= CPU_ARCH_ARMv6 || cpu_is_xsc3())
+		&& domain == 0) {
+		/*
+		 * Align to supersection boundary if !high pages.
+		 * High pages have already been checked for proper
+		 * alignment above and they will fail the SUPSERSECTION_MASK
+		 * check because of the way the address is encoded into
+		 * offset.
+		 */
+		if (md->pfn <= 0x100000) {
+			while ((virt & ~SUPERSECTION_MASK ||
+			        (virt + off) & ~SUPERSECTION_MASK) &&
+				length >= (PGDIR_SIZE / 2)) {
+				alloc_init_section(virt, virt + off, prot_sect);
+
+				virt   += (PGDIR_SIZE / 2);
+				length -= (PGDIR_SIZE / 2);
+			}
+		}
+
+		while (length >= SUPERSECTION_SIZE) {
+			alloc_init_supersection(virt, virt + off, prot_sect);
+
+			virt   += SUPERSECTION_SIZE;
+			length -= SUPERSECTION_SIZE;
+		}
+	}
+
+	/*
+	 * A section mapping covers half a "pgdir" entry.
+	 */
+	while (length >= (PGDIR_SIZE / 2)) {
+		alloc_init_section(virt, virt + off, prot_sect);
+
+		virt   += (PGDIR_SIZE / 2);
+		length -= (PGDIR_SIZE / 2);
+	}
+
+	while (length >= PAGE_SIZE) {
+		alloc_init_page(virt, virt + off, prot_l1, prot_pte);
+
+		virt   += PAGE_SIZE;
+		length -= PAGE_SIZE;
+	}
+}
+
+/*
+ * Create the architecture specific mappings
+ */
+void __init iotable_init(struct map_desc *io_desc, int nr)
+{
+	int i;
+
+	for (i = 0; i < nr; i++)
+		create_mapping(io_desc + i);
+}
+
 static inline void prepare_page_table(struct meminfo *mi)
 {
 	unsigned long addr;
@@ -227,3 +738,34 @@ void __init paging_init(struct meminfo *mi, struct machine_desc *mdesc)
 	empty_zero_page = virt_to_page(zero_page);
 	flush_dcache_page(empty_zero_page);
 }
+
+/*
+ * In order to soft-boot, we need to insert a 1:1 mapping in place of
+ * the user-mode pages.  This will then ensure that we have predictable
+ * results when turning the mmu off
+ */
+void setup_mm_for_reboot(char mode)
+{
+	unsigned long base_pmdval;
+	pgd_t *pgd;
+	int i;
+
+	if (current->mm && current->mm->pgd)
+		pgd = current->mm->pgd;
+	else
+		pgd = init_mm.pgd;
+
+	base_pmdval = PMD_SECT_AP_WRITE | PMD_SECT_AP_READ | PMD_TYPE_SECT;
+	if (cpu_architecture() <= CPU_ARCH_ARMv5TEJ && !cpu_is_xscale())
+		base_pmdval |= PMD_BIT4;
+
+	for (i = 0; i < FIRST_USER_PGD_NR + USER_PTRS_PER_PGD; i++, pgd++) {
+		unsigned long pmdval = (i << PGDIR_SHIFT) | base_pmdval;
+		pmd_t *pmd;
+
+		pmd = pmd_off(pgd, i << PGDIR_SHIFT);
+		pmd[0] = __pmd(pmdval);
+		pmd[1] = __pmd(pmdval + (1 << (PGDIR_SHIFT - 1)));
+		flush_pmd_entry(pmd);
+	}
+}

commit d111e8f9644aa585c1a7e198d74a4d2682ef1374
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Sep 27 15:27:33 2006 +0100

    [ARM] Split ARM MM initialisation for !mmu
    
    Move the MMU specific code from init.c into mmu.c, and add nommu
    fixups to nommu.c
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
new file mode 100644
index 000000000000..9648e6800ffe
--- /dev/null
+++ b/arch/arm/mm/mmu.c
@@ -0,0 +1,229 @@
+/*
+ *  linux/arch/arm/mm/mmu.c
+ *
+ *  Copyright (C) 1995-2005 Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/bootmem.h>
+#include <linux/mman.h>
+#include <linux/nodemask.h>
+
+#include <asm/mach-types.h>
+#include <asm/setup.h>
+#include <asm/sizes.h>
+#include <asm/tlb.h>
+
+#include <asm/mach/arch.h>
+#include <asm/mach/map.h>
+
+#include "mm.h"
+
+DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+
+extern void _stext, __data_start, _end;
+extern pgd_t swapper_pg_dir[PTRS_PER_PGD];
+
+/*
+ * empty_zero_page is a special page that is used for
+ * zero-initialized data and COW.
+ */
+struct page *empty_zero_page;
+
+/*
+ * The pmd table for the upper-most set of pages.
+ */
+pmd_t *top_pmd;
+
+static inline void prepare_page_table(struct meminfo *mi)
+{
+	unsigned long addr;
+
+	/*
+	 * Clear out all the mappings below the kernel image.
+	 */
+	for (addr = 0; addr < MODULE_START; addr += PGDIR_SIZE)
+		pmd_clear(pmd_off_k(addr));
+
+#ifdef CONFIG_XIP_KERNEL
+	/* The XIP kernel is mapped in the module area -- skip over it */
+	addr = ((unsigned long)&_etext + PGDIR_SIZE - 1) & PGDIR_MASK;
+#endif
+	for ( ; addr < PAGE_OFFSET; addr += PGDIR_SIZE)
+		pmd_clear(pmd_off_k(addr));
+
+	/*
+	 * Clear out all the kernel space mappings, except for the first
+	 * memory bank, up to the end of the vmalloc region.
+	 */
+	for (addr = __phys_to_virt(mi->bank[0].start + mi->bank[0].size);
+	     addr < VMALLOC_END; addr += PGDIR_SIZE)
+		pmd_clear(pmd_off_k(addr));
+}
+
+/*
+ * Reserve the various regions of node 0
+ */
+void __init reserve_node_zero(pg_data_t *pgdat)
+{
+	unsigned long res_size = 0;
+
+	/*
+	 * Register the kernel text and data with bootmem.
+	 * Note that this can only be in node 0.
+	 */
+#ifdef CONFIG_XIP_KERNEL
+	reserve_bootmem_node(pgdat, __pa(&__data_start), &_end - &__data_start);
+#else
+	reserve_bootmem_node(pgdat, __pa(&_stext), &_end - &_stext);
+#endif
+
+	/*
+	 * Reserve the page tables.  These are already in use,
+	 * and can only be in node 0.
+	 */
+	reserve_bootmem_node(pgdat, __pa(swapper_pg_dir),
+			     PTRS_PER_PGD * sizeof(pgd_t));
+
+	/*
+	 * Hmm... This should go elsewhere, but we really really need to
+	 * stop things allocating the low memory; ideally we need a better
+	 * implementation of GFP_DMA which does not assume that DMA-able
+	 * memory starts at zero.
+	 */
+	if (machine_is_integrator() || machine_is_cintegrator())
+		res_size = __pa(swapper_pg_dir) - PHYS_OFFSET;
+
+	/*
+	 * These should likewise go elsewhere.  They pre-reserve the
+	 * screen memory region at the start of main system memory.
+	 */
+	if (machine_is_edb7211())
+		res_size = 0x00020000;
+	if (machine_is_p720t())
+		res_size = 0x00014000;
+
+#ifdef CONFIG_SA1111
+	/*
+	 * Because of the SA1111 DMA bug, we want to preserve our
+	 * precious DMA-able memory...
+	 */
+	res_size = __pa(swapper_pg_dir) - PHYS_OFFSET;
+#endif
+	if (res_size)
+		reserve_bootmem_node(pgdat, PHYS_OFFSET, res_size);
+}
+
+/*
+ * Set up device the mappings.  Since we clear out the page tables for all
+ * mappings above VMALLOC_END, we will remove any debug device mappings.
+ * This means you have to be careful how you debug this function, or any
+ * called function.  This means you can't use any function or debugging
+ * method which may touch any device, otherwise the kernel _will_ crash.
+ */
+static void __init devicemaps_init(struct machine_desc *mdesc)
+{
+	struct map_desc map;
+	unsigned long addr;
+	void *vectors;
+
+	/*
+	 * Allocate the vector page early.
+	 */
+	vectors = alloc_bootmem_low_pages(PAGE_SIZE);
+	BUG_ON(!vectors);
+
+	for (addr = VMALLOC_END; addr; addr += PGDIR_SIZE)
+		pmd_clear(pmd_off_k(addr));
+
+	/*
+	 * Map the kernel if it is XIP.
+	 * It is always first in the modulearea.
+	 */
+#ifdef CONFIG_XIP_KERNEL
+	map.pfn = __phys_to_pfn(CONFIG_XIP_PHYS_ADDR & SECTION_MASK);
+	map.virtual = MODULE_START;
+	map.length = ((unsigned long)&_etext - map.virtual + ~SECTION_MASK) & SECTION_MASK;
+	map.type = MT_ROM;
+	create_mapping(&map);
+#endif
+
+	/*
+	 * Map the cache flushing regions.
+	 */
+#ifdef FLUSH_BASE
+	map.pfn = __phys_to_pfn(FLUSH_BASE_PHYS);
+	map.virtual = FLUSH_BASE;
+	map.length = SZ_1M;
+	map.type = MT_CACHECLEAN;
+	create_mapping(&map);
+#endif
+#ifdef FLUSH_BASE_MINICACHE
+	map.pfn = __phys_to_pfn(FLUSH_BASE_PHYS + SZ_1M);
+	map.virtual = FLUSH_BASE_MINICACHE;
+	map.length = SZ_1M;
+	map.type = MT_MINICLEAN;
+	create_mapping(&map);
+#endif
+
+	/*
+	 * Create a mapping for the machine vectors at the high-vectors
+	 * location (0xffff0000).  If we aren't using high-vectors, also
+	 * create a mapping at the low-vectors virtual address.
+	 */
+	map.pfn = __phys_to_pfn(virt_to_phys(vectors));
+	map.virtual = 0xffff0000;
+	map.length = PAGE_SIZE;
+	map.type = MT_HIGH_VECTORS;
+	create_mapping(&map);
+
+	if (!vectors_high()) {
+		map.virtual = 0;
+		map.type = MT_LOW_VECTORS;
+		create_mapping(&map);
+	}
+
+	/*
+	 * Ask the machine support to map in the statically mapped devices.
+	 */
+	if (mdesc->map_io)
+		mdesc->map_io();
+
+	/*
+	 * Finally flush the caches and tlb to ensure that we're in a
+	 * consistent state wrt the writebuffer.  This also ensures that
+	 * any write-allocated cache lines in the vector page are written
+	 * back.  After this point, we can start to touch devices again.
+	 */
+	local_flush_tlb_all();
+	flush_cache_all();
+}
+
+/*
+ * paging_init() sets up the page tables, initialises the zone memory
+ * maps, and sets up the zero page, bad page and bad page tables.
+ */
+void __init paging_init(struct meminfo *mi, struct machine_desc *mdesc)
+{
+	void *zero_page;
+
+	build_mem_type_table();
+	prepare_page_table(mi);
+	bootmem_init(mi);
+	devicemaps_init(mdesc);
+
+	top_pmd = pmd_off_k(0xffff0000);
+
+	/*
+	 * allocate the zero page.  Note that we count on this going ok.
+	 */
+	zero_page = alloc_bootmem_low_pages(PAGE_SIZE);
+	memzero(zero_page, PAGE_SIZE);
+	empty_zero_page = virt_to_page(zero_page);
+	flush_dcache_page(empty_zero_page);
+}

commit d84b47115a04d9f6b0da777e8aa8cd930d5b6b8b
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Aug 21 19:23:38 2006 +0100

    [ARM] Move mmu.c out of the way
    
    Rename mmu.c to context.c - it's the ARMv6 ASID context handling
    code rather than generic "mmu" handling code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
deleted file mode 100644
index 0d90227a0a32..000000000000
--- a/arch/arm/mm/mmu.c
+++ /dev/null
@@ -1,45 +0,0 @@
-/*
- *  linux/arch/arm/mm/mmu.c
- *
- *  Copyright (C) 2002-2003 Deep Blue Solutions Ltd, all rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- */
-#include <linux/init.h>
-#include <linux/sched.h>
-#include <linux/mm.h>
-
-#include <asm/mmu_context.h>
-#include <asm/tlbflush.h>
-
-unsigned int cpu_last_asid = { 1 << ASID_BITS };
-
-/*
- * We fork()ed a process, and we need a new context for the child
- * to run in.  We reserve version 0 for initial tasks so we will
- * always allocate an ASID.
- */
-void __init_new_context(struct task_struct *tsk, struct mm_struct *mm)
-{
-	mm->context.id = 0;
-}
-
-void __new_context(struct mm_struct *mm)
-{
-	unsigned int asid;
-
-	asid = ++cpu_last_asid;
-	if (asid == 0)
-		asid = cpu_last_asid = 1 << ASID_BITS;
-
-	/*
-	 * If we've used up all our ASIDs, we need
-	 * to start a new version and flush the TLB.
-	 */
-	if ((asid & ~ASID_MASK) == 0)
-		flush_tlb_all();
-
-	mm->context.id = asid;
-}

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
new file mode 100644
index 000000000000..0d90227a0a32
--- /dev/null
+++ b/arch/arm/mm/mmu.c
@@ -0,0 +1,45 @@
+/*
+ *  linux/arch/arm/mm/mmu.c
+ *
+ *  Copyright (C) 2002-2003 Deep Blue Solutions Ltd, all rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+
+#include <asm/mmu_context.h>
+#include <asm/tlbflush.h>
+
+unsigned int cpu_last_asid = { 1 << ASID_BITS };
+
+/*
+ * We fork()ed a process, and we need a new context for the child
+ * to run in.  We reserve version 0 for initial tasks so we will
+ * always allocate an ASID.
+ */
+void __init_new_context(struct task_struct *tsk, struct mm_struct *mm)
+{
+	mm->context.id = 0;
+}
+
+void __new_context(struct mm_struct *mm)
+{
+	unsigned int asid;
+
+	asid = ++cpu_last_asid;
+	if (asid == 0)
+		asid = cpu_last_asid = 1 << ASID_BITS;
+
+	/*
+	 * If we've used up all our ASIDs, we need
+	 * to start a new version and flush the TLB.
+	 */
+	if ((asid & ~ASID_MASK) == 0)
+		flush_tlb_all();
+
+	mm->context.id = asid;
+}
