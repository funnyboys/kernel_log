commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index a08158241ad1..6f0909dda2f9 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -1,12 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  linux/arch/arm/mm/copypage-xsc3.S
  *
  *  Copyright (C) 2004 Intel Corp.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
  * Adapted for 3rd gen XScale core, no more mini-dcache
  * Author: Matt Gilbert (matthew.m.gilbert@intel.com)
  */

commit bc2eca9a682881f9da3cc7e2d75b752e549a134d
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Nov 9 04:26:39 2018 +0100

    ARM: 8811/1: always list both ldrd/strd registers explicitly
    
    The ldrd and strd instructions work on a pair of consecutive registers.
    It is possible to specify either the first register in the pair, or both
    registers explicitly. Let's always do the later to make things clearer.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Suggested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 55cbc3a89d85..a08158241ad1 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -37,25 +37,25 @@ static void xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 1:	pld	[%1, #64]			\n\
 	pld	[%1, #96]			\n\
 						\n\
-2:	ldrd	r2, [%1], #8			\n\
-	ldrd	r4, [%1], #8			\n\
+2:	ldrd	r2, r3, [%1], #8		\n\
+	ldrd	r4, r5, [%1], #8		\n\
 	mcr	p15, 0, %0, c7, c6, 1		@ invalidate\n\
-	strd	r2, [%0], #8			\n\
-	ldrd	r2, [%1], #8			\n\
-	strd	r4, [%0], #8			\n\
-	ldrd	r4, [%1], #8			\n\
-	strd	r2, [%0], #8			\n\
-	strd	r4, [%0], #8			\n\
-	ldrd	r2, [%1], #8			\n\
-	ldrd	r4, [%1], #8			\n\
+	strd	r2, r3, [%0], #8		\n\
+	ldrd	r2, r3, [%1], #8		\n\
+	strd	r4, r5, [%0], #8		\n\
+	ldrd	r4, r5, [%1], #8		\n\
+	strd	r2, r3, [%0], #8		\n\
+	strd	r4, r5, [%0], #8		\n\
+	ldrd	r2, r3, [%1], #8		\n\
+	ldrd	r4, r5, [%1], #8		\n\
 	mcr	p15, 0, %0, c7, c6, 1		@ invalidate\n\
-	strd	r2, [%0], #8			\n\
-	ldrd	r2, [%1], #8			\n\
+	strd	r2, r3, [%0], #8		\n\
+	ldrd	r2, r3, [%1], #8		\n\
 	subs	%2, %2, #1			\n\
-	strd	r4, [%0], #8			\n\
-	ldrd	r4, [%1], #8			\n\
-	strd	r2, [%0], #8			\n\
-	strd	r4, [%0], #8			\n\
+	strd	r4, r5, [%0], #8		\n\
+	ldrd	r4, r5, [%1], #8		\n\
+	strd	r2, r3, [%0], #8		\n\
+	strd	r4, r5, [%0], #8		\n\
 	bgt	1b				\n\
 	beq	2b				"
 	: "+&r" (kto), "+&r" (kfrom), "=&r" (tmp)
@@ -87,10 +87,10 @@ void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 	mov	r2, #0				\n\
 	mov	r3, #0				\n\
 1:	mcr	p15, 0, %0, c7, c6, 1		@ invalidate line\n\
-	strd	r2, [%0], #8			\n\
-	strd	r2, [%0], #8			\n\
-	strd	r2, [%0], #8			\n\
-	strd	r2, [%0], #8			\n\
+	strd	r2, r3, [%0], #8		\n\
+	strd	r2, r3, [%0], #8		\n\
+	strd	r2, r3, [%0], #8		\n\
+	strd	r2, r3, [%0], #8		\n\
 	subs	r1, r1, #1			\n\
 	bne	1b"
 	: "=r" (ptr)

commit b99afae1390140f5b0039e6b37a7380de31ae874
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Nov 7 17:49:00 2018 +0100

    ARM: 8805/2: remove unneeded naked function usage
    
    The naked attribute is known to confuse some old gcc versions when
    function arguments aren't explicitly listed as inline assembly operands
    despite the gcc documentation. That resulted in commit 9a40ac86152c
    ("ARM: 6164/1: Add kto and kfrom to input operands list.").
    
    Yet that commit has problems of its own by having assembly operand
    constraints completely wrong. If the generated code has been OK since
    then, it is due to luck rather than correctness. So this patch also
    provides proper assembly operand constraints, and removes two instances
    of redundant register usages in the implementation while at it.
    
    Inspection of the generated code with this patch doesn't show any
    obvious quality degradation either, so not relying on __naked at all
    will make the code less fragile, and avoid some issues with clang.
    
    The only remaining __naked instances (excluding the kprobes test cases)
    are exynos_pm_power_up_setup(), tc2_pm_power_up_setup() and
    
    cci_enable_port_for_self(. But in the first two cases, only the function
    address is used by the compiler with no chance of inlining it by
    mistake, and the third case is called from assembly code only. And the
    fact that no stack is available when the corresponding code is executed
    does warrant the __naked usage in those cases.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Stefan Agner <stefan@agner.ch>
    Tested-by: Stefan Agner <stefan@agner.ch>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 03a2042aced5..55cbc3a89d85 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -21,53 +21,46 @@
 
 /*
  * XSC3 optimised copy_user_highpage
- *  r0 = destination
- *  r1 = source
  *
  * The source page may have some clean entries in the cache already, but we
  * can safely ignore them - break_cow() will flush them out of the cache
  * if we eventually end up using our copied page.
  *
  */
-static void __naked
-xsc3_mc_copy_user_page(void *kto, const void *kfrom)
+static void xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 {
-	asm("\
-	stmfd	sp!, {r4, r5, lr}		\n\
-	mov	lr, %2				\n\
-						\n\
-	pld	[r1, #0]			\n\
-	pld	[r1, #32]			\n\
-1:	pld	[r1, #64]			\n\
-	pld	[r1, #96]			\n\
+	int tmp;
+
+	asm volatile ("\
+	pld	[%1, #0]			\n\
+	pld	[%1, #32]			\n\
+1:	pld	[%1, #64]			\n\
+	pld	[%1, #96]			\n\
 						\n\
-2:	ldrd	r2, [r1], #8			\n\
-	mov	ip, r0				\n\
-	ldrd	r4, [r1], #8			\n\
-	mcr	p15, 0, ip, c7, c6, 1		@ invalidate\n\
-	strd	r2, [r0], #8			\n\
-	ldrd	r2, [r1], #8			\n\
-	strd	r4, [r0], #8			\n\
-	ldrd	r4, [r1], #8			\n\
-	strd	r2, [r0], #8			\n\
-	strd	r4, [r0], #8			\n\
-	ldrd	r2, [r1], #8			\n\
-	mov	ip, r0				\n\
-	ldrd	r4, [r1], #8			\n\
-	mcr	p15, 0, ip, c7, c6, 1		@ invalidate\n\
-	strd	r2, [r0], #8			\n\
-	ldrd	r2, [r1], #8			\n\
-	subs	lr, lr, #1			\n\
-	strd	r4, [r0], #8			\n\
-	ldrd	r4, [r1], #8			\n\
-	strd	r2, [r0], #8			\n\
-	strd	r4, [r0], #8			\n\
+2:	ldrd	r2, [%1], #8			\n\
+	ldrd	r4, [%1], #8			\n\
+	mcr	p15, 0, %0, c7, c6, 1		@ invalidate\n\
+	strd	r2, [%0], #8			\n\
+	ldrd	r2, [%1], #8			\n\
+	strd	r4, [%0], #8			\n\
+	ldrd	r4, [%1], #8			\n\
+	strd	r2, [%0], #8			\n\
+	strd	r4, [%0], #8			\n\
+	ldrd	r2, [%1], #8			\n\
+	ldrd	r4, [%1], #8			\n\
+	mcr	p15, 0, %0, c7, c6, 1		@ invalidate\n\
+	strd	r2, [%0], #8			\n\
+	ldrd	r2, [%1], #8			\n\
+	subs	%2, %2, #1			\n\
+	strd	r4, [%0], #8			\n\
+	ldrd	r4, [%1], #8			\n\
+	strd	r2, [%0], #8			\n\
+	strd	r4, [%0], #8			\n\
 	bgt	1b				\n\
-	beq	2b				\n\
-						\n\
-	ldmfd	sp!, {r4, r5, pc}"
-	:
-	: "r" (kto), "r" (kfrom), "I" (PAGE_SIZE / 64 - 1));
+	beq	2b				"
+	: "+&r" (kto), "+&r" (kfrom), "=&r" (tmp)
+	: "2" (PAGE_SIZE / 64 - 1)
+	: "r2", "r3", "r4", "r5");
 }
 
 void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
@@ -85,8 +78,6 @@ void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
 
 /*
  * XScale optimised clear_user_page
- *  r0 = destination
- *  r1 = virtual user address of ultimate destination page
  */
 void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 {

commit 5472e862de2bc4a47f18d216a4a626d5c7eeef90
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:15 2011 +0800

    arm: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index f9cde0702f1e..03a2042aced5 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -75,12 +75,12 @@ void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
 {
 	void *kto, *kfrom;
 
-	kto = kmap_atomic(to, KM_USER0);
-	kfrom = kmap_atomic(from, KM_USER1);
+	kto = kmap_atomic(to);
+	kfrom = kmap_atomic(from);
 	flush_cache_page(vma, vaddr, page_to_pfn(from));
 	xsc3_mc_copy_user_page(kto, kfrom);
-	kunmap_atomic(kfrom, KM_USER1);
-	kunmap_atomic(kto, KM_USER0);
+	kunmap_atomic(kfrom);
+	kunmap_atomic(kto);
 }
 
 /*
@@ -90,7 +90,7 @@ void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
  */
 void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 {
-	void *ptr, *kaddr = kmap_atomic(page, KM_USER0);
+	void *ptr, *kaddr = kmap_atomic(page);
 	asm volatile ("\
 	mov	r1, %2				\n\
 	mov	r2, #0				\n\
@@ -105,7 +105,7 @@ void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 	: "=r" (ptr)
 	: "0" (kaddr), "I" (PAGE_SIZE / 32)
 	: "r1", "r2", "r3");
-	kunmap_atomic(kaddr, KM_USER0);
+	kunmap_atomic(kaddr);
 }
 
 struct cpu_user_fns xsc3_mc_user_fns __initdata = {

commit 9a40ac86152c9cffd3dca482a15ddf9a8c5716b3
Author: Khem Raj <raj.khem@gmail.com>
Date:   Fri Jun 4 04:05:15 2010 +0100

    ARM: 6164/1: Add kto and kfrom to input operands list.
    
    When functions incoming parameters are not in input operands list gcc
    4.5 does not load the parameters into registers before calling this
    function but the inline assembly assumes valid addresses inside this
    function. This breaks the code because r0 and r1 are invalid when
    execution enters v4wb_copy_user_page ()
    
    Also the constant needs to be used as third input operand so account
    for that as well.
    
    Tested on qemu arm.
    
    CC: <stable@kernel.org>
    Signed-off-by: Khem Raj <raj.khem@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 747ad4140fc7..f9cde0702f1e 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -34,7 +34,7 @@ xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 {
 	asm("\
 	stmfd	sp!, {r4, r5, lr}		\n\
-	mov	lr, %0				\n\
+	mov	lr, %2				\n\
 						\n\
 	pld	[r1, #0]			\n\
 	pld	[r1, #32]			\n\
@@ -67,7 +67,7 @@ xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 						\n\
 	ldmfd	sp!, {r4, r5, pc}"
 	:
-	: "I" (PAGE_SIZE / 64 - 1));
+	: "r" (kto), "r" (kfrom), "I" (PAGE_SIZE / 64 - 1));
 }
 
 void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,

commit 2725898fc9bb2121ac0fb1b5e4faf4fc09014729
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 5 15:34:22 2009 +0100

    ARM: Flush user mapping on VIVT processors when copying a page
    
    Steven Walter <stevenrwalter@gmail.com> writes:
    > I've been tracking down an instance of userspace data corruption,
    > and I believe I have found a window during fork where data can be
    > lost.  The corruption is occurring on an ARMv5 system with VIVT
    > caches.  Here's the scenario in question.  Thread A is forking,
    > Thread B is running in userspace:
    >
    > Thread A: flush_cache_mm() (dup_mmap)
    > Thread B: writes to a page in the above mm
    > Thread A: pte_wrprotect() the above page (copy_one_pte)
    > Thread B: writes to the same page again
    >
    > During thread B's second write, he'll take a fault and enter the
    > do_wp_page() case.  We'll end up calling copy_page(), which notably
    > uses the kernel virtual addresses for the old and new pages.  This
    > means that the new page does not necessarily have the data from the
    > first write.  Now there are two conflicting copies of the same
    > cache-line in dcache.  If the userspace cache-line flushes before
    > the kernel cache-line, we lose the changes made during the first
    > write.  do_wp_page does call flush_dcache_page on the newly-copied
    > page, but there's still a window where the CPU could flush the
    > userspace cache-line before then.
    
    Resolve this by flushing the user mapping before copying the page
    on processors with a writeback VIVT cache.
    
    Note: this does have a performance impact, and so needs further
    consideration before being merged - can we optimize out some of
    the cache flushes if, eg, we know that the page isn't yet mapped?
    
    Thread: <e06498070903061426o5875ad13hc6328aa0d3f08ed7@mail.gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 18ae05d5829c..747ad4140fc7 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -77,6 +77,7 @@ void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
 
 	kto = kmap_atomic(to, KM_USER0);
 	kfrom = kmap_atomic(from, KM_USER1);
+	flush_cache_page(vma, vaddr, page_to_pfn(from));
 	xsc3_mc_copy_user_page(kto, kfrom);
 	kunmap_atomic(kfrom, KM_USER1);
 	kunmap_atomic(kto, KM_USER0);

commit f00a75c094c340c4e7435665816c3273c870e849
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 5 15:17:45 2009 +0100

    ARM: Pass VMA to copy_user_highpage() implementations
    
    Our copy_user_highpage() implementations may require cache maintainence.
    Ensure that implementations have all necessary details to perform this
    maintainence.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index bc4525f5ab23..18ae05d5829c 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -71,7 +71,7 @@ xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 }
 
 void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
-	unsigned long vaddr)
+	unsigned long vaddr, struct vm_area_struct *vma)
 {
 	void *kto, *kfrom;
 

commit 446c92b2901bedb3725d29b4e73def8aba623ffc
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu Mar 12 18:03:16 2009 +0100

    [ARM] 5421/1: ftrace: fix crash due to tracing of __naked functions
    
    This is a fix for the following crash observed in 2.6.29-rc3:
    http://lkml.org/lkml/2009/1/29/150
    
    On ARM it doesn't make sense to trace a naked function because then
    mcount is called without stack and frame pointer being set up and there
    is no chance to restore the lr register to the value before mcount was
    called.
    
    Reported-by: Matthias Kaehlcke <matthias@kaehlcke.net>
    Tested-by: Matthias Kaehlcke <matthias@kaehlcke.net>
    
    Cc: Abhishek Sagar <sagar.abhishek@gmail.com>
    Cc: Steven Rostedt <rostedt@home.goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 39a994542cad..bc4525f5ab23 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -29,7 +29,7 @@
  * if we eventually end up using our copied page.
  *
  */
-static void __attribute__((naked))
+static void __naked
 xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 {
 	asm("\

commit 43ae286b7d4d8c4983bc263ef2e3cccc10dedb2b
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Nov 4 02:42:27 2008 -0500

    [ARM] fix a couple clear_user_highpage assembly constraints
    
    In all cases the kaddr is assigned an input register even though it is
    modified in the assembly code.  Let's assign a new variable to the
    modified value and mark those inline asm with volatile otherwise they
    get optimized away because the output variable is otherwise not used.
    
    Also fix a few conversion errors in copypage-feroceon.c and
    copypage-v4mc.c.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 0e7cb325ca4c..39a994542cad 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -89,9 +89,9 @@ void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
  */
 void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 {
-	void *kaddr = kmap_atomic(page, KM_USER0);
-	asm("\
-	mov	r1, %1				\n\
+	void *ptr, *kaddr = kmap_atomic(page, KM_USER0);
+	asm volatile ("\
+	mov	r1, %2				\n\
 	mov	r2, #0				\n\
 	mov	r3, #0				\n\
 1:	mcr	p15, 0, %0, c7, c6, 1		@ invalidate line\n\
@@ -101,8 +101,8 @@ void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 	strd	r2, [%0], #8			\n\
 	subs	r1, r1, #1			\n\
 	bne	1b"
-	:
-	: "r" (kaddr), "I" (PAGE_SIZE / 32)
+	: "=r" (ptr)
+	: "0" (kaddr), "I" (PAGE_SIZE / 32)
 	: "r1", "r2", "r3");
 	kunmap_atomic(kaddr, KM_USER0);
 }

commit 303c6443659bc1dc911356f5de149f48ff1d97b8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 16:32:19 2008 +0000

    [ARM] clearpage: provide our own clear_user_highpage()
    
    For similar reasons as copy_user_page(), we want to avoid the
    additional kmap_atomic if it's unnecessary.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index caa697ccd8db..0e7cb325ca4c 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -87,26 +87,27 @@ void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
  *  r0 = destination
  *  r1 = virtual user address of ultimate destination page
  */
-void __attribute__((naked))
-xsc3_mc_clear_user_page(void *kaddr, unsigned long vaddr)
+void xsc3_mc_clear_user_highpage(struct page *page, unsigned long vaddr)
 {
+	void *kaddr = kmap_atomic(page, KM_USER0);
 	asm("\
-	mov	r1, %0				\n\
+	mov	r1, %1				\n\
 	mov	r2, #0				\n\
 	mov	r3, #0				\n\
-1:	mcr	p15, 0, r0, c7, c6, 1		@ invalidate line\n\
-	strd	r2, [r0], #8			\n\
-	strd	r2, [r0], #8			\n\
-	strd	r2, [r0], #8			\n\
-	strd	r2, [r0], #8			\n\
+1:	mcr	p15, 0, %0, c7, c6, 1		@ invalidate line\n\
+	strd	r2, [%0], #8			\n\
+	strd	r2, [%0], #8			\n\
+	strd	r2, [%0], #8			\n\
+	strd	r2, [%0], #8			\n\
 	subs	r1, r1, #1			\n\
-	bne	1b				\n\
-	mov	pc, lr"
+	bne	1b"
 	:
-	: "I" (PAGE_SIZE / 32));
+	: "r" (kaddr), "I" (PAGE_SIZE / 32)
+	: "r1", "r2", "r3");
+	kunmap_atomic(kaddr, KM_USER0);
 }
 
 struct cpu_user_fns xsc3_mc_user_fns __initdata = {
-	.cpu_clear_user_page	= xsc3_mc_clear_user_page,
+	.cpu_clear_user_highpage = xsc3_mc_clear_user_highpage,
 	.cpu_copy_user_highpage	= xsc3_mc_copy_user_highpage,
 };

commit 063b0a4207e43acbeff3d4b09f43e750e0212b48
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 15:08:35 2008 +0000

    [ARM] copypage: provide our own copy_user_highpage()
    
    We used to override the copy_user_page() function.  However, this
    is not only inefficient, it also causes additional complexity for
    highmem support, since we convert from a struct page to a kernel
    direct mapped address and back to a struct page again.
    
    Moreover, with highmem support, we end up pointlessly setting up
    kmap entries for pages which we're going to remap.  So, push the
    kmapping down into the copypage implementation files where it's
    required.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
index 51ed502e5777..caa697ccd8db 100644
--- a/arch/arm/mm/copypage-xsc3.c
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -11,8 +11,7 @@
  * Author: Matt Gilbert (matthew.m.gilbert@intel.com)
  */
 #include <linux/init.h>
-
-#include <asm/page.h>
+#include <linux/highmem.h>
 
 /*
  * General note:
@@ -21,18 +20,17 @@
  */
 
 /*
- * XSC3 optimised copy_user_page
+ * XSC3 optimised copy_user_highpage
  *  r0 = destination
  *  r1 = source
- *  r2 = virtual user address of ultimate destination page
  *
  * The source page may have some clean entries in the cache already, but we
  * can safely ignore them - break_cow() will flush them out of the cache
  * if we eventually end up using our copied page.
  *
  */
-void __attribute__((naked))
-xsc3_mc_copy_user_page(void *kto, const void *kfrom, unsigned long vaddr)
+static void __attribute__((naked))
+xsc3_mc_copy_user_page(void *kto, const void *kfrom)
 {
 	asm("\
 	stmfd	sp!, {r4, r5, lr}		\n\
@@ -72,6 +70,18 @@ xsc3_mc_copy_user_page(void *kto, const void *kfrom, unsigned long vaddr)
 	: "I" (PAGE_SIZE / 64 - 1));
 }
 
+void xsc3_mc_copy_user_highpage(struct page *to, struct page *from,
+	unsigned long vaddr)
+{
+	void *kto, *kfrom;
+
+	kto = kmap_atomic(to, KM_USER0);
+	kfrom = kmap_atomic(from, KM_USER1);
+	xsc3_mc_copy_user_page(kto, kfrom);
+	kunmap_atomic(kfrom, KM_USER1);
+	kunmap_atomic(kto, KM_USER0);
+}
+
 /*
  * XScale optimised clear_user_page
  *  r0 = destination
@@ -98,5 +108,5 @@ xsc3_mc_clear_user_page(void *kaddr, unsigned long vaddr)
 
 struct cpu_user_fns xsc3_mc_user_fns __initdata = {
 	.cpu_clear_user_page	= xsc3_mc_clear_user_page,
-	.cpu_copy_user_page	= xsc3_mc_copy_user_page,
+	.cpu_copy_user_highpage	= xsc3_mc_copy_user_highpage,
 };

commit d73e60b7144a86baf0fdfcc9537a70bb4f72e11c
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 13:08:02 2008 +0000

    [ARM] copypage: convert assembly files to C
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/mm/copypage-xsc3.c b/arch/arm/mm/copypage-xsc3.c
new file mode 100644
index 000000000000..51ed502e5777
--- /dev/null
+++ b/arch/arm/mm/copypage-xsc3.c
@@ -0,0 +1,102 @@
+/*
+ *  linux/arch/arm/mm/copypage-xsc3.S
+ *
+ *  Copyright (C) 2004 Intel Corp.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Adapted for 3rd gen XScale core, no more mini-dcache
+ * Author: Matt Gilbert (matthew.m.gilbert@intel.com)
+ */
+#include <linux/init.h>
+
+#include <asm/page.h>
+
+/*
+ * General note:
+ *  We don't really want write-allocate cache behaviour for these functions
+ *  since that will just eat through 8K of the cache.
+ */
+
+/*
+ * XSC3 optimised copy_user_page
+ *  r0 = destination
+ *  r1 = source
+ *  r2 = virtual user address of ultimate destination page
+ *
+ * The source page may have some clean entries in the cache already, but we
+ * can safely ignore them - break_cow() will flush them out of the cache
+ * if we eventually end up using our copied page.
+ *
+ */
+void __attribute__((naked))
+xsc3_mc_copy_user_page(void *kto, const void *kfrom, unsigned long vaddr)
+{
+	asm("\
+	stmfd	sp!, {r4, r5, lr}		\n\
+	mov	lr, %0				\n\
+						\n\
+	pld	[r1, #0]			\n\
+	pld	[r1, #32]			\n\
+1:	pld	[r1, #64]			\n\
+	pld	[r1, #96]			\n\
+						\n\
+2:	ldrd	r2, [r1], #8			\n\
+	mov	ip, r0				\n\
+	ldrd	r4, [r1], #8			\n\
+	mcr	p15, 0, ip, c7, c6, 1		@ invalidate\n\
+	strd	r2, [r0], #8			\n\
+	ldrd	r2, [r1], #8			\n\
+	strd	r4, [r0], #8			\n\
+	ldrd	r4, [r1], #8			\n\
+	strd	r2, [r0], #8			\n\
+	strd	r4, [r0], #8			\n\
+	ldrd	r2, [r1], #8			\n\
+	mov	ip, r0				\n\
+	ldrd	r4, [r1], #8			\n\
+	mcr	p15, 0, ip, c7, c6, 1		@ invalidate\n\
+	strd	r2, [r0], #8			\n\
+	ldrd	r2, [r1], #8			\n\
+	subs	lr, lr, #1			\n\
+	strd	r4, [r0], #8			\n\
+	ldrd	r4, [r1], #8			\n\
+	strd	r2, [r0], #8			\n\
+	strd	r4, [r0], #8			\n\
+	bgt	1b				\n\
+	beq	2b				\n\
+						\n\
+	ldmfd	sp!, {r4, r5, pc}"
+	:
+	: "I" (PAGE_SIZE / 64 - 1));
+}
+
+/*
+ * XScale optimised clear_user_page
+ *  r0 = destination
+ *  r1 = virtual user address of ultimate destination page
+ */
+void __attribute__((naked))
+xsc3_mc_clear_user_page(void *kaddr, unsigned long vaddr)
+{
+	asm("\
+	mov	r1, %0				\n\
+	mov	r2, #0				\n\
+	mov	r3, #0				\n\
+1:	mcr	p15, 0, r0, c7, c6, 1		@ invalidate line\n\
+	strd	r2, [r0], #8			\n\
+	strd	r2, [r0], #8			\n\
+	strd	r2, [r0], #8			\n\
+	strd	r2, [r0], #8			\n\
+	subs	r1, r1, #1			\n\
+	bne	1b				\n\
+	mov	pc, lr"
+	:
+	: "I" (PAGE_SIZE / 32));
+}
+
+struct cpu_user_fns xsc3_mc_user_fns __initdata = {
+	.cpu_clear_user_page	= xsc3_mc_clear_user_page,
+	.cpu_copy_user_page	= xsc3_mc_copy_user_page,
+};
