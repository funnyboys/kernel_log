commit 81b6b96475ac7a4ebfceae9f16fb3758327adbfe
Merge: a308a7102215 a7ba70f1787f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 28 11:16:43 2019 -0800

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux; tag 'dma-mapping-5.5' of git://git.infradead.org/users/hch/dma-mapping
    
    Pull dma-mapping updates from Christoph Hellwig:
    
     - improve dma-debug scalability (Eric Dumazet)
    
     - tiny dma-debug cleanup (Dan Carpenter)
    
     - check for vmap memory in dma_map_single (Kees Cook)
    
     - check for dma_addr_t overflows in dma-direct when using DMA offsets
       (Nicolas Saenz Julienne)
    
     - switch the x86 sta2x11 SOC to use more generic DMA code (Nicolas
       Saenz Julienne)
    
     - fix arm-nommu dma-ranges handling (Vladimir Murzin)
    
     - use __initdata in CMA (Shyam Saini)
    
     - replace the bus dma mask with a limit (Nicolas Saenz Julienne)
    
     - merge the remapping helpers into the main dma-direct flow (me)
    
     - switch xtensa to the generic dma remap handling (me)
    
     - various cleanups around dma_capable (me)
    
     - remove unused dev arguments to various dma-noncoherent helpers (me)
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux:
    
    * tag 'dma-mapping-5.5' of git://git.infradead.org/users/hch/dma-mapping: (22 commits)
      dma-mapping: treat dev->bus_dma_mask as a DMA limit
      dma-direct: exclude dma_direct_map_resource from the min_low_pfn check
      dma-direct: don't check swiotlb=force in dma_direct_map_resource
      dma-debug: clean up put_hash_bucket()
      powerpc: remove support for NULL dev in __phys_to_dma / __dma_to_phys
      dma-direct: avoid a forward declaration for phys_to_dma
      dma-direct: unify the dma_capable definitions
      dma-mapping: drop the dev argument to arch_sync_dma_for_*
      x86/PCI: sta2x11: use default DMA address translation
      dma-direct: check for overflows on 32 bit DMA addresses
      dma-debug: increase HASH_SIZE
      dma-debug: reorder struct dma_debug_entry fields
      xtensa: use the generic uncached segment support
      dma-mapping: merge the generic remapping helpers into dma-direct
      dma-direct: provide mmap and get_sgtable method overrides
      dma-direct: remove the dma_handle argument to __dma_direct_alloc_pages
      dma-direct: remove __dma_direct_free_pages
      usb: core: Remove redundant vmap checks
      kernel: dma-contiguous: mark CMA parameters __initdata/__initconst
      dma-debug: add a schedule point in debug_dma_dump_mappings()
      ...

commit 56e35f9c5b87ec1ae93e483284e189c84388de16
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Nov 7 18:03:11 2019 +0100

    dma-mapping: drop the dev argument to arch_sync_dma_for_*
    
    These are pure cache maintainance routines, so drop the unused
    struct device argument.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Suggested-by: Daniel Vetter <daniel.vetter@ffwll.ch>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 38fa917c8585..a6a2514e5fe8 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -70,20 +70,20 @@ static void dma_cache_maint(dma_addr_t handle, size_t size, u32 op)
  * pfn_valid returns true the pages is local and we can use the native
  * dma-direct functions, otherwise we call the Xen specific version.
  */
-void xen_dma_sync_for_cpu(struct device *dev, dma_addr_t handle,
-		phys_addr_t paddr, size_t size, enum dma_data_direction dir)
+void xen_dma_sync_for_cpu(dma_addr_t handle, phys_addr_t paddr, size_t size,
+		enum dma_data_direction dir)
 {
 	if (pfn_valid(PFN_DOWN(handle)))
-		arch_sync_dma_for_cpu(dev, paddr, size, dir);
+		arch_sync_dma_for_cpu(paddr, size, dir);
 	else if (dir != DMA_TO_DEVICE)
 		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 }
 
-void xen_dma_sync_for_device(struct device *dev, dma_addr_t handle,
-		phys_addr_t paddr, size_t size, enum dma_data_direction dir)
+void xen_dma_sync_for_device(dma_addr_t handle, phys_addr_t paddr, size_t size,
+		enum dma_data_direction dir)
 {
 	if (pfn_valid(PFN_DOWN(handle)))
-		arch_sync_dma_for_device(dev, paddr, size, dir);
+		arch_sync_dma_for_device(paddr, size, dir);
 	else if (dir == DMA_FROM_DEVICE)
 		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 	else

commit 88920ddebd2f4afe67eb39a9fe3661275e6e3e2d
Author: Ben Dooks (Codethink) <ben.dooks@codethink.co.uk>
Date:   Tue Oct 22 13:52:51 2019 +0100

    xen: mm: make xen_mm_init static
    
    The xen_mm_init is not exported or used outside of the file
    it is declared in, so make it static. This fixes the following
    sparse warning:
    
    arch/arm/xen/mm.c:136:12: warning: symbol 'xen_mm_init' was not declared. Should it be static?
    
    Signed-off-by: Ben Dooks (Codethink) <ben.dooks@codethink.co.uk>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 6feb6b78b13c..3c7645d7b9b4 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -134,7 +134,7 @@ void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 	return;
 }
 
-int __init xen_mm_init(void)
+static int __init xen_mm_init(void)
 {
 	struct gnttab_cache_flush cflush;
 	if (!xen_initial_domain())

commit e8d255e4703a820bab46f856460f318a60d42ace
Author: Ben Dooks (Codethink) <ben.dooks@codethink.co.uk>
Date:   Tue Oct 22 13:50:06 2019 +0100

    xen: mm: include <xen/xen-ops.h> for missing declarations
    
    Include <xen/xen-ops.h> for xen_{create,destroy}_contigous_region
    call declarations. Fixes the following sparse warnings:
    
    arch/arm/xen/mm.c:119:5: warning: symbol 'xen_create_contiguous_region' was not declared. Should it be static?
    arch/arm/xen/mm.c:131:6: warning: symbol 'xen_destroy_contiguous_region' was not declared. Should it be static?
    
    Signed-off-by: Ben Dooks (Codethink) <ben.dooks@codethink.co.uk>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 38fa917c8585..6feb6b78b13c 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -15,6 +15,7 @@
 #include <xen/interface/grant_table.h>
 #include <xen/interface/memory.h>
 #include <xen/page.h>
+#include <xen/xen-ops.h>
 #include <xen/swiotlb-xen.h>
 
 #include <asm/cacheflush.h>

commit 50dfd03d9579cde9150679e90f8f244c626b7a09
Merge: e524d16e7e32 a8fabb38525c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 4 11:13:09 2019 -0700

    Merge tag 'for-linus-5.4-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen fixes and cleanups from Juergen Gross:
    
     - a fix in the Xen balloon driver avoiding hitting a BUG_ON() in some
       cases, plus a follow-on cleanup series for that driver
    
     - a patch for introducing non-blocking EFI callbacks in Xen's EFI
       driver, plu a cleanup patch for Xen EFI handling merging the x86 and
       ARM arch specific initialization into the Xen EFI driver
    
     - a fix of the Xen xenbus driver avoiding a self-deadlock when cleaning
       up after a user process has died
    
     - a fix for Xen on ARM after removal of ZONE_DMA
    
     - a cleanup patch for avoiding build warnings for Xen on ARM
    
    * tag 'for-linus-5.4-rc2-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      xen/xenbus: fix self-deadlock after killing user process
      xen/efi: have a common runtime setup function
      arm: xen: mm: use __GPF_DMA32 for arm64
      xen/balloon: Clear PG_offline in balloon_retrieve()
      xen/balloon: Mark pages PG_offline in balloon_append()
      xen/balloon: Drop __balloon_append()
      xen/balloon: Set pages PageOffline() in balloon_add_region()
      ARM: xen: unexport HYPERVISOR_platform_op function
      xen/efi: Set nonblocking callbacks

commit ec066de1a567169c4737a5933218c329cedf52bd
Author: Peng Fan <peng.fan@nxp.com>
Date:   Mon Sep 16 09:51:33 2019 +0000

    arm: xen: mm: use __GPF_DMA32 for arm64
    
    arm64 shares some code under arch/arm/xen, including mm.c.
    However ZONE_DMA is removed by commit
    ad67f5a6545("arm64: replace ZONE_DMA with ZONE_DMA32").
    
    So add a check if CONFIG_ZONE_DMA32 is enabled use __GFP_DMA32.
    
    Signed-off-by: Peng Fan <peng.fan@nxp.com>
    Acked-by: Stefano Stabellini <sstabellini@kernel.org>
    Signed-off-by: Stefano Stabellini <sstabellini@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index d33b77e9add3..018093fef80a 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -28,7 +28,10 @@ unsigned long xen_get_swiotlb_free_pages(unsigned int order)
 
 	for_each_memblock(memory, reg) {
 		if (reg->base < (phys_addr_t)0xffffffff) {
-			flags |= __GFP_DMA;
+			if (IS_ENABLED(CONFIG_ZONE_DMA32))
+				flags |= __GFP_DMA32;
+			else
+				flags |= __GFP_DMA;
 			break;
 		}
 	}

commit b4dca1512941aa8fec33c28939abc2bba4a2c78c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Sep 5 10:04:30 2019 +0200

    swiotlb-xen: simplify cache maintainance
    
    Now that we know we always have the dma-noncoherent.h helpers available
    if we are on an architecture with support for non-coherent devices,
    we can just call them directly, and remove the calls to the dma-direct
    routines, including the fact that we call the dma_direct_map_page
    routines but ignore the value returned from it.  Instead we now have
    Xen wrappers for the arch_sync_dma_for_{device,cpu} helpers that call
    the special Xen versions of those routines for foreign pages.
    
    Note that the new helpers get the physical address passed in addition
    to the dma address to avoid another translation for the local cache
    maintainance.  The pfn_valid checks remain on the dma address as in
    the old code, even if that looks a little funny.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 9d73fa4a5991..2b2c208408bb 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -60,63 +60,33 @@ static void dma_cache_maint(dma_addr_t handle, size_t size, u32 op)
 	} while (size);
 }
 
-static void __xen_dma_page_dev_to_cpu(struct device *hwdev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir)
+/*
+ * Dom0 is mapped 1:1, and while the Linux page can span across multiple Xen
+ * pages, it is not possible for it to contain a mix of local and foreign Xen
+ * pages.  Calling pfn_valid on a foreign mfn will always return false, so if
+ * pfn_valid returns true the pages is local and we can use the native
+ * dma-direct functions, otherwise we call the Xen specific version.
+ */
+void xen_dma_sync_for_cpu(struct device *dev, dma_addr_t handle,
+		phys_addr_t paddr, size_t size, enum dma_data_direction dir)
 {
-	if (dir != DMA_TO_DEVICE)
+	if (pfn_valid(PFN_DOWN(handle)))
+		arch_sync_dma_for_cpu(dev, paddr, size, dir);
+	else if (dir != DMA_TO_DEVICE)
 		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 }
 
-static void __xen_dma_page_cpu_to_dev(struct device *hwdev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir)
+void xen_dma_sync_for_device(struct device *dev, dma_addr_t handle,
+		phys_addr_t paddr, size_t size, enum dma_data_direction dir)
 {
-	if (dir == DMA_FROM_DEVICE)
+	if (pfn_valid(PFN_DOWN(handle)))
+		arch_sync_dma_for_device(dev, paddr, size, dir);
+	else if (dir == DMA_FROM_DEVICE)
 		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 	else
 		dma_cache_maint(handle, size, GNTTAB_CACHE_CLEAN);
 }
 
-void __xen_dma_map_page(struct device *hwdev, struct page *page,
-	     dma_addr_t dev_addr, unsigned long offset, size_t size,
-	     enum dma_data_direction dir, unsigned long attrs)
-{
-	if (dev_is_dma_coherent(hwdev))
-		return;
-	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
-		return;
-
-	__xen_dma_page_cpu_to_dev(hwdev, dev_addr, size, dir);
-}
-
-void __xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir,
-		unsigned long attrs)
-
-{
-	if (dev_is_dma_coherent(hwdev))
-		return;
-	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
-		return;
-
-	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
-}
-
-void __xen_dma_sync_single_for_cpu(struct device *hwdev,
-		dma_addr_t handle, size_t size, enum dma_data_direction dir)
-{
-	if (dev_is_dma_coherent(hwdev))
-		return;
-	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
-}
-
-void __xen_dma_sync_single_for_device(struct device *hwdev,
-		dma_addr_t handle, size_t size, enum dma_data_direction dir)
-{
-	if (dev_is_dma_coherent(hwdev))
-		return;
-	__xen_dma_page_cpu_to_dev(hwdev, handle, size, dir);
-}
-
 bool xen_arch_need_swiotlb(struct device *dev,
 			   phys_addr_t phys,
 			   dma_addr_t dev_addr)

commit 7b7a5776ec510eab279d7ed7701f6a3c9da8b928
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 24 16:01:28 2019 +0200

    xen: remove the exports for xen_{create,destroy}_contiguous_region
    
    These routines are only used by swiotlb-xen, which cannot be modular.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 11d5ad26fcfe..9d73fa4a5991 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -154,13 +154,11 @@ int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
 	*dma_handle = pstart;
 	return 0;
 }
-EXPORT_SYMBOL_GPL(xen_create_contiguous_region);
 
 void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 {
 	return;
 }
-EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
 
 int __init xen_mm_init(void)
 {

commit 0e0d26e779d30086a7e86fcfae2d897bbc5579da
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 24 14:09:39 2019 +0200

    xen/arm: remove xen_dma_ops
    
    arm and arm64 can just use xen_swiotlb_dma_ops directly like x86, no
    need for a pointer indirection.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 2fde161733b0..11d5ad26fcfe 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -162,16 +162,12 @@ void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 }
 EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
 
-const struct dma_map_ops *xen_dma_ops;
-EXPORT_SYMBOL(xen_dma_ops);
-
 int __init xen_mm_init(void)
 {
 	struct gnttab_cache_flush cflush;
 	if (!xen_initial_domain())
 		return 0;
 	xen_swiotlb_init(1, false);
-	xen_dma_ops = &xen_swiotlb_dma_ops;
 
 	cflush.op = 0;
 	cflush.a.dev_bus_addr = 0;

commit efcd5729d97cd19d32911ffdf478bb86ae8e818f
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 24 15:26:08 2019 +0200

    xen/arm: simplify dma_cache_maint
    
    Calculate the required operation in the caller, and pass it directly
    instead of recalculating it for each page, and use simple arithmetics
    to get from the physical address to Xen page size aligned chunks.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 90574d89d0d4..2fde161733b0 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -35,64 +35,45 @@ unsigned long xen_get_swiotlb_free_pages(unsigned int order)
 	return __get_free_pages(flags, order);
 }
 
-enum dma_cache_op {
-       DMA_UNMAP,
-       DMA_MAP,
-};
 static bool hypercall_cflush = false;
 
-/* functions called by SWIOTLB */
-
-static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
-	size_t size, enum dma_data_direction dir, enum dma_cache_op op)
+/* buffers in highmem or foreign pages cannot cross page boundaries */
+static void dma_cache_maint(dma_addr_t handle, size_t size, u32 op)
 {
 	struct gnttab_cache_flush cflush;
-	unsigned long xen_pfn;
-	size_t left = size;
 
-	xen_pfn = (handle >> XEN_PAGE_SHIFT) + offset / XEN_PAGE_SIZE;
-	offset %= XEN_PAGE_SIZE;
+	cflush.a.dev_bus_addr = handle & XEN_PAGE_MASK;
+	cflush.offset = xen_offset_in_page(handle);
+	cflush.op = op;
 
 	do {
-		size_t len = left;
-	
-		/* buffers in highmem or foreign pages cannot cross page
-		 * boundaries */
-		if (len + offset > XEN_PAGE_SIZE)
-			len = XEN_PAGE_SIZE - offset;
-
-		cflush.op = 0;
-		cflush.a.dev_bus_addr = xen_pfn << XEN_PAGE_SHIFT;
-		cflush.offset = offset;
-		cflush.length = len;
-
-		if (op == DMA_UNMAP && dir != DMA_TO_DEVICE)
-			cflush.op = GNTTAB_CACHE_INVAL;
-		if (op == DMA_MAP) {
-			if (dir == DMA_FROM_DEVICE)
-				cflush.op = GNTTAB_CACHE_INVAL;
-			else
-				cflush.op = GNTTAB_CACHE_CLEAN;
-		}
-		if (cflush.op)
-			HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1);
+		if (size + cflush.offset > XEN_PAGE_SIZE)
+			cflush.length = XEN_PAGE_SIZE - cflush.offset;
+		else
+			cflush.length = size;
+
+		HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1);
 
-		offset = 0;
-		xen_pfn++;
-		left -= len;
-	} while (left);
+		cflush.offset = 0;
+		cflush.a.dev_bus_addr += cflush.length;
+		size -= cflush.length;
+	} while (size);
 }
 
 static void __xen_dma_page_dev_to_cpu(struct device *hwdev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	dma_cache_maint(handle & PAGE_MASK, handle & ~PAGE_MASK, size, dir, DMA_UNMAP);
+	if (dir != DMA_TO_DEVICE)
+		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
 }
 
 static void __xen_dma_page_cpu_to_dev(struct device *hwdev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	dma_cache_maint(handle & PAGE_MASK, handle & ~PAGE_MASK, size, dir, DMA_MAP);
+	if (dir == DMA_FROM_DEVICE)
+		dma_cache_maint(handle, size, GNTTAB_CACHE_INVAL);
+	else
+		dma_cache_maint(handle, size, GNTTAB_CACHE_CLEAN);
 }
 
 void __xen_dma_map_page(struct device *hwdev, struct page *page,

commit a8e6d950cdc1012aede8a84d85f902a53d895174
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 24 14:06:54 2019 +0200

    xen/arm: use dev_is_dma_coherent
    
    Use the dma-noncoherent dev_is_dma_coherent helper instead of the home
    grown variant.  Note that both are always initialized to the same
    value in arch_setup_dma_ops.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index d33b77e9add3..90574d89d0d4 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0-only
 #include <linux/cpu.h>
-#include <linux/dma-mapping.h>
+#include <linux/dma-noncoherent.h>
 #include <linux/gfp.h>
 #include <linux/highmem.h>
 #include <linux/export.h>
@@ -99,7 +99,7 @@ void __xen_dma_map_page(struct device *hwdev, struct page *page,
 	     dma_addr_t dev_addr, unsigned long offset, size_t size,
 	     enum dma_data_direction dir, unsigned long attrs)
 {
-	if (is_device_dma_coherent(hwdev))
+	if (dev_is_dma_coherent(hwdev))
 		return;
 	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 		return;
@@ -112,7 +112,7 @@ void __xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,
 		unsigned long attrs)
 
 {
-	if (is_device_dma_coherent(hwdev))
+	if (dev_is_dma_coherent(hwdev))
 		return;
 	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 		return;
@@ -123,7 +123,7 @@ void __xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,
 void __xen_dma_sync_single_for_cpu(struct device *hwdev,
 		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 {
-	if (is_device_dma_coherent(hwdev))
+	if (dev_is_dma_coherent(hwdev))
 		return;
 	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
 }
@@ -131,7 +131,7 @@ void __xen_dma_sync_single_for_cpu(struct device *hwdev,
 void __xen_dma_sync_single_for_device(struct device *hwdev,
 		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 {
-	if (is_device_dma_coherent(hwdev))
+	if (dev_is_dma_coherent(hwdev))
 		return;
 	__xen_dma_page_cpu_to_dev(hwdev, handle, size, dir);
 }
@@ -159,7 +159,7 @@ bool xen_arch_need_swiotlb(struct device *dev,
 	 * memory and we are not able to flush the cache.
 	 */
 	return (!hypercall_cflush && (xen_pfn != bfn) &&
-		!is_device_dma_coherent(dev));
+		!dev_is_dma_coherent(dev));
 }
 
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index e1d44b903dfc..d33b77e9add3 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 #include <linux/cpu.h>
 #include <linux/dma-mapping.h>
 #include <linux/gfp.h>

commit 8d2df8a2d0fa59d1214841202d8e9176057c42ae
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Mon Jan 7 23:07:19 2019 +0530

    arch/arm/xen: Remove duplicate header
    
    Remove duplicate header which is included twice.
    
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: Oleksandr Andrushchenko <oleksandr_andrushchenko@epam.com>
    Acked-by: Stefano Stabellini <sstabellini@kernel.org>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index cb44aa290e73..e1d44b903dfc 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -7,7 +7,6 @@
 #include <linux/of_address.h>
 #include <linux/slab.h>
 #include <linux/types.h>
-#include <linux/dma-mapping.h>
 #include <linux/vmalloc.h>
 #include <linux/swiotlb.h>
 

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 785d2a562a23..cb44aa290e73 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -1,6 +1,5 @@
 #include <linux/cpu.h>
 #include <linux/dma-mapping.h>
-#include <linux/bootmem.h>
 #include <linux/gfp.h>
 #include <linux/highmem.h>
 #include <linux/export.h>

commit dceb1a6819ab2c8b5564354543447b1af4fccedd
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun May 21 13:15:13 2017 +0200

    xen-swiotlb: consolidate xen_swiotlb_dma_ops
    
    ARM and x86 had duplicated versions of the dma_ops structure, the
    only difference is that x86 hasn't wired up the set_dma_mask,
    mmap, and get_sgtable ops yet.  On x86 all of them are identical
    to the generic version, so they aren't needed but harmless.
    
    All the symbols used only for xen_swiotlb_dma_ops can now be marked
    static as well.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index f0325d96b97a..785d2a562a23 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -185,23 +185,6 @@ EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
 const struct dma_map_ops *xen_dma_ops;
 EXPORT_SYMBOL(xen_dma_ops);
 
-static const struct dma_map_ops xen_swiotlb_dma_ops = {
-	.alloc = xen_swiotlb_alloc_coherent,
-	.free = xen_swiotlb_free_coherent,
-	.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu,
-	.sync_single_for_device = xen_swiotlb_sync_single_for_device,
-	.sync_sg_for_cpu = xen_swiotlb_sync_sg_for_cpu,
-	.sync_sg_for_device = xen_swiotlb_sync_sg_for_device,
-	.map_sg = xen_swiotlb_map_sg_attrs,
-	.unmap_sg = xen_swiotlb_unmap_sg_attrs,
-	.map_page = xen_swiotlb_map_page,
-	.unmap_page = xen_swiotlb_unmap_page,
-	.dma_supported = xen_swiotlb_dma_supported,
-	.set_dma_mask = xen_swiotlb_set_dma_mask,
-	.mmap = xen_swiotlb_dma_mmap,
-	.get_sgtable = xen_swiotlb_get_sgtable,
-};
-
 int __init xen_mm_init(void)
 {
 	struct gnttab_cache_flush cflush;

commit 8c2c8ed8b85d4012d26caf1c221c291c687d89f0
Merge: 304362a8bce1 69369f52d28a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 7 10:23:17 2017 -0800

    Merge branch 'stable/for-linus-4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/swiotlb
    
    Pull swiotlb updates from Konrad Rzeszutek Wilk:
     "Two tiny implementations of the DMA API for callback in ARM (for Xen)"
    
    * 'stable/for-linus-4.11' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/swiotlb:
      swiotlb-xen: implement xen_swiotlb_get_sgtable callback
      swiotlb-xen: implement xen_swiotlb_dma_mmap callback

commit 69369f52d28a34c84acb6f2a8a585e743441566a
Author: Andrii Anisov <andrii_anisov@epam.com>
Date:   Tue Feb 7 19:58:03 2017 +0200

    swiotlb-xen: implement xen_swiotlb_get_sgtable callback
    
    Signed-off-by: Andrii Anisov <andrii_anisov@epam.com>
    Signed-off-by: Stefano Stabellini <sstabellini@kernel.org>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index cd1684e4e864..76ea48a614e1 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -199,6 +199,7 @@ static struct dma_map_ops xen_swiotlb_dma_ops = {
 	.dma_supported = xen_swiotlb_dma_supported,
 	.set_dma_mask = xen_swiotlb_set_dma_mask,
 	.mmap = xen_swiotlb_dma_mmap,
+	.get_sgtable = xen_swiotlb_get_sgtable,
 };
 
 int __init xen_mm_init(void)

commit 7e91c7df29b5e196de3dc6f086c8937973bd0b88
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Tue Feb 7 19:58:02 2017 +0200

    swiotlb-xen: implement xen_swiotlb_dma_mmap callback
    
    This function creates userspace mapping for the DMA-coherent memory.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Oleksandr Dmytryshyn <oleksandr.dmytryshyn@globallogic.com>
    Signed-off-by: Andrii Anisov <andrii_anisov@epam.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index bd62d94f8ac5..cd1684e4e864 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -198,6 +198,7 @@ static struct dma_map_ops xen_swiotlb_dma_ops = {
 	.unmap_page = xen_swiotlb_unmap_page,
 	.dma_supported = xen_swiotlb_dma_supported,
 	.set_dma_mask = xen_swiotlb_set_dma_mask,
+	.mmap = xen_swiotlb_dma_mmap,
 };
 
 int __init xen_mm_init(void)

commit 5299709d0a87342dadc1fc9850484fadeb488bf8
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:01 2017 -0800

    treewide: Constify most dma_map_ops structures
    
    Most dma_map_ops structures are never modified. Constify these
    structures such that these can be write-protected. This patch
    has been generated as follows:
    
    git grep -l 'struct dma_map_ops' |
      xargs -d\\n sed -i \
        -e 's/struct dma_map_ops/const struct dma_map_ops/g' \
        -e 's/const struct dma_map_ops {/struct dma_map_ops {/g' \
        -e 's/^const struct dma_map_ops;$/struct dma_map_ops;/' \
        -e 's/const const struct dma_map_ops /const struct dma_map_ops /g';
    sed -i -e 's/const \(struct dma_map_ops intel_dma_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops intel_dma_ops');
    sed -i -e 's/const \(struct dma_map_ops dma_iommu_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops' | grep ^arch/powerpc);
    sed -i -e '/^struct vmd_dev {$/,/^};$/ s/const \(struct dma_map_ops[[:blank:]]dma_ops;\)/\1/' \
           -e '/^static void vmd_setup_dma_ops/,/^}$/ s/const \(struct dma_map_ops \*dest\)/\1/' \
           -e 's/const \(struct dma_map_ops \*dest = \&vmd->dma_ops\)/\1/' \
        drivers/pci/host/*.c
    sed -i -e '/^void __init pci_iommu_alloc(void)$/,/^}$/ s/dma_ops->/intel_dma_ops./' arch/ia64/kernel/pci-dma.c
    sed -i -e 's/static const struct dma_map_ops sn_dma_ops/static struct dma_map_ops sn_dma_ops/' arch/ia64/sn/pci/pci_dma.c
    sed -i -e 's/(const struct dma_map_ops \*)//' drivers/misc/mic/bus/vop_bus.c
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index bd62d94f8ac5..ce18c91b50a1 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -182,10 +182,10 @@ void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 }
 EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
 
-struct dma_map_ops *xen_dma_ops;
+const struct dma_map_ops *xen_dma_ops;
 EXPORT_SYMBOL(xen_dma_ops);
 
-static struct dma_map_ops xen_swiotlb_dma_ops = {
+static const struct dma_map_ops xen_swiotlb_dma_ops = {
 	.alloc = xen_swiotlb_alloc_coherent,
 	.free = xen_swiotlb_free_coherent,
 	.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu,

commit 7641842164c34b672ef3e70e881e8a72735305c1
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Wed Nov 2 07:12:47 2016 -0400

    swiotlb-xen: Enforce return of DMA_ERROR_CODE in mapping function
    
    The mapping function should always return DMA_ERROR_CODE when a mapping has
    failed as this is what the DMA API expects when a DMA error has occurred.
    The current function for mapping a page in Xen was returning either
    DMA_ERROR_CODE or 0 depending on where it failed.
    
    On x86 DMA_ERROR_CODE is 0, but on other architectures such as ARM it is
    ~0. We need to make sure we return the same error value if either the
    mapping failed or the device is not capable of accessing the mapping.
    
    If we are returning DMA_ERROR_CODE as our error value we can drop the
    function for checking the error code as the default is to compare the
    return value against DMA_ERROR_CODE if no function is defined.
    
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad@kernel.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index d062f08f5020..bd62d94f8ac5 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -186,7 +186,6 @@ struct dma_map_ops *xen_dma_ops;
 EXPORT_SYMBOL(xen_dma_ops);
 
 static struct dma_map_ops xen_swiotlb_dma_ops = {
-	.mapping_error = xen_swiotlb_dma_mapping_error,
 	.alloc = xen_swiotlb_alloc_coherent,
 	.free = xen_swiotlb_free_coherent,
 	.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu,

commit 00085f1efa387a8ce100e3734920f7639c80caa3
Author: Krzysztof Kozlowski <k.kozlowski@samsung.com>
Date:   Wed Aug 3 13:46:00 2016 -0700

    dma-mapping: use unsigned long for dma_attrs
    
    The dma-mapping core and the implementations do not change the DMA
    attributes passed by pointer.  Thus the pointer can point to const data.
    However the attributes do not have to be a bitfield.  Instead unsigned
    long will do fine:
    
    1. This is just simpler.  Both in terms of reading the code and setting
       attributes.  Instead of initializing local attributes on the stack
       and passing pointer to it to dma_set_attr(), just set the bits.
    
    2. It brings safeness and checking for const correctness because the
       attributes are passed by value.
    
    Semantic patches for this change (at least most of them):
    
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
    
        @@
        f(...,
        - struct dma_attrs *attrs
        + unsigned long attrs
        , ...)
        {
        ...
        }
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    and
    
        // Options: --all-includes
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
        type t;
    
        @@
        t f(..., struct dma_attrs *attrs);
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    Link: http://lkml.kernel.org/r/1468399300-5399-2-git-send-email-k.kozlowski@samsung.com
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no>
    Acked-by: Mark Salter <msalter@redhat.com> [c6x]
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com> [cris]
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch> [drm]
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Fabien Dessenne <fabien.dessenne@st.com> [bdisp]
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com> [vb2-core]
    Acked-by: David Vrabel <david.vrabel@citrix.com> [xen]
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen swiotlb]
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Richard Kuo <rkuo@codeaurora.org> [hexagon]
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> [m68k]
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com> [s390]
    Acked-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no> [avr32]
    Acked-by: Vineet Gupta <vgupta@synopsys.com> [arc]
    Acked-by: Robin Murphy <robin.murphy@arm.com> [arm64 and dma-iommu]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index c5f9a9e3d1f3..d062f08f5020 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -98,11 +98,11 @@ static void __xen_dma_page_cpu_to_dev(struct device *hwdev, dma_addr_t handle,
 
 void __xen_dma_map_page(struct device *hwdev, struct page *page,
 	     dma_addr_t dev_addr, unsigned long offset, size_t size,
-	     enum dma_data_direction dir, struct dma_attrs *attrs)
+	     enum dma_data_direction dir, unsigned long attrs)
 {
 	if (is_device_dma_coherent(hwdev))
 		return;
-	if (dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
+	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 		return;
 
 	__xen_dma_page_cpu_to_dev(hwdev, dev_addr, size, dir);
@@ -110,12 +110,12 @@ void __xen_dma_map_page(struct device *hwdev, struct page *page,
 
 void __xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir,
-		struct dma_attrs *attrs)
+		unsigned long attrs)
 
 {
 	if (is_device_dma_coherent(hwdev))
 		return;
-	if (dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
+	if (attrs & DMA_ATTR_SKIP_CPU_SYNC)
 		return;
 
 	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);

commit d0164adc89f6bb374d304ffcc375c6d2652fe67d
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Nov 6 16:28:21 2015 -0800

    mm, page_alloc: distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd
    
    __GFP_WAIT has been used to identify atomic context in callers that hold
    spinlocks or are in interrupts.  They are expected to be high priority and
    have access one of two watermarks lower than "min" which can be referred
    to as the "atomic reserve".  __GFP_HIGH users get access to the first
    lower watermark and can be called the "high priority reserve".
    
    Over time, callers had a requirement to not block when fallback options
    were available.  Some have abused __GFP_WAIT leading to a situation where
    an optimisitic allocation with a fallback option can access atomic
    reserves.
    
    This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
    cannot sleep and have no alternative.  High priority users continue to use
    __GFP_HIGH.  __GFP_DIRECT_RECLAIM identifies callers that can sleep and
    are willing to enter direct reclaim.  __GFP_KSWAPD_RECLAIM to identify
    callers that want to wake kswapd for background reclaim.  __GFP_WAIT is
    redefined as a caller that is willing to enter direct reclaim and wake
    kswapd for background reclaim.
    
    This patch then converts a number of sites
    
    o __GFP_ATOMIC is used by callers that are high priority and have memory
      pools for those requests. GFP_ATOMIC uses this flag.
    
    o Callers that have a limited mempool to guarantee forward progress clear
      __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM. bio allocations fall
      into this category where kswapd will still be woken but atomic reserves
      are not used as there is a one-entry mempool to guarantee progress.
    
    o Callers that are checking if they are non-blocking should use the
      helper gfpflags_allow_blocking() where possible. This is because
      checking for __GFP_WAIT as was done historically now can trigger false
      positives. Some exceptions like dm-crypt.c exist where the code intent
      is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
      flag manipulations.
    
    o Callers that built their own GFP flags instead of starting with GFP_KERNEL
      and friends now also need to specify __GFP_KSWAPD_RECLAIM.
    
    The first key hazard to watch out for is callers that removed __GFP_WAIT
    and was depending on access to atomic reserves for inconspicuous reasons.
    In some cases it may be appropriate for them to use __GFP_HIGH.
    
    The second key hazard is callers that assembled their own combination of
    GFP flags instead of starting with something like GFP_KERNEL.  They may
    now wish to specify __GFP_KSWAPD_RECLAIM.  It's almost certainly harmless
    if it's missed in most cases as other activity will wake kswapd.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Vitaly Wool <vitalywool@gmail.com>
    Cc: Rik van Riel <riel@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 7c34f7126b04..c5f9a9e3d1f3 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -25,7 +25,7 @@
 unsigned long xen_get_swiotlb_free_pages(unsigned int order)
 {
 	struct memblock_region *reg;
-	gfp_t flags = __GFP_NOWARN;
+	gfp_t flags = __GFP_NOWARN|__GFP_KSWAPD_RECLAIM;
 
 	for_each_memblock(memory, reg) {
 		if (reg->base < (phys_addr_t)0xffffffff) {

commit 9435cce87950d805e6c8315410f2cb8ff6b2c6a2
Author: Julien Grall <julien.grall@citrix.com>
Date:   Wed Sep 9 15:18:45 2015 +0100

    xen/swiotlb: Add support for 64KB page granularity
    
    Swiotlb is used on ARM64 to support DMA on platform where devices are
    not protected by an SMMU. Furthermore it's only enabled for DOM0.
    
    While Xen is always using 4KB page granularity in the stage-2 page table,
    Linux ARM64 may either use 4KB or 64KB. This means that a Linux page
    can be spanned accross multiple Xen page.
    
    The Swiotlb code has to validate that the buffer used for DMA is
    physically contiguous in the memory. As a Linux page can't be shared
    between local memory and foreign page by design (the balloon code always
    removing entirely a Linux page), the changes in the code are very
    minimal because we only need to check the first Xen PFN.
    
    Note that it may be possible to optimize the function
    check_page_physically_contiguous to avoid looping over every Xen PFN
    for local memory. Although I will let this optimization for a follow-up.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 7b517e913762..7c34f7126b04 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -48,22 +48,22 @@ static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
 	size_t size, enum dma_data_direction dir, enum dma_cache_op op)
 {
 	struct gnttab_cache_flush cflush;
-	unsigned long pfn;
+	unsigned long xen_pfn;
 	size_t left = size;
 
-	pfn = (handle >> PAGE_SHIFT) + offset / PAGE_SIZE;
-	offset %= PAGE_SIZE;
+	xen_pfn = (handle >> XEN_PAGE_SHIFT) + offset / XEN_PAGE_SIZE;
+	offset %= XEN_PAGE_SIZE;
 
 	do {
 		size_t len = left;
 	
 		/* buffers in highmem or foreign pages cannot cross page
 		 * boundaries */
-		if (len + offset > PAGE_SIZE)
-			len = PAGE_SIZE - offset;
+		if (len + offset > XEN_PAGE_SIZE)
+			len = XEN_PAGE_SIZE - offset;
 
 		cflush.op = 0;
-		cflush.a.dev_bus_addr = pfn << PAGE_SHIFT;
+		cflush.a.dev_bus_addr = xen_pfn << XEN_PAGE_SHIFT;
 		cflush.offset = offset;
 		cflush.length = len;
 
@@ -79,7 +79,7 @@ static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
 			HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1);
 
 		offset = 0;
-		pfn++;
+		xen_pfn++;
 		left -= len;
 	} while (left);
 }
@@ -141,10 +141,26 @@ bool xen_arch_need_swiotlb(struct device *dev,
 			   phys_addr_t phys,
 			   dma_addr_t dev_addr)
 {
-	unsigned long pfn = PFN_DOWN(phys);
-	unsigned long bfn = PFN_DOWN(dev_addr);
-
-	return (!hypercall_cflush && (pfn != bfn) && !is_device_dma_coherent(dev));
+	unsigned int xen_pfn = XEN_PFN_DOWN(phys);
+	unsigned int bfn = XEN_PFN_DOWN(dev_addr);
+
+	/*
+	 * The swiotlb buffer should be used if
+	 *	- Xen doesn't have the cache flush hypercall
+	 *	- The Linux page refers to foreign memory
+	 *	- The device doesn't support coherent DMA request
+	 *
+	 * The Linux page may be spanned acrros multiple Xen page, although
+	 * it's not possible to have a mix of local and foreign Xen page.
+	 * Furthermore, range_straddles_page_boundary is already checking
+	 * if buffer is physically contiguous in the host RAM.
+	 *
+	 * Therefore we only need to check the first Xen page to know if we
+	 * require a bounce buffer because the device doesn't support coherent
+	 * memory and we are not able to flush the cache.
+	 */
+	return (!hypercall_cflush && (xen_pfn != bfn) &&
+		!is_device_dma_coherent(dev));
 }
 
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,

commit 291be10fd7511101d44cf98166d049bd31bc7600
Author: Julien Grall <julien.grall@citrix.com>
Date:   Wed Sep 9 15:17:33 2015 +0100

    xen/swiotlb: Pass addresses rather than frame numbers to xen_arch_need_swiotlb
    
    With 64KB page granularity support, the frame number will be different.
    
    It will be easier to modify the behavior in a single place rather than
    in each caller.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 6dd911d1f0ac..7b517e913762 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -138,9 +138,12 @@ void __xen_dma_sync_single_for_device(struct device *hwdev,
 }
 
 bool xen_arch_need_swiotlb(struct device *dev,
-			   unsigned long pfn,
-			   unsigned long bfn)
+			   phys_addr_t phys,
+			   dma_addr_t dev_addr)
 {
+	unsigned long pfn = PFN_DOWN(phys);
+	unsigned long bfn = PFN_DOWN(dev_addr);
+
 	return (!hypercall_cflush && (pfn != bfn) && !is_device_dma_coherent(dev));
 }
 

commit 32e09870eedfb501a6cb5729d8c23f44f8a7cbdd
Author: Julien Grall <julien.grall@citrix.com>
Date:   Fri Aug 7 17:34:35 2015 +0100

    xen: Make clear that swiotlb and biomerge are dealing with DMA address
    
    The swiotlb is required when programming a DMA address on ARM when a
    device is not protected by an IOMMU.
    
    In this case, the DMA address should always be equal to the machine address.
    For DOM0 memory, Xen ensure it by have an identity mapping between the
    guest address and host address. However, when mapping a foreign grant
    reference, the 1:1 model doesn't work.
    
    For ARM guest, most of the callers of pfn_to_mfn expects to get a GFN
    (Guest Frame Number), i.e a PFN (Page Frame Number) from the Linux point
    of view given that all ARM guest are auto-translated.
    
    Even though the name pfn_to_mfn is misleading, we need to ensure that
    those caller get a GFN and not by mistake a MFN. In pratical, I haven't
    seen error related to this but we should fix it for the sake of
    correctness.
    
    In order to fix the implementation of pfn_to_mfn on ARM in a follow-up
    patch, we have to introduce new helpers to return the DMA from a PFN and
    the invert.
    
    On x86, the new helpers will be an alias of pfn_to_mfn and mfn_to_pfn.
    
    The helpers will be used in swiotlb and xen_biovec_phys_mergeable.
    
    This is necessary in the latter because we have to ensure that the
    biovec code will not try to merge a biovec using foreign page and
    another using Linux memory.
    
    Lastly, the helper mfn_to_local_pfn has been renamed to bfn_to_local_pfn
    given that the only usage was in swiotlb.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 03e75fef15b8..6dd911d1f0ac 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -139,9 +139,9 @@ void __xen_dma_sync_single_for_device(struct device *hwdev,
 
 bool xen_arch_need_swiotlb(struct device *dev,
 			   unsigned long pfn,
-			   unsigned long mfn)
+			   unsigned long bfn)
 {
-	return (!hypercall_cflush && (pfn != mfn) && !is_device_dma_coherent(dev));
+	return (!hypercall_cflush && (pfn != bfn) && !is_device_dma_coherent(dev));
 }
 
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,

commit a9fd60e2683fb80f5b26a7d686aebe3327a63e70
Author: Julien Grall <julien.grall@citrix.com>
Date:   Wed Jun 17 15:28:02 2015 +0100

    xen: Include xen/page.h rather than asm/xen/page.h
    
    Using xen/page.h will be necessary later for using common xen page
    helpers.
    
    As xen/page.h already include asm/xen/page.h, always use the later.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: netdev@vger.kernel.org
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 498325074a06..03e75fef15b8 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -15,10 +15,10 @@
 #include <xen/xen.h>
 #include <xen/interface/grant_table.h>
 #include <xen/interface/memory.h>
+#include <xen/page.h>
 #include <xen/swiotlb-xen.h>
 
 #include <asm/cacheflush.h>
-#include <asm/xen/page.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/interface.h>
 

commit 8746515d7f04c9ea94cf43e2db1fd2cfca93276d
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Apr 24 10:16:40 2015 +0100

    xen: Add __GFP_DMA flag when xen_swiotlb_init gets free pages on ARM
    
    Make sure that xen_swiotlb_init allocates buffers that are DMA capable
    when at least one memblock is available below 4G. Otherwise we assume
    that all devices on the SoC can cope with >4G addresses. We do this on
    ARM and ARM64, where dom0 is mapped 1:1, so pfn == mfn in this case.
    
    No functional changes on x86.
    
    From: Chen Baozi <baozich@gmail.com>
    
    Signed-off-by: Chen Baozi <baozich@gmail.com>
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Tested-by: Chen Baozi <baozich@gmail.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 793551d15f1d..498325074a06 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -4,6 +4,7 @@
 #include <linux/gfp.h>
 #include <linux/highmem.h>
 #include <linux/export.h>
+#include <linux/memblock.h>
 #include <linux/of_address.h>
 #include <linux/slab.h>
 #include <linux/types.h>
@@ -21,6 +22,20 @@
 #include <asm/xen/hypercall.h>
 #include <asm/xen/interface.h>
 
+unsigned long xen_get_swiotlb_free_pages(unsigned int order)
+{
+	struct memblock_region *reg;
+	gfp_t flags = __GFP_NOWARN;
+
+	for_each_memblock(memory, reg) {
+		if (reg->base < (phys_addr_t)0xffffffff) {
+			flags |= __GFP_DMA;
+			break;
+		}
+	}
+	return __get_free_pages(flags, order);
+}
+
 enum dma_cache_op {
        DMA_UNMAP,
        DMA_MAP,

commit 35c8ab4c5c31bbe9e90ee63ba1b199cb979f6520
Author: Chuck Tuffli <chuck.tuffli@emulex.com>
Date:   Sun Dec 21 12:30:58 2014 -0800

    arm64: Relax licensing of arm64 Xen DMA operations
    
    With Xen configured into the arm64 kernel, any driver allocating
    DMA'able memory for PCI operations, must be GPL compatible, regardless
    of its interaction with Xen. This patch relaxes the GPL requirement of
    xen_dma_ops and its dependencies to allow open source drivers to be
    compiled for the arm64 architecture.
    
    Signed-off-by: Chuck Tuffli <chuck.tuffli@emulex.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 351b24a979d4..793551d15f1d 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -149,7 +149,7 @@ void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
 
 struct dma_map_ops *xen_dma_ops;
-EXPORT_SYMBOL_GPL(xen_dma_ops);
+EXPORT_SYMBOL(xen_dma_ops);
 
 static struct dma_map_ops xen_swiotlb_dma_ops = {
 	.mapping_error = xen_swiotlb_dma_mapping_error,

commit da095a996090a412c3b6292c7a8680661dca157d
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Nov 21 11:08:39 2014 +0000

    xen/arm: introduce GNTTABOP_cache_flush
    
    Introduce support for new hypercall GNTTABOP_cache_flush.
    Use it to perform cache flashing on pages used for dma when necessary.
    
    If GNTTABOP_cache_flush is supported by the hypervisor, we don't need to
    bounce dma map operations that involve foreign grants and non-coherent
    devices.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 28ebf3ecee4e..351b24a979d4 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -12,6 +12,7 @@
 #include <linux/swiotlb.h>
 
 #include <xen/xen.h>
+#include <xen/interface/grant_table.h>
 #include <xen/interface/memory.h>
 #include <xen/swiotlb-xen.h>
 
@@ -24,12 +25,14 @@ enum dma_cache_op {
        DMA_UNMAP,
        DMA_MAP,
 };
+static bool hypercall_cflush = false;
 
 /* functions called by SWIOTLB */
 
 static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
 	size_t size, enum dma_data_direction dir, enum dma_cache_op op)
 {
+	struct gnttab_cache_flush cflush;
 	unsigned long pfn;
 	size_t left = size;
 
@@ -39,7 +42,26 @@ static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
 	do {
 		size_t len = left;
 	
-		/* TODO: cache flush */
+		/* buffers in highmem or foreign pages cannot cross page
+		 * boundaries */
+		if (len + offset > PAGE_SIZE)
+			len = PAGE_SIZE - offset;
+
+		cflush.op = 0;
+		cflush.a.dev_bus_addr = pfn << PAGE_SHIFT;
+		cflush.offset = offset;
+		cflush.length = len;
+
+		if (op == DMA_UNMAP && dir != DMA_TO_DEVICE)
+			cflush.op = GNTTAB_CACHE_INVAL;
+		if (op == DMA_MAP) {
+			if (dir == DMA_FROM_DEVICE)
+				cflush.op = GNTTAB_CACHE_INVAL;
+			else
+				cflush.op = GNTTAB_CACHE_CLEAN;
+		}
+		if (cflush.op)
+			HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1);
 
 		offset = 0;
 		pfn++;
@@ -104,7 +126,7 @@ bool xen_arch_need_swiotlb(struct device *dev,
 			   unsigned long pfn,
 			   unsigned long mfn)
 {
-	return ((pfn != mfn) && !is_device_dma_coherent(dev));
+	return (!hypercall_cflush && (pfn != mfn) && !is_device_dma_coherent(dev));
 }
 
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
@@ -147,10 +169,18 @@ static struct dma_map_ops xen_swiotlb_dma_ops = {
 
 int __init xen_mm_init(void)
 {
+	struct gnttab_cache_flush cflush;
 	if (!xen_initial_domain())
 		return 0;
 	xen_swiotlb_init(1, false);
 	xen_dma_ops = &xen_swiotlb_dma_ops;
+
+	cflush.op = 0;
+	cflush.a.dev_bus_addr = 0;
+	cflush.offset = 0;
+	cflush.length = 0;
+	if (HYPERVISOR_grant_table_op(GNTTABOP_cache_flush, &cflush, 1) != -ENOSYS)
+		hypercall_cflush = true;
 	return 0;
 }
 arch_initcall(xen_mm_init);

commit a4dba130891271084344c12537731542ec77cb85
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Nov 21 11:07:39 2014 +0000

    xen/arm/arm64: introduce xen_arch_need_swiotlb
    
    Introduce an arch specific function to find out whether a particular dma
    mapping operation needs to bounce on the swiotlb buffer.
    
    On ARM and ARM64, if the page involved is a foreign page and the device
    is not coherent, we need to bounce because at unmap time we cannot
    execute any required cache maintenance operations (we don't know how to
    find the pfn from the mfn).
    
    No change of behaviour for x86.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index ab700e1e5922..28ebf3ecee4e 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -100,6 +100,13 @@ void __xen_dma_sync_single_for_device(struct device *hwdev,
 	__xen_dma_page_cpu_to_dev(hwdev, handle, size, dir);
 }
 
+bool xen_arch_need_swiotlb(struct device *dev,
+			   unsigned long pfn,
+			   unsigned long mfn)
+{
+	return ((pfn != mfn) && !is_device_dma_coherent(dev));
+}
+
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
 				 unsigned int address_bits,
 				 dma_addr_t *dma_handle)

commit 5121872afe0e6470bc6b41637b258fa6e314be4f
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Nov 21 11:06:39 2014 +0000

    xen/arm/arm64: merge xen/mm32.c into xen/mm.c
    
    Merge xen/mm32.c into xen/mm.c.
    As a consequence the code gets compiled on arm64 too.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index b0e77de99148..ab700e1e5922 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -1,6 +1,10 @@
+#include <linux/cpu.h>
+#include <linux/dma-mapping.h>
 #include <linux/bootmem.h>
 #include <linux/gfp.h>
+#include <linux/highmem.h>
 #include <linux/export.h>
+#include <linux/of_address.h>
 #include <linux/slab.h>
 #include <linux/types.h>
 #include <linux/dma-mapping.h>
@@ -16,6 +20,86 @@
 #include <asm/xen/hypercall.h>
 #include <asm/xen/interface.h>
 
+enum dma_cache_op {
+       DMA_UNMAP,
+       DMA_MAP,
+};
+
+/* functions called by SWIOTLB */
+
+static void dma_cache_maint(dma_addr_t handle, unsigned long offset,
+	size_t size, enum dma_data_direction dir, enum dma_cache_op op)
+{
+	unsigned long pfn;
+	size_t left = size;
+
+	pfn = (handle >> PAGE_SHIFT) + offset / PAGE_SIZE;
+	offset %= PAGE_SIZE;
+
+	do {
+		size_t len = left;
+	
+		/* TODO: cache flush */
+
+		offset = 0;
+		pfn++;
+		left -= len;
+	} while (left);
+}
+
+static void __xen_dma_page_dev_to_cpu(struct device *hwdev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir)
+{
+	dma_cache_maint(handle & PAGE_MASK, handle & ~PAGE_MASK, size, dir, DMA_UNMAP);
+}
+
+static void __xen_dma_page_cpu_to_dev(struct device *hwdev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir)
+{
+	dma_cache_maint(handle & PAGE_MASK, handle & ~PAGE_MASK, size, dir, DMA_MAP);
+}
+
+void __xen_dma_map_page(struct device *hwdev, struct page *page,
+	     dma_addr_t dev_addr, unsigned long offset, size_t size,
+	     enum dma_data_direction dir, struct dma_attrs *attrs)
+{
+	if (is_device_dma_coherent(hwdev))
+		return;
+	if (dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
+		return;
+
+	__xen_dma_page_cpu_to_dev(hwdev, dev_addr, size, dir);
+}
+
+void __xen_dma_unmap_page(struct device *hwdev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir,
+		struct dma_attrs *attrs)
+
+{
+	if (is_device_dma_coherent(hwdev))
+		return;
+	if (dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
+		return;
+
+	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
+}
+
+void __xen_dma_sync_single_for_cpu(struct device *hwdev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	if (is_device_dma_coherent(hwdev))
+		return;
+	__xen_dma_page_dev_to_cpu(hwdev, handle, size, dir);
+}
+
+void __xen_dma_sync_single_for_device(struct device *hwdev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	if (is_device_dma_coherent(hwdev))
+		return;
+	__xen_dma_page_cpu_to_dev(hwdev, handle, size, dir);
+}
+
 int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
 				 unsigned int address_bits,
 				 dma_addr_t *dma_handle)

commit 1b65c4e5a9af1a1c61e792e2d0ed481e0c1f21a9
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Oct 10 13:41:10 2013 +0000

    swiotlb-xen: use xen_alloc/free_coherent_pages
    
    Use xen_alloc_coherent_pages and xen_free_coherent_pages to allocate or
    free coherent pages.
    
    We need to be careful handling the pointer returned by
    xen_alloc_coherent_pages, because on ARM the pointer is not equal to
    phys_to_virt(*dma_handle). In fact virt_to_phys only works for kernel
    direct mapped RAM memory.
    In ARM case the pointer could be an ioremap address, therefore passing
    it to virt_to_phys would give you another physical address that doesn't
    correspond to it.
    
    Make xen_create_contiguous_region take a phys_addr_t as start parameter to
    avoid the virt_to_phys calls which would be incorrect.
    
    Changes in v6:
    - remove extra spaces.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index 0d69b874d249..b0e77de99148 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -16,7 +16,7 @@
 #include <asm/xen/hypercall.h>
 #include <asm/xen/interface.h>
 
-int xen_create_contiguous_region(unsigned long vstart, unsigned int order,
+int xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,
 				 unsigned int address_bits,
 				 dma_addr_t *dma_handle)
 {
@@ -24,12 +24,12 @@ int xen_create_contiguous_region(unsigned long vstart, unsigned int order,
 		return -EINVAL;
 
 	/* we assume that dom0 is mapped 1:1 for now */
-	*dma_handle = virt_to_phys(pstart);
+	*dma_handle = pstart;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(xen_create_contiguous_region);
 
-void xen_destroy_contiguous_region(unsigned long vstart, unsigned int order)
+void xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)
 {
 	return;
 }

commit eb1ddc00b81cb8cb23891b6c7c5fbfaea29f3c73
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Wed Oct 9 16:56:33 2013 +0000

    swiotlb-xen: introduce xen_swiotlb_set_dma_mask
    
    Implement xen_swiotlb_set_dma_mask, use it for set_dma_mask on arm.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
index d56b8c6e4fbb..0d69b874d249 100644
--- a/arch/arm/xen/mm.c
+++ b/arch/arm/xen/mm.c
@@ -51,6 +51,7 @@ static struct dma_map_ops xen_swiotlb_dma_ops = {
 	.map_page = xen_swiotlb_map_page,
 	.unmap_page = xen_swiotlb_unmap_page,
 	.dma_supported = xen_swiotlb_dma_supported,
+	.set_dma_mask = xen_swiotlb_set_dma_mask,
 };
 
 int __init xen_mm_init(void)

commit 83862ccfc0a03212fde43b4ac29c28381828768b
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Oct 10 13:40:44 2013 +0000

    xen/arm,arm64: enable SWIOTLB_XEN
    
    Xen on arm and arm64 needs SWIOTLB_XEN: when running on Xen we need to
    program the hardware with mfns rather than pfns for dma addresses.
    Remove SWIOTLB_XEN dependency on X86 and PCI and make XEN select
    SWIOTLB_XEN on arm and arm64.
    
    At the moment always rely on swiotlb-xen, but when Xen starts supporting
    hardware IOMMUs we'll be able to avoid it conditionally on the presence
    of an IOMMU on the platform.
    
    Implement xen_create_contiguous_region on arm and arm64: for the moment
    we assume that dom0 has been mapped 1:1 (physical addresses == machine
    addresses) therefore we don't need to call XENMEM_exchange. Simply
    return the physical address as dma address.
    
    Initialize the xen-swiotlb from xen_early_init (before the native
    dma_ops are initialized), set xen_dma_ops to &xen_swiotlb_dma_ops.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    
    
    Changes in v8:
    - assume dom0 is mapped 1:1, no need to call XENMEM_exchange.
    
    Changes in v7:
    - call __set_phys_to_machine_multi from xen_create_contiguous_region and
    xen_destroy_contiguous_region to update the P2M;
    - don't call XENMEM_unpin, it has been removed;
    - call XENMEM_exchange instead of XENMEM_exchange_and_pin;
    - set nr_exchanged to 0 before calling the hypercall.
    
    Changes in v6:
    - introduce and export xen_dma_ops;
    - call xen_mm_init from as arch_initcall.
    
    Changes in v4:
    - remove redefinition of DMA_ERROR_CODE;
    - update the code to use XENMEM_exchange_and_pin and XENMEM_unpin;
    - add a note about hardware IOMMU in the commit message.
    
    Changes in v3:
    - code style changes;
    - warn on XENMEM_put_dma_buf failures.

diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c
new file mode 100644
index 000000000000..d56b8c6e4fbb
--- /dev/null
+++ b/arch/arm/xen/mm.c
@@ -0,0 +1,64 @@
+#include <linux/bootmem.h>
+#include <linux/gfp.h>
+#include <linux/export.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/dma-mapping.h>
+#include <linux/vmalloc.h>
+#include <linux/swiotlb.h>
+
+#include <xen/xen.h>
+#include <xen/interface/memory.h>
+#include <xen/swiotlb-xen.h>
+
+#include <asm/cacheflush.h>
+#include <asm/xen/page.h>
+#include <asm/xen/hypercall.h>
+#include <asm/xen/interface.h>
+
+int xen_create_contiguous_region(unsigned long vstart, unsigned int order,
+				 unsigned int address_bits,
+				 dma_addr_t *dma_handle)
+{
+	if (!xen_initial_domain())
+		return -EINVAL;
+
+	/* we assume that dom0 is mapped 1:1 for now */
+	*dma_handle = virt_to_phys(pstart);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xen_create_contiguous_region);
+
+void xen_destroy_contiguous_region(unsigned long vstart, unsigned int order)
+{
+	return;
+}
+EXPORT_SYMBOL_GPL(xen_destroy_contiguous_region);
+
+struct dma_map_ops *xen_dma_ops;
+EXPORT_SYMBOL_GPL(xen_dma_ops);
+
+static struct dma_map_ops xen_swiotlb_dma_ops = {
+	.mapping_error = xen_swiotlb_dma_mapping_error,
+	.alloc = xen_swiotlb_alloc_coherent,
+	.free = xen_swiotlb_free_coherent,
+	.sync_single_for_cpu = xen_swiotlb_sync_single_for_cpu,
+	.sync_single_for_device = xen_swiotlb_sync_single_for_device,
+	.sync_sg_for_cpu = xen_swiotlb_sync_sg_for_cpu,
+	.sync_sg_for_device = xen_swiotlb_sync_sg_for_device,
+	.map_sg = xen_swiotlb_map_sg_attrs,
+	.unmap_sg = xen_swiotlb_unmap_sg_attrs,
+	.map_page = xen_swiotlb_map_page,
+	.unmap_page = xen_swiotlb_unmap_page,
+	.dma_supported = xen_swiotlb_dma_supported,
+};
+
+int __init xen_mm_init(void)
+{
+	if (!xen_initial_domain())
+		return 0;
+	xen_swiotlb_init(1, false);
+	xen_dma_ops = &xen_swiotlb_dma_ops;
+	return 0;
+}
+arch_initcall(xen_mm_init);
