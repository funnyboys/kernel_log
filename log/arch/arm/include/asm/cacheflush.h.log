commit fca7f8e6fdeb716420a348acf6ca9bc7e609f5c0
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Jun 7 21:42:34 2020 -0700

    arm: rename flush_cache_user_range to flush_icache_user_range
    
    flush_icache_user_range will be the name for a generic primitive.  Move
    the arm name so that arm already has an implementation.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Link: http://lkml.kernel.org/r/20200515143646.3857579-24-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index c78e14fcfb5d..2e24e765e6d3 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -258,11 +258,11 @@ extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr
 #define flush_cache_dup_mm(mm) flush_cache_mm(mm)
 
 /*
- * flush_cache_user_range is used when we want to ensure that the
+ * flush_icache_user_range is used when we want to ensure that the
  * Harvard caches are synchronised for the user space address range.
  * This is used for the ARM private sys_cacheflush system call.
  */
-#define flush_cache_user_range(s,e)	__cpuc_coherent_user_range(s,e)
+#define flush_icache_user_range(s,e)	__cpuc_coherent_user_range(s,e)
 
 /*
  * Perform necessary cache operations to ensure that data previously

commit 97f52c1536f041fd9f0b62667864930c92ac93cf
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Jun 7 21:42:19 2020 -0700

    arm,sparc,unicore32: remove flush_icache_user_range
    
    flush_icache_user_range is only used by <asm-generic/cacheflush.h>, so
    remove it from the architectures that implement it, but don't use
    <asm-generic/cacheflush.h>.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Link: http://lkml.kernel.org/r/20200515143646.3857579-19-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 7114b9aa46b8..c78e14fcfb5d 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -318,9 +318,6 @@ extern void flush_kernel_dcache_page(struct page *);
 #define flush_dcache_mmap_lock(mapping)		xa_lock_irq(&mapping->i_pages)
 #define flush_dcache_mmap_unlock(mapping)	xa_unlock_irq(&mapping->i_pages)
 
-#define flush_icache_user_range(vma,page,addr,len) \
-	flush_dcache_page(page)
-
 /*
  * We don't appear to need to do anything here.  In fact, if we did, we'd
  * duplicate cache flushing elsewhere performed by flush_dcache_page().

commit 2b49350b16fa3171136d7cf351ac7e9e6673b8f2
Merge: 4d2fa8b44b89 5ccd3bd992cf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 21:08:34 2019 -0700

    Merge tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - Add a "cut here" to make it clearer where oops dumps should be cut
       from - we already have a marker for the end of the dumps.
    
     - Add logging severity to show_pte()
    
     - Drop unnecessary common-page-size linker flag
    
     - Errata workarounds for Cortex A12 857271, Cortex A17 857272 and
       Cortex A7 814220.
    
     - Remove some unused variables that had started to provoke a compiler
       warning.
    
    * tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm:
      ARM: 8863/1: stm32: select ARM errata 814220
      ARM: 8862/1: errata: 814220-B-Cache maintenance by set/way operations can execute out of order
      ARM: 8865/1: mm: remove unused variables
      ARM: 8864/1: Add workaround for I-Cache line size mismatch between CPU cores
      ARM: 8861/1: errata: Workaround errata A12 857271 / A17 857272
      ARM: 8860/1: VDSO: Drop implicit common-page-size linker flag
      ARM: arrange show_pte() to issue severity-based messages
      ARM: add "8<--- cut here ---" to kernel dumps

commit 5f41f9198f296091c6a58bc2e86af1e9f019b2a3
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue May 28 09:38:14 2019 +0100

    ARM: 8864/1: Add workaround for I-Cache line size mismatch between CPU cores
    
    Some big.LITTLE systems have I-Cache line size mismatch between
    LITTLE and big cores. This patch adds a workaround for proper I-Cache
    support on such systems. Without it, some class of the userspace code
    (typically self-modifying) might suffer from random SIGILL failures.
    
    Similar workaround already exists for ARM64 architecture. I has been
    added by commit 116c81f427ff ("arm64: Work around systems with mismatched
    cache line sizes").
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index ec1a5fd0d294..ec4fd2e2dd60 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -479,4 +479,11 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 void flush_uprobe_xol_access(struct page *page, unsigned long uaddr,
 			     void *kaddr, unsigned long len);
 
+
+#ifdef CONFIG_CPU_ICACHE_MISMATCH_WORKAROUND
+void check_cpu_icache_size(int cpuid);
+#else
+static inline void check_cpu_icache_size(int cpuid) { }
+#endif
+
 #endif

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index ec1a5fd0d294..d6667b8cfca5 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -1,11 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  *  arch/arm/include/asm/cacheflush.h
  *
  *  Copyright (C) 1999-2002 Russell King
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #ifndef _ASMARM_CACHEFLUSH_H
 #define _ASMARM_CACHEFLUSH_H

commit 5fb94e9ca333f0fe1d96de06704a79942b3832c3
Author: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
Date:   Tue May 8 15:14:57 2018 -0300

    docs: Fix some broken references
    
    As we move stuff around, some doc references are broken. Fix some of
    them via this script:
            ./scripts/documentation-file-ref-check --fix
    
    Manually checked if the produced result is valid, removing a few
    false-positives.
    
    Acked-by: Takashi Iwai <tiwai@suse.de>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Stephen Boyd <sboyd@kernel.org>
    Acked-by: Charles Keepax <ckeepax@opensource.wolfsonmicro.com>
    Acked-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Reviewed-by: Coly Li <colyli@suse.de>
    Signed-off-by: Mauro Carvalho Chehab <mchehab+samsung@kernel.org>
    Acked-by: Jonathan Corbet <corbet@lwn.net>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 869080bedb89..ec1a5fd0d294 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -35,7 +35,7 @@
  *	Start addresses are inclusive and end addresses are exclusive;
  *	start addresses should be rounded down, end addresses up.
  *
- *	See Documentation/cachetlb.txt for more information.
+ *	See Documentation/core-api/cachetlb.rst for more information.
  *	Please note that the implementation of these, and the required
  *	effects are cache-type (VIVT/VIPT/PIPT) specific.
  *

commit b93b016313b3ba8003c3b8bb71f569af91f19fc7
Author: Matthew Wilcox <mawilcox@microsoft.com>
Date:   Tue Apr 10 16:36:56 2018 -0700

    page cache: use xa_lock
    
    Remove the address_space ->tree_lock and use the xa_lock newly added to
    the radix_tree_root.  Rename the address_space ->page_tree to ->i_pages,
    since we don't really care that it's a tree.
    
    [willy@infradead.org: fix nds32, fs/dax.c]
      Link: http://lkml.kernel.org/r/20180406145415.GB20605@bombadil.infradead.orgLink: http://lkml.kernel.org/r/20180313132639.17387-9-willy@infradead.org
    Signed-off-by: Matthew Wilcox <mawilcox@microsoft.com>
    Acked-by: Jeff Layton <jlayton@redhat.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Ryusuke Konishi <konishi.ryusuke@lab.ntt.co.jp>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 74504b154256..869080bedb89 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -318,10 +318,8 @@ static inline void flush_anon_page(struct vm_area_struct *vma,
 #define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
 extern void flush_kernel_dcache_page(struct page *);
 
-#define flush_dcache_mmap_lock(mapping) \
-	spin_lock_irq(&(mapping)->tree_lock)
-#define flush_dcache_mmap_unlock(mapping) \
-	spin_unlock_irq(&(mapping)->tree_lock)
+#define flush_dcache_mmap_lock(mapping)		xa_lock_irq(&mapping->i_pages)
+#define flush_dcache_mmap_unlock(mapping)	xa_unlock_irq(&mapping->i_pages)
 
 #define flush_icache_user_range(vma,page,addr,len) \
 	flush_dcache_page(page)

commit 8acdf5055974e49d337d51ac7011449cfd7b7d05
Author: Kees Cook <keescook@chromium.org>
Date:   Fri Oct 28 00:45:16 2016 -0700

    randstruct: opt-out externally exposed function pointer structs
    
    Some function pointer structures are used externally to the kernel, like
    the paravirt structures. These should never be randomized, so mark them
    as such, in preparation for enabling randstruct's automatic selection
    of all-function-pointer structures.
    
    These markings are verbatim from Brad Spengler/PaX Team's code in the
    last public patch of grsecurity/PaX based on my understanding of the
    code. Changes or omissions from the original code are mine and don't
    reflect the original grsecurity/PaX code.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index d69bebf697e7..74504b154256 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -116,7 +116,7 @@ struct cpu_cache_fns {
 	void (*dma_unmap_area)(const void *, size_t, int);
 
 	void (*dma_flush_range)(const void *, const void *);
-};
+} __no_randomize_layout;
 
 /*
  * Select the calling method

commit e6ccbff0e90cf4bf012bf369dbdaf84c6faaedaa
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:58:47 2017 -0700

    treewide: decouple cacheflush.h and set_memory.h
    
    Now that all call sites, completely decouple cacheflush.h and
    set_memory.h
    
    [sfr@canb.auug.org.au: kprobes/x86: merge fix for set_memory.h decoupling]
      Link: http://lkml.kernel.org/r/20170418180903.10300fd3@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1488920133-27229-17-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 1cb9d118bb16..d69bebf697e7 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -16,7 +16,6 @@
 #include <asm/shmparam.h>
 #include <asm/cachetype.h>
 #include <asm/outercache.h>
-#include <asm/set_memory.h>
 
 #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
 

commit 299878bac326c890699c696ebba26f56fe93fc75
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon May 8 15:57:59 2017 -0700

    treewide: move set_memory_* functions away from cacheflush.h
    
    Patch series "set_memory_* functions header refactor", v3.
    
    The set_memory_* APIs came out of a desire to have a better way to
    change memory attributes.  Many of these attributes were linked to cache
    functionality so the prototypes were put in cacheflush.h.  These days,
    the APIs have grown and have a much wider use than just cache APIs.  To
    support this growth, split off set_memory_* and friends into a separate
    header file to avoid growing cacheflush.h for APIs that have nothing to
    do with caches.
    
    Link: http://lkml.kernel.org/r/1488920133-27229-2-git-send-email-labbott@redhat.com
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 02454fa15d2c..1cb9d118bb16 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -16,6 +16,7 @@
 #include <asm/shmparam.h>
 #include <asm/cachetype.h>
 #include <asm/outercache.h>
+#include <asm/set_memory.h>
 
 #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
 
@@ -478,26 +479,6 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 	: : : "r0","r1","r2","r3","r4","r5","r6","r7", \
 	      "r9","r10","lr","memory" )
 
-#ifdef CONFIG_MMU
-int set_memory_ro(unsigned long addr, int numpages);
-int set_memory_rw(unsigned long addr, int numpages);
-int set_memory_x(unsigned long addr, int numpages);
-int set_memory_nx(unsigned long addr, int numpages);
-#else
-static inline int set_memory_ro(unsigned long addr, int numpages) { return 0; }
-static inline int set_memory_rw(unsigned long addr, int numpages) { return 0; }
-static inline int set_memory_x(unsigned long addr, int numpages) { return 0; }
-static inline int set_memory_nx(unsigned long addr, int numpages) { return 0; }
-#endif
-
-#ifdef CONFIG_STRICT_KERNEL_RWX
-void set_kernel_text_rw(void);
-void set_kernel_text_ro(void);
-#else
-static inline void set_kernel_text_rw(void) { }
-static inline void set_kernel_text_ro(void) { }
-#endif
-
 void flush_uprobe_xol_access(struct page *page, unsigned long uaddr,
 			     void *kaddr, unsigned long len);
 

commit 0f5bf6d0afe4be6e1391908ff2d6dc9730e91550
Author: Laura Abbott <labbott@redhat.com>
Date:   Mon Feb 6 16:31:58 2017 -0800

    arch: Rename CONFIG_DEBUG_RODATA and CONFIG_DEBUG_MODULE_RONX
    
    Both of these options are poorly named. The features they provide are
    necessary for system security and should not be considered debug only.
    Change the names to CONFIG_STRICT_KERNEL_RWX and
    CONFIG_STRICT_MODULE_RWX to better describe what these options do.
    
    Signed-off-by: Laura Abbott <labbott@redhat.com>
    Acked-by: Jessica Yu <jeyu@redhat.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index bdd283bc5842..02454fa15d2c 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -490,7 +490,7 @@ static inline int set_memory_x(unsigned long addr, int numpages) { return 0; }
 static inline int set_memory_nx(unsigned long addr, int numpages) { return 0; }
 #endif
 
-#ifdef CONFIG_DEBUG_RODATA
+#ifdef CONFIG_STRICT_KERNEL_RWX
 void set_kernel_text_rw(void);
 void set_kernel_text_ro(void);
 #else

commit 5a0e0691147a7f841ddb54d067b165071dff592f
Author: Andy Gross <andy.gross@linaro.org>
Date:   Tue Aug 23 14:25:32 2016 +0100

    ARM: 8601/1: Remove unused secure_flush_area API
    
    This patch removes the unused secure_flush_area function.  The only
    consumer of this function has moved to using the streaming DMA APIs.
    
    Signed-off-by: Andy Gross <andy.gross@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 9156fc303afd..bdd283bc5842 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -501,21 +501,4 @@ static inline void set_kernel_text_ro(void) { }
 void flush_uprobe_xol_access(struct page *page, unsigned long uaddr,
 			     void *kaddr, unsigned long len);
 
-/**
- * secure_flush_area - ensure coherency across the secure boundary
- * @addr: virtual address
- * @size: size of region
- *
- * Ensure that the specified area of memory is coherent across the secure
- * boundary from the non-secure side.  This is used when calling secure
- * firmware where the secure firmware does not ensure coherency.
- */
-static inline void secure_flush_area(const void *addr, size_t size)
-{
-	phys_addr_t phys = __pa(addr);
-
-	__cpuc_flush_dcache_area((void *)addr, size);
-	outer_flush_range(phys, phys + size);
-}
-
 #endif

commit e267d97b83d9cecc16c54825f9f3ac7f72dc1e1e
Author: Kees Cook <keescook@chromium.org>
Date:   Wed Feb 17 14:41:12 2016 -0800

    asm-generic: Consolidate mark_rodata_ro()
    
    Instead of defining mark_rodata_ro() in each architecture, consolidate it.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Gross <agross@codeaurora.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Ashok Kumar <ashoks@broadcom.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Brown <david.brown@linaro.org>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Emese Revfy <re.emese@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mathias Krause <minipli@googlemail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: PaX Team <pageexec@freemail.hu>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: kernel-hardening@lists.openwall.com
    Cc: linux-arch <linux-arch@vger.kernel.org>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-parisc@vger.kernel.org
    Link: http://lkml.kernel.org/r/1455748879-21872-2-git-send-email-keescook@chromium.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index d5525bfc7e3e..9156fc303afd 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -491,7 +491,6 @@ static inline int set_memory_nx(unsigned long addr, int numpages) { return 0; }
 #endif
 
 #ifdef CONFIG_DEBUG_RODATA
-void mark_rodata_ro(void);
 void set_kernel_text_rw(void);
 void set_kernel_text_ro(void);
 #else

commit efaa6e266ba70439da00e7f1c8a218e243ae140a
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 24 10:21:02 2015 +0100

    firmware: qcom_scm-32: replace open-coded call to __cpuc_flush_dcache_area()
    
    Rathe rthan directly accessing architecture internal functions, provide
    an "method"-centric wrapper for qcom_scm-32 to do what's necessary to
    ensure that the secure monitor can see the data.  This is called
    "secure_flush_area" and ensures that the specified memory area is
    coherent across the secure boundary.
    
    Acked-by: Andy Gross <agross@codeaurora.org>
    Reviewed-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index c5230a44eeca..d5525bfc7e3e 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -502,4 +502,21 @@ static inline void set_kernel_text_ro(void) { }
 void flush_uprobe_xol_access(struct page *page, unsigned long uaddr,
 			     void *kaddr, unsigned long len);
 
+/**
+ * secure_flush_area - ensure coherency across the secure boundary
+ * @addr: virtual address
+ * @size: size of region
+ *
+ * Ensure that the specified area of memory is coherent across the secure
+ * boundary from the non-secure side.  This is used when calling secure
+ * firmware where the secure firmware does not ensure coherency.
+ */
+static inline void secure_flush_area(const void *addr, size_t size)
+{
+	phys_addr_t phys = __pa(addr);
+
+	__cpuc_flush_dcache_area((void *)addr, size);
+	outer_flush_range(phys, phys + size);
+}
+
 #endif

commit 1234e3fda9aa24b2d650bbcd9ef09d5f6a12dc86
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 24 09:10:55 2015 +0100

    ARM: reduce visibility of dmac_* functions
    
    The dmac_* functions are private to the ARM DMA API implementation, and
    should not be used by drivers.  In order to discourage their use, remove
    their prototypes and macros from asm/*.h.
    
    We have to leave dmac_flush_range() behind as Exynos and MSM IOMMU code
    use these; once these sites are fixed, this can be moved also.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 4812cda8fd17..c5230a44eeca 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -140,8 +140,6 @@ extern struct cpu_cache_fns cpu_cache;
  * is visible to DMA, or data written by DMA to system memory is
  * visible to the CPU.
  */
-#define dmac_map_area			cpu_cache.dma_map_area
-#define dmac_unmap_area			cpu_cache.dma_unmap_area
 #define dmac_flush_range		cpu_cache.dma_flush_range
 
 #else
@@ -161,8 +159,6 @@ extern void __cpuc_flush_dcache_area(void *, size_t);
  * is visible to DMA, or data written by DMA to system memory is
  * visible to the CPU.
  */
-extern void dmac_map_area(const void *, size_t, int);
-extern void dmac_unmap_area(const void *, size_t, int);
 extern void dmac_flush_range(const void *, const void *);
 
 #endif

commit 6e27549bbcdb35975d5c157a2170b81bcb1291db
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Tue May 26 16:08:46 2015 +0100

    ARM: 8380/1: bpf: fix NOMMU build
    
    arch/arm/net/built-in.o: In function `bpf_jit_compile':
    :(.text+0x2758): undefined reference to `set_memory_ro'
    arch/arm/net/built-in.o: In function `bpf_jit_free':
    :(.text+0x27ac): undefined reference to `set_memory_rw'
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 2d46862e7bef..4812cda8fd17 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -482,10 +482,17 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 	: : : "r0","r1","r2","r3","r4","r5","r6","r7", \
 	      "r9","r10","lr","memory" )
 
+#ifdef CONFIG_MMU
 int set_memory_ro(unsigned long addr, int numpages);
 int set_memory_rw(unsigned long addr, int numpages);
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
+#else
+static inline int set_memory_ro(unsigned long addr, int numpages) { return 0; }
+static inline int set_memory_rw(unsigned long addr, int numpages) { return 0; }
+static inline int set_memory_x(unsigned long addr, int numpages) { return 0; }
+static inline int set_memory_nx(unsigned long addr, int numpages) { return 0; }
+#endif
 
 #ifdef CONFIG_DEBUG_RODATA
 void mark_rodata_ro(void);

commit 80d6b0c2eed2a504f6740cd1f5ea76dc50abfc4d
Author: Kees Cook <keescook@chromium.org>
Date:   Thu Apr 3 13:29:50 2014 -0700

    ARM: mm: allow text and rodata sections to be read-only
    
    This introduces CONFIG_DEBUG_RODATA, making kernel text and rodata
    read-only. Additionally, this splits rodata from text so that rodata can
    also be NX, which may lead to wasted memory when aligning to SECTION_SIZE.
    The read-only areas are made writable during ftrace updates and kexec.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Tested-by: Laura Abbott <lauraa@codeaurora.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 10e78d00a0bb..2d46862e7bef 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -487,6 +487,16 @@ int set_memory_rw(unsigned long addr, int numpages);
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
 
+#ifdef CONFIG_DEBUG_RODATA
+void mark_rodata_ro(void);
+void set_kernel_text_rw(void);
+void set_kernel_text_ro(void);
+#else
+static inline void set_kernel_text_rw(void) { }
+static inline void set_kernel_text_ro(void) { }
+#endif
+
 void flush_uprobe_xol_access(struct page *page, unsigned long uaddr,
 			     void *kaddr, unsigned long len);
+
 #endif

commit ebc77251a4c50d7c310893915fb07581550c1ba5
Author: Krzysztof Kozlowski <k.kozlowski@samsung.com>
Date:   Sun Sep 28 05:36:46 2014 +0100

    ARM: 8177/1: cacheflush: Fix v7_exit_coherency_flush exynos build breakage on ARMv6
    
    This fixes build breakage of platsmp.c if ARMv6 was chosen for compile
    time options (e.g. by building allmodconfig):
    
    $ make allmodconfig
    $ make
      CC      arch/arm/mach-exynos/platsmp.o
    /tmp/ccdQM0Eg.s: Assembler messages:
    /tmp/ccdQM0Eg.s:432: Error: selected processor does not support ARM mode `isb '
    /tmp/ccdQM0Eg.s:437: Error: selected processor does not support ARM mode `isb '
    /tmp/ccdQM0Eg.s:438: Error: selected processor does not support ARM mode `dsb '
    make[1]: *** [arch/arm/mach-exynos/platsmp.o] Error 1
    
    The error was introduced in commit "ARM: EXYNOS: Move code from
    hotplug.c to platsmp.c".  Previously code using
    v7_exit_coherency_flush() macro was built with '-march=armv7-a' flag but
    this flag dissapeared during the movement.
    
    Fix this by annotating the v7_exit_coherency_flush() asm code with
    armv7-a architecture.
    
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Reported-by: Mark Brown <broonie@kernel.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Kukjin Kim <kgene.kim@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 79ecb4f34ffb..10e78d00a0bb 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -466,6 +466,7 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
  */
 #define v7_exit_coherency_flush(level) \
 	asm volatile( \
+	".arch	armv7-a \n\t" \
 	"stmfd	sp!, {fp, ip} \n\t" \
 	"mrc	p15, 0, r0, c1, c0, 0	@ get SCTLR \n\t" \
 	"bic	r0, r0, #"__stringify(CR_C)" \n\t" \

commit 2c32c65e3726c773760038910be30cce1b4d4149
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Fri Aug 15 12:11:50 2014 +0100

    ARM: 8129/1: errata: work around Cortex-A15 erratum 830321 using dummy strex
    
    On revisions of Cortex-A15 prior to r3p3, a CLREX instruction at PL1 may
    falsely trigger a watchpoint exception, leading to potential data aborts
    during exception return and/or livelock.
    
    This patch resolves the issue in the following ways:
    
      - Replacing our uses of CLREX with a dummy STREX sequence instead (as
        we did for v6 CPUs).
    
      - Removing the clrex code from v7_exit_coherency_flush and derivatives,
        since this only exists as a minor performance improvement when
        non-cached exclusives are in use (Linux doesn't use these).
    
    Benchmarking on a variety of ARM cores revealed no measurable
    performance difference with this change applied, so the change is
    performed unconditionally and no new Kconfig entry is added.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index fd43f7f55b70..79ecb4f34ffb 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -472,7 +472,6 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 	"mcr	p15, 0, r0, c1, c0, 0	@ set SCTLR \n\t" \
 	"isb	\n\t" \
 	"bl	v7_flush_dcache_"__stringify(level)" \n\t" \
-	"clrex	\n\t" \
 	"mrc	p15, 0, r0, c1, c0, 1	@ get ACTLR \n\t" \
 	"bic	r0, r0, #(1 << 6)	@ disable local coherency \n\t" \
 	"mcr	p15, 0, r0, c1, c0, 1	@ set ACTLR \n\t" \

commit 72e6ae285a1dbff553734985bedadf409d99c02d
Author: Victor Kamensky <victor.kamensky@linaro.org>
Date:   Tue Apr 29 04:20:52 2014 +0100

    ARM: 8043/1: uprobes need icache flush after xol write
    
    After instruction write into xol area, on ARM V7
    architecture code need to flush dcache and icache to sync
    them up for given set of addresses. Having just
    'flush_dcache_page(page)' call is not enough - it is
    possible to have stale instruction sitting in icache
    for given xol area slot address.
    
    Introduce arch_uprobe_ixol_copy weak function
    that by default calls uprobes copy_to_page function and
    than flush_dcache_page function and on ARM define new one
    that handles xol slot copy in ARM specific way
    
    flush_uprobe_xol_access function shares/reuses implementation
    with/of flush_ptrace_access function and takes care of writing
    instruction to user land address space on given variety of
    different cache types on ARM CPUs. Because
    flush_uprobe_xol_access does not have vma around
    flush_ptrace_access was split into two parts. First that
    retrieves set of condition from vma and common that receives
    those conditions as flags.
    
    Note ARM cache flush function need kernel address
    through which instruction write happened, so instead
    of using uprobes copy_to_page function changed
    code to explicitly map page and do memcpy.
    
    Note arch_uprobe_copy_ixol function, in similar way as
    copy_to_user_page function, has preempt_disable/preempt_enable.
    
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Reviewed-by: David A. Long <dave.long@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 00af9fe435e6..fd43f7f55b70 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -487,4 +487,6 @@ int set_memory_rw(unsigned long addr, int numpages);
 int set_memory_x(unsigned long addr, int numpages);
 int set_memory_nx(unsigned long addr, int numpages);
 
+void flush_uprobe_xol_access(struct page *page, unsigned long uaddr,
+			     void *kaddr, unsigned long len);
 #endif

commit 9581960a40ab0e281b695bf03744c8924ec3b5d0
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 9 18:36:27 2014 +0100

    ARM: 8055/1: cacheflush: use -st dsb option for ensuring completion
    
    dsb st can be used to ensure completion of pending cache maintenance
    operations, so use it for the v7 cache maintenance operations.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 8b8b61685a34..00af9fe435e6 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -212,7 +212,7 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 static inline void __flush_icache_all(void)
 {
 	__flush_icache_preferred();
-	dsb();
+	dsb(ishst);
 }
 
 /*

commit 39544ac9df20f73e49fc6b9ac19ff533388c82c0
Author: Vinayak Kale <vkale@apm.com>
Date:   Wed Feb 12 07:30:01 2014 +0100

    ARM: 7957/1: add DSB after icache flush in __flush_icache_all()
    
    Add DSB after icache flush to complete the cache maintenance operation.
    
    Signed-off-by: Vinayak Kale <vkale@apm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index e9a49fe0284e..8b8b61685a34 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -212,6 +212,7 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 static inline void __flush_icache_all(void)
 {
 	__flush_icache_preferred();
+	dsb();
 }
 
 /*

commit 75374ad47c6465de6c098d7b913fe02d6131195c
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Mon Jun 17 10:29:13 2013 -0700

    ARM: mm: Define set_memory_* functions for ARM
    
    Other architectures define various set_memory functions to allow
    attributes to be changed (e.g. set_memory_x, set_memory_rw, etc.)
    Currently, these functions are missing on ARM. Define these in an
    appropriate manner for ARM.
    
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index ee753f1749cd..e9a49fe0284e 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -481,4 +481,9 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 	: : : "r0","r1","r2","r3","r4","r5","r6","r7", \
 	      "r9","r10","lr","memory" )
 
+int set_memory_ro(unsigned long addr, int numpages);
+int set_memory_rw(unsigned long addr, int numpages);
+int set_memory_x(unsigned long addr, int numpages);
+int set_memory_nx(unsigned long addr, int numpages);
+
 #endif

commit 39792c7cf3111d69dc4aa0923859d8b929e9039f
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Oct 18 22:06:03 2013 +0100

    ARM: 7861/1: cacheflush: consolidate single-CPU ARMv7 cache disabling code
    
    This code is becoming duplicated in many places.  So let's consolidate
    it into a handy macro that is known to be right and available for reuse.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 15f2d5bf8875..ee753f1749cd 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -435,4 +435,50 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 #define sync_cache_w(ptr) __sync_cache_range_w(ptr, sizeof *(ptr))
 #define sync_cache_r(ptr) __sync_cache_range_r(ptr, sizeof *(ptr))
 
+/*
+ * Disabling cache access for one CPU in an ARMv7 SMP system is tricky.
+ * To do so we must:
+ *
+ * - Clear the SCTLR.C bit to prevent further cache allocations
+ * - Flush the desired level of cache
+ * - Clear the ACTLR "SMP" bit to disable local coherency
+ *
+ * ... and so without any intervening memory access in between those steps,
+ * not even to the stack.
+ *
+ * WARNING -- After this has been called:
+ *
+ * - No ldrex/strex (and similar) instructions must be used.
+ * - The CPU is obviously no longer coherent with the other CPUs.
+ * - This is unlikely to work as expected if Linux is running non-secure.
+ *
+ * Note:
+ *
+ * - This is known to apply to several ARMv7 processor implementations,
+ *   however some exceptions may exist.  Caveat emptor.
+ *
+ * - The clobber list is dictated by the call to v7_flush_dcache_*.
+ *   fp is preserved to the stack explicitly prior disabling the cache
+ *   since adding it to the clobber list is incompatible with having
+ *   CONFIG_FRAME_POINTER=y.  ip is saved as well if ever r12-clobbering
+ *   trampoline are inserted by the linker and to keep sp 64-bit aligned.
+ */
+#define v7_exit_coherency_flush(level) \
+	asm volatile( \
+	"stmfd	sp!, {fp, ip} \n\t" \
+	"mrc	p15, 0, r0, c1, c0, 0	@ get SCTLR \n\t" \
+	"bic	r0, r0, #"__stringify(CR_C)" \n\t" \
+	"mcr	p15, 0, r0, c1, c0, 0	@ set SCTLR \n\t" \
+	"isb	\n\t" \
+	"bl	v7_flush_dcache_"__stringify(level)" \n\t" \
+	"clrex	\n\t" \
+	"mrc	p15, 0, r0, c1, c0, 1	@ get ACTLR \n\t" \
+	"bic	r0, r0, #(1 << 6)	@ disable local coherency \n\t" \
+	"mcr	p15, 0, r0, c1, c0, 1	@ set ACTLR \n\t" \
+	"isb	\n\t" \
+	"dsb	\n\t" \
+	"ldmfd	sp!, {fp, ip}" \
+	: : : "r0","r1","r2","r3","r4","r5","r6","r7", \
+	      "r9","r10","lr","memory" )
+
 #endif

commit 5cc91e0460889c8461620904968e193dddb1beb3
Merge: cdf0bfb0126b 97c72d89ce0e
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Aug 28 18:38:40 2013 +0100

    Merge branch 'for-rmk/cacheflush-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/will/linux into devel-stable

commit d9524dc32cab52714dee0c8e59c7437ee33a239a
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Aug 21 15:33:19 2012 +0100

    ARM: cacheflush: don't round address range up to nearest page
    
    The flush_cache_user_range macro takes a pair of addresses describing
    the start and end of the virtual address range to flush. Due to an
    accidental oversight when flush_cache_range_user was introduced, the
    address range was rounded up so that the start and end addresses were
    page-aligned.
    
    For historical reference, the interesting commits in history.git are:
    
    10eacf1775e1 ("[ARM] Clean up ARM cache handling interfaces (part 1)")
    71432e79b76b ("[ARM] Add flush_cache_user_page() for sys_cacheflush()")
    
    This patch removes the alignment code, reducing the amount of flushing
    required for ranges that are not an exact multiple of PAGE_SIZE.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 17d0ae8672fa..bfd37e58f855 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -268,8 +268,7 @@ extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr
  * Harvard caches are synchronised for the user space address range.
  * This is used for the ARM private sys_cacheflush system call.
  */
-#define flush_cache_user_range(start,end) \
-	__cpuc_coherent_user_range((start) & PAGE_MASK, PAGE_ALIGN(end))
+#define flush_cache_user_range(s,e)	__cpuc_coherent_user_range(s,e)
 
 /*
  * Perform necessary cache operations to ensure that data previously

commit 6af396a6b6c698eb3834184518fc9a59bc22c817
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jun 12 10:03:30 2013 +0100

    ARM: cacheflush: use -ishst dsb variant for ensuring flush completion
    
    flush_cache_vmap contains a dsb to ensure that any cacheflushing
    operations to flush out newly written ptes have completed.
    
    This patch adds the -ishst option to the dsb, since that is all that is
    required for completing cacheflushing in the inner-shareable domain.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 17d0ae8672fa..04d73262e003 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -352,7 +352,7 @@ static inline void flush_cache_vmap(unsigned long start, unsigned long end)
 		 * set_pte_at() called from vmap_pte_range() does not
 		 * have a DSB after cleaning the cache line.
 		 */
-		dsb();
+		dsb(ishst);
 }
 
 static inline void flush_cache_vunmap(unsigned long start, unsigned long end)

commit 1bc39742aab09248169ef9d3727c9def3528b3f3
Author: Simon Baatz <gmbnomis@gmail.com>
Date:   Mon Jun 10 21:10:12 2013 +0100

    ARM: 7755/1: handle user space mapped pages in flush_kernel_dcache_page
    
    Commit f8b63c1 made flush_kernel_dcache_page a no-op assuming that
    the pages it needs to handle are kernel mapped only.  However, for
    example when doing direct I/O, pages with user space mappings may
    occur.
    
    Thus, continue to do lazy flushing if there are no user space
    mappings.  Otherwise, flush the kernel cache lines directly.
    
    Signed-off-by: Simon Baatz <gmbnomis@gmail.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: <stable@vger.kernel.org> # 3.2+
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index bff71388e72a..17d0ae8672fa 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -320,9 +320,7 @@ static inline void flush_anon_page(struct vm_area_struct *vma,
 }
 
 #define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
-static inline void flush_kernel_dcache_page(struct page *page)
-{
-}
+extern void flush_kernel_dcache_page(struct page *);
 
 #define flush_dcache_mmap_lock(mapping) \
 	spin_lock_irq(&(mapping)->tree_lock)

commit 0c91e7e07ebf08092bf8e28d8cd8d420732fc716
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Apr 23 16:45:40 2013 -0400

    ARM: cacheflush: add synchronization helpers for mixed cache state accesses
    
    Algorithms used by the MCPM layer rely on state variables which are
    accessed while the cache is either active or inactive, depending
    on the code path and the active state.
    
    This patch introduces generic cache maintenance helpers to provide the
    necessary cache synchronization for such state variables to always hit
    main memory in an ordered way.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Dave Martin <dave.martin@linaro.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index e1489c54cd12..bff71388e72a 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -363,4 +363,79 @@ static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
 		flush_cache_all();
 }
 
+/*
+ * Memory synchronization helpers for mixed cached vs non cached accesses.
+ *
+ * Some synchronization algorithms have to set states in memory with the
+ * cache enabled or disabled depending on the code path.  It is crucial
+ * to always ensure proper cache maintenance to update main memory right
+ * away in that case.
+ *
+ * Any cached write must be followed by a cache clean operation.
+ * Any cached read must be preceded by a cache invalidate operation.
+ * Yet, in the read case, a cache flush i.e. atomic clean+invalidate
+ * operation is needed to avoid discarding possible concurrent writes to the
+ * accessed memory.
+ *
+ * Also, in order to prevent a cached writer from interfering with an
+ * adjacent non-cached writer, each state variable must be located to
+ * a separate cache line.
+ */
+
+/*
+ * This needs to be >= the max cache writeback size of all
+ * supported platforms included in the current kernel configuration.
+ * This is used to align state variables to their own cache lines.
+ */
+#define __CACHE_WRITEBACK_ORDER 6  /* guessed from existing platforms */
+#define __CACHE_WRITEBACK_GRANULE (1 << __CACHE_WRITEBACK_ORDER)
+
+/*
+ * There is no __cpuc_clean_dcache_area but we use it anyway for
+ * code intent clarity, and alias it to __cpuc_flush_dcache_area.
+ */
+#define __cpuc_clean_dcache_area __cpuc_flush_dcache_area
+
+/*
+ * Ensure preceding writes to *p by this CPU are visible to
+ * subsequent reads by other CPUs:
+ */
+static inline void __sync_cache_range_w(volatile void *p, size_t size)
+{
+	char *_p = (char *)p;
+
+	__cpuc_clean_dcache_area(_p, size);
+	outer_clean_range(__pa(_p), __pa(_p + size));
+}
+
+/*
+ * Ensure preceding writes to *p by other CPUs are visible to
+ * subsequent reads by this CPU.  We must be careful not to
+ * discard data simultaneously written by another CPU, hence the
+ * usage of flush rather than invalidate operations.
+ */
+static inline void __sync_cache_range_r(volatile void *p, size_t size)
+{
+	char *_p = (char *)p;
+
+#ifdef CONFIG_OUTER_CACHE
+	if (outer_cache.flush_range) {
+		/*
+		 * Ensure dirty data migrated from other CPUs into our cache
+		 * are cleaned out safely before the outer cache is cleaned:
+		 */
+		__cpuc_clean_dcache_area(_p, size);
+
+		/* Clean and invalidate stale data for *p from outer ... */
+		outer_flush_range(__pa(_p), __pa(_p + size));
+	}
+#endif
+
+	/* ... and inner cache: */
+	__cpuc_flush_dcache_area(_p, size);
+}
+
+#define sync_cache_w(ptr) __sync_cache_range_w(ptr, sizeof *(ptr))
+#define sync_cache_r(ptr) __sync_cache_range_r(ptr, sizeof *(ptr))
+
 #endif

commit 031bd879f79d59d2f4fccd44377adf24fb977b5a
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu Sep 6 18:35:13 2012 +0530

    ARM: mm: implement LoUIS API for cache maintenance ops
    
    ARM v7 architecture introduced the concept of cache levels and related
    control registers. New processors like A7 and A15 embed an L2 unified cache
    controller that becomes part of the cache level hierarchy. Some operations in
    the kernel like cpu_suspend and __cpu_disable do not require a flush of the
    entire cache hierarchy to DRAM but just the cache levels belonging to the
    Level of Unification Inner Shareable (LoUIS), which in most of ARM v7 systems
    correspond to L1.
    
    The current cache flushing API used in cpu_suspend and __cpu_disable,
    flush_cache_all(), ends up flushing the whole cache hierarchy since for
    v7 it cleans and invalidates all cache levels up to Level of Coherency
    (LoC) which cripples system performance when used in hot paths like hotplug
    and cpuidle.
    
    Therefore a new kernel cache maintenance API must be added to cope with
    latest ARM system requirements.
    
    This patch adds flush_cache_louis() to the ARM kernel cache maintenance API.
    
    This function cleans and invalidates all data cache levels up to the
    Level of Unification Inner Shareable (LoUIS) and invalidates the instruction
    cache for processors that support it (> v7).
    
    This patch also creates an alias of the cache LoUIS function to flush_kern_all
    for all processor versions prior to v7, so that the current cache flushing
    behaviour is unchanged for those processors.
    
    v7 cache maintenance code implements a cache LoUIS function that cleans and
    invalidates the D-cache up to LoUIS and invalidates the I-cache, according
    to the new API.
    
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Tested-by: Shawn Guo <shawn.guo@linaro.org>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index e4448e16046d..e1489c54cd12 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -49,6 +49,13 @@
  *
  *		Unconditionally clean and invalidate the entire cache.
  *
+ *     flush_kern_louis()
+ *
+ *             Flush data cache levels up to the level of unification
+ *             inner shareable and invalidate the I-cache.
+ *             Only needed from v7 onwards, falls back to flush_cache_all()
+ *             for all other processor versions.
+ *
  *	flush_user_all()
  *
  *		Clean and invalidate all user space cache entries
@@ -97,6 +104,7 @@
 struct cpu_cache_fns {
 	void (*flush_icache_all)(void);
 	void (*flush_kern_all)(void);
+	void (*flush_kern_louis)(void);
 	void (*flush_user_all)(void);
 	void (*flush_user_range)(unsigned long, unsigned long, unsigned int);
 
@@ -119,6 +127,7 @@ extern struct cpu_cache_fns cpu_cache;
 
 #define __cpuc_flush_icache_all		cpu_cache.flush_icache_all
 #define __cpuc_flush_kern_all		cpu_cache.flush_kern_all
+#define __cpuc_flush_kern_louis		cpu_cache.flush_kern_louis
 #define __cpuc_flush_user_all		cpu_cache.flush_user_all
 #define __cpuc_flush_user_range		cpu_cache.flush_user_range
 #define __cpuc_coherent_kern_range	cpu_cache.coherent_kern_range
@@ -139,6 +148,7 @@ extern struct cpu_cache_fns cpu_cache;
 
 extern void __cpuc_flush_icache_all(void);
 extern void __cpuc_flush_kern_all(void);
+extern void __cpuc_flush_kern_louis(void);
 extern void __cpuc_flush_user_all(void);
 extern void __cpuc_flush_user_range(unsigned long, unsigned long, unsigned int);
 extern void __cpuc_coherent_kern_range(unsigned long, unsigned long);
@@ -204,6 +214,11 @@ static inline void __flush_icache_all(void)
 	__flush_icache_preferred();
 }
 
+/*
+ * Flush caches up to Level of Unification Inner Shareable
+ */
+#define flush_cache_louis()		__cpuc_flush_kern_louis()
+
 #define flush_cache_all()		__cpuc_flush_kern_all()
 
 static inline void vivt_flush_cache_mm(struct mm_struct *mm)

commit b74253f78400f9a4b42da84bb1de7540b88ce7c4
Author: Will Deacon <will.deacon@arm.com>
Date:   Mon Jul 23 14:18:13 2012 +0100

    ARM: 7479/1: mm: avoid NULL dereference when flushing gate_vma with VIVT caches
    
    The vivt_flush_cache_{range,page} functions check that the mm_struct
    of the VMA being flushed has been active on the current CPU before
    performing the cache maintenance.
    
    The gate_vma has a NULL mm_struct pointer and, as such, will cause a
    kernel fault if we try to flush it with the above operations. This
    happens during ELF core dumps, which include the gate_vma as it may be
    useful for debugging purposes.
    
    This patch adds checks to the VIVT cache flushing functions so that VMAs
    with a NULL mm_struct are flushed unconditionally (the vectors page may
    be dirty if we use it to store the current TLS pointer).
    
    Cc: <stable@vger.kernel.org> # 3.4+
    Reported-by: Gilles Chanteperdrix <gilles.chanteperdrix@xenomai.org>
    Tested-by: Uros Bizjak <ubizjak@gmail.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 004c1bc95d2b..e4448e16046d 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -215,7 +215,9 @@ static inline void vivt_flush_cache_mm(struct mm_struct *mm)
 static inline void
 vivt_flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
 {
-	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm)))
+	struct mm_struct *mm = vma->vm_mm;
+
+	if (!mm || cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm)))
 		__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
 					vma->vm_flags);
 }
@@ -223,7 +225,9 @@ vivt_flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned
 static inline void
 vivt_flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn)
 {
-	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm))) {
+	struct mm_struct *mm = vma->vm_mm;
+
+	if (!mm || cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm))) {
 		unsigned long addr = user_addr & PAGE_MASK;
 		__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
 	}

commit c5102f5935503ebebad46e137d0eef68f272cc16
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Apr 27 13:08:53 2012 +0100

    ARM: 7408/1: cacheflush: return error to userspace when flushing syscall fails
    
    The cacheflush syscall can fail for two reasons:
    
    (1) The arguments are invalid (nonsensical address range or no VMA)
    
    (2) The region generates a translation fault on a VIPT or PIPT cache
    
    This patch allows do_cache_op to return an error code to userspace in
    the case of the above. The various coherent_user_range implementations
    are modified to return 0 in the case of VIVT caches or -EFAULT in the
    case of an abort on v6/v7 cores.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 1252a2675ca9..004c1bc95d2b 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -101,7 +101,7 @@ struct cpu_cache_fns {
 	void (*flush_user_range)(unsigned long, unsigned long, unsigned int);
 
 	void (*coherent_kern_range)(unsigned long, unsigned long);
-	void (*coherent_user_range)(unsigned long, unsigned long);
+	int  (*coherent_user_range)(unsigned long, unsigned long);
 	void (*flush_kern_dcache_area)(void *, size_t);
 
 	void (*dma_map_area)(const void *, size_t, int);
@@ -142,7 +142,7 @@ extern void __cpuc_flush_kern_all(void);
 extern void __cpuc_flush_user_all(void);
 extern void __cpuc_flush_user_range(unsigned long, unsigned long, unsigned int);
 extern void __cpuc_coherent_kern_range(unsigned long, unsigned long);
-extern void __cpuc_coherent_user_range(unsigned long, unsigned long);
+extern int  __cpuc_coherent_user_range(unsigned long, unsigned long);
 extern void __cpuc_flush_dcache_area(void *, size_t);
 
 /*

commit 4542b6a0fa6b48d9ae6b41c1efeb618b7a221b2a
Author: Dima Zavin <dima@android.com>
Date:   Thu Mar 29 20:44:06 2012 +0100

    ARM: 7365/1: drop unused parameter from flush_cache_user_range
    
    vma isn't used and flush_cache_user_range isn't a standard macro that
    is used on several archs with the same prototype. In fact only unicore32
    has a macro with the same name (with an identical implementation and no
    in-tree users).
    
    This is a part of a patch proposed by Dima Zavin (with Message-id:
    1272439931-12795-1-git-send-email-dima@android.com) that didn't get
    accepted.
    
    Cc: Dima Zavin <dima@android.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index d5d8d5c72682..1252a2675ca9 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -249,7 +249,7 @@ extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr
  * Harvard caches are synchronised for the user space address range.
  * This is used for the ARM private sys_cacheflush system call.
  */
-#define flush_cache_user_range(vma,start,end) \
+#define flush_cache_user_range(start,end) \
 	__cpuc_coherent_user_range((start) & PAGE_MASK, PAGE_ALIGN(end))
 
 /*

commit bd1274dc005c2cee41771a7cc616f4709a6e6323
Merge: 1f0090a1eaa1 3ba6e69ad887
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Mar 16 23:35:26 2011 +0000

    Merge branch 'v6v7' into devel
    
    Conflicts:
            arch/arm/include/asm/cacheflush.h
            arch/arm/include/asm/proc-fns.h
            arch/arm/mm/Kconfig

commit 753790e713d80b50b867fa1ed32ec0eb5e82ae8e
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Feb 6 15:32:24 2011 +0000

    ARM: move cache/processor/fault glue to separate include files
    
    This allows the cache/processor/fault glue to be more easily used
    from assembler code.  Tested on Assabet and Tegra 2.
    
    Tested-by: Colin Cross <ccross@android.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 3acd8fa25e34..18a56640d97d 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -12,130 +12,13 @@
 
 #include <linux/mm.h>
 
-#include <asm/glue.h>
+#include <asm/glue-cache.h>
 #include <asm/shmparam.h>
 #include <asm/cachetype.h>
 #include <asm/outercache.h>
 
 #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
 
-/*
- *	Cache Model
- *	===========
- */
-#undef _CACHE
-#undef MULTI_CACHE
-
-#if defined(CONFIG_CPU_CACHE_V3)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE v3
-# endif
-#endif
-
-#if defined(CONFIG_CPU_CACHE_V4)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE v4
-# endif
-#endif
-
-#if defined(CONFIG_CPU_ARM920T) || defined(CONFIG_CPU_ARM922T) || \
-    defined(CONFIG_CPU_ARM925T) || defined(CONFIG_CPU_ARM1020) || \
-    defined(CONFIG_CPU_ARM1026)
-# define MULTI_CACHE 1
-#endif
-
-#if defined(CONFIG_CPU_FA526)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE fa
-# endif
-#endif
-
-#if defined(CONFIG_CPU_ARM926T)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE arm926
-# endif
-#endif
-
-#if defined(CONFIG_CPU_ARM940T)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE arm940
-# endif
-#endif
-
-#if defined(CONFIG_CPU_ARM946E)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE arm946
-# endif
-#endif
-
-#if defined(CONFIG_CPU_CACHE_V4WB)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE v4wb
-# endif
-#endif
-
-#if defined(CONFIG_CPU_XSCALE)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE xscale
-# endif
-#endif
-
-#if defined(CONFIG_CPU_XSC3)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE xsc3
-# endif
-#endif
-
-#if defined(CONFIG_CPU_MOHAWK)
-# ifdef _CACHE
-#  define MULTI_CACHE 1
-# else
-#  define _CACHE mohawk
-# endif
-#endif
-
-#if defined(CONFIG_CPU_FEROCEON)
-# define MULTI_CACHE 1
-#endif
-
-#if defined(CONFIG_CPU_V6)
-//# ifdef _CACHE
-#  define MULTI_CACHE 1
-//# else
-//#  define _CACHE v6
-//# endif
-#endif
-
-#if defined(CONFIG_CPU_V7)
-//# ifdef _CACHE
-#  define MULTI_CACHE 1
-//# else
-//#  define _CACHE v7
-//# endif
-#endif
-
-#if !defined(_CACHE) && !defined(MULTI_CACHE)
-#error Unknown cache maintainence model
-#endif
-
 /*
  * This flag is used to indicate that the page pointed to by a pte is clean
  * and does not require cleaning before returning it to the user.
@@ -249,19 +132,11 @@ extern struct cpu_cache_fns cpu_cache;
  * visible to the CPU.
  */
 #define dmac_map_area			cpu_cache.dma_map_area
-#define dmac_unmap_area		cpu_cache.dma_unmap_area
+#define dmac_unmap_area			cpu_cache.dma_unmap_area
 #define dmac_flush_range		cpu_cache.dma_flush_range
 
 #else
 
-#define __cpuc_flush_icache_all		__glue(_CACHE,_flush_icache_all)
-#define __cpuc_flush_kern_all		__glue(_CACHE,_flush_kern_cache_all)
-#define __cpuc_flush_user_all		__glue(_CACHE,_flush_user_cache_all)
-#define __cpuc_flush_user_range		__glue(_CACHE,_flush_user_cache_range)
-#define __cpuc_coherent_kern_range	__glue(_CACHE,_coherent_kern_range)
-#define __cpuc_coherent_user_range	__glue(_CACHE,_coherent_user_range)
-#define __cpuc_flush_dcache_area	__glue(_CACHE,_flush_kern_dcache_area)
-
 extern void __cpuc_flush_icache_all(void);
 extern void __cpuc_flush_kern_all(void);
 extern void __cpuc_flush_user_all(void);
@@ -276,10 +151,6 @@ extern void __cpuc_flush_dcache_area(void *, size_t);
  * is visible to DMA, or data written by DMA to system memory is
  * visible to the CPU.
  */
-#define dmac_map_area			__glue(_CACHE,_dma_map_area)
-#define dmac_unmap_area		__glue(_CACHE,_dma_unmap_area)
-#define dmac_flush_range		__glue(_CACHE,_dma_flush_range)
-
 extern void dmac_map_area(const void *, size_t, int);
 extern void dmac_unmap_area(const void *, size_t, int);
 extern void dmac_flush_range(const void *, const void *);

commit 774c096bf9e49eebf7b5d2d9fdddf632c29ccea0
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jan 23 13:04:53 2011 +0000

    ARM: v6/v7 cache: allow cache calls to be optimized
    
    The v6 cache call optimization was disabled to allow the optional block
    cache operations to be subsituted on CPUs which supported those
    operations.  However, as that functionality was removed, we no longer
    need to prevent this optimization being taken advantage of.
    
    The v7 cache call optimization was just a copy of the v6, so also fix
    that too.
    
    Tested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 7d0614f599a7..d9b4c42d62ff 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -116,20 +116,20 @@
 # define MULTI_CACHE 1
 #endif
 
-#if defined(CONFIG_CPU_V6) || defined(CONFIG_CPU_V6K)
-//# ifdef _CACHE
+#if defined(CONFIG_CPU_CACHE_V6)
+# ifdef _CACHE
 #  define MULTI_CACHE 1
-//# else
-//#  define _CACHE v6
-//# endif
+# else
+#  define _CACHE v6
+# endif
 #endif
 
-#if defined(CONFIG_CPU_V7)
-//# ifdef _CACHE
+#if defined(CONFIG_CPU_CACHE_V7)
+# ifdef _CACHE
 #  define MULTI_CACHE 1
-//# else
-//#  define _CACHE v7
-//# endif
+# else
+#  define _CACHE v7
+# endif
 #endif
 
 #if !defined(_CACHE) && !defined(MULTI_CACHE)

commit e399b1a4e1d205bdc816cb550d2064f2eb1ddc4c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jan 17 15:08:32 2011 +0000

    ARM: v6k: introduce CPU_V6K option
    
    Introduce a CPU_V6K configuration option for platforms to select if they
    have a V6K CPU core.  This allows us to identify whether we need to
    support ARMv6 CPUs without the V6K SMP extensions at build time.
    
    Currently CPU_V6K is just an alias for CPU_V6, and all places which
    reference CPU_V6 are replaced by (CPU_V6 || CPU_V6K).
    
    Select CPU_V6K from platforms which are known to be V6K-only.
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Tested-by: Sourav Poddar <sourav.poddar@ti.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 3acd8fa25e34..7d0614f599a7 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -116,7 +116,7 @@
 # define MULTI_CACHE 1
 #endif
 
-#if defined(CONFIG_CPU_V6)
+#if defined(CONFIG_CPU_V6) || defined(CONFIG_CPU_V6K)
 //# ifdef _CACHE
 #  define MULTI_CACHE 1
 //# else
@@ -316,7 +316,8 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
  * Optimized __flush_icache_all for the common cases. Note that UP ARMv7
  * will fall through to use __flush_icache_all_generic.
  */
-#if (defined(CONFIG_CPU_V7) && defined(CONFIG_CPU_V6)) ||		\
+#if (defined(CONFIG_CPU_V7) && \
+     (defined(CONFIG_CPU_V6) || defined(CONFIG_CPU_V6K))) || \
 	defined(CONFIG_SMP_ON_UP)
 #define __flush_icache_preferred	__cpuc_flush_icache_all
 #elif __LINUX_ARM_ARCH__ >= 7 && defined(CONFIG_SMP)

commit 23beab76b490172a9ff3d52843e4d27a35b2a4c6
Merge: 8ed9059533eb 5fb31a96e1e0 80be7a7f6427 19852e59002f 29e29f27486e 725343fa748f 9e978f096241 f3af03de0b1c 5333a3de3cdd
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 18 22:34:25 2010 +0100

    Merge branches 'at91', 'dcache', 'ftrace', 'hwbpt', 'misc', 'mmci', 's3c', 'st-ux' and 'unwind' into devel

commit 81d11955bf0b5ae25e3adbec930cd84840385dae
Author: Tony Lindgren <tony@atomide.com>
Date:   Tue Sep 21 17:16:40 2010 +0100

    ARM: 6405/1: Handle __flush_icache_all for CONFIG_SMP_ON_UP
    
    Do this by adding flush_icache_all to cache_fns for ARMv6 and 7.
    As flush_icache_all may neeed to be called from flush_kern_cache_all,
    add it as the first entry in the cache_fns.
    
    Note that now we can remove the ARM_ERRATA_411920 dependency
    to !SMP so it can be selected on UP ARMv6 processors, such
    as omap2.
    
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Anand Gadiyar <gadiyar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 4656a24058d2..a3db768ee5cc 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -156,6 +156,12 @@
  *	Please note that the implementation of these, and the required
  *	effects are cache-type (VIVT/VIPT/PIPT) specific.
  *
+ *	flush_icache_all()
+ *
+ *		Unconditionally clean and invalidate the entire icache.
+ *		Currently only needed for cache-v6.S and cache-v7.S, see
+ *		__flush_icache_all for the generic implementation.
+ *
  *	flush_kern_all()
  *
  *		Unconditionally clean and invalidate the entire cache.
@@ -206,6 +212,7 @@
  */
 
 struct cpu_cache_fns {
+	void (*flush_icache_all)(void);
 	void (*flush_kern_all)(void);
 	void (*flush_user_all)(void);
 	void (*flush_user_range)(unsigned long, unsigned long, unsigned int);
@@ -227,6 +234,7 @@ struct cpu_cache_fns {
 
 extern struct cpu_cache_fns cpu_cache;
 
+#define __cpuc_flush_icache_all		cpu_cache.flush_icache_all
 #define __cpuc_flush_kern_all		cpu_cache.flush_kern_all
 #define __cpuc_flush_user_all		cpu_cache.flush_user_all
 #define __cpuc_flush_user_range		cpu_cache.flush_user_range
@@ -246,6 +254,7 @@ extern struct cpu_cache_fns cpu_cache;
 
 #else
 
+#define __cpuc_flush_icache_all		__glue(_CACHE,_flush_icache_all)
 #define __cpuc_flush_kern_all		__glue(_CACHE,_flush_kern_cache_all)
 #define __cpuc_flush_user_all		__glue(_CACHE,_flush_user_cache_all)
 #define __cpuc_flush_user_range		__glue(_CACHE,_flush_user_cache_range)
@@ -253,6 +262,7 @@ extern struct cpu_cache_fns cpu_cache;
 #define __cpuc_coherent_user_range	__glue(_CACHE,_coherent_user_range)
 #define __cpuc_flush_dcache_area	__glue(_CACHE,_flush_kern_dcache_area)
 
+extern void __cpuc_flush_icache_all(void);
 extern void __cpuc_flush_kern_all(void);
 extern void __cpuc_flush_user_all(void);
 extern void __cpuc_flush_user_range(unsigned long, unsigned long, unsigned int);
@@ -291,6 +301,37 @@ extern void copy_to_user_page(struct vm_area_struct *, struct page *,
 /*
  * Convert calls to our calling convention.
  */
+
+/* Invalidate I-cache */
+#define __flush_icache_all_generic()					\
+	asm("mcr	p15, 0, %0, c7, c5, 0"				\
+	    : : "r" (0));
+
+/* Invalidate I-cache inner shareable */
+#define __flush_icache_all_v7_smp()					\
+	asm("mcr	p15, 0, %0, c7, c1, 0"				\
+	    : : "r" (0));
+
+/*
+ * Optimized __flush_icache_all for the common cases. Note that UP ARMv7
+ * will fall through to use __flush_icache_all_generic.
+ */
+#if (defined(CONFIG_CPU_V7) && defined(CONFIG_CPU_V6)) ||		\
+	defined(CONFIG_SMP_ON_UP)
+#define __flush_icache_preferred	__cpuc_flush_icache_all
+#elif __LINUX_ARM_ARCH__ >= 7 && defined(CONFIG_SMP)
+#define __flush_icache_preferred	__flush_icache_all_v7_smp
+#elif __LINUX_ARM_ARCH__ == 6 && defined(CONFIG_ARM_ERRATA_411920)
+#define __flush_icache_preferred	__cpuc_flush_icache_all
+#else
+#define __flush_icache_preferred	__flush_icache_all_generic
+#endif
+
+static inline void __flush_icache_all(void)
+{
+	__flush_icache_preferred();
+}
+
 #define flush_cache_all()		__cpuc_flush_kern_all()
 
 static inline void vivt_flush_cache_mm(struct mm_struct *mm)
@@ -366,21 +407,6 @@ extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr
 #define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
 extern void flush_dcache_page(struct page *);
 
-static inline void __flush_icache_all(void)
-{
-#ifdef CONFIG_ARM_ERRATA_411920
-	extern void v6_icache_inval_all(void);
-	v6_icache_inval_all();
-#elif defined(CONFIG_SMP) && __LINUX_ARM_ARCH__ >= 7
-	asm("mcr	p15, 0, %0, c7, c1, 0	@ invalidate I-cache inner shareable\n"
-	    :
-	    : "r" (0));
-#else
-	asm("mcr	p15, 0, %0, c7, c5, 0	@ invalidate I-cache\n"
-	    :
-	    : "r" (0));
-#endif
-}
 static inline void flush_kernel_vmap_range(void *addr, int size)
 {
 	if ((cache_is_vivt() || cache_is_vipt_aliasing()))

commit f8b63c184ad13cc8adc3dadb557d4fbc29f76e4d
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 13 15:59:07 2010 +0100

    ARM: 6382/1: Remove superfluous flush_kernel_dcache_page()
    
    Since page cache pages are now considered 'dirty' by default, the cache
    flushing is handled via __flush_dcache_page() when a page gets mapped to
    user space. Highmem pages on VIVT systems are flushed during kunmap()
    and flush_kernel_dcache_page() was already a no-op in this case.
    
    ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE is still defined since ARM needs
    specific implementations for flush_kernel_vmap_range() and
    invalidate_kernel_vmap_range().
    
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index d3730f0f4b50..042e13994d38 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -405,9 +405,6 @@ static inline void flush_anon_page(struct vm_area_struct *vma,
 #define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
 static inline void flush_kernel_dcache_page(struct page *page)
 {
-	/* highmem pages are always flushed upon kunmap already */
-	if ((cache_is_vivt() || cache_is_vipt_aliasing()) && !PageHighMem(page))
-		__cpuc_flush_dcache_area(page_address(page), PAGE_SIZE);
 }
 
 #define flush_dcache_mmap_lock(mapping) \

commit c01778001a4f5ad9c62d882776235f3f31922fdd
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 13 15:57:36 2010 +0100

    ARM: 6379/1: Assume new page cache pages have dirty D-cache
    
    There are places in Linux where writes to newly allocated page cache
    pages happen without a subsequent call to flush_dcache_page() (several
    PIO drivers including USB HCD). This patch changes the meaning of
    PG_arch_1 to be PG_dcache_clean and always flush the D-cache for a newly
    mapped page in update_mmu_cache().
    
    The patch also sets the PG_arch_1 bit in the DMA cache maintenance
    function to avoid additional cache flushing in update_mmu_cache().
    
    Tested-by: Rabin Vincent <rabin.vincent@stericsson.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 4656a24058d2..d3730f0f4b50 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -137,10 +137,10 @@
 #endif
 
 /*
- * This flag is used to indicate that the page pointed to by a pte
- * is dirty and requires cleaning before returning it to the user.
+ * This flag is used to indicate that the page pointed to by a pte is clean
+ * and does not require cleaning before returning it to the user.
  */
-#define PG_dcache_dirty PG_arch_1
+#define PG_dcache_clean PG_arch_1
 
 /*
  *	MM Cache Management

commit b8349b569aae661dea9d59d7d2ee587ccea3336c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 7 18:03:05 2010 +0100

    ARM: 6112/1: Use the Inner Shareable I-cache and BTB ops on ARMv7 SMP
    
    The standard I-cache Invalidate All (ICIALLU) and Branch Predication
    Invalidate All (BPIALL) operations are not automatically broadcast to
    the other CPUs in an ARMv7 MP system. The patch adds the Inner Shareable
    variants, ICIALLUIS and BPIALLIS, if ARMv7 and SMP.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 0d08d4170b64..4656a24058d2 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -371,6 +371,10 @@ static inline void __flush_icache_all(void)
 #ifdef CONFIG_ARM_ERRATA_411920
 	extern void v6_icache_inval_all(void);
 	v6_icache_inval_all();
+#elif defined(CONFIG_SMP) && __LINUX_ARM_ARCH__ >= 7
+	asm("mcr	p15, 0, %0, c7, c1, 0	@ invalidate I-cache inner shareable\n"
+	    :
+	    : "r" (0));
 #else
 	asm("mcr	p15, 0, %0, c7, c5, 0	@ invalidate I-cache\n"
 	    :

commit 33f663ff9186da1bcc88dc7830b3a632bd472da5
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Mar 24 16:46:52 2010 +0100

    ARM: 5993/1: ARM: Move the outer_cache definitions into a separate file (1/4)
    
    To avoid #include collisions with subsequent patches in the series, this
    patch moves the outer_cache definitions to a separate asm/outercache.h
    file.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 72da7e045c6b..0d08d4170b64 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -15,6 +15,7 @@
 #include <asm/glue.h>
 #include <asm/shmparam.h>
 #include <asm/cachetype.h>
+#include <asm/outercache.h>
 
 #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
 
@@ -219,12 +220,6 @@ struct cpu_cache_fns {
 	void (*dma_flush_range)(const void *, const void *);
 };
 
-struct outer_cache_fns {
-	void (*inv_range)(unsigned long, unsigned long);
-	void (*clean_range)(unsigned long, unsigned long);
-	void (*flush_range)(unsigned long, unsigned long);
-};
-
 /*
  * Select the calling method
  */
@@ -281,37 +276,6 @@ extern void dmac_flush_range(const void *, const void *);
 
 #endif
 
-#ifdef CONFIG_OUTER_CACHE
-
-extern struct outer_cache_fns outer_cache;
-
-static inline void outer_inv_range(unsigned long start, unsigned long end)
-{
-	if (outer_cache.inv_range)
-		outer_cache.inv_range(start, end);
-}
-static inline void outer_clean_range(unsigned long start, unsigned long end)
-{
-	if (outer_cache.clean_range)
-		outer_cache.clean_range(start, end);
-}
-static inline void outer_flush_range(unsigned long start, unsigned long end)
-{
-	if (outer_cache.flush_range)
-		outer_cache.flush_range(start, end);
-}
-
-#else
-
-static inline void outer_inv_range(unsigned long start, unsigned long end)
-{ }
-static inline void outer_clean_range(unsigned long start, unsigned long end)
-{ }
-static inline void outer_flush_range(unsigned long start, unsigned long end)
-{ }
-
-#endif
-
 /*
  * Copy user data from/to a page which is mapped into a different
  * processes address space.  Really, we want to allow our "user

commit ac0f6f927db539e03e1f3f61bcd4ed57d5cde7a9
Merge: b1bf9368407a 9f33be2c3a80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 1 09:15:15 2010 -0800

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm: (100 commits)
      ARM: Eliminate decompressor -Dstatic= PIC hack
      ARM: 5958/1: ARM: U300: fix inverted clk round rate
      ARM: 5956/1: misplaced parentheses
      ARM: 5955/1: ep93xx: move timer defines into core.c and document
      ARM: 5954/1: ep93xx: move gpio interrupt support to gpio.c
      ARM: 5953/1: ep93xx: fix broken build of clock.c
      ARM: 5952/1: ARM: MM: Add ARM_L1_CACHE_SHIFT_6 for handle inside each ARCH Kconfig
      ARM: 5949/1: NUC900 add gpio virtual memory map
      ARM: 5948/1: Enable timer0 to time4 clock support for nuc910
      ARM: 5940/2: ARM: MMCI: remove custom DBG macro and printk
      ARM: make_coherent(): fix problems with highpte, part 2
      MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself
      ARM: 5945/1: ep93xx: include correct irq.h in core.c
      ARM: 5933/1: amba-pl011: support hardware flow control
      ARM: 5930/1: Add PKMAP area description to memory.txt.
      ARM: 5929/1: Add checks to detect overlap of memory regions.
      ARM: 5928/1: Change type of VMALLOC_END to unsigned long.
      ARM: 5927/1: Make delimiters of DMA area globally visibly.
      ARM: 5926/1: Add "Virtual kernel memory..." printout.
      ARM: 5920/1: OMAP4: Enable L2 Cache
      ...
    
    Fix up trivial conflict in arch/arm/mach-mx25/clock.c

commit f24407d2bde27a4eb75ce5e52069bf4f1ba0e8a5
Merge: 654451748b77 73c77e2ccc14
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 26 17:05:10 2010 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/jejb/xfs-vipt
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/jejb/xfs-vipt:
      xfs: fix xfs to work with Virtually Indexed architectures
      sh: add mm API for DMA to vmalloc/vmap areas
      arm: add mm API for DMA to vmalloc/vmap areas
      parisc: add mm API for DMA to vmalloc/vmap areas
      mm: add coherence API for DMA to vmalloc/vmap areas

commit 9f33be2c3a80bdc2cc08342dd77fac87652e0548
Merge: 2741ecb4ce5c eed18b5fa4d2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Feb 25 22:10:38 2010 +0000

    Merge branches 'clks' and 'pnx' into devel

commit 3560adf62075829393743667ff338dd8fd26a5c2
Merge: c9a8fdd8b3ec 2ef7f3dbd7a7 2725898fc9bb 1587a373f06a e00d9d4b1796 6d636c68e925 2ffe2da3e716 cd71f8f48eda ae1402022edb
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Feb 25 22:06:43 2010 +0000

    Merge branches 'at91', 'cache', 'cup', 'ep93xx', 'ixp4xx', 'nuc', 'pending-dma-streaming', 'u300' and 'umc' into devel

commit 1c8e170aaa7ba62c0160e96a52e25ad004419109
Author: Abdoulaye Walsimou Gaye <walsimou@walsimou.com>
Date:   Fri Feb 19 12:47:14 2010 +0100

    ARM: 5950/1: ARM: Fix build error for arm1026ej-s processor
    
    This patch fix the below build error for arm1026ej-s processor (IntegratorCP/arm1026ej-s board).
      CC      init/main.o
    In file included from include/linux/highmem.h:8,
                     from include/linux/pagemap.h:10,
                     from include/linux/mempolicy.h:62,
                     from init/main.c:52:
    arch/arm/include/asm/cacheflush.h:134:2: error: #error Unknown cache maintainence model
    make[1]: *** [init/main.o] Erreur 1
    make: *** [init] Erreur 2
    
    Signed-off-by: Abdoulaye Walsimou Gaye <walsimou@walsimou.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index c77d2fa1f6e5..8113bb5fb66e 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -42,7 +42,8 @@
 #endif
 
 #if defined(CONFIG_CPU_ARM920T) || defined(CONFIG_CPU_ARM922T) || \
-    defined(CONFIG_CPU_ARM925T) || defined(CONFIG_CPU_ARM1020)
+    defined(CONFIG_CPU_ARM925T) || defined(CONFIG_CPU_ARM1020) || \
+    defined(CONFIG_CPU_ARM1026)
 # define MULTI_CACHE 1
 #endif
 

commit 702b94bff3c50542a6e4ab9a4f4cef093262fe65
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Nov 26 16:24:19 2009 +0000

    ARM: dma-mapping: remove dmac_clean_range and dmac_inv_range
    
    These are now unused, and so can be removed.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 4c733236e342..e29088587412 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -182,21 +182,6 @@
  *	DMA Cache Coherency
  *	===================
  *
- *	dma_inv_range(start, end)
- *
- *		Invalidate (discard) the specified virtual address range.
- *		May not write back any entries.  If 'start' or 'end'
- *		are not cache line aligned, those lines must be written
- *		back.
- *		- start  - virtual start address
- *		- end    - virtual end address
- *
- *	dma_clean_range(start, end)
- *
- *		Clean (write back) the specified virtual address range.
- *		- start  - virtual start address
- *		- end    - virtual end address
- *
  *	dma_flush_range(start, end)
  *
  *		Clean and invalidate the specified virtual address range.
@@ -216,8 +201,6 @@ struct cpu_cache_fns {
 	void (*dma_map_area)(const void *, size_t, int);
 	void (*dma_unmap_area)(const void *, size_t, int);
 
-	void (*dma_inv_range)(const void *, const void *);
-	void (*dma_clean_range)(const void *, const void *);
 	void (*dma_flush_range)(const void *, const void *);
 };
 
@@ -249,8 +232,6 @@ extern struct cpu_cache_fns cpu_cache;
  */
 #define dmac_map_area			cpu_cache.dma_map_area
 #define dmac_unmap_area		cpu_cache.dma_unmap_area
-#define dmac_inv_range			cpu_cache.dma_inv_range
-#define dmac_clean_range		cpu_cache.dma_clean_range
 #define dmac_flush_range		cpu_cache.dma_flush_range
 
 #else
@@ -277,14 +258,10 @@ extern void __cpuc_flush_dcache_area(void *, size_t);
  */
 #define dmac_map_area			__glue(_CACHE,_dma_map_area)
 #define dmac_unmap_area		__glue(_CACHE,_dma_unmap_area)
-#define dmac_inv_range			__glue(_CACHE,_dma_inv_range)
-#define dmac_clean_range		__glue(_CACHE,_dma_clean_range)
 #define dmac_flush_range		__glue(_CACHE,_dma_flush_range)
 
 extern void dmac_map_area(const void *, size_t, int);
 extern void dmac_unmap_area(const void *, size_t, int);
-extern void dmac_inv_range(const void *, const void *);
-extern void dmac_clean_range(const void *, const void *);
 extern void dmac_flush_range(const void *, const void *);
 
 #endif

commit a9c9147eb9b1dba0ce567a41897c7773b4d1b0bc
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Nov 26 16:19:58 2009 +0000

    ARM: dma-mapping: provide per-cpu type map/unmap functions
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 730aefcfbee3..4c733236e342 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -213,6 +213,9 @@ struct cpu_cache_fns {
 	void (*coherent_user_range)(unsigned long, unsigned long);
 	void (*flush_kern_dcache_area)(void *, size_t);
 
+	void (*dma_map_area)(const void *, size_t, int);
+	void (*dma_unmap_area)(const void *, size_t, int);
+
 	void (*dma_inv_range)(const void *, const void *);
 	void (*dma_clean_range)(const void *, const void *);
 	void (*dma_flush_range)(const void *, const void *);
@@ -244,6 +247,8 @@ extern struct cpu_cache_fns cpu_cache;
  * is visible to DMA, or data written by DMA to system memory is
  * visible to the CPU.
  */
+#define dmac_map_area			cpu_cache.dma_map_area
+#define dmac_unmap_area		cpu_cache.dma_unmap_area
 #define dmac_inv_range			cpu_cache.dma_inv_range
 #define dmac_clean_range		cpu_cache.dma_clean_range
 #define dmac_flush_range		cpu_cache.dma_flush_range
@@ -270,10 +275,14 @@ extern void __cpuc_flush_dcache_area(void *, size_t);
  * is visible to DMA, or data written by DMA to system memory is
  * visible to the CPU.
  */
+#define dmac_map_area			__glue(_CACHE,_dma_map_area)
+#define dmac_unmap_area		__glue(_CACHE,_dma_unmap_area)
 #define dmac_inv_range			__glue(_CACHE,_dma_inv_range)
 #define dmac_clean_range		__glue(_CACHE,_dma_clean_range)
 #define dmac_flush_range		__glue(_CACHE,_dma_flush_range)
 
+extern void dmac_map_area(const void *, size_t, int);
+extern void dmac_unmap_area(const void *, size_t, int);
 extern void dmac_inv_range(const void *, const void *);
 extern void dmac_clean_range(const void *, const void *);
 extern void dmac_flush_range(const void *, const void *);

commit 252a9afff76097667429b583e8b5b170b47665a4
Author: James Bottomley <James.Bottomley@suse.de>
Date:   Mon Jan 25 11:42:22 2010 -0600

    arm: add mm API for DMA to vmalloc/vmap areas
    
    ARM cannot prevent cache movein, so this patch implements both the
    flush and invalidate pieces of the API.
    
    Signed-off-by: James Bottomley <James.Bottomley@suse.de>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 730aefcfbee3..4ae503cb1b8a 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -432,6 +432,16 @@ static inline void __flush_icache_all(void)
 	    : "r" (0));
 #endif
 }
+static inline void flush_kernel_vmap_range(void *addr, int size)
+{
+	if ((cache_is_vivt() || cache_is_vipt_aliasing()))
+	  __cpuc_flush_dcache_area(addr, (size_t)size);
+}
+static inline void invalidate_kernel_vmap_range(void *addr, int size)
+{
+	if ((cache_is_vivt() || cache_is_vipt_aliasing()))
+	  __cpuc_flush_dcache_area(addr, (size_t)size);
+}
 
 #define ARCH_HAS_FLUSH_ANON_PAGE
 static inline void flush_anon_page(struct vm_area_struct *vma,

commit 2045124ffd1a5e46d157349016a2c50f19c8c91d
Author: Tony Lindgren <tony@atomide.com>
Date:   Tue Jan 19 23:42:08 2010 +0100

    ARM: 5888/1: arm: Update comments in cacheflush.h and remove unnecessary V6 and V7 comments
    
    The comments in cacheflush.h should follow what's in
    struct cpu_cache_fns. The comments for V6 and V7 are
    unnecessary.
    
    Signed-off-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 730aefcfbee3..c77d2fa1f6e5 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -154,16 +154,16 @@
  *	Please note that the implementation of these, and the required
  *	effects are cache-type (VIVT/VIPT/PIPT) specific.
  *
- *	flush_cache_kern_all()
+ *	flush_kern_all()
  *
  *		Unconditionally clean and invalidate the entire cache.
  *
- *	flush_cache_user_mm(mm)
+ *	flush_user_all()
  *
  *		Clean and invalidate all user space cache entries
  *		before a change of page tables.
  *
- *	flush_cache_user_range(start, end, flags)
+ *	flush_user_range(start, end, flags)
  *
  *		Clean and invalidate a range of cache entries in the
  *		specified address space before a change of page tables.
@@ -179,6 +179,20 @@
  *		- start  - virtual start address
  *		- end    - virtual end address
  *
+ *	coherent_user_range(start, end)
+ *
+ *		Ensure coherency between the Icache and the Dcache in the
+ *		region described by start, end.  If you have non-snooping
+ *		Harvard caches, you need to implement this function.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	flush_kern_dcache_area(kaddr, size)
+ *
+ *		Ensure that the data held in page is written back.
+ *		- kaddr  - page address
+ *		- size   - region size
+ *
  *	DMA Cache Coherency
  *	===================
  *

commit 2ef7f3dbd7a70a48c3f09b498df528cb00ea03a4
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Nov 5 13:29:36 2009 +0000

    ARM: Fix ptrace accesses
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 730aefcfbee3..3d2ef54c7cb9 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -316,12 +316,8 @@ static inline void outer_flush_range(unsigned long start, unsigned long end)
  * processes address space.  Really, we want to allow our "user
  * space" model to handle this.
  */
-#define copy_to_user_page(vma, page, vaddr, dst, src, len) \
-	do {							\
-		memcpy(dst, src, len);				\
-		flush_ptrace_access(vma, page, vaddr, dst, len, 1);\
-	} while (0)
-
+extern void copy_to_user_page(struct vm_area_struct *, struct page *,
+	unsigned long, void *, const void *, unsigned long);
 #define copy_from_user_page(vma, page, vaddr, dst, src, len) \
 	do {							\
 		memcpy(dst, src, len);				\
@@ -355,17 +351,6 @@ vivt_flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsig
 	}
 }
 
-static inline void
-vivt_flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
-			 unsigned long uaddr, void *kaddr,
-			 unsigned long len, int write)
-{
-	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm))) {
-		unsigned long addr = (unsigned long)kaddr;
-		__cpuc_coherent_kern_range(addr, addr + len);
-	}
-}
-
 #ifndef CONFIG_CPU_CACHE_VIPT
 #define flush_cache_mm(mm) \
 		vivt_flush_cache_mm(mm)
@@ -373,15 +358,10 @@ vivt_flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 		vivt_flush_cache_range(vma,start,end)
 #define flush_cache_page(vma,addr,pfn) \
 		vivt_flush_cache_page(vma,addr,pfn)
-#define flush_ptrace_access(vma,page,ua,ka,len,write) \
-		vivt_flush_ptrace_access(vma,page,ua,ka,len,write)
 #else
 extern void flush_cache_mm(struct mm_struct *mm);
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn);
-extern void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
-				unsigned long uaddr, void *kaddr,
-				unsigned long len, int write);
 #endif
 
 #define flush_cache_dup_mm(mm) flush_cache_mm(mm)

commit 2c9b9c8490b60428fa2d1c64042f7c7caed93940
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Nov 26 12:56:21 2009 +0000

    ARM: add size argument to __cpuc_flush_dcache_page
    
    ... and rename the function since it no longer operates on just
    pages.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 3db7acd39a62..730aefcfbee3 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -211,7 +211,7 @@ struct cpu_cache_fns {
 
 	void (*coherent_kern_range)(unsigned long, unsigned long);
 	void (*coherent_user_range)(unsigned long, unsigned long);
-	void (*flush_kern_dcache_page)(void *);
+	void (*flush_kern_dcache_area)(void *, size_t);
 
 	void (*dma_inv_range)(const void *, const void *);
 	void (*dma_clean_range)(const void *, const void *);
@@ -236,7 +236,7 @@ extern struct cpu_cache_fns cpu_cache;
 #define __cpuc_flush_user_range		cpu_cache.flush_user_range
 #define __cpuc_coherent_kern_range	cpu_cache.coherent_kern_range
 #define __cpuc_coherent_user_range	cpu_cache.coherent_user_range
-#define __cpuc_flush_dcache_page	cpu_cache.flush_kern_dcache_page
+#define __cpuc_flush_dcache_area	cpu_cache.flush_kern_dcache_area
 
 /*
  * These are private to the dma-mapping API.  Do not use directly.
@@ -255,14 +255,14 @@ extern struct cpu_cache_fns cpu_cache;
 #define __cpuc_flush_user_range		__glue(_CACHE,_flush_user_cache_range)
 #define __cpuc_coherent_kern_range	__glue(_CACHE,_coherent_kern_range)
 #define __cpuc_coherent_user_range	__glue(_CACHE,_coherent_user_range)
-#define __cpuc_flush_dcache_page	__glue(_CACHE,_flush_kern_dcache_page)
+#define __cpuc_flush_dcache_area	__glue(_CACHE,_flush_kern_dcache_area)
 
 extern void __cpuc_flush_kern_all(void);
 extern void __cpuc_flush_user_all(void);
 extern void __cpuc_flush_user_range(unsigned long, unsigned long, unsigned int);
 extern void __cpuc_coherent_kern_range(unsigned long, unsigned long);
 extern void __cpuc_coherent_user_range(unsigned long, unsigned long);
-extern void __cpuc_flush_dcache_page(void *);
+extern void __cpuc_flush_dcache_area(void *, size_t);
 
 /*
  * These are private to the dma-mapping API.  Do not use directly.
@@ -448,7 +448,7 @@ static inline void flush_kernel_dcache_page(struct page *page)
 {
 	/* highmem pages are always flushed upon kunmap already */
 	if ((cache_is_vivt() || cache_is_vipt_aliasing()) && !PageHighMem(page))
-		__cpuc_flush_dcache_page(page_address(page));
+		__cpuc_flush_dcache_area(page_address(page), PAGE_SIZE);
 }
 
 #define flush_dcache_mmap_lock(mapping) \

commit ccaf5f05b218e5eb41e2f5cdfd26b18dce4a0218
Author: Nicolas Pitre <nico@fluxnic.net>
Date:   Fri Dec 11 02:21:57 2009 +0100

    ARM: 5848/1: kill flush_ioremap_region()
    
    There is not enough users to warrant its existence, and it is actually
    an obstacle to progress with the new DMA API which cannot cover this
    case properly.
    
    To keep backward compatibility, let's perform the necessary custom
    cache maintenance locally in the only driver affected.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 73eceb87e588..3db7acd39a62 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -465,13 +465,6 @@ static inline void flush_kernel_dcache_page(struct page *page)
  */
 #define flush_icache_page(vma,page)	do { } while (0)
 
-static inline void flush_ioremap_region(unsigned long phys, void __iomem *virt,
-	unsigned offset, size_t size)
-{
-	const void *start = (void __force *)virt + offset;
-	dmac_inv_range(start, start + size);
-}
-
 /*
  * flush_cache_vmap() is used when creating mappings (eg, via vmap,
  * vmalloc, ioremap etc) in kernel space for pages.  On non-VIPT

commit 6035ccd8e9e40bb654fbfdef325902ab531679a5
Merge: 23eb3b64b5e4 878eaddd05d2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 8 08:19:16 2009 -0800

    Merge branch 'for-2.6.33' of git://git.kernel.dk/linux-2.6-block
    
    * 'for-2.6.33' of git://git.kernel.dk/linux-2.6-block: (113 commits)
      cfq-iosched: Do not access cfqq after freeing it
      block: include linux/err.h to use ERR_PTR
      cfq-iosched: use call_rcu() instead of doing grace period stall on queue exit
      blkio: Allow CFQ group IO scheduling even when CFQ is a module
      blkio: Implement dynamic io controlling policy registration
      blkio: Export some symbols from blkio as its user CFQ can be a module
      block: Fix io_context leak after failure of clone with CLONE_IO
      block: Fix io_context leak after clone with CLONE_IO
      cfq-iosched: make nonrot check logic consistent
      io controller: quick fix for blk-cgroup and modular CFQ
      cfq-iosched: move IO controller declerations to a header file
      cfq-iosched: fix compile problem with !CONFIG_CGROUP
      blkio: Documentation
      blkio: Wait on sync-noidle queue even if rq_noidle = 1
      blkio: Implement group_isolation tunable
      blkio: Determine async workload length based on total number of queues
      blkio: Wait for cfq queue to get backlogged if group is empty
      blkio: Propagate cgroup weight updation to cfq groups
      blkio: Drop the reference to queue once the task changes cgroup
      blkio: Provide some isolation between groups
      ...

commit 7b0a1003e780193e2a11c27f5059c26b65f60679
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 24 14:11:59 2009 +0100

    ARM: Reduce __flush_dcache_page() visibility
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 61ae25caaacb..9fd6d3ab68c0 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -420,8 +420,6 @@ extern void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
  */
 extern void flush_dcache_page(struct page *);
 
-extern void __flush_dcache_page(struct address_space *mapping, struct page *page);
-
 static inline void __flush_icache_all(void)
 {
 #ifdef CONFIG_ARM_ERRATA_411920

commit 220d0b1dbf78c6417a658c96e571415552d3abac
Merge: 474b18ccc264 22763c5cf369
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Thu Dec 3 13:49:39 2009 +0100

    Merge branch 'master' into for-2.6.33

commit 2f0b192633f1fbf253b21c90938733491549edae
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Oct 25 10:40:02 2009 +0000

    ARM: Avoid duplicated implementation for VIVT cache flushing
    
    We had two copies of the wrapper code for VIVT cache flushing - one in
    asm/cacheflush.h and one in arch/arm/mm/flush.c.  Reduce this down to
    one common copy.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 3d0cdd21b882..61ae25caaacb 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -331,15 +331,15 @@ static inline void outer_flush_range(unsigned long start, unsigned long end)
  * Convert calls to our calling convention.
  */
 #define flush_cache_all()		__cpuc_flush_kern_all()
-#ifndef CONFIG_CPU_CACHE_VIPT
-static inline void flush_cache_mm(struct mm_struct *mm)
+
+static inline void vivt_flush_cache_mm(struct mm_struct *mm)
 {
 	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm)))
 		__cpuc_flush_user_all();
 }
 
 static inline void
-flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+vivt_flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
 {
 	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm)))
 		__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
@@ -347,7 +347,7 @@ flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long
 }
 
 static inline void
-flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn)
+vivt_flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn)
 {
 	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm))) {
 		unsigned long addr = user_addr & PAGE_MASK;
@@ -356,7 +356,7 @@ flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned l
 }
 
 static inline void
-flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
+vivt_flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 			 unsigned long uaddr, void *kaddr,
 			 unsigned long len, int write)
 {
@@ -365,6 +365,16 @@ flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 		__cpuc_coherent_kern_range(addr, addr + len);
 	}
 }
+
+#ifndef CONFIG_CPU_CACHE_VIPT
+#define flush_cache_mm(mm) \
+		vivt_flush_cache_mm(mm)
+#define flush_cache_range(vma,start,end) \
+		vivt_flush_cache_range(vma,start,end)
+#define flush_cache_page(vma,addr,pfn) \
+		vivt_flush_cache_page(vma,addr,pfn)
+#define flush_ptrace_access(vma,page,ua,ka,len,write) \
+		vivt_flush_ptrace_access(vma,page,ua,ka,len,write)
 #else
 extern void flush_cache_mm(struct mm_struct *mm);
 extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);

commit 2d4dc890b5c8fabd818a8586607e6843c4375e62
Author: Ilya Loginov <isloginov@gmail.com>
Date:   Thu Nov 26 09:16:19 2009 +0100

    block: add helpers to run flush_dcache_page() against a bio and a request's pages
    
    Mtdblock driver doesn't call flush_dcache_page for pages in request.  So,
    this causes problems on architectures where the icache doesn't fill from
    the dcache or with dcache aliases.  The patch fixes this.
    
    The ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE symbol was introduced to avoid
    pointless empty cache-thrashing loops on architectures for which
    flush_dcache_page() is a no-op.  Every architecture was provided with this
    flush pages on architectires where ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE is
    equal 1 or do nothing otherwise.
    
    See "fix mtd_blkdevs problem with caches on some architectures" discussion
    on LKML for more information.
    
    Signed-off-by: Ilya Loginov <isloginov@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Peter Horton <phorton@bitbox.co.uk>
    Cc: "Ed L. Cashin" <ecashin@coraid.com>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index fd03fb63a332..247b7b0adc2a 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -408,6 +408,7 @@ extern void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
  * about to change to user space.  This is the same method as used on SPARC64.
  * See update_mmu_cache for the user space part.
  */
+#define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1
 extern void flush_dcache_page(struct page *);
 
 extern void __flush_dcache_page(struct address_space *mapping, struct page *page);

commit df71dfd4ca01130f98d9dbfab76c440d72a177c6
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 24 22:36:36 2009 +0100

    ARM: Fix errata 411920 workarounds
    
    Errata 411920 indicates that any "invalidate entire instruction cache"
    operation can fail if the right conditions are present.  This is not
    limited just to those operations in flush.c, but elsewhere.  Place the
    workaround in the already existing __flush_icache_all() function
    instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index fd03fb63a332..3d0cdd21b882 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -414,9 +414,14 @@ extern void __flush_dcache_page(struct address_space *mapping, struct page *page
 
 static inline void __flush_icache_all(void)
 {
+#ifdef CONFIG_ARM_ERRATA_411920
+	extern void v6_icache_inval_all(void);
+	v6_icache_inval_all();
+#else
 	asm("mcr	p15, 0, %0, c7, c5, 0	@ invalidate I-cache\n"
 	    :
 	    : "r" (0));
+#endif
 }
 
 #define ARCH_HAS_FLUSH_ANON_PAGE

commit 56f8ba83a52b9f9e3711eff8e54168ac14aa288f
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Sep 24 09:34:49 2009 -0600

    cpumask: use mm_cpumask() wrapper: arm
    
    Makes code futureproof against the impending change to mm->cpu_vm_mask.
    
    It's also a chance to use the new cpumask_ ops which take a pointer
    (the older ones are deprecated, but there's no hurry for arch code).
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 1a711ea8418b..fd03fb63a332 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -334,14 +334,14 @@ static inline void outer_flush_range(unsigned long start, unsigned long end)
 #ifndef CONFIG_CPU_CACHE_VIPT
 static inline void flush_cache_mm(struct mm_struct *mm)
 {
-	if (cpu_isset(smp_processor_id(), mm->cpu_vm_mask))
+	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm)))
 		__cpuc_flush_user_all();
 }
 
 static inline void
 flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
 {
-	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask))
+	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm)))
 		__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
 					vma->vm_flags);
 }
@@ -349,7 +349,7 @@ flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long
 static inline void
 flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn)
 {
-	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
+	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm))) {
 		unsigned long addr = user_addr & PAGE_MASK;
 		__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
 	}
@@ -360,7 +360,7 @@ flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
 			 unsigned long uaddr, void *kaddr,
 			 unsigned long len, int write)
 {
-	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
+	if (cpumask_test_cpu(smp_processor_id(), mm_cpumask(vma->vm_mm))) {
 		unsigned long addr = (unsigned long)kaddr;
 		__cpuc_coherent_kern_range(addr, addr + len);
 	}

commit 73be1591579084a8103a7005dd3172f3e9dd7362
Author: Nicolas Pitre <nico@cam.org>
Date:   Fri Jun 12 03:09:29 2009 +0100

    [ARM] 5545/2: add flush_kernel_dcache_page() for ARM
    
    Without this, the default implementation is a no op which is completely
    wrong with a VIVT cache, and usage of sg_copy_buffer() produces
    unpredictable results.
    
    Tested-by: Sebastian Andrzej Siewior <bigeasy@breakpoint.cc>
    
    CC: stable@kernel.org
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index bb7d695f3900..1a711ea8418b 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -429,6 +429,14 @@ static inline void flush_anon_page(struct vm_area_struct *vma,
 		__flush_anon_page(vma, page, vmaddr);
 }
 
+#define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE
+static inline void flush_kernel_dcache_page(struct page *page)
+{
+	/* highmem pages are always flushed upon kunmap already */
+	if ((cache_is_vivt() || cache_is_vipt_aliasing()) && !PageHighMem(page))
+		__cpuc_flush_dcache_page(page_address(page));
+}
+
 #define flush_dcache_mmap_lock(mapping) \
 	spin_lock_irq(&(mapping)->tree_lock)
 #define flush_dcache_mmap_unlock(mapping) \

commit 542f869f1826f092606efd0c4c771f070d1314f5
Merge: e8b374bb6c88 839e642f3dda
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Mar 26 23:10:11 2009 +0000

    Merge branch 'for-rmk' of git://gitorious.org/linux-gemini/mainline into devel
    
    Conflicts:
            arch/arm/mm/Kconfig
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

commit 28853ac8fe5221de74a14f1182d7b2b383dfd85c
Author: Paulius Zaleckas <paulius.zaleckas@teltonika.lt>
Date:   Wed Mar 25 13:10:01 2009 +0200

    ARM: Add support for FA526 v2
    
    Adds support for Faraday FA526 core. This core is used at least by:
    Cortina Systems Gemini and Centroid family
    Cavium Networks ECONA family
    Grain Media GM8120
    Pixelplus ImageARM
    Prolific PL-1029
    Faraday IP evaluation boards
    
    v2:
    - move TLB_BTB to separate patch
    - update copyrights
    
    Signed-off-by: Paulius Zaleckas <paulius.zaleckas@teltonika.lt>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 6cbd8fdc9f1f..a6b8b90ed57f 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -46,6 +46,14 @@
 # define MULTI_CACHE 1
 #endif
 
+#if defined(CONFIG_CPU_FA526)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE fa
+# endif
+#endif
+
 #if defined(CONFIG_CPU_ARM926T)
 # ifdef _CACHE
 #  define MULTI_CACHE 1

commit 49cbe78637eb0503f45fc9b556ec08918a616534
Author: Eric Miao <eric.miao@marvell.com>
Date:   Tue Jan 20 14:15:18 2009 +0800

    [ARM] pxa: add base support for Marvell's PXA168 processor line
    
    """The Marvell® PXA168 processor is the first in a family of application
    processors targeted at mass market opportunities in computing and consumer
    devices. It balances high computing and multimedia performance with low
    power consumption to support extended battery life, and includes a wealth
    of integrated peripherals to reduce overall BOM cost .... """
    
    See http://www.marvell.com/featured/pxa168.jsp for more information.
    
      1. Marvell Mohawk core is a hybrid of xscale3 and its own ARM core,
         there are many enhancements like instructions for flushing the
         whole D-cache, and so on
    
      2. Clock reuses Russell's common clkdev, and added the basic support
         for UART1/2.
    
      3. Devices are a bit different from the 'mach-pxa' way, the platform
         devices are now dynamically allocated only when necessary (i.e.
         when pxa_register_device() is called). Description for each device
         are stored in an array of 'struct pxa_device_desc'. Now that:
    
         a. this array of device description is marked with __initdata and
            can be freed up system is fully up
    
         b. which means board code has to add all needed devices early in
            his initializing function
    
         c. platform specific data can now be marked as __initdata since
            they are allocated and copied by platform_device_add_data()
    
      4. only the basic UART1/2/3 are added, more devices will come later.
    
    Signed-off-by: Jason Chagas <chagas@marvell.com>
    Signed-off-by: Eric Miao <eric.miao@marvell.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 6cbd8fdc9f1f..bfb0cb9aaa97 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -94,6 +94,14 @@
 # endif
 #endif
 
+#if defined(CONFIG_CPU_MOHAWK)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE mohawk
+# endif
+#endif
+
 #if defined(CONFIG_CPU_FEROCEON)
 # define MULTI_CACHE 1
 #endif

commit 87c52578bd050ba395b0cae7079b1128abd2422d
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Nov 29 17:35:51 2008 +0000

    [ARM] Remove linux/sched.h from asm/cacheflush.h and asm/uaccess.h
    
    ... and fix those drivers that were incorrectly relying upon
    that include.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 85a2514cbffc..6cbd8fdc9f1f 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -10,7 +10,6 @@
 #ifndef _ASMARM_CACHEFLUSH_H
 #define _ASMARM_CACHEFLUSH_H
 
-#include <linux/sched.h>
 #include <linux/mm.h>
 
 #include <asm/glue.h>

commit 376e14218d3d791127e9b9bfbe2f99c44c2a19c2
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Thu Nov 6 13:23:08 2008 +0000

    Do not flush the cache in flush_cache_v(un)map for VIPT caches
    
    In case of non-aliasing VIPT caches, there is no need to flush the whole
    cache when new mapping is created. The patch introduces this condition
    check. In the non-aliasing VIPT case flush_cache_vmap() needs a DSB
    since the set_pte_at() function called from vmap_pte_range() does not
    have such barrier (done usually via TLB flushing functions).
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index de6c59f814a1..85a2514cbffc 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -15,6 +15,7 @@
 
 #include <asm/glue.h>
 #include <asm/shmparam.h>
+#include <asm/cachetype.h>
 
 #define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
 
@@ -295,16 +296,6 @@ static inline void outer_flush_range(unsigned long start, unsigned long end)
 
 #endif
 
-/*
- * flush_cache_vmap() is used when creating mappings (eg, via vmap,
- * vmalloc, ioremap etc) in kernel space for pages.  Since the
- * direct-mappings of these pages may contain cached data, we need
- * to do a full cache flush to ensure that writebacks don't corrupt
- * data placed into these pages via the new mappings.
- */
-#define flush_cache_vmap(start, end)		flush_cache_all()
-#define flush_cache_vunmap(start, end)		flush_cache_all()
-
 /*
  * Copy user data from/to a page which is mapped into a different
  * processes address space.  Really, we want to allow our "user
@@ -444,4 +435,29 @@ static inline void flush_ioremap_region(unsigned long phys, void __iomem *virt,
 	dmac_inv_range(start, start + size);
 }
 
+/*
+ * flush_cache_vmap() is used when creating mappings (eg, via vmap,
+ * vmalloc, ioremap etc) in kernel space for pages.  On non-VIPT
+ * caches, since the direct-mappings of these pages may contain cached
+ * data, we need to do a full cache flush to ensure that writebacks
+ * don't corrupt data placed into these pages via the new mappings.
+ */
+static inline void flush_cache_vmap(unsigned long start, unsigned long end)
+{
+	if (!cache_is_vipt_nonaliasing())
+		flush_cache_all();
+	else
+		/*
+		 * set_pte_at() called from vmap_pte_range() does not
+		 * have a DSB after cleaning the cache line.
+		 */
+		dsb();
+}
+
+static inline void flush_cache_vunmap(unsigned long start, unsigned long end)
+{
+	if (!cache_is_vipt_nonaliasing())
+		flush_cache_all();
+}
+
 #endif

commit 46097c7dd8bfaf9fb86565b6de45ab5a63afdd53
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 18:10:19 2008 +0100

    [ARM] cachetype: move definitions to separate header
    
    Rather than pollute asm/cacheflush.h with the cache type definitions,
    move them to asm/cachetype.h, and include this new header where
    necessary.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index 9073d9c6567e..de6c59f814a1 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -444,94 +444,4 @@ static inline void flush_ioremap_region(unsigned long phys, void __iomem *virt,
 	dmac_inv_range(start, start + size);
 }
 
-#define __cacheid_present(val)			(val != read_cpuid(CPUID_ID))
-#define __cacheid_type_v7(val)			((val & (7 << 29)) == (4 << 29))
-
-#define __cacheid_vivt_prev7(val)		((val & (15 << 25)) != (14 << 25))
-#define __cacheid_vipt_prev7(val)		((val & (15 << 25)) == (14 << 25))
-#define __cacheid_vipt_nonaliasing_prev7(val)	((val & (15 << 25 | 1 << 23)) == (14 << 25))
-#define __cacheid_vipt_aliasing_prev7(val)	((val & (15 << 25 | 1 << 23)) == (14 << 25 | 1 << 23))
-
-#define __cacheid_vivt(val)			(__cacheid_type_v7(val) ? 0 : __cacheid_vivt_prev7(val))
-#define __cacheid_vipt(val)			(__cacheid_type_v7(val) ? 1 : __cacheid_vipt_prev7(val))
-#define __cacheid_vipt_nonaliasing(val)		(__cacheid_type_v7(val) ? 1 : __cacheid_vipt_nonaliasing_prev7(val))
-#define __cacheid_vipt_aliasing(val)		(__cacheid_type_v7(val) ? 0 : __cacheid_vipt_aliasing_prev7(val))
-#define __cacheid_vivt_asid_tagged_instr(val)	(__cacheid_type_v7(val) ? ((val & (3 << 14)) == (1 << 14)) : 0)
-
-#if defined(CONFIG_CPU_CACHE_VIVT) && !defined(CONFIG_CPU_CACHE_VIPT)
-/*
- * VIVT caches only
- */
-#define cache_is_vivt()			1
-#define cache_is_vipt()			0
-#define cache_is_vipt_nonaliasing()	0
-#define cache_is_vipt_aliasing()	0
-#define icache_is_vivt_asid_tagged()	0
-
-#elif !defined(CONFIG_CPU_CACHE_VIVT) && defined(CONFIG_CPU_CACHE_VIPT)
-/*
- * VIPT caches only
- */
-#define cache_is_vivt()			0
-#define cache_is_vipt()			1
-#define cache_is_vipt_nonaliasing()					\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_vipt_nonaliasing(__val);			\
-	})
-
-#define cache_is_vipt_aliasing()					\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_vipt_aliasing(__val);				\
-	})
-
-#define icache_is_vivt_asid_tagged()					\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_vivt_asid_tagged_instr(__val);		\
-	})
-
-#else
-/*
- * VIVT or VIPT caches.  Note that this is unreliable since ARM926
- * and V6 CPUs satisfy the "(val & (15 << 25)) == (14 << 25)" test.
- * There's no way to tell from the CacheType register what type (!)
- * the cache is.
- */
-#define cache_is_vivt()							\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		(!__cacheid_present(__val)) || __cacheid_vivt(__val);	\
-	})
-		
-#define cache_is_vipt()							\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_present(__val) && __cacheid_vipt(__val);	\
-	})
-
-#define cache_is_vipt_nonaliasing()					\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_present(__val) &&				\
-		 __cacheid_vipt_nonaliasing(__val);			\
-	})
-
-#define cache_is_vipt_aliasing()					\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_present(__val) &&				\
-		 __cacheid_vipt_aliasing(__val);			\
-	})
-
-#define icache_is_vivt_asid_tagged()					\
-	({								\
-		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
-		__cacheid_present(__val) &&				\
-		 __cacheid_vivt_asid_tagged_instr(__val);		\
-	})
-
-#endif
-
 #endif

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
new file mode 100644
index 000000000000..9073d9c6567e
--- /dev/null
+++ b/arch/arm/include/asm/cacheflush.h
@@ -0,0 +1,537 @@
+/*
+ *  arch/arm/include/asm/cacheflush.h
+ *
+ *  Copyright (C) 1999-2002 Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef _ASMARM_CACHEFLUSH_H
+#define _ASMARM_CACHEFLUSH_H
+
+#include <linux/sched.h>
+#include <linux/mm.h>
+
+#include <asm/glue.h>
+#include <asm/shmparam.h>
+
+#define CACHE_COLOUR(vaddr)	((vaddr & (SHMLBA - 1)) >> PAGE_SHIFT)
+
+/*
+ *	Cache Model
+ *	===========
+ */
+#undef _CACHE
+#undef MULTI_CACHE
+
+#if defined(CONFIG_CPU_CACHE_V3)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE v3
+# endif
+#endif
+
+#if defined(CONFIG_CPU_CACHE_V4)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE v4
+# endif
+#endif
+
+#if defined(CONFIG_CPU_ARM920T) || defined(CONFIG_CPU_ARM922T) || \
+    defined(CONFIG_CPU_ARM925T) || defined(CONFIG_CPU_ARM1020)
+# define MULTI_CACHE 1
+#endif
+
+#if defined(CONFIG_CPU_ARM926T)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE arm926
+# endif
+#endif
+
+#if defined(CONFIG_CPU_ARM940T)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE arm940
+# endif
+#endif
+
+#if defined(CONFIG_CPU_ARM946E)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE arm946
+# endif
+#endif
+
+#if defined(CONFIG_CPU_CACHE_V4WB)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE v4wb
+# endif
+#endif
+
+#if defined(CONFIG_CPU_XSCALE)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE xscale
+# endif
+#endif
+
+#if defined(CONFIG_CPU_XSC3)
+# ifdef _CACHE
+#  define MULTI_CACHE 1
+# else
+#  define _CACHE xsc3
+# endif
+#endif
+
+#if defined(CONFIG_CPU_FEROCEON)
+# define MULTI_CACHE 1
+#endif
+
+#if defined(CONFIG_CPU_V6)
+//# ifdef _CACHE
+#  define MULTI_CACHE 1
+//# else
+//#  define _CACHE v6
+//# endif
+#endif
+
+#if defined(CONFIG_CPU_V7)
+//# ifdef _CACHE
+#  define MULTI_CACHE 1
+//# else
+//#  define _CACHE v7
+//# endif
+#endif
+
+#if !defined(_CACHE) && !defined(MULTI_CACHE)
+#error Unknown cache maintainence model
+#endif
+
+/*
+ * This flag is used to indicate that the page pointed to by a pte
+ * is dirty and requires cleaning before returning it to the user.
+ */
+#define PG_dcache_dirty PG_arch_1
+
+/*
+ *	MM Cache Management
+ *	===================
+ *
+ *	The arch/arm/mm/cache-*.S and arch/arm/mm/proc-*.S files
+ *	implement these methods.
+ *
+ *	Start addresses are inclusive and end addresses are exclusive;
+ *	start addresses should be rounded down, end addresses up.
+ *
+ *	See Documentation/cachetlb.txt for more information.
+ *	Please note that the implementation of these, and the required
+ *	effects are cache-type (VIVT/VIPT/PIPT) specific.
+ *
+ *	flush_cache_kern_all()
+ *
+ *		Unconditionally clean and invalidate the entire cache.
+ *
+ *	flush_cache_user_mm(mm)
+ *
+ *		Clean and invalidate all user space cache entries
+ *		before a change of page tables.
+ *
+ *	flush_cache_user_range(start, end, flags)
+ *
+ *		Clean and invalidate a range of cache entries in the
+ *		specified address space before a change of page tables.
+ *		- start - user start address (inclusive, page aligned)
+ *		- end   - user end address   (exclusive, page aligned)
+ *		- flags - vma->vm_flags field
+ *
+ *	coherent_kern_range(start, end)
+ *
+ *		Ensure coherency between the Icache and the Dcache in the
+ *		region described by start, end.  If you have non-snooping
+ *		Harvard caches, you need to implement this function.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	DMA Cache Coherency
+ *	===================
+ *
+ *	dma_inv_range(start, end)
+ *
+ *		Invalidate (discard) the specified virtual address range.
+ *		May not write back any entries.  If 'start' or 'end'
+ *		are not cache line aligned, those lines must be written
+ *		back.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	dma_clean_range(start, end)
+ *
+ *		Clean (write back) the specified virtual address range.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ *
+ *	dma_flush_range(start, end)
+ *
+ *		Clean and invalidate the specified virtual address range.
+ *		- start  - virtual start address
+ *		- end    - virtual end address
+ */
+
+struct cpu_cache_fns {
+	void (*flush_kern_all)(void);
+	void (*flush_user_all)(void);
+	void (*flush_user_range)(unsigned long, unsigned long, unsigned int);
+
+	void (*coherent_kern_range)(unsigned long, unsigned long);
+	void (*coherent_user_range)(unsigned long, unsigned long);
+	void (*flush_kern_dcache_page)(void *);
+
+	void (*dma_inv_range)(const void *, const void *);
+	void (*dma_clean_range)(const void *, const void *);
+	void (*dma_flush_range)(const void *, const void *);
+};
+
+struct outer_cache_fns {
+	void (*inv_range)(unsigned long, unsigned long);
+	void (*clean_range)(unsigned long, unsigned long);
+	void (*flush_range)(unsigned long, unsigned long);
+};
+
+/*
+ * Select the calling method
+ */
+#ifdef MULTI_CACHE
+
+extern struct cpu_cache_fns cpu_cache;
+
+#define __cpuc_flush_kern_all		cpu_cache.flush_kern_all
+#define __cpuc_flush_user_all		cpu_cache.flush_user_all
+#define __cpuc_flush_user_range		cpu_cache.flush_user_range
+#define __cpuc_coherent_kern_range	cpu_cache.coherent_kern_range
+#define __cpuc_coherent_user_range	cpu_cache.coherent_user_range
+#define __cpuc_flush_dcache_page	cpu_cache.flush_kern_dcache_page
+
+/*
+ * These are private to the dma-mapping API.  Do not use directly.
+ * Their sole purpose is to ensure that data held in the cache
+ * is visible to DMA, or data written by DMA to system memory is
+ * visible to the CPU.
+ */
+#define dmac_inv_range			cpu_cache.dma_inv_range
+#define dmac_clean_range		cpu_cache.dma_clean_range
+#define dmac_flush_range		cpu_cache.dma_flush_range
+
+#else
+
+#define __cpuc_flush_kern_all		__glue(_CACHE,_flush_kern_cache_all)
+#define __cpuc_flush_user_all		__glue(_CACHE,_flush_user_cache_all)
+#define __cpuc_flush_user_range		__glue(_CACHE,_flush_user_cache_range)
+#define __cpuc_coherent_kern_range	__glue(_CACHE,_coherent_kern_range)
+#define __cpuc_coherent_user_range	__glue(_CACHE,_coherent_user_range)
+#define __cpuc_flush_dcache_page	__glue(_CACHE,_flush_kern_dcache_page)
+
+extern void __cpuc_flush_kern_all(void);
+extern void __cpuc_flush_user_all(void);
+extern void __cpuc_flush_user_range(unsigned long, unsigned long, unsigned int);
+extern void __cpuc_coherent_kern_range(unsigned long, unsigned long);
+extern void __cpuc_coherent_user_range(unsigned long, unsigned long);
+extern void __cpuc_flush_dcache_page(void *);
+
+/*
+ * These are private to the dma-mapping API.  Do not use directly.
+ * Their sole purpose is to ensure that data held in the cache
+ * is visible to DMA, or data written by DMA to system memory is
+ * visible to the CPU.
+ */
+#define dmac_inv_range			__glue(_CACHE,_dma_inv_range)
+#define dmac_clean_range		__glue(_CACHE,_dma_clean_range)
+#define dmac_flush_range		__glue(_CACHE,_dma_flush_range)
+
+extern void dmac_inv_range(const void *, const void *);
+extern void dmac_clean_range(const void *, const void *);
+extern void dmac_flush_range(const void *, const void *);
+
+#endif
+
+#ifdef CONFIG_OUTER_CACHE
+
+extern struct outer_cache_fns outer_cache;
+
+static inline void outer_inv_range(unsigned long start, unsigned long end)
+{
+	if (outer_cache.inv_range)
+		outer_cache.inv_range(start, end);
+}
+static inline void outer_clean_range(unsigned long start, unsigned long end)
+{
+	if (outer_cache.clean_range)
+		outer_cache.clean_range(start, end);
+}
+static inline void outer_flush_range(unsigned long start, unsigned long end)
+{
+	if (outer_cache.flush_range)
+		outer_cache.flush_range(start, end);
+}
+
+#else
+
+static inline void outer_inv_range(unsigned long start, unsigned long end)
+{ }
+static inline void outer_clean_range(unsigned long start, unsigned long end)
+{ }
+static inline void outer_flush_range(unsigned long start, unsigned long end)
+{ }
+
+#endif
+
+/*
+ * flush_cache_vmap() is used when creating mappings (eg, via vmap,
+ * vmalloc, ioremap etc) in kernel space for pages.  Since the
+ * direct-mappings of these pages may contain cached data, we need
+ * to do a full cache flush to ensure that writebacks don't corrupt
+ * data placed into these pages via the new mappings.
+ */
+#define flush_cache_vmap(start, end)		flush_cache_all()
+#define flush_cache_vunmap(start, end)		flush_cache_all()
+
+/*
+ * Copy user data from/to a page which is mapped into a different
+ * processes address space.  Really, we want to allow our "user
+ * space" model to handle this.
+ */
+#define copy_to_user_page(vma, page, vaddr, dst, src, len) \
+	do {							\
+		memcpy(dst, src, len);				\
+		flush_ptrace_access(vma, page, vaddr, dst, len, 1);\
+	} while (0)
+
+#define copy_from_user_page(vma, page, vaddr, dst, src, len) \
+	do {							\
+		memcpy(dst, src, len);				\
+	} while (0)
+
+/*
+ * Convert calls to our calling convention.
+ */
+#define flush_cache_all()		__cpuc_flush_kern_all()
+#ifndef CONFIG_CPU_CACHE_VIPT
+static inline void flush_cache_mm(struct mm_struct *mm)
+{
+	if (cpu_isset(smp_processor_id(), mm->cpu_vm_mask))
+		__cpuc_flush_user_all();
+}
+
+static inline void
+flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
+{
+	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask))
+		__cpuc_flush_user_range(start & PAGE_MASK, PAGE_ALIGN(end),
+					vma->vm_flags);
+}
+
+static inline void
+flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn)
+{
+	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
+		unsigned long addr = user_addr & PAGE_MASK;
+		__cpuc_flush_user_range(addr, addr + PAGE_SIZE, vma->vm_flags);
+	}
+}
+
+static inline void
+flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
+			 unsigned long uaddr, void *kaddr,
+			 unsigned long len, int write)
+{
+	if (cpu_isset(smp_processor_id(), vma->vm_mm->cpu_vm_mask)) {
+		unsigned long addr = (unsigned long)kaddr;
+		__cpuc_coherent_kern_range(addr, addr + len);
+	}
+}
+#else
+extern void flush_cache_mm(struct mm_struct *mm);
+extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
+extern void flush_cache_page(struct vm_area_struct *vma, unsigned long user_addr, unsigned long pfn);
+extern void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
+				unsigned long uaddr, void *kaddr,
+				unsigned long len, int write);
+#endif
+
+#define flush_cache_dup_mm(mm) flush_cache_mm(mm)
+
+/*
+ * flush_cache_user_range is used when we want to ensure that the
+ * Harvard caches are synchronised for the user space address range.
+ * This is used for the ARM private sys_cacheflush system call.
+ */
+#define flush_cache_user_range(vma,start,end) \
+	__cpuc_coherent_user_range((start) & PAGE_MASK, PAGE_ALIGN(end))
+
+/*
+ * Perform necessary cache operations to ensure that data previously
+ * stored within this range of addresses can be executed by the CPU.
+ */
+#define flush_icache_range(s,e)		__cpuc_coherent_kern_range(s,e)
+
+/*
+ * Perform necessary cache operations to ensure that the TLB will
+ * see data written in the specified area.
+ */
+#define clean_dcache_area(start,size)	cpu_dcache_clean_area(start, size)
+
+/*
+ * flush_dcache_page is used when the kernel has written to the page
+ * cache page at virtual address page->virtual.
+ *
+ * If this page isn't mapped (ie, page_mapping == NULL), or it might
+ * have userspace mappings, then we _must_ always clean + invalidate
+ * the dcache entries associated with the kernel mapping.
+ *
+ * Otherwise we can defer the operation, and clean the cache when we are
+ * about to change to user space.  This is the same method as used on SPARC64.
+ * See update_mmu_cache for the user space part.
+ */
+extern void flush_dcache_page(struct page *);
+
+extern void __flush_dcache_page(struct address_space *mapping, struct page *page);
+
+static inline void __flush_icache_all(void)
+{
+	asm("mcr	p15, 0, %0, c7, c5, 0	@ invalidate I-cache\n"
+	    :
+	    : "r" (0));
+}
+
+#define ARCH_HAS_FLUSH_ANON_PAGE
+static inline void flush_anon_page(struct vm_area_struct *vma,
+			 struct page *page, unsigned long vmaddr)
+{
+	extern void __flush_anon_page(struct vm_area_struct *vma,
+				struct page *, unsigned long);
+	if (PageAnon(page))
+		__flush_anon_page(vma, page, vmaddr);
+}
+
+#define flush_dcache_mmap_lock(mapping) \
+	spin_lock_irq(&(mapping)->tree_lock)
+#define flush_dcache_mmap_unlock(mapping) \
+	spin_unlock_irq(&(mapping)->tree_lock)
+
+#define flush_icache_user_range(vma,page,addr,len) \
+	flush_dcache_page(page)
+
+/*
+ * We don't appear to need to do anything here.  In fact, if we did, we'd
+ * duplicate cache flushing elsewhere performed by flush_dcache_page().
+ */
+#define flush_icache_page(vma,page)	do { } while (0)
+
+static inline void flush_ioremap_region(unsigned long phys, void __iomem *virt,
+	unsigned offset, size_t size)
+{
+	const void *start = (void __force *)virt + offset;
+	dmac_inv_range(start, start + size);
+}
+
+#define __cacheid_present(val)			(val != read_cpuid(CPUID_ID))
+#define __cacheid_type_v7(val)			((val & (7 << 29)) == (4 << 29))
+
+#define __cacheid_vivt_prev7(val)		((val & (15 << 25)) != (14 << 25))
+#define __cacheid_vipt_prev7(val)		((val & (15 << 25)) == (14 << 25))
+#define __cacheid_vipt_nonaliasing_prev7(val)	((val & (15 << 25 | 1 << 23)) == (14 << 25))
+#define __cacheid_vipt_aliasing_prev7(val)	((val & (15 << 25 | 1 << 23)) == (14 << 25 | 1 << 23))
+
+#define __cacheid_vivt(val)			(__cacheid_type_v7(val) ? 0 : __cacheid_vivt_prev7(val))
+#define __cacheid_vipt(val)			(__cacheid_type_v7(val) ? 1 : __cacheid_vipt_prev7(val))
+#define __cacheid_vipt_nonaliasing(val)		(__cacheid_type_v7(val) ? 1 : __cacheid_vipt_nonaliasing_prev7(val))
+#define __cacheid_vipt_aliasing(val)		(__cacheid_type_v7(val) ? 0 : __cacheid_vipt_aliasing_prev7(val))
+#define __cacheid_vivt_asid_tagged_instr(val)	(__cacheid_type_v7(val) ? ((val & (3 << 14)) == (1 << 14)) : 0)
+
+#if defined(CONFIG_CPU_CACHE_VIVT) && !defined(CONFIG_CPU_CACHE_VIPT)
+/*
+ * VIVT caches only
+ */
+#define cache_is_vivt()			1
+#define cache_is_vipt()			0
+#define cache_is_vipt_nonaliasing()	0
+#define cache_is_vipt_aliasing()	0
+#define icache_is_vivt_asid_tagged()	0
+
+#elif !defined(CONFIG_CPU_CACHE_VIVT) && defined(CONFIG_CPU_CACHE_VIPT)
+/*
+ * VIPT caches only
+ */
+#define cache_is_vivt()			0
+#define cache_is_vipt()			1
+#define cache_is_vipt_nonaliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_vipt_nonaliasing(__val);			\
+	})
+
+#define cache_is_vipt_aliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_vipt_aliasing(__val);				\
+	})
+
+#define icache_is_vivt_asid_tagged()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_vivt_asid_tagged_instr(__val);		\
+	})
+
+#else
+/*
+ * VIVT or VIPT caches.  Note that this is unreliable since ARM926
+ * and V6 CPUs satisfy the "(val & (15 << 25)) == (14 << 25)" test.
+ * There's no way to tell from the CacheType register what type (!)
+ * the cache is.
+ */
+#define cache_is_vivt()							\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		(!__cacheid_present(__val)) || __cacheid_vivt(__val);	\
+	})
+		
+#define cache_is_vipt()							\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) && __cacheid_vipt(__val);	\
+	})
+
+#define cache_is_vipt_nonaliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) &&				\
+		 __cacheid_vipt_nonaliasing(__val);			\
+	})
+
+#define cache_is_vipt_aliasing()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) &&				\
+		 __cacheid_vipt_aliasing(__val);			\
+	})
+
+#define icache_is_vivt_asid_tagged()					\
+	({								\
+		unsigned int __val = read_cpuid(CPUID_CACHETYPE);	\
+		__cacheid_present(__val) &&				\
+		 __cacheid_vivt_asid_tagged_instr(__val);		\
+	})
+
+#endif
+
+#endif
