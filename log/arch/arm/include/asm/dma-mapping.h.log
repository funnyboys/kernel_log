commit a8e6d950cdc1012aede8a84d85f902a53d895174
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jul 24 14:06:54 2019 +0200

    xen/arm: use dev_is_dma_coherent
    
    Use the dma-noncoherent dev_is_dma_coherent helper instead of the home
    grown variant.  Note that both are always initialized to the same
    value in arch_setup_dma_ops.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Julien Grall <julien.grall@arm.com>
    Reviewed-by: Stefano Stabellini <sstabellini@kernel.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index dba9355e2484..bdd80ddbca34 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -91,12 +91,6 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
-/* do not use this function in a driver */
-static inline bool is_device_dma_coherent(struct device *dev)
-{
-	return dev->archdata.dma_coherent;
-}
-
 /**
  * arm_dma_alloc - allocate consistent memory for DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

commit ad3c7b18c5b362be5dbd0f2c0bcf1fd5fd659315
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jul 23 11:33:12 2019 +0200

    arm: use swiotlb for bounce buffering on LPAE configs
    
    The DMA API requires that 32-bit DMA masks are always supported, but on
    arm LPAE configs they do not currently work when memory is present
    above 4GB.  Wire up the swiotlb code like for all other architectures
    to provide the bounce buffering in that case.
    
    Fixes: 21e07dba9fb11 ("scsi: reduce use of block bounce buffers").
    Reported-by: Roger Quadros <rogerq@ti.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Vignesh Raghavendra <vigneshr@ti.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 7e0486ad1318..dba9355e2484 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -18,7 +18,9 @@ extern const struct dma_map_ops arm_coherent_dma_ops;
 
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
-	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : NULL;
+	if (IS_ENABLED(CONFIG_MMU) && !IS_ENABLED(CONFIG_ARM_LPAE))
+		return &arm_dma_ops;
+	return NULL;
 }
 
 #ifdef __arch_page_to_dma

commit 79a986721decf428ba539e6e2c941c987acce655
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jun 25 11:20:42 2019 +0200

    dma-mapping: remove dma_max_pfn
    
    These days, the DMA mapping code must bounce buffers for any unsupported
    address. If the driver needs to optimize for natively supported ranges,
    then it should use dma_get_required_mask.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Marc Gonzalez <marc.w.gonzalez@free.fr>
    Signed-off-by: Ulf Hansson <ulf.hansson@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 03ba90ffc0f8..7e0486ad1318 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -89,13 +89,6 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
-/* The ARM override for dma_max_pfn() */
-static inline unsigned long dma_max_pfn(struct device *dev)
-{
-	return dma_to_pfn(dev, *dev->dma_mask);
-}
-#define dma_max_pfn(dev) dma_max_pfn(dev)
-
 /* do not use this function in a driver */
 static inline bool is_device_dma_coherent(struct device *dev)
 {

commit dc2acded38957dfa6b7b7e0203b4b8cb8d818ce6
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 21 22:14:44 2018 +0100

    dma-mapping: add a kconfig symbol for arch_teardown_dma_ops availability
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com> # arm64

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index a224b6e39e58..03ba90ffc0f8 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -96,11 +96,6 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 
-#ifdef CONFIG_MMU
-#define arch_teardown_dma_ops arch_teardown_dma_ops
-extern void arch_teardown_dma_ops(struct device *dev);
-#endif
-
 /* do not use this function in a driver */
 static inline bool is_device_dma_coherent(struct device *dev)
 {

commit 347cb6af8710b72cf9685fdc09d07873cf42d51f
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Jan 7 13:36:20 2019 -0500

    dma-mapping: add a kconfig symbol for arch_setup_dma_ops availability
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Paul Burton <paul.burton@mips.com> # MIPS
    Acked-by: Catalin Marinas <catalin.marinas@arm.com> # arm64

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 31d3b96f0f4b..a224b6e39e58 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -96,10 +96,6 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 
-#define arch_setup_dma_ops arch_setup_dma_ops
-extern void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
-			       const struct iommu_ops *iommu, bool coherent);
-
 #ifdef CONFIG_MMU
 #define arch_teardown_dma_ops arch_teardown_dma_ops
 extern void arch_teardown_dma_ops(struct device *dev);

commit 356da6d0cde3323236977fce54c1f9612a742036
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Dec 6 13:39:32 2018 -0800

    dma-mapping: bypass indirect calls for dma-direct
    
    Avoid expensive indirect calls in the fast path DMA mapping
    operations by directly calling the dma_direct_* ops if we are using
    the directly mapped DMA operations.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 965b7c846ecb..31d3b96f0f4b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -18,7 +18,7 @@ extern const struct dma_map_ops arm_coherent_dma_ops;
 
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
-	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : &dma_direct_ops;
+	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : NULL;
 }
 
 #ifdef __arch_page_to_dma

commit 58643a3a8097e93b1c965d0ad3f0d65613b95b2c
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 24 09:47:20 2018 +0200

    arm-nommu: don't define arch_teardown_dma_ops
    
    We can just use the default implementation.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 8436f6ade57d..965b7c846ecb 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -100,8 +100,10 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 extern void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 			       const struct iommu_ops *iommu, bool coherent);
 
+#ifdef CONFIG_MMU
 #define arch_teardown_dma_ops arch_teardown_dma_ops
 extern void arch_teardown_dma_ops(struct device *dev);
+#endif
 
 /* do not use this function in a driver */
 static inline bool is_device_dma_coherent(struct device *dev)

commit 002e67454f61bb67d8071ac4d0cacb86a01d18e0
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 9 16:30:23 2018 +0100

    dma-direct: rename dma_noop to dma_direct
    
    The trivial direct mapping implementation already does a virtual to
    physical translation which isn't strictly a noop, and will soon learn
    to do non-direct but linear physical to dma translations through the
    device offset and a few small tricks.  Rename it to a better fitting
    name.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Vladimir Murzin <vladimir.murzin@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index e5d9020c9ee1..8436f6ade57d 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -18,7 +18,7 @@ extern const struct dma_map_ops arm_coherent_dma_ops;
 
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
-	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : &dma_noop_ops;
+	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : &dma_direct_ops;
 }
 
 #ifdef __arch_page_to_dma

commit b49efd76248242169f28ffd20ada05064d01ed9f
Author: Christoph Hellwig <hch@lst.de>
Date:   Tue Jan 9 22:11:31 2018 +0100

    dma-mapping: move dma_mark_clean to dma-direct.h
    
    And unlike the other helpers we don't require a <asm/dma-direct.h> as
    this helper is a special case for ia64 only, and this keeps it as
    simple as possible.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5fb1b7fbdfbe..e5d9020c9ee1 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -109,8 +109,6 @@ static inline bool is_device_dma_coherent(struct device *dev)
 	return dev->archdata.dma_coherent;
 }
 
-static inline void dma_mark_clean(void *addr, size_t size) { }
-
 /**
  * arm_dma_alloc - allocate consistent memory for DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

commit ea8c64ace86647260ec4255f483e5844d62af2df
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 10 16:21:13 2018 +0100

    dma-mapping: move swiotlb arch helpers to a new header
    
    phys_to_dma, dma_to_phys and dma_capable are helpers published by
    architecture code for use of swiotlb and xen-swiotlb only.  Drivers are
    not supposed to use these directly, but use the DMA API instead.
    
    Move these to a new asm/dma-direct.h helper, included by a
    linux/dma-direct.h wrapper that provides the default linear mapping
    unless the architecture wants to override it.
    
    In the MIPS case the existing dma-coherent.h is reused for now as
    untangling it will take a bit of work.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Robin Murphy <robin.murphy@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index daf837423a76..5fb1b7fbdfbe 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -109,37 +109,6 @@ static inline bool is_device_dma_coherent(struct device *dev)
 	return dev->archdata.dma_coherent;
 }
 
-static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
-{
-	unsigned int offset = paddr & ~PAGE_MASK;
-	return pfn_to_dma(dev, __phys_to_pfn(paddr)) + offset;
-}
-
-static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
-{
-	unsigned int offset = dev_addr & ~PAGE_MASK;
-	return __pfn_to_phys(dma_to_pfn(dev, dev_addr)) + offset;
-}
-
-static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
-{
-	u64 limit, mask;
-
-	if (!dev->dma_mask)
-		return 0;
-
-	mask = *dev->dma_mask;
-
-	limit = (mask + 1) & ~mask;
-	if (limit && size > limit)
-		return 0;
-
-	if ((addr | (addr + size - 1)) & ~mask)
-		return 0;
-
-	return 1;
-}
-
 static inline void dma_mark_clean(void *addr, size_t size) { }
 
 /**

commit 441692aafc1731087bbaf657a8b6059d95c2a6df
Merge: 5b0e2cb02008 02196144a0a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:50:35 2017 -0800

    Merge branch 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - add support for ELF fdpic binaries on both MMU and noMMU platforms
    
     - linker script cleanups
    
     - support for compressed .data section for XIP images
    
     - discard memblock arrays when possible
    
     - various cleanups
    
     - atomic DMA pool updates
    
     - better diagnostics of missing/corrupt device tree
    
     - export information to allow userspace kexec tool to place images more
       inteligently, so that the device tree isn't overwritten by the
       booting kernel
    
     - make early_printk more efficient on semihosted systems
    
     - noMMU cleanups
    
     - SA1111 PCMCIA update in preparation for further cleanups
    
    * 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm: (38 commits)
      ARM: 8719/1: NOMMU: work around maybe-uninitialized warning
      ARM: 8717/2: debug printch/printascii: translate '\n' to "\r\n" not "\n\r"
      ARM: 8713/1: NOMMU: Support MPU in XIP configuration
      ARM: 8712/1: NOMMU: Use more MPU regions to cover memory
      ARM: 8711/1: V7M: Add support for MPU to M-class
      ARM: 8710/1: Kconfig: Kill CONFIG_VECTORS_BASE
      ARM: 8709/1: NOMMU: Disallow MPU for XIP
      ARM: 8708/1: NOMMU: Rework MPU to be mostly done in C
      ARM: 8707/1: NOMMU: Update MPU accessors to use cp15 helpers
      ARM: 8706/1: NOMMU: Move out MPU setup in separate module
      ARM: 8702/1: head-common.S: Clear lr before jumping to start_kernel()
      ARM: 8705/1: early_printk: use printascii() rather than printch()
      ARM: 8703/1: debug.S: move hexbuf to a writable section
      ARM: add additional table to compressed kernel
      ARM: decompressor: fix BSS size calculation
      pcmcia: sa1111: remove special sa1111 mmio accessors
      pcmcia: sa1111: use sa1111_get_irq() to obtain IRQ resources
      ARM: better diagnostics with missing/corrupt dtb
      ARM: 8699/1: dma-mapping: Remove init_dma_coherent_pool_size()
      ARM: 8698/1: dma-mapping: Mark atomic_pool as __ro_after_init
      ..

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 4e0285a66ef8..3ca119997818 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef ASMARM_DMA_MAPPING_H
 #define ASMARM_DMA_MAPPING_H
 

commit 0f7c4c15a30ffc5f5fd563ad1159cb899e121407
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Mon Sep 25 10:31:15 2017 +0100

    ARM: 8699/1: dma-mapping: Remove init_dma_coherent_pool_size()
    
    There are no users of init_dma_coherent_pool_size() left due to
    387870f ("mm: dmapool: use provided gfp flags for all
    dma_alloc_coherent() calls"), so remove it.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 4e0285a66ef8..462803f3a6a0 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -189,13 +189,6 @@ extern int arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 			void *cpu_addr, dma_addr_t dma_addr, size_t size,
 			unsigned long attrs);
 
-/*
- * This can be called during early boot to increase the size of the atomic
- * coherent DMA pool above the default value of 256KiB. It must be called
- * before postcore_initcall.
- */
-extern void __init init_dma_coherent_pool_size(unsigned long size);
-
 /*
  * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
  * and utilize bounce buffers as needed to work around limited DMA windows.

commit 1c51c429f30ea10428337f3a33c12059ba59f668
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Wed May 24 11:24:30 2017 +0100

    ARM: NOMMU: Introduce dma operations for noMMU
    
    R/M classes of cpus can have memory covered by MPU which in turn might
    configure RAM as Normal i.e. bufferable and cacheable. It breaks
    dma_alloc_coherent() and friends, since data can stuck in caches now
    or be buffered.
    
    This patch factors out DMA support for NOMMU configuration into
    separate entity which provides dedicated dma_ops. We have to handle
    there several cases:
    - configurations with MMU/MPU setup
    - configurations without MMU/MPU setup
    - special case for M-class, since caches and MPU there are optional
    
    In general we rely on default DMA area for coherent allocations or/and
    per-device memory reserves suitable for coherent DMA, so if such
    regions are set coherent allocations go from there.
    
    In case MMU/MPU was not setup we fallback to normal page allocator for
    DMA memory allocation.
    
    In case we run M-class cpus, for configuration without cache support
    (like Cortex-M3/M4) dma operations are forced to be coherent and wired
    with dma-noop (such decision is made based on cacheid global
    variable); however, if caches are detected there and no DMA coherent
    region is given (either default or per-device), dma is disallowed even
    MPU is not set - it is because M-class implement system memory map
    which defines part of address space as Normal memory.
    
    Reported-by: Alexandre Torgue <alexandre.torgue@st.com>
    Reported-by: Andras Szemzo <sza@esh.hu>
    Tested-by: Benjamin Gaignard <benjamin.gaignard@linaro.org>
    Tested-by: Andras Szemzo <sza@esh.hu>
    Tested-by: Alexandre TORGUE <alexandre.torgue@st.com>
    Reviewed-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    [hch: removed the dma_supported() implementation that isn't required anymore]
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 8dabcfdf4505..4e0285a66ef8 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -17,7 +17,7 @@ extern const struct dma_map_ops arm_coherent_dma_ops;
 
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
-	return &arm_dma_ops;
+	return IS_ENABLED(CONFIG_MMU) ? &arm_dma_ops : &dma_noop_ops;
 }
 
 #ifdef __arch_page_to_dma

commit 418a7a7e4f05f36d6e4ab5b8548ea71f0b602140
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 11:20:18 2017 +0200

    arm: remove arch specific dma_supported implementation
    
    And instead wire it up as method for all the dma_map_ops instances.
    
    Note that the code seems a little fishy for dmabounce and iommu, but
    for now I'd like to preserve the existing behavior 1:1.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 52a8fd5a8edb..8dabcfdf4505 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -20,9 +20,6 @@ static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 	return &arm_dma_ops;
 }
 
-#define HAVE_ARCH_DMA_SUPPORTED 1
-extern int dma_supported(struct device *dev, u64 mask);
-
 #ifdef __arch_page_to_dma
 #error Please update to __arch_pfn_to_dma
 #endif

commit 9eef8b8cc26559fe5f2575daf7d08c6a17e81ff8
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 10:53:03 2017 +0200

    arm: implement ->mapping_error
    
    DMA_ERROR_CODE is going to go away, so don't rely on it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 680d3f3889e7..52a8fd5a8edb 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -12,7 +12,6 @@
 #include <xen/xen.h>
 #include <asm/xen/hypervisor.h>
 
-#define DMA_ERROR_CODE	(~(dma_addr_t)0x0)
 extern const struct dma_map_ops arm_dma_ops;
 extern const struct dma_map_ops arm_coherent_dma_ops;
 

commit e058632670b709145730a134acc3f83f392f7aa7
Author: Stefano Stabellini <sstabellini@kernel.org>
Date:   Thu Apr 13 14:04:21 2017 -0700

    xen/arm,arm64: fix xen_dma_ops after 815dd18 "Consolidate get_dma_ops..."
    
    The following commit:
    
      commit 815dd18788fe0d41899f51b91d0560279cf16b0d
      Author: Bart Van Assche <bart.vanassche@sandisk.com>
      Date:   Fri Jan 20 13:04:04 2017 -0800
    
          treewide: Consolidate get_dma_ops() implementations
    
    rearranges get_dma_ops in a way that xen_dma_ops are not returned when
    running on Xen anymore, dev->dma_ops is returned instead (see
    arch/arm/include/asm/dma-mapping.h:get_arch_dma_ops and
    include/linux/dma-mapping.h:get_dma_ops).
    
    Fix the problem by storing dev->dma_ops in dev_archdata, and setting
    dev->dma_ops to xen_dma_ops. This way, xen_dma_ops is returned naturally
    by get_dma_ops. The Xen code can retrieve the original dev->dma_ops from
    dev_archdata when needed. It also allows us to remove __generic_dma_ops
    from common headers.
    
    Signed-off-by: Stefano Stabellini <sstabellini@kernel.org>
    Tested-by: Julien Grall <julien.grall@arm.com>
    Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: <stable@vger.kernel.org>        [4.11+]
    CC: linux@armlinux.org.uk
    CC: catalin.marinas@arm.com
    CC: will.deacon@arm.com
    CC: boris.ostrovsky@oracle.com
    CC: jgross@suse.com
    CC: Julien Grall <julien.grall@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 716656925975..680d3f3889e7 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -16,19 +16,9 @@
 extern const struct dma_map_ops arm_dma_ops;
 extern const struct dma_map_ops arm_coherent_dma_ops;
 
-static inline const struct dma_map_ops *__generic_dma_ops(struct device *dev)
-{
-	if (dev && dev->dma_ops)
-		return dev->dma_ops;
-	return &arm_dma_ops;
-}
-
 static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
-	if (xen_initial_domain())
-		return xen_dma_ops;
-	else
-		return __generic_dma_ops(NULL);
+	return &arm_dma_ops;
 }
 
 #define HAVE_ARCH_DMA_SUPPORTED 1

commit 815dd18788fe0d41899f51b91d0560279cf16b0d
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:04 2017 -0800

    treewide: Consolidate get_dma_ops() implementations
    
    Introduce a new architecture-specific get_arch_dma_ops() function
    that takes a struct bus_type * argument. Add get_dma_ops() in
    <linux/dma-mapping.h>.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index c7432d647e53..716656925975 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -23,12 +23,12 @@ static inline const struct dma_map_ops *__generic_dma_ops(struct device *dev)
 	return &arm_dma_ops;
 }
 
-static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
+static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)
 {
 	if (xen_initial_domain())
 		return xen_dma_ops;
 	else
-		return __generic_dma_ops(dev);
+		return __generic_dma_ops(NULL);
 }
 
 #define HAVE_ARCH_DMA_SUPPORTED 1

commit ca6e8e1031419549f67291ca31b43126f07cecdb
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:03 2017 -0800

    treewide: Consolidate set_dma_ops() implementations
    
    Now that all set_dma_ops() implementations are identical (ignoring
    BUG_ON() statements), remove the architecture specific definitions
    and add a definition in <linux/dma-mapping.h>.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 312f4d0564d6..c7432d647e53 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -31,12 +31,6 @@ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 		return __generic_dma_ops(dev);
 }
 
-static inline void set_dma_ops(struct device *dev, const struct dma_map_ops *ops)
-{
-	BUG_ON(!dev);
-	dev->dma_ops = ops;
-}
-
 #define HAVE_ARCH_DMA_SUPPORTED 1
 extern int dma_supported(struct device *dev, u64 mask);
 

commit 5657933dbb6e25feaf5d8df8c88f96cdade693a3
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:02 2017 -0800

    treewide: Move dma_ops from struct dev_archdata into struct device
    
    Some but not all architectures provide set_dma_ops(). Move dma_ops
    from struct dev_archdata into struct device such that it becomes
    possible on all architectures to configure dma_ops per device.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 1aabd781306f..312f4d0564d6 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -18,8 +18,8 @@ extern const struct dma_map_ops arm_coherent_dma_ops;
 
 static inline const struct dma_map_ops *__generic_dma_ops(struct device *dev)
 {
-	if (dev && dev->archdata.dma_ops)
-		return dev->archdata.dma_ops;
+	if (dev && dev->dma_ops)
+		return dev->dma_ops;
 	return &arm_dma_ops;
 }
 
@@ -34,7 +34,7 @@ static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 static inline void set_dma_ops(struct device *dev, const struct dma_map_ops *ops)
 {
 	BUG_ON(!dev);
-	dev->archdata.dma_ops = ops;
+	dev->dma_ops = ops;
 }
 
 #define HAVE_ARCH_DMA_SUPPORTED 1

commit 5299709d0a87342dadc1fc9850484fadeb488bf8
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:01 2017 -0800

    treewide: Constify most dma_map_ops structures
    
    Most dma_map_ops structures are never modified. Constify these
    structures such that these can be write-protected. This patch
    has been generated as follows:
    
    git grep -l 'struct dma_map_ops' |
      xargs -d\\n sed -i \
        -e 's/struct dma_map_ops/const struct dma_map_ops/g' \
        -e 's/const struct dma_map_ops {/struct dma_map_ops {/g' \
        -e 's/^const struct dma_map_ops;$/struct dma_map_ops;/' \
        -e 's/const const struct dma_map_ops /const struct dma_map_ops /g';
    sed -i -e 's/const \(struct dma_map_ops intel_dma_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops intel_dma_ops');
    sed -i -e 's/const \(struct dma_map_ops dma_iommu_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops' | grep ^arch/powerpc);
    sed -i -e '/^struct vmd_dev {$/,/^};$/ s/const \(struct dma_map_ops[[:blank:]]dma_ops;\)/\1/' \
           -e '/^static void vmd_setup_dma_ops/,/^}$/ s/const \(struct dma_map_ops \*dest\)/\1/' \
           -e 's/const \(struct dma_map_ops \*dest = \&vmd->dma_ops\)/\1/' \
        drivers/pci/host/*.c
    sed -i -e '/^void __init pci_iommu_alloc(void)$/,/^}$/ s/dma_ops->/intel_dma_ops./' arch/ia64/kernel/pci-dma.c
    sed -i -e 's/static const struct dma_map_ops sn_dma_ops/static struct dma_map_ops sn_dma_ops/' arch/ia64/sn/pci/pci_dma.c
    sed -i -e 's/(const struct dma_map_ops \*)//' drivers/misc/mic/bus/vop_bus.c
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index bf02dbd9ccda..1aabd781306f 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -13,17 +13,17 @@
 #include <asm/xen/hypervisor.h>
 
 #define DMA_ERROR_CODE	(~(dma_addr_t)0x0)
-extern struct dma_map_ops arm_dma_ops;
-extern struct dma_map_ops arm_coherent_dma_ops;
+extern const struct dma_map_ops arm_dma_ops;
+extern const struct dma_map_ops arm_coherent_dma_ops;
 
-static inline struct dma_map_ops *__generic_dma_ops(struct device *dev)
+static inline const struct dma_map_ops *__generic_dma_ops(struct device *dev)
 {
 	if (dev && dev->archdata.dma_ops)
 		return dev->archdata.dma_ops;
 	return &arm_dma_ops;
 }
 
-static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+static inline const struct dma_map_ops *get_dma_ops(struct device *dev)
 {
 	if (xen_initial_domain())
 		return xen_dma_ops;
@@ -31,7 +31,7 @@ static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 		return __generic_dma_ops(dev);
 }
 
-static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
+static inline void set_dma_ops(struct device *dev, const struct dma_map_ops *ops)
 {
 	BUG_ON(!dev);
 	dev->archdata.dma_ops = ops;

commit d248220f0465b818887baa9829e691fe662b2c5e
Author: Roger Quadros <rogerq@ti.com>
Date:   Thu Sep 29 08:32:55 2016 +0100

    ARM: 8617/1: dma: fix dma_max_pfn()
    
    Since commit 6ce0d2001692 ("ARM: dma: Use dma_pfn_offset for dma address translation"),
    dma_to_pfn() already returns the PFN with the physical memory start offset
    so we don't need to add it again.
    
    This fixes USB mass storage lock-up problem on systems that can't do DMA
    over the entire physical memory range (e.g.) Keystone 2 systems with 4GB RAM
    can only do DMA over the first 2GB. [K2E-EVM].
    
    What happens there is that without this patch SCSI layer sets a wrong
    bounce buffer limit in scsi_calculate_bounce_limit() for the USB mass
    storage device. dma_max_pfn() evaluates to 0x8fffff and bounce_limit
    is set to 0x8fffff000 whereas maximum DMA'ble physical memory on Keystone 2
    is 0x87fffffff. This results in non DMA'ble pages being given to the
    USB controller and hence the lock-up.
    
    NOTE: in the above case, USB-SCSI-device's dma_pfn_offset was showing as 0.
    This should have really been 0x780000 as on K2e, LOWMEM_START is 0x80000000
    and HIGHMEM_START is 0x800000000. DMA zone is 2GB so dma_max_pfn should be
    0x87ffff. The incorrect dma_pfn_offset for the USB storage device is because
    USB devices are not correctly inheriting the dma_pfn_offset from the
    USB host controller. This will be fixed by a separate patch.
    
    Fixes: 6ce0d2001692 ("ARM: dma: Use dma_pfn_offset for dma address translation")
    Cc: stable@vger.kernel.org
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Reported-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Signed-off-by: Roger Quadros <rogerq@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index d009f7911ffc..bf02dbd9ccda 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -111,7 +111,7 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 /* The ARM override for dma_max_pfn() */
 static inline unsigned long dma_max_pfn(struct device *dev)
 {
-	return PHYS_PFN_OFFSET + dma_to_pfn(dev, *dev->dma_mask);
+	return dma_to_pfn(dev, *dev->dma_mask);
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 

commit 00085f1efa387a8ce100e3734920f7639c80caa3
Author: Krzysztof Kozlowski <k.kozlowski@samsung.com>
Date:   Wed Aug 3 13:46:00 2016 -0700

    dma-mapping: use unsigned long for dma_attrs
    
    The dma-mapping core and the implementations do not change the DMA
    attributes passed by pointer.  Thus the pointer can point to const data.
    However the attributes do not have to be a bitfield.  Instead unsigned
    long will do fine:
    
    1. This is just simpler.  Both in terms of reading the code and setting
       attributes.  Instead of initializing local attributes on the stack
       and passing pointer to it to dma_set_attr(), just set the bits.
    
    2. It brings safeness and checking for const correctness because the
       attributes are passed by value.
    
    Semantic patches for this change (at least most of them):
    
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
    
        @@
        f(...,
        - struct dma_attrs *attrs
        + unsigned long attrs
        , ...)
        {
        ...
        }
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    and
    
        // Options: --all-includes
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
        type t;
    
        @@
        t f(..., struct dma_attrs *attrs);
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    Link: http://lkml.kernel.org/r/1468399300-5399-2-git-send-email-k.kozlowski@samsung.com
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no>
    Acked-by: Mark Salter <msalter@redhat.com> [c6x]
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com> [cris]
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch> [drm]
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Fabien Dessenne <fabien.dessenne@st.com> [bdisp]
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com> [vb2-core]
    Acked-by: David Vrabel <david.vrabel@citrix.com> [xen]
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen swiotlb]
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Richard Kuo <rkuo@codeaurora.org> [hexagon]
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> [m68k]
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com> [s390]
    Acked-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no> [avr32]
    Acked-by: Vineet Gupta <vgupta@synopsys.com> [arc]
    Acked-by: Robin Murphy <robin.murphy@arm.com> [arm64 and dma-iommu]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index a83570f10124..d009f7911ffc 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -5,7 +5,6 @@
 
 #include <linux/mm_types.h>
 #include <linux/scatterlist.h>
-#include <linux/dma-attrs.h>
 #include <linux/dma-debug.h>
 
 #include <asm/memory.h>
@@ -174,7 +173,7 @@ static inline void dma_mark_clean(void *addr, size_t size) { }
  * to be the device-viewed address.
  */
 extern void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
-			   gfp_t gfp, struct dma_attrs *attrs);
+			   gfp_t gfp, unsigned long attrs);
 
 /**
  * arm_dma_free - free memory allocated by arm_dma_alloc
@@ -191,7 +190,7 @@ extern void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
  * during and after this call executing are illegal.
  */
 extern void arm_dma_free(struct device *dev, size_t size, void *cpu_addr,
-			 dma_addr_t handle, struct dma_attrs *attrs);
+			 dma_addr_t handle, unsigned long attrs);
 
 /**
  * arm_dma_mmap - map a coherent DMA allocation into user space
@@ -208,7 +207,7 @@ extern void arm_dma_free(struct device *dev, size_t size, void *cpu_addr,
  */
 extern int arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 			void *cpu_addr, dma_addr_t dma_addr, size_t size,
-			struct dma_attrs *attrs);
+			unsigned long attrs);
 
 /*
  * This can be called during early boot to increase the size of the atomic
@@ -262,16 +261,16 @@ extern void dmabounce_unregister_dev(struct device *);
  * The scatter list versions of the above methods.
  */
 extern int arm_dma_map_sg(struct device *, struct scatterlist *, int,
-		enum dma_data_direction, struct dma_attrs *attrs);
+		enum dma_data_direction, unsigned long attrs);
 extern void arm_dma_unmap_sg(struct device *, struct scatterlist *, int,
-		enum dma_data_direction, struct dma_attrs *attrs);
+		enum dma_data_direction, unsigned long attrs);
 extern void arm_dma_sync_sg_for_cpu(struct device *, struct scatterlist *, int,
 		enum dma_data_direction);
 extern void arm_dma_sync_sg_for_device(struct device *, struct scatterlist *, int,
 		enum dma_data_direction);
 extern int arm_dma_get_sgtable(struct device *dev, struct sg_table *sgt,
 		void *cpu_addr, dma_addr_t dma_addr, size_t size,
-		struct dma_attrs *attrs);
+		unsigned long attrs);
 
 #endif /* __KERNEL__ */
 #endif

commit a1c28b75a95808161cacbb3531c418abe248994e
Merge: a05a70db34ba 5632a9fbcd45
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 10:01:38 2016 -0700

    Merge branch 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
     "Changes included in this pull request:
    
       - revert pxa2xx-flash back to using ioremap_cached() and switch
         memremap() to use arch_memremap_wb()
    
       - remove pci=firmware command line argument handling
    
       - remove unnecessary arm_dma_set_mask() implementation, the generic
         implementation will do for ARM
    
       - removal of the ARM kallsyms "hack" to work around mode switching
         veneers and vectors located below PAGE_OFFSET
    
       - tidy up build system output a little
    
       - add L2 cache power management DT bindings
    
       - remove duplicated local_irq_disable() in reboot paths
    
       - handle AMBA primecell devices better at registration time with PM
         domains (needed for Samsung SoCs)
    
       - ARM specific preparation to support Keystone II kexec"
    
    * 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm:
      ARM: 8567/1: cache-uniphier: activate ways for secondary CPUs
      ARM: 8570/2: Documentation: devicetree: Add PL310 PM bindings
      ARM: 8569/1: pl2x0: Add OF control of cache power management
      ARM: 8568/1: reboot: remove duplicated local_irq_disable()
      ARM: 8566/1: drivers: amba: properly handle devices with power domains
      ARM: provide arm_has_idmap_alias() helper
      ARM: kexec: remove 512MB restriction on kexec crashdump
      ARM: provide improved virt_to_idmap() functionality
      ARM: kexec: fix crashkernel= handling
      ARM: 8557/1: specify install, zinstall, and uinstall as PHONY targets
      ARM: 8562/1: suppress "include/generated/mach-types.h is up to date."
      ARM: 8553/1: kallsyms: remove --page-offset command line option
      ARM: 8552/1: kallsyms: remove special lower address limit for CONFIG_ARM
      ARM: 8555/1: kallsyms: ignore ARM mode switching veneers
      ARM: 8548/1: dma-mapping: remove arm_dma_set_mask()
      ARM: 8554/1: kernel: pci: remove pci=firmware command line parameter handling
      ARM: memremap: implement arch_memremap_wb()
      memremap: add arch specific hook for MEMREMAP_WB mappings
      mtd: pxa2xx-flash: switch back from memremap to ioremap_cached
      ARM: reintroduce ioremap_cached() for creating cached I/O mappings

commit 53c92d793395fdab9edbd2f79b084bb6b2e6ae79
Author: Robin Murphy <Robin.Murphy@arm.com>
Date:   Thu Apr 7 18:42:05 2016 +0100

    iommu: of: enforce const-ness of struct iommu_ops
    
    As a set of driver-provided callbacks and static data, there is no
    compelling reason for struct iommu_ops to be mutable in core code, so
    enforce const-ness throughout.
    
    Acked-by: Thierry Reding <treding@nvidia.com>
    Signed-off-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Joerg Roedel <jroedel@suse.de>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 6ad1ceda62a5..02283eb2f5b2 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -118,7 +118,7 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 
 #define arch_setup_dma_ops arch_setup_dma_ops
 extern void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
-			       struct iommu_ops *iommu, bool coherent);
+			       const struct iommu_ops *iommu, bool coherent);
 
 #define arch_teardown_dma_ops arch_teardown_dma_ops
 extern void arch_teardown_dma_ops(struct device *dev);

commit b67dd2e9bdc0f79a3389e57c51189ed2c2127c00
Author: Alexandre Courbot <acourbot@nvidia.com>
Date:   Mon Mar 7 03:35:56 2016 +0100

    ARM: 8548/1: dma-mapping: remove arm_dma_set_mask()
    
    arm_dma_set_mask() implements exactly the same behavior as the fallback
    that dma_set_mask() takes if the set_dma_mask op is not set. Remove it
    and use that fallback instead like what is already done for
    dma_get_mask().
    
    Signed-off-by: Alexandre Courbot <acourbot@nvidia.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 6ad1ceda62a5..0f2034d28885 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -162,8 +162,6 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 
 static inline void dma_mark_clean(void *addr, size_t size) { }
 
-extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);
-
 /**
  * arm_dma_alloc - allocate consistent memory for DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

commit e1c7e324539ada3b2b13ca2898bcb4948a9ef9db
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jan 20 15:02:05 2016 -0800

    dma-mapping: always provide the dma_map_ops based implementation
    
    Move the generic implementation to <linux/dma-mapping.h> now that all
    architectures support it and remove the HAVE_DMA_ATTR Kconfig symbol now
    that everyone supports them.
    
    [valentinrothberg@gmail.com: remove leftovers in Kconfig]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Helge Deller <deller@gmx.de>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Joerg Roedel <jroedel@suse.de>
    Cc: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Valentin Rothberg <valentinrothberg@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index ccb3aa64640d..6ad1ceda62a5 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -41,13 +41,6 @@ static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 #define HAVE_ARCH_DMA_SUPPORTED 1
 extern int dma_supported(struct device *dev, u64 mask);
 
-/*
- * Note that while the generic code provides dummy dma_{alloc,free}_noncoherent
- * implementations, we don't provide a dma_cache_sync function so drivers using
- * this API are highlighted with build warnings.
- */
-#include <asm-generic/dma-mapping-common.h>
-
 #ifdef __arch_page_to_dma
 #error Please update to __arch_pfn_to_dma
 #endif

commit 452e06af1f0149b01201f94264d452cd7a95db7a
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 9 15:39:53 2015 -0700

    dma-mapping: consolidate dma_set_mask
    
    Almost everyone implements dma_set_mask the same way, although some time
    that's hidden in ->set_dma_mask methods.
    
    This patch consolidates those into a common implementation that either
    calls ->set_dma_mask if present or otherwise uses the default
    implementation.  Some architectures used to only call ->set_dma_mask
    after the initial checks, and those instance have been fixed to do the
    full work.  h8300 implemented dma_set_mask bogusly as a no-ops and has
    been fixed.
    
    Unfortunately some architectures overload unrelated semantics like changing
    the dma_ops into it so we still need to allow for an architecture override
    for now.
    
    [jcmvbkbc@gmail.com: fix xtensa]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 2f9c731691c0..ccb3aa64640d 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -48,11 +48,6 @@ extern int dma_supported(struct device *dev, u64 mask);
  */
 #include <asm-generic/dma-mapping-common.h>
 
-static inline int dma_set_mask(struct device *dev, u64 mask)
-{
-	return get_dma_ops(dev)->set_dma_mask(dev, mask);
-}
-
 #ifdef __arch_page_to_dma
 #error Please update to __arch_pfn_to_dma
 #endif

commit ee196371d5cb1942ebdccc16bdce389812aa265e
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 9 15:39:49 2015 -0700

    dma-mapping: consolidate dma_supported
    
    Most architectures just call into ->dma_supported, but some also return 1
    if the method is not present, or 0 if no dma ops are present (although
    that should never happeb). Consolidate this more broad version into
    common code.
    
    Also fix h8300 which inorrectly always returned 0, which would have been
    a problem if it's dma_set_mask implementation wasn't a similarly buggy
    noop.
    
    As a few architectures have much more elaborate implementations, we
    still allow for arch overrides.
    
    [jcmvbkbc@gmail.com: fix xtensa]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 9bef3c541c39..2f9c731691c0 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -38,12 +38,14 @@ static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 	dev->archdata.dma_ops = ops;
 }
 
+#define HAVE_ARCH_DMA_SUPPORTED 1
+extern int dma_supported(struct device *dev, u64 mask);
+
 /*
  * Note that while the generic code provides dummy dma_{alloc,free}_noncoherent
  * implementations, we don't provide a dma_cache_sync function so drivers using
  * this API are highlighted with build warnings.
  */
-
 #include <asm-generic/dma-mapping-common.h>
 
 static inline int dma_set_mask(struct device *dev, u64 mask)
@@ -172,8 +174,6 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 
 static inline void dma_mark_clean(void *addr, size_t size) { }
 
-extern int dma_supported(struct device *dev, u64 mask);
-
 extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);
 
 /**

commit efa21e432c7b3c8ae976039d614a017799b6e874
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 9 15:39:46 2015 -0700

    dma-mapping: cosolidate dma_mapping_error
    
    Currently there are three valid implementations of dma_mapping_error:
    
     (1) call ->mapping_error
     (2) check for a hardcoded error code
     (3) always return 0
    
    This patch provides a common implementation that calls ->mapping_error
    if present, then checks for DMA_ERROR_CODE if defined or otherwise
    returns 0.
    
    [jcmvbkbc@gmail.com: fix xtensa]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 0b7787167b64..9bef3c541c39 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -172,15 +172,6 @@ static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
 
 static inline void dma_mark_clean(void *addr, size_t size) { }
 
-/*
- * DMA errors are defined by all-bits-set in the DMA address.
- */
-static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
-{
-	debug_dma_mapping_error(dev, dma_addr);
-	return dma_addr == DMA_ERROR_CODE;
-}
-
 extern int dma_supported(struct device *dev, u64 mask);
 
 extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);

commit 1e8937526e2309d48fccd81bb30a590ac21a5516
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 9 15:39:42 2015 -0700

    dma-mapping: consolidate dma_{alloc,free}_noncoherent
    
    Most architectures do not support non-coherent allocations and either
    define dma_{alloc,free}_noncoherent to their coherent versions or stub
    them out.
    
    Openrisc uses dma_{alloc,free}_attrs to implement them, and only Mips
    implements them directly.
    
    This patch moves the Openrisc version to common code, and handles the
    DMA_ATTR_NON_CONSISTENT case in the mips dma_map_ops instance.
    
    Note that actual non-coherent allocations require a dma_cache_sync
    implementation, so if non-coherent allocations didn't work on
    an architecture before this patch they still won't work after it.
    
    [jcmvbkbc@gmail.com: fix xtensa]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index bc404473f1ca..0b7787167b64 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -38,6 +38,12 @@ static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 	dev->archdata.dma_ops = ops;
 }
 
+/*
+ * Note that while the generic code provides dummy dma_{alloc,free}_noncoherent
+ * implementations, we don't provide a dma_cache_sync function so drivers using
+ * this API are highlighted with build warnings.
+ */
+
 #include <asm-generic/dma-mapping-common.h>
 
 static inline int dma_set_mask(struct device *dev, u64 mask)
@@ -175,21 +181,6 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 	return dma_addr == DMA_ERROR_CODE;
 }
 
-/*
- * Dummy noncoherent implementation.  We don't provide a dma_cache_sync
- * function so drivers using this API are highlighted with build warnings.
- */
-static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
-		dma_addr_t *handle, gfp_t gfp)
-{
-	return NULL;
-}
-
-static inline void dma_free_noncoherent(struct device *dev, size_t size,
-		void *cpu_addr, dma_addr_t handle)
-{
-}
-
 extern int dma_supported(struct device *dev, u64 mask);
 
 extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);

commit 6894258eda2f9badc28c878086c0e54bd5b7fb30
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Sep 9 15:39:39 2015 -0700

    dma-mapping: consolidate dma_{alloc,free}_{attrs,coherent}
    
    Since 2009 we have a nice asm-generic header implementing lots of DMA API
    functions for architectures using struct dma_map_ops, but unfortunately
    it's still missing a lot of APIs that all architectures still have to
    duplicate.
    
    This series consolidates the remaining functions, although we still need
    arch opt outs for two of them as a few architectures have very
    non-standard implementations.
    
    This patch (of 5):
    
    The coherent DMA allocator works the same over all architectures supporting
    dma_map operations.
    
    This patch consolidates them and converges the minor differences:
    
     - the debug_dma helpers are now called from all architectures, including
       those that were previously missing them
     - dma_alloc_from_coherent and dma_release_from_coherent are now always
       called from the generic alloc/free routines instead of the ops
       dma-mapping-common.h always includes dma-coherent.h to get the defintions
       for them, or the stubs if the architecture doesn't support this feature
     - checks for ->alloc / ->free presence are removed.  There is only one
       magic instead of dma_map_ops without them (mic_dma_ops) and that one
       is x86 only anyway.
    
    Besides that only x86 needs special treatment to replace a default devices
    if none is passed and tweak the gfp_flags.  An optional arch hook is provided
    for that.
    
    [linux@roeck-us.net: fix build]
    [jcmvbkbc@gmail.com: fix xtensa]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Andy Shevchenko <andy.shevchenko@gmail.com>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Signed-off-by: Max Filippov <jcmvbkbc@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index a68b9d8a71fe..bc404473f1ca 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -8,7 +8,6 @@
 #include <linux/dma-attrs.h>
 #include <linux/dma-debug.h>
 
-#include <asm-generic/dma-coherent.h>
 #include <asm/memory.h>
 
 #include <xen/xen.h>
@@ -209,21 +208,6 @@ extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);
 extern void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
 			   gfp_t gfp, struct dma_attrs *attrs);
 
-#define dma_alloc_coherent(d, s, h, f) dma_alloc_attrs(d, s, h, f, NULL)
-
-static inline void *dma_alloc_attrs(struct device *dev, size_t size,
-				       dma_addr_t *dma_handle, gfp_t flag,
-				       struct dma_attrs *attrs)
-{
-	struct dma_map_ops *ops = get_dma_ops(dev);
-	void *cpu_addr;
-	BUG_ON(!ops);
-
-	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
-	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
-	return cpu_addr;
-}
-
 /**
  * arm_dma_free - free memory allocated by arm_dma_alloc
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -241,19 +225,6 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,
 extern void arm_dma_free(struct device *dev, size_t size, void *cpu_addr,
 			 dma_addr_t handle, struct dma_attrs *attrs);
 
-#define dma_free_coherent(d, s, c, h) dma_free_attrs(d, s, c, h, NULL)
-
-static inline void dma_free_attrs(struct device *dev, size_t size,
-				     void *cpu_addr, dma_addr_t dma_handle,
-				     struct dma_attrs *attrs)
-{
-	struct dma_map_ops *ops = get_dma_ops(dev);
-	BUG_ON(!ops);
-
-	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
-	ops->free(dev, size, cpu_addr, dma_handle, attrs);
-}
-
 /**
  * arm_dma_mmap - map a coherent DMA allocation into user space
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

commit 96231b2686b53f71838a335bdc404cb5285d1a01
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Tue Aug 18 09:14:15 2015 +0100

    ARM: 8419/1: dma-mapping: harmonize definition of DMA_ERROR_CODE
    
    All architectures except arm that define DMA_ERROR_CODE are casting it
    to (dma_addr_t) - as it is always compared to dma_addr_t in arm as well
    this could be harmonized.
    
    Signed-off-by: Nicholas Mc Guire <hofrat@osadl.org>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index b52101d37ec7..a68b9d8a71fe 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -14,7 +14,7 @@
 #include <xen/xen.h>
 #include <asm/xen/hypervisor.h>
 
-#define DMA_ERROR_CODE	(~0)
+#define DMA_ERROR_CODE	(~(dma_addr_t)0x0)
 extern struct dma_map_ops arm_dma_ops;
 extern struct dma_map_ops arm_coherent_dma_ops;
 

commit 6f51ee709e4c6b56f2c2a071da2d056a109b9d26
Merge: 205dc205ed3b fd522d279235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 16 14:53:01 2014 -0800

    Merge tag 'iommu-config-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC/iommu configuration update from Arnd Bergmann:
     "The iomm-config branch contains work from Will Deacon, quoting his
      description:
    
        This series adds automatic IOMMU and DMA-mapping configuration for
        OF-based DMA masters described using the generic IOMMU devicetree
        bindings. Although there is plenty of future work around splitting up
        iommu_ops, adding default IOMMU domains and sorting out automatic IOMMU
        group creation for the platform_bus, this is already useful enough for
        people to port over their IOMMU drivers and start using the new probing
        infrastructure (indeed, Marek has patches queued for the Exynos IOMMU).
    
      The branch touches core ARM and IOMMU driver files, and the respective
      maintainers (Russell King and Joerg Roedel) agreed to have the
      contents merged through the arm-soc tree.
    
      The final version was ready just before the merge window, so we ended
      up delaying it a bit longer than the rest, but we don't expect to see
      regressions because this is just additional infrastructure that will
      get used in drivers starting in 3.20 but is unused so far"
    
    * tag 'iommu-config-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc:
      iommu: store DT-probed IOMMU data privately
      arm: dma-mapping: plumb our iommu mapping ops into arch_setup_dma_ops
      arm: call iommu_init before of_platform_populate
      dma-mapping: detect and configure IOMMU in of_dma_configure
      iommu: fix initialization without 'add_device' callback
      iommu: provide helper function to configure an IOMMU for an of master
      iommu: add new iommu_ops callback for adding an OF device
      dma-mapping: replace set_arch_dma_coherent_ops with arch_setup_dma_ops
      iommu: provide early initialisation hook for IOMMU drivers

commit 3d5391ac6f5e3cb1ab01e23851140d3f6e9ab573
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Thu Nov 20 10:41:40 2014 +0000

    arm: introduce is_device_dma_coherent
    
    Introduce a boolean flag and an accessor function to check whether a
    device is dma_coherent. Set the flag from set_arch_dma_coherent_ops.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Russell King <linux@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 85738b200023..e6e3446abdf6 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -123,11 +123,18 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 
 static inline int set_arch_dma_coherent_ops(struct device *dev)
 {
+	dev->archdata.dma_coherent = true;
 	set_dma_ops(dev, &arm_coherent_dma_ops);
 	return 0;
 }
 #define set_arch_dma_coherent_ops(dev)	set_arch_dma_coherent_ops(dev)
 
+/* do not use this function in a driver */
+static inline bool is_device_dma_coherent(struct device *dev)
+{
+	return dev->archdata.dma_coherent;
+}
+
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
 	unsigned int offset = paddr & ~PAGE_MASK;

commit 4bb25789ed28228a52c030bf28edb2fcdb214be8
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 27 17:52:44 2014 +0100

    arm: dma-mapping: plumb our iommu mapping ops into arch_setup_dma_ops
    
    This patch plumbs the existing ARM IOMMU DMA infrastructure (which isn't
    actually called outside of a few drivers) into arch_setup_dma_ops, so
    that we can use IOMMUs for DMA transfers in a more generic fashion.
    
    Since this significantly complicates the arch_setup_dma_ops function,
    it is moved out of line into dma-mapping.c. If CONFIG_ARM_DMA_USE_IOMMU
    is not set, the iommu parameter is ignored and the normal ops are used
    instead.
    
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index f3c0d953f6a2..9410b7e548fc 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -121,14 +121,12 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 
-static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
-				      u64 size, struct iommu_ops *iommu,
-				      bool coherent)
-{
-	if (coherent)
-		set_dma_ops(dev, &arm_coherent_dma_ops);
-}
 #define arch_setup_dma_ops arch_setup_dma_ops
+extern void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
+			       struct iommu_ops *iommu, bool coherent);
+
+#define arch_teardown_dma_ops arch_teardown_dma_ops
+extern void arch_teardown_dma_ops(struct device *dev);
 
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {

commit 97890ba9289c66e23f2f2d431937693b6498d6f6
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 27 16:24:20 2014 +0100

    dma-mapping: detect and configure IOMMU in of_dma_configure
    
    This patch extends of_dma_configure so that it sets up the IOMMU for a
    device, as well as the coherent/non-coherent DMA mapping ops.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index dc3420e77758..f3c0d953f6a2 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -121,7 +121,9 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 
-static inline void arch_setup_dma_ops(struct device *dev, bool coherent)
+static inline void arch_setup_dma_ops(struct device *dev, u64 dma_base,
+				      u64 size, struct iommu_ops *iommu,
+				      bool coherent)
 {
 	if (coherent)
 		set_dma_ops(dev, &arm_coherent_dma_ops);

commit a3a60f81ee6f8fa65a57fa186b395bcd1f1bb097
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Aug 27 15:49:10 2014 +0100

    dma-mapping: replace set_arch_dma_coherent_ops with arch_setup_dma_ops
    
    set_arch_dma_coherent_ops is called from of_dma_configure in order to
    swizzle the architectural dma-mapping functions over to a cache-coherent
    implementation. This is currently implemented only for ARM.
    
    In anticipation of re-using this mechanism for IOMMU-backed dma-mapping
    ops too, this patch replaces the function with a broader
    arch_setup_dma_ops callback which will be extended in future.
    
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Tested-by: Robin Murphy <robin.murphy@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 85738b200023..dc3420e77758 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -121,12 +121,12 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 
-static inline int set_arch_dma_coherent_ops(struct device *dev)
+static inline void arch_setup_dma_ops(struct device *dev, bool coherent)
 {
-	set_dma_ops(dev, &arm_coherent_dma_ops);
-	return 0;
+	if (coherent)
+		set_dma_ops(dev, &arm_coherent_dma_ops);
 }
-#define set_arch_dma_coherent_ops(dev)	set_arch_dma_coherent_ops(dev)
+#define arch_setup_dma_ops arch_setup_dma_ops
 
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {

commit b4bbb107d73bbc0d92c9ae7fd8e69580aa9381e7
Author: Thierry Reding <treding@nvidia.com>
Date:   Fri Jun 27 11:56:58 2014 +0200

    dma-mapping: Provide write-combine allocations
    
    Provide an implementation for dma_{alloc,free,mmap}_writecombine() when
    the architecture supports DMA attributes.
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index c45b61a4b4a5..85738b200023 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -265,22 +265,6 @@ extern int arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 			void *cpu_addr, dma_addr_t dma_addr, size_t size,
 			struct dma_attrs *attrs);
 
-static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
-				       dma_addr_t *dma_handle, gfp_t flag)
-{
-	DEFINE_DMA_ATTRS(attrs);
-	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
-	return dma_alloc_attrs(dev, size, dma_handle, flag, &attrs);
-}
-
-static inline void dma_free_writecombine(struct device *dev, size_t size,
-				     void *cpu_addr, dma_addr_t dma_handle)
-{
-	DEFINE_DMA_ATTRS(attrs);
-	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
-	return dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
-}
-
 /*
  * This can be called during early boot to increase the size of the atomic
  * coherent DMA pool above the default value of 256KiB. It must be called

commit 812b99e4b024d6f83e9281ec1a0bd4bf63dad90f
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Thu Apr 24 11:30:06 2014 -0400

    ARM: dma: implement set_arch_dma_coherent_ops()
    
    Implement the set_arch_dma_coherent_ops() for ARM architecture.
    
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index b0c79fdd9375..c45b61a4b4a5 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -121,6 +121,13 @@ static inline unsigned long dma_max_pfn(struct device *dev)
 }
 #define dma_max_pfn(dev) dma_max_pfn(dev)
 
+static inline int set_arch_dma_coherent_ops(struct device *dev)
+{
+	set_dma_ops(dev, &arm_coherent_dma_ops);
+	return 0;
+}
+#define set_arch_dma_coherent_ops(dev)	set_arch_dma_coherent_ops(dev)
+
 static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
 {
 	unsigned int offset = paddr & ~PAGE_MASK;

commit 6ce0d20016925d031f1e24d64302e4c976d7cec6
Author: Grygorii Strashko <grygorii.strashko@ti.com>
Date:   Thu Apr 24 11:30:05 2014 -0400

    ARM: dma: Use dma_pfn_offset for dma address translation
    
    In most of cases DMA addresses can be performed using offset value of
     Bus address space relatively to physical address space as following:
    
    PFN->DMA:
     __pfn_to_phys(pfn + [-]dma_pfn_offset)
    
    DMA->PFN:
     __phys_to_pfn(dma_addr) + [-]dma_pfn_offset
    
    Thanks to Russell King for suggesting the optimised macro's for
    conversion.
    
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Grant Likely <grant.likely@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Linus Walleij <linus.walleij@linaro.org>
    Reviewed-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index e701a4d9aa59..b0c79fdd9375 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -58,21 +58,37 @@ static inline int dma_set_mask(struct device *dev, u64 mask)
 #ifndef __arch_pfn_to_dma
 static inline dma_addr_t pfn_to_dma(struct device *dev, unsigned long pfn)
 {
+	if (dev)
+		pfn -= dev->dma_pfn_offset;
 	return (dma_addr_t)__pfn_to_bus(pfn);
 }
 
 static inline unsigned long dma_to_pfn(struct device *dev, dma_addr_t addr)
 {
-	return __bus_to_pfn(addr);
+	unsigned long pfn = __bus_to_pfn(addr);
+
+	if (dev)
+		pfn += dev->dma_pfn_offset;
+
+	return pfn;
 }
 
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
 {
+	if (dev) {
+		unsigned long pfn = dma_to_pfn(dev, addr);
+
+		return phys_to_virt(__pfn_to_phys(pfn));
+	}
+
 	return (void *)__bus_to_virt((unsigned long)addr);
 }
 
 static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 {
+	if (dev)
+		return pfn_to_dma(dev, virt_to_pfn(addr));
+
 	return (dma_addr_t)__virt_to_bus((unsigned long)(addr));
 }
 

commit eda670c626a4f53eb8ac5f20d8c10d3f0b54c583
Merge: b746f9c7941f 18c51e1a3fab
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 13:34:37 2013 +0900

    Merge tag 'stable/for-linus-3.13-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull Xen updates from Konrad Rzeszutek Wilk:
     "This has tons of fixes and two major features which are concentrated
      around the Xen SWIOTLB library.
    
      The short <blurb> is that the tracing facility (just one function) has
      been added to SWIOTLB to make it easier to track I/O progress.
      Additionally under Xen and ARM (32 & 64) the Xen-SWIOTLB driver
      "is used to translate physical to machine and machine to physical
      addresses of foreign[guest] pages for DMA operations" (Stefano) when
      booting under hardware without proper IOMMU.
    
      There are also bug-fixes, cleanups, compile warning fixes, etc.
    
      The commit times for some of the commits is a bit fresh - that is b/c
      we wanted to make sure we have the Ack's from the ARM folks - which
      with the string of back-to-back conferences took a bit of time.  Rest
      assured - the code has been stewing in #linux-next for some time.
    
      Features:
       - SWIOTLB has tracing added when doing bounce buffer.
       - Xen ARM/ARM64 can use Xen-SWIOTLB.  This work allows Linux to
         safely program real devices for DMA operations when running as a
         guest on Xen on ARM, without IOMMU support. [*1]
       - xen_raw_printk works with PVHVM guests if needed.
    
      Bug-fixes:
       - Make memory ballooning work under HVM with large MMIO region.
       - Inform hypervisor of MCFG regions found in ACPI DSDT.
       - Remove deprecated IRQF_DISABLED.
       - Remove deprecated __cpuinit.
    
      [*1]:
      "On arm and arm64 all Xen guests, including dom0, run with second
       stage translation enabled.  As a consequence when dom0 programs a
       device for a DMA operation is going to use (pseudo) physical
       addresses instead machine addresses.  This work introduces two trees
       to track physical to machine and machine to physical mappings of
       foreign pages.  Local pages are assumed mapped 1:1 (physical address
       == machine address).  It enables the SWIOTLB-Xen driver on ARM and
       ARM64, so that Linux can translate physical addresses to machine
       addresses for dma operations when necessary.  " (Stefano)"
    
    * tag 'stable/for-linus-3.13-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip: (32 commits)
      xen/arm: pfn_to_mfn and mfn_to_pfn return the argument if nothing is in the p2m
      arm,arm64/include/asm/io.h: define struct bio_vec
      swiotlb-xen: missing include dma-direction.h
      pci-swiotlb-xen: call pci_request_acs only ifdef CONFIG_PCI
      arm: make SWIOTLB available
      xen: delete new instances of added __cpuinit
      xen/balloon: Set balloon's initial state to number of existing RAM pages
      xen/mcfg: Call PHYSDEVOP_pci_mmcfg_reserved for MCFG areas.
      xen: remove deprecated IRQF_DISABLED
      x86/xen: remove deprecated IRQF_DISABLED
      swiotlb-xen: fix error code returned by xen_swiotlb_map_sg_attrs
      swiotlb-xen: static inline xen_phys_to_bus, xen_bus_to_phys, xen_virt_to_bus and range_straddles_page_boundary
      grant-table: call set_phys_to_machine after mapping grant refs
      arm,arm64: do not always merge biovec if we are running on Xen
      swiotlb: print a warning when the swiotlb is full
      swiotlb-xen: use xen_dma_map/unmap_page, xen_dma_sync_single_for_cpu/device
      xen: introduce xen_dma_map/unmap_page and xen_dma_sync_single_for_cpu/device
      tracing/events: Fix swiotlb tracepoint creation
      swiotlb-xen: use xen_alloc/free_coherent_pages
      xen: introduce xen_alloc/free_coherent_pages
      ...

commit e1d8f62ad49a6a7068aa1bdc30252911d71c4dc4
Merge: bad97817dece 15177608c703
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Nov 8 15:36:09 2013 -0500

    Merge remote-tracking branch 'stefano/swiotlb-xen-9.1' into stable/for-linus-3.13
    
    * stefano/swiotlb-xen-9.1:
      swiotlb-xen: fix error code returned by xen_swiotlb_map_sg_attrs
      swiotlb-xen: static inline xen_phys_to_bus, xen_bus_to_phys, xen_virt_to_bus and range_straddles_page_boundary
      grant-table: call set_phys_to_machine after mapping grant refs
      arm,arm64: do not always merge biovec if we are running on Xen
      swiotlb: print a warning when the swiotlb is full
      swiotlb-xen: use xen_dma_map/unmap_page, xen_dma_sync_single_for_cpu/device
      xen: introduce xen_dma_map/unmap_page and xen_dma_sync_single_for_cpu/device
      swiotlb-xen: use xen_alloc/free_coherent_pages
      xen: introduce xen_alloc/free_coherent_pages
      arm64/xen: get_dma_ops: return xen_dma_ops if we are running as xen_initial_domain
      arm/xen: get_dma_ops: return xen_dma_ops if we are running as xen_initial_domain
      swiotlb-xen: introduce xen_swiotlb_set_dma_mask
      xen/arm,arm64: enable SWIOTLB_XEN
      xen: make xen_create_contiguous_region return the dma address
      xen/x86: allow __set_phys_to_machine for autotranslate guests
      arm/xen,arm64/xen: introduce p2m
      arm64: define DMA_ERROR_CODE
      arm: make SWIOTLB available
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    
    Conflicts:
            arch/arm/include/asm/dma-mapping.h
            drivers/xen/swiotlb-xen.c
    
    [Conflicts arose b/c "arm: make SWIOTLB available" v8 was in Stefano's
    branch, while I had v9 + Ack from Russel. I also fixed up white-space
    issues]

commit fbd989b1d73e3b3565dad5227a581e6f456c895f
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Wed Oct 30 01:21:27 2013 +0000

    arm: make SWIOTLB available
    
    IOMMU_HELPER is needed because SWIOTLB calls iommu_is_span_boundary,
    provided by lib/iommu_helper.c.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    CC: will.deacon@arm.com
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    
    Changes in v9:
    - remove uneeded include asm/cacheflush.h;
    - just return 0 if !dev->dma_mask in dma_capable.
    
    Changes in v8:
    - use __phys_to_pfn and __pfn_to_phys.
    
    Changes in v7:
    - dma_mark_clean: empty implementation;
    - in dma_capable use coherent_dma_mask if dma_mask hasn't been
      allocated.
    
    Changes in v6:
    - check for dev->dma_mask being NULL in dma_capable.
    
    Changes in v5:
    - implement dma_mark_clean using dmac_flush_range.
    
    Changes in v3:
    - dma_capable: do not treat dma_mask as a limit;
    - remove SWIOTLB dependency on NEED_SG_DMA_LENGTH.

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5b579b951503..1ad2c171054b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -86,6 +86,39 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
+{
+	unsigned int offset = paddr & ~PAGE_MASK;
+	return pfn_to_dma(dev, __phys_to_pfn(paddr)) + offset;
+}
+
+static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
+{
+	unsigned int offset = dev_addr & ~PAGE_MASK;
+	return __pfn_to_phys(dma_to_pfn(dev, dev_addr)) + offset;
+}
+
+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
+{
+	u64 limit, mask;
+
+	if (!dev->dma_mask)
+		return 0;
+
+	mask = *dev->dma_mask;
+
+	limit = (mask + 1) & ~mask;
+	if (limit && size > limit)
+		return 0;
+
+	if ((addr | (addr + size - 1)) & ~mask)
+		return 0;
+
+	return 1;
+}
+
+static inline void dma_mark_clean(void *addr, size_t size) { }
+
 /*
  * DMA errors are defined by all-bits-set in the DMA address.
  */

commit 26ba47b18318abe7dadbe9294a611c0e932651d8
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Thu Aug 1 03:12:01 2013 +0100

    ARM: 7805/1: mm: change max*pfn to include the physical offset of memory
    
    Most of the kernel code assumes that max*pfn is maximum pfns because
    the physical start of memory is expected to be PFN0. Since this
    assumption is not true on ARM architectures, the meaning of max*pfn
    is number of memory pages. This is done to keep drivers happy which
    are making use of of these variable to calculate the dma bounce limit
    using dma_mask.
    
    Now since we have a architecture override possibility for DMAable
    maximum pfns, lets make meaning of max*pfns as maximum pnfs on ARM
    as well.
    
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5b579b951503..863cd84eb1a2 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -64,6 +64,7 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 {
 	return (dma_addr_t)__virt_to_bus((unsigned long)(addr));
 }
+
 #else
 static inline dma_addr_t pfn_to_dma(struct device *dev, unsigned long pfn)
 {
@@ -86,6 +87,13 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
+/* The ARM override for dma_max_pfn() */
+static inline unsigned long dma_max_pfn(struct device *dev)
+{
+	return PHYS_PFN_OFFSET + dma_to_pfn(dev, *dev->dma_mask);
+}
+#define dma_max_pfn(dev) dma_max_pfn(dev)
+
 /*
  * DMA errors are defined by all-bits-set in the DMA address.
  */

commit c7e9bc548325f19635e7ac7cd5f3ec587228952e
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Fri Oct 18 16:01:26 2013 +0000

    arm/xen: get_dma_ops: return xen_dma_ops if we are running as xen_initial_domain
    
    We can't simply override arm_dma_ops with xen_dma_ops because devices
    are allowed to have their own dma_ops and they take precedence over
    arm_dma_ops. When running on Xen as initial domain, we always want
    xen_dma_ops to be the one in use.
    
    We introduce __generic_dma_ops to allow xen_dma_ops functions to call
    back to the native implementation.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Suggested-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    CC: will.deacon@arm.com
    CC: linux@arm.linux.org.uk
    
    Changes in v7:
    - return xen_dma_ops only if we are the initial domain;
    - rename __get_dma_ops to __generic_dma_ops.

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 01b5a3d92197..f5945d4e4e9f 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -12,17 +12,28 @@
 #include <asm/memory.h>
 #include <asm/cacheflush.h>
 
+#include <xen/xen.h>
+#include <asm/xen/hypervisor.h>
+
 #define DMA_ERROR_CODE	(~0)
 extern struct dma_map_ops arm_dma_ops;
 extern struct dma_map_ops arm_coherent_dma_ops;
 
-static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+static inline struct dma_map_ops *__generic_dma_ops(struct device *dev)
 {
 	if (dev && dev->archdata.dma_ops)
 		return dev->archdata.dma_ops;
 	return &arm_dma_ops;
 }
 
+static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+{
+	if (xen_initial_domain())
+		return xen_dma_ops;
+	else
+		return __generic_dma_ops(dev);
+}
+
 static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
 {
 	BUG_ON(!dev);

commit 06e6295bcecefea9dc29fc84b5fd6848061365a0
Author: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
Date:   Tue Oct 15 15:47:14 2013 +0000

    arm: make SWIOTLB available
    
    IOMMU_HELPER is needed because SWIOTLB calls iommu_is_span_boundary,
    provided by lib/iommu_helper.c.
    
    Signed-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    CC: will.deacon@arm.com
    CC: linux@arm.linux.org.uk
    
    
    Changes in v8:
    - use __phys_to_pfn and __pfn_to_phys.
    
    Changes in v7:
    - dma_mark_clean: empty implementation;
    - in dma_capable use coherent_dma_mask if dma_mask hasn't been
      allocated.
    
    Changes in v6:
    - check for dev->dma_mask being NULL in dma_capable.
    
    Changes in v5:
    - implement dma_mark_clean using dmac_flush_range.
    
    Changes in v3:
    - dma_capable: do not treat dma_mask as a limit;
    - remove SWIOTLB dependency on NEED_SG_DMA_LENGTH.

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5b579b951503..01b5a3d92197 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -10,6 +10,7 @@
 
 #include <asm-generic/dma-coherent.h>
 #include <asm/memory.h>
+#include <asm/cacheflush.h>
 
 #define DMA_ERROR_CODE	(~0)
 extern struct dma_map_ops arm_dma_ops;
@@ -86,6 +87,42 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)
+{
+	unsigned int offset = paddr & ~PAGE_MASK;
+	return pfn_to_dma(dev, __phys_to_pfn(paddr)) + offset;
+}
+
+static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)
+{
+	unsigned int offset = dev_addr & ~PAGE_MASK;
+	return __pfn_to_phys(dma_to_pfn(dev, dev_addr)) + offset;
+}
+
+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)
+{
+	u64 limit, mask;
+	
+	if (dev->dma_mask)
+		mask = *dev->dma_mask;
+	else 
+		mask = dev->coherent_dma_mask;
+
+	if (mask == 0)
+		return 0;
+
+	limit = (mask + 1) & ~mask;
+	if (limit && size > limit)
+		return 0;
+
+	if ((addr | (addr + size - 1)) & ~mask)
+		return 0;
+
+	return 1;
+}
+
+static inline void dma_mark_clean(void *addr, size_t size) { }
+
 /*
  * DMA errors are defined by all-bits-set in the DMA address.
  */

commit 787314c35fbb97e02823a1b8eb8cfa58f366cd49
Merge: 6491d4d02893 9c6ecf6a3ade
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 20 10:07:25 2012 -0800

    Merge tag 'iommu-updates-v3.8' of git://git.kernel.org/pub/scm/linux/kernel/git/joro/iommu
    
    Pull IOMMU updates from Joerg Roedel:
     "A few new features this merge-window.  The most important one is
      probably, that dma-debug now warns if a dma-handle is not checked with
      dma_mapping_error by the device driver.  This requires minor changes
      to some architectures which make use of dma-debug.  Most of these
      changes have the respective Acks by the Arch-Maintainers.
    
      Besides that there are updates to the AMD IOMMU driver for refactor
      the IOMMU-Groups support and to make sure it does not trigger a
      hardware erratum.
    
      The OMAP changes (for which I pulled in a branch from Tony Lindgren's
      tree) have a conflict in linux-next with the arm-soc tree.  The
      conflict is in the file arch/arm/mach-omap2/clock44xx_data.c which is
      deleted in the arm-soc tree.  It is safe to delete the file too so
      solve the conflict.  Similar changes are done in the arm-soc tree in
      the common clock framework migration.  A missing hunk from the patch
      in the IOMMU tree will be submitted as a seperate patch when the
      merge-window is closed."
    
    * tag 'iommu-updates-v3.8' of git://git.kernel.org/pub/scm/linux/kernel/git/joro/iommu: (29 commits)
      ARM: dma-mapping: support debug_dma_mapping_error
      ARM: OMAP4: hwmod data: ipu and dsp to use parent clocks instead of leaf clocks
      iommu/omap: Adapt to runtime pm
      iommu/omap: Migrate to hwmod framework
      iommu/omap: Keep mmu enabled when requested
      iommu/omap: Remove redundant clock handling on ISR
      iommu/amd: Remove obsolete comment
      iommu/amd: Don't use 512GB pages
      iommu/tegra: smmu: Move bus_set_iommu after probe for multi arch
      iommu/tegra: gart: Move bus_set_iommu after probe for multi arch
      iommu/tegra: smmu: Remove unnecessary PTC/TLB flush all
      tile: dma_debug: add debug_dma_mapping_error support
      sh: dma_debug: add debug_dma_mapping_error support
      powerpc: dma_debug: add debug_dma_mapping_error support
      mips: dma_debug: add debug_dma_mapping_error support
      microblaze: dma-mapping: support debug_dma_mapping_error
      ia64: dma_debug: add debug_dma_mapping_error support
      c6x: dma_debug: add debug_dma_mapping_error support
      ARM64: dma_debug: add debug_dma_mapping_error support
      intel-iommu: Prevent devices with RMRRs from being placed into SI Domain
      ...

commit c2714334b944abbeaaadda8cddde619eff0292a1
Merge: 0beb58783f21 5e5d8999a316
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 14 14:54:26 2012 -0800

    Merge tag 'mvebu' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull ARM SoC updates for Marvell mvebu/kirkwood from Olof Johansson:
     "This is a branch with updates for Marvell's mvebu/kirkwood platforms.
      They came in late-ish, and were heavily interdependent such that it
      didn't make sense to split them up across the cross-platform topic
      branches.  So here they are (for the second release in a row) in a
      branch on their own."
    
    * tag 'mvebu' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (88 commits)
      arm: l2x0: add aurora related properties to OF binding
      arm: mvebu: add Aurora L2 Cache Controller to the DT
      arm: mvebu: add L2 cache support
      dma: mv_xor: fix error handling path
      dma: mv_xor: fix error checking of irq_of_parse_and_map()
      dma: mv_xor: use request_irq() instead of devm_request_irq()
      dma: mv_xor: clear the window override control registers
      arm: mvebu: fix address decoding armada_cfg_base() function
      ARM: mvebu: update defconfig with I2C and RTC support
      ARM: mvebu: Add SATA support for OpenBlocks AX3-4
      ARM: mvebu: Add support for the RTC in OpenBlocks AX3-4
      ARM: mvebu: Add support for I2C on OpenBlocks AX3-4
      ARM: mvebu: Add support for I2C controllers in Armada 370/XP
      arm: mvebu: Add hardware I/O Coherency support
      arm: plat-orion: Add coherency attribute when setup mbus target
      arm: dma mapping: Export a dma ops function arm_dma_set_mask
      arm: mvebu: Add SMP support for Armada XP
      arm: mm: Add support for PJ4B cpu and init routines
      arm: mvebu: Add IPI support via doorbells
      arm: mvebu: Add initial support for power managmement service unit
      ...

commit a0157573041403e7507a6f3f32279fc14ff5c02e
Author: Ming Lei <ming.lei@canonical.com>
Date:   Mon Oct 22 20:44:03 2012 +0800

    ARM: dma-mapping: support debug_dma_mapping_error
    
    Without the patch, kind of below warning will be dumped if DMA-API
    debug is enabled:
    
    [   11.069763] ------------[ cut here ]------------
    [   11.074645] WARNING: at lib/dma-debug.c:948 check_unmap+0x770/0x860()
    [   11.081420] ehci-omap ehci-omap.0: DMA-API: device driver failed to
    check map error[device address=0x0000000
    0adb78e80] [size=8 bytes] [mapped as single]
    [   11.095611] Modules linked in:
    
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Joerg Roedel <joro@8bytes.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 23004847bb05..78d8e9b5544f 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -91,6 +91,7 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
  */
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
+	debug_dma_mapping_error(dev, dma_addr);
 	return dma_addr == DMA_ERROR_CODE;
 }
 

commit 87b54e786afda828984645a8364a228ae8ac71f4
Author: Gregory CLEMENT <gregory.clement@free-electrons.com>
Date:   Wed Nov 21 09:39:19 2012 +0100

    arm: dma mapping: Export a dma ops function arm_dma_set_mask
    
    Expose another DMA operations function: arm_dma_set_mask. This
    function will be added to a custom DMA ops for Armada 370/XP.
    Depending of its configuration Armada 370/XP can be set as a "nearly"
    coherent architecture. In this case the DMA ops is made of:
    - specific functions for this architecture
    - already exposed arm DMA related functions
    - the arm_dma_set_mask which was not exposed yet.
    
    Signed-off-by: Gregory CLEMENT <gregory.clement@free-electrons.com>
    Acked-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 23004847bb05..98d4dabb2c10 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -111,6 +111,8 @@ static inline void dma_free_noncoherent(struct device *dev, size_t size,
 
 extern int dma_supported(struct device *dev, u64 mask);
 
+extern int arm_dma_set_mask(struct device *dev, u64 dma_mask);
+
 /**
  * arm_dma_alloc - allocate consistent memory for DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices

commit 16cf8a80a8f0f4757427b17cdfb6c4897674db68
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Nov 8 08:46:23 2012 +0100

    ARM: dma-mapping: remove init_consistent_dma_size() stub
    
    Since commit e9da6e9905e639b0 ("ARM: dma-mapping: remove custom consistent
    dma region") setting consistent dma memory size is not longer required. All
    calls to this function has been already removed, so the
    init_consistent_dma_size() stub can also be gone.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 23004847bb05..8ea02ac3ec1a 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -210,13 +210,6 @@ static inline void dma_free_writecombine(struct device *dev, size_t size,
  */
 extern void __init init_dma_coherent_pool_size(unsigned long size);
 
-/*
- * This can be called during boot to increase the size of the consistent
- * DMA region above it's default value of 2MB. It must be called before the
- * memory allocator is initialised, i.e. before any core_initcall.
- */
-static inline void init_consistent_dma_size(unsigned long size) { }
-
 /*
  * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
  * and utilize bounce buffers as needed to work around limited DMA windows.

commit 697575896670ba9e76760ce8bbc1f5a3001967d6
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Oct 26 07:39:13 2012 +0200

    Revert "ARM: dma-mapping: support debug_dma_mapping_error"
    
    This reverts commit 871ae57adc5ed092c1341f411514d0e8482e2611, which is
    scheduled for v3.8 and accidently got into v3.7-rc series.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 78d8e9b5544f..23004847bb05 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -91,7 +91,6 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
  */
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
-	debug_dma_mapping_error(dev, dma_addr);
 	return dma_addr == DMA_ERROR_CODE;
 }
 

commit 871ae57adc5ed092c1341f411514d0e8482e2611
Author: Ming Lei <ming.lei@canonical.com>
Date:   Mon Oct 22 20:44:03 2012 +0800

    ARM: dma-mapping: support debug_dma_mapping_error
    
    Without the patch, kind of below warning will be dumped if DMA-API
    debug is enabled:
    
    [   11.069763] ------------[ cut here ]------------
    [   11.074645] WARNING: at lib/dma-debug.c:948 check_unmap+0x770/0x860()
    [   11.081420] ehci-omap ehci-omap.0: DMA-API: device driver failed to
    check map error[device address=0x0000000
    0adb78e80] [size=8 bytes] [mapped as single]
    [   11.095611] Modules linked in:
    
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Ming Lei <ming.lei@canonical.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 23004847bb05..78d8e9b5544f 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -91,6 +91,7 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
  */
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
+	debug_dma_mapping_error(dev, dma_addr);
 	return dma_addr == DMA_ERROR_CODE;
 }
 

commit dd37e9405a8e85be49a60b2530efeb5f06bcb753
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Tue Aug 21 12:20:17 2012 +0200

    ARM: add coherent dma ops
    
    arch_is_coherent is problematic as it is a global symbol. This
    doesn't work for multi-platform kernels or platforms which can support
    per device coherent DMA.
    
    This adds arm_coherent_dma_ops to be used for devices which connected
    coherently (i.e. to the ACP port on Cortex-A9 or A15). The arm_dma_ops
    are modified at boot when arch_is_coherent is true.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5c44dcb0987b..23004847bb05 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -13,6 +13,7 @@
 
 #define DMA_ERROR_CODE	(~0)
 extern struct dma_map_ops arm_dma_ops;
+extern struct dma_map_ops arm_coherent_dma_ops;
 
 static inline struct dma_map_ops *get_dma_ops(struct device *dev)
 {

commit 6e5267aa543817015edb4a65c66e15f9809f92bd
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Aug 20 11:19:25 2012 +0200

    ARM: DMA-Mapping: add function for setting coherent pool size from platform code
    
    Some platforms might require to increase atomic coherent pool to make
    sure that their device will be able to allocate all their buffers from
    atomic context. This function can be also used to decrease atomic
    coherent pool size if coherent allocations are not used for the given
    sub-platform.
    
    Suggested-by: Josh Coombs <josh.coombs@gmail.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 2ae842df4551..5c44dcb0987b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -202,6 +202,13 @@ static inline void dma_free_writecombine(struct device *dev, size_t size,
 	return dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
 }
 
+/*
+ * This can be called during early boot to increase the size of the atomic
+ * coherent DMA pool above the default value of 256KiB. It must be called
+ * before postcore_initcall.
+ */
+extern void __init init_dma_coherent_pool_size(unsigned long size);
+
 /*
  * This can be called during boot to increase the size of the consistent
  * DMA region above it's default value of 2MB. It must be called before the

commit dc2832e1e7db3f9ad465d2fe894bd69ef05d1e4b
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jun 13 10:01:15 2012 +0200

    ARM: dma-mapping: add support for dma_get_sgtable()
    
    This patch adds support for dma_get_sgtable() function which is required
    to let drivers to share the buffers allocated by DMA-mapping subsystem.
    
    Generic implementation based on virt_to_page() is not suitable for ARM
    dma-mapping subsystem.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index a04803331144..2ae842df4551 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -261,6 +261,9 @@ extern void arm_dma_sync_sg_for_cpu(struct device *, struct scatterlist *, int,
 		enum dma_data_direction);
 extern void arm_dma_sync_sg_for_device(struct device *, struct scatterlist *, int,
 		enum dma_data_direction);
+extern int arm_dma_get_sgtable(struct device *dev, struct sg_table *sgt,
+		void *cpu_addr, dma_addr_t dma_addr, size_t size,
+		struct dma_attrs *attrs);
 
 #endif /* __KERNEL__ */
 #endif

commit 64ccc9c033c6089b2d426dad3c56477ab066c999
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Thu Jun 14 13:03:04 2012 +0200

    common: dma-mapping: add support for generic dma_mmap_* calls
    
    Commit 9adc5374 ('common: dma-mapping: introduce mmap method') added a
    generic method for implementing mmap user call to dma_map_ops structure.
    
    This patch converts ARM and PowerPC architectures (the only providers of
    dma_mmap_coherent/dma_mmap_writecombine calls) to use this generic
    dma_map_ops based call and adds a generic cross architecture
    definition for dma_mmap_attrs, dma_mmap_coherent, dma_mmap_writecombine
    functions.
    
    The generic mmap virt_to_page-based fallback implementation is provided for
    architectures which don't provide their own implementation for mmap method.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 80777d871422..a04803331144 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -186,17 +186,6 @@ extern int arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,
 			void *cpu_addr, dma_addr_t dma_addr, size_t size,
 			struct dma_attrs *attrs);
 
-#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, NULL)
-
-static inline int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
-				  void *cpu_addr, dma_addr_t dma_addr,
-				  size_t size, struct dma_attrs *attrs)
-{
-	struct dma_map_ops *ops = get_dma_ops(dev);
-	BUG_ON(!ops);
-	return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
-}
-
 static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
 				       dma_addr_t *dma_handle, gfp_t flag)
 {
@@ -213,14 +202,6 @@ static inline void dma_free_writecombine(struct device *dev, size_t size,
 	return dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
 }
 
-static inline int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
-		      void *cpu_addr, dma_addr_t dma_addr, size_t size)
-{
-	DEFINE_DMA_ATTRS(attrs);
-	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
-	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
-}
-
 /*
  * This can be called during boot to increase the size of the consistent
  * DMA region above it's default value of 2MB. It must be called before the

commit e9da6e9905e639b0f842a244bc770b48ad0523e9
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Mon Jul 30 09:11:33 2012 +0200

    ARM: dma-mapping: remove custom consistent dma region
    
    This patch changes dma-mapping subsystem to use generic vmalloc areas
    for all consistent dma allocations. This increases the total size limit
    of the consistent allocations and removes platform hacks and a lot of
    duplicated code.
    
    Atomic allocations are served from special pool preallocated on boot,
    because vmalloc areas cannot be reliably created in atomic context.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>
    Reviewed-by: Minchan Kim <minchan@kernel.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index bbef15d04890..80777d871422 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -226,7 +226,7 @@ static inline int dma_mmap_writecombine(struct device *dev, struct vm_area_struc
  * DMA region above it's default value of 2MB. It must be called before the
  * memory allocator is initialised, i.e. before any core_initcall.
  */
-extern void __init init_consistent_dma_size(unsigned long size);
+static inline void init_consistent_dma_size(unsigned long size) { }
 
 /*
  * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"

commit f99d60341238fe73fc514129cd9ae4e44e1b2c47
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed May 16 18:31:23 2012 +0200

    ARM: dma-mapping: use alloc, mmap, free from dma_ops
    
    This patch converts dma_alloc/free/mmap_{coherent,writecombine}
    functions to use generic alloc/free/mmap methods from dma_map_ops
    structure. A new DMA_ATTR_WRITE_COMBINE DMA attribute have been
    introduced to implement writecombine methods.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 7a7c3c762f5f..bbef15d04890 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -5,6 +5,7 @@
 
 #include <linux/mm_types.h>
 #include <linux/scatterlist.h>
+#include <linux/dma-attrs.h>
 #include <linux/dma-debug.h>
 
 #include <asm-generic/dma-coherent.h>
@@ -110,68 +111,115 @@ static inline void dma_free_noncoherent(struct device *dev, size_t size,
 extern int dma_supported(struct device *dev, u64 mask);
 
 /**
- * dma_alloc_coherent - allocate consistent memory for DMA
+ * arm_dma_alloc - allocate consistent memory for DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
  * @size: required memory size
  * @handle: bus-specific DMA address
+ * @attrs: optinal attributes that specific mapping properties
  *
- * Allocate some uncached, unbuffered memory for a device for
- * performing DMA.  This function allocates pages, and will
- * return the CPU-viewed address, and sets @handle to be the
- * device-viewed address.
+ * Allocate some memory for a device for performing DMA.  This function
+ * allocates pages, and will return the CPU-viewed address, and sets @handle
+ * to be the device-viewed address.
  */
-extern void *dma_alloc_coherent(struct device *, size_t, dma_addr_t *, gfp_t);
+extern void *arm_dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
+			   gfp_t gfp, struct dma_attrs *attrs);
+
+#define dma_alloc_coherent(d, s, h, f) dma_alloc_attrs(d, s, h, f, NULL)
+
+static inline void *dma_alloc_attrs(struct device *dev, size_t size,
+				       dma_addr_t *dma_handle, gfp_t flag,
+				       struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	void *cpu_addr;
+	BUG_ON(!ops);
+
+	cpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);
+	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr);
+	return cpu_addr;
+}
 
 /**
- * dma_free_coherent - free memory allocated by dma_alloc_coherent
+ * arm_dma_free - free memory allocated by arm_dma_alloc
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
  * @size: size of memory originally requested in dma_alloc_coherent
  * @cpu_addr: CPU-view address returned from dma_alloc_coherent
  * @handle: device-view address returned from dma_alloc_coherent
+ * @attrs: optinal attributes that specific mapping properties
  *
  * Free (and unmap) a DMA buffer previously allocated by
- * dma_alloc_coherent().
+ * arm_dma_alloc().
  *
  * References to memory and mappings associated with cpu_addr/handle
  * during and after this call executing are illegal.
  */
-extern void dma_free_coherent(struct device *, size_t, void *, dma_addr_t);
+extern void arm_dma_free(struct device *dev, size_t size, void *cpu_addr,
+			 dma_addr_t handle, struct dma_attrs *attrs);
+
+#define dma_free_coherent(d, s, c, h) dma_free_attrs(d, s, c, h, NULL)
+
+static inline void dma_free_attrs(struct device *dev, size_t size,
+				     void *cpu_addr, dma_addr_t dma_handle,
+				     struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+
+	debug_dma_free_coherent(dev, size, cpu_addr, dma_handle);
+	ops->free(dev, size, cpu_addr, dma_handle, attrs);
+}
 
 /**
- * dma_mmap_coherent - map a coherent DMA allocation into user space
+ * arm_dma_mmap - map a coherent DMA allocation into user space
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
  * @vma: vm_area_struct describing requested user mapping
  * @cpu_addr: kernel CPU-view address returned from dma_alloc_coherent
  * @handle: device-view address returned from dma_alloc_coherent
  * @size: size of memory originally requested in dma_alloc_coherent
+ * @attrs: optinal attributes that specific mapping properties
  *
  * Map a coherent DMA buffer previously allocated by dma_alloc_coherent
  * into user space.  The coherent DMA buffer must not be freed by the
  * driver until the user space mapping has been released.
  */
-int dma_mmap_coherent(struct device *, struct vm_area_struct *,
-		void *, dma_addr_t, size_t);
+extern int arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,
+			void *cpu_addr, dma_addr_t dma_addr, size_t size,
+			struct dma_attrs *attrs);
 
+#define dma_mmap_coherent(d, v, c, h, s) dma_mmap_attrs(d, v, c, h, s, NULL)
 
-/**
- * dma_alloc_writecombine - allocate writecombining memory for DMA
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @size: required memory size
- * @handle: bus-specific DMA address
- *
- * Allocate some uncached, buffered memory for a device for
- * performing DMA.  This function allocates pages, and will
- * return the CPU-viewed address, and sets @handle to be the
- * device-viewed address.
- */
-extern void *dma_alloc_writecombine(struct device *, size_t, dma_addr_t *,
-		gfp_t);
+static inline int dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,
+				  void *cpu_addr, dma_addr_t dma_addr,
+				  size_t size, struct dma_attrs *attrs)
+{
+	struct dma_map_ops *ops = get_dma_ops(dev);
+	BUG_ON(!ops);
+	return ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);
+}
 
-#define dma_free_writecombine(dev,size,cpu_addr,handle) \
-	dma_free_coherent(dev,size,cpu_addr,handle)
+static inline void *dma_alloc_writecombine(struct device *dev, size_t size,
+				       dma_addr_t *dma_handle, gfp_t flag)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	return dma_alloc_attrs(dev, size, dma_handle, flag, &attrs);
+}
 
-int dma_mmap_writecombine(struct device *, struct vm_area_struct *,
-		void *, dma_addr_t, size_t);
+static inline void dma_free_writecombine(struct device *dev, size_t size,
+				     void *cpu_addr, dma_addr_t dma_handle)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	return dma_free_attrs(dev, size, cpu_addr, dma_handle, &attrs);
+}
+
+static inline int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
+		      void *cpu_addr, dma_addr_t dma_addr, size_t size)
+{
+	DEFINE_DMA_ATTRS(attrs);
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	return dma_mmap_attrs(dev, vma, cpu_addr, dma_addr, size, &attrs);
+}
 
 /*
  * This can be called during boot to increase the size of the consistent
@@ -180,7 +228,6 @@ int dma_mmap_writecombine(struct device *, struct vm_area_struct *,
  */
 extern void __init init_consistent_dma_size(unsigned long size);
 
-
 /*
  * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
  * and utilize bounce buffers as needed to work around limited DMA windows.

commit 15237e1f505b3e5c2276f240b01cd2133e110cbc
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 10 19:55:20 2012 +0100

    ARM: dma-mapping: move all dma bounce code to separate dma ops structure
    
    This patch removes dma bounce hooks from the common dma mapping
    implementation on ARM architecture and creates a separate set of
    dma_map_ops for dma bounce devices.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 6725a08a5c21..7a7c3c762f5f 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -84,62 +84,6 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
-/*
- * The DMA API is built upon the notion of "buffer ownership".  A buffer
- * is either exclusively owned by the CPU (and therefore may be accessed
- * by it) or exclusively owned by the DMA device.  These helper functions
- * represent the transitions between these two ownership states.
- *
- * Note, however, that on later ARMs, this notion does not work due to
- * speculative prefetches.  We model our approach on the assumption that
- * the CPU does do speculative prefetches, which means we clean caches
- * before transfers and delay cache invalidation until transfer completion.
- *
- * Private support functions: these are not part of the API and are
- * liable to change.  Drivers must not use these.
- */
-static inline void __dma_single_cpu_to_dev(const void *kaddr, size_t size,
-	enum dma_data_direction dir)
-{
-	extern void ___dma_single_cpu_to_dev(const void *, size_t,
-		enum dma_data_direction);
-
-	if (!arch_is_coherent())
-		___dma_single_cpu_to_dev(kaddr, size, dir);
-}
-
-static inline void __dma_single_dev_to_cpu(const void *kaddr, size_t size,
-	enum dma_data_direction dir)
-{
-	extern void ___dma_single_dev_to_cpu(const void *, size_t,
-		enum dma_data_direction);
-
-	if (!arch_is_coherent())
-		___dma_single_dev_to_cpu(kaddr, size, dir);
-}
-
-static inline void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
-	size_t size, enum dma_data_direction dir)
-{
-	extern void ___dma_page_cpu_to_dev(struct page *, unsigned long,
-		size_t, enum dma_data_direction);
-
-	if (!arch_is_coherent())
-		___dma_page_cpu_to_dev(page, off, size, dir);
-}
-
-static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
-	size_t size, enum dma_data_direction dir)
-{
-	extern void ___dma_page_dev_to_cpu(struct page *, unsigned long,
-		size_t, enum dma_data_direction);
-
-	if (!arch_is_coherent())
-		___dma_page_dev_to_cpu(page, off, size, dir);
-}
-
-extern int dma_supported(struct device *, u64);
-extern int dma_set_mask(struct device *, u64);
 /*
  * DMA errors are defined by all-bits-set in the DMA address.
  */
@@ -163,6 +107,8 @@ static inline void dma_free_noncoherent(struct device *dev, size_t size,
 {
 }
 
+extern int dma_supported(struct device *dev, u64 mask);
+
 /**
  * dma_alloc_coherent - allocate consistent memory for DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -235,7 +181,6 @@ int dma_mmap_writecombine(struct device *, struct vm_area_struct *,
 extern void __init init_consistent_dma_size(unsigned long size);
 
 
-#ifdef CONFIG_DMABOUNCE
 /*
  * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
  * and utilize bounce buffers as needed to work around limited DMA windows.
@@ -275,47 +220,7 @@ extern int dmabounce_register_dev(struct device *, unsigned long,
  */
 extern void dmabounce_unregister_dev(struct device *);
 
-/*
- * The DMA API, implemented by dmabounce.c.  See below for descriptions.
- */
-extern dma_addr_t __dma_map_page(struct device *, struct page *,
-		unsigned long, size_t, enum dma_data_direction);
-extern void __dma_unmap_page(struct device *, dma_addr_t, size_t,
-		enum dma_data_direction);
-
-/*
- * Private functions
- */
-int dmabounce_sync_for_cpu(struct device *, dma_addr_t, size_t, enum dma_data_direction);
-int dmabounce_sync_for_device(struct device *, dma_addr_t, size_t, enum dma_data_direction);
-#else
-static inline int dmabounce_sync_for_cpu(struct device *d, dma_addr_t addr,
-	size_t size, enum dma_data_direction dir)
-{
-	return 1;
-}
-
-static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
-	size_t size, enum dma_data_direction dir)
-{
-	return 1;
-}
-
 
-static inline dma_addr_t __dma_map_page(struct device *dev, struct page *page,
-	     unsigned long offset, size_t size, enum dma_data_direction dir)
-{
-	__dma_page_cpu_to_dev(page, offset, size, dir);
-	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
-}
-
-static inline void __dma_unmap_page(struct device *dev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir)
-{
-	__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),
-		handle & ~PAGE_MASK, size, dir);
-}
-#endif /* CONFIG_DMABOUNCE */
 
 /*
  * The scatter list versions of the above methods.

commit 2dc6a016bbedf18f18ad73997e5338307d6dbde9
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 10 19:55:20 2012 +0100

    ARM: dma-mapping: use asm-generic/dma-mapping-common.h
    
    This patch modifies dma-mapping implementation on ARM architecture to
    use common dma_map_ops structure and asm-generic/dma-mapping-common.h
    helpers.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index eeddbe201e24..6725a08a5c21 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -11,6 +11,27 @@
 #include <asm/memory.h>
 
 #define DMA_ERROR_CODE	(~0)
+extern struct dma_map_ops arm_dma_ops;
+
+static inline struct dma_map_ops *get_dma_ops(struct device *dev)
+{
+	if (dev && dev->archdata.dma_ops)
+		return dev->archdata.dma_ops;
+	return &arm_dma_ops;
+}
+
+static inline void set_dma_ops(struct device *dev, struct dma_map_ops *ops)
+{
+	BUG_ON(!dev);
+	dev->archdata.dma_ops = ops;
+}
+
+#include <asm-generic/dma-mapping-common.h>
+
+static inline int dma_set_mask(struct device *dev, u64 mask)
+{
+	return get_dma_ops(dev)->set_dma_mask(dev, mask);
+}
 
 #ifdef __arch_page_to_dma
 #error Please update to __arch_pfn_to_dma
@@ -119,7 +140,6 @@ static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
 
 extern int dma_supported(struct device *, u64);
 extern int dma_set_mask(struct device *, u64);
-
 /*
  * DMA errors are defined by all-bits-set in the DMA address.
  */
@@ -297,179 +317,17 @@ static inline void __dma_unmap_page(struct device *dev, dma_addr_t handle,
 }
 #endif /* CONFIG_DMABOUNCE */
 
-/**
- * dma_map_single - map a single buffer for streaming DMA
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @cpu_addr: CPU direct mapped address of buffer
- * @size: size of buffer to map
- * @dir: DMA transfer direction
- *
- * Ensure that any data held in the cache is appropriately discarded
- * or written back.
- *
- * The device owns this memory once this call has completed.  The CPU
- * can regain ownership by calling dma_unmap_single() or
- * dma_sync_single_for_cpu().
- */
-static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
-		size_t size, enum dma_data_direction dir)
-{
-	unsigned long offset;
-	struct page *page;
-	dma_addr_t addr;
-
-	BUG_ON(!virt_addr_valid(cpu_addr));
-	BUG_ON(!virt_addr_valid(cpu_addr + size - 1));
-	BUG_ON(!valid_dma_direction(dir));
-
-	page = virt_to_page(cpu_addr);
-	offset = (unsigned long)cpu_addr & ~PAGE_MASK;
-	addr = __dma_map_page(dev, page, offset, size, dir);
-	debug_dma_map_page(dev, page, offset, size, dir, addr, true);
-
-	return addr;
-}
-
-/**
- * dma_map_page - map a portion of a page for streaming DMA
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @page: page that buffer resides in
- * @offset: offset into page for start of buffer
- * @size: size of buffer to map
- * @dir: DMA transfer direction
- *
- * Ensure that any data held in the cache is appropriately discarded
- * or written back.
- *
- * The device owns this memory once this call has completed.  The CPU
- * can regain ownership by calling dma_unmap_page().
- */
-static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
-	     unsigned long offset, size_t size, enum dma_data_direction dir)
-{
-	dma_addr_t addr;
-
-	BUG_ON(!valid_dma_direction(dir));
-
-	addr = __dma_map_page(dev, page, offset, size, dir);
-	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
-
-	return addr;
-}
-
-/**
- * dma_unmap_single - unmap a single buffer previously mapped
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @handle: DMA address of buffer
- * @size: size of buffer (same as passed to dma_map_single)
- * @dir: DMA transfer direction (same as passed to dma_map_single)
- *
- * Unmap a single streaming mode DMA translation.  The handle and size
- * must match what was provided in the previous dma_map_single() call.
- * All other usages are undefined.
- *
- * After this call, reads by the CPU to the buffer are guaranteed to see
- * whatever the device wrote there.
- */
-static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir)
-{
-	debug_dma_unmap_page(dev, handle, size, dir, true);
-	__dma_unmap_page(dev, handle, size, dir);
-}
-
-/**
- * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @handle: DMA address of buffer
- * @size: size of buffer (same as passed to dma_map_page)
- * @dir: DMA transfer direction (same as passed to dma_map_page)
- *
- * Unmap a page streaming mode DMA translation.  The handle and size
- * must match what was provided in the previous dma_map_page() call.
- * All other usages are undefined.
- *
- * After this call, reads by the CPU to the buffer are guaranteed to see
- * whatever the device wrote there.
- */
-static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir)
-{
-	debug_dma_unmap_page(dev, handle, size, dir, false);
-	__dma_unmap_page(dev, handle, size, dir);
-}
-
-
-static inline void dma_sync_single_for_cpu(struct device *dev,
-		dma_addr_t handle, size_t size, enum dma_data_direction dir)
-{
-	BUG_ON(!valid_dma_direction(dir));
-
-	debug_dma_sync_single_for_cpu(dev, handle, size, dir);
-
-	if (!dmabounce_sync_for_cpu(dev, handle, size, dir))
-		return;
-
-	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
-}
-
-static inline void dma_sync_single_for_device(struct device *dev,
-		dma_addr_t handle, size_t size, enum dma_data_direction dir)
-{
-	BUG_ON(!valid_dma_direction(dir));
-
-	debug_dma_sync_single_for_device(dev, handle, size, dir);
-
-	if (!dmabounce_sync_for_device(dev, handle, size, dir))
-		return;
-
-	__dma_single_cpu_to_dev(dma_to_virt(dev, handle), size, dir);
-}
-
-/**
- * dma_sync_single_range_for_cpu
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @handle: DMA address of buffer
- * @offset: offset of region to start sync
- * @size: size of region to sync
- * @dir: DMA transfer direction (same as passed to dma_map_single)
- *
- * Make physical memory consistent for a single streaming mode DMA
- * translation after a transfer.
- *
- * If you perform a dma_map_single() but wish to interrogate the
- * buffer using the cpu, yet do not wish to teardown the PCI dma
- * mapping, you must call this function before doing so.  At the
- * next point you give the PCI dma address back to the card, you
- * must first the perform a dma_sync_for_device, and then the
- * device again owns the buffer.
- */
-static inline void dma_sync_single_range_for_cpu(struct device *dev,
-		dma_addr_t handle, unsigned long offset, size_t size,
-		enum dma_data_direction dir)
-{
-	dma_sync_single_for_cpu(dev, handle + offset, size, dir);
-}
-
-static inline void dma_sync_single_range_for_device(struct device *dev,
-		dma_addr_t handle, unsigned long offset, size_t size,
-		enum dma_data_direction dir)
-{
-	dma_sync_single_for_device(dev, handle + offset, size, dir);
-}
-
 /*
  * The scatter list versions of the above methods.
  */
-extern int dma_map_sg(struct device *, struct scatterlist *, int,
-		enum dma_data_direction);
-extern void dma_unmap_sg(struct device *, struct scatterlist *, int,
+extern int arm_dma_map_sg(struct device *, struct scatterlist *, int,
+		enum dma_data_direction, struct dma_attrs *attrs);
+extern void arm_dma_unmap_sg(struct device *, struct scatterlist *, int,
+		enum dma_data_direction, struct dma_attrs *attrs);
+extern void arm_dma_sync_sg_for_cpu(struct device *, struct scatterlist *, int,
 		enum dma_data_direction);
-extern void dma_sync_sg_for_cpu(struct device *, struct scatterlist *, int,
+extern void arm_dma_sync_sg_for_device(struct device *, struct scatterlist *, int,
 		enum dma_data_direction);
-extern void dma_sync_sg_for_device(struct device *, struct scatterlist *, int,
-		enum dma_data_direction);
-
 
 #endif /* __KERNEL__ */
 #endif

commit a227fb92a0f5f0dd8282719386e9b3a29f0d16b2
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 10 19:55:20 2012 +0100

    ARM: dma-mapping: remove offset parameter to prepare for generic dma_ops
    
    This patch removes the need for the offset parameter in dma bounce
    functions. This is required to let dma-mapping framework on ARM
    architecture to use common, generic dma_map_ops based dma-mapping
    helpers.
    
    Background and more detailed explaination:
    
    dma_*_range_* functions are available from the early days of the dma
    mapping api. They are the correct way of doing a partial syncs on the
    buffer (usually used by the network device drivers). This patch changes
    only the internal implementation of the dma bounce functions to let
    them tunnel through dma_map_ops structure. The driver api stays
    unchanged, so driver are obliged to call dma_*_range_* functions to
    keep code clean and easy to understand.
    
    The only drawback from this patch is reduced detection of the dma api
    abuse. Let us consider the following code:
    
    dma_addr = dma_map_single(dev, ptr, 64, DMA_TO_DEVICE);
    dma_sync_single_range_for_cpu(dev, dma_addr+16, 0, 32, DMA_TO_DEVICE);
    
    Without the patch such code fails, because dma bounce code is unable
    to find the bounce buffer for the given dma_address. After the patch
    the above sync call will be equivalent to:
    
    dma_sync_single_range_for_cpu(dev, dma_addr, 16, 32, DMA_TO_DEVICE);
    
    which succeeds.
    
    I don't consider this as a real problem, because DMA API abuse should be
    caught by debug_dma_* function family. This patch lets us to simplify
    the internal low-level implementation without chaning the driver visible
    API.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 6a838da03c5b..eeddbe201e24 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -266,19 +266,17 @@ extern void __dma_unmap_page(struct device *, dma_addr_t, size_t,
 /*
  * Private functions
  */
-int dmabounce_sync_for_cpu(struct device *, dma_addr_t, unsigned long,
-		size_t, enum dma_data_direction);
-int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
-		size_t, enum dma_data_direction);
+int dmabounce_sync_for_cpu(struct device *, dma_addr_t, size_t, enum dma_data_direction);
+int dmabounce_sync_for_device(struct device *, dma_addr_t, size_t, enum dma_data_direction);
 #else
 static inline int dmabounce_sync_for_cpu(struct device *d, dma_addr_t addr,
-	unsigned long offset, size_t size, enum dma_data_direction dir)
+	size_t size, enum dma_data_direction dir)
 {
 	return 1;
 }
 
 static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
-	unsigned long offset, size_t size, enum dma_data_direction dir)
+	size_t size, enum dma_data_direction dir)
 {
 	return 1;
 }
@@ -401,6 +399,33 @@ static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 	__dma_unmap_page(dev, handle, size, dir);
 }
 
+
+static inline void dma_sync_single_for_cpu(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	BUG_ON(!valid_dma_direction(dir));
+
+	debug_dma_sync_single_for_cpu(dev, handle, size, dir);
+
+	if (!dmabounce_sync_for_cpu(dev, handle, size, dir))
+		return;
+
+	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
+}
+
+static inline void dma_sync_single_for_device(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	BUG_ON(!valid_dma_direction(dir));
+
+	debug_dma_sync_single_for_device(dev, handle, size, dir);
+
+	if (!dmabounce_sync_for_device(dev, handle, size, dir))
+		return;
+
+	__dma_single_cpu_to_dev(dma_to_virt(dev, handle), size, dir);
+}
+
 /**
  * dma_sync_single_range_for_cpu
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -423,40 +448,14 @@ static inline void dma_sync_single_range_for_cpu(struct device *dev,
 		dma_addr_t handle, unsigned long offset, size_t size,
 		enum dma_data_direction dir)
 {
-	BUG_ON(!valid_dma_direction(dir));
-
-	debug_dma_sync_single_for_cpu(dev, handle + offset, size, dir);
-
-	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
-		return;
-
-	__dma_single_dev_to_cpu(dma_to_virt(dev, handle) + offset, size, dir);
+	dma_sync_single_for_cpu(dev, handle + offset, size, dir);
 }
 
 static inline void dma_sync_single_range_for_device(struct device *dev,
 		dma_addr_t handle, unsigned long offset, size_t size,
 		enum dma_data_direction dir)
 {
-	BUG_ON(!valid_dma_direction(dir));
-
-	debug_dma_sync_single_for_device(dev, handle + offset, size, dir);
-
-	if (!dmabounce_sync_for_device(dev, handle, offset, size, dir))
-		return;
-
-	__dma_single_cpu_to_dev(dma_to_virt(dev, handle) + offset, size, dir);
-}
-
-static inline void dma_sync_single_for_cpu(struct device *dev,
-		dma_addr_t handle, size_t size, enum dma_data_direction dir)
-{
-	dma_sync_single_range_for_cpu(dev, handle, 0, size, dir);
-}
-
-static inline void dma_sync_single_for_device(struct device *dev,
-		dma_addr_t handle, size_t size, enum dma_data_direction dir)
-{
-	dma_sync_single_range_for_device(dev, handle, 0, size, dir);
+	dma_sync_single_for_device(dev, handle + offset, size, dir);
 }
 
 /*

commit 553ac78877242b6d8b591323731df304140d0f99
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Feb 29 14:45:28 2012 +0100

    ARM: dma-mapping: introduce DMA_ERROR_CODE constant
    
    Replace all uses of ~0 with DMA_ERROR_CODE, what should make the code
    easier to read.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index cb3b7c981c4b..6a838da03c5b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -10,6 +10,8 @@
 #include <asm-generic/dma-coherent.h>
 #include <asm/memory.h>
 
+#define DMA_ERROR_CODE	(~0)
+
 #ifdef __arch_page_to_dma
 #error Please update to __arch_pfn_to_dma
 #endif
@@ -123,7 +125,7 @@ extern int dma_set_mask(struct device *, u64);
  */
 static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
 {
-	return dma_addr == ~0;
+	return dma_addr == DMA_ERROR_CODE;
 }
 
 /*

commit 1fdb24e969110fafea36d3b393bea438f702c87f
Merge: f362f98e7c44 531a6a941745
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 28 12:02:27 2011 -0700

    Merge branch 'devel-stable' of http://ftp.arm.linux.org.uk/pub/linux/arm/kernel/git-cur/linux-2.6-arm
    
    * 'devel-stable' of http://ftp.arm.linux.org.uk/pub/linux/arm/kernel/git-cur/linux-2.6-arm: (178 commits)
      ARM: 7139/1: fix compilation with CONFIG_ARM_ATAG_DTB_COMPAT and large TEXT_OFFSET
      ARM: gic, local timers: use the request_percpu_irq() interface
      ARM: gic: consolidate PPI handling
      ARM: switch from NO_MACH_MEMORY_H to NEED_MACH_MEMORY_H
      ARM: mach-s5p64x0: remove mach/memory.h
      ARM: mach-s3c64xx: remove mach/memory.h
      ARM: plat-mxc: remove mach/memory.h
      ARM: mach-prima2: remove mach/memory.h
      ARM: mach-zynq: remove mach/memory.h
      ARM: mach-bcmring: remove mach/memory.h
      ARM: mach-davinci: remove mach/memory.h
      ARM: mach-pxa: remove mach/memory.h
      ARM: mach-ixp4xx: remove mach/memory.h
      ARM: mach-h720x: remove mach/memory.h
      ARM: mach-vt8500: remove mach/memory.h
      ARM: mach-s5pc100: remove mach/memory.h
      ARM: mach-tegra: remove mach/memory.h
      ARM: plat-tcc: remove mach/memory.h
      ARM: mach-mmp: remove mach/memory.h
      ARM: mach-cns3xxx: remove mach/memory.h
      ...
    
    Fix up mostly pretty trivial conflicts in:
     - arch/arm/Kconfig
     - arch/arm/include/asm/localtimer.h
     - arch/arm/kernel/Makefile
     - arch/arm/mach-shmobile/board-ap4evb.c
     - arch/arm/mach-u300/core.c
     - arch/arm/mm/dma-mapping.c
     - arch/arm/mm/proc-v7.S
     - arch/arm/plat-omap/Kconfig
    largely due to some CONFIG option renaming (ie CONFIG_PM_SLEEP ->
    CONFIG_ARM_CPU_SUSPEND for the arm-specific suspend code etc) and
    addition of NEED_MACH_MEMORY_H next to HAVE_IDE.

commit 01f461a3a4321a9f98b6b508f32d2396c5704b7c
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Aug 23 13:59:14 2011 +0100

    ARM: 7058/1: LPAE: Cast the dma_addr_t argument to unsigned long in dma_to_virt
    
    This is to avoid a compiler warning when invoking the __bus_to_virt()
    macro. The dma_to_virt() function gets addresses within the 32-bit
    range.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 7a21d0bf7134..28b7ee8d7398 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -32,7 +32,7 @@ static inline unsigned long dma_to_pfn(struct device *dev, dma_addr_t addr)
 
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
 {
-	return (void *)__bus_to_virt(addr);
+	return (void *)__bus_to_virt((unsigned long)addr);
 }
 
 static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)

commit 99d1717dd7fecf2b10195b0d864323b952b4eba0
Author: Jon Medhurst <tixy@yxit.co.uk>
Date:   Tue Aug 2 17:28:27 2011 +0100

    ARM: Add init_consistent_dma_size()
    
    This function can be called during boot to increase the size of the consistent
    DMA region above it's default value of 2MB. It must be called before the memory
    allocator is initialised, i.e. before any core_initcall.
    
    Signed-off-by: Jon Medhurst <tixy@yxit.co.uk>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 7a21d0bf7134..7f27fab9d404 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -205,6 +205,13 @@ extern void *dma_alloc_writecombine(struct device *, size_t, dma_addr_t *,
 int dma_mmap_writecombine(struct device *, struct vm_area_struct *,
 		void *, dma_addr_t, size_t);
 
+/*
+ * This can be called during boot to increase the size of the consistent
+ * DMA region above it's default value of 2MB. It must be called before the
+ * memory allocator is initialised, i.e. before any core_initcall.
+ */
+extern void __init init_consistent_dma_size(unsigned long size);
+
 
 #ifdef CONFIG_DMABOUNCE
 /*

commit 022ae537b23cb14a391565e9ad9e9945f4b17138
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 8 21:26:59 2011 +0100

    ARM: dma: replace ISA_DMA_THRESHOLD with a variable
    
    ISA_DMA_THRESHOLD has been unused by non-arch code, so lets now get
    rid of it from ARM by replacing it with arm_dma_zone_mask.  Move
    dma_supported() and dma_set_mask() out of line, and have
    dma_supported() check this new variable instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 94662f4c9ea7..7a21d0bf7134 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -115,33 +115,8 @@ static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
 		___dma_page_dev_to_cpu(page, off, size, dir);
 }
 
-/*
- * Return whether the given device DMA address mask can be supported
- * properly.  For example, if your device can only drive the low 24-bits
- * during bus mastering, then you would pass 0x00ffffff as the mask
- * to this function.
- *
- * FIXME: This should really be a platform specific issue - we should
- * return false if GFP_DMA allocations may not satisfy the supplied 'mask'.
- */
-static inline int dma_supported(struct device *dev, u64 mask)
-{
-	if (mask < ISA_DMA_THRESHOLD)
-		return 0;
-	return 1;
-}
-
-static inline int dma_set_mask(struct device *dev, u64 dma_mask)
-{
-	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
-		return -EIO;
-
-#ifndef CONFIG_DMABOUNCE
-	*dev->dma_mask = dma_mask;
-#endif
-
-	return 0;
-}
+extern int dma_supported(struct device *, u64);
+extern int dma_set_mask(struct device *, u64);
 
 /*
  * DMA errors are defined by all-bits-set in the DMA address.

commit 3973c337759cd201773a0ecc7b6f39f1ea2a6287
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 8 14:02:21 2011 +0100

    ARM: dmabounce: simplify dma_set_mask()
    
    Simplify the dmabounce specific code in dma_set_mask().  We can just
    omit setting the dma mask if dmabounce is enabled (we will have already
    set dma mask via callbacks when the device is created in that case.)
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 4ad25337f927..94662f4c9ea7 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -133,18 +133,12 @@ static inline int dma_supported(struct device *dev, u64 mask)
 
 static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 {
-#ifdef CONFIG_DMABOUNCE
-	if (dev->archdata.dmabounce) {
-		if (dma_mask >= ISA_DMA_THRESHOLD)
-			return 0;
-		else
-			return -EIO;
-	}
-#endif
 	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 		return -EIO;
 
+#ifndef CONFIG_DMABOUNCE
 	*dev->dma_mask = dma_mask;
+#endif
 
 	return 0;
 }

commit 0703ed2a6b260cd743adf49a8281eb064d728832
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jul 4 08:32:21 2011 +0100

    ARM: dmabounce: get rid of dma_needs_bounce global function
    
    Pass the device type specific needs_bounce function in at dmabounce
    register time, avoiding the need for a platform specific global
    function to do this.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index d2903d0c3085..4ad25337f927 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -256,14 +256,14 @@ int dma_mmap_writecombine(struct device *, struct vm_area_struct *,
  * @dev: valid struct device pointer
  * @small_buf_size: size of buffers to use with small buffer pool
  * @large_buf_size: size of buffers to use with large buffer pool (can be 0)
+ * @needs_bounce_fn: called to determine whether buffer needs bouncing
  *
  * This function should be called by low-level platform code to register
  * a device as requireing DMA buffer bouncing. The function will allocate
  * appropriate DMA pools for the device.
- *
  */
 extern int dmabounce_register_dev(struct device *, unsigned long,
-		unsigned long);
+		unsigned long, int (*)(struct device *, dma_addr_t, size_t));
 
 /**
  * dmabounce_unregister_dev
@@ -277,24 +277,6 @@ extern int dmabounce_register_dev(struct device *, unsigned long,
  */
 extern void dmabounce_unregister_dev(struct device *);
 
-/**
- * dma_needs_bounce
- *
- * @dev: valid struct device pointer
- * @dma_handle: dma_handle of unbounced buffer
- * @size: size of region being mapped
- *
- * Platforms that utilize the dmabounce mechanism must implement
- * this function.
- *
- * The dmabounce routines call this function whenever a dma-mapping
- * is requested to determine whether a given buffer needs to be bounced
- * or not. The function must return 0 if the buffer is OK for
- * DMA access and 1 if the buffer needs to be bounced.
- *
- */
-extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
-
 /*
  * The DMA API, implemented by dmabounce.c.  See below for descriptions.
  */

commit 8021a4a048a85906302bd0236f3d125473be65b1
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 16:13:58 2011 +0100

    ARM: dma-mapping: define dma_(un)?map_single in terms of dma_(un)?map_page
    
    Use dma_map_page()/dma_unmap_page() internals to handle dma_map_single()
    and dma_unmap_single().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 4fff837363ed..d2903d0c3085 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -298,10 +298,6 @@ extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
 /*
  * The DMA API, implemented by dmabounce.c.  See below for descriptions.
  */
-extern dma_addr_t __dma_map_single(struct device *, void *, size_t,
-		enum dma_data_direction);
-extern void __dma_unmap_single(struct device *, dma_addr_t, size_t,
-		enum dma_data_direction);
 extern dma_addr_t __dma_map_page(struct device *, struct page *,
 		unsigned long, size_t, enum dma_data_direction);
 extern void __dma_unmap_page(struct device *, dma_addr_t, size_t,
@@ -328,13 +324,6 @@ static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
 }
 
 
-static inline dma_addr_t __dma_map_single(struct device *dev, void *cpu_addr,
-		size_t size, enum dma_data_direction dir)
-{
-	__dma_single_cpu_to_dev(cpu_addr, size, dir);
-	return virt_to_dma(dev, cpu_addr);
-}
-
 static inline dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size, enum dma_data_direction dir)
 {
@@ -342,12 +331,6 @@ static inline dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
 }
 
-static inline void __dma_unmap_single(struct device *dev, dma_addr_t handle,
-		size_t size, enum dma_data_direction dir)
-{
-	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
-}
-
 static inline void __dma_unmap_page(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
@@ -373,14 +356,18 @@ static inline void __dma_unmap_page(struct device *dev, dma_addr_t handle,
 static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 		size_t size, enum dma_data_direction dir)
 {
+	unsigned long offset;
+	struct page *page;
 	dma_addr_t addr;
 
+	BUG_ON(!virt_addr_valid(cpu_addr));
+	BUG_ON(!virt_addr_valid(cpu_addr + size - 1));
 	BUG_ON(!valid_dma_direction(dir));
 
-	addr = __dma_map_single(dev, cpu_addr, size, dir);
-	debug_dma_map_page(dev, virt_to_page(cpu_addr),
-			(unsigned long)cpu_addr & ~PAGE_MASK, size,
-			dir, addr, true);
+	page = virt_to_page(cpu_addr);
+	offset = (unsigned long)cpu_addr & ~PAGE_MASK;
+	addr = __dma_map_page(dev, page, offset, size, dir);
+	debug_dma_map_page(dev, page, offset, size, dir, addr, true);
 
 	return addr;
 }
@@ -430,7 +417,7 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
 	debug_dma_unmap_page(dev, handle, size, dir, true);
-	__dma_unmap_single(dev, handle, size, dir);
+	__dma_unmap_page(dev, handle, size, dir);
 }
 
 /**

commit 24056f525051a9e186af28904b396320e18bf9a0
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jan 3 11:29:28 2011 +0000

    ARM: DMA: add support for DMA debugging
    
    Add ARM support for the DMA debug infrastructure, which allows the
    DMA API usage to be debugged.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 8f69b98f68fc..4fff837363ed 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -5,6 +5,7 @@
 
 #include <linux/mm_types.h>
 #include <linux/scatterlist.h>
+#include <linux/dma-debug.h>
 
 #include <asm-generic/dma-coherent.h>
 #include <asm/memory.h>
@@ -297,13 +298,13 @@ extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
 /*
  * The DMA API, implemented by dmabounce.c.  See below for descriptions.
  */
-extern dma_addr_t dma_map_single(struct device *, void *, size_t,
+extern dma_addr_t __dma_map_single(struct device *, void *, size_t,
 		enum dma_data_direction);
-extern void dma_unmap_single(struct device *, dma_addr_t, size_t,
+extern void __dma_unmap_single(struct device *, dma_addr_t, size_t,
 		enum dma_data_direction);
-extern dma_addr_t dma_map_page(struct device *, struct page *,
+extern dma_addr_t __dma_map_page(struct device *, struct page *,
 		unsigned long, size_t, enum dma_data_direction);
-extern void dma_unmap_page(struct device *, dma_addr_t, size_t,
+extern void __dma_unmap_page(struct device *, dma_addr_t, size_t,
 		enum dma_data_direction);
 
 /*
@@ -327,6 +328,34 @@ static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
 }
 
 
+static inline dma_addr_t __dma_map_single(struct device *dev, void *cpu_addr,
+		size_t size, enum dma_data_direction dir)
+{
+	__dma_single_cpu_to_dev(cpu_addr, size, dir);
+	return virt_to_dma(dev, cpu_addr);
+}
+
+static inline dma_addr_t __dma_map_page(struct device *dev, struct page *page,
+	     unsigned long offset, size_t size, enum dma_data_direction dir)
+{
+	__dma_page_cpu_to_dev(page, offset, size, dir);
+	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
+}
+
+static inline void __dma_unmap_single(struct device *dev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir)
+{
+	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
+}
+
+static inline void __dma_unmap_page(struct device *dev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir)
+{
+	__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),
+		handle & ~PAGE_MASK, size, dir);
+}
+#endif /* CONFIG_DMABOUNCE */
+
 /**
  * dma_map_single - map a single buffer for streaming DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -344,11 +373,16 @@ static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
 static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 		size_t size, enum dma_data_direction dir)
 {
+	dma_addr_t addr;
+
 	BUG_ON(!valid_dma_direction(dir));
 
-	__dma_single_cpu_to_dev(cpu_addr, size, dir);
+	addr = __dma_map_single(dev, cpu_addr, size, dir);
+	debug_dma_map_page(dev, virt_to_page(cpu_addr),
+			(unsigned long)cpu_addr & ~PAGE_MASK, size,
+			dir, addr, true);
 
-	return virt_to_dma(dev, cpu_addr);
+	return addr;
 }
 
 /**
@@ -368,11 +402,14 @@ static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size, enum dma_data_direction dir)
 {
+	dma_addr_t addr;
+
 	BUG_ON(!valid_dma_direction(dir));
 
-	__dma_page_cpu_to_dev(page, offset, size, dir);
+	addr = __dma_map_page(dev, page, offset, size, dir);
+	debug_dma_map_page(dev, page, offset, size, dir, addr, false);
 
-	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
+	return addr;
 }
 
 /**
@@ -392,7 +429,8 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
+	debug_dma_unmap_page(dev, handle, size, dir, true);
+	__dma_unmap_single(dev, handle, size, dir);
 }
 
 /**
@@ -412,10 +450,9 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),
-		handle & ~PAGE_MASK, size, dir);
+	debug_dma_unmap_page(dev, handle, size, dir, false);
+	__dma_unmap_page(dev, handle, size, dir);
 }
-#endif /* CONFIG_DMABOUNCE */
 
 /**
  * dma_sync_single_range_for_cpu
@@ -441,6 +478,8 @@ static inline void dma_sync_single_range_for_cpu(struct device *dev,
 {
 	BUG_ON(!valid_dma_direction(dir));
 
+	debug_dma_sync_single_for_cpu(dev, handle + offset, size, dir);
+
 	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
 		return;
 
@@ -453,6 +492,8 @@ static inline void dma_sync_single_range_for_device(struct device *dev,
 {
 	BUG_ON(!valid_dma_direction(dir));
 
+	debug_dma_sync_single_for_device(dev, handle + offset, size, dir);
+
 	if (!dmabounce_sync_for_device(dev, handle, offset, size, dir))
 		return;
 

commit 9eedd96301cad8ab58ee8c1e579677d0a75c2ba1
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jan 3 00:00:17 2011 +0000

    ARM: DMA: Replace page_to_dma()/dma_to_page() with pfn_to_dma()/dma_to_pfn()
    
    Replace the page_to_dma() and dma_to_page() macros with their PFN
    equivalents.  This allows us to map parts of memory which do not have
    a struct page allocated to them to bus addresses.  This will be used
    internally by dma_alloc_coherent()/dma_alloc_writecombine().
    
    Build tested on Versatile, OMAP1, IOP13xx and KS8695.
    
    Tested-by: Janusz Krzysztofik <jkrzyszt@tis.icnet.pl>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index c568da7dcae4..8f69b98f68fc 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -9,20 +9,24 @@
 #include <asm-generic/dma-coherent.h>
 #include <asm/memory.h>
 
+#ifdef __arch_page_to_dma
+#error Please update to __arch_pfn_to_dma
+#endif
+
 /*
- * page_to_dma/dma_to_virt/virt_to_dma are architecture private functions
- * used internally by the DMA-mapping API to provide DMA addresses. They
- * must not be used by drivers.
+ * dma_to_pfn/pfn_to_dma/dma_to_virt/virt_to_dma are architecture private
+ * functions used internally by the DMA-mapping API to provide DMA
+ * addresses. They must not be used by drivers.
  */
-#ifndef __arch_page_to_dma
-static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
+#ifndef __arch_pfn_to_dma
+static inline dma_addr_t pfn_to_dma(struct device *dev, unsigned long pfn)
 {
-	return (dma_addr_t)__pfn_to_bus(page_to_pfn(page));
+	return (dma_addr_t)__pfn_to_bus(pfn);
 }
 
-static inline struct page *dma_to_page(struct device *dev, dma_addr_t addr)
+static inline unsigned long dma_to_pfn(struct device *dev, dma_addr_t addr)
 {
-	return pfn_to_page(__bus_to_pfn(addr));
+	return __bus_to_pfn(addr);
 }
 
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
@@ -35,14 +39,14 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 	return (dma_addr_t)__virt_to_bus((unsigned long)(addr));
 }
 #else
-static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
+static inline dma_addr_t pfn_to_dma(struct device *dev, unsigned long pfn)
 {
-	return __arch_page_to_dma(dev, page);
+	return __arch_pfn_to_dma(dev, pfn);
 }
 
-static inline struct page *dma_to_page(struct device *dev, dma_addr_t addr)
+static inline unsigned long dma_to_pfn(struct device *dev, dma_addr_t addr)
 {
-	return __arch_dma_to_page(dev, addr);
+	return __arch_dma_to_pfn(dev, addr);
 }
 
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
@@ -368,7 +372,7 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 
 	__dma_page_cpu_to_dev(page, offset, size, dir);
 
-	return page_to_dma(dev, page) + offset;
+	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
 }
 
 /**
@@ -408,8 +412,8 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	__dma_page_dev_to_cpu(dma_to_page(dev, handle), handle & ~PAGE_MASK,
-		size, dir);
+	__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),
+		handle & ~PAGE_MASK, size, dir);
 }
 #endif /* CONFIG_DMABOUNCE */
 

commit 0485e18bc4112a3b548baa314c24bfbece4d156b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Sep 5 12:19:50 2010 +0100

    Revert "[ARM] pxa: remove now unnecessary dma_needs_bounce()"
    
    This reverts commit 4fa5518, which causes a compilation regression for
    IXP4xx platforms.
    
    Reported-by: Richard Cochran <richardcochran@gmail.com>
    Acked-by: Eric Miao <eric.y.miao@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index c226fe10553e..c568da7dcae4 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -288,15 +288,7 @@ extern void dmabounce_unregister_dev(struct device *);
  * DMA access and 1 if the buffer needs to be bounced.
  *
  */
-#ifdef CONFIG_SA1111
 extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
-#else
-static inline int dma_needs_bounce(struct device *dev, dma_addr_t addr,
-				   size_t size)
-{
-	return 0;
-}
-#endif
 
 /*
  * The DMA API, implemented by dmabounce.c.  See below for descriptions.

commit 636d17427b1ef0e97bd9df9b3b0e0f314ff889d3
Merge: dd21e9bdff14 0b019a41553a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 11 09:13:19 2010 -0700

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm: (226 commits)
      ARM: 6323/1: cam60: don't use __init for cam60_spi_{flash_platform_data,partitions}
      ARM: 6324/1: cam60: move cam60_spi_devices to .init.data
      ARM: 6322/1: imx/pca100: Fix name of spi platform data
      ARM: 6321/1: fix syntax error in main Kconfig file
      ARM: 6297/1: move U300 timer to dynamic clock lookup
      ARM: 6296/1: clock U300 intcon and timer properly
      ARM: 6295/1: fix U300 apb_pclk split
      ARM: 6306/1: fix inverted MMC card detect in U300
      ARM: 6299/1: errata: TLBIASIDIS and TLBIMVAIS operations can broadcast a faulty ASID
      ARM: 6294/1: etm: do a dummy read from OSSRR during initialization
      ARM: 6292/1: coresight: add ETM management registers
      ARM: 6288/1: ftrace: document mcount formats
      ARM: 6287/1: ftrace: clean up mcount assembly indentation
      ARM: 6286/1: fix Thumb-2 decompressor broken by "Auto calculate ZRELADDR"
      ARM: 6281/1: video/imxfb.c: allow usage without BACKLIGHT_CLASS_DEVICE
      ARM: 6280/1: imx: Fix build failure when including <mach/gpio.h> without <linux/spinlock.h>
      ARM: S5PV210: Fix on missing s3c-sdhci card detection method for hsmmc3
      ARM: S5P: Fix on missing S5P_DEV_FIMC in plat-s5p/Kconfig
      ARM: S5PV210: Override FIMC driver name on Aquila board
      ARM: S5PC100: enable FIMC on SMDKC100
      ...
    
    Fix up conflicts in arch/arm/mach-{s5pc100,s5pv210}/cpu.c due to
    different subsystem 'setname' calls, and trivial port types in
    include/linux/serial_core.h

commit 3b9c6c11f519718d618f5d7c9508daf78b207f6f
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 10 18:03:25 2010 -0700

    dma-mapping: remove dma_is_consistent API
    
    Architectures implement dma_is_consistent() in different ways (some
    misinterpret the definition of API in DMA-API.txt).  So it hasn't been so
    useful for drivers.  We have only one user of the API in tree.  Unlikely
    out-of-tree drivers use the API.
    
    Even if we fix dma_is_consistent() in some architectures, it doesn't look
    useful at all.  It was invented long ago for some old systems that can't
    allocate coherent memory at all.  It's better to export only APIs that are
    definitely necessary for drivers.
    
    Let's remove this API.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: James Bottomley <James.Bottomley@HansenPartnership.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index f4a996d5ae96..c568da7dcae4 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -144,11 +144,6 @@ static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 	return 0;
 }
 
-static inline int dma_is_consistent(struct device *dev, dma_addr_t handle)
-{
-	return !!arch_is_coherent();
-}
-
 /*
  * DMA errors are defined by all-bits-set in the DMA address.
  */

commit 4565f0170dfc849b3629c27d769db800467baa62
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Aug 10 18:03:22 2010 -0700

    dma-mapping: unify dma_get_cache_alignment implementations
    
    dma_get_cache_alignment returns the minimum DMA alignment.  Architectures
    defines it as ARCH_DMA_MINALIGN (formally ARCH_KMALLOC_MINALIGN).  So we
    can unify dma_get_cache_alignment implementations.
    
    Note that some architectures implement dma_get_cache_alignment wrongly.
    dma_get_cache_alignment() should return the minimum DMA alignment.  So
    fully-coherent architectures should return 1.  This patch also fixes this
    issue.
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 69ce0727edb5..f4a996d5ae96 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -144,11 +144,6 @@ static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 	return 0;
 }
 
-static inline int dma_get_cache_alignment(void)
-{
-	return 32;
-}
-
 static inline int dma_is_consistent(struct device *dev, dma_addr_t handle)
 {
 	return !!arch_is_coherent();

commit 4fa5518c65df7a2c4b6c58061ac53d0b01228042
Author: Eric Miao <eric.y.miao@gmail.com>
Date:   Sat Jun 5 15:16:17 2010 +0800

    [ARM] pxa: remove now unnecessary dma_needs_bounce()
    
    With a correct dev->dma_mask before calling dmabounce_register_dev(),
    dma_needs_bounce() is not necessary.
    
    The sa1111, though, is a bit complicated. Until it's fully understood
    and fixed, dma_needs_bounce() for sa1111 is kept if CONFIG_SA1111 is
    enabled with no side effect (with the condition of machine_is_*)
    
    Thanks for Mike Rapoport to fix one error in the original version of
    the patch and get this tested.
    
    Acked-by: Mike Rapoport <mike@compulab.co.il>
    Signed-off-by: Eric Miao <eric.y.miao@gmail.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 69ce0727edb5..b5ccc6a993d5 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -298,7 +298,15 @@ extern void dmabounce_unregister_dev(struct device *);
  * DMA access and 1 if the buffer needs to be bounced.
  *
  */
+#ifdef CONFIG_SA1111
 extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
+#else
+static inline int dma_needs_bounce(struct device *dev, dma_addr_t addr,
+				   size_t size)
+{
+	return 0;
+}
+#endif
 
 /*
  * The DMA API, implemented by dmabounce.c.  See below for descriptions.

commit 6fee48cd330c68332f9712bc968d934a1a84a32a
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Wed Mar 10 15:23:40 2010 -0800

    dma-mapping: arm: use generic pci_set_dma_mask and pci_set_consistent_dma_mask
    
    This converts arm to the generic pci_set_dma_mask and
    pci_set_consistent_dma_mask (removes HAVE_ARCH_PCI_SET_DMA_MASK for
    dmabounce).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Looked-over-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Cc: Jesse Barnes <jbarnes@virtuousgeek.org>
    Cc: Greg KH <greg@kroah.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 256ee1c9f51a..69ce0727edb5 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -128,6 +128,14 @@ static inline int dma_supported(struct device *dev, u64 mask)
 
 static inline int dma_set_mask(struct device *dev, u64 dma_mask)
 {
+#ifdef CONFIG_DMABOUNCE
+	if (dev->archdata.dmabounce) {
+		if (dma_mask >= ISA_DMA_THRESHOLD)
+			return 0;
+		else
+			return -EIO;
+	}
+#endif
 	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
 		return -EIO;
 

commit 4ea0d7371e808628d11154b0d44140b70f05b998
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 24 16:27:17 2009 +0000

    ARM: dma-mapping: push buffer ownership down into dma-mapping.c
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index e850f5c1607b..256ee1c9f51a 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -56,47 +56,58 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 }
 #endif
 
-/*
- * Private support functions: these are not part of the API and are
- * liable to change.  Drivers must not use these.
- */
-extern void dma_cache_maint(const void *kaddr, size_t size, int rw);
-extern void dma_cache_maint_page(struct page *page, unsigned long offset,
-				 size_t size, int rw);
-
 /*
  * The DMA API is built upon the notion of "buffer ownership".  A buffer
  * is either exclusively owned by the CPU (and therefore may be accessed
  * by it) or exclusively owned by the DMA device.  These helper functions
  * represent the transitions between these two ownership states.
  *
- * As above, these are private support functions and not part of the API.
- * Drivers must not use these.
+ * Note, however, that on later ARMs, this notion does not work due to
+ * speculative prefetches.  We model our approach on the assumption that
+ * the CPU does do speculative prefetches, which means we clean caches
+ * before transfers and delay cache invalidation until transfer completion.
+ *
+ * Private support functions: these are not part of the API and are
+ * liable to change.  Drivers must not use these.
  */
 static inline void __dma_single_cpu_to_dev(const void *kaddr, size_t size,
 	enum dma_data_direction dir)
 {
+	extern void ___dma_single_cpu_to_dev(const void *, size_t,
+		enum dma_data_direction);
+
 	if (!arch_is_coherent())
-		dma_cache_maint(kaddr, size, dir);
+		___dma_single_cpu_to_dev(kaddr, size, dir);
 }
 
 static inline void __dma_single_dev_to_cpu(const void *kaddr, size_t size,
 	enum dma_data_direction dir)
 {
-	/* nothing to do */
+	extern void ___dma_single_dev_to_cpu(const void *, size_t,
+		enum dma_data_direction);
+
+	if (!arch_is_coherent())
+		___dma_single_dev_to_cpu(kaddr, size, dir);
 }
 
 static inline void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
 	size_t size, enum dma_data_direction dir)
 {
+	extern void ___dma_page_cpu_to_dev(struct page *, unsigned long,
+		size_t, enum dma_data_direction);
+
 	if (!arch_is_coherent())
-		dma_cache_maint_page(page, off, size, dir);
+		___dma_page_cpu_to_dev(page, off, size, dir);
 }
 
 static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
 	size_t size, enum dma_data_direction dir)
 {
-	/* nothing to do */
+	extern void ___dma_page_dev_to_cpu(struct page *, unsigned long,
+		size_t, enum dma_data_direction);
+
+	if (!arch_is_coherent())
+		___dma_page_dev_to_cpu(page, off, size, dir);
 }
 
 /*

commit 18eabe2347ae7a11b3db768695913724166dfb0e
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 16:52:16 2009 +0000

    ARM: dma-mapping: introduce the idea of buffer ownership
    
    The DMA API has the notion of buffer ownership; make it explicit in the
    ARM implementation of this API.  This gives us a set of hooks to allow
    us to deal with CPU cache issues arising from non-cache coherent DMA.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-By: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index a96300bf83fd..e850f5c1607b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -57,19 +57,48 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
 #endif
 
 /*
- * DMA-consistent mapping functions.  These allocate/free a region of
- * uncached, unwrite-buffered mapped memory space for use with DMA
- * devices.  This is the "generic" version.  The PCI specific version
- * is in pci.h
- *
- * Note: Drivers should NOT use this function directly, as it will break
- * platforms with CONFIG_DMABOUNCE.
- * Use the driver DMA support - see dma-mapping.h (dma_sync_*)
+ * Private support functions: these are not part of the API and are
+ * liable to change.  Drivers must not use these.
  */
 extern void dma_cache_maint(const void *kaddr, size_t size, int rw);
 extern void dma_cache_maint_page(struct page *page, unsigned long offset,
 				 size_t size, int rw);
 
+/*
+ * The DMA API is built upon the notion of "buffer ownership".  A buffer
+ * is either exclusively owned by the CPU (and therefore may be accessed
+ * by it) or exclusively owned by the DMA device.  These helper functions
+ * represent the transitions between these two ownership states.
+ *
+ * As above, these are private support functions and not part of the API.
+ * Drivers must not use these.
+ */
+static inline void __dma_single_cpu_to_dev(const void *kaddr, size_t size,
+	enum dma_data_direction dir)
+{
+	if (!arch_is_coherent())
+		dma_cache_maint(kaddr, size, dir);
+}
+
+static inline void __dma_single_dev_to_cpu(const void *kaddr, size_t size,
+	enum dma_data_direction dir)
+{
+	/* nothing to do */
+}
+
+static inline void __dma_page_cpu_to_dev(struct page *page, unsigned long off,
+	size_t size, enum dma_data_direction dir)
+{
+	if (!arch_is_coherent())
+		dma_cache_maint_page(page, off, size, dir);
+}
+
+static inline void __dma_page_dev_to_cpu(struct page *page, unsigned long off,
+	size_t size, enum dma_data_direction dir)
+{
+	/* nothing to do */
+}
+
 /*
  * Return whether the given device DMA address mask can be supported
  * properly.  For example, if your device can only drive the low 24-bits
@@ -304,8 +333,7 @@ static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 {
 	BUG_ON(!valid_dma_direction(dir));
 
-	if (!arch_is_coherent())
-		dma_cache_maint(cpu_addr, size, dir);
+	__dma_single_cpu_to_dev(cpu_addr, size, dir);
 
 	return virt_to_dma(dev, cpu_addr);
 }
@@ -329,8 +357,7 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 {
 	BUG_ON(!valid_dma_direction(dir));
 
-	if (!arch_is_coherent())
-		dma_cache_maint_page(page, offset, size, dir);
+	__dma_page_cpu_to_dev(page, offset, size, dir);
 
 	return page_to_dma(dev, page) + offset;
 }
@@ -352,7 +379,7 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	/* nothing to do */
+	__dma_single_dev_to_cpu(dma_to_virt(dev, handle), size, dir);
 }
 
 /**
@@ -372,7 +399,8 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	/* nothing to do */
+	__dma_page_dev_to_cpu(dma_to_page(dev, handle), handle & ~PAGE_MASK,
+		size, dir);
 }
 #endif /* CONFIG_DMABOUNCE */
 
@@ -400,7 +428,10 @@ static inline void dma_sync_single_range_for_cpu(struct device *dev,
 {
 	BUG_ON(!valid_dma_direction(dir));
 
-	dmabounce_sync_for_cpu(dev, handle, offset, size, dir);
+	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
+		return;
+
+	__dma_single_dev_to_cpu(dma_to_virt(dev, handle) + offset, size, dir);
 }
 
 static inline void dma_sync_single_range_for_device(struct device *dev,
@@ -412,8 +443,7 @@ static inline void dma_sync_single_range_for_device(struct device *dev,
 	if (!dmabounce_sync_for_device(dev, handle, offset, size, dir))
 		return;
 
-	if (!arch_is_coherent())
-		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
+	__dma_single_cpu_to_dev(dma_to_virt(dev, handle) + offset, size, dir);
 }
 
 static inline void dma_sync_single_for_cpu(struct device *dev,

commit 29cb8d0d249f6b8fa33683cc17622ff16ada834c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 16:10:10 2009 +0000

    ARM: dma-mapping: split dma_unmap_page() from dma_unmap_single()
    
    We will need to treat dma_unmap_page() differently from dma_unmap_single()
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index f06d80c22748..a96300bf83fd 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -257,9 +257,11 @@ extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
  */
 extern dma_addr_t dma_map_single(struct device *, void *, size_t,
 		enum dma_data_direction);
+extern void dma_unmap_single(struct device *, dma_addr_t, size_t,
+		enum dma_data_direction);
 extern dma_addr_t dma_map_page(struct device *, struct page *,
 		unsigned long, size_t, enum dma_data_direction);
-extern void dma_unmap_single(struct device *, dma_addr_t, size_t,
+extern void dma_unmap_page(struct device *, dma_addr_t, size_t,
 		enum dma_data_direction);
 
 /*
@@ -352,7 +354,6 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 {
 	/* nothing to do */
 }
-#endif /* CONFIG_DMABOUNCE */
 
 /**
  * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()
@@ -371,8 +372,9 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
 static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
 		size_t size, enum dma_data_direction dir)
 {
-	dma_unmap_single(dev, handle, size, dir);
+	/* nothing to do */
 }
+#endif /* CONFIG_DMABOUNCE */
 
 /**
  * dma_sync_single_range_for_cpu

commit ef1baed8870d1eebb0c08d9a466e703f1a21b484
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 16:07:16 2009 +0000

    ARM: dma-mapping: provide dma_to_page()
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 5d78eb163953..f06d80c22748 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -20,6 +20,11 @@ static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
 	return (dma_addr_t)__pfn_to_bus(page_to_pfn(page));
 }
 
+static inline struct page *dma_to_page(struct device *dev, dma_addr_t addr)
+{
+	return pfn_to_page(__bus_to_pfn(addr));
+}
+
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
 {
 	return (void *)__bus_to_virt(addr);
@@ -35,6 +40,11 @@ static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
 	return __arch_page_to_dma(dev, page);
 }
 
+static inline struct page *dma_to_page(struct device *dev, dma_addr_t addr)
+{
+	return __arch_dma_to_page(dev, addr);
+}
+
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
 {
 	return __arch_dma_to_virt(dev, addr);

commit 1c4a4f48a14861a567c8861355bc8252da3a003f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 15:58:30 2009 +0000

    ARM: dma-mapping: simplify page_to_dma() and __pfn_to_bus()
    
    The non-highmem() and the __pfn_to_bus() based page_to_dma() both
    compile to the same code, so its pointless having these two different
    approaches.  Use the __pfn_to_bus() based version.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Jamie Iles <jamie@jamieiles.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index ff46dfa68a97..5d78eb163953 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -15,20 +15,10 @@
  * must not be used by drivers.
  */
 #ifndef __arch_page_to_dma
-
-#if !defined(CONFIG_HIGHMEM)
-static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
-{
-	return (dma_addr_t)__virt_to_bus((unsigned long)page_address(page));
-}
-#elif defined(__pfn_to_bus)
 static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
 {
 	return (dma_addr_t)__pfn_to_bus(page_to_pfn(page));
 }
-#else
-#error "this machine class needs to define __arch_page_to_dma to use HIGHMEM"
-#endif
 
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
 {

commit 58edb515724f9e63e569536d01ac8d8f8ddb367a
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Sep 9 15:54:13 2008 -0400

    [ARM] make page_to_dma() highmem aware
    
    If a machine class has a custom __virt_to_bus() implementation then it
    must provide a __arch_page_to_dma() implementation as well which is
    _not_ based on page_address() to support highmem.
    
    This patch fixes existing __arch_page_to_dma() and provide a default
    implementation otherwise.  The default implementation for highmem is
    based on __pfn_to_bus() which is defined only when no custom
    __virt_to_bus() is provided by the machine class.
    
    That leaves only ebsa110 and footbridge which cannot support highmem
    until they provide their own __arch_page_to_dma() implementation.
    But highmem support on those legacy platforms with limited memory is
    certainly not a priority.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 59fa762e9c66..ff46dfa68a97 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -15,10 +15,20 @@
  * must not be used by drivers.
  */
 #ifndef __arch_page_to_dma
+
+#if !defined(CONFIG_HIGHMEM)
 static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
 {
 	return (dma_addr_t)__virt_to_bus((unsigned long)page_address(page));
 }
+#elif defined(__pfn_to_bus)
+static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
+{
+	return (dma_addr_t)__pfn_to_bus(page_to_pfn(page));
+}
+#else
+#error "this machine class needs to define __arch_page_to_dma to use HIGHMEM"
+#endif
 
 static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
 {

commit 43377453af83b8ff8c1c731da1508bd6b84ebfea
Author: Nicolas Pitre <nico@cam.org>
Date:   Thu Mar 12 22:52:09 2009 -0400

    [ARM] introduce dma_cache_maint_page()
    
    This is a helper to be used by the DMA mapping API to handle cache
    maintenance for memory identified by a page structure instead of a
    virtual address.  Those pages may or may not be highmem pages, and
    when they're highmem pages, they may or may not be virtually mapped.
    When they're not mapped then there is no L1 cache to worry about. But
    even in that case the L2 cache must be processed since unmapped highmem
    pages can still be L2 cached.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 22cb14ec3438..59fa762e9c66 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -57,6 +57,8 @@ static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
  * Use the driver DMA support - see dma-mapping.h (dma_sync_*)
  */
 extern void dma_cache_maint(const void *kaddr, size_t size, int rw);
+extern void dma_cache_maint_page(struct page *page, unsigned long offset,
+				 size_t size, int rw);
 
 /*
  * Return whether the given device DMA address mask can be supported
@@ -316,7 +318,7 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 	BUG_ON(!valid_dma_direction(dir));
 
 	if (!arch_is_coherent())
-		dma_cache_maint(page_address(page) + offset, size, dir);
+		dma_cache_maint_page(page, offset, size, dir);
 
 	return page_to_dma(dev, page) + offset;
 }

commit 1124d6d21f80ec10cc962e2961c21a8dd1e0ca6a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Oct 20 11:18:40 2008 +0100

    [ARM] dma: correct dma_supported() implementation
    
    dma_supported() is supposed to indicate whether the system can support
    the DMA mask it was passed, which depends on the maximal address which
    can be returned for DMA allocations.  If the mask is smaller than that,
    we are unable to guarantee that the driver can reliably obtain suitable
    memory.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 4ed149cbb32a..22cb14ec3438 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -69,7 +69,9 @@ extern void dma_cache_maint(const void *kaddr, size_t size, int rw);
  */
 static inline int dma_supported(struct device *dev, u64 mask)
 {
-	return dev->dma_mask && *dev->dma_mask != 0;
+	if (mask < ISA_DMA_THRESHOLD)
+		return 0;
+	return 1;
 }
 
 static inline int dma_set_mask(struct device *dev, u64 dma_mask)

commit 9fa767928fe738aba8e99dae511e91f02fe20b28
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Nov 13 14:33:51 2008 +0000

    [ARM] dma-mapping: fix compiler warning
    
    arch/arm/mm/dma-mapping.c: In function `dma_sync_sg_for_cpu':
    arch/arm/mm/dma-mapping.c:588: warning: statement with no effect
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 1cb8602dd9d5..4ed149cbb32a 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -256,8 +256,17 @@ int dmabounce_sync_for_cpu(struct device *, dma_addr_t, unsigned long,
 int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
 		size_t, enum dma_data_direction);
 #else
-#define dmabounce_sync_for_cpu(dev,dma,off,sz,dir)	(1)
-#define dmabounce_sync_for_device(dev,dma,off,sz,dir)	(1)
+static inline int dmabounce_sync_for_cpu(struct device *d, dma_addr_t addr,
+	unsigned long offset, size_t size, enum dma_data_direction dir)
+{
+	return 1;
+}
+
+static inline int dmabounce_sync_for_device(struct device *d, dma_addr_t addr,
+	unsigned long offset, size_t size, enum dma_data_direction dir)
+{
+	return 1;
+}
 
 
 /**

commit 7807c6098a716567fe408775c1c1999467088305
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Sep 30 11:30:24 2008 +0100

    [ARM] dma: fix some comments in dma-mapping.h
    
    ... to prevent people being mislead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index ad62020763f1..1cb8602dd9d5 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -297,8 +297,7 @@ static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
  * or written back.
  *
  * The device owns this memory once this call has completed.  The CPU
- * can regain ownership by calling dma_unmap_page() or
- * dma_sync_single_for_cpu().
+ * can regain ownership by calling dma_unmap_page().
  */
 static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size, enum dma_data_direction dir)
@@ -315,8 +314,8 @@ static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
  * dma_unmap_single - unmap a single buffer previously mapped
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
  * @handle: DMA address of buffer
- * @size: size of buffer to map
- * @dir: DMA transfer direction
+ * @size: size of buffer (same as passed to dma_map_single)
+ * @dir: DMA transfer direction (same as passed to dma_map_single)
  *
  * Unmap a single streaming mode DMA translation.  The handle and size
  * must match what was provided in the previous dma_map_single() call.
@@ -336,11 +335,11 @@ static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
  * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
  * @handle: DMA address of buffer
- * @size: size of buffer to map
- * @dir: DMA transfer direction
+ * @size: size of buffer (same as passed to dma_map_page)
+ * @dir: DMA transfer direction (same as passed to dma_map_page)
  *
- * Unmap a single streaming mode DMA translation.  The handle and size
- * must match what was provided in the previous dma_map_single() call.
+ * Unmap a page streaming mode DMA translation.  The handle and size
+ * must match what was provided in the previous dma_map_page() call.
  * All other usages are undefined.
  *
  * After this call, reads by the CPU to the buffer are guaranteed to see

commit 309dbbabee7b19e003e1ba4b98f43d28f390a84e
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Sep 29 19:50:59 2008 +0100

    [ARM] dma: don't touch cache on dma_*_for_cpu()
    
    As per the dma_unmap_* calls, we don't touch the cache when a DMA
    buffer transitions from device to CPU ownership.  Presently, no
    problems have been identified with speculative cache prefetching
    which in itself is a new feature in later architectures.  We may
    have to revisit the DMA API later for these architectures anyway.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 2544a087c213..ad62020763f1 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -376,11 +376,7 @@ static inline void dma_sync_single_range_for_cpu(struct device *dev,
 {
 	BUG_ON(!valid_dma_direction(dir));
 
-	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
-		return;
-
-	if (!arch_is_coherent())
-		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
+	dmabounce_sync_for_cpu(dev, handle, offset, size, dir);
 }
 
 static inline void dma_sync_single_range_for_device(struct device *dev,

commit 0e18b5d7c6339311f1e32e7b186ae3556c5b6d33
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Sep 29 13:48:17 2008 +0100

    [ARM] dma: add validation of DMA params
    
    Validate the direction argument like x86 does.  In addition,
    validate the dma_unmap_* parameters against those passed to
    dma_map_* when using the DMA bounce code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 1532b7a6079d..2544a087c213 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -277,6 +277,8 @@ int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
 static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 		size_t size, enum dma_data_direction dir)
 {
+	BUG_ON(!valid_dma_direction(dir));
+
 	if (!arch_is_coherent())
 		dma_cache_maint(cpu_addr, size, dir);
 
@@ -301,6 +303,8 @@ static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
 static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size, enum dma_data_direction dir)
 {
+	BUG_ON(!valid_dma_direction(dir));
+
 	if (!arch_is_coherent())
 		dma_cache_maint(page_address(page) + offset, size, dir);
 
@@ -370,6 +374,8 @@ static inline void dma_sync_single_range_for_cpu(struct device *dev,
 		dma_addr_t handle, unsigned long offset, size_t size,
 		enum dma_data_direction dir)
 {
+	BUG_ON(!valid_dma_direction(dir));
+
 	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
 		return;
 
@@ -381,6 +387,8 @@ static inline void dma_sync_single_range_for_device(struct device *dev,
 		dma_addr_t handle, unsigned long offset, size_t size,
 		enum dma_data_direction dir)
 {
+	BUG_ON(!valid_dma_direction(dir));
+
 	if (!dmabounce_sync_for_device(dev, handle, offset, size, dir))
 		return;
 

commit 3216a97bb0d5166ec5795aa3db1c3a02415ac060
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 22:23:31 2008 +0100

    [ARM] dma: coding style cleanups
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 1204dc958c43..1532b7a6079d 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -104,15 +104,14 @@ static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
  * Dummy noncoherent implementation.  We don't provide a dma_cache_sync
  * function so drivers using this API are highlighted with build warnings.
  */
-static inline void *
-dma_alloc_noncoherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp)
+static inline void *dma_alloc_noncoherent(struct device *dev, size_t size,
+		dma_addr_t *handle, gfp_t gfp)
 {
 	return NULL;
 }
 
-static inline void
-dma_free_noncoherent(struct device *dev, size_t size, void *cpu_addr,
-		     dma_addr_t handle)
+static inline void dma_free_noncoherent(struct device *dev, size_t size,
+		void *cpu_addr, dma_addr_t handle)
 {
 }
 
@@ -127,8 +126,7 @@ dma_free_noncoherent(struct device *dev, size_t size, void *cpu_addr,
  * return the CPU-viewed address, and sets @handle to be the
  * device-viewed address.
  */
-extern void *
-dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp);
+extern void *dma_alloc_coherent(struct device *, size_t, dma_addr_t *, gfp_t);
 
 /**
  * dma_free_coherent - free memory allocated by dma_alloc_coherent
@@ -143,9 +141,7 @@ dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gf
  * References to memory and mappings associated with cpu_addr/handle
  * during and after this call executing are illegal.
  */
-extern void
-dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
-		  dma_addr_t handle);
+extern void dma_free_coherent(struct device *, size_t, void *, dma_addr_t);
 
 /**
  * dma_mmap_coherent - map a coherent DMA allocation into user space
@@ -159,8 +155,8 @@ dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
  * into user space.  The coherent DMA buffer must not be freed by the
  * driver until the user space mapping has been released.
  */
-int dma_mmap_coherent(struct device *dev, struct vm_area_struct *vma,
-		      void *cpu_addr, dma_addr_t handle, size_t size);
+int dma_mmap_coherent(struct device *, struct vm_area_struct *,
+		void *, dma_addr_t, size_t);
 
 
 /**
@@ -174,14 +170,14 @@ int dma_mmap_coherent(struct device *dev, struct vm_area_struct *vma,
  * return the CPU-viewed address, and sets @handle to be the
  * device-viewed address.
  */
-extern void *
-dma_alloc_writecombine(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp);
+extern void *dma_alloc_writecombine(struct device *, size_t, dma_addr_t *,
+		gfp_t);
 
 #define dma_free_writecombine(dev,size,cpu_addr,handle) \
 	dma_free_coherent(dev,size,cpu_addr,handle)
 
-int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
-			  void *cpu_addr, dma_addr_t handle, size_t size);
+int dma_mmap_writecombine(struct device *, struct vm_area_struct *,
+		void *, dma_addr_t, size_t);
 
 
 #ifdef CONFIG_DMABOUNCE
@@ -209,7 +205,8 @@ int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
  * appropriate DMA pools for the device.
  *
  */
-extern int dmabounce_register_dev(struct device *, unsigned long, unsigned long);
+extern int dmabounce_register_dev(struct device *, unsigned long,
+		unsigned long);
 
 /**
  * dmabounce_unregister_dev
@@ -244,19 +241,20 @@ extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
 /*
  * The DMA API, implemented by dmabounce.c.  See below for descriptions.
  */
-extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_direction);
-extern dma_addr_t dma_map_page(struct device *dev, struct page *page,
-			unsigned long offset, size_t size,
-			enum dma_data_direction dir);
-extern void dma_unmap_single(struct device *, dma_addr_t, size_t, enum dma_data_direction);
+extern dma_addr_t dma_map_single(struct device *, void *, size_t,
+		enum dma_data_direction);
+extern dma_addr_t dma_map_page(struct device *, struct page *,
+		unsigned long, size_t, enum dma_data_direction);
+extern void dma_unmap_single(struct device *, dma_addr_t, size_t,
+		enum dma_data_direction);
 
 /*
  * Private functions
  */
 int dmabounce_sync_for_cpu(struct device *, dma_addr_t, unsigned long,
-			size_t, enum dma_data_direction);
+		size_t, enum dma_data_direction);
 int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
-			size_t, enum dma_data_direction);
+		size_t, enum dma_data_direction);
 #else
 #define dmabounce_sync_for_cpu(dev,dma,off,sz,dir)	(1)
 #define dmabounce_sync_for_device(dev,dma,off,sz,dir)	(1)
@@ -276,9 +274,8 @@ int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
  * can regain ownership by calling dma_unmap_single() or
  * dma_sync_single_for_cpu().
  */
-static inline dma_addr_t
-dma_map_single(struct device *dev, void *cpu_addr, size_t size,
-	       enum dma_data_direction dir)
+static inline dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
+		size_t size, enum dma_data_direction dir)
 {
 	if (!arch_is_coherent())
 		dma_cache_maint(cpu_addr, size, dir);
@@ -286,7 +283,6 @@ dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 	return virt_to_dma(dev, cpu_addr);
 }
 
-
 /**
  * dma_map_page - map a portion of a page for streaming DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -302,10 +298,8 @@ dma_map_single(struct device *dev, void *cpu_addr, size_t size,
  * can regain ownership by calling dma_unmap_page() or
  * dma_sync_single_for_cpu().
  */
-static inline dma_addr_t
-dma_map_page(struct device *dev, struct page *page,
-	     unsigned long offset, size_t size,
-	     enum dma_data_direction dir)
+static inline dma_addr_t dma_map_page(struct device *dev, struct page *page,
+	     unsigned long offset, size_t size, enum dma_data_direction dir)
 {
 	if (!arch_is_coherent())
 		dma_cache_maint(page_address(page) + offset, size, dir);
@@ -327,9 +321,8 @@ dma_map_page(struct device *dev, struct page *page,
  * After this call, reads by the CPU to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
-static inline void
-dma_unmap_single(struct device *dev, dma_addr_t handle, size_t size,
-		 enum dma_data_direction dir)
+static inline void dma_unmap_single(struct device *dev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir)
 {
 	/* nothing to do */
 }
@@ -349,9 +342,8 @@ dma_unmap_single(struct device *dev, dma_addr_t handle, size_t size,
  * After this call, reads by the CPU to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
-static inline void
-dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,
-	       enum dma_data_direction dir)
+static inline void dma_unmap_page(struct device *dev, dma_addr_t handle,
+		size_t size, enum dma_data_direction dir)
 {
 	dma_unmap_single(dev, handle, size, dir);
 }
@@ -374,10 +366,9 @@ dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,
  * must first the perform a dma_sync_for_device, and then the
  * device again owns the buffer.
  */
-static inline void
-dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t handle,
-			      unsigned long offset, size_t size,
-			      enum dma_data_direction dir)
+static inline void dma_sync_single_range_for_cpu(struct device *dev,
+		dma_addr_t handle, unsigned long offset, size_t size,
+		enum dma_data_direction dir)
 {
 	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
 		return;
@@ -386,10 +377,9 @@ dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t handle,
 		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
 }
 
-static inline void
-dma_sync_single_range_for_device(struct device *dev, dma_addr_t handle,
-				 unsigned long offset, size_t size,
-				 enum dma_data_direction dir)
+static inline void dma_sync_single_range_for_device(struct device *dev,
+		dma_addr_t handle, unsigned long offset, size_t size,
+		enum dma_data_direction dir)
 {
 	if (!dmabounce_sync_for_device(dev, handle, offset, size, dir))
 		return;
@@ -398,16 +388,14 @@ dma_sync_single_range_for_device(struct device *dev, dma_addr_t handle,
 		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
 }
 
-static inline void
-dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
-			enum dma_data_direction dir)
+static inline void dma_sync_single_for_cpu(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 {
 	dma_sync_single_range_for_cpu(dev, handle, 0, size, dir);
 }
 
-static inline void
-dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
-			   enum dma_data_direction dir)
+static inline void dma_sync_single_for_device(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
 {
 	dma_sync_single_range_for_device(dev, handle, 0, size, dir);
 }
@@ -415,10 +403,14 @@ dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
 /*
  * The scatter list versions of the above methods.
  */
-extern int dma_map_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
-extern void dma_unmap_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
-extern void dma_sync_sg_for_cpu(struct device*, struct scatterlist*, int, enum dma_data_direction);
-extern void dma_sync_sg_for_device(struct device*, struct scatterlist*, int, enum dma_data_direction);
+extern int dma_map_sg(struct device *, struct scatterlist *, int,
+		enum dma_data_direction);
+extern void dma_unmap_sg(struct device *, struct scatterlist *, int,
+		enum dma_data_direction);
+extern void dma_sync_sg_for_cpu(struct device *, struct scatterlist *, int,
+		enum dma_data_direction);
+extern void dma_sync_sg_for_device(struct device *, struct scatterlist *, int,
+		enum dma_data_direction);
 
 
 #endif /* __KERNEL__ */

commit 125ab12acf64ff86b55d20e14db20becd917b7c4
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 22:16:22 2008 +0100

    [ARM] dma: fix dmabounce dma_sync_xxx() implementations
    
    The dmabounce dma_sync_xxx() implementation have been broken for
    quite some time; they all copy data between the DMA buffer and
    the CPU visible buffer no irrespective of the change of ownership.
    (IOW, a DMA_FROM_DEVICE mapping copies data from the DMA buffer
    to the CPU buffer during a call to dma_sync_single_for_device().)
    
    Fix it by getting rid of sync_single(), moving the contents into
    the recently created dmabounce_sync_for_xxx() functions and adjusting
    appropriately.
    
    This also makes it possible to properly support the DMA range sync
    functions.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index c003ad390def..1204dc958c43 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -241,6 +241,15 @@ extern void dmabounce_unregister_dev(struct device *);
  */
 extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
 
+/*
+ * The DMA API, implemented by dmabounce.c.  See below for descriptions.
+ */
+extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_direction);
+extern dma_addr_t dma_map_page(struct device *dev, struct page *page,
+			unsigned long offset, size_t size,
+			enum dma_data_direction dir);
+extern void dma_unmap_single(struct device *, dma_addr_t, size_t, enum dma_data_direction);
+
 /*
  * Private functions
  */
@@ -251,7 +260,6 @@ int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
 #else
 #define dmabounce_sync_for_cpu(dev,dma,off,sz,dir)	(1)
 #define dmabounce_sync_for_device(dev,dma,off,sz,dir)	(1)
-#endif /* CONFIG_DMABOUNCE */
 
 
 /**
@@ -268,7 +276,6 @@ int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
  * can regain ownership by calling dma_unmap_single() or
  * dma_sync_single_for_cpu().
  */
-#ifndef CONFIG_DMABOUNCE
 static inline dma_addr_t
 dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 	       enum dma_data_direction dir)
@@ -278,9 +285,7 @@ dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 
 	return virt_to_dma(dev, cpu_addr);
 }
-#else
-extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_direction);
-#endif
+
 
 /**
  * dma_map_page - map a portion of a page for streaming DMA
@@ -297,7 +302,6 @@ extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_d
  * can regain ownership by calling dma_unmap_page() or
  * dma_sync_single_for_cpu().
  */
-#ifndef CONFIG_DMABOUNCE
 static inline dma_addr_t
 dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size,
@@ -308,11 +312,6 @@ dma_map_page(struct device *dev, struct page *page,
 
 	return page_to_dma(dev, page) + offset;
 }
-#else
-extern dma_addr_t dma_map_page(struct device *dev, struct page *page,
-			unsigned long offset, size_t size,
-			enum dma_data_direction dir);
-#endif
 
 /**
  * dma_unmap_single - unmap a single buffer previously mapped
@@ -328,16 +327,13 @@ extern dma_addr_t dma_map_page(struct device *dev, struct page *page,
  * After this call, reads by the CPU to the buffer are guaranteed to see
  * whatever the device wrote there.
  */
-#ifndef CONFIG_DMABOUNCE
 static inline void
 dma_unmap_single(struct device *dev, dma_addr_t handle, size_t size,
 		 enum dma_data_direction dir)
 {
 	/* nothing to do */
 }
-#else
-extern void dma_unmap_single(struct device *, dma_addr_t, size_t, enum dma_data_direction);
-#endif
+#endif /* CONFIG_DMABOUNCE */
 
 /**
  * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()

commit 8c8a0ec57ee285ff407e9a64b3a5a37eaf800ad8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 21:52:49 2008 +0100

    [ARM] dma: use new dmabounce_sync_for_xxx() for dma_sync_single_xxx()
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 29404f71ab87..c003ad390def 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -184,6 +184,76 @@ int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
 			  void *cpu_addr, dma_addr_t handle, size_t size);
 
 
+#ifdef CONFIG_DMABOUNCE
+/*
+ * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
+ * and utilize bounce buffers as needed to work around limited DMA windows.
+ *
+ * On the SA-1111, a bug limits DMA to only certain regions of RAM.
+ * On the IXP425, the PCI inbound window is 64MB (256MB total RAM)
+ * On some ADI engineering systems, PCI inbound window is 32MB (12MB total RAM)
+ *
+ * The following are helper functions used by the dmabounce subystem
+ *
+ */
+
+/**
+ * dmabounce_register_dev
+ *
+ * @dev: valid struct device pointer
+ * @small_buf_size: size of buffers to use with small buffer pool
+ * @large_buf_size: size of buffers to use with large buffer pool (can be 0)
+ *
+ * This function should be called by low-level platform code to register
+ * a device as requireing DMA buffer bouncing. The function will allocate
+ * appropriate DMA pools for the device.
+ *
+ */
+extern int dmabounce_register_dev(struct device *, unsigned long, unsigned long);
+
+/**
+ * dmabounce_unregister_dev
+ *
+ * @dev: valid struct device pointer
+ *
+ * This function should be called by low-level platform code when device
+ * that was previously registered with dmabounce_register_dev is removed
+ * from the system.
+ *
+ */
+extern void dmabounce_unregister_dev(struct device *);
+
+/**
+ * dma_needs_bounce
+ *
+ * @dev: valid struct device pointer
+ * @dma_handle: dma_handle of unbounced buffer
+ * @size: size of region being mapped
+ *
+ * Platforms that utilize the dmabounce mechanism must implement
+ * this function.
+ *
+ * The dmabounce routines call this function whenever a dma-mapping
+ * is requested to determine whether a given buffer needs to be bounced
+ * or not. The function must return 0 if the buffer is OK for
+ * DMA access and 1 if the buffer needs to be bounced.
+ *
+ */
+extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
+
+/*
+ * Private functions
+ */
+int dmabounce_sync_for_cpu(struct device *, dma_addr_t, unsigned long,
+			size_t, enum dma_data_direction);
+int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
+			size_t, enum dma_data_direction);
+#else
+#define dmabounce_sync_for_cpu(dev,dma,off,sz,dir)	(1)
+#define dmabounce_sync_for_device(dev,dma,off,sz,dir)	(1)
+#endif /* CONFIG_DMABOUNCE */
+
+
 /**
  * dma_map_single - map a single buffer for streaming DMA
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -308,12 +378,14 @@ dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,
  * must first the perform a dma_sync_for_device, and then the
  * device again owns the buffer.
  */
-#ifndef CONFIG_DMABOUNCE
 static inline void
 dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t handle,
 			      unsigned long offset, size_t size,
 			      enum dma_data_direction dir)
 {
+	if (!dmabounce_sync_for_cpu(dev, handle, offset, size, dir))
+		return;
+
 	if (!arch_is_coherent())
 		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
 }
@@ -323,13 +395,12 @@ dma_sync_single_range_for_device(struct device *dev, dma_addr_t handle,
 				 unsigned long offset, size_t size,
 				 enum dma_data_direction dir)
 {
+	if (!dmabounce_sync_for_device(dev, handle, offset, size, dir))
+		return;
+
 	if (!arch_is_coherent())
 		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
 }
-#else
-extern void dma_sync_single_range_for_cpu(struct device *, dma_addr_t, unsigned long, size_t, enum dma_data_direction);
-extern void dma_sync_single_range_for_device(struct device *, dma_addr_t, unsigned long, size_t, enum dma_data_direction);
-#endif
 
 static inline void
 dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
@@ -354,74 +425,5 @@ extern void dma_sync_sg_for_cpu(struct device*, struct scatterlist*, int, enum d
 extern void dma_sync_sg_for_device(struct device*, struct scatterlist*, int, enum dma_data_direction);
 
 
-#ifdef CONFIG_DMABOUNCE
-/*
- * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
- * and utilize bounce buffers as needed to work around limited DMA windows.
- *
- * On the SA-1111, a bug limits DMA to only certain regions of RAM.
- * On the IXP425, the PCI inbound window is 64MB (256MB total RAM)
- * On some ADI engineering systems, PCI inbound window is 32MB (12MB total RAM)
- *
- * The following are helper functions used by the dmabounce subystem
- *
- */
-
-/**
- * dmabounce_register_dev
- *
- * @dev: valid struct device pointer
- * @small_buf_size: size of buffers to use with small buffer pool
- * @large_buf_size: size of buffers to use with large buffer pool (can be 0)
- *
- * This function should be called by low-level platform code to register
- * a device as requireing DMA buffer bouncing. The function will allocate
- * appropriate DMA pools for the device.
- *
- */
-extern int dmabounce_register_dev(struct device *, unsigned long, unsigned long);
-
-/**
- * dmabounce_unregister_dev
- *
- * @dev: valid struct device pointer
- *
- * This function should be called by low-level platform code when device
- * that was previously registered with dmabounce_register_dev is removed
- * from the system.
- *
- */
-extern void dmabounce_unregister_dev(struct device *);
-
-/**
- * dma_needs_bounce
- *
- * @dev: valid struct device pointer
- * @dma_handle: dma_handle of unbounced buffer
- * @size: size of region being mapped
- *
- * Platforms that utilize the dmabounce mechanism must implement
- * this function.
- *
- * The dmabounce routines call this function whenever a dma-mapping
- * is requested to determine whether a given buffer needs to be bounced
- * or not. The function must return 0 if the buffer is OK for
- * DMA access and 1 if the buffer needs to be bounced.
- *
- */
-extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
-
-/*
- * Private functions
- */
-int dmabounce_sync_for_cpu(struct device *, dma_addr_t, unsigned long,
-			size_t, enum dma_data_direction);
-int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
-			size_t, enum dma_data_direction);
-#else
-#define dmabounce_sync_for_cpu(dev,dma,off,sz,dir)	(1)
-#define dmabounce_sync_for_device(dev,dma,off,sz,dir)	(1)
-#endif /* CONFIG_DMABOUNCE */
-
 #endif /* __KERNEL__ */
 #endif

commit 2638b4dbe768aba023a06acd8e7eba708bb76ee6
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 21:38:41 2008 +0100

    [ARM] dma: Reduce to one dma_sync_sg_* implementation
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 856ee1bdee57..29404f71ab87 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -410,6 +410,17 @@ extern void dmabounce_unregister_dev(struct device *);
  *
  */
 extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
+
+/*
+ * Private functions
+ */
+int dmabounce_sync_for_cpu(struct device *, dma_addr_t, unsigned long,
+			size_t, enum dma_data_direction);
+int dmabounce_sync_for_device(struct device *, dma_addr_t, unsigned long,
+			size_t, enum dma_data_direction);
+#else
+#define dmabounce_sync_for_cpu(dev,dma,off,sz,dir)	(1)
+#define dmabounce_sync_for_device(dev,dma,off,sz,dir)	(1)
 #endif /* CONFIG_DMABOUNCE */
 
 #endif /* __KERNEL__ */

commit 56f55f8b58a02e95b401cb50df05086cabeaeeb5
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 20:59:12 2008 +0100

    [ARM] dma: provide a better dma_map_page() implementation
    
    We can translate a struct page directly to a DMA address using
    page_to_dma().  No need to use page_address() followed by
    virt_to_dma().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index eff954852c2b..856ee1bdee57 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -227,13 +227,22 @@ extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_d
  * can regain ownership by calling dma_unmap_page() or
  * dma_sync_single_for_cpu().
  */
+#ifndef CONFIG_DMABOUNCE
 static inline dma_addr_t
 dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size,
 	     enum dma_data_direction dir)
 {
-	return dma_map_single(dev, page_address(page) + offset, size, dir);
+	if (!arch_is_coherent())
+		dma_cache_maint(page_address(page) + offset, size, dir);
+
+	return page_to_dma(dev, page) + offset;
 }
+#else
+extern dma_addr_t dma_map_page(struct device *dev, struct page *page,
+			unsigned long offset, size_t size,
+			enum dma_data_direction dir);
+#endif
 
 /**
  * dma_unmap_single - unmap a single buffer previously mapped

commit afd1a321c49a250dab97cef6f2d3c3c9b9d0174a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 16:30:57 2008 +0100

    [ARM] Update dma_map_sg()/dma_unmap_sg() API
    
    Update the ARM DMA scatter gather APIs for the scatterlist changes.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 7b95d2058395..eff954852c2b 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -281,75 +281,6 @@ dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,
 	dma_unmap_single(dev, handle, size, dir);
 }
 
-/**
- * dma_map_sg - map a set of SG buffers for streaming mode DMA
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @sg: list of buffers
- * @nents: number of buffers to map
- * @dir: DMA transfer direction
- *
- * Map a set of buffers described by scatterlist in streaming
- * mode for DMA.  This is the scatter-gather version of the
- * above dma_map_single interface.  Here the scatter gather list
- * elements are each tagged with the appropriate dma address
- * and length.  They are obtained via sg_dma_{address,length}(SG).
- *
- * NOTE: An implementation may be able to use a smaller number of
- *       DMA address/length pairs than there are SG table elements.
- *       (for example via virtual mapping capabilities)
- *       The routine returns the number of addr/length pairs actually
- *       used, at most nents.
- *
- * Device ownership issues as mentioned above for dma_map_single are
- * the same here.
- */
-#ifndef CONFIG_DMABOUNCE
-static inline int
-dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
-	   enum dma_data_direction dir)
-{
-	int i;
-
-	for (i = 0; i < nents; i++, sg++) {
-		char *virt;
-
-		sg->dma_address = page_to_dma(dev, sg_page(sg)) + sg->offset;
-		virt = sg_virt(sg);
-
-		if (!arch_is_coherent())
-			dma_cache_maint(virt, sg->length, dir);
-	}
-
-	return nents;
-}
-#else
-extern int dma_map_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
-#endif
-
-/**
- * dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @sg: list of buffers
- * @nents: number of buffers to map
- * @dir: DMA transfer direction
- *
- * Unmap a set of streaming mode DMA translations.
- * Again, CPU read rules concerning calls here are the same as for
- * dma_unmap_single() above.
- */
-#ifndef CONFIG_DMABOUNCE
-static inline void
-dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
-	     enum dma_data_direction dir)
-{
-
-	/* nothing to do */
-}
-#else
-extern void dma_unmap_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
-#endif
-
-
 /**
  * dma_sync_single_range_for_cpu
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
@@ -405,50 +336,14 @@ dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
 	dma_sync_single_range_for_device(dev, handle, 0, size, dir);
 }
 
-
-/**
- * dma_sync_sg_for_cpu
- * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
- * @sg: list of buffers
- * @nents: number of buffers to map
- * @dir: DMA transfer direction
- *
- * Make physical memory consistent for a set of streaming
- * mode DMA translations after a transfer.
- *
- * The same as dma_sync_single_for_* but for a scatter-gather list,
- * same rules and usage.
+/*
+ * The scatter list versions of the above methods.
  */
-#ifndef CONFIG_DMABOUNCE
-static inline void
-dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
-		    enum dma_data_direction dir)
-{
-	int i;
-
-	for (i = 0; i < nents; i++, sg++) {
-		char *virt = sg_virt(sg);
-		if (!arch_is_coherent())
-			dma_cache_maint(virt, sg->length, dir);
-	}
-}
-
-static inline void
-dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
-		       enum dma_data_direction dir)
-{
-	int i;
-
-	for (i = 0; i < nents; i++, sg++) {
-		char *virt = sg_virt(sg);
-		if (!arch_is_coherent())
-			dma_cache_maint(virt, sg->length, dir);
-	}
-}
-#else
+extern int dma_map_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
+extern void dma_unmap_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
 extern void dma_sync_sg_for_cpu(struct device*, struct scatterlist*, int, enum dma_data_direction);
 extern void dma_sync_sg_for_device(struct device*, struct scatterlist*, int, enum dma_data_direction);
-#endif
+
 
 #ifdef CONFIG_DMABOUNCE
 /*

commit 9dd428680573d7867ee5e40fa3f059a98301d416
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 12:18:26 2008 +0100

    [ARM] dma-mapping: provide sync_range APIs
    
    Convert the existing dma_sync_single_for_* APIs to the new range based
    APIs, and make the dma_sync_single_for_* API a superset of it.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 6f45959fe2cc..7b95d2058395 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -351,11 +351,12 @@ extern void dma_unmap_sg(struct device *, struct scatterlist *, int, enum dma_da
 
 
 /**
- * dma_sync_single_for_cpu
+ * dma_sync_single_range_for_cpu
  * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
  * @handle: DMA address of buffer
- * @size: size of buffer to map
- * @dir: DMA transfer direction
+ * @offset: offset of region to start sync
+ * @size: size of region to sync
+ * @dir: DMA transfer direction (same as passed to dma_map_single)
  *
  * Make physical memory consistent for a single streaming mode DMA
  * translation after a transfer.
@@ -369,25 +370,41 @@ extern void dma_unmap_sg(struct device *, struct scatterlist *, int, enum dma_da
  */
 #ifndef CONFIG_DMABOUNCE
 static inline void
-dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
-			enum dma_data_direction dir)
+dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t handle,
+			      unsigned long offset, size_t size,
+			      enum dma_data_direction dir)
 {
 	if (!arch_is_coherent())
-		dma_cache_maint(dma_to_virt(dev, handle), size, dir);
+		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
 }
 
 static inline void
-dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
-			   enum dma_data_direction dir)
+dma_sync_single_range_for_device(struct device *dev, dma_addr_t handle,
+				 unsigned long offset, size_t size,
+				 enum dma_data_direction dir)
 {
 	if (!arch_is_coherent())
-		dma_cache_maint(dma_to_virt(dev, handle), size, dir);
+		dma_cache_maint(dma_to_virt(dev, handle) + offset, size, dir);
 }
 #else
-extern void dma_sync_single_for_cpu(struct device*, dma_addr_t, size_t, enum dma_data_direction);
-extern void dma_sync_single_for_device(struct device*, dma_addr_t, size_t, enum dma_data_direction);
+extern void dma_sync_single_range_for_cpu(struct device *, dma_addr_t, unsigned long, size_t, enum dma_data_direction);
+extern void dma_sync_single_range_for_device(struct device *, dma_addr_t, unsigned long, size_t, enum dma_data_direction);
 #endif
 
+static inline void
+dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
+			enum dma_data_direction dir)
+{
+	dma_sync_single_range_for_cpu(dev, handle, 0, size, dir);
+}
+
+static inline void
+dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
+			   enum dma_data_direction dir)
+{
+	dma_sync_single_range_for_device(dev, handle, 0, size, dir);
+}
+
 
 /**
  * dma_sync_sg_for_cpu

commit 98ed7d4b1a4eebc1ac25929b6968673bef4d54c3
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 12:10:49 2008 +0100

    [ARM] dma-mapping: improve type-safeness of DMA translations
    
    OMAP at least gets the return type(s) for the DMA translation functions
    wrong, which can lead to subtle errors.  Avoid this by moving the DMA
    translation functions to asm/dma-mapping.h, and converting them to
    inline functions.
    
    Fix the OMAP DMA translation macros to use the correct argument and
    result types.
    
    Also, remove the unnecessary casts in dmabounce.c.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
index 45329fca1b64..6f45959fe2cc 100644
--- a/arch/arm/include/asm/dma-mapping.h
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -3,11 +3,48 @@
 
 #ifdef __KERNEL__
 
-#include <linux/mm.h> /* need struct page */
-
+#include <linux/mm_types.h>
 #include <linux/scatterlist.h>
 
 #include <asm-generic/dma-coherent.h>
+#include <asm/memory.h>
+
+/*
+ * page_to_dma/dma_to_virt/virt_to_dma are architecture private functions
+ * used internally by the DMA-mapping API to provide DMA addresses. They
+ * must not be used by drivers.
+ */
+#ifndef __arch_page_to_dma
+static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
+{
+	return (dma_addr_t)__virt_to_bus((unsigned long)page_address(page));
+}
+
+static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
+{
+	return (void *)__bus_to_virt(addr);
+}
+
+static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
+{
+	return (dma_addr_t)__virt_to_bus((unsigned long)(addr));
+}
+#else
+static inline dma_addr_t page_to_dma(struct device *dev, struct page *page)
+{
+	return __arch_page_to_dma(dev, page);
+}
+
+static inline void *dma_to_virt(struct device *dev, dma_addr_t addr)
+{
+	return __arch_dma_to_virt(dev, addr);
+}
+
+static inline dma_addr_t virt_to_dma(struct device *dev, void *addr)
+{
+	return __arch_virt_to_dma(dev, addr);
+}
+#endif
 
 /*
  * DMA-consistent mapping functions.  These allocate/free a region of
@@ -169,7 +206,7 @@ dma_map_single(struct device *dev, void *cpu_addr, size_t size,
 	if (!arch_is_coherent())
 		dma_cache_maint(cpu_addr, size, dir);
 
-	return virt_to_dma(dev, (unsigned long)cpu_addr);
+	return virt_to_dma(dev, cpu_addr);
 }
 #else
 extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_direction);
@@ -195,7 +232,7 @@ dma_map_page(struct device *dev, struct page *page,
 	     unsigned long offset, size_t size,
 	     enum dma_data_direction dir)
 {
-	return dma_map_single(dev, page_address(page) + offset, size, (int)dir);
+	return dma_map_single(dev, page_address(page) + offset, size, dir);
 }
 
 /**
@@ -241,7 +278,7 @@ static inline void
 dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,
 	       enum dma_data_direction dir)
 {
-	dma_unmap_single(dev, handle, size, (int)dir);
+	dma_unmap_single(dev, handle, size, dir);
 }
 
 /**
@@ -336,7 +373,7 @@ dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
 			enum dma_data_direction dir)
 {
 	if (!arch_is_coherent())
-		dma_cache_maint((void *)dma_to_virt(dev, handle), size, dir);
+		dma_cache_maint(dma_to_virt(dev, handle), size, dir);
 }
 
 static inline void
@@ -344,7 +381,7 @@ dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
 			   enum dma_data_direction dir)
 {
 	if (!arch_is_coherent())
-		dma_cache_maint((void *)dma_to_virt(dev, handle), size, dir);
+		dma_cache_maint(dma_to_virt(dev, handle), size, dir);
 }
 #else
 extern void dma_sync_single_for_cpu(struct device*, dma_addr_t, size_t, enum dma_data_direction);

commit 7e31aa11fc672bbe0dd0da59513c9efe3809ced7
Merge: 071f4924844c 9cb7117fa485
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 3 10:39:02 2008 -0700

    Merge branch 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm
    
    * 'for-linus' of master.kernel.org:/home/rmk/linux-2.6-arm:
      [ARM] 5182/1: pxa: Fix pcm990 compilation
      [ARM] Fix explicit asm(-arm)?/arch-foo references
      [ARM] move include/asm-arm to arch/arm/include/asm
      [ARM] Remove explicit dependency for misc.o from compressed/Makefile
      [ARM] initrd: claim initrd memory exclusively
      [ARM] pxa: add support for L2 outer cache on XScale3 (attempt 2)
      [ARM] 5180/1: at91: Fix at91_nand -> atmel_nand rename fallout
      [ARM] add Sascha Hauer as Freescale i.MX Maintainer
      [ARM] i.MX: add missing clock functions exports
      [ARM] i.MX: remove set_imx_fb_info() export
      [ARM] mx1ads: make mmc platform data available for modules
      [ARM] mx2: add missing Kconfig dependency

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/dma-mapping.h b/arch/arm/include/asm/dma-mapping.h
new file mode 100644
index 000000000000..f41335ba6337
--- /dev/null
+++ b/arch/arm/include/asm/dma-mapping.h
@@ -0,0 +1,456 @@
+#ifndef ASMARM_DMA_MAPPING_H
+#define ASMARM_DMA_MAPPING_H
+
+#ifdef __KERNEL__
+
+#include <linux/mm.h> /* need struct page */
+
+#include <linux/scatterlist.h>
+
+/*
+ * DMA-consistent mapping functions.  These allocate/free a region of
+ * uncached, unwrite-buffered mapped memory space for use with DMA
+ * devices.  This is the "generic" version.  The PCI specific version
+ * is in pci.h
+ *
+ * Note: Drivers should NOT use this function directly, as it will break
+ * platforms with CONFIG_DMABOUNCE.
+ * Use the driver DMA support - see dma-mapping.h (dma_sync_*)
+ */
+extern void dma_cache_maint(const void *kaddr, size_t size, int rw);
+
+/*
+ * Return whether the given device DMA address mask can be supported
+ * properly.  For example, if your device can only drive the low 24-bits
+ * during bus mastering, then you would pass 0x00ffffff as the mask
+ * to this function.
+ *
+ * FIXME: This should really be a platform specific issue - we should
+ * return false if GFP_DMA allocations may not satisfy the supplied 'mask'.
+ */
+static inline int dma_supported(struct device *dev, u64 mask)
+{
+	return dev->dma_mask && *dev->dma_mask != 0;
+}
+
+static inline int dma_set_mask(struct device *dev, u64 dma_mask)
+{
+	if (!dev->dma_mask || !dma_supported(dev, dma_mask))
+		return -EIO;
+
+	*dev->dma_mask = dma_mask;
+
+	return 0;
+}
+
+static inline int dma_get_cache_alignment(void)
+{
+	return 32;
+}
+
+static inline int dma_is_consistent(struct device *dev, dma_addr_t handle)
+{
+	return !!arch_is_coherent();
+}
+
+/*
+ * DMA errors are defined by all-bits-set in the DMA address.
+ */
+static inline int dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+{
+	return dma_addr == ~0;
+}
+
+/*
+ * Dummy noncoherent implementation.  We don't provide a dma_cache_sync
+ * function so drivers using this API are highlighted with build warnings.
+ */
+static inline void *
+dma_alloc_noncoherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp)
+{
+	return NULL;
+}
+
+static inline void
+dma_free_noncoherent(struct device *dev, size_t size, void *cpu_addr,
+		     dma_addr_t handle)
+{
+}
+
+/**
+ * dma_alloc_coherent - allocate consistent memory for DMA
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @size: required memory size
+ * @handle: bus-specific DMA address
+ *
+ * Allocate some uncached, unbuffered memory for a device for
+ * performing DMA.  This function allocates pages, and will
+ * return the CPU-viewed address, and sets @handle to be the
+ * device-viewed address.
+ */
+extern void *
+dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp);
+
+/**
+ * dma_free_coherent - free memory allocated by dma_alloc_coherent
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @size: size of memory originally requested in dma_alloc_coherent
+ * @cpu_addr: CPU-view address returned from dma_alloc_coherent
+ * @handle: device-view address returned from dma_alloc_coherent
+ *
+ * Free (and unmap) a DMA buffer previously allocated by
+ * dma_alloc_coherent().
+ *
+ * References to memory and mappings associated with cpu_addr/handle
+ * during and after this call executing are illegal.
+ */
+extern void
+dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
+		  dma_addr_t handle);
+
+/**
+ * dma_mmap_coherent - map a coherent DMA allocation into user space
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @vma: vm_area_struct describing requested user mapping
+ * @cpu_addr: kernel CPU-view address returned from dma_alloc_coherent
+ * @handle: device-view address returned from dma_alloc_coherent
+ * @size: size of memory originally requested in dma_alloc_coherent
+ *
+ * Map a coherent DMA buffer previously allocated by dma_alloc_coherent
+ * into user space.  The coherent DMA buffer must not be freed by the
+ * driver until the user space mapping has been released.
+ */
+int dma_mmap_coherent(struct device *dev, struct vm_area_struct *vma,
+		      void *cpu_addr, dma_addr_t handle, size_t size);
+
+
+/**
+ * dma_alloc_writecombine - allocate writecombining memory for DMA
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @size: required memory size
+ * @handle: bus-specific DMA address
+ *
+ * Allocate some uncached, buffered memory for a device for
+ * performing DMA.  This function allocates pages, and will
+ * return the CPU-viewed address, and sets @handle to be the
+ * device-viewed address.
+ */
+extern void *
+dma_alloc_writecombine(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp);
+
+#define dma_free_writecombine(dev,size,cpu_addr,handle) \
+	dma_free_coherent(dev,size,cpu_addr,handle)
+
+int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
+			  void *cpu_addr, dma_addr_t handle, size_t size);
+
+
+/**
+ * dma_map_single - map a single buffer for streaming DMA
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @cpu_addr: CPU direct mapped address of buffer
+ * @size: size of buffer to map
+ * @dir: DMA transfer direction
+ *
+ * Ensure that any data held in the cache is appropriately discarded
+ * or written back.
+ *
+ * The device owns this memory once this call has completed.  The CPU
+ * can regain ownership by calling dma_unmap_single() or
+ * dma_sync_single_for_cpu().
+ */
+#ifndef CONFIG_DMABOUNCE
+static inline dma_addr_t
+dma_map_single(struct device *dev, void *cpu_addr, size_t size,
+	       enum dma_data_direction dir)
+{
+	if (!arch_is_coherent())
+		dma_cache_maint(cpu_addr, size, dir);
+
+	return virt_to_dma(dev, (unsigned long)cpu_addr);
+}
+#else
+extern dma_addr_t dma_map_single(struct device *,void *, size_t, enum dma_data_direction);
+#endif
+
+/**
+ * dma_map_page - map a portion of a page for streaming DMA
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @page: page that buffer resides in
+ * @offset: offset into page for start of buffer
+ * @size: size of buffer to map
+ * @dir: DMA transfer direction
+ *
+ * Ensure that any data held in the cache is appropriately discarded
+ * or written back.
+ *
+ * The device owns this memory once this call has completed.  The CPU
+ * can regain ownership by calling dma_unmap_page() or
+ * dma_sync_single_for_cpu().
+ */
+static inline dma_addr_t
+dma_map_page(struct device *dev, struct page *page,
+	     unsigned long offset, size_t size,
+	     enum dma_data_direction dir)
+{
+	return dma_map_single(dev, page_address(page) + offset, size, (int)dir);
+}
+
+/**
+ * dma_unmap_single - unmap a single buffer previously mapped
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @handle: DMA address of buffer
+ * @size: size of buffer to map
+ * @dir: DMA transfer direction
+ *
+ * Unmap a single streaming mode DMA translation.  The handle and size
+ * must match what was provided in the previous dma_map_single() call.
+ * All other usages are undefined.
+ *
+ * After this call, reads by the CPU to the buffer are guaranteed to see
+ * whatever the device wrote there.
+ */
+#ifndef CONFIG_DMABOUNCE
+static inline void
+dma_unmap_single(struct device *dev, dma_addr_t handle, size_t size,
+		 enum dma_data_direction dir)
+{
+	/* nothing to do */
+}
+#else
+extern void dma_unmap_single(struct device *, dma_addr_t, size_t, enum dma_data_direction);
+#endif
+
+/**
+ * dma_unmap_page - unmap a buffer previously mapped through dma_map_page()
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @handle: DMA address of buffer
+ * @size: size of buffer to map
+ * @dir: DMA transfer direction
+ *
+ * Unmap a single streaming mode DMA translation.  The handle and size
+ * must match what was provided in the previous dma_map_single() call.
+ * All other usages are undefined.
+ *
+ * After this call, reads by the CPU to the buffer are guaranteed to see
+ * whatever the device wrote there.
+ */
+static inline void
+dma_unmap_page(struct device *dev, dma_addr_t handle, size_t size,
+	       enum dma_data_direction dir)
+{
+	dma_unmap_single(dev, handle, size, (int)dir);
+}
+
+/**
+ * dma_map_sg - map a set of SG buffers for streaming mode DMA
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @sg: list of buffers
+ * @nents: number of buffers to map
+ * @dir: DMA transfer direction
+ *
+ * Map a set of buffers described by scatterlist in streaming
+ * mode for DMA.  This is the scatter-gather version of the
+ * above dma_map_single interface.  Here the scatter gather list
+ * elements are each tagged with the appropriate dma address
+ * and length.  They are obtained via sg_dma_{address,length}(SG).
+ *
+ * NOTE: An implementation may be able to use a smaller number of
+ *       DMA address/length pairs than there are SG table elements.
+ *       (for example via virtual mapping capabilities)
+ *       The routine returns the number of addr/length pairs actually
+ *       used, at most nents.
+ *
+ * Device ownership issues as mentioned above for dma_map_single are
+ * the same here.
+ */
+#ifndef CONFIG_DMABOUNCE
+static inline int
+dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+	   enum dma_data_direction dir)
+{
+	int i;
+
+	for (i = 0; i < nents; i++, sg++) {
+		char *virt;
+
+		sg->dma_address = page_to_dma(dev, sg_page(sg)) + sg->offset;
+		virt = sg_virt(sg);
+
+		if (!arch_is_coherent())
+			dma_cache_maint(virt, sg->length, dir);
+	}
+
+	return nents;
+}
+#else
+extern int dma_map_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
+#endif
+
+/**
+ * dma_unmap_sg - unmap a set of SG buffers mapped by dma_map_sg
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @sg: list of buffers
+ * @nents: number of buffers to map
+ * @dir: DMA transfer direction
+ *
+ * Unmap a set of streaming mode DMA translations.
+ * Again, CPU read rules concerning calls here are the same as for
+ * dma_unmap_single() above.
+ */
+#ifndef CONFIG_DMABOUNCE
+static inline void
+dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
+	     enum dma_data_direction dir)
+{
+
+	/* nothing to do */
+}
+#else
+extern void dma_unmap_sg(struct device *, struct scatterlist *, int, enum dma_data_direction);
+#endif
+
+
+/**
+ * dma_sync_single_for_cpu
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @handle: DMA address of buffer
+ * @size: size of buffer to map
+ * @dir: DMA transfer direction
+ *
+ * Make physical memory consistent for a single streaming mode DMA
+ * translation after a transfer.
+ *
+ * If you perform a dma_map_single() but wish to interrogate the
+ * buffer using the cpu, yet do not wish to teardown the PCI dma
+ * mapping, you must call this function before doing so.  At the
+ * next point you give the PCI dma address back to the card, you
+ * must first the perform a dma_sync_for_device, and then the
+ * device again owns the buffer.
+ */
+#ifndef CONFIG_DMABOUNCE
+static inline void
+dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle, size_t size,
+			enum dma_data_direction dir)
+{
+	if (!arch_is_coherent())
+		dma_cache_maint((void *)dma_to_virt(dev, handle), size, dir);
+}
+
+static inline void
+dma_sync_single_for_device(struct device *dev, dma_addr_t handle, size_t size,
+			   enum dma_data_direction dir)
+{
+	if (!arch_is_coherent())
+		dma_cache_maint((void *)dma_to_virt(dev, handle), size, dir);
+}
+#else
+extern void dma_sync_single_for_cpu(struct device*, dma_addr_t, size_t, enum dma_data_direction);
+extern void dma_sync_single_for_device(struct device*, dma_addr_t, size_t, enum dma_data_direction);
+#endif
+
+
+/**
+ * dma_sync_sg_for_cpu
+ * @dev: valid struct device pointer, or NULL for ISA and EISA-like devices
+ * @sg: list of buffers
+ * @nents: number of buffers to map
+ * @dir: DMA transfer direction
+ *
+ * Make physical memory consistent for a set of streaming
+ * mode DMA translations after a transfer.
+ *
+ * The same as dma_sync_single_for_* but for a scatter-gather list,
+ * same rules and usage.
+ */
+#ifndef CONFIG_DMABOUNCE
+static inline void
+dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
+		    enum dma_data_direction dir)
+{
+	int i;
+
+	for (i = 0; i < nents; i++, sg++) {
+		char *virt = sg_virt(sg);
+		if (!arch_is_coherent())
+			dma_cache_maint(virt, sg->length, dir);
+	}
+}
+
+static inline void
+dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
+		       enum dma_data_direction dir)
+{
+	int i;
+
+	for (i = 0; i < nents; i++, sg++) {
+		char *virt = sg_virt(sg);
+		if (!arch_is_coherent())
+			dma_cache_maint(virt, sg->length, dir);
+	}
+}
+#else
+extern void dma_sync_sg_for_cpu(struct device*, struct scatterlist*, int, enum dma_data_direction);
+extern void dma_sync_sg_for_device(struct device*, struct scatterlist*, int, enum dma_data_direction);
+#endif
+
+#ifdef CONFIG_DMABOUNCE
+/*
+ * For SA-1111, IXP425, and ADI systems  the dma-mapping functions are "magic"
+ * and utilize bounce buffers as needed to work around limited DMA windows.
+ *
+ * On the SA-1111, a bug limits DMA to only certain regions of RAM.
+ * On the IXP425, the PCI inbound window is 64MB (256MB total RAM)
+ * On some ADI engineering systems, PCI inbound window is 32MB (12MB total RAM)
+ *
+ * The following are helper functions used by the dmabounce subystem
+ *
+ */
+
+/**
+ * dmabounce_register_dev
+ *
+ * @dev: valid struct device pointer
+ * @small_buf_size: size of buffers to use with small buffer pool
+ * @large_buf_size: size of buffers to use with large buffer pool (can be 0)
+ *
+ * This function should be called by low-level platform code to register
+ * a device as requireing DMA buffer bouncing. The function will allocate
+ * appropriate DMA pools for the device.
+ *
+ */
+extern int dmabounce_register_dev(struct device *, unsigned long, unsigned long);
+
+/**
+ * dmabounce_unregister_dev
+ *
+ * @dev: valid struct device pointer
+ *
+ * This function should be called by low-level platform code when device
+ * that was previously registered with dmabounce_register_dev is removed
+ * from the system.
+ *
+ */
+extern void dmabounce_unregister_dev(struct device *);
+
+/**
+ * dma_needs_bounce
+ *
+ * @dev: valid struct device pointer
+ * @dma_handle: dma_handle of unbounced buffer
+ * @size: size of region being mapped
+ *
+ * Platforms that utilize the dmabounce mechanism must implement
+ * this function.
+ *
+ * The dmabounce routines call this function whenever a dma-mapping
+ * is requested to determine whether a given buffer needs to be bounced
+ * or not. The function must return 0 if the buffer is OK for
+ * DMA access and 1 if the buffer needs to be bounced.
+ *
+ */
+extern int dma_needs_bounce(struct device*, dma_addr_t, size_t);
+#endif /* CONFIG_DMABOUNCE */
+
+#endif /* __KERNEL__ */
+#endif
