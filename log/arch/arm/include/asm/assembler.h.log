commit c2b0fc847f3122e5a4176c3772626a7a8facced0
Merge: 533b220f7be4 fb597f2a3932
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:36:32 2020 -0700

    Merge tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - remove a now unnecessary usage of the KERNEL_DS for
       sys_oabi_epoll_ctl()
    
     - update my email address in a number of drivers
    
     - decompressor EFI updates from Ard Biesheuvel
    
     - module unwind section handling updates
    
     - sparsemem Kconfig cleanups
    
     - make act_mm macro respect THREAD_SIZE
    
    * tag 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm:
      ARM: 8980/1: Allow either FLATMEM or SPARSEMEM on the multiplatform build
      ARM: 8979/1: Remove redundant ARCH_SPARSEMEM_DEFAULT setting
      ARM: 8978/1: mm: make act_mm() respect THREAD_SIZE
      ARM: decompressor: run decompressor in place if loaded via UEFI
      ARM: decompressor: move GOT into .data for EFI enabled builds
      ARM: decompressor: defer loading of the contents of the LC0 structure
      ARM: decompressor: split off _edata and stack base into separate object
      ARM: decompressor: move headroom variable out of LC0
      ARM: 8976/1: module: allow arch overrides for .init section names
      ARM: 8975/1: module: fix handling of unwind init sections
      ARM: 8974/1: use SPARSMEM_STATIC when SPARSEMEM is enabled
      ARM: 8971/1: replace the sole use of a symbol with its definition
      ARM: 8969/1: decompressor: simplify libfdt builds
      Update rmk's email address in various drivers
      ARM: compat: remove KERNEL_DS usage in sys_oabi_epoll_ctl()

commit 747ffc2fcf969eff9309d7f2d1d61cb8b9e1bb40
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Sun May 3 13:03:54 2020 +0100

    ARM: uaccess: consolidate uaccess asm to asm/uaccess-asm.h
    
    Consolidate the user access assembly code to asm/uaccess-asm.h.  This
    moves the csdb, check_uaccess, uaccess_mask_range_ptr, uaccess_enable,
    uaccess_disable, uaccess_save, uaccess_restore macros, and creates two
    new ones for exception entry and exit - uaccess_entry and uaccess_exit.
    
    This makes the uaccess_save and uaccess_restore macros private to
    asm/uaccess-asm.h.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 99929122dad7..3546d294d55f 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -18,11 +18,11 @@
 #endif
 
 #include <asm/ptrace.h>
-#include <asm/domain.h>
 #include <asm/opcodes-virt.h>
 #include <asm/asm-offsets.h>
 #include <asm/page.h>
 #include <asm/thread_info.h>
+#include <asm/uaccess-asm.h>
 
 #define IOMEM(x)	(x)
 
@@ -446,79 +446,6 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.size \name , . - \name
 	.endm
 
-	.macro	csdb
-#ifdef CONFIG_THUMB2_KERNEL
-	.inst.w	0xf3af8014
-#else
-	.inst	0xe320f014
-#endif
-	.endm
-
-	.macro check_uaccess, addr:req, size:req, limit:req, tmp:req, bad:req
-#ifndef CONFIG_CPU_USE_DOMAINS
-	adds	\tmp, \addr, #\size - 1
-	sbcscc	\tmp, \tmp, \limit
-	bcs	\bad
-#ifdef CONFIG_CPU_SPECTRE
-	movcs	\addr, #0
-	csdb
-#endif
-#endif
-	.endm
-
-	.macro uaccess_mask_range_ptr, addr:req, size:req, limit:req, tmp:req
-#ifdef CONFIG_CPU_SPECTRE
-	sub	\tmp, \limit, #1
-	subs	\tmp, \tmp, \addr	@ tmp = limit - 1 - addr
-	addhs	\tmp, \tmp, #1		@ if (tmp >= 0) {
-	subshs	\tmp, \tmp, \size	@ tmp = limit - (addr + size) }
-	movlo	\addr, #0		@ if (tmp < 0) addr = NULL
-	csdb
-#endif
-	.endm
-
-	.macro	uaccess_disable, tmp, isb=1
-#ifdef CONFIG_CPU_SW_DOMAIN_PAN
-	/*
-	 * Whenever we re-enter userspace, the domains should always be
-	 * set appropriately.
-	 */
-	mov	\tmp, #DACR_UACCESS_DISABLE
-	mcr	p15, 0, \tmp, c3, c0, 0		@ Set domain register
-	.if	\isb
-	instr_sync
-	.endif
-#endif
-	.endm
-
-	.macro	uaccess_enable, tmp, isb=1
-#ifdef CONFIG_CPU_SW_DOMAIN_PAN
-	/*
-	 * Whenever we re-enter userspace, the domains should always be
-	 * set appropriately.
-	 */
-	mov	\tmp, #DACR_UACCESS_ENABLE
-	mcr	p15, 0, \tmp, c3, c0, 0
-	.if	\isb
-	instr_sync
-	.endif
-#endif
-	.endm
-
-	.macro	uaccess_save, tmp
-#ifdef CONFIG_CPU_SW_DOMAIN_PAN
-	mrc	p15, 0, \tmp, c3, c0, 0
-	str	\tmp, [sp, #SVC_DACR]
-#endif
-	.endm
-
-	.macro	uaccess_restore
-#ifdef CONFIG_CPU_SW_DOMAIN_PAN
-	ldr	r0, [sp, #SVC_DACR]
-	mcr	p15, 0, r0, c3, c0, 0
-#endif
-	.endm
-
 	.irp	c,,eq,ne,cs,cc,mi,pl,vs,vc,hi,ls,ge,lt,gt,le,hs,lo
 	.macro	ret\c, reg
 #if __LINUX_ARM_ARCH__ < 6

commit a780e485b5768e78aef087502499714901b68cc4
Author: Jian Cai <caij2003@gmail.com>
Date:   Wed Apr 29 01:20:11 2020 +0100

    ARM: 8971/1: replace the sole use of a symbol with its definition
    
    ALT_UP_B macro sets symbol up_b_offset via .equ to an expression
    involving another symbol. The macro gets expanded twice when
    arch/arm/kernel/sleep.S is assembled, creating a scenario where
    up_b_offset is set to another expression involving symbols while its
    current value is based on symbols. LLVM integrated assembler does not
    allow such cases, and based on the documentation of binutils, "Values
    that are based on expressions involving other symbols are allowed, but
    some targets may restrict this to only being done once per assembly", so
    it may be better to avoid such cases as it is not clearly stated which
    targets should support or disallow them. The fix in this case is simple,
    as up_b_offset has only one use, so we can replace the use with the
    definition and get rid of up_b_offset.
    
     Link:https://github.com/ClangBuiltLinux/linux/issues/920
    
     Reviewed-by: Stefan Agner <stefan@agner.ch>
    
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Jian Cai <caij2003@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 99929122dad7..adee13126c62 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -269,10 +269,9 @@
 	.endif							;\
 	.popsection
 #define ALT_UP_B(label)					\
-	.equ	up_b_offset, label - 9998b			;\
 	.pushsection ".alt.smp.init", "a"			;\
 	.long	9998b						;\
-	W(b)	. + up_b_offset					;\
+	W(b)	. + (label - 9998b)					;\
 	.popsection
 #else
 #define ALT_SMP(instr...)

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index b59921a560da..99929122dad7 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -1,12 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  *  arch/arm/include/asm/assembler.h
  *
  *  Copyright (C) 1996-2000 Russell King
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
  *  This file contains arm architecture specific defines
  *  for the different processors.
  *

commit c001899a5d6c2d7a0f3b75b2307ddef137fb46a6
Author: Stefan Agner <stefan@agner.ch>
Date:   Mon Feb 18 00:56:58 2019 +0100

    ARM: 8843/1: use unified assembler in headers
    
    Use unified assembler syntax (UAL) in headers. Divided syntax is
    considered deprecated. This will also allow to build the kernel
    using LLVM's integrated assembler.
    
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 28a48e0d4cca..b59921a560da 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -376,9 +376,9 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.macro	usraccoff, instr, reg, ptr, inc, off, cond, abort, t=TUSER()
 9999:
 	.if	\inc == 1
-	\instr\cond\()b\()\t\().w \reg, [\ptr, #\off]
+	\instr\()b\t\cond\().w \reg, [\ptr, #\off]
 	.elseif	\inc == 4
-	\instr\cond\()\t\().w \reg, [\ptr, #\off]
+	\instr\t\cond\().w \reg, [\ptr, #\off]
 	.else
 	.error	"Unsupported inc macro argument"
 	.endif
@@ -417,9 +417,9 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.rept	\rept
 9999:
 	.if	\inc == 1
-	\instr\cond\()b\()\t \reg, [\ptr], #\inc
+	\instr\()b\t\cond \reg, [\ptr], #\inc
 	.elseif	\inc == 4
-	\instr\cond\()\t \reg, [\ptr], #\inc
+	\instr\t\cond \reg, [\ptr], #\inc
 	.else
 	.error	"Unsupported inc macro argument"
 	.endif
@@ -460,7 +460,7 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.macro check_uaccess, addr:req, size:req, limit:req, tmp:req, bad:req
 #ifndef CONFIG_CPU_USE_DOMAINS
 	adds	\tmp, \addr, #\size - 1
-	sbcccs	\tmp, \tmp, \limit
+	sbcscc	\tmp, \tmp, \limit
 	bcs	\bad
 #ifdef CONFIG_CPU_SPECTRE
 	movcs	\addr, #0
@@ -474,7 +474,7 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	sub	\tmp, \limit, #1
 	subs	\tmp, \tmp, \addr	@ tmp = limit - 1 - addr
 	addhs	\tmp, \tmp, #1		@ if (tmp >= 0) {
-	subhss	\tmp, \tmp, \size	@ tmp = limit - (addr + size) }
+	subshs	\tmp, \tmp, \size	@ tmp = limit - (addr + size) }
 	movlo	\addr, #0		@ if (tmp < 0) addr = NULL
 	csdb
 #endif

commit f441882a5229ffaef61a47bccd4518f7e2749cbc
Author: Vincent Whitchurch <vincent.whitchurch@axis.com>
Date:   Fri Nov 9 10:09:48 2018 +0100

    ARM: 8812/1: Optimise copy_{from/to}_user for !CPU_USE_DOMAINS
    
    ARMv6+ processors do not use CONFIG_CPU_USE_DOMAINS and use privileged
    ldr/str instructions in copy_{from/to}_user.  They are currently
    unnecessarily using single ldr/str instructions and can use ldm/stm
    instructions instead like memcpy does (but with appropriate fixup
    tables).
    
    This speeds up a "dd if=foo of=bar bs=32k" on a tmpfs filesystem by
    about 4% on my Cortex-A9.
    
    before:134217728 bytes (128.0MB) copied, 0.543848 seconds, 235.4MB/s
    before:134217728 bytes (128.0MB) copied, 0.538610 seconds, 237.6MB/s
    before:134217728 bytes (128.0MB) copied, 0.544356 seconds, 235.1MB/s
    before:134217728 bytes (128.0MB) copied, 0.544364 seconds, 235.1MB/s
    before:134217728 bytes (128.0MB) copied, 0.537130 seconds, 238.3MB/s
    before:134217728 bytes (128.0MB) copied, 0.533443 seconds, 240.0MB/s
    before:134217728 bytes (128.0MB) copied, 0.545691 seconds, 234.6MB/s
    before:134217728 bytes (128.0MB) copied, 0.534695 seconds, 239.4MB/s
    before:134217728 bytes (128.0MB) copied, 0.540561 seconds, 236.8MB/s
    before:134217728 bytes (128.0MB) copied, 0.541025 seconds, 236.6MB/s
    
     after:134217728 bytes (128.0MB) copied, 0.520445 seconds, 245.9MB/s
     after:134217728 bytes (128.0MB) copied, 0.527846 seconds, 242.5MB/s
     after:134217728 bytes (128.0MB) copied, 0.519510 seconds, 246.4MB/s
     after:134217728 bytes (128.0MB) copied, 0.527231 seconds, 242.8MB/s
     after:134217728 bytes (128.0MB) copied, 0.525030 seconds, 243.8MB/s
     after:134217728 bytes (128.0MB) copied, 0.524236 seconds, 244.2MB/s
     after:134217728 bytes (128.0MB) copied, 0.523659 seconds, 244.4MB/s
     after:134217728 bytes (128.0MB) copied, 0.525018 seconds, 243.8MB/s
     after:134217728 bytes (128.0MB) copied, 0.519249 seconds, 246.5MB/s
     after:134217728 bytes (128.0MB) copied, 0.518527 seconds, 246.9MB/s
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Vincent Whitchurch <vincent.whitchurch@axis.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 88286dd483ff..28a48e0d4cca 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -243,13 +243,15 @@
 	.endm
 #endif
 
-#define USER(x...)				\
+#define USERL(l, x...)				\
 9999:	x;					\
 	.pushsection __ex_table,"a";		\
 	.align	3;				\
-	.long	9999b,9001f;			\
+	.long	9999b,l;			\
 	.popsection
 
+#define USER(x...)	USERL(9001f, x)
+
 #ifdef CONFIG_SMP
 #define ALT_SMP(instr...)					\
 9998:	instr

commit 3e98d240981a33290afc9435d01ec248e5880354
Merge: 3a58ac65e2d7 f18aef742c8f a1d09e074250
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Wed Oct 10 13:53:33 2018 +0100

    Merge branches 'fixes', 'misc' and 'spectre' into for-next

commit afaf6838f4bc896a711180b702b388b8cfa638fc
Author: Julien Thierry <julien.thierry@arm.com>
Date:   Tue Sep 11 10:14:50 2018 +0100

    ARM: 8796/1: spectre-v1,v1.1: provide helpers for address sanitization
    
    Introduce C and asm helpers to sanitize user address, taking the
    address range they target into account.
    
    Use asm helper for existing sanitization in __copy_from_user().
    
    Signed-off-by: Julien Thierry <julien.thierry@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index f0515f60cff5..39651c1ec157 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -467,6 +467,17 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 #endif
 	.endm
 
+	.macro uaccess_mask_range_ptr, addr:req, size:req, limit:req, tmp:req
+#ifdef CONFIG_CPU_SPECTRE
+	sub	\tmp, \limit, #1
+	subs	\tmp, \tmp, \addr	@ tmp = limit - 1 - addr
+	addhs	\tmp, \tmp, #1		@ if (tmp >= 0) {
+	subhss	\tmp, \tmp, \size	@ tmp = limit - (addr + size) }
+	movlo	\addr, #0		@ if (tmp < 0) addr = NULL
+	csdb
+#endif
+	.endm
+
 	.macro	uaccess_disable, tmp, isb=1
 #ifdef CONFIG_CPU_SW_DOMAIN_PAN
 	/*

commit c61b466d4f886613c7e71de8282701646a4d999d
Merge: afc9f65e01cd 001a30c4d0e8 a3c0f84765bb
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Mon Aug 13 16:28:50 2018 +0100

    Merge branches 'fixes', 'misc' and 'spectre' into for-linus
    
    Conflicts:
            arch/arm/include/asm/uaccess.h
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

commit a3c0f84765bb429ba0fd23de1c57b5e1591c9389
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Mon May 14 09:40:24 2018 +0100

    ARM: spectre-v1: mitigate user accesses
    
    Spectre variant 1 attacks are about this sequence of pseudo-code:
    
            index = load(user-manipulated pointer);
            access(base + index * stride);
    
    In order for the cache side-channel to work, the access() must me made
    to memory which userspace can detect whether cache lines have been
    loaded.  On 32-bit ARM, this must be either user accessible memory, or
    a kernel mapping of that same user accessible memory.
    
    The problem occurs when the load() speculatively loads privileged data,
    and the subsequent access() is made to user accessible memory.
    
    Any load() which makes use of a user-maniplated pointer is a potential
    problem if the data it has loaded is used in a subsequent access.  This
    also applies for the access() if the data loaded by that access is used
    by a subsequent access.
    
    Harden the get_user() accessors against Spectre attacks by forcing out
    of bounds addresses to a NULL pointer.  This prevents get_user() being
    used as the load() step above.  As a side effect, put_user() will also
    be affected even though it isn't implicated.
    
    Also harden copy_from_user() by redoing the bounds check within the
    arm_copy_from_user() code, and NULLing the pointer if out of bounds.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index ef1386b1af9b..f0515f60cff5 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -460,6 +460,10 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	adds	\tmp, \addr, #\size - 1
 	sbcccs	\tmp, \tmp, \limit
 	bcs	\bad
+#ifdef CONFIG_CPU_SPECTRE
+	movcs	\addr, #0
+	csdb
+#endif
 #endif
 	.endm
 

commit 0ac000e86703dedea1000513dbb8a64d02930668
Merge: 92d44a42af81 83d41fb9c0eb 10573ae547c8
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Tue Jun 5 10:03:27 2018 +0100

    Merge branches 'fixes', 'misc' and 'spectre' into for-linus

commit a78d156587931a2c3b354534aa772febf6c9e855
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri May 11 11:15:29 2018 +0100

    ARM: spectre-v1: add speculation barrier (csdb) macros
    
    Add assembly and C macros for the new CSDB instruction.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Boot-tested-by: Tony Lindgren <tony@atomide.com>
    Reviewed-by: Tony Lindgren <tony@atomide.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index bc8d4bbd82e2..ef1386b1af9b 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -447,6 +447,14 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.size \name , . - \name
 	.endm
 
+	.macro	csdb
+#ifdef CONFIG_THUMB2_KERNEL
+	.inst.w	0xf3af8014
+#else
+	.inst	0xe320f014
+#endif
+	.endm
+
 	.macro check_uaccess, addr:req, size:req, limit:req, tmp:req, bad:req
 #ifndef CONFIG_CPU_USE_DOMAINS
 	adds	\tmp, \addr, #\size - 1

commit 0d73c3f8e7f6ee2aab1bb350f60c180f5ae21a2c
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sun May 13 05:04:29 2018 +0100

    ARM: 8772/1: kprobes: Prohibit kprobes on get_user functions
    
    Since do_undefinstr() uses get_user to get the undefined
    instruction, it can be called before kprobes processes
    recursive check. This can cause an infinit recursive
    exception.
    Prohibit probing on get_user functions.
    
    Fixes: 24ba613c9d6c ("ARM kprobes: core code")
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index bc8d4bbd82e2..9342904cccca 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -536,4 +536,14 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 #endif
 	.endm
 
+#ifdef CONFIG_KPROBES
+#define _ASM_NOKPROBE(entry)				\
+	.pushsection "_kprobe_blacklist", "aw" ;	\
+	.balign 4 ;					\
+	.long entry;					\
+	.popsection
+#else
+#define _ASM_NOKPROBE(entry)
+#endif
+
 #endif /* __ASM_ASSEMBLER_H__ */

commit 8bafae202c82dc257f649ea3c275a0f35ee15113
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri Nov 24 23:49:34 2017 +0000

    ARM: BUG if jumping to usermode address in kernel mode
    
    Detect if we are returning to usermode via the normal kernel exit paths
    but the saved PSR value indicates that we are in kernel mode.  This
    could occur due to corrupted stack state, which has been observed with
    "ftracetest".
    
    This ensures that we catch the problem case before we get to user code.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index ad301f107dd2..bc8d4bbd82e2 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -518,4 +518,22 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 #endif
 	.endm
 
+	.macro	bug, msg, line
+#ifdef CONFIG_THUMB2_KERNEL
+1:	.inst	0xde02
+#else
+1:	.inst	0xe7f001f2
+#endif
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+	.pushsection .rodata.str, "aMS", %progbits, 1
+2:	.asciz	"\msg"
+	.popsection
+	.pushsection __bug_table, "aw"
+	.align	2
+	.word	1b, 2b
+	.hword	\line
+	.popsection
+#endif
+	.endm
+
 #endif /* __ASM_ASSEMBLER_H__ */

commit ffa47aa678cfaa9b88e8a26cfb115b4768325121
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jun 30 18:03:59 2017 +0200

    ARM: Prepare for randomized task_struct
    
    With the new task struct randomization, we can run into a build
    failure for certain random seeds, which will place fields beyond
    the allow immediate size in the assembly:
    
    arch/arm/kernel/entry-armv.S: Assembler messages:
    arch/arm/kernel/entry-armv.S:803: Error: bad immediate value for offset (4096)
    
    Only two constants in asm-offset.h are affected, and I'm changing
    both of them here to work correctly in all configurations.
    
    One more macro has the problem, but is currently unused, so this
    removes it instead of adding complexity.
    
    Suggested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    [kees: Adjust commit log slightly]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 68b06f9c65de..ad301f107dd2 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -87,6 +87,8 @@
 #define CALGN(code...)
 #endif
 
+#define IMM12_MASK 0xfff
+
 /*
  * Enable and disable interrupts
  */

commit b2bf482a5099264fb75936b5b552cdf3c247c93a
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Tue Aug 30 17:28:43 2016 +0100

    ARM: 8605/1: V7M: fix notrace variant of save_and_disable_irqs
    
    Commit 8e43a905 "ARM: 7325/1: fix v7 boot with lockdep enabled"
    introduced notrace variant of save_and_disable_irqs to balance notrace
    variant of restore_irqs; however V7M case has been missed. It was not
    noticed because cache-v7.S the only place where notrace variant is used.
    So fix it, since we are going to extend V7 cache routines to handle V7M
    case too.
    
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Tested-by: Andras Szemzo <sza@esh.hu>
    Tested-by: Joachim Eastwood <manabian@gmail.com>
    Tested-by: Alexandre TORGUE <alexandre.torgue@st.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 4eaea2173bf8..68b06f9c65de 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -159,7 +159,11 @@
 	.endm
 
 	.macro	save_and_disable_irqs_notrace, oldcpsr
+#ifdef CONFIG_CPU_V7M
+	mrs	\oldcpsr, primask
+#else
 	mrs	\oldcpsr, cpsr
+#endif
 	disable_irq_notrace
 	.endm
 

commit e6a9dc6129d23cd3025e841c4e13a70910a37135
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri May 13 10:22:38 2016 +0100

    ARM: introduce svc_pt_regs structure
    
    Since the privileged mode pt_regs are an extended version of the saved
    userland pt_regs, introduce a new svc_pt_regs structure to describe this
    layout.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 60d9c5471eb6..4eaea2173bf8 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -480,13 +480,13 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.macro	uaccess_save, tmp
 #ifdef CONFIG_CPU_SW_DOMAIN_PAN
 	mrc	p15, 0, \tmp, c3, c0, 0
-	str	\tmp, [sp, #PT_REGS_SIZE]
+	str	\tmp, [sp, #SVC_DACR]
 #endif
 	.endm
 
 	.macro	uaccess_restore
 #ifdef CONFIG_CPU_SW_DOMAIN_PAN
-	ldr	r0, [sp, #PT_REGS_SIZE]
+	ldr	r0, [sp, #SVC_DACR]
 	mcr	p15, 0, r0, c3, c0, 0
 #endif
 	.endm

commit 5745eef6b813194b4dd3e2aee1dd712d8512bf91
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Tue May 10 16:34:27 2016 +0100

    ARM: rename S_FRAME_SIZE to PT_REGS_SIZE
    
    S_FRAME_SIZE is no longer the size of the kernel stack frame, so this
    name is misleading.  It is the size of the kernel pt_regs structure.
    Name it so.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index b2bc8e11471d..60d9c5471eb6 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -480,13 +480,13 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.macro	uaccess_save, tmp
 #ifdef CONFIG_CPU_SW_DOMAIN_PAN
 	mrc	p15, 0, \tmp, c3, c0, 0
-	str	\tmp, [sp, #S_FRAME_SIZE]
+	str	\tmp, [sp, #PT_REGS_SIZE]
 #endif
 	.endm
 
 	.macro	uaccess_restore
 #ifdef CONFIG_CPU_SW_DOMAIN_PAN
-	ldr	r0, [sp, #S_FRAME_SIZE]
+	ldr	r0, [sp, #PT_REGS_SIZE]
 	mcr	p15, 0, r0, c3, c0, 0
 #endif
 	.endm

commit 57e6bbcb4beb4c87aa93e78a1db08fdaab9ee65f
Merge: 6ff33f3902c3 c2172ce23030
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 14 12:24:10 2015 -0700

    Merge branch 'fixes' of git://ftp.arm.linux.org.uk/~rmk/linux-arm
    
    Pull ARM fixes from Russell King:
     "A number of fixes for the merge window, fixing a number of cases
      missed when testing the uaccess code, particularly cases which only
      show up with certain compiler versions"
    
    * 'fixes' of git://ftp.arm.linux.org.uk/~rmk/linux-arm:
      ARM: 8431/1: fix alignement of __bug_table section entries
      arm/xen: Enable user access to the kernel before issuing a privcmd call
      ARM: domains: add memory dependencies to get_domain/set_domain
      ARM: domains: thread_info.h no longer needs asm/domains.h
      ARM: uaccess: fix undefined instruction on ARMv7M/noMMU
      ARM: uaccess: remove unneeded uaccess_save_and_disable macro
      ARM: swpan: fix nwfpe for uaccess changes
      ARM: 8429/1: disable GCC SRA optimization

commit 296254f3223d201f2aa53f5f717eedfdc63f3db8
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Sep 7 00:30:06 2015 +0100

    ARM: uaccess: remove unneeded uaccess_save_and_disable macro
    
    This macro is never referenced, remove it.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 3ae0eda5e64f..9007c518d1d8 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -487,11 +487,6 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 #endif
 	.endm
 
-	.macro	uaccess_save_and_disable, tmp
-	uaccess_save \tmp
-	uaccess_disable \tmp
-	.endm
-
 	.irp	c,,eq,ne,cs,cc,mi,pl,vs,vc,hi,ls,ge,lt,gt,le,hs,lo
 	.macro	ret\c, reg
 #if __LINUX_ARM_ARCH__ < 6

commit 40d3f02851577da27b5cbb1538888301245ef1e7
Merge: e0aa3a665782 3939f3345050 9205b797dbe5 3fa609755c11 a5e090acbf54
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Sep 3 15:28:37 2015 +0100

    Merge branches 'cleanup', 'fixes', 'misc', 'omap-barrier' and 'uaccess' into for-linus

commit a5e090acbf545c0a3b04080f8a488b17ec41fe02
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Aug 19 20:40:41 2015 +0100

    ARM: software-based priviledged-no-access support
    
    Provide a software-based implementation of the priviledged no access
    support found in ARMv8.1.
    
    Userspace pages are mapped using a different domain number from the
    kernel and IO mappings.  If we switch the user domain to "no access"
    when we enter the kernel, we can prevent the kernel from touching
    userspace.
    
    However, the kernel needs to be able to access userspace via the
    various user accessor functions.  With the wrapping in the previous
    patch, we can temporarily enable access when the kernel needs user
    access, and re-disable it afterwards.
    
    This allows us to trap non-intended accesses to userspace, eg, caused
    by an inadvertent dereference of the LIST_POISON* values, which, with
    appropriate user mappings setup, can be made to succeed.  This in turn
    can allow use-after-free bugs to be further exploited than would
    otherwise be possible.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index a91177043467..3ae0eda5e64f 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -446,15 +446,45 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	.endm
 
 	.macro	uaccess_disable, tmp, isb=1
+#ifdef CONFIG_CPU_SW_DOMAIN_PAN
+	/*
+	 * Whenever we re-enter userspace, the domains should always be
+	 * set appropriately.
+	 */
+	mov	\tmp, #DACR_UACCESS_DISABLE
+	mcr	p15, 0, \tmp, c3, c0, 0		@ Set domain register
+	.if	\isb
+	instr_sync
+	.endif
+#endif
 	.endm
 
 	.macro	uaccess_enable, tmp, isb=1
+#ifdef CONFIG_CPU_SW_DOMAIN_PAN
+	/*
+	 * Whenever we re-enter userspace, the domains should always be
+	 * set appropriately.
+	 */
+	mov	\tmp, #DACR_UACCESS_ENABLE
+	mcr	p15, 0, \tmp, c3, c0, 0
+	.if	\isb
+	instr_sync
+	.endif
+#endif
 	.endm
 
 	.macro	uaccess_save, tmp
+#ifdef CONFIG_CPU_SW_DOMAIN_PAN
+	mrc	p15, 0, \tmp, c3, c0, 0
+	str	\tmp, [sp, #S_FRAME_SIZE]
+#endif
 	.endm
 
 	.macro	uaccess_restore
+#ifdef CONFIG_CPU_SW_DOMAIN_PAN
+	ldr	r0, [sp, #S_FRAME_SIZE]
+	mcr	p15, 0, r0, c3, c0, 0
+#endif
 	.endm
 
 	.macro	uaccess_save_and_disable, tmp

commit 2190fed67ba6f3e8129513929f2395843645e928
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Aug 20 10:32:02 2015 +0100

    ARM: entry: provide uaccess assembly macro hooks
    
    Provide hooks into the kernel entry and exit paths to permit control
    of userspace visibility to the kernel.  The intended use is:
    
    - on entry to kernel from user, uaccess_disable will be called to
      disable userspace visibility
    - on exit from kernel to user, uaccess_enable will be called to
      enable userspace visibility
    - on entry from a kernel exception, uaccess_save_and_disable will be
      called to save the current userspace visibility setting, and disable
      access
    - on exit from a kernel exception, uaccess_restore will be called to
      restore the userspace visibility as it was before the exception
      occurred.
    
    These hooks allows us to keep userspace visibility disabled for the
    vast majority of the kernel, except for localised regions where we
    want to explicitly access userspace.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 4abe57279c66..a91177043467 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -445,6 +445,23 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 #endif
 	.endm
 
+	.macro	uaccess_disable, tmp, isb=1
+	.endm
+
+	.macro	uaccess_enable, tmp, isb=1
+	.endm
+
+	.macro	uaccess_save, tmp
+	.endm
+
+	.macro	uaccess_restore
+	.endm
+
+	.macro	uaccess_save_and_disable, tmp
+	uaccess_save \tmp
+	uaccess_disable \tmp
+	.endm
+
 	.irp	c,,eq,ne,cs,cc,mi,pl,vs,vc,hi,ls,ge,lt,gt,le,hs,lo
 	.macro	ret\c, reg
 #if __LINUX_ARM_ARCH__ < 6

commit 3302caddf10ad50710dbb7a94ccbdb3ad5bf1412
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Aug 20 16:13:37 2015 +0100

    ARM: entry: efficiency cleanups
    
    Make the "fast" syscall return path fast again.  The addition of IRQ
    tracing and context tracking has made this path grossly inefficient.
    We can do much better if these options are enabled if we save the
    syscall return code on the stack - we then don't need to save a bunch
    of registers around every single callout to C code.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 742495eb5526..5a5504f90d5f 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -108,29 +108,37 @@
 	.endm
 #endif
 
-	.macro asm_trace_hardirqs_off
+	.macro asm_trace_hardirqs_off, save=1
 #if defined(CONFIG_TRACE_IRQFLAGS)
+	.if \save
 	stmdb   sp!, {r0-r3, ip, lr}
+	.endif
 	bl	trace_hardirqs_off
+	.if \save
 	ldmia	sp!, {r0-r3, ip, lr}
+	.endif
 #endif
 	.endm
 
-	.macro asm_trace_hardirqs_on, cond=al
+	.macro asm_trace_hardirqs_on, cond=al, save=1
 #if defined(CONFIG_TRACE_IRQFLAGS)
 	/*
 	 * actually the registers should be pushed and pop'd conditionally, but
 	 * after bl the flags are certainly clobbered
 	 */
+	.if \save
 	stmdb   sp!, {r0-r3, ip, lr}
+	.endif
 	bl\cond	trace_hardirqs_on
+	.if \save
 	ldmia	sp!, {r0-r3, ip, lr}
+	.endif
 #endif
 	.endm
 
-	.macro disable_irq
+	.macro disable_irq, save=1
 	disable_irq_notrace
-	asm_trace_hardirqs_off
+	asm_trace_hardirqs_off \save
 	.endm
 
 	.macro enable_irq

commit 01e09a28167c338684606b70797422da3bbb6650
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Aug 20 14:22:48 2015 +0100

    ARM: entry: get rid of asm_trace_hardirqs_on_cond
    
    There's no need for this macro, it can use a default for the
    condition argument.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 4abe57279c66..742495eb5526 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -116,7 +116,7 @@
 #endif
 	.endm
 
-	.macro asm_trace_hardirqs_on_cond, cond
+	.macro asm_trace_hardirqs_on, cond=al
 #if defined(CONFIG_TRACE_IRQFLAGS)
 	/*
 	 * actually the registers should be pushed and pop'd conditionally, but
@@ -128,10 +128,6 @@
 #endif
 	.endm
 
-	.macro asm_trace_hardirqs_on
-	asm_trace_hardirqs_on_cond al
-	.endm
-
 	.macro disable_irq
 	disable_irq_notrace
 	asm_trace_hardirqs_off
@@ -173,7 +169,7 @@
 
 	.macro restore_irqs, oldcpsr
 	tst	\oldcpsr, #PSR_I_BIT
-	asm_trace_hardirqs_on_cond eq
+	asm_trace_hardirqs_on cond=eq
 	restore_irqs_notrace \oldcpsr
 	.endm
 

commit 14327c662822e5e874cb971a7162067519300ca8
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Apr 21 14:17:25 2015 +0100

    ARM: replace BSYM() with badr assembly macro
    
    BSYM() was invented to allow us to work around a problem with the
    assembler, where local symbols resolved by the assembler for the 'adr'
    instruction did not take account of their ISA.
    
    Since we don't want BSYM() used elsewhere, replace BSYM() with a new
    macro 'badr', which is like the 'adr' pseudo-op, but with the BSYM()
    mechanics integrated into it.  This ensures that the BSYM()-ification
    is only used in conjunction with 'adr'.
    
    Acked-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 186270b3e194..4abe57279c66 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -177,6 +177,21 @@
 	restore_irqs_notrace \oldcpsr
 	.endm
 
+/*
+ * Assembly version of "adr rd, BSYM(sym)".  This should only be used to
+ * reference local symbols in the same assembly file which are to be
+ * resolved by the assembler.  Other usage is undefined.
+ */
+	.irp	c,,eq,ne,cs,cc,mi,pl,vs,vc,hi,ls,ge,lt,gt,le,hs,lo
+	.macro	badr\c, rd, sym
+#ifdef CONFIG_THUMB2_KERNEL
+	adr\c	\rd, \sym + 1
+#else
+	adr\c	\rd, \sym
+#endif
+	.endm
+	.endr
+
 /*
  * Get current thread_info.
  */
@@ -326,7 +341,7 @@
 THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	bne	1f
 	orr	\reg, \reg, #PSR_A_BIT
-	adr	lr, BSYM(2f)
+	badr	lr, 2f
 	msr	spsr_cxsf, \reg
 	__MSR_ELR_HYP(14)
 	__ERET

commit 89c6bc5884e52ec004f03071f268ba3f27003aba
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Apr 9 12:59:35 2015 +0100

    ARM: allow 16-bit instructions in ALT_UP()
    
    Allow ALT_UP() to cope with a 16-bit Thumb instruction by automatically
    inserting a following nop instruction.  This allows us to care less
    about getting the assembler to emit a 32-bit thumb instruction.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index f67fd3afebdf..186270b3e194 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -237,6 +237,9 @@
 	.pushsection ".alt.smp.init", "a"			;\
 	.long	9998b						;\
 9997:	instr							;\
+	.if . - 9997b == 2					;\
+		nop						;\
+	.endif							;\
 	.if . - 9997b != 4					;\
 		.error "ALT_UP() content must assemble to exactly 4 bytes";\
 	.endif							;\

commit 6ebbf2ce437b33022d30badd49dc94d33ecfa498
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jun 30 16:29:12 2014 +0100

    ARM: convert all "mov.* pc, reg" to "bx reg" for ARMv6+
    
    ARMv6 and greater introduced a new instruction ("bx") which can be used
    to return from function calls.  Recent CPUs perform better when the
    "bx lr" instruction is used rather than the "mov pc, lr" instruction,
    and this sequence is strongly recommended to be used by the ARM
    architecture manual (section A.4.1.1).
    
    We provide a new macro "ret" with all its variants for the condition
    code which will resolve to the appropriate instruction.
    
    Rather than doing this piecemeal, and miss some instances, change all
    the "mov pc" instances to use the new macro, with the exception of
    the "movs" instruction and the kprobes code.  This allows us to detect
    the "mov pc, lr" case and fix it up - and also gives us the possibility
    of deploying this for other registers depending on the CPU selection.
    
    Reported-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Stephen Warren <swarren@nvidia.com> # Tegra Jetson TK1
    Tested-by: Robert Jarzmik <robert.jarzmik@free.fr> # mioa701_bootresume.S
    Tested-by: Andrew Lunn <andrew@lunn.ch> # Kirkwood
    Tested-by: Shawn Guo <shawn.guo@freescale.com>
    Tested-by: Tony Lindgren <tony@atomide.com> # OMAPs
    Tested-by: Gregory CLEMENT <gregory.clement@free-electrons.com> # Armada XP, 375, 385
    Acked-by: Sekhar Nori <nsekhar@ti.com> # DaVinci
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org> # kvm/hyp
    Acked-by: Haojian Zhuang <haojian.zhuang@gmail.com> # PXA3xx
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com> # Xen
    Tested-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de> # ARMv7M
    Tested-by: Simon Horman <horms+renesas@verge.net.au> # Shmobile
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 906703a5b564..f67fd3afebdf 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -427,4 +427,25 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 #endif
 	.endm
 
+	.irp	c,,eq,ne,cs,cc,mi,pl,vs,vc,hi,ls,ge,lt,gt,le,hs,lo
+	.macro	ret\c, reg
+#if __LINUX_ARM_ARCH__ < 6
+	mov\c	pc, \reg
+#else
+	.ifeqs	"\reg", "lr"
+	bx\c	\reg
+	.else
+	mov\c	pc, \reg
+	.endif
+#endif
+	.endm
+	.endr
+
+	.macro	ret.w, reg
+	ret	\reg
+#ifdef CONFIG_THUMB2_KERNEL
+	nop
+#endif
+	.endm
+
 #endif /* __ASM_ASSEMBLER_H__ */

commit 9a2b51b6ca93fc42e5676aa444c956f7506185db
Author: Andrey Ryabinin <a.ryabinin@samsung.com>
Date:   Wed Jun 18 16:12:40 2014 +0100

    ARM: 8078/1: get rid of hardcoded assumptions about kernel stack size
    
    Changing kernel stack size on arm is not as simple as it should be:
    1) THREAD_SIZE macro doesn't respect PAGE_SIZE and THREAD_SIZE_ORDER
    2) stack size is hardcoded in get_thread_info macro
    
    This patch fixes it by calculating THREAD_SIZE and thread_info address
    taking into account PAGE_SIZE and THREAD_SIZE_ORDER.
    
    Now changing stack size becomes simply changing THREAD_SIZE_ORDER.
    
    Signed-off-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 57f0584e8d97..906703a5b564 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -24,6 +24,8 @@
 #include <asm/domain.h>
 #include <asm/opcodes-virt.h>
 #include <asm/asm-offsets.h>
+#include <asm/page.h>
+#include <asm/thread_info.h>
 
 #define IOMEM(x)	(x)
 
@@ -179,10 +181,10 @@
  * Get current thread_info.
  */
 	.macro	get_thread_info, rd
- ARM(	mov	\rd, sp, lsr #13	)
+ ARM(	mov	\rd, sp, lsr #THREAD_SIZE_ORDER + PAGE_SHIFT	)
  THUMB(	mov	\rd, sp			)
- THUMB(	lsr	\rd, \rd, #13		)
-	mov	\rd, \rd, lsl #13
+ THUMB(	lsr	\rd, \rd, #THREAD_SIZE_ORDER + PAGE_SHIFT	)
+	mov	\rd, \rd, lsl #THREAD_SIZE_ORDER + PAGE_SHIFT
 	.endm
 
 /*

commit 0e0779da2233f2dfc85e9c3a6ea142476d326811
Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
Date:   Thu May 8 17:31:40 2014 +0100

    ARM: 8053/1: kernel: sleep: restore HYP mode configuration in cpu_resume
    
    On CPUs with virtualization extensions the kernel installs HYP mode
    configuration on both primary and secondary cpus upon cold boot.
    
    On platforms where CPUs are shutdown in idle paths (ie CPU core gating),
    when a CPU resumes from low-power states it currently does not execute
    code that reinstalls the HYP configuration, which means that the kernel
    cannot run eg KVM properly on such machines.
    
    This patch, mirroring cold-boot behaviour, executes position independent
    code that reinstalls HYP configuration and drops to SVC mode safely on
    warmboot, so that deep idle states can be enabled in kernel running as
    hosts on platforms with power management HW.
    
    Cc: Christoffer Dall <christoffer.dall@linaro.org>
    Cc: Dave Martin <dave.martin@arm.com>
    Cc: Marc Zyngier <marc.zyngier@arm.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Reviewed-by: Dave Martin <Dave.Martin@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index b974184f9941..57f0584e8d97 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -312,7 +312,7 @@
  * you cannot return to the original mode.
  */
 .macro safe_svcmode_maskall reg:req
-#if __LINUX_ARM_ARCH__ >= 6
+#if __LINUX_ARM_ARCH__ >= 6 && !defined(CONFIG_CPU_V7M)
 	mrs	\reg , cpsr
 	eor	\reg, \reg, #HYP_MODE
 	tst	\reg, #MODE_MASK

commit 0b1f68e836bcf1ca2861f95066985c57ecfb2f1a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 2 10:57:49 2014 +0100

    ARM: 8018/1: Add {inc,dec}_preempt_count asm macros
    
    The patch adds asm macros for inc_preempt_count and dec_preempt_count_ti
    (which also gets the current thread_info) instead of open-coding them in
    arch/arm/vfp/*.S files.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Arun KS <getarunks@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 9a4965ad6867..b974184f9941 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -23,6 +23,7 @@
 #include <asm/ptrace.h>
 #include <asm/domain.h>
 #include <asm/opcodes-virt.h>
+#include <asm/asm-offsets.h>
 
 #define IOMEM(x)	(x)
 
@@ -184,6 +185,37 @@
 	mov	\rd, \rd, lsl #13
 	.endm
 
+/*
+ * Increment/decrement the preempt count.
+ */
+#ifdef CONFIG_PREEMPT_COUNT
+	.macro	inc_preempt_count, ti, tmp
+	ldr	\tmp, [\ti, #TI_PREEMPT]	@ get preempt count
+	add	\tmp, \tmp, #1			@ increment it
+	str	\tmp, [\ti, #TI_PREEMPT]
+	.endm
+
+	.macro	dec_preempt_count, ti, tmp
+	ldr	\tmp, [\ti, #TI_PREEMPT]	@ get preempt count
+	sub	\tmp, \tmp, #1			@ decrement it
+	str	\tmp, [\ti, #TI_PREEMPT]
+	.endm
+
+	.macro	dec_preempt_count_ti, ti, tmp
+	get_thread_info \ti
+	dec_preempt_count \ti, \tmp
+	.endm
+#else
+	.macro	inc_preempt_count, ti, tmp
+	.endm
+
+	.macro	dec_preempt_count, ti, tmp
+	.endm
+
+	.macro	dec_preempt_count_ti, ti, tmp
+	.endm
+#endif
+
 #define USER(x...)				\
 9999:	x;					\
 	.pushsection __ex_table,"a";		\

commit 39ad04ccd6e1b235601e9ac5a7f508d05728a97a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Apr 2 10:57:48 2014 +0100

    ARM: 8017/1: Move asm macro get_thread_info to asm/assembler.h
    
    asm/assembler.h is a better place for this macro since it is used by
    asm files outside arch/arm/kernel/
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Arun KS <getarunks@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 380ac4f20000..9a4965ad6867 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -174,6 +174,16 @@
 	restore_irqs_notrace \oldcpsr
 	.endm
 
+/*
+ * Get current thread_info.
+ */
+	.macro	get_thread_info, rd
+ ARM(	mov	\rd, sp, lsr #13	)
+ THUMB(	mov	\rd, sp			)
+ THUMB(	lsr	\rd, \rd, #13		)
+	mov	\rd, \rd, lsl #13
+	.endm
+
 #define USER(x...)				\
 9999:	x;					\
 	.pushsection __ex_table,"a";		\

commit d98b90ea22b0a28d9d787769704a9cf1ea5a513a
Author: Victor Kamensky <victor.kamensky@linaro.org>
Date:   Tue Feb 25 08:41:09 2014 +0100

    ARM: 7990/1: asm: rename logical shift macros push pull into lspush lspull
    
    Renames logical shift macros, 'push' and 'pull', defined in
    arch/arm/include/asm/assembler.h, into 'lspush' and 'lspull'.
    That eliminates name conflict between 'push' logical shift macro
    and 'push' instruction mnemonic. That allows assembler.h to be
    included in .S files that use 'push' instruction.
    
    Suggested-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 5c2285160575..380ac4f20000 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -30,8 +30,8 @@
  * Endian independent macros for shifting bytes within registers.
  */
 #ifndef __ARMEB__
-#define pull            lsr
-#define push            lsl
+#define lspull          lsr
+#define lspush          lsl
 #define get_byte_0      lsl #0
 #define get_byte_1	lsr #8
 #define get_byte_2	lsr #16
@@ -41,8 +41,8 @@
 #define put_byte_2	lsl #16
 #define put_byte_3	lsl #24
 #else
-#define pull            lsl
-#define push            lsr
+#define lspull          lsl
+#define lspush          lsr
 #define get_byte_0	lsr #24
 #define get_byte_1	lsr #16
 #define get_byte_2	lsr #8

commit 457c2403c513c74f60d5757fd11ae927e5554a38
Author: Ben Dooks <ben.dooks@codethink.co.uk>
Date:   Tue Feb 12 18:59:57 2013 +0000

    ARM: asm: Add ARM_BE8() assembly helper
    
    Add ARM_BE8() helper to wrap any code conditional on being
    compile when CONFIG_ARM_ENDIAN_BE8 is selected and convert
    existing places where this is to use it.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index fcc1b5bf6979..5c2285160575 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -53,6 +53,13 @@
 #define put_byte_3      lsl #0
 #endif
 
+/* Select code for any configuration running in BE8 mode */
+#ifdef CONFIG_CPU_ENDIAN_BE8
+#define ARM_BE8(code...) code
+#else
+#define ARM_BE8(code...)
+#endif
+
 /*
  * Data preload for architectures that support it
  */

commit 3ea128065ed20d33bd02ff6dab689f88e38000be
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 10 18:07:19 2013 +0100

    ARM: barrier: allow options to be passed to memory barrier instructions
    
    On ARMv7, the memory barrier instructions take an optional `option'
    field which can be used to constrain the effects of a memory barrier
    based on shareability and access type.
    
    This patch allows the caller to pass these options if required, and
    updates the smp_*() barriers to request inner-shareable barriers,
    affecting only stores for the _wmb variant. wmb() is also changed to
    use the -st version of dsb.
    
    Reported-by: Albin Tonnerre <albin.tonnerre@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index a5fef710af32..fcc1b5bf6979 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -220,9 +220,9 @@
 #ifdef CONFIG_SMP
 #if __LINUX_ARM_ARCH__ >= 7
 	.ifeqs "\mode","arm"
-	ALT_SMP(dmb)
+	ALT_SMP(dmb	ish)
 	.else
-	ALT_SMP(W(dmb))
+	ALT_SMP(W(dmb)	ish)
 	.endif
 #elif __LINUX_ARM_ARCH__ == 6
 	ALT_SMP(mcr	p15, 0, r0, c7, c10, 5)	@ dmb

commit 55bdd694116597d2f16510b121463cd579ba78da
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri May 21 18:06:41 2010 +0100

    ARM: Add base support for ARMv7-M
    
    This patch adds the base support for the ARMv7-M
    architecture. It consists of the corresponding arch/arm/mm/ files and
    various #ifdef's around the kernel. Exception handling is implemented by
    a subsequent patch.
    
    [ukleinek: squash in some changes originating from commit
    
    b5717ba (Cortex-M3: Add support for the Microcontroller Prototyping System)
    
    from the v2.6.33-arm1 patch stack, port to post 3.6, drop zImage
    support, drop reorganisation of pt_regs, assert CONFIG_CPU_V7M doesn't
    leak into installed headers and a few cosmetic changes]
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Jonathan Austin <jonathan.austin@arm.com>
    Tested-by: Jonathan Austin <jonathan.austin@arm.com>
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 05ee9eebad6b..a5fef710af32 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -136,7 +136,11 @@
  * assumes FIQs are enabled, and that the processor is in SVC mode.
  */
 	.macro	save_and_disable_irqs, oldcpsr
+#ifdef CONFIG_CPU_V7M
+	mrs	\oldcpsr, primask
+#else
 	mrs	\oldcpsr, cpsr
+#endif
 	disable_irq
 	.endm
 
@@ -150,7 +154,11 @@
  * guarantee that this will preserve the flags.
  */
 	.macro	restore_irqs_notrace, oldcpsr
+#ifdef CONFIG_CPU_V7M
+	msr	primask, \oldcpsr
+#else
 	msr	cpsr_c, \oldcpsr
+#endif
 	.endm
 
 	.macro restore_irqs, oldcpsr
@@ -229,7 +237,14 @@
 #endif
 	.endm
 
-#ifdef CONFIG_THUMB2_KERNEL
+#if defined(CONFIG_CPU_V7M)
+	/*
+	 * setmode is used to assert to be in svc mode during boot. For v7-M
+	 * this is done in __v7m_setup, so setmode can be empty here.
+	 */
+	.macro	setmode, mode, reg
+	.endm
+#elif defined(CONFIG_THUMB2_KERNEL)
 	.macro	setmode, mode, reg
 	mov	\reg, #\mode
 	msr	cpsr_c, \reg

commit 8e9c24a2b2e00368262b974d6ea1ac5310570bbe
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Dec 3 15:39:43 2012 +0000

    ARM: virt: avoid clobbering lr when forcing svc mode
    
    The safe_svcmode_maskall macro is used to ensure that we are running in
    svc mode, causing an exception return from hvc mode if required.
    
    This patch removes the unneeded lr clobber from the macro and operates
    entirely on the temporary parameter register instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    [will: updated comment]
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index eb87200aa4b5..05ee9eebad6b 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -246,18 +246,14 @@
  *
  * This macro is intended for forcing the CPU into SVC mode at boot time.
  * you cannot return to the original mode.
- *
- * Beware, it also clobers LR.
  */
 .macro safe_svcmode_maskall reg:req
 #if __LINUX_ARM_ARCH__ >= 6
 	mrs	\reg , cpsr
-	mov	lr , \reg
-	and	lr , lr , #MODE_MASK
-	cmp	lr , #HYP_MODE
-	orr	\reg , \reg , #PSR_I_BIT | PSR_F_BIT
+	eor	\reg, \reg, #HYP_MODE
+	tst	\reg, #MODE_MASK
 	bic	\reg , \reg , #MODE_MASK
-	orr	\reg , \reg , #SVC_MODE
+	orr	\reg , \reg , #PSR_I_BIT | PSR_F_BIT | SVC_MODE
 THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	bne	1f
 	orr	\reg, \reg, #PSR_A_BIT

commit 1ecec696c8bb9b4cefb09495d81d081d1c81b578
Author: Dave Martin <dave.martin@linaro.org>
Date:   Mon Dec 10 18:35:22 2012 +0100

    ARM: 7599/1: head: Remove boot-time HYP mode check for v5 and below
    
    The kernel can only be entered on HYP mode on CPUs which actually
    support it, i.e.  >= ARMv7.  pre-v6 platform support cannot coexist
    in the same kernel as support for v7 and higher, so there is no
    advantage in having the HYP mode check on pre-v6 hardware.
    
    At least one pre-v6 board is known to fail when the HYP mode check
    code is present, although the exact cause remains unknown and may
    be unrelated.  [1]
    
    This patch restores the old behaviour for pre-v6 platforms, whereby
    the CPSR is forced directly to SVC mode with IRQs and FIQs masked.
    All kernels capable of booting on v7 hardware will retain the
    check, so this should not impair functionality.
    
    [1] http://lists.arm.linux.org.uk/lurker/message/20121130.013814.19218413.en.html
    ([ARM] head.S change broke platform device registration?)
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 2ef95813fce0..eb87200aa4b5 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -250,6 +250,7 @@
  * Beware, it also clobers LR.
  */
 .macro safe_svcmode_maskall reg:req
+#if __LINUX_ARM_ARCH__ >= 6
 	mrs	\reg , cpsr
 	mov	lr , \reg
 	and	lr , lr , #MODE_MASK
@@ -266,6 +267,13 @@ THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
 	__ERET
 1:	msr	cpsr_c, \reg
 2:
+#else
+/*
+ * workaround for possibly broken pre-v6 hardware
+ * (akita, Sharp Zaurus C-1000, PXA270-based)
+ */
+	setmode	PSR_F_BIT | PSR_I_BIT | SVC_MODE, \reg
+#endif
 .endm
 
 /*

commit 2a552d5e63d7fa602c9a9a0717008737f55625a6
Author: Marc Zyngier <Marc.Zyngier@arm.com>
Date:   Sat Oct 6 17:03:17 2012 +0100

    ARM: 7549/1: HYP: fix boot on some ARM1136 cores
    
    It appears that performing a "movs pc, lr" to force the kernel into
    SVC mode on the OMAP2420 (ARM1136) prevents the platform from booting
    correctly (change introduced in 80c59da [ARM: virt: allow the kernel
    to be entered in HYP mode]).
    
    While the reason it fails is not understood yet (the same code runs
    fine on the OMAP2430, ARM1136 as well), partially revert that change
    for platforms that do not enter in HYP mode, preserving the new
    feature and restoring a working kernel on the OMAP2420.
    
    Reported-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 683a1e6b6020..2ef95813fce0 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -254,16 +254,17 @@
 	mov	lr , \reg
 	and	lr , lr , #MODE_MASK
 	cmp	lr , #HYP_MODE
-	orr	\reg , \reg , #PSR_A_BIT | PSR_I_BIT | PSR_F_BIT
+	orr	\reg , \reg , #PSR_I_BIT | PSR_F_BIT
 	bic	\reg , \reg , #MODE_MASK
 	orr	\reg , \reg , #SVC_MODE
 THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
-	msr	spsr_cxsf, \reg
-	adr	lr, BSYM(2f)
 	bne	1f
+	orr	\reg, \reg, #PSR_A_BIT
+	adr	lr, BSYM(2f)
+	msr	spsr_cxsf, \reg
 	__MSR_ELR_HYP(14)
 	__ERET
-1:	movs	pc, lr
+1:	msr	cpsr_c, \reg
 2:
 .endm
 

commit 648f3b69986b4d0ade57e59504a431b973ce2875
Merge: 8ee777fd915b 8ec58be9f3ff
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Sep 30 09:03:44 2012 +0100

    Merge branch 'hyp-boot-mode-rmk' of git://git.kernel.org/pub/scm/linux/kernel/git/maz/arm-platforms into devel-stable

commit 80c59dafb1a9a86fa996e6e34d06b60567c925ca
Author: Dave Martin <dave.martin@linaro.org>
Date:   Thu Feb 9 08:47:17 2012 -0800

    ARM: virt: allow the kernel to be entered in HYP mode
    
    This patch does two things:
    
      * Ensure that asynchronous aborts are masked at kernel entry.
        The bootloader should be masking these anyway, but this reduces
        the damage window just in case it doesn't.
    
      * Enter svc mode via exception return to ensure that CPU state is
        properly serialised.  This does not matter when switching from
        an ordinary privileged mode ("PL1" modes in ARMv7-AR rev C
        parlance), but it potentially does matter when switching from a
        another privileged mode such as hyp mode.
    
    This should allow the kernel to boot safely either from svc mode or
    hyp mode, even if no support for use of the ARM Virtualization
    Extensions is built into the kernel.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 03fb93621d0d..658a15dbc87e 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -22,6 +22,7 @@
 
 #include <asm/ptrace.h>
 #include <asm/domain.h>
+#include <asm/opcodes-virt.h>
 
 #define IOMEM(x)	(x)
 
@@ -239,6 +240,33 @@
 	.endm
 #endif
 
+/*
+ * Helper macro to enter SVC mode cleanly and mask interrupts. reg is
+ * a scratch register for the macro to overwrite.
+ *
+ * This macro is intended for forcing the CPU into SVC mode at boot time.
+ * you cannot return to the original mode.
+ *
+ * Beware, it also clobers LR.
+ */
+.macro safe_svcmode_maskall reg:req
+	mrs	\reg , cpsr
+	mov	lr , \reg
+	and	lr , lr , #MODE_MASK
+	cmp	lr , #HYP_MODE
+	orr	\reg , \reg , #PSR_A_BIT | PSR_I_BIT | PSR_F_BIT
+	bic	\reg , \reg , #MODE_MASK
+	orr	\reg , \reg , #SVC_MODE
+THUMB(	orr	\reg , \reg , #PSR_T_BIT	)
+	msr	spsr_cxsf, \reg
+	adr	lr, BSYM(2f)
+	bne	1f
+	__MSR_ELR_HYP(14)
+	__ERET
+1:	movs	pc, lr
+2:
+.endm
+
 /*
  * STRT/LDRT access macros with ARM and Thumb-2 variants
  */

commit 8404663f81d212918ff85f493649a7991209fa04
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Sep 7 18:22:28 2012 +0100

    ARM: 7527/1: uaccess: explicitly check __user pointer when !CPU_USE_DOMAINS
    
    The {get,put}_user macros don't perform range checking on the provided
    __user address when !CPU_HAS_DOMAINS.
    
    This patch reworks the out-of-line assembly accessors to check the user
    address against a specified limit, returning -EFAULT if is is out of
    range.
    
    [will: changed get_user register allocation to match put_user]
    [rmk: fixed building on older ARM architectures]
    
    Reported-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 03fb93621d0d..5c8b3bf4d825 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -320,4 +320,12 @@
 	.size \name , . - \name
 	.endm
 
+	.macro check_uaccess, addr:req, size:req, limit:req, tmp:req, bad:req
+#ifndef CONFIG_CPU_USE_DOMAINS
+	adds	\tmp, \addr, #\size - 1
+	sbcccs	\tmp, \tmp, \limit
+	bcs	\bad
+#endif
+	.endm
+
 #endif /* __ASM_ASSEMBLER_H__ */

commit 820d41cf0cd0e94a5661e093821e2e5c6b36a9d8
Merge: 6268b325c306 88b48684fe2d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 29 18:02:10 2012 -0700

    Merge tag 'cleanup2' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull "ARM: cleanups of io includes" from Olof Johansson:
     "Rob Herring has done a sweeping change cleaning up all of the
      mach/io.h includes, moving some of the oft-repeated macros to a common
      location and removing a bunch of boiler plate.  This is another step
      closer to a common zImage for multiple platforms."
    
    Fix up various fairly trivial conflicts (<mach/io.h> removal vs changes
    around it, tegra localtimer.o is *still* gone, yadda-yadda).
    
    * tag 'cleanup2' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (29 commits)
      ARM: tegra: Include assembler.h in sleep.S to fix build break
      ARM: pxa: use common IOMEM definition
      ARM: dma-mapping: convert ARCH_HAS_DMA_SET_COHERENT_MASK to kconfig symbol
      ARM: __io abuse cleanup
      ARM: create a common IOMEM definition
      ARM: iop13xx: fix missing declaration of iop13xx_init_early
      ARM: fix ioremap/iounmap for !CONFIG_MMU
      ARM: kill off __mem_pci
      ARM: remove bunch of now unused mach/io.h files
      ARM: make mach/io.h include optional
      ARM: clps711x: remove unneeded include of mach/io.h
      ARM: dove: add explicit include of dove.h to addr-map.c
      ARM: at91: add explicit include of hardware.h to uncompressor
      ARM: ep93xx: clean-up mach/io.h
      ARM: tegra: clean-up mach/io.h
      ARM: orion5x: clean-up mach/io.h
      ARM: davinci: remove unneeded mach/io.h include
      [media] davinci: remove includes of mach/io.h
      ARM: OMAP: Remove remaining includes for mach/io.h
      ARM: msm: clean-up mach/io.h
      ...

commit 6f6f6a70295c6a4f89c7aca015c5db247a79d609
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Sat Mar 10 10:30:31 2012 -0600

    ARM: create a common IOMEM definition
    
    Several platforms create IOMEM defines for casting to 'void __iomem *',
    and other platforms are incorrectly using __io() macro for the same
    purpose. This creates a common definition and removes all the platform
    specific versions. Rather than try to make linux/io.h and asm/io.h
    assembly safe, the assembly version of IOMEM is moved into
    asm/assembler.h.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Sekhar Nori <nsekhar@ti.com>
    Cc: Kevin Hilman <khilman@ti.com>
    Acked-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Cc: Ryan Mallon <rmallon@gmail.com>
    Cc: Eric Miao <eric.y.miao@gmail.com>
    Cc: Haojian Zhuang <haojian.zhuang@marvell.com>
    Acked-by: David Brown <davidb@codeaurora.org>
    Cc: Daniel Walker <dwalker@fifo99.com>
    Cc: Bryan Huntsman <bryanh@codeaurora.org>
    Cc: Sascha Hauer <kernel@pengutronix.de>
    Cc: Shawn Guo <shawn.guo@linaro.org>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Paul Walmsley <paul@pwsan.com>
    Acked-by: Viresh Kumar <viresh.kumar@st.com>
    Cc: Rajeev Kumar <rajeev-dlh.kumar@st.com>
    Cc: Colin Cross <ccross@android.com>
    Cc: Olof Johansson <olof@lixom.net>
    Cc: Stephen Warren <swarren@nvidia.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 62f8095d46de..88374dd30fb9 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -23,6 +23,8 @@
 #include <asm/ptrace.h>
 #include <asm/domain.h>
 
+#define IOMEM(x)	(x)
+
 /*
  * Endian independent macros for shifting bytes within registers.
  */

commit 8e43a905dd574f54c5715d978318290ceafbe275
Author: Rabin Vincent <rabin@rab.in>
Date:   Wed Feb 15 16:01:42 2012 +0100

    ARM: 7325/1: fix v7 boot with lockdep enabled
    
    Bootup with lockdep enabled has been broken on v7 since b46c0f74657d
    ("ARM: 7321/1: cache-v7: Disable preemption when reading CCSIDR").
    
    This is because v7_setup (which is called very early during boot) calls
    v7_flush_dcache_all, and the save_and_disable_irqs added by that patch
    ends up attempting to call into lockdep C code (trace_hardirqs_off())
    when we are in no position to execute it (no stack, MMU off).
    
    Fix this by using a notrace variant of save_and_disable_irqs.  The code
    already uses the notrace variant of restore_irqs.
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Rabin Vincent <rabin@rab.in>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 62f8095d46de..23371b17b23e 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -137,6 +137,11 @@
 	disable_irq
 	.endm
 
+	.macro	save_and_disable_irqs_notrace, oldcpsr
+	mrs	\oldcpsr, cpsr
+	disable_irq_notrace
+	.endm
+
 /*
  * Restore interrupt state previously stored in a register.  We don't
  * guarantee that this will preserve the flags.

commit 4e7682d077d693e34a993ae7a2831b522930ebcb
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jan 25 11:38:13 2012 +0100

    ARM: 7301/1: Rename the T() macro to TUSER() to avoid namespace conflicts
    
    This macro is used to generate unprivileged accesses (LDRT/STRT) to user
    space.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index b6e65dedfd71..62f8095d46de 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -237,7 +237,7 @@
  */
 #ifdef CONFIG_THUMB2_KERNEL
 
-	.macro	usraccoff, instr, reg, ptr, inc, off, cond, abort, t=T()
+	.macro	usraccoff, instr, reg, ptr, inc, off, cond, abort, t=TUSER()
 9999:
 	.if	\inc == 1
 	\instr\cond\()b\()\t\().w \reg, [\ptr, #\off]
@@ -277,7 +277,7 @@
 
 #else	/* !CONFIG_THUMB2_KERNEL */
 
-	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort, t=T()
+	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort, t=TUSER()
 	.rept	\rept
 9999:
 	.if	\inc == 1

commit d675d0bc47f28c5414fbbe17fcc801f69c45b960
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Nov 22 17:30:28 2011 +0000

    ARM: LPAE: add ISBs around MMU enabling code
    
    Before we enable the MMU, we must ensure that the TTBR registers contain
    sane values. After the MMU has been enabled, we jump to the *virtual*
    address of the following function, so we also need to ensure that the
    SCTLR write has taken effect.
    
    This patch adds ISB instructions around the SCTLR write to ensure the
    visibility of the above.
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 29035e86a59d..b6e65dedfd71 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -186,6 +186,17 @@
 #define ALT_UP_B(label) b label
 #endif
 
+/*
+ * Instruction barrier
+ */
+	.macro	instr_sync
+#if __LINUX_ARM_ARCH__ >= 7
+	isb
+#elif __LINUX_ARM_ARCH__ == 6
+	mcr	p15, 0, r0, c7, c5, 4
+#endif
+	.endm
+
 /*
  * SMP data memory barrier
  */

commit 8f51965e7033441cb10ce577d1ef2d580a80af08
Author: Dave Martin <dave.martin@linaro.org>
Date:   Thu Jun 23 17:10:05 2011 +0100

    ARM: assembler.h: Add string declaration macro
    
    Declaring strings in assembler source involves a certain amount of
    tedious boilerplate code in order to annotate the resulting symbol
    correctly.
    
    Encapsulating this boilerplate in a macro should help to avoid some
    duplication and the occasional mistake.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 65c3f2474f5e..29035e86a59d 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -293,4 +293,13 @@
 	.macro	ldrusr, reg, ptr, inc, cond=al, rept=1, abort=9001f
 	usracc	ldr, \reg, \ptr, \inc, \cond, \rept, \abort
 	.endm
+
+/* Utility macro for declaring string literals */
+	.macro	string name:req, string
+	.type \name , #object
+\name:
+	.asciz "\string"
+	.size \name , . - \name
+	.endm
+
 #endif /* __ASM_ASSEMBLER_H__ */

commit 2bc58a6fd76f89052c7f151d78fb2d8b804aacfe
Author: Magnus Damm <damm@opensource.se>
Date:   Mon Jun 13 06:46:44 2011 +0100

    ARM: 6959/1: SMP build fix for entry-macro-multi.S
    
    The assembly code in entry-macro-multi.S does not build without
    the include asm/assembler.h in the case of CONFIG_SMP=y.
    
    Fixes the rather theoretical SMP build of mach-shmobile/entry-intc.c:
    
    arch/arm/include/asm/entry-macro-multi.S: Assembler messages:
    arch/arm/include/asm/entry-macro-multi.S:20: Error: bad instruction `alt_smp(test_for_ipi r0,r6,r5,lr)'
    arch/arm/include/asm/entry-macro-multi.S:20: Error: bad instruction `alt_up_b(9997f)'
    make[1]: *** [arch/arm/mach-shmobile/entry-intc.o] Error 1
    make: *** [arch/arm/mach-shmobile] Error 2
    make: *** Waiting for unfinished jobs....
    
    Signed-off-by: Magnus Damm <damm@opensource.se>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index bc2d2d75f706..65c3f2474f5e 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -13,6 +13,9 @@
  *  Do not include any C declarations in this file - it is included by
  *  assembler source.
  */
+#ifndef __ASM_ASSEMBLER_H__
+#define __ASM_ASSEMBLER_H__
+
 #ifndef __ASSEMBLY__
 #error "Only include this from assembly code"
 #endif
@@ -290,3 +293,4 @@
 	.macro	ldrusr, reg, ptr, inc, cond=al, rept=1, abort=9001f
 	usracc	ldr, \reg, \ptr, \inc, \cond, \rept, \abort
 	.endm
+#endif /* __ASM_ASSEMBLER_H__ */

commit 4ec3eb13634529c0bc7466658d84d0bbe3244aea
Merge: 24056f525051 15095bb0fe77
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jan 6 22:31:35 2011 +0000

    Merge branch 'smp' into misc
    
    Conflicts:
            arch/arm/kernel/entry-armv.S
            arch/arm/mm/ioremap.c

commit ed3768a8d9dc2d345d4f27eb44ee1e4825056c08
Author: Dave Martin <dave.martin@linaro.org>
Date:   Wed Dec 1 15:39:23 2010 +0100

    ARM: 6516/1: Allow SMP_ON_UP to work with Thumb-2 kernels.
    
      * __fixup_smp_on_up has been modified with support for the
        THUMB2_KERNEL case.  For THUMB2_KERNEL only, fixups are split
        into halfwords in case of misalignment, since we can't rely on
        unaligned accesses working before turning the MMU on.
    
        No attempt is made to optimise the aligned case, since the
        number of fixups is typically small, and it seems best to keep
        the code as simple as possible.
    
      * Add a rotate in the fixup_smp code in order to support
        CPU_BIG_ENDIAN, as suggested by Nicolas Pitre.
    
      * Add an assembly-time sanity-check to ALT_UP() to ensure that
        the content really is the right size (4 bytes).
    
        (No check is done for ALT_SMP().  Possibly, this could be fixed
        by splitting the two uses ot ALT_SMP() (ALT_SMP...SMP_UP versus
        ALT_SMP...SMP_UP_B) into two macros.  In the first case,
        ALT_SMP needs to expand to >= 4 bytes, not == 4.)
    
      * smp_mpidr.h (which implements ALT_SMP()/ALT_UP() manually due
        to macro limitations) has not been modified: the affected
        instruction (mov) has no 16-bit encoding, so the correct
        instruction size is satisfied in this case.
    
      * A "mode" parameter has been added to smp_dmb:
    
        smp_dmb arm @ assumes 4-byte instructions (for ARM code, e.g. kuser)
        smp_dmb     @ uses W() to ensure 4-byte instructions for ALT_SMP()
    
        This avoids assembly failures due to use of W() inside smp_dmb,
        when assembling pure-ARM code in the vectors page.
    
        There might be a better way to achieve this.
    
      * Kconfig: make SMP_ON_UP depend on
        (!THUMB2_KERNEL || !BIG_ENDIAN) i.e., THUMB2_KERNEL is now
        supported, but only if !BIG_ENDIAN (The fixup code for Thumb-2
        currently assumes little-endian order.)
    
    Tested using a single generic realview kernel on:
            ARM RealView PB-A8 (CONFIG_THUMB2_KERNEL={n,y})
            ARM RealView PBX-A9 (SMP)
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 749bb6622404..72d3389e9c12 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -157,16 +157,24 @@
 #ifdef CONFIG_SMP
 #define ALT_SMP(instr...)					\
 9998:	instr
+/*
+ * Note: if you get assembler errors from ALT_UP() when building with
+ * CONFIG_THUMB2_KERNEL, you almost certainly need to use
+ * ALT_SMP( W(instr) ... )
+ */
 #define ALT_UP(instr...)					\
 	.pushsection ".alt.smp.init", "a"			;\
 	.long	9998b						;\
-	instr							;\
+9997:	instr							;\
+	.if . - 9997b != 4					;\
+		.error "ALT_UP() content must assemble to exactly 4 bytes";\
+	.endif							;\
 	.popsection
 #define ALT_UP_B(label)					\
 	.equ	up_b_offset, label - 9998b			;\
 	.pushsection ".alt.smp.init", "a"			;\
 	.long	9998b						;\
-	b	. + up_b_offset					;\
+	W(b)	. + up_b_offset					;\
 	.popsection
 #else
 #define ALT_SMP(instr...)
@@ -177,16 +185,24 @@
 /*
  * SMP data memory barrier
  */
-	.macro	smp_dmb
+	.macro	smp_dmb mode
 #ifdef CONFIG_SMP
 #if __LINUX_ARM_ARCH__ >= 7
+	.ifeqs "\mode","arm"
 	ALT_SMP(dmb)
+	.else
+	ALT_SMP(W(dmb))
+	.endif
 #elif __LINUX_ARM_ARCH__ == 6
 	ALT_SMP(mcr	p15, 0, r0, c7, c10, 5)	@ dmb
 #else
 #error Incompatible SMP platform
 #endif
+	.ifeqs "\mode","arm"
 	ALT_UP(nop)
+	.else
+	ALT_UP(W(nop))
+	.endif
 #endif
 	.endm
 

commit 1142b71d85894dcff1466dd6c871ea3c89e0352c
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Nov 19 13:18:31 2010 +0100

    ARM: 6489/1: thumb2: fix incorrect optimisation in usracc
    
    Commit 8b592783 added a Thumb-2 variant of usracc which, when it is
    called with \rept=2, calls usraccoff once with an offset of 0 and
    secondly with a hard-coded offset of 4 in order to avoid incrementing
    the pointer again. If \inc != 4 then we will store the data to the wrong
    offset from \ptr. Luckily, the only caller that passes \rept=2 to this
    function is __clear_user so we haven't been actively corrupting user data.
    
    This patch fixes usracc to pass \inc instead of #4 to usraccoff
    when it is called a second time.
    
    Cc: <stable@kernel.org>
    Reported-by: Tony Thompson <tony.thompson@arm.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 062b58c029ab..749bb6622404 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -238,7 +238,7 @@
 	@ Slightly optimised to avoid incrementing the pointer twice
 	usraccoff \instr, \reg, \ptr, \inc, 0, \cond, \abort
 	.if	\rept == 2
-	usraccoff \instr, \reg, \ptr, \inc, 4, \cond, \abort
+	usraccoff \instr, \reg, \ptr, \inc, \inc, \cond, \abort
 	.endif
 
 	add\cond \ptr, #\rept * \inc

commit 247055aa21ffef1c49dd64710d5e94c2aee19b58
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 13 16:03:21 2010 +0100

    ARM: 6384/1: Remove the domain switching on ARMv6k/v7 CPUs
    
    This patch removes the domain switching functionality via the set_fs and
    __switch_to functions on cores that have a TLS register.
    
    Currently, the ioremap and vmalloc areas share the same level 1 page
    tables and therefore have the same domain (DOMAIN_KERNEL). When the
    kernel domain is modified from Client to Manager (via the __set_fs or in
    the __switch_to function), the XN (eXecute Never) bit is overridden and
    newer CPUs can speculatively prefetch the ioremap'ed memory.
    
    Linux performs the kernel domain switching to allow user-specific
    functions (copy_to/from_user, get/put_user etc.) to access kernel
    memory. In order for these functions to work with the kernel domain set
    to Client, the patch modifies the LDRT/STRT and related instructions to
    the LDR/STR ones.
    
    The user pages access rights are also modified for kernel read-only
    access rather than read/write so that the copy-on-write mechanism still
    works. CPU_USE_DOMAINS gets disabled only if the hardware has a TLS register
    (CPU_32v6K is defined) since writing the TLS value to the high vectors page
    isn't possible.
    
    The user addresses passed to the kernel are checked by the access_ok()
    function so that they do not point to the kernel space.
    
    Tested-by: Anton Vorontsov <cbouatmailru@gmail.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 062b58c029ab..4e84d09c9c1b 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -18,6 +18,7 @@
 #endif
 
 #include <asm/ptrace.h>
+#include <asm/domain.h>
 
 /*
  * Endian independent macros for shifting bytes within registers.
@@ -206,12 +207,12 @@
  */
 #ifdef CONFIG_THUMB2_KERNEL
 
-	.macro	usraccoff, instr, reg, ptr, inc, off, cond, abort
+	.macro	usraccoff, instr, reg, ptr, inc, off, cond, abort, t=T()
 9999:
 	.if	\inc == 1
-	\instr\cond\()bt \reg, [\ptr, #\off]
+	\instr\cond\()b\()\t\().w \reg, [\ptr, #\off]
 	.elseif	\inc == 4
-	\instr\cond\()t \reg, [\ptr, #\off]
+	\instr\cond\()\t\().w \reg, [\ptr, #\off]
 	.else
 	.error	"Unsupported inc macro argument"
 	.endif
@@ -246,13 +247,13 @@
 
 #else	/* !CONFIG_THUMB2_KERNEL */
 
-	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort
+	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort, t=T()
 	.rept	\rept
 9999:
 	.if	\inc == 1
-	\instr\cond\()bt \reg, [\ptr], #\inc
+	\instr\cond\()b\()\t \reg, [\ptr], #\inc
 	.elseif	\inc == 4
-	\instr\cond\()t \reg, [\ptr], #\inc
+	\instr\cond\()\t \reg, [\ptr], #\inc
 	.else
 	.error	"Unsupported inc macro argument"
 	.endif

commit f00ec48fadf5e37e7889f14cff900aa70d18b644
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Sep 4 10:47:48 2010 +0100

    ARM: Allow SMP kernels to boot on UP systems
    
    UP systems do not implement all the instructions that SMP systems have,
    so in order to boot a SMP kernel on a UP system, we need to rewrite
    parts of the kernel.
    
    Do this using an 'alternatives' scheme, where the kernel code and data
    is modified prior to initialization to replace the SMP instructions,
    thereby rendering the problematical code ineffectual.  We use the linker
    to generate a list of 32-bit word locations and their replacement values,
    and run through these replacements when we detect a UP system.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 6e8f05c8a1c8..062b58c029ab 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -154,16 +154,39 @@
 	.long	9999b,9001f;			\
 	.popsection
 
+#ifdef CONFIG_SMP
+#define ALT_SMP(instr...)					\
+9998:	instr
+#define ALT_UP(instr...)					\
+	.pushsection ".alt.smp.init", "a"			;\
+	.long	9998b						;\
+	instr							;\
+	.popsection
+#define ALT_UP_B(label)					\
+	.equ	up_b_offset, label - 9998b			;\
+	.pushsection ".alt.smp.init", "a"			;\
+	.long	9998b						;\
+	b	. + up_b_offset					;\
+	.popsection
+#else
+#define ALT_SMP(instr...)
+#define ALT_UP(instr...) instr
+#define ALT_UP_B(label) b label
+#endif
+
 /*
  * SMP data memory barrier
  */
 	.macro	smp_dmb
 #ifdef CONFIG_SMP
 #if __LINUX_ARM_ARCH__ >= 7
-	dmb
+	ALT_SMP(dmb)
 #elif __LINUX_ARM_ARCH__ == 6
-	mcr	p15, 0, r0, c7, c10, 5	@ dmb
+	ALT_SMP(mcr	p15, 0, r0, c7, c10, 5)	@ dmb
+#else
+#error Incompatible SMP platform
 #endif
+	ALT_UP(nop)
 #endif
 	.endm
 

commit 4260415f6a3b92c5c986398d96c314df37a4ccbf
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Apr 19 10:15:03 2010 +0100

    ARM: fix build error in arch/arm/kernel/process.c
    
    /tmp/ccJ3ssZW.s: Assembler messages:
    /tmp/ccJ3ssZW.s:1952: Error: can't resolve `.text' {.text section} - `.LFB1077'
    
    This is caused because:
    
            .section .data
            .section .text
            .section .text
            .previous
    
    does not return us to the .text section, but the .data section; this
    makes use of .previous dangerous if the ordering of previous sections
    is not known.
    
    Fix up the other users of .previous; .pushsection and .popsection are
    a safer pairing to use than .section and .previous.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 00f46d9ce299..6e8f05c8a1c8 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -149,10 +149,10 @@
 
 #define USER(x...)				\
 9999:	x;					\
-	.section __ex_table,"a";		\
+	.pushsection __ex_table,"a";		\
 	.align	3;				\
 	.long	9999b,9001f;			\
-	.previous
+	.popsection
 
 /*
  * SMP data memory barrier
@@ -193,10 +193,10 @@
 	.error	"Unsupported inc macro argument"
 	.endif
 
-	.section __ex_table,"a"
+	.pushsection __ex_table,"a"
 	.align	3
 	.long	9999b, \abort
-	.previous
+	.popsection
 	.endm
 
 	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort
@@ -234,10 +234,10 @@
 	.error	"Unsupported inc macro argument"
 	.endif
 
-	.section __ex_table,"a"
+	.pushsection __ex_table,"a"
 	.align	3
 	.long	9999b, \abort
-	.previous
+	.popsection
 	.endr
 	.endm
 

commit 9b2616c2e8cc98ca98bbb40cad83a8d3d859e840
Merge: 590a94d93475 ac25150f2c55
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 15 16:51:48 2009 +0100

    Merge branch 'for-rmk-2.6.32' of git://git.pengutronix.de/git/ukl/linux-2.6 into devel-stable

commit 0d928b0b616d1c5c5fe76019a87cba171ca91633
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Thu Aug 13 20:38:17 2009 +0200

    Complete irq tracing support for ARM
    
    Before this patch enabling and disabling irqs in assembler code and by
    the hardware wasn't tracked completly.
    
    I had to transpose two instructions in arch/arm/lib/bitops.h because
    restore_irqs doesn't preserve the flags with CONFIG_TRACE_IRQFLAGS=y
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 15f8a092b700..44912cd5da13 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -74,23 +74,56 @@
  * Enable and disable interrupts
  */
 #if __LINUX_ARM_ARCH__ >= 6
-	.macro	disable_irq
+	.macro	disable_irq_notrace
 	cpsid	i
 	.endm
 
-	.macro	enable_irq
+	.macro	enable_irq_notrace
 	cpsie	i
 	.endm
 #else
-	.macro	disable_irq
+	.macro	disable_irq_notrace
 	msr	cpsr_c, #PSR_I_BIT | SVC_MODE
 	.endm
 
-	.macro	enable_irq
+	.macro	enable_irq_notrace
 	msr	cpsr_c, #SVC_MODE
 	.endm
 #endif
 
+	.macro asm_trace_hardirqs_off
+#if defined(CONFIG_TRACE_IRQFLAGS)
+	stmdb   sp!, {r0-r3, ip, lr}
+	bl	trace_hardirqs_off
+	ldmia	sp!, {r0-r3, ip, lr}
+#endif
+	.endm
+
+	.macro asm_trace_hardirqs_on_cond, cond
+#if defined(CONFIG_TRACE_IRQFLAGS)
+	/*
+	 * actually the registers should be pushed and pop'd conditionally, but
+	 * after bl the flags are certainly clobbered
+	 */
+	stmdb   sp!, {r0-r3, ip, lr}
+	bl\cond	trace_hardirqs_on
+	ldmia	sp!, {r0-r3, ip, lr}
+#endif
+	.endm
+
+	.macro asm_trace_hardirqs_on
+	asm_trace_hardirqs_on_cond al
+	.endm
+
+	.macro disable_irq
+	disable_irq_notrace
+	asm_trace_hardirqs_off
+	.endm
+
+	.macro enable_irq
+	asm_trace_hardirqs_on
+	enable_irq_notrace
+	.endm
 /*
  * Save the current IRQ state and disable IRQs.  Note that this macro
  * assumes FIQs are enabled, and that the processor is in SVC mode.
@@ -104,10 +137,16 @@
  * Restore interrupt state previously stored in a register.  We don't
  * guarantee that this will preserve the flags.
  */
-	.macro	restore_irqs, oldcpsr
+	.macro	restore_irqs_notrace, oldcpsr
 	msr	cpsr_c, \oldcpsr
 	.endm
 
+	.macro restore_irqs, oldcpsr
+	tst	\oldcpsr, #PSR_I_BIT
+	asm_trace_hardirqs_on_cond eq
+	restore_irqs_notrace \oldcpsr
+	.endm
+
 #define USER(x...)				\
 9999:	x;					\
 	.section __ex_table,"a";		\

commit 8b592783a2e8b7721a99730bd549aab5208f36af
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 24 12:32:57 2009 +0100

    Thumb-2: Implement the unified arch/arm/lib functions
    
    This patch adds the ARM/Thumb-2 unified support for the arch/arm/lib/*
    files.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 1acd1da36d00..2b60c7d05770 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -138,3 +138,76 @@
 	msr	cpsr_c, #\mode
 	.endm
 #endif
+
+/*
+ * STRT/LDRT access macros with ARM and Thumb-2 variants
+ */
+#ifdef CONFIG_THUMB2_KERNEL
+
+	.macro	usraccoff, instr, reg, ptr, inc, off, cond, abort
+9999:
+	.if	\inc == 1
+	\instr\cond\()bt \reg, [\ptr, #\off]
+	.elseif	\inc == 4
+	\instr\cond\()t \reg, [\ptr, #\off]
+	.else
+	.error	"Unsupported inc macro argument"
+	.endif
+
+	.section __ex_table,"a"
+	.align	3
+	.long	9999b, \abort
+	.previous
+	.endm
+
+	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort
+	@ explicit IT instruction needed because of the label
+	@ introduced by the USER macro
+	.ifnc	\cond,al
+	.if	\rept == 1
+	itt	\cond
+	.elseif	\rept == 2
+	ittt	\cond
+	.else
+	.error	"Unsupported rept macro argument"
+	.endif
+	.endif
+
+	@ Slightly optimised to avoid incrementing the pointer twice
+	usraccoff \instr, \reg, \ptr, \inc, 0, \cond, \abort
+	.if	\rept == 2
+	usraccoff \instr, \reg, \ptr, \inc, 4, \cond, \abort
+	.endif
+
+	add\cond \ptr, #\rept * \inc
+	.endm
+
+#else	/* !CONFIG_THUMB2_KERNEL */
+
+	.macro	usracc, instr, reg, ptr, inc, cond, rept, abort
+	.rept	\rept
+9999:
+	.if	\inc == 1
+	\instr\cond\()bt \reg, [\ptr], #\inc
+	.elseif	\inc == 4
+	\instr\cond\()t \reg, [\ptr], #\inc
+	.else
+	.error	"Unsupported inc macro argument"
+	.endif
+
+	.section __ex_table,"a"
+	.align	3
+	.long	9999b, \abort
+	.previous
+	.endr
+	.endm
+
+#endif	/* CONFIG_THUMB2_KERNEL */
+
+	.macro	strusr, reg, ptr, inc, cond=al, rept=1, abort=9001f
+	usracc	str, \reg, \ptr, \inc, \cond, \rept, \abort
+	.endm
+
+	.macro	ldrusr, reg, ptr, inc, cond=al, rept=1, abort=9001f
+	usracc	ldr, \reg, \ptr, \inc, \cond, \rept, \abort
+	.endm

commit b86040a59feb255a8193173caa4d5199464433d5
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 24 12:32:54 2009 +0100

    Thumb-2: Implementation of the unified start-up and exceptions code
    
    This patch implements the ARM/Thumb-2 unified kernel start-up and
    exception handling code.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 15f8a092b700..1acd1da36d00 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -127,3 +127,14 @@
 #endif
 #endif
 	.endm
+
+#ifdef CONFIG_THUMB2_KERNEL
+	.macro	setmode, mode, reg
+	mov	\reg, #\mode
+	msr	cpsr_c, \reg
+	.endm
+#else
+	.macro	setmode, mode, reg
+	msr	cpsr_c, #\mode
+	.endm
+#endif

commit bac4e960b5ce2453d862beaf20e59aa68af3b43a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon May 25 20:58:00 2009 +0100

    [ARM] barriers: improve xchg, bitops and atomic SMP barriers
    
    Mathieu Desnoyers pointed out that the ARM barriers were lacking:
    
    - cmpxchg, xchg and atomic add return need memory barriers on
      architectures which can reorder the relative order in which memory
      read/writes can be seen between CPUs, which seems to include recent
      ARM architectures. Those barriers are currently missing on ARM.
    
    - test_and_xxx_bit were missing SMP barriers.
    
    So put these barriers in.  Provide separate atomic_add/atomic_sub
    operations which do not require barriers.
    
    Reported-Reviewed-and-Acked-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
index 6116e4893c0a..15f8a092b700 100644
--- a/arch/arm/include/asm/assembler.h
+++ b/arch/arm/include/asm/assembler.h
@@ -114,3 +114,16 @@
 	.align	3;				\
 	.long	9999b,9001f;			\
 	.previous
+
+/*
+ * SMP data memory barrier
+ */
+	.macro	smp_dmb
+#ifdef CONFIG_SMP
+#if __LINUX_ARM_ARCH__ >= 7
+	dmb
+#elif __LINUX_ARM_ARCH__ == 6
+	mcr	p15, 0, r0, c7, c10, 5	@ dmb
+#endif
+#endif
+	.endm

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/assembler.h b/arch/arm/include/asm/assembler.h
new file mode 100644
index 000000000000..6116e4893c0a
--- /dev/null
+++ b/arch/arm/include/asm/assembler.h
@@ -0,0 +1,116 @@
+/*
+ *  arch/arm/include/asm/assembler.h
+ *
+ *  Copyright (C) 1996-2000 Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *  This file contains arm architecture specific defines
+ *  for the different processors.
+ *
+ *  Do not include any C declarations in this file - it is included by
+ *  assembler source.
+ */
+#ifndef __ASSEMBLY__
+#error "Only include this from assembly code"
+#endif
+
+#include <asm/ptrace.h>
+
+/*
+ * Endian independent macros for shifting bytes within registers.
+ */
+#ifndef __ARMEB__
+#define pull            lsr
+#define push            lsl
+#define get_byte_0      lsl #0
+#define get_byte_1	lsr #8
+#define get_byte_2	lsr #16
+#define get_byte_3	lsr #24
+#define put_byte_0      lsl #0
+#define put_byte_1	lsl #8
+#define put_byte_2	lsl #16
+#define put_byte_3	lsl #24
+#else
+#define pull            lsl
+#define push            lsr
+#define get_byte_0	lsr #24
+#define get_byte_1	lsr #16
+#define get_byte_2	lsr #8
+#define get_byte_3      lsl #0
+#define put_byte_0	lsl #24
+#define put_byte_1	lsl #16
+#define put_byte_2	lsl #8
+#define put_byte_3      lsl #0
+#endif
+
+/*
+ * Data preload for architectures that support it
+ */
+#if __LINUX_ARM_ARCH__ >= 5
+#define PLD(code...)	code
+#else
+#define PLD(code...)
+#endif
+
+/*
+ * This can be used to enable code to cacheline align the destination
+ * pointer when bulk writing to memory.  Experiments on StrongARM and
+ * XScale didn't show this a worthwhile thing to do when the cache is not
+ * set to write-allocate (this would need further testing on XScale when WA
+ * is used).
+ *
+ * On Feroceon there is much to gain however, regardless of cache mode.
+ */
+#ifdef CONFIG_CPU_FEROCEON
+#define CALGN(code...) code
+#else
+#define CALGN(code...)
+#endif
+
+/*
+ * Enable and disable interrupts
+ */
+#if __LINUX_ARM_ARCH__ >= 6
+	.macro	disable_irq
+	cpsid	i
+	.endm
+
+	.macro	enable_irq
+	cpsie	i
+	.endm
+#else
+	.macro	disable_irq
+	msr	cpsr_c, #PSR_I_BIT | SVC_MODE
+	.endm
+
+	.macro	enable_irq
+	msr	cpsr_c, #SVC_MODE
+	.endm
+#endif
+
+/*
+ * Save the current IRQ state and disable IRQs.  Note that this macro
+ * assumes FIQs are enabled, and that the processor is in SVC mode.
+ */
+	.macro	save_and_disable_irqs, oldcpsr
+	mrs	\oldcpsr, cpsr
+	disable_irq
+	.endm
+
+/*
+ * Restore interrupt state previously stored in a register.  We don't
+ * guarantee that this will preserve the flags.
+ */
+	.macro	restore_irqs, oldcpsr
+	msr	cpsr_c, \oldcpsr
+	.endm
+
+#define USER(x...)				\
+9999:	x;					\
+	.section __ex_table,"a";		\
+	.align	3;				\
+	.long	9999b,9001f;			\
+	.previous
