commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index ed8fd0d19a3e..99035b5891ef 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -1,13 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  *  arch/arm/include/asm/memory.h
  *
  *  Copyright (C) 2000-2002 Russell King
  *  modification for nommu, Hyok S. Choi, 2004
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
  *  Note: this file should not be included by non-asm/.h files
  */
 #ifndef __ASM_ARM_MEMORY_H

commit 2dd8a62c647691161a2346546834262597739872
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Apr 10 16:36:19 2018 -0700

    linux/const.h: move UL() macro to include/linux/const.h
    
    ARM, ARM64 and UniCore32 duplicate the definition of UL():
    
      #define UL(x) _AC(x, UL)
    
    This is not actually arch-specific, so it will be useful to move it to a
    common header.  Currently, we only have the uapi variant for
    linux/const.h, so I am creating include/linux/const.h.
    
    I also added _UL(), _ULL() and ULL() because _AC() is mostly used in
    the form either _AC(..., UL) or _AC(..., ULL).  I expect they will be
    replaced in follow-up cleanups.  The underscore-prefixed ones should
    be used for exported headers.
    
    Link: http://lkml.kernel.org/r/1519301715-31798-4-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 496667703693..ed8fd0d19a3e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -22,12 +22,6 @@
 #include <mach/memory.h>
 #endif
 
-/*
- * Allow for constants defined here to be used from assembly code
- * by prepending the UL suffix only with actual C code compilation.
- */
-#define UL(x) _AC(x, UL)
-
 /* PAGE_OFFSET - the virtual address of the start of the kernel image */
 #define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
 

commit 62d1c95d577ce4d40189e4c01025b616917e3c65
Author: Vladimir Murzin <vladimir.murzin@arm.com>
Date:   Wed Dec 27 10:38:55 2017 +0100

    ARM: 8739/1: NOMMU: Setup VBAR/Hivecs for secondaries cores
    
    With switch to dynamic exception base address setting, VBAR/Hivecs
    set only for boot CPU, but secondaries stay unaware of that. That
    might lead to weird effects when trying up to bring up secondaries.
    
    Fixes: ad475117d201 ("ARM: 8649/2: nommu: remove Hivecs configuration is asm")
    Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Acked-by: afzal mohammed <afzal.mohd.ma@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 1f54e4e98c1e..496667703693 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -88,6 +88,7 @@
 #else /* CONFIG_MMU */
 
 #ifndef __ASSEMBLY__
+extern unsigned long setup_vectors_base(void);
 extern unsigned long vectors_base;
 #define VECTORS_BASE		vectors_base
 #endif

commit 58c16709f9cad7e6daabeaa5c94ac4dcb260aedd
Author: Afzal Mohammed <afzal.mohd.ma@gmail.com>
Date:   Wed Feb 1 13:39:18 2017 +0100

    ARM: 8648/2: nommu: display vectors base
    
    VECTORS_BASE displays the exception base address. Now on no-MMU as
    the exception base address is dynamically estimated, define
    VECTORS_BASE to the variable holding it.
    
    As it is the case, limit VECTORS_BASE constant definition to MMU.
    
    Suggested-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: afzal mohammed <afzal.mohd.ma@gmail.com>
    Tested-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 00bd3529854a..1f54e4e98c1e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -83,8 +83,15 @@
 #define IOREMAP_MAX_ORDER	24
 #endif
 
+#define VECTORS_BASE		UL(0xffff0000)
+
 #else /* CONFIG_MMU */
 
+#ifndef __ASSEMBLY__
+extern unsigned long vectors_base;
+#define VECTORS_BASE		vectors_base
+#endif
+
 /*
  * The limitation of user task size can grow up to the end of free ram region.
  * It is difficult to define and perhaps will never meet the original meaning
@@ -111,8 +118,6 @@
 
 #endif /* !CONFIG_MMU */
 
-#define VECTORS_BASE		UL(0xffff0000)
-
 #ifdef CONFIG_XIP_KERNEL
 #define KERNEL_START		_sdata
 #else

commit d2ca5f2491c1246adf3847101fdc538a3b89439c
Author: Afzal Mohammed <afzal.mohd.ma@gmail.com>
Date:   Sun Jan 29 17:31:32 2017 +0100

    ARM: 8646/1: mmu: decouple VECTORS_BASE from Kconfig
    
    For MMU configurations, VECTORS_BASE is always 0xffff0000, a macro
    definition will suffice.
    
    For no-MMU, exception base address is dynamically determined in
    subsequent patches. To preserve bisectability, now make the
    macro applicable for no-MMU scenario too.
    
    Thanks to 0-DAY kernel test infrastructure that found the
    bisectability issue. This macro will be restricted to MMU case upon
    dynamically determining exception base address for no-MMU.
    
    Once exception address is handled dynamically for no-MMU,
    VECTORS_BASE can be removed from Kconfig.
    
    Signed-off-by: afzal mohammed <afzal.mohd.ma@gmail.com>
    Tested-by: Vladimir Murzin <vladimir.murzin@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index c30d0d82a105..00bd3529854a 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -111,6 +111,8 @@
 
 #endif /* !CONFIG_MMU */
 
+#define VECTORS_BASE		UL(0xffff0000)
+
 #ifdef CONFIG_XIP_KERNEL
 #define KERNEL_START		_sdata
 #else

commit e377cd8221ebbe0b517861aa3d823bb42f9abbd4
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Sun Jan 15 03:59:00 2017 +0100

    ARM: 8640/1: Add support for CONFIG_DEBUG_VIRTUAL
    
    x86 has an option: CONFIG_DEBUG_VIRTUAL to do additional checks on
    virt_to_phys calls. The goal is to catch users who are calling
    virt_to_phys on non-linear addresses immediately. This includes caller
    using __virt_to_phys() on image addresses instead of __pa_symbol(). This
    is a generally useful debug feature to spot bad code (particulary in
    drivers).
    
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Laura Abbott <labbott@redhat.com>
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index bee7511c5098..c30d0d82a105 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -213,7 +213,7 @@ extern const void *__pv_table_begin, *__pv_table_end;
 	: "r" (x), "I" (__PV_BITS_31_24)		\
 	: "cc")
 
-static inline phys_addr_t __virt_to_phys(unsigned long x)
+static inline phys_addr_t __virt_to_phys_nodebug(unsigned long x)
 {
 	phys_addr_t t;
 
@@ -245,7 +245,7 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 #define PHYS_OFFSET	PLAT_PHYS_OFFSET
 #define PHYS_PFN_OFFSET	((unsigned long)(PHYS_OFFSET >> PAGE_SHIFT))
 
-static inline phys_addr_t __virt_to_phys(unsigned long x)
+static inline phys_addr_t __virt_to_phys_nodebug(unsigned long x)
 {
 	return (phys_addr_t)x - PAGE_OFFSET + PHYS_OFFSET;
 }
@@ -261,6 +261,16 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 	((((unsigned long)(kaddr) - PAGE_OFFSET) >> PAGE_SHIFT) + \
 	 PHYS_PFN_OFFSET)
 
+#define __pa_symbol_nodebug(x)	__virt_to_phys_nodebug((x))
+
+#ifdef CONFIG_DEBUG_VIRTUAL
+extern phys_addr_t __virt_to_phys(unsigned long x);
+extern phys_addr_t __phys_addr_symbol(unsigned long x);
+#else
+#define __virt_to_phys(x)	__virt_to_phys_nodebug(x)
+#define __phys_addr_symbol(x)	__pa_symbol_nodebug(x)
+#endif
+
 /*
  * These are *only* valid on the kernel direct mapped RAM memory.
  * Note: Drivers should NOT use these.  They are the wrong
@@ -283,6 +293,7 @@ static inline void *phys_to_virt(phys_addr_t x)
  * Drivers should NOT use these either.
  */
 #define __pa(x)			__virt_to_phys((unsigned long)(x))
+#define __pa_symbol(x)		__phys_addr_symbol(RELOC_HIDE((unsigned long)(x), 0))
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((phys_addr_t)(pfn) << PAGE_SHIFT)
 

commit a09975bf6c756e4555a95258ff4b2286dcfddc4e
Author: Florian Fainelli <f.fainelli@gmail.com>
Date:   Sun Jan 15 03:57:40 2017 +0100

    ARM: 8639/1: Define KERNEL_START and KERNEL_END
    
    In preparation for adding CONFIG_DEBUG_VIRTUAL support, define a set of
    common constants: KERNEL_START and KERNEL_END which abstract
    CONFIG_XIP_KERNEL vs. !CONFIG_XIP_KERNEL. Update the code where
    relevant.
    
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Signed-off-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 76cbd9c674df..bee7511c5098 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -111,6 +111,13 @@
 
 #endif /* !CONFIG_MMU */
 
+#ifdef CONFIG_XIP_KERNEL
+#define KERNEL_START		_sdata
+#else
+#define KERNEL_START		_stext
+#endif
+#define KERNEL_END		_end
+
 /*
  * We fix the TCM memories max 32 KiB ITCM resp DTCM at these
  * locations

commit 7d281b620d229486429d851b10a05da871d22e79
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Jul 28 19:37:21 2016 +0100

    ARM: 8589/1: asm/memory.h: remove dead definitions
    
    The last ad-hoc __phys_to_virt definition was removed in commit fd0053c9
    ("ARM: realview: remove sparsemem hack"). Therefore we can remove the
    unneeded definitions and unduplicate the virt_to_pfn macro from
    asm/memory.h.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 31c07a2cc100..76cbd9c674df 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -159,13 +159,8 @@
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.
  */
-#if defined(__virt_to_phys)
-#define PHYS_OFFSET	PLAT_PHYS_OFFSET
-#define PHYS_PFN_OFFSET	((unsigned long)(PHYS_OFFSET >> PAGE_SHIFT))
-
-#define virt_to_pfn(kaddr) (__pa(kaddr) >> PAGE_SHIFT)
 
-#elif defined(CONFIG_ARM_PATCH_PHYS_VIRT)
+#if defined(CONFIG_ARM_PATCH_PHYS_VIRT)
 
 /*
  * Constants used to force the right instruction encodings and shifts
@@ -182,10 +177,6 @@ extern const void *__pv_table_begin, *__pv_table_end;
 #define PHYS_OFFSET	((phys_addr_t)__pv_phys_pfn_offset << PAGE_SHIFT)
 #define PHYS_PFN_OFFSET	(__pv_phys_pfn_offset)
 
-#define virt_to_pfn(kaddr) \
-	((((unsigned long)(kaddr) - PAGE_OFFSET) >> PAGE_SHIFT) + \
-	 PHYS_PFN_OFFSET)
-
 #define __pv_stub(from,to,instr,type)			\
 	__asm__("@ __pv_stub\n"				\
 	"1:	" instr "	%0, %1, %2\n"		\
@@ -257,12 +248,12 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 	return x - PHYS_OFFSET + PAGE_OFFSET;
 }
 
+#endif
+
 #define virt_to_pfn(kaddr) \
 	((((unsigned long)(kaddr) - PAGE_OFFSET) >> PAGE_SHIFT) + \
 	 PHYS_PFN_OFFSET)
 
-#endif
-
 /*
  * These are *only* valid on the kernel direct mapped RAM memory.
  * Note: Drivers should NOT use these.  They are the wrong

commit 07a7056ccce3ffdb65908bf502aeb2503714da46
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Mar 15 15:00:30 2016 +0000

    ARM: provide arm_has_idmap_alias() helper
    
    Provide a helper to indicate whether we need to perform special handling
    for boot identity mapping aliases or not.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Reviewed-by: Pratyush Anand <panand@redhat.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index ca208335fde6..31c07a2cc100 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -295,6 +295,11 @@ extern long long arch_phys_to_idmap_offset;
  * of physical memory for idmap purposes.  Most cases should leave these
  * untouched.  Note: this can only return addresses less than 4GiB.
  */
+static inline bool arm_has_idmap_alias(void)
+{
+	return IS_ENABLED(CONFIG_MMU) && arch_phys_to_idmap_offset != 0;
+}
+
 #define IDMAP_INVALID_ADDR ((u32)~0)
 
 static inline unsigned long phys_to_idmap(phys_addr_t addr)

commit 981b6714dbd26609212536b9fed43e49db1459cf
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Mar 15 14:55:03 2016 +0000

    ARM: provide improved virt_to_idmap() functionality
    
    For kexec, we need more functionality from the IDMAP system.  We need to
    be able to convert physical addresses to their identity mappped versions
    as well as virtual addresses.
    
    Convert the existing arch_virt_to_idmap() to deal with physical
    addresses instead.
    
    Acked-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 9427fd632552..ca208335fde6 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -288,19 +288,38 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((phys_addr_t)(pfn) << PAGE_SHIFT)
 
-extern unsigned long (*arch_virt_to_idmap)(unsigned long x);
+extern long long arch_phys_to_idmap_offset;
 
 /*
- * These are for systems that have a hardware interconnect supported alias of
- * physical memory for idmap purposes.  Most cases should leave these
+ * These are for systems that have a hardware interconnect supported alias
+ * of physical memory for idmap purposes.  Most cases should leave these
  * untouched.  Note: this can only return addresses less than 4GiB.
  */
+#define IDMAP_INVALID_ADDR ((u32)~0)
+
+static inline unsigned long phys_to_idmap(phys_addr_t addr)
+{
+	if (IS_ENABLED(CONFIG_MMU) && arch_phys_to_idmap_offset) {
+		addr += arch_phys_to_idmap_offset;
+		if (addr > (u32)~0)
+			addr = IDMAP_INVALID_ADDR;
+	}
+	return addr;
+}
+
+static inline phys_addr_t idmap_to_phys(unsigned long idmap)
+{
+	phys_addr_t addr = idmap;
+
+	if (IS_ENABLED(CONFIG_MMU) && arch_phys_to_idmap_offset)
+		addr -= arch_phys_to_idmap_offset;
+
+	return addr;
+}
+
 static inline unsigned long __virt_to_idmap(unsigned long x)
 {
-	if (IS_ENABLED(CONFIG_MMU) && arch_virt_to_idmap)
-		return arch_virt_to_idmap(x);
-	else
-		return __virt_to_phys(x);
+	return phys_to_idmap(__virt_to_phys(x));
 }
 
 #define virt_to_idmap(x)	__virt_to_idmap((unsigned long)(x))

commit 9e0087e64e67cfe0b63b629add793a4aa019a629
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Feb 18 15:55:59 2016 +0100

    ARM: 8530/1: remove VIRT_TO_BUS
    
    All drivers that are relevant for rpc or footbridge have stopped
    using virt_to_bus a while ago, so we can remove it and avoid some
    harmless randconfig warnings for drivers that we do not care about:
    
    drivers/atm/zatm.c: In function 'poll_rx':
    drivers/atm/zatm.c:401:18: warning: 'bus_to_virt' is deprecated [-Wdeprecated-declarations]
       skb = ((struct rx_buffer_head *) bus_to_virt(here[2]))->skb;
    
    FWIW, the remaining drivers using this are:
    
    ATM:  firestream, zatm, ambassador, horizon
    ISDN: hisax/netjet
    V4L:  STA2X11, zoran
    Net:  Appletalk LTPC, Tulip DE4x5, Toshiba IrDA
    WAN:  comtrol sv11, cosa, lanmedia, sealevel
    SCSI: DPT_I2O, buslogic
    VME:  CA91C142
    
    My best guess is that all of the above are so hopelessly obsolete that
    we are best off removing all of them form the kernel, but that can be
    done another time.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 51fc9b3b52ea..9427fd632552 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -318,20 +318,6 @@ static inline unsigned long __virt_to_idmap(unsigned long x)
 #define __bus_to_pfn(x)	__phys_to_pfn(x)
 #endif
 
-#ifdef CONFIG_VIRT_TO_BUS
-#define virt_to_bus virt_to_bus
-static inline __deprecated unsigned long virt_to_bus(void *x)
-{
-	return __virt_to_bus((unsigned long)x);
-}
-
-#define bus_to_virt bus_to_virt
-static inline __deprecated void *bus_to_virt(unsigned long x)
-{
-	return (void *)__bus_to_virt(x);
-}
-#endif
-
 /*
  * Conversion between a struct page and a physical address.
  *

commit 8ff97fa31333e8d0f4f7029798d9c7d59359b05c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Feb 16 17:33:56 2016 +0000

    ARM: make the physical-relative calculation more obvious
    
    The physical-relative calculation between the XIP text and data sections
    introduced by the previous patch was far from obvious. Let's simplify it
    by turning it into a macro which takes the two (virtual) addresses.
    
    This allows us to arrange the calculation in a more obvious manner - we
    can make it two sub-expressions which calculate the physical address for
    each symbol, and then takes the difference of those physical addresses.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index ebdaaf7dd19f..51fc9b3b52ea 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -135,11 +135,18 @@
 #define PLAT_PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
 
 #ifdef CONFIG_XIP_KERNEL
-#define PHYS_OFFSET_FIXUP \
-	( XIP_VIRT_ADDR(CONFIG_XIP_PHYS_ADDR) - PAGE_OFFSET + \
-	  PLAT_PHYS_OFFSET - CONFIG_XIP_PHYS_ADDR )
+/*
+ * When referencing data in RAM from the XIP region in a relative manner
+ * with the MMU off, we need the relative offset between the two physical
+ * addresses.  The macro below achieves this, which is:
+ *    __pa(v_data) - __xip_pa(v_text)
+ */
+#define PHYS_RELATIVE(v_data, v_text) \
+	(((v_data) - PAGE_OFFSET + PLAT_PHYS_OFFSET) - \
+	 ((v_text) - XIP_VIRT_ADDR(CONFIG_XIP_PHYS_ADDR) + \
+          CONFIG_XIP_PHYS_ADDR))
 #else
-#define PHYS_OFFSET_FIXUP 0
+#define PHYS_RELATIVE(v_data, v_text) ((v_data) - (v_text))
 #endif
 
 #ifndef __ASSEMBLY__

commit d78114554939aec0344b494e759d0679224562db
Author: Nicolas Pitre <nico@linaro.org>
Date:   Tue Feb 2 00:14:53 2016 +0100

    ARM: 8512/1: proc-v7.S: Adjust stack address when XIP_KERNEL
    
    When XIP_KERNEL is enabled, the virt to phys address translation for RAM
    is not the same as the virt to phys address translation for .text.
    The only way to know where physical RAM is located is to use
    PLAT_PHYS_OFFSET.
    The MACRO will be useful for other places where there is a similar problem.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Chris Brandt <chris.brandt@renesas.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 49bf6b1e2177..ebdaaf7dd19f 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -134,6 +134,14 @@
  */
 #define PLAT_PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
 
+#ifdef CONFIG_XIP_KERNEL
+#define PHYS_OFFSET_FIXUP \
+	( XIP_VIRT_ADDR(CONFIG_XIP_PHYS_ADDR) - PAGE_OFFSET + \
+	  PLAT_PHYS_OFFSET - CONFIG_XIP_PHYS_ADDR )
+#else
+#define PHYS_OFFSET_FIXUP 0
+#endif
+
 #ifndef __ASSEMBLY__
 
 /*

commit 2841029393fad551b49b6de34d44bfa9ef256441
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jan 11 17:15:58 2016 +0000

    ARM: make virt_to_idmap() return unsigned long
    
    Make virt_to_idmap() return an unsigned long rather than phys_addr_t.
    
    Returning phys_addr_t here makes no sense, because the definition of
    virt_to_idmap() is that it shall return a physical address which maps
    identically with the virtual address.  Since virtual addresses are
    limited to 32-bit, identity mapped physical addresses are as well.
    
    Almost all users already had an implicit narrowing cast to unsigned long
    so let's make this official and part of this interface.
    
    Tested-by: Grygorii Strashko <grygorii.strashko@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index c79b57bf71c4..49bf6b1e2177 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -273,14 +273,14 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((phys_addr_t)(pfn) << PAGE_SHIFT)
 
-extern phys_addr_t (*arch_virt_to_idmap)(unsigned long x);
+extern unsigned long (*arch_virt_to_idmap)(unsigned long x);
 
 /*
  * These are for systems that have a hardware interconnect supported alias of
  * physical memory for idmap purposes.  Most cases should leave these
- * untouched.
+ * untouched.  Note: this can only return addresses less than 4GiB.
  */
-static inline phys_addr_t __virt_to_idmap(unsigned long x)
+static inline unsigned long __virt_to_idmap(unsigned long x)
 {
 	if (IS_ENABLED(CONFIG_MMU) && arch_virt_to_idmap)
 		return arch_virt_to_idmap(x);

commit 803e3dbcb4cf80c898faccf01875f6ff6e5e76fd
Author: Sergey Dyasly <dserrg@gmail.com>
Date:   Wed Sep 9 16:27:18 2015 +0100

    ARM: 8430/1: use default ioremap alignment for SMP or LPAE
    
    16MB alignment for ioremap mappings was added by commit a069c896d0d6 ("[ARM]
    3705/1: add supersection support to ioremap()") in order to support supersection
    mappings. But __arm_ioremap_pfn_caller uses section and supersection mappings
    only in !SMP && !LPAE case. There is no need for such big alignment if either
    SMP or LPAE is enabled.
    
    After this change, ioremap will use default maximum alignment of 128 pages.
    
    Link: https://lkml.kernel.org/g/1419328813-2211-1-git-send-email-d.safonov@partner.samsung.com
    
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: James Bottomley <JBottomley@parallels.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Arnd Bergmann <arnd.bergmann@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Dmitry Safonov <d.safonov@partner.samsung.com>
    Signed-off-by: Sergey Dyasly <s.dyasly@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 98d58bb04ac5..c79b57bf71c4 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -76,10 +76,12 @@
  */
 #define XIP_VIRT_ADDR(physaddr)  (MODULES_VADDR + ((physaddr) & 0x000fffff))
 
+#if !defined(CONFIG_SMP) && !defined(CONFIG_ARM_LPAE)
 /*
  * Allow 16MB-aligned ioremap pages
  */
 #define IOREMAP_MAX_ORDER	24
+#endif
 
 #else /* CONFIG_MMU */
 

commit 012dcef3f058385268630c0003e9b7f8dcafbeb4
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Aug 7 17:41:01 2015 -0400

    mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
    
    Three architectures already define these, and we'll need them genericly
    soon.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index b7f6fb462ea0..98d58bb04ac5 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -118,12 +118,6 @@
 #define DTCM_OFFSET	UL(0xfffe8000)
 #endif
 
-/*
- * Convert a physical address to a Page Frame Number and back
- */
-#define	__phys_to_pfn(paddr)	((unsigned long)((paddr) >> PAGE_SHIFT))
-#define	__pfn_to_phys(pfn)	((phys_addr_t)(pfn) << PAGE_SHIFT)
-
 /*
  * Convert a page to/from a physical address
  */

commit 0871b7248113ebfccbfabcd3fd1f867a2bc681f4
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 17 10:33:04 2015 +0100

    ARM: fix __virt_to_idmap build error on !MMU
    
    Fengguang Wu reports that building ARM with !MMU results in the
    following build error:
    
       arch/arm/kernel/built-in.o: In function `__soft_restart':
    >> :(.text+0x1624): undefined reference to `arch_virt_to_idmap'
    
    Fix this by adding an appropriate IS_ENABLED(CONFIG_MMU) into the
    __virt_to_idmap() inline function.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 6f225acc07c5..b7f6fb462ea0 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -286,7 +286,7 @@ extern phys_addr_t (*arch_virt_to_idmap)(unsigned long x);
  */
 static inline phys_addr_t __virt_to_idmap(unsigned long x)
 {
-	if (arch_virt_to_idmap)
+	if (IS_ENABLED(CONFIG_MMU) && arch_virt_to_idmap)
 		return arch_virt_to_idmap(x);
 	else
 		return __virt_to_phys(x);

commit 06be5eefe1192eb8ce8d07497f67595b6bfe9741
Merge: 11b8b25ce4f8 1bd46782d08b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jul 7 12:35:33 2015 +0100

    Merge branches 'fixes' and 'ioremap' into for-linus

commit e48866647b486f31ff7c3927b48de8bbb1c6a4c0
Author: Vitaly Andrianov <vitalya@ti.com>
Date:   Fri Jun 26 17:13:03 2015 +0100

    ARM: 8396/1: use phys_addr_t in pfn_to_kaddr()
    
    This patch fixes pfn_to_kaddr() to use phys_addr_t.  Without this,
    this macro is broken on LPAE systems. For physical addresses above
    first 4GB result of shifting pfn with PAGE_SHIFT may be truncated.
    
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 184def0e1652..063ef314cca9 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -291,7 +291,7 @@ static inline void *phys_to_virt(phys_addr_t x)
  */
 #define __pa(x)			__virt_to_phys((unsigned long)(x))
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
-#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+#define pfn_to_kaddr(pfn)	__va((phys_addr_t)(pfn) << PAGE_SHIFT)
 
 extern phys_addr_t (*arch_virt_to_idmap)(unsigned long x);
 

commit b2c3e38a54714e917c9e8675ff5812dca1c0f39d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Apr 4 20:09:46 2015 +0100

    ARM: redo TTBR setup code for LPAE
    
    Re-engineer the LPAE TTBR setup code.  Rather than passing some shifted
    address in order to fit in a CPU register, pass either a full physical
    address (in the case of r4, r5 for TTBR0) or a PFN (for TTBR1).
    
    This removes the ARCH_PGD_SHIFT hack, and the last dangerous user of
    cpu_set_ttbr() in the secondary CPU startup code path (which was there
    to re-set TTBR1 to the appropriate high physical address space on
    Keystone2.)
    
    Tested-by: Murali Karicheri <m-karicheri2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 184def0e1652..3a72d69b3255 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -18,8 +18,6 @@
 #include <linux/types.h>
 #include <linux/sizes.h>
 
-#include <asm/cache.h>
-
 #ifdef CONFIG_NEED_MACH_MEMORY_H
 #include <mach/memory.h>
 #endif
@@ -132,20 +130,6 @@
 #define page_to_phys(page)	(__pfn_to_phys(page_to_pfn(page)))
 #define phys_to_page(phys)	(pfn_to_page(__phys_to_pfn(phys)))
 
-/*
- * Minimum guaranted alignment in pgd_alloc().  The page table pointers passed
- * around in head.S and proc-*.S are shifted by this amount, in order to
- * leave spare high bits for systems with physical address extension.  This
- * does not fully accomodate the 40-bit addressing capability of ARM LPAE, but
- * gives us about 38-bits or so.
- */
-#ifdef CONFIG_ARM_LPAE
-#define ARCH_PGD_SHIFT		L1_CACHE_SHIFT
-#else
-#define ARCH_PGD_SHIFT		0
-#endif
-#define ARCH_PGD_MASK		((1 << ARCH_PGD_SHIFT) - 1)
-
 /*
  * PLAT_PHYS_OFFSET is the offset (from zero) of the start of physical
  * memory.  This is used for XIP and NoMMU kernels, and on platforms that don't

commit 84c4d3a6d438f59438e15cc046fe1a7cafc9069a
Author: Thierry Reding <treding@nvidia.com>
Date:   Mon Jul 28 16:34:18 2014 +0200

    ARM: Use include/asm-generic/io.h
    
    Include the generic I/O header file so that duplicate implementations
    can be removed. This will also help to establish consistency across more
    architectures regarding which accessors they support.
    
    Signed-off-by: Thierry Reding <treding@nvidia.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index e731018869a7..184def0e1652 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -274,11 +274,13 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
  * translation for translating DMA addresses.  Use the driver
  * DMA support - see dma-mapping.h.
  */
+#define virt_to_phys virt_to_phys
 static inline phys_addr_t virt_to_phys(const volatile void *x)
 {
 	return __virt_to_phys((unsigned long)(x));
 }
 
+#define phys_to_virt phys_to_virt
 static inline void *phys_to_virt(phys_addr_t x)
 {
 	return (void *)__phys_to_virt(x);
@@ -322,11 +324,13 @@ static inline phys_addr_t __virt_to_idmap(unsigned long x)
 #endif
 
 #ifdef CONFIG_VIRT_TO_BUS
+#define virt_to_bus virt_to_bus
 static inline __deprecated unsigned long virt_to_bus(void *x)
 {
 	return __virt_to_bus((unsigned long)x);
 }
 
+#define bus_to_virt bus_to_virt
 static inline __deprecated void *bus_to_virt(unsigned long x)
 {
 	return (void *)__bus_to_virt(x);

commit f15bdfe4fb264ac30d9c176f898cbd52cfd1ffa9
Merge: c89c3a6acb84 c70fbb01b11c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Aug 5 10:27:25 2014 +0100

    Merge branch 'devel-stable' into for-next
    
    Conflicts:
            arch/arm/kernel/perf_event_cpu.c

commit c6f54a9b39626090c934646f7d732e31b70ffce7
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Wed Jul 23 20:37:43 2014 +0100

    ARM: 8113/1: remove remaining definitions of PLAT_PHYS_OFFSET from <mach/memory.h>
    
    The platforms selecting NEED_MACH_MEMORY_H defined the start address of
    their physical memory in the respective <mach/memory.h>. With
    ARM_PATCH_PHYS_VIRT=y (which is quite common today) this is useless
    though because the definition isn't used but determined dynamically.
    
    So remove the definitions from all <mach/memory.h> and provide the
    Kconfig symbol PHYS_OFFSET with the respective defaults in case
    ARM_PATCH_PHYS_VIRT isn't enabled.
    
    This allows to drop the dependency of PHYS_OFFSET on !NEED_MACH_MEMORY_H
    which prevents compiling an integrator nommu-kernel.
    (CONFIG_PAGE_OFFSET which has "default PHYS_OFFSET if !MMU" expanded to
    "0x" because CONFIG_PHYS_OFFSET doesn't exist as INTEGRATOR selects
    NEED_MACH_MEMORY_H.)
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 2b751464d6ff..04ccf1c0a1af 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -150,13 +150,11 @@
 
 /*
  * PLAT_PHYS_OFFSET is the offset (from zero) of the start of physical
- * memory.  This is used for XIP and NoMMU kernels, or by kernels which
- * have their own mach/memory.h.  Assembly code must always use
+ * memory.  This is used for XIP and NoMMU kernels, and on platforms that don't
+ * have CONFIG_ARM_PATCH_PHYS_VIRT. Assembly code must always use
  * PLAT_PHYS_OFFSET and not PHYS_OFFSET.
  */
-#ifndef PLAT_PHYS_OFFSET
 #define PLAT_PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
-#endif
 
 #ifndef __ASSEMBLY__
 

commit 03eca200064381d05a54126a711203d443508d80
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Tue Jun 3 17:24:51 2014 +0200

    ARM: nommu: don't limit TASK_SIZE
    
    With TASK_SIZE set to the maximal RAM address booting in some XIP
    configurations fails (e.g. on efm32 DK3750). The problem is that
    strncpy_from_user et al. check for the address not being above TASK_SIZE
    (since 8c56cc8be5b3 (ARM: 7449/1: use generic strnlen_user and
    strncpy_from_user functions)) and this makes booting fail if the XIP
    flash is above the RAM address space.
    
    This change is in line with blackfin, frv and m68k which also use
    0xffffffff for TASK_SIZE with !MMU.
    
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 2b751464d6ff..c6bbb7daea59 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -91,9 +91,7 @@
  * of this define that was meant to.
  * Fortunately, there is no reference for this in noMMU mode, for now.
  */
-#ifndef TASK_SIZE
-#define TASK_SIZE		(CONFIG_DRAM_SIZE)
-#endif
+#define TASK_SIZE		UL(0xffffffff)
 
 #ifndef TASK_UNMAPPED_BASE
 #define TASK_UNMAPPED_BASE	UL(0x00000000)

commit 64d3b6a3f480154b6727dd2187f5f2b58c15da77
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Apr 9 21:24:02 2014 +0100

    ARM: 8023/1: remove remnants of the static DMA mapping
    
    It looks like the static mapping area for DMA was replaced by dynamic
    allocation into the vmalloc area by commit e9da6e9905e6 but the
    information in Documentation/arm/memory.txt was not removed accordingly.
    
    CONSISTENT_END in arch/arm/include/asm/memory.h has no more users and
    can be removed as well.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 02fa2558f662..2b751464d6ff 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -83,8 +83,6 @@
  */
 #define IOREMAP_MAX_ORDER	24
 
-#define CONSISTENT_END		(0xffe00000UL)
-
 #else /* CONFIG_MMU */
 
 /*

commit 95959e6a06720834fc80a210e37898341c63cb91
Merge: f210c53a82ab 95c52fe06335 e26a9e00afc4 dfdf5f63b438 aa4c5b962a7a 1ef2bf8227ab
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Apr 4 00:33:32 2014 +0100

    Merge branches 'amba', 'fixes', 'misc', 'mmci', 'unstable/omap-dma' and 'unstable/sa11x0' into for-next

commit e26a9e00afc482b971afcaef1db8c9034d4d6d7c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Mar 25 19:45:31 2014 +0000

    ARM: Better virt_to_page() handling
    
    virt_to_page() is incredibly inefficient when virt-to-phys patching is
    enabled.  This is because we end up with this calculation:
    
      page = &mem_map[asm virt_to_phys(addr) >> 12 - __pv_phys_offset >> 12]
    
    in assembly.  The asm virt_to_phys() is equivalent this this operation:
    
      addr - PAGE_OFFSET + __pv_phys_offset
    
    and we can see that because this is assembly, the compiler has no chance
    to optimise some of that away.  This should reduce down to:
    
      page = &mem_map[(addr - PAGE_OFFSET) >> 12]
    
    for the common cases.  Permit the compiler to make this optimisation by
    giving it more of the information it needs - do this by providing a
    virt_to_pfn() macro.
    
    Another issue which makes this more complex is that __pv_phys_offset is
    a 64-bit type on all platforms.  This is needlessly wasteful - if we
    store the physical offset as a PFN, we can save a lot of work having
    to deal with 64-bit values, which sometimes ends up producing incredibly
    horrid code:
    
         a4c:       e3009000        movw    r9, #0
                            a4c: R_ARM_MOVW_ABS_NC  __pv_phys_offset
         a50:       e3409000        movt    r9, #0          ; r9 = &__pv_phys_offset
                            a50: R_ARM_MOVT_ABS     __pv_phys_offset
         a54:       e3002000        movw    r2, #0
                            a54: R_ARM_MOVW_ABS_NC  __pv_phys_offset
         a58:       e3402000        movt    r2, #0          ; r2 = &__pv_phys_offset
                            a58: R_ARM_MOVT_ABS     __pv_phys_offset
         a5c:       e5999004        ldr     r9, [r9, #4]    ; r9 = high word of __pv_phys_offset
         a60:       e3001000        movw    r1, #0
                            a60: R_ARM_MOVW_ABS_NC  mem_map
         a64:       e592c000        ldr     ip, [r2]        ; ip = low word of __pv_phys_offset
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 8756e4bcdba0..2438d72cf4e6 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -169,9 +169,17 @@
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h
  * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
+ *
+ * PFNs are used to describe any physical page; this means
+ * PFN 0 == physical address 0.
  */
-#ifndef __virt_to_phys
-#ifdef CONFIG_ARM_PATCH_PHYS_VIRT
+#if defined(__virt_to_phys)
+#define PHYS_OFFSET	PLAT_PHYS_OFFSET
+#define PHYS_PFN_OFFSET	((unsigned long)(PHYS_OFFSET >> PAGE_SHIFT))
+
+#define virt_to_pfn(kaddr) (__pa(kaddr) >> PAGE_SHIFT)
+
+#elif defined(CONFIG_ARM_PATCH_PHYS_VIRT)
 
 /*
  * Constants used to force the right instruction encodings and shifts
@@ -180,12 +188,17 @@
 #define __PV_BITS_31_24	0x81000000
 #define __PV_BITS_7_0	0x81
 
-extern u64 __pv_phys_offset;
+extern unsigned long __pv_phys_pfn_offset;
 extern u64 __pv_offset;
 extern void fixup_pv_table(const void *, unsigned long);
 extern const void *__pv_table_begin, *__pv_table_end;
 
-#define PHYS_OFFSET __pv_phys_offset
+#define PHYS_OFFSET	((phys_addr_t)__pv_phys_pfn_offset << PAGE_SHIFT)
+#define PHYS_PFN_OFFSET	(__pv_phys_pfn_offset)
+
+#define virt_to_pfn(kaddr) \
+	((((unsigned long)(kaddr) - PAGE_OFFSET) >> PAGE_SHIFT) + \
+	 PHYS_PFN_OFFSET)
 
 #define __pv_stub(from,to,instr,type)			\
 	__asm__("@ __pv_stub\n"				\
@@ -246,6 +259,7 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 #else
 
 #define PHYS_OFFSET	PLAT_PHYS_OFFSET
+#define PHYS_PFN_OFFSET	((unsigned long)(PHYS_OFFSET >> PAGE_SHIFT))
 
 static inline phys_addr_t __virt_to_phys(unsigned long x)
 {
@@ -257,18 +271,11 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 	return x - PHYS_OFFSET + PAGE_OFFSET;
 }
 
-#endif
-#endif
+#define virt_to_pfn(kaddr) \
+	((((unsigned long)(kaddr) - PAGE_OFFSET) >> PAGE_SHIFT) + \
+	 PHYS_PFN_OFFSET)
 
-/*
- * PFNs are used to describe any physical page; this means
- * PFN 0 == physical address 0.
- *
- * This is the PFN of the first RAM page in the kernel
- * direct-mapped view.  We assume this is the first page
- * of RAM in the mem_map as well.
- */
-#define PHYS_PFN_OFFSET	((unsigned long)(PHYS_OFFSET >> PAGE_SHIFT))
+#endif
 
 /*
  * These are *only* valid on the kernel direct mapped RAM memory.
@@ -346,9 +353,9 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
  */
 #define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
 
-#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define virt_addr_valid(kaddr)	(((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory) \
-					&& pfn_valid(__pa(kaddr) >> PAGE_SHIFT) )
+					&& pfn_valid(virt_to_pfn(kaddr)))
 
 #endif
 

commit 006fa2599bf0daf107cbb7a8a99fcfb9a998a169
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Feb 26 19:40:46 2014 +0000

    ARM: fix noMMU kallsyms symbol filtering
    
    With noMMU, CONFIG_PAGE_OFFSET was not being set correctly.  As there's
    no MMU, PAGE_OFFSET should be equal to PHYS_OFFSET in all cases.  This
    commit makes that explicit.
    
    Since we do this, we don't need to mess around in asm/memory.h with
    ifdefs to sort this out, so let's get rid of that, and there's no point
    offering the "Memory split" option for noMMU as that's meaningless
    there.
    
    Fixes: b9b32bf70f2f ("ARM: use linker magic for vectors and vector stubs")
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 8756e4bcdba0..4afb376d9c7c 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -30,14 +30,15 @@
  */
 #define UL(x) _AC(x, UL)
 
+/* PAGE_OFFSET - the virtual address of the start of the kernel image */
+#define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
+
 #ifdef CONFIG_MMU
 
 /*
- * PAGE_OFFSET - the virtual address of the start of the kernel image
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area
  */
-#define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
 #define TASK_SIZE		(UL(CONFIG_PAGE_OFFSET) - UL(SZ_16M))
 #define TASK_UNMAPPED_BASE	ALIGN(TASK_SIZE / 3, SZ_16M)
 
@@ -104,10 +105,6 @@
 #define END_MEM     		(UL(CONFIG_DRAM_BASE) + CONFIG_DRAM_SIZE)
 #endif
 
-#ifndef PAGE_OFFSET
-#define PAGE_OFFSET		PLAT_PHYS_OFFSET
-#endif
-
 /*
  * The module can be at any place in ram in nommu mode.
  */

commit efea3403d4b7c6d1dd5d5ac3234c161e8b314d66
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Sat Dec 21 01:03:06 2013 +0100

    ARM: 7931/1: Correct virt_addr_valid
    
    The definition of virt_addr_valid is that virt_addr_valid should
    return true if and only if virt_to_page returns a valid pointer.
    The current definition of virt_addr_valid only checks against the
    virtual address range. There's no guarantee that just because a
    virtual address falls bewteen PAGE_OFFSET and high_memory the
    associated physical memory has a valid backing struct page. Follow
    the example of other architectures and convert to pfn_valid to
    verify that the virtual address is actually valid. The check for
    an address between PAGE_OFFSET and high_memory is still necessary
    as vmalloc/highmem addresses are not valid with virt_to_page.
    
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 6976b03e5213..8756e4bcdba0 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -347,7 +347,8 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 #define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define virt_addr_valid(kaddr)	((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory)
+#define virt_addr_valid(kaddr)	(((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory) \
+					&& pfn_valid(__pa(kaddr) >> PAGE_SHIFT) )
 
 #endif
 

commit b713aa0b15015a65ad5421543b80df86de043d62
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Dec 10 19:21:08 2013 +0000

    ARM: fix asm/memory.h build error
    
    Jason Gunthorpe reports a build failure when ARM_PATCH_PHYS_VIRT is
    not defined:
    
    In file included from arch/arm/include/asm/page.h:163:0,
                     from include/linux/mm_types.h:16,
                     from include/linux/sched.h:24,
                     from arch/arm/kernel/asm-offsets.c:13:
    arch/arm/include/asm/memory.h: In function '__virt_to_phys':
    arch/arm/include/asm/memory.h:244:40: error: 'PHYS_OFFSET' undeclared (first use in this function)
    arch/arm/include/asm/memory.h:244:40: note: each undeclared identifier is reported only once for each function it appears in
    arch/arm/include/asm/memory.h: In function '__phys_to_virt':
    arch/arm/include/asm/memory.h:249:13: error: 'PHYS_OFFSET' undeclared (first use in this function)
    
    Fixes: ca5a45c06cd4 ("ARM: mm: use phys_addr_t appropriately in p2v and v2p conversions")
    Tested-By: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 9ecccc865046..6976b03e5213 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -100,23 +100,19 @@
 #define TASK_UNMAPPED_BASE	UL(0x00000000)
 #endif
 
-#ifndef PHYS_OFFSET
-#define PHYS_OFFSET 		UL(CONFIG_DRAM_BASE)
-#endif
-
 #ifndef END_MEM
 #define END_MEM     		(UL(CONFIG_DRAM_BASE) + CONFIG_DRAM_SIZE)
 #endif
 
 #ifndef PAGE_OFFSET
-#define PAGE_OFFSET		(PHYS_OFFSET)
+#define PAGE_OFFSET		PLAT_PHYS_OFFSET
 #endif
 
 /*
  * The module can be at any place in ram in nommu mode.
  */
 #define MODULES_END		(END_MEM)
-#define MODULES_VADDR		(PHYS_OFFSET)
+#define MODULES_VADDR		PAGE_OFFSET
 
 #define XIP_VIRT_ADDR(physaddr)  (physaddr)
 
@@ -157,6 +153,16 @@
 #endif
 #define ARCH_PGD_MASK		((1 << ARCH_PGD_SHIFT) - 1)
 
+/*
+ * PLAT_PHYS_OFFSET is the offset (from zero) of the start of physical
+ * memory.  This is used for XIP and NoMMU kernels, or by kernels which
+ * have their own mach/memory.h.  Assembly code must always use
+ * PLAT_PHYS_OFFSET and not PHYS_OFFSET.
+ */
+#ifndef PLAT_PHYS_OFFSET
+#define PLAT_PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
+#endif
+
 #ifndef __ASSEMBLY__
 
 /*
@@ -239,6 +245,8 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 
 #else
 
+#define PHYS_OFFSET	PLAT_PHYS_OFFSET
+
 static inline phys_addr_t __virt_to_phys(unsigned long x)
 {
 	return (phys_addr_t)x - PAGE_OFFSET + PHYS_OFFSET;
@@ -251,17 +259,6 @@ static inline unsigned long __phys_to_virt(phys_addr_t x)
 
 #endif
 #endif
-#endif /* __ASSEMBLY__ */
-
-#ifndef PHYS_OFFSET
-#ifdef PLAT_PHYS_OFFSET
-#define PHYS_OFFSET	PLAT_PHYS_OFFSET
-#else
-#define PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
-#endif
-#endif
-
-#ifndef __ASSEMBLY__
 
 /*
  * PFNs are used to describe any physical page; this means

commit 139cc2ba7400dab80228a2bfa683e2f49cf5d3ff
Author: Victor Kamensky <victor.kamensky@linaro.org>
Date:   Thu Nov 7 08:42:41 2013 +0100

    ARM: 7882/1: mm: fix __phys_to_virt to work with 64 bit phys_addr_t in BE case
    
    Make sure that inline assembler that expects 'r' operand
    receives 32 bit value.
    
    Before this fix in case of CONFIG_ARCH_PHYS_ADDR_T_64BIT and
    CONFIG_ARM_PATCH_PHYS_VIRT __phys_to_virt function passed 64 bit
    value to __pv_stub inline assembler where 'r' operand is
    expected. Compiler behavior in such case is not well specified.
    It worked in little endian case, but in big endian case
    incorrect code was generated, where compiler confused which
    part of 64 bit value it needed to modify. For example BE
    snippet looked like this:
    
    N:0x80904E08 : MOV      r2,#0
    N:0x80904E0C : SUB      r2,r2,#0x81000000
    
    when LE similar code looked like this
    
    N:0x808FCE2C : MOV      r2,r0
    N:0x808FCE30 : SUB      r2,r2,#0xc0, 8 ; #0xc0000000
    
    Note 'r0' register is va that have to be translated into phys
    
    To avoid this situation use explicit cast to 'unsigned long',
    which explicitly discard upper part of phys address and convert
    value to 32 bit. Also add comment so such cast will not be
    removed in the future.
    
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 4dd21457ef9d..9ecccc865046 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -226,7 +226,14 @@ static inline phys_addr_t __virt_to_phys(unsigned long x)
 static inline unsigned long __phys_to_virt(phys_addr_t x)
 {
 	unsigned long t;
-	__pv_stub(x, t, "sub", __PV_BITS_31_24);
+
+	/*
+	 * 'unsigned long' cast discard upper word when
+	 * phys_addr_t is 64 bit, and makes sure that inline
+	 * assembler expression receives 32 bit argument
+	 * in place where 'r' 32 bit operand is expected.
+	 */
+	__pv_stub((unsigned long) x, t, "sub", __PV_BITS_31_24);
 	return t;
 }
 

commit 5e4432d3bd6b5b19e10bb263e7dbe8e74d7cf1c2
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Oct 29 23:06:44 2013 +0000

    ARM: fix misplaced arch_virt_to_idmap()
    
    Olof Johansson reported:
    
    In file included from arch/arm/include/asm/page.h:163:0,
                     from include/linux/mm_types.h:16,
                     from include/linux/sched.h:24,
                     from arch/arm/kernel/asm-offsets.c:13:
    arch/arm/include/asm/memory.h: In function '__virt_to_idmap':
    arch/arm/include/asm/memory.h:300:6: error: 'arch_virt_to_idmap' undeclared (first use in this function)
    
    caused by arch_virt_to_idmap being placed inside a different
    preprocessor conditional to its user.  Move it along side its user.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 6748d6295a1a..4dd21457ef9d 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -174,7 +174,6 @@
 #define __PV_BITS_31_24	0x81000000
 #define __PV_BITS_7_0	0x81
 
-extern phys_addr_t (*arch_virt_to_idmap) (unsigned long x);
 extern u64 __pv_phys_offset;
 extern u64 __pv_offset;
 extern void fixup_pv_table(const void *, unsigned long);
@@ -290,6 +289,8 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
+extern phys_addr_t (*arch_virt_to_idmap)(unsigned long x);
+
 /*
  * These are for systems that have a hardware interconnect supported alias of
  * physical memory for idmap purposes.  Most cases should leave these

commit f52bb722547f43caeaecbcc62db9f3c3b80ead9b
Author: Sricharan R <r.sricharan@ti.com>
Date:   Mon Jul 29 20:26:22 2013 +0530

    ARM: mm: Correct virt_to_phys patching for 64 bit physical addresses
    
    The current phys_to_virt patching mechanism works only for 32 bit
    physical addresses and this patch extends the idea for 64bit physical
    addresses.
    
    The 64bit v2p patching mechanism patches the higher 8 bits of physical
    address with a constant using 'mov' instruction and lower 32bits are patched
    using 'add'. While this is correct, in those platforms where the lowmem addressable
    physical memory spawns across 4GB boundary, a carry bit can be produced as a
    result of addition of lower 32bits. This has to be taken in to account and added
    in to the upper. The patched __pv_offset and va are added in lower 32bits, where
    __pv_offset can be in two's complement form when PA_START < VA_START and that can
    result in a false carry bit.
    
    e.g
        1) PA = 0x80000000; VA = 0xC0000000
           __pv_offset = PA - VA = 0xC0000000 (2's complement)
    
        2) PA = 0x2 80000000; VA = 0xC000000
           __pv_offset = PA - VA = 0x1 C0000000
    
    So adding __pv_offset + VA should never result in a true overflow for (1).
    So in order to differentiate between a true carry, a __pv_offset is extended
    to 64bit and the upper 32bits will have 0xffffffff if __pv_offset is
    2's complement. So 'mvn #0' is inserted instead of 'mov' while patching
    for the same reason. Since mov, add, sub instruction are to patched
    with different constants inside the same stub, the rotation field
    of the opcode is using to differentiate between them.
    
    So the above examples for v2p translation becomes for VA=0xC0000000,
        1) PA[63:32] = 0xffffffff
           PA[31:0] = VA + 0xC0000000 --> results in a carry
           PA[63:32] = PA[63:32] + carry
    
           PA[63:0] = 0x0 80000000
    
        2) PA[63:32] = 0x1
           PA[31:0] = VA + 0xC0000000 --> results in a carry
           PA[63:32] = PA[63:32] + carry
    
           PA[63:0] = 0x2 80000000
    
    The above ideas were suggested by Nicolas Pitre <nico@linaro.org> as
    part of the review of first and second versions of the subject patch.
    
    There is no corresponding change on the phys_to_virt() side, because
    computations on the upper 32-bits would be discarded anyway.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Sricharan R <r.sricharan@ti.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index d9b96c65e594..6748d6295a1a 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -172,9 +172,14 @@
  * so that all we need to do is modify the 8-bit constant field.
  */
 #define __PV_BITS_31_24	0x81000000
+#define __PV_BITS_7_0	0x81
 
 extern phys_addr_t (*arch_virt_to_idmap) (unsigned long x);
-extern unsigned long __pv_phys_offset;
+extern u64 __pv_phys_offset;
+extern u64 __pv_offset;
+extern void fixup_pv_table(const void *, unsigned long);
+extern const void *__pv_table_begin, *__pv_table_end;
+
 #define PHYS_OFFSET __pv_phys_offset
 
 #define __pv_stub(from,to,instr,type)			\
@@ -186,10 +191,36 @@ extern unsigned long __pv_phys_offset;
 	: "=r" (to)					\
 	: "r" (from), "I" (type))
 
+#define __pv_stub_mov_hi(t)				\
+	__asm__ volatile("@ __pv_stub_mov\n"		\
+	"1:	mov	%R0, %1\n"			\
+	"	.pushsection .pv_table,\"a\"\n"		\
+	"	.long	1b\n"				\
+	"	.popsection\n"				\
+	: "=r" (t)					\
+	: "I" (__PV_BITS_7_0))
+
+#define __pv_add_carry_stub(x, y)			\
+	__asm__ volatile("@ __pv_add_carry_stub\n"	\
+	"1:	adds	%Q0, %1, %2\n"			\
+	"	adc	%R0, %R0, #0\n"			\
+	"	.pushsection .pv_table,\"a\"\n"		\
+	"	.long	1b\n"				\
+	"	.popsection\n"				\
+	: "+r" (y)					\
+	: "r" (x), "I" (__PV_BITS_31_24)		\
+	: "cc")
+
 static inline phys_addr_t __virt_to_phys(unsigned long x)
 {
-	unsigned long t;
-	__pv_stub(x, t, "add", __PV_BITS_31_24);
+	phys_addr_t t;
+
+	if (sizeof(phys_addr_t) == 4) {
+		__pv_stub(x, t, "add", __PV_BITS_31_24);
+	} else {
+		__pv_stub_mov_hi(t);
+		__pv_add_carry_stub(x, t);
+	}
 	return t;
 }
 

commit 4dc9a81715973cb137a14399420bb35b0ed7d6ef
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Wed Jul 31 12:44:42 2013 -0400

    ARM: mm: Introduce virt_to_idmap() with an arch hook
    
    On some PAE systems (e.g. TI Keystone), memory is above the
    32-bit addressable limit, and the interconnect provides an
    aliased view of parts of physical memory in the 32-bit addressable
    space.  This alias is strictly for boot time usage, and is not
    otherwise usable because of coherency limitations. On such systems,
    the idmap mechanism needs to take this aliased mapping into account.
    
    This patch introduces virt_to_idmap() and a arch function pointer which
    can be populated by platform which needs it. Also populate necessary
    idmap spots with now available virt_to_idmap(). Avoided #ifdef approach
    to be compatible with multi-platform builds.
    
    Most architecture won't touch it and in that case virt_to_idmap()
    fall-back to existing virt_to_phys() macro.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index c133bd915f48..d9b96c65e594 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -173,6 +173,7 @@
  */
 #define __PV_BITS_31_24	0x81000000
 
+extern phys_addr_t (*arch_virt_to_idmap) (unsigned long x);
 extern unsigned long __pv_phys_offset;
 #define PHYS_OFFSET __pv_phys_offset
 
@@ -258,6 +259,21 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
+/*
+ * These are for systems that have a hardware interconnect supported alias of
+ * physical memory for idmap purposes.  Most cases should leave these
+ * untouched.
+ */
+static inline phys_addr_t __virt_to_idmap(unsigned long x)
+{
+	if (arch_virt_to_idmap)
+		return arch_virt_to_idmap(x);
+	else
+		return __virt_to_phys(x);
+}
+
+#define virt_to_idmap(x)	__virt_to_idmap((unsigned long)(x))
+
 /*
  * Virtual <-> DMA view memory address translations
  * Again, these are *only* valid on the kernel direct mapped RAM

commit ca5a45c06cd4764fb8510740f7fc550d9a0208d4
Author: Santosh Shilimkar <santosh.shilimkar@ti.com>
Date:   Wed Jul 31 12:44:41 2013 -0400

    ARM: mm: use phys_addr_t appropriately in p2v and v2p conversions
    
    Fix remainder types used when converting back and forth between
    physical and virtual addresses.
    
    Cc: Russell King <linux@arm.linux.org.uk>
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@ti.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index e750a938fd3c..c133bd915f48 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -185,22 +185,32 @@ extern unsigned long __pv_phys_offset;
 	: "=r" (to)					\
 	: "r" (from), "I" (type))
 
-static inline unsigned long __virt_to_phys(unsigned long x)
+static inline phys_addr_t __virt_to_phys(unsigned long x)
 {
 	unsigned long t;
 	__pv_stub(x, t, "add", __PV_BITS_31_24);
 	return t;
 }
 
-static inline unsigned long __phys_to_virt(unsigned long x)
+static inline unsigned long __phys_to_virt(phys_addr_t x)
 {
 	unsigned long t;
 	__pv_stub(x, t, "sub", __PV_BITS_31_24);
 	return t;
 }
+
 #else
-#define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
-#define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
+
+static inline phys_addr_t __virt_to_phys(unsigned long x)
+{
+	return (phys_addr_t)x - PAGE_OFFSET + PHYS_OFFSET;
+}
+
+static inline unsigned long __phys_to_virt(phys_addr_t x)
+{
+	return x - PHYS_OFFSET + PAGE_OFFSET;
+}
+
 #endif
 #endif
 #endif /* __ASSEMBLY__ */
@@ -238,14 +248,14 @@ static inline phys_addr_t virt_to_phys(const volatile void *x)
 
 static inline void *phys_to_virt(phys_addr_t x)
 {
-	return (void *)(__phys_to_virt((unsigned long)(x)));
+	return (void *)__phys_to_virt(x);
 }
 
 /*
  * Drivers should NOT use these either.
  */
 #define __pa(x)			__virt_to_phys((unsigned long)(x))
-#define __va(x)			((void *)__phys_to_virt((unsigned long)(x)))
+#define __va(x)			((void *)__phys_to_virt((phys_addr_t)(x)))
 #define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
 
 /*

commit 5b84de34642bad442942ace0a57c4dee00266657
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:04:40 2013 -0700

    mm/ARM: fix stale comment about VALID_PAGE()
    
    VALID_PAGE() has been removed from kernel long time ago,
    so fix the comment.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Giancarlo Asnaghi <giancarlo.asnaghi@st.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 584786f740f9..e750a938fd3c 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -276,12 +276,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 /*
  * Conversion between a struct page and a physical address.
  *
- * Note: when converting an unknown physical address to a
- * struct page, the resulting pointer must be validated
- * using VALID_PAGE().  It must return an invalid struct page
- * for any physical address not corresponding to a system
- * RAM address.
- *
  *  page_to_pfn(page)	convert a struct page * to a PFN number
  *  pfn_to_page(pfn)	convert a _valid_ PFN number to struct page *
  *

commit 5b20c5b2f014ecc0a6310988af69cd7ede9e7c67
Author: Cyril Chemparathy <cyril@ti.com>
Date:   Wed Sep 12 10:19:05 2012 -0400

    ARM: fix type of PHYS_PFN_OFFSET to unsigned long
    
    On LPAE machines, PHYS_OFFSET evaluates to a phys_addr_t and this type is
    inherited by the PHYS_PFN_OFFSET definition as well.  Consequently, the kernel
    build emits warnings of the form:
    
    init/main.c: In function 'start_kernel':
    init/main.c:588:7: warning: format '%lx' expects argument of type 'long unsigned int', but argument 2 has type 'phys_addr_t' [-Wformat]
    
    This patch fixes this warning by pinning down the PFN type to unsigned long.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index e506088a7678..584786f740f9 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -223,7 +223,7 @@ static inline unsigned long __phys_to_virt(unsigned long x)
  * direct-mapped view.  We assume this is the first page
  * of RAM in the mem_map as well.
  */
-#define PHYS_PFN_OFFSET	(PHYS_OFFSET >> PAGE_SHIFT)
+#define PHYS_PFN_OFFSET	((unsigned long)(PHYS_OFFSET >> PAGE_SHIFT))
 
 /*
  * These are *only* valid on the kernel direct mapped RAM memory.

commit 4756dcbfd37819a8359d3c69a22be2ee41666d0f
Author: Cyril Chemparathy <cyril@ti.com>
Date:   Sat Jul 21 15:55:04 2012 -0400

    ARM: LPAE: accomodate >32-bit addresses for page table base
    
    This patch redefines the early boot time use of the R4 register to steal a few
    low order bits (ARCH_PGD_SHIFT bits) on LPAE systems.  This allows for up to
    38-bit physical addresses.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 57870ab313c5..e506088a7678 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -18,6 +18,8 @@
 #include <linux/types.h>
 #include <linux/sizes.h>
 
+#include <asm/cache.h>
+
 #ifdef CONFIG_NEED_MACH_MEMORY_H
 #include <mach/memory.h>
 #endif
@@ -141,6 +143,20 @@
 #define page_to_phys(page)	(__pfn_to_phys(page_to_pfn(page)))
 #define phys_to_page(phys)	(pfn_to_page(__phys_to_pfn(phys)))
 
+/*
+ * Minimum guaranted alignment in pgd_alloc().  The page table pointers passed
+ * around in head.S and proc-*.S are shifted by this amount, in order to
+ * leave spare high bits for systems with physical address extension.  This
+ * does not fully accomodate the 40-bit addressing capability of ARM LPAE, but
+ * gives us about 38-bits or so.
+ */
+#ifdef CONFIG_ARM_LPAE
+#define ARCH_PGD_SHIFT		L1_CACHE_SHIFT
+#else
+#define ARCH_PGD_SHIFT		0
+#endif
+#define ARCH_PGD_MASK		((1 << ARCH_PGD_SHIFT) - 1)
+
 #ifndef __ASSEMBLY__
 
 /*

commit b24174b0cbbe383c5bb6097aeb24480b8fd2d338
Merge: 7ed214ac2095 be8fd292f9b1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 21 14:54:55 2013 -0800

    Merge tag 'fixes-non-critical' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc
    
    Pull non-critical ARM SoC bug fixes from Arnd Bergmann:
     "Bug fixes that did not make it into v3.8, mostly because they were not
      considered important enough, and in some cases because bugs only show
      up in combination with other patches destined for 3.9.  This includes
      a few larger patches for GPIO on the Marvell PXA platform and a lot of
      Samsung specific bug fixes, as well as a series from Arnd to fix older
      build warnings."
    
    * tag 'fixes-non-critical' of git://git.kernel.org/pub/scm/linux/kernel/git/arm/arm-soc: (54 commits)
      ARM: SPEAr13xx: Enable CONFIG_ARCH_HAS_CPUFREQ
      ARM: imx: MACH_MX31ADS_WM1133_EV1 needs REGULATOR_WM8350
      scripts/sortextable: silence script output
      ARM: s3c: i2c: add platform_device forward declaration
      ARM: mvebu: allow selecting mvebu without Armada XP
      ARM: pick Versatile by default for !MMU
      ARM: integrator: fix build with INTEGRATOR_AP off
      ARM: integrator/versatile: fix NOMMU warnings
      ARM: sa1100: don't warn about mach/ide.h
      ARM: shmobile: fix defconfig warning on CONFIG_USB
      ARM: w90x900: fix legacy assembly syntax
      ARM: samsung: fix assembly syntax for new gas
      ARM: disable virt_to_bus/virt_to_bus almost everywhere
      ARM: dts: Correct pin configuration of SD 4 for exynos4x12-pinctrl
      ARM: SAMSUNG: Silence empty switch warning in fimc-core.h
      ARM: SAMSUNG: Silence empty switch warning in sdhci.h
      ARM: msm: proc_comm_boot_wait should not be __init
      arm: vt8500: Update MAINTAINERS entry for arch-vt8500
      ARM: integrator: ensure ap_syscon_base is initialised when !CONFIG_MMU
      ARM: S5PV210: Fix early uart output in fifo mode
      ...

commit 1b1c7409b75a8b62906b78b84c8469002072b738
Merge: 573f8c8d1337 b28748fb5d21
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Feb 20 14:35:58 2013 +0000

    Merge branch 'misc' into for-linus
    
    Conflicts:
            arch/arm/include/asm/memory.h

commit 5d1c20bce5bc1a7a94bca1324451532c28c89e36
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Jan 31 19:19:30 2013 +0100

    ARM: 7637/1: memory: use SZ_ constants for defining the virtual memory layout
    
    Parts of the virtual memory layout (mainly the modules area) are
    described using open-coded immediate values.
    
    Use the SZ_ definitions from linux/sizes.h instead to make the code
    clearer.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 73cf03aa981e..924320f4f22a 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -36,23 +36,23 @@
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area
  */
 #define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
-#define TASK_SIZE		(UL(CONFIG_PAGE_OFFSET) - UL(0x01000000))
+#define TASK_SIZE		(UL(CONFIG_PAGE_OFFSET) - UL(SZ_16M))
 #define TASK_UNMAPPED_BASE	(UL(CONFIG_PAGE_OFFSET) / 3)
 
 /*
  * The maximum size of a 26-bit user space task.
  */
-#define TASK_SIZE_26		UL(0x04000000)
+#define TASK_SIZE_26		(UL(1) << 26)
 
 /*
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 32MB of the kernel text.
  */
 #ifndef CONFIG_THUMB2_KERNEL
-#define MODULES_VADDR		(PAGE_OFFSET - 16*1024*1024)
+#define MODULES_VADDR		(PAGE_OFFSET - SZ_16M)
 #else
 /* smaller range for Thumb-2 symbols relocation (2^24)*/
-#define MODULES_VADDR		(PAGE_OFFSET - 8*1024*1024)
+#define MODULES_VADDR		(PAGE_OFFSET - SZ_8M)
 #endif
 
 #if TASK_SIZE > MODULES_VADDR

commit a5d533ee07690b9f904ca7b3732eed3d1134d4bc
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Nov 12 22:16:12 2012 +0000

    ARM: disable virt_to_bus/virt_to_bus almost everywhere
    
    We are getting a number of warnings about the use of the deprecated
    bus_to_virt function in drivers using the ARM ISA DMA API:
    
    drivers/parport/parport_pc.c: In function 'parport_pc_fifo_write_block_dma':
    drivers/parport/parport_pc.c:622:3: warning: 'bus_to_virt' is deprecated
    (declared at arch/arm/include/asm/memory.h:253) [-Wdeprecated-declarations]
    
    This is only because that function gets used by the inline
    set_dma_addr() helper. We know that any driver for the ISA DMA API
    is correctly using the DMA addresses, so we can change this
    to use the __bus_to_virt() function instead, which does not warn.
    
    After this, there are no remaining drivers that are used on
    any defconfigs on ARM using virt_to_bus or bus_to_virt, with
    the exception of the OSS sound driver. That driver is only used
    on RiscPC, NetWinder and Shark, so we can set ARCH_NO_VIRT_TO_BUS
    on all other platforms and hide the deprecated functions, which
    is far more effective than marking them as deprecated, in order
    to avoid any new users of that code.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Russell King <linux@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 73cf03aa981e..b11105c8599a 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -245,6 +245,7 @@ static inline void *phys_to_virt(phys_addr_t x)
 #define __bus_to_pfn(x)	__phys_to_pfn(x)
 #endif
 
+#ifdef CONFIG_VIRT_TO_BUS
 static inline __deprecated unsigned long virt_to_bus(void *x)
 {
 	return __virt_to_bus((unsigned long)x);
@@ -254,6 +255,7 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 {
 	return (void *)__bus_to_virt(x);
 }
+#endif
 
 /*
  * Conversion between a struct page and a physical address.

commit 79d1f5c9acf9fc8d06e5537083b19114ce87159f
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Feb 8 12:52:29 2013 +0100

    ARM: 7641/1: memory: fix broken mmap by ensuring TASK_UNMAPPED_BASE is aligned
    
    We have received multiple reports of mmap failures when running with a
    2:2 vm split. These manifest as either -EINVAL with a non page-aligned
    address (ending 0xaaa) or a SEGV, depending on the application. The
    issue is commonly observed in children of make, which appears to use
    bottom-up mmap (assumedly because it changes the stack rlimit).
    
    Further investigation reveals that this regression was triggered by
    394ef6403abc ("mm: use vm_unmapped_area() on arm architecture"), whereby
    TASK_UNMAPPED_BASE is no longer page-aligned for bottom-up mmap, causing
    get_unmapped_area to choke on misaligned addressed.
    
    This patch fixes the problem by defining TASK_UNMAPPED_BASE in terms of
    TASK_SIZE and explicitly aligns the result to 16M, matching the other
    end of the heap.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Reported-by: Steve Capper <steve.capper@arm.com>
    Reported-by: Jean-Francois Moine <moinejf@free.fr>
    Reported-by: Christoffer Dall <cdall@cs.columbia.edu>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 73cf03aa981e..1c4df27f9332 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -37,7 +37,7 @@
  */
 #define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
 #define TASK_SIZE		(UL(CONFIG_PAGE_OFFSET) - UL(0x01000000))
-#define TASK_UNMAPPED_BASE	(UL(CONFIG_PAGE_OFFSET) / 3)
+#define TASK_UNMAPPED_BASE	ALIGN(TASK_SIZE / 3, SZ_16M)
 
 /*
  * The maximum size of a 26-bit user space task.

commit 48aa820f1e3824e46dde6251db98e5961abf605d
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Tue Aug 21 12:26:24 2012 +0200

    ARM: kill off arch_is_coherent
    
    With ixp2xxx removed, there are no platforms that define arch_is_coherent,
    so the last occurrences of arch_is_coherent can be removed. Any new
    platform with coherent i/o should use coherent dma mapping functions.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 5f6ddcc56452..73cf03aa981e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -275,14 +275,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory)
 
-/*
- * Optional coherency support.  Currently used only by selected
- * Intel XSC3-based systems.
- */
-#ifndef arch_is_coherent
-#define arch_is_coherent()		0
-#endif
-
 #endif
 
 #include <asm-generic/memory_model.h>

commit b4ad51559c2f12c34a0340b26ffb02e4b285bc51
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Sep 4 20:04:35 2012 +0100

    ARM: 7512/1: Fix XIP build due to PHYS_OFFSET definition moving
    
    During the p2v changes, the PHYS_OFFSET #define moved into a
    !__ASSEMBLY__ section. This causes a XIP build to fail with
    
     arch/arm/kernel/head.o: In function 'stext':
     arch/arm/kernel/head.S:146: undefined reference to 'PHYS_OFFSET'
    
    Momentarily leave the #ifndef __ASSEMBLY__ section so we can
    define PHYS_OFFSET for all compilation units.
    
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index e965f1b560f1..5f6ddcc56452 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -187,6 +187,7 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 #define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
 #endif
 #endif
+#endif /* __ASSEMBLY__ */
 
 #ifndef PHYS_OFFSET
 #ifdef PLAT_PHYS_OFFSET
@@ -196,6 +197,8 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 #endif
 #endif
 
+#ifndef __ASSEMBLY__
+
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.

commit 158e8bfe802f730f9ea7cde32eee8b43285bdd4a
Author: Alessandro Rubini <rubini@gnudd.com>
Date:   Sun Jun 24 12:46:26 2012 +0100

    ARM: 7432/1: use the new linux/sizes.h
    
    Signed-off-by: Alessandro Rubini <rubini@gnudd.com>
    Acked-by: Giancarlo Asnaghi <giancarlo.asnaghi@st.com>
    Acked-by: Linus Walleij <linus.walleij@linaro.org>
    Cc: Alan Cox <alan@linux.intel.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index fcb575747e5e..e965f1b560f1 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -16,7 +16,7 @@
 #include <linux/compiler.h>
 #include <linux/const.h>
 #include <linux/types.h>
-#include <asm/sizes.h>
+#include <linux/sizes.h>
 
 #ifdef CONFIG_NEED_MACH_MEMORY_H
 #include <mach/memory.h>

commit 38b4205abeaed7eaae0a2a9e9af8e1252fa5c86a
Author: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
Date:   Wed Mar 14 10:30:52 2012 +0100

    ARM: 7361/1: provide XIP_VIRT_ADDR for no-MMU builds
    
    XIP_VIRT_ADDR is needed for XIP builds and currently only defined for
    builds with CONFIG_MMU.
    
    Also provide it for no-MMU builds to make it possible to build an XIP
    kernel for MMU-less machines. As these lack an MMU it has to be an
    identity mapping.
    
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Uwe Kleine-König <u.kleine-koenig@pengutronix.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index a8997d71084e..fcb575747e5e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -116,6 +116,8 @@
 #define MODULES_END		(END_MEM)
 #define MODULES_VADDR		(PHYS_OFFSET)
 
+#define XIP_VIRT_ADDR(physaddr)  (physaddr)
+
 #endif /* !CONFIG_MMU */
 
 /*

commit 0cdc8b921d68817b687755b4f6ae20cd8ff1d026
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Sep 2 22:26:55 2011 -0400

    ARM: switch from NO_MACH_MEMORY_H to NEED_MACH_MEMORY_H
    
    Given that we want the default to not have any <mach/memory.h> and given
    that there are now fewer cases where it is still provided than the cases
    where it is not at this point, this makes sense to invert the logic and
    just identify the exception cases.
    
    The word "need" instead of "have" was chosen to construct the config
    symbol so not to suggest that having a mach/memory.h file is actually
    a feature that one should aim for.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 046c915694cd..a8997d71084e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -18,7 +18,7 @@
 #include <linux/types.h>
 #include <asm/sizes.h>
 
-#ifndef CONFIG_NO_MACH_MEMORY_H
+#ifdef CONFIG_NEED_MACH_MEMORY_H
 #include <mach/memory.h>
 #endif
 

commit 1b9f95f8ade9efc2bd49f0e7b9dc61a038ac3eef
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Jul 5 22:52:51 2011 -0400

    ARM: prepare for removal of a bunch of <mach/memory.h> files
    
    When the CONFIG_NO_MACH_MEMORY_H symbol is selected by a particular
    machine class, the machine specific memory.h include file is no longer
    used and can be removed.  In that case the equivalent information can
    be obtained dynamically at runtime by enabling CONFIG_ARM_PATCH_PHYS_VIRT
    or by specifying the physical memory address at kernel configuration time.
    
    If/when all instances of mach/memory.h are removed then this symbol could
    be removed.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 90bca427e367..046c915694cd 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -16,9 +16,12 @@
 #include <linux/compiler.h>
 #include <linux/const.h>
 #include <linux/types.h>
-#include <mach/memory.h>
 #include <asm/sizes.h>
 
+#ifndef CONFIG_NO_MACH_MEMORY_H
+#include <mach/memory.h>
+#endif
+
 /*
  * Allow for constants defined here to be used from assembly code
  * by prepending the UL suffix only with actual C code compilation.
@@ -184,7 +187,11 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 #endif
 
 #ifndef PHYS_OFFSET
+#ifdef PLAT_PHYS_OFFSET
 #define PHYS_OFFSET	PLAT_PHYS_OFFSET
+#else
+#define PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
+#endif
 #endif
 
 /*

commit 96f90c791512bf8ceb50572a0e65d4cabb665c60
Merge: ad30a2bbdc20 daece59689e7
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon Aug 29 15:29:00 2011 -0400

    Merge the enabling by default of ARM_PATCH_PHYS_VIRT
    
    Conflicts:
            arch/arm/mach-msm/board-msm7x30.c

commit 99d1717dd7fecf2b10195b0d864323b952b4eba0
Author: Jon Medhurst <tixy@yxit.co.uk>
Date:   Tue Aug 2 17:28:27 2011 +0100

    ARM: Add init_consistent_dma_size()
    
    This function can be called during boot to increase the size of the consistent
    DMA region above it's default value of 2MB. It must be called before the memory
    allocator is initialised, i.e. before any core_initcall.
    
    Signed-off-by: Jon Medhurst <tixy@yxit.co.uk>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index b8de516e600e..652fccca4952 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -77,16 +77,7 @@
  */
 #define IOREMAP_MAX_ORDER	24
 
-/*
- * Size of DMA-consistent memory region.  Must be multiple of 2M,
- * between 2MB and 14MB inclusive.
- */
-#ifndef CONSISTENT_DMA_SIZE
-#define CONSISTENT_DMA_SIZE 	SZ_2M
-#endif
-
 #define CONSISTENT_END		(0xffe00000UL)
-#define CONSISTENT_BASE		(CONSISTENT_END - CONSISTENT_DMA_SIZE)
 
 #else /* CONFIG_MMU */
 

commit daece59689e76ed55d8863cae04993679a8e844e
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Aug 12 00:14:29 2011 +0100

    ARM: 7013/1: P2V: Remove ARM_PATCH_PHYS_VIRT_16BIT
    
    This code can be removed now that MSM targets no longer need the 16-bit
    offsets for P2V.
    
    Signed-off-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index b8de516e600e..441fc4fe8263 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -160,7 +160,6 @@
  * so that all we need to do is modify the 8-bit constant field.
  */
 #define __PV_BITS_31_24	0x81000000
-#define __PV_BITS_23_16	0x00810000
 
 extern unsigned long __pv_phys_offset;
 #define PHYS_OFFSET __pv_phys_offset
@@ -178,9 +177,6 @@ static inline unsigned long __virt_to_phys(unsigned long x)
 {
 	unsigned long t;
 	__pv_stub(x, t, "add", __PV_BITS_31_24);
-#ifdef CONFIG_ARM_PATCH_PHYS_VIRT_16BIT
-	__pv_stub(t, t, "add", __PV_BITS_23_16);
-#endif
 	return t;
 }
 
@@ -188,9 +184,6 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 {
 	unsigned long t;
 	__pv_stub(x, t, "sub", __PV_BITS_31_24);
-#ifdef CONFIG_ARM_PATCH_PHYS_VIRT_16BIT
-	__pv_stub(t, t, "sub", __PV_BITS_23_16);
-#endif
 	return t;
 }
 #else

commit 022ae537b23cb14a391565e9ad9e9945f4b17138
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Jul 8 21:26:59 2011 +0100

    ARM: dma: replace ISA_DMA_THRESHOLD with a variable
    
    ISA_DMA_THRESHOLD has been unused by non-arch code, so lets now get
    rid of it from ARM by replacing it with arm_dma_zone_mask.  Move
    dma_supported() and dma_set_mask() out of line, and have
    dma_supported() check this new variable instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index af44a8fb3480..b8de516e600e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -203,18 +203,6 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 #define PHYS_OFFSET	PLAT_PHYS_OFFSET
 #endif
 
-/*
- * The DMA mask corresponding to the maximum bus address allocatable
- * using GFP_DMA.  The default here places no restriction on DMA
- * allocations.  This must be the smallest DMA mask in the system,
- * so a successful GFP_DMA allocation will always satisfy this.
- */
-#ifndef ARM_DMA_ZONE_SIZE
-#define ISA_DMA_THRESHOLD	(0xffffffffULL)
-#else
-#define ISA_DMA_THRESHOLD	(PHYS_OFFSET + ARM_DMA_ZONE_SIZE - 1)
-#endif
-
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.

commit be20902ba67de70b38c995903321f4152dee57b7
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed May 11 15:39:00 2011 +0100

    ARM: use ARM_DMA_ZONE_SIZE to adjust the zone sizes
    
    Rather than each platform providing its own function to adjust the
    zone sizes, use the new ARM_DMA_ZONE_SIZE definition to perform this
    adjustment.  This ensures that the actual DMA zone size and the
    ISA_DMA_THRESHOLD/MAX_DMA_ADDRESS definitions are consistent with
    each other, and moves this complexity out of the platform code.
    
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index ee5ff41027a3..af44a8fb3480 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -215,12 +215,6 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 #define ISA_DMA_THRESHOLD	(PHYS_OFFSET + ARM_DMA_ZONE_SIZE - 1)
 #endif
 
-#ifndef arch_adjust_zones
-#define arch_adjust_zones(size,holes) do { } while (0)
-#elif !defined(CONFIG_ZONE_DMA)
-#error "custom arch_adjust_zones() requires CONFIG_ZONE_DMA"
-#endif
-
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.

commit 2fb3ec5c9503ba8874e24170de2b40e8f1a58370
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed May 11 16:06:29 2011 +0100

    ARM: Replace platform definition of ISA_DMA_THRESHOLD/MAX_DMA_ADDRESS
    
    The values of ISA_DMA_THRESHOLD and MAX_DMA_ADDRESS are related; one is
    the physical/bus address, the other is the virtual address.  Both need
    to be kept in step, so rather than having platforms define both, allow
    them to define a single macro which sets both of these macros
    appropraitely.
    
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 431077c5a867..ee5ff41027a3 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -209,8 +209,10 @@ static inline unsigned long __phys_to_virt(unsigned long x)
  * allocations.  This must be the smallest DMA mask in the system,
  * so a successful GFP_DMA allocation will always satisfy this.
  */
-#ifndef ISA_DMA_THRESHOLD
+#ifndef ARM_DMA_ZONE_SIZE
 #define ISA_DMA_THRESHOLD	(0xffffffffULL)
+#else
+#define ISA_DMA_THRESHOLD	(PHYS_OFFSET + ARM_DMA_ZONE_SIZE - 1)
 #endif
 
 #ifndef arch_adjust_zones

commit 3a6b1676c6f27f7fad1a3d6fab5a95f90b1e7402
Author: Will Deacon <will.deacon@arm.com>
Date:   Tue Feb 15 17:28:28 2011 +0100

    ARM: 6675/1: use phys_addr_t instead of unsigned long in conversion code
    
    The unsigned long datatype is not sufficient for mapping physical addresses
    >= 4GB.
    
    This patch ensures that the address conversion code in asm/memory.h casts
    to the correct type when handling physical addresses. The internal v2p
    macros only deal with lowmem addresses, so these do not need to be modified.
    
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 2398b3fc0268..431077c5a867 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -15,6 +15,7 @@
 
 #include <linux/compiler.h>
 #include <linux/const.h>
+#include <linux/types.h>
 #include <mach/memory.h>
 #include <asm/sizes.h>
 
@@ -135,8 +136,8 @@
 /*
  * Convert a physical address to a Page Frame Number and back
  */
-#define	__phys_to_pfn(paddr)	((paddr) >> PAGE_SHIFT)
-#define	__pfn_to_phys(pfn)	((pfn) << PAGE_SHIFT)
+#define	__phys_to_pfn(paddr)	((unsigned long)((paddr) >> PAGE_SHIFT))
+#define	__pfn_to_phys(pfn)	((phys_addr_t)(pfn) << PAGE_SHIFT)
 
 /*
  * Convert a page to/from a physical address
@@ -234,12 +235,12 @@ static inline unsigned long __phys_to_virt(unsigned long x)
  * translation for translating DMA addresses.  Use the driver
  * DMA support - see dma-mapping.h.
  */
-static inline unsigned long virt_to_phys(const volatile void *x)
+static inline phys_addr_t virt_to_phys(const volatile void *x)
 {
 	return __virt_to_phys((unsigned long)(x));
 }
 
-static inline void *phys_to_virt(unsigned long x)
+static inline void *phys_to_virt(phys_addr_t x)
 {
 	return (void *)(__phys_to_virt((unsigned long)(x)));
 }

commit cada3c0841e1deaec4c0f92654610b028dc683ff
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jan 4 19:39:29 2011 +0000

    ARM: P2V: extend to 16-bit translation offsets
    
    MSM's memory is aligned to 2MB, which is more than we can do with our
    existing method as we're limited to the upper 8 bits.  Extend this by
    using two instructions to 16 bits, automatically selected when MSM is
    enabled.
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Reviewed-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 7197879e1cb7..2398b3fc0268 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -154,29 +154,42 @@
 #ifndef __virt_to_phys
 #ifdef CONFIG_ARM_PATCH_PHYS_VIRT
 
+/*
+ * Constants used to force the right instruction encodings and shifts
+ * so that all we need to do is modify the 8-bit constant field.
+ */
+#define __PV_BITS_31_24	0x81000000
+#define __PV_BITS_23_16	0x00810000
+
 extern unsigned long __pv_phys_offset;
 #define PHYS_OFFSET __pv_phys_offset
 
-#define __pv_stub(from,to,instr)			\
+#define __pv_stub(from,to,instr,type)			\
 	__asm__("@ __pv_stub\n"				\
 	"1:	" instr "	%0, %1, %2\n"		\
 	"	.pushsection .pv_table,\"a\"\n"		\
 	"	.long	1b\n"				\
 	"	.popsection\n"				\
 	: "=r" (to)					\
-	: "r" (from), "I" (0x81000000))
+	: "r" (from), "I" (type))
 
 static inline unsigned long __virt_to_phys(unsigned long x)
 {
 	unsigned long t;
-	__pv_stub(x, t, "add");
+	__pv_stub(x, t, "add", __PV_BITS_31_24);
+#ifdef CONFIG_ARM_PATCH_PHYS_VIRT_16BIT
+	__pv_stub(t, t, "add", __PV_BITS_23_16);
+#endif
 	return t;
 }
 
 static inline unsigned long __phys_to_virt(unsigned long x)
 {
 	unsigned long t;
-	__pv_stub(x, t, "sub");
+	__pv_stub(x, t, "sub", __PV_BITS_31_24);
+#ifdef CONFIG_ARM_PATCH_PHYS_VIRT_16BIT
+	__pv_stub(t, t, "sub", __PV_BITS_23_16);
+#endif
 	return t;
 }
 #else

commit dc21af99fadcfa0ae65b52fd0895f85824f0c288
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jan 4 19:09:43 2011 +0000

    ARM: P2V: introduce phys_to_virt/virt_to_phys runtime patching
    
    This idea came from Nicolas, Eric Miao produced an initial version,
    which was then rewritten into this.
    
    Patch the physical to virtual translations at runtime.  As we modify
    the code, this makes it incompatible with XIP kernels, but allows us
    to achieve this with minimal loss of performance.
    
    As many translations are of the form:
    
            physical = virtual + (PHYS_OFFSET - PAGE_OFFSET)
            virtual = physical - (PHYS_OFFSET - PAGE_OFFSET)
    
    we generate an 'add' instruction for __virt_to_phys(), and a 'sub'
    instruction for __phys_to_virt().  We calculate at run time (PHYS_OFFSET
    - PAGE_OFFSET) by comparing the address prior to MMU initialization with
    where it should be once the MMU has been initialized, and place this
    constant into the above add/sub instructions.
    
    Once we have (PHYS_OFFSET - PAGE_OFFSET), we can calculate the real
    PHYS_OFFSET as PAGE_OFFSET is a build-time constant, and save this for
    the C-mode PHYS_OFFSET variable definition to use.
    
    At present, we are unable to support Realview with Sparsemem enabled
    as this uses a complex mapping function, and MSM as this requires a
    constant which will not fit in our math instruction.
    
    Add a module version magic string for this feature to prevent
    incompatible modules being loaded.
    
    Tested-by: Tony Lindgren <tony@atomide.com>
    Reviewed-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Tested-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 2efec578a62e..7197879e1cb7 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -24,8 +24,6 @@
  */
 #define UL(x) _AC(x, UL)
 
-#define PHYS_OFFSET	PLAT_PHYS_OFFSET
-
 #ifdef CONFIG_MMU
 
 /*
@@ -134,16 +132,6 @@
 #define DTCM_OFFSET	UL(0xfffe8000)
 #endif
 
-/*
- * Physical vs virtual RAM address space conversion.  These are
- * private definitions which should NOT be used outside memory.h
- * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
- */
-#ifndef __virt_to_phys
-#define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
-#define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
-#endif
-
 /*
  * Convert a physical address to a Page Frame Number and back
  */
@@ -158,6 +146,49 @@
 
 #ifndef __ASSEMBLY__
 
+/*
+ * Physical vs virtual RAM address space conversion.  These are
+ * private definitions which should NOT be used outside memory.h
+ * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
+ */
+#ifndef __virt_to_phys
+#ifdef CONFIG_ARM_PATCH_PHYS_VIRT
+
+extern unsigned long __pv_phys_offset;
+#define PHYS_OFFSET __pv_phys_offset
+
+#define __pv_stub(from,to,instr)			\
+	__asm__("@ __pv_stub\n"				\
+	"1:	" instr "	%0, %1, %2\n"		\
+	"	.pushsection .pv_table,\"a\"\n"		\
+	"	.long	1b\n"				\
+	"	.popsection\n"				\
+	: "=r" (to)					\
+	: "r" (from), "I" (0x81000000))
+
+static inline unsigned long __virt_to_phys(unsigned long x)
+{
+	unsigned long t;
+	__pv_stub(x, t, "add");
+	return t;
+}
+
+static inline unsigned long __phys_to_virt(unsigned long x)
+{
+	unsigned long t;
+	__pv_stub(x, t, "sub");
+	return t;
+}
+#else
+#define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
+#define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
+#endif
+#endif
+
+#ifndef PHYS_OFFSET
+#define PHYS_OFFSET	PLAT_PHYS_OFFSET
+#endif
+
 /*
  * The DMA mask corresponding to the maximum bus address allocatable
  * using GFP_DMA.  The default here places no restriction on DMA

commit f4117ac9e237b74afdf5e001d5ea26a4d15e9847
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Jan 4 18:07:14 2011 +0000

    ARM: P2V: separate PHYS_OFFSET from platform definitions
    
    This uncouple PHYS_OFFSET from the platform definitions, thereby
    facilitating run-time computation of the physical memory offset.
    
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Acked-by: Viresh Kumar <viresh.kumar@st.com>
    Acked-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Acked-by: Magnus Damm <damm@opensource.se>
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Jean-Christophe PLAGNIOL-VILLARD <plagnioj@jcrosoft.com>
    Acked-by: Wan ZongShun <mcuos.com@gmail.com>
    Acked-by: Kukjin Kim <kgene.kim@samsung.com>
    Acked-by: Eric Miao <eric.y.miao@gmail.com>
    Acked-by: Jiandong Zheng <jdzheng@broadcom.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index d0ee74b7cf86..2efec578a62e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -24,6 +24,8 @@
  */
 #define UL(x) _AC(x, UL)
 
+#define PHYS_OFFSET	PLAT_PHYS_OFFSET
+
 #ifdef CONFIG_MMU
 
 /*

commit 05b112ff98070dc1f3293e8cd8e4c6f468d1084a
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jan 25 11:18:25 2011 +0100

    ARM: 6637/1: Make the argument to virt_to_phys() "const volatile"
    
    Changing the virt_to_phys() argument to "const volatile void *" avoids
    compiler warnings in some situations where this function is used.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: Stephen Boyd <sboyd@codeaurora.org>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 23c2e8e5c0fa..d0ee74b7cf86 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -188,7 +188,7 @@
  * translation for translating DMA addresses.  Use the driver
  * DMA support - see dma-mapping.h.
  */
-static inline unsigned long virt_to_phys(void *x)
+static inline unsigned long virt_to_phys(const volatile void *x)
 {
 	return __virt_to_phys((unsigned long)(x));
 }

commit ceb0885d3b01bb2e2f18765770e212914f2864be
Merge: b31fc7af78e1 08458ef6eede
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Jul 31 14:20:02 2010 +0100

    Merge branch 'misc' into devel
    
    Conflicts:
            arch/arm/mm/init.c

commit 1dbd30e9890fd69e50b17edd70ca583546b0fe4e
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Mon Jul 12 21:53:28 2010 +0100

    ARM: 6225/1: make TCM allocation static and common for all archs
    
    This changes the TCM handling so that a fixed area is reserved at
    0xfffe0000-0xfffeffff for TCM. This areas is used by XScale but
    XScale does not have TCM so the mechanisms are mutually exclusive.
    
    This change is needed to make TCM detection more dynamic while
    still being able to compile code into it, and is a must for the
    unified ARM goals: the current TCM allocation at different places
    in memory for each machine would be a nightmare if you want to
    compile a single image for more than one machine with TCM so it
    has to be nailed down in one place.
    
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 4312ee5e3d0b..ab08d977ad49 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -123,6 +123,15 @@
 
 #endif /* !CONFIG_MMU */
 
+/*
+ * We fix the TCM memories max 32 KiB ITCM resp DTCM at these
+ * locations
+ */
+#ifdef CONFIG_HAVE_TCM
+#define ITCM_OFFSET	UL(0xfffe0000)
+#define DTCM_OFFSET	UL(0xfffe8000)
+#endif
+
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit b65b4781fbd5846a82cdac0c32818af1a7452d1f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat May 22 20:58:51 2010 +0100

    ARM: Remove 'node' argument form arch_adjust_zones()
    
    Since we no longer support discontigmem, node is always zero, so
    remove this argument.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index e263ec7c5dcb..82df0ae71bb4 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -158,7 +158,7 @@
 #endif
 
 #ifndef arch_adjust_zones
-#define arch_adjust_zones(node,size,holes) do { } while (0)
+#define arch_adjust_zones(size,holes) do { } while (0)
 #elif !defined(CONFIG_ZONE_DMA)
 #error "custom arch_adjust_zones() requires CONFIG_ZONE_DMA"
 #endif

commit be370302742ff9948f2a42b15cb2ba174d97b930
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri May 7 17:40:33 2010 +0100

    ARM: Remove DISCONTIGMEM support
    
    Everything should now be using sparsemem rather than discontigmem, so
    remove the code supporting discontigmem from ARM.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 4312ee5e3d0b..e263ec7c5dcb 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -234,76 +234,11 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
  *  virt_addr_valid(k)	indicates whether a virtual address is valid
  */
-#ifndef CONFIG_DISCONTIGMEM
-
 #define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory)
 
-#define PHYS_TO_NID(addr)	(0)
-
-#else /* CONFIG_DISCONTIGMEM */
-
-/*
- * This is more complex.  We have a set of mem_map arrays spread
- * around in memory.
- */
-#include <linux/numa.h>
-
-#define arch_pfn_to_nid(pfn)	PFN_TO_NID(pfn)
-#define arch_local_page_offset(pfn, nid) LOCAL_MAP_NR((pfn) << PAGE_SHIFT)
-
-#define virt_to_page(kaddr)					\
-	(ADDR_TO_MAPBASE(kaddr) + LOCAL_MAP_NR(kaddr))
-
-#define virt_addr_valid(kaddr)	(KVADDR_TO_NID(kaddr) < MAX_NUMNODES)
-
-/*
- * Common discontigmem stuff.
- *  PHYS_TO_NID is used by the ARM kernel/setup.c
- */
-#define PHYS_TO_NID(addr)	PFN_TO_NID((addr) >> PAGE_SHIFT)
-
-/*
- * Given a kaddr, ADDR_TO_MAPBASE finds the owning node of the memory
- * and returns the mem_map of that node.
- */
-#define ADDR_TO_MAPBASE(kaddr)	NODE_MEM_MAP(KVADDR_TO_NID(kaddr))
-
-/*
- * Given a page frame number, find the owning node of the memory
- * and returns the mem_map of that node.
- */
-#define PFN_TO_MAPBASE(pfn)	NODE_MEM_MAP(PFN_TO_NID(pfn))
-
-#ifdef NODE_MEM_SIZE_BITS
-#define NODE_MEM_SIZE_MASK	((1 << NODE_MEM_SIZE_BITS) - 1)
-
-/*
- * Given a kernel address, find the home node of the underlying memory.
- */
-#define KVADDR_TO_NID(addr) \
-	(((unsigned long)(addr) - PAGE_OFFSET) >> NODE_MEM_SIZE_BITS)
-
-/*
- * Given a page frame number, convert it to a node id.
- */
-#define PFN_TO_NID(pfn) \
-	(((pfn) - PHYS_PFN_OFFSET) >> (NODE_MEM_SIZE_BITS - PAGE_SHIFT))
-
-/*
- * Given a kaddr, LOCAL_MEM_MAP finds the owning node of the memory
- * and returns the index corresponding to the appropriate page in the
- * node's mem_map.
- */
-#define LOCAL_MAP_NR(addr) \
-	(((unsigned long)(addr) & NODE_MEM_SIZE_MASK) >> PAGE_SHIFT)
-
-#endif /* NODE_MEM_SIZE_BITS */
-
-#endif /* !CONFIG_DISCONTIGMEM */
-
 /*
  * Optional coherency support.  Currently used only by selected
  * Intel XSC3-based systems.

commit c931b4f655a1b86c929384e674eb8c31795f3bd7
Author: Fenkart/Bostandzhyan <andreas.fenkart@streamunlimited.com>
Date:   Sun Feb 7 21:47:17 2010 +0100

    ARM: 5928/1: Change type of VMALLOC_END to unsigned long.
    
    Makes it consistent with VMALLOC_START
    
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Andreas Fenkart <andreas.fenkart@streamunlimited.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index f5e693b8bab3..4312ee5e3d0b 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -104,11 +104,11 @@
 #endif
 
 #ifndef PHYS_OFFSET
-#define PHYS_OFFSET 		(CONFIG_DRAM_BASE)
+#define PHYS_OFFSET 		UL(CONFIG_DRAM_BASE)
 #endif
 
 #ifndef END_MEM
-#define END_MEM     		(CONFIG_DRAM_BASE + CONFIG_DRAM_SIZE)
+#define END_MEM     		(UL(CONFIG_DRAM_BASE) + CONFIG_DRAM_SIZE)
 #endif
 
 #ifndef PAGE_OFFSET

commit a7bd08c82e4f74387a39eeebb942712f23967420
Author: Fenkart/Bostandzhyan <andreas.fenkart@streamunlimited.com>
Date:   Sun Feb 7 21:46:33 2010 +0100

    ARM: 5927/1: Make delimiters of DMA area globally visibly.
    
    Adds DMA area to 'virtual memory map' startup message
    
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Andreas Fenkart <andreas.fenkart@streamunlimited.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 5421d82a2572..f5e693b8bab3 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -76,6 +76,17 @@
  */
 #define IOREMAP_MAX_ORDER	24
 
+/*
+ * Size of DMA-consistent memory region.  Must be multiple of 2M,
+ * between 2MB and 14MB inclusive.
+ */
+#ifndef CONSISTENT_DMA_SIZE
+#define CONSISTENT_DMA_SIZE 	SZ_2M
+#endif
+
+#define CONSISTENT_END		(0xffe00000UL)
+#define CONSISTENT_BASE		(CONSISTENT_END - CONSISTENT_DMA_SIZE)
+
 #else /* CONFIG_MMU */
 
 /*
@@ -112,14 +123,6 @@
 
 #endif /* !CONFIG_MMU */
 
-/*
- * Size of DMA-consistent memory region.  Must be multiple of 2M,
- * between 2MB and 14MB inclusive.
- */
-#ifndef CONSISTENT_DMA_SIZE
-#define CONSISTENT_DMA_SIZE SZ_2M
-#endif
-
 /*
  * Physical vs virtual RAM address space conversion.  These are
  * private definitions which should NOT be used outside memory.h

commit 2fc42814d8a9dd757abc7f80fbf11e9247e97d40
Merge: c6baa1963c2a 29cb8d0d249f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Dec 4 15:00:08 2009 +0000

    Merge branch 'pending-dma-streaming' (early part) into devel

commit 1c4a4f48a14861a567c8861355bc8252da3a003f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 15:58:30 2009 +0000

    ARM: dma-mapping: simplify page_to_dma() and __pfn_to_bus()
    
    The non-highmem() and the __pfn_to_bus() based page_to_dma() both
    compile to the same code, so its pointless having these two different
    approaches.  Use the __pfn_to_bus() based version.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Jamie Iles <jamie@jamieiles.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index e0f8f4a4d45f..9099ada9da0c 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -200,7 +200,8 @@ static inline void *phys_to_virt(unsigned long x)
 #ifndef __virt_to_bus
 #define __virt_to_bus	__virt_to_phys
 #define __bus_to_virt	__phys_to_virt
-#define __pfn_to_bus(x)	((x) << PAGE_SHIFT)
+#define __pfn_to_bus(x)	__pfn_to_phys(x)
+#define __bus_to_pfn(x)	__phys_to_pfn(x)
 #endif
 
 static inline __deprecated unsigned long virt_to_bus(void *x)

commit 719301ff1c77b6da7b1b6f78a1e51af64a678619
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 17:51:57 2009 +0000

    ARM: provide phys_to_page() to complement page_to_phys()
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Jamie Iles <jamie@jamieiles.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index cefedf062138..e0f8f4a4d45f 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -134,6 +134,12 @@
 #define	__phys_to_pfn(paddr)	((paddr) >> PAGE_SHIFT)
 #define	__pfn_to_phys(pfn)	((pfn) << PAGE_SHIFT)
 
+/*
+ * Convert a page to/from a physical address
+ */
+#define page_to_phys(page)	(__pfn_to_phys(page_to_pfn(page)))
+#define phys_to_page(phys)	(pfn_to_page(__phys_to_pfn(phys)))
+
 #ifndef __ASSEMBLY__
 
 /*
@@ -292,11 +298,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 
 #endif /* !CONFIG_DISCONTIGMEM */
 
-/*
- * For BIO.  "will die".  Kill me when bio_to_phys() and bvec_to_phys() die.
- */
-#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
-
 /*
  * Optional coherency support.  Currently used only by selected
  * Intel XSC3-based systems.

commit a8cf81ffe0284660fe405e7189f47f1b032f5261
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 19 16:51:28 2009 +0100

    Revert "[ARM] unconditionally define __virt_to_phys and __phys_to_virt"
    
    This reverts commit 75f4aa15cf05ce6d99c8261cf57dcd749877fd1c.
    
    We have a couple of platforms which require non-linear P:V mappings,
    so we need these to be overridable.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index cefedf062138..bc2ff8b28133 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -125,8 +125,10 @@
  * private definitions which should NOT be used outside memory.h
  * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
  */
+#ifndef __virt_to_phys
 #define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
 #define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
+#endif
 
 /*
  * Convert a physical address to a Page Frame Number and back

commit 87d721ad7a37b7650dd710c88dd5c6a5bf9fe996
Merge: ddd559b13f6d b7cfda9fc3d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Sep 12 12:04:37 2009 +0100

    Merge branch 'master' into devel

commit b7cfda9fc3d7aa60cffab5367f2a72a4a70060cd
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Sep 7 15:06:42 2009 +0100

    ARM: Fix pfn_valid() for sparse memory
    
    On OMAP platforms, some people want to declare to segment up the memory
    between the kernel and a separate application such that there is a hole
    in the middle of the memory as far as Linux is concerned.  However,
    they want to be able to mmap() the hole.
    
    This currently causes problems, because update_mmu_cache() thinks that
    there are valid struct pages for the "hole".  Fix this by making
    pfn_valid() slightly more expensive, by checking whether the PFN is
    contained within the meminfo array.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Khasim Syed Mohammed <khasim@ti.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 85763db87449..b3b54c6216a4 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -212,7 +212,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
  *
  *  page_to_pfn(page)	convert a struct page * to a PFN number
  *  pfn_to_page(pfn)	convert a _valid_ PFN number to struct page *
- *  pfn_valid(pfn)	indicates whether a PFN number is valid
  *
  *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
  *  virt_addr_valid(k)	indicates whether a virtual address is valid
@@ -221,10 +220,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 
 #define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
 
-#ifndef CONFIG_SPARSEMEM
-#define pfn_valid(pfn)		((pfn) >= PHYS_PFN_OFFSET && (pfn) < (PHYS_PFN_OFFSET + max_mapnr))
-#endif
-
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory)
 
@@ -241,18 +236,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 #define arch_pfn_to_nid(pfn)	PFN_TO_NID(pfn)
 #define arch_local_page_offset(pfn, nid) LOCAL_MAP_NR((pfn) << PAGE_SHIFT)
 
-#define pfn_valid(pfn)						\
-	({							\
-		unsigned int nid = PFN_TO_NID(pfn);		\
-		int valid = nid < MAX_NUMNODES;			\
-		if (valid) {					\
-			pg_data_t *node = NODE_DATA(nid);	\
-			valid = (pfn - node->node_start_pfn) <	\
-				node->node_spanned_pages;	\
-		}						\
-		valid;						\
-	})
-
 #define virt_to_page(kaddr)					\
 	(ADDR_TO_MAPBASE(kaddr) + LOCAL_MAP_NR(kaddr))
 

commit adca6dc23bc620ea95392659625200a252b97be3
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Fri Jul 24 12:32:59 2009 +0100

    Thumb-2: Add support for loadable modules
    
    Modules compiled to Thumb-2 have two additional relocations needing to
    be resolved at load time, R_ARM_THM_CALL and R_ARM_THM_JUMP24, for BL
    and B.W instructions. The maximum Thumb-2 addressing range is +/-2^24
    (+/-16MB) therefore the MODULES_VADDR macro in asm/memory.h is set to
    (MODULES_END - 8MB) for the Thumb-2 compiled kernel.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 85763db87449..376be1a62866 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -44,7 +44,13 @@
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 32MB of the kernel text.
  */
+#ifndef CONFIG_THUMB2_KERNEL
 #define MODULES_VADDR		(PAGE_OFFSET - 16*1024*1024)
+#else
+/* smaller range for Thumb-2 symbols relocation (2^24)*/
+#define MODULES_VADDR		(PAGE_OFFSET - 8*1024*1024)
+#endif
+
 #if TASK_SIZE > MODULES_VADDR
 #error Top of user space clashes with start of module space
 #endif

commit 58edb515724f9e63e569536d01ac8d8f8ddb367a
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Sep 9 15:54:13 2008 -0400

    [ARM] make page_to_dma() highmem aware
    
    If a machine class has a custom __virt_to_bus() implementation then it
    must provide a __arch_page_to_dma() implementation as well which is
    _not_ based on page_address() to support highmem.
    
    This patch fixes existing __arch_page_to_dma() and provide a default
    implementation otherwise.  The default implementation for highmem is
    based on __pfn_to_bus() which is defined only when no custom
    __virt_to_bus() is provided by the machine class.
    
    That leaves only ebsa110 and footbridge which cannot support highmem
    until they provide their own __arch_page_to_dma() implementation.
    But highmem support on those legacy platforms with limited memory is
    certainly not a priority.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index ae472bc376d3..85763db87449 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -188,6 +188,7 @@ static inline void *phys_to_virt(unsigned long x)
 #ifndef __virt_to_bus
 #define __virt_to_bus	__virt_to_phys
 #define __bus_to_virt	__phys_to_virt
+#define __pfn_to_bus(x)	((x) << PAGE_SHIFT)
 #endif
 
 static inline __deprecated unsigned long virt_to_bus(void *x)

commit d73cd42893f4cdc06e6829fea2347bb92cb789d1
Author: Nicolas Pitre <nico@cam.org>
Date:   Mon Sep 15 16:44:55 2008 -0400

    [ARM] kmap support
    
    The kmap virtual area borrows a 2MB range at the top of the 16MB area
    below PAGE_OFFSET currently reserved for kernel modules and/or the
    XIP kernel.  This 2MB corresponds to the range covered by 2 consecutive
    second-level page tables, or a single pmd entry as seen by the Linux
    page table abstraction.  Because XIP kernels are unlikely to be seen
    on systems needing highmem support, there shouldn't be any shortage of
    VM space for modules (14 MB for modules is still way more than twice the
    typical usage).
    
    Because the virtual mapping of highmem pages can go away at any moment
    after kunmap() is called on them, we need to bypass the delayed cache
    flushing provided by flush_dcache_page() in that case.
    
    The atomic kmap versions are based on fixmaps, and
    __cpuc_flush_dcache_page() is used directly in that case.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 0202a7c20e62..ae472bc376d3 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -44,13 +44,20 @@
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 32MB of the kernel text.
  */
-#define MODULES_END		(PAGE_OFFSET)
-#define MODULES_VADDR		(MODULES_END - 16*1048576)
-
+#define MODULES_VADDR		(PAGE_OFFSET - 16*1024*1024)
 #if TASK_SIZE > MODULES_VADDR
 #error Top of user space clashes with start of module space
 #endif
 
+/*
+ * The highmem pkmap virtual space shares the end of the module area.
+ */
+#ifdef CONFIG_HIGHMEM
+#define MODULES_END		(PAGE_OFFSET - PMD_SIZE)
+#else
+#define MODULES_END		(PAGE_OFFSET)
+#endif
+
 /*
  * The XIP kernel gets mapped at the bottom of the module vm area.
  * Since we use sections to map it, this macro replaces the physical address

commit b5ee9002583fc14e6d45a04c18f208987a8fbced
Author: Nicolas Pitre <nico@cam.org>
Date:   Fri Sep 5 21:53:30 2008 -0400

    [ARM] remove a common set of __virt_to_bus definitions
    
    Let's provide an overridable default instead of having every machine
    class define __virt_to_bus and __bus_to_virt to the same thing.  What
    most platforms are using is bus_addr == phys_addr so such is the default.
    
    One exception is ebsa110 which has no DMA what so ever, so the actual
    definition is not important except only for proper compilation.  Also
    added a comment about the special footbridge bus translation.
    
    Let's also remove comments alluding to set_dma_addr which is not
    (and should not) be commonly used.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 7238b3b50643..0202a7c20e62 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -178,6 +178,11 @@ static inline void *phys_to_virt(unsigned long x)
  * memory.  Use of these is *deprecated* (and that doesn't mean
  * use the __ prefixed forms instead.)  See dma-mapping.h.
  */
+#ifndef __virt_to_bus
+#define __virt_to_bus	__virt_to_phys
+#define __bus_to_virt	__phys_to_virt
+#endif
+
 static inline __deprecated unsigned long virt_to_bus(void *x)
 {
 	return __virt_to_bus((unsigned long)x);

commit 75f4aa15cf05ce6d99c8261cf57dcd749877fd1c
Author: Nicolas Pitre <nico@cam.org>
Date:   Fri Sep 5 16:05:14 2008 -0400

    [ARM] unconditionally define __virt_to_phys and __phys_to_virt
    
    There is no machine class overriding this.  If non linear translations
    are implemented again for some machines then this could be restored at
    that time.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 77764301844b..7238b3b50643 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -112,10 +112,8 @@
  * private definitions which should NOT be used outside memory.h
  * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
  */
-#ifndef __virt_to_phys
 #define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
 #define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
-#endif
 
 /*
  * Convert a physical address to a Page Frame Number and back

commit ab4f2ee130d5ffcf35616e1f5c6ab75af5b463b6
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Nov 6 17:11:07 2008 +0000

    [ARM] fix naming of MODULE_START / MODULE_END
    
    As of 73bdf0a60e607f4b8ecc5aec597105976565a84f, the kernel needs
    to know where modules are located in the virtual address space.
    On ARM, we located this region between MODULE_START and MODULE_END.
    Unfortunately, everyone else calls it MODULES_VADDR and MODULES_END.
    Update ARM to use the same naming, so is_vmalloc_or_module_addr()
    can work properly.  Also update the comment on mm/vmalloc.c to
    reflect that ARM also places modules in a separate region from the
    vmalloc space.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 809ff9ab853a..77764301844b 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -44,10 +44,10 @@
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 32MB of the kernel text.
  */
-#define MODULE_END		(PAGE_OFFSET)
-#define MODULE_START		(MODULE_END - 16*1048576)
+#define MODULES_END		(PAGE_OFFSET)
+#define MODULES_VADDR		(MODULES_END - 16*1048576)
 
-#if TASK_SIZE > MODULE_START
+#if TASK_SIZE > MODULES_VADDR
 #error Top of user space clashes with start of module space
 #endif
 
@@ -56,7 +56,7 @@
  * Since we use sections to map it, this macro replaces the physical address
  * with its virtual address while keeping offset from the base section.
  */
-#define XIP_VIRT_ADDR(physaddr)  (MODULE_START + ((physaddr) & 0x000fffff))
+#define XIP_VIRT_ADDR(physaddr)  (MODULES_VADDR + ((physaddr) & 0x000fffff))
 
 /*
  * Allow 16MB-aligned ioremap pages
@@ -94,8 +94,8 @@
 /*
  * The module can be at any place in ram in nommu mode.
  */
-#define MODULE_END		(END_MEM)
-#define MODULE_START		(PHYS_OFFSET)
+#define MODULES_END		(END_MEM)
+#define MODULES_VADDR		(PHYS_OFFSET)
 
 #endif /* !CONFIG_MMU */
 

commit 3bca103a1e658d23737d20e1989139d9ca8973bf
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Oct 7 20:14:55 2008 +0100

    [ARM] 5295/1: make ZONE_DMA optional
    
    Most ARM machines don't need a special "DMA" memory zone, and
    when configured out, the kernel becomes a bit smaller:
    
    |   text    data     bss     dec     hex filename
    |3826182  102384  111700 4040266  3da64a vmlinux
    |3823593  101616  111700 4036909  3d992d vmlinux.nodmazone
    
    This is because the system now has only one zone total which effect is
    to optimize away many conditionals in page allocation paths.
    
    So let's configure this zone only on machines that need split zones.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 7834adbe1774..809ff9ab853a 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -137,6 +137,8 @@
 
 #ifndef arch_adjust_zones
 #define arch_adjust_zones(node,size,holes) do { } while (0)
+#elif !defined(CONFIG_ZONE_DMA)
+#error "custom arch_adjust_zones() requires CONFIG_ZONE_DMA"
 #endif
 
 /*

commit 6c5da7aced798c7781f054a76c769b85f0173561
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Sep 30 19:31:44 2008 +0100

    [ARM] mm: move vmalloc= parsing to arch/arm/mm/mmu.c
    
    There's no point scattering this around the tree, the parsing
    of the parameter might as well live beside the code which uses
    it.  That also means we can make vmalloc_reserve a static
    variable.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 7e8d22fef29c..7834adbe1774 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -139,14 +139,6 @@
 #define arch_adjust_zones(node,size,holes) do { } while (0)
 #endif
 
-/*
- * Amount of memory reserved for the vmalloc() area, and minimum
- * address for vmalloc mappings.
- */
-extern unsigned long vmalloc_reserve;
-
-#define VMALLOC_MIN		(void *)(VMALLOC_END - vmalloc_reserve)
-
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.

commit 8d5796d2ec6b5a4e7a52861144e63af438d6f8f7
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Mon Aug 25 21:03:32 2008 +0100

    [ARM] 5222/1: Allow configuring user:kernel split via Kconfig
    
    This patch adds a config option (CONFIG_VMSPLIT_*) to allow choosing
    between 3:1, 2:2 and 1:3 user:kernel memory splits.
    
    Tested-by: Riku Voipio <riku.voipio@iki.fi>
    Signed-off-by: Lennert Buytenhek <buytenh@marvell.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index bf7c737c9226..7e8d22fef29c 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -13,43 +13,33 @@
 #ifndef __ASM_ARM_MEMORY_H
 #define __ASM_ARM_MEMORY_H
 
+#include <linux/compiler.h>
+#include <linux/const.h>
+#include <mach/memory.h>
+#include <asm/sizes.h>
+
 /*
  * Allow for constants defined here to be used from assembly code
  * by prepending the UL suffix only with actual C code compilation.
  */
-#ifndef __ASSEMBLY__
-#define UL(x) (x##UL)
-#else
-#define UL(x) (x)
-#endif
-
-#include <linux/compiler.h>
-#include <mach/memory.h>
-#include <asm/sizes.h>
+#define UL(x) _AC(x, UL)
 
 #ifdef CONFIG_MMU
 
-#ifndef TASK_SIZE
 /*
+ * PAGE_OFFSET - the virtual address of the start of the kernel image
  * TASK_SIZE - the maximum size of a user space task.
  * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area
  */
-#define TASK_SIZE		UL(0xbf000000)
-#define TASK_UNMAPPED_BASE	UL(0x40000000)
-#endif
+#define PAGE_OFFSET		UL(CONFIG_PAGE_OFFSET)
+#define TASK_SIZE		(UL(CONFIG_PAGE_OFFSET) - UL(0x01000000))
+#define TASK_UNMAPPED_BASE	(UL(CONFIG_PAGE_OFFSET) / 3)
 
 /*
  * The maximum size of a 26-bit user space task.
  */
 #define TASK_SIZE_26		UL(0x04000000)
 
-/*
- * Page offset: 3GB
- */
-#ifndef PAGE_OFFSET
-#define PAGE_OFFSET		UL(0xc0000000)
-#endif
-
 /*
  * The module space lives between the addresses given by TASK_SIZE
  * and PAGE_OFFSET - it must be within 32MB of the kernel text.

commit 98ed7d4b1a4eebc1ac25929b6968673bef4d54c3
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 12:10:49 2008 +0100

    [ARM] dma-mapping: improve type-safeness of DMA translations
    
    OMAP at least gets the return type(s) for the DMA translation functions
    wrong, which can lead to subtle errors.  Avoid this by moving the DMA
    translation functions to asm/dma-mapping.h, and converting them to
    inline functions.
    
    Fix the OMAP DMA translation macros to use the correct argument and
    result types.
    
    Also, remove the unnecessary casts in dmabounce.c.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 7bcd69a9a88c..bf7c737c9226 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -313,20 +313,6 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
  */
 #define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
 
-/*
- * Optional device DMA address remapping. Do _not_ use directly!
- * We should really eliminate virt_to_bus() here - it's deprecated.
- */
-#ifndef __arch_page_to_dma
-#define page_to_dma(dev, page)		((dma_addr_t)__virt_to_bus((unsigned long)page_address(page)))
-#define dma_to_virt(dev, addr)		((void *)__bus_to_virt(addr))
-#define virt_to_dma(dev, addr)		((dma_addr_t)__virt_to_bus((unsigned long)(addr)))
-#else
-#define page_to_dma(dev, page)		(__arch_page_to_dma(dev, page))
-#define dma_to_virt(dev, addr)		(__arch_dma_to_virt(dev, addr))
-#define virt_to_dma(dev, addr)		(__arch_virt_to_dma(dev, addr))
-#endif
-
 /*
  * Optional coherency support.  Currently used only by selected
  * Intel XSC3-based systems.

commit 60296c71f6c5063e3c1f1d2619ca0b60940162e7
Author: Lennert Buytenhek <buytenh@wantstofly.org>
Date:   Tue Aug 5 01:56:13 2008 +0200

    [ARM] prevent crashing when too much RAM installed
    
    This patch will truncate and/or ignore memory banks if their kernel
    direct mappings would (partially) overlap with the vmalloc area or
    the mappings between the vmalloc area and the address space top, to
    prevent crashing during early boot if there happens to be more RAM
    installed than we are expecting.
    
    Since the start of the vmalloc area is not at a fixed address (but
    the vmalloc end address is, via the per-platform VMALLOC_END define),
    a default area of 128M is reserved for vmalloc mappings, which can
    be shrunk or enlarged by passing an appropriate vmalloc= command line
    option as it is done on x86.
    
    On a board with a 3:1 user:kernel split, VMALLOC_END at 0xfe000000,
    two 512M RAM banks and vmalloc=128M (the default), this patch gives:
    
            Truncating RAM at 20000000-3fffffff to -35ffffff (vmalloc region overlap).
            Memory: 512MB 352MB = 864MB total
    
    On a board with a 3:1 user:kernel split, VMALLOC_END at 0xfe800000,
    two 256M RAM banks and vmalloc=768M, this patch gives:
    
            Truncating RAM at 00000000-0fffffff to -0e7fffff (vmalloc region overlap).
            Ignoring RAM at 10000000-1fffffff (vmalloc region overlap).
    
    Signed-off-by: Lennert Buytenhek <buytenh@marvell.com>
    Tested-by: Riku Voipio <riku.voipio@iki.fi>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 1e070a2b561a..7bcd69a9a88c 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -149,6 +149,14 @@
 #define arch_adjust_zones(node,size,holes) do { } while (0)
 #endif
 
+/*
+ * Amount of memory reserved for the vmalloc() area, and minimum
+ * address for vmalloc mappings.
+ */
+extern unsigned long vmalloc_reserve;
+
+#define VMALLOC_MIN		(void *)(VMALLOC_END - vmalloc_reserve)
+
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.

commit a09e64fbc0094e3073dbb09c3b4bfe4ab669244b
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Aug 5 16:14:15 2008 +0100

    [ARM] Move include/asm-arm/arch-* to arch/arm/*/include/mach
    
    This just leaves include/asm-arm/plat-* to deal with.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 92069221dca9..1e070a2b561a 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -24,7 +24,7 @@
 #endif
 
 #include <linux/compiler.h>
-#include <asm/arch/memory.h>
+#include <mach/memory.h>
 #include <asm/sizes.h>
 
 #ifdef CONFIG_MMU

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
new file mode 100644
index 000000000000..92069221dca9
--- /dev/null
+++ b/arch/arm/include/asm/memory.h
@@ -0,0 +1,334 @@
+/*
+ *  arch/arm/include/asm/memory.h
+ *
+ *  Copyright (C) 2000-2002 Russell King
+ *  modification for nommu, Hyok S. Choi, 2004
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ *  Note: this file should not be included by non-asm/.h files
+ */
+#ifndef __ASM_ARM_MEMORY_H
+#define __ASM_ARM_MEMORY_H
+
+/*
+ * Allow for constants defined here to be used from assembly code
+ * by prepending the UL suffix only with actual C code compilation.
+ */
+#ifndef __ASSEMBLY__
+#define UL(x) (x##UL)
+#else
+#define UL(x) (x)
+#endif
+
+#include <linux/compiler.h>
+#include <asm/arch/memory.h>
+#include <asm/sizes.h>
+
+#ifdef CONFIG_MMU
+
+#ifndef TASK_SIZE
+/*
+ * TASK_SIZE - the maximum size of a user space task.
+ * TASK_UNMAPPED_BASE - the lower boundary of the mmap VM area
+ */
+#define TASK_SIZE		UL(0xbf000000)
+#define TASK_UNMAPPED_BASE	UL(0x40000000)
+#endif
+
+/*
+ * The maximum size of a 26-bit user space task.
+ */
+#define TASK_SIZE_26		UL(0x04000000)
+
+/*
+ * Page offset: 3GB
+ */
+#ifndef PAGE_OFFSET
+#define PAGE_OFFSET		UL(0xc0000000)
+#endif
+
+/*
+ * The module space lives between the addresses given by TASK_SIZE
+ * and PAGE_OFFSET - it must be within 32MB of the kernel text.
+ */
+#define MODULE_END		(PAGE_OFFSET)
+#define MODULE_START		(MODULE_END - 16*1048576)
+
+#if TASK_SIZE > MODULE_START
+#error Top of user space clashes with start of module space
+#endif
+
+/*
+ * The XIP kernel gets mapped at the bottom of the module vm area.
+ * Since we use sections to map it, this macro replaces the physical address
+ * with its virtual address while keeping offset from the base section.
+ */
+#define XIP_VIRT_ADDR(physaddr)  (MODULE_START + ((physaddr) & 0x000fffff))
+
+/*
+ * Allow 16MB-aligned ioremap pages
+ */
+#define IOREMAP_MAX_ORDER	24
+
+#else /* CONFIG_MMU */
+
+/*
+ * The limitation of user task size can grow up to the end of free ram region.
+ * It is difficult to define and perhaps will never meet the original meaning
+ * of this define that was meant to.
+ * Fortunately, there is no reference for this in noMMU mode, for now.
+ */
+#ifndef TASK_SIZE
+#define TASK_SIZE		(CONFIG_DRAM_SIZE)
+#endif
+
+#ifndef TASK_UNMAPPED_BASE
+#define TASK_UNMAPPED_BASE	UL(0x00000000)
+#endif
+
+#ifndef PHYS_OFFSET
+#define PHYS_OFFSET 		(CONFIG_DRAM_BASE)
+#endif
+
+#ifndef END_MEM
+#define END_MEM     		(CONFIG_DRAM_BASE + CONFIG_DRAM_SIZE)
+#endif
+
+#ifndef PAGE_OFFSET
+#define PAGE_OFFSET		(PHYS_OFFSET)
+#endif
+
+/*
+ * The module can be at any place in ram in nommu mode.
+ */
+#define MODULE_END		(END_MEM)
+#define MODULE_START		(PHYS_OFFSET)
+
+#endif /* !CONFIG_MMU */
+
+/*
+ * Size of DMA-consistent memory region.  Must be multiple of 2M,
+ * between 2MB and 14MB inclusive.
+ */
+#ifndef CONSISTENT_DMA_SIZE
+#define CONSISTENT_DMA_SIZE SZ_2M
+#endif
+
+/*
+ * Physical vs virtual RAM address space conversion.  These are
+ * private definitions which should NOT be used outside memory.h
+ * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
+ */
+#ifndef __virt_to_phys
+#define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
+#define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
+#endif
+
+/*
+ * Convert a physical address to a Page Frame Number and back
+ */
+#define	__phys_to_pfn(paddr)	((paddr) >> PAGE_SHIFT)
+#define	__pfn_to_phys(pfn)	((pfn) << PAGE_SHIFT)
+
+#ifndef __ASSEMBLY__
+
+/*
+ * The DMA mask corresponding to the maximum bus address allocatable
+ * using GFP_DMA.  The default here places no restriction on DMA
+ * allocations.  This must be the smallest DMA mask in the system,
+ * so a successful GFP_DMA allocation will always satisfy this.
+ */
+#ifndef ISA_DMA_THRESHOLD
+#define ISA_DMA_THRESHOLD	(0xffffffffULL)
+#endif
+
+#ifndef arch_adjust_zones
+#define arch_adjust_zones(node,size,holes) do { } while (0)
+#endif
+
+/*
+ * PFNs are used to describe any physical page; this means
+ * PFN 0 == physical address 0.
+ *
+ * This is the PFN of the first RAM page in the kernel
+ * direct-mapped view.  We assume this is the first page
+ * of RAM in the mem_map as well.
+ */
+#define PHYS_PFN_OFFSET	(PHYS_OFFSET >> PAGE_SHIFT)
+
+/*
+ * These are *only* valid on the kernel direct mapped RAM memory.
+ * Note: Drivers should NOT use these.  They are the wrong
+ * translation for translating DMA addresses.  Use the driver
+ * DMA support - see dma-mapping.h.
+ */
+static inline unsigned long virt_to_phys(void *x)
+{
+	return __virt_to_phys((unsigned long)(x));
+}
+
+static inline void *phys_to_virt(unsigned long x)
+{
+	return (void *)(__phys_to_virt((unsigned long)(x)));
+}
+
+/*
+ * Drivers should NOT use these either.
+ */
+#define __pa(x)			__virt_to_phys((unsigned long)(x))
+#define __va(x)			((void *)__phys_to_virt((unsigned long)(x)))
+#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+
+/*
+ * Virtual <-> DMA view memory address translations
+ * Again, these are *only* valid on the kernel direct mapped RAM
+ * memory.  Use of these is *deprecated* (and that doesn't mean
+ * use the __ prefixed forms instead.)  See dma-mapping.h.
+ */
+static inline __deprecated unsigned long virt_to_bus(void *x)
+{
+	return __virt_to_bus((unsigned long)x);
+}
+
+static inline __deprecated void *bus_to_virt(unsigned long x)
+{
+	return (void *)__bus_to_virt(x);
+}
+
+/*
+ * Conversion between a struct page and a physical address.
+ *
+ * Note: when converting an unknown physical address to a
+ * struct page, the resulting pointer must be validated
+ * using VALID_PAGE().  It must return an invalid struct page
+ * for any physical address not corresponding to a system
+ * RAM address.
+ *
+ *  page_to_pfn(page)	convert a struct page * to a PFN number
+ *  pfn_to_page(pfn)	convert a _valid_ PFN number to struct page *
+ *  pfn_valid(pfn)	indicates whether a PFN number is valid
+ *
+ *  virt_to_page(k)	convert a _valid_ virtual address to struct page *
+ *  virt_addr_valid(k)	indicates whether a virtual address is valid
+ */
+#ifndef CONFIG_DISCONTIGMEM
+
+#define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
+
+#ifndef CONFIG_SPARSEMEM
+#define pfn_valid(pfn)		((pfn) >= PHYS_PFN_OFFSET && (pfn) < (PHYS_PFN_OFFSET + max_mapnr))
+#endif
+
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory)
+
+#define PHYS_TO_NID(addr)	(0)
+
+#else /* CONFIG_DISCONTIGMEM */
+
+/*
+ * This is more complex.  We have a set of mem_map arrays spread
+ * around in memory.
+ */
+#include <linux/numa.h>
+
+#define arch_pfn_to_nid(pfn)	PFN_TO_NID(pfn)
+#define arch_local_page_offset(pfn, nid) LOCAL_MAP_NR((pfn) << PAGE_SHIFT)
+
+#define pfn_valid(pfn)						\
+	({							\
+		unsigned int nid = PFN_TO_NID(pfn);		\
+		int valid = nid < MAX_NUMNODES;			\
+		if (valid) {					\
+			pg_data_t *node = NODE_DATA(nid);	\
+			valid = (pfn - node->node_start_pfn) <	\
+				node->node_spanned_pages;	\
+		}						\
+		valid;						\
+	})
+
+#define virt_to_page(kaddr)					\
+	(ADDR_TO_MAPBASE(kaddr) + LOCAL_MAP_NR(kaddr))
+
+#define virt_addr_valid(kaddr)	(KVADDR_TO_NID(kaddr) < MAX_NUMNODES)
+
+/*
+ * Common discontigmem stuff.
+ *  PHYS_TO_NID is used by the ARM kernel/setup.c
+ */
+#define PHYS_TO_NID(addr)	PFN_TO_NID((addr) >> PAGE_SHIFT)
+
+/*
+ * Given a kaddr, ADDR_TO_MAPBASE finds the owning node of the memory
+ * and returns the mem_map of that node.
+ */
+#define ADDR_TO_MAPBASE(kaddr)	NODE_MEM_MAP(KVADDR_TO_NID(kaddr))
+
+/*
+ * Given a page frame number, find the owning node of the memory
+ * and returns the mem_map of that node.
+ */
+#define PFN_TO_MAPBASE(pfn)	NODE_MEM_MAP(PFN_TO_NID(pfn))
+
+#ifdef NODE_MEM_SIZE_BITS
+#define NODE_MEM_SIZE_MASK	((1 << NODE_MEM_SIZE_BITS) - 1)
+
+/*
+ * Given a kernel address, find the home node of the underlying memory.
+ */
+#define KVADDR_TO_NID(addr) \
+	(((unsigned long)(addr) - PAGE_OFFSET) >> NODE_MEM_SIZE_BITS)
+
+/*
+ * Given a page frame number, convert it to a node id.
+ */
+#define PFN_TO_NID(pfn) \
+	(((pfn) - PHYS_PFN_OFFSET) >> (NODE_MEM_SIZE_BITS - PAGE_SHIFT))
+
+/*
+ * Given a kaddr, LOCAL_MEM_MAP finds the owning node of the memory
+ * and returns the index corresponding to the appropriate page in the
+ * node's mem_map.
+ */
+#define LOCAL_MAP_NR(addr) \
+	(((unsigned long)(addr) & NODE_MEM_SIZE_MASK) >> PAGE_SHIFT)
+
+#endif /* NODE_MEM_SIZE_BITS */
+
+#endif /* !CONFIG_DISCONTIGMEM */
+
+/*
+ * For BIO.  "will die".  Kill me when bio_to_phys() and bvec_to_phys() die.
+ */
+#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
+
+/*
+ * Optional device DMA address remapping. Do _not_ use directly!
+ * We should really eliminate virt_to_bus() here - it's deprecated.
+ */
+#ifndef __arch_page_to_dma
+#define page_to_dma(dev, page)		((dma_addr_t)__virt_to_bus((unsigned long)page_address(page)))
+#define dma_to_virt(dev, addr)		((void *)__bus_to_virt(addr))
+#define virt_to_dma(dev, addr)		((dma_addr_t)__virt_to_bus((unsigned long)(addr)))
+#else
+#define page_to_dma(dev, page)		(__arch_page_to_dma(dev, page))
+#define dma_to_virt(dev, addr)		(__arch_dma_to_virt(dev, addr))
+#define virt_to_dma(dev, addr)		(__arch_virt_to_dma(dev, addr))
+#endif
+
+/*
+ * Optional coherency support.  Currently used only by selected
+ * Intel XSC3-based systems.
+ */
+#ifndef arch_is_coherent
+#define arch_is_coherent()		0
+#endif
+
+#endif
+
+#include <asm-generic/memory_model.h>
+
+#endif
