commit 9952f6918daa4ab5fc81307a9f90e31a4df3b200
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:04 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 201
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms and conditions of the gnu general public license
      version 2 as published by the free software foundation this program
      is distributed in the hope it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not see http www gnu org
      licenses
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 228 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171438.107155473@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
index a89b4076cde4..f44f448537f2 100644
--- a/arch/arm/include/asm/percpu.h
+++ b/arch/arm/include/asm/percpu.h
@@ -1,17 +1,6 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright 2012 Calxeda, Inc.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program.  If not, see <http://www.gnu.org/licenses/>.
  */
 #ifndef _ASM_ARM_PERCPU_H_
 #define _ASM_ARM_PERCPU_H_

commit ccbd2da50db2111f6d055e3201c39a37e542ceb6
Author: Mark Charlebois <charlebm@gmail.com>
Date:   Sat Sep 27 00:31:05 2014 +0100

    ARM: 8174/1: Use global stack register variable for percpu
    
    Using global current_stack_pointer works on both clang and gcc.
    current_stack_pointer is an unsigned long and needs to be cast
    as a pointer to dereference.
    
    Signed-off-by: Mark Charlebois <charlebm@gmail.com>
    Signed-off-by: Behan Webster <behanw@converseincode.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
index 209e6504922e..a89b4076cde4 100644
--- a/arch/arm/include/asm/percpu.h
+++ b/arch/arm/include/asm/percpu.h
@@ -30,14 +30,14 @@ static inline void set_my_cpu_offset(unsigned long off)
 static inline unsigned long __my_cpu_offset(void)
 {
 	unsigned long off;
-	register unsigned long *sp asm ("sp");
 
 	/*
 	 * Read TPIDRPRW.
 	 * We want to allow caching the value, so avoid using volatile and
 	 * instead use a fake stack read to hazard against barrier().
 	 */
-	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : "Q" (*sp));
+	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off)
+		: "Q" (*(const unsigned long *)current_stack_pointer));
 
 	return off;
 }

commit 509eb76ebf9771abc9fe51859382df2571f11447
Author: Will Deacon <will.deacon@arm.com>
Date:   Wed Jun 5 11:20:33 2013 +0100

    ARM: 7747/1: pcpu: ensure __my_cpu_offset cannot be re-ordered across barrier()
    
    __my_cpu_offset is non-volatile, since we want its value to be cached
    when we access several per-cpu variables in a row with preemption
    disabled. This means that we rely on preempt_{en,dis}able to hazard
    with the operation via the barrier() macro, so that we can't end up
    migrating CPUs without reloading the per-cpu offset.
    
    Unfortunately, GCC doesn't treat a "memory" clobber on a non-volatile
    asm block as a side-effect, and will happily re-order it before other
    memory clobbers (including those in prempt_disable()) and cache the
    value. This has been observed to break the cmpxchg logic in the slub
    allocator, leading to livelock in kmem_cache_alloc in mainline kernels.
    
    This patch adds a dummy memory input operand to __my_cpu_offset,
    forcing it to be ordered with respect to the barrier() macro.
    
    Cc: <stable@vger.kernel.org>
    Cc: Rob Herring <rob.herring@calxeda.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
index 968c0a14e0a3..209e6504922e 100644
--- a/arch/arm/include/asm/percpu.h
+++ b/arch/arm/include/asm/percpu.h
@@ -30,8 +30,15 @@ static inline void set_my_cpu_offset(unsigned long off)
 static inline unsigned long __my_cpu_offset(void)
 {
 	unsigned long off;
-	/* Read TPIDRPRW */
-	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : : "memory");
+	register unsigned long *sp asm ("sp");
+
+	/*
+	 * Read TPIDRPRW.
+	 * We want to allow caching the value, so avoid using volatile and
+	 * instead use a fake stack read to hazard against barrier().
+	 */
+	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : "Q" (*sp));
+
 	return off;
 }
 #define __my_cpu_offset __my_cpu_offset()

commit 14318efb322e2fe1a034c69463d725209eb9d548
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Thu Nov 29 20:39:54 2012 +0100

    ARM: 7587/1: implement optimized percpu variable access
    
    Use the previously unused TPIDRPRW register to store percpu offsets.
    TPIDRPRW is only accessible in PL1, so it can only be used in the kernel.
    
    This replaces 2 loads with a mrc instruction for each percpu variable
    access. With hackbench, the performance improvement is 1.4% on Cortex-A9
    (highbank). Taking an average of 30 runs of "hackbench -l 1000" yields:
    
    Before: 6.2191
    After: 6.1348
    
    Will Deacon reported similar delta on v6 with 11MPCore.
    
    The asm "memory clobber" are needed here to ensure the percpu offset
    gets reloaded. Testing by Will found that this would not happen in
    __schedule() which is a bit of a special case as preemption is disabled
    but the execution can move cores.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
new file mode 100644
index 000000000000..968c0a14e0a3
--- /dev/null
+++ b/arch/arm/include/asm/percpu.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright 2012 Calxeda, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License along with
+ * this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef _ASM_ARM_PERCPU_H_
+#define _ASM_ARM_PERCPU_H_
+
+/*
+ * Same as asm-generic/percpu.h, except that we store the per cpu offset
+ * in the TPIDRPRW. TPIDRPRW only exists on V6K and V7
+ */
+#if defined(CONFIG_SMP) && !defined(CONFIG_CPU_V6)
+static inline void set_my_cpu_offset(unsigned long off)
+{
+	/* Set TPIDRPRW */
+	asm volatile("mcr p15, 0, %0, c13, c0, 4" : : "r" (off) : "memory");
+}
+
+static inline unsigned long __my_cpu_offset(void)
+{
+	unsigned long off;
+	/* Read TPIDRPRW */
+	asm("mrc p15, 0, %0, c13, c0, 4" : "=r" (off) : : "memory");
+	return off;
+}
+#define __my_cpu_offset __my_cpu_offset()
+#else
+#define set_my_cpu_offset(x)	do {} while(0)
+
+#endif /* CONFIG_SMP */
+
+#include <asm-generic/percpu.h>
+
+#endif /* _ASM_ARM_PERCPU_H_ */

commit 3f8e288033ec7f52b570efad7c2eb42741f6d710
Author: Stephen Boyd <sboyd@codeaurora.org>
Date:   Tue Jul 26 18:45:54 2011 +0100

    ARM: 7006/1: Migrate to asm-generic wrapper support
    
    With d8ecc5c (kbuild: asm-generic support, 2011-04-27) we can
    remove a handful of asm-generic wrappers in ARM code. Since the
    generic version of sizes.h doesn't contain SZ_48M, we replace
    the 4 users of SZ_48M with the equivalent SZ_32M + SZ_16M.
    
    Signed-off-by: Stephen Boyd <sboyd@codeaurora.org>
    Cc: Imre Kaloz <kaloz@openwrt.org>
    Acked-by: Krzysztof Halasa <khc@pm.waw.pl>
    Cc: Eric Miao <eric.y.miao@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
deleted file mode 100644
index b4e32d8ec072..000000000000
--- a/arch/arm/include/asm/percpu.h
+++ /dev/null
@@ -1,6 +0,0 @@
-#ifndef __ARM_PERCPU
-#define __ARM_PERCPU
-
-#include <asm-generic/percpu.h>
-
-#endif

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/percpu.h b/arch/arm/include/asm/percpu.h
new file mode 100644
index 000000000000..b4e32d8ec072
--- /dev/null
+++ b/arch/arm/include/asm/percpu.h
@@ -0,0 +1,6 @@
+#ifndef __ARM_PERCPU
+#define __ARM_PERCPU
+
+#include <asm-generic/percpu.h>
+
+#endif
