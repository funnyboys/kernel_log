commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index acd4983d9b1f..755c97de348c 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -1,12 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * arch/arm/include/asm/mcpm.h
  *
  * Created by:  Nicolas Pitre, April 2012
  * Copyright:   (C) 2012-2013  Linaro Limited
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #ifndef MCPM_H

commit 7895f73169ade9a74940ae6b0b4ee82faf286861
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Apr 28 15:51:19 2015 -0400

    ARM: MCPM: remove residency argument from mcpm_cpu_suspend()
    
    This is currently unused.
    
    If a suspend must be limited to CPU level only by preventing the last man
    from triggering a cluster level suspend then this should be determined
    according to many other criteria the MCPM layer is currently not aware of.
    It is unlikely that mcpm_cpu_suspend() would be the proper conduit for
    that information anyway.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Dave Martin <Dave.Martin@arm.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 6a40d5f8db60..acd4983d9b1f 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -137,17 +137,12 @@ int mcpm_wait_for_cpu_powerdown(unsigned int cpu, unsigned int cluster);
 /**
  * mcpm_cpu_suspend - bring the calling CPU in a suspended state
  *
- * @expected_residency: duration in microseconds the CPU is expected
- *			to remain suspended, or 0 if unknown/infinity.
- *
- * The calling CPU is suspended.  The expected residency argument is used
- * as a hint by the platform specific backend to implement the appropriate
- * sleep state level according to the knowledge it has on wake-up latency
- * for the given hardware.
+ * The calling CPU is suspended.  This is similar to mcpm_cpu_power_down()
+ * except for possible extra platform specific configuration steps to allow
+ * an asynchronous wake-up e.g. with a pending interrupt.
  *
  * If this CPU is found to be the "last man standing" in the cluster
- * then the cluster may be prepared for power-down too, if the expected
- * residency makes it worthwhile.
+ * then the cluster may be prepared for power-down too.
  *
  * This must be called with interrupts disabled.
  *
@@ -157,7 +152,7 @@ int mcpm_wait_for_cpu_powerdown(unsigned int cpu, unsigned int cluster);
  * This will return if mcpm_platform_register() has not been called
  * previously in which case the caller should take appropriate action.
  */
-void mcpm_cpu_suspend(u64 expected_residency);
+void mcpm_cpu_suspend(void);
 
 /**
  * mcpm_cpu_powered_up - housekeeping workafter a CPU has been powered up

commit 7cc8b991cdc985aaa73bf9c429c810cd442fb74d
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Apr 28 14:11:07 2015 -0400

    ARM: MCPM: make internal helpers private to the core code
    
    This concerns the following helpers:
    
            __mcpm_cpu_going_down()
            __mcpm_cpu_down()
            __mcpm_outbound_enter_critical()
            __mcpm_outbound_leave_critical()
            __mcpm_cluster_state()
    
    They are and should only be used by the core code now.  Therefore their
    declarations are removed from mcpm.h and their definitions are made
    static, hence the need to move them before their users which accounts
    for the bulk of this patch.
    
    This left the mcpm_sync_struct definition at an odd location, therefore
    it is moved as well with some comment clarifications.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Dave Martin <Dave.Martin@arm.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index e2118c941dbf..6a40d5f8db60 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -245,35 +245,6 @@ struct mcpm_platform_ops {
  */
 int __init mcpm_platform_register(const struct mcpm_platform_ops *ops);
 
-/* Synchronisation structures for coordinating safe cluster setup/teardown: */
-
-/*
- * When modifying this structure, make sure you update the MCPM_SYNC_ defines
- * to match.
- */
-struct mcpm_sync_struct {
-	/* individual CPU states */
-	struct {
-		s8 cpu __aligned(__CACHE_WRITEBACK_GRANULE);
-	} cpus[MAX_CPUS_PER_CLUSTER];
-
-	/* cluster state */
-	s8 cluster __aligned(__CACHE_WRITEBACK_GRANULE);
-
-	/* inbound-side state */
-	s8 inbound __aligned(__CACHE_WRITEBACK_GRANULE);
-};
-
-struct sync_struct {
-	struct mcpm_sync_struct clusters[MAX_NR_CLUSTERS];
-};
-
-void __mcpm_cpu_going_down(unsigned int cpu, unsigned int cluster);
-void __mcpm_cpu_down(unsigned int cpu, unsigned int cluster);
-void __mcpm_outbound_leave_critical(unsigned int cluster, int state);
-bool __mcpm_outbound_enter_critical(unsigned int this_cpu, unsigned int cluster);
-int __mcpm_cluster_state(unsigned int cluster);
-
 /**
  * mcpm_sync_init - Initialize the cluster synchronization support
  *
@@ -312,6 +283,29 @@ int __init mcpm_loopback(void (*cache_disable)(void));
 
 void __init mcpm_smp_set_ops(void);
 
+/*
+ * Synchronisation structures for coordinating safe cluster setup/teardown.
+ * This is private to the MCPM core code and shared between C and assembly.
+ * When modifying this structure, make sure you update the MCPM_SYNC_ defines
+ * to match.
+ */
+struct mcpm_sync_struct {
+	/* individual CPU states */
+	struct {
+		s8 cpu __aligned(__CACHE_WRITEBACK_GRANULE);
+	} cpus[MAX_CPUS_PER_CLUSTER];
+
+	/* cluster state */
+	s8 cluster __aligned(__CACHE_WRITEBACK_GRANULE);
+
+	/* inbound-side state */
+	s8 inbound __aligned(__CACHE_WRITEBACK_GRANULE);
+};
+
+struct sync_struct {
+	struct mcpm_sync_struct clusters[MAX_NR_CLUSTERS];
+};
+
 #else
 
 /* 

commit 77404d81cadf192cc1261d6269f622a06b83cdd5
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Apr 28 13:44:00 2015 -0400

    ARM: MCPM: remove backward compatibility code
    
    Now that no one uses the old callbacks anymore, let's remove them
    and associated support code.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Dave Martin <Dave.Martin@arm.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 50b378f59e08..e2118c941dbf 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -234,12 +234,6 @@ struct mcpm_platform_ops {
 	void (*cpu_is_up)(unsigned int cpu, unsigned int cluster);
 	void (*cluster_is_up)(unsigned int cluster);
 	int (*wait_for_powerdown)(unsigned int cpu, unsigned int cluster);
-
-	/* deprecated callbacks */
-	int (*power_up)(unsigned int cpu, unsigned int cluster);
-	void (*power_down)(void);
-	void (*suspend)(u64);
-	void (*powered_up)(void);
 };
 
 /**

commit d3a875444ad8d5e64c5a932361ca579312e49801
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Mar 11 18:16:13 2015 -0400

    ARM: MCPM: move the algorithmic complexity to the core code
    
    All backends are reimplementing a variation of the same CPU reference
    count handling. They are also responsible for driving the MCPM special
    low-level locking. This is needless duplication, involving algorithmic
    requirements that are not necessarily obvious to the uninitiated.
    And from past code review experience, those were all initially
    implemented badly.
    
    After 3 years, it is time to refactor as much common code to the core
    MCPM facility to make the backends as simple as possible.  To avoid a
    flag day, the new scheme is introduced in parallel to the existing
    backend interface.  When all backends are converted over, the
    compatibility interface could be removed.
    
    The new MCPM backend interface implements simpler methods addressing
    very platform specific tasks performed under lock protection while
    keeping the algorithmic complexity and race avoidance local to the
    core code.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Olof Johansson <olof@lixom.net>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 3446f6a1d9fa..50b378f59e08 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -171,12 +171,73 @@ void mcpm_cpu_suspend(u64 expected_residency);
 int mcpm_cpu_powered_up(void);
 
 /*
- * Platform specific methods used in the implementation of the above API.
+ * Platform specific callbacks used in the implementation of the above API.
+ *
+ * cpu_powerup:
+ * Make given CPU runable. Called with MCPM lock held and IRQs disabled.
+ * The given cluster is assumed to be set up (cluster_powerup would have
+ * been called beforehand). Must return 0 for success or negative error code.
+ *
+ * cluster_powerup:
+ * Set up power for given cluster. Called with MCPM lock held and IRQs
+ * disabled. Called before first cpu_powerup when cluster is down. Must
+ * return 0 for success or negative error code.
+ *
+ * cpu_suspend_prepare:
+ * Special suspend configuration. Called on target CPU with MCPM lock held
+ * and IRQs disabled. This callback is optional. If provided, it is called
+ * before cpu_powerdown_prepare.
+ *
+ * cpu_powerdown_prepare:
+ * Configure given CPU for power down. Called on target CPU with MCPM lock
+ * held and IRQs disabled. Power down must be effective only at the next WFI instruction.
+ *
+ * cluster_powerdown_prepare:
+ * Configure given cluster for power down. Called on one CPU from target
+ * cluster with MCPM lock held and IRQs disabled. A cpu_powerdown_prepare
+ * for each CPU in the cluster has happened when this occurs.
+ *
+ * cpu_cache_disable:
+ * Clean and disable CPU level cache for the calling CPU. Called on with IRQs
+ * disabled only. The CPU is no longer cache coherent with the rest of the
+ * system when this returns.
+ *
+ * cluster_cache_disable:
+ * Clean and disable the cluster wide cache as well as the CPU level cache
+ * for the calling CPU. No call to cpu_cache_disable will happen for this
+ * CPU. Called with IRQs disabled and only when all the other CPUs are done
+ * with their own cpu_cache_disable. The cluster is no longer cache coherent
+ * with the rest of the system when this returns.
+ *
+ * cpu_is_up:
+ * Called on given CPU after it has been powered up or resumed. The MCPM lock
+ * is held and IRQs disabled. This callback is optional.
+ *
+ * cluster_is_up:
+ * Called by the first CPU to be powered up or resumed in given cluster.
+ * The MCPM lock is held and IRQs disabled. This callback is optional. If
+ * provided, it is called before cpu_is_up for that CPU.
+ *
+ * wait_for_powerdown:
+ * Wait until given CPU is powered down. This is called in sleeping context.
+ * Some reasonable timeout must be considered. Must return 0 for success or
+ * negative error code.
  */
 struct mcpm_platform_ops {
+	int (*cpu_powerup)(unsigned int cpu, unsigned int cluster);
+	int (*cluster_powerup)(unsigned int cluster);
+	void (*cpu_suspend_prepare)(unsigned int cpu, unsigned int cluster);
+	void (*cpu_powerdown_prepare)(unsigned int cpu, unsigned int cluster);
+	void (*cluster_powerdown_prepare)(unsigned int cluster);
+	void (*cpu_cache_disable)(void);
+	void (*cluster_cache_disable)(void);
+	void (*cpu_is_up)(unsigned int cpu, unsigned int cluster);
+	void (*cluster_is_up)(unsigned int cluster);
+	int (*wait_for_powerdown)(unsigned int cpu, unsigned int cluster);
+
+	/* deprecated callbacks */
 	int (*power_up)(unsigned int cpu, unsigned int cluster);
 	void (*power_down)(void);
-	int (*wait_for_powerdown)(unsigned int cpu, unsigned int cluster);
 	void (*suspend)(u64);
 	void (*powered_up)(void);
 };

commit 216b4688cc351e97514c40dbedd14f67ce59639a
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sat Nov 29 03:19:42 2014 +0100

    ARM: 8240/1: MCPM: document mcpm_sync_init()
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index d428e386c88e..3446f6a1d9fa 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -219,6 +219,23 @@ void __mcpm_outbound_leave_critical(unsigned int cluster, int state);
 bool __mcpm_outbound_enter_critical(unsigned int this_cpu, unsigned int cluster);
 int __mcpm_cluster_state(unsigned int cluster);
 
+/**
+ * mcpm_sync_init - Initialize the cluster synchronization support
+ *
+ * @power_up_setup: platform specific function invoked during very
+ * 		    early CPU/cluster bringup stage.
+ *
+ * This prepares memory used by vlocks and the MCPM state machine used
+ * across CPUs that may have their caches active or inactive. Must be
+ * called only after a successful call to mcpm_platform_register().
+ *
+ * The power_up_setup argument is a pointer to assembly code called when
+ * the MMU and caches are still disabled during boot  and no stack space is
+ * available. The affinity level passed to that code corresponds to the
+ * resource that needs to be initialized (e.g. 1 for cluster level, 0 for
+ * CPU level).  Proper exclusion mechanisms are already activated at that
+ * point.
+ */
 int __init mcpm_sync_init(
 	void (*power_up_setup)(unsigned int affinity_level));
 

commit ebf4a5c5b4027b682ed8877a938e6d1d92f37745
Author: Haojian Zhuang <haojian.zhuang@linaro.org>
Date:   Tue Apr 15 14:52:00 2014 +0800

    ARM: mcpm: support 4 clusters
    
    Add the CONFIG_MCPM_QUAD_CLUSTER configuration to enlarge cluster number
    from 2 to 4.
    
    Signed-off-by: Haojian Zhuang <haojian.zhuang@linaro.org>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Wei Xu <xuwei5@hisilicon.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 57ff7f2a3084..d428e386c88e 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -20,7 +20,12 @@
  * to consider dynamic allocation.
  */
 #define MAX_CPUS_PER_CLUSTER	4
+
+#ifdef CONFIG_MCPM_QUAD_CLUSTER
+#define MAX_NR_CLUSTERS		4
+#else
 #define MAX_NR_CLUSTERS		2
+#endif
 
 #ifndef __ASSEMBLY__
 

commit 3721924c81541d828d73d0e36dcbae8fd93f0885
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Jun 24 18:32:51 2014 +0100

    ARM: 8081/1: MCPM: provide infrastructure to allow for MCPM loopback
    
    The kernel already has the responsibility to handle resources such as the
    
    CCI when hotplugging CPUs, during the booting of secondary CPUs, and when
    resuming from suspend/idle.  It would be more coherent and less confusing
    if the CCI for the boot CPU (or cluster)  was also initialized by the
    kernel rather than expecting the firmware/bootloader to do it and only in
    that case. After all, the kernel has all the necessary code already and
    the bootloader shouldn't have to care at all.
    
    The CCI may be turned on only when the cache is off. Leveraging the CPU
    suspend code to loop back through the low-level MCPM entry point is all
    that is needed to properly turn on the CCI from the kernel by using the
    same code as during secondary boot.
    
    Let's provide a generic MCPM loopback function that can be invoked by
    backend initialization code to set things (CCI or similar) on the boot
    CPU just as it is done for the other CPUs.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Doug Anderson <dianders@chromium.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 94060adba174..57ff7f2a3084 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -217,6 +217,22 @@ int __mcpm_cluster_state(unsigned int cluster);
 int __init mcpm_sync_init(
 	void (*power_up_setup)(unsigned int affinity_level));
 
+/**
+ * mcpm_loopback - make a run through the MCPM low-level code
+ *
+ * @cache_disable: pointer to function performing cache disabling
+ *
+ * This exercises the MCPM machinery by soft resetting the CPU and branching
+ * to the MCPM low-level entry code before returning to the caller.
+ * The @cache_disable function must do the necessary cache disabling to
+ * let the regular kernel init code turn it back on as if the CPU was
+ * hotplugged in. The MCPM state machine is set as if the cluster was
+ * initialized meaning the power_up_setup callback passed to mcpm_sync_init()
+ * will be invoked for all affinity levels. This may be useful to initialize
+ * some resources such as enabling the CCI that requires the cache to be off, or simply for testing purposes.
+ */
+int __init mcpm_loopback(void (*cache_disable)(void));
+
 void __init mcpm_smp_set_ops(void);
 
 #else

commit d0ba7cc02cc20a3ae6ad60b842c5e786f584bb47
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Jun 19 22:57:01 2014 +0100

    ARM: 8080/1: mcpm.h: remove unused variable declaration
    
    The sync_phys variable has been replaced by link time computation in
    mcpm_head.S before the code was submitted upstream.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index d9702eb0b02b..94060adba174 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -208,8 +208,6 @@ struct sync_struct {
 	struct mcpm_sync_struct clusters[MAX_NR_CLUSTERS];
 };
 
-extern unsigned long sync_phys;	/* physical address of *mcpm_sync */
-
 void __mcpm_cpu_going_down(unsigned int cpu, unsigned int cluster);
 void __mcpm_cpu_down(unsigned int cpu, unsigned int cluster);
 void __mcpm_outbound_leave_critical(unsigned int cluster, int state);

commit 1fb333489fb917c704ad43e51b45c12f52215a9c
Merge: 20e7e364331d 3f8517e7937d 8ef418c7178f 1c2f87c22566
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Jun 5 12:35:52 2014 +0100

    Merge branches 'alignment', 'fixes', 'l2c' (early part) and 'misc' into for-next

commit 166aaf396654b533f536f2cf84d7558eb42f1c9f
Author: Dave Martin <Dave.Martin@arm.com>
Date:   Thu Apr 17 16:58:39 2014 +0100

    ARM: 8029/1: mcpm: Rename the power_down_finish() functions to be less confusing
    
    The name "power_down_finish" seems to be causing some confusion,
    because it suggests that this function is responsible for taking
    some action to cause the specified CPU to complete its power down.
    
    This patch renames the affected functions to "wait_for_powerdown"
    and similar, since this function's intended purpose is just to wait
    for the hardware to finish a powerdown initiated by a previous
    cpu_power_down.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 608516ebabfe..90ed21942456 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -91,14 +91,14 @@ int mcpm_cpu_power_up(unsigned int cpu, unsigned int cluster);
  * previously in which case the caller should take appropriate action.
  *
  * On success, the CPU is not guaranteed to be truly halted until
- * mcpm_cpu_power_down_finish() subsequently returns non-zero for the
+ * mcpm_wait_for_cpu_powerdown() subsequently returns non-zero for the
  * specified cpu.  Until then, other CPUs should make sure they do not
  * trash memory the target CPU might be executing/accessing.
  */
 void mcpm_cpu_power_down(void);
 
 /**
- * mcpm_cpu_power_down_finish - wait for a specified CPU to halt, and
+ * mcpm_wait_for_cpu_powerdown - wait for a specified CPU to halt, and
  *	make sure it is powered off
  *
  * @cpu: CPU number within given cluster
@@ -120,7 +120,7 @@ void mcpm_cpu_power_down(void);
  *	- zero if the CPU is in a safely parked state
  *	- nonzero otherwise (e.g., timeout)
  */
-int mcpm_cpu_power_down_finish(unsigned int cpu, unsigned int cluster);
+int mcpm_wait_for_cpu_powerdown(unsigned int cpu, unsigned int cluster);
 
 /**
  * mcpm_cpu_suspend - bring the calling CPU in a suspended state
@@ -164,7 +164,7 @@ int mcpm_cpu_powered_up(void);
 struct mcpm_platform_ops {
 	int (*power_up)(unsigned int cpu, unsigned int cluster);
 	void (*power_down)(void);
-	int (*power_down_finish)(unsigned int cpu, unsigned int cluster);
+	int (*wait_for_powerdown)(unsigned int cpu, unsigned int cluster);
 	void (*suspend)(u64);
 	void (*powered_up)(void);
 };

commit 4530e4b6a450af14973c2b0703edfb02d66cbd41
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Apr 22 00:25:35 2014 +0100

    ARM: 8032/1: bL_switcher: fix validation check before its activation
    
    The switcher should not depend on MAX_CLUSTER to determine ifit should
    be activated or not. In a multiplatform kernel binary it is possible to
    have dual-cluster and quad-cluster platforms configured in. In that case
    MAX_CLUSTER which is a build time limit should be 4 and that shouldn't
    prevent the switcher from working if the kernel is booted on a b.L
    dual-cluster system.
    
    In bL_switcher_halve_cpus() we already have a runtime validation check
    to make sure we're dealing with only two clusters, so booting on a quad
    cluster system will be caught and switcher activation aborted.
    
    However, the b.L switcher must ensure the MCPM layer is initialized on
    the booted hardware before doing anything.  The mcpm_is_available()
    function is added to that effect.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Tested-by: Abhilash Kesavan <kesavan.abhilash@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 608516ebabfe..a5ff410dcdb6 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -53,6 +53,13 @@ void mcpm_set_early_poke(unsigned cpu, unsigned cluster,
  * CPU/cluster power operations API for higher subsystems to use.
  */
 
+/**
+ * mcpm_is_available - returns whether MCPM is initialized and available
+ *
+ * This returns true or false accordingly.
+ */
+bool mcpm_is_available(void);
+
 /**
  * mcpm_cpu_power_up - make given CPU in given cluster runable
  *

commit df762eccbadf87850fbee444d729e0f1b1e946f1
Merge: ec1e20a02fe3 70d42126877b
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 12 10:58:59 2013 +0000

    Merge branch 'devel-stable' into for-next
    
    Conflicts:
            arch/arm/include/asm/atomic.h
            arch/arm/include/asm/hardirq.h
            arch/arm/kernel/smp.c

commit 0de0d64675259bf21d06b18985318ffb66a5218f
Author: Dave Martin <dave.martin@linaro.org>
Date:   Tue Oct 1 19:58:17 2013 +0100

    ARM: 7848/1: mcpm: Implement cpu_kill() to synchronise on powerdown
    
    CPU hotplug and kexec rely on smp_ops.cpu_kill(), which is supposed
    to wait for the CPU to park or power down, and perform the last
    rites (such as disabling clocks etc., where the platform doesn't do
    this automatically).
    
    kexec in particular is unsafe without performing this
    synchronisation to park secondaries.  Without it, the secondaries
    might not be parked when kexec trashes the kernel.
    
    There is no generic way to do this synchronisation, so a new mcpm
    platform_ops method power_down_finish() is added by this patch.
    
    The new method is mandatory.  A platform which provides no way to
    detect when CPUs are parked is likely broken.
    
    Signed-off-by: Dave Martin <Dave.Martin@arm.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index fc82a88f5b69..1cf26010a6f3 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -81,9 +81,39 @@ int mcpm_cpu_power_up(unsigned int cpu, unsigned int cluster);
  *
  * This will return if mcpm_platform_register() has not been called
  * previously in which case the caller should take appropriate action.
+ *
+ * On success, the CPU is not guaranteed to be truly halted until
+ * mcpm_cpu_power_down_finish() subsequently returns non-zero for the
+ * specified cpu.  Until then, other CPUs should make sure they do not
+ * trash memory the target CPU might be executing/accessing.
  */
 void mcpm_cpu_power_down(void);
 
+/**
+ * mcpm_cpu_power_down_finish - wait for a specified CPU to halt, and
+ *	make sure it is powered off
+ *
+ * @cpu: CPU number within given cluster
+ * @cluster: cluster number for the CPU
+ *
+ * Call this function to ensure that a pending powerdown has taken
+ * effect and the CPU is safely parked before performing non-mcpm
+ * operations that may affect the CPU (such as kexec trashing the
+ * kernel text).
+ *
+ * It is *not* necessary to call this function if you only need to
+ * serialise a pending powerdown with mcpm_cpu_power_up() or a wakeup
+ * event.
+ *
+ * Do not call this function unless the specified CPU has already
+ * called mcpm_cpu_power_down() or has committed to doing so.
+ *
+ * @return:
+ *	- zero if the CPU is in a safely parked state
+ *	- nonzero otherwise (e.g., timeout)
+ */
+int mcpm_cpu_power_down_finish(unsigned int cpu, unsigned int cluster);
+
 /**
  * mcpm_cpu_suspend - bring the calling CPU in a suspended state
  *
@@ -126,6 +156,7 @@ int mcpm_cpu_powered_up(void);
 struct mcpm_platform_ops {
 	int (*power_up)(unsigned int cpu, unsigned int cluster);
 	void (*power_down)(void);
+	int (*power_down_finish)(unsigned int cpu, unsigned int cluster);
 	void (*suspend)(u64);
 	void (*powered_up)(void);
 };

commit d0cdef6e87ebc1241d7e407d5e1b14e6bb836ae9
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Sep 25 23:26:24 2013 +0100

    ARM: 7842/1: MCPM: don't explode if invoked without being initialized first
    
    Currently mcpm_cpu_power_down() and mcpm_cpu_suspend() trigger BUG()
    if mcpm_platform_register() is not called beforehand.  This may occur
    for many reasons such as some incomplete device tree passed to the kernel
    or the like.
    
    Let's be nicer to users and avoid killing the kernel if that happens by
    logging a warning and returning to the caller.  The mcpm_cpu_suspend()
    user is already set to deal with this situation, and so is cpu_die()
    invoking mcpm_cpu_die().
    
    The problematic case would have been the B.L switcher's usage of
    mcpm_cpu_power_down(), however it has to call mcpm_cpu_power_up() first
    which is already set to catch an error resulting from a missing
    mcpm_platform_register() call.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 0f7b7620e9a5..fc82a88f5b69 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -76,8 +76,11 @@ int mcpm_cpu_power_up(unsigned int cpu, unsigned int cluster);
  *
  * This must be called with interrupts disabled.
  *
- * This does not return.  Re-entry in the kernel is expected via
- * mcpm_entry_point.
+ * On success this does not return.  Re-entry in the kernel is expected
+ * via mcpm_entry_point.
+ *
+ * This will return if mcpm_platform_register() has not been called
+ * previously in which case the caller should take appropriate action.
  */
 void mcpm_cpu_power_down(void);
 
@@ -98,8 +101,11 @@ void mcpm_cpu_power_down(void);
  *
  * This must be called with interrupts disabled.
  *
- * This does not return.  Re-entry in the kernel is expected via
- * mcpm_entry_point.
+ * On success this does not return.  Re-entry in the kernel is expected
+ * via mcpm_entry_point.
+ *
+ * This will return if mcpm_platform_register() has not been called
+ * previously in which case the caller should take appropriate action.
  */
 void mcpm_cpu_suspend(u64 expected_residency);
 

commit de885d147ad2c4a66777e3557440247efde1cc8d
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Nov 27 23:11:20 2012 -0500

    ARM: mcpm: add a simple poke mechanism to the early entry code
    
    This allows to poke a predetermined value into a specific address
    upon entering the early boot code in bL_head.S.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 0f7b7620e9a5..7626a7fd4938 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -41,6 +41,14 @@ extern void mcpm_entry_point(void);
  */
 void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr);
 
+/*
+ * This sets an early poke i.e a value to be poked into some address
+ * from very early assembly code before the CPU is ungated.  The
+ * address must be physical, and if 0 then nothing will happen.
+ */
+void mcpm_set_early_poke(unsigned cpu, unsigned cluster,
+			 unsigned long poke_phys_addr, unsigned long poke_val);
+
 /*
  * CPU/cluster power operations API for higher subsystems to use.
  */

commit a7eb7c6f9a657a01a8359edae31bbeacd18b072c
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Tue Apr 9 01:29:17 2013 -0400

    ARM: mcpm: provide an interface to set the SMP ops at run time
    
    This is cleaner than exporting the mcpm_smp_ops structure.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Jon Medhurst <tixy@linaro.org>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 3046e90210cb..0f7b7620e9a5 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -167,6 +167,8 @@ int __mcpm_cluster_state(unsigned int cluster);
 int __init mcpm_sync_init(
 	void (*power_up_setup)(unsigned int affinity_level));
 
+void __init mcpm_smp_set_ops(void);
+
 #else
 
 /* 

commit 7fe31d28e839f9565c8176ec584676a045970802
Author: Dave Martin <dave.martin@linaro.org>
Date:   Tue Jul 17 14:25:42 2012 +0100

    ARM: mcpm: introduce helpers for platform coherency exit/setup
    
    This provides helper methods to coordinate between CPUs coming down
    and CPUs going up, as well as documentation on the used algorithms,
    so that cluster teardown and setup
    operations are not done for a cluster simultaneously.
    
    For use in the power_down() implementation:
      * __mcpm_cpu_going_down(unsigned int cluster, unsigned int cpu)
      * __mcpm_outbound_enter_critical(unsigned int cluster)
      * __mcpm_outbound_leave_critical(unsigned int cluster)
      * __mcpm_cpu_down(unsigned int cluster, unsigned int cpu)
    
    The power_up_setup() helper should do platform-specific setup in
    preparation for turning the CPU on, such as invalidating local caches
    or entering coherency.  It must be assembler for now, since it must
    run before the MMU can be switched on.  It is passed the affinity level
    for which initialization should be performed.
    
    Because the mcpm_sync_struct content is looked-up and modified
    with the cache enabled or disabled depending on the code path, it is
    crucial to always ensure proper cache maintenance to update main memory
    right away.  The sync_cache_*() helpers are used to that end.
    
    Also, in order to prevent a cached writer from interfering with an
    adjacent non-cached writer, we ensure each state variable is located to
    a separate cache line.
    
    Thanks to Nicolas Pitre and Achin Gupta for the help with this
    patch.
    
    Signed-off-by: Dave Martin <dave.martin@linaro.org>
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 627761fce780..3046e90210cb 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -24,6 +24,9 @@
 
 #ifndef __ASSEMBLY__
 
+#include <linux/types.h>
+#include <asm/cacheflush.h>
+
 /*
  * Platform specific code should use this symbol to set up secondary
  * entry location for processors to use when released from reset.
@@ -130,5 +133,75 @@ struct mcpm_platform_ops {
  */
 int __init mcpm_platform_register(const struct mcpm_platform_ops *ops);
 
+/* Synchronisation structures for coordinating safe cluster setup/teardown: */
+
+/*
+ * When modifying this structure, make sure you update the MCPM_SYNC_ defines
+ * to match.
+ */
+struct mcpm_sync_struct {
+	/* individual CPU states */
+	struct {
+		s8 cpu __aligned(__CACHE_WRITEBACK_GRANULE);
+	} cpus[MAX_CPUS_PER_CLUSTER];
+
+	/* cluster state */
+	s8 cluster __aligned(__CACHE_WRITEBACK_GRANULE);
+
+	/* inbound-side state */
+	s8 inbound __aligned(__CACHE_WRITEBACK_GRANULE);
+};
+
+struct sync_struct {
+	struct mcpm_sync_struct clusters[MAX_NR_CLUSTERS];
+};
+
+extern unsigned long sync_phys;	/* physical address of *mcpm_sync */
+
+void __mcpm_cpu_going_down(unsigned int cpu, unsigned int cluster);
+void __mcpm_cpu_down(unsigned int cpu, unsigned int cluster);
+void __mcpm_outbound_leave_critical(unsigned int cluster, int state);
+bool __mcpm_outbound_enter_critical(unsigned int this_cpu, unsigned int cluster);
+int __mcpm_cluster_state(unsigned int cluster);
+
+int __init mcpm_sync_init(
+	void (*power_up_setup)(unsigned int affinity_level));
+
+#else
+
+/* 
+ * asm-offsets.h causes trouble when included in .c files, and cacheflush.h
+ * cannot be included in asm files.  Let's work around the conflict like this.
+ */
+#include <asm/asm-offsets.h>
+#define __CACHE_WRITEBACK_GRANULE CACHE_WRITEBACK_GRANULE
+
 #endif /* ! __ASSEMBLY__ */
+
+/* Definitions for mcpm_sync_struct */
+#define CPU_DOWN		0x11
+#define CPU_COMING_UP		0x12
+#define CPU_UP			0x13
+#define CPU_GOING_DOWN		0x14
+
+#define CLUSTER_DOWN		0x21
+#define CLUSTER_UP		0x22
+#define CLUSTER_GOING_DOWN	0x23
+
+#define INBOUND_NOT_COMING_UP	0x31
+#define INBOUND_COMING_UP	0x32
+
+/*
+ * Offsets for the mcpm_sync_struct members, for use in asm.
+ * We don't want to make them global to the kernel via asm-offsets.c.
+ */
+#define MCPM_SYNC_CLUSTER_CPUS	0
+#define MCPM_SYNC_CPU_SIZE	__CACHE_WRITEBACK_GRANULE
+#define MCPM_SYNC_CLUSTER_CLUSTER \
+	(MCPM_SYNC_CLUSTER_CPUS + MCPM_SYNC_CPU_SIZE * MAX_CPUS_PER_CLUSTER)
+#define MCPM_SYNC_CLUSTER_INBOUND \
+	(MCPM_SYNC_CLUSTER_CLUSTER + __CACHE_WRITEBACK_GRANULE)
+#define MCPM_SYNC_CLUSTER_SIZE \
+	(MCPM_SYNC_CLUSTER_INBOUND + __CACHE_WRITEBACK_GRANULE)
+
 #endif

commit 7c2b860534d02d11923dd0504b961f21508173f1
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Sep 20 16:05:37 2012 -0400

    ARM: mcpm: introduce the CPU/cluster power API
    
    This is the basic API used to handle the powering up/down of individual
    CPUs in a (multi-)cluster system.  The platform specific backend
    implementation has the responsibility to also handle the cluster level
    power as well when the first/last CPU in a cluster is brought up/down.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 470a417d1351..627761fce780 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -38,5 +38,97 @@ extern void mcpm_entry_point(void);
  */
 void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr);
 
+/*
+ * CPU/cluster power operations API for higher subsystems to use.
+ */
+
+/**
+ * mcpm_cpu_power_up - make given CPU in given cluster runable
+ *
+ * @cpu: CPU number within given cluster
+ * @cluster: cluster number for the CPU
+ *
+ * The identified CPU is brought out of reset.  If the cluster was powered
+ * down then it is brought up as well, taking care not to let the other CPUs
+ * in the cluster run, and ensuring appropriate cluster setup.
+ *
+ * Caller must ensure the appropriate entry vector is initialized with
+ * mcpm_set_entry_vector() prior to calling this.
+ *
+ * This must be called in a sleepable context.  However, the implementation
+ * is strongly encouraged to return early and let the operation happen
+ * asynchronously, especially when significant delays are expected.
+ *
+ * If the operation cannot be performed then an error code is returned.
+ */
+int mcpm_cpu_power_up(unsigned int cpu, unsigned int cluster);
+
+/**
+ * mcpm_cpu_power_down - power the calling CPU down
+ *
+ * The calling CPU is powered down.
+ *
+ * If this CPU is found to be the "last man standing" in the cluster
+ * then the cluster is prepared for power-down too.
+ *
+ * This must be called with interrupts disabled.
+ *
+ * This does not return.  Re-entry in the kernel is expected via
+ * mcpm_entry_point.
+ */
+void mcpm_cpu_power_down(void);
+
+/**
+ * mcpm_cpu_suspend - bring the calling CPU in a suspended state
+ *
+ * @expected_residency: duration in microseconds the CPU is expected
+ *			to remain suspended, or 0 if unknown/infinity.
+ *
+ * The calling CPU is suspended.  The expected residency argument is used
+ * as a hint by the platform specific backend to implement the appropriate
+ * sleep state level according to the knowledge it has on wake-up latency
+ * for the given hardware.
+ *
+ * If this CPU is found to be the "last man standing" in the cluster
+ * then the cluster may be prepared for power-down too, if the expected
+ * residency makes it worthwhile.
+ *
+ * This must be called with interrupts disabled.
+ *
+ * This does not return.  Re-entry in the kernel is expected via
+ * mcpm_entry_point.
+ */
+void mcpm_cpu_suspend(u64 expected_residency);
+
+/**
+ * mcpm_cpu_powered_up - housekeeping workafter a CPU has been powered up
+ *
+ * This lets the platform specific backend code perform needed housekeeping
+ * work.  This must be called by the newly activated CPU as soon as it is
+ * fully operational in kernel space, before it enables interrupts.
+ *
+ * If the operation cannot be performed then an error code is returned.
+ */
+int mcpm_cpu_powered_up(void);
+
+/*
+ * Platform specific methods used in the implementation of the above API.
+ */
+struct mcpm_platform_ops {
+	int (*power_up)(unsigned int cpu, unsigned int cluster);
+	void (*power_down)(void);
+	void (*suspend)(u64);
+	void (*powered_up)(void);
+};
+
+/**
+ * mcpm_platform_register - register platform specific power methods
+ *
+ * @ops: mcpm_platform_ops structure to register
+ *
+ * An error is returned if the registration has been done previously.
+ */
+int __init mcpm_platform_register(const struct mcpm_platform_ops *ops);
+
 #endif /* ! __ASSEMBLY__ */
 #endif

commit e8db288e05e588ad3f416b3a24354d60d02f35f2
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Thu Apr 12 02:45:22 2012 -0400

    ARM: multi-cluster PM: secondary kernel entry code
    
    CPUs in cluster based systems, such as big.LITTLE, have special needs
    when entering the kernel due to a hotplug event, or when resuming from
    a deep sleep mode.
    
    This is vectorized so multiple CPUs can enter the kernel in parallel
    without serialization.
    
    The mcpm prefix stands for "multi cluster power management", however
    this is usable on single cluster systems as well.  Only the basic
    structure is introduced here.  This will be extended with later patches.
    
    In order not to complexify things more than they currently have to,
    the planned work to make runtime adjusted MPIDR based indexing and
    dynamic memory allocation for cluster states is postponed to a later
    cycle. The MAX_NR_CLUSTERS and MAX_CPUS_PER_CLUSTER static definitions
    should be sufficient for those systems expected to be available in the
    near future.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
new file mode 100644
index 000000000000..470a417d1351
--- /dev/null
+++ b/arch/arm/include/asm/mcpm.h
@@ -0,0 +1,42 @@
+/*
+ * arch/arm/include/asm/mcpm.h
+ *
+ * Created by:  Nicolas Pitre, April 2012
+ * Copyright:   (C) 2012-2013  Linaro Limited
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef MCPM_H
+#define MCPM_H
+
+/*
+ * Maximum number of possible clusters / CPUs per cluster.
+ *
+ * This should be sufficient for quite a while, while keeping the
+ * (assembly) code simpler.  When this starts to grow then we'll have
+ * to consider dynamic allocation.
+ */
+#define MAX_CPUS_PER_CLUSTER	4
+#define MAX_NR_CLUSTERS		2
+
+#ifndef __ASSEMBLY__
+
+/*
+ * Platform specific code should use this symbol to set up secondary
+ * entry location for processors to use when released from reset.
+ */
+extern void mcpm_entry_point(void);
+
+/*
+ * This is used to indicate where the given CPU from given cluster should
+ * branch once it is ready to re-enter the kernel using ptr, or NULL if it
+ * should be gated.  A gated CPU is held in a WFE loop until its vector
+ * becomes non NULL.
+ */
+void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr);
+
+#endif /* ! __ASSEMBLY__ */
+#endif
