commit 441692aafc1731087bbaf657a8b6059d95c2a6df
Merge: 5b0e2cb02008 02196144a0a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 12:50:35 2017 -0800

    Merge branch 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm
    
    Pull ARM updates from Russell King:
    
     - add support for ELF fdpic binaries on both MMU and noMMU platforms
    
     - linker script cleanups
    
     - support for compressed .data section for XIP images
    
     - discard memblock arrays when possible
    
     - various cleanups
    
     - atomic DMA pool updates
    
     - better diagnostics of missing/corrupt device tree
    
     - export information to allow userspace kexec tool to place images more
       inteligently, so that the device tree isn't overwritten by the
       booting kernel
    
     - make early_printk more efficient on semihosted systems
    
     - noMMU cleanups
    
     - SA1111 PCMCIA update in preparation for further cleanups
    
    * 'for-linus' of git://git.armlinux.org.uk/~rmk/linux-arm: (38 commits)
      ARM: 8719/1: NOMMU: work around maybe-uninitialized warning
      ARM: 8717/2: debug printch/printascii: translate '\n' to "\r\n" not "\n\r"
      ARM: 8713/1: NOMMU: Support MPU in XIP configuration
      ARM: 8712/1: NOMMU: Use more MPU regions to cover memory
      ARM: 8711/1: V7M: Add support for MPU to M-class
      ARM: 8710/1: Kconfig: Kill CONFIG_VECTORS_BASE
      ARM: 8709/1: NOMMU: Disallow MPU for XIP
      ARM: 8708/1: NOMMU: Rework MPU to be mostly done in C
      ARM: 8707/1: NOMMU: Update MPU accessors to use cp15 helpers
      ARM: 8706/1: NOMMU: Move out MPU setup in separate module
      ARM: 8702/1: head-common.S: Clear lr before jumping to start_kernel()
      ARM: 8705/1: early_printk: use printascii() rather than printch()
      ARM: 8703/1: debug.S: move hexbuf to a writable section
      ARM: add additional table to compressed kernel
      ARM: decompressor: fix BSS size calculation
      pcmcia: sa1111: remove special sa1111 mmio accessors
      pcmcia: sa1111: use sa1111_get_irq() to obtain IRQ resources
      ARM: better diagnostics with missing/corrupt dtb
      ARM: 8699/1: dma-mapping: Remove init_dma_coherent_pool_size()
      ARM: 8698/1: dma-mapping: Mark atomic_pool as __ro_after_init
      ..

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index a5b47421059d..65669b9ce128 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __ARM_MMU_H
 #define __ARM_MMU_H
 

commit 382e67aec6a7eea8ed4403e86950b468a191c468
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Fri Aug 11 00:53:39 2017 -0400

    ARM: enable elf_fdpic on systems with an MMU
    
    Provide the necessary changes to be able to execute ELF-FDPIC binaries
    on ARM systems with an MMU.
    
    The default for CONFIG_BINFMT_ELF_FDPIC is also set to n if the regular
    ELF loader is already configured so not to force FDPIC support on
    everyone. Given that CONFIG_BINFMT_ELF depends on CONFIG_MMU, this means
    CONFIG_BINFMT_ELF_FDPIC will still default to y when !MMU.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Mickael GUENE <mickael.guene@st.com>
    Tested-by: Vincent Abriou <vincent.abriou@st.com>
    Tested-by: Andras Szemzo <szemzo.andras@gmail.com>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index e0eb16680a5b..bdec37c6ac35 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -14,6 +14,10 @@ typedef struct {
 #ifdef CONFIG_VDSO
 	unsigned long	vdso;
 #endif
+#ifdef CONFIG_BINFMT_ELF_FDPIC
+	unsigned long	exec_fdpic_loadmap;
+	unsigned long	interp_fdpic_loadmap;
+#endif
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID

commit 50b2b2e691cd4ff30331ba9a6156b29a07b60f90
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sat Jul 22 01:02:40 2017 -0400

    ARM: add ELF_FDPIC support
    
    This includes the necessary code to recognise the FDPIC format on ARM
    and the ptrace command definitions used by the common ptrace code.
    
    Based on patches originally from Mickael Guene <mickael.guene@st.com>.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Acked-by: Mickael GUENE <mickael.guene@st.com>
    Tested-by: Vincent Abriou <vincent.abriou@st.com>
    Tested-by: Andras Szemzo <szemzo.andras@gmail.com>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index a5b47421059d..e0eb16680a5b 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -33,6 +33,10 @@ typedef struct {
  */
 typedef struct {
 	unsigned long	end_brk;
+#ifdef CONFIG_BINFMT_ELF_FDPIC
+	unsigned long	exec_fdpic_loadmap;
+	unsigned long	interp_fdpic_loadmap;
+#endif
 } mm_context_t;
 
 #endif

commit 1713ce7c43755fe8b0f31ea317513129bf784909
Author: Nathan Lynch <nathan_lynch@mentor.com>
Date:   Wed Mar 25 19:13:16 2015 +0100

    ARM: 8329/1: miscellaneous vdso infrastructure, preparation
    
    Define the layout of the data structure shared between kernel and
    userspace.
    
    Track the vdso address in the mm_context; needed for communicating
    AT_SYSINFO_EHDR to the ELF loader.
    
    Add declarations for arm_install_vdso; implementation is in a
    following patch.
    
    Define AT_SYSINFO_EHDR, and, if CONFIG_VDSO=y, report the vdso shared
    object address via the ELF auxiliary vector.
    
    Note - this adds the AT_SYSINFO_EHDR in a new user-visible header
    asm/auxvec.h; this is consistent with other architectures.
    
    Signed-off-by: Nathan Lynch <nathan_lynch@mentor.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 64fd15159b7d..a5b47421059d 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -11,6 +11,9 @@ typedef struct {
 #endif
 	unsigned int	vmalloc_seq;
 	unsigned long	sigpage;
+#ifdef CONFIG_VDSO
+	unsigned long	vdso;
+#endif
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID

commit a1af3474487cc3b8731b990dceac6b6aad7f3ed8
Author: Victor Kamensky <victor.kamensky@linaro.org>
Date:   Mon Oct 7 08:48:23 2013 -0700

    ARM: tlb: ASID macro should give 32bit result for BE correct operation
    
    In order for ASID macro to be used as expression passed to
    inline asm as 'r' operand it needs to give 32 bit unsigned result,
    not unsigned 64bit expression.
    
    Otherwise when 64bit ASID is passed to inline assembler statement
    as 'r' operand (32bit) compiler behavior is not well specified.
    For example when __flush_tlb_mm function compiled in big endian
    case, and ASID is passed to tlb_op macro directly, 0 will be passed
    as 'mcr 15, 0, r4, cr8, cr3, {2}' argument in r4, unless ASID
    macro changed to produce 32 bit result.
    
    Signed-off-by: Victor Kamensky <victor.kamensky@linaro.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ben Dooks <ben.dooks@codethink.co.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 6f18da09668b..64fd15159b7d 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -16,7 +16,7 @@ typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
 #define ASID_BITS	8
 #define ASID_MASK	((~0ULL) << ASID_BITS)
-#define ASID(mm)	((mm)->context.id.counter & ~ASID_MASK)
+#define ASID(mm)	((unsigned int)((mm)->context.id.counter & ~ASID_MASK))
 #else
 #define ASID(mm)	(0)
 #endif

commit 24195cad3e00557da166d629c8b0fd2f984f2170
Merge: 2449189bb7c7 a5463cd34354
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Aug 1 20:51:13 2013 +0100

    Merge branch 'security-fixes' into fixes

commit 48be69a026b2c17350a5ef18a1959a919f60be7d
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jul 24 00:29:18 2013 +0100

    ARM: move signal handlers into a vdso-like page
    
    Move the signal handlers into a VDSO page rather than keeping them in
    the vectors page.  This allows us to place them randomly within this
    page, and also map the page at a random location within userspace
    further protecting these code fragments from ROP attacks.  The new
    VDSO page is also poisoned in the same way as the vector page.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index e3d55547e755..7345e37155d5 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -8,6 +8,7 @@ typedef struct {
 	atomic64_t	id;
 #endif
 	unsigned int	vmalloc_seq;
+	unsigned long	sigpage;
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID

commit bdae73cd374e28db544fdd9b77de689a36e3c129
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jul 23 16:15:36 2013 +0100

    ARM: 7790/1: Fix deferred mm switch on VIVT processors
    
    As of commit b9d4d42ad9 (ARM: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW on
    pre-ARMv6 CPUs), the mm switching on VIVT processors is done in the
    finish_arch_post_lock_switch() function to avoid whole cache flushing
    with interrupts disabled. The need for deferred mm switch is stored as a
    thread flag (TIF_SWITCH_MM). However, with preemption enabled, we can
    have another thread switch before finish_arch_post_lock_switch(). If the
    new thread has the same mm as the previous 'next' thread, the scheduler
    will not call switch_mm() and the TIF_SWITCH_MM flag won't be set for
    the new thread.
    
    This patch moves the switch pending flag to the mm_context_t structure
    since this is specific to the mm rather than thread.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reported-by: Marc Kleine-Budde <mkl@pengutronix.de>
    Tested-by: Marc Kleine-Budde <mkl@pengutronix.de>
    Cc: <stable@vger.kernel.org> # 3.5+
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index e3d55547e755..d1b4998e4f43 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -6,6 +6,8 @@
 typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
 	atomic64_t	id;
+#else
+	int		switch_pending;
 #endif
 	unsigned int	vmalloc_seq;
 } mm_context_t;

commit 8a4e3a9ead7e37ce1505602b564c15da09ac039f
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu Feb 28 17:47:36 2013 +0100

    ARM: 7659/1: mm: make mm->context.id an atomic64_t variable
    
    mm->context.id is updated under asid_lock when a new ASID is allocated
    to an mm_struct. However, it is also read without the lock when a task
    is being scheduled and checking whether or not the current ASID
    generation is up-to-date.
    
    If two threads of the same process are being scheduled in parallel and
    the bottom bits of the generation in their mm->context.id match the
    current generation (that is, the mm_struct has not been used for ~2^24
    rollovers) then the non-atomic, lockless access to mm->context.id may
    yield the incorrect ASID.
    
    This patch fixes this issue by making mm->context.id and atomic64_t,
    ensuring that the generation is always read consistently. For code that
    only requires access to the ASID bits (e.g. TLB flushing by mm), then
    the value is accessed directly, which GCC converts to an ldrb.
    
    Cc: <stable@vger.kernel.org> # 3.8
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 9f77e7804f3b..e3d55547e755 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -5,15 +5,15 @@
 
 typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
-	u64 id;
+	atomic64_t	id;
 #endif
-	unsigned int vmalloc_seq;
+	unsigned int	vmalloc_seq;
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID
 #define ASID_BITS	8
 #define ASID_MASK	((~0ULL) << ASID_BITS)
-#define ASID(mm)	((mm)->context.id & ~ASID_MASK)
+#define ASID(mm)	((mm)->context.id.counter & ~ASID_MASK)
 #else
 #define ASID(mm)	(0)
 #endif
@@ -26,7 +26,7 @@ typedef struct {
  *  modified for 2.6 by Hyok S. Choi <hyok.choi@samsung.com>
  */
 typedef struct {
-	unsigned long		end_brk;
+	unsigned long	end_brk;
 } mm_context_t;
 
 #endif

commit 3e99675af1b25a191c467700499b1cbe5585a778
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sun Nov 25 03:24:32 2012 +0100

    ARM: 7582/2: rename kvm_seq to vmalloc_seq so to avoid confusion with KVM
    
    The kvm_seq value has nothing to do what so ever with this other KVM.
    Given that KVM support on ARM is imminent, it's best to rename kvm_seq
    into something else to clearly identify what it is about i.e. a sequence
    number for vmalloc section mappings.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 5b53b53ab5cf..9f77e7804f3b 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -7,7 +7,7 @@ typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
 	u64 id;
 #endif
-	unsigned int kvm_seq;
+	unsigned int vmalloc_seq;
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID

commit b5466f8728527a05a493cc4abe9e6f034a1bbaab
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jun 15 14:47:31 2012 +0100

    ARM: mm: remove IPI broadcasting on ASID rollover
    
    ASIDs are allocated to MMU contexts based on a rolling counter. This
    means that after 255 allocations we must invalidate all existing ASIDs
    via an expensive IPI mechanism to synchronise all of the online CPUs and
    ensure that all tasks execute with an ASID from the new generation.
    
    This patch changes the rollover behaviour so that we rely instead on the
    hardware broadcasting of the TLB invalidation to avoid the IPI calls.
    This works by keeping track of the active ASID on each core, which is
    then reserved in the case of a rollover so that currently scheduled
    tasks can continue to run. For cores without hardware TLB broadcasting,
    we keep track of pending flushes in a cpumask, so cores can flush their
    local TLB before scheduling a new mm.
    
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 14965658a923..5b53b53ab5cf 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -5,18 +5,15 @@
 
 typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
-	unsigned int id;
-	raw_spinlock_t id_lock;
+	u64 id;
 #endif
 	unsigned int kvm_seq;
 } mm_context_t;
 
 #ifdef CONFIG_CPU_HAS_ASID
-#define ASID(mm)	((mm)->context.id & 255)
-
-/* init_mm.context.id_lock should be initialized. */
-#define INIT_MM_CONTEXT(name)                                                 \
-	.context.id_lock    = __RAW_SPIN_LOCK_UNLOCKED(name.context.id_lock),
+#define ASID_BITS	8
+#define ASID_MASK	((~0ULL) << ASID_BITS)
+#define ASID(mm)	((mm)->context.id & ~ASID_MASK)
 #else
 #define ASID(mm)	(0)
 #endif

commit b9d4d42ad901cc848ac87f1cb8923fded3645568
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Nov 28 21:57:24 2011 +0000

    ARM: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW on pre-ARMv6 CPUs
    
    This patch removes the __ARCH_WANT_INTERRUPTS_ON_CTXSW definition for
    ARMv5 and earlier processors. On such processors, the context switch
    requires a full cache flush. To avoid high interrupt latencies, this
    patch defers the mm switching to the post-lock switch hook if the
    interrupts are disabled.
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Tested-by: Marc Zyngier <Marc.Zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 20b43d6f23b3..14965658a923 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -34,13 +34,4 @@ typedef struct {
 
 #endif
 
-/*
- * switch_mm() may do a full cache flush over the context switch,
- * so enable interrupts over the context switch to avoid high
- * latency.
- */
-#ifndef CONFIG_CPU_HAS_ASID
-#define __ARCH_WANT_INTERRUPTS_ON_CTXSW
-#endif
-
 #endif

commit 7fec1b57b8a925d83c194f995f83d9f8442fd48e
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Nov 28 13:53:28 2011 +0000

    ARM: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW on ASID-capable CPUs
    
    Since the ASIDs must be unique to an mm across all the CPUs in a system,
    the __new_context() function needs to broadcast a context reset event to
    all the CPUs during ASID allocation if a roll-over occurred. Such IPIs
    cannot be issued with interrupts disabled and ARM had to define
    __ARCH_WANT_INTERRUPTS_ON_CTXSW.
    
    This patch changes the check_context() function to
    check_and_switch_context() called from switch_mm(). In case of
    ASID-capable CPUs (ARMv6 onwards), if a new ASID is needed and the
    interrupts are disabled, it defers the __new_context() and
    cpu_switch_mm() calls to the post-lock switch hook where the interrupts
    are enabled. Setting the reserved TTBR0 was also moved to
    check_and_switch_context() from cpu_v7_switch_mm().
    
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Tested-by: Marc Zyngier <Marc.Zyngier@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index b8e580a297e4..20b43d6f23b3 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -39,6 +39,8 @@ typedef struct {
  * so enable interrupts over the context switch to avoid high
  * latency.
  */
+#ifndef CONFIG_CPU_HAS_ASID
 #define __ARCH_WANT_INTERRUPTS_ON_CTXSW
+#endif
 
 #endif

commit 9f97da78bf018206fb623cd351d454af2f105fe0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:01 2012 +0100

    Disintegrate asm/system.h for ARM
    
    Disintegrate asm/system.h for ARM.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Russell King <linux@arm.linux.org.uk>
    cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 14965658a923..b8e580a297e4 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -34,4 +34,11 @@ typedef struct {
 
 #endif
 
+/*
+ * switch_mm() may do a full cache flush over the context switch,
+ * so enable interrupts over the context switch to avoid high
+ * latency.
+ */
+#define __ARCH_WANT_INTERRUPTS_ON_CTXSW
+
 #endif

commit bd31b85960a7fcb2d7ede216460b8da71a88411c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 3 08:44:46 2009 -0500

    locking, ARM: Annotate low level hw locks as raw
    
    Annotate the low level hardware locks which must not be preempted.
    
    In mainline this change documents the low level nature of
    the lock - otherwise there's no functional difference. Lockdep
    and Sparse checking will work as usual.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index b4ffe9d5b526..14965658a923 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -6,7 +6,7 @@
 typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
 	unsigned int id;
-	spinlock_t id_lock;
+	raw_spinlock_t id_lock;
 #endif
 	unsigned int kvm_seq;
 } mm_context_t;
@@ -16,7 +16,7 @@ typedef struct {
 
 /* init_mm.context.id_lock should be initialized. */
 #define INIT_MM_CONTEXT(name)                                                 \
-	.context.id_lock    = __SPIN_LOCK_UNLOCKED(name.context.id_lock),
+	.context.id_lock    = __RAW_SPIN_LOCK_UNLOCKED(name.context.id_lock),
 #else
 #define ASID(mm)	(0)
 #endif

commit 28c22d7dc99486ef4186dde41d5260e75b3076f7
Author: MyungJoo Ham <myungjoo.ham@gmail.com>
Date:   Tue Nov 23 11:39:23 2010 +0100

    ARM: 6490/1: MM: bugfix: initialize spinlock for init_mm.context
    
    init_mm used at kernel/sched.c:idle_task_exit() has spin_lock
    (init_mm.context.id_lock) that is not initialized when spin_lock/unlock
    is called at an ARM machine. Note that mm_struct.context.id_lock is
    usually initialized except for the instance of init_mm at
    linux/arch/arm/mm/context.c
    
    Not initializing this spinlock incurs "BUG: pinlock bad magic"
    warning when spinlock debug is enabled. We have observed such
    instances when testing PM in S5PC210 machines.
    
    Signed-off-by: MyungJoo Ham <myungjoo.ham@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 68870c776671..b4ffe9d5b526 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -13,6 +13,10 @@ typedef struct {
 
 #ifdef CONFIG_CPU_HAS_ASID
 #define ASID(mm)	((mm)->context.id & 255)
+
+/* init_mm.context.id_lock should be initialized. */
+#define INIT_MM_CONTEXT(name)                                                 \
+	.context.id_lock    = __SPIN_LOCK_UNLOCKED(name.context.id_lock),
 #else
 #define ASID(mm)	(0)
 #endif

commit 11805bcfa411c816b7c76fc40724be6733c74ffc
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Jan 26 19:09:42 2010 +0100

    ARM: 5905/1: ARM: Global ASID allocation on SMP
    
    The current ASID allocation algorithm doesn't ensure the notification
    of the other CPUs when the ASID rolls over. This may lead to two
    processes using the same ASID (but different generation) or multiple
    threads of the same process using different ASIDs.
    
    This patch adds the broadcasting of the ASID rollover event to the
    other CPUs. To avoid a race on multiple CPUs modifying "cpu_last_asid"
    during the handling of the broadcast, the ASID numbering now starts at
    "smp_processor_id() + 1". At rollover, the cpu_last_asid will be set
    to NR_CPUS.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index b561584d04a1..68870c776671 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -6,6 +6,7 @@
 typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
 	unsigned int id;
+	spinlock_t id_lock;
 #endif
 	unsigned int kvm_seq;
 } mm_context_t;

commit 8feae13110d60cc6287afabc2887366b0eb226c2
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 8 12:04:47 2009 +0000

    NOMMU: Make VMAs per MM as for MMU-mode linux
    
    Make VMAs per mm_struct as for MMU-mode linux.  This solves two problems:
    
     (1) In SYSV SHM where nattch for a segment does not reflect the number of
         shmat's (and forks) done.
    
     (2) In mmap() where the VMA's vm_mm is set to point to the parent mm by an
         exec'ing process when VM_EXECUTABLE is specified, regardless of the fact
         that a VMA might be shared and already have its vm_mm assigned to another
         process or a dead process.
    
    A new struct (vm_region) is introduced to track a mapped region and to remember
    the circumstances under which it may be shared and the vm_list_struct structure
    is discarded as it's no longer required.
    
    This patch makes the following additional changes:
    
     (1) Regions are now allocated with alloc_pages() rather than kmalloc() and
         with no recourse to __GFP_COMP, so the pages are not composite.  Instead,
         each page has a reference on it held by the region.  Anything else that is
         interested in such a page will have to get a reference on it to retain it.
         When the pages are released due to unmapping, each page is passed to
         put_page() and will be freed when the page usage count reaches zero.
    
     (2) Excess pages are trimmed after an allocation as the allocation must be
         made as a power-of-2 quantity of pages.
    
     (3) VMAs are added to the parent MM's R/B tree and mmap lists.  As an MM may
         end up with overlapping VMAs within the tree, the VMA struct address is
         appended to the sort key.
    
     (4) Non-anonymous VMAs are now added to the backing inode's prio list.
    
     (5) Holes may be punched in anonymous VMAs with munmap(), releasing parts of
         the backing region.  The VMA and region structs will be split if
         necessary.
    
     (6) sys_shmdt() only releases one attachment to a SYSV IPC shared memory
         segment instead of all the attachments at that addresss.  Multiple
         shmat()'s return the same address under NOMMU-mode instead of different
         virtual addresses as under MMU-mode.
    
     (7) Core dumping for ELF-FDPIC requires fewer exceptions for NOMMU-mode.
    
     (8) /proc/maps is now the global list of mapped regions, and may list bits
         that aren't actually mapped anywhere.
    
     (9) /proc/meminfo gains a line (tagged "MmapCopy") that indicates the amount
         of RAM currently allocated by mmap to hold mappable regions that can't be
         mapped directly.  These are copies of the backing device or file if not
         anonymous.
    
    These changes make NOMMU mode more similar to MMU mode.  The downside is that
    NOMMU mode requires some extra memory to track things over NOMMU without this
    patch (VMAs are no longer shared, and there are now region structs).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Mike Frysinger <vapier.adi@gmail.com>
    Acked-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 53099d4ee421..b561584d04a1 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -24,7 +24,6 @@ typedef struct {
  *  modified for 2.6 by Hyok S. Choi <hyok.choi@samsung.com>
  */
 typedef struct {
-	struct vm_list_struct	*vmlist;
 	unsigned long		end_brk;
 } mm_context_t;
 

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
new file mode 100644
index 000000000000..53099d4ee421
--- /dev/null
+++ b/arch/arm/include/asm/mmu.h
@@ -0,0 +1,33 @@
+#ifndef __ARM_MMU_H
+#define __ARM_MMU_H
+
+#ifdef CONFIG_MMU
+
+typedef struct {
+#ifdef CONFIG_CPU_HAS_ASID
+	unsigned int id;
+#endif
+	unsigned int kvm_seq;
+} mm_context_t;
+
+#ifdef CONFIG_CPU_HAS_ASID
+#define ASID(mm)	((mm)->context.id & 255)
+#else
+#define ASID(mm)	(0)
+#endif
+
+#else
+
+/*
+ * From nommu.h:
+ *  Copyright (C) 2002, David McCullough <davidm@snapgear.com>
+ *  modified for 2.6 by Hyok S. Choi <hyok.choi@samsung.com>
+ */
+typedef struct {
+	struct vm_list_struct	*vmlist;
+	unsigned long		end_brk;
+} mm_context_t;
+
+#endif
+
+#endif
