commit c62da0c35d58518ddb26ff641d2485596567fd96
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:05 2020 -0700

    mm/vma: define a default value for VM_DATA_DEFAULT_FLAGS
    
    There are many platforms with exact same value for VM_DATA_DEFAULT_FLAGS
    This creates a default value for VM_DATA_DEFAULT_FLAGS in line with the
    existing VM_STACK_DEFAULT_FLAGS.  While here, also define some more
    macros with standard VMA access flag combinations that are used
    frequently across many platforms.  Apart from simplification, this
    reduces code duplication as well.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Link: http://lkml.kernel.org/r/1583391014-8170-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index c2b75cba26df..11b058a72a5b 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -161,9 +161,7 @@ extern int pfn_valid(unsigned long);
 
 #endif /* !__ASSEMBLY__ */
 
-#define VM_DATA_DEFAULT_FLAGS \
-	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
-	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS	VM_DATA_FLAGS_TSK_EXEC
 
 #include <asm-generic/getorder.h>
 

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 4355f0ec44d6..c2b75cba26df 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -1,11 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  *  arch/arm/include/asm/page.h
  *
  *  Copyright (C) 1995-2003 Russell King
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #ifndef _ASMARM_PAGE_H
 #define _ASMARM_PAGE_H

commit a5463cd3435475386cbbe7b06e01292ac169d36f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jul 31 21:58:56 2013 +0100

    ARM: make vectors page inaccessible from userspace
    
    If kuser helpers are not provided by the kernel, disable user access to
    the vectors page.  With the kuser helpers gone, there is no reason for
    this page to be visible to userspace.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 6363f3d1d505..4355f0ec44d6 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -142,7 +142,9 @@ extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);
 
+#ifdef CONFIG_KUSER_HELPERS
 #define __HAVE_ARCH_GATE_AREA 1
+#endif
 
 #ifdef CONFIG_ARM_LPAE
 #include <asm/pgtable-3level-types.h>

commit 926edcc747e2efb3c9add7ed4dbc4e7a3a959d02
Author: Cyril Chemparathy <cyril@ti.com>
Date:   Sun Jul 22 13:40:38 2012 -0400

    ARM: LPAE: use signed arithmetic for mask definitions
    
    This patch applies to PAGE_MASK, PMD_MASK, and PGDIR_MASK, where forcing
    unsigned long math truncates the mask at the 32-bits.  This clearly does bad
    things on PAE systems.
    
    This patch fixes this problem by defining these masks as signed quantities.
    We then rely on sign extension to do the right thing.
    
    Signed-off-by: Cyril Chemparathy <cyril@ti.com>
    Signed-off-by: Vitaly Andrianov <vitalya@ti.com>
    Reviewed-by: Nicolas Pitre <nico@linaro.org>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-by: Subash Patel <subash.rp@samsung.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 812a4944e783..6363f3d1d505 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -13,7 +13,7 @@
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT		12
 #define PAGE_SIZE		(_AC(1,UL) << PAGE_SHIFT)
-#define PAGE_MASK		(~(PAGE_SIZE-1))
+#define PAGE_MASK		(~((1 << PAGE_SHIFT) - 1))
 
 #ifndef __ASSEMBLY__
 

commit a1ce39288e6fbefdd8d607021d02384eb4a20b99
Author: David Howells <dhowells@redhat.com>
Date:   Tue Oct 2 18:01:25 2012 +0100

    UAPI: (Scripted) Convert #include "..." to #include <path/...> in kernel system headers
    
    Convert #include "..." to #include <path/...> in kernel system headers.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index ecf901902e44..812a4944e783 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -19,7 +19,7 @@
 
 #ifndef CONFIG_MMU
 
-#include "page-nommu.h"
+#include <asm/page-nommu.h>
 
 #else
 

commit 357c9c1f07d4546bc3fbc0fd1044d96b114d14ed
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri May 4 12:04:26 2012 +0100

    ARM: Remove support for ARMv3 ARM610 and ARM710 CPUs
    
    This patch removes support for ARMv3 CPUs, which haven't worked properly
    for quite some time (see the FIXME comment in arch/arm/mm/fault.c).  The
    only V3 parts left is the cache model for ARMv3, which is needed for some
    odd reason by ARM740T CPUs, and being able to build with -march=armv3,
    which is required for the RiscPC platform due to its bus structure.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Jean-Christophe PLAGNIOL-VILLARD <plagnioj@jcrosoft.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 5838361c48b3..ecf901902e44 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -34,7 +34,6 @@
  *	processor(s) we're building for.
  *
  *	We have the following to choose from:
- *	  v3		- ARMv3
  *	  v4wt		- ARMv4 with writethrough cache, without minicache
  *	  v4wb		- ARMv4 with writeback cache, without minicache
  *	  v4_mc		- ARMv4 with minicache
@@ -44,14 +43,6 @@
 #undef _USER
 #undef MULTI_USER
 
-#ifdef CONFIG_CPU_COPY_V3
-# ifdef _USER
-#  define MULTI_USER 1
-# else
-#  define _USER v3
-# endif
-#endif
-
 #ifdef CONFIG_CPU_COPY_V4WT
 # ifdef _USER
 #  define MULTI_USER 1

commit f9d4861fc32b995b1616775614459b8f266c803c
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri Jan 20 12:01:13 2012 +0100

    ARM: 7294/1: vectors: use gate_vma for vectors user mapping
    
    The current user mapping for the vectors page is inserted as a `horrible
    hack vma' into each task via arch_setup_additional_pages. This causes
    problems with the MM subsystem and vm_normal_page, as described here:
    
    https://lkml.org/lkml/2012/1/14/55
    
    Following the suggestion from Hugh in the above thread, this patch uses
    the gate_vma for the vectors user mapping, therefore consolidating
    the horrible hack VMAs into one.
    
    Acked-and-Tested-by: Nicolas Pitre <nico@linaro.org>
    
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 97b440c25c58..5838361c48b3 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -151,6 +151,8 @@ extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);
 
+#define __HAVE_ARCH_GATE_AREA 1
+
 #ifdef CONFIG_ARM_LPAE
 #include <asm/pgtable-3level-types.h>
 #else

commit dcfdae04bd92e8a2ea155db0e21e3bddc09e0a89
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Tue Nov 22 17:30:29 2011 +0000

    ARM: LPAE: Introduce the 3-level page table format definitions
    
    This patch introduces the pgtable-3level*.h files with definitions
    specific to the LPAE page table format (3 levels of page tables).
    
    Each table is 4KB and has 512 64-bit entries. An entry can point to a
    40-bit physical address. The young, write and exec software bits share
    the corresponding hardware bits (negated). Other software bits use spare
    bits in the PTE.
    
    The patch also changes some variable types from unsigned long or int to
    pteval_t or pgprot_t.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index ca94653f1ecb..97b440c25c58 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -151,7 +151,11 @@ extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);
 
+#ifdef CONFIG_ARM_LPAE
+#include <asm/pgtable-3level-types.h>
+#else
 #include <asm/pgtable-2level-types.h>
+#endif
 
 #endif /* CONFIG_MMU */
 

commit 17f57211969bddca2e922299a2530b1c65ccabfa
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Sep 5 17:41:02 2011 +0100

    ARM: 7075/1: LPAE: Factor out 2-level page table definitions into separate files
    
    This patch moves page table definitions from asm/page.h, asm/pgtable.h
    and asm/ptgable-hwdef.h into corresponding *-2level* files.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index ac75d0848889..ca94653f1ecb 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -151,47 +151,7 @@ extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);
 
-typedef unsigned long pteval_t;
-
-#undef STRICT_MM_TYPECHECKS
-
-#ifdef STRICT_MM_TYPECHECKS
-/*
- * These are used to make use of C type-checking..
- */
-typedef struct { pteval_t pte; } pte_t;
-typedef struct { unsigned long pmd; } pmd_t;
-typedef struct { unsigned long pgd[2]; } pgd_t;
-typedef struct { unsigned long pgprot; } pgprot_t;
-
-#define pte_val(x)      ((x).pte)
-#define pmd_val(x)      ((x).pmd)
-#define pgd_val(x)	((x).pgd[0])
-#define pgprot_val(x)   ((x).pgprot)
-
-#define __pte(x)        ((pte_t) { (x) } )
-#define __pmd(x)        ((pmd_t) { (x) } )
-#define __pgprot(x)     ((pgprot_t) { (x) } )
-
-#else
-/*
- * .. while these make it easier on the compiler
- */
-typedef pteval_t pte_t;
-typedef unsigned long pmd_t;
-typedef unsigned long pgd_t[2];
-typedef unsigned long pgprot_t;
-
-#define pte_val(x)      (x)
-#define pmd_val(x)      (x)
-#define pgd_val(x)	((x)[0])
-#define pgprot_val(x)   (x)
-
-#define __pte(x)        (x)
-#define __pmd(x)        (x)
-#define __pgprot(x)     (x)
-
-#endif /* STRICT_MM_TYPECHECKS */
+#include <asm/pgtable-2level-types.h>
 
 #endif /* CONFIG_MMU */
 

commit 7b7bf499f79de3f6c85a340c8453a78789523f85
Author: Will Deacon <will.deacon@arm.com>
Date:   Thu May 19 13:21:14 2011 +0100

    ARM: 6913/1: sparsemem: allow pfn_valid to be overridden when using SPARSEMEM
    
    In commit eb33575c ("[ARM] Double check memmap is actually valid with a
    memmap has unexpected holes V2"), a new function, memmap_valid_within,
    was introduced to mmzone.h so that holes in the memmap which pass
    pfn_valid in SPARSEMEM configurations can be detected and avoided.
    
    The fix to this problem checks that the pfn <-> page linkages are
    correct by calculating the page for the pfn and then checking that
    page_to_pfn on that page returns the original pfn. Unfortunately, in
    SPARSEMEM configurations, this results in reading from the page flags to
    determine the correct section. Since the memmap here has been freed,
    junk is read from memory and the check is no longer robust.
    
    In the best case, reading from /proc/pagetypeinfo will give you the
    wrong answer. In the worst case, you get SEGVs, Kernel OOPses and hung
    CPUs. Furthermore, ioremap implementations that use pfn_valid to
    disallow the remapping of normal memory will break.
    
    This patch allows architectures to provide their own pfn_valid function
    instead of using the default implementation used by sparsemem. The
    architecture-specific version is aware of the memmap state and will
    return false when passed a pfn for a freed page within a valid section.
    
    Acked-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Tested-by: H Hartley Sweeten <hsweeten@visionengravers.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index f51a69595f6e..ac75d0848889 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -197,7 +197,7 @@ typedef unsigned long pgprot_t;
 
 typedef struct page *pgtable_t;
 
-#ifndef CONFIG_SPARSEMEM
+#ifdef CONFIG_HAVE_ARCH_PFN_VALID
 extern int pfn_valid(unsigned long);
 #endif
 

commit f6e3354d02aa1f30672e3671098c12cb49c7da25
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Tue Nov 16 00:22:09 2010 +0000

    ARM: pgtable: introduce pteval_t to represent a pte value
    
    This makes everywhere dealing with pte values use the same type.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index a485ac3c8696..f51a69595f6e 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -151,13 +151,15 @@ extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);
 
+typedef unsigned long pteval_t;
+
 #undef STRICT_MM_TYPECHECKS
 
 #ifdef STRICT_MM_TYPECHECKS
 /*
  * These are used to make use of C type-checking..
  */
-typedef struct { unsigned long pte; } pte_t;
+typedef struct { pteval_t pte; } pte_t;
 typedef struct { unsigned long pmd; } pmd_t;
 typedef struct { unsigned long pgd[2]; } pgd_t;
 typedef struct { unsigned long pgprot; } pgprot_t;
@@ -175,7 +177,7 @@ typedef struct { unsigned long pgprot; } pgprot_t;
 /*
  * .. while these make it easier on the compiler
  */
-typedef unsigned long pte_t;
+typedef pteval_t pte_t;
 typedef unsigned long pmd_t;
 typedef unsigned long pgd_t[2];
 typedef unsigned long pgprot_t;

commit f00a75c094c340c4e7435665816c3273c870e849
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Oct 5 15:17:45 2009 +0100

    ARM: Pass VMA to copy_user_highpage() implementations
    
    Our copy_user_highpage() implementations may require cache maintainence.
    Ensure that implementations have all necessary details to perform this
    maintainence.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 3a32af4cce30..a485ac3c8696 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -117,11 +117,12 @@
 #endif
 
 struct page;
+struct vm_area_struct;
 
 struct cpu_user_fns {
 	void (*cpu_clear_user_highpage)(struct page *page, unsigned long vaddr);
 	void (*cpu_copy_user_highpage)(struct page *to, struct page *from,
-			unsigned long vaddr);
+			unsigned long vaddr, struct vm_area_struct *vma);
 };
 
 #ifdef MULTI_USER
@@ -137,7 +138,7 @@ extern struct cpu_user_fns cpu_user;
 
 extern void __cpu_clear_user_highpage(struct page *page, unsigned long vaddr);
 extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
-			unsigned long vaddr);
+			unsigned long vaddr, struct vm_area_struct *vma);
 #endif
 
 #define clear_user_highpage(page,vaddr)		\
@@ -145,7 +146,7 @@ extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 
 #define __HAVE_ARCH_COPY_USER_HIGHPAGE
 #define copy_user_highpage(to,from,vaddr,vma)	\
-	__cpu_copy_user_highpage(to, from, vaddr)
+	__cpu_copy_user_highpage(to, from, vaddr, vma)
 
 #define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);

commit b7cfda9fc3d7aa60cffab5367f2a72a4a70060cd
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Sep 7 15:06:42 2009 +0100

    ARM: Fix pfn_valid() for sparse memory
    
    On OMAP platforms, some people want to declare to segment up the memory
    between the kernel and a separate application such that there is a hole
    in the middle of the memory as far as Linux is concerned.  However,
    they want to be able to mmap() the hole.
    
    This currently causes problems, because update_mmu_cache() thinks that
    there are valid struct pages for the "hole".  Fix this by making
    pfn_valid() slightly more expensive, by checking whether the PFN is
    contained within the meminfo array.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-by: Khasim Syed Mohammed <khasim@ti.com>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 9c746af1bf6e..3a32af4cce30 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -194,6 +194,10 @@ typedef unsigned long pgprot_t;
 
 typedef struct page *pgtable_t;
 
+#ifndef CONFIG_SPARSEMEM
+extern int pfn_valid(unsigned long);
+#endif
+
 #include <asm/memory.h>
 
 #endif /* !__ASSEMBLY__ */

commit f6430a938dc6d77e33722aaf6a58382b3423935d
Author: Linus Walleij <linus.walleij@stericsson.com>
Date:   Wed Jun 24 23:38:56 2009 +0100

    [ARM] 5565/2: Use PAGE_SIZE and RO_DATA() in link script
    
    Update the link script for ARM to use PAGE_SIZE instead of hard-
    coded 4096. Also the old RODATA macro is deprecated
    for the RO_DATA(PAGE_SIZE) macro. As a consequence the PAGE_SIZE
    was changed from (1UL << PAGE_SHIFT) to (_AC(1,UL) << PAGE_SHIFT)
    because the linker does not understand the "UL" suffix to numeric
    constants.
    
    Signed-off-by: Linus Walleij <linus.walleij@stericsson.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index be962c1349c4..9c746af1bf6e 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -12,7 +12,7 @@
 
 /* PAGE_SHIFT determines the page size */
 #define PAGE_SHIFT		12
-#define PAGE_SIZE		(1UL << PAGE_SHIFT)
+#define PAGE_SIZE		(_AC(1,UL) << PAGE_SHIFT)
 #define PAGE_MASK		(~(PAGE_SIZE-1))
 
 #ifndef __ASSEMBLY__

commit 5b17e1cd8928ae65932758ce6478ac6d3e9a86b2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 13 22:56:30 2009 +0000

    asm-generic: rename page.h and uaccess.h
    
    The current asm-generic/page.h only contains the get_order
    function, and asm-generic/uaccess.h only implements
    unaligned accesses. This renames the file to getorder.h
    and uaccess-unaligned.h to make room for new page.h
    and uaccess.h file that will be usable by all simple
    (e.g. nommu) architectures.
    
    Signed-off-by: Remis Lima Baima <remis.developer@googlemail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 7b522770f29d..be962c1349c4 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -202,6 +202,6 @@ typedef struct page *pgtable_t;
 	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
 	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
-#include <asm-generic/page.h>
+#include <asm-generic/getorder.h>
 
 #endif

commit eb5f4ca9536ba297c98721ecbbdf41ec5b987bd5
Author: Martin Fuzzey <mfuzzey@gmail.com>
Date:   Mon Jun 1 09:19:37 2009 +0100

    [ARM] 5534/1: kmalloc must return a cache line aligned buffer
    
    Define ARCH_KMALLOC_MINALIGN in asm/cache.h
    At the request of Russell also move ARCH_SLAB_MINALIGN to this file.
    
    Signed-off-by: Martin Fuzzey <mfuzzey@gmail.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index e6eb8a67b807..7b522770f29d 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -202,13 +202,6 @@ typedef struct page *pgtable_t;
 	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
 	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
-/*
- * With EABI on ARMv5 and above we must have 64-bit aligned slab pointers.
- */
-#if defined(CONFIG_AEABI) && (__LINUX_ARM_ARCH__ >= 5)
-#define ARCH_SLAB_MINALIGN 8
-#endif
-
 #include <asm-generic/page.h>
 
 #endif

commit 28853ac8fe5221de74a14f1182d7b2b383dfd85c
Author: Paulius Zaleckas <paulius.zaleckas@teltonika.lt>
Date:   Wed Mar 25 13:10:01 2009 +0200

    ARM: Add support for FA526 v2
    
    Adds support for Faraday FA526 core. This core is used at least by:
    Cortina Systems Gemini and Centroid family
    Cavium Networks ECONA family
    Grain Media GM8120
    Pixelplus ImageARM
    Prolific PL-1029
    Faraday IP evaluation boards
    
    v2:
    - move TLB_BTB to separate patch
    - update copyrights
    
    Signed-off-by: Paulius Zaleckas <paulius.zaleckas@teltonika.lt>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index f341c9dbd662..e6eb8a67b807 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -76,6 +76,14 @@
 # endif
 #endif
 
+#ifdef CONFIG_CPU_COPY_FA
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER fa
+# endif
+#endif
+
 #ifdef CONFIG_CPU_SA1100
 # ifdef _USER
 #  define MULTI_USER 1

commit 7ef4de17cc55a3c3b8d093743b1e3b845d8eba47
Merge: f412b09f4ed7 b5ee9002583f
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Nov 28 15:39:02 2008 +0000

    Merge branch 'highmem' into devel
    
    Conflicts:
    
            arch/arm/mach-clps7500/include/mach/memory.h

commit 303c6443659bc1dc911356f5de149f48ff1d97b8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 16:32:19 2008 +0000

    [ARM] clearpage: provide our own clear_user_highpage()
    
    For similar reasons as copy_user_page(), we want to avoid the
    additional kmap_atomic if it's unnecessary.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index 1581b8cf8f33..77747df713b4 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -111,7 +111,7 @@
 struct page;
 
 struct cpu_user_fns {
-	void (*cpu_clear_user_page)(void *p, unsigned long user);
+	void (*cpu_clear_user_highpage)(struct page *page, unsigned long vaddr);
 	void (*cpu_copy_user_highpage)(struct page *to, struct page *from,
 			unsigned long vaddr);
 };
@@ -119,20 +119,21 @@ struct cpu_user_fns {
 #ifdef MULTI_USER
 extern struct cpu_user_fns cpu_user;
 
-#define __cpu_clear_user_page		cpu_user.cpu_clear_user_page
+#define __cpu_clear_user_highpage	cpu_user.cpu_clear_user_highpage
 #define __cpu_copy_user_highpage	cpu_user.cpu_copy_user_highpage
 
 #else
 
-#define __cpu_clear_user_page		__glue(_USER,_clear_user_page)
+#define __cpu_clear_user_highpage	__glue(_USER,_clear_user_highpage)
 #define __cpu_copy_user_highpage	__glue(_USER,_copy_user_highpage)
 
-extern void __cpu_clear_user_page(void *p, unsigned long user);
+extern void __cpu_clear_user_highpage(struct page *page, unsigned long vaddr);
 extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
 			unsigned long vaddr);
 #endif
 
-#define clear_user_page(addr,vaddr,pg)	 __cpu_clear_user_page(addr, vaddr)
+#define clear_user_highpage(page,vaddr)		\
+	 __cpu_clear_user_highpage(page, vaddr)
 
 #define __HAVE_ARCH_COPY_USER_HIGHPAGE
 #define copy_user_highpage(to,from,vaddr,vma)	\

commit 063b0a4207e43acbeff3d4b09f43e750e0212b48
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Fri Oct 31 15:08:35 2008 +0000

    [ARM] copypage: provide our own copy_user_highpage()
    
    We used to override the copy_user_page() function.  However, this
    is not only inefficient, it also causes additional complexity for
    highmem support, since we convert from a struct page to a kernel
    direct mapped address and back to a struct page again.
    
    Moreover, with highmem support, we end up pointlessly setting up
    kmap entries for pages which we're going to remap.  So, push the
    kmapping down into the copypage implementation files where it's
    required.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index bed1c0a00368..1581b8cf8f33 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -108,30 +108,35 @@
 #error Unknown user operations model
 #endif
 
+struct page;
+
 struct cpu_user_fns {
 	void (*cpu_clear_user_page)(void *p, unsigned long user);
-	void (*cpu_copy_user_page)(void *to, const void *from,
-				   unsigned long user);
+	void (*cpu_copy_user_highpage)(struct page *to, struct page *from,
+			unsigned long vaddr);
 };
 
 #ifdef MULTI_USER
 extern struct cpu_user_fns cpu_user;
 
-#define __cpu_clear_user_page	cpu_user.cpu_clear_user_page
-#define __cpu_copy_user_page	cpu_user.cpu_copy_user_page
+#define __cpu_clear_user_page		cpu_user.cpu_clear_user_page
+#define __cpu_copy_user_highpage	cpu_user.cpu_copy_user_highpage
 
 #else
 
-#define __cpu_clear_user_page	__glue(_USER,_clear_user_page)
-#define __cpu_copy_user_page	__glue(_USER,_copy_user_page)
+#define __cpu_clear_user_page		__glue(_USER,_clear_user_page)
+#define __cpu_copy_user_highpage	__glue(_USER,_copy_user_highpage)
 
 extern void __cpu_clear_user_page(void *p, unsigned long user);
-extern void __cpu_copy_user_page(void *to, const void *from,
-				 unsigned long user);
+extern void __cpu_copy_user_highpage(struct page *to, struct page *from,
+			unsigned long vaddr);
 #endif
 
 #define clear_user_page(addr,vaddr,pg)	 __cpu_clear_user_page(addr, vaddr)
-#define copy_user_page(to,from,vaddr,pg) __cpu_copy_user_page(to, from, vaddr)
+
+#define __HAVE_ARCH_COPY_USER_HIGHPAGE
+#define copy_user_highpage(to,from,vaddr,vma)	\
+	__cpu_copy_user_highpage(to, from, vaddr)
 
 #define clear_page(page)	memzero((void *)(page), PAGE_SIZE)
 extern void copy_page(void *to, const void *from);

commit 59f0cb0fddc14ffc6676ae62e911f8115ebc8ccf
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Oct 27 11:24:09 2008 +0000

    [ARM] remove memzero()
    
    As suggested by Andrew Morton, remove memzero() - it's not supported
    on other architectures so use of it is a potential build breaking bug.
    Since the compiler optimizes memset(x,0,n) to __memzero() perfectly
    well, we don't miss out on the underlying benefits of memzero().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index bed1c0a00368..5fee45e23038 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -133,7 +133,7 @@ extern void __cpu_copy_user_page(void *to, const void *from,
 #define clear_user_page(addr,vaddr,pg)	 __cpu_clear_user_page(addr, vaddr)
 #define copy_user_page(to,from,vaddr,pg) __cpu_copy_user_page(to, from, vaddr)
 
-#define clear_page(page)	memzero((void *)(page), PAGE_SIZE)
+#define clear_page(page)	memset((void *)(page), 0, PAGE_SIZE)
 extern void copy_page(void *to, const void *from);
 
 #undef STRICT_MM_TYPECHECKS

commit 8ec53663d2698076468b3e1edc4e1b418bd54de3
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Sep 7 17:16:54 2008 +0100

    [ARM] Improve non-executable support
    
    Add support for detecting non-executable stack binaries, and adjust
    permissions to prevent execution from data and stack areas.  Also,
    ensure that READ_IMPLIES_EXEC is enabled for older CPUs where that
    is true, and for any executable-stack binary.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
index cf2e2680daaa..bed1c0a00368 100644
--- a/arch/arm/include/asm/page.h
+++ b/arch/arm/include/asm/page.h
@@ -184,8 +184,9 @@ typedef struct page *pgtable_t;
 
 #endif /* !__ASSEMBLY__ */
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS \
+	(((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
+	 VM_READ | VM_WRITE | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
 /*
  * With EABI on ARMv5 and above we must have 64-bit aligned slab pointers.

commit 4baa9922430662431231ac637adedddbb0cfb2d7
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Aug 2 10:55:55 2008 +0100

    [ARM] move include/asm-arm to arch/arm/include/asm
    
    Move platform independent header files to arch/arm/include/asm, leaving
    those in asm/arch* and asm/plat* alone.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/page.h b/arch/arm/include/asm/page.h
new file mode 100644
index 000000000000..cf2e2680daaa
--- /dev/null
+++ b/arch/arm/include/asm/page.h
@@ -0,0 +1,199 @@
+/*
+ *  arch/arm/include/asm/page.h
+ *
+ *  Copyright (C) 1995-2003 Russell King
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef _ASMARM_PAGE_H
+#define _ASMARM_PAGE_H
+
+/* PAGE_SHIFT determines the page size */
+#define PAGE_SHIFT		12
+#define PAGE_SIZE		(1UL << PAGE_SHIFT)
+#define PAGE_MASK		(~(PAGE_SIZE-1))
+
+#ifndef __ASSEMBLY__
+
+#ifndef CONFIG_MMU
+
+#include "page-nommu.h"
+
+#else
+
+#include <asm/glue.h>
+
+/*
+ *	User Space Model
+ *	================
+ *
+ *	This section selects the correct set of functions for dealing with
+ *	page-based copying and clearing for user space for the particular
+ *	processor(s) we're building for.
+ *
+ *	We have the following to choose from:
+ *	  v3		- ARMv3
+ *	  v4wt		- ARMv4 with writethrough cache, without minicache
+ *	  v4wb		- ARMv4 with writeback cache, without minicache
+ *	  v4_mc		- ARMv4 with minicache
+ *	  xscale	- Xscale
+ *	  xsc3		- XScalev3
+ */
+#undef _USER
+#undef MULTI_USER
+
+#ifdef CONFIG_CPU_COPY_V3
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER v3
+# endif
+#endif
+
+#ifdef CONFIG_CPU_COPY_V4WT
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER v4wt
+# endif
+#endif
+
+#ifdef CONFIG_CPU_COPY_V4WB
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER v4wb
+# endif
+#endif
+
+#ifdef CONFIG_CPU_COPY_FEROCEON
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER feroceon
+# endif
+#endif
+
+#ifdef CONFIG_CPU_SA1100
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER v4_mc
+# endif
+#endif
+
+#ifdef CONFIG_CPU_XSCALE
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER xscale_mc
+# endif
+#endif
+
+#ifdef CONFIG_CPU_XSC3
+# ifdef _USER
+#  define MULTI_USER 1
+# else
+#  define _USER xsc3_mc
+# endif
+#endif
+
+#ifdef CONFIG_CPU_COPY_V6
+# define MULTI_USER 1
+#endif
+
+#if !defined(_USER) && !defined(MULTI_USER)
+#error Unknown user operations model
+#endif
+
+struct cpu_user_fns {
+	void (*cpu_clear_user_page)(void *p, unsigned long user);
+	void (*cpu_copy_user_page)(void *to, const void *from,
+				   unsigned long user);
+};
+
+#ifdef MULTI_USER
+extern struct cpu_user_fns cpu_user;
+
+#define __cpu_clear_user_page	cpu_user.cpu_clear_user_page
+#define __cpu_copy_user_page	cpu_user.cpu_copy_user_page
+
+#else
+
+#define __cpu_clear_user_page	__glue(_USER,_clear_user_page)
+#define __cpu_copy_user_page	__glue(_USER,_copy_user_page)
+
+extern void __cpu_clear_user_page(void *p, unsigned long user);
+extern void __cpu_copy_user_page(void *to, const void *from,
+				 unsigned long user);
+#endif
+
+#define clear_user_page(addr,vaddr,pg)	 __cpu_clear_user_page(addr, vaddr)
+#define copy_user_page(to,from,vaddr,pg) __cpu_copy_user_page(to, from, vaddr)
+
+#define clear_page(page)	memzero((void *)(page), PAGE_SIZE)
+extern void copy_page(void *to, const void *from);
+
+#undef STRICT_MM_TYPECHECKS
+
+#ifdef STRICT_MM_TYPECHECKS
+/*
+ * These are used to make use of C type-checking..
+ */
+typedef struct { unsigned long pte; } pte_t;
+typedef struct { unsigned long pmd; } pmd_t;
+typedef struct { unsigned long pgd[2]; } pgd_t;
+typedef struct { unsigned long pgprot; } pgprot_t;
+
+#define pte_val(x)      ((x).pte)
+#define pmd_val(x)      ((x).pmd)
+#define pgd_val(x)	((x).pgd[0])
+#define pgprot_val(x)   ((x).pgprot)
+
+#define __pte(x)        ((pte_t) { (x) } )
+#define __pmd(x)        ((pmd_t) { (x) } )
+#define __pgprot(x)     ((pgprot_t) { (x) } )
+
+#else
+/*
+ * .. while these make it easier on the compiler
+ */
+typedef unsigned long pte_t;
+typedef unsigned long pmd_t;
+typedef unsigned long pgd_t[2];
+typedef unsigned long pgprot_t;
+
+#define pte_val(x)      (x)
+#define pmd_val(x)      (x)
+#define pgd_val(x)	((x)[0])
+#define pgprot_val(x)   (x)
+
+#define __pte(x)        (x)
+#define __pmd(x)        (x)
+#define __pgprot(x)     (x)
+
+#endif /* STRICT_MM_TYPECHECKS */
+
+#endif /* CONFIG_MMU */
+
+typedef struct page *pgtable_t;
+
+#include <asm/memory.h>
+
+#endif /* !__ASSEMBLY__ */
+
+#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | VM_EXEC | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+/*
+ * With EABI on ARMv5 and above we must have 64-bit aligned slab pointers.
+ */
+#if defined(CONFIG_AEABI) && (__LINUX_ARM_ARCH__ >= 5)
+#define ARCH_SLAB_MINALIGN 8
+#endif
+
+#include <asm-generic/page.h>
+
+#endif
