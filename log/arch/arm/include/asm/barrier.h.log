commit 5388a5b82199facacd3d7ac0d05aca6e8f902fed
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Tue Apr 10 11:35:36 2018 +0100

    ARM: avoid Cortex-A9 livelock on tight dmb loops
    
    machine_crash_nonpanic_core() does this:
    
            while (1)
                    cpu_relax();
    
    because the kernel has crashed, and we have no known safe way to deal
    with the CPU.  So, we place the CPU into an infinite loop which we
    expect it to never exit - at least not until the system as a whole is
    reset by some method.
    
    In the absence of erratum 754327, this code assembles to:
    
            b       .
    
    In other words, an infinite loop.  When erratum 754327 is enabled,
    this becomes:
    
    1:      dmb
            b       1b
    
    It has been observed that on some systems (eg, OMAP4) where, if a
    crash is triggered, the system tries to kexec into the panic kernel,
    but fails after taking the secondary CPU down - placing it into one
    of these loops.  This causes the system to livelock, and the most
    noticable effect is the system stops after issuing:
    
            Loading crashdump kernel...
    
    to the system console.
    
    The tested as working solution I came up with was to add wfe() to
    these infinite loops thusly:
    
            while (1) {
                    cpu_relax();
                    wfe();
            }
    
    which, without 754327 builds to:
    
    1:      wfe
            b       1b
    
    or with 754327 is enabled:
    
    1:      dmb
            wfe
            b       1b
    
    Adding "wfe" does two things depending on the environment we're running
    under:
    - where we're running on bare metal, and the processor implements
      "wfe", it stops us spinning endlessly in a loop where we're never
      going to do any useful work.
    - if we're running in a VM, it allows the CPU to be given back to the
      hypervisor and rescheduled for other purposes (maybe a different VM)
      rather than wasting CPU cycles inside a crashed VM.
    
    However, in light of erratum 794072, Will Deacon wanted to see 10 nops
    as well - which is reasonable to cover the case where we have erratum
    754327 enabled _and_ we have a processor that doesn't implement the
    wfe hint.
    
    So, we now end up with:
    
    1:      wfe
            b       1b
    
    when erratum 754327 is disabled, or:
    
    1:      dmb
            nop
            nop
            nop
            nop
            nop
            nop
            nop
            nop
            nop
            nop
            wfe
            b       1b
    
    when erratum 754327 is enabled.  We also get the dmb + 10 nop
    sequence elsewhere in the kernel, in terminating loops.
    
    This is reasonable - it means we get the workaround for erratum
    794072 when erratum 754327 is enabled, but still relinquish the dead
    processor - either by placing it in a lower power mode when wfe is
    implemented as such or by returning it to the hypervisior, or in the
    case where wfe is a no-op, we use the workaround specified in erratum
    794072 to avoid the problem.
    
    These as two entirely orthogonal problems - the 10 nops addresses
    erratum 794072, and the wfe is an optimisation that makes the system
    more efficient when crashed either in terms of power consumption or
    by allowing the host/other VMs to make use of the CPU.
    
    I don't see any reason not to use kexec() inside a VM - it has the
    potential to provide automated recovery from a failure of the VMs
    kernel with the opportunity for saving a crashdump of the failure.
    A panic() with a reboot timeout won't do that, and reading the
    libvirt documentation, setting on_reboot to "preserve" won't either
    (the documentation states "The preserve action for an on_reboot event
    is treated as a destroy".)  Surely it has to be a good thing to
    avoiding having CPUs spinning inside a VM that is doing no useful
    work.
    
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 69772e742a0a..83ae97c049d9 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -11,6 +11,8 @@
 #define sev()	__asm__ __volatile__ ("sev" : : : "memory")
 #define wfe()	__asm__ __volatile__ ("wfe" : : : "memory")
 #define wfi()	__asm__ __volatile__ ("wfi" : : : "memory")
+#else
+#define wfe()	do { } while (0)
 #endif
 
 #if __LINUX_ARM_ARCH__ >= 7

commit 1d4238c56f9816ce0f9c8dbe42d7f2ad81cb6613
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri May 11 15:06:58 2018 +0100

    ARM: spectre-v1: add array_index_mask_nospec() implementation
    
    Add an implementation of the array_index_mask_nospec() function for
    mitigating Spectre variant 1 throughout the kernel.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Boot-tested-by: Tony Lindgren <tony@atomide.com>
    Reviewed-by: Tony Lindgren <tony@atomide.com>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 3d9c1d4b7e75..69772e742a0a 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -76,6 +76,25 @@ extern void arm_heavy_mb(void);
 #define __smp_rmb()	__smp_mb()
 #define __smp_wmb()	dmb(ishst)
 
+#ifdef CONFIG_CPU_SPECTRE
+static inline unsigned long array_index_mask_nospec(unsigned long idx,
+						    unsigned long sz)
+{
+	unsigned long mask;
+
+	asm volatile(
+		"cmp	%1, %2\n"
+	"	sbc	%0, %1, %1\n"
+	CSDB
+	: "=r" (mask)
+	: "r" (idx), "Ir" (sz)
+	: "cc");
+
+	return mask;
+}
+#define array_index_mask_nospec array_index_mask_nospec
+#endif
+
 #include <asm-generic/barrier.h>
 
 #endif /* !__ASSEMBLY__ */

commit a78d156587931a2c3b354534aa772febf6c9e855
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Fri May 11 11:15:29 2018 +0100

    ARM: spectre-v1: add speculation barrier (csdb) macros
    
    Add assembly and C macros for the new CSDB instruction.
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Boot-tested-by: Tony Lindgren <tony@atomide.com>
    Reviewed-by: Tony Lindgren <tony@atomide.com>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 40f5c410fd8c..3d9c1d4b7e75 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -17,6 +17,12 @@
 #define isb(option) __asm__ __volatile__ ("isb " #option : : : "memory")
 #define dsb(option) __asm__ __volatile__ ("dsb " #option : : : "memory")
 #define dmb(option) __asm__ __volatile__ ("dmb " #option : : : "memory")
+#ifdef CONFIG_THUMB2_KERNEL
+#define CSDB	".inst.w 0xf3af8014"
+#else
+#define CSDB	".inst	0xe320f014"
+#endif
+#define csdb() __asm__ __volatile__(CSDB : : : "memory")
 #elif defined(CONFIG_CPU_XSC3) || __LINUX_ARM_ARCH__ == 6
 #define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
 				    : : "r" (0) : "memory")
@@ -37,6 +43,13 @@
 #define dmb(x) __asm__ __volatile__ ("" : : : "memory")
 #endif
 
+#ifndef CSDB
+#define CSDB
+#endif
+#ifndef csdb
+#define csdb()
+#endif
+
 #ifdef CONFIG_ARM_HEAVY_MB
 extern void (*soc_mb)(void);
 extern void arm_heavy_mb(void);

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index f5d698182d50..40f5c410fd8c 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef __ASM_BARRIER_H
 #define __ASM_BARRIER_H
 

commit 520319de0ced43c21c0aaef3c2392fb607d05419
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Tue Jun 21 02:32:25 2016 +0100

    ARM: 8582/1: remove unused CONFIG_ARCH_HAS_BARRIERS
    
    Since commit 2b749cb3a515 ("ARM: realview: remove private barrier
    implementation"), this config is not used by any platform.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 112cc1a5d47f..f5d698182d50 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -44,9 +44,7 @@ extern void arm_heavy_mb(void);
 #define __arm_heavy_mb(x...) dsb(x)
 #endif
 
-#ifdef CONFIG_ARCH_HAS_BARRIERS
-#include <mach/barriers.h>
-#elif defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
+#if defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
 #define mb()		__arm_heavy_mb()
 #define rmb()		dsb()
 #define wmb()		__arm_heavy_mb(st)

commit 2b1f3de10267dd1034d24f9e77dd5e8f07793925
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Sun Dec 27 15:04:42 2015 +0200

    arm: define __smp_xxx
    
    This defines __smp_xxx barriers for arm,
    for use by virtualization.
    
    smp_xxx barriers are removed as they are
    defined correctly by asm-generic/barriers.h
    
    This reduces the amount of arch-specific boiler-plate code.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 31152e8c7501..112cc1a5d47f 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -60,15 +60,9 @@ extern void arm_heavy_mb(void);
 #define dma_wmb()	barrier()
 #endif
 
-#ifndef CONFIG_SMP
-#define smp_mb()	barrier()
-#define smp_rmb()	barrier()
-#define smp_wmb()	barrier()
-#else
-#define smp_mb()	dmb(ish)
-#define smp_rmb()	smp_mb()
-#define smp_wmb()	dmb(ishst)
-#endif
+#define __smp_mb()	dmb(ish)
+#define __smp_rmb()	__smp_mb()
+#define __smp_wmb()	dmb(ishst)
 
 #include <asm-generic/barrier.h>
 

commit 335390d6096f647311980f50312b304b377e616f
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Mon Dec 21 09:22:18 2015 +0200

    arm: reuse asm-generic/barrier.h
    
    On arm smp_store_mb, read_barrier_depends, smp_read_barrier_depends,
    smp_store_release, smp_load_acquire, smp_mb__before_atomic and
    smp_mb__after_atomic match the asm-generic variants exactly. Drop the
    local definitions and pull in asm-generic/barrier.h instead.
    
    This is in preparation to refactoring this code area.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 3ff5642d9788..31152e8c7501 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -70,28 +70,7 @@ extern void arm_heavy_mb(void);
 #define smp_wmb()	dmb(ishst)
 #endif
 
-#define smp_store_release(p, v)						\
-do {									\
-	compiletime_assert_atomic_type(*p);				\
-	smp_mb();							\
-	WRITE_ONCE(*p, v);						\
-} while (0)
-
-#define smp_load_acquire(p)						\
-({									\
-	typeof(*p) ___p1 = READ_ONCE(*p);				\
-	compiletime_assert_atomic_type(*p);				\
-	smp_mb();							\
-	___p1;								\
-})
-
-#define read_barrier_depends()		do { } while(0)
-#define smp_read_barrier_depends()	do { } while(0)
-
-#define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
-
-#define smp_mb__before_atomic()	smp_mb()
-#define smp_mb__after_atomic()	smp_mb()
+#include <asm-generic/barrier.h>
 
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_BARRIER_H */

commit c706c7eb0d08098f0d768aeef945d7cf1f8858b4
Merge: 79b0691d0c1d 3ff32a0def6e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 3 16:27:01 2015 -0700

    Merge branch 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm
    
    Pull ARM development updates from Russell King:
     "Included in this update:
    
       - moving PSCI code from ARM64/ARM to drivers/
    
       - removal of some architecture internals from global kernel view
    
       - addition of software based "privileged no access" support using the
         old domains register to turn off the ability for kernel
         loads/stores to access userspace.  Only the proper accessors will
         be usable.
    
       - addition of early fixup support for early console
    
       - re-addition (and reimplementation) of OMAP special interconnect
         barrier
    
       - removal of finish_arch_switch()
    
       - only expose cpuX/online in sysfs if hotpluggable
    
       - a number of code cleanups"
    
    * 'for-linus' of git://ftp.arm.linux.org.uk/~rmk/linux-arm: (41 commits)
      ARM: software-based priviledged-no-access support
      ARM: entry: provide uaccess assembly macro hooks
      ARM: entry: get rid of multiple macro definitions
      ARM: 8421/1: smp: Collapse arch_cpu_idle_dead() into cpu_die()
      ARM: uaccess: provide uaccess_save_and_enable() and uaccess_restore()
      ARM: mm: improve do_ldrd_abort macro
      ARM: entry: ensure that IRQs are enabled when calling syscall_trace_exit()
      ARM: entry: efficiency cleanups
      ARM: entry: get rid of asm_trace_hardirqs_on_cond
      ARM: uaccess: simplify user access assembly
      ARM: domains: remove DOMAIN_TABLE
      ARM: domains: keep vectors in separate domain
      ARM: domains: get rid of manager mode for user domain
      ARM: domains: move initial domain setting value to asm/domains.h
      ARM: domains: provide domain_mask()
      ARM: domains: switch to keeping domain value in register
      ARM: 8419/1: dma-mapping: harmonize definition of DMA_ERROR_CODE
      ARM: 8417/1: refactor bitops functions with BIT_MASK() and BIT_WORD()
      ARM: 8416/1: Feroceon: use of_iomap() to map register base
      ARM: 8415/1: early fixmap support for earlycon
      ...

commit 76695af20c015206cffb84b15912be6797d0cca2
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Sun Aug 2 17:11:04 2015 +0200

    locking, arch: use WRITE_ONCE()/READ_ONCE() in smp_store_release()/smp_load_acquire()
    
    Replace ACCESS_ONCE() macro in smp_store_release() and smp_load_acquire()
    with WRITE_ONCE() and READ_ONCE() on x86, arm, arm64, ia64, metag, mips,
    powerpc, s390, sparc and asm-generic since ACCESS_ONCE() does not work
    reliably on non-scalar types.
    
    WRITE_ONCE() and READ_ONCE() were introduced in the following commits:
    
      230fa253df63 ("kernel: Provide READ_ONCE and ASSIGN_ONCE")
      43239cbe79fc ("kernel: Change ASSIGN_ONCE(val, x) to WRITE_ONCE(x, val)")
    
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Acked-by: Ralf Baechle <ralf@linux-mips.org>
    Cc: Alexander Duyck <alexander.h.duyck@redhat.com>
    Cc: Andre Przywara <andre.przywara@arm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arch@vger.kernel.org
    Link: http://lkml.kernel.org/r/1438528264-714-1-git-send-email-andreyknvl@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 6c2327e1c732..70393574e0fa 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -67,12 +67,12 @@
 do {									\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
-	ACCESS_ONCE(*p) = (v);						\
+	WRITE_ONCE(*p, v);						\
 } while (0)
 
 #define smp_load_acquire(p)						\
 ({									\
-	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	typeof(*p) ___p1 = READ_ONCE(*p);				\
 	compiletime_assert_atomic_type(*p);				\
 	smp_mb();							\
 	___p1;								\

commit 4e1f8a6f1d978f033f1751e2887b3a69fab3f878
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Wed Jun 3 13:10:16 2015 +0100

    ARM: add soc memory barrier extension
    
    Add an extension to the heavy barrier code to allow a SoC specific
    memory barrier function to be provided.  This is needed for platforms
    where the interconnect has weak ordering, and thus needs assistance
    to ensure that memory writes are properly visible in the correct order
    to other parts of the system.
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Richard Woodruff <r-woodruff2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index fea99b0e2087..3d8f1d3ad9a7 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -37,6 +37,7 @@
 #endif
 
 #ifdef CONFIG_ARM_HEAVY_MB
+extern void (*soc_mb)(void);
 extern void arm_heavy_mb(void);
 #define __arm_heavy_mb(x...) do { dsb(x); arm_heavy_mb(); } while (0)
 #else

commit f81309067ff2d84788316c513a415f6bb8c9171f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jun 1 23:44:46 2015 +0100

    ARM: move heavy barrier support out of line
    
    The existing memory barrier macro causes a significant amount of code
    to be inserted inline at every call site.  For example, in
    gpio_set_irq_type(), we have this for mb():
    
    c0344c08:       f57ff04e        dsb     st
    c0344c0c:       e59f8190        ldr     r8, [pc, #400]  ; c0344da4 <gpio_set_irq_type+0x230>
    c0344c10:       e3590004        cmp     r9, #4
    c0344c14:       e5983014        ldr     r3, [r8, #20]
    c0344c18:       0a000054        beq     c0344d70 <gpio_set_irq_type+0x1fc>
    c0344c1c:       e3530000        cmp     r3, #0
    c0344c20:       0a000004        beq     c0344c38 <gpio_set_irq_type+0xc4>
    c0344c24:       e50b2030        str     r2, [fp, #-48]  ; 0xffffffd0
    c0344c28:       e50bc034        str     ip, [fp, #-52]  ; 0xffffffcc
    c0344c2c:       e12fff33        blx     r3
    c0344c30:       e51bc034        ldr     ip, [fp, #-52]  ; 0xffffffcc
    c0344c34:       e51b2030        ldr     r2, [fp, #-48]  ; 0xffffffd0
    c0344c38:       e5963004        ldr     r3, [r6, #4]
    
    Moving the outer_cache_sync() call out of line reduces the impact of
    the barrier:
    
    c0344968:       f57ff04e        dsb     st
    c034496c:       e35a0004        cmp     sl, #4
    c0344970:       e50b2030        str     r2, [fp, #-48]  ; 0xffffffd0
    c0344974:       0a000044        beq     c0344a8c <gpio_set_irq_type+0x1b8>
    c0344978:       ebf363dd        bl      c001d8f4 <arm_heavy_mb>
    c034497c:       e5953004        ldr     r3, [r5, #4]
    
    This should reduce the cache footprint of this code.  Overall, this
    results in a reduction of around 20K in the kernel size:
    
        text    data      bss      dec     hex filename
    10773970  667392 10369656 21811018 14ccf4a ../build/imx6/vmlinux-old
    10754219  667392 10369656 21791267 14c8223 ../build/imx6/vmlinux-new
    
    Another advantage to this approach is that we can finally resolve the
    issue of SoCs which have their own memory barrier requirements within
    multiplatform kernels (such as OMAP.)  Here, the bus interconnects
    need additional handling to ensure that writes become visible in the
    correct order (eg, between dma_map() operations, writes to DMA
    coherent memory, and MMIO accesses.)
    
    Acked-by: Tony Lindgren <tony@atomide.com>
    Acked-by: Richard Woodruff <r-woodruff2@ti.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 6c2327e1c732..fea99b0e2087 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -2,7 +2,6 @@
 #define __ASM_BARRIER_H
 
 #ifndef __ASSEMBLY__
-#include <asm/outercache.h>
 
 #define nop() __asm__ __volatile__("mov\tr0,r0\t@ nop\n\t");
 
@@ -37,12 +36,19 @@
 #define dmb(x) __asm__ __volatile__ ("" : : : "memory")
 #endif
 
+#ifdef CONFIG_ARM_HEAVY_MB
+extern void arm_heavy_mb(void);
+#define __arm_heavy_mb(x...) do { dsb(x); arm_heavy_mb(); } while (0)
+#else
+#define __arm_heavy_mb(x...) dsb(x)
+#endif
+
 #ifdef CONFIG_ARCH_HAS_BARRIERS
 #include <mach/barriers.h>
 #elif defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
-#define mb()		do { dsb(); outer_sync(); } while (0)
+#define mb()		__arm_heavy_mb()
 #define rmb()		dsb()
-#define wmb()		do { dsb(st); outer_sync(); } while (0)
+#define wmb()		__arm_heavy_mb(st)
 #define dma_rmb()	dmb(osh)
 #define dma_wmb()	dmb(oshst)
 #else

commit b92b8b35a2e38bde319fd1d68ec84628c1f1b0fb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 12 10:51:55 2015 +0200

    locking/arch: Rename set_mb() to smp_store_mb()
    
    Since set_mb() is really about an smp_mb() -- not a IO/DMA barrier
    like mb() rename it to match the recent smp_load_acquire() and
    smp_store_release().
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 993150aea681..6c2327e1c732 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -81,7 +81,7 @@ do {									\
 #define read_barrier_depends()		do { } while(0)
 #define smp_read_barrier_depends()	do { } while(0)
 
-#define set_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
+#define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
 
 #define smp_mb__before_atomic()	smp_mb()
 #define smp_mb__after_atomic()	smp_mb()

commit ab3f02fc237211f0583c1e7ba3bf504747be9b8d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 12 10:52:27 2015 +0200

    locking/arch: Add WRITE_ONCE() to set_mb()
    
    Since we assume set_mb() to result in a single store followed by a
    full memory barrier, employ WRITE_ONCE().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index d2f81e6b8c1c..993150aea681 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -81,7 +81,7 @@ do {									\
 #define read_barrier_depends()		do { } while(0)
 #define smp_read_barrier_depends()	do { } while(0)
 
-#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)
+#define set_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)
 
 #define smp_mb__before_atomic()	smp_mb()
 #define smp_mb__after_atomic()	smp_mb()

commit 1077fa36f23e259858caf6f269a47393a5aff523
Author: Alexander Duyck <alexander.h.duyck@redhat.com>
Date:   Thu Dec 11 15:02:06 2014 -0800

    arch: Add lightweight memory barriers dma_rmb() and dma_wmb()
    
    There are a number of situations where the mandatory barriers rmb() and
    wmb() are used to order memory/memory operations in the device drivers
    and those barriers are much heavier than they actually need to be.  For
    example in the case of PowerPC wmb() calls the heavy-weight sync
    instruction when for coherent memory operations all that is really needed
    is an lsync or eieio instruction.
    
    This commit adds a coherent only version of the mandatory memory barriers
    rmb() and wmb().  In most cases this should result in the barrier being the
    same as the SMP barriers for the SMP case, however in some cases we use a
    barrier that is somewhere in between rmb() and smp_rmb().  For example on
    ARM the rmb barriers break down as follows:
    
      Barrier   Call     Explanation
      --------- -------- ----------------------------------
      rmb()     dsb()    Data synchronization barrier - system
      dma_rmb() dmb(osh) data memory barrier - outer sharable
      smp_rmb() dmb(ish) data memory barrier - inner sharable
    
    These new barriers are not as safe as the standard rmb() and wmb().
    Specifically they do not guarantee ordering between coherent and incoherent
    memories.  The primary use case for these would be to enforce ordering of
    reads and writes when accessing coherent memory that is shared between the
    CPU and a device.
    
    It may also be noted that there is no dma_mb().  Most architectures don't
    provide a good mechanism for performing a coherent only full barrier without
    resorting to the same mechanism used in mb().  As such there isn't much to
    be gained in trying to define such a function.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: David Miller <davem@davemloft.net>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Alexander Duyck <alexander.h.duyck@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index c6a3e73a6e24..d2f81e6b8c1c 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -43,10 +43,14 @@
 #define mb()		do { dsb(); outer_sync(); } while (0)
 #define rmb()		dsb()
 #define wmb()		do { dsb(st); outer_sync(); } while (0)
+#define dma_rmb()	dmb(osh)
+#define dma_wmb()	dmb(oshst)
 #else
 #define mb()		barrier()
 #define rmb()		barrier()
 #define wmb()		barrier()
+#define dma_rmb()	barrier()
+#define dma_wmb()	barrier()
 #endif
 
 #ifndef CONFIG_SMP

commit 030d0178bdbd237c1f0577f03bbc1d7140a75044
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Mar 12 17:11:00 2014 +0100

    arch,arm: Convert smp_mb__*()
    
    ARM uses ll/sc primitives that do not imply barriers for all regular
    atomic ops, therefore smp_mb__{before,after} need be a full barrier.
    
    Since ARM doesn't use asm-generic/barrier.h include the required
    definitions in its asm/barrier.h
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-yijo7sglsl7uusbp13upcuvo@git.kernel.org
    Cc: Albin Tonnerre <albin.tonnerre@arm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Gang <gang.chen@asianux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicolas Pitre <nico@linaro.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Victor Kamensky <victor.kamensky@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 2f59f7443396..c6a3e73a6e24 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -79,5 +79,8 @@ do {									\
 
 #define set_mb(var, value)	do { var = value; smp_mb(); } while (0)
 
+#define smp_mb__before_atomic()	smp_mb()
+#define smp_mb__after_atomic()	smp_mb()
+
 #endif /* !__ASSEMBLY__ */
 #endif /* __ASM_BARRIER_H */

commit 47933ad41a86a4a9b50bed7c9b9bd2ba242aac63
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Nov 6 14:57:36 2013 +0100

    arch: Introduce smp_load_acquire(), smp_store_release()
    
    A number of situations currently require the heavyweight smp_mb(),
    even though there is no need to order prior stores against later
    loads.  Many architectures have much cheaper ways to handle these
    situations, but the Linux kernel currently has no portable way
    to make use of them.
    
    This commit therefore supplies smp_load_acquire() and
    smp_store_release() to remedy this situation.  The new
    smp_load_acquire() primitive orders the specified load against
    any subsequent reads or writes, while the new smp_store_release()
    primitive orders the specifed store against any prior reads or
    writes.  These primitives allow array-based circular FIFOs to be
    implemented without an smp_mb(), and also allow a theoretical
    hole in rcu_assign_pointer() to be closed at no additional
    expense on most architectures.
    
    In addition, the RCU experience transitioning from explicit
    smp_read_barrier_depends() and smp_wmb() to rcu_dereference()
    and rcu_assign_pointer(), respectively resulted in substantial
    improvements in readability.  It therefore seems likely that
    replacing other explicit barriers with smp_load_acquire() and
    smp_store_release() will provide similar benefits.  It appears
    that roughly half of the explicit barriers in core kernel code
    might be so replaced.
    
    [Changelog by PaulMck]
    
    Reviewed-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Will Deacon <will.deacon@arm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Michael Ellerman <michael@ellerman.id.au>
    Cc: Michael Neuling <mikey@neuling.org>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Victor Kaplansky <VICTORK@il.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Link: http://lkml.kernel.org/r/20131213150640.908486364@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 60f15e274e6d..2f59f7443396 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -59,6 +59,21 @@
 #define smp_wmb()	dmb(ishst)
 #endif
 
+#define smp_store_release(p, v)						\
+do {									\
+	compiletime_assert_atomic_type(*p);				\
+	smp_mb();							\
+	ACCESS_ONCE(*p) = (v);						\
+} while (0)
+
+#define smp_load_acquire(p)						\
+({									\
+	typeof(*p) ___p1 = ACCESS_ONCE(*p);				\
+	compiletime_assert_atomic_type(*p);				\
+	smp_mb();							\
+	___p1;								\
+})
+
 #define read_barrier_depends()		do { } while(0)
 #define smp_read_barrier_depends()	do { } while(0)
 

commit 3ea128065ed20d33bd02ff6dab689f88e38000be
Author: Will Deacon <will.deacon@arm.com>
Date:   Fri May 10 18:07:19 2013 +0100

    ARM: barrier: allow options to be passed to memory barrier instructions
    
    On ARMv7, the memory barrier instructions take an optional `option'
    field which can be used to constrain the effects of a memory barrier
    based on shareability and access type.
    
    This patch allows the caller to pass these options if required, and
    updates the smp_*() barriers to request inner-shareable barriers,
    affecting only stores for the _wmb variant. wmb() is also changed to
    use the -st version of dsb.
    
    Reported-by: Albin Tonnerre <albin.tonnerre@arm.com>
    Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Will Deacon <will.deacon@arm.com>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 8dcd9c702d90..60f15e274e6d 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -14,27 +14,27 @@
 #endif
 
 #if __LINUX_ARM_ARCH__ >= 7
-#define isb() __asm__ __volatile__ ("isb" : : : "memory")
-#define dsb() __asm__ __volatile__ ("dsb" : : : "memory")
-#define dmb() __asm__ __volatile__ ("dmb" : : : "memory")
+#define isb(option) __asm__ __volatile__ ("isb " #option : : : "memory")
+#define dsb(option) __asm__ __volatile__ ("dsb " #option : : : "memory")
+#define dmb(option) __asm__ __volatile__ ("dmb " #option : : : "memory")
 #elif defined(CONFIG_CPU_XSC3) || __LINUX_ARM_ARCH__ == 6
-#define isb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
+#define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
 				    : : "r" (0) : "memory")
-#define dsb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
+#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
 				    : : "r" (0) : "memory")
-#define dmb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 5" \
+#define dmb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 5" \
 				    : : "r" (0) : "memory")
 #elif defined(CONFIG_CPU_FA526)
-#define isb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
+#define isb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
 				    : : "r" (0) : "memory")
-#define dsb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
+#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
 				    : : "r" (0) : "memory")
-#define dmb() __asm__ __volatile__ ("" : : : "memory")
+#define dmb(x) __asm__ __volatile__ ("" : : : "memory")
 #else
-#define isb() __asm__ __volatile__ ("" : : : "memory")
-#define dsb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
+#define isb(x) __asm__ __volatile__ ("" : : : "memory")
+#define dsb(x) __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
 				    : : "r" (0) : "memory")
-#define dmb() __asm__ __volatile__ ("" : : : "memory")
+#define dmb(x) __asm__ __volatile__ ("" : : : "memory")
 #endif
 
 #ifdef CONFIG_ARCH_HAS_BARRIERS
@@ -42,7 +42,7 @@
 #elif defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
 #define mb()		do { dsb(); outer_sync(); } while (0)
 #define rmb()		dsb()
-#define wmb()		mb()
+#define wmb()		do { dsb(st); outer_sync(); } while (0)
 #else
 #define mb()		barrier()
 #define rmb()		barrier()
@@ -54,9 +54,9 @@
 #define smp_rmb()	barrier()
 #define smp_wmb()	barrier()
 #else
-#define smp_mb()	dmb()
-#define smp_rmb()	dmb()
-#define smp_wmb()	dmb()
+#define smp_mb()	dmb(ish)
+#define smp_rmb()	smp_mb()
+#define smp_wmb()	dmb(ishst)
 #endif
 
 #define read_barrier_depends()		do { } while(0)

commit 48aa820f1e3824e46dde6251db98e5961abf605d
Author: Rob Herring <rob.herring@calxeda.com>
Date:   Tue Aug 21 12:26:24 2012 +0200

    ARM: kill off arch_is_coherent
    
    With ixp2xxx removed, there are no platforms that define arch_is_coherent,
    so the last occurrences of arch_is_coherent can be removed. Any new
    platform with coherent i/o should use coherent dma mapping functions.
    
    Signed-off-by: Rob Herring <rob.herring@calxeda.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 05112380dc53..8dcd9c702d90 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -44,10 +44,9 @@
 #define rmb()		dsb()
 #define wmb()		mb()
 #else
-#include <asm/memory.h>
-#define mb()	do { if (arch_is_coherent()) dmb(); else barrier(); } while (0)
-#define rmb()	do { if (arch_is_coherent()) dmb(); else barrier(); } while (0)
-#define wmb()	do { if (arch_is_coherent()) dmb(); else barrier(); } while (0)
+#define mb()		barrier()
+#define rmb()		barrier()
+#define wmb()		barrier()
 #endif
 
 #ifndef CONFIG_SMP

commit b7782d3f08b69bea64ed639284b5be948f63c6df
Author: R Sricharan <r.sricharan@ti.com>
Date:   Fri Mar 30 14:27:43 2012 +0530

    ARM: OMAP2+: Fix omap2+ build error
    
    With CONFIG_OMAP4_ERRATA_I688 enabled, omap2+ build
    was broken as below:
    
    arch/arm/kernel/io.c: In function '_memcpy_toio':
    arch/arm/kernel/io.c:29: error: implicit declaration of function 'outer_sync'
    make[1]: *** [arch/arm/kernel/io.o] Error 1
    
    This was caused by commit 9f97da78 (Disintegrate asm/system.h for ARM).
    
    Signed-off-by: R Sricharan <r.sricharan@ti.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@ti.com>
    [tony@atomide.com: updated comments]
    Signed-off-by: Tony Lindgren <tony@atomide.com>

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
index 44f4a09ff37b..05112380dc53 100644
--- a/arch/arm/include/asm/barrier.h
+++ b/arch/arm/include/asm/barrier.h
@@ -2,6 +2,7 @@
 #define __ASM_BARRIER_H
 
 #ifndef __ASSEMBLY__
+#include <asm/outercache.h>
 
 #define nop() __asm__ __volatile__("mov\tr0,r0\t@ nop\n\t");
 
@@ -39,7 +40,6 @@
 #ifdef CONFIG_ARCH_HAS_BARRIERS
 #include <mach/barriers.h>
 #elif defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
-#include <asm/outercache.h>
 #define mb()		do { dsb(); outer_sync(); } while (0)
 #define rmb()		dsb()
 #define wmb()		mb()

commit 9f97da78bf018206fb623cd351d454af2f105fe0
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:01 2012 +0100

    Disintegrate asm/system.h for ARM
    
    Disintegrate asm/system.h for ARM.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Russell King <linux@arm.linux.org.uk>
    cc: linux-arm-kernel@lists.infradead.org

diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h
new file mode 100644
index 000000000000..44f4a09ff37b
--- /dev/null
+++ b/arch/arm/include/asm/barrier.h
@@ -0,0 +1,69 @@
+#ifndef __ASM_BARRIER_H
+#define __ASM_BARRIER_H
+
+#ifndef __ASSEMBLY__
+
+#define nop() __asm__ __volatile__("mov\tr0,r0\t@ nop\n\t");
+
+#if __LINUX_ARM_ARCH__ >= 7 ||		\
+	(__LINUX_ARM_ARCH__ == 6 && defined(CONFIG_CPU_32v6K))
+#define sev()	__asm__ __volatile__ ("sev" : : : "memory")
+#define wfe()	__asm__ __volatile__ ("wfe" : : : "memory")
+#define wfi()	__asm__ __volatile__ ("wfi" : : : "memory")
+#endif
+
+#if __LINUX_ARM_ARCH__ >= 7
+#define isb() __asm__ __volatile__ ("isb" : : : "memory")
+#define dsb() __asm__ __volatile__ ("dsb" : : : "memory")
+#define dmb() __asm__ __volatile__ ("dmb" : : : "memory")
+#elif defined(CONFIG_CPU_XSC3) || __LINUX_ARM_ARCH__ == 6
+#define isb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
+				    : : "r" (0) : "memory")
+#define dsb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
+				    : : "r" (0) : "memory")
+#define dmb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 5" \
+				    : : "r" (0) : "memory")
+#elif defined(CONFIG_CPU_FA526)
+#define isb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c5, 4" \
+				    : : "r" (0) : "memory")
+#define dsb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
+				    : : "r" (0) : "memory")
+#define dmb() __asm__ __volatile__ ("" : : : "memory")
+#else
+#define isb() __asm__ __volatile__ ("" : : : "memory")
+#define dsb() __asm__ __volatile__ ("mcr p15, 0, %0, c7, c10, 4" \
+				    : : "r" (0) : "memory")
+#define dmb() __asm__ __volatile__ ("" : : : "memory")
+#endif
+
+#ifdef CONFIG_ARCH_HAS_BARRIERS
+#include <mach/barriers.h>
+#elif defined(CONFIG_ARM_DMA_MEM_BUFFERABLE) || defined(CONFIG_SMP)
+#include <asm/outercache.h>
+#define mb()		do { dsb(); outer_sync(); } while (0)
+#define rmb()		dsb()
+#define wmb()		mb()
+#else
+#include <asm/memory.h>
+#define mb()	do { if (arch_is_coherent()) dmb(); else barrier(); } while (0)
+#define rmb()	do { if (arch_is_coherent()) dmb(); else barrier(); } while (0)
+#define wmb()	do { if (arch_is_coherent()) dmb(); else barrier(); } while (0)
+#endif
+
+#ifndef CONFIG_SMP
+#define smp_mb()	barrier()
+#define smp_rmb()	barrier()
+#define smp_wmb()	barrier()
+#else
+#define smp_mb()	dmb()
+#define smp_rmb()	dmb()
+#define smp_wmb()	dmb()
+#endif
+
+#define read_barrier_depends()		do { } while(0)
+#define smp_read_barrier_depends()	do { } while(0)
+
+#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)
+
+#endif /* !__ASSEMBLY__ */
+#endif /* __ASM_BARRIER_H */
