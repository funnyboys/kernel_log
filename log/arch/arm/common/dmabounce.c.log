commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 5ba4622030ca..f4b719bde763 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  *  arch/arm/common/dmabounce.c
  *
@@ -16,10 +17,6 @@
  *
  *  Copyright (C) 2002 Hewlett Packard Company.
  *  Copyright (C) 2004 MontaVista Software, Inc.
- *
- *  This program is free software; you can redistribute it and/or
- *  modify it under the terms of the GNU General Public License
- *  version 2 as published by the Free Software Foundation.
  */
 
 #include <linux/module.h>

commit 72fd97bf4e75e37552640614a0ea98897fc1dd77
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Nov 21 18:57:36 2018 +0100

    arm: remove the mapping_error dma_map_ops method
    
    Arm already returns (~(dma_addr_t)0x0) on mapping failures, so we can
    switch over to returning DMA_MAPPING_ERROR and let the core dma-mapping
    code handle the rest.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 9a92de63426f..5ba4622030ca 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -257,7 +257,7 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 	if (buf == NULL) {
 		dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
 		       __func__, ptr);
-		return ARM_MAPPING_ERROR;
+		return DMA_MAPPING_ERROR;
 	}
 
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
@@ -327,7 +327,7 @@ static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
 
 	ret = needs_bounce(dev, dma_addr, size);
 	if (ret < 0)
-		return ARM_MAPPING_ERROR;
+		return DMA_MAPPING_ERROR;
 
 	if (ret == 0) {
 		arm_dma_ops.sync_single_for_device(dev, dma_addr, size, dir);
@@ -336,7 +336,7 @@ static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
 
 	if (PageHighMem(page)) {
 		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages is not supported\n");
-		return ARM_MAPPING_ERROR;
+		return DMA_MAPPING_ERROR;
 	}
 
 	return map_single(dev, page_address(page) + offset, size, dir, attrs);
@@ -453,11 +453,6 @@ static int dmabounce_dma_supported(struct device *dev, u64 dma_mask)
 	return arm_dma_ops.dma_supported(dev, dma_mask);
 }
 
-static int dmabounce_mapping_error(struct device *dev, dma_addr_t dma_addr)
-{
-	return arm_dma_ops.mapping_error(dev, dma_addr);
-}
-
 static const struct dma_map_ops dmabounce_ops = {
 	.alloc			= arm_dma_alloc,
 	.free			= arm_dma_free,
@@ -472,7 +467,6 @@ static const struct dma_map_ops dmabounce_ops = {
 	.sync_sg_for_cpu	= arm_dma_sync_sg_for_cpu,
 	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
 	.dma_supported		= dmabounce_dma_supported,
-	.mapping_error		= dmabounce_mapping_error,
 };
 
 static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,

commit 64273a61b2e247e29c2647e83edc090fd01a118b
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 11:53:36 2017 +0200

    arm: implement ->dma_supported instead of ->set_dma_mask
    
    Same behavior, less code duplication.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 6ecd5be5d37e..9a92de63426f 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -445,12 +445,12 @@ static void dmabounce_sync_for_device(struct device *dev,
 	arm_dma_ops.sync_single_for_device(dev, handle, size, dir);
 }
 
-static int dmabounce_set_mask(struct device *dev, u64 dma_mask)
+static int dmabounce_dma_supported(struct device *dev, u64 dma_mask)
 {
 	if (dev->archdata.dmabounce)
 		return 0;
 
-	return arm_dma_ops.set_dma_mask(dev, dma_mask);
+	return arm_dma_ops.dma_supported(dev, dma_mask);
 }
 
 static int dmabounce_mapping_error(struct device *dev, dma_addr_t dma_addr)
@@ -471,9 +471,8 @@ static const struct dma_map_ops dmabounce_ops = {
 	.unmap_sg		= arm_dma_unmap_sg,
 	.sync_sg_for_cpu	= arm_dma_sync_sg_for_cpu,
 	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
-	.set_dma_mask		= dmabounce_set_mask,
+	.dma_supported		= dmabounce_dma_supported,
 	.mapping_error		= dmabounce_mapping_error,
-	.dma_supported		= arm_dma_supported,
 };
 
 static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,

commit 418a7a7e4f05f36d6e4ab5b8548ea71f0b602140
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 11:20:18 2017 +0200

    arm: remove arch specific dma_supported implementation
    
    And instead wire it up as method for all the dma_map_ops instances.
    
    Note that the code seems a little fishy for dmabounce and iommu, but
    for now I'd like to preserve the existing behavior 1:1.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 4060378e0f14..6ecd5be5d37e 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -473,6 +473,7 @@ static const struct dma_map_ops dmabounce_ops = {
 	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
 	.set_dma_mask		= dmabounce_set_mask,
 	.mapping_error		= dmabounce_mapping_error,
+	.dma_supported		= arm_dma_supported,
 };
 
 static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,

commit 9eef8b8cc26559fe5f2575daf7d08c6a17e81ff8
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 22 10:53:03 2017 +0200

    arm: implement ->mapping_error
    
    DMA_ERROR_CODE is going to go away, so don't rely on it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 9b1b7be2ec0e..4060378e0f14 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -33,6 +33,7 @@
 #include <linux/scatterlist.h>
 
 #include <asm/cacheflush.h>
+#include <asm/dma-iommu.h>
 
 #undef STATS
 
@@ -256,7 +257,7 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 	if (buf == NULL) {
 		dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
 		       __func__, ptr);
-		return DMA_ERROR_CODE;
+		return ARM_MAPPING_ERROR;
 	}
 
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
@@ -326,7 +327,7 @@ static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
 
 	ret = needs_bounce(dev, dma_addr, size);
 	if (ret < 0)
-		return DMA_ERROR_CODE;
+		return ARM_MAPPING_ERROR;
 
 	if (ret == 0) {
 		arm_dma_ops.sync_single_for_device(dev, dma_addr, size, dir);
@@ -335,7 +336,7 @@ static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
 
 	if (PageHighMem(page)) {
 		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages is not supported\n");
-		return DMA_ERROR_CODE;
+		return ARM_MAPPING_ERROR;
 	}
 
 	return map_single(dev, page_address(page) + offset, size, dir, attrs);
@@ -452,6 +453,11 @@ static int dmabounce_set_mask(struct device *dev, u64 dma_mask)
 	return arm_dma_ops.set_dma_mask(dev, dma_mask);
 }
 
+static int dmabounce_mapping_error(struct device *dev, dma_addr_t dma_addr)
+{
+	return arm_dma_ops.mapping_error(dev, dma_addr);
+}
+
 static const struct dma_map_ops dmabounce_ops = {
 	.alloc			= arm_dma_alloc,
 	.free			= arm_dma_free,
@@ -466,6 +472,7 @@ static const struct dma_map_ops dmabounce_ops = {
 	.sync_sg_for_cpu	= arm_dma_sync_sg_for_cpu,
 	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
 	.set_dma_mask		= dmabounce_set_mask,
+	.mapping_error		= dmabounce_mapping_error,
 };
 
 static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,

commit 5299709d0a87342dadc1fc9850484fadeb488bf8
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:01 2017 -0800

    treewide: Constify most dma_map_ops structures
    
    Most dma_map_ops structures are never modified. Constify these
    structures such that these can be write-protected. This patch
    has been generated as follows:
    
    git grep -l 'struct dma_map_ops' |
      xargs -d\\n sed -i \
        -e 's/struct dma_map_ops/const struct dma_map_ops/g' \
        -e 's/const struct dma_map_ops {/struct dma_map_ops {/g' \
        -e 's/^const struct dma_map_ops;$/struct dma_map_ops;/' \
        -e 's/const const struct dma_map_ops /const struct dma_map_ops /g';
    sed -i -e 's/const \(struct dma_map_ops intel_dma_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops intel_dma_ops');
    sed -i -e 's/const \(struct dma_map_ops dma_iommu_ops\)/\1/' \
      $(git grep -l 'struct dma_map_ops' | grep ^arch/powerpc);
    sed -i -e '/^struct vmd_dev {$/,/^};$/ s/const \(struct dma_map_ops[[:blank:]]dma_ops;\)/\1/' \
           -e '/^static void vmd_setup_dma_ops/,/^}$/ s/const \(struct dma_map_ops \*dest\)/\1/' \
           -e 's/const \(struct dma_map_ops \*dest = \&vmd->dma_ops\)/\1/' \
        drivers/pci/host/*.c
    sed -i -e '/^void __init pci_iommu_alloc(void)$/,/^}$/ s/dma_ops->/intel_dma_ops./' arch/ia64/kernel/pci-dma.c
    sed -i -e 's/static const struct dma_map_ops sn_dma_ops/static struct dma_map_ops sn_dma_ops/' arch/ia64/sn/pci/pci_dma.c
    sed -i -e 's/(const struct dma_map_ops \*)//' drivers/misc/mic/bus/vop_bus.c
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: David Woodhouse <dwmw2@infradead.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: x86@kernel.org
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 75055df1cda3..9b1b7be2ec0e 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -452,7 +452,7 @@ static int dmabounce_set_mask(struct device *dev, u64 dma_mask)
 	return arm_dma_ops.set_dma_mask(dev, dma_mask);
 }
 
-static struct dma_map_ops dmabounce_ops = {
+static const struct dma_map_ops dmabounce_ops = {
 	.alloc			= arm_dma_alloc,
 	.free			= arm_dma_free,
 	.mmap			= arm_dma_mmap,

commit fc1b138de7917d8fda5995aeecdb10c8182d9f75
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Wed Dec 14 15:04:32 2016 -0800

    arch/arm: add option to skip sync on DMA map and unmap
    
    The use of DMA_ATTR_SKIP_CPU_SYNC was not consistent across all of the
    DMA APIs in the arch/arm folder.  This change is meant to correct that
    so that we get consistent behavior.
    
    Link: http://lkml.kernel.org/r/20161110113424.76501.2715.stgit@ahduyck-blue-test.jf.intel.com
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 301281645d08..75055df1cda3 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -243,7 +243,8 @@ static int needs_bounce(struct device *dev, dma_addr_t dma_addr, size_t size)
 }
 
 static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
-		enum dma_data_direction dir)
+				    enum dma_data_direction dir,
+				    unsigned long attrs)
 {
 	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 	struct safe_buffer *buf;
@@ -262,7 +263,8 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
 		buf->safe, buf->safe_dma_addr);
 
-	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) {
+	if ((dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) &&
+	    !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {
 		dev_dbg(dev, "%s: copy unsafe %p to safe %p, size %d\n",
 			__func__, ptr, buf->safe, size);
 		memcpy(buf->safe, ptr, size);
@@ -272,7 +274,8 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 }
 
 static inline void unmap_single(struct device *dev, struct safe_buffer *buf,
-		size_t size, enum dma_data_direction dir)
+				size_t size, enum dma_data_direction dir,
+				unsigned long attrs)
 {
 	BUG_ON(buf->size != size);
 	BUG_ON(buf->direction != dir);
@@ -283,7 +286,8 @@ static inline void unmap_single(struct device *dev, struct safe_buffer *buf,
 
 	DO_STATS(dev->archdata.dmabounce->bounce_count++);
 
-	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
+	if ((dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) &&
+	    !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {
 		void *ptr = buf->ptr;
 
 		dev_dbg(dev, "%s: copy back safe %p to unsafe %p size %d\n",
@@ -334,7 +338,7 @@ static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
 		return DMA_ERROR_CODE;
 	}
 
-	return map_single(dev, page_address(page) + offset, size, dir);
+	return map_single(dev, page_address(page) + offset, size, dir, attrs);
 }
 
 /*
@@ -357,7 +361,7 @@ static void dmabounce_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t
 		return;
 	}
 
-	unmap_single(dev, buf, size, dir);
+	unmap_single(dev, buf, size, dir, attrs);
 }
 
 static int __dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,

commit 00085f1efa387a8ce100e3734920f7639c80caa3
Author: Krzysztof Kozlowski <k.kozlowski@samsung.com>
Date:   Wed Aug 3 13:46:00 2016 -0700

    dma-mapping: use unsigned long for dma_attrs
    
    The dma-mapping core and the implementations do not change the DMA
    attributes passed by pointer.  Thus the pointer can point to const data.
    However the attributes do not have to be a bitfield.  Instead unsigned
    long will do fine:
    
    1. This is just simpler.  Both in terms of reading the code and setting
       attributes.  Instead of initializing local attributes on the stack
       and passing pointer to it to dma_set_attr(), just set the bits.
    
    2. It brings safeness and checking for const correctness because the
       attributes are passed by value.
    
    Semantic patches for this change (at least most of them):
    
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
    
        @@
        f(...,
        - struct dma_attrs *attrs
        + unsigned long attrs
        , ...)
        {
        ...
        }
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    and
    
        // Options: --all-includes
        virtual patch
        virtual context
    
        @r@
        identifier f, attrs;
        type t;
    
        @@
        t f(..., struct dma_attrs *attrs);
    
        @@
        identifier r.f;
        @@
        f(...,
        - NULL
        + 0
         )
    
    Link: http://lkml.kernel.org/r/1468399300-5399-2-git-send-email-k.kozlowski@samsung.com
    Signed-off-by: Krzysztof Kozlowski <k.kozlowski@samsung.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Robin Murphy <robin.murphy@arm.com>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no>
    Acked-by: Mark Salter <msalter@redhat.com> [c6x]
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com> [cris]
    Acked-by: Daniel Vetter <daniel.vetter@ffwll.ch> [drm]
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Fabien Dessenne <fabien.dessenne@st.com> [bdisp]
    Reviewed-by: Marek Szyprowski <m.szyprowski@samsung.com> [vb2-core]
    Acked-by: David Vrabel <david.vrabel@citrix.com> [xen]
    Acked-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com> [xen swiotlb]
    Acked-by: Joerg Roedel <jroedel@suse.de> [iommu]
    Acked-by: Richard Kuo <rkuo@codeaurora.org> [hexagon]
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org> [m68k]
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com> [s390]
    Acked-by: Bjorn Andersson <bjorn.andersson@linaro.org>
    Acked-by: Hans-Christian Noren Egtvedt <egtvedt@samfundet.no> [avr32]
    Acked-by: Vineet Gupta <vgupta@synopsys.com> [arc]
    Acked-by: Robin Murphy <robin.murphy@arm.com> [arm64 and dma-iommu]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 1143c4d5c567..301281645d08 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -310,7 +310,7 @@ static inline void unmap_single(struct device *dev, struct safe_buffer *buf,
  */
 static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir,
-		struct dma_attrs *attrs)
+		unsigned long attrs)
 {
 	dma_addr_t dma_addr;
 	int ret;
@@ -344,7 +344,7 @@ static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
  * should be)
  */
 static void dmabounce_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
-		enum dma_data_direction dir, struct dma_attrs *attrs)
+		enum dma_data_direction dir, unsigned long attrs)
 {
 	struct safe_buffer *buf;
 

commit dc2832e1e7db3f9ad465d2fe894bd69ef05d1e4b
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jun 13 10:01:15 2012 +0200

    ARM: dma-mapping: add support for dma_get_sgtable()
    
    This patch adds support for dma_get_sgtable() function which is required
    to let drivers to share the buffers allocated by DMA-mapping subsystem.
    
    Generic implementation based on virt_to_page() is not suitable for ARM
    dma-mapping subsystem.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: Kyungmin Park <kyungmin.park@samsung.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index aa07f5938f05..1143c4d5c567 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -452,6 +452,7 @@ static struct dma_map_ops dmabounce_ops = {
 	.alloc			= arm_dma_alloc,
 	.free			= arm_dma_free,
 	.mmap			= arm_dma_mmap,
+	.get_sgtable		= arm_dma_get_sgtable,
 	.map_page		= dmabounce_map_page,
 	.unmap_page		= dmabounce_unmap_page,
 	.sync_single_for_cpu	= dmabounce_sync_for_cpu,

commit fdb1117325ad719dc39e81209bc622d511db70e0
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Jun 13 14:04:58 2012 +0200

    ARM: dma-mapping: fix debug messages in dmabounce code
    
    This patch fixes the usage of uninitialized variables in dmabounce code
    intoduced by commit a227fb92 ('ARM: dma-mapping: remove offset parameter
    to prepare for generic dma_ops'):
    arch/arm/common/dmabounce.c: In function ‘dmabounce_sync_for_device’:
    arch/arm/common/dmabounce.c:409: warning: ‘off’ may be used uninitialized in this function
    arch/arm/common/dmabounce.c:407: note: ‘off’ was declared here
    arch/arm/common/dmabounce.c: In function ‘dmabounce_sync_for_cpu’:
    arch/arm/common/dmabounce.c:369: warning: ‘off’ may be used uninitialized in this function
    arch/arm/common/dmabounce.c:367: note: ‘off’ was declared here
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 9d7eb530f95f..aa07f5938f05 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -366,8 +366,8 @@ static int __dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 	struct safe_buffer *buf;
 	unsigned long off;
 
-	dev_dbg(dev, "%s(dma=%#x,off=%#lx,sz=%zx,dir=%x)\n",
-		__func__, addr, off, sz, dir);
+	dev_dbg(dev, "%s(dma=%#x,sz=%zx,dir=%x)\n",
+		__func__, addr, sz, dir);
 
 	buf = find_safe_buffer_dev(dev, addr, __func__);
 	if (!buf)
@@ -377,8 +377,8 @@ static int __dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 
 	BUG_ON(buf->direction != dir);
 
-	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
-		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x off=%#lx) mapped to %p (dma=%#x)\n",
+		__func__, buf->ptr, virt_to_dma(dev, buf->ptr), off,
 		buf->safe, buf->safe_dma_addr);
 
 	DO_STATS(dev->archdata.dmabounce->bounce_count++);
@@ -406,8 +406,8 @@ static int __dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 	struct safe_buffer *buf;
 	unsigned long off;
 
-	dev_dbg(dev, "%s(dma=%#x,off=%#lx,sz=%zx,dir=%x)\n",
-		__func__, addr, off, sz, dir);
+	dev_dbg(dev, "%s(dma=%#x,sz=%zx,dir=%x)\n",
+		__func__, addr, sz, dir);
 
 	buf = find_safe_buffer_dev(dev, addr, __func__);
 	if (!buf)
@@ -417,8 +417,8 @@ static int __dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 
 	BUG_ON(buf->direction != dir);
 
-	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
-		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x off=%#lx) mapped to %p (dma=%#x)\n",
+		__func__, buf->ptr, virt_to_dma(dev, buf->ptr), off,
 		buf->safe, buf->safe_dma_addr);
 
 	DO_STATS(dev->archdata.dmabounce->bounce_count++);

commit f99d60341238fe73fc514129cd9ae4e44e1b2c47
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed May 16 18:31:23 2012 +0200

    ARM: dma-mapping: use alloc, mmap, free from dma_ops
    
    This patch converts dma_alloc/free/mmap_{coherent,writecombine}
    functions to use generic alloc/free/mmap methods from dma_map_ops
    structure. A new DMA_ATTR_WRITE_COMBINE DMA attribute have been
    introduced to implement writecombine methods.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 813c29dc6613..9d7eb530f95f 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -449,6 +449,9 @@ static int dmabounce_set_mask(struct device *dev, u64 dma_mask)
 }
 
 static struct dma_map_ops dmabounce_ops = {
+	.alloc			= arm_dma_alloc,
+	.free			= arm_dma_free,
+	.mmap			= arm_dma_mmap,
 	.map_page		= dmabounce_map_page,
 	.unmap_page		= dmabounce_unmap_page,
 	.sync_single_for_cpu	= dmabounce_sync_for_cpu,

commit 15237e1f505b3e5c2276f240b01cd2133e110cbc
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 10 19:55:20 2012 +0100

    ARM: dma-mapping: move all dma bounce code to separate dma ops structure
    
    This patch removes dma bounce hooks from the common dma mapping
    implementation on ARM architecture and creates a separate set of
    dma_map_ops for dma bounce devices.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 32e9cc6ca7d9..813c29dc6613 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -308,8 +308,9 @@ static inline void unmap_single(struct device *dev, struct safe_buffer *buf,
  * substitute the safe buffer for the unsafe one.
  * (basically move the buffer from an unsafe area to a safe one)
  */
-dma_addr_t __dma_map_page(struct device *dev, struct page *page,
-		unsigned long offset, size_t size, enum dma_data_direction dir)
+static dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,
+		unsigned long offset, size_t size, enum dma_data_direction dir,
+		struct dma_attrs *attrs)
 {
 	dma_addr_t dma_addr;
 	int ret;
@@ -324,7 +325,7 @@ dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 		return DMA_ERROR_CODE;
 
 	if (ret == 0) {
-		__dma_page_cpu_to_dev(page, offset, size, dir);
+		arm_dma_ops.sync_single_for_device(dev, dma_addr, size, dir);
 		return dma_addr;
 	}
 
@@ -335,7 +336,6 @@ dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 
 	return map_single(dev, page_address(page) + offset, size, dir);
 }
-EXPORT_SYMBOL(__dma_map_page);
 
 /*
  * see if a mapped address was really a "safe" buffer and if so, copy
@@ -343,8 +343,8 @@ EXPORT_SYMBOL(__dma_map_page);
  * the safe buffer.  (basically return things back to the way they
  * should be)
  */
-void __dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
-		enum dma_data_direction dir)
+static void dmabounce_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
+		enum dma_data_direction dir, struct dma_attrs *attrs)
 {
 	struct safe_buffer *buf;
 
@@ -353,16 +353,14 @@ void __dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 
 	buf = find_safe_buffer_dev(dev, dma_addr, __func__);
 	if (!buf) {
-		__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, dma_addr)),
-			dma_addr & ~PAGE_MASK, size, dir);
+		arm_dma_ops.sync_single_for_cpu(dev, dma_addr, size, dir);
 		return;
 	}
 
 	unmap_single(dev, buf, size, dir);
 }
-EXPORT_SYMBOL(__dma_unmap_page);
 
-int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
+static int __dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 		size_t sz, enum dma_data_direction dir)
 {
 	struct safe_buffer *buf;
@@ -392,9 +390,17 @@ int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 	}
 	return 0;
 }
-EXPORT_SYMBOL(dmabounce_sync_for_cpu);
 
-int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
+static void dmabounce_sync_for_cpu(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	if (!__dmabounce_sync_for_cpu(dev, handle, size, dir))
+		return;
+
+	arm_dma_ops.sync_single_for_cpu(dev, handle, size, dir);
+}
+
+static int __dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 		size_t sz, enum dma_data_direction dir)
 {
 	struct safe_buffer *buf;
@@ -424,7 +430,35 @@ int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 	}
 	return 0;
 }
-EXPORT_SYMBOL(dmabounce_sync_for_device);
+
+static void dmabounce_sync_for_device(struct device *dev,
+		dma_addr_t handle, size_t size, enum dma_data_direction dir)
+{
+	if (!__dmabounce_sync_for_device(dev, handle, size, dir))
+		return;
+
+	arm_dma_ops.sync_single_for_device(dev, handle, size, dir);
+}
+
+static int dmabounce_set_mask(struct device *dev, u64 dma_mask)
+{
+	if (dev->archdata.dmabounce)
+		return 0;
+
+	return arm_dma_ops.set_dma_mask(dev, dma_mask);
+}
+
+static struct dma_map_ops dmabounce_ops = {
+	.map_page		= dmabounce_map_page,
+	.unmap_page		= dmabounce_unmap_page,
+	.sync_single_for_cpu	= dmabounce_sync_for_cpu,
+	.sync_single_for_device	= dmabounce_sync_for_device,
+	.map_sg			= arm_dma_map_sg,
+	.unmap_sg		= arm_dma_unmap_sg,
+	.sync_sg_for_cpu	= arm_dma_sync_sg_for_cpu,
+	.sync_sg_for_device	= arm_dma_sync_sg_for_device,
+	.set_dma_mask		= dmabounce_set_mask,
+};
 
 static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,
 		const char *name, unsigned long size)
@@ -486,6 +520,7 @@ int dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 #endif
 
 	dev->archdata.dmabounce = device_info;
+	set_dma_ops(dev, &dmabounce_ops);
 
 	dev_info(dev, "dmabounce: registered device\n");
 
@@ -504,6 +539,7 @@ void dmabounce_unregister_dev(struct device *dev)
 	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 
 	dev->archdata.dmabounce = NULL;
+	set_dma_ops(dev, NULL);
 
 	if (!device_info) {
 		dev_warn(dev,

commit a227fb92a0f5f0dd8282719386e9b3a29f0d16b2
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Fri Feb 10 19:55:20 2012 +0100

    ARM: dma-mapping: remove offset parameter to prepare for generic dma_ops
    
    This patch removes the need for the offset parameter in dma bounce
    functions. This is required to let dma-mapping framework on ARM
    architecture to use common, generic dma_map_ops based dma-mapping
    helpers.
    
    Background and more detailed explaination:
    
    dma_*_range_* functions are available from the early days of the dma
    mapping api. They are the correct way of doing a partial syncs on the
    buffer (usually used by the network device drivers). This patch changes
    only the internal implementation of the dma bounce functions to let
    them tunnel through dma_map_ops structure. The driver api stays
    unchanged, so driver are obliged to call dma_*_range_* functions to
    keep code clean and easy to understand.
    
    The only drawback from this patch is reduced detection of the dma api
    abuse. Let us consider the following code:
    
    dma_addr = dma_map_single(dev, ptr, 64, DMA_TO_DEVICE);
    dma_sync_single_range_for_cpu(dev, dma_addr+16, 0, 32, DMA_TO_DEVICE);
    
    Without the patch such code fails, because dma bounce code is unable
    to find the bounce buffer for the given dma_address. After the patch
    the above sync call will be equivalent to:
    
    dma_sync_single_range_for_cpu(dev, dma_addr, 16, 32, DMA_TO_DEVICE);
    
    which succeeds.
    
    I don't consider this as a real problem, because DMA API abuse should be
    caught by debug_dma_* function family. This patch lets us to simplify
    the internal low-level implementation without chaning the driver visible
    API.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 210ad1bef7e5..32e9cc6ca7d9 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -173,7 +173,8 @@ find_safe_buffer(struct dmabounce_device_info *device_info, dma_addr_t safe_dma_
 	read_lock_irqsave(&device_info->lock, flags);
 
 	list_for_each_entry(b, &device_info->safe_buffers, node)
-		if (b->safe_dma_addr == safe_dma_addr) {
+		if (b->safe_dma_addr <= safe_dma_addr &&
+		    b->safe_dma_addr + b->size > safe_dma_addr) {
 			rb = b;
 			break;
 		}
@@ -362,9 +363,10 @@ void __dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 EXPORT_SYMBOL(__dma_unmap_page);
 
 int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
-		unsigned long off, size_t sz, enum dma_data_direction dir)
+		size_t sz, enum dma_data_direction dir)
 {
 	struct safe_buffer *buf;
+	unsigned long off;
 
 	dev_dbg(dev, "%s(dma=%#x,off=%#lx,sz=%zx,dir=%x)\n",
 		__func__, addr, off, sz, dir);
@@ -373,6 +375,8 @@ int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 	if (!buf)
 		return 1;
 
+	off = addr - buf->safe_dma_addr;
+
 	BUG_ON(buf->direction != dir);
 
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
@@ -391,9 +395,10 @@ int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 EXPORT_SYMBOL(dmabounce_sync_for_cpu);
 
 int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
-		unsigned long off, size_t sz, enum dma_data_direction dir)
+		size_t sz, enum dma_data_direction dir)
 {
 	struct safe_buffer *buf;
+	unsigned long off;
 
 	dev_dbg(dev, "%s(dma=%#x,off=%#lx,sz=%zx,dir=%x)\n",
 		__func__, addr, off, sz, dir);
@@ -402,6 +407,8 @@ int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 	if (!buf)
 		return 1;
 
+	off = addr - buf->safe_dma_addr;
+
 	BUG_ON(buf->direction != dir);
 
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",

commit 553ac78877242b6d8b591323731df304140d0f99
Author: Marek Szyprowski <m.szyprowski@samsung.com>
Date:   Wed Feb 29 14:45:28 2012 +0100

    ARM: dma-mapping: introduce DMA_ERROR_CODE constant
    
    Replace all uses of ~0 with DMA_ERROR_CODE, what should make the code
    easier to read.
    
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: Kyungmin Park <kyungmin.park@samsung.com>
    Tested-By: Subash Patel <subash.ramaswamy@linaro.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 595ecd290ebf..210ad1bef7e5 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -254,7 +254,7 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 	if (buf == NULL) {
 		dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
 		       __func__, ptr);
-		return ~0;
+		return DMA_ERROR_CODE;
 	}
 
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
@@ -320,7 +320,7 @@ dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 
 	ret = needs_bounce(dev, dma_addr, size);
 	if (ret < 0)
-		return ~0;
+		return DMA_ERROR_CODE;
 
 	if (ret == 0) {
 		__dma_page_cpu_to_dev(page, offset, size, dir);
@@ -329,7 +329,7 @@ dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 
 	if (PageHighMem(page)) {
 		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages is not supported\n");
-		return ~0;
+		return DMA_ERROR_CODE;
 	}
 
 	return map_single(dev, page_address(page) + offset, size, dir);

commit 0703ed2a6b260cd743adf49a8281eb064d728832
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jul 4 08:32:21 2011 +0100

    ARM: dmabounce: get rid of dma_needs_bounce global function
    
    Pass the device type specific needs_bounce function in at dmabounce
    register time, avoiding the need for a platform specific global
    function to do this.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 4f13505ac936..595ecd290ebf 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -79,6 +79,8 @@ struct dmabounce_device_info {
 	struct dmabounce_pool	large;
 
 	rwlock_t lock;
+
+	int (*needs_bounce)(struct device *, dma_addr_t, size_t);
 };
 
 #ifdef STATS
@@ -236,7 +238,7 @@ static int needs_bounce(struct device *dev, dma_addr_t dma_addr, size_t size)
 			return 1;
 	}
 
-	return dma_needs_bounce(dev, dma_addr, size) ? 1 : 0;
+	return !!dev->archdata.dmabounce->needs_bounce(dev, dma_addr, size);
 }
 
 static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
@@ -430,7 +432,8 @@ static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,
 }
 
 int dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
-		unsigned long large_buffer_size)
+		unsigned long large_buffer_size,
+		int (*needs_bounce_fn)(struct device *, dma_addr_t, size_t))
 {
 	struct dmabounce_device_info *device_info;
 	int ret;
@@ -466,6 +469,7 @@ int dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 	device_info->dev = dev;
 	INIT_LIST_HEAD(&device_info->safe_buffers);
 	rwlock_init(&device_info->lock);
+	device_info->needs_bounce = needs_bounce_fn;
 
 #ifdef STATS
 	device_info->total_allocs = 0;

commit c289b2e0ccff1142908e20398930dc2e14697e74
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 23:56:17 2011 +0100

    ARM: dmabounce: correct unmap_single dev_dbg
    
    DMA addresses should not be casted to void * for printing.  Fix
    that to be consistent with the rest of the file.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index b4a8759e0fa4..4f13505ac936 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -345,8 +345,8 @@ void __dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 {
 	struct safe_buffer *buf;
 
-	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
-		__func__, (void *) dma_addr, size, dir);
+	dev_dbg(dev, "%s(dma=%#x,size=%d,dir=%x)\n",
+		__func__, dma_addr, size, dir);
 
 	buf = find_safe_buffer_dev(dev, dma_addr, __func__);
 	if (!buf) {

commit dfa322fceb15227c07670c415e835d2dfa8a4307
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 23:54:34 2011 +0100

    ARM: dmabounce: check pointer against NULL not 0
    
    Pointers should be checked against NULL rather than 0, otherwise we
    get sparse warnings.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 0077c1b7d7ff..b4a8759e0fa4 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -249,7 +249,7 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 		DO_STATS ( device_info->map_op_count++ );
 
 	buf = alloc_safe_buffer(device_info, ptr, size, dir);
-	if (buf == 0) {
+	if (buf == NULL) {
 		dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
 		       __func__, ptr);
 		return ~0;

commit e2f521e247576c897e1bf0ada801e87c7e2db89f
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 23:53:13 2011 +0100

    ARM: dmabounce: remove useless pr_err
    
    We already check that dev != NULL, so this won't be reached.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 6ae292cc43bf..0077c1b7d7ff 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -210,10 +210,7 @@ static struct safe_buffer *find_safe_buffer_dev(struct device *dev,
 	if (!dev || !dev->archdata.dmabounce)
 		return NULL;
 	if (dma_mapping_error(dev, dma_addr)) {
-		if (dev)
-			dev_err(dev, "Trying to %s invalid mapping\n", where);
-		else
-			pr_err("unknown device: Trying to %s invalid mapping\n", where);
+		dev_err(dev, "Trying to %s invalid mapping\n", where);
 		return NULL;
 	}
 	return find_safe_buffer(dev->archdata.dmabounce, dma_addr);

commit dd3641fc3cf6d10b7cf4e266c2f651517779e727
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 22:39:43 2011 +0100

    ARM: dmabounce: move decision for bouncing into __dma_map_page()
    
    Move the decision whether to bounce into __dma_map_page(), before
    the check for high pages.  This avoids triggering the high page
    check for devices which aren't using dmabounce.  Fix the unmap path
    to cope too.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 643e1d660677..6ae292cc43bf 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -246,88 +246,58 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
 {
 	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
-	dma_addr_t dma_addr;
-	int ret;
+	struct safe_buffer *buf;
 
 	if (device_info)
 		DO_STATS ( device_info->map_op_count++ );
 
-	dma_addr = virt_to_dma(dev, ptr);
-
-	ret = needs_bounce(dev, dma_addr, size);
-	if (ret < 0)
+	buf = alloc_safe_buffer(device_info, ptr, size, dir);
+	if (buf == 0) {
+		dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
+		       __func__, ptr);
 		return ~0;
+	}
 
-	if (ret > 0) {
-		struct safe_buffer *buf;
-
-		buf = alloc_safe_buffer(device_info, ptr, size, dir);
-		if (buf == 0) {
-			dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
-			       __func__, ptr);
-			return ~0;
-		}
-
-		dev_dbg(dev,
-			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
-			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
-			buf->safe, buf->safe_dma_addr);
-
-		if ((dir == DMA_TO_DEVICE) ||
-		    (dir == DMA_BIDIRECTIONAL)) {
-			dev_dbg(dev, "%s: copy unsafe %p to safe %p, size %d\n",
-				__func__, ptr, buf->safe, size);
-			memcpy(buf->safe, ptr, size);
-		}
-		ptr = buf->safe;
+	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+		buf->safe, buf->safe_dma_addr);
 
-		dma_addr = buf->safe_dma_addr;
-	} else {
-		/*
-		 * We don't need to sync the DMA buffer since
-		 * it was allocated via the coherent allocators.
-		 */
-		__dma_single_cpu_to_dev(ptr, size, dir);
+	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) {
+		dev_dbg(dev, "%s: copy unsafe %p to safe %p, size %d\n",
+			__func__, ptr, buf->safe, size);
+		memcpy(buf->safe, ptr, size);
 	}
 
-	return dma_addr;
+	return buf->safe_dma_addr;
 }
 
-static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
+static inline void unmap_single(struct device *dev, struct safe_buffer *buf,
 		size_t size, enum dma_data_direction dir)
 {
-	struct safe_buffer *buf = find_safe_buffer_dev(dev, dma_addr, "unmap");
-
-	if (buf) {
-		BUG_ON(buf->size != size);
-		BUG_ON(buf->direction != dir);
+	BUG_ON(buf->size != size);
+	BUG_ON(buf->direction != dir);
 
-		dev_dbg(dev,
-			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
-			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
-			buf->safe, buf->safe_dma_addr);
+	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+		buf->safe, buf->safe_dma_addr);
 
-		DO_STATS(dev->archdata.dmabounce->bounce_count++);
+	DO_STATS(dev->archdata.dmabounce->bounce_count++);
 
-		if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
-			void *ptr = buf->ptr;
+	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
+		void *ptr = buf->ptr;
 
-			dev_dbg(dev,
-				"%s: copy back safe %p to unsafe %p size %d\n",
-				__func__, buf->safe, ptr, size);
-			memcpy(ptr, buf->safe, size);
+		dev_dbg(dev, "%s: copy back safe %p to unsafe %p size %d\n",
+			__func__, buf->safe, ptr, size);
+		memcpy(ptr, buf->safe, size);
 
-			/*
-			 * Since we may have written to a page cache page,
-			 * we need to ensure that the data will be coherent
-			 * with user mappings.
-			 */
-			__cpuc_flush_dcache_area(ptr, size);
-		}
-		free_safe_buffer(dev->archdata.dmabounce, buf);
-	} else {
-		__dma_single_dev_to_cpu(dma_to_virt(dev, dma_addr), size, dir);
+		/*
+		 * Since we may have written to a page cache page,
+		 * we need to ensure that the data will be coherent
+		 * with user mappings.
+		 */
+		__cpuc_flush_dcache_area(ptr, size);
 	}
+	free_safe_buffer(dev->archdata.dmabounce, buf);
 }
 
 /* ************************************************** */
@@ -341,12 +311,25 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
 dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir)
 {
+	dma_addr_t dma_addr;
+	int ret;
+
 	dev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",
 		__func__, page, offset, size, dir);
 
+	dma_addr = pfn_to_dma(dev, page_to_pfn(page)) + offset;
+
+	ret = needs_bounce(dev, dma_addr, size);
+	if (ret < 0)
+		return ~0;
+
+	if (ret == 0) {
+		__dma_page_cpu_to_dev(page, offset, size, dir);
+		return dma_addr;
+	}
+
 	if (PageHighMem(page)) {
-		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages "
-			     "is not supported\n");
+		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages is not supported\n");
 		return ~0;
 	}
 
@@ -363,10 +346,19 @@ EXPORT_SYMBOL(__dma_map_page);
 void __dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
+	struct safe_buffer *buf;
+
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
 
-	unmap_single(dev, dma_addr, size, dir);
+	buf = find_safe_buffer_dev(dev, dma_addr, __func__);
+	if (!buf) {
+		__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, dma_addr)),
+			dma_addr & ~PAGE_MASK, size, dir);
+		return;
+	}
+
+	unmap_single(dev, buf, size, dir);
 }
 EXPORT_SYMBOL(__dma_unmap_page);
 

commit 23bc9873ba60ee661d8e9f3a6b22fc3bcc4b7015
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 22:28:32 2011 +0100

    ARM: dmabounce: separate out decision to bounce
    
    Move the decision to perform DMA bouncing out of map_single() into its
    own stand-alone function.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 3e0fa1548582..643e1d660677 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -219,36 +219,46 @@ static struct safe_buffer *find_safe_buffer_dev(struct device *dev,
 	return find_safe_buffer(dev->archdata.dmabounce, dma_addr);
 }
 
-static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
-		enum dma_data_direction dir)
+static int needs_bounce(struct device *dev, dma_addr_t dma_addr, size_t size)
 {
-	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
-	dma_addr_t dma_addr;
-	int needs_bounce = 0;
-
-	if (device_info)
-		DO_STATS ( device_info->map_op_count++ );
-
-	dma_addr = virt_to_dma(dev, ptr);
+	if (!dev || !dev->archdata.dmabounce)
+		return 0;
 
 	if (dev->dma_mask) {
-		unsigned long mask = *dev->dma_mask;
-		unsigned long limit;
+		unsigned long limit, mask = *dev->dma_mask;
 
 		limit = (mask + 1) & ~mask;
 		if (limit && size > limit) {
 			dev_err(dev, "DMA mapping too big (requested %#x "
 				"mask %#Lx)\n", size, *dev->dma_mask);
-			return ~0;
+			return -E2BIG;
 		}
 
-		/*
-		 * Figure out if we need to bounce from the DMA mask.
-		 */
-		needs_bounce = (dma_addr | (dma_addr + size - 1)) & ~mask;
+		/* Figure out if we need to bounce from the DMA mask. */
+		if ((dma_addr | (dma_addr + size - 1)) & ~mask)
+			return 1;
 	}
 
-	if (device_info && (needs_bounce || dma_needs_bounce(dev, dma_addr, size))) {
+	return dma_needs_bounce(dev, dma_addr, size) ? 1 : 0;
+}
+
+static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
+		enum dma_data_direction dir)
+{
+	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
+	dma_addr_t dma_addr;
+	int ret;
+
+	if (device_info)
+		DO_STATS ( device_info->map_op_count++ );
+
+	dma_addr = virt_to_dma(dev, ptr);
+
+	ret = needs_bounce(dev, dma_addr, size);
+	if (ret < 0)
+		return ~0;
+
+	if (ret > 0) {
 		struct safe_buffer *buf;
 
 		buf = alloc_safe_buffer(device_info, ptr, size, dir);

commit 71695dd8b9eacfcda1b548a5b1780d34213ad654
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 19:46:39 2011 +0100

    ARM: dmabounce: avoid needless valid_dma_direction() check
    
    This check is done at the DMA API level, so there's no point repeating
    it here.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 8a0588b007e4..3e0fa1548582 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -334,8 +334,6 @@ dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 	dev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",
 		__func__, page, offset, size, dir);
 
-	BUG_ON(!valid_dma_direction(dir));
-
 	if (PageHighMem(page)) {
 		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages "
 			     "is not supported\n");

commit 8021a4a048a85906302bd0236f3d125473be65b1
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 16:13:58 2011 +0100

    ARM: dma-mapping: define dma_(un)?map_single in terms of dma_(un)?map_page
    
    Use dma_map_page()/dma_unmap_page() internals to handle dma_map_single()
    and dma_unmap_single().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 841df7d21c2f..8a0588b007e4 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -328,34 +328,6 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
  * substitute the safe buffer for the unsafe one.
  * (basically move the buffer from an unsafe area to a safe one)
  */
-dma_addr_t __dma_map_single(struct device *dev, void *ptr, size_t size,
-		enum dma_data_direction dir)
-{
-	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
-		__func__, ptr, size, dir);
-
-	BUG_ON(!valid_dma_direction(dir));
-
-	return map_single(dev, ptr, size, dir);
-}
-EXPORT_SYMBOL(__dma_map_single);
-
-/*
- * see if a mapped address was really a "safe" buffer and if so, copy
- * the data from the safe buffer back to the unsafe buffer and free up
- * the safe buffer.  (basically return things back to the way they
- * should be)
- */
-void __dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
-		enum dma_data_direction dir)
-{
-	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
-		__func__, (void *) dma_addr, size, dir);
-
-	unmap_single(dev, dma_addr, size, dir);
-}
-EXPORT_SYMBOL(__dma_unmap_single);
-
 dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir)
 {

commit 178783622ce0fd629fad21b33b8f8f56b64c5e45
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sun Jul 3 23:24:21 2011 +0100

    ARM: dmabounce: fix map_single() error return value
    
    When map_single() is unable to obtain a safe buffer, we must return
    the dma_addr_t error value, which is ~0 rather than 0.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index e5681636626f..841df7d21c2f 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -255,7 +255,7 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 		if (buf == 0) {
 			dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
 			       __func__, ptr);
-			return 0;
+			return ~0;
 		}
 
 		dev_dbg(dev,

commit 24056f525051a9e186af28904b396320e18bf9a0
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Mon Jan 3 11:29:28 2011 +0000

    ARM: DMA: add support for DMA debugging
    
    Add ARM support for the DMA debug infrastructure, which allows the
    DMA API usage to be debugged.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index cc0a932bbea9..e5681636626f 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -328,7 +328,7 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
  * substitute the safe buffer for the unsafe one.
  * (basically move the buffer from an unsafe area to a safe one)
  */
-dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
+dma_addr_t __dma_map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
@@ -338,7 +338,7 @@ dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
 
 	return map_single(dev, ptr, size, dir);
 }
-EXPORT_SYMBOL(dma_map_single);
+EXPORT_SYMBOL(__dma_map_single);
 
 /*
  * see if a mapped address was really a "safe" buffer and if so, copy
@@ -346,7 +346,7 @@ EXPORT_SYMBOL(dma_map_single);
  * the safe buffer.  (basically return things back to the way they
  * should be)
  */
-void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+void __dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
@@ -354,9 +354,9 @@ void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 
 	unmap_single(dev, dma_addr, size, dir);
 }
-EXPORT_SYMBOL(dma_unmap_single);
+EXPORT_SYMBOL(__dma_unmap_single);
 
-dma_addr_t dma_map_page(struct device *dev, struct page *page,
+dma_addr_t __dma_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",
@@ -372,7 +372,7 @@ dma_addr_t dma_map_page(struct device *dev, struct page *page,
 
 	return map_single(dev, page_address(page) + offset, size, dir);
 }
-EXPORT_SYMBOL(dma_map_page);
+EXPORT_SYMBOL(__dma_map_page);
 
 /*
  * see if a mapped address was really a "safe" buffer and if so, copy
@@ -380,7 +380,7 @@ EXPORT_SYMBOL(dma_map_page);
  * the safe buffer.  (basically return things back to the way they
  * should be)
  */
-void dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
+void __dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
@@ -388,7 +388,7 @@ void dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 
 	unmap_single(dev, dma_addr, size, dir);
 }
-EXPORT_SYMBOL(dma_unmap_page);
+EXPORT_SYMBOL(__dma_unmap_page);
 
 int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 		unsigned long off, size_t sz, enum dma_data_direction dir)

commit 3560adf62075829393743667ff338dd8fd26a5c2
Merge: c9a8fdd8b3ec 2ef7f3dbd7a7 2725898fc9bb 1587a373f06a e00d9d4b1796 6d636c68e925 2ffe2da3e716 cd71f8f48eda ae1402022edb
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Feb 25 22:06:43 2010 +0000

    Merge branches 'at91', 'cache', 'cup', 'ep93xx', 'ixp4xx', 'nuc', 'pending-dma-streaming', 'u300' and 'umc' into devel

commit 18eabe2347ae7a11b3db768695913724166dfb0e
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 16:52:16 2009 +0000

    ARM: dma-mapping: introduce the idea of buffer ownership
    
    The DMA API has the notion of buffer ownership; make it explicit in the
    ARM implementation of this API.  This gives us a set of hooks to allow
    us to deal with CPU cache issues arising from non-cache coherent DMA.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Santosh Shilimkar <santosh.shilimkar@ti.com>
    Tested-By: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index bc90364a96c7..51499d68b161 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -277,7 +277,7 @@ static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 		 * We don't need to sync the DMA buffer since
 		 * it was allocated via the coherent allocators.
 		 */
-		dma_cache_maint(ptr, size, dir);
+		__dma_single_cpu_to_dev(ptr, size, dir);
 	}
 
 	return dma_addr;
@@ -315,6 +315,8 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
 			__cpuc_flush_kernel_dcache_area(ptr, size);
 		}
 		free_safe_buffer(dev->archdata.dmabounce, buf);
+	} else {
+		__dma_single_dev_to_cpu(dma_to_virt(dev, dma_addr), size, dir);
 	}
 }
 

commit d9fd3ab89ff58ca70c8cc0eadc87cd4a96950f95
Author: Mike Rapoport <mike@compulab.co.il>
Date:   Tue Dec 22 07:31:04 2009 +0100

    ARM: 5857/1: ARM: dmabounce: fix build
    
    Commit f74f7e57ae9fa12b2951ae62ce3557799b318399 (ARM: use
    flush_kernel_dcache_area() for dmabounce) has broken dmabounce build:
    
      CC      arch/arm/common/dmabounce.o
    arch/arm/common/dmabounce.c: In function 'unmap_single':
    arch/arm/common/dmabounce.c:315: error: implicit declaration of function '__cpuc_flush_kernel_dcache_area'
    make[2]: *** [arch/arm/common/dmabounce.o] Error 1
    
    Fix it.
    
    Signed-off-by: Mike Rapoport <mike@compulab.co.il>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index bc90364a96c7..cc32c1e54a59 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -312,7 +312,7 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
 			 * we need to ensure that the data will be coherent
 			 * with user mappings.
 			 */
-			__cpuc_flush_kernel_dcache_area(ptr, size);
+			__cpuc_flush_dcache_area(ptr, size);
 		}
 		free_safe_buffer(dev->archdata.dmabounce, buf);
 	}

commit f74f7e57ae9fa12b2951ae62ce3557799b318399
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Thu Nov 26 12:31:15 2009 +0000

    ARM: use flush_kernel_dcache_area() for dmabounce
    
    After copying data from the bounce buffer to the real buffer, use
    flush_kernel_dcache_page() to ensure that data is written back in
    manner coherent with future userspace mappings.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 5a375e5fef21..bc90364a96c7 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -308,15 +308,11 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
 			memcpy(ptr, buf->safe, size);
 
 			/*
-			 * DMA buffers must have the same cache properties
-			 * as if they were really used for DMA - which means
-			 * data must be written back to RAM.  Note that
-			 * we don't use dmac_flush_range() here for the
-			 * bidirectional case because we know the cache
-			 * lines will be coherent with the data written.
+			 * Since we may have written to a page cache page,
+			 * we need to ensure that the data will be coherent
+			 * with user mappings.
 			 */
-			dmac_clean_range(ptr, ptr + size);
-			outer_clean_range(__pa(ptr), __pa(ptr) + size);
+			__cpuc_flush_kernel_dcache_area(ptr, size);
 		}
 		free_safe_buffer(dev->archdata.dmabounce, buf);
 	}

commit 29cb8d0d249f6b8fa33683cc17622ff16ada834c
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Sat Oct 31 16:10:10 2009 +0000

    ARM: dma-mapping: split dma_unmap_page() from dma_unmap_single()
    
    We will need to treat dma_unmap_page() differently from dma_unmap_single()
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Tested-By: Jamie Iles <jamie@jamieiles.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 734ac9135998..5a375e5fef21 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -342,6 +342,22 @@ dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
 }
 EXPORT_SYMBOL(dma_map_single);
 
+/*
+ * see if a mapped address was really a "safe" buffer and if so, copy
+ * the data from the safe buffer back to the unsafe buffer and free up
+ * the safe buffer.  (basically return things back to the way they
+ * should be)
+ */
+void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		enum dma_data_direction dir)
+{
+	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
+		__func__, (void *) dma_addr, size, dir);
+
+	unmap_single(dev, dma_addr, size, dir);
+}
+EXPORT_SYMBOL(dma_unmap_single);
+
 dma_addr_t dma_map_page(struct device *dev, struct page *page,
 		unsigned long offset, size_t size, enum dma_data_direction dir)
 {
@@ -366,8 +382,7 @@ EXPORT_SYMBOL(dma_map_page);
  * the safe buffer.  (basically return things back to the way they
  * should be)
  */
-
-void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+void dma_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
@@ -375,7 +390,7 @@ void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 
 	unmap_single(dev, dma_addr, size, dir);
 }
-EXPORT_SYMBOL(dma_unmap_single);
+EXPORT_SYMBOL(dma_unmap_page);
 
 int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 		unsigned long off, size_t sz, enum dma_data_direction dir)

commit 58edb515724f9e63e569536d01ac8d8f8ddb367a
Author: Nicolas Pitre <nico@cam.org>
Date:   Tue Sep 9 15:54:13 2008 -0400

    [ARM] make page_to_dma() highmem aware
    
    If a machine class has a custom __virt_to_bus() implementation then it
    must provide a __arch_page_to_dma() implementation as well which is
    _not_ based on page_address() to support highmem.
    
    This patch fixes existing __arch_page_to_dma() and provide a default
    implementation otherwise.  The default implementation for highmem is
    based on __pfn_to_bus() which is defined only when no custom
    __virt_to_bus() is provided by the machine class.
    
    That leaves only ebsa110 and footbridge which cannot support highmem
    until they provide their own __arch_page_to_dma() implementation.
    But highmem support on those legacy platforms with limited memory is
    certainly not a priority.
    
    Signed-off-by: Nicolas Pitre <nico@marvell.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index f030f0775be7..734ac9135998 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -25,6 +25,7 @@
 #include <linux/module.h>
 #include <linux/init.h>
 #include <linux/slab.h>
+#include <linux/page-flags.h>
 #include <linux/device.h>
 #include <linux/dma-mapping.h>
 #include <linux/dmapool.h>
@@ -349,6 +350,12 @@ dma_addr_t dma_map_page(struct device *dev, struct page *page,
 
 	BUG_ON(!valid_dma_direction(dir));
 
+	if (PageHighMem(page)) {
+		dev_err(dev, "DMA buffer bouncing of HIGHMEM pages "
+			     "is not supported\n");
+		return ~0;
+	}
+
 	return map_single(dev, page_address(page) + offset, size, dir);
 }
 EXPORT_SYMBOL(dma_map_page);

commit 0e18b5d7c6339311f1e32e7b186ae3556c5b6d33
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Sep 29 13:48:17 2008 +0100

    [ARM] dma: add validation of DMA params
    
    Validate the direction argument like x86 does.  In addition,
    validate the dma_unmap_* parameters against those passed to
    dma_map_* when using the DMA bounce code.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 22aec95c9863..f030f0775be7 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -289,6 +289,7 @@ static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
 
 	if (buf) {
 		BUG_ON(buf->size != size);
+		BUG_ON(buf->direction != dir);
 
 		dev_dbg(dev,
 			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
@@ -334,7 +335,7 @@ dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, ptr, size, dir);
 
-	BUG_ON(dir == DMA_NONE);
+	BUG_ON(!valid_dma_direction(dir));
 
 	return map_single(dev, ptr, size, dir);
 }
@@ -346,7 +347,7 @@ dma_addr_t dma_map_page(struct device *dev, struct page *page,
 	dev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",
 		__func__, page, offset, size, dir);
 
-	BUG_ON(dir == DMA_NONE);
+	BUG_ON(!valid_dma_direction(dir));
 
 	return map_single(dev, page_address(page) + offset, size, dir);
 }
@@ -365,8 +366,6 @@ void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
 
-	BUG_ON(dir == DMA_NONE);
-
 	unmap_single(dev, dma_addr, size, dir);
 }
 EXPORT_SYMBOL(dma_unmap_single);
@@ -383,6 +382,8 @@ int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 	if (!buf)
 		return 1;
 
+	BUG_ON(buf->direction != dir);
+
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
 		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
 		buf->safe, buf->safe_dma_addr);
@@ -410,6 +411,8 @@ int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 	if (!buf)
 		return 1;
 
+	BUG_ON(buf->direction != dir);
+
 	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
 		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
 		buf->safe, buf->safe_dma_addr);

commit 3216a97bb0d5166ec5795aa3db1c3a02415ac060
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 22:23:31 2008 +0100

    [ARM] dma: coding style cleanups
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index d4b0c608fdee..22aec95c9863 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -154,9 +154,7 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 #endif
 
 	write_lock_irqsave(&device_info->lock, flags);
-
 	list_add(&buf->node, &device_info->safe_buffers);
-
 	write_unlock_irqrestore(&device_info->lock, flags);
 
 	return buf;
@@ -220,8 +218,7 @@ static struct safe_buffer *find_safe_buffer_dev(struct device *dev,
 	return find_safe_buffer(dev->archdata.dmabounce, dma_addr);
 }
 
-static inline dma_addr_t
-map_single(struct device *dev, void *ptr, size_t size,
+static inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
 {
 	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
@@ -285,9 +282,8 @@ map_single(struct device *dev, void *ptr, size_t size,
 	return dma_addr;
 }
 
-static inline void
-unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
-		enum dma_data_direction dir)
+static inline void unmap_single(struct device *dev, dma_addr_t dma_addr,
+		size_t size, enum dma_data_direction dir)
 {
 	struct safe_buffer *buf = find_safe_buffer_dev(dev, dma_addr, "unmap");
 
@@ -332,25 +328,20 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
  * substitute the safe buffer for the unsafe one.
  * (basically move the buffer from an unsafe area to a safe one)
  */
-dma_addr_t
-dma_map_single(struct device *dev, void *ptr, size_t size,
+dma_addr_t dma_map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
 {
-	dma_addr_t dma_addr;
-
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, ptr, size, dir);
 
 	BUG_ON(dir == DMA_NONE);
 
-	dma_addr = map_single(dev, ptr, size, dir);
-
-	return dma_addr;
+	return map_single(dev, ptr, size, dir);
 }
+EXPORT_SYMBOL(dma_map_single);
 
 dma_addr_t dma_map_page(struct device *dev, struct page *page,
-			unsigned long offset, size_t size,
-			enum dma_data_direction dir)
+		unsigned long offset, size_t size, enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",
 		__func__, page, offset, size, dir);
@@ -368,9 +359,8 @@ EXPORT_SYMBOL(dma_map_page);
  * should be)
  */
 
-void
-dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
-			enum dma_data_direction dir)
+void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		enum dma_data_direction dir)
 {
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
@@ -379,6 +369,7 @@ dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 
 	unmap_single(dev, dma_addr, size, dir);
 }
+EXPORT_SYMBOL(dma_unmap_single);
 
 int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 		unsigned long off, size_t sz, enum dma_data_direction dir)
@@ -434,9 +425,8 @@ int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 }
 EXPORT_SYMBOL(dmabounce_sync_for_device);
 
-static int
-dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev, const char *name,
-		    unsigned long size)
+static int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,
+		const char *name, unsigned long size)
 {
 	pool->size = size;
 	DO_STATS(pool->allocs = 0);
@@ -447,9 +437,8 @@ dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev, const char
 	return pool->pool ? 0 : -ENOMEM;
 }
 
-int
-dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
-			unsigned long large_buffer_size)
+int dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
+		unsigned long large_buffer_size)
 {
 	struct dmabounce_device_info *device_info;
 	int ret;
@@ -505,9 +494,9 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 	kfree(device_info);
 	return ret;
 }
+EXPORT_SYMBOL(dmabounce_register_dev);
 
-void
-dmabounce_unregister_dev(struct device *dev)
+void dmabounce_unregister_dev(struct device *dev)
 {
 	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 
@@ -540,11 +529,6 @@ dmabounce_unregister_dev(struct device *dev)
 
 	dev_info(dev, "dmabounce: device unregistered\n");
 }
-
-
-EXPORT_SYMBOL(dma_map_single);
-EXPORT_SYMBOL(dma_unmap_single);
-EXPORT_SYMBOL(dmabounce_register_dev);
 EXPORT_SYMBOL(dmabounce_unregister_dev);
 
 MODULE_AUTHOR("Christopher Hoover <ch@hpl.hp.com>, Deepak Saxena <dsaxena@plexity.net>");

commit 125ab12acf64ff86b55d20e14db20becd917b7c4
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 22:16:22 2008 +0100

    [ARM] dma: fix dmabounce dma_sync_xxx() implementations
    
    The dmabounce dma_sync_xxx() implementation have been broken for
    quite some time; they all copy data between the DMA buffer and
    the CPU visible buffer no irrespective of the change of ownership.
    (IOW, a DMA_FROM_DEVICE mapping copies data from the DMA buffer
    to the CPU buffer during a call to dma_sync_single_for_device().)
    
    Fix it by getting rid of sync_single(), moving the contents into
    the recently created dmabounce_sync_for_xxx() functions and adjusting
    appropriately.
    
    This also makes it possible to properly support the DMA range sync
    functions.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 1cb880b734df..d4b0c608fdee 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -205,6 +205,21 @@ free_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *
 
 /* ************************************************** */
 
+static struct safe_buffer *find_safe_buffer_dev(struct device *dev,
+		dma_addr_t dma_addr, const char *where)
+{
+	if (!dev || !dev->archdata.dmabounce)
+		return NULL;
+	if (dma_mapping_error(dev, dma_addr)) {
+		if (dev)
+			dev_err(dev, "Trying to %s invalid mapping\n", where);
+		else
+			pr_err("unknown device: Trying to %s invalid mapping\n", where);
+		return NULL;
+	}
+	return find_safe_buffer(dev->archdata.dmabounce, dma_addr);
+}
+
 static inline dma_addr_t
 map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
@@ -274,19 +289,7 @@ static inline void
 unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
-	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
-	struct safe_buffer *buf = NULL;
-
-	/*
-	 * Trying to unmap an invalid mapping
-	 */
-	if (dma_mapping_error(dev, dma_addr)) {
-		dev_err(dev, "Trying to unmap invalid mapping\n");
-		return;
-	}
-
-	if (device_info)
-		buf = find_safe_buffer(device_info, dma_addr);
+	struct safe_buffer *buf = find_safe_buffer_dev(dev, dma_addr, "unmap");
 
 	if (buf) {
 		BUG_ON(buf->size != size);
@@ -296,7 +299,7 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
 			buf->safe, buf->safe_dma_addr);
 
-		DO_STATS ( device_info->bounce_count++ );
+		DO_STATS(dev->archdata.dmabounce->bounce_count++);
 
 		if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
 			void *ptr = buf->ptr;
@@ -317,74 +320,7 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 			dmac_clean_range(ptr, ptr + size);
 			outer_clean_range(__pa(ptr), __pa(ptr) + size);
 		}
-		free_safe_buffer(device_info, buf);
-	}
-}
-
-static int sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
-			enum dma_data_direction dir)
-{
-	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
-	struct safe_buffer *buf = NULL;
-
-	if (device_info)
-		buf = find_safe_buffer(device_info, dma_addr);
-
-	if (buf) {
-		/*
-		 * Both of these checks from original code need to be
-		 * commented out b/c some drivers rely on the following:
-		 *
-		 * 1) Drivers may map a large chunk of memory into DMA space
-		 *    but only sync a small portion of it. Good example is
-		 *    allocating a large buffer, mapping it, and then
-		 *    breaking it up into small descriptors. No point
-		 *    in syncing the whole buffer if you only have to
-		 *    touch one descriptor.
-		 *
-		 * 2) Buffers that are mapped as DMA_BIDIRECTIONAL are
-		 *    usually only synced in one dir at a time.
-		 *
-		 * See drivers/net/eepro100.c for examples of both cases.
-		 *
-		 * -ds
-		 *
-		 * BUG_ON(buf->size != size);
-		 * BUG_ON(buf->direction != dir);
-		 */
-
-		dev_dbg(dev,
-			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
-			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
-			buf->safe, buf->safe_dma_addr);
-
-		DO_STATS ( device_info->bounce_count++ );
-
-		switch (dir) {
-		case DMA_FROM_DEVICE:
-			dev_dbg(dev,
-				"%s: copy back safe %p to unsafe %p size %d\n",
-				__func__, buf->safe, buf->ptr, size);
-			memcpy(buf->ptr, buf->safe, size);
-			break;
-		case DMA_TO_DEVICE:
-			dev_dbg(dev,
-				"%s: copy out unsafe %p to safe %p, size %d\n",
-				__func__,buf->ptr, buf->safe, size);
-			memcpy(buf->safe, buf->ptr, size);
-			break;
-		case DMA_BIDIRECTIONAL:
-			BUG();	/* is this allowed?  what does it mean? */
-		default:
-			BUG();
-		}
-		/*
-		 * No need to sync the safe buffer - it was allocated
-		 * via the coherent allocators.
-		 */
-		return 0;
-	} else {
-		return 1;
+		free_safe_buffer(dev->archdata.dmabounce, buf);
 	}
 }
 
@@ -447,18 +383,54 @@ dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 		unsigned long off, size_t sz, enum dma_data_direction dir)
 {
-	dev_dbg(dev, "%s(dma=%#lx,off=%#lx,sz=%zx,dir=%x)\n",
+	struct safe_buffer *buf;
+
+	dev_dbg(dev, "%s(dma=%#x,off=%#lx,sz=%zx,dir=%x)\n",
 		__func__, addr, off, sz, dir);
-	return sync_single(dev, addr, off + sz, dir);
+
+	buf = find_safe_buffer_dev(dev, addr, __func__);
+	if (!buf)
+		return 1;
+
+	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+		buf->safe, buf->safe_dma_addr);
+
+	DO_STATS(dev->archdata.dmabounce->bounce_count++);
+
+	if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
+		dev_dbg(dev, "%s: copy back safe %p to unsafe %p size %d\n",
+			__func__, buf->safe + off, buf->ptr + off, sz);
+		memcpy(buf->ptr + off, buf->safe + off, sz);
+	}
+	return 0;
 }
 EXPORT_SYMBOL(dmabounce_sync_for_cpu);
 
 int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
 		unsigned long off, size_t sz, enum dma_data_direction dir)
 {
-	dev_dbg(dev, "%s(dma=%#lx,off=%#lx,sz=%zx,dir=%x)\n",
+	struct safe_buffer *buf;
+
+	dev_dbg(dev, "%s(dma=%#x,off=%#lx,sz=%zx,dir=%x)\n",
 		__func__, addr, off, sz, dir);
-	return sync_single(dev, addr, off + sz, dir);
+
+	buf = find_safe_buffer_dev(dev, addr, __func__);
+	if (!buf)
+		return 1;
+
+	dev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+		__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+		buf->safe, buf->safe_dma_addr);
+
+	DO_STATS(dev->archdata.dmabounce->bounce_count++);
+
+	if (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) {
+		dev_dbg(dev, "%s: copy out unsafe %p to safe %p, size %d\n",
+			__func__,buf->ptr + off, buf->safe + off, sz);
+		memcpy(buf->safe + off, buf->ptr + off, sz);
+	}
+	return 0;
 }
 EXPORT_SYMBOL(dmabounce_sync_for_device);
 

commit 8c8a0ec57ee285ff407e9a64b3a5a37eaf800ad8
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 21:52:49 2008 +0100

    [ARM] dma: use new dmabounce_sync_for_xxx() for dma_sync_single_xxx()
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 0a98148279b1..1cb880b734df 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -444,30 +444,6 @@ dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	unmap_single(dev, dma_addr, size, dir);
 }
 
-void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_addr,
-				   unsigned long offset, size_t size,
-				   enum dma_data_direction dir)
-{
-	dev_dbg(dev, "%s(dma=%#x,off=%#lx,size=%zx,dir=%x)\n",
-		__func__, dma_addr, offset, size, dir);
-
-	if (sync_single(dev, dma_addr, offset + size, dir))
-		dma_cache_maint(dma_to_virt(dev, dma_addr) + offset, size, dir);
-}
-EXPORT_SYMBOL(dma_sync_single_range_for_cpu);
-
-void dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_addr,
-				      unsigned long offset, size_t size,
-				      enum dma_data_direction dir)
-{
-	dev_dbg(dev, "%s(dma=%#x,off=%#lx,size=%zx,dir=%x)\n",
-		__func__, dma_addr, offset, size, dir);
-
-	if (sync_single(dev, dma_addr, offset + size, dir))
-		dma_cache_maint(dma_to_virt(dev, dma_addr) + offset, size, dir);
-}
-EXPORT_SYMBOL(dma_sync_single_range_for_device);
-
 int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
 		unsigned long off, size_t sz, enum dma_data_direction dir)
 {

commit 2638b4dbe768aba023a06acd8e7eba708bb76ee6
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 21:38:41 2008 +0100

    [ARM] dma: Reduce to one dma_sync_sg_* implementation
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 20d967376fa9..0a98148279b1 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -468,45 +468,23 @@ void dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_addr,
 }
 EXPORT_SYMBOL(dma_sync_single_range_for_device);
 
-void
-dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
-			enum dma_data_direction dir)
+int dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,
+		unsigned long off, size_t sz, enum dma_data_direction dir)
 {
-	struct scatterlist *s;
-	int i;
-
-	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
-		__func__, sg, nents, dir);
-
-	BUG_ON(dir == DMA_NONE);
-
-	for_each_sg(sg, s, nents, i) {
-		dma_addr_t dma_addr = s->dma_address;
-		unsigned int length = s->length;
-
-		sync_single(dev, dma_addr, length, dir);
-	}
+	dev_dbg(dev, "%s(dma=%#lx,off=%#lx,sz=%zx,dir=%x)\n",
+		__func__, addr, off, sz, dir);
+	return sync_single(dev, addr, off + sz, dir);
 }
+EXPORT_SYMBOL(dmabounce_sync_for_cpu);
 
-void
-dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
-			enum dma_data_direction dir)
+int dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,
+		unsigned long off, size_t sz, enum dma_data_direction dir)
 {
-	struct scatterlist *s;
-	int i;
-
-	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
-		__func__, sg, nents, dir);
-
-	BUG_ON(dir == DMA_NONE);
-
-	for_each_sg(sg, s, nents, i) {
-		dma_addr_t dma_addr = s->dma_address;
-		unsigned int length = s->length;
-
-		sync_single(dev, dma_addr, length, dir);
-	}
+	dev_dbg(dev, "%s(dma=%#lx,off=%#lx,sz=%zx,dir=%x)\n",
+		__func__, addr, off, sz, dir);
+	return sync_single(dev, addr, off + sz, dir);
 }
+EXPORT_SYMBOL(dmabounce_sync_for_device);
 
 static int
 dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev, const char *name,
@@ -618,8 +596,6 @@ dmabounce_unregister_dev(struct device *dev)
 
 EXPORT_SYMBOL(dma_map_single);
 EXPORT_SYMBOL(dma_unmap_single);
-EXPORT_SYMBOL(dma_sync_sg_for_cpu);
-EXPORT_SYMBOL(dma_sync_sg_for_device);
 EXPORT_SYMBOL(dmabounce_register_dev);
 EXPORT_SYMBOL(dmabounce_unregister_dev);
 

commit 01135d92c1a540cd3370f7cf3d1c762320b85034
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 21:05:02 2008 +0100

    [ARM] dma: Reduce to one dma_map_sg()/dma_unmap_sg() implementation
    
    No point having two of these; dma_map_page() can do all the work
    for us.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index c7f23ced0a36..20d967376fa9 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -444,50 +444,6 @@ dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	unmap_single(dev, dma_addr, size, dir);
 }
 
-int
-dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
-		enum dma_data_direction dir)
-{
-	struct scatterlist *s;
-	int i;
-
-	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
-		__func__, sg, nents, dir);
-
-	BUG_ON(dir == DMA_NONE);
-
-	for_each_sg(sg, s, nents, i) {
-		struct page *page = sg_page(s);
-		unsigned int offset = s->offset;
-		unsigned int length = s->length;
-		void *ptr = page_address(page) + offset;
-
-		s->dma_address = map_single(dev, ptr, length, dir);
-	}
-
-	return nents;
-}
-
-void
-dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
-		enum dma_data_direction dir)
-{
-	struct scatterlist *s;
-	int i;
-
-	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
-		__func__, sg, nents, dir);
-
-	BUG_ON(dir == DMA_NONE);
-
-	for_each_sg(sg, s, nents, i) {
-		dma_addr_t dma_addr = s->dma_address;
-		unsigned int length = s->length;
-
-		unmap_single(dev, dma_addr, length, dir);
-	}
-}
-
 void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_addr,
 				   unsigned long offset, size_t size,
 				   enum dma_data_direction dir)
@@ -662,8 +618,6 @@ dmabounce_unregister_dev(struct device *dev)
 
 EXPORT_SYMBOL(dma_map_single);
 EXPORT_SYMBOL(dma_unmap_single);
-EXPORT_SYMBOL(dma_map_sg);
-EXPORT_SYMBOL(dma_unmap_sg);
 EXPORT_SYMBOL(dma_sync_sg_for_cpu);
 EXPORT_SYMBOL(dma_sync_sg_for_device);
 EXPORT_SYMBOL(dmabounce_register_dev);

commit 56f55f8b58a02e95b401cb50df05086cabeaeeb5
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 20:59:12 2008 +0100

    [ARM] dma: provide a better dma_map_page() implementation
    
    We can translate a struct page directly to a DMA address using
    page_to_dma().  No need to use page_address() followed by
    virt_to_dma().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 35c72bcf3d0b..c7f23ced0a36 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -412,6 +412,19 @@ dma_map_single(struct device *dev, void *ptr, size_t size,
 	return dma_addr;
 }
 
+dma_addr_t dma_map_page(struct device *dev, struct page *page,
+			unsigned long offset, size_t size,
+			enum dma_data_direction dir)
+{
+	dev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",
+		__func__, page, offset, size, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	return map_single(dev, page_address(page) + offset, size, dir);
+}
+EXPORT_SYMBOL(dma_map_page);
+
 /*
  * see if a mapped address was really a "safe" buffer and if so, copy
  * the data from the safe buffer back to the unsafe buffer and free up

commit afd1a321c49a250dab97cef6f2d3c3c9b9d0174a
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Thu Sep 25 16:30:57 2008 +0100

    [ARM] Update dma_map_sg()/dma_unmap_sg() API
    
    Update the ARM DMA scatter gather APIs for the scatterlist changes.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index aecc6c3f908f..35c72bcf3d0b 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -435,6 +435,7 @@ int
 dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 		enum dma_data_direction dir)
 {
+	struct scatterlist *s;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -442,14 +443,13 @@ dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	for (i = 0; i < nents; i++, sg++) {
-		struct page *page = sg_page(sg);
-		unsigned int offset = sg->offset;
-		unsigned int length = sg->length;
+	for_each_sg(sg, s, nents, i) {
+		struct page *page = sg_page(s);
+		unsigned int offset = s->offset;
+		unsigned int length = s->length;
 		void *ptr = page_address(page) + offset;
 
-		sg->dma_address =
-			map_single(dev, ptr, length, dir);
+		s->dma_address = map_single(dev, ptr, length, dir);
 	}
 
 	return nents;
@@ -459,6 +459,7 @@ void
 dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
 		enum dma_data_direction dir)
 {
+	struct scatterlist *s;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -466,9 +467,9 @@ dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	for (i = 0; i < nents; i++, sg++) {
-		dma_addr_t dma_addr = sg->dma_address;
-		unsigned int length = sg->length;
+	for_each_sg(sg, s, nents, i) {
+		dma_addr_t dma_addr = s->dma_address;
+		unsigned int length = s->length;
 
 		unmap_single(dev, dma_addr, length, dir);
 	}
@@ -502,6 +503,7 @@ void
 dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
 			enum dma_data_direction dir)
 {
+	struct scatterlist *s;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -509,9 +511,9 @@ dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	for (i = 0; i < nents; i++, sg++) {
-		dma_addr_t dma_addr = sg->dma_address;
-		unsigned int length = sg->length;
+	for_each_sg(sg, s, nents, i) {
+		dma_addr_t dma_addr = s->dma_address;
+		unsigned int length = s->length;
 
 		sync_single(dev, dma_addr, length, dir);
 	}
@@ -521,6 +523,7 @@ void
 dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
 			enum dma_data_direction dir)
 {
+	struct scatterlist *s;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -528,9 +531,9 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	for (i = 0; i < nents; i++, sg++) {
-		dma_addr_t dma_addr = sg->dma_address;
-		unsigned int length = sg->length;
+	for_each_sg(sg, s, nents, i) {
+		dma_addr_t dma_addr = s->dma_address;
+		unsigned int length = s->length;
 
 		sync_single(dev, dma_addr, length, dir);
 	}

commit 9dd428680573d7867ee5e40fa3f059a98301d416
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 12:18:26 2008 +0100

    [ARM] dma-mapping: provide sync_range APIs
    
    Convert the existing dma_sync_single_for_* APIs to the new range based
    APIs, and make the dma_sync_single_for_* API a superset of it.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 1ea6482cdf6e..aecc6c3f908f 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -321,9 +321,8 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	}
 }
 
-static inline void
-sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
-		enum dma_data_direction dir)
+static int sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+			enum dma_data_direction dir)
 {
 	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 	struct safe_buffer *buf = NULL;
@@ -383,8 +382,9 @@ sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		 * No need to sync the safe buffer - it was allocated
 		 * via the coherent allocators.
 		 */
+		return 0;
 	} else {
-		dma_cache_maint(dma_to_virt(dev, dma_addr), size, dir);
+		return 1;
 	}
 }
 
@@ -474,25 +474,29 @@ dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
 	}
 }
 
-void
-dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr, size_t size,
-				enum dma_data_direction dir)
+void dma_sync_single_range_for_cpu(struct device *dev, dma_addr_t dma_addr,
+				   unsigned long offset, size_t size,
+				   enum dma_data_direction dir)
 {
-	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
-		__func__, (void *) dma_addr, size, dir);
+	dev_dbg(dev, "%s(dma=%#x,off=%#lx,size=%zx,dir=%x)\n",
+		__func__, dma_addr, offset, size, dir);
 
-	sync_single(dev, dma_addr, size, dir);
+	if (sync_single(dev, dma_addr, offset + size, dir))
+		dma_cache_maint(dma_to_virt(dev, dma_addr) + offset, size, dir);
 }
+EXPORT_SYMBOL(dma_sync_single_range_for_cpu);
 
-void
-dma_sync_single_for_device(struct device *dev, dma_addr_t dma_addr, size_t size,
-				enum dma_data_direction dir)
+void dma_sync_single_range_for_device(struct device *dev, dma_addr_t dma_addr,
+				      unsigned long offset, size_t size,
+				      enum dma_data_direction dir)
 {
-	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
-		__func__, (void *) dma_addr, size, dir);
+	dev_dbg(dev, "%s(dma=%#x,off=%#lx,size=%zx,dir=%x)\n",
+		__func__, dma_addr, offset, size, dir);
 
-	sync_single(dev, dma_addr, size, dir);
+	if (sync_single(dev, dma_addr, offset + size, dir))
+		dma_cache_maint(dma_to_virt(dev, dma_addr) + offset, size, dir);
 }
+EXPORT_SYMBOL(dma_sync_single_range_for_device);
 
 void
 dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
@@ -644,8 +648,6 @@ EXPORT_SYMBOL(dma_map_single);
 EXPORT_SYMBOL(dma_unmap_single);
 EXPORT_SYMBOL(dma_map_sg);
 EXPORT_SYMBOL(dma_unmap_sg);
-EXPORT_SYMBOL(dma_sync_single_for_cpu);
-EXPORT_SYMBOL(dma_sync_single_for_device);
 EXPORT_SYMBOL(dma_sync_sg_for_cpu);
 EXPORT_SYMBOL(dma_sync_sg_for_device);
 EXPORT_SYMBOL(dmabounce_register_dev);

commit 98ed7d4b1a4eebc1ac25929b6968673bef4d54c3
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Aug 10 12:10:49 2008 +0100

    [ARM] dma-mapping: improve type-safeness of DMA translations
    
    OMAP at least gets the return type(s) for the DMA translation functions
    wrong, which can lead to subtle errors.  Avoid this by moving the DMA
    translation functions to asm/dma-mapping.h, and converting them to
    inline functions.
    
    Fix the OMAP DMA translation macros to use the correct argument and
    result types.
    
    Also, remove the unnecessary casts in dmabounce.c.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 69130f365904..1ea6482cdf6e 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -246,9 +246,9 @@ map_single(struct device *dev, void *ptr, size_t size,
 		}
 
 		dev_dbg(dev,
-			"%s: unsafe buffer %p (phy=%p) mapped to %p (phy=%p)\n",
-			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
-			buf->safe, (void *) buf->safe_dma_addr);
+			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+			buf->safe, buf->safe_dma_addr);
 
 		if ((dir == DMA_TO_DEVICE) ||
 		    (dir == DMA_BIDIRECTIONAL)) {
@@ -292,9 +292,9 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		BUG_ON(buf->size != size);
 
 		dev_dbg(dev,
-			"%s: unsafe buffer %p (phy=%p) mapped to %p (phy=%p)\n",
-			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
-			buf->safe, (void *) buf->safe_dma_addr);
+			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+			buf->safe, buf->safe_dma_addr);
 
 		DO_STATS ( device_info->bounce_count++ );
 
@@ -355,9 +355,9 @@ sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		 */
 
 		dev_dbg(dev,
-			"%s: unsafe buffer %p (phy=%p) mapped to %p (phy=%p)\n",
-			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
-			buf->safe, (void *) buf->safe_dma_addr);
+			"%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",
+			__func__, buf->ptr, virt_to_dma(dev, buf->ptr),
+			buf->safe, buf->safe_dma_addr);
 
 		DO_STATS ( device_info->bounce_count++ );
 

commit 8d8bb39b9eba32dd70e87fd5ad5c5dd4ba118e06
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Fri Jul 25 19:44:49 2008 -0700

    dma-mapping: add the device argument to dma_mapping_error()
    
    Add per-device dma_mapping_ops support for CONFIG_X86_64 as POWER
    architecture does:
    
    This enables us to cleanly fix the Calgary IOMMU issue that some devices
    are not behind the IOMMU (http://lkml.org/lkml/2008/5/8/423).
    
    I think that per-device dma_mapping_ops support would be also helpful for
    KVM people to support PCI passthrough but Andi thinks that this makes it
    difficult to support the PCI passthrough (see the above thread).  So I
    CC'ed this to KVM camp.  Comments are appreciated.
    
    A pointer to dma_mapping_ops to struct dev_archdata is added.  If the
    pointer is non NULL, DMA operations in asm/dma-mapping.h use it.  If it's
    NULL, the system-wide dma_ops pointer is used as before.
    
    If it's useful for KVM people, I plan to implement a mechanism to register
    a hook called when a new pci (or dma capable) device is created (it works
    with hot plugging).  It enables IOMMUs to set up an appropriate
    dma_mapping_ops per device.
    
    The major obstacle is that dma_mapping_error doesn't take a pointer to the
    device unlike other DMA operations.  So x86 can't have dma_mapping_ops per
    device.  Note all the POWER IOMMUs use the same dma_mapping_error function
    so this is not a problem for POWER but x86 IOMMUs use different
    dma_mapping_error functions.
    
    The first patch adds the device argument to dma_mapping_error.  The patch
    is trivial but large since it touches lots of drivers and dma-mapping.h in
    all the architecture.
    
    This patch:
    
    dma_mapping_error() doesn't take a pointer to the device unlike other DMA
    operations.  So we can't have dma_mapping_ops per device.
    
    Note that POWER already has dma_mapping_ops per device but all the POWER
    IOMMUs use the same dma_mapping_error function.  x86 IOMMUs use device
    argument.
    
    [akpm@linux-foundation.org: fix sge]
    [akpm@linux-foundation.org: fix svc_rdma]
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: fix bnx2x]
    [akpm@linux-foundation.org: fix s2io]
    [akpm@linux-foundation.org: fix pasemi_mac]
    [akpm@linux-foundation.org: fix sdhci]
    [akpm@linux-foundation.org: build fix]
    [akpm@linux-foundation.org: fix sparc]
    [akpm@linux-foundation.org: fix ibmvscsi]
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Cc: Muli Ben-Yehuda <muli@il.ibm.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Avi Kivity <avi@qumranet.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index dd2947342604..69130f365904 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -280,7 +280,7 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	/*
 	 * Trying to unmap an invalid mapping
 	 */
-	if (dma_mapping_error(dma_addr)) {
+	if (dma_mapping_error(dev, dma_addr)) {
 		dev_err(dev, "Trying to unmap invalid mapping\n");
 		return;
 	}

commit fc3a8828b139c24aade3f9d608775e36c248f8f5
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Fri May 2 06:02:41 2008 +0200

    driver core: fix a lot of printk usages of bus_id
    
    We have the dev_printk() variants for this kind of thing, use them
    instead of directly trying to access the bus_id field of struct device.
    
    This is done in order to remove bus_id entirely.
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 2744673314b4..dd2947342604 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -554,9 +554,8 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 
 	device_info = kmalloc(sizeof(struct dmabounce_device_info), GFP_ATOMIC);
 	if (!device_info) {
-		printk(KERN_ERR
-			"Could not allocated dmabounce_device_info for %s",
-			dev->bus_id);
+		dev_err(dev,
+			"Could not allocated dmabounce_device_info\n");
 		return -ENOMEM;
 	}
 
@@ -594,8 +593,7 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 
 	dev->archdata.dmabounce = device_info;
 
-	printk(KERN_INFO "dmabounce: registered device %s on %s bus\n",
-		dev->bus_id, dev->bus->name);
+	dev_info(dev, "dmabounce: registered device\n");
 
 	return 0;
 
@@ -614,16 +612,15 @@ dmabounce_unregister_dev(struct device *dev)
 	dev->archdata.dmabounce = NULL;
 
 	if (!device_info) {
-		printk(KERN_WARNING
-			"%s: Never registered with dmabounce but attempting" \
-			"to unregister!\n", dev->bus_id);
+		dev_warn(dev,
+			 "Never registered with dmabounce but attempting"
+			 "to unregister!\n");
 		return;
 	}
 
 	if (!list_empty(&device_info->safe_buffers)) {
-		printk(KERN_ERR
-			"%s: Removing from dmabounce with pending buffers!\n",
-			dev->bus_id);
+		dev_err(dev,
+			"Removing from dmabounce with pending buffers!\n");
 		BUG();
 	}
 
@@ -639,8 +636,7 @@ dmabounce_unregister_dev(struct device *dev)
 
 	kfree(device_info);
 
-	printk(KERN_INFO "dmabounce: device %s on %s bus unregistered\n",
-		dev->bus_id, dev->bus->name);
+	dev_info(dev, "dmabounce: device unregistered\n");
 }
 
 

commit fc6e14f4fb50bc734d08e65eb8a5798d54a2ad77
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Jun 22 15:41:30 2008 +0100

    [ARM] Export dma_sync_sg_for_device()
    
    Noticed by Martin Michlmayr, this missing export prevents IEEE1394
    from building with:
    
    ERROR: "dma_sync_sg_for_device" [drivers/ieee1394/ieee1394.ko] undefined!
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 52fc6a883281..2744673314b4 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -650,7 +650,8 @@ EXPORT_SYMBOL(dma_map_sg);
 EXPORT_SYMBOL(dma_unmap_sg);
 EXPORT_SYMBOL(dma_sync_single_for_cpu);
 EXPORT_SYMBOL(dma_sync_single_for_device);
-EXPORT_SYMBOL(dma_sync_sg);
+EXPORT_SYMBOL(dma_sync_sg_for_cpu);
+EXPORT_SYMBOL(dma_sync_sg_for_device);
 EXPORT_SYMBOL(dmabounce_register_dev);
 EXPORT_SYMBOL(dmabounce_unregister_dev);
 

commit 9f2326be52f4fa83d20a75998cd3c87b300588c4
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Oct 23 09:11:41 2007 +0200

    arm: build fix
    
    arch/arm/common/dmabounce.c: In function 'dma_map_sg':
    arch/arm/common/dmabounce.c:445: error: implicit declaration of function 'sg_page'
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 9d371e476552..52fc6a883281 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -29,6 +29,7 @@
 #include <linux/dma-mapping.h>
 #include <linux/dmapool.h>
 #include <linux/list.h>
+#include <linux/scatterlist.h>
 
 #include <asm/cacheflush.h>
 

commit 58b053e4ce9d2fc3023645c1b96e537c72aa8d9a
Author: Jens Axboe <jens.axboe@oracle.com>
Date:   Mon Oct 22 20:02:46 2007 +0200

    Update arch/ to use sg helpers
    
    Signed-off-by: Jens Axboe <jens.axboe@oracle.com>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 44ab0dad4035..9d371e476552 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -442,7 +442,7 @@ dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 	BUG_ON(dir == DMA_NONE);
 
 	for (i = 0; i < nents; i++, sg++) {
-		struct page *page = sg->page;
+		struct page *page = sg_page(sg);
 		unsigned int offset = sg->offset;
 		unsigned int length = sg->length;
 		void *ptr = page_address(page) + offset;

commit 84aa462e2c2cd1b921f6b8e283f8d41666e02e8e
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Oct 9 14:17:01 2007 +0100

    [ARM] Rename consistent_sync() as dma_cache_maint()
    
    consistent_sync() is used to handle the cache maintainence issues with
    DMA operations.  Since we've now removed the misuse of this function
    from the two MTD drivers, rename it to prevent future mis-use.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index b36b1e8a105d..44ab0dad4035 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -263,7 +263,7 @@ map_single(struct device *dev, void *ptr, size_t size,
 		 * We don't need to sync the DMA buffer since
 		 * it was allocated via the coherent allocators.
 		 */
-		consistent_sync(ptr, size, dir);
+		dma_cache_maint(ptr, size, dir);
 	}
 
 	return dma_addr;
@@ -383,7 +383,7 @@ sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		 * via the coherent allocators.
 		 */
 	} else {
-		consistent_sync(dma_to_virt(dev, dma_addr), size, dir);
+		dma_cache_maint(dma_to_virt(dev, dma_addr), size, dir);
 	}
 }
 

commit 6cbdc8c5357276307a77deeada3f04626ff17da6
Author: Simon Arlott <simon@fire.lp0.eu>
Date:   Fri May 11 20:40:30 2007 +0100

    [ARM] spelling fixes
    
    Spelling fixes in arch/arm/.
    
    Signed-off-by: Simon Arlott <simon@fire.lp0.eu>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 6fbe7722aa44..b36b1e8a105d 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -6,7 +6,7 @@
  *  copy data to/from buffers located outside the DMA region. This
  *  only works for systems in which DMA memory is at the bottom of
  *  RAM, the remainder of memory is at the top and the DMA memory
- *  can be marked as ZONE_DMA. Anything beyond that such as discontigous
+ *  can be marked as ZONE_DMA. Anything beyond that such as discontiguous
  *  DMA windows will require custom implementations that reserve memory
  *  areas at early bootup.
  *

commit f69eda00d4e9808f0ddc88826278f93512cd2c98
Merge: bc43fd40ea9d dd1d2f4439af 271f5ca638b3 108d093fdd75 78c2644feccf 0e0ba76926c3 850b42933e70
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sat Feb 17 20:09:53 2007 +0000

    Merge AT91, EP93xx, General devel, PXA, S3C, V6+ and Xscale trees

commit 017cc022b6f0a0619cc3b0bba43e1c3247b06779
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Feb 12 10:53:50 2007 +0000

    [ARM] Convert dmabounce statistics to use a device attribute
    
    Rather than printk'ing the dmabounce statistics occasionally to
    the kernel log, provide a sysfs file to allow this information
    to be periodically read.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 490e1fe286d5..4d5b031ac745 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -32,7 +32,6 @@
 
 #include <asm/cacheflush.h>
 
-#undef DEBUG
 #undef STATS
 
 #ifdef STATS
@@ -72,6 +71,7 @@ struct dmabounce_device_info {
 	unsigned long total_allocs;
 	unsigned long map_op_count;
 	unsigned long bounce_count;
+	int attr_res;
 #endif
 	struct dmabounce_pool	small;
 	struct dmabounce_pool	large;
@@ -80,16 +80,21 @@ struct dmabounce_device_info {
 };
 
 #ifdef STATS
-static void print_alloc_stats(struct dmabounce_device_info *device_info)
+static ssize_t dmabounce_show(struct device *dev, struct device_attribute *attr,
+			      char *buf)
 {
-	printk(KERN_INFO
-		"%s: dmabounce: sbp: %lu, lbp: %lu, other: %lu, total: %lu\n",
-		device_info->dev->bus_id,
-		device_info->small.allocs, device_info->large.allocs,
+	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
+	return sprintf(buf, "%lu %lu %lu %lu %lu %lu\n",
+		device_info->small.allocs,
+		device_info->large.allocs,
 		device_info->total_allocs - device_info->small.allocs -
 			device_info->large.allocs,
-		device_info->total_allocs);
+		device_info->total_allocs,
+		device_info->map_op_count,
+		device_info->bounce_count);
 }
+
+static DEVICE_ATTR(dmabounce_stats, 0400, dmabounce_show, NULL);
 #endif
 
 
@@ -145,8 +150,6 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 	if (pool)
 		pool->allocs++;
 	device_info->total_allocs++;
-	if (device_info->total_allocs % 1000 == 0)
-		print_alloc_stats(device_info);
 #endif
 
 	write_lock_irqsave(&device_info->lock, flags);
@@ -201,15 +204,6 @@ free_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *
 
 /* ************************************************** */
 
-#ifdef STATS
-static void print_map_stats(struct dmabounce_device_info *device_info)
-{
-	dev_info(device_info->dev,
-		"dmabounce: map_op_count=%lu, bounce_count=%lu\n",
-		device_info->map_op_count, device_info->bounce_count);
-}
-#endif
-
 static inline dma_addr_t
 map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
@@ -587,6 +581,7 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 	device_info->total_allocs = 0;
 	device_info->map_op_count = 0;
 	device_info->bounce_count = 0;
+	device_info->attr_res = device_create_file(dev, &dev_attr_dmabounce_stats);
 #endif
 
 	dev->archdata.dmabounce = device_info;
@@ -630,8 +625,8 @@ dmabounce_unregister_dev(struct device *dev)
 		dma_pool_destroy(device_info->large.pool);
 
 #ifdef STATS
-	print_alloc_stats(device_info);
-	print_map_stats(device_info);
+	if (device_info->attr_res == 0)
+		device_remove_file(dev, &dev_attr_dmabounce_stats);
 #endif
 
 	kfree(device_info);

commit ab2c21529df6ee0f06787773882a1abc6bc2d665
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Feb 12 10:28:24 2007 +0000

    [ARM] Add a reference from struct device to the dma bounce info
    
    dmabounce keeps a per-device structure, and finds the correct
    structure by walking a list.  Since architectures can now add
    fields to struct device, we can attach this structure direct to
    the struct device, thereby eliminating the code to search the
    list.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 2e635b814c14..490e1fe286d5 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -66,8 +66,6 @@ struct dmabounce_pool {
 };
 
 struct dmabounce_device_info {
-	struct list_head node;
-
 	struct device *dev;
 	struct list_head safe_buffers;
 #ifdef STATS
@@ -81,8 +79,6 @@ struct dmabounce_device_info {
 	rwlock_t lock;
 };
 
-static LIST_HEAD(dmabounce_devs);
-
 #ifdef STATS
 static void print_alloc_stats(struct dmabounce_device_info *device_info)
 {
@@ -96,19 +92,6 @@ static void print_alloc_stats(struct dmabounce_device_info *device_info)
 }
 #endif
 
-/* find the given device in the dmabounce device list */
-static inline struct dmabounce_device_info *
-find_dmabounce_dev(struct device *dev)
-{
-	struct dmabounce_device_info *d;
-
-	list_for_each_entry(d, &dmabounce_devs, node)
-		if (d->dev == dev)
-			return d;
-
-	return NULL;
-}
-
 
 /* allocate a 'safe' buffer and keep track of it */
 static inline struct safe_buffer *
@@ -231,7 +214,7 @@ static inline dma_addr_t
 map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
 {
-	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 	dma_addr_t dma_addr;
 	int needs_bounce = 0;
 
@@ -292,7 +275,7 @@ static inline void
 unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
-	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 	struct safe_buffer *buf = NULL;
 
 	/*
@@ -343,7 +326,7 @@ static inline void
 sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		enum dma_data_direction dir)
 {
-	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
 	struct safe_buffer *buf = NULL;
 
 	if (device_info)
@@ -606,7 +589,7 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 	device_info->bounce_count = 0;
 #endif
 
-	list_add(&device_info->node, &dmabounce_devs);
+	dev->archdata.dmabounce = device_info;
 
 	printk(KERN_INFO "dmabounce: registered device %s on %s bus\n",
 		dev->bus_id, dev->bus->name);
@@ -623,7 +606,9 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 void
 dmabounce_unregister_dev(struct device *dev)
 {
-	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	struct dmabounce_device_info *device_info = dev->archdata.dmabounce;
+
+	dev->archdata.dmabounce = NULL;
 
 	if (!device_info) {
 		printk(KERN_WARNING
@@ -649,8 +634,6 @@ dmabounce_unregister_dev(struct device *dev)
 	print_map_stats(device_info);
 #endif
 
-	list_del(&device_info->node);
-
 	kfree(device_info);
 
 	printk(KERN_INFO "dmabounce: device %s on %s bus unregistered\n",

commit 7ae5a761d2ffc4cf7d3248e09f4d3da234434f30
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Feb 6 17:39:31 2007 +0000

    [ARM] Convert DMA cache handling to take const void * args
    
    The DMA cache handling functions take virtual addresses, but in the
    form of unsigned long arguments.  This leads to a little confusion
    about what exactly they take.  So, convert them to take const void *
    instead.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index b4748e3171c6..2362c498f52e 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -321,12 +321,12 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		DO_STATS ( device_info->bounce_count++ );
 
 		if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
-			unsigned long ptr;
+			void *ptr = buf->ptr;
 
 			dev_dbg(dev,
 				"%s: copy back safe %p to unsafe %p size %d\n",
-				__func__, buf->safe, buf->ptr, size);
-			memcpy(buf->ptr, buf->safe, size);
+				__func__, buf->safe, ptr, size);
+			memcpy(ptr, buf->safe, size);
 
 			/*
 			 * DMA buffers must have the same cache properties
@@ -336,7 +336,6 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 			 * bidirectional case because we know the cache
 			 * lines will be coherent with the data written.
 			 */
-			ptr = (unsigned long)buf->ptr;
 			dmac_clean_range(ptr, ptr + size);
 			outer_clean_range(__pa(ptr), __pa(ptr) + size);
 		}

commit 953233dc9958ba2b29753d0f24e37a33a076a5f6
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Mon Feb 5 14:48:08 2007 +0100

    [ARM] 4134/1: Add generic support for outer caches
    
    The outer cache can be L2 as on RealView/EB MPCore platform or even L3
    or further on ARMv7 cores. This patch adds the generic support for
    flushing the outer cache in the DMA operations.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 272702accd8b..b4748e3171c6 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -338,6 +338,7 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 			 */
 			ptr = (unsigned long)buf->ptr;
 			dmac_clean_range(ptr, ptr + size);
+			outer_clean_range(__pa(ptr), __pa(ptr) + size);
 		}
 		free_safe_buffer(device_info, buf);
 	}

commit 7f8e33546d17c7d8849be3a6623c3b6b3c9b588b
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Tue Feb 6 17:29:53 2007 +0000

    [ARM] Don't call consistent_sync() for DMA coherent memory
    
    Memory allocated by the coherent memory allocators will be marked
    uncacheable, which means it's pointless calling consistent_sync()
    to perform cache maintainence on this memory; it's just a waste of
    CPU cycles.
    
    Moreover, with the (subsequent) merge of outer cache support, it
    actually breaks things to call consistent_sync() on anything but
    direct-mapped memory.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 2e635b814c14..272702accd8b 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -281,10 +281,14 @@ map_single(struct device *dev, void *ptr, size_t size,
 		ptr = buf->safe;
 
 		dma_addr = buf->safe_dma_addr;
+	} else {
+		/*
+		 * We don't need to sync the DMA buffer since
+		 * it was allocated via the coherent allocators.
+		 */
+		consistent_sync(ptr, size, dir);
 	}
 
-	consistent_sync(ptr, size, dir);
-
 	return dma_addr;
 }
 
@@ -397,7 +401,10 @@ sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 		default:
 			BUG();
 		}
-		consistent_sync(buf->safe, size, dir);
+		/*
+		 * No need to sync the safe buffer - it was allocated
+		 * via the coherent allocators.
+		 */
 	} else {
 		consistent_sync(dma_to_virt(dev, dma_addr), size, dir);
 	}

commit 73218187e8650c08d4e4735dbeeeb860f438da7c
Author: Kevin Hilman <khilman@com.rmk.(none)>
Date:   Thu Nov 2 23:44:24 2006 +0100

    [ARM] 3917/1: Fix dmabounce symbol exports
    
    dma_sync_single is no more (and to be removed in 2.7) so this export should be dma_sync_single_for_cpu.
    
    Also export dma_sync_single_for_device.
    
    Signed-off-by: Kevin Hilman <khilman@mvista.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 028bdc9228fb..2e635b814c14 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -662,7 +662,8 @@ EXPORT_SYMBOL(dma_map_single);
 EXPORT_SYMBOL(dma_unmap_single);
 EXPORT_SYMBOL(dma_map_sg);
 EXPORT_SYMBOL(dma_unmap_sg);
-EXPORT_SYMBOL(dma_sync_single);
+EXPORT_SYMBOL(dma_sync_single_for_cpu);
+EXPORT_SYMBOL(dma_sync_single_for_device);
 EXPORT_SYMBOL(dma_sync_sg);
 EXPORT_SYMBOL(dmabounce_register_dev);
 EXPORT_SYMBOL(dmabounce_unregister_dev);

commit e2785f0d4251c35d6e82ec819354374c8c17d63d
Author: Kevin Hilman <khilman@mvista.com>
Date:   Fri Aug 18 15:32:14 2006 +0100

    [ARM] 3755/1: dmabounce: fix return value for find_safe_buffer
    
    Patch from Kevin Hilman
    
    Previous locking changes to dmabounce incorrectly return non-NULL even
    when buffer not found.  Fix it up.
    
    Signed-off-by: Kevin Hilman <khilman@mvista.com>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 5b7c26395b44..028bdc9228fb 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -179,17 +179,19 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 static inline struct safe_buffer *
 find_safe_buffer(struct dmabounce_device_info *device_info, dma_addr_t safe_dma_addr)
 {
-	struct safe_buffer *b = NULL;
+	struct safe_buffer *b, *rb = NULL;
 	unsigned long flags;
 
 	read_lock_irqsave(&device_info->lock, flags);
 
 	list_for_each_entry(b, &device_info->safe_buffers, node)
-		if (b->safe_dma_addr == safe_dma_addr)
+		if (b->safe_dma_addr == safe_dma_addr) {
+			rb = b;
 			break;
+		}
 
 	read_unlock_irqrestore(&device_info->lock, flags);
-	return b;
+	return rb;
 }
 
 static inline void

commit 823588c18689ddd49d4643eda7654302f18a275f
Author: Kevin Hilman <khilman@deeprooted.net>
Date:   Thu Jun 22 22:27:14 2006 +0100

    [ARM] 3537/1: Rework DMA-bounce locking for finer granularity
    
    Patch from Kevin Hilman
    
    This time with IRQ versions of locks.
    
    Rework also enables compatability with realtime-preemption patch.
    With the current locking via interrupt disabling, under RT,
    potentially sleeping functions can be called with interrupts
    disabled.
    
    Signed-off-by: Kevin Hilman <khilman@mvista.com>
    Signed-off-by: Deepak Saxena <dsaxena@plexity.net>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 7971d0dc6892..5b7c26395b44 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -77,6 +77,8 @@ struct dmabounce_device_info {
 #endif
 	struct dmabounce_pool	small;
 	struct dmabounce_pool	large;
+
+	rwlock_t lock;
 };
 
 static LIST_HEAD(dmabounce_devs);
@@ -116,6 +118,7 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 	struct safe_buffer *buf;
 	struct dmabounce_pool *pool;
 	struct device *dev = device_info->dev;
+	unsigned long flags;
 
 	dev_dbg(dev, "%s(ptr=%p, size=%d, dir=%d)\n",
 		__func__, ptr, size, dir);
@@ -163,8 +166,12 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 		print_alloc_stats(device_info);
 #endif
 
+	write_lock_irqsave(&device_info->lock, flags);
+
 	list_add(&buf->node, &device_info->safe_buffers);
 
+	write_unlock_irqrestore(&device_info->lock, flags);
+
 	return buf;
 }
 
@@ -172,22 +179,32 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 static inline struct safe_buffer *
 find_safe_buffer(struct dmabounce_device_info *device_info, dma_addr_t safe_dma_addr)
 {
-	struct safe_buffer *b;
+	struct safe_buffer *b = NULL;
+	unsigned long flags;
+
+	read_lock_irqsave(&device_info->lock, flags);
 
 	list_for_each_entry(b, &device_info->safe_buffers, node)
 		if (b->safe_dma_addr == safe_dma_addr)
-			return b;
+			break;
 
-	return NULL;
+	read_unlock_irqrestore(&device_info->lock, flags);
+	return b;
 }
 
 static inline void
 free_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *buf)
 {
+	unsigned long flags;
+
 	dev_dbg(device_info->dev, "%s(buf=%p)\n", __func__, buf);
 
+	write_lock_irqsave(&device_info->lock, flags);
+
 	list_del(&buf->node);
 
+	write_unlock_irqrestore(&device_info->lock, flags);
+
 	if (buf->pool)
 		dma_pool_free(buf->pool->pool, buf->safe, buf->safe_dma_addr);
 	else
@@ -396,7 +413,6 @@ dma_addr_t
 dma_map_single(struct device *dev, void *ptr, size_t size,
 		enum dma_data_direction dir)
 {
-	unsigned long flags;
 	dma_addr_t dma_addr;
 
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
@@ -404,12 +420,8 @@ dma_map_single(struct device *dev, void *ptr, size_t size,
 
 	BUG_ON(dir == DMA_NONE);
 
-	local_irq_save(flags);
-
 	dma_addr = map_single(dev, ptr, size, dir);
 
-	local_irq_restore(flags);
-
 	return dma_addr;
 }
 
@@ -424,25 +436,18 @@ void
 dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 			enum dma_data_direction dir)
 {
-	unsigned long flags;
-
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
 
 	BUG_ON(dir == DMA_NONE);
 
-	local_irq_save(flags);
-
 	unmap_single(dev, dma_addr, size, dir);
-
-	local_irq_restore(flags);
 }
 
 int
 dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 		enum dma_data_direction dir)
 {
-	unsigned long flags;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -450,8 +455,6 @@ dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	local_irq_save(flags);
-
 	for (i = 0; i < nents; i++, sg++) {
 		struct page *page = sg->page;
 		unsigned int offset = sg->offset;
@@ -462,8 +465,6 @@ dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
 			map_single(dev, ptr, length, dir);
 	}
 
-	local_irq_restore(flags);
-
 	return nents;
 }
 
@@ -471,7 +472,6 @@ void
 dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
 		enum dma_data_direction dir)
 {
-	unsigned long flags;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -479,55 +479,38 @@ dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	local_irq_save(flags);
-
 	for (i = 0; i < nents; i++, sg++) {
 		dma_addr_t dma_addr = sg->dma_address;
 		unsigned int length = sg->length;
 
 		unmap_single(dev, dma_addr, length, dir);
 	}
-
-	local_irq_restore(flags);
 }
 
 void
 dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr, size_t size,
 				enum dma_data_direction dir)
 {
-	unsigned long flags;
-
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
 
-	local_irq_save(flags);
-
 	sync_single(dev, dma_addr, size, dir);
-
-	local_irq_restore(flags);
 }
 
 void
 dma_sync_single_for_device(struct device *dev, dma_addr_t dma_addr, size_t size,
 				enum dma_data_direction dir)
 {
-	unsigned long flags;
-
 	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
 		__func__, (void *) dma_addr, size, dir);
 
-	local_irq_save(flags);
-
 	sync_single(dev, dma_addr, size, dir);
-
-	local_irq_restore(flags);
 }
 
 void
 dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
 			enum dma_data_direction dir)
 {
-	unsigned long flags;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -535,23 +518,18 @@ dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	local_irq_save(flags);
-
 	for (i = 0; i < nents; i++, sg++) {
 		dma_addr_t dma_addr = sg->dma_address;
 		unsigned int length = sg->length;
 
 		sync_single(dev, dma_addr, length, dir);
 	}
-
-	local_irq_restore(flags);
 }
 
 void
 dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
 			enum dma_data_direction dir)
 {
-	unsigned long flags;
 	int i;
 
 	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
@@ -559,16 +537,12 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
 
 	BUG_ON(dir == DMA_NONE);
 
-	local_irq_save(flags);
-
 	for (i = 0; i < nents; i++, sg++) {
 		dma_addr_t dma_addr = sg->dma_address;
 		unsigned int length = sg->length;
 
 		sync_single(dev, dma_addr, length, dir);
 	}
-
-	local_irq_restore(flags);
 }
 
 static int
@@ -622,6 +596,7 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 
 	device_info->dev = dev;
 	INIT_LIST_HEAD(&device_info->safe_buffers);
+	rwlock_init(&device_info->lock);
 
 #ifdef STATS
 	device_info->total_allocs = 0;

commit 3a2916aa289504d694072a98876d23ca31d6401e
Author: Erik Hovland <erik@hovland.org>
Date:   Wed Mar 22 21:02:11 2006 +0000

    [ARM] 3389/1: typo and grammar fix
    
    Patch from Erik Hovland
    
    I found a typo and what seems to be a run-on sentence in
    arch/arm/common/dmabounce.c
    
    This patch corrects both.
    
    Signed-off-by: Erik Hovland <erik@hovland.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index ad6c89a555bb..7971d0dc6892 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -5,7 +5,7 @@
  *  limited DMA windows. These functions utilize bounce buffers to
  *  copy data to/from buffers located outside the DMA region. This
  *  only works for systems in which DMA memory is at the bottom of
- *  RAM and the remainder of memory is at the top an the DMA memory
+ *  RAM, the remainder of memory is at the top and the DMA memory
  *  can be marked as ZONE_DMA. Anything beyond that such as discontigous
  *  DMA windows will require custom implementations that reserve memory
  *  areas at early bootup.

commit cb7610d018235653c73ff1fea79b962c16317474
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Sun Oct 30 21:12:08 2005 +0000

    [ARM] Clean up dmabounce
    
    Encapsulate pool data into dmabounce_pool.  Only account successful
    allocations.  Use dma_mapping_error().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index cbf2165476b0..ad6c89a555bb 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -33,8 +33,8 @@
 #include <asm/cacheflush.h>
 
 #undef DEBUG
-
 #undef STATS
+
 #ifdef STATS
 #define DO_STATS(X) do { X ; } while (0)
 #else
@@ -52,26 +52,31 @@ struct safe_buffer {
 	int		direction;
 
 	/* safe buffer info */
-	struct dma_pool *pool;
+	struct dmabounce_pool *pool;
 	void		*safe;
 	dma_addr_t	safe_dma_addr;
 };
 
+struct dmabounce_pool {
+	unsigned long	size;
+	struct dma_pool	*pool;
+#ifdef STATS
+	unsigned long	allocs;
+#endif
+};
+
 struct dmabounce_device_info {
 	struct list_head node;
 
 	struct device *dev;
-	struct dma_pool *small_buffer_pool;
-	struct dma_pool *large_buffer_pool;
 	struct list_head safe_buffers;
-	unsigned long small_buffer_size, large_buffer_size;
 #ifdef STATS
-	unsigned long sbp_allocs;
-	unsigned long lbp_allocs;
 	unsigned long total_allocs;
 	unsigned long map_op_count;
 	unsigned long bounce_count;
 #endif
+	struct dmabounce_pool	small;
+	struct dmabounce_pool	large;
 };
 
 static LIST_HEAD(dmabounce_devs);
@@ -82,9 +87,9 @@ static void print_alloc_stats(struct dmabounce_device_info *device_info)
 	printk(KERN_INFO
 		"%s: dmabounce: sbp: %lu, lbp: %lu, other: %lu, total: %lu\n",
 		device_info->dev->bus_id,
-		device_info->sbp_allocs, device_info->lbp_allocs,
-		device_info->total_allocs - device_info->sbp_allocs -
-			device_info->lbp_allocs,
+		device_info->small.allocs, device_info->large.allocs,
+		device_info->total_allocs - device_info->small.allocs -
+			device_info->large.allocs,
 		device_info->total_allocs);
 }
 #endif
@@ -106,18 +111,22 @@ find_dmabounce_dev(struct device *dev)
 /* allocate a 'safe' buffer and keep track of it */
 static inline struct safe_buffer *
 alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
-			size_t size, enum dma_data_direction dir)
+		  size_t size, enum dma_data_direction dir)
 {
 	struct safe_buffer *buf;
-	struct dma_pool *pool;
+	struct dmabounce_pool *pool;
 	struct device *dev = device_info->dev;
-	void *safe;
-	dma_addr_t safe_dma_addr;
 
 	dev_dbg(dev, "%s(ptr=%p, size=%d, dir=%d)\n",
 		__func__, ptr, size, dir);
 
-	DO_STATS ( device_info->total_allocs++ );
+	if (size <= device_info->small.size) {
+		pool = &device_info->small;
+	} else if (size <= device_info->large.size) {
+		pool = &device_info->large;
+	} else {
+		pool = NULL;
+	}
 
 	buf = kmalloc(sizeof(struct safe_buffer), GFP_ATOMIC);
 	if (buf == NULL) {
@@ -125,41 +134,35 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 		return NULL;
 	}
 
-	if (size <= device_info->small_buffer_size) {
-		pool = device_info->small_buffer_pool;
-		safe = dma_pool_alloc(pool, GFP_ATOMIC, &safe_dma_addr);
-
-		DO_STATS ( device_info->sbp_allocs++ );
-	} else if (size <= device_info->large_buffer_size) {
-		pool = device_info->large_buffer_pool;
-		safe = dma_pool_alloc(pool, GFP_ATOMIC, &safe_dma_addr);
+	buf->ptr = ptr;
+	buf->size = size;
+	buf->direction = dir;
+	buf->pool = pool;
 
-		DO_STATS ( device_info->lbp_allocs++ );
+	if (pool) {
+		buf->safe = dma_pool_alloc(pool->pool, GFP_ATOMIC,
+					   &buf->safe_dma_addr);
 	} else {
-		pool = NULL;
-		safe = dma_alloc_coherent(dev, size, &safe_dma_addr, GFP_ATOMIC);
+		buf->safe = dma_alloc_coherent(dev, size, &buf->safe_dma_addr,
+					       GFP_ATOMIC);
 	}
 
-	if (safe == NULL) {
-		dev_warn(device_info->dev,
-			"%s: could not alloc dma memory (size=%d)\n",
-		       __func__, size);
+	if (buf->safe == NULL) {
+		dev_warn(dev,
+			 "%s: could not alloc dma memory (size=%d)\n",
+			 __func__, size);
 		kfree(buf);
 		return NULL;
 	}
 
 #ifdef STATS
+	if (pool)
+		pool->allocs++;
+	device_info->total_allocs++;
 	if (device_info->total_allocs % 1000 == 0)
 		print_alloc_stats(device_info);
 #endif
 
-	buf->ptr = ptr;
-	buf->size = size;
-	buf->direction = dir;
-	buf->pool = pool;
-	buf->safe = safe;
-	buf->safe_dma_addr = safe_dma_addr;
-
 	list_add(&buf->node, &device_info->safe_buffers);
 
 	return buf;
@@ -186,7 +189,7 @@ free_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *
 	list_del(&buf->node);
 
 	if (buf->pool)
-		dma_pool_free(buf->pool, buf->safe, buf->safe_dma_addr);
+		dma_pool_free(buf->pool->pool, buf->safe, buf->safe_dma_addr);
 	else
 		dma_free_coherent(device_info->dev, buf->size, buf->safe,
 				    buf->safe_dma_addr);
@@ -197,12 +200,10 @@ free_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *
 /* ************************************************** */
 
 #ifdef STATS
-
 static void print_map_stats(struct dmabounce_device_info *device_info)
 {
-	printk(KERN_INFO
-		"%s: dmabounce: map_op_count=%lu, bounce_count=%lu\n",
-		device_info->dev->bus_id,
+	dev_info(device_info->dev,
+		"dmabounce: map_op_count=%lu, bounce_count=%lu\n",
 		device_info->map_op_count, device_info->bounce_count);
 }
 #endif
@@ -258,13 +259,13 @@ map_single(struct device *dev, void *ptr, size_t size,
 				__func__, ptr, buf->safe, size);
 			memcpy(buf->safe, ptr, size);
 		}
-		consistent_sync(buf->safe, size, dir);
+		ptr = buf->safe;
 
 		dma_addr = buf->safe_dma_addr;
-	} else {
-		consistent_sync(ptr, size, dir);
 	}
 
+	consistent_sync(ptr, size, dir);
+
 	return dma_addr;
 }
 
@@ -278,7 +279,7 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 	/*
 	 * Trying to unmap an invalid mapping
 	 */
-	if (dma_addr == ~0) {
+	if (dma_mapping_error(dma_addr)) {
 		dev_err(dev, "Trying to unmap invalid mapping\n");
 		return;
 	}
@@ -570,11 +571,25 @@ dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
 	local_irq_restore(flags);
 }
 
+static int
+dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev, const char *name,
+		    unsigned long size)
+{
+	pool->size = size;
+	DO_STATS(pool->allocs = 0);
+	pool->pool = dma_pool_create(name, dev, size,
+				     0 /* byte alignment */,
+				     0 /* no page-crossing issues */);
+
+	return pool->pool ? 0 : -ENOMEM;
+}
+
 int
 dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 			unsigned long large_buffer_size)
 {
 	struct dmabounce_device_info *device_info;
+	int ret;
 
 	device_info = kmalloc(sizeof(struct dmabounce_device_info), GFP_ATOMIC);
 	if (!device_info) {
@@ -584,45 +599,31 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 		return -ENOMEM;
 	}
 
-	device_info->small_buffer_pool =
-		dma_pool_create("small_dmabounce_pool",
-				dev,
-				small_buffer_size,
-				0 /* byte alignment */,
-				0 /* no page-crossing issues */);
-	if (!device_info->small_buffer_pool) {
-		printk(KERN_ERR
-			"dmabounce: could not allocate small DMA pool for %s\n",
-			dev->bus_id);
-		kfree(device_info);
-		return -ENOMEM;
+	ret = dmabounce_init_pool(&device_info->small, dev,
+				  "small_dmabounce_pool", small_buffer_size);
+	if (ret) {
+		dev_err(dev,
+			"dmabounce: could not allocate DMA pool for %ld byte objects\n",
+			small_buffer_size);
+		goto err_free;
 	}
 
 	if (large_buffer_size) {
-		device_info->large_buffer_pool =
-			dma_pool_create("large_dmabounce_pool",
-					dev,
-					large_buffer_size,
-					0 /* byte alignment */,
-					0 /* no page-crossing issues */);
-		if (!device_info->large_buffer_pool) {
-		printk(KERN_ERR
-			"dmabounce: could not allocate large DMA pool for %s\n",
-			dev->bus_id);
-			dma_pool_destroy(device_info->small_buffer_pool);
-
-			return -ENOMEM;
+		ret = dmabounce_init_pool(&device_info->large, dev,
+					  "large_dmabounce_pool",
+					  large_buffer_size);
+		if (ret) {
+			dev_err(dev,
+				"dmabounce: could not allocate DMA pool for %ld byte objects\n",
+				large_buffer_size);
+			goto err_destroy;
 		}
 	}
 
 	device_info->dev = dev;
-	device_info->small_buffer_size = small_buffer_size;
-	device_info->large_buffer_size = large_buffer_size;
 	INIT_LIST_HEAD(&device_info->safe_buffers);
 
 #ifdef STATS
-	device_info->sbp_allocs = 0;
-	device_info->lbp_allocs = 0;
 	device_info->total_allocs = 0;
 	device_info->map_op_count = 0;
 	device_info->bounce_count = 0;
@@ -634,6 +635,12 @@ dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
 		dev->bus_id, dev->bus->name);
 
 	return 0;
+
+ err_destroy:
+	dma_pool_destroy(device_info->small.pool);
+ err_free:
+	kfree(device_info);
+	return ret;
 }
 
 void
@@ -655,10 +662,10 @@ dmabounce_unregister_dev(struct device *dev)
 		BUG();
 	}
 
-	if (device_info->small_buffer_pool)
-		dma_pool_destroy(device_info->small_buffer_pool);
-	if (device_info->large_buffer_pool)
-		dma_pool_destroy(device_info->large_buffer_pool);
+	if (device_info->small.pool)
+		dma_pool_destroy(device_info->small.pool);
+	if (device_info->large.pool)
+		dma_pool_destroy(device_info->large.pool);
 
 #ifdef STATS
 	print_alloc_stats(device_info);

commit b46a58fd4e3cdd107ffccedaa5e233deeb94c5c0
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Wed Jun 22 21:25:58 2005 +0100

    [PATCH] ARM: Use list_for_each_entry() for dmabounce
    
    Convert dmabounce.c to use list_for_each_entry() instead of
    list_for_each() + list_entry().
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 9d63a01214eb..cbf2165476b0 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -93,15 +93,12 @@ static void print_alloc_stats(struct dmabounce_device_info *device_info)
 static inline struct dmabounce_device_info *
 find_dmabounce_dev(struct device *dev)
 {
-	struct list_head *entry;
-
-	list_for_each(entry, &dmabounce_devs) {
-		struct dmabounce_device_info *d =
-			list_entry(entry, struct dmabounce_device_info, node);
+	struct dmabounce_device_info *d;
 
+	list_for_each_entry(d, &dmabounce_devs, node)
 		if (d->dev == dev)
 			return d;
-	}
+
 	return NULL;
 }
 
@@ -172,15 +169,11 @@ alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
 static inline struct safe_buffer *
 find_safe_buffer(struct dmabounce_device_info *device_info, dma_addr_t safe_dma_addr)
 {
-	struct list_head *entry;
-
-	list_for_each(entry, &device_info->safe_buffers) {
-		struct safe_buffer *b =
-			list_entry(entry, struct safe_buffer, node);
+	struct safe_buffer *b;
 
+	list_for_each_entry(b, &device_info->safe_buffers, node)
 		if (b->safe_dma_addr == safe_dma_addr)
 			return b;
-	}
 
 	return NULL;
 }
@@ -301,7 +294,6 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
 			buf->safe, (void *) buf->safe_dma_addr);
 
-
 		DO_STATS ( device_info->bounce_count++ );
 
 		if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {

commit 14eb75b6f83e3213d752dfe821df2fa57efbe519
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Jun 20 16:56:08 2005 +0100

    [PATCH] ARM: Add missed include for dmabounce.c
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 39dd33ff6f37..9d63a01214eb 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -30,6 +30,8 @@
 #include <linux/dmapool.h>
 #include <linux/list.h>
 
+#include <asm/cacheflush.h>
+
 #undef DEBUG
 
 #undef STATS

commit 5abc100e882a979f8f5be8184938305b3fb2df2b
Author: Russell King <rmk@dyn-67.arm.linux.org.uk>
Date:   Mon Jun 20 12:31:14 2005 +0100

    [PATCH] ARM: Ensure DMA-bounced buffers are properly written to RAM
    
    When DMA bounce buffers were unmapped and the data was memcpy'd to
    the original buffer, we were not ensuring that the data was written
    to RAM.  This means that there was the potential for page cache
    pages to have different cache states depending whether they've been
    bounced or not.
    
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
index 5797b1b100a1..39dd33ff6f37 100644
--- a/arch/arm/common/dmabounce.c
+++ b/arch/arm/common/dmabounce.c
@@ -302,12 +302,24 @@ unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
 
 		DO_STATS ( device_info->bounce_count++ );
 
-		if ((dir == DMA_FROM_DEVICE) ||
-		    (dir == DMA_BIDIRECTIONAL)) {
+		if (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {
+			unsigned long ptr;
+
 			dev_dbg(dev,
 				"%s: copy back safe %p to unsafe %p size %d\n",
 				__func__, buf->safe, buf->ptr, size);
 			memcpy(buf->ptr, buf->safe, size);
+
+			/*
+			 * DMA buffers must have the same cache properties
+			 * as if they were really used for DMA - which means
+			 * data must be written back to RAM.  Note that
+			 * we don't use dmac_flush_range() here for the
+			 * bidirectional case because we know the cache
+			 * lines will be coherent with the data written.
+			 */
+			ptr = (unsigned long)buf->ptr;
+			dmac_clean_range(ptr, ptr + size);
 		}
 		free_safe_buffer(device_info, buf);
 	}

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/arch/arm/common/dmabounce.c b/arch/arm/common/dmabounce.c
new file mode 100644
index 000000000000..5797b1b100a1
--- /dev/null
+++ b/arch/arm/common/dmabounce.c
@@ -0,0 +1,682 @@
+/*
+ *  arch/arm/common/dmabounce.c
+ *
+ *  Special dma_{map/unmap/dma_sync}_* routines for systems that have
+ *  limited DMA windows. These functions utilize bounce buffers to
+ *  copy data to/from buffers located outside the DMA region. This
+ *  only works for systems in which DMA memory is at the bottom of
+ *  RAM and the remainder of memory is at the top an the DMA memory
+ *  can be marked as ZONE_DMA. Anything beyond that such as discontigous
+ *  DMA windows will require custom implementations that reserve memory
+ *  areas at early bootup.
+ *
+ *  Original version by Brad Parker (brad@heeltoe.com)
+ *  Re-written by Christopher Hoover <ch@murgatroid.com>
+ *  Made generic by Deepak Saxena <dsaxena@plexity.net>
+ *
+ *  Copyright (C) 2002 Hewlett Packard Company.
+ *  Copyright (C) 2004 MontaVista Software, Inc.
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmapool.h>
+#include <linux/list.h>
+
+#undef DEBUG
+
+#undef STATS
+#ifdef STATS
+#define DO_STATS(X) do { X ; } while (0)
+#else
+#define DO_STATS(X) do { } while (0)
+#endif
+
+/* ************************************************** */
+
+struct safe_buffer {
+	struct list_head node;
+
+	/* original request */
+	void		*ptr;
+	size_t		size;
+	int		direction;
+
+	/* safe buffer info */
+	struct dma_pool *pool;
+	void		*safe;
+	dma_addr_t	safe_dma_addr;
+};
+
+struct dmabounce_device_info {
+	struct list_head node;
+
+	struct device *dev;
+	struct dma_pool *small_buffer_pool;
+	struct dma_pool *large_buffer_pool;
+	struct list_head safe_buffers;
+	unsigned long small_buffer_size, large_buffer_size;
+#ifdef STATS
+	unsigned long sbp_allocs;
+	unsigned long lbp_allocs;
+	unsigned long total_allocs;
+	unsigned long map_op_count;
+	unsigned long bounce_count;
+#endif
+};
+
+static LIST_HEAD(dmabounce_devs);
+
+#ifdef STATS
+static void print_alloc_stats(struct dmabounce_device_info *device_info)
+{
+	printk(KERN_INFO
+		"%s: dmabounce: sbp: %lu, lbp: %lu, other: %lu, total: %lu\n",
+		device_info->dev->bus_id,
+		device_info->sbp_allocs, device_info->lbp_allocs,
+		device_info->total_allocs - device_info->sbp_allocs -
+			device_info->lbp_allocs,
+		device_info->total_allocs);
+}
+#endif
+
+/* find the given device in the dmabounce device list */
+static inline struct dmabounce_device_info *
+find_dmabounce_dev(struct device *dev)
+{
+	struct list_head *entry;
+
+	list_for_each(entry, &dmabounce_devs) {
+		struct dmabounce_device_info *d =
+			list_entry(entry, struct dmabounce_device_info, node);
+
+		if (d->dev == dev)
+			return d;
+	}
+	return NULL;
+}
+
+
+/* allocate a 'safe' buffer and keep track of it */
+static inline struct safe_buffer *
+alloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,
+			size_t size, enum dma_data_direction dir)
+{
+	struct safe_buffer *buf;
+	struct dma_pool *pool;
+	struct device *dev = device_info->dev;
+	void *safe;
+	dma_addr_t safe_dma_addr;
+
+	dev_dbg(dev, "%s(ptr=%p, size=%d, dir=%d)\n",
+		__func__, ptr, size, dir);
+
+	DO_STATS ( device_info->total_allocs++ );
+
+	buf = kmalloc(sizeof(struct safe_buffer), GFP_ATOMIC);
+	if (buf == NULL) {
+		dev_warn(dev, "%s: kmalloc failed\n", __func__);
+		return NULL;
+	}
+
+	if (size <= device_info->small_buffer_size) {
+		pool = device_info->small_buffer_pool;
+		safe = dma_pool_alloc(pool, GFP_ATOMIC, &safe_dma_addr);
+
+		DO_STATS ( device_info->sbp_allocs++ );
+	} else if (size <= device_info->large_buffer_size) {
+		pool = device_info->large_buffer_pool;
+		safe = dma_pool_alloc(pool, GFP_ATOMIC, &safe_dma_addr);
+
+		DO_STATS ( device_info->lbp_allocs++ );
+	} else {
+		pool = NULL;
+		safe = dma_alloc_coherent(dev, size, &safe_dma_addr, GFP_ATOMIC);
+	}
+
+	if (safe == NULL) {
+		dev_warn(device_info->dev,
+			"%s: could not alloc dma memory (size=%d)\n",
+		       __func__, size);
+		kfree(buf);
+		return NULL;
+	}
+
+#ifdef STATS
+	if (device_info->total_allocs % 1000 == 0)
+		print_alloc_stats(device_info);
+#endif
+
+	buf->ptr = ptr;
+	buf->size = size;
+	buf->direction = dir;
+	buf->pool = pool;
+	buf->safe = safe;
+	buf->safe_dma_addr = safe_dma_addr;
+
+	list_add(&buf->node, &device_info->safe_buffers);
+
+	return buf;
+}
+
+/* determine if a buffer is from our "safe" pool */
+static inline struct safe_buffer *
+find_safe_buffer(struct dmabounce_device_info *device_info, dma_addr_t safe_dma_addr)
+{
+	struct list_head *entry;
+
+	list_for_each(entry, &device_info->safe_buffers) {
+		struct safe_buffer *b =
+			list_entry(entry, struct safe_buffer, node);
+
+		if (b->safe_dma_addr == safe_dma_addr)
+			return b;
+	}
+
+	return NULL;
+}
+
+static inline void
+free_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *buf)
+{
+	dev_dbg(device_info->dev, "%s(buf=%p)\n", __func__, buf);
+
+	list_del(&buf->node);
+
+	if (buf->pool)
+		dma_pool_free(buf->pool, buf->safe, buf->safe_dma_addr);
+	else
+		dma_free_coherent(device_info->dev, buf->size, buf->safe,
+				    buf->safe_dma_addr);
+
+	kfree(buf);
+}
+
+/* ************************************************** */
+
+#ifdef STATS
+
+static void print_map_stats(struct dmabounce_device_info *device_info)
+{
+	printk(KERN_INFO
+		"%s: dmabounce: map_op_count=%lu, bounce_count=%lu\n",
+		device_info->dev->bus_id,
+		device_info->map_op_count, device_info->bounce_count);
+}
+#endif
+
+static inline dma_addr_t
+map_single(struct device *dev, void *ptr, size_t size,
+		enum dma_data_direction dir)
+{
+	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	dma_addr_t dma_addr;
+	int needs_bounce = 0;
+
+	if (device_info)
+		DO_STATS ( device_info->map_op_count++ );
+
+	dma_addr = virt_to_dma(dev, ptr);
+
+	if (dev->dma_mask) {
+		unsigned long mask = *dev->dma_mask;
+		unsigned long limit;
+
+		limit = (mask + 1) & ~mask;
+		if (limit && size > limit) {
+			dev_err(dev, "DMA mapping too big (requested %#x "
+				"mask %#Lx)\n", size, *dev->dma_mask);
+			return ~0;
+		}
+
+		/*
+		 * Figure out if we need to bounce from the DMA mask.
+		 */
+		needs_bounce = (dma_addr | (dma_addr + size - 1)) & ~mask;
+	}
+
+	if (device_info && (needs_bounce || dma_needs_bounce(dev, dma_addr, size))) {
+		struct safe_buffer *buf;
+
+		buf = alloc_safe_buffer(device_info, ptr, size, dir);
+		if (buf == 0) {
+			dev_err(dev, "%s: unable to map unsafe buffer %p!\n",
+			       __func__, ptr);
+			return 0;
+		}
+
+		dev_dbg(dev,
+			"%s: unsafe buffer %p (phy=%p) mapped to %p (phy=%p)\n",
+			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
+			buf->safe, (void *) buf->safe_dma_addr);
+
+		if ((dir == DMA_TO_DEVICE) ||
+		    (dir == DMA_BIDIRECTIONAL)) {
+			dev_dbg(dev, "%s: copy unsafe %p to safe %p, size %d\n",
+				__func__, ptr, buf->safe, size);
+			memcpy(buf->safe, ptr, size);
+		}
+		consistent_sync(buf->safe, size, dir);
+
+		dma_addr = buf->safe_dma_addr;
+	} else {
+		consistent_sync(ptr, size, dir);
+	}
+
+	return dma_addr;
+}
+
+static inline void
+unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		enum dma_data_direction dir)
+{
+	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	struct safe_buffer *buf = NULL;
+
+	/*
+	 * Trying to unmap an invalid mapping
+	 */
+	if (dma_addr == ~0) {
+		dev_err(dev, "Trying to unmap invalid mapping\n");
+		return;
+	}
+
+	if (device_info)
+		buf = find_safe_buffer(device_info, dma_addr);
+
+	if (buf) {
+		BUG_ON(buf->size != size);
+
+		dev_dbg(dev,
+			"%s: unsafe buffer %p (phy=%p) mapped to %p (phy=%p)\n",
+			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
+			buf->safe, (void *) buf->safe_dma_addr);
+
+
+		DO_STATS ( device_info->bounce_count++ );
+
+		if ((dir == DMA_FROM_DEVICE) ||
+		    (dir == DMA_BIDIRECTIONAL)) {
+			dev_dbg(dev,
+				"%s: copy back safe %p to unsafe %p size %d\n",
+				__func__, buf->safe, buf->ptr, size);
+			memcpy(buf->ptr, buf->safe, size);
+		}
+		free_safe_buffer(device_info, buf);
+	}
+}
+
+static inline void
+sync_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		enum dma_data_direction dir)
+{
+	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+	struct safe_buffer *buf = NULL;
+
+	if (device_info)
+		buf = find_safe_buffer(device_info, dma_addr);
+
+	if (buf) {
+		/*
+		 * Both of these checks from original code need to be
+		 * commented out b/c some drivers rely on the following:
+		 *
+		 * 1) Drivers may map a large chunk of memory into DMA space
+		 *    but only sync a small portion of it. Good example is
+		 *    allocating a large buffer, mapping it, and then
+		 *    breaking it up into small descriptors. No point
+		 *    in syncing the whole buffer if you only have to
+		 *    touch one descriptor.
+		 *
+		 * 2) Buffers that are mapped as DMA_BIDIRECTIONAL are
+		 *    usually only synced in one dir at a time.
+		 *
+		 * See drivers/net/eepro100.c for examples of both cases.
+		 *
+		 * -ds
+		 *
+		 * BUG_ON(buf->size != size);
+		 * BUG_ON(buf->direction != dir);
+		 */
+
+		dev_dbg(dev,
+			"%s: unsafe buffer %p (phy=%p) mapped to %p (phy=%p)\n",
+			__func__, buf->ptr, (void *) virt_to_dma(dev, buf->ptr),
+			buf->safe, (void *) buf->safe_dma_addr);
+
+		DO_STATS ( device_info->bounce_count++ );
+
+		switch (dir) {
+		case DMA_FROM_DEVICE:
+			dev_dbg(dev,
+				"%s: copy back safe %p to unsafe %p size %d\n",
+				__func__, buf->safe, buf->ptr, size);
+			memcpy(buf->ptr, buf->safe, size);
+			break;
+		case DMA_TO_DEVICE:
+			dev_dbg(dev,
+				"%s: copy out unsafe %p to safe %p, size %d\n",
+				__func__,buf->ptr, buf->safe, size);
+			memcpy(buf->safe, buf->ptr, size);
+			break;
+		case DMA_BIDIRECTIONAL:
+			BUG();	/* is this allowed?  what does it mean? */
+		default:
+			BUG();
+		}
+		consistent_sync(buf->safe, size, dir);
+	} else {
+		consistent_sync(dma_to_virt(dev, dma_addr), size, dir);
+	}
+}
+
+/* ************************************************** */
+
+/*
+ * see if a buffer address is in an 'unsafe' range.  if it is
+ * allocate a 'safe' buffer and copy the unsafe buffer into it.
+ * substitute the safe buffer for the unsafe one.
+ * (basically move the buffer from an unsafe area to a safe one)
+ */
+dma_addr_t
+dma_map_single(struct device *dev, void *ptr, size_t size,
+		enum dma_data_direction dir)
+{
+	unsigned long flags;
+	dma_addr_t dma_addr;
+
+	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
+		__func__, ptr, size, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	local_irq_save(flags);
+
+	dma_addr = map_single(dev, ptr, size, dir);
+
+	local_irq_restore(flags);
+
+	return dma_addr;
+}
+
+/*
+ * see if a mapped address was really a "safe" buffer and if so, copy
+ * the data from the safe buffer back to the unsafe buffer and free up
+ * the safe buffer.  (basically return things back to the way they
+ * should be)
+ */
+
+void
+dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+			enum dma_data_direction dir)
+{
+	unsigned long flags;
+
+	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
+		__func__, (void *) dma_addr, size, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	local_irq_save(flags);
+
+	unmap_single(dev, dma_addr, size, dir);
+
+	local_irq_restore(flags);
+}
+
+int
+dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction dir)
+{
+	unsigned long flags;
+	int i;
+
+	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
+		__func__, sg, nents, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	local_irq_save(flags);
+
+	for (i = 0; i < nents; i++, sg++) {
+		struct page *page = sg->page;
+		unsigned int offset = sg->offset;
+		unsigned int length = sg->length;
+		void *ptr = page_address(page) + offset;
+
+		sg->dma_address =
+			map_single(dev, ptr, length, dir);
+	}
+
+	local_irq_restore(flags);
+
+	return nents;
+}
+
+void
+dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction dir)
+{
+	unsigned long flags;
+	int i;
+
+	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
+		__func__, sg, nents, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	local_irq_save(flags);
+
+	for (i = 0; i < nents; i++, sg++) {
+		dma_addr_t dma_addr = sg->dma_address;
+		unsigned int length = sg->length;
+
+		unmap_single(dev, dma_addr, length, dir);
+	}
+
+	local_irq_restore(flags);
+}
+
+void
+dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_addr, size_t size,
+				enum dma_data_direction dir)
+{
+	unsigned long flags;
+
+	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
+		__func__, (void *) dma_addr, size, dir);
+
+	local_irq_save(flags);
+
+	sync_single(dev, dma_addr, size, dir);
+
+	local_irq_restore(flags);
+}
+
+void
+dma_sync_single_for_device(struct device *dev, dma_addr_t dma_addr, size_t size,
+				enum dma_data_direction dir)
+{
+	unsigned long flags;
+
+	dev_dbg(dev, "%s(ptr=%p,size=%d,dir=%x)\n",
+		__func__, (void *) dma_addr, size, dir);
+
+	local_irq_save(flags);
+
+	sync_single(dev, dma_addr, size, dir);
+
+	local_irq_restore(flags);
+}
+
+void
+dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,
+			enum dma_data_direction dir)
+{
+	unsigned long flags;
+	int i;
+
+	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
+		__func__, sg, nents, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	local_irq_save(flags);
+
+	for (i = 0; i < nents; i++, sg++) {
+		dma_addr_t dma_addr = sg->dma_address;
+		unsigned int length = sg->length;
+
+		sync_single(dev, dma_addr, length, dir);
+	}
+
+	local_irq_restore(flags);
+}
+
+void
+dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg, int nents,
+			enum dma_data_direction dir)
+{
+	unsigned long flags;
+	int i;
+
+	dev_dbg(dev, "%s(sg=%p,nents=%d,dir=%x)\n",
+		__func__, sg, nents, dir);
+
+	BUG_ON(dir == DMA_NONE);
+
+	local_irq_save(flags);
+
+	for (i = 0; i < nents; i++, sg++) {
+		dma_addr_t dma_addr = sg->dma_address;
+		unsigned int length = sg->length;
+
+		sync_single(dev, dma_addr, length, dir);
+	}
+
+	local_irq_restore(flags);
+}
+
+int
+dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,
+			unsigned long large_buffer_size)
+{
+	struct dmabounce_device_info *device_info;
+
+	device_info = kmalloc(sizeof(struct dmabounce_device_info), GFP_ATOMIC);
+	if (!device_info) {
+		printk(KERN_ERR
+			"Could not allocated dmabounce_device_info for %s",
+			dev->bus_id);
+		return -ENOMEM;
+	}
+
+	device_info->small_buffer_pool =
+		dma_pool_create("small_dmabounce_pool",
+				dev,
+				small_buffer_size,
+				0 /* byte alignment */,
+				0 /* no page-crossing issues */);
+	if (!device_info->small_buffer_pool) {
+		printk(KERN_ERR
+			"dmabounce: could not allocate small DMA pool for %s\n",
+			dev->bus_id);
+		kfree(device_info);
+		return -ENOMEM;
+	}
+
+	if (large_buffer_size) {
+		device_info->large_buffer_pool =
+			dma_pool_create("large_dmabounce_pool",
+					dev,
+					large_buffer_size,
+					0 /* byte alignment */,
+					0 /* no page-crossing issues */);
+		if (!device_info->large_buffer_pool) {
+		printk(KERN_ERR
+			"dmabounce: could not allocate large DMA pool for %s\n",
+			dev->bus_id);
+			dma_pool_destroy(device_info->small_buffer_pool);
+
+			return -ENOMEM;
+		}
+	}
+
+	device_info->dev = dev;
+	device_info->small_buffer_size = small_buffer_size;
+	device_info->large_buffer_size = large_buffer_size;
+	INIT_LIST_HEAD(&device_info->safe_buffers);
+
+#ifdef STATS
+	device_info->sbp_allocs = 0;
+	device_info->lbp_allocs = 0;
+	device_info->total_allocs = 0;
+	device_info->map_op_count = 0;
+	device_info->bounce_count = 0;
+#endif
+
+	list_add(&device_info->node, &dmabounce_devs);
+
+	printk(KERN_INFO "dmabounce: registered device %s on %s bus\n",
+		dev->bus_id, dev->bus->name);
+
+	return 0;
+}
+
+void
+dmabounce_unregister_dev(struct device *dev)
+{
+	struct dmabounce_device_info *device_info = find_dmabounce_dev(dev);
+
+	if (!device_info) {
+		printk(KERN_WARNING
+			"%s: Never registered with dmabounce but attempting" \
+			"to unregister!\n", dev->bus_id);
+		return;
+	}
+
+	if (!list_empty(&device_info->safe_buffers)) {
+		printk(KERN_ERR
+			"%s: Removing from dmabounce with pending buffers!\n",
+			dev->bus_id);
+		BUG();
+	}
+
+	if (device_info->small_buffer_pool)
+		dma_pool_destroy(device_info->small_buffer_pool);
+	if (device_info->large_buffer_pool)
+		dma_pool_destroy(device_info->large_buffer_pool);
+
+#ifdef STATS
+	print_alloc_stats(device_info);
+	print_map_stats(device_info);
+#endif
+
+	list_del(&device_info->node);
+
+	kfree(device_info);
+
+	printk(KERN_INFO "dmabounce: device %s on %s bus unregistered\n",
+		dev->bus_id, dev->bus->name);
+}
+
+
+EXPORT_SYMBOL(dma_map_single);
+EXPORT_SYMBOL(dma_unmap_single);
+EXPORT_SYMBOL(dma_map_sg);
+EXPORT_SYMBOL(dma_unmap_sg);
+EXPORT_SYMBOL(dma_sync_single);
+EXPORT_SYMBOL(dma_sync_sg);
+EXPORT_SYMBOL(dmabounce_register_dev);
+EXPORT_SYMBOL(dmabounce_unregister_dev);
+
+MODULE_AUTHOR("Christopher Hoover <ch@hpl.hp.com>, Deepak Saxena <dsaxena@plexity.net>");
+MODULE_DESCRIPTION("Special dma_{map/unmap/dma_sync}_* routines for systems with limited DMA windows");
+MODULE_LICENSE("GPL");
