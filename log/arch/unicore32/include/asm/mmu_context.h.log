commit 42222eae17f7c930833dfda7896ef280879de94a
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Thu Jan 23 10:41:16 2020 -0800

    mm: remove arch_bprm_mm_init() hook
    
    From: Dave Hansen <dave.hansen@linux.intel.com>
    
    MPX is being removed from the kernel due to a lack of support
    in the toolchain going forward (gcc).
    
    arch_bprm_mm_init() is used at execve() time.  The only non-stub
    implementation is on x86 for MPX.  Remove the hook entirely from
    all architectures and generic code.
    
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: x86@kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arch@vger.kernel.org
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 247a07ae2cdc..388c0c811c68 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -89,11 +89,6 @@ static inline void arch_unmap(struct mm_struct *mm,
 {
 }
 
-static inline void arch_bprm_mm_init(struct mm_struct *mm,
-				     struct vm_area_struct *vma)
-{
-}
-
 static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
 		bool write, bool execute, bool foreign)
 {

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 9f06ea5466dd..247a07ae2cdc 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -1,13 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * linux/arch/unicore32/include/asm/mmu_context.h
  *
  * Code specific to PKUnity SoC and UniCore ISA
  *
  * Copyright (C) 2001-2010 GUAN Xue-tao
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #ifndef __UNICORE_MMU_CONTEXT_H__
 #define __UNICORE_MMU_CONTEXT_H__

commit 5a28fc94c9143db766d1ba5480cae82d856ad080
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Apr 19 12:47:47 2019 -0700

    x86/mpx, mm/core: Fix recursive munmap() corruption
    
    This is a bit of a mess, to put it mildly.  But, it's a bug
    that only seems to have showed up in 4.20 but wasn't noticed
    until now, because nobody uses MPX.
    
    MPX has the arch_unmap() hook inside of munmap() because MPX
    uses bounds tables that protect other areas of memory.  When
    memory is unmapped, there is also a need to unmap the MPX
    bounds tables.  Barring this, unused bounds tables can eat 80%
    of the address space.
    
    But, the recursive do_munmap() that gets called vi arch_unmap()
    wreaks havoc with __do_munmap()'s state.  It can result in
    freeing populated page tables, accessing bogus VMA state,
    double-freed VMAs and more.
    
    See the "long story" further below for the gory details.
    
    To fix this, call arch_unmap() before __do_unmap() has a chance
    to do anything meaningful.  Also, remove the 'vma' argument
    and force the MPX code to do its own, independent VMA lookup.
    
    == UML / unicore32 impact ==
    
    Remove unused 'vma' argument to arch_unmap().  No functional
    change.
    
    I compile tested this on UML but not unicore32.
    
    == powerpc impact ==
    
    powerpc uses arch_unmap() well to watch for munmap() on the
    VDSO and zeroes out 'current->mm->context.vdso_base'.  Moving
    arch_unmap() makes this happen earlier in __do_munmap().  But,
    'vdso_base' seems to only be used in perf and in the signal
    delivery that happens near the return to userspace.  I can not
    find any likely impact to powerpc, other than the zeroing
    happening a little earlier.
    
    powerpc does not use the 'vma' argument and is unaffected by
    its removal.
    
    I compile-tested a 64-bit powerpc defconfig.
    
    == x86 impact ==
    
    For the common success case this is functionally identical to
    what was there before.  For the munmap() failure case, it's
    possible that some MPX tables will be zapped for memory that
    continues to be in use.  But, this is an extraordinarily
    unlikely scenario and the harm would be that MPX provides no
    protection since the bounds table got reset (zeroed).
    
    I can't imagine anyone doing this:
    
            ptr = mmap();
            // use ptr
            ret = munmap(ptr);
            if (ret)
                    // oh, there was an error, I'll
                    // keep using ptr.
    
    Because if you're doing munmap(), you are *done* with the
    memory.  There's probably no good data in there _anyway_.
    
    This passes the original reproducer from Richard Biener as
    well as the existing mpx selftests/.
    
    The long story:
    
    munmap() has a couple of pieces:
    
     1. Find the affected VMA(s)
     2. Split the start/end one(s) if neceesary
     3. Pull the VMAs out of the rbtree
     4. Actually zap the memory via unmap_region(), including
        freeing page tables (or queueing them to be freed).
     5. Fix up some of the accounting (like fput()) and actually
        free the VMA itself.
    
    This specific ordering was actually introduced by:
    
      dd2283f2605e ("mm: mmap: zap pages with read mmap_sem in munmap")
    
    during the 4.20 merge window.  The previous __do_munmap() code
    was actually safe because the only thing after arch_unmap() was
    remove_vma_list().  arch_unmap() could not see 'vma' in the
    rbtree because it was detached, so it is not even capable of
    doing operations unsafe for remove_vma_list()'s use of 'vma'.
    
    Richard Biener reported a test that shows this in dmesg:
    
      [1216548.787498] BUG: Bad rss-counter state mm:0000000017ce560b idx:1 val:551
      [1216548.787500] BUG: non-zero pgtables_bytes on freeing mm: 24576
    
    What triggered this was the recursive do_munmap() called via
    arch_unmap().  It was freeing page tables that has not been
    properly zapped.
    
    But, the problem was bigger than this.  For one, arch_unmap()
    can free VMAs.  But, the calling __do_munmap() has variables
    that *point* to VMAs and obviously can't handle them just
    getting freed while the pointer is still in use.
    
    I tried a couple of things here.  First, I tried to fix the page
    table freeing problem in isolation, but I then found the VMA
    issue.  I also tried having the MPX code return a flag if it
    modified the rbtree which would force __do_munmap() to re-walk
    to restart.  That spiralled out of control in complexity pretty
    fast.
    
    Just moving arch_unmap() and accepting that the bonkers failure
    case might eat some bounds tables seems like the simplest viable
    fix.
    
    This was also reported in the following kernel bugzilla entry:
    
      https://bugzilla.kernel.org/show_bug.cgi?id=203123
    
    There are some reports that this commit triggered this bug:
    
      dd2283f2605 ("mm: mmap: zap pages with read mmap_sem in munmap")
    
    While that commit certainly made the issues easier to hit, I believe
    the fundamental issue has been with us as long as MPX itself, thus
    the Fixes: tag below is for one of the original MPX commits.
    
    [ mingo: Minor edits to the changelog and the patch. ]
    
    Reported-by: Richard Biener <rguenther@suse.de>
    Reported-by: H.J. Lu <hjl.tools@gmail.com>
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Yang Shi <yang.shi@linux.alibaba.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Anton Ivanov <anton.ivanov@cambridgegreys.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-um@lists.infradead.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: stable@vger.kernel.org
    Fixes: dd2283f2605e ("mm: mmap: zap pages with read mmap_sem in munmap")
    Link: http://lkml.kernel.org/r/20190419194747.5E1AD6DC@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 5c205a9cb5a6..9f06ea5466dd 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -88,7 +88,6 @@ static inline int arch_dup_mmap(struct mm_struct *oldmm,
 }
 
 static inline void arch_unmap(struct mm_struct *mm,
-			struct vm_area_struct *vma,
 			unsigned long start, unsigned long end)
 {
 }

commit c10e83f598d08046dd1ebc8360d4bb12d802d51b
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 14 12:27:29 2017 +0100

    arch, mm: Allow arch_dup_mmap() to fail
    
    In order to sanitize the LDT initialization on x86 arch_dup_mmap() must be
    allowed to fail. Fix up all instances.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirsky <luto@kernel.org>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Borislav Petkov <bpetkov@suse.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Laight <David.Laight@aculab.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Eduardo Valentin <eduval@amazon.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: aliguori@amazon.com
    Cc: dan.j.williams@intel.com
    Cc: hughd@google.com
    Cc: keescook@google.com
    Cc: kirill.shutemov@linux.intel.com
    Cc: linux-mm@kvack.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 59b06b48f27d..5c205a9cb5a6 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -81,9 +81,10 @@ do { \
 	} \
 } while (0)
 
-static inline void arch_dup_mmap(struct mm_struct *oldmm,
-				 struct mm_struct *mm)
+static inline int arch_dup_mmap(struct mm_struct *oldmm,
+				struct mm_struct *mm)
 {
+	return 0;
 }
 
 static inline void arch_unmap(struct mm_struct *mm,

commit 9a804fecee232e71b47ac37d62fd3d5d66b08b91
Author: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
Date:   Thu Mar 16 18:26:49 2017 +0300

    mm/gup: Drop the arch_pte_access_permitted() MMU callback
    
    The only arch that defines it to something meaningful is x86.
    But x86 doesn't use the generic GUP_fast() implementation -- the
    only place where the callback is called.
    
    Let's drop it.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aneesh Kumar K . V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dann Frazier <dann.frazier@canonical.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Steve Capper <steve.capper@linaro.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-mm@kvack.org
    Link: http://lkml.kernel.org/r/20170316152655.37789-2-kirill.shutemov@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 62dfc644c908..59b06b48f27d 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -103,10 +103,4 @@ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
 	/* by default, allow everything */
 	return true;
 }
-
-static inline bool arch_pte_access_permitted(pte_t pte, bool write)
-{
-	/* by default, allow everything */
-	return true;
-}
 #endif

commit 783011b13095430262333fd64e17666954064664
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Mon Mar 21 04:20:53 2016 -0700

    unicore32: mm: Add missing parameter to arch_vma_access_permitted
    
    unicore32 fails to compile with the following errors.
    
    mm/memory.c: In function ‘__handle_mm_fault’:
    mm/memory.c:3381: error:
            too many arguments to function ‘arch_vma_access_permitted’
    mm/gup.c: In function ‘check_vma_flags’:
    mm/gup.c:456: error:
            too many arguments to function ‘arch_vma_access_permitted’
    mm/gup.c: In function ‘vma_permits_fault’:
    mm/gup.c:640: error:
            too many arguments to function ‘arch_vma_access_permitted’
    
    Fixes: d61172b4b695b ("mm/core, x86/mm/pkeys: Differentiate instruction fetches")
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Acked-by: Guan Xuetao <gxt@mprc.pku.edu.cn>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index e35632ef23c7..62dfc644c908 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -98,7 +98,7 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,
 }
 
 static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
-		bool write, bool foreign)
+		bool write, bool execute, bool foreign)
 {
 	/* by default, allow everything */
 	return true;

commit 1b2ee1266ea647713dbaf44825967c180dfc8d76
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:21 2016 -0800

    mm/core: Do not enforce PKEY permissions on remote mm access
    
    We try to enforce protection keys in software the same way that we
    do in hardware.  (See long example below).
    
    But, we only want to do this when accessing our *own* process's
    memory.  If GDB set PKRU[6].AD=1 (disable access to PKEY 6), then
    tried to PTRACE_POKE a target process which just happened to have
    some mprotect_pkey(pkey=6) memory, we do *not* want to deny the
    debugger access to that memory.  PKRU is fundamentally a
    thread-local structure and we do not want to enforce it on access
    to _another_ thread's data.
    
    This gets especially tricky when we have workqueues or other
    delayed-work mechanisms that might run in a random process's context.
    We can check that we only enforce pkeys when operating on our *own* mm,
    but delayed work gets performed when a random user context is active.
    We might end up with a situation where a delayed-work gup fails when
    running randomly under its "own" task but succeeds when running under
    another process.  We want to avoid that.
    
    To avoid that, we use the new GUP flag: FOLL_REMOTE and add a
    fault flag: FAULT_FLAG_REMOTE.  They indicate that we are
    walking an mm which is not guranteed to be the same as
    current->mm and should not be subject to protection key
    enforcement.
    
    Thanks to Jerome Glisse for pointing out this scenario.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexey Kardashevskiy <aik@ozlabs.ru>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boaz Harrosh <boaz@plexistor.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Chinner <dchinner@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Dominik Vogt <vogt@linux.vnet.ibm.com>
    Cc: Eric B Munson <emunson@akamai.com>
    Cc: Geliang Tang <geliangtang@163.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Joerg Roedel <joro@8bytes.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Konstantin Khlebnikov <koct9i@gmail.com>
    Cc: Laurent Dufour <ldufour@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mikulas Patocka <mpatocka@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Shachar Raindel <raindel@mellanox.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Xie XiuQi <xiexiuqi@huawei.com>
    Cc: iommu@lists.linux-foundation.org
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 3133f947ade2..e35632ef23c7 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -97,7 +97,8 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,
 {
 }
 
-static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
+		bool write, bool foreign)
 {
 	/* by default, allow everything */
 	return true;

commit 33a709b25a760b91184bb335cf7d7c32b8123013
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Feb 12 13:02:19 2016 -0800

    mm/gup, x86/mm/pkeys: Check VMAs and PTEs for protection keys
    
    Today, for normal faults and page table walks, we check the VMA
    and/or PTE to ensure that it is compatible with the action.  For
    instance, if we get a write fault on a non-writeable VMA, we
    SIGSEGV.
    
    We try to do the same thing for protection keys.  Basically, we
    try to make sure that if a user does this:
    
            mprotect(ptr, size, PROT_NONE);
            *ptr = foo;
    
    they see the same effects with protection keys when they do this:
    
            mprotect(ptr, size, PROT_READ|PROT_WRITE);
            set_pkey(ptr, size, 4);
            wrpkru(0xffffff3f); // access disable pkey 4
            *ptr = foo;
    
    The state to do that checking is in the VMA, but we also
    sometimes have to do it on the page tables only, like when doing
    a get_user_pages_fast() where we have no VMA.
    
    We add two functions and expose them to generic code:
    
            arch_pte_access_permitted(pte_flags, write)
            arch_vma_access_permitted(vma, write)
    
    These are, of course, backed up in x86 arch code with checks
    against the PTE or VMA's protection key.
    
    But, there are also cases where we do not want to respect
    protection keys.  When we ptrace(), for instance, we do not want
    to apply the tracer's PKRU permissions to the PTEs from the
    process being traced.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Alexey Kardashevskiy <aik@ozlabs.ru>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Boaz Harrosh <boaz@plexistor.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: David Gibson <david@gibson.dropbear.id.au>
    Cc: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Cc: Dominik Vogt <vogt@linux.vnet.ibm.com>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Low <jason.low2@hp.com>
    Cc: Jerome Marchand <jmarchan@redhat.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Laurent Dufour <ldufour@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matthew Wilcox <willy@linux.intel.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mikulas Patocka <mpatocka@redhat.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Cc: Shachar Raindel <raindel@mellanox.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: linux-s390@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/20160212210219.14D5D715@viggo.jf.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index 1cb5220afaf9..3133f947ade2 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -97,4 +97,15 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,
 {
 }
 
+static inline bool arch_vma_access_permitted(struct vm_area_struct *vma, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
+
+static inline bool arch_pte_access_permitted(pte_t pte, bool write)
+{
+	/* by default, allow everything */
+	return true;
+}
 #endif

commit 62e88b1c00de9cb30d937841ed5debed871070b8
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Tue Nov 18 10:23:50 2014 -0800

    mm: Make arch_unmap()/bprm_mm_init() available to all architectures
    
    The x86 MPX patch set calls arch_unmap() and arch_bprm_mm_init()
    from fs/exec.c, so we need at least a stub for them in all
    architectures.  They are only called under an #ifdef for
    CONFIG_MMU=y, so we can at least restict this to architectures
    with MMU support.
    
    blackfin/c6x have no MMU support, so do not call arch_unmap().
    They also do not include mm_hooks.h or mmu_context.h at all and
    do not need to be touched.
    
    s390, um and unicore32 do not use asm-generic/mm_hooks.h, so got
    their own arch_unmap() versions.  (I also moved um's
    arch_dup_mmap() to be closer to the other mm_hooks.h functions).
    
    xtensa only includes mm_hooks when MMU=y, which should be fine
    since arch_unmap() is called only from MMU=y code.
    
    For the rest, we use the stub copies of these functions in
    asm-generic/mm_hook.h.
    
    I cross compiled defconfigs for cris (to check NOMMU) and s390
    to make sure that this works.  I also checked a 64-bit build
    of UML and all my normal x86 builds including PARAVIRT on and
    off.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Dave Hansen <dave@sr71.net>
    Cc: linux-arch@vger.kernel.org
    Cc: x86@kernel.org
    Link: http://lkml.kernel.org/r/20141118182350.8B4AA2C2@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index ef470a7a3d0f..1cb5220afaf9 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -86,4 +86,15 @@ static inline void arch_dup_mmap(struct mm_struct *oldmm,
 {
 }
 
+static inline void arch_unmap(struct mm_struct *mm,
+			struct vm_area_struct *vma,
+			unsigned long start, unsigned long end)
+{
+}
+
+static inline void arch_bprm_mm_init(struct mm_struct *mm,
+				     struct vm_area_struct *vma)
+{
+}
+
 #endif

commit 615d6e8756c87149f2d4c1b93d471bca002bd849
Author: Davidlohr Bueso <davidlohr@hp.com>
Date:   Mon Apr 7 15:37:25 2014 -0700

    mm: per-thread vma caching
    
    This patch is a continuation of efforts trying to optimize find_vma(),
    avoiding potentially expensive rbtree walks to locate a vma upon faults.
    The original approach (https://lkml.org/lkml/2013/11/1/410), where the
    largest vma was also cached, ended up being too specific and random,
    thus further comparison with other approaches were needed.  There are
    two things to consider when dealing with this, the cache hit rate and
    the latency of find_vma().  Improving the hit-rate does not necessarily
    translate in finding the vma any faster, as the overhead of any fancy
    caching schemes can be too high to consider.
    
    We currently cache the last used vma for the whole address space, which
    provides a nice optimization, reducing the total cycles in find_vma() by
    up to 250%, for workloads with good locality.  On the other hand, this
    simple scheme is pretty much useless for workloads with poor locality.
    Analyzing ebizzy runs shows that, no matter how many threads are
    running, the mmap_cache hit rate is less than 2%, and in many situations
    below 1%.
    
    The proposed approach is to replace this scheme with a small per-thread
    cache, maximizing hit rates at a very low maintenance cost.
    Invalidations are performed by simply bumping up a 32-bit sequence
    number.  The only expensive operation is in the rare case of a seq
    number overflow, where all caches that share the same address space are
    flushed.  Upon a miss, the proposed replacement policy is based on the
    page number that contains the virtual address in question.  Concretely,
    the following results are seen on an 80 core, 8 socket x86-64 box:
    
    1) System bootup: Most programs are single threaded, so the per-thread
       scheme does improve ~50% hit rate by just adding a few more slots to
       the cache.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 50.61%   | 19.90            |
    | patched        | 73.45%   | 13.58            |
    +----------------+----------+------------------+
    
    2) Kernel build: This one is already pretty good with the current
       approach as we're dealing with good locality.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 75.28%   | 11.03            |
    | patched        | 88.09%   | 9.31             |
    +----------------+----------+------------------+
    
    3) Oracle 11g Data Mining (4k pages): Similar to the kernel build workload.
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 70.66%   | 17.14            |
    | patched        | 91.15%   | 12.57            |
    +----------------+----------+------------------+
    
    4) Ebizzy: There's a fair amount of variation from run to run, but this
       approach always shows nearly perfect hit rates, while baseline is just
       about non-existent.  The amounts of cycles can fluctuate between
       anywhere from ~60 to ~116 for the baseline scheme, but this approach
       reduces it considerably.  For instance, with 80 threads:
    
    +----------------+----------+------------------+
    | caching scheme | hit-rate | cycles (billion) |
    +----------------+----------+------------------+
    | baseline       | 1.06%    | 91.54            |
    | patched        | 99.97%   | 14.18            |
    +----------------+----------+------------------+
    
    [akpm@linux-foundation.org: fix nommu build, per Davidlohr]
    [akpm@linux-foundation.org: document vmacache_valid() logic]
    [akpm@linux-foundation.org: attempt to untangle header files]
    [akpm@linux-foundation.org: add vmacache_find() BUG_ON]
    [hughd@google.com: add vmacache_valid_mm() (from Oleg)]
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: adjust and enhance comments]
    Signed-off-by: Davidlohr Bueso <davidlohr@hp.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Michel Lespinasse <walken@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Tested-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
index fb5e4c658f7a..ef470a7a3d0f 100644
--- a/arch/unicore32/include/asm/mmu_context.h
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -14,6 +14,8 @@
 
 #include <linux/compiler.h>
 #include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/vmacache.h>
 #include <linux/io.h>
 
 #include <asm/cacheflush.h>
@@ -73,7 +75,7 @@ do { \
 		else \
 			mm->mmap = NULL; \
 		rb_erase(&high_vma->vm_rb, &mm->mm_rb); \
-		mm->mmap_cache = NULL; \
+		vmacache_invalidate(mm); \
 		mm->map_count--; \
 		remove_vma(high_vma); \
 	} \

commit 56372b0b2f533c9a25bd40a0577405f6ddb7cff2
Author: GuanXuetao <gxt@mprc.pku.edu.cn>
Date:   Sat Jan 15 18:17:56 2011 +0800

    unicore32 core architecture: mm related: fault handling
    
    This patch implements fault handling of memory management.
    
    Signed-off-by: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/unicore32/include/asm/mmu_context.h b/arch/unicore32/include/asm/mmu_context.h
new file mode 100644
index 000000000000..fb5e4c658f7a
--- /dev/null
+++ b/arch/unicore32/include/asm/mmu_context.h
@@ -0,0 +1,87 @@
+/*
+ * linux/arch/unicore32/include/asm/mmu_context.h
+ *
+ * Code specific to PKUnity SoC and UniCore ISA
+ *
+ * Copyright (C) 2001-2010 GUAN Xue-tao
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __UNICORE_MMU_CONTEXT_H__
+#define __UNICORE_MMU_CONTEXT_H__
+
+#include <linux/compiler.h>
+#include <linux/sched.h>
+#include <linux/io.h>
+
+#include <asm/cacheflush.h>
+#include <asm/cpu-single.h>
+
+#define init_new_context(tsk, mm)	0
+
+#define destroy_context(mm)		do { } while (0)
+
+/*
+ * This is called when "tsk" is about to enter lazy TLB mode.
+ *
+ * mm:  describes the currently active mm context
+ * tsk: task which is entering lazy tlb
+ * cpu: cpu number which is entering lazy tlb
+ *
+ * tsk->mm will be NULL
+ */
+static inline void
+enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
+{
+}
+
+/*
+ * This is the actual mm switch as far as the scheduler
+ * is concerned.  No registers are touched.  We avoid
+ * calling the CPU specific function when the mm hasn't
+ * actually changed.
+ */
+static inline void
+switch_mm(struct mm_struct *prev, struct mm_struct *next,
+	  struct task_struct *tsk)
+{
+	unsigned int cpu = smp_processor_id();
+
+	if (!cpumask_test_and_set_cpu(cpu, mm_cpumask(next)) || prev != next)
+		cpu_switch_mm(next->pgd, next);
+}
+
+#define deactivate_mm(tsk, mm)	do { } while (0)
+#define activate_mm(prev, next)	switch_mm(prev, next, NULL)
+
+/*
+ * We are inserting a "fake" vma for the user-accessible vector page so
+ * gdb and friends can get to it through ptrace and /proc/<pid>/mem.
+ * But we also want to remove it before the generic code gets to see it
+ * during process exit or the unmapping of it would  cause total havoc.
+ * (the macro is used as remove_vma() is static to mm/mmap.c)
+ */
+#define arch_exit_mmap(mm) \
+do { \
+	struct vm_area_struct *high_vma = find_vma(mm, 0xffff0000); \
+	if (high_vma) { \
+		BUG_ON(high_vma->vm_next);  /* it should be last */ \
+		if (high_vma->vm_prev) \
+			high_vma->vm_prev->vm_next = NULL; \
+		else \
+			mm->mmap = NULL; \
+		rb_erase(&high_vma->vm_rb, &mm->mm_rb); \
+		mm->mmap_cache = NULL; \
+		mm->map_count--; \
+		remove_vma(high_vma); \
+	} \
+} while (0)
+
+static inline void arch_dup_mmap(struct mm_struct *oldmm,
+				 struct mm_struct *mm)
+{
+}
+
+#endif
