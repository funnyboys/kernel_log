commit b4ed71f557e458257e0f71b11969954acb389240
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Wed Sep 25 16:49:46 2019 -0700

    mm: treewide: clarify pgtable_page_{ctor,dtor}() naming
    
    The naming of pgtable_page_{ctor,dtor}() seems to have confused a few
    people, and until recently arm64 used these erroneously/pointlessly for
    other levels of page table.
    
    To make it incredibly clear that these only apply to the PTE level, and to
    align with the naming of pgtable_pmd_page_{ctor,dtor}(), let's rename them
    to pgtable_pte_page_{ctor,dtor}().
    
    These changes were generated with the following shell script:
    
    ----
    git grep -lw 'pgtable_page_.tor' | while read FILE; do
        sed -i '{s/pgtable_page_ctor/pgtable_pte_page_ctor/}' $FILE;
        sed -i '{s/pgtable_page_dtor/pgtable_pte_page_dtor/}' $FILE;
    done
    ----
    
    ... with the documentation re-flowed to remain under 80 columns, and
    whitespace fixed up in macros to keep backslashes aligned.
    
    There should be no functional change as a result of this patch.
    
    Link: http://lkml.kernel.org/r/20190722141133.3116-1-mark.rutland@arm.com
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Mike Rapoport <rppt@linux.ibm.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>     [m68k]
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Yu Zhao <yuzhao@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/unicore32/include/asm/tlb.h b/arch/unicore32/include/asm/tlb.h
index 10d2356bfddd..4663d8cc80ef 100644
--- a/arch/unicore32/include/asm/tlb.h
+++ b/arch/unicore32/include/asm/tlb.h
@@ -15,7 +15,7 @@
 
 #define __pte_free_tlb(tlb, pte, addr)				\
 	do {							\
-		pgtable_page_dtor(pte);				\
+		pgtable_pte_page_dtor(pte);			\
 		tlb_remove_page((tlb), (pte));			\
 	} while (0)
 

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/unicore32/include/asm/tlb.h b/arch/unicore32/include/asm/tlb.h
index 00a8477333f6..10d2356bfddd 100644
--- a/arch/unicore32/include/asm/tlb.h
+++ b/arch/unicore32/include/asm/tlb.h
@@ -1,13 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * linux/arch/unicore32/include/asm/tlb.h
  *
  * Code specific to PKUnity SoC and UniCore ISA
  *
  * Copyright (C) 2001-2010 GUAN Xue-tao
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #ifndef __UNICORE_TLB_H__
 #define __UNICORE_TLB_H__

commit 6137fed0823247e32306bde2b48cac627c24f894
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 4 17:04:07 2018 +0200

    arch/tlb: Clean up simple architectures
    
    For the architectures that do not implement their own tlb_flush() but
    do already use the generic mmu_gather, there are two options:
    
     1) the platform has an efficient flush_tlb_range() and
        asm-generic/tlb.h doesn't need any overrides at all.
    
     2) the platform lacks an efficient flush_tlb_range() and
        we select MMU_GATHER_NO_RANGE to minimize full invalidates.
    
    Convert all 'simple' architectures to one of these two forms.
    
    alpha:      has no range invalidate -> 2
    arc:        already used flush_tlb_range() -> 1
    c6x:        has no range invalidate -> 2
    hexagon:    has an efficient flush_tlb_range() -> 1
                (flush_tlb_mm() is in fact a full range invalidate,
                 so no need to shoot down everything)
    m68k:       has inefficient flush_tlb_range() -> 2
    microblaze: has no flush_tlb_range() -> 2
    mips:       has efficient flush_tlb_range() -> 1
                (even though it currently seems to use flush_tlb_mm())
    nds32:      already uses flush_tlb_range() -> 1
    nios2:      has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    openrisc:   has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    parisc:     already uses flush_tlb_range() -> 1
    sparc32:    already uses flush_tlb_range() -> 1
    unicore32:  has inefficient flush_tlb_range() -> 2
                (no limit on range iteration)
    xtensa:     has efficient flush_tlb_range() -> 1
    
    Note this also fixes a bug in the existing code for a number
    platforms. Those platforms that did:
    
      tlb_end_vma() -> if (!full_mm) flush_tlb_*()
      tlb_flush -> if (full_mm) flush_tlb_mm()
    
    missed the case of shift_arg_pages(), which doesn't have @fullmm set,
    nor calls into tlb_*vma(), but still frees page-tables and thus needs
    an invalidate. The new code handles this by detecting a non-empty
    range, and either issuing the matching range invalidate or a full
    invalidate, depending on the capabilities.
    
    No change in behavior intended.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Piggin <npiggin@gmail.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/unicore32/include/asm/tlb.h b/arch/unicore32/include/asm/tlb.h
index 9cca15cdae94..00a8477333f6 100644
--- a/arch/unicore32/include/asm/tlb.h
+++ b/arch/unicore32/include/asm/tlb.h
@@ -12,10 +12,9 @@
 #ifndef __UNICORE_TLB_H__
 #define __UNICORE_TLB_H__
 
-#define tlb_start_vma(tlb, vma)				do { } while (0)
-#define tlb_end_vma(tlb, vma)				do { } while (0)
-#define __tlb_remove_tlb_entry(tlb, ptep, address)	do { } while (0)
-#define tlb_flush(tlb) flush_tlb_mm((tlb)->mm)
+/*
+ * unicore32 lacks an efficient flush_tlb_range(), use flush_tlb_mm().
+ */
 
 #define __pte_free_tlb(tlb, pte, addr)				\
 	do {							\

commit 289d6b0e287e0acd85f3e6b7ea6c2cb5c234909a
Author: GuanXuetao <gxt@mprc.pku.edu.cn>
Date:   Fri Mar 4 20:00:11 2011 +0800

    unicore32: rewrite arch-specific tlb.h to use asm-generic version
    
    Signed-off-by: Guan Xuetao <gxt@mprc.pku.edu.cn>

diff --git a/arch/unicore32/include/asm/tlb.h b/arch/unicore32/include/asm/tlb.h
index 02ee40e47a0d..9cca15cdae94 100644
--- a/arch/unicore32/include/asm/tlb.h
+++ b/arch/unicore32/include/asm/tlb.h
@@ -12,87 +12,17 @@
 #ifndef __UNICORE_TLB_H__
 #define __UNICORE_TLB_H__
 
-#include <asm/cacheflush.h>
-#include <asm/tlbflush.h>
-#include <asm/pgalloc.h>
-
-/*
- * TLB handling.  This allows us to remove pages from the page
- * tables, and efficiently handle the TLB issues.
- */
-struct mmu_gather {
-	struct mm_struct	*mm;
-	unsigned int		fullmm;
-	unsigned long		range_start;
-	unsigned long		range_end;
-};
-
-DECLARE_PER_CPU(struct mmu_gather, mmu_gathers);
-
-static inline struct mmu_gather *
-tlb_gather_mmu(struct mm_struct *mm, unsigned int full_mm_flush)
-{
-	struct mmu_gather *tlb = &get_cpu_var(mmu_gathers);
-
-	tlb->mm = mm;
-	tlb->fullmm = full_mm_flush;
-
-	return tlb;
-}
-
-static inline void
-tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
-{
-	if (tlb->fullmm)
-		flush_tlb_mm(tlb->mm);
-
-	/* keep the page table cache within bounds */
-	check_pgt_cache();
-
-	put_cpu_var(mmu_gathers);
-}
-
-/*
- * Memorize the range for the TLB flush.
- */
-static inline void
-tlb_remove_tlb_entry(struct mmu_gather *tlb, pte_t *ptep, unsigned long addr)
-{
-	if (!tlb->fullmm) {
-		if (addr < tlb->range_start)
-			tlb->range_start = addr;
-		if (addr + PAGE_SIZE > tlb->range_end)
-			tlb->range_end = addr + PAGE_SIZE;
-	}
-}
-
-/*
- * In the case of tlb vma handling, we can optimise these away in the
- * case where we're doing a full MM flush.  When we're doing a munmap,
- * the vmas are adjusted to only cover the region to be torn down.
- */
-static inline void
-tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
-{
-	if (!tlb->fullmm) {
-		flush_cache_range(vma, vma->vm_start, vma->vm_end);
-		tlb->range_start = TASK_SIZE;
-		tlb->range_end = 0;
-	}
-}
-
-static inline void
-tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
-{
-	if (!tlb->fullmm && tlb->range_end > 0)
-		flush_tlb_range(vma, tlb->range_start, tlb->range_end);
-}
-
-#define tlb_remove_page(tlb, page)	free_page_and_swap_cache(page)
-#define pte_free_tlb(tlb, ptep, addr)	pte_free((tlb)->mm, ptep)
-#define pmd_free_tlb(tlb, pmdp, addr)	pmd_free((tlb)->mm, pmdp)
-#define pud_free_tlb(tlb, x, addr)      do { } while (0)
-
-#define tlb_migrate_finish(mm)		do { } while (0)
+#define tlb_start_vma(tlb, vma)				do { } while (0)
+#define tlb_end_vma(tlb, vma)				do { } while (0)
+#define __tlb_remove_tlb_entry(tlb, ptep, address)	do { } while (0)
+#define tlb_flush(tlb) flush_tlb_mm((tlb)->mm)
+
+#define __pte_free_tlb(tlb, pte, addr)				\
+	do {							\
+		pgtable_page_dtor(pte);				\
+		tlb_remove_page((tlb), (pte));			\
+	} while (0)
+
+#include <asm-generic/tlb.h>
 
 #endif

commit b50f1704e9c441c58cf6dc05e72953ca30e1d4d2
Author: GuanXuetao <gxt@mprc.pku.edu.cn>
Date:   Sat Jan 15 18:16:59 2011 +0800

    unicore32 core architecture: mm related: generic codes
    
    This patch includes generic codes for memory management.
    
    Signed-off-by: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/unicore32/include/asm/tlb.h b/arch/unicore32/include/asm/tlb.h
new file mode 100644
index 000000000000..02ee40e47a0d
--- /dev/null
+++ b/arch/unicore32/include/asm/tlb.h
@@ -0,0 +1,98 @@
+/*
+ * linux/arch/unicore32/include/asm/tlb.h
+ *
+ * Code specific to PKUnity SoC and UniCore ISA
+ *
+ * Copyright (C) 2001-2010 GUAN Xue-tao
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __UNICORE_TLB_H__
+#define __UNICORE_TLB_H__
+
+#include <asm/cacheflush.h>
+#include <asm/tlbflush.h>
+#include <asm/pgalloc.h>
+
+/*
+ * TLB handling.  This allows us to remove pages from the page
+ * tables, and efficiently handle the TLB issues.
+ */
+struct mmu_gather {
+	struct mm_struct	*mm;
+	unsigned int		fullmm;
+	unsigned long		range_start;
+	unsigned long		range_end;
+};
+
+DECLARE_PER_CPU(struct mmu_gather, mmu_gathers);
+
+static inline struct mmu_gather *
+tlb_gather_mmu(struct mm_struct *mm, unsigned int full_mm_flush)
+{
+	struct mmu_gather *tlb = &get_cpu_var(mmu_gathers);
+
+	tlb->mm = mm;
+	tlb->fullmm = full_mm_flush;
+
+	return tlb;
+}
+
+static inline void
+tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
+{
+	if (tlb->fullmm)
+		flush_tlb_mm(tlb->mm);
+
+	/* keep the page table cache within bounds */
+	check_pgt_cache();
+
+	put_cpu_var(mmu_gathers);
+}
+
+/*
+ * Memorize the range for the TLB flush.
+ */
+static inline void
+tlb_remove_tlb_entry(struct mmu_gather *tlb, pte_t *ptep, unsigned long addr)
+{
+	if (!tlb->fullmm) {
+		if (addr < tlb->range_start)
+			tlb->range_start = addr;
+		if (addr + PAGE_SIZE > tlb->range_end)
+			tlb->range_end = addr + PAGE_SIZE;
+	}
+}
+
+/*
+ * In the case of tlb vma handling, we can optimise these away in the
+ * case where we're doing a full MM flush.  When we're doing a munmap,
+ * the vmas are adjusted to only cover the region to be torn down.
+ */
+static inline void
+tlb_start_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
+{
+	if (!tlb->fullmm) {
+		flush_cache_range(vma, vma->vm_start, vma->vm_end);
+		tlb->range_start = TASK_SIZE;
+		tlb->range_end = 0;
+	}
+}
+
+static inline void
+tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)
+{
+	if (!tlb->fullmm && tlb->range_end > 0)
+		flush_tlb_range(vma, tlb->range_start, tlb->range_end);
+}
+
+#define tlb_remove_page(tlb, page)	free_page_and_swap_cache(page)
+#define pte_free_tlb(tlb, ptep, addr)	pte_free((tlb)->mm, ptep)
+#define pmd_free_tlb(tlb, pmdp, addr)	pmd_free((tlb)->mm, pmdp)
+#define pud_free_tlb(tlb, x, addr)      do { } while (0)
+
+#define tlb_migrate_finish(mm)		do { } while (0)
+
+#endif
