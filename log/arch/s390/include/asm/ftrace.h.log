commit ec7bf4789d95a0053bac0dfa36fbefd8cc584eea
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Feb 18 16:51:28 2019 +0100

    s390/ftrace: use HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
    
    Make the call chain more reliable by tagging the ftrace stack entries
    with the stack pointer that is associated with the return address.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 6e0ed0338785..68d362f8d6c1 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -11,6 +11,8 @@
 #define MCOUNT_RETURN_FIXUP	18
 #endif
 
+#define HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
+
 #ifndef __ASSEMBLY__
 
 #ifdef CONFIG_CC_IS_CLANG

commit 6e042492a272bdc507d37982b890dc5266d5bb01
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Apr 8 23:26:23 2019 +0200

    s390: avoid __builtin_return_address(n) on clang
    
    llvm on s390 has problems with __builtin_return_address(n), with n>0,
    this results in a somewhat cryptic error message:
    
    fatal error: error in backend: Unsupported stack frame traversal count
    
    To work around it, use the direct return address directly. This
    is probably not ideal here, but gets things to compile and should
    only lead to inferior reporting, not to misbehavior of the generated
    code.
    
    Link: https://bugs.llvm.org/show_bug.cgi?id=41424
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 5a3c95b11952..6e0ed0338785 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -13,7 +13,12 @@
 
 #ifndef __ASSEMBLY__
 
+#ifdef CONFIG_CC_IS_CLANG
+/* https://bugs.llvm.org/show_bug.cgi?id=41424 */
+#define ftrace_return_address(n) 0UL
+#else
 #define ftrace_return_address(n) __builtin_return_address(n)
+#endif
 
 void _mcount(void);
 void ftrace_caller(void);

commit fb8bfca06cbc8280976ea751501767ae1f37dff5
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Jan 21 10:30:44 2019 +0100

    s390: fix system call tracing
    
    When converting to autogenerated compat syscall wrappers all system
    call entry points got a different symbol name: they all got a __s390x_
    prefix.
    
    This caused breakage with system call tracing, since an appropriate
    arch_syscall_match_sym_name() was not provided. Add this function, and
    while at it also add code to avoid compat system call tracing. s390
    has different system call tables for native 64 bit system calls and
    compat system calls. This isn't really supported in the common
    code. However there are hardly any compat binaries left, therefore
    just ignore compat system calls, like x86 and arm64 also do for the
    same reason.
    
    Fixes: aa0d6e70d3b3 ("s390: autogenerate compat syscall wrappers")
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 8ea270fdc7fb..5a3c95b11952 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -81,5 +81,30 @@ static inline void ftrace_generate_call_insn(struct ftrace_insn *insn,
 #endif
 }
 
+/*
+ * Even though the system call numbers are identical for s390/s390x a
+ * different system call table is used for compat tasks. This may lead
+ * to e.g. incorrect or missing trace event sysfs files.
+ * Therefore simply do not trace compat system calls at all.
+ * See kernel/trace/trace_syscalls.c.
+ */
+#define ARCH_TRACE_IGNORE_COMPAT_SYSCALLS
+static inline bool arch_trace_is_compat_syscall(struct pt_regs *regs)
+{
+	return is_compat_task();
+}
+
+#define ARCH_HAS_SYSCALL_MATCH_SYM_NAME
+static inline bool arch_syscall_match_sym_name(const char *sym,
+					       const char *name)
+{
+	/*
+	 * Skip __s390_ and __s390x_ prefix - due to compat wrappers
+	 * and aliasing some symbols of 64 bit system call functions
+	 * may get the __s390_ prefix instead of the __s390x_ prefix.
+	 */
+	return !strcmp(sym + 7, name) || !strcmp(sym + 8, name);
+}
+
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_S390_FTRACE_H */

commit d983c89cc96a87db0c00821e81aa3d8296c12225
Author: Vasily Gorbik <gor@linux.ibm.com>
Date:   Mon Aug 6 15:17:47 2018 +0200

    s390/ftrace: Add -mfentry and -mnop-mcount support
    
    Utilize -mfentry and -mnop-mcount gcc options together with
    -mrecord-mcount to get compiler generated calls to the profiling functions
    as nops which are compatible with current -mhotpatch=0,3 approach.  At the
    same time -mrecord-mcount enables __mcount_loc section generation by
    the compiler which allows to avoid using scripts/recordmcount.pl script.
    
    Link: http://lkml.kernel.org/r/patch-4.thread-aa7b8d.git-aa7b8dbf236f.your-ad-here.call-01533557518-ext-9465@work.hours
    
    Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index cfccc0edd00d..8ea270fdc7fb 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -4,7 +4,7 @@
 
 #define ARCH_SUPPORTS_FTRACE_OPS 1
 
-#ifdef CC_USING_HOTPATCH
+#if defined(CC_USING_HOTPATCH) || defined(CC_USING_NOP_MCOUNT)
 #define MCOUNT_INSN_SIZE	6
 #else
 #define MCOUNT_INSN_SIZE	24
@@ -42,7 +42,7 @@ struct ftrace_insn {
 static inline void ftrace_generate_nop_insn(struct ftrace_insn *insn)
 {
 #ifdef CONFIG_FUNCTION_TRACER
-#ifdef CC_USING_HOTPATCH
+#if defined(CC_USING_HOTPATCH) || defined(CC_USING_NOP_MCOUNT)
 	/* brcl 0,0 */
 	insn->opc = 0xc004;
 	insn->disp = 0;
@@ -57,7 +57,7 @@ static inline void ftrace_generate_nop_insn(struct ftrace_insn *insn)
 static inline int is_ftrace_nop(struct ftrace_insn *insn)
 {
 #ifdef CONFIG_FUNCTION_TRACER
-#ifdef CC_USING_HOTPATCH
+#if defined(CC_USING_HOTPATCH) || defined(CC_USING_NOP_MCOUNT)
 	if (insn->disp == 0)
 		return 1;
 #else

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 836c56290499..cfccc0edd00d 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _ASM_S390_FTRACE_H
 #define _ASM_S390_FTRACE_H
 

commit dcddba96cdbc5d0e4d4a17bf22cfd9b2f038a4ca
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Oct 17 13:07:46 2016 +0200

    s390/dumpstack: get rid of return_address again
    
    With commit ef6000b4c670 ("Disable the __builtin_return_address()
    warning globally after all)" the kernel does not warn at all again if
    __builtin_return_address(n) is called with n > 0.
    
    Besides the fact that this was a false warning on s390 anyway, due to
    the always present backchain, we can now revert commit 5606330627ab
    ("s390/dumpstack: implement and use return_address()") again, to
    simplify the code again.
    
    After all I shouldn't have had return_address() implememted at all to
    workaround this issue. So get rid of this again.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 64053d9ac3f2..836c56290499 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -12,9 +12,7 @@
 
 #ifndef __ASSEMBLY__
 
-unsigned long return_address(int depth);
-
-#define ftrace_return_address(n) return_address(n)
+#define ftrace_return_address(n) __builtin_return_address(n)
 
 void _mcount(void);
 void ftrace_caller(void);

commit 5606330627ab680f5e6b7549d14ec3ffdae58c15
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon May 2 14:38:29 2016 +0200

    s390/dumpstack: implement and use return_address()
    
    Implement return_address() and use it instead of __builtin_return_address(n).
    
    __builtin_return_address(n) is not guaranteed to work for n > 0,
    therefore implement a private return_address() function which walks
    the stack frames and returns the proper return address.
    
    This way we get also rid of a compile warning which gcc 6.1 emits and
    look like all other architectures.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 836c56290499..64053d9ac3f2 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -12,7 +12,9 @@
 
 #ifndef __ASSEMBLY__
 
-#define ftrace_return_address(n) __builtin_return_address(n)
+unsigned long return_address(int depth);
+
+#define ftrace_return_address(n) return_address(n)
 
 void _mcount(void);
 void ftrace_caller(void);

commit e6d60b368b45b9be3aa068f8e5fa98c3487c9d4e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jan 9 13:08:28 2015 +0100

    s390/ftrace: hotpatch support for function tracing
    
    Make use of gcc's hotpatch support to generate better code for ftrace
    function tracing.
    The generated code now contains only a six byte nop in each function
    prologue instead of a 24 byte code block which will be runtime patched to
    support function tracing.
    With the new code generation the runtime overhead for supporting function
    tracing is close to zero, while the original code did show a significant
    performance impact.
    
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index abb618f1ead2..836c56290499 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -3,8 +3,12 @@
 
 #define ARCH_SUPPORTS_FTRACE_OPS 1
 
+#ifdef CC_USING_HOTPATCH
+#define MCOUNT_INSN_SIZE	6
+#else
 #define MCOUNT_INSN_SIZE	24
 #define MCOUNT_RETURN_FIXUP	18
+#endif
 
 #ifndef __ASSEMBLY__
 
@@ -37,17 +41,28 @@ struct ftrace_insn {
 static inline void ftrace_generate_nop_insn(struct ftrace_insn *insn)
 {
 #ifdef CONFIG_FUNCTION_TRACER
+#ifdef CC_USING_HOTPATCH
+	/* brcl 0,0 */
+	insn->opc = 0xc004;
+	insn->disp = 0;
+#else
 	/* jg .+24 */
 	insn->opc = 0xc0f4;
 	insn->disp = MCOUNT_INSN_SIZE / 2;
 #endif
+#endif
 }
 
 static inline int is_ftrace_nop(struct ftrace_insn *insn)
 {
 #ifdef CONFIG_FUNCTION_TRACER
+#ifdef CC_USING_HOTPATCH
+	if (insn->disp == 0)
+		return 1;
+#else
 	if (insn->disp == MCOUNT_INSN_SIZE / 2)
 		return 1;
+#endif
 #endif
 	return 0;
 }

commit 7eed2e09abf0a0064a7e3f4a911b37d71202dfa4
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Nov 24 14:30:32 2014 +0100

    s390/ftrace: provide working ftrace_return_address()
    
    The common code ftrace_return_address(n), which is just a wrapper for
    __builtin_return_address(n), will only work for n > 0 if CONFIG_FRAME_POINTER
    is set to 'y'. Otherwise it will return 0.
    Since on s390 we will never have that config option set to 'y'
    ftrace_return_address() won't work at all for n > 0.
    
    Luckily we always compile the kernel with -mkernel-backchain which
    in turn means that __builtin_return_address(n) will always work.
    
    So let ftrace_return_address(n) map to __builtin_return_address(n).
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 785041f1dc77..abb618f1ead2 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -8,6 +8,8 @@
 
 #ifndef __ASSEMBLY__
 
+#define ftrace_return_address(n) __builtin_return_address(n)
+
 void _mcount(void);
 void ftrace_caller(void);
 

commit c933146a5e41e42ea3eb4f34fa02e201da3f068e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Oct 15 12:17:38 2014 +0200

    s390/ftrace,kprobes: allow to patch first instruction
    
    If the function tracer is enabled, allow to set kprobes on the first
    instruction of a function (which is the function trace caller):
    
    If no kprobe is set handling of enabling and disabling function tracing
    of a function simply patches the first instruction. Either it is a nop
    (right now it's an unconditional branch, which skips the mcount block),
    or it's a branch to the ftrace_caller() function.
    
    If a kprobe is being placed on a function tracer calling instruction
    we encode if we actually have a nop or branch in the remaining bytes
    after the breakpoint instruction (illegal opcode).
    This is possible, since the size of the instruction used for the nop
    and branch is six bytes, while the size of the breakpoint is only
    two bytes.
    Therefore the first two bytes contain the illegal opcode and the last
    four bytes contain either "0" for nop or "1" for branch. The kprobes
    code will then execute/simulate the correct instruction.
    
    Instruction patching for kprobes and function tracer is always done
    with stop_machine(). Therefore we don't have any races where an
    instruction is patched concurrently on a different cpu.
    Besides that also the program check handler which executes the function
    trace caller instruction won't be executed concurrently to any
    stop_machine() execution.
    
    This allows to keep full fault based kprobes handling which generates
    correct pt_regs contents automatically.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 3aef8afec336..785041f1dc77 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -1,25 +1,67 @@
 #ifndef _ASM_S390_FTRACE_H
 #define _ASM_S390_FTRACE_H
 
+#define ARCH_SUPPORTS_FTRACE_OPS 1
+
+#define MCOUNT_INSN_SIZE	24
+#define MCOUNT_RETURN_FIXUP	18
+
 #ifndef __ASSEMBLY__
 
-extern void _mcount(void);
+void _mcount(void);
+void ftrace_caller(void);
+
 extern char ftrace_graph_caller_end;
+extern unsigned long ftrace_plt;
 
 struct dyn_arch_ftrace { };
 
-#define MCOUNT_ADDR ((long)_mcount)
+#define MCOUNT_ADDR ((unsigned long)_mcount)
+#define FTRACE_ADDR ((unsigned long)ftrace_caller)
 
+#define KPROBE_ON_FTRACE_NOP	0
+#define KPROBE_ON_FTRACE_CALL	1
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
 	return addr;
 }
 
-#endif /* __ASSEMBLY__ */
+struct ftrace_insn {
+	u16 opc;
+	s32 disp;
+} __packed;
 
-#define MCOUNT_INSN_SIZE  18
+static inline void ftrace_generate_nop_insn(struct ftrace_insn *insn)
+{
+#ifdef CONFIG_FUNCTION_TRACER
+	/* jg .+24 */
+	insn->opc = 0xc0f4;
+	insn->disp = MCOUNT_INSN_SIZE / 2;
+#endif
+}
 
-#define ARCH_SUPPORTS_FTRACE_OPS 1
+static inline int is_ftrace_nop(struct ftrace_insn *insn)
+{
+#ifdef CONFIG_FUNCTION_TRACER
+	if (insn->disp == MCOUNT_INSN_SIZE / 2)
+		return 1;
+#endif
+	return 0;
+}
+
+static inline void ftrace_generate_call_insn(struct ftrace_insn *insn,
+					     unsigned long ip)
+{
+#ifdef CONFIG_FUNCTION_TRACER
+	unsigned long target;
 
+	/* brasl r0,ftrace_caller */
+	target = is_module_addr((void *) ip) ? ftrace_plt : FTRACE_ADDR;
+	insn->opc = 0xc005;
+	insn->disp = (target - ip) / 2;
+#endif
+}
+
+#endif /* __ASSEMBLY__ */
 #endif /* _ASM_S390_FTRACE_H */

commit 53255c9a4dade6ff2162121430d13aaadb38a69c
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Oct 7 15:45:10 2014 +0200

    s390/ftrace: remove 31 bit ftrace support
    
    31 bit and 64 bit diverge more and more and it is rather painful
    to keep both parts running.
    To make things simpler just remove the 31 bit support which nobody
    uses anyway.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index d419362dc231..3aef8afec336 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -18,14 +18,8 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 
 #endif /* __ASSEMBLY__ */
 
-#ifdef CONFIG_64BIT
 #define MCOUNT_INSN_SIZE  18
-#else
-#define MCOUNT_INSN_SIZE  22
-#endif
 
-#ifdef CONFIG_64BIT
 #define ARCH_SUPPORTS_FTRACE_OPS 1
-#endif
 
 #endif /* _ASM_S390_FTRACE_H */

commit 3d1e220d08c6a00ffa83d39030b8162f66665b2b
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Sep 3 13:26:23 2014 +0200

    s390/ftrace: optimize mcount code
    
    Reduce the number of executed instructions within the mcount block if
    function tracing is enabled. We achieve that by using a non-standard
    C function call ABI. Since the called function is also written in
    assembler this is not a problem.
    This also allows to replace the unconditional store at the beginning
    of the mcount block with a larl instruction, which doesn't touch
    memory.
    
    In theory we could also patch the first instruction of the mcount block
    to enable and disable function tracing. However this would break kprobes.
    This could be fixed with implementing the "kprobes_on_ftrace" feature;
    however keeping the odd jprobes working seems not to be possible without
    a lot of code churn. Therefore keep the code easy and simply accept one
    wasted 1-cycle "larl" instruction per function prologue.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 1759d73fb95b..d419362dc231 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -19,7 +19,7 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 #endif /* __ASSEMBLY__ */
 
 #ifdef CONFIG_64BIT
-#define MCOUNT_INSN_SIZE  12
+#define MCOUNT_INSN_SIZE  18
 #else
 #define MCOUNT_INSN_SIZE  22
 #endif

commit 10dec7dbd50ab0be96dda085d625d54ce800e426
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Aug 15 13:01:46 2014 +0200

    s390/ftrace: add HAVE_DYNAMIC_FTRACE_WITH_REGS support
    
    This code is based on a patch from Vojtech Pavlik.
    http://marc.info/?l=linux-s390&m=140438885114413&w=2
    
    The actual implementation now differs significantly:
    Instead of adding a second function "ftrace_regs_caller" which would be nearly
    identical to the existing ftrace_caller function, the current ftrace_caller
    function is now an alias to ftrace_regs_caller and always passes the needed
    pt_regs structure and function_trace_op parameters unconditionally.
    
    Besides that also use asm offsets to correctly allocate and access the new
    struct pt_regs on the stack.
    
    While at it we can make use of new instruction to get rid of some indirect
    loads if compiled for new machines.
    
    The passed struct pt_regs can be changed by the called function and it's new
    contents will replace the current contents.
    
    Note: to change the return address the embedded psw member of the pt_regs
    structure must be changed. The psw member is right now incomplete, since
    the mask part is missing. For all current use cases this should be sufficent.
    Providing and restoring a sane mask would mean we need to add an epsw/lpswe
    pair to the mcount code. Only these two instruction would cost us ~120 cycles
    which currently seems not necessary.
    
    Cc: Vojtech Pavlik <vojtech@suse.cz>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 7b8e456d76c9..1759d73fb95b 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -24,4 +24,8 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 #define MCOUNT_INSN_SIZE  22
 #endif
 
+#ifdef CONFIG_64BIT
+#define ARCH_SUPPORTS_FTRACE_OPS 1
+#endif
+
 #endif /* _ASM_S390_FTRACE_H */

commit 2481a87b0250bbf429fc8cdc78331efbc44a0221
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Aug 15 12:33:46 2014 +0200

    s390/ftrace: optimize function graph caller code
    
    When the function graph tracer is disabled we can skip three additional
    instructions. So let's just do this.
    
    So if function tracing is enabled but function graph tracing is
    runtime disabled, we get away with a single unconditional branch.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index bf246dae1367..7b8e456d76c9 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -4,6 +4,7 @@
 #ifndef __ASSEMBLY__
 
 extern void _mcount(void);
+extern char ftrace_graph_caller_end;
 
 struct dyn_arch_ftrace { };
 

commit aca91209775514b5a335e7ef11f1d636618590c7
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon May 13 14:48:52 2013 +0200

    s390/ftrace: fix mcount adjustment
    
    Tony Jones reported that the ftrace self tests on s390 do not work:
    
    <6>Testing dynamic ftrace ops #1: (0 0 0 0 0) FAILED!
    <6>Testing tracer irqsoff:
    <3>failed to start irqsoff tracer
    <4>.. no entries found ..FAILED!
    <6>Testing tracer wakeup:
    <3>failed to start wakeup tracer
    <4>.. no entries found ..FAILED!
    <6>Testing tracer function_graph:
    <4>Failed to init function_graph tracer, init returned -19
    <4>FAILED!
    
    This happens because we forgot to adjust the instruction pointer that gets
    passed to the ftrace trace function by MCOUNT_INSN_SIZE.
    
    In addition change MCOUNT_INSN_SIZE to the correct value on 31 bit.
    It only worked so far because the to be patched instruction was identical.
    
    Reported-by: Tony Jones <tonyj@suse.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index b7931faaef6d..bf246dae1367 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -9,11 +9,6 @@ struct dyn_arch_ftrace { };
 
 #define MCOUNT_ADDR ((long)_mcount)
 
-#ifdef CONFIG_64BIT
-#define MCOUNT_INSN_SIZE  12
-#else
-#define MCOUNT_INSN_SIZE  20
-#endif
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
@@ -21,4 +16,11 @@ static inline unsigned long ftrace_call_adjust(unsigned long addr)
 }
 
 #endif /* __ASSEMBLY__ */
+
+#ifdef CONFIG_64BIT
+#define MCOUNT_INSN_SIZE  12
+#else
+#define MCOUNT_INSN_SIZE  22
+#endif
+
 #endif /* _ASM_S390_FTRACE_H */

commit f29638868280534ed7e2fdd93b31557232597940
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue May 10 10:10:43 2011 +0200

    ftrace/s390: mcount offset calculation
    
    Do the mcount offset adjustment in the recordmcount.pl/recordmcount.[ch]
    at compile time and not in ftrace_call_adjust at run time.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 3c29be4836ed..b7931faaef6d 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -11,15 +11,13 @@ struct dyn_arch_ftrace { };
 
 #ifdef CONFIG_64BIT
 #define MCOUNT_INSN_SIZE  12
-#define MCOUNT_OFFSET	   8
 #else
 #define MCOUNT_INSN_SIZE  20
-#define MCOUNT_OFFSET	   4
 #endif
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)
 {
-	return addr - MCOUNT_OFFSET;
+	return addr;
 }
 
 #endif /* __ASSEMBLY__ */

commit 4cc9bed034d1ae588e5b773ee0edeb74ef3c0ff4
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Jan 5 12:48:11 2011 +0100

    [S390] cleanup ftrace backend functions
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 96c14a9102b8..3c29be4836ed 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -4,20 +4,17 @@
 #ifndef __ASSEMBLY__
 
 extern void _mcount(void);
-extern unsigned long ftrace_dyn_func;
 
 struct dyn_arch_ftrace { };
 
 #define MCOUNT_ADDR ((long)_mcount)
 
 #ifdef CONFIG_64BIT
-#define MCOUNT_OFFSET_RET 18
-#define MCOUNT_INSN_SIZE  24
-#define MCOUNT_OFFSET	  14
-#else
-#define MCOUNT_OFFSET_RET 26
-#define MCOUNT_INSN_SIZE  30
+#define MCOUNT_INSN_SIZE  12
 #define MCOUNT_OFFSET	   8
+#else
+#define MCOUNT_INSN_SIZE  20
+#define MCOUNT_OFFSET	   4
 #endif
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)

commit 88dbd2037229bd2ed7543ffd0d8f2d9dec9d31d2
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jun 12 10:26:46 2009 +0200

    [S390] ftrace: add function graph tracer support
    
    Function graph tracer support for s390.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index ba23d8f97d07..96c14a9102b8 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -11,11 +11,13 @@ struct dyn_arch_ftrace { };
 #define MCOUNT_ADDR ((long)_mcount)
 
 #ifdef CONFIG_64BIT
-#define MCOUNT_INSN_SIZE 24
-#define MCOUNT_OFFSET	 14
+#define MCOUNT_OFFSET_RET 18
+#define MCOUNT_INSN_SIZE  24
+#define MCOUNT_OFFSET	  14
 #else
-#define MCOUNT_INSN_SIZE 30
-#define MCOUNT_OFFSET	  8
+#define MCOUNT_OFFSET_RET 26
+#define MCOUNT_INSN_SIZE  30
+#define MCOUNT_OFFSET	   8
 #endif
 
 static inline unsigned long ftrace_call_adjust(unsigned long addr)

commit dfd9f7abc0fb67b5781f340d982384cea53b2884
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jun 12 10:26:44 2009 +0200

    [S390] ftrace: add dynamic ftrace support
    
    Dynamic ftrace support for s390.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
index 5a5bc75e19d4..ba23d8f97d07 100644
--- a/arch/s390/include/asm/ftrace.h
+++ b/arch/s390/include/asm/ftrace.h
@@ -2,7 +2,26 @@
 #define _ASM_S390_FTRACE_H
 
 #ifndef __ASSEMBLY__
+
 extern void _mcount(void);
+extern unsigned long ftrace_dyn_func;
+
+struct dyn_arch_ftrace { };
+
+#define MCOUNT_ADDR ((long)_mcount)
+
+#ifdef CONFIG_64BIT
+#define MCOUNT_INSN_SIZE 24
+#define MCOUNT_OFFSET	 14
+#else
+#define MCOUNT_INSN_SIZE 30
+#define MCOUNT_OFFSET	  8
 #endif
 
+static inline unsigned long ftrace_call_adjust(unsigned long addr)
+{
+	return addr - MCOUNT_OFFSET;
+}
+
+#endif /* __ASSEMBLY__ */
 #endif /* _ASM_S390_FTRACE_H */

commit 5d360a75f87f288e9a25d56cca503a7c7939e490
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Dec 25 13:38:56 2008 +0100

    [S390] ftrace: function tracer backend for s390
    
    This implements just the basic function tracer (_mcount) backend for s390.
    The dynamic variant will come later.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/ftrace.h b/arch/s390/include/asm/ftrace.h
new file mode 100644
index 000000000000..5a5bc75e19d4
--- /dev/null
+++ b/arch/s390/include/asm/ftrace.h
@@ -0,0 +1,8 @@
+#ifndef _ASM_S390_FTRACE_H
+#define _ASM_S390_FTRACE_H
+
+#ifndef __ASSEMBLY__
+extern void _mcount(void);
+#endif
+
+#endif /* _ASM_S390_FTRACE_H */
