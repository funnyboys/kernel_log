commit cb2cceaefb4c4dc28fc27ff1f1b2d258bfc10353
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Wed Apr 29 10:24:47 2020 -0500

    s390: Change s390_kernel_write() return type to match memcpy()
    
    s390_kernel_write()'s function type is almost identical to memcpy().
    Change its return type to "void *" so they can be used interchangeably.
    
    Cc: linux-s390@vger.kernel.org
    Cc: heiko.carstens@de.ibm.com
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Gerald Schaefer <gerald.schaefer@de.ibm.com> # s390
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index a470f1fa9f2a..324438889fe1 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -276,6 +276,6 @@ static inline unsigned long __must_check clear_user(void __user *to, unsigned lo
 }
 
 int copy_to_user_real(void __user *dest, void *src, unsigned long count);
-void s390_kernel_write(void *dst, const void *src, size_t size);
+void *s390_kernel_write(void *dst, const void *src, size_t size);
 
 #endif /* __S390_UACCESS_H */

commit 062795fcdcb2d22822fb42644b1d76a8ad8439b3
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Tue Oct 8 17:02:32 2019 +0200

    s390/uaccess: avoid (false positive) compiler warnings
    
    Depending on inlining decisions by the compiler, __get/put_user_fn
    might become out of line. Then the compiler is no longer able to tell
    that size can only be 1,2,4 or 8 due to the check in __get/put_user
    resulting in false positives like
    
    ./arch/s390/include/asm/uaccess.h: In function ‘__put_user_fn’:
    ./arch/s390/include/asm/uaccess.h:113:9: warning: ‘rc’ may be used uninitialized in this function [-Wmaybe-uninitialized]
      113 |  return rc;
          |         ^~
    ./arch/s390/include/asm/uaccess.h: In function ‘__get_user_fn’:
    ./arch/s390/include/asm/uaccess.h:143:9: warning: ‘rc’ may be used uninitialized in this function [-Wmaybe-uninitialized]
      143 |  return rc;
          |         ^~
    
    These functions are supposed to be always inlined. Mark it as such.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index bd2fd9a7821d..a470f1fa9f2a 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -83,7 +83,7 @@ raw_copy_to_user(void __user *to, const void *from, unsigned long n);
 	__rc;							\
 })
 
-static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
+static __always_inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 {
 	unsigned long spec = 0x010000UL;
 	int rc;
@@ -113,7 +113,7 @@ static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 	return rc;
 }
 
-static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long size)
+static __always_inline int __get_user_fn(void *x, const void __user *ptr, unsigned long size)
 {
 	unsigned long spec = 0x01UL;
 	int rc;

commit 01eb42afb45719cb41bb32c278e068073738899d
Author: Vasily Gorbik <gor@linux.ibm.com>
Date:   Tue Apr 23 15:36:36 2019 +0200

    s390/kasan: fix strncpy_from_user kasan checks
    
    arch/s390/lib/uaccess.c is built without kasan instrumentation. Kasan
    checks are performed explicitly in copy_from_user/copy_to_user
    functions. But since those functions could be inlined, calls from
    files like uaccess.c with instrumentation disabled won't generate
    kasan reports. This is currently the case with strncpy_from_user
    function which was revealed by newly added kasan test. Avoid inlining of
    copy_from_user/copy_to_user when the kernel is built with kasan support
    to make sure kasan checks are fully functional.
    
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 007fcb9aeeb8..bd2fd9a7821d 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -55,8 +55,10 @@ raw_copy_from_user(void *to, const void __user *from, unsigned long n);
 unsigned long __must_check
 raw_copy_to_user(void __user *to, const void *from, unsigned long n);
 
+#ifndef CONFIG_KASAN
 #define INLINE_COPY_FROM_USER
 #define INLINE_COPY_TO_USER
+#endif
 
 #ifdef CONFIG_HAVE_MARCH_Z10_FEATURES
 

commit 736706bee3298208343a76096370e4f6a5c55915
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 4 10:39:05 2019 -0800

    get rid of legacy 'get_ds()' function
    
    Every in-kernel use of this function defined it to KERNEL_DS (either as
    an actual define, or as an inline function).  It's an entirely
    historical artifact, and long long long ago used to actually read the
    segment selector valueof '%ds' on x86.
    
    Which in the kernel is always KERNEL_DS.
    
    Inspired by a patch from Jann Horn that just did this for a very small
    subset of users (the ones in fs/), along with Al who suggested a script.
    I then just took it to the logical extreme and removed all the remaining
    gunk.
    
    Roughly scripted with
    
       git grep -l '(get_ds())' -- :^tools/ | xargs sed -i 's/(get_ds())/(KERNEL_DS)/'
       git grep -lw 'get_ds' -- :^tools/ | xargs sed -i '/^#define get_ds()/d'
    
    plus manual fixups to remove a few unusual usage patterns, the couple of
    inline function cases and to fix up a comment that had become stale.
    
    The 'get_ds()' function remains in an x86 kvm selftest, since in user
    space it actually does something relevant.
    
    Inspired-by: Jann Horn <jannh@google.com>
    Inspired-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index bd2545977ad3..007fcb9aeeb8 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -31,7 +31,6 @@
 #define USER_DS		(2)
 #define USER_DS_SACF	(3)
 
-#define get_ds()        (KERNEL_DS)
 #define get_fs()        (current->thread.mm_segment)
 #define segment_eq(a,b) (((a) & 2) == ((b) & 2))
 

commit 96d4f267e40f9509e8a66e2b39e8b95655617693
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 3 18:57:57 2019 -0800

    Remove 'type' argument from access_ok() function
    
    Nobody has actually used the type (VERIFY_READ vs VERIFY_WRITE) argument
    of the user address range verification function since we got rid of the
    old racy i386-only code to walk page tables by hand.
    
    It existed because the original 80386 would not honor the write protect
    bit when in kernel mode, so you had to do COW by hand before doing any
    user access.  But we haven't supported that in a long time, and these
    days the 'type' argument is a purely historical artifact.
    
    A discussion about extending 'user_access_begin()' to do the range
    checking resulted this patch, because there is no way we're going to
    move the old VERIFY_xyz interface to that model.  And it's best done at
    the end of the merge window when I've done most of my merges, so let's
    just get this done once and for all.
    
    This patch was mostly done with a sed-script, with manual fix-ups for
    the cases that weren't of the trivial 'access_ok(VERIFY_xyz' form.
    
    There were a couple of notable cases:
    
     - csky still had the old "verify_area()" name as an alias.
    
     - the iter_iov code had magical hardcoded knowledge of the actual
       values of VERIFY_{READ,WRITE} (not that they mattered, since nothing
       really used it)
    
     - microblaze used the type argument for a debug printout
    
    but other than those oddities this should be a total no-op patch.
    
    I tried to fix up all architectures, did fairly extensive grepping for
    access_ok() uses, and the changes are trivial, but I may have missed
    something.  Any missed conversion should be trivially fixable, though.
    
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index ad6b91013a05..bd2545977ad3 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -48,7 +48,7 @@ static inline int __range_ok(unsigned long addr, unsigned long size)
 	__range_ok((unsigned long)(addr), (size));	\
 })
 
-#define access_ok(type, addr, size) __access_ok(addr, size)
+#define access_ok(addr, size) __access_ok(addr, size)
 
 unsigned long __must_check
 raw_copy_from_user(void *to, const void __user *from, unsigned long n);

commit 0aaba41b58bc5f3074c0c0a6136b9500b5e29e19
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Aug 22 12:08:22 2017 +0200

    s390: remove all code using the access register mode
    
    The vdso code for the getcpu() and the clock_gettime() call use the access
    register mode to access the per-CPU vdso data page with the current code.
    
    An alternative to the complicated AR mode is to use the secondary space
    mode. This makes the vdso faster and quite a bit simpler. The downside is
    that the uaccess code has to be changed quite a bit.
    
    Which instructions are used depends on the machine and what kind of uaccess
    operation is requested. The instruction dictates which ASCE value needs
    to be loaded into %cr1 and %cr7.
    
    The different cases:
    
    * User copy with MVCOS for z10 and newer machines
      The MVCOS instruction can copy between the primary space (aka user) and
      the home space (aka kernel) directly. For set_fs(KERNEL_DS) the kernel
      ASCE is loaded into %cr1. For set_fs(USER_DS) the user space is already
      loaded in %cr1.
    
    * User copy with MVCP/MVCS for older machines
      To be able to execute the MVCP/MVCS instructions the kernel needs to
      switch to primary mode. The control register %cr1 has to be set to the
      kernel ASCE and %cr7 to either the kernel ASCE or the user ASCE dependent
      on set_fs(KERNEL_DS) vs set_fs(USER_DS).
    
    * Data access in the user address space for strnlen / futex
      To use "normal" instruction with data from the user address space the
      secondary space mode is used. The kernel needs to switch to primary mode,
      %cr1 has to contain the kernel ASCE and %cr7 either the user ASCE or the
      kernel ASCE, dependent on set_fs.
    
    To load a new value into %cr1 or %cr7 is an expensive operation, the kernel
    tries to be lazy about it. E.g. for multiple user copies in a row with
    MVCP/MVCS the replacement of the vdso ASCE in %cr7 with the user ASCE is
    done only once. On return to user space a CPU bit is checked that loads the
    vdso ASCE again.
    
    To enable and disable the data access via the secondary space two new
    functions are added, enable_sacf_uaccess and disable_sacf_uaccess. The fact
    that a context is in secondary space uaccess mode is stored in the
    mm_segment_t value for the task. The code of an interrupt may use set_fs
    as long as it returns to the previous state it got with get_fs with another
    call to set_fs. The code in finish_arch_post_lock_switch simply has to do a
    set_fs with the current mm_segment_t value for the task.
    
    For CPUs with MVCOS:
    
    CPU running in                        | %cr1 ASCE | %cr7 ASCE |
    --------------------------------------|-----------|-----------|
    user space                            |  user     |  vdso     |
    kernel, USER_DS, normal-mode          |  user     |  vdso     |
    kernel, USER_DS, normal-mode, lazy    |  user     |  user     |
    kernel, USER_DS, sacf-mode            |  kernel   |  user     |
    kernel, KERNEL_DS, normal-mode        |  kernel   |  vdso     |
    kernel, KERNEL_DS, normal-mode, lazy  |  kernel   |  kernel   |
    kernel, KERNEL_DS, sacf-mode          |  kernel   |  kernel   |
    
    For CPUs without MVCOS:
    
    CPU running in                        | %cr1 ASCE | %cr7 ASCE |
    --------------------------------------|-----------|-----------|
    user space                            |  user     |  vdso     |
    kernel, USER_DS, normal-mode          |  user     |  vdso     |
    kernel, USER_DS, normal-mode lazy     |  kernel   |  user     |
    kernel, USER_DS, sacf-mode            |  kernel   |  user     |
    kernel, KERNEL_DS, normal-mode        |  kernel   |  vdso     |
    kernel, KERNEL_DS, normal-mode, lazy  |  kernel   |  kernel   |
    kernel, KERNEL_DS, sacf-mode          |  kernel   |  kernel   |
    
    The lines with "lazy" refer to the state after a copy via the secondary
    space with a delayed reload of %cr1 and %cr7.
    
    There are three hardware address spaces that can cause a DAT exception,
    primary, secondary and home space. The exception can be related to
    four different fault types: user space fault, vdso fault, kernel fault,
    and the gmap faults.
    
    Dependent on the set_fs state and normal vs. sacf mode there are a number
    of fault combinations:
    
    1) user address space fault via the primary ASCE
    2) gmap address space fault via the primary ASCE
    3) kernel address space fault via the primary ASCE for machines with
       MVCOS and set_fs(KERNEL_DS)
    4) vdso address space faults via the secondary ASCE with an invalid
       address while running in secondary space in problem state
    5) user address space fault via the secondary ASCE for user-copy
       based on the secondary space mode, e.g. futex_ops or strnlen_user
    6) kernel address space fault via the secondary ASCE for user-copy
       with secondary space mode with set_fs(KERNEL_DS)
    7) kernel address space fault via the primary ASCE for user-copy
       with secondary space mode with set_fs(USER_DS) on machines without
       MVCOS.
    8) kernel address space fault via the home space ASCE
    
    Replace user_space_fault() with a new function get_fault_type() that
    can distinguish all four different fault types.
    
    With these changes the futex atomic ops from the kernel and the
    strnlen_user will get a little bit slower, as well as the old style
    uaccess with MVCP/MVCS. All user accesses based on MVCOS will be as
    fast as before. On the positive side, the user space vdso code is a
    lot faster and Linux ceases to use the complicated AR mode.
    
    Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index cdd0f0d999e2..ad6b91013a05 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -16,7 +16,7 @@
 #include <asm/processor.h>
 #include <asm/ctl_reg.h>
 #include <asm/extable.h>
-
+#include <asm/facility.h>
 
 /*
  * The fs value determines whether argument validity checking should be
@@ -26,27 +26,16 @@
  * For historical reasons, these macros are grossly misnamed.
  */
 
-#define MAKE_MM_SEG(a)  ((mm_segment_t) { (a) })
-
-
-#define KERNEL_DS       MAKE_MM_SEG(0)
-#define USER_DS         MAKE_MM_SEG(1)
+#define KERNEL_DS	(0)
+#define KERNEL_DS_SACF	(1)
+#define USER_DS		(2)
+#define USER_DS_SACF	(3)
 
 #define get_ds()        (KERNEL_DS)
 #define get_fs()        (current->thread.mm_segment)
-#define segment_eq(a,b) ((a).ar4 == (b).ar4)
+#define segment_eq(a,b) (((a) & 2) == ((b) & 2))
 
-static inline void set_fs(mm_segment_t fs)
-{
-	current->thread.mm_segment = fs;
-	if (uaccess_kernel()) {
-		set_cpu_flag(CIF_ASCE_SECONDARY);
-		__ctl_load(S390_lowcore.kernel_asce, 7, 7);
-	} else {
-		clear_cpu_flag(CIF_ASCE_SECONDARY);
-		__ctl_load(S390_lowcore.user_asce, 7, 7);
-	}
-}
+void set_fs(mm_segment_t fs);
 
 static inline int __range_ok(unsigned long addr, unsigned long size)
 {
@@ -95,7 +84,7 @@ raw_copy_to_user(void __user *to, const void *from, unsigned long n);
 
 static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 {
-	unsigned long spec = 0x810000UL;
+	unsigned long spec = 0x010000UL;
 	int rc;
 
 	switch (size) {
@@ -125,7 +114,7 @@ static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 
 static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long size)
 {
-	unsigned long spec = 0x81UL;
+	unsigned long spec = 0x01UL;
 	int rc;
 
 	switch (size) {

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 304cfe44df50..cdd0f0d999e2 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  *  S390 version
  *    Copyright IBM Corp. 1999, 2000

commit 89cbec71fead552fdd1fa38c57186669dfbba734
Merge: 2173bd063151 3170d8d226c2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 15 11:17:52 2017 -0700

    Merge branch 'work.uaccess-unaligned' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull uacess-unaligned removal from Al Viro:
     "That stuff had just one user, and an exotic one, at that - binfmt_flat
      on arm and m68k"
    
    * 'work.uaccess-unaligned' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      kill {__,}{get,put}_user_unaligned()
      binfmt_flat: flat_{get,put}_addr_from_rp() should be able to fail

commit 3170d8d226c2053355f3946b4b5ded4c006fe6d4
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue May 2 20:06:33 2017 -0400

    kill {__,}{get,put}_user_unaligned()
    
    no users left
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 78f3f093d143..fad9df965ec2 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -249,9 +249,6 @@ int __put_user_bad(void) __attribute__((noreturn));
 
 int __get_user_bad(void) __attribute__((noreturn));
 
-#define __put_user_unaligned __put_user
-#define __get_user_unaligned __get_user
-
 unsigned long __must_check
 raw_copy_in_user(void __user *to, const void __user *from, unsigned long n);
 

commit 82985258390e85289940d3663344197344e071f2
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Apr 7 17:20:01 2017 -0400

    kill strlen_user()
    
    no callers, no consistent semantics, no sane way to use it...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 78f3f093d143..28b528197cf5 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -276,23 +276,6 @@ static inline unsigned long strnlen_user(const char __user *src, unsigned long n
 	return __strnlen_user(src, n);
 }
 
-/**
- * strlen_user: - Get the size of a string in user space.
- * @str: The string to measure.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Get the size of a NUL-terminated string in user space.
- *
- * Returns the size of the string INCLUDING the terminating NUL.
- * On exception, returns 0.
- *
- * If there is a limit on the length of a valid string, you may wish to
- * consider using strnlen_user() instead.
- */
-#define strlen_user(str) strnlen_user(str, ~0UL)
-
 /*
  * Zero Userspace
  */

commit 5db6db0d400edd8bec274e34960cfa22838e1df5
Merge: 5fab10041b43 2fefc97b2180
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 14:41:04 2017 -0700

    Merge branch 'work.uaccess' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull uaccess unification updates from Al Viro:
     "This is the uaccess unification pile. It's _not_ the end of uaccess
      work, but the next batch of that will go into the next cycle. This one
      mostly takes copy_from_user() and friends out of arch/* and gets the
      zero-padding behaviour in sync for all architectures.
    
      Dealing with the nocache/writethrough mess is for the next cycle;
      fortunately, that's x86-only. Same for cleanups in iov_iter.c (I am
      sold on access_ok() in there, BTW; just not in this pile), same for
      reducing __copy_... callsites, strn*... stuff, etc. - there will be a
      pile about as large as this one in the next merge window.
    
      This one sat in -next for weeks. -3KLoC"
    
    * 'work.uaccess' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (96 commits)
      HAVE_ARCH_HARDENED_USERCOPY is unconditional now
      CONFIG_ARCH_HAS_RAW_COPY_USER is unconditional now
      m32r: switch to RAW_COPY_USER
      hexagon: switch to RAW_COPY_USER
      microblaze: switch to RAW_COPY_USER
      get rid of padding, switch to RAW_COPY_USER
      ia64: get rid of copy_in_user()
      ia64: sanitize __access_ok()
      ia64: get rid of 'segment' argument of __do_{get,put}_user()
      ia64: get rid of 'segment' argument of __{get,put}_user_check()
      ia64: add extable.h
      powerpc: get rid of zeroing, switch to RAW_COPY_USER
      esas2r: don't open-code memdup_user()
      alpha: fix stack smashing in old_adjtimex(2)
      don't open-code kernel_setsockopt()
      mips: switch to RAW_COPY_USER
      mips: get rid of tail-zeroing in primitives
      mips: make copy_from_user() zero tail explicitly
      mips: clean and reorder the forest of macros...
      mips: consolidate __invoke_... wrappers
      ...

commit 37096003c8a7de5d24c6cf86234004c635fdd617
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Mar 28 15:06:24 2017 -0400

    s390: get rid of zeroing, switch to RAW_COPY_USER
    
    [folded a fix from Martin]
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 29f5bf24e5fa..72d69b9585c2 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -60,47 +60,14 @@ static inline int __range_ok(unsigned long addr, unsigned long size)
 
 #define access_ok(type, addr, size) __access_ok(addr, size)
 
-/**
- * __copy_from_user: - Copy a block of data from user space, with less checking.
- * @to:   Destination address, in kernel space.
- * @from: Source address, in user space.
- * @n:	  Number of bytes to copy.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Copy data from user space to kernel space.  Caller must check
- * the specified block with access_ok() before calling this function.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- *
- * If some data could not be copied, this function will pad the copied
- * data to the requested size using zero bytes.
- */
-unsigned long __must_check __copy_from_user(void *to, const void __user *from,
-					    unsigned long n);
+unsigned long __must_check
+raw_copy_from_user(void *to, const void __user *from, unsigned long n);
 
-/**
- * __copy_to_user: - Copy a block of data into user space, with less checking.
- * @to:   Destination address, in user space.
- * @from: Source address, in kernel space.
- * @n:	  Number of bytes to copy.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Copy data from kernel space to user space.  Caller must check
- * the specified block with access_ok() before calling this function.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- */
-unsigned long __must_check __copy_to_user(void __user *to, const void *from,
-					  unsigned long n);
+unsigned long __must_check
+raw_copy_to_user(void __user *to, const void *from, unsigned long n);
 
-#define __copy_to_user_inatomic __copy_to_user
-#define __copy_from_user_inatomic __copy_from_user
+#define INLINE_COPY_FROM_USER
+#define INLINE_COPY_TO_USER
 
 #ifdef CONFIG_HAVE_MARCH_Z10_FEATURES
 
@@ -189,13 +156,13 @@ static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long s
 
 static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 {
-	size = __copy_to_user(ptr, x, size);
+	size = raw_copy_to_user(ptr, x, size);
 	return size ? -EFAULT : 0;
 }
 
 static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long size)
 {
-	size = __copy_from_user(x, ptr, size);
+	size = raw_copy_from_user(x, ptr, size);
 	return size ? -EFAULT : 0;
 }
 
@@ -285,77 +252,8 @@ int __get_user_bad(void) __attribute__((noreturn));
 #define __put_user_unaligned __put_user
 #define __get_user_unaligned __get_user
 
-extern void __compiletime_error("usercopy buffer size is too small")
-__bad_copy_user(void);
-
-static inline void copy_user_overflow(int size, unsigned long count)
-{
-	WARN(1, "Buffer overflow detected (%d < %lu)!\n", size, count);
-}
-
-/**
- * copy_to_user: - Copy a block of data into user space.
- * @to:   Destination address, in user space.
- * @from: Source address, in kernel space.
- * @n:    Number of bytes to copy.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Copy data from kernel space to user space.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- */
-static inline unsigned long __must_check
-copy_to_user(void __user *to, const void *from, unsigned long n)
-{
-	might_fault();
-	return __copy_to_user(to, from, n);
-}
-
-/**
- * copy_from_user: - Copy a block of data from user space.
- * @to:   Destination address, in kernel space.
- * @from: Source address, in user space.
- * @n:    Number of bytes to copy.
- *
- * Context: User context only. This function may sleep if pagefaults are
- *          enabled.
- *
- * Copy data from user space to kernel space.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- *
- * If some data could not be copied, this function will pad the copied
- * data to the requested size using zero bytes.
- */
-static inline unsigned long __must_check
-copy_from_user(void *to, const void __user *from, unsigned long n)
-{
-	unsigned int sz = __compiletime_object_size(to);
-
-	might_fault();
-	if (unlikely(sz != -1 && sz < n)) {
-		if (!__builtin_constant_p(n))
-			copy_user_overflow(sz, n);
-		else
-			__bad_copy_user();
-		return n;
-	}
-	return __copy_from_user(to, from, n);
-}
-
 unsigned long __must_check
-__copy_in_user(void __user *to, const void __user *from, unsigned long n);
-
-static inline unsigned long __must_check
-copy_in_user(void __user *to, const void __user *from, unsigned long n)
-{
-	might_fault();
-	return __copy_in_user(to, from, n);
-}
+raw_copy_in_user(void __user *to, const void __user *from, unsigned long n);
 
 /*
  * Copy a null terminated string from userspace.

commit d09c5373e8e4eaaa09233552cbf75dc4c4f21203
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Mar 27 09:48:04 2017 +0200

    s390/uaccess: get_user() should zero on failure (again)
    
    Commit fd2d2b191fe7 ("s390: get_user() should zero on failure")
    intended to fix s390's get_user() implementation which did not zero
    the target operand if the read from user space faulted. Unfortunately
    the patch has no effect: the corresponding inline assembly specifies
    that the operand is only written to ("=") and the previous value is
    discarded.
    
    Therefore the compiler is free to and actually does omit the zero
    initialization.
    
    To fix this simply change the contraint modifier to "+", so the
    compiler cannot omit the initialization anymore.
    
    Fixes: c9ca78415ac1 ("s390/uaccess: provide inline variants of get_user/put_user")
    Fixes: fd2d2b191fe7 ("s390: get_user() should zero on failure")
    Cc: stable@vger.kernel.org
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 136932ff4250..3ea1554d04b3 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -147,7 +147,7 @@ unsigned long __must_check __copy_to_user(void __user *to, const void *from,
 		"	jg	2b\n"				\
 		".popsection\n"					\
 		EX_TABLE(0b,3b) EX_TABLE(1b,3b)			\
-		: "=d" (__rc), "=Q" (*(to))			\
+		: "=d" (__rc), "+Q" (*(to))			\
 		: "d" (size), "Q" (*(from)),			\
 		  "d" (__reg0), "K" (-EFAULT)			\
 		: "cc");					\

commit e70f1d59be747a959c240bf2fe2ea9489b629496
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Dec 26 00:48:37 2016 -0500

    s390: switch to extable.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 7228ed8da67d..29f5bf24e5fa 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -14,6 +14,7 @@
  */
 #include <asm/processor.h>
 #include <asm/ctl_reg.h>
+#include <asm/extable.h>
 
 
 /*
@@ -59,31 +60,6 @@ static inline int __range_ok(unsigned long addr, unsigned long size)
 
 #define access_ok(type, addr, size) __access_ok(addr, size)
 
-/*
- * The exception table consists of pairs of addresses: the first is the
- * address of an instruction that is allowed to fault, and the second is
- * the address at which the program should continue.  No registers are
- * modified, so it is entirely up to the continuation code to figure out
- * what to do.
- *
- * All the routines below use bits of fixup code that are out of line
- * with the main instruction path.  This means when everything is well,
- * we don't even have to jump over them.  Further, they do not intrude
- * on our cache or tlb entries.
- */
-
-struct exception_table_entry
-{
-	int insn, fixup;
-};
-
-static inline unsigned long extable_fixup(const struct exception_table_entry *x)
-{
-	return (unsigned long)&x->fixup + x->fixup;
-}
-
-#define ARCH_HAS_RELATIVE_EXTABLE
-
 /**
  * __copy_from_user: - Copy a block of data from user space, with less checking.
  * @to:   Destination address, in kernel space.

commit db68ce10c4f0a27c1ff9fa0e789e5c41f8c4ea63
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Mar 20 21:08:07 2017 -0400

    new helper: uaccess_kernel()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 9e9a5e8d6cf6..7228ed8da67d 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -37,7 +37,7 @@
 static inline void set_fs(mm_segment_t fs)
 {
 	current->thread.mm_segment = fs;
-	if (segment_eq(fs, KERNEL_DS)) {
+	if (uaccess_kernel()) {
 		set_cpu_flag(CIF_ASCE_SECONDARY);
 		__ctl_load(S390_lowcore.kernel_asce, 7, 7);
 	} else {

commit af1d5b37d6211c814fac0d5d0b71ec695618054a
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Dec 27 18:14:09 2016 -0500

    uaccess: drop duplicate includes from asm/uaccess.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 0e1f515d239b..9e9a5e8d6cf6 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -12,8 +12,6 @@
 /*
  * User space memory access functions
  */
-#include <linux/sched.h>
-#include <linux/errno.h>
 #include <asm/processor.h>
 #include <asm/ctl_reg.h>
 

commit 5e6039d8a307d8411422c154f3d446b44fa32b6d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Tue Dec 27 18:00:15 2016 -0500

    uaccess: move VERIFY_{READ,WRITE} definitions to linux/uaccess.h
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 136932ff4250..0e1f515d239b 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -17,9 +17,6 @@
 #include <asm/processor.h>
 #include <asm/ctl_reg.h>
 
-#define VERIFY_READ     0
-#define VERIFY_WRITE    1
-
 
 /*
  * The fs value determines whether argument validity checking should be

commit b5a882fcf146c87cb6b67c6df353e1c042b8773d
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Feb 17 08:13:28 2017 +0100

    s390: restore address space when returning to user space
    
    Unbalanced set_fs usages (e.g. early exit from a function and a
    forgotten set_fs(USER_DS) call) may lead to a situation where the
    secondary asce is the kernel space asce when returning to user
    space. This would allow user space to modify kernel space at will.
    
    This would only be possible with the above mentioned kernel bug,
    however we can detect this and fix the secondary asce before returning
    to user space.
    
    Therefore a new TIF_ASCE_SECONDARY which is used within set_fs. When
    returning to user space check if TIF_ASCE_SECONDARY is set, which
    would indicate a bug. If it is set print a message to the console,
    fixup the secondary asce, and then return to user space.
    
    This is similar to what is being discussed for x86 and arm:
    "[RFC] syscalls: Restore address limit after a syscall".
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index b2988fc60f65..136932ff4250 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -14,6 +14,7 @@
  */
 #include <linux/sched.h>
 #include <linux/errno.h>
+#include <asm/processor.h>
 #include <asm/ctl_reg.h>
 
 #define VERIFY_READ     0
@@ -36,18 +37,20 @@
 
 #define get_ds()        (KERNEL_DS)
 #define get_fs()        (current->thread.mm_segment)
-
-#define set_fs(x)							\
-do {									\
-	unsigned long __pto;						\
-	current->thread.mm_segment = (x);				\
-	__pto = current->thread.mm_segment.ar4 ?			\
-		S390_lowcore.user_asce : S390_lowcore.kernel_asce;	\
-	__ctl_load(__pto, 7, 7);					\
-} while (0)
-
 #define segment_eq(a,b) ((a).ar4 == (b).ar4)
 
+static inline void set_fs(mm_segment_t fs)
+{
+	current->thread.mm_segment = fs;
+	if (segment_eq(fs, KERNEL_DS)) {
+		set_cpu_flag(CIF_ASCE_SECONDARY);
+		__ctl_load(S390_lowcore.kernel_asce, 7, 7);
+	} else {
+		clear_cpu_flag(CIF_ASCE_SECONDARY);
+		__ctl_load(S390_lowcore.user_asce, 7, 7);
+	}
+}
+
 static inline int __range_ok(unsigned long addr, unsigned long size)
 {
 	return 1;

commit 1228f7befbf3280906e75f532a1700c7d3138117
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Feb 16 10:41:52 2017 +0100

    s390: add missing "do {} while (0)" loop constructs to multiline macros
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 3d1d0f7b7d28..b2988fc60f65 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -38,13 +38,13 @@
 #define get_fs()        (current->thread.mm_segment)
 
 #define set_fs(x)							\
-{									\
+do {									\
 	unsigned long __pto;						\
 	current->thread.mm_segment = (x);				\
 	__pto = current->thread.mm_segment.ar4 ?			\
 		S390_lowcore.user_asce : S390_lowcore.kernel_asce;	\
 	__ctl_load(__pto, 7, 7);					\
-}
+} while (0)
 
 #define segment_eq(a,b) ((a).ar4 == (b).ar4)
 

commit 0b92515916799ca2711c7c46abccb96c275cc88a
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Jan 2 08:51:02 2017 +0100

    s390: remove couple of unneeded semicolons
    
    Remove a couple of unneeded semicolons. This is just to reduce the
    noise that the coccinelle static code checker generates.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index f82b04e85a21..3d1d0f7b7d28 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -177,7 +177,7 @@ static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 					(unsigned long *)x,
 					size, spec);
 		break;
-	};
+	}
 	return rc;
 }
 
@@ -207,7 +207,7 @@ static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long s
 					(unsigned long __user *)ptr,
 					size, spec);
 		break;
-	};
+	}
 	return rc;
 }
 

commit 56e9219a8248321b71199ca3b0279e07d23d3576
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Nov 14 14:28:51 2016 +0100

    s390/uaccess: make setfs macro return void
    
    For an unknown (historic) reason the s390 specific implementation of
    set_fs returns whatever the __ctl_load would return. The set_fs macro
    however is supposed to return void.
    Change the macro to do that.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 52d7c8709279..f82b04e85a21 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -37,14 +37,14 @@
 #define get_ds()        (KERNEL_DS)
 #define get_fs()        (current->thread.mm_segment)
 
-#define set_fs(x) \
-({									\
+#define set_fs(x)							\
+{									\
 	unsigned long __pto;						\
 	current->thread.mm_segment = (x);				\
 	__pto = current->thread.mm_segment.ar4 ?			\
 		S390_lowcore.user_asce : S390_lowcore.kernel_asce;	\
 	__ctl_load(__pto, 7, 7);					\
-})
+}
 
 #define segment_eq(a,b) ((a).ar4 == (b).ar4)
 

commit 77e5bdf9f7b2d20939c8d807f3e68778d6e1557a
Merge: b8f26e880c81 8630c32275ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 14 09:35:05 2016 -0700

    Merge branch 'uaccess-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull uaccess fixes from Al Viro:
     "Fixes for broken uaccess primitives - mostly lack of proper zeroing
      in copy_from_user()/get_user()/__get_user(), but for several
      architectures there's more (broken clear_user() on frv and
      strncpy_from_user() on hexagon)"
    
    * 'uaccess-fixes' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (28 commits)
      avr32: fix copy_from_user()
      microblaze: fix __get_user()
      microblaze: fix copy_from_user()
      m32r: fix __get_user()
      blackfin: fix copy_from_user()
      sparc32: fix copy_from_user()
      sh: fix copy_from_user()
      sh64: failing __get_user() should zero
      score: fix copy_from_user() and friends
      score: fix __get_user/get_user
      s390: get_user() should zero on failure
      ppc32: fix copy_from_user()
      parisc: fix copy_from_user()
      openrisc: fix copy_from_user()
      nios2: fix __get_user()
      nios2: copy_from_user() should zero the tail of destination
      mn10300: copy_from_user() should zero on access_ok() failure...
      mn10300: failing __get_user() and get_user() should zero
      mips: copy_from_user() must zero the destination on access_ok() failure
      ARC: uaccess: get_user to zero out dest in cause of fault
      ...

commit fd2d2b191fe75825c4c7a6f12f3fef35aaed7dd7
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Aug 21 22:00:54 2016 -0400

    s390: get_user() should zero on failure
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 9b49cf1daa8f..2c5d292ff752 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -266,28 +266,28 @@ int __put_user_bad(void) __attribute__((noreturn));
 	__chk_user_ptr(ptr);					\
 	switch (sizeof(*(ptr))) {				\
 	case 1: {						\
-		unsigned char __x;				\
+		unsigned char __x = 0;				\
 		__gu_err = __get_user_fn(&__x, ptr,		\
 					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
 	case 2: {						\
-		unsigned short __x;				\
+		unsigned short __x = 0;				\
 		__gu_err = __get_user_fn(&__x, ptr,		\
 					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
 	case 4: {						\
-		unsigned int __x;				\
+		unsigned int __x = 0;				\
 		__gu_err = __get_user_fn(&__x, ptr,		\
 					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
 	case 8: {						\
-		unsigned long long __x;				\
+		unsigned long long __x = 0;			\
 		__gu_err = __get_user_fn(&__x, ptr,		\
 					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\

commit 0d025d271e55f3de21f0aaaf54b42d20404d2b23
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Tue Aug 30 08:04:16 2016 -0500

    mm/usercopy: get rid of CONFIG_DEBUG_STRICT_USER_COPY_CHECKS
    
    There are three usercopy warnings which are currently being silenced for
    gcc 4.6 and newer:
    
    1) "copy_from_user() buffer size is too small" compile warning/error
    
       This is a static warning which happens when object size and copy size
       are both const, and copy size > object size.  I didn't see any false
       positives for this one.  So the function warning attribute seems to
       be working fine here.
    
       Note this scenario is always a bug and so I think it should be
       changed to *always* be an error, regardless of
       CONFIG_DEBUG_STRICT_USER_COPY_CHECKS.
    
    2) "copy_from_user() buffer size is not provably correct" compile warning
    
       This is another static warning which happens when I enable
       __compiletime_object_size() for new compilers (and
       CONFIG_DEBUG_STRICT_USER_COPY_CHECKS).  It happens when object size
       is const, but copy size is *not*.  In this case there's no way to
       compare the two at build time, so it gives the warning.  (Note the
       warning is a byproduct of the fact that gcc has no way of knowing
       whether the overflow function will be called, so the call isn't dead
       code and the warning attribute is activated.)
    
       So this warning seems to only indicate "this is an unusual pattern,
       maybe you should check it out" rather than "this is a bug".
    
       I get 102(!) of these warnings with allyesconfig and the
       __compiletime_object_size() gcc check removed.  I don't know if there
       are any real bugs hiding in there, but from looking at a small
       sample, I didn't see any.  According to Kees, it does sometimes find
       real bugs.  But the false positive rate seems high.
    
    3) "Buffer overflow detected" runtime warning
    
       This is a runtime warning where object size is const, and copy size >
       object size.
    
    All three warnings (both static and runtime) were completely disabled
    for gcc 4.6 with the following commit:
    
      2fb0815c9ee6 ("gcc4: disable __compiletime_object_size for GCC 4.6+")
    
    That commit mistakenly assumed that the false positives were caused by a
    gcc bug in __compiletime_object_size().  But in fact,
    __compiletime_object_size() seems to be working fine.  The false
    positives were instead triggered by #2 above.  (Though I don't have an
    explanation for why the warnings supposedly only started showing up in
    gcc 4.6.)
    
    So remove warning #2 to get rid of all the false positives, and re-enable
    warnings #1 and #3 by reverting the above commit.
    
    Furthermore, since #1 is a real bug which is detected at compile time,
    upgrade it to always be an error.
    
    Having done all that, CONFIG_DEBUG_STRICT_USER_COPY_CHECKS is no longer
    needed.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: "H . Peter Anvin" <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Nilay Vaish <nilayvaish@gmail.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 9b49cf1daa8f..95aefdba4be2 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -311,6 +311,14 @@ int __get_user_bad(void) __attribute__((noreturn));
 #define __put_user_unaligned __put_user
 #define __get_user_unaligned __get_user
 
+extern void __compiletime_error("usercopy buffer size is too small")
+__bad_copy_user(void);
+
+static inline void copy_user_overflow(int size, unsigned long count)
+{
+	WARN(1, "Buffer overflow detected (%d < %lu)!\n", size, count);
+}
+
 /**
  * copy_to_user: - Copy a block of data into user space.
  * @to:   Destination address, in user space.
@@ -332,12 +340,6 @@ copy_to_user(void __user *to, const void *from, unsigned long n)
 	return __copy_to_user(to, from, n);
 }
 
-void copy_from_user_overflow(void)
-#ifdef CONFIG_DEBUG_STRICT_USER_COPY_CHECKS
-__compiletime_warning("copy_from_user() buffer size is not provably correct")
-#endif
-;
-
 /**
  * copy_from_user: - Copy a block of data from user space.
  * @to:   Destination address, in kernel space.
@@ -362,7 +364,10 @@ copy_from_user(void *to, const void __user *from, unsigned long n)
 
 	might_fault();
 	if (unlikely(sz != -1 && sz < n)) {
-		copy_from_user_overflow();
+		if (!__builtin_constant_p(n))
+			copy_user_overflow(sz, n);
+		else
+			__bad_copy_user();
 		return n;
 	}
 	return __copy_from_user(to, from, n);

commit dc4aace160165bd15400a79f9f5ab97405278f48
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Jun 20 10:35:20 2016 +0200

    s390/uaccess: fix __put_get_user_asm define
    
    The __put_get_user_asm defines an inline assmembly which makes use of
    the asm register construct. The parameters passed to that define may
    also contain function calls.
    
    It is a gcc restriction that between register asm statements and the
    use of any such annotated variables function calls may clobber the
    register / variable contents. Or in other words: gcc would generate
    broken code.
    
    This can be achieved e.g. with the following code:
    
        get_user(x, func() ? a : b);
    
    where the call of func would clobber register zero which is used by
    the __put_get_user_asm define.
    To avoid this add two static inline functions which don't have these
    side effects.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 2ea7f3208cd2..9b49cf1daa8f 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -151,8 +151,65 @@ unsigned long __must_check __copy_to_user(void __user *to, const void *from,
 	__rc;							\
 })
 
-#define __put_user_fn(x, ptr, size) __put_get_user_asm(ptr, x, size, 0x810000UL)
-#define __get_user_fn(x, ptr, size) __put_get_user_asm(x, ptr, size, 0x81UL)
+static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
+{
+	unsigned long spec = 0x810000UL;
+	int rc;
+
+	switch (size) {
+	case 1:
+		rc = __put_get_user_asm((unsigned char __user *)ptr,
+					(unsigned char *)x,
+					size, spec);
+		break;
+	case 2:
+		rc = __put_get_user_asm((unsigned short __user *)ptr,
+					(unsigned short *)x,
+					size, spec);
+		break;
+	case 4:
+		rc = __put_get_user_asm((unsigned int __user *)ptr,
+					(unsigned int *)x,
+					size, spec);
+		break;
+	case 8:
+		rc = __put_get_user_asm((unsigned long __user *)ptr,
+					(unsigned long *)x,
+					size, spec);
+		break;
+	};
+	return rc;
+}
+
+static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long size)
+{
+	unsigned long spec = 0x81UL;
+	int rc;
+
+	switch (size) {
+	case 1:
+		rc = __put_get_user_asm((unsigned char *)x,
+					(unsigned char __user *)ptr,
+					size, spec);
+		break;
+	case 2:
+		rc = __put_get_user_asm((unsigned short *)x,
+					(unsigned short __user *)ptr,
+					size, spec);
+		break;
+	case 4:
+		rc = __put_get_user_asm((unsigned int *)x,
+					(unsigned int __user *)ptr,
+					size, spec);
+		break;
+	case 8:
+		rc = __put_get_user_asm((unsigned long *)x,
+					(unsigned long __user *)ptr,
+					size, spec);
+		break;
+	};
+	return rc;
+}
 
 #else /* CONFIG_HAVE_MARCH_Z10_FEATURES */
 

commit ee64baf4ea3b7affb7e325bb3d188515ec4bb66e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Jun 13 10:17:20 2016 +0200

    s390/uaccess: use __builtin_expect for get_user/put_user
    
    We always expect that get_user and put_user return with zero. Give the
    compiler a hint so it can slightly optimize the code and avoid
    branches.
    This is the same what x86 got with commit a76cf66e948a ("x86/uaccess:
    Tell the compiler that uaccess is unlikely to fault").
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index e0900ddf91dd..2ea7f3208cd2 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -191,7 +191,7 @@ static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long s
 		__put_user_bad();				\
 		break;						\
 	 }							\
-	__pu_err;						\
+	__builtin_expect(__pu_err, 0);				\
 })
 
 #define put_user(x, ptr)					\
@@ -240,7 +240,7 @@ int __put_user_bad(void) __attribute__((noreturn));
 		__get_user_bad();				\
 		break;						\
 	}							\
-	__gu_err;						\
+	__builtin_expect(__gu_err, 0);				\
 })
 
 #define get_user(x, ptr)					\

commit c352e8b6de9808ec5edc792d54f8785de6626d02
Author: Ard Biesheuvel <ard.biesheuvel@linaro.org>
Date:   Tue Mar 22 14:28:14 2016 -0700

    s390/extable: use generic search and sort routines
    
    Replace the arch specific versions of search_extable() and
    sort_extable() with calls to the generic ones, which now support
    relative exception tables as well.
    
    Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 9dd4cc47ddc7..e0900ddf91dd 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -79,18 +79,12 @@ struct exception_table_entry
 	int insn, fixup;
 };
 
-static inline unsigned long extable_insn(const struct exception_table_entry *x)
-{
-	return (unsigned long)&x->insn + x->insn;
-}
-
 static inline unsigned long extable_fixup(const struct exception_table_entry *x)
 {
 	return (unsigned long)&x->fixup + x->fixup;
 }
 
-#define ARCH_HAS_SORT_EXTABLE
-#define ARCH_HAS_SEARCH_EXTABLE
+#define ARCH_HAS_RELATIVE_EXTABLE
 
 /**
  * __copy_from_user: - Copy a block of data from user space, with less checking.

commit b3c395ef5556a6c60f4426cc060f5b7bdcf82d5b
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon May 11 17:52:08 2015 +0200

    mm/uaccess, mm/fault: Clarify that uaccess may only sleep if pagefaults are enabled
    
    In general, non-atomic variants of user access functions must not sleep
    if pagefaults are disabled.
    
    Let's update all relevant comments in uaccess code. This also reflects
    the might_sleep() checks in might_fault().
    
    Reviewed-and-tested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: David.Laight@ACULAB.COM
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: airlied@linux.ie
    Cc: akpm@linux-foundation.org
    Cc: benh@kernel.crashing.org
    Cc: bigeasy@linutronix.de
    Cc: borntraeger@de.ibm.com
    Cc: daniel.vetter@intel.com
    Cc: heiko.carstens@de.ibm.com
    Cc: herbert@gondor.apana.org.au
    Cc: hocko@suse.cz
    Cc: hughd@google.com
    Cc: mst@redhat.com
    Cc: paulus@samba.org
    Cc: ralf@linux-mips.org
    Cc: schwidefsky@de.ibm.com
    Cc: yang.shi@windriver.com
    Link: http://lkml.kernel.org/r/1431359540-32227-4-git-send-email-dahi@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index d64a7a62164f..9dd4cc47ddc7 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -98,7 +98,8 @@ static inline unsigned long extable_fixup(const struct exception_table_entry *x)
  * @from: Source address, in user space.
  * @n:	  Number of bytes to copy.
  *
- * Context: User context only.	This function may sleep.
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
  *
  * Copy data from user space to kernel space.  Caller must check
  * the specified block with access_ok() before calling this function.
@@ -118,7 +119,8 @@ unsigned long __must_check __copy_from_user(void *to, const void __user *from,
  * @from: Source address, in kernel space.
  * @n:	  Number of bytes to copy.
  *
- * Context: User context only.	This function may sleep.
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
  *
  * Copy data from kernel space to user space.  Caller must check
  * the specified block with access_ok() before calling this function.
@@ -264,7 +266,8 @@ int __get_user_bad(void) __attribute__((noreturn));
  * @from: Source address, in kernel space.
  * @n:    Number of bytes to copy.
  *
- * Context: User context only.  This function may sleep.
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
  *
  * Copy data from kernel space to user space.
  *
@@ -290,7 +293,8 @@ __compiletime_warning("copy_from_user() buffer size is not provably correct")
  * @from: Source address, in user space.
  * @n:    Number of bytes to copy.
  *
- * Context: User context only.  This function may sleep.
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
  *
  * Copy data from user space to kernel space.
  *
@@ -348,7 +352,8 @@ static inline unsigned long strnlen_user(const char __user *src, unsigned long n
  * strlen_user: - Get the size of a string in user space.
  * @str: The string to measure.
  *
- * Context: User context only.  This function may sleep.
+ * Context: User context only. This function may sleep if pagefaults are
+ *          enabled.
  *
  * Get the size of a NUL-terminated string in user space.
  *

commit 8a5d8473dd7e2b0bc2864e34bd6836b520589fa1
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Mar 13 12:55:56 2015 +0100

    s390/maccess: remove potentially broken probe_kernel_write()
    
    Remove the s390 architecture implementation of probe_kernel_write() and
    instead use a new function s390_kernel_write() to modify kernel text and
    data everywhere.
    
    The s390 implementation of probe_kernel_write() was potentially broken
    since it modified memory in a read-modify-write fashion, which read four
    bytes, modified the requested bytes within those four bytes and wrote
    the result back.
    If two cpus would modify the same four byte area at different locations
    within that area, this could lead to corruption.
    Right now the only places which called probe_kernel_write() did run within
    stop_machine_run. Therefore the scenario can't happen right now, however
    that might change at any time.
    
    To fix this rename probe_kernel_write() to s390_kernel_write() which can
    have special semantics, like only call it while running within stop_machine().
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index cd4c68e0398d..d64a7a62164f 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -372,5 +372,6 @@ static inline unsigned long __must_check clear_user(void __user *to, unsigned lo
 }
 
 int copy_to_user_real(void __user *dest, void *src, unsigned long count);
+void s390_kernel_write(void *dst, const void *src, size_t size);
 
 #endif /* __S390_UACCESS_H */

commit c9ca78415ac1cbb0e9846111d5f01376266cf6f3
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Apr 17 14:16:03 2014 +0200

    s390/uaccess: provide inline variants of get_user/put_user
    
    This shortens the code by ~17k (performace_defconfig, march=z196).
    The number of exception table entries however increases from 164
    entries to 2500 entries (+~18k).
    However the executed code is shorter and also faster since we save
    the branches to the out-of-line copy_to/from_user implementations.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 1be64a1506d0..cd4c68e0398d 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -132,6 +132,34 @@ unsigned long __must_check __copy_to_user(void __user *to, const void *from,
 #define __copy_to_user_inatomic __copy_to_user
 #define __copy_from_user_inatomic __copy_from_user
 
+#ifdef CONFIG_HAVE_MARCH_Z10_FEATURES
+
+#define __put_get_user_asm(to, from, size, spec)		\
+({								\
+	register unsigned long __reg0 asm("0") = spec;		\
+	int __rc;						\
+								\
+	asm volatile(						\
+		"0:	mvcos	%1,%3,%2\n"			\
+		"1:	xr	%0,%0\n"			\
+		"2:\n"						\
+		".pushsection .fixup, \"ax\"\n"			\
+		"3:	lhi	%0,%5\n"			\
+		"	jg	2b\n"				\
+		".popsection\n"					\
+		EX_TABLE(0b,3b) EX_TABLE(1b,3b)			\
+		: "=d" (__rc), "=Q" (*(to))			\
+		: "d" (size), "Q" (*(from)),			\
+		  "d" (__reg0), "K" (-EFAULT)			\
+		: "cc");					\
+	__rc;							\
+})
+
+#define __put_user_fn(x, ptr, size) __put_get_user_asm(ptr, x, size, 0x810000UL)
+#define __get_user_fn(x, ptr, size) __put_get_user_asm(x, ptr, size, 0x81UL)
+
+#else /* CONFIG_HAVE_MARCH_Z10_FEATURES */
+
 static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 {
 	size = __copy_to_user(ptr, x, size);
@@ -144,6 +172,8 @@ static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long s
 	return size ? -EFAULT : 0;
 }
 
+#endif /* CONFIG_HAVE_MARCH_Z10_FEATURES */
+
 /*
  * These are the main single-value transfer routines.  They automatically
  * use the right size if we just have the right pointer type.

commit 457f2180951cdcbfb4657ddcc83b486e93497f56
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Mar 21 10:42:25 2014 +0100

    s390/uaccess: rework uaccess code - fix locking issues
    
    The current uaccess code uses a page table walk in some circumstances,
    e.g. in case of the in atomic futex operations or if running on old
    hardware which doesn't support the mvcos instruction.
    
    However it turned out that the page table walk code does not correctly
    lock page tables when accessing page table entries.
    In other words: a different cpu may invalidate a page table entry while
    the current cpu inspects the pte. This may lead to random data corruption.
    
    Adding correct locking however isn't trivial for all uaccess operations.
    Especially copy_in_user() is problematic since that requires to hold at
    least two locks, but must be protected against ABBA deadlock when a
    different cpu also performs a copy_in_user() operation.
    
    So the solution is a different approach where we change address spaces:
    
    User space runs in primary address mode, or access register mode within
    vdso code, like it currently already does.
    
    The kernel usually also runs in home space mode, however when accessing
    user space the kernel switches to primary or secondary address mode if
    the mvcos instruction is not available or if a compare-and-swap (futex)
    instruction on a user space address is performed.
    KVM however is special, since that requires the kernel to run in home
    address space while implicitly accessing user space with the sie
    instruction.
    
    So we end up with:
    
    User space:
    - runs in primary or access register mode
    - cr1 contains the user asce
    - cr7 contains the user asce
    - cr13 contains the kernel asce
    
    Kernel space:
    - runs in home space mode
    - cr1 contains the user or kernel asce
      -> the kernel asce is loaded when a uaccess requires primary or
         secondary address mode
    - cr7 contains the user or kernel asce, (changed with set_fs())
    - cr13 contains the kernel asce
    
    In case of uaccess the kernel changes to:
    - primary space mode in case of a uaccess (copy_to_user) and uses
      e.g. the mvcp instruction to access user space. However the kernel
      will stay in home space mode if the mvcos instruction is available
    - secondary space mode in case of futex atomic operations, so that the
      instructions come from primary address space and data from secondary
      space
    
    In case of kvm the kernel runs in home space mode, but cr1 gets switched
    to contain the gmap asce before the sie instruction gets executed. When
    the sie instruction is finished cr1 will be switched back to contain the
    user asce.
    
    A context switch between two processes will always load the kernel asce
    for the next process in cr1. So the first exit to user space is a bit
    more expensive (one extra load control register instruction) than before,
    however keeps the code rather simple.
    
    In sum this means there is no need to perform any error prone page table
    walks anymore when accessing user space.
    
    The patch seems to be rather large, however it mainly removes the
    the page table walk code and restores the previously deleted "standard"
    uaccess code, with a couple of changes.
    
    The uaccess without mvcos mode can be enforced with the "uaccess_primary"
    kernel parameter.
    
    Reported-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 4133b3f72fb0..1be64a1506d0 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -92,8 +92,6 @@ static inline unsigned long extable_fixup(const struct exception_table_entry *x)
 #define ARCH_HAS_SORT_EXTABLE
 #define ARCH_HAS_SEARCH_EXTABLE
 
-int __handle_fault(unsigned long, unsigned long, int);
-
 /**
  * __copy_from_user: - Copy a block of data from user space, with less checking.
  * @to:   Destination address, in kernel space.

commit 823002023da6d9124ed63bc622267c15ab2a7347
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Feb 24 16:18:55 2014 +0100

    s390/uaccess: remove copy_from_user_real()
    
    There is no user left, so remove it.
    It was also potentially broken, since the function didn't clear destination
    memory if copy_from_user() failed. Which would allow for information leaks.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 2710b41eed22..4133b3f72fb0 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -344,6 +344,5 @@ static inline unsigned long __must_check clear_user(void __user *to, unsigned lo
 }
 
 int copy_to_user_real(void __user *dest, void *src, unsigned long count);
-int copy_from_user_real(void *dest, void __user *src, unsigned long count);
 
 #endif /* __S390_UACCESS_H */

commit 211deca6bf413560b562d69748ebc4df5d80d65e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jan 24 12:51:27 2014 +0100

    s390/uaccess: consistent types
    
    The types 'size_t' and 'unsigned long' have been used randomly for the
    uaccess functions. This looks rather confusing.
    So let's change all functions to use unsigned long instead and get rid
    of size_t in order to have a consistent interface.
    
    The only exception is strncpy_from_user() which uses 'long' since it
    may return a signed value (-EFAULT).
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 49885a518e5e..2710b41eed22 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -111,8 +111,8 @@ int __handle_fault(unsigned long, unsigned long, int);
  * If some data could not be copied, this function will pad the copied
  * data to the requested size using zero bytes.
  */
-size_t __must_check __copy_from_user(void *to, const void __user *from,
-				     size_t n);
+unsigned long __must_check __copy_from_user(void *to, const void __user *from,
+					    unsigned long n);
 
 /**
  * __copy_to_user: - Copy a block of data into user space, with less checking.
@@ -134,13 +134,13 @@ unsigned long __must_check __copy_to_user(void __user *to, const void *from,
 #define __copy_to_user_inatomic __copy_to_user
 #define __copy_from_user_inatomic __copy_from_user
 
-static inline int __put_user_fn(void *x, void __user *ptr, size_t size)
+static inline int __put_user_fn(void *x, void __user *ptr, unsigned long size)
 {
 	size = __copy_to_user(ptr, x, size);
 	return size ? -EFAULT : 0;
 }
 
-static inline int __get_user_fn(void *x, const void __user *ptr, size_t size)
+static inline int __get_user_fn(void *x, const void __user *ptr, unsigned long size)
 {
 	size = __copy_from_user(x, ptr, size);
 	return size ? -EFAULT : 0;
@@ -308,9 +308,9 @@ strncpy_from_user(char *dst, const char __user *src, long count)
 	return __strncpy_from_user(dst, src, count);
 }
 
-size_t __must_check __strnlen_user(const char __user *src, size_t count);
+unsigned long __must_check __strnlen_user(const char __user *src, unsigned long count);
 
-static inline size_t strnlen_user(const char __user *src, size_t n)
+static inline unsigned long strnlen_user(const char __user *src, unsigned long n)
 {
 	might_fault();
 	return __strnlen_user(src, n);
@@ -335,15 +335,15 @@ static inline size_t strnlen_user(const char __user *src, size_t n)
 /*
  * Zero Userspace
  */
-size_t __must_check __clear_user(void __user *to, size_t size);
+unsigned long __must_check __clear_user(void __user *to, unsigned long size);
 
-static inline size_t __must_check clear_user(void __user *to, size_t n)
+static inline unsigned long __must_check clear_user(void __user *to, unsigned long n)
 {
 	might_fault();
 	return __clear_user(to, n);
 }
 
-int copy_to_user_real(void __user *dest, void *src, size_t count);
-int copy_from_user_real(void *dest, void __user *src, size_t count);
+int copy_to_user_real(void __user *dest, void *src, unsigned long count);
+int copy_from_user_real(void *dest, void __user *src, unsigned long count);
 
 #endif /* __S390_UACCESS_H */

commit 4f41c2b4567dbfb7ff93e5c552b869e2865bcd9d
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Jan 23 11:18:36 2014 +0100

    s390/uaccess: get rid of indirect function calls
    
    There are only two uaccess variants on s390 left: the version that is used
    if the mvcos instruction is available, and the page table walk variant.
    So there is no need for expensive indirect function calls.
    
    By default the mvcos variant will be called. If the mvcos instruction is not
    available it will call the page table walk variant.
    
    For minimal performance impact the "if (mvcos_is_available)" is implemented
    with a jump label, which will be a six byte nop on machines with mvcos.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 73199636ba98..49885a518e5e 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -92,33 +92,58 @@ static inline unsigned long extable_fixup(const struct exception_table_entry *x)
 #define ARCH_HAS_SORT_EXTABLE
 #define ARCH_HAS_SEARCH_EXTABLE
 
-struct uaccess_ops {
-	size_t (*copy_from_user)(void *, const void __user *, size_t);
-	size_t (*copy_to_user)(void __user *, const void *, size_t);
-	size_t (*copy_in_user)(void __user *, const void __user *, size_t);
-	size_t (*clear_user)(void __user *, size_t);
-	size_t (*strnlen_user)(const char __user *, size_t);
-	size_t (*strncpy_from_user)(char *, const char __user *, size_t);
-	int (*futex_atomic_op)(int op, u32 __user *, int oparg, int *old);
-	int (*futex_atomic_cmpxchg)(u32 *, u32 __user *, u32 old, u32 new);
-};
+int __handle_fault(unsigned long, unsigned long, int);
 
-extern struct uaccess_ops uaccess;
-extern struct uaccess_ops uaccess_mvcos;
-extern struct uaccess_ops uaccess_pt;
+/**
+ * __copy_from_user: - Copy a block of data from user space, with less checking.
+ * @to:   Destination address, in kernel space.
+ * @from: Source address, in user space.
+ * @n:	  Number of bytes to copy.
+ *
+ * Context: User context only.	This function may sleep.
+ *
+ * Copy data from user space to kernel space.  Caller must check
+ * the specified block with access_ok() before calling this function.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ *
+ * If some data could not be copied, this function will pad the copied
+ * data to the requested size using zero bytes.
+ */
+size_t __must_check __copy_from_user(void *to, const void __user *from,
+				     size_t n);
+
+/**
+ * __copy_to_user: - Copy a block of data into user space, with less checking.
+ * @to:   Destination address, in user space.
+ * @from: Source address, in kernel space.
+ * @n:	  Number of bytes to copy.
+ *
+ * Context: User context only.	This function may sleep.
+ *
+ * Copy data from kernel space to user space.  Caller must check
+ * the specified block with access_ok() before calling this function.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ */
+unsigned long __must_check __copy_to_user(void __user *to, const void *from,
+					  unsigned long n);
 
-extern int __handle_fault(unsigned long, unsigned long, int);
+#define __copy_to_user_inatomic __copy_to_user
+#define __copy_from_user_inatomic __copy_from_user
 
 static inline int __put_user_fn(void *x, void __user *ptr, size_t size)
 {
-	size = uaccess.copy_to_user(ptr, x, size);
-	return size ? -EFAULT : size;
+	size = __copy_to_user(ptr, x, size);
+	return size ? -EFAULT : 0;
 }
 
 static inline int __get_user_fn(void *x, const void __user *ptr, size_t size)
 {
-	size = uaccess.copy_from_user(x, ptr, size);
-	return size ? -EFAULT : size;
+	size = __copy_from_user(x, ptr, size);
+	return size ? -EFAULT : 0;
 }
 
 /*
@@ -152,7 +177,7 @@ static inline int __get_user_fn(void *x, const void __user *ptr, size_t size)
 })
 
 
-extern int __put_user_bad(void) __attribute__((noreturn));
+int __put_user_bad(void) __attribute__((noreturn));
 
 #define __get_user(x, ptr)					\
 ({								\
@@ -200,34 +225,11 @@ extern int __put_user_bad(void) __attribute__((noreturn));
 	__get_user(x, ptr);					\
 })
 
-extern int __get_user_bad(void) __attribute__((noreturn));
+int __get_user_bad(void) __attribute__((noreturn));
 
 #define __put_user_unaligned __put_user
 #define __get_user_unaligned __get_user
 
-/**
- * __copy_to_user: - Copy a block of data into user space, with less checking.
- * @to:   Destination address, in user space.
- * @from: Source address, in kernel space.
- * @n:    Number of bytes to copy.
- *
- * Context: User context only.  This function may sleep.
- *
- * Copy data from kernel space to user space.  Caller must check
- * the specified block with access_ok() before calling this function.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- */
-static inline unsigned long __must_check
-__copy_to_user(void __user *to, const void *from, unsigned long n)
-{
-	return uaccess.copy_to_user(to, from, n);
-}
-
-#define __copy_to_user_inatomic __copy_to_user
-#define __copy_from_user_inatomic __copy_from_user
-
 /**
  * copy_to_user: - Copy a block of data into user space.
  * @to:   Destination address, in user space.
@@ -248,30 +250,7 @@ copy_to_user(void __user *to, const void *from, unsigned long n)
 	return __copy_to_user(to, from, n);
 }
 
-/**
- * __copy_from_user: - Copy a block of data from user space, with less checking.
- * @to:   Destination address, in kernel space.
- * @from: Source address, in user space.
- * @n:    Number of bytes to copy.
- *
- * Context: User context only.  This function may sleep.
- *
- * Copy data from user space to kernel space.  Caller must check
- * the specified block with access_ok() before calling this function.
- *
- * Returns number of bytes that could not be copied.
- * On success, this will be zero.
- *
- * If some data could not be copied, this function will pad the copied
- * data to the requested size using zero bytes.
- */
-static inline unsigned long __must_check
-__copy_from_user(void *to, const void __user *from, unsigned long n)
-{
-	return uaccess.copy_from_user(to, from, n);
-}
-
-extern void copy_from_user_overflow(void)
+void copy_from_user_overflow(void)
 #ifdef CONFIG_DEBUG_STRICT_USER_COPY_CHECKS
 __compiletime_warning("copy_from_user() buffer size is not provably correct")
 #endif
@@ -306,11 +285,8 @@ copy_from_user(void *to, const void __user *from, unsigned long n)
 	return __copy_from_user(to, from, n);
 }
 
-static inline unsigned long __must_check
-__copy_in_user(void __user *to, const void __user *from, unsigned long n)
-{
-	return uaccess.copy_in_user(to, from, n);
-}
+unsigned long __must_check
+__copy_in_user(void __user *to, const void __user *from, unsigned long n);
 
 static inline unsigned long __must_check
 copy_in_user(void __user *to, const void __user *from, unsigned long n)
@@ -322,18 +298,22 @@ copy_in_user(void __user *to, const void __user *from, unsigned long n)
 /*
  * Copy a null terminated string from userspace.
  */
+
+long __strncpy_from_user(char *dst, const char __user *src, long count);
+
 static inline long __must_check
 strncpy_from_user(char *dst, const char __user *src, long count)
 {
 	might_fault();
-	return uaccess.strncpy_from_user(dst, src, count);
+	return __strncpy_from_user(dst, src, count);
 }
 
-static inline unsigned long
-strnlen_user(const char __user * src, unsigned long n)
+size_t __must_check __strnlen_user(const char __user *src, size_t count);
+
+static inline size_t strnlen_user(const char __user *src, size_t n)
 {
 	might_fault();
-	return uaccess.strnlen_user(src, n);
+	return __strnlen_user(src, n);
 }
 
 /**
@@ -355,21 +335,15 @@ strnlen_user(const char __user * src, unsigned long n)
 /*
  * Zero Userspace
  */
+size_t __must_check __clear_user(void __user *to, size_t size);
 
-static inline unsigned long __must_check
-__clear_user(void __user *to, unsigned long n)
-{
-	return uaccess.clear_user(to, n);
-}
-
-static inline unsigned long __must_check
-clear_user(void __user *to, unsigned long n)
+static inline size_t __must_check clear_user(void __user *to, size_t n)
 {
 	might_fault();
-	return uaccess.clear_user(to, n);
+	return __clear_user(to, n);
 }
 
-extern int copy_to_user_real(void __user *dest, void *src, size_t count);
-extern int copy_from_user_real(void *dest, void __user *src, size_t count);
+int copy_to_user_real(void __user *dest, void *src, size_t count);
+int copy_from_user_real(void *dest, void __user *src, size_t count);
 
 #endif /* __S390_UACCESS_H */

commit cfa785e623577cdad2aa721acb23bd3a95eced9a
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Jan 22 14:49:30 2014 +0100

    s390/uaccess: normalize order of parameters of indirect uaccess function calls
    
    For some unknown reason the indirect uaccess functions on s390 implement a
    different parameter order than what is usual.
    
    e.g.:
    
    unsigned long copy_to_user(void *to, const void *from, unsigned long n);
    vs.
    size_t (*copy_to_user)(size_t n, void __user * to, const void *from);
    
    Let's get rid of this confusing parameter reordering.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 79330af9a5f8..73199636ba98 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -93,12 +93,12 @@ static inline unsigned long extable_fixup(const struct exception_table_entry *x)
 #define ARCH_HAS_SEARCH_EXTABLE
 
 struct uaccess_ops {
-	size_t (*copy_from_user)(size_t, const void __user *, void *);
-	size_t (*copy_to_user)(size_t, void __user *, const void *);
-	size_t (*copy_in_user)(size_t, void __user *, const void __user *);
-	size_t (*clear_user)(size_t, void __user *);
-	size_t (*strnlen_user)(size_t, const char __user *);
-	size_t (*strncpy_from_user)(size_t, const char __user *, char *);
+	size_t (*copy_from_user)(void *, const void __user *, size_t);
+	size_t (*copy_to_user)(void __user *, const void *, size_t);
+	size_t (*copy_in_user)(void __user *, const void __user *, size_t);
+	size_t (*clear_user)(void __user *, size_t);
+	size_t (*strnlen_user)(const char __user *, size_t);
+	size_t (*strncpy_from_user)(char *, const char __user *, size_t);
 	int (*futex_atomic_op)(int op, u32 __user *, int oparg, int *old);
 	int (*futex_atomic_cmpxchg)(u32 *, u32 __user *, u32 old, u32 new);
 };
@@ -109,15 +109,15 @@ extern struct uaccess_ops uaccess_pt;
 
 extern int __handle_fault(unsigned long, unsigned long, int);
 
-static inline int __put_user_fn(size_t size, void __user *ptr, void *x)
+static inline int __put_user_fn(void *x, void __user *ptr, size_t size)
 {
-	size = uaccess.copy_to_user(size, ptr, x);
+	size = uaccess.copy_to_user(ptr, x, size);
 	return size ? -EFAULT : size;
 }
 
-static inline int __get_user_fn(size_t size, const void __user *ptr, void *x)
+static inline int __get_user_fn(void *x, const void __user *ptr, size_t size)
 {
-	size = uaccess.copy_from_user(size, ptr, x);
+	size = uaccess.copy_from_user(x, ptr, size);
 	return size ? -EFAULT : size;
 }
 
@@ -135,8 +135,8 @@ static inline int __get_user_fn(size_t size, const void __user *ptr, void *x)
 	case 2:							\
 	case 4:							\
 	case 8:							\
-		__pu_err = __put_user_fn(sizeof (*(ptr)),	\
-					 ptr, &__x);		\
+		__pu_err = __put_user_fn(&__x, ptr,		\
+					 sizeof(*(ptr)));	\
 		break;						\
 	default:						\
 		__put_user_bad();				\
@@ -161,29 +161,29 @@ extern int __put_user_bad(void) __attribute__((noreturn));
 	switch (sizeof(*(ptr))) {				\
 	case 1: {						\
 		unsigned char __x;				\
-		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
-					 ptr, &__x);		\
+		__gu_err = __get_user_fn(&__x, ptr,		\
+					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
 	case 2: {						\
 		unsigned short __x;				\
-		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
-					 ptr, &__x);		\
+		__gu_err = __get_user_fn(&__x, ptr,		\
+					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
 	case 4: {						\
 		unsigned int __x;				\
-		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
-					 ptr, &__x);		\
+		__gu_err = __get_user_fn(&__x, ptr,		\
+					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
 	case 8: {						\
 		unsigned long long __x;				\
-		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
-					 ptr, &__x);		\
+		__gu_err = __get_user_fn(&__x, ptr,		\
+					 sizeof(*(ptr)));	\
 		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
 		break;						\
 	};							\
@@ -222,7 +222,7 @@ extern int __get_user_bad(void) __attribute__((noreturn));
 static inline unsigned long __must_check
 __copy_to_user(void __user *to, const void *from, unsigned long n)
 {
-	return uaccess.copy_to_user(n, to, from);
+	return uaccess.copy_to_user(to, from, n);
 }
 
 #define __copy_to_user_inatomic __copy_to_user
@@ -268,7 +268,7 @@ copy_to_user(void __user *to, const void *from, unsigned long n)
 static inline unsigned long __must_check
 __copy_from_user(void *to, const void __user *from, unsigned long n)
 {
-	return uaccess.copy_from_user(n, from, to);
+	return uaccess.copy_from_user(to, from, n);
 }
 
 extern void copy_from_user_overflow(void)
@@ -309,7 +309,7 @@ copy_from_user(void *to, const void __user *from, unsigned long n)
 static inline unsigned long __must_check
 __copy_in_user(void __user *to, const void __user *from, unsigned long n)
 {
-	return uaccess.copy_in_user(n, to, from);
+	return uaccess.copy_in_user(to, from, n);
 }
 
 static inline unsigned long __must_check
@@ -326,14 +326,14 @@ static inline long __must_check
 strncpy_from_user(char *dst, const char __user *src, long count)
 {
 	might_fault();
-	return uaccess.strncpy_from_user(count, src, dst);
+	return uaccess.strncpy_from_user(dst, src, count);
 }
 
 static inline unsigned long
 strnlen_user(const char __user * src, unsigned long n)
 {
 	might_fault();
-	return uaccess.strnlen_user(n, src);
+	return uaccess.strnlen_user(src, n);
 }
 
 /**
@@ -359,14 +359,14 @@ strnlen_user(const char __user * src, unsigned long n)
 static inline unsigned long __must_check
 __clear_user(void __user *to, unsigned long n)
 {
-	return uaccess.clear_user(n, to);
+	return uaccess.clear_user(to, n);
 }
 
 static inline unsigned long __must_check
 clear_user(void __user *to, unsigned long n)
 {
 	might_fault();
-	return uaccess.clear_user(n, to);
+	return uaccess.clear_user(to, n);
 }
 
 extern int copy_to_user_real(void __user *dest, void *src, size_t count);

commit e258d719ff28ecc7a048eb8f78380e68c4b3a3f0
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Sep 24 09:14:56 2013 +0200

    s390/uaccess: always run the kernel in home space
    
    Simplify the uaccess code by removing the user_mode=home option.
    The kernel will now always run in the home space mode.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 9c33ed4e666f..79330af9a5f8 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -94,9 +94,7 @@ static inline unsigned long extable_fixup(const struct exception_table_entry *x)
 
 struct uaccess_ops {
 	size_t (*copy_from_user)(size_t, const void __user *, void *);
-	size_t (*copy_from_user_small)(size_t, const void __user *, void *);
 	size_t (*copy_to_user)(size_t, void __user *, const void *);
-	size_t (*copy_to_user_small)(size_t, void __user *, const void *);
 	size_t (*copy_in_user)(size_t, void __user *, const void __user *);
 	size_t (*clear_user)(size_t, void __user *);
 	size_t (*strnlen_user)(size_t, const char __user *);
@@ -106,22 +104,20 @@ struct uaccess_ops {
 };
 
 extern struct uaccess_ops uaccess;
-extern struct uaccess_ops uaccess_std;
 extern struct uaccess_ops uaccess_mvcos;
-extern struct uaccess_ops uaccess_mvcos_switch;
 extern struct uaccess_ops uaccess_pt;
 
 extern int __handle_fault(unsigned long, unsigned long, int);
 
 static inline int __put_user_fn(size_t size, void __user *ptr, void *x)
 {
-	size = uaccess.copy_to_user_small(size, ptr, x);
+	size = uaccess.copy_to_user(size, ptr, x);
 	return size ? -EFAULT : size;
 }
 
 static inline int __get_user_fn(size_t size, const void __user *ptr, void *x)
 {
-	size = uaccess.copy_from_user_small(size, ptr, x);
+	size = uaccess.copy_from_user(size, ptr, x);
 	return size ? -EFAULT : size;
 }
 
@@ -226,10 +222,7 @@ extern int __get_user_bad(void) __attribute__((noreturn));
 static inline unsigned long __must_check
 __copy_to_user(void __user *to, const void *from, unsigned long n)
 {
-	if (__builtin_constant_p(n) && (n <= 256))
-		return uaccess.copy_to_user_small(n, to, from);
-	else
-		return uaccess.copy_to_user(n, to, from);
+	return uaccess.copy_to_user(n, to, from);
 }
 
 #define __copy_to_user_inatomic __copy_to_user
@@ -275,10 +268,7 @@ copy_to_user(void __user *to, const void *from, unsigned long n)
 static inline unsigned long __must_check
 __copy_from_user(void *to, const void __user *from, unsigned long n)
 {
-	if (__builtin_constant_p(n) && (n <= 256))
-		return uaccess.copy_from_user_small(n, from, to);
-	else
-		return uaccess.copy_from_user(n, from, to);
+	return uaccess.copy_from_user(n, from, to);
 }
 
 extern void copy_from_user_overflow(void)

commit d12a2970385cfe0c1b0439f53c3ed1055d4dee3e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Feb 21 16:57:42 2013 +0100

    s390/uaccess: remove pointless access_ok() checks
    
    access_ok() always returns 'true' on s390. Therefore all calls
    are quite pointless and can be removed.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 34268df959a3..9c33ed4e666f 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -252,9 +252,7 @@ static inline unsigned long __must_check
 copy_to_user(void __user *to, const void *from, unsigned long n)
 {
 	might_fault();
-	if (access_ok(VERIFY_WRITE, to, n))
-		n = __copy_to_user(to, from, n);
-	return n;
+	return __copy_to_user(to, from, n);
 }
 
 /**
@@ -315,11 +313,7 @@ copy_from_user(void *to, const void __user *from, unsigned long n)
 		copy_from_user_overflow();
 		return n;
 	}
-	if (access_ok(VERIFY_READ, from, n))
-		n = __copy_from_user(to, from, n);
-	else
-		memset(to, 0, n);
-	return n;
+	return __copy_from_user(to, from, n);
 }
 
 static inline unsigned long __must_check
@@ -332,9 +326,7 @@ static inline unsigned long __must_check
 copy_in_user(void __user *to, const void __user *from, unsigned long n)
 {
 	might_fault();
-	if (__access_ok(from,n) && __access_ok(to,n))
-		n = __copy_in_user(to, from, n);
-	return n;
+	return __copy_in_user(to, from, n);
 }
 
 /*
@@ -343,11 +335,8 @@ copy_in_user(void __user *to, const void __user *from, unsigned long n)
 static inline long __must_check
 strncpy_from_user(char *dst, const char __user *src, long count)
 {
-        long res = -EFAULT;
 	might_fault();
-        if (access_ok(VERIFY_READ, src, 1))
-		res = uaccess.strncpy_from_user(count, src, dst);
-        return res;
+	return uaccess.strncpy_from_user(count, src, dst);
 }
 
 static inline unsigned long
@@ -387,9 +376,7 @@ static inline unsigned long __must_check
 clear_user(void __user *to, unsigned long n)
 {
 	might_fault();
-	if (access_ok(VERIFY_WRITE, to, n))
-		n = uaccess.clear_user(n, to);
-	return n;
+	return uaccess.clear_user(n, to);
 }
 
 extern int copy_to_user_real(void __user *dest, void *src, size_t count);

commit eb608fb366de123a97227437e5306f731f4a63c5
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Wed Sep 5 13:26:11 2012 +0200

    s390/exceptions: switch to relative exception table entries
    
    This is the s390 port of 70627654 "x86, extable: Switch to relative
    exception table entries".
    Reduces the size of our exception tables by 50% on 64 bit builds.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index a8ab18b18b54..34268df959a3 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -76,9 +76,22 @@ static inline int __range_ok(unsigned long addr, unsigned long size)
 
 struct exception_table_entry
 {
-        unsigned long insn, fixup;
+	int insn, fixup;
 };
 
+static inline unsigned long extable_insn(const struct exception_table_entry *x)
+{
+	return (unsigned long)&x->insn + x->insn;
+}
+
+static inline unsigned long extable_fixup(const struct exception_table_entry *x)
+{
+	return (unsigned long)&x->fixup + x->fixup;
+}
+
+#define ARCH_HAS_SORT_EXTABLE
+#define ARCH_HAS_SEARCH_EXTABLE
+
 struct uaccess_ops {
 	size_t (*copy_from_user)(size_t, const void __user *, void *);
 	size_t (*copy_from_user_small)(size_t, const void __user *, void *);

commit a53c8fab3f87c995c30ac226a03af95361243144
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jul 20 11:15:04 2012 +0200

    s390/comments: unify copyright messages and remove file names
    
    Remove the file name from the comment at top of many files. In most
    cases the file name was wrong anyway, so it's rather pointless.
    
    Also unify the IBM copyright statement. We did have a lot of sightly
    different statements and wanted to change them one after another
    whenever a file gets touched. However that never happened. Instead
    people start to take the old/"wrong" statements to use as a template
    for new files.
    So unify all of them in one go.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 7e7285179aad..a8ab18b18b54 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -1,8 +1,6 @@
 /*
- *  include/asm-s390/uaccess.h
- *
  *  S390 version
- *    Copyright (C) 1999,2000 IBM Deutschland Entwicklung GmbH, IBM Corporation
+ *    Copyright IBM Corp. 1999, 2000
  *    Author(s): Hartmut Penner (hp@de.ibm.com),
  *               Martin Schwidefsky (schwidefsky@de.ibm.com)
  *

commit fbe765680d1fe9d08187ea4dad5041a7955a2c3a
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Jun 5 09:59:52 2012 +0200

    s390/smp: make absolute lowcore / cpu restart parameter accesses more robust
    
    Setting the cpu restart parameters is done in three different fashions:
    - directly setting the four parameters individually
    - copying the four parameters with memcpy (using 4 * sizeof(long))
    - copying the four parameters using a private structure
    
    In addition code in entry*.S relies on a certain order of the restart
    members of struct _lowcore.
    
    Make all of this more robust to future changes by adding a
    mem_absolute_assign(dest, val) define, which assigns val to dest
    using absolute addressing mode. Also the load multiple instructions
    in entry*.S have been split into separate load instruction so the
    order of the struct _lowcore members doesn't matter anymore.
    
    In addition move the prototypes of memcpy_real/absolute from uaccess.h
    to processor.h. These memcpy* variants are not related to uaccess at all.
    string.h doesn't seem to match as well, so lets use processor.h.
    
    Also replace the eight byte array in struct _lowcore which represents a
    misaliged u64 with a u64. The compiler will always create code that
    handles the misaligned u64 correctly.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 1f3a79bcd262..7e7285179aad 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -381,8 +381,6 @@ clear_user(void __user *to, unsigned long n)
 	return n;
 }
 
-extern int memcpy_real(void *, void *, size_t);
-extern void memcpy_absolute(void *, void *, size_t);
 extern int copy_to_user_real(void __user *dest, void *src, size_t count);
 extern int copy_from_user_real(void *dest, void __user *src, size_t count);
 

commit 491af9903b858ee7c36735dc31708fe4074ce56f
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue May 29 07:33:59 2012 +0200

    s390/uaccess: fix access_ok compile warnings
    
    On s390 access_ok is a macro which discards all parameters and always
    returns 1. This can result in compile warnings which warn about unused
    variables like this:
    
    fs/read_write.c: In function 'rw_copy_check_uvector':
    fs/read_write.c:684:16: warning: unused variable 'buf' [-Wunused-variable]
    
    Fix this by adding a __range_ok() function which consumes all parameters
    but still always returns 1.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 8e83da66df0d..1f3a79bcd262 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -50,10 +50,15 @@
 
 #define segment_eq(a,b) ((a).ar4 == (b).ar4)
 
-#define __access_ok(addr, size)	\
-({				\
-	__chk_user_ptr(addr);	\
-	1;			\
+static inline int __range_ok(unsigned long addr, unsigned long size)
+{
+	return 1;
+}
+
+#define __access_ok(addr, size)				\
+({							\
+	__chk_user_ptr(addr);				\
+	__range_ok((unsigned long)(addr), (size));	\
 })
 
 #define access_ok(type, addr, size) __access_ok(addr, size)

commit 73bf463efaba6a1efe69349c6d7275d03468adf4
Author: Michael Holzheu <holzheu@linux.vnet.ibm.com>
Date:   Thu May 24 14:35:16 2012 +0200

    s390/kernel: Introduce memcpy_absolute() function
    
    This patch introduces the new function memcpy_absolute() that allows to
    copy memory using absolute addressing. This means that the prefix swap
    does not apply when this function is used.
    
    With this patch also all s390 kernel code that accesses absolute zero
    now uses the new memcpy_absolute() function. The old and less generic
    copy_to_absolute_zero() function is removed.
    
    Signed-off-by: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 8f2cada4f7c9..8e83da66df0d 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -377,7 +377,7 @@ clear_user(void __user *to, unsigned long n)
 }
 
 extern int memcpy_real(void *, void *, size_t);
-extern void copy_to_absolute_zero(void *dest, void *src, size_t count);
+extern void memcpy_absolute(void *, void *, size_t);
 extern int copy_to_user_real(void __user *dest, void *src, size_t count);
 extern int copy_from_user_real(void *dest, void __user *src, size_t count);
 

commit a0616cdebcfd575dcd4c46102d1b52fbb827fc29
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for S390
    
    Disintegrate asm/system.h for S390.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: linux-s390@vger.kernel.org

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 2b23885e81e9..8f2cada4f7c9 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -16,6 +16,7 @@
  */
 #include <linux/sched.h>
 #include <linux/errno.h>
+#include <asm/ctl_reg.h>
 
 #define VERIFY_READ     0
 #define VERIFY_WRITE    1
@@ -375,4 +376,9 @@ clear_user(void __user *to, unsigned long n)
 	return n;
 }
 
+extern int memcpy_real(void *, void *, size_t);
+extern void copy_to_absolute_zero(void *dest, void *src, size_t count);
+extern int copy_to_user_real(void __user *dest, void *src, size_t count);
+extern int copy_from_user_real(void *dest, void __user *src, size_t count);
+
 #endif /* __S390_UACCESS_H */

commit 7683f7444875c822f48f03a9f9c8b1b1e98b2ef0
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu May 26 09:48:25 2011 +0200

    [S390] uaccess: turn __access_ok() into a define
    
    Turn __access_ok() into a define and add a __chk_user_ptr() call
    instead.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 2d9ea11f919a..2b23885e81e9 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -49,12 +49,13 @@
 
 #define segment_eq(a,b) ((a).ar4 == (b).ar4)
 
+#define __access_ok(addr, size)	\
+({				\
+	__chk_user_ptr(addr);	\
+	1;			\
+})
 
-static inline int __access_ok(const void __user *addr, unsigned long size)
-{
-	return 1;
-}
-#define access_ok(type,addr,size) __access_ok(addr,size)
+#define access_ok(type, addr, size) __access_ok(addr, size)
 
 /*
  * The exception table consists of pairs of addresses: the first is the

commit 8d7718aa082aaf30a0b4989e1f04858952f941bc
Author: Michel Lespinasse <walken@google.com>
Date:   Thu Mar 10 18:50:58 2011 -0800

    futex: Sanitize futex ops argument types
    
    Change futex_atomic_op_inuser and futex_atomic_cmpxchg_inatomic
    prototypes to use u32 types for the futex as this is the data type the
    futex core code uses all over the place.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Cc: Darren Hart <darren@dvhart.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <20110311025058.GD26122@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 549adf6a9b8b..2d9ea11f919a 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -83,8 +83,8 @@ struct uaccess_ops {
 	size_t (*clear_user)(size_t, void __user *);
 	size_t (*strnlen_user)(size_t, const char __user *);
 	size_t (*strncpy_from_user)(size_t, const char __user *, char *);
-	int (*futex_atomic_op)(int op, int __user *, int oparg, int *old);
-	int (*futex_atomic_cmpxchg)(int *, int __user *, int old, int new);
+	int (*futex_atomic_op)(int op, u32 __user *, int oparg, int *old);
+	int (*futex_atomic_cmpxchg)(u32 *, u32 __user *, u32 old, u32 new);
 };
 
 extern struct uaccess_ops uaccess;

commit 37a9d912b24f96a0591773e6e6c3642991ae5a70
Author: Michel Lespinasse <walken@google.com>
Date:   Thu Mar 10 18:48:51 2011 -0800

    futex: Sanitize cmpxchg_futex_value_locked API
    
    The cmpxchg_futex_value_locked API was funny in that it returned either
    the original, user-exposed futex value OR an error code such as -EFAULT.
    This was confusing at best, and could be a source of livelocks in places
    that retry the cmpxchg_futex_value_locked after trying to fix the issue
    by running fault_in_user_writeable().
    
    This change makes the cmpxchg_futex_value_locked API more similar to the
    get_futex_value_locked one, returning an error code and updating the
    original value through a reference argument.
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>  [tile]
    Acked-by: Tony Luck <tony.luck@intel.com>  [ia64]
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Michal Simek <monstr@monstr.eu>  [microblaze]
    Acked-by: David Howells <dhowells@redhat.com> [frv]
    Cc: Darren Hart <darren@dvhart.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    LKML-Reference: <20110311024851.GC26122@google.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index d6b1ed0ec52b..549adf6a9b8b 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -84,7 +84,7 @@ struct uaccess_ops {
 	size_t (*strnlen_user)(size_t, const char __user *);
 	size_t (*strncpy_from_user)(size_t, const char __user *, char *);
 	int (*futex_atomic_op)(int op, int __user *, int oparg, int *old);
-	int (*futex_atomic_cmpxchg)(int __user *, int old, int new);
+	int (*futex_atomic_cmpxchg)(int *, int __user *, int old, int new);
 };
 
 extern struct uaccess_ops uaccess;

commit 1dcec254afe5bc700a4cacf810b71a28bd994ea9
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Feb 26 22:37:22 2010 +0100

    [S390] uaccess: implement strict user copy checks
    
    Same as on x86 and sparc, besides the fact that enabling the option
    will just emit compile time warnings instead of errors.
    Keeps allyesconfig kernels compiling.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index cbf0a8745bf4..d6b1ed0ec52b 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -265,6 +265,12 @@ __copy_from_user(void *to, const void __user *from, unsigned long n)
 		return uaccess.copy_from_user(n, from, to);
 }
 
+extern void copy_from_user_overflow(void)
+#ifdef CONFIG_DEBUG_STRICT_USER_COPY_CHECKS
+__compiletime_warning("copy_from_user() buffer size is not provably correct")
+#endif
+;
+
 /**
  * copy_from_user: - Copy a block of data from user space.
  * @to:   Destination address, in kernel space.
@@ -284,7 +290,13 @@ __copy_from_user(void *to, const void __user *from, unsigned long n)
 static inline unsigned long __must_check
 copy_from_user(void *to, const void __user *from, unsigned long n)
 {
+	unsigned int sz = __compiletime_object_size(to);
+
 	might_fault();
+	if (unlikely(sz != -1 && sz < n)) {
+		copy_from_user_overflow();
+		return n;
+	}
 	if (access_ok(VERIFY_READ, from, n))
 		n = __copy_from_user(to, from, n);
 	else

commit 6c1e3e79430615d0472dbf9f8fed89c571e66423
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Mon Dec 7 12:51:47 2009 +0100

    [S390] Use do_exception() in pagetable walk usercopy functions.
    
    The pagetable walk usercopy functions have used a modified copy of the
    do_exception() function for fault handling. This lead to inconsistencies
    with recent changes to do_exception(), e.g. performance counters. This
    patch changes the pagetable walk usercopy code to call do_exception()
    directly, eliminating the redundancy. A new parameter is added to
    do_exception() to specify the fault address.
    
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 8377e91533d2..cbf0a8745bf4 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -93,6 +93,8 @@ extern struct uaccess_ops uaccess_mvcos;
 extern struct uaccess_ops uaccess_mvcos_switch;
 extern struct uaccess_ops uaccess_pt;
 
+extern int __handle_fault(unsigned long, unsigned long, int);
+
 static inline int __put_user_fn(size_t size, void __user *ptr, void *x)
 {
 	size = uaccess.copy_to_user_small(size, ptr, x);

commit dab4079d5b5ac421208499d5e554a07f9beb16e4
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jun 12 10:26:32 2009 +0200

    [S390] uaccess: use might_fault() instead of might_sleep()
    
    Adds more checking in case lockdep is turned on.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
index 0235970278f0..8377e91533d2 100644
--- a/arch/s390/include/asm/uaccess.h
+++ b/arch/s390/include/asm/uaccess.h
@@ -131,7 +131,7 @@ static inline int __get_user_fn(size_t size, const void __user *ptr, void *x)
 
 #define put_user(x, ptr)					\
 ({								\
-	might_sleep();						\
+	might_fault();						\
 	__put_user(x, ptr);					\
 })
 
@@ -180,7 +180,7 @@ extern int __put_user_bad(void) __attribute__((noreturn));
 
 #define get_user(x, ptr)					\
 ({								\
-	might_sleep();						\
+	might_fault();						\
 	__get_user(x, ptr);					\
 })
 
@@ -231,7 +231,7 @@ __copy_to_user(void __user *to, const void *from, unsigned long n)
 static inline unsigned long __must_check
 copy_to_user(void __user *to, const void *from, unsigned long n)
 {
-	might_sleep();
+	might_fault();
 	if (access_ok(VERIFY_WRITE, to, n))
 		n = __copy_to_user(to, from, n);
 	return n;
@@ -282,7 +282,7 @@ __copy_from_user(void *to, const void __user *from, unsigned long n)
 static inline unsigned long __must_check
 copy_from_user(void *to, const void __user *from, unsigned long n)
 {
-	might_sleep();
+	might_fault();
 	if (access_ok(VERIFY_READ, from, n))
 		n = __copy_from_user(to, from, n);
 	else
@@ -299,7 +299,7 @@ __copy_in_user(void __user *to, const void __user *from, unsigned long n)
 static inline unsigned long __must_check
 copy_in_user(void __user *to, const void __user *from, unsigned long n)
 {
-	might_sleep();
+	might_fault();
 	if (__access_ok(from,n) && __access_ok(to,n))
 		n = __copy_in_user(to, from, n);
 	return n;
@@ -312,7 +312,7 @@ static inline long __must_check
 strncpy_from_user(char *dst, const char __user *src, long count)
 {
         long res = -EFAULT;
-        might_sleep();
+	might_fault();
         if (access_ok(VERIFY_READ, src, 1))
 		res = uaccess.strncpy_from_user(count, src, dst);
         return res;
@@ -321,7 +321,7 @@ strncpy_from_user(char *dst, const char __user *src, long count)
 static inline unsigned long
 strnlen_user(const char __user * src, unsigned long n)
 {
-	might_sleep();
+	might_fault();
 	return uaccess.strnlen_user(n, src);
 }
 
@@ -354,7 +354,7 @@ __clear_user(void __user *to, unsigned long n)
 static inline unsigned long __must_check
 clear_user(void __user *to, unsigned long n)
 {
-	might_sleep();
+	might_fault();
 	if (access_ok(VERIFY_WRITE, to, n))
 		n = uaccess.clear_user(n, to);
 	return n;

commit c6557e7f2b6ae76a44653d38f835174074c42e05
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 1 20:42:05 2008 +0200

    [S390] move include/asm-s390 to arch/s390/include/asm
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/uaccess.h b/arch/s390/include/asm/uaccess.h
new file mode 100644
index 000000000000..0235970278f0
--- /dev/null
+++ b/arch/s390/include/asm/uaccess.h
@@ -0,0 +1,363 @@
+/*
+ *  include/asm-s390/uaccess.h
+ *
+ *  S390 version
+ *    Copyright (C) 1999,2000 IBM Deutschland Entwicklung GmbH, IBM Corporation
+ *    Author(s): Hartmut Penner (hp@de.ibm.com),
+ *               Martin Schwidefsky (schwidefsky@de.ibm.com)
+ *
+ *  Derived from "include/asm-i386/uaccess.h"
+ */
+#ifndef __S390_UACCESS_H
+#define __S390_UACCESS_H
+
+/*
+ * User space memory access functions
+ */
+#include <linux/sched.h>
+#include <linux/errno.h>
+
+#define VERIFY_READ     0
+#define VERIFY_WRITE    1
+
+
+/*
+ * The fs value determines whether argument validity checking should be
+ * performed or not.  If get_fs() == USER_DS, checking is performed, with
+ * get_fs() == KERNEL_DS, checking is bypassed.
+ *
+ * For historical reasons, these macros are grossly misnamed.
+ */
+
+#define MAKE_MM_SEG(a)  ((mm_segment_t) { (a) })
+
+
+#define KERNEL_DS       MAKE_MM_SEG(0)
+#define USER_DS         MAKE_MM_SEG(1)
+
+#define get_ds()        (KERNEL_DS)
+#define get_fs()        (current->thread.mm_segment)
+
+#define set_fs(x) \
+({									\
+	unsigned long __pto;						\
+	current->thread.mm_segment = (x);				\
+	__pto = current->thread.mm_segment.ar4 ?			\
+		S390_lowcore.user_asce : S390_lowcore.kernel_asce;	\
+	__ctl_load(__pto, 7, 7);					\
+})
+
+#define segment_eq(a,b) ((a).ar4 == (b).ar4)
+
+
+static inline int __access_ok(const void __user *addr, unsigned long size)
+{
+	return 1;
+}
+#define access_ok(type,addr,size) __access_ok(addr,size)
+
+/*
+ * The exception table consists of pairs of addresses: the first is the
+ * address of an instruction that is allowed to fault, and the second is
+ * the address at which the program should continue.  No registers are
+ * modified, so it is entirely up to the continuation code to figure out
+ * what to do.
+ *
+ * All the routines below use bits of fixup code that are out of line
+ * with the main instruction path.  This means when everything is well,
+ * we don't even have to jump over them.  Further, they do not intrude
+ * on our cache or tlb entries.
+ */
+
+struct exception_table_entry
+{
+        unsigned long insn, fixup;
+};
+
+struct uaccess_ops {
+	size_t (*copy_from_user)(size_t, const void __user *, void *);
+	size_t (*copy_from_user_small)(size_t, const void __user *, void *);
+	size_t (*copy_to_user)(size_t, void __user *, const void *);
+	size_t (*copy_to_user_small)(size_t, void __user *, const void *);
+	size_t (*copy_in_user)(size_t, void __user *, const void __user *);
+	size_t (*clear_user)(size_t, void __user *);
+	size_t (*strnlen_user)(size_t, const char __user *);
+	size_t (*strncpy_from_user)(size_t, const char __user *, char *);
+	int (*futex_atomic_op)(int op, int __user *, int oparg, int *old);
+	int (*futex_atomic_cmpxchg)(int __user *, int old, int new);
+};
+
+extern struct uaccess_ops uaccess;
+extern struct uaccess_ops uaccess_std;
+extern struct uaccess_ops uaccess_mvcos;
+extern struct uaccess_ops uaccess_mvcos_switch;
+extern struct uaccess_ops uaccess_pt;
+
+static inline int __put_user_fn(size_t size, void __user *ptr, void *x)
+{
+	size = uaccess.copy_to_user_small(size, ptr, x);
+	return size ? -EFAULT : size;
+}
+
+static inline int __get_user_fn(size_t size, const void __user *ptr, void *x)
+{
+	size = uaccess.copy_from_user_small(size, ptr, x);
+	return size ? -EFAULT : size;
+}
+
+/*
+ * These are the main single-value transfer routines.  They automatically
+ * use the right size if we just have the right pointer type.
+ */
+#define __put_user(x, ptr) \
+({								\
+	__typeof__(*(ptr)) __x = (x);				\
+	int __pu_err = -EFAULT;					\
+        __chk_user_ptr(ptr);                                    \
+	switch (sizeof (*(ptr))) {				\
+	case 1:							\
+	case 2:							\
+	case 4:							\
+	case 8:							\
+		__pu_err = __put_user_fn(sizeof (*(ptr)),	\
+					 ptr, &__x);		\
+		break;						\
+	default:						\
+		__put_user_bad();				\
+		break;						\
+	 }							\
+	__pu_err;						\
+})
+
+#define put_user(x, ptr)					\
+({								\
+	might_sleep();						\
+	__put_user(x, ptr);					\
+})
+
+
+extern int __put_user_bad(void) __attribute__((noreturn));
+
+#define __get_user(x, ptr)					\
+({								\
+	int __gu_err = -EFAULT;					\
+	__chk_user_ptr(ptr);					\
+	switch (sizeof(*(ptr))) {				\
+	case 1: {						\
+		unsigned char __x;				\
+		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
+					 ptr, &__x);		\
+		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
+		break;						\
+	};							\
+	case 2: {						\
+		unsigned short __x;				\
+		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
+					 ptr, &__x);		\
+		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
+		break;						\
+	};							\
+	case 4: {						\
+		unsigned int __x;				\
+		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
+					 ptr, &__x);		\
+		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
+		break;						\
+	};							\
+	case 8: {						\
+		unsigned long long __x;				\
+		__gu_err = __get_user_fn(sizeof (*(ptr)),	\
+					 ptr, &__x);		\
+		(x) = *(__force __typeof__(*(ptr)) *) &__x;	\
+		break;						\
+	};							\
+	default:						\
+		__get_user_bad();				\
+		break;						\
+	}							\
+	__gu_err;						\
+})
+
+#define get_user(x, ptr)					\
+({								\
+	might_sleep();						\
+	__get_user(x, ptr);					\
+})
+
+extern int __get_user_bad(void) __attribute__((noreturn));
+
+#define __put_user_unaligned __put_user
+#define __get_user_unaligned __get_user
+
+/**
+ * __copy_to_user: - Copy a block of data into user space, with less checking.
+ * @to:   Destination address, in user space.
+ * @from: Source address, in kernel space.
+ * @n:    Number of bytes to copy.
+ *
+ * Context: User context only.  This function may sleep.
+ *
+ * Copy data from kernel space to user space.  Caller must check
+ * the specified block with access_ok() before calling this function.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ */
+static inline unsigned long __must_check
+__copy_to_user(void __user *to, const void *from, unsigned long n)
+{
+	if (__builtin_constant_p(n) && (n <= 256))
+		return uaccess.copy_to_user_small(n, to, from);
+	else
+		return uaccess.copy_to_user(n, to, from);
+}
+
+#define __copy_to_user_inatomic __copy_to_user
+#define __copy_from_user_inatomic __copy_from_user
+
+/**
+ * copy_to_user: - Copy a block of data into user space.
+ * @to:   Destination address, in user space.
+ * @from: Source address, in kernel space.
+ * @n:    Number of bytes to copy.
+ *
+ * Context: User context only.  This function may sleep.
+ *
+ * Copy data from kernel space to user space.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ */
+static inline unsigned long __must_check
+copy_to_user(void __user *to, const void *from, unsigned long n)
+{
+	might_sleep();
+	if (access_ok(VERIFY_WRITE, to, n))
+		n = __copy_to_user(to, from, n);
+	return n;
+}
+
+/**
+ * __copy_from_user: - Copy a block of data from user space, with less checking.
+ * @to:   Destination address, in kernel space.
+ * @from: Source address, in user space.
+ * @n:    Number of bytes to copy.
+ *
+ * Context: User context only.  This function may sleep.
+ *
+ * Copy data from user space to kernel space.  Caller must check
+ * the specified block with access_ok() before calling this function.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ *
+ * If some data could not be copied, this function will pad the copied
+ * data to the requested size using zero bytes.
+ */
+static inline unsigned long __must_check
+__copy_from_user(void *to, const void __user *from, unsigned long n)
+{
+	if (__builtin_constant_p(n) && (n <= 256))
+		return uaccess.copy_from_user_small(n, from, to);
+	else
+		return uaccess.copy_from_user(n, from, to);
+}
+
+/**
+ * copy_from_user: - Copy a block of data from user space.
+ * @to:   Destination address, in kernel space.
+ * @from: Source address, in user space.
+ * @n:    Number of bytes to copy.
+ *
+ * Context: User context only.  This function may sleep.
+ *
+ * Copy data from user space to kernel space.
+ *
+ * Returns number of bytes that could not be copied.
+ * On success, this will be zero.
+ *
+ * If some data could not be copied, this function will pad the copied
+ * data to the requested size using zero bytes.
+ */
+static inline unsigned long __must_check
+copy_from_user(void *to, const void __user *from, unsigned long n)
+{
+	might_sleep();
+	if (access_ok(VERIFY_READ, from, n))
+		n = __copy_from_user(to, from, n);
+	else
+		memset(to, 0, n);
+	return n;
+}
+
+static inline unsigned long __must_check
+__copy_in_user(void __user *to, const void __user *from, unsigned long n)
+{
+	return uaccess.copy_in_user(n, to, from);
+}
+
+static inline unsigned long __must_check
+copy_in_user(void __user *to, const void __user *from, unsigned long n)
+{
+	might_sleep();
+	if (__access_ok(from,n) && __access_ok(to,n))
+		n = __copy_in_user(to, from, n);
+	return n;
+}
+
+/*
+ * Copy a null terminated string from userspace.
+ */
+static inline long __must_check
+strncpy_from_user(char *dst, const char __user *src, long count)
+{
+        long res = -EFAULT;
+        might_sleep();
+        if (access_ok(VERIFY_READ, src, 1))
+		res = uaccess.strncpy_from_user(count, src, dst);
+        return res;
+}
+
+static inline unsigned long
+strnlen_user(const char __user * src, unsigned long n)
+{
+	might_sleep();
+	return uaccess.strnlen_user(n, src);
+}
+
+/**
+ * strlen_user: - Get the size of a string in user space.
+ * @str: The string to measure.
+ *
+ * Context: User context only.  This function may sleep.
+ *
+ * Get the size of a NUL-terminated string in user space.
+ *
+ * Returns the size of the string INCLUDING the terminating NUL.
+ * On exception, returns 0.
+ *
+ * If there is a limit on the length of a valid string, you may wish to
+ * consider using strnlen_user() instead.
+ */
+#define strlen_user(str) strnlen_user(str, ~0UL)
+
+/*
+ * Zero Userspace
+ */
+
+static inline unsigned long __must_check
+__clear_user(void __user *to, unsigned long n)
+{
+	return uaccess.clear_user(n, to);
+}
+
+static inline unsigned long __must_check
+clear_user(void __user *to, unsigned long n)
+{
+	might_sleep();
+	if (access_ok(VERIFY_WRITE, to, n))
+		n = uaccess.clear_user(n, to);
+	return n;
+}
+
+#endif /* __S390_UACCESS_H */
