commit c62da0c35d58518ddb26ff641d2485596567fd96
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Fri Apr 10 14:33:05 2020 -0700

    mm/vma: define a default value for VM_DATA_DEFAULT_FLAGS
    
    There are many platforms with exact same value for VM_DATA_DEFAULT_FLAGS
    This creates a default value for VM_DATA_DEFAULT_FLAGS in line with the
    existing VM_STACK_DEFAULT_FLAGS.  While here, also define some more
    macros with standard VMA access flag combinations that are used
    frequently across many platforms.  Apart from simplification, this
    reduces code duplication as well.
    
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Chris Zankel <chris@zankel.net>
    Link: http://lkml.kernel.org/r/1583391014-8170-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index f2d4c1bd3429..cc98f9b78fd4 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -181,8 +181,7 @@ int arch_make_page_accessible(struct page *page);
 
 #define virt_addr_valid(kaddr)	pfn_valid(virt_to_pfn(kaddr))
 
-#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
-				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+#define VM_DATA_DEFAULT_FLAGS	VM_DATA_FLAGS_NON_EXEC
 
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>

commit ad0bf4eb91c2f9b93479b679e5472094ddb76da8
Merge: 5364abc57993 1058c163dc31
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Apr 4 09:45:50 2020 -0700

    Merge tag 's390-5.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 updates from Vasily Gorbik:
    
     - Update maintainers. Niklas Schnelle takes over zpci and Vineeth
       Vijayan common io code.
    
     - Extend cpuinfo to include topology information.
    
     - Add new extended counters for IBM z15 and sampling buffer allocation
       rework in perf code.
    
     - Add control over zeroing out memory during system restart.
    
     - CCA protected key block version 2 support and other
       fixes/improvements in crypto code.
    
     - Convert to new fallthrough; annotations.
    
     - Replace zero-length arrays with flexible-arrays.
    
     - QDIO debugfs and other small improvements.
    
     - Drop 2-level paging support optimization for compat tasks. Varios mm
       cleanups.
    
     - Remove broken and unused hibernate / power management support.
    
     - Remove fake numa support which does not bring any benefits.
    
     - Exclude offline CPUs from CPU topology masks to be more consistent
       with other architectures.
    
     - Prevent last branching instruction address leaking to userspace.
    
     - Other small various fixes and improvements all over the code.
    
    * tag 's390-5.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (57 commits)
      s390/mm: cleanup init_new_context() callback
      s390/mm: cleanup virtual memory constants usage
      s390/mm: remove page table downgrade support
      s390/qdio: set qdio_irq->cdev at allocation time
      s390/qdio: remove unused function declarations
      s390/ccwgroup: remove pm support
      s390/ap: remove power management code from ap bus and drivers
      s390/zcrypt: use kvmalloc instead of kmalloc for 256k alloc
      s390/mm: cleanup arch_get_unmapped_area() and friends
      s390/ism: remove pm support
      s390/cio: use fallthrough;
      s390/vfio: use fallthrough;
      s390/zcrypt: use fallthrough;
      s390: use fallthrough;
      s390/cpum_sf: Fix wrong page count in error message
      s390/diag: fix display of diagnose call statistics
      s390/ap: Remove ap device suspend and resume callbacks
      s390/pci: Improve handling of unset UID
      s390/pci: Fix zpci_alloc_domain() over allocation
      s390/qdio: pass ISC as parameter to chsc_sadc()
      ...

commit cf39d37539068d53e015d8b4f1dcf42c65306b0d
Merge: 830948eb6826 463050599742
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Mar 31 10:44:53 2020 -0400

    Merge tag 'kvmarm-5.7' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for Linux 5.7
    
    - GICv4.1 support
    - 32bit host removal

commit 014b020475d4b9670d3cff11b751a7c20208f78b
Author: Alexander Gordeev <agordeev@linux.ibm.com>
Date:   Tue Feb 25 12:44:06 2020 +0100

    s390/mm: cleanup phys_to_pfn() and friends
    
    Make page, frame, virtual and physical address conversion macros
    more expressive by avoiding redundant definitions and defining
    new macros using existing ones.
    
    Signed-off-by: Alexander Gordeev <agordeev@linux.ibm.com>
    Reviewed-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 85e944f04c70..2e53b27f4f1a 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -161,20 +161,20 @@ static inline int devmem_is_allowed(unsigned long pfn)
 #define __pa(x)			((unsigned long)(x))
 #define __va(x)			((void *)(unsigned long)(x))
 
-#define virt_to_pfn(kaddr)	(__pa(kaddr) >> PAGE_SHIFT)
-#define pfn_to_virt(pfn)	__va((pfn) << PAGE_SHIFT)
+#define phys_to_pfn(phys)	((phys) >> PAGE_SHIFT)
+#define pfn_to_phys(pfn)	((pfn) << PAGE_SHIFT)
+
+#define phys_to_page(phys)	pfn_to_page(phys_to_pfn(phys))
+#define page_to_phys(page)	pfn_to_phys(page_to_pfn(page))
+
+#define pfn_to_virt(pfn)	__va(pfn_to_phys(pfn))
+#define virt_to_pfn(kaddr)	(phys_to_pfn(__pa(kaddr)))
 #define pfn_to_kaddr(pfn)	pfn_to_virt(pfn)
 
 #define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define page_to_virt(page)	pfn_to_virt(page_to_pfn(page))
 
-#define phys_to_pfn(kaddr)	((kaddr) >> PAGE_SHIFT)
-#define pfn_to_phys(pfn)	((pfn) << PAGE_SHIFT)
-
-#define phys_to_page(kaddr)	pfn_to_page(phys_to_pfn(kaddr))
-#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
-
-#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	pfn_valid(virt_to_pfn(kaddr))
 
 #define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)

commit 214d9bbcd3a67230b932f6cea83c078ab34d9e70
Author: Claudio Imbrenda <imbrenda@linux.ibm.com>
Date:   Tue Jan 21 09:48:44 2020 +0100

    s390/mm: provide memory management functions for protected KVM guests
    
    This provides the basic ultravisor calls and page table handling to cope
    with secure guests:
    - provide arch_make_page_accessible
    - make pages accessible after unmapping of secure guests
    - provide the ultravisor commands convert to/from secure
    - provide the ultravisor commands pin/unpin shared
    - provide callbacks to make pages secure (inacccessible)
     - we check for the expected pin count to only make pages secure if the
       host is not accessing them
     - we fence hugetlbfs for secure pages
    - add missing radix-tree include into gmap.h
    
    The basic idea is that a page can have 3 states: secure, normal or
    shared. The hypervisor can call into a firmware function called
    ultravisor that allows to change the state of a page: convert from/to
    secure. The convert from secure will encrypt the page and make it
    available to the host and host I/O. The convert to secure will remove
    the host capability to access this page.
    The design is that on convert to secure we will wait until writeback and
    page refs are indicating no host usage. At the same time the convert
    from secure (export to host) will be called in common code when the
    refcount or the writeback bit is already set. This avoids races between
    convert from and to secure.
    
    Then there is also the concept of shared pages. Those are kind of secure
    where the host can still access those pages. We need to be notified when
    the guest "unshares" such a page, basically doing a convert to secure by
    then. There is a call "pin shared page" that we use instead of convert
    from secure when possible.
    
    We do use PG_arch_1 as an optimization to minimize the convert from
    secure/pin shared.
    
    Several comments have been added in the code to explain the logic in
    the relevant places.
    
    Co-developed-by: Ulrich Weigand <Ulrich.Weigand@de.ibm.com>
    Signed-off-by: Ulrich Weigand <Ulrich.Weigand@de.ibm.com>
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
    Acked-by: David Hildenbrand <david@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 85e944f04c70..4ebcf891ff3c 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -153,6 +153,11 @@ static inline int devmem_is_allowed(unsigned long pfn)
 #define HAVE_ARCH_FREE_PAGE
 #define HAVE_ARCH_ALLOC_PAGE
 
+#if IS_ENABLED(CONFIG_PGSTE)
+int arch_make_page_accessible(struct page *page);
+#define HAVE_ARCH_MAKE_PAGE_ACCESSIBLE
+#endif
+
 #endif /* !__ASSEMBLY__ */
 
 #define __PAGE_OFFSET		0x0UL

commit 380324734956c64cd060e1db4304f3117ac15809
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Thu Feb 13 23:42:07 2020 -0700

    s390/mm: Explicitly compare PAGE_DEFAULT_KEY against zero in storage_key_init_range
    
    Clang warns:
    
     In file included from ../arch/s390/purgatory/purgatory.c:10:
     In file included from ../include/linux/kexec.h:18:
     In file included from ../include/linux/crash_core.h:6:
     In file included from ../include/linux/elfcore.h:5:
     In file included from ../include/linux/user.h:1:
     In file included from ../arch/s390/include/asm/user.h:11:
     ../arch/s390/include/asm/page.h:45:6: warning: converting the result of
     '<<' to a boolean always evaluates to false
     [-Wtautological-constant-compare]
             if (PAGE_DEFAULT_KEY)
                ^
     ../arch/s390/include/asm/page.h:23:44: note: expanded from macro
     'PAGE_DEFAULT_KEY'
     #define PAGE_DEFAULT_KEY        (PAGE_DEFAULT_ACC << 4)
                                                      ^
     1 warning generated.
    
    Explicitly compare this against zero to silence the warning as it is
    intended to be used in a boolean context.
    
    Fixes: de3fa841e429 ("s390/mm: fix compile for PAGE_DEFAULT_KEY != 0")
    Link: https://github.com/ClangBuiltLinux/linux/issues/860
    Link: https://lkml.kernel.org/r/20200214064207.10381-1-natechancellor@gmail.com
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 85e944f04c70..1019efd85b9d 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -42,7 +42,7 @@ void __storage_key_init_range(unsigned long start, unsigned long end);
 
 static inline void storage_key_init_range(unsigned long start, unsigned long end)
 {
-	if (PAGE_DEFAULT_KEY)
+	if (PAGE_DEFAULT_KEY != 0)
 		__storage_key_init_range(start, end);
 }
 

commit 5f490a520bcb393389a4d44bec90afcb332eb112
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Thu Jan 16 19:59:04 2020 +0100

    s390/mm: fix dynamic pagetable upgrade for hugetlbfs
    
    Commit ee71d16d22bb ("s390/mm: make TASK_SIZE independent from the number
    of page table levels") changed the logic of TASK_SIZE and also removed the
    arch_mmap_check() implementation for s390. This combination has a subtle
    effect on how get_unmapped_area() for hugetlbfs pages works. It is now
    possible that a user process establishes a hugetlbfs mapping at an address
    above 4 TB, without triggering a dynamic pagetable upgrade from 3 to 4
    levels.
    
    This is because hugetlbfs mappings will not use mm->get_unmapped_area, but
    rather file->f_op->get_unmapped_area, which currently is the generic
    implementation of hugetlb_get_unmapped_area() that does not know about s390
    dynamic pagetable upgrades, but with the new definition of TASK_SIZE, it
    will now allow mappings above 4 TB.
    
    Subsequent access to such a mapped address above 4 TB will result in a page
    fault loop, because the CPU cannot translate such a large address with 3
    pagetable levels. The fault handler will try to map in a hugepage at the
    address, but due to the folded pagetable logic it will end up with creating
    entries in the 3 level pagetable, possibly overwriting existing mappings,
    and then it all repeats when the access is retried.
    
    Apart from the page fault loop, this can have various nasty effects, e.g.
    kernel panic from one of the BUG_ON() checks in memory management code,
    or even data loss if an existing mapping gets overwritten.
    
    Fix this by implementing HAVE_ARCH_HUGETLB_UNMAPPED_AREA support for s390,
    providing an s390 version for hugetlb_get_unmapped_area() with pagetable
    upgrade support similar to arch_get_unmapped_area(), which will then be
    used instead of the generic version.
    
    Fixes: ee71d16d22bb ("s390/mm: make TASK_SIZE independent from the number of page table levels")
    Cc: <stable@vger.kernel.org> # 4.12+
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index a4d38092530a..85e944f04c70 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -33,6 +33,8 @@
 #define ARCH_HAS_PREPARE_HUGEPAGE
 #define ARCH_HAS_HUGEPAGE_CLEAR_FLUSH
 
+#define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 

commit 8b5369ea580964dbc982781bfb9fb93459fc5e8d
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Mon Oct 14 20:31:03 2019 +0200

    dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
    
    Some architectures, notably ARM, are interested in tweaking this
    depending on their runtime DMA addressing limitations.
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 823578c6b9e2..a4d38092530a 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -177,8 +177,6 @@ static inline int devmem_is_allowed(unsigned long pfn)
 #define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
-#define ARCH_ZONE_DMA_BITS	31
-
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>
 

commit 1a2dcff881059dedc14fafc8a442664c8dbd60f1
Author: Halil Pasic <pasic@linux.ibm.com>
Date:   Wed Jul 24 00:51:55 2019 +0200

    s390/dma: provide proper ARCH_ZONE_DMA_BITS value
    
    On s390 ZONE_DMA is up to 2G, i.e. ARCH_ZONE_DMA_BITS should be 31 bits.
    The current value is 24 and makes __dma_direct_alloc_pages() take a
    wrong turn first (but __dma_direct_alloc_pages() recovers then).
    
    Let's correct ARCH_ZONE_DMA_BITS value and avoid wrong turns.
    
    Signed-off-by: Halil Pasic <pasic@linux.ibm.com>
    Reported-by: Petr Tesarik <ptesarik@suse.cz>
    Fixes: c61e9637340e ("dma-direct: add support for allocation from ZONE_DMA and ZONE_DMA32")
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index a4d38092530a..823578c6b9e2 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -177,6 +177,8 @@ static inline int devmem_is_allowed(unsigned long pfn)
 #define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
+#define ARCH_ZONE_DMA_BITS	31
+
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>
 

commit 75f195420a987490d3ce79db2d06c70fbfce4ac4
Author: Vasily Gorbik <gor@linux.ibm.com>
Date:   Fri Nov 17 11:21:49 2017 +0100

    s390/mm: add missing pfn_to_kaddr helper
    
    kasan common code uses pfn_to_kaddr, which is defined by many other
    architectures. Adding it as well to avoid a build error.
    
    Reviewed-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 41e3908b397f..a4d38092530a 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -161,6 +161,7 @@ static inline int devmem_is_allowed(unsigned long pfn)
 
 #define virt_to_pfn(kaddr)	(__pa(kaddr) >> PAGE_SHIFT)
 #define pfn_to_virt(pfn)	__va((pfn) << PAGE_SHIFT)
+#define pfn_to_kaddr(pfn)	pfn_to_virt(pfn)
 
 #define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define page_to_virt(page)	pfn_to_virt(page_to_pfn(page))

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 5d5c2b3500a4..41e3908b397f 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  *  S390 version
  *    Copyright IBM Corp. 1999, 2000

commit 16f48641716000ae674159a021c93632d172f466
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Mon Aug 7 15:16:15 2017 +0200

    s390/mm: add missing virt_to_pfn() etc. helper functions
    
    Add all the missing little helper functions like virt_to_pfn(),
    phys_to_page(), etc.  While we had a couple of these helper functions
    like e.g. page_to_phys() the other functions were missing, which is
    quite annoying if one is looking for exactly such a function.
    
    Therefore finally add all those little helper functions.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index c68b21b12182..5d5c2b3500a4 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -152,16 +152,26 @@ static inline int devmem_is_allowed(unsigned long pfn)
 
 #endif /* !__ASSEMBLY__ */
 
-#define __PAGE_OFFSET           0x0UL
-#define PAGE_OFFSET             0x0UL
-#define __pa(x)                 (unsigned long)(x)
-#define __va(x)                 (void *)(unsigned long)(x)
-#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
-#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define __PAGE_OFFSET		0x0UL
+#define PAGE_OFFSET		0x0UL
+
+#define __pa(x)			((unsigned long)(x))
+#define __va(x)			((void *)(unsigned long)(x))
+
+#define virt_to_pfn(kaddr)	(__pa(kaddr) >> PAGE_SHIFT)
 #define pfn_to_virt(pfn)	__va((pfn) << PAGE_SHIFT)
+
+#define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
 #define page_to_virt(page)	pfn_to_virt(page_to_pfn(page))
 
+#define phys_to_pfn(kaddr)	((kaddr) >> PAGE_SHIFT)
+#define pfn_to_phys(pfn)	((pfn) << PAGE_SHIFT)
+
+#define phys_to_page(kaddr)	pfn_to_page(phys_to_pfn(kaddr))
+#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
+
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+
 #define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 

commit c67da7c7c5d4c0a45b079b21f6991cb7e753856e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jun 16 17:24:39 2017 +0200

    s390/mm: introduce defines to reflect the hardware mmu
    
    Add various defines like e.g. _REGION1_SHIFT to reflect the hardware
    mmu. We have quite a bit code that does not make use of the Linux
    memory management primitives but directly modifies page, segment and
    region values.
    
    Most of this is open-coded like e.g. "1UL << 53". In order to clean
    this up introduce a couple of new defines. The existing Linux memory
    management defines are changed, so the mapping to the hardware
    implementation is reflected.
    
    Reviewed-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index b463df89f344..c68b21b12182 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -10,10 +10,14 @@
 #include <linux/const.h>
 #include <asm/types.h>
 
+#define _PAGE_SHIFT	12
+#define _PAGE_SIZE	(_AC(1, UL) << _PAGE_SHIFT)
+#define _PAGE_MASK	(~(_PAGE_SIZE - 1))
+
 /* PAGE_SHIFT determines the page size */
-#define PAGE_SHIFT      12
-#define PAGE_SIZE	(_AC(1,UL) << PAGE_SHIFT)
-#define PAGE_MASK       (~(PAGE_SIZE-1))
+#define PAGE_SHIFT	_PAGE_SHIFT
+#define PAGE_SIZE	_PAGE_SIZE
+#define PAGE_MASK	_PAGE_MASK
 #define PAGE_DEFAULT_ACC	0
 #define PAGE_DEFAULT_KEY	(PAGE_DEFAULT_ACC << 4)
 

commit c9b5ad546e7d486465a3dd8c89245ac3707a4384
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Jun 14 12:56:01 2016 +0200

    s390/mm: tag normal pages vs pages used in page tables
    
    The ESSA instruction has a new option that allows to tag pages that
    are not used as a page table. Without the tag the hypervisor has to
    assume that any guest page could be used in a page table inside the
    guest. This forces the hypervisor to flush all guest TLB entries
    whenever a host page table entry is invalidated. With the tag
    the host can skip the TLB flush if the page is tagged as normal page.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 624deaa44230..b463df89f344 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -133,6 +133,9 @@ static inline int page_reset_referenced(unsigned long addr)
 struct page;
 void arch_free_page(struct page *page, int order);
 void arch_alloc_page(struct page *page, int order);
+void arch_set_page_dat(struct page *page, int order);
+void arch_set_page_nodat(struct page *page, int order);
+int arch_test_page_nodat(struct page *page);
 void arch_set_page_states(int make_stable);
 
 static inline int devmem_is_allowed(unsigned long pfn)

commit 1aea9b3f921003f0880f0676ae85d87c9f1cb4a2
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Apr 24 18:19:10 2017 +0200

    s390/mm: implement 5 level pages tables
    
    Add the logic to upgrade the page table for a 64-bit process to
    five levels. This increases the TASK_SIZE from 8PB to 16EB-4K.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 69b8a41fca84..624deaa44230 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -74,6 +74,7 @@ typedef struct { unsigned long pgste; } pgste_t;
 typedef struct { unsigned long pte; } pte_t;
 typedef struct { unsigned long pmd; } pmd_t;
 typedef struct { unsigned long pud; } pud_t;
+typedef struct { unsigned long p4d; } p4d_t;
 typedef struct { unsigned long pgd; } pgd_t;
 typedef pte_t *pgtable_t;
 
@@ -82,12 +83,14 @@ typedef pte_t *pgtable_t;
 #define pte_val(x)	((x).pte)
 #define pmd_val(x)	((x).pmd)
 #define pud_val(x)	((x).pud)
+#define p4d_val(x)	((x).p4d)
 #define pgd_val(x)      ((x).pgd)
 
 #define __pgste(x)	((pgste_t) { (x) } )
 #define __pte(x)        ((pte_t) { (x) } )
 #define __pmd(x)        ((pmd_t) { (x) } )
 #define __pud(x)	((pud_t) { (x) } )
+#define __p4d(x)	((p4d_t) { (x) } )
 #define __pgd(x)        ((pgd_t) { (x) } )
 #define __pgprot(x)     ((pgprot_t) { (x) } )
 

commit 221bb8a46e230b9824204ae86537183d9991ff2a
Merge: f7b32e4c021f 23528bb21ee2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Aug 2 16:11:27 2016 -0400

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
    
     - ARM: GICv3 ITS emulation and various fixes.  Removal of the
       old VGIC implementation.
    
     - s390: support for trapping software breakpoints, nested
       virtualization (vSIE), the STHYI opcode, initial extensions
       for CPU model support.
    
     - MIPS: support for MIPS64 hosts (32-bit guests only) and lots
       of cleanups, preliminary to this and the upcoming support for
       hardware virtualization extensions.
    
     - x86: support for execute-only mappings in nested EPT; reduced
       vmexit latency for TSC deadline timer (by about 30%) on Intel
       hosts; support for more than 255 vCPUs.
    
     - PPC: bugfixes.
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (302 commits)
      KVM: PPC: Introduce KVM_CAP_PPC_HTM
      MIPS: Select HAVE_KVM for MIPS64_R{2,6}
      MIPS: KVM: Reset CP0_PageMask during host TLB flush
      MIPS: KVM: Fix ptr->int cast via KVM_GUEST_KSEGX()
      MIPS: KVM: Sign extend MFC0/RDHWR results
      MIPS: KVM: Fix 64-bit big endian dynamic translation
      MIPS: KVM: Fail if ebase doesn't fit in CP0_EBase
      MIPS: KVM: Use 64-bit CP0_EBase when appropriate
      MIPS: KVM: Set CP0_Status.KX on MIPS64
      MIPS: KVM: Make entry code MIPS64 friendly
      MIPS: KVM: Use kmap instead of CKSEG0ADDR()
      MIPS: KVM: Use virt_to_phys() to get commpage PFN
      MIPS: Fix definition of KSEGX() for 64-bit
      KVM: VMX: Add VMCS to CPU's loaded VMCSs before VMPTRLD
      kvm: x86: nVMX: maintain internal copy of current VMCS
      KVM: PPC: Book3S HV: Save/restore TM state in H_CEDE
      KVM: PPC: Book3S HV: Pull out TM state save/restore into separate procedures
      KVM: arm64: vgic-its: Simplify MAPI error handling
      KVM: arm64: vgic-its: Make vgic_its_cmd_handle_mapi similar to other handlers
      KVM: arm64: vgic-its: Turn device_id validation into generic ID validation
      ...

commit d08de8e2d86744f91d9d5d57c56ca2b6e33bf6ec
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Mon Jul 4 14:47:01 2016 +0200

    s390/mm: add support for 2GB hugepages
    
    This adds support for 2GB hugetlbfs pages on s390.
    
    Reviewed-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 42fd5fea42ee..b2146c4119b2 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -21,6 +21,7 @@
 #define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
 #define HPAGE_MASK	(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+#define HUGE_MAX_HSTATE		2
 
 #define ARCH_HAS_SETCLEAR_HUGE_PTE
 #define ARCH_HAS_HUGE_PTE_TYPE

commit 3d84683bd737e397ae200e881a3230e469c59ad6
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Nov 16 10:45:03 2015 +0100

    s390: introduce page_to_virt() and pfn_to_virt()
    
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index f874e7d51c19..b5edff32e547 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -147,6 +147,8 @@ static inline int devmem_is_allowed(unsigned long pfn)
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
 #define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
 #define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+#define pfn_to_virt(pfn)	__va((pfn) << PAGE_SHIFT)
+#define page_to_virt(page)	pfn_to_virt(page_to_pfn(page))
 
 #define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)

commit de3fa841e429de7e288facf9b642948677fac581
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Jun 14 06:55:43 2016 +0200

    s390/mm: fix compile for PAGE_DEFAULT_KEY != 0
    
    The usual problem for code that is ifdef'ed out is that it doesn't
    compile after a while. That's also the case for the storage key
    initialisation code, if it would be used (set PAGE_DEFAULT_KEY to
    something not zero):
    
    ./arch/s390/include/asm/page.h: In function 'storage_key_init_range':
    ./arch/s390/include/asm/page.h:36:2: error: implicit declaration of function '__storage_key_init_range'
    
    Since the code itself has been useful for debugging purposes several
    times, remove the ifdefs and make sure the code gets compiler
    coverage. The cost for this is eight bytes.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 53eacbd4f09b..42fd5fea42ee 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -30,11 +30,12 @@
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 
+void __storage_key_init_range(unsigned long start, unsigned long end);
+
 static inline void storage_key_init_range(unsigned long start, unsigned long end)
 {
-#if PAGE_DEFAULT_KEY
-	__storage_key_init_range(start, end);
-#endif
+	if (PAGE_DEFAULT_KEY)
+		__storage_key_init_range(start, end);
 }
 
 #define clear_page(page)	memset((page), 0, PAGE_SIZE)

commit a7e19ab55ffdd82f1a8d12694b9a0c0beeef534c
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue May 10 09:50:21 2016 +0200

    KVM: s390: handle missing storage-key facility
    
    Without the storage-key facility, SIE won't interpret SSKE, ISKE and
    RRBE for us. So let's add proper interception handlers that will be called
    if lazy sske cannot be enabled.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 53eacbd4f09b..f874e7d51c19 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -109,13 +109,14 @@ static inline unsigned char page_get_storage_key(unsigned long addr)
 
 static inline int page_reset_referenced(unsigned long addr)
 {
-	unsigned int ipm;
+	int cc;
 
 	asm volatile(
 		"	rrbe	0,%1\n"
 		"	ipm	%0\n"
-		: "=d" (ipm) : "a" (addr) : "cc");
-	return !!(ipm & 0x20000000);
+		"	srl	%0,28\n"
+		: "=d" (cc) : "a" (addr) : "cc");
+	return cc;
 }
 
 /* Bits int the storage key */

commit 41318bfe2af5a4bc3c3a2f904b0cd3679f952c75
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Fri Jul 17 16:23:34 2015 -0700

    revert "s390/mm: make hugepages_supported a boot time decision"
    
    Heiko noticed that the current check for hugepage support on s390 is a
    little bit too harsh as systems which do not support will crash.
    
    The reason is that pageblock_order can now get negative when we set
    HPAGE_SHIFT to 0.  To avoid all this and to avoid opening another can of
    worms with enabling HUGETLB_PAGE_SIZE_VARIABLE I think it would be best
    to simply allow architectures to define their own hugepages_supported().
    
    Revert bea41197ead3 ("s390/mm: make hugepages_supported a boot time
    decision") in preparation.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Cc: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 0844b780c6a4..53eacbd4f09b 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -17,10 +17,7 @@
 #define PAGE_DEFAULT_ACC	0
 #define PAGE_DEFAULT_KEY	(PAGE_DEFAULT_ACC << 4)
 
-#include <asm/setup.h>
-#ifndef __ASSEMBLY__
-
-extern unsigned int HPAGE_SHIFT;
+#define HPAGE_SHIFT	20
 #define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
 #define HPAGE_MASK	(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
@@ -30,6 +27,9 @@ extern unsigned int HPAGE_SHIFT;
 #define ARCH_HAS_PREPARE_HUGEPAGE
 #define ARCH_HAS_HUGEPAGE_CLEAR_FLUSH
 
+#include <asm/setup.h>
+#ifndef __ASSEMBLY__
+
 static inline void storage_key_init_range(unsigned long start, unsigned long end)
 {
 #if PAGE_DEFAULT_KEY

commit ad4f99e888192e53d463cab75075e88e65ad7d50
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Fri Jul 17 16:23:31 2015 -0700

    revert "s390/mm: change HPAGE_SHIFT type to int"
    
    Heiko noticed that the current check for hugepage support on s390 is a
    little bit too harsh as systems which do not support will crash.
    
    The reason is that pageblock_order can now get negative when we set
    HPAGE_SHIFT to 0.  To avoid all this and to avoid opening another can of
    worms with enabling HUGETLB_PAGE_SIZE_VARIABLE I think it would be best
    to simply allow architectures to define their own hugepages_supported().
    
    This patch (of 4): revert commit cf54e2fce51c ("s390/mm: change
    HPAGE_SHIFT type to int") in preparation.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    Cc: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index dd345238d9a7..0844b780c6a4 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -20,7 +20,7 @@
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 
-extern int HPAGE_SHIFT;
+extern unsigned int HPAGE_SHIFT;
 #define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
 #define HPAGE_MASK	(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)

commit cf54e2fce51c7ad2479fe8cf213a2ed618a8189b
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Jun 25 14:59:52 2015 -0700

    s390/mm: change HPAGE_SHIFT type to int
    
    With making HPAGE_SHIFT an unsigned integer we also accidentally changed
    pageblock_order.  In order to avoid compiler warnings we make
    HPAGE_SHFIT an int again.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Suggested-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 0844b780c6a4..dd345238d9a7 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -20,7 +20,7 @@
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 
-extern unsigned int HPAGE_SHIFT;
+extern int HPAGE_SHIFT;
 #define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
 #define HPAGE_MASK	(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)

commit bea41197ead3e03308bdd10c11db3ce91ae5c8ab
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Jun 25 14:59:39 2015 -0700

    s390/mm: make hugepages_supported a boot time decision
    
    There is a potential bug with KVM and hugetlbfs if the hardware does not
    support hugepages (EDAT1).  We fix this by making EDAT1 a hard requirement
    for hugepages and therefore removing and simplifying code.
    
    As s390, with the sw-emulated hugepages, was the only user of
    arch_prepare/release_hugepage I also removed theses calls from common and
    other architecture code.
    
    This patch (of 5):
    
    By dropping support for hugepages on machines which do not have the
    hardware feature EDAT1, we fix a potential s390 KVM bug.
    
    The bug would happen if a guest is backed by hugetlbfs (not supported
    currently), but does not get pagetables with PGSTE.  This would lead to
    random memory overwrites.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 53eacbd4f09b..0844b780c6a4 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -17,7 +17,10 @@
 #define PAGE_DEFAULT_ACC	0
 #define PAGE_DEFAULT_KEY	(PAGE_DEFAULT_ACC << 4)
 
-#define HPAGE_SHIFT	20
+#include <asm/setup.h>
+#ifndef __ASSEMBLY__
+
+extern unsigned int HPAGE_SHIFT;
 #define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
 #define HPAGE_MASK	(~(HPAGE_SIZE - 1))
 #define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
@@ -27,9 +30,6 @@
 #define ARCH_HAS_PREPARE_HUGEPAGE
 #define ARCH_HAS_HUGEPAGE_CLEAR_FLUSH
 
-#include <asm/setup.h>
-#ifndef __ASSEMBLY__
-
 static inline void storage_key_init_range(unsigned long start, unsigned long end)
 {
 #if PAGE_DEFAULT_KEY

commit fb3d1c085c05e0e4b112d915dbd06b20b259e6c5
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Fri Jan 30 10:31:13 2015 +0100

    s390: let the compiler do page clearing
    
    The hardware folks told me that for page clearing "when you exactly
    know what to do, hand written xc+pfd is usally faster then mvcl for
    page clearing, as it saves millicode overhead and parameter parsing
    and checking" as long as you dont need the cache bypassing.
    Turns out that gcc already does a proper xc,pfd loop.
    
    A small test on z196 that does
    
    buff = mmap(NULL, bufsize,PROT_EXEC|PROT_WRITE|PROT_READ,AP_PRIVATE| MAP_ANONYMOUS,0,0);
    for ( i = 0; i < bufsize; i+= 256)
        buff[i] = 0x5;
    
    gets 20% faster (touches every cache line of a page)
    
    and
    
    buff = mmap(NULL, bufsize,PROT_EXEC|PROT_WRITE|PROT_READ,AP_PRIVATE| MAP_ANONYMOUS,0,0);
    for ( i = 0; i < bufsize; i+= 4096)
        buff[i] = 0x5;
    
    is within noise ratio (touches one cache line of a page).
    
    As the clear_page is usually called for first memory accesses
    we can assume that at least one cache line is used afterwards,
    so this change should be always better.
    Another benchmark, a make -j 40 of my testsuite in tmpfs with
    hot caches on a 32cpu system:
    
     -- unpatched --       --  patched  --
    real     0m1.017s     real     0m0.994s   (~2% faster, but in noise)
    user     0m5.339s     user     0m5.016s   (~6% faster)
    sys      0m0.691s     sys      0m0.632s   (~8% faster)
    
    Let use the same define to memset as the asm-generic variant
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 7b2ac6e44166..53eacbd4f09b 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -37,16 +37,7 @@ static inline void storage_key_init_range(unsigned long start, unsigned long end
 #endif
 }
 
-static inline void clear_page(void *page)
-{
-	register unsigned long reg1 asm ("1") = 0;
-	register void *reg2 asm ("2") = page;
-	register unsigned long reg3 asm ("3") = 4096;
-	asm volatile(
-		"	mvcl	2,0"
-		: "+d" (reg2), "+d" (reg3) : "d" (reg1)
-		: "memory", "cc");
-}
+#define clear_page(page)	memset((page), 0, PAGE_SIZE)
 
 /*
  * copy_page uses the mvcl instruction with 0xb0 padding byte in order to

commit a6c19dfe39941a5d3f4d072121c0a4841e7e26fd
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Fri Aug 8 14:23:40 2014 -0700

    arm64,ia64,ppc,s390,sh,tile,um,x86,mm: remove default gate area
    
    The core mm code will provide a default gate area based on
    FIXADDR_USER_START and FIXADDR_USER_END if
    !defined(__HAVE_ARCH_GATE_AREA) && defined(AT_SYSINFO_EHDR).
    
    This default is only useful for ia64.  arm64, ppc, s390, sh, tile, 64-bit
    UML, and x86_32 have their own code just to disable it.  arm, 32-bit UML,
    and x86_64 have gate areas, but they have their own implementations.
    
    This gets rid of the default and moves the code into ia64.
    
    This should save some code on architectures without a gate area: it's now
    possible to inline the gate_area functions in the default case.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Acked-by: Nathan Lynch <nathan_lynch@mentor.com>
    Acked-by: H. Peter Anvin <hpa@linux.intel.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org> [in principle]
    Acked-by: Richard Weinberger <richard@nod.at> [for um]
    Acked-by: Will Deacon <will.deacon@arm.com> [for arm64]
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Nathan Lynch <Nathan_Lynch@mentor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 114258eeaacd..7b2ac6e44166 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -162,6 +162,4 @@ static inline int devmem_is_allowed(unsigned long pfn)
 #include <asm-generic/memory_model.h>
 #include <asm-generic/getorder.h>
 
-#define __HAVE_ARCH_GATE_AREA 1
-
 #endif /* _S390_PAGE_H */

commit dba6bb60043ed73abca8990f237db63a8cea6c50
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Tue Nov 19 14:25:17 2013 +0100

    s390/mm: optimize copy_page
    
    Always use the mvcl instruction to copy a page instead of mvpg or a
    couple of mvc instructions.
    Copying a huge page is 25% faster this way. Also bypass caches when
    copying pages since only parts of a page will be used afterwards.
    Especially when copying a huge page this would kick everything out
    of the L1 and L2 data caches on a zEC12 machine.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 316c8503a3b4..114258eeaacd 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -48,33 +48,21 @@ static inline void clear_page(void *page)
 		: "memory", "cc");
 }
 
+/*
+ * copy_page uses the mvcl instruction with 0xb0 padding byte in order to
+ * bypass caches when copying a page. Especially when copying huge pages
+ * this keeps L1 and L2 data caches alive.
+ */
 static inline void copy_page(void *to, void *from)
 {
-	if (MACHINE_HAS_MVPG) {
-		register unsigned long reg0 asm ("0") = 0;
-		asm volatile(
-			"	mvpg	%0,%1"
-			: : "a" (to), "a" (from), "d" (reg0)
-			: "memory", "cc");
-	} else
-		asm volatile(
-			"	mvc	0(256,%0),0(%1)\n"
-			"	mvc	256(256,%0),256(%1)\n"
-			"	mvc	512(256,%0),512(%1)\n"
-			"	mvc	768(256,%0),768(%1)\n"
-			"	mvc	1024(256,%0),1024(%1)\n"
-			"	mvc	1280(256,%0),1280(%1)\n"
-			"	mvc	1536(256,%0),1536(%1)\n"
-			"	mvc	1792(256,%0),1792(%1)\n"
-			"	mvc	2048(256,%0),2048(%1)\n"
-			"	mvc	2304(256,%0),2304(%1)\n"
-			"	mvc	2560(256,%0),2560(%1)\n"
-			"	mvc	2816(256,%0),2816(%1)\n"
-			"	mvc	3072(256,%0),3072(%1)\n"
-			"	mvc	3328(256,%0),3328(%1)\n"
-			"	mvc	3584(256,%0),3584(%1)\n"
-			"	mvc	3840(256,%0),3840(%1)\n"
-			: : "a" (to), "a" (from) : "memory");
+	register void *reg2 asm ("2") = to;
+	register unsigned long reg3 asm ("3") = 0x1000;
+	register void *reg4 asm ("4") = from;
+	register unsigned long reg5 asm ("5") = 0xb0001000;
+	asm volatile(
+		"	mvcl	2,4"
+		: "+d" (reg2), "+d" (reg3), "+d" (reg4), "+d" (reg5)
+		: : "memory", "cc");
 }
 
 #define clear_user_page(page, vaddr, pg)	clear_page(page)

commit 127c1feffff8078400f01372712c600d0f9eef51
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Oct 7 12:12:32 2013 +0200

    s390/mm: do not initialize storage keys
    
    With dirty and referenced bits implemented in software it is unnecessary
    to initialize the storage key for every page. With this patch not a single
    storage key operation is done for a system that does not use KVM.
    For KVM set_pte_at/pgste_set_key will do the initialization for the guest
    view of the storage key when the mapping for the page is established in
    the host.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 1e51f2915b2e..316c8503a3b4 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -30,7 +30,12 @@
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 
-void storage_key_init_range(unsigned long start, unsigned long end);
+static inline void storage_key_init_range(unsigned long start, unsigned long end)
+{
+#if PAGE_DEFAULT_KEY
+	__storage_key_init_range(start, end);
+#endif
+}
 
 static inline void clear_page(void *page)
 {

commit 0944fe3f4a323f436180d39402cae7f9c46ead17
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Jul 23 22:11:42 2013 +0200

    s390/mm: implement software referenced bits
    
    The last remaining use for the storage key of the s390 architecture
    is reference counting. The alternative is to make page table entries
    invalid while they are old. On access the fault handler marks the
    pte/pmd as young which makes the pte/pmd valid if the access rights
    allow read access. The pte/pmd invalidations required for software
    managed reference bits cost a bit of performance, on the other hand
    the RRBE/RRBM instructions to read and reset the referenced bits are
    quite expensive as well.
    
    Reviewed-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 27f04801ec8b..1e51f2915b2e 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -140,15 +140,6 @@ static inline int page_reset_referenced(unsigned long addr)
 #define _PAGE_FP_BIT		0x08	/* HW fetch protection bit	*/
 #define _PAGE_ACC_BITS		0xf0	/* HW access control bits	*/
 
-/*
- * Test and clear referenced bit in storage key.
- */
-#define __HAVE_ARCH_PAGE_TEST_AND_CLEAR_YOUNG
-static inline int page_test_and_clear_young(unsigned long pfn)
-{
-	return page_reset_referenced(pfn << PAGE_SHIFT);
-}
-
 struct page;
 void arch_free_page(struct page *page, int order);
 void arch_alloc_page(struct page *page, int order);

commit 416fd0ffb14afead5b1feea14bbf33c2277942ef
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Jul 25 09:39:30 2013 +0200

    s390/mm: remove dead pfmf inline assembly
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 5d64fb7619cc..27f04801ec8b 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -32,16 +32,6 @@
 
 void storage_key_init_range(unsigned long start, unsigned long end);
 
-static inline unsigned long pfmf(unsigned long function, unsigned long address)
-{
-	asm volatile(
-		"	.insn	rre,0xb9af0000,%[function],%[address]"
-		: [address] "+a" (address)
-		: [function] "d" (function)
-		: "memory");
-	return address;
-}
-
 static inline void clear_page(void *page)
 {
 	register unsigned long reg1 asm ("1") = 0;

commit 617e164c35c94c936e3632b387d6dbb3d80c2c02
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon May 6 21:44:36 2013 +0200

    s390: disable pfmf for clear page instruction
    
    Wit the introduction of large pages Linux also used pfmf for page
    clearing. The current implementation is not ideal, though:
    - currently we set usage intent=0, but cleared pages are often used
    directly after the clearing
    - z/VM does not yet provide EDAT
    - KVM does have to intercept PFMF even for resident pages
    
    Lets just the mvcl loop in all cases until we have a well defined
    pattern were pfmf is besser.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 75ce9b065f9f..5d64fb7619cc 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -32,7 +32,7 @@
 
 void storage_key_init_range(unsigned long start, unsigned long end);
 
-static unsigned long pfmf(unsigned long function, unsigned long address)
+static inline unsigned long pfmf(unsigned long function, unsigned long address)
 {
 	asm volatile(
 		"	.insn	rre,0xb9af0000,%[function],%[address]"
@@ -44,17 +44,13 @@ static unsigned long pfmf(unsigned long function, unsigned long address)
 
 static inline void clear_page(void *page)
 {
-	if (MACHINE_HAS_PFMF) {
-		pfmf(0x10000, (unsigned long)page);
-	} else {
-		register unsigned long reg1 asm ("1") = 0;
-		register void *reg2 asm ("2") = page;
-		register unsigned long reg3 asm ("3") = 4096;
-		asm volatile(
-			"	mvcl	2,0"
-			: "+d" (reg2), "+d" (reg3) : "d" (reg1)
-			: "memory", "cc");
-	}
+	register unsigned long reg1 asm ("1") = 0;
+	register void *reg2 asm ("2") = page;
+	register unsigned long reg3 asm ("3") = 4096;
+	asm volatile(
+		"	mvcl	2,0"
+		: "+d" (reg2), "+d" (reg3) : "d" (reg1)
+		: "memory", "cc");
 }
 
 static inline void copy_page(void *to, void *from)

commit abf09bed3cceadd809f0356065c2ada6cee90d4a
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Nov 7 13:17:37 2012 +0100

    s390/mm: implement software dirty bits
    
    The s390 architecture is unique in respect to dirty page detection,
    it uses the change bit in the per-page storage key to track page
    modifications. All other architectures track dirty bits by means
    of page table entries. This property of s390 has caused numerous
    problems in the past, e.g. see git commit ef5d437f71afdf4a
    "mm: fix XFS oops due to dirty pages without buffers on s390".
    
    To avoid future issues in regard to per-page dirty bits convert
    s390 to a fault based software dirty bit detection mechanism. All
    user page table entries which are marked as clean will be hardware
    read-only, even if the pte is supposed to be writable. A write by
    the user process will trigger a protection fault which will cause
    the user pte to be marked as dirty and the hardware read-only bit
    is removed.
    
    With this change the dirty bit in the storage key is irrelevant
    for Linux as a host, but the storage key is still required for
    KVM guests. The effect is that page_test_and_clear_dirty and the
    related code can be removed. The referenced bit in the storage
    key is still used by the page_test_and_clear_young primitive to
    provide page age information.
    
    For page cache pages of mappings with mapping_cap_account_dirty
    there will not be any change in behavior as the dirty bit tracking
    already uses read-only ptes to control the amount of dirty pages.
    Only for swap cache pages and pages of mappings without
    mapping_cap_account_dirty there can be additional protection faults.
    To avoid an excessive number of additional faults the mk_pte
    primitive checks for PageDirty if the pgprot value allows for writes
    and pre-dirties the pte. That avoids all additional faults for
    tmpfs and shmem pages until these pages are added to the swap cache.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index a86ad4084073..75ce9b065f9f 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -154,28 +154,6 @@ static inline int page_reset_referenced(unsigned long addr)
 #define _PAGE_FP_BIT		0x08	/* HW fetch protection bit	*/
 #define _PAGE_ACC_BITS		0xf0	/* HW access control bits	*/
 
-/*
- * Test and clear dirty bit in storage key.
- * We can't clear the changed bit atomically. This is a potential
- * race against modification of the referenced bit. This function
- * should therefore only be called if it is not mapped in any
- * address space.
- *
- * Note that the bit gets set whenever page content is changed. That means
- * also when the page is modified by DMA or from inside the kernel.
- */
-#define __HAVE_ARCH_PAGE_TEST_AND_CLEAR_DIRTY
-static inline int page_test_and_clear_dirty(unsigned long pfn, int mapped)
-{
-	unsigned char skey;
-
-	skey = page_get_storage_key(pfn << PAGE_SHIFT);
-	if (!(skey & _PAGE_CHANGED))
-		return 0;
-	page_set_storage_key(pfn << PAGE_SHIFT, skey & ~_PAGE_CHANGED, mapped);
-	return 1;
-}
-
 /*
  * Test and clear referenced bit in storage key.
  */

commit c7708fac5a878d6e0f2de0aa19f9749cff4f707f
Merge: 3127f23f013e 6726a807c38d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 13 14:20:19 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux
    
    Pull s390 update from Martin Schwidefsky:
     "Add support to generate code for the latest machine zEC12, MOD and XOR
      instruction support for the BPF jit compiler, the dasd safe offline
      feature and the big one: the s390 architecture gets PCI support!!
      Right before the world ends on the 21st ;-)"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/s390/linux: (41 commits)
      s390/qdio: rename the misleading PCI flag of qdio devices
      s390/pci: remove obsolete email addresses
      s390/pci: speed up __iowrite64_copy by using pci store block insn
      s390/pci: enable NEED_DMA_MAP_STATE
      s390/pci: no msleep in potential IRQ context
      s390/pci: fix potential NULL pointer dereference in dma_free_seg_table()
      s390/pci: use kmem_cache_zalloc instead of kmem_cache_alloc/memset
      s390/bpf,jit: add support for XOR instruction
      s390/bpf,jit: add support MOD instruction
      s390/cio: fix pgid reserved check
      vga: compile fix, disable vga for s390
      s390/pci: add PCI Kconfig options
      s390/pci: s390 specific PCI sysfs attributes
      s390/pci: PCI hotplug support via SCLP
      s390/pci: CHSC PCI support for error and availability events
      s390/pci: DMA support
      s390/pci: PCI adapter interrupts for MSI/MSI-X
      s390/bitops: find leftmost bit instruction support
      s390/pci: CLP interface
      s390/pci: base support
      ...

commit e749eb95531ac8349df47f8d46ce2641dcb16589
Author: Jan Kara <jack@suse.cz>
Date:   Tue Dec 11 16:01:09 2012 -0800

    mm: add comment on storage key dirty bit semantics
    
    Add comments that dirty bit in storage key gets set whenever page content
    is changed.  Hopefully if someone will use this function, he'll have a
    look at one of the two places where we comment on this.
    
    Signed-off-by: Jan Kara <jack@suse.cz>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 6d5367060a56..39faa4ac9660 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -158,6 +158,9 @@ static inline int page_reset_referenced(unsigned long addr)
  * race against modification of the referenced bit. This function
  * should therefore only be called if it is not mapped in any
  * address space.
+ *
+ * Note that the bit gets set whenever page content is changed. That means
+ * also when the page is modified by DMA or from inside the kernel.
  */
 #define __HAVE_ARCH_PAGE_TEST_AND_CLEAR_DIRTY
 static inline int page_test_and_clear_dirty(unsigned long pfn, int mapped)

commit 6b70a92080be508e16b2f53efba39e5155eff809
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Nov 2 12:56:43 2012 +0100

    s390/memory hotplug: use pfmf instruction to initialize storage keys
    
    Move and rename init_storage_keys() to pageattr.c, so it can also be
    used from the sclp memory hotplug code in order to initialize
    storage keys.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 6d5367060a56..24afd079e5ce 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -30,6 +30,8 @@
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 
+void storage_key_init_range(unsigned long start, unsigned long end);
+
 static unsigned long pfmf(unsigned long function, unsigned long address)
 {
 	asm volatile(

commit 85e9d0e5ffabfede5facbac5b0d9b90768bc6e90
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Thu Sep 27 10:45:06 2012 +0200

    s390/mm: use pfmf instruction to initialize storage keys
    
    Make use of the pfmf instruction, if available, to initialize storage keys
    of whole 1MB or 2GB frames instead of initializing every single page with
    the sske instruction.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 27ab3c7c1e8b..6d5367060a56 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -30,12 +30,20 @@
 #include <asm/setup.h>
 #ifndef __ASSEMBLY__
 
+static unsigned long pfmf(unsigned long function, unsigned long address)
+{
+	asm volatile(
+		"	.insn	rre,0xb9af0000,%[function],%[address]"
+		: [address] "+a" (address)
+		: [function] "d" (function)
+		: "memory");
+	return address;
+}
+
 static inline void clear_page(void *page)
 {
 	if (MACHINE_HAS_PFMF) {
-		asm volatile(
-			"	.insn	rre,0xb9af0000,%0,%1"
-			: : "d" (0x10000), "a" (page) : "memory", "cc");
+		pfmf(0x10000, (unsigned long)page);
 	} else {
 		register unsigned long reg1 asm ("1") = 0;
 		register void *reg2 asm ("2") = page;

commit a53c8fab3f87c995c30ac226a03af95361243144
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Jul 20 11:15:04 2012 +0200

    s390/comments: unify copyright messages and remove file names
    
    Remove the file name from the comment at top of many files. In most
    cases the file name was wrong anyway, so it's rather pointless.
    
    Also unify the IBM copyright statement. We did have a lot of sightly
    different statements and wanted to change them one after another
    whenever a file gets touched. However that never happened. Instead
    people start to take the old/"wrong" statements to use as a template
    for new files.
    So unify all of them in one go.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index f7ec548c2b9d..27ab3c7c1e8b 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -1,8 +1,6 @@
 /*
- *  include/asm-s390/page.h
- *
  *  S390 version
- *    Copyright (C) 1999,2000 IBM Deutschland Entwicklung GmbH, IBM Corporation
+ *    Copyright IBM Corp. 1999, 2000
  *    Author(s): Hartmut Penner (hp@de.ibm.com)
  */
 

commit 638ad34a8811119b32247b7722288ef8b90907d1
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Sun Oct 30 15:17:13 2011 +0100

    [S390] sparse: fix sparse warnings about missing prototypes
    
    Add prototypes and includes for functions used in different modules.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index accb372ddc7e..f7ec548c2b9d 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -177,6 +177,7 @@ static inline int page_test_and_clear_young(unsigned long pfn)
 struct page;
 void arch_free_page(struct page *page, int order);
 void arch_alloc_page(struct page *page, int order);
+void arch_set_page_states(int make_stable);
 
 static inline int devmem_is_allowed(unsigned long pfn)
 {

commit b2fa47e6bf5148aa6dbf22ec79f18141b421eeba
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon May 23 10:24:40 2011 +0200

    [S390] refactor page table functions for better pgste support
    
    Rework the architecture page table functions to access the bits in the
    page table extension array (pgste). There are a number of changes:
    1) Fix missing pgste update if the attach_count for the mm is <= 1.
    2) For every operation that affects the invalid bit in the pte or the
       rcp byte in the pgste the pcl lock needs to be acquired. The function
       pgste_get_lock gets the pcl lock and returns the current pgste value
       for a pte pointer. The function pgste_set_unlock stores the pgste
       and releases the lock. Between these two calls the bits in the pgste
       can be shuffled.
    3) Define two software bits in the pte _PAGE_SWR and _PAGE_SWC to avoid
       calling SetPageDirty and SetPageReferenced from pgtable.h. If the
       host reference backup bit or the host change backup bit has been
       set the dirty/referenced state is transfered to the pte. The common
       code will pick up the state from the pte.
    4) Add ptep_modify_prot_start and ptep_modify_prot_commit for mprotect.
    5) Remove pgd_populate_kernel, pud_populate_kernel, pmd_populate_kernel
       pgd_clear_kernel, pud_clear_kernel, pmd_clear_kernel and ptep_invalidate.
    6) Rename kvm_s390_test_and_clear_page_dirty to
       ptep_test_and_clear_user_dirty and add ptep_test_and_clear_user_young.
    7) Define mm_exclusive() and mm_has_pgste() helper to improve readability.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 81ee2776088d..accb372ddc7e 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -90,6 +90,7 @@ static inline void copy_page(void *to, void *from)
  */
 
 typedef struct { unsigned long pgprot; } pgprot_t;
+typedef struct { unsigned long pgste; } pgste_t;
 typedef struct { unsigned long pte; } pte_t;
 typedef struct { unsigned long pmd; } pmd_t;
 typedef struct { unsigned long pud; } pud_t;
@@ -97,13 +98,16 @@ typedef struct { unsigned long pgd; } pgd_t;
 typedef pte_t *pgtable_t;
 
 #define pgprot_val(x)	((x).pgprot)
+#define pgste_val(x)	((x).pgste)
 #define pte_val(x)	((x).pte)
 #define pmd_val(x)	((x).pmd)
 #define pud_val(x)	((x).pud)
 #define pgd_val(x)      ((x).pgd)
 
+#define __pgste(x)	((pgste_t) { (x) } )
 #define __pte(x)        ((pte_t) { (x) } )
 #define __pmd(x)        ((pmd_t) { (x) } )
+#define __pud(x)	((pud_t) { (x) } )
 #define __pgd(x)        ((pgd_t) { (x) } )
 #define __pgprot(x)     ((pgprot_t) { (x) } )
 

commit 2d42552d1c1659b014851cf449ad2fe458509128
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon May 23 10:24:39 2011 +0200

    [S390] merge page_test_dirty and page_clear_dirty
    
    The page_clear_dirty primitive always sets the default storage key
    which resets the access control bits and the fetch protection bit.
    That will surprise a KVM guest that sets non-zero access control
    bits or the fetch protection bit. Merge page_test_dirty and
    page_clear_dirty back to a single function and only clear the
    dirty bit from the storage key.
    
    In addition move the function page_test_and_clear_dirty and
    page_test_and_clear_young to page.h where they belong. This
    requires to change the parameter from a struct page * to a page
    frame number.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 3c987e9ec8d6..81ee2776088d 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -107,8 +107,8 @@ typedef pte_t *pgtable_t;
 #define __pgd(x)        ((pgd_t) { (x) } )
 #define __pgprot(x)     ((pgprot_t) { (x) } )
 
-static inline void
-page_set_storage_key(unsigned long addr, unsigned int skey, int mapped)
+static inline void page_set_storage_key(unsigned long addr,
+					unsigned char skey, int mapped)
 {
 	if (!mapped)
 		asm volatile(".insn rrf,0xb22b0000,%0,%1,8,0"
@@ -117,15 +117,59 @@ page_set_storage_key(unsigned long addr, unsigned int skey, int mapped)
 		asm volatile("sske %0,%1" : : "d" (skey), "a" (addr));
 }
 
-static inline unsigned int
-page_get_storage_key(unsigned long addr)
+static inline unsigned char page_get_storage_key(unsigned long addr)
 {
-	unsigned int skey;
+	unsigned char skey;
 
-	asm volatile("iske %0,%1" : "=d" (skey) : "a" (addr), "0" (0));
+	asm volatile("iske %0,%1" : "=d" (skey) : "a" (addr));
 	return skey;
 }
 
+static inline int page_reset_referenced(unsigned long addr)
+{
+	unsigned int ipm;
+
+	asm volatile(
+		"	rrbe	0,%1\n"
+		"	ipm	%0\n"
+		: "=d" (ipm) : "a" (addr) : "cc");
+	return !!(ipm & 0x20000000);
+}
+
+/* Bits int the storage key */
+#define _PAGE_CHANGED		0x02	/* HW changed bit		*/
+#define _PAGE_REFERENCED	0x04	/* HW referenced bit		*/
+#define _PAGE_FP_BIT		0x08	/* HW fetch protection bit	*/
+#define _PAGE_ACC_BITS		0xf0	/* HW access control bits	*/
+
+/*
+ * Test and clear dirty bit in storage key.
+ * We can't clear the changed bit atomically. This is a potential
+ * race against modification of the referenced bit. This function
+ * should therefore only be called if it is not mapped in any
+ * address space.
+ */
+#define __HAVE_ARCH_PAGE_TEST_AND_CLEAR_DIRTY
+static inline int page_test_and_clear_dirty(unsigned long pfn, int mapped)
+{
+	unsigned char skey;
+
+	skey = page_get_storage_key(pfn << PAGE_SHIFT);
+	if (!(skey & _PAGE_CHANGED))
+		return 0;
+	page_set_storage_key(pfn << PAGE_SHIFT, skey & ~_PAGE_CHANGED, mapped);
+	return 1;
+}
+
+/*
+ * Test and clear referenced bit in storage key.
+ */
+#define __HAVE_ARCH_PAGE_TEST_AND_CLEAR_YOUNG
+static inline int page_test_and_clear_young(unsigned long pfn)
+{
+	return page_reset_referenced(pfn << PAGE_SHIFT);
+}
+
 struct page;
 void arch_free_page(struct page *page, int order);
 void arch_alloc_page(struct page *page, int order);

commit ec6743bb06510c7b629603ce35713d6ae9273579
Author: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
Date:   Wed Nov 10 10:05:55 2010 +0100

    [S390] mm: add devmem_is_allowed() for STRICT_DEVMEM checking
    
    Provide the devmem_is_allowed() routine to restrict access to
    kernel memory from userspace.
    Set the CONFIG_STRICT_DEVMEM config option to switch on checking.
    
    Signed-off-by: Hendrik Brueckner <brueckner@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index a8729ea7e9ac..3c987e9ec8d6 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -130,6 +130,11 @@ struct page;
 void arch_free_page(struct page *page, int order);
 void arch_alloc_page(struct page *page, int order);
 
+static inline int devmem_is_allowed(unsigned long pfn)
+{
+	return 0;
+}
+
 #define HAVE_ARCH_FREE_PAGE
 #define HAVE_ARCH_ALLOC_PAGE
 

commit e2b8d7af0e3a9234de06606f9151f28cf847a8d6
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Mon Oct 25 16:10:14 2010 +0200

    [S390] add support for nonquiescing sske
    
    Improve performance of the sske operation by using the nonquiescing
    variant if the affected page has no mappings established. On machines
    with no support for the new sske variant the mask bit will be ignored.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index af650fb47206..a8729ea7e9ac 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -108,9 +108,13 @@ typedef pte_t *pgtable_t;
 #define __pgprot(x)     ((pgprot_t) { (x) } )
 
 static inline void
-page_set_storage_key(unsigned long addr, unsigned int skey)
+page_set_storage_key(unsigned long addr, unsigned int skey, int mapped)
 {
-	asm volatile("sske %0,%1" : : "d" (skey), "a" (addr));
+	if (!mapped)
+		asm volatile(".insn rrf,0xb22b0000,%0,%1,8,0"
+			     : : "d" (skey), "a" (addr));
+	else
+		asm volatile("sske %0,%1" : : "d" (skey), "a" (addr));
 }
 
 static inline unsigned int

commit d1bf85902c28dd990c08f1703ea94109223549a7
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Feb 26 22:37:30 2010 +0100

    [S390] cio: fix storage key handling
    
    Some parts of cio do not shift PAGE_DEFAULT_KEY correctly and end up
    with an incorrect key in their data structures.
    Since the default key is zero this doesn't really matter. However if
    somebody would use key-controlled protection for debugging purposes
    it would be quite helpful if all of this would work as expected.
    
    Also remove a stale declaration.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 5e9daf5d7f22..af650fb47206 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -107,9 +107,6 @@ typedef pte_t *pgtable_t;
 #define __pgd(x)        ((pgd_t) { (x) } )
 #define __pgprot(x)     ((pgprot_t) { (x) } )
 
-/* default storage key used for all pages */
-extern unsigned int default_storage_key;
-
 static inline void
 page_set_storage_key(unsigned long addr, unsigned int skey)
 {

commit 2ddddf3e0a55a7fcd6f240a7416cfcb12dd38b7e
Author: Heiko Carstens <heiko.carstens@de.ibm.com>
Date:   Fri Sep 11 10:29:01 2009 +0200

    [S390] Enable guest page hinting by default.
    
    Get rid of the PAGE_STATES config option and enable guest page hinting
    by default.
    It can be disabled by specifying "cmma=off" at the command line.
    
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 3e3594d01f83..5e9daf5d7f22 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -125,8 +125,6 @@ page_get_storage_key(unsigned long addr)
 	return skey;
 }
 
-#ifdef CONFIG_PAGE_STATES
-
 struct page;
 void arch_free_page(struct page *page, int order);
 void arch_alloc_page(struct page *page, int order);
@@ -134,8 +132,6 @@ void arch_alloc_page(struct page *page, int order);
 #define HAVE_ARCH_FREE_PAGE
 #define HAVE_ARCH_ALLOC_PAGE
 
-#endif
-
 #endif /* !__ASSEMBLY__ */
 
 #define __PAGE_OFFSET           0x0UL

commit 5b17e1cd8928ae65932758ce6478ac6d3e9a86b2
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Wed May 13 22:56:30 2009 +0000

    asm-generic: rename page.h and uaccess.h
    
    The current asm-generic/page.h only contains the get_order
    function, and asm-generic/uaccess.h only implements
    unaligned accesses. This renames the file to getorder.h
    and uaccess-unaligned.h to make room for new page.h
    and uaccess.h file that will be usable by all simple
    (e.g. nommu) architectures.
    
    Signed-off-by: Remis Lima Baima <remis.developer@googlemail.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 32e8f6aa4384..3e3594d01f83 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -150,7 +150,7 @@ void arch_alloc_page(struct page *page, int order);
 				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
 #include <asm-generic/memory_model.h>
-#include <asm-generic/page.h>
+#include <asm-generic/getorder.h>
 
 #define __HAVE_ARCH_GATE_AREA 1
 

commit b020632e40c3ed5e8c0c066d022672907e8401cf
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Thu Dec 25 13:38:36 2008 +0100

    [S390] introduce vdso on s390
    
    Add a vdso to speed up gettimeofday and clock_getres/clock_gettime for
    CLOCK_REALTIME/CLOCK_MONOTONIC.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
index 991ba939408c..32e8f6aa4384 100644
--- a/arch/s390/include/asm/page.h
+++ b/arch/s390/include/asm/page.h
@@ -152,4 +152,6 @@ void arch_alloc_page(struct page *page, int order);
 #include <asm-generic/memory_model.h>
 #include <asm-generic/page.h>
 
+#define __HAVE_ARCH_GATE_AREA 1
+
 #endif /* _S390_PAGE_H */

commit c6557e7f2b6ae76a44653d38f835174074c42e05
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Fri Aug 1 20:42:05 2008 +0200

    [S390] move include/asm-s390 to arch/s390/include/asm
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/page.h b/arch/s390/include/asm/page.h
new file mode 100644
index 000000000000..991ba939408c
--- /dev/null
+++ b/arch/s390/include/asm/page.h
@@ -0,0 +1,155 @@
+/*
+ *  include/asm-s390/page.h
+ *
+ *  S390 version
+ *    Copyright (C) 1999,2000 IBM Deutschland Entwicklung GmbH, IBM Corporation
+ *    Author(s): Hartmut Penner (hp@de.ibm.com)
+ */
+
+#ifndef _S390_PAGE_H
+#define _S390_PAGE_H
+
+#include <linux/const.h>
+#include <asm/types.h>
+
+/* PAGE_SHIFT determines the page size */
+#define PAGE_SHIFT      12
+#define PAGE_SIZE	(_AC(1,UL) << PAGE_SHIFT)
+#define PAGE_MASK       (~(PAGE_SIZE-1))
+#define PAGE_DEFAULT_ACC	0
+#define PAGE_DEFAULT_KEY	(PAGE_DEFAULT_ACC << 4)
+
+#define HPAGE_SHIFT	20
+#define HPAGE_SIZE	(1UL << HPAGE_SHIFT)
+#define HPAGE_MASK	(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+
+#define ARCH_HAS_SETCLEAR_HUGE_PTE
+#define ARCH_HAS_HUGE_PTE_TYPE
+#define ARCH_HAS_PREPARE_HUGEPAGE
+#define ARCH_HAS_HUGEPAGE_CLEAR_FLUSH
+
+#include <asm/setup.h>
+#ifndef __ASSEMBLY__
+
+static inline void clear_page(void *page)
+{
+	if (MACHINE_HAS_PFMF) {
+		asm volatile(
+			"	.insn	rre,0xb9af0000,%0,%1"
+			: : "d" (0x10000), "a" (page) : "memory", "cc");
+	} else {
+		register unsigned long reg1 asm ("1") = 0;
+		register void *reg2 asm ("2") = page;
+		register unsigned long reg3 asm ("3") = 4096;
+		asm volatile(
+			"	mvcl	2,0"
+			: "+d" (reg2), "+d" (reg3) : "d" (reg1)
+			: "memory", "cc");
+	}
+}
+
+static inline void copy_page(void *to, void *from)
+{
+	if (MACHINE_HAS_MVPG) {
+		register unsigned long reg0 asm ("0") = 0;
+		asm volatile(
+			"	mvpg	%0,%1"
+			: : "a" (to), "a" (from), "d" (reg0)
+			: "memory", "cc");
+	} else
+		asm volatile(
+			"	mvc	0(256,%0),0(%1)\n"
+			"	mvc	256(256,%0),256(%1)\n"
+			"	mvc	512(256,%0),512(%1)\n"
+			"	mvc	768(256,%0),768(%1)\n"
+			"	mvc	1024(256,%0),1024(%1)\n"
+			"	mvc	1280(256,%0),1280(%1)\n"
+			"	mvc	1536(256,%0),1536(%1)\n"
+			"	mvc	1792(256,%0),1792(%1)\n"
+			"	mvc	2048(256,%0),2048(%1)\n"
+			"	mvc	2304(256,%0),2304(%1)\n"
+			"	mvc	2560(256,%0),2560(%1)\n"
+			"	mvc	2816(256,%0),2816(%1)\n"
+			"	mvc	3072(256,%0),3072(%1)\n"
+			"	mvc	3328(256,%0),3328(%1)\n"
+			"	mvc	3584(256,%0),3584(%1)\n"
+			"	mvc	3840(256,%0),3840(%1)\n"
+			: : "a" (to), "a" (from) : "memory");
+}
+
+#define clear_user_page(page, vaddr, pg)	clear_page(page)
+#define copy_user_page(to, from, vaddr, pg)	copy_page(to, from)
+
+#define __alloc_zeroed_user_highpage(movableflags, vma, vaddr) \
+	alloc_page_vma(GFP_HIGHUSER | __GFP_ZERO | movableflags, vma, vaddr)
+#define __HAVE_ARCH_ALLOC_ZEROED_USER_HIGHPAGE
+
+/*
+ * These are used to make use of C type-checking..
+ */
+
+typedef struct { unsigned long pgprot; } pgprot_t;
+typedef struct { unsigned long pte; } pte_t;
+typedef struct { unsigned long pmd; } pmd_t;
+typedef struct { unsigned long pud; } pud_t;
+typedef struct { unsigned long pgd; } pgd_t;
+typedef pte_t *pgtable_t;
+
+#define pgprot_val(x)	((x).pgprot)
+#define pte_val(x)	((x).pte)
+#define pmd_val(x)	((x).pmd)
+#define pud_val(x)	((x).pud)
+#define pgd_val(x)      ((x).pgd)
+
+#define __pte(x)        ((pte_t) { (x) } )
+#define __pmd(x)        ((pmd_t) { (x) } )
+#define __pgd(x)        ((pgd_t) { (x) } )
+#define __pgprot(x)     ((pgprot_t) { (x) } )
+
+/* default storage key used for all pages */
+extern unsigned int default_storage_key;
+
+static inline void
+page_set_storage_key(unsigned long addr, unsigned int skey)
+{
+	asm volatile("sske %0,%1" : : "d" (skey), "a" (addr));
+}
+
+static inline unsigned int
+page_get_storage_key(unsigned long addr)
+{
+	unsigned int skey;
+
+	asm volatile("iske %0,%1" : "=d" (skey) : "a" (addr), "0" (0));
+	return skey;
+}
+
+#ifdef CONFIG_PAGE_STATES
+
+struct page;
+void arch_free_page(struct page *page, int order);
+void arch_alloc_page(struct page *page, int order);
+
+#define HAVE_ARCH_FREE_PAGE
+#define HAVE_ARCH_ALLOC_PAGE
+
+#endif
+
+#endif /* !__ASSEMBLY__ */
+
+#define __PAGE_OFFSET           0x0UL
+#define PAGE_OFFSET             0x0UL
+#define __pa(x)                 (unsigned long)(x)
+#define __va(x)                 (void *)(unsigned long)(x)
+#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
+#define page_to_phys(page)	(page_to_pfn(page) << PAGE_SHIFT)
+#define virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+
+#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \
+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+#include <asm-generic/memory_model.h>
+#include <asm-generic/page.h>
+
+#endif /* _S390_PAGE_H */
