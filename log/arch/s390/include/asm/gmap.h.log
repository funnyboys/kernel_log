commit 1274800792dced8e5b6d54c71ec049c4d1e34189
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Mon Dec 16 10:48:11 2019 -0500

    KVM: s390/mm: Make pages accessible before destroying the guest
    
    Before we destroy the secure configuration, we better make all
    pages accessible again. This also happens during reboot, where we reboot
    into a non-secure guest that then can go again into secure mode. As
    this "new" secure guest will have a new ID we cannot reuse the old page
    state.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 6f9ff7a69fa2..a816fb4734b8 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -149,4 +149,5 @@ int gmap_mprotect_notify(struct gmap *, unsigned long start,
 void gmap_sync_dirty_log_pmd(struct gmap *gmap, unsigned long dirty_bitmap[4],
 			     unsigned long gaddr, unsigned long vmaddr);
 int gmap_mark_unmergeable(void);
+void s390_reset_acc(struct mm_struct *mm);
 #endif /* _ASM_S390_GMAP_H */

commit fa0c5eabbdd33012b369cf75d6a39389cc9ae707
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Tue Jul 16 13:08:37 2019 +0200

    KVM: s390: protvirt: Secure memory is not mergeable
    
    KSM will not work on secure pages, because when the kernel reads a
    secure page, it will be encrypted and hence no two pages will look the
    same.
    
    Let's mark the guest pages as unmergeable when we transition to secure
    mode.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 3c4926aa78f4..6f9ff7a69fa2 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -148,4 +148,5 @@ int gmap_mprotect_notify(struct gmap *, unsigned long start,
 
 void gmap_sync_dirty_log_pmd(struct gmap *gmap, unsigned long dirty_bitmap[4],
 			     unsigned long gaddr, unsigned long vmaddr);
+int gmap_mark_unmergeable(void);
 #endif /* _ASM_S390_GMAP_H */

commit 214d9bbcd3a67230b932f6cea83c078ab34d9e70
Author: Claudio Imbrenda <imbrenda@linux.ibm.com>
Date:   Tue Jan 21 09:48:44 2020 +0100

    s390/mm: provide memory management functions for protected KVM guests
    
    This provides the basic ultravisor calls and page table handling to cope
    with secure guests:
    - provide arch_make_page_accessible
    - make pages accessible after unmapping of secure guests
    - provide the ultravisor commands convert to/from secure
    - provide the ultravisor commands pin/unpin shared
    - provide callbacks to make pages secure (inacccessible)
     - we check for the expected pin count to only make pages secure if the
       host is not accessing them
     - we fence hugetlbfs for secure pages
    - add missing radix-tree include into gmap.h
    
    The basic idea is that a page can have 3 states: secure, normal or
    shared. The hypervisor can call into a firmware function called
    ultravisor that allows to change the state of a page: convert from/to
    secure. The convert from secure will encrypt the page and make it
    available to the host and host I/O. The convert to secure will remove
    the host capability to access this page.
    The design is that on convert to secure we will wait until writeback and
    page refs are indicating no host usage. At the same time the convert
    from secure (export to host) will be called in common code when the
    refcount or the writeback bit is already set. This avoids races between
    convert from and to secure.
    
    Then there is also the concept of shared pages. Those are kind of secure
    where the host can still access those pages. We need to be notified when
    the guest "unshares" such a page, basically doing a convert to secure by
    then. There is a call "pin shared page" that we use instead of convert
    from secure when possible.
    
    We do use PG_arch_1 as an optimization to minimize the convert from
    secure/pin shared.
    
    Several comments have been added in the code to explain the logic in
    the relevant places.
    
    Co-developed-by: Ulrich Weigand <Ulrich.Weigand@de.ibm.com>
    Signed-off-by: Ulrich Weigand <Ulrich.Weigand@de.ibm.com>
    Signed-off-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
    Acked-by: David Hildenbrand <david@redhat.com>
    Acked-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    [borntraeger@de.ibm.com: patch merging, splitting, fixing]
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 37f96b6f0e61..3c4926aa78f4 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -9,6 +9,7 @@
 #ifndef _ASM_S390_GMAP_H
 #define _ASM_S390_GMAP_H
 
+#include <linux/radix-tree.h>
 #include <linux/refcount.h>
 
 /* Generic bits for GMAP notification on DAT table entry changes. */
@@ -31,6 +32,7 @@
  * @table: pointer to the page directory
  * @asce: address space control element for gmap page table
  * @pfault_enabled: defines if pfaults are applicable for the guest
+ * @guest_handle: protected virtual machine handle for the ultravisor
  * @host_to_rmap: radix tree with gmap_rmap lists
  * @children: list of shadow gmap structures
  * @pt_list: list of all page tables used in the shadow guest address space
@@ -54,6 +56,8 @@ struct gmap {
 	unsigned long asce_end;
 	void *private;
 	bool pfault_enabled;
+	/* only set for protected virtual machines */
+	unsigned long guest_handle;
 	/* Additional data for shadow guest address spaces */
 	struct radix_tree_root host_to_rmap;
 	struct list_head children;

commit 40e90656c13587c0826e8afeef3cebe585db2d2c
Author: Chuhong Yuan <hslester96@gmail.com>
Date:   Thu Aug 8 15:18:26 2019 +0800

    s390/mm: use refcount_t for refcount
    
    Reference counters are preferred to use refcount_t instead of
    atomic_t.
    This is because the implementation of refcount_t can prevent
    overflows and detect possible use-after-free.
    So convert atomic_t ref counters to refcount_t.
    
    Link: http://lkml.kernel.org/r/20190808071826.6649-1-hslester96@gmail.com
    Signed-off-by: Chuhong Yuan <hslester96@gmail.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Vasily Gorbik <gor@linux.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index fcbd638fb9f4..37f96b6f0e61 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -9,6 +9,8 @@
 #ifndef _ASM_S390_GMAP_H
 #define _ASM_S390_GMAP_H
 
+#include <linux/refcount.h>
+
 /* Generic bits for GMAP notification on DAT table entry changes. */
 #define GMAP_NOTIFY_SHADOW	0x2
 #define GMAP_NOTIFY_MPROT	0x1
@@ -46,7 +48,7 @@ struct gmap {
 	struct radix_tree_root guest_to_host;
 	struct radix_tree_root host_to_guest;
 	spinlock_t guest_table_lock;
-	atomic_t ref_count;
+	refcount_t ref_count;
 	unsigned long *table;
 	unsigned long asce;
 	unsigned long asce_end;

commit 0959e168678d2d95648317e1e5e46bcb358272eb
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Tue Jul 17 13:21:22 2018 +0100

    s390/mm: Add huge page dirty sync support
    
    To do dirty loging with huge pages, we protect huge pmds in the
    gmap. When they are written to, we unprotect them and mark them dirty.
    
    We introduce the function gmap_test_and_clear_dirty_pmd which handles
    dirty sync for huge pages.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>
    Acked-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 276268b48aff..fcbd638fb9f4 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -15,6 +15,7 @@
 
 /* Status bits only for huge segment entries */
 #define _SEGMENT_ENTRY_GMAP_IN		0x8000	/* invalidation notify bit */
+#define _SEGMENT_ENTRY_GMAP_UC		0x4000	/* dirty (migration) */
 
 /**
  * struct gmap_struct - guest address space
@@ -139,4 +140,6 @@ void gmap_pte_notify(struct mm_struct *, unsigned long addr, pte_t *,
 int gmap_mprotect_notify(struct gmap *, unsigned long start,
 			 unsigned long len, int prot);
 
+void gmap_sync_dirty_log_pmd(struct gmap *gmap, unsigned long dirty_bitmap[4],
+			     unsigned long gaddr, unsigned long vmaddr);
 #endif /* _ASM_S390_GMAP_H */

commit 7c4b13a7c042fd6b71dc48d291208f8a97fad333
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jul 13 11:28:21 2018 +0100

    s390/mm: Add gmap pmd notification bit setting
    
    Like for ptes, we also need invalidation notification for pmds, to
    make sure the guest lowcore pages are always accessible and later
    addition of shadowed pmds.
    
    With PMDs we do not have PGSTEs or some other bits we could use in the
    host PMD. Instead we pick one of the free bits in the gmap PMD. Every
    time a host pmd will be invalidated, we will check if the respective
    gmap PMD has the bit set and in that case fire up the notifier.
    
    Signed-off-by: Janosch Frank <frankja@linux.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index c1bc5633fc6e..276268b48aff 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -13,6 +13,9 @@
 #define GMAP_NOTIFY_SHADOW	0x2
 #define GMAP_NOTIFY_MPROT	0x1
 
+/* Status bits only for huge segment entries */
+#define _SEGMENT_ENTRY_GMAP_IN		0x8000	/* invalidation notify bit */
+
 /**
  * struct gmap_struct - guest address space
  * @list: list head for the mm->context gmap list

commit 2c46e974dd8b5316e65637af0ff6d4bc78554b2e
Author: Janosch Frank <frankja@linux.ibm.com>
Date:   Fri Jul 13 11:28:18 2018 +0100

    s390/mm: Abstract gmap notify bit setting
    
    Currently we use the software PGSTE bits PGSTE_IN_BIT and
    PGSTE_VSIE_BIT to notify before an invalidation occurs on a prefix
    page or a VSIE page respectively. Both bits are pgste specific, but
    are used when protecting a memory range.
    
    Let's introduce abstract GMAP_NOTIFY_* bits that will be realized into
    the respective bits when gmap DAT table entries are protected.
    
    Signed-off-by: Janosch Frank <frankja@linux.vnet.ibm.com>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index e07cce88dfb0..c1bc5633fc6e 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -9,6 +9,10 @@
 #ifndef _ASM_S390_GMAP_H
 #define _ASM_S390_GMAP_H
 
+/* Generic bits for GMAP notification on DAT table entry changes. */
+#define GMAP_NOTIFY_SHADOW	0x2
+#define GMAP_NOTIFY_MPROT	0x1
+
 /**
  * struct gmap_struct - guest address space
  * @list: list head for the mm->context gmap list

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 741ddba0bf11..e07cce88dfb0 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 /*
  *  KVM guest address space mapping code
  *

commit 37d9df98b71afdf3baf41ee5451b6206c13328c6
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Wed Mar 11 16:47:33 2015 +0100

    KVM: s390: backup the currently enabled gmap when scheduled out
    
    Nested virtualization will have to enable own gmaps. Current code
    would enable the wrong gmap whenever scheduled out and back in,
    therefore resulting in the wrong gmap being enabled.
    
    This patch reenables the last enabled gmap, therefore avoiding having to
    touch vcpu->arch.gmap when enabling a different gmap.
    
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index c67fb854705e..741ddba0bf11 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -94,6 +94,7 @@ void gmap_put(struct gmap *gmap);
 
 void gmap_enable(struct gmap *gmap);
 void gmap_disable(struct gmap *gmap);
+struct gmap *gmap_get_enabled(void);
 int gmap_map_segment(struct gmap *gmap, unsigned long from,
 		     unsigned long to, unsigned long len);
 int gmap_unmap_segment(struct gmap *gmap, unsigned long to, unsigned long len);

commit 5b6c963bcef5c3a857e3f8ba84aa9380069fc95f
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Fri May 27 18:57:33 2016 +0200

    s390/mm: allow to check if a gmap shadow is valid
    
    It will be very helpful to have a mechanism to check without any locks
    if a given gmap shadow is still valid and matches the given properties.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 752cf47a81ab..c67fb854705e 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -109,6 +109,7 @@ int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val);
 
 struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,
 			 int edat_level);
+int gmap_shadow_valid(struct gmap *sg, unsigned long asce, int edat_level);
 int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,
 		    int fake);
 int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,

commit 3218f7094b6b583f4f01bffcf84572c6beacdcc2
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 18 16:22:24 2016 +0200

    s390/mm: support real-space for gmap shadows
    
    We can easily support real-space designation just like EDAT1 and EDAT2.
    So guest2 can provide for guest3 an asce with the real-space control being
    set.
    
    We simply have to allocate the biggest page table possible and fake all
    levels.
    
    There is no protection to consider. If we exceed guest memory, vsie code
    will inject an addressing exception (via program intercept). In the future,
    we could limit the fake table level to the gmap page table.
    
    As the top level page table can never go away, such gmap shadows will never
    get unshadowed, we'll have to come up with another way to limit the number
    of kept gmap shadows.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 2e4c3b222a96..752cf47a81ab 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -109,8 +109,10 @@ int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val);
 
 struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,
 			 int edat_level);
-int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t);
-int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t);
+int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,
+		    int fake);
+int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,
+		    int fake);
 int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,
 		    int fake);
 int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,

commit 18b89809881834cecd2977e6048a30c4c8f140fe
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 18 13:42:05 2016 +0200

    s390/mm: support EDAT2 for gmap shadows
    
    If the guest is enabled for EDAT2, we can easily create shadows for
    guest2 -> guest3 provided tables that make use of EDAT2.
    
    If guest2 references a 2GB page, this memory looks consecutive for guest2,
    but it does not have to be so for us. Therefore we have to create fake
    segment and page tables.
    
    This works just like EDAT1 support, so page tables are removed when the
    parent table (r3t table entry) is changed.
    
    We don't hve to care about:
    - ACCF-Validity Control in RTTE
    - Access-Control Bits in RTTE
    - Fetch-Protection Bit in RTTE
    - Common-Region Bit in RTTE
    
    Just like for EDAT1, all bits might be dropped and there is no guaranteed
    that they are active.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index c8ba5a197b1d..2e4c3b222a96 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -111,7 +111,8 @@ struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,
 			 int edat_level);
 int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t);
 int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t);
-int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt);
+int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,
+		    int fake);
 int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,
 		    int fake);
 int gmap_shadow_pgt_lookup(struct gmap *sg, unsigned long saddr,

commit fd8d4e3ab6993e194287a59c4d3a6a43da86b8dc
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Mon Apr 18 13:24:52 2016 +0200

    s390/mm: support EDAT1 for gmap shadows
    
    If the guest is enabled for EDAT1, we can easily create shadows for
    guest2 -> guest3 provided tables that make use of EDAT1.
    
    If guest2 references a 1MB page, this memory looks consecutive for guest2,
    but it might not be so for us. Therefore we have to create fake page tables.
    
    We can easily add that to our existing infrastructure. The invalidation
    mechanism will make sure that fake page tables are removed when the parent
    table (sgt table entry) is changed.
    
    As EDAT1 also introduced protection on all page table levels, we have to
    also shadow these correctly.
    
    We don't have to care about:
    - ACCF-Validity Control in STE
    - Access-Control Bits in STE
    - Fetch-Protection Bit in STE
    - Common-Segment Bit in STE
    
    As all bits might be dropped and there is no guaranteed that they are
    active ("unpredictable whether the CPU uses these bits", "may be used").
    Without using EDAT1 in the shadow ourselfes (STE-format control == 0),
    simply shadowing these bits would not be enough. They would be ignored.
    
    Please note that we are using the "fake" flag to make this look consistent
    with further changes (EDAT2, real-space designation support) and don't let
    the shadow functions handle fc=1 stes.
    
    In the future, with huge pages in the host, gmap_shadow_pgt() could simply
    try to map a huge host page if "fake" is set to one and indicate via return
    value that no lower fake tables / shadow ptes are required.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 2ab397c8ca09..c8ba5a197b1d 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -112,9 +112,10 @@ struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,
 int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t);
 int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t);
 int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt);
-int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt);
+int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,
+		    int fake);
 int gmap_shadow_pgt_lookup(struct gmap *sg, unsigned long saddr,
-			   unsigned long *pgt, int *dat_protection);
+			   unsigned long *pgt, int *dat_protection, int *fake);
 int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte);
 
 void gmap_register_pte_notifier(struct gmap_notifier *);

commit 5b062bd4940f81e0bd26b0d75f56d7abebf0309f
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Mar 8 12:17:40 2016 +0100

    s390/mm: prepare for EDAT1/EDAT2 support in gmap shadow
    
    In preparation for EDAT1/EDAT2 support for gmap shadows, we have to store
    the requested edat level in the gmap shadow.
    
    The edat level used during shadow translation is a property of the gmap
    shadow. Depending on that level, the gmap shadow will look differently for
    the same guest tables. We have to store it internally in order to support
    it later.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 54a2487efce4..2ab397c8ca09 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -26,6 +26,7 @@
  * @shadow_lock: spinlock to protect the shadow gmap list
  * @parent: pointer to the parent gmap for shadow guest address spaces
  * @orig_asce: ASCE for which the shadow page table has been created
+ * @edat_level: edat level to be used for the shadow translation
  * @removed: flag to indicate if a shadow guest address space has been removed
  * @initialized: flag to indicate if a shadow guest address space can be used
  */
@@ -49,6 +50,7 @@ struct gmap {
 	spinlock_t shadow_lock;
 	struct gmap *parent;
 	unsigned long orig_asce;
+	int edat_level;
 	bool removed;
 	bool initialized;
 };
@@ -105,7 +107,8 @@ void gmap_unlink(struct mm_struct *, unsigned long *table, unsigned long vmaddr)
 
 int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val);
 
-struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce);
+struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,
+			 int edat_level);
 int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t);
 int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t);
 int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt);

commit 0f7f84891516dc1ff7500fae12143710d2d9d11f
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Mar 8 12:30:46 2016 +0100

    s390/mm: fix races on gmap_shadow creation
    
    Before any thread is allowed to use a gmap_shadow, it has to be fully
    initialized. However, for invalidation to work properly, we have to
    register the new gmap_shadow before we protect the parent gmap table.
    
    Because locking is tricky, and we have to avoid duplicate gmaps, let's
    introduce an initialized field, that signalizes other threads if that
    gmap_shadow can already be used or if they have to retry.
    
    Let's properly return errors using ERR_PTR() instead of simply returning
    NULL, so a caller can properly react on the error.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 4a47055f58d7..54a2487efce4 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -27,6 +27,7 @@
  * @parent: pointer to the parent gmap for shadow guest address spaces
  * @orig_asce: ASCE for which the shadow page table has been created
  * @removed: flag to indicate if a shadow guest address space has been removed
+ * @initialized: flag to indicate if a shadow guest address space can be used
  */
 struct gmap {
 	struct list_head list;
@@ -49,6 +50,7 @@ struct gmap {
 	struct gmap *parent;
 	unsigned long orig_asce;
 	bool removed;
+	bool initialized;
 };
 
 /**

commit a9d23e71d7716e394a772686bfd994f4e181b235
Author: David Hildenbrand <dahi@linux.vnet.ibm.com>
Date:   Tue Mar 8 12:21:41 2016 +0100

    s390/mm: shadow pages with real guest requested protection
    
    We really want to avoid manually handling protection for nested
    virtualization. By shadowing pages with the protection the guest asked us
    for, the SIE can handle most protection-related actions for us (e.g.
    special handling for MVPG) and we can directly forward protection
    exceptions to the guest.
    
    PTEs will now always be shadowed with the correct _PAGE_PROTECT flag.
    Unshadowing will take care of any guest changes to the parent PTE and
    any host changes to the host PTE. If the host PTE doesn't have the
    fitting access rights or is not available, we have to fix it up.
    
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 58e65ee5b2d2..4a47055f58d7 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -110,8 +110,7 @@ int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt);
 int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt);
 int gmap_shadow_pgt_lookup(struct gmap *sg, unsigned long saddr,
 			   unsigned long *pgt, int *dat_protection);
-int gmap_shadow_page(struct gmap *sg, unsigned long saddr,
-		     unsigned long paddr, int write);
+int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte);
 
 void gmap_register_pte_notifier(struct gmap_notifier *);
 void gmap_unregister_pte_notifier(struct gmap_notifier *);

commit 4be130a08420d6918d80c1067f8078f425eb98df
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 12:12:18 2016 +0100

    s390/mm: add shadow gmap support
    
    For a nested KVM guest the outer KVM host needs to create shadow
    page tables for the nested guest. This patch adds the basic support
    to the guest address space (gmap) code.
    
    For each guest address space the inner KVM host creates, the first
    outer KVM host needs to create shadow page tables. The address space
    is identified by the ASCE loaded into the control register 1 at the
    time the inner SIE instruction for the second nested KVM guest is
    executed. The outer KVM host creates the shadow tables starting with
    the table identified by the ASCE on a on-demand basis. The outer KVM
    host will get repeated faults for all the shadow tables needed to
    run the second KVM guest.
    
    While a shadow page table for the second KVM guest is active the access
    to the origin region, segment and page tables needs to be restricted
    for the first KVM guest. For region and segment and page tables the first
    KVM guest may read the memory, but write attempt has to lead to an
    unshadow.  This is done using the page invalid and read-only bits in the
    page table of the first KVM guest. If the first guest re-accesses one of
    the origin pages of a shadow, it gets a fault and the affected parts of
    the shadow page table hierarchy needs to be removed again.
    
    PGSTE tables don't have to be shadowed, as all interpretation assist can't
    deal with the invalid bits in the shadow pte being set differently than
    the original ones provided by the first KVM guest.
    
    Many bug fixes and improvements by David Hildenbrand.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index e69853ce55da..58e65ee5b2d2 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -10,6 +10,7 @@
 
 /**
  * struct gmap_struct - guest address space
+ * @list: list head for the mm->context gmap list
  * @crst_list: list of all crst tables used in the guest address space
  * @mm: pointer to the parent mm_struct
  * @guest_to_host: radix tree with guest to host address translation
@@ -19,6 +20,13 @@
  * @table: pointer to the page directory
  * @asce: address space control element for gmap page table
  * @pfault_enabled: defines if pfaults are applicable for the guest
+ * @host_to_rmap: radix tree with gmap_rmap lists
+ * @children: list of shadow gmap structures
+ * @pt_list: list of all page tables used in the shadow guest address space
+ * @shadow_lock: spinlock to protect the shadow gmap list
+ * @parent: pointer to the parent gmap for shadow guest address spaces
+ * @orig_asce: ASCE for which the shadow page table has been created
+ * @removed: flag to indicate if a shadow guest address space has been removed
  */
 struct gmap {
 	struct list_head list;
@@ -33,8 +41,32 @@ struct gmap {
 	unsigned long asce_end;
 	void *private;
 	bool pfault_enabled;
+	/* Additional data for shadow guest address spaces */
+	struct radix_tree_root host_to_rmap;
+	struct list_head children;
+	struct list_head pt_list;
+	spinlock_t shadow_lock;
+	struct gmap *parent;
+	unsigned long orig_asce;
+	bool removed;
 };
 
+/**
+ * struct gmap_rmap - reverse mapping for shadow page table entries
+ * @next: pointer to next rmap in the list
+ * @raddr: virtual rmap address in the shadow guest address space
+ */
+struct gmap_rmap {
+	struct gmap_rmap *next;
+	unsigned long raddr;
+};
+
+#define gmap_for_each_rmap(pos, head) \
+	for (pos = (head); pos; pos = pos->next)
+
+#define gmap_for_each_rmap_safe(pos, n, head) \
+	for (pos = (head); n = pos ? pos->next : NULL, pos; pos = n)
+
 /**
  * struct gmap_notifier - notify function block for page invalidation
  * @notifier_call: address of callback function
@@ -46,6 +78,11 @@ struct gmap_notifier {
 			      unsigned long end);
 };
 
+static inline int gmap_is_shadow(struct gmap *gmap)
+{
+	return !!gmap->parent;
+}
+
 struct gmap *gmap_create(struct mm_struct *mm, unsigned long limit);
 void gmap_remove(struct gmap *gmap);
 struct gmap *gmap_get(struct gmap *gmap);
@@ -64,9 +101,22 @@ void gmap_discard(struct gmap *, unsigned long from, unsigned long to);
 void __gmap_zap(struct gmap *, unsigned long gaddr);
 void gmap_unlink(struct mm_struct *, unsigned long *table, unsigned long vmaddr);
 
+int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val);
+
+struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce);
+int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t);
+int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t);
+int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt);
+int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt);
+int gmap_shadow_pgt_lookup(struct gmap *sg, unsigned long saddr,
+			   unsigned long *pgt, int *dat_protection);
+int gmap_shadow_page(struct gmap *sg, unsigned long saddr,
+		     unsigned long paddr, int write);
+
 void gmap_register_pte_notifier(struct gmap_notifier *);
 void gmap_unregister_pte_notifier(struct gmap_notifier *);
-void gmap_pte_notify(struct mm_struct *, unsigned long addr, pte_t *);
+void gmap_pte_notify(struct mm_struct *, unsigned long addr, pte_t *,
+		     unsigned long bits);
 
 int gmap_mprotect_notify(struct gmap *, unsigned long start,
 			 unsigned long len, int prot);

commit 6ea427bbbd4078297bb1dbd6c5cb83f3f48aac46
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:55:04 2016 +0100

    s390/mm: add reference counter to gmap structure
    
    Let's use a reference counter mechanism to control the lifetime of
    gmap structures. This will be needed for further changes related to
    gmap shadows.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 6897a0919446..e69853ce55da 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -15,6 +15,7 @@
  * @guest_to_host: radix tree with guest to host address translation
  * @host_to_guest: radix tree with pointer to segment table entries
  * @guest_table_lock: spinlock to protect all entries in the guest page table
+ * @ref_count: reference counter for the gmap structure
  * @table: pointer to the page directory
  * @asce: address space control element for gmap page table
  * @pfault_enabled: defines if pfaults are applicable for the guest
@@ -26,6 +27,7 @@ struct gmap {
 	struct radix_tree_root guest_to_host;
 	struct radix_tree_root host_to_guest;
 	spinlock_t guest_table_lock;
+	atomic_t ref_count;
 	unsigned long *table;
 	unsigned long asce;
 	unsigned long asce_end;
@@ -44,8 +46,11 @@ struct gmap_notifier {
 			      unsigned long end);
 };
 
-struct gmap *gmap_alloc(struct mm_struct *mm, unsigned long limit);
-void gmap_free(struct gmap *gmap);
+struct gmap *gmap_create(struct mm_struct *mm, unsigned long limit);
+void gmap_remove(struct gmap *gmap);
+struct gmap *gmap_get(struct gmap *gmap);
+void gmap_put(struct gmap *gmap);
+
 void gmap_enable(struct gmap *gmap);
 void gmap_disable(struct gmap *gmap);
 int gmap_map_segment(struct gmap *gmap, unsigned long from,

commit b2d73b2a0ad1c758cb0c1acb01a911744b845942
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:54:42 2016 +0100

    s390/mm: extended gmap pte notifier
    
    The current gmap pte notifier forces a pte into to a read-write state.
    If the pte is invalidated the gmap notifier is called to inform KVM
    that the mapping will go away.
    
    Extend this approach to allow read-write, read-only and no-access
    as possible target states and call the pte notifier for any change
    to the pte.
    
    This mechanism is used to temporarily set specific access rights for
    a pte without doing the heavy work of a true mprotect call.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index 2cf49624af99..6897a0919446 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -59,8 +59,11 @@ void gmap_discard(struct gmap *, unsigned long from, unsigned long to);
 void __gmap_zap(struct gmap *, unsigned long gaddr);
 void gmap_unlink(struct mm_struct *, unsigned long *table, unsigned long vmaddr);
 
-void gmap_register_ipte_notifier(struct gmap_notifier *);
-void gmap_unregister_ipte_notifier(struct gmap_notifier *);
-int gmap_ipte_notify(struct gmap *, unsigned long start, unsigned long len);
+void gmap_register_pte_notifier(struct gmap_notifier *);
+void gmap_unregister_pte_notifier(struct gmap_notifier *);
+void gmap_pte_notify(struct mm_struct *, unsigned long addr, pte_t *);
+
+int gmap_mprotect_notify(struct gmap *, unsigned long start,
+			 unsigned long len, int prot);
 
 #endif /* _ASM_S390_GMAP_H */

commit 8ecb1a59d6c6674bc98e4eee0c2482490748e21a
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:54:14 2016 +0100

    s390/mm: use RCU for gmap notifier list and the per-mm gmap list
    
    The gmap notifier list and the gmap list in the mm_struct change rarely.
    Use RCU to optimize the reader of these lists.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index bc0eadf9ed8e..2cf49624af99 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -39,6 +39,7 @@ struct gmap {
  */
 struct gmap_notifier {
 	struct list_head list;
+	struct rcu_head rcu;
 	void (*notifier_call)(struct gmap *gmap, unsigned long start,
 			      unsigned long end);
 };

commit 414d3b07496604a4372466a6b474ca24291a143c
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:52:54 2016 +0100

    s390/kvm: page table invalidation notifier
    
    Pass an address range to the page table invalidation notifier
    for KVM. This allows to notify changes that affect a larger
    virtual memory area, e.g. for 1MB pages.
    
    Reviewed-by: David Hildenbrand <dahi@linux.vnet.ibm.com>
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
index d054c1b07a3c..bc0eadf9ed8e 100644
--- a/arch/s390/include/asm/gmap.h
+++ b/arch/s390/include/asm/gmap.h
@@ -39,7 +39,8 @@ struct gmap {
  */
 struct gmap_notifier {
 	struct list_head list;
-	void (*notifier_call)(struct gmap *gmap, unsigned long gaddr);
+	void (*notifier_call)(struct gmap *gmap, unsigned long start,
+			      unsigned long end);
 };
 
 struct gmap *gmap_alloc(struct mm_struct *mm, unsigned long limit);

commit 1e133ab296f3ff8d9e58a5e758291ed39ba72ad7
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Tue Mar 8 11:49:57 2016 +0100

    s390/mm: split arch/s390/mm/pgtable.c
    
    The pgtable.c file is quite big, before it grows any larger split it
    into pgtable.c, pgalloc.c and gmap.c. In addition move the gmap related
    header definitions into the new gmap.h header and all of the pgste
    helpers from pgtable.h to pgtable.c.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h
new file mode 100644
index 000000000000..d054c1b07a3c
--- /dev/null
+++ b/arch/s390/include/asm/gmap.h
@@ -0,0 +1,64 @@
+/*
+ *  KVM guest address space mapping code
+ *
+ *    Copyright IBM Corp. 2007, 2016
+ *    Author(s): Martin Schwidefsky <schwidefsky@de.ibm.com>
+ */
+
+#ifndef _ASM_S390_GMAP_H
+#define _ASM_S390_GMAP_H
+
+/**
+ * struct gmap_struct - guest address space
+ * @crst_list: list of all crst tables used in the guest address space
+ * @mm: pointer to the parent mm_struct
+ * @guest_to_host: radix tree with guest to host address translation
+ * @host_to_guest: radix tree with pointer to segment table entries
+ * @guest_table_lock: spinlock to protect all entries in the guest page table
+ * @table: pointer to the page directory
+ * @asce: address space control element for gmap page table
+ * @pfault_enabled: defines if pfaults are applicable for the guest
+ */
+struct gmap {
+	struct list_head list;
+	struct list_head crst_list;
+	struct mm_struct *mm;
+	struct radix_tree_root guest_to_host;
+	struct radix_tree_root host_to_guest;
+	spinlock_t guest_table_lock;
+	unsigned long *table;
+	unsigned long asce;
+	unsigned long asce_end;
+	void *private;
+	bool pfault_enabled;
+};
+
+/**
+ * struct gmap_notifier - notify function block for page invalidation
+ * @notifier_call: address of callback function
+ */
+struct gmap_notifier {
+	struct list_head list;
+	void (*notifier_call)(struct gmap *gmap, unsigned long gaddr);
+};
+
+struct gmap *gmap_alloc(struct mm_struct *mm, unsigned long limit);
+void gmap_free(struct gmap *gmap);
+void gmap_enable(struct gmap *gmap);
+void gmap_disable(struct gmap *gmap);
+int gmap_map_segment(struct gmap *gmap, unsigned long from,
+		     unsigned long to, unsigned long len);
+int gmap_unmap_segment(struct gmap *gmap, unsigned long to, unsigned long len);
+unsigned long __gmap_translate(struct gmap *, unsigned long gaddr);
+unsigned long gmap_translate(struct gmap *, unsigned long gaddr);
+int __gmap_link(struct gmap *gmap, unsigned long gaddr, unsigned long vmaddr);
+int gmap_fault(struct gmap *, unsigned long gaddr, unsigned int fault_flags);
+void gmap_discard(struct gmap *, unsigned long from, unsigned long to);
+void __gmap_zap(struct gmap *, unsigned long gaddr);
+void gmap_unlink(struct mm_struct *, unsigned long *table, unsigned long vmaddr);
+
+void gmap_register_ipte_notifier(struct gmap_notifier *);
+void gmap_unregister_ipte_notifier(struct gmap_notifier *);
+int gmap_ipte_notify(struct gmap *, unsigned long start, unsigned long len);
+
+#endif /* _ASM_S390_GMAP_H */
