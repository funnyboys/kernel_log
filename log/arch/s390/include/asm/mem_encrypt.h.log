commit 5cbdaeefb655072d304744812708b3f3a31c6b51
Author: Thiago Jung Bauermann <bauerman@linux.ibm.com>
Date:   Tue Aug 6 01:49:19 2019 -0300

    s390/mm: Remove sev_active() function
    
    All references to sev_active() were moved to arch/x86 so we don't need to
    define it for s390 anymore.
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.ibm.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Halil Pasic <pasic@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190806044919.10622-7-bauerman@linux.ibm.com

diff --git a/arch/s390/include/asm/mem_encrypt.h b/arch/s390/include/asm/mem_encrypt.h
index ff813a56bc30..2542cbf7e2d1 100644
--- a/arch/s390/include/asm/mem_encrypt.h
+++ b/arch/s390/include/asm/mem_encrypt.h
@@ -5,7 +5,6 @@
 #ifndef __ASSEMBLY__
 
 static inline bool mem_encrypt_active(void) { return false; }
-extern bool sev_active(void);
 
 int set_memory_encrypted(unsigned long addr, int numpages);
 int set_memory_decrypted(unsigned long addr, int numpages);

commit 284e21fab2cfcf90dacce565e0b12f29e5df00c1
Author: Thiago Jung Bauermann <bauerman@linux.ibm.com>
Date:   Tue Aug 6 01:49:17 2019 -0300

    x86, s390/mm: Move sme_active() and sme_me_mask to x86-specific header
    
    Now that generic code doesn't reference them, move sme_active() and
    sme_me_mask to x86's <asm/mem_encrypt.h>.
    
    Also remove the export for sme_active() since it's only used in files that
    won't be built as modules. sme_me_mask on the other hand is used in
    arch/x86/kvm/svm.c (via __sme_set() and __psp_pa()) which can be built as a
    module so its export needs to stay.
    
    Signed-off-by: Thiago Jung Bauermann <bauerman@linux.ibm.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Tom Lendacky <thomas.lendacky@amd.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190806044919.10622-5-bauerman@linux.ibm.com

diff --git a/arch/s390/include/asm/mem_encrypt.h b/arch/s390/include/asm/mem_encrypt.h
index 3eb018508190..ff813a56bc30 100644
--- a/arch/s390/include/asm/mem_encrypt.h
+++ b/arch/s390/include/asm/mem_encrypt.h
@@ -4,9 +4,7 @@
 
 #ifndef __ASSEMBLY__
 
-#define sme_me_mask	0ULL
-
-static inline bool sme_active(void) { return false; }
+static inline bool mem_encrypt_active(void) { return false; }
 extern bool sev_active(void);
 
 int set_memory_encrypted(unsigned long addr, int numpages);

commit 64e1f0c531d1072cd97939bf0d8df42b26713543
Author: Halil Pasic <pasic@linux.ibm.com>
Date:   Thu Sep 13 18:57:16 2018 +0200

    s390/mm: force swiotlb for protected virtualization
    
    On s390, protected virtualization guests have to use bounced I/O
    buffers.  That requires some plumbing.
    
    Let us make sure, any device that uses DMA API with direct ops correctly
    is spared from the problems, that a hypervisor attempting I/O to a
    non-shared page would bring.
    
    Signed-off-by: Halil Pasic <pasic@linux.ibm.com>
    Reviewed-by: Claudio Imbrenda <imbrenda@linux.ibm.com>
    Reviewed-by: Michael Mueller <mimu@linux.ibm.com>
    Tested-by: Michael Mueller <mimu@linux.ibm.com>
    Signed-off-by: Heiko Carstens <heiko.carstens@de.ibm.com>

diff --git a/arch/s390/include/asm/mem_encrypt.h b/arch/s390/include/asm/mem_encrypt.h
new file mode 100644
index 000000000000..3eb018508190
--- /dev/null
+++ b/arch/s390/include/asm/mem_encrypt.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef S390_MEM_ENCRYPT_H__
+#define S390_MEM_ENCRYPT_H__
+
+#ifndef __ASSEMBLY__
+
+#define sme_me_mask	0ULL
+
+static inline bool sme_active(void) { return false; }
+extern bool sev_active(void);
+
+int set_memory_encrypted(unsigned long addr, int numpages);
+int set_memory_decrypted(unsigned long addr, int numpages);
+
+#endif	/* __ASSEMBLY__ */
+
+#endif	/* S390_MEM_ENCRYPT_H__ */
